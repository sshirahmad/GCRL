2022-11-20 13:32:10,045:INFO: Initializing Training Set
2022-11-20 13:32:11,512:INFO: Initializing Validation Set
2022-11-20 13:32:11,688:INFO: Initializing Validation O Set
2022-11-20 13:32:13,630:INFO: model ./models/E1//P6/CRMF_epoch_1345.pth.tar not found
2022-11-20 13:32:13,631:INFO: 
===> EPOCH: 401 (P4)
2022-11-20 13:32:13,631:INFO: - Computing loss (training)
2022-11-20 13:32:14,172:INFO: Dataset: hotel               Batch: 1/4	Loss 85.0305 (85.0305)
2022-11-20 13:32:14,529:INFO: Dataset: hotel               Batch: 2/4	Loss 86.6572 (85.8439)
2022-11-20 13:32:14,893:INFO: Dataset: hotel               Batch: 3/4	Loss 85.3121 (85.6661)
2022-11-20 13:32:15,225:INFO: Dataset: hotel               Batch: 4/4	Loss 52.6167 (80.1288)
2022-11-20 13:32:15,870:INFO: Dataset: univ                Batch:  1/15	Loss 85.3969 (85.3969)
2022-11-20 13:32:16,320:INFO: Dataset: univ                Batch:  2/15	Loss 86.4420 (85.9021)
2022-11-20 13:32:16,756:INFO: Dataset: univ                Batch:  3/15	Loss 87.3536 (86.3490)
2022-11-20 13:32:17,191:INFO: Dataset: univ                Batch:  4/15	Loss 86.6405 (86.4206)
2022-11-20 13:32:17,624:INFO: Dataset: univ                Batch:  5/15	Loss 88.4401 (86.8050)
2022-11-20 13:32:18,056:INFO: Dataset: univ                Batch:  6/15	Loss 86.5108 (86.7566)
2022-11-20 13:32:18,483:INFO: Dataset: univ                Batch:  7/15	Loss 89.3675 (87.1168)
2022-11-20 13:32:18,912:INFO: Dataset: univ                Batch:  8/15	Loss 86.6579 (87.0615)
2022-11-20 13:32:19,347:INFO: Dataset: univ                Batch:  9/15	Loss 89.9871 (87.3741)
2022-11-20 13:32:19,895:INFO: Dataset: univ                Batch: 10/15	Loss 86.3197 (87.2702)
2022-11-20 13:32:20,405:INFO: Dataset: univ                Batch: 11/15	Loss 87.2490 (87.2682)
2022-11-20 13:32:20,878:INFO: Dataset: univ                Batch: 12/15	Loss 88.0953 (87.3319)
2022-11-20 13:32:21,330:INFO: Dataset: univ                Batch: 13/15	Loss 85.9875 (87.2213)
2022-11-20 13:32:21,764:INFO: Dataset: univ                Batch: 14/15	Loss 88.8215 (87.3325)
2022-11-20 13:32:22,068:INFO: Dataset: univ                Batch: 15/15	Loss 16.1477 (86.2699)
2022-11-20 13:32:22,619:INFO: Dataset: zara1               Batch: 1/8	Loss 95.0269 (95.0269)
2022-11-20 13:32:22,982:INFO: Dataset: zara1               Batch: 2/8	Loss 95.6412 (95.3467)
2022-11-20 13:32:23,345:INFO: Dataset: zara1               Batch: 3/8	Loss 95.9920 (95.5487)
2022-11-20 13:32:23,715:INFO: Dataset: zara1               Batch: 4/8	Loss 94.1404 (95.1977)
2022-11-20 13:32:24,144:INFO: Dataset: zara1               Batch: 5/8	Loss 95.6442 (95.2754)
2022-11-20 13:32:24,579:INFO: Dataset: zara1               Batch: 6/8	Loss 93.7797 (95.0201)
2022-11-20 13:32:25,006:INFO: Dataset: zara1               Batch: 7/8	Loss 93.2555 (94.7827)
2022-11-20 13:32:25,366:INFO: Dataset: zara1               Batch: 8/8	Loss 80.0508 (92.9606)
2022-11-20 13:32:25,928:INFO: Dataset: zara2               Batch:  1/18	Loss 88.8555 (88.8555)
2022-11-20 13:32:26,296:INFO: Dataset: zara2               Batch:  2/18	Loss 89.4694 (89.1712)
2022-11-20 13:32:26,659:INFO: Dataset: zara2               Batch:  3/18	Loss 88.4855 (88.9350)
2022-11-20 13:32:27,025:INFO: Dataset: zara2               Batch:  4/18	Loss 89.4582 (89.0685)
2022-11-20 13:32:27,389:INFO: Dataset: zara2               Batch:  5/18	Loss 88.8641 (89.0274)
2022-11-20 13:32:27,767:INFO: Dataset: zara2               Batch:  6/18	Loss 90.2410 (89.2536)
2022-11-20 13:32:28,133:INFO: Dataset: zara2               Batch:  7/18	Loss 89.7479 (89.3236)
2022-11-20 13:32:28,505:INFO: Dataset: zara2               Batch:  8/18	Loss 90.1221 (89.4424)
2022-11-20 13:32:28,871:INFO: Dataset: zara2               Batch:  9/18	Loss 85.9879 (89.0550)
2022-11-20 13:32:29,239:INFO: Dataset: zara2               Batch: 10/18	Loss 89.3450 (89.0859)
2022-11-20 13:32:29,601:INFO: Dataset: zara2               Batch: 11/18	Loss 89.4469 (89.1199)
2022-11-20 13:32:29,966:INFO: Dataset: zara2               Batch: 12/18	Loss 88.5050 (89.0615)
2022-11-20 13:32:30,327:INFO: Dataset: zara2               Batch: 13/18	Loss 90.0710 (89.1283)
2022-11-20 13:32:30,690:INFO: Dataset: zara2               Batch: 14/18	Loss 90.0115 (89.1909)
2022-11-20 13:32:31,053:INFO: Dataset: zara2               Batch: 15/18	Loss 88.7546 (89.1650)
2022-11-20 13:32:31,417:INFO: Dataset: zara2               Batch: 16/18	Loss 88.9591 (89.1521)
2022-11-20 13:32:31,779:INFO: Dataset: zara2               Batch: 17/18	Loss 89.6245 (89.1810)
2022-11-20 13:32:32,129:INFO: Dataset: zara2               Batch: 18/18	Loss 76.1787 (88.6371)
2022-11-20 13:32:32,159:INFO: - Computing loss (validation)
2022-11-20 13:32:32,500:INFO: Dataset: hotel               Batch: 1/2	Loss 81.8623 (81.8623)
2022-11-20 13:32:32,647:INFO: Dataset: hotel               Batch: 2/2	Loss 6.2510 (77.2172)
2022-11-20 13:32:32,996:INFO: Dataset: univ                Batch: 1/3	Loss 88.9006 (88.9006)
2022-11-20 13:32:33,196:INFO: Dataset: univ                Batch: 2/3	Loss 88.4723 (88.6684)
2022-11-20 13:32:33,383:INFO: Dataset: univ                Batch: 3/3	Loss 82.5877 (87.2416)
2022-11-20 13:32:33,732:INFO: Dataset: zara1               Batch: 1/2	Loss 94.0351 (94.0351)
2022-11-20 13:32:33,896:INFO: Dataset: zara1               Batch: 2/2	Loss 31.2079 (79.8939)
2022-11-20 13:32:34,224:INFO: Dataset: zara2               Batch: 1/5	Loss 85.3007 (85.3007)
2022-11-20 13:32:34,408:INFO: Dataset: zara2               Batch: 2/5	Loss 87.0386 (86.2032)
2022-11-20 13:32:34,595:INFO: Dataset: zara2               Batch: 3/5	Loss 87.4652 (86.6093)
2022-11-20 13:32:34,778:INFO: Dataset: zara2               Batch: 4/5	Loss 85.6888 (86.3804)
2022-11-20 13:32:34,960:INFO: Dataset: zara2               Batch: 5/5	Loss 84.9049 (86.0901)
2022-11-20 13:32:34,985:INFO: - Computing ADE (validation)
2022-11-20 13:32:35,165:INFO: 		 ADE on hotel                     dataset:	 1.2299784421920776
2022-11-20 13:32:35,421:INFO: 		 ADE on univ                      dataset:	 1.6485607624053955
2022-11-20 13:32:35,598:INFO: 		 ADE on zara1                     dataset:	 2.6085500717163086
2022-11-20 13:32:35,888:INFO: 		 ADE on zara2                     dataset:	 1.6282068490982056
2022-11-20 13:32:35,888:INFO: Average validation:	ADE  1.6740	FDE  2.8877
2022-11-20 13:32:35,889:INFO: - Computing ADE (training)
2022-11-20 13:32:36,155:INFO: 		 ADE on hotel                     dataset:	 1.545775294303894
2022-11-20 13:32:36,821:INFO: 		 ADE on univ                      dataset:	 1.537096619606018
2022-11-20 13:32:37,197:INFO: 		 ADE on zara1                     dataset:	 2.5454699993133545
2022-11-20 13:32:37,886:INFO: 		 ADE on zara2                     dataset:	 1.8925918340682983
2022-11-20 13:32:37,887:INFO: Average training:	ADE  1.6737	FDE  2.8990
2022-11-20 13:32:37,903:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_401.pth.tar
2022-11-20 13:32:37,903:INFO: 
===> EPOCH: 402 (P4)
2022-11-20 13:32:37,903:INFO: - Computing loss (training)
2022-11-20 13:32:38,399:INFO: Dataset: hotel               Batch: 1/4	Loss 86.2212 (86.2212)
2022-11-20 13:32:38,761:INFO: Dataset: hotel               Batch: 2/4	Loss 86.4149 (86.3190)
2022-11-20 13:32:39,119:INFO: Dataset: hotel               Batch: 3/4	Loss 88.2046 (86.9515)
2022-11-20 13:32:39,444:INFO: Dataset: hotel               Batch: 4/4	Loss 52.6092 (81.1070)
2022-11-20 13:32:40,082:INFO: Dataset: univ                Batch:  1/15	Loss 89.4241 (89.4241)
2022-11-20 13:32:40,505:INFO: Dataset: univ                Batch:  2/15	Loss 90.3750 (89.8775)
2022-11-20 13:32:40,925:INFO: Dataset: univ                Batch:  3/15	Loss 88.5038 (89.4443)
2022-11-20 13:32:41,351:INFO: Dataset: univ                Batch:  4/15	Loss 89.2878 (89.4049)
2022-11-20 13:32:41,781:INFO: Dataset: univ                Batch:  5/15	Loss 86.9394 (88.8988)
2022-11-20 13:32:42,223:INFO: Dataset: univ                Batch:  6/15	Loss 88.0175 (88.7477)
2022-11-20 13:32:42,654:INFO: Dataset: univ                Batch:  7/15	Loss 87.4849 (88.5675)
2022-11-20 13:32:43,104:INFO: Dataset: univ                Batch:  8/15	Loss 86.7139 (88.3104)
2022-11-20 13:32:43,533:INFO: Dataset: univ                Batch:  9/15	Loss 87.5813 (88.2279)
2022-11-20 13:32:43,961:INFO: Dataset: univ                Batch: 10/15	Loss 87.6172 (88.1658)
2022-11-20 13:32:44,379:INFO: Dataset: univ                Batch: 11/15	Loss 87.6096 (88.1186)
2022-11-20 13:32:44,806:INFO: Dataset: univ                Batch: 12/15	Loss 88.7360 (88.1677)
2022-11-20 13:32:45,234:INFO: Dataset: univ                Batch: 13/15	Loss 88.1566 (88.1668)
2022-11-20 13:32:45,684:INFO: Dataset: univ                Batch: 14/15	Loss 87.7464 (88.1334)
2022-11-20 13:32:45,978:INFO: Dataset: univ                Batch: 15/15	Loss 17.6078 (87.4349)
2022-11-20 13:32:46,520:INFO: Dataset: zara1               Batch: 1/8	Loss 96.3772 (96.3772)
2022-11-20 13:32:46,883:INFO: Dataset: zara1               Batch: 2/8	Loss 94.4427 (95.3556)
2022-11-20 13:32:47,248:INFO: Dataset: zara1               Batch: 3/8	Loss 95.9122 (95.5299)
2022-11-20 13:32:47,611:INFO: Dataset: zara1               Batch: 4/8	Loss 94.2923 (95.2269)
2022-11-20 13:32:47,973:INFO: Dataset: zara1               Batch: 5/8	Loss 95.6731 (95.3110)
2022-11-20 13:32:48,335:INFO: Dataset: zara1               Batch: 6/8	Loss 96.3756 (95.4998)
2022-11-20 13:32:48,696:INFO: Dataset: zara1               Batch: 7/8	Loss 94.2455 (95.3153)
2022-11-20 13:32:49,046:INFO: Dataset: zara1               Batch: 8/8	Loss 83.3067 (93.9628)
2022-11-20 13:32:49,582:INFO: Dataset: zara2               Batch:  1/18	Loss 88.9827 (88.9827)
2022-11-20 13:32:49,947:INFO: Dataset: zara2               Batch:  2/18	Loss 91.3254 (90.2074)
2022-11-20 13:32:50,311:INFO: Dataset: zara2               Batch:  3/18	Loss 90.6057 (90.3520)
2022-11-20 13:32:50,677:INFO: Dataset: zara2               Batch:  4/18	Loss 90.2336 (90.3220)
2022-11-20 13:32:51,039:INFO: Dataset: zara2               Batch:  5/18	Loss 88.7041 (90.0257)
2022-11-20 13:32:51,403:INFO: Dataset: zara2               Batch:  6/18	Loss 89.9735 (90.0169)
2022-11-20 13:32:51,766:INFO: Dataset: zara2               Batch:  7/18	Loss 90.5369 (90.0899)
2022-11-20 13:32:52,128:INFO: Dataset: zara2               Batch:  8/18	Loss 90.8324 (90.1756)
2022-11-20 13:32:52,490:INFO: Dataset: zara2               Batch:  9/18	Loss 89.8156 (90.1357)
2022-11-20 13:32:52,853:INFO: Dataset: zara2               Batch: 10/18	Loss 90.2931 (90.1509)
2022-11-20 13:32:53,216:INFO: Dataset: zara2               Batch: 11/18	Loss 91.1992 (90.2368)
2022-11-20 13:32:53,580:INFO: Dataset: zara2               Batch: 12/18	Loss 87.6139 (90.0128)
2022-11-20 13:32:53,941:INFO: Dataset: zara2               Batch: 13/18	Loss 89.7818 (89.9963)
2022-11-20 13:32:54,303:INFO: Dataset: zara2               Batch: 14/18	Loss 90.9520 (90.0616)
2022-11-20 13:32:54,664:INFO: Dataset: zara2               Batch: 15/18	Loss 89.4413 (90.0168)
2022-11-20 13:32:55,024:INFO: Dataset: zara2               Batch: 16/18	Loss 90.5264 (90.0469)
2022-11-20 13:32:55,385:INFO: Dataset: zara2               Batch: 17/18	Loss 89.5931 (90.0208)
2022-11-20 13:32:55,733:INFO: Dataset: zara2               Batch: 18/18	Loss 76.4836 (89.3964)
2022-11-20 13:32:55,772:INFO: - Computing loss (validation)
2022-11-20 13:32:56,107:INFO: Dataset: hotel               Batch: 1/2	Loss 83.1586 (83.1586)
2022-11-20 13:32:56,257:INFO: Dataset: hotel               Batch: 2/2	Loss 6.5173 (78.7118)
2022-11-20 13:32:56,610:INFO: Dataset: univ                Batch: 1/3	Loss 88.7287 (88.7287)
2022-11-20 13:32:56,801:INFO: Dataset: univ                Batch: 2/3	Loss 90.7411 (89.6865)
2022-11-20 13:32:56,990:INFO: Dataset: univ                Batch: 3/3	Loss 82.1688 (87.5093)
2022-11-20 13:32:57,321:INFO: Dataset: zara1               Batch: 1/2	Loss 95.0101 (95.0101)
2022-11-20 13:32:57,482:INFO: Dataset: zara1               Batch: 2/2	Loss 31.1477 (78.7878)
2022-11-20 13:32:57,816:INFO: Dataset: zara2               Batch: 1/5	Loss 86.9446 (86.9446)
2022-11-20 13:32:58,003:INFO: Dataset: zara2               Batch: 2/5	Loss 87.3819 (87.1498)
2022-11-20 13:32:58,189:INFO: Dataset: zara2               Batch: 3/5	Loss 87.8357 (87.3760)
2022-11-20 13:32:58,374:INFO: Dataset: zara2               Batch: 4/5	Loss 87.2597 (87.3473)
2022-11-20 13:32:58,559:INFO: Dataset: zara2               Batch: 5/5	Loss 84.7976 (86.8548)
2022-11-20 13:32:58,591:INFO: - Computing ADE (validation)
2022-11-20 13:32:58,766:INFO: 		 ADE on hotel                     dataset:	 1.2232718467712402
2022-11-20 13:32:59,017:INFO: 		 ADE on univ                      dataset:	 1.6421363353729248
2022-11-20 13:32:59,202:INFO: 		 ADE on zara1                     dataset:	 2.57875657081604
2022-11-20 13:32:59,512:INFO: 		 ADE on zara2                     dataset:	 1.6265476942062378
2022-11-20 13:32:59,512:INFO: Average validation:	ADE  1.6679	FDE  2.8793
2022-11-20 13:32:59,513:INFO: - Computing ADE (training)
2022-11-20 13:32:59,781:INFO: 		 ADE on hotel                     dataset:	 1.5235382318496704
2022-11-20 13:33:00,475:INFO: 		 ADE on univ                      dataset:	 1.5260721445083618
2022-11-20 13:33:00,854:INFO: 		 ADE on zara1                     dataset:	 2.547443151473999
2022-11-20 13:33:01,557:INFO: 		 ADE on zara2                     dataset:	 1.8836262226104736
2022-11-20 13:33:01,557:INFO: Average training:	ADE  1.6637	FDE  2.8881
2022-11-20 13:33:01,573:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_402.pth.tar
2022-11-20 13:33:01,573:INFO: 
===> EPOCH: 403 (P4)
2022-11-20 13:33:01,574:INFO: - Computing loss (training)
2022-11-20 13:33:02,056:INFO: Dataset: hotel               Batch: 1/4	Loss 87.4897 (87.4897)
2022-11-20 13:33:02,417:INFO: Dataset: hotel               Batch: 2/4	Loss 88.2290 (87.8356)
2022-11-20 13:33:02,773:INFO: Dataset: hotel               Batch: 3/4	Loss 88.7152 (88.1256)
2022-11-20 13:33:03,096:INFO: Dataset: hotel               Batch: 4/4	Loss 52.4191 (82.0017)
2022-11-20 13:33:03,677:INFO: Dataset: univ                Batch:  1/15	Loss 90.5508 (90.5508)
2022-11-20 13:33:04,103:INFO: Dataset: univ                Batch:  2/15	Loss 89.4653 (89.9890)
2022-11-20 13:33:04,543:INFO: Dataset: univ                Batch:  3/15	Loss 89.3884 (89.7719)
2022-11-20 13:33:04,973:INFO: Dataset: univ                Batch:  4/15	Loss 89.1381 (89.6113)
2022-11-20 13:33:05,416:INFO: Dataset: univ                Batch:  5/15	Loss 87.7174 (89.2026)
2022-11-20 13:33:05,845:INFO: Dataset: univ                Batch:  6/15	Loss 87.0290 (88.8414)
2022-11-20 13:33:06,265:INFO: Dataset: univ                Batch:  7/15	Loss 89.8839 (88.9805)
2022-11-20 13:33:06,690:INFO: Dataset: univ                Batch:  8/15	Loss 87.9378 (88.8510)
2022-11-20 13:33:07,115:INFO: Dataset: univ                Batch:  9/15	Loss 88.0727 (88.7649)
2022-11-20 13:33:07,544:INFO: Dataset: univ                Batch: 10/15	Loss 88.8588 (88.7747)
2022-11-20 13:33:07,965:INFO: Dataset: univ                Batch: 11/15	Loss 90.9893 (88.9626)
2022-11-20 13:33:08,412:INFO: Dataset: univ                Batch: 12/15	Loss 87.4771 (88.8246)
2022-11-20 13:33:08,932:INFO: Dataset: univ                Batch: 13/15	Loss 88.7006 (88.8146)
2022-11-20 13:33:09,379:INFO: Dataset: univ                Batch: 14/15	Loss 88.4263 (88.7837)
2022-11-20 13:33:09,670:INFO: Dataset: univ                Batch: 15/15	Loss 17.3308 (88.0083)
2022-11-20 13:33:10,185:INFO: Dataset: zara1               Batch: 1/8	Loss 95.6168 (95.6168)
2022-11-20 13:33:10,545:INFO: Dataset: zara1               Batch: 2/8	Loss 96.8475 (96.2382)
2022-11-20 13:33:10,906:INFO: Dataset: zara1               Batch: 3/8	Loss 95.3530 (95.9428)
2022-11-20 13:33:11,264:INFO: Dataset: zara1               Batch: 4/8	Loss 95.0435 (95.7293)
2022-11-20 13:33:11,623:INFO: Dataset: zara1               Batch: 5/8	Loss 96.0245 (95.7832)
2022-11-20 13:33:11,983:INFO: Dataset: zara1               Batch: 6/8	Loss 96.0632 (95.8316)
2022-11-20 13:33:12,342:INFO: Dataset: zara1               Batch: 7/8	Loss 95.9584 (95.8483)
2022-11-20 13:33:12,689:INFO: Dataset: zara1               Batch: 8/8	Loss 82.6742 (94.4200)
2022-11-20 13:33:13,228:INFO: Dataset: zara2               Batch:  1/18	Loss 89.8513 (89.8513)
2022-11-20 13:33:13,602:INFO: Dataset: zara2               Batch:  2/18	Loss 91.0787 (90.4449)
2022-11-20 13:33:13,972:INFO: Dataset: zara2               Batch:  3/18	Loss 93.5872 (91.5128)
2022-11-20 13:33:14,335:INFO: Dataset: zara2               Batch:  4/18	Loss 90.1066 (91.1310)
2022-11-20 13:33:14,699:INFO: Dataset: zara2               Batch:  5/18	Loss 90.0171 (90.9110)
2022-11-20 13:33:15,065:INFO: Dataset: zara2               Batch:  6/18	Loss 89.5659 (90.6932)
2022-11-20 13:33:15,428:INFO: Dataset: zara2               Batch:  7/18	Loss 89.9528 (90.5815)
2022-11-20 13:33:15,791:INFO: Dataset: zara2               Batch:  8/18	Loss 90.5325 (90.5753)
2022-11-20 13:33:16,155:INFO: Dataset: zara2               Batch:  9/18	Loss 91.6827 (90.6961)
2022-11-20 13:33:16,518:INFO: Dataset: zara2               Batch: 10/18	Loss 90.8849 (90.7159)
2022-11-20 13:33:16,881:INFO: Dataset: zara2               Batch: 11/18	Loss 89.0214 (90.5498)
2022-11-20 13:33:17,246:INFO: Dataset: zara2               Batch: 12/18	Loss 90.4629 (90.5426)
2022-11-20 13:33:17,611:INFO: Dataset: zara2               Batch: 13/18	Loss 91.7561 (90.6380)
2022-11-20 13:33:17,977:INFO: Dataset: zara2               Batch: 14/18	Loss 91.0233 (90.6658)
2022-11-20 13:33:18,340:INFO: Dataset: zara2               Batch: 15/18	Loss 89.1904 (90.5774)
2022-11-20 13:33:18,705:INFO: Dataset: zara2               Batch: 16/18	Loss 89.8240 (90.5259)
2022-11-20 13:33:19,069:INFO: Dataset: zara2               Batch: 17/18	Loss 90.1802 (90.5076)
2022-11-20 13:33:19,422:INFO: Dataset: zara2               Batch: 18/18	Loss 76.6650 (89.7822)
2022-11-20 13:33:19,460:INFO: - Computing loss (validation)
2022-11-20 13:33:19,786:INFO: Dataset: hotel               Batch: 1/2	Loss 84.0559 (84.0559)
2022-11-20 13:33:19,942:INFO: Dataset: hotel               Batch: 2/2	Loss 6.4726 (79.0249)
2022-11-20 13:33:20,292:INFO: Dataset: univ                Batch: 1/3	Loss 91.4979 (91.4979)
2022-11-20 13:33:20,487:INFO: Dataset: univ                Batch: 2/3	Loss 90.7730 (91.1123)
2022-11-20 13:33:20,680:INFO: Dataset: univ                Batch: 3/3	Loss 81.8977 (88.0595)
2022-11-20 13:33:21,005:INFO: Dataset: zara1               Batch: 1/2	Loss 95.4961 (95.4961)
2022-11-20 13:33:21,167:INFO: Dataset: zara1               Batch: 2/2	Loss 31.6366 (80.0959)
2022-11-20 13:33:21,511:INFO: Dataset: zara2               Batch: 1/5	Loss 87.3570 (87.3570)
2022-11-20 13:33:21,697:INFO: Dataset: zara2               Batch: 2/5	Loss 87.6688 (87.5183)
2022-11-20 13:33:21,883:INFO: Dataset: zara2               Batch: 3/5	Loss 86.3920 (87.1228)
2022-11-20 13:33:22,067:INFO: Dataset: zara2               Batch: 4/5	Loss 89.1981 (87.5930)
2022-11-20 13:33:22,251:INFO: Dataset: zara2               Batch: 5/5	Loss 86.9940 (87.4818)
2022-11-20 13:33:22,281:INFO: - Computing ADE (validation)
2022-11-20 13:33:22,469:INFO: 		 ADE on hotel                     dataset:	 1.207783818244934
2022-11-20 13:33:22,737:INFO: 		 ADE on univ                      dataset:	 1.6305402517318726
2022-11-20 13:33:22,950:INFO: 		 ADE on zara1                     dataset:	 2.582874059677124
2022-11-20 13:33:23,248:INFO: 		 ADE on zara2                     dataset:	 1.609556794166565
2022-11-20 13:33:23,249:INFO: Average validation:	ADE  1.6551	FDE  2.8638
2022-11-20 13:33:23,249:INFO: - Computing ADE (training)
2022-11-20 13:33:23,525:INFO: 		 ADE on hotel                     dataset:	 1.5260637998580933
2022-11-20 13:33:24,200:INFO: 		 ADE on univ                      dataset:	 1.5227521657943726
2022-11-20 13:33:24,579:INFO: 		 ADE on zara1                     dataset:	 2.5447654724121094
2022-11-20 13:33:25,250:INFO: 		 ADE on zara2                     dataset:	 1.8552734851837158
2022-11-20 13:33:25,250:INFO: Average training:	ADE  1.6555	FDE  2.8811
2022-11-20 13:33:25,266:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_403.pth.tar
2022-11-20 13:33:25,266:INFO: 
===> EPOCH: 404 (P4)
2022-11-20 13:33:25,267:INFO: - Computing loss (training)
2022-11-20 13:33:25,765:INFO: Dataset: hotel               Batch: 1/4	Loss 89.0788 (89.0788)
2022-11-20 13:33:26,129:INFO: Dataset: hotel               Batch: 2/4	Loss 87.0815 (88.1191)
2022-11-20 13:33:26,491:INFO: Dataset: hotel               Batch: 3/4	Loss 90.1219 (88.8018)
2022-11-20 13:33:26,814:INFO: Dataset: hotel               Batch: 4/4	Loss 52.7800 (82.3387)
2022-11-20 13:33:27,432:INFO: Dataset: univ                Batch:  1/15	Loss 88.7479 (88.7479)
2022-11-20 13:33:27,879:INFO: Dataset: univ                Batch:  2/15	Loss 87.2499 (87.9797)
2022-11-20 13:33:28,309:INFO: Dataset: univ                Batch:  3/15	Loss 89.6901 (88.5188)
2022-11-20 13:33:28,732:INFO: Dataset: univ                Batch:  4/15	Loss 90.6780 (89.0091)
2022-11-20 13:33:29,162:INFO: Dataset: univ                Batch:  5/15	Loss 88.2878 (88.8668)
2022-11-20 13:33:29,594:INFO: Dataset: univ                Batch:  6/15	Loss 89.2591 (88.9326)
2022-11-20 13:33:30,044:INFO: Dataset: univ                Batch:  7/15	Loss 89.7600 (89.0618)
2022-11-20 13:33:30,459:INFO: Dataset: univ                Batch:  8/15	Loss 91.2507 (89.2940)
2022-11-20 13:33:30,884:INFO: Dataset: univ                Batch:  9/15	Loss 88.3041 (89.1879)
2022-11-20 13:33:31,309:INFO: Dataset: univ                Batch: 10/15	Loss 90.6568 (89.3318)
2022-11-20 13:33:31,747:INFO: Dataset: univ                Batch: 11/15	Loss 88.8169 (89.2835)
2022-11-20 13:33:32,169:INFO: Dataset: univ                Batch: 12/15	Loss 89.0247 (89.2633)
2022-11-20 13:33:32,593:INFO: Dataset: univ                Batch: 13/15	Loss 89.5117 (89.2815)
2022-11-20 13:33:33,025:INFO: Dataset: univ                Batch: 14/15	Loss 88.2025 (89.2022)
2022-11-20 13:33:33,319:INFO: Dataset: univ                Batch: 15/15	Loss 16.3652 (87.9872)
2022-11-20 13:33:33,838:INFO: Dataset: zara1               Batch: 1/8	Loss 95.8077 (95.8077)
2022-11-20 13:33:34,198:INFO: Dataset: zara1               Batch: 2/8	Loss 95.7166 (95.7616)
2022-11-20 13:33:34,557:INFO: Dataset: zara1               Batch: 3/8	Loss 96.6710 (96.0606)
2022-11-20 13:33:34,917:INFO: Dataset: zara1               Batch: 4/8	Loss 95.6743 (95.9696)
2022-11-20 13:33:35,276:INFO: Dataset: zara1               Batch: 5/8	Loss 95.8850 (95.9518)
2022-11-20 13:33:35,643:INFO: Dataset: zara1               Batch: 6/8	Loss 96.1646 (95.9868)
2022-11-20 13:33:36,004:INFO: Dataset: zara1               Batch: 7/8	Loss 96.5970 (96.0800)
2022-11-20 13:33:36,352:INFO: Dataset: zara1               Batch: 8/8	Loss 82.5365 (94.6187)
2022-11-20 13:33:36,890:INFO: Dataset: zara2               Batch:  1/18	Loss 91.5021 (91.5021)
2022-11-20 13:33:37,260:INFO: Dataset: zara2               Batch:  2/18	Loss 90.2242 (90.8344)
2022-11-20 13:33:37,625:INFO: Dataset: zara2               Batch:  3/18	Loss 89.7012 (90.4476)
2022-11-20 13:33:37,991:INFO: Dataset: zara2               Batch:  4/18	Loss 90.2133 (90.3890)
2022-11-20 13:33:38,357:INFO: Dataset: zara2               Batch:  5/18	Loss 91.6698 (90.6268)
2022-11-20 13:33:38,724:INFO: Dataset: zara2               Batch:  6/18	Loss 90.6816 (90.6363)
2022-11-20 13:33:39,087:INFO: Dataset: zara2               Batch:  7/18	Loss 89.4153 (90.4766)
2022-11-20 13:33:39,451:INFO: Dataset: zara2               Batch:  8/18	Loss 90.5270 (90.4824)
2022-11-20 13:33:39,814:INFO: Dataset: zara2               Batch:  9/18	Loss 91.4192 (90.5784)
2022-11-20 13:33:40,180:INFO: Dataset: zara2               Batch: 10/18	Loss 87.7464 (90.2598)
2022-11-20 13:33:40,546:INFO: Dataset: zara2               Batch: 11/18	Loss 91.0658 (90.3413)
2022-11-20 13:33:40,912:INFO: Dataset: zara2               Batch: 12/18	Loss 90.5810 (90.3618)
2022-11-20 13:33:41,276:INFO: Dataset: zara2               Batch: 13/18	Loss 92.2816 (90.5018)
2022-11-20 13:33:41,641:INFO: Dataset: zara2               Batch: 14/18	Loss 91.3621 (90.5656)
2022-11-20 13:33:42,010:INFO: Dataset: zara2               Batch: 15/18	Loss 91.2507 (90.6090)
2022-11-20 13:33:42,376:INFO: Dataset: zara2               Batch: 16/18	Loss 90.0060 (90.5718)
2022-11-20 13:33:42,742:INFO: Dataset: zara2               Batch: 17/18	Loss 89.6143 (90.5207)
2022-11-20 13:33:43,097:INFO: Dataset: zara2               Batch: 18/18	Loss 79.5221 (90.0116)
2022-11-20 13:33:43,129:INFO: - Computing loss (validation)
2022-11-20 13:33:43,455:INFO: Dataset: hotel               Batch: 1/2	Loss 83.8601 (83.8601)
2022-11-20 13:33:43,601:INFO: Dataset: hotel               Batch: 2/2	Loss 7.1941 (78.3653)
2022-11-20 13:33:43,947:INFO: Dataset: univ                Batch: 1/3	Loss 90.4050 (90.4050)
2022-11-20 13:33:44,141:INFO: Dataset: univ                Batch: 2/3	Loss 92.1398 (91.2997)
2022-11-20 13:33:44,333:INFO: Dataset: univ                Batch: 3/3	Loss 81.9914 (88.3664)
2022-11-20 13:33:44,673:INFO: Dataset: zara1               Batch: 1/2	Loss 95.6762 (95.6762)
2022-11-20 13:33:44,837:INFO: Dataset: zara1               Batch: 2/2	Loss 31.6620 (79.0037)
2022-11-20 13:33:45,160:INFO: Dataset: zara2               Batch: 1/5	Loss 88.2689 (88.2689)
2022-11-20 13:33:45,345:INFO: Dataset: zara2               Batch: 2/5	Loss 88.0560 (88.1649)
2022-11-20 13:33:45,529:INFO: Dataset: zara2               Batch: 3/5	Loss 87.9130 (88.0812)
2022-11-20 13:33:45,712:INFO: Dataset: zara2               Batch: 4/5	Loss 87.5083 (87.9356)
2022-11-20 13:33:45,897:INFO: Dataset: zara2               Batch: 5/5	Loss 86.5832 (87.6654)
2022-11-20 13:33:45,929:INFO: - Computing ADE (validation)
2022-11-20 13:33:46,105:INFO: 		 ADE on hotel                     dataset:	 1.2098268270492554
2022-11-20 13:33:46,357:INFO: 		 ADE on univ                      dataset:	 1.625691533088684
2022-11-20 13:33:46,566:INFO: 		 ADE on zara1                     dataset:	 2.5495636463165283
2022-11-20 13:33:46,886:INFO: 		 ADE on zara2                     dataset:	 1.6021819114685059
2022-11-20 13:33:46,887:INFO: Average validation:	ADE  1.6480	FDE  2.8592
2022-11-20 13:33:46,887:INFO: - Computing ADE (training)
2022-11-20 13:33:47,139:INFO: 		 ADE on hotel                     dataset:	 1.5158692598342896
2022-11-20 13:33:47,848:INFO: 		 ADE on univ                      dataset:	 1.5187277793884277
2022-11-20 13:33:48,232:INFO: 		 ADE on zara1                     dataset:	 2.5374557971954346
2022-11-20 13:33:48,952:INFO: 		 ADE on zara2                     dataset:	 1.8616496324539185
2022-11-20 13:33:48,952:INFO: Average training:	ADE  1.6532	FDE  2.8814
2022-11-20 13:33:48,968:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_404.pth.tar
2022-11-20 13:33:48,968:INFO: 
===> EPOCH: 405 (P4)
2022-11-20 13:33:48,969:INFO: - Computing loss (training)
2022-11-20 13:33:49,471:INFO: Dataset: hotel               Batch: 1/4	Loss 87.4698 (87.4698)
2022-11-20 13:33:49,833:INFO: Dataset: hotel               Batch: 2/4	Loss 89.8107 (88.5896)
2022-11-20 13:33:50,191:INFO: Dataset: hotel               Batch: 3/4	Loss 89.6100 (88.9220)
2022-11-20 13:33:50,514:INFO: Dataset: hotel               Batch: 4/4	Loss 53.8698 (82.4017)
2022-11-20 13:33:51,125:INFO: Dataset: univ                Batch:  1/15	Loss 90.4701 (90.4701)
2022-11-20 13:33:51,568:INFO: Dataset: univ                Batch:  2/15	Loss 89.1973 (89.8122)
2022-11-20 13:33:51,998:INFO: Dataset: univ                Batch:  3/15	Loss 90.6119 (90.0848)
2022-11-20 13:33:52,427:INFO: Dataset: univ                Batch:  4/15	Loss 91.2556 (90.3851)
2022-11-20 13:33:52,860:INFO: Dataset: univ                Batch:  5/15	Loss 89.1921 (90.1339)
2022-11-20 13:33:53,298:INFO: Dataset: univ                Batch:  6/15	Loss 87.9892 (89.7623)
2022-11-20 13:33:53,729:INFO: Dataset: univ                Batch:  7/15	Loss 89.4615 (89.7177)
2022-11-20 13:33:54,177:INFO: Dataset: univ                Batch:  8/15	Loss 87.8807 (89.4683)
2022-11-20 13:33:54,606:INFO: Dataset: univ                Batch:  9/15	Loss 89.5359 (89.4759)
2022-11-20 13:33:55,034:INFO: Dataset: univ                Batch: 10/15	Loss 89.2240 (89.4508)
2022-11-20 13:33:55,458:INFO: Dataset: univ                Batch: 11/15	Loss 90.5921 (89.5508)
2022-11-20 13:33:55,904:INFO: Dataset: univ                Batch: 12/15	Loss 88.4983 (89.4572)
2022-11-20 13:33:56,351:INFO: Dataset: univ                Batch: 13/15	Loss 89.5240 (89.4628)
2022-11-20 13:33:56,778:INFO: Dataset: univ                Batch: 14/15	Loss 88.7493 (89.4143)
2022-11-20 13:33:57,073:INFO: Dataset: univ                Batch: 15/15	Loss 16.4071 (88.4387)
2022-11-20 13:33:57,618:INFO: Dataset: zara1               Batch: 1/8	Loss 96.5282 (96.5282)
2022-11-20 13:33:57,984:INFO: Dataset: zara1               Batch: 2/8	Loss 97.2893 (96.8835)
2022-11-20 13:33:58,348:INFO: Dataset: zara1               Batch: 3/8	Loss 96.2918 (96.6782)
2022-11-20 13:33:58,714:INFO: Dataset: zara1               Batch: 4/8	Loss 96.5689 (96.6523)
2022-11-20 13:33:59,082:INFO: Dataset: zara1               Batch: 5/8	Loss 96.0143 (96.5324)
2022-11-20 13:33:59,448:INFO: Dataset: zara1               Batch: 6/8	Loss 96.7293 (96.5695)
2022-11-20 13:33:59,890:INFO: Dataset: zara1               Batch: 7/8	Loss 94.5943 (96.2931)
2022-11-20 13:34:00,242:INFO: Dataset: zara1               Batch: 8/8	Loss 82.0848 (94.8573)
2022-11-20 13:34:00,785:INFO: Dataset: zara2               Batch:  1/18	Loss 90.8496 (90.8496)
2022-11-20 13:34:01,151:INFO: Dataset: zara2               Batch:  2/18	Loss 91.2895 (91.0708)
2022-11-20 13:34:01,516:INFO: Dataset: zara2               Batch:  3/18	Loss 89.4236 (90.5481)
2022-11-20 13:34:01,883:INFO: Dataset: zara2               Batch:  4/18	Loss 91.5403 (90.7974)
2022-11-20 13:34:02,248:INFO: Dataset: zara2               Batch:  5/18	Loss 90.3515 (90.7095)
2022-11-20 13:34:02,619:INFO: Dataset: zara2               Batch:  6/18	Loss 91.1604 (90.7819)
2022-11-20 13:34:02,986:INFO: Dataset: zara2               Batch:  7/18	Loss 89.2977 (90.5686)
2022-11-20 13:34:03,351:INFO: Dataset: zara2               Batch:  8/18	Loss 90.4522 (90.5538)
2022-11-20 13:34:03,715:INFO: Dataset: zara2               Batch:  9/18	Loss 90.1305 (90.5050)
2022-11-20 13:34:04,080:INFO: Dataset: zara2               Batch: 10/18	Loss 91.7686 (90.6177)
2022-11-20 13:34:04,444:INFO: Dataset: zara2               Batch: 11/18	Loss 90.7845 (90.6333)
2022-11-20 13:34:04,810:INFO: Dataset: zara2               Batch: 12/18	Loss 89.9495 (90.5772)
2022-11-20 13:34:05,175:INFO: Dataset: zara2               Batch: 13/18	Loss 91.0530 (90.6150)
2022-11-20 13:34:05,541:INFO: Dataset: zara2               Batch: 14/18	Loss 90.7429 (90.6238)
2022-11-20 13:34:05,908:INFO: Dataset: zara2               Batch: 15/18	Loss 91.3596 (90.6696)
2022-11-20 13:34:06,274:INFO: Dataset: zara2               Batch: 16/18	Loss 90.3433 (90.6511)
2022-11-20 13:34:06,640:INFO: Dataset: zara2               Batch: 17/18	Loss 88.9529 (90.5534)
2022-11-20 13:34:06,993:INFO: Dataset: zara2               Batch: 18/18	Loss 77.9327 (89.9379)
2022-11-20 13:34:07,031:INFO: - Computing loss (validation)
2022-11-20 13:34:07,369:INFO: Dataset: hotel               Batch: 1/2	Loss 84.4187 (84.4187)
2022-11-20 13:34:07,521:INFO: Dataset: hotel               Batch: 2/2	Loss 6.8437 (76.7406)
2022-11-20 13:34:07,889:INFO: Dataset: univ                Batch: 1/3	Loss 90.0430 (90.0430)
2022-11-20 13:34:08,091:INFO: Dataset: univ                Batch: 2/3	Loss 90.1562 (90.1001)
2022-11-20 13:34:08,287:INFO: Dataset: univ                Batch: 3/3	Loss 83.9121 (88.2324)
2022-11-20 13:34:08,635:INFO: Dataset: zara1               Batch: 1/2	Loss 95.4039 (95.4039)
2022-11-20 13:34:08,798:INFO: Dataset: zara1               Batch: 2/2	Loss 31.3987 (79.1454)
2022-11-20 13:34:09,148:INFO: Dataset: zara2               Batch: 1/5	Loss 87.6165 (87.6165)
2022-11-20 13:34:09,349:INFO: Dataset: zara2               Batch: 2/5	Loss 87.2275 (87.4310)
2022-11-20 13:34:09,540:INFO: Dataset: zara2               Batch: 3/5	Loss 88.3391 (87.7216)
2022-11-20 13:34:09,731:INFO: Dataset: zara2               Batch: 4/5	Loss 88.5741 (87.9363)
2022-11-20 13:34:09,922:INFO: Dataset: zara2               Batch: 5/5	Loss 86.3196 (87.6446)
2022-11-20 13:34:09,948:INFO: - Computing ADE (validation)
2022-11-20 13:34:10,136:INFO: 		 ADE on hotel                     dataset:	 1.1945356130599976
2022-11-20 13:34:10,390:INFO: 		 ADE on univ                      dataset:	 1.613486886024475
2022-11-20 13:34:10,587:INFO: 		 ADE on zara1                     dataset:	 2.5279643535614014
2022-11-20 13:34:10,912:INFO: 		 ADE on zara2                     dataset:	 1.6035269498825073
2022-11-20 13:34:10,912:INFO: Average validation:	ADE  1.6401	FDE  2.8532
2022-11-20 13:34:10,913:INFO: - Computing ADE (training)
2022-11-20 13:34:11,166:INFO: 		 ADE on hotel                     dataset:	 1.5143194198608398
2022-11-20 13:34:11,885:INFO: 		 ADE on univ                      dataset:	 1.5059823989868164
2022-11-20 13:34:12,265:INFO: 		 ADE on zara1                     dataset:	 2.520191192626953
2022-11-20 13:34:12,952:INFO: 		 ADE on zara2                     dataset:	 1.8593319654464722
2022-11-20 13:34:12,952:INFO: Average training:	ADE  1.6425	FDE  2.8698
2022-11-20 13:34:12,968:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_405.pth.tar
2022-11-20 13:34:12,968:INFO: 
===> EPOCH: 406 (P4)
2022-11-20 13:34:12,969:INFO: - Computing loss (training)
2022-11-20 13:34:13,468:INFO: Dataset: hotel               Batch: 1/4	Loss 89.3871 (89.3871)
2022-11-20 13:34:13,837:INFO: Dataset: hotel               Batch: 2/4	Loss 88.5149 (88.9449)
2022-11-20 13:34:14,203:INFO: Dataset: hotel               Batch: 3/4	Loss 89.0268 (88.9712)
2022-11-20 13:34:14,530:INFO: Dataset: hotel               Batch: 4/4	Loss 54.0106 (83.2059)
2022-11-20 13:34:15,167:INFO: Dataset: univ                Batch:  1/15	Loss 88.0458 (88.0458)
2022-11-20 13:34:15,606:INFO: Dataset: univ                Batch:  2/15	Loss 89.7636 (88.8790)
2022-11-20 13:34:16,037:INFO: Dataset: univ                Batch:  3/15	Loss 89.5255 (89.0794)
2022-11-20 13:34:16,472:INFO: Dataset: univ                Batch:  4/15	Loss 90.1812 (89.3521)
2022-11-20 13:34:16,906:INFO: Dataset: univ                Batch:  5/15	Loss 89.8324 (89.4489)
2022-11-20 13:34:17,363:INFO: Dataset: univ                Batch:  6/15	Loss 88.2477 (89.2314)
2022-11-20 13:34:17,801:INFO: Dataset: univ                Batch:  7/15	Loss 88.3744 (89.1104)
2022-11-20 13:34:18,225:INFO: Dataset: univ                Batch:  8/15	Loss 89.8278 (89.1890)
2022-11-20 13:34:18,650:INFO: Dataset: univ                Batch:  9/15	Loss 90.4306 (89.3130)
2022-11-20 13:34:19,115:INFO: Dataset: univ                Batch: 10/15	Loss 86.5122 (88.9784)
2022-11-20 13:34:19,571:INFO: Dataset: univ                Batch: 11/15	Loss 88.3120 (88.9135)
2022-11-20 13:34:19,994:INFO: Dataset: univ                Batch: 12/15	Loss 90.4571 (89.0276)
2022-11-20 13:34:20,413:INFO: Dataset: univ                Batch: 13/15	Loss 90.4394 (89.1209)
2022-11-20 13:34:20,848:INFO: Dataset: univ                Batch: 14/15	Loss 88.9099 (89.1062)
2022-11-20 13:34:21,150:INFO: Dataset: univ                Batch: 15/15	Loss 17.4327 (88.1008)
2022-11-20 13:34:21,694:INFO: Dataset: zara1               Batch: 1/8	Loss 97.5715 (97.5715)
2022-11-20 13:34:22,066:INFO: Dataset: zara1               Batch: 2/8	Loss 93.7273 (95.5121)
2022-11-20 13:34:22,436:INFO: Dataset: zara1               Batch: 3/8	Loss 96.5935 (95.8786)
2022-11-20 13:34:22,807:INFO: Dataset: zara1               Batch: 4/8	Loss 96.0298 (95.9147)
2022-11-20 13:34:23,179:INFO: Dataset: zara1               Batch: 5/8	Loss 95.8482 (95.9004)
2022-11-20 13:34:23,551:INFO: Dataset: zara1               Batch: 6/8	Loss 96.9738 (96.0723)
2022-11-20 13:34:23,921:INFO: Dataset: zara1               Batch: 7/8	Loss 95.9698 (96.0572)
2022-11-20 13:34:24,280:INFO: Dataset: zara1               Batch: 8/8	Loss 82.0656 (94.4739)
2022-11-20 13:34:24,834:INFO: Dataset: zara2               Batch:  1/18	Loss 90.2973 (90.2973)
2022-11-20 13:34:25,216:INFO: Dataset: zara2               Batch:  2/18	Loss 89.8179 (90.0572)
2022-11-20 13:34:25,591:INFO: Dataset: zara2               Batch:  3/18	Loss 90.0931 (90.0701)
2022-11-20 13:34:25,966:INFO: Dataset: zara2               Batch:  4/18	Loss 92.0145 (90.5334)
2022-11-20 13:34:26,347:INFO: Dataset: zara2               Batch:  5/18	Loss 90.0814 (90.4414)
2022-11-20 13:34:26,725:INFO: Dataset: zara2               Batch:  6/18	Loss 89.6364 (90.3147)
2022-11-20 13:34:27,099:INFO: Dataset: zara2               Batch:  7/18	Loss 91.1151 (90.4347)
2022-11-20 13:34:27,475:INFO: Dataset: zara2               Batch:  8/18	Loss 90.7037 (90.4702)
2022-11-20 13:34:27,850:INFO: Dataset: zara2               Batch:  9/18	Loss 90.4380 (90.4667)
2022-11-20 13:34:28,227:INFO: Dataset: zara2               Batch: 10/18	Loss 89.7252 (90.3972)
2022-11-20 13:34:28,603:INFO: Dataset: zara2               Batch: 11/18	Loss 90.3348 (90.3915)
2022-11-20 13:34:28,980:INFO: Dataset: zara2               Batch: 12/18	Loss 89.0093 (90.2781)
2022-11-20 13:34:29,356:INFO: Dataset: zara2               Batch: 13/18	Loss 90.5710 (90.3014)
2022-11-20 13:34:29,731:INFO: Dataset: zara2               Batch: 14/18	Loss 88.6596 (90.1907)
2022-11-20 13:34:30,110:INFO: Dataset: zara2               Batch: 15/18	Loss 88.9162 (90.0928)
2022-11-20 13:34:30,488:INFO: Dataset: zara2               Batch: 16/18	Loss 90.2288 (90.1010)
2022-11-20 13:34:30,864:INFO: Dataset: zara2               Batch: 17/18	Loss 91.2874 (90.1734)
2022-11-20 13:34:31,227:INFO: Dataset: zara2               Batch: 18/18	Loss 79.2415 (89.6655)
2022-11-20 13:34:31,259:INFO: - Computing loss (validation)
2022-11-20 13:34:31,587:INFO: Dataset: hotel               Batch: 1/2	Loss 84.3633 (84.3633)
2022-11-20 13:34:31,741:INFO: Dataset: hotel               Batch: 2/2	Loss 6.8331 (77.7481)
2022-11-20 13:34:32,104:INFO: Dataset: univ                Batch: 1/3	Loss 89.5014 (89.5014)
2022-11-20 13:34:32,299:INFO: Dataset: univ                Batch: 2/3	Loss 90.8278 (90.1403)
2022-11-20 13:34:32,490:INFO: Dataset: univ                Batch: 3/3	Loss 82.9162 (87.7989)
2022-11-20 13:34:32,809:INFO: Dataset: zara1               Batch: 1/2	Loss 95.6187 (95.6187)
2022-11-20 13:34:32,973:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8964 (78.7618)
2022-11-20 13:34:33,325:INFO: Dataset: zara2               Batch: 1/5	Loss 87.7680 (87.7680)
2022-11-20 13:34:33,516:INFO: Dataset: zara2               Batch: 2/5	Loss 87.0520 (87.4211)
2022-11-20 13:34:33,707:INFO: Dataset: zara2               Batch: 3/5	Loss 88.4881 (87.7675)
2022-11-20 13:34:33,898:INFO: Dataset: zara2               Batch: 4/5	Loss 87.3600 (87.6676)
2022-11-20 13:34:34,087:INFO: Dataset: zara2               Batch: 5/5	Loss 85.5530 (87.2549)
2022-11-20 13:34:34,119:INFO: - Computing ADE (validation)
2022-11-20 13:34:34,314:INFO: 		 ADE on hotel                     dataset:	 1.1777937412261963
2022-11-20 13:34:34,578:INFO: 		 ADE on univ                      dataset:	 1.6051826477050781
2022-11-20 13:34:34,777:INFO: 		 ADE on zara1                     dataset:	 2.5411007404327393
2022-11-20 13:34:35,093:INFO: 		 ADE on zara2                     dataset:	 1.5883219242095947
2022-11-20 13:34:35,094:INFO: Average validation:	ADE  1.6300	FDE  2.8415
2022-11-20 13:34:35,095:INFO: - Computing ADE (training)
2022-11-20 13:34:35,355:INFO: 		 ADE on hotel                     dataset:	 1.5177161693572998
2022-11-20 13:34:36,056:INFO: 		 ADE on univ                      dataset:	 1.5002418756484985
2022-11-20 13:34:36,433:INFO: 		 ADE on zara1                     dataset:	 2.513883590698242
2022-11-20 13:34:37,127:INFO: 		 ADE on zara2                     dataset:	 1.8312532901763916
2022-11-20 13:34:37,127:INFO: Average training:	ADE  1.6325	FDE  2.8587
2022-11-20 13:34:37,143:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_406.pth.tar
2022-11-20 13:34:37,144:INFO: 
===> EPOCH: 407 (P4)
2022-11-20 13:34:37,144:INFO: - Computing loss (training)
2022-11-20 13:34:37,654:INFO: Dataset: hotel               Batch: 1/4	Loss 90.4470 (90.4470)
2022-11-20 13:34:38,025:INFO: Dataset: hotel               Batch: 2/4	Loss 87.6877 (89.0974)
2022-11-20 13:34:38,390:INFO: Dataset: hotel               Batch: 3/4	Loss 87.8849 (88.6913)
2022-11-20 13:34:38,719:INFO: Dataset: hotel               Batch: 4/4	Loss 53.3897 (82.3109)
2022-11-20 13:34:39,339:INFO: Dataset: univ                Batch:  1/15	Loss 89.6786 (89.6786)
2022-11-20 13:34:39,745:INFO: Dataset: univ                Batch:  2/15	Loss 91.6370 (90.5015)
2022-11-20 13:34:40,196:INFO: Dataset: univ                Batch:  3/15	Loss 87.6229 (89.4459)
2022-11-20 13:34:40,645:INFO: Dataset: univ                Batch:  4/15	Loss 87.2607 (88.8530)
2022-11-20 13:34:41,075:INFO: Dataset: univ                Batch:  5/15	Loss 89.0965 (88.9005)
2022-11-20 13:34:41,505:INFO: Dataset: univ                Batch:  6/15	Loss 88.3013 (88.8020)
2022-11-20 13:34:41,938:INFO: Dataset: univ                Batch:  7/15	Loss 89.4841 (88.9024)
2022-11-20 13:34:42,368:INFO: Dataset: univ                Batch:  8/15	Loss 89.1724 (88.9364)
2022-11-20 13:34:42,819:INFO: Dataset: univ                Batch:  9/15	Loss 87.9179 (88.8100)
2022-11-20 13:34:43,255:INFO: Dataset: univ                Batch: 10/15	Loss 88.3710 (88.7655)
2022-11-20 13:34:43,678:INFO: Dataset: univ                Batch: 11/15	Loss 90.5844 (88.9140)
2022-11-20 13:34:44,125:INFO: Dataset: univ                Batch: 12/15	Loss 87.0817 (88.7553)
2022-11-20 13:34:44,555:INFO: Dataset: univ                Batch: 13/15	Loss 88.4275 (88.7300)
2022-11-20 13:34:45,014:INFO: Dataset: univ                Batch: 14/15	Loss 86.6530 (88.5622)
2022-11-20 13:34:45,313:INFO: Dataset: univ                Batch: 15/15	Loss 17.3390 (87.9884)
2022-11-20 13:34:45,870:INFO: Dataset: zara1               Batch: 1/8	Loss 95.7295 (95.7295)
2022-11-20 13:34:46,234:INFO: Dataset: zara1               Batch: 2/8	Loss 95.1178 (95.3872)
2022-11-20 13:34:46,596:INFO: Dataset: zara1               Batch: 3/8	Loss 96.6393 (95.8297)
2022-11-20 13:34:46,958:INFO: Dataset: zara1               Batch: 4/8	Loss 95.1122 (95.6462)
2022-11-20 13:34:47,320:INFO: Dataset: zara1               Batch: 5/8	Loss 95.4352 (95.6034)
2022-11-20 13:34:47,686:INFO: Dataset: zara1               Batch: 6/8	Loss 94.0997 (95.3479)
2022-11-20 13:34:48,048:INFO: Dataset: zara1               Batch: 7/8	Loss 94.0917 (95.1598)
2022-11-20 13:34:48,399:INFO: Dataset: zara1               Batch: 8/8	Loss 83.8828 (93.9906)
2022-11-20 13:34:48,929:INFO: Dataset: zara2               Batch:  1/18	Loss 89.8663 (89.8663)
2022-11-20 13:34:49,301:INFO: Dataset: zara2               Batch:  2/18	Loss 91.9068 (90.8675)
2022-11-20 13:34:49,676:INFO: Dataset: zara2               Batch:  3/18	Loss 90.4662 (90.7293)
2022-11-20 13:34:50,060:INFO: Dataset: zara2               Batch:  4/18	Loss 89.5927 (90.4845)
2022-11-20 13:34:50,527:INFO: Dataset: zara2               Batch:  5/18	Loss 89.0010 (90.1710)
2022-11-20 13:34:50,903:INFO: Dataset: zara2               Batch:  6/18	Loss 88.9400 (89.9651)
2022-11-20 13:34:51,271:INFO: Dataset: zara2               Batch:  7/18	Loss 88.5046 (89.7538)
2022-11-20 13:34:51,641:INFO: Dataset: zara2               Batch:  8/18	Loss 89.5444 (89.7277)
2022-11-20 13:34:52,009:INFO: Dataset: zara2               Batch:  9/18	Loss 89.7036 (89.7251)
2022-11-20 13:34:52,377:INFO: Dataset: zara2               Batch: 10/18	Loss 88.6969 (89.6255)
2022-11-20 13:34:52,746:INFO: Dataset: zara2               Batch: 11/18	Loss 90.1103 (89.6663)
2022-11-20 13:34:53,115:INFO: Dataset: zara2               Batch: 12/18	Loss 90.0864 (89.7029)
2022-11-20 13:34:53,484:INFO: Dataset: zara2               Batch: 13/18	Loss 89.3487 (89.6773)
2022-11-20 13:34:53,852:INFO: Dataset: zara2               Batch: 14/18	Loss 91.1032 (89.7856)
2022-11-20 13:34:54,221:INFO: Dataset: zara2               Batch: 15/18	Loss 88.6212 (89.7059)
2022-11-20 13:34:54,590:INFO: Dataset: zara2               Batch: 16/18	Loss 89.9324 (89.7206)
2022-11-20 13:34:54,959:INFO: Dataset: zara2               Batch: 17/18	Loss 88.8723 (89.6738)
2022-11-20 13:34:55,314:INFO: Dataset: zara2               Batch: 18/18	Loss 77.9363 (89.1770)
2022-11-20 13:34:55,351:INFO: - Computing loss (validation)
2022-11-20 13:34:55,693:INFO: Dataset: hotel               Batch: 1/2	Loss 84.3605 (84.3605)
2022-11-20 13:34:55,842:INFO: Dataset: hotel               Batch: 2/2	Loss 6.5514 (80.9082)
2022-11-20 13:34:56,206:INFO: Dataset: univ                Batch: 1/3	Loss 88.8660 (88.8660)
2022-11-20 13:34:56,408:INFO: Dataset: univ                Batch: 2/3	Loss 90.0098 (89.4082)
2022-11-20 13:34:56,602:INFO: Dataset: univ                Batch: 3/3	Loss 83.3492 (87.7580)
2022-11-20 13:34:56,941:INFO: Dataset: zara1               Batch: 1/2	Loss 94.6964 (94.6964)
2022-11-20 13:34:57,104:INFO: Dataset: zara1               Batch: 2/2	Loss 31.1406 (81.0043)
2022-11-20 13:34:57,440:INFO: Dataset: zara2               Batch: 1/5	Loss 85.8510 (85.8510)
2022-11-20 13:34:57,628:INFO: Dataset: zara2               Batch: 2/5	Loss 86.7751 (86.2852)
2022-11-20 13:34:57,816:INFO: Dataset: zara2               Batch: 3/5	Loss 88.4219 (87.0173)
2022-11-20 13:34:58,005:INFO: Dataset: zara2               Batch: 4/5	Loss 87.0560 (87.0268)
2022-11-20 13:34:58,193:INFO: Dataset: zara2               Batch: 5/5	Loss 86.3480 (86.8926)
2022-11-20 13:34:58,218:INFO: - Computing ADE (validation)
2022-11-20 13:34:58,401:INFO: 		 ADE on hotel                     dataset:	 1.1969250440597534
2022-11-20 13:34:58,645:INFO: 		 ADE on univ                      dataset:	 1.603116750717163
2022-11-20 13:34:58,836:INFO: 		 ADE on zara1                     dataset:	 2.5272903442382812
2022-11-20 13:34:59,140:INFO: 		 ADE on zara2                     dataset:	 1.5729743242263794
2022-11-20 13:34:59,140:INFO: Average validation:	ADE  1.6235	FDE  2.8383
2022-11-20 13:34:59,141:INFO: - Computing ADE (training)
2022-11-20 13:34:59,419:INFO: 		 ADE on hotel                     dataset:	 1.4955601692199707
2022-11-20 13:35:00,118:INFO: 		 ADE on univ                      dataset:	 1.4902962446212769
2022-11-20 13:35:00,516:INFO: 		 ADE on zara1                     dataset:	 2.511272430419922
2022-11-20 13:35:01,217:INFO: 		 ADE on zara2                     dataset:	 1.8423430919647217
2022-11-20 13:35:01,218:INFO: Average training:	ADE  1.6269	FDE  2.8559
2022-11-20 13:35:01,235:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_407.pth.tar
2022-11-20 13:35:01,235:INFO: 
===> EPOCH: 408 (P4)
2022-11-20 13:35:01,236:INFO: - Computing loss (training)
2022-11-20 13:35:01,741:INFO: Dataset: hotel               Batch: 1/4	Loss 89.4883 (89.4883)
2022-11-20 13:35:02,111:INFO: Dataset: hotel               Batch: 2/4	Loss 86.3911 (87.9361)
2022-11-20 13:35:02,497:INFO: Dataset: hotel               Batch: 3/4	Loss 89.6684 (88.4885)
2022-11-20 13:35:02,848:INFO: Dataset: hotel               Batch: 4/4	Loss 53.3331 (82.2737)
2022-11-20 13:35:03,454:INFO: Dataset: univ                Batch:  1/15	Loss 89.8866 (89.8866)
2022-11-20 13:35:03,935:INFO: Dataset: univ                Batch:  2/15	Loss 86.9229 (88.3200)
2022-11-20 13:35:04,426:INFO: Dataset: univ                Batch:  3/15	Loss 87.7289 (88.1146)
2022-11-20 13:35:04,883:INFO: Dataset: univ                Batch:  4/15	Loss 89.7469 (88.5252)
2022-11-20 13:35:05,328:INFO: Dataset: univ                Batch:  5/15	Loss 88.5698 (88.5344)
2022-11-20 13:35:05,789:INFO: Dataset: univ                Batch:  6/15	Loss 87.9771 (88.4296)
2022-11-20 13:35:06,229:INFO: Dataset: univ                Batch:  7/15	Loss 86.9151 (88.2099)
2022-11-20 13:35:06,671:INFO: Dataset: univ                Batch:  8/15	Loss 88.2282 (88.2121)
2022-11-20 13:35:07,133:INFO: Dataset: univ                Batch:  9/15	Loss 87.7479 (88.1572)
2022-11-20 13:35:07,577:INFO: Dataset: univ                Batch: 10/15	Loss 89.1403 (88.2534)
2022-11-20 13:35:08,032:INFO: Dataset: univ                Batch: 11/15	Loss 88.2816 (88.2559)
2022-11-20 13:35:08,577:INFO: Dataset: univ                Batch: 12/15	Loss 87.9866 (88.2325)
2022-11-20 13:35:09,097:INFO: Dataset: univ                Batch: 13/15	Loss 87.5689 (88.1845)
2022-11-20 13:35:09,551:INFO: Dataset: univ                Batch: 14/15	Loss 89.0023 (88.2422)
2022-11-20 13:35:09,875:INFO: Dataset: univ                Batch: 15/15	Loss 16.5538 (87.3555)
2022-11-20 13:35:10,428:INFO: Dataset: zara1               Batch: 1/8	Loss 94.5865 (94.5865)
2022-11-20 13:35:10,812:INFO: Dataset: zara1               Batch: 2/8	Loss 96.8486 (95.8035)
2022-11-20 13:35:11,197:INFO: Dataset: zara1               Batch: 3/8	Loss 93.6462 (95.0711)
2022-11-20 13:35:11,573:INFO: Dataset: zara1               Batch: 4/8	Loss 93.4574 (94.7116)
2022-11-20 13:35:11,947:INFO: Dataset: zara1               Batch: 5/8	Loss 94.6049 (94.6907)
2022-11-20 13:35:12,327:INFO: Dataset: zara1               Batch: 6/8	Loss 95.4723 (94.8366)
2022-11-20 13:35:12,715:INFO: Dataset: zara1               Batch: 7/8	Loss 96.0609 (95.0010)
2022-11-20 13:35:13,091:INFO: Dataset: zara1               Batch: 8/8	Loss 80.8100 (93.6566)
2022-11-20 13:35:13,665:INFO: Dataset: zara2               Batch:  1/18	Loss 90.6247 (90.6247)
2022-11-20 13:35:14,040:INFO: Dataset: zara2               Batch:  2/18	Loss 89.0621 (89.8581)
2022-11-20 13:35:14,410:INFO: Dataset: zara2               Batch:  3/18	Loss 90.2030 (89.9750)
2022-11-20 13:35:14,776:INFO: Dataset: zara2               Batch:  4/18	Loss 91.1202 (90.2591)
2022-11-20 13:35:15,155:INFO: Dataset: zara2               Batch:  5/18	Loss 87.9477 (89.7303)
2022-11-20 13:35:15,522:INFO: Dataset: zara2               Batch:  6/18	Loss 89.3814 (89.6675)
2022-11-20 13:35:15,948:INFO: Dataset: zara2               Batch:  7/18	Loss 89.5857 (89.6546)
2022-11-20 13:35:16,349:INFO: Dataset: zara2               Batch:  8/18	Loss 89.2680 (89.6014)
2022-11-20 13:35:16,777:INFO: Dataset: zara2               Batch:  9/18	Loss 88.7271 (89.5010)
2022-11-20 13:35:17,166:INFO: Dataset: zara2               Batch: 10/18	Loss 88.8609 (89.4313)
2022-11-20 13:35:17,546:INFO: Dataset: zara2               Batch: 11/18	Loss 88.3763 (89.3278)
2022-11-20 13:35:17,941:INFO: Dataset: zara2               Batch: 12/18	Loss 88.9888 (89.2986)
2022-11-20 13:35:18,418:INFO: Dataset: zara2               Batch: 13/18	Loss 89.0754 (89.2786)
2022-11-20 13:35:18,844:INFO: Dataset: zara2               Batch: 14/18	Loss 87.2070 (89.1027)
2022-11-20 13:35:19,231:INFO: Dataset: zara2               Batch: 15/18	Loss 89.4541 (89.1265)
2022-11-20 13:35:19,599:INFO: Dataset: zara2               Batch: 16/18	Loss 89.5421 (89.1529)
2022-11-20 13:35:19,970:INFO: Dataset: zara2               Batch: 17/18	Loss 89.1904 (89.1552)
2022-11-20 13:35:20,326:INFO: Dataset: zara2               Batch: 18/18	Loss 76.7369 (88.5229)
2022-11-20 13:35:20,365:INFO: - Computing loss (validation)
2022-11-20 13:35:20,721:INFO: Dataset: hotel               Batch: 1/2	Loss 84.1610 (84.1610)
2022-11-20 13:35:20,876:INFO: Dataset: hotel               Batch: 2/2	Loss 6.4348 (78.0596)
2022-11-20 13:35:21,227:INFO: Dataset: univ                Batch: 1/3	Loss 88.7472 (88.7472)
2022-11-20 13:35:21,426:INFO: Dataset: univ                Batch: 2/3	Loss 89.4668 (89.1145)
2022-11-20 13:35:21,639:INFO: Dataset: univ                Batch: 3/3	Loss 82.3334 (87.0873)
2022-11-20 13:35:21,999:INFO: Dataset: zara1               Batch: 1/2	Loss 94.0936 (94.0936)
2022-11-20 13:35:22,164:INFO: Dataset: zara1               Batch: 2/2	Loss 31.0221 (79.0862)
2022-11-20 13:35:22,523:INFO: Dataset: zara2               Batch: 1/5	Loss 86.6633 (86.6633)
2022-11-20 13:35:22,713:INFO: Dataset: zara2               Batch: 2/5	Loss 87.0314 (86.8384)
2022-11-20 13:35:22,912:INFO: Dataset: zara2               Batch: 3/5	Loss 86.8069 (86.8275)
2022-11-20 13:35:23,113:INFO: Dataset: zara2               Batch: 4/5	Loss 86.1773 (86.6812)
2022-11-20 13:35:23,306:INFO: Dataset: zara2               Batch: 5/5	Loss 85.4510 (86.4373)
2022-11-20 13:35:23,337:INFO: - Computing ADE (validation)
2022-11-20 13:35:23,530:INFO: 		 ADE on hotel                     dataset:	 1.1844923496246338
2022-11-20 13:35:23,769:INFO: 		 ADE on univ                      dataset:	 1.594323754310608
2022-11-20 13:35:23,988:INFO: 		 ADE on zara1                     dataset:	 2.5421087741851807
2022-11-20 13:35:24,307:INFO: 		 ADE on zara2                     dataset:	 1.5809322595596313
2022-11-20 13:35:24,307:INFO: Average validation:	ADE  1.6221	FDE  2.8394
2022-11-20 13:35:24,308:INFO: - Computing ADE (training)
2022-11-20 13:35:24,572:INFO: 		 ADE on hotel                     dataset:	 1.5109663009643555
2022-11-20 13:35:25,254:INFO: 		 ADE on univ                      dataset:	 1.4888808727264404
2022-11-20 13:35:25,649:INFO: 		 ADE on zara1                     dataset:	 2.5041675567626953
2022-11-20 13:35:26,349:INFO: 		 ADE on zara2                     dataset:	 1.8293207883834839
2022-11-20 13:35:26,349:INFO: Average training:	ADE  1.6232	FDE  2.8523
2022-11-20 13:35:26,365:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_408.pth.tar
2022-11-20 13:35:26,366:INFO: 
===> EPOCH: 409 (P4)
2022-11-20 13:35:26,366:INFO: - Computing loss (training)
2022-11-20 13:35:26,863:INFO: Dataset: hotel               Batch: 1/4	Loss 89.7480 (89.7480)
2022-11-20 13:35:27,233:INFO: Dataset: hotel               Batch: 2/4	Loss 88.9441 (89.3269)
2022-11-20 13:35:27,601:INFO: Dataset: hotel               Batch: 3/4	Loss 86.5122 (88.3120)
2022-11-20 13:35:27,935:INFO: Dataset: hotel               Batch: 4/4	Loss 52.3617 (81.9566)
2022-11-20 13:35:28,536:INFO: Dataset: univ                Batch:  1/15	Loss 89.1142 (89.1142)
2022-11-20 13:35:28,969:INFO: Dataset: univ                Batch:  2/15	Loss 89.7811 (89.4500)
2022-11-20 13:35:29,408:INFO: Dataset: univ                Batch:  3/15	Loss 86.6054 (88.4704)
2022-11-20 13:35:29,841:INFO: Dataset: univ                Batch:  4/15	Loss 88.4014 (88.4529)
2022-11-20 13:35:30,286:INFO: Dataset: univ                Batch:  5/15	Loss 87.1729 (88.1838)
2022-11-20 13:35:30,760:INFO: Dataset: univ                Batch:  6/15	Loss 87.1089 (87.9986)
2022-11-20 13:35:31,264:INFO: Dataset: univ                Batch:  7/15	Loss 88.6592 (88.0861)
2022-11-20 13:35:31,755:INFO: Dataset: univ                Batch:  8/15	Loss 86.8031 (87.9072)
2022-11-20 13:35:32,201:INFO: Dataset: univ                Batch:  9/15	Loss 88.8317 (88.0099)
2022-11-20 13:35:32,643:INFO: Dataset: univ                Batch: 10/15	Loss 88.9897 (88.1065)
2022-11-20 13:35:33,090:INFO: Dataset: univ                Batch: 11/15	Loss 87.4478 (88.0470)
2022-11-20 13:35:33,590:INFO: Dataset: univ                Batch: 12/15	Loss 86.7255 (87.9271)
2022-11-20 13:35:34,183:INFO: Dataset: univ                Batch: 13/15	Loss 86.9389 (87.8448)
2022-11-20 13:35:34,728:INFO: Dataset: univ                Batch: 14/15	Loss 87.8772 (87.8471)
2022-11-20 13:35:35,044:INFO: Dataset: univ                Batch: 15/15	Loss 16.1753 (86.8791)
2022-11-20 13:35:35,637:INFO: Dataset: zara1               Batch: 1/8	Loss 95.1896 (95.1896)
2022-11-20 13:35:36,014:INFO: Dataset: zara1               Batch: 2/8	Loss 93.8380 (94.5604)
2022-11-20 13:35:36,400:INFO: Dataset: zara1               Batch: 3/8	Loss 94.8657 (94.6656)
2022-11-20 13:35:36,782:INFO: Dataset: zara1               Batch: 4/8	Loss 93.0559 (94.2338)
2022-11-20 13:35:37,175:INFO: Dataset: zara1               Batch: 5/8	Loss 94.2517 (94.2371)
2022-11-20 13:35:37,588:INFO: Dataset: zara1               Batch: 6/8	Loss 94.0452 (94.2052)
2022-11-20 13:35:37,957:INFO: Dataset: zara1               Batch: 7/8	Loss 92.8811 (94.0231)
2022-11-20 13:35:38,311:INFO: Dataset: zara1               Batch: 8/8	Loss 81.5037 (92.7185)
2022-11-20 13:35:38,878:INFO: Dataset: zara2               Batch:  1/18	Loss 88.7937 (88.7937)
2022-11-20 13:35:39,269:INFO: Dataset: zara2               Batch:  2/18	Loss 90.0889 (89.4432)
2022-11-20 13:35:39,662:INFO: Dataset: zara2               Batch:  3/18	Loss 88.5166 (89.1230)
2022-11-20 13:35:40,082:INFO: Dataset: zara2               Batch:  4/18	Loss 88.1963 (88.8860)
2022-11-20 13:35:40,488:INFO: Dataset: zara2               Batch:  5/18	Loss 88.6878 (88.8487)
2022-11-20 13:35:40,884:INFO: Dataset: zara2               Batch:  6/18	Loss 87.8552 (88.6727)
2022-11-20 13:35:41,346:INFO: Dataset: zara2               Batch:  7/18	Loss 89.3193 (88.7579)
2022-11-20 13:35:41,755:INFO: Dataset: zara2               Batch:  8/18	Loss 88.7534 (88.7573)
2022-11-20 13:35:42,164:INFO: Dataset: zara2               Batch:  9/18	Loss 88.0835 (88.6797)
2022-11-20 13:35:42,547:INFO: Dataset: zara2               Batch: 10/18	Loss 87.6653 (88.5789)
2022-11-20 13:35:42,946:INFO: Dataset: zara2               Batch: 11/18	Loss 88.0442 (88.5303)
2022-11-20 13:35:43,369:INFO: Dataset: zara2               Batch: 12/18	Loss 88.4727 (88.5256)
2022-11-20 13:35:43,752:INFO: Dataset: zara2               Batch: 13/18	Loss 88.9304 (88.5550)
2022-11-20 13:35:44,154:INFO: Dataset: zara2               Batch: 14/18	Loss 87.9142 (88.5094)
2022-11-20 13:35:44,553:INFO: Dataset: zara2               Batch: 15/18	Loss 90.5272 (88.6406)
2022-11-20 13:35:44,977:INFO: Dataset: zara2               Batch: 16/18	Loss 87.8036 (88.5876)
2022-11-20 13:35:45,376:INFO: Dataset: zara2               Batch: 17/18	Loss 87.5012 (88.5208)
2022-11-20 13:35:45,792:INFO: Dataset: zara2               Batch: 18/18	Loss 75.7318 (87.9373)
2022-11-20 13:35:45,826:INFO: - Computing loss (validation)
2022-11-20 13:35:46,187:INFO: Dataset: hotel               Batch: 1/2	Loss 83.7422 (83.7422)
2022-11-20 13:35:46,345:INFO: Dataset: hotel               Batch: 2/2	Loss 6.9194 (77.9739)
2022-11-20 13:35:46,749:INFO: Dataset: univ                Batch: 1/3	Loss 89.8172 (89.8172)
2022-11-20 13:35:46,966:INFO: Dataset: univ                Batch: 2/3	Loss 87.0454 (88.3745)
2022-11-20 13:35:47,169:INFO: Dataset: univ                Batch: 3/3	Loss 82.6715 (86.6204)
2022-11-20 13:35:47,582:INFO: Dataset: zara1               Batch: 1/2	Loss 94.1673 (94.1673)
2022-11-20 13:35:47,761:INFO: Dataset: zara1               Batch: 2/2	Loss 29.9532 (78.2686)
2022-11-20 13:35:48,114:INFO: Dataset: zara2               Batch: 1/5	Loss 86.9394 (86.9394)
2022-11-20 13:35:48,305:INFO: Dataset: zara2               Batch: 2/5	Loss 85.9785 (86.4736)
2022-11-20 13:35:48,494:INFO: Dataset: zara2               Batch: 3/5	Loss 84.7102 (85.8662)
2022-11-20 13:35:48,683:INFO: Dataset: zara2               Batch: 4/5	Loss 87.1910 (86.1895)
2022-11-20 13:35:48,882:INFO: Dataset: zara2               Batch: 5/5	Loss 84.1040 (85.7941)
2022-11-20 13:35:48,913:INFO: - Computing ADE (validation)
2022-11-20 13:35:49,118:INFO: 		 ADE on hotel                     dataset:	 1.2066811323165894
2022-11-20 13:35:49,381:INFO: 		 ADE on univ                      dataset:	 1.5896133184432983
2022-11-20 13:35:49,584:INFO: 		 ADE on zara1                     dataset:	 2.5408458709716797
2022-11-20 13:35:49,931:INFO: 		 ADE on zara2                     dataset:	 1.568601131439209
2022-11-20 13:35:49,931:INFO: Average validation:	ADE  1.6162	FDE  2.8320
2022-11-20 13:35:49,932:INFO: - Computing ADE (training)
2022-11-20 13:35:50,201:INFO: 		 ADE on hotel                     dataset:	 1.498968482017517
2022-11-20 13:35:50,924:INFO: 		 ADE on univ                      dataset:	 1.4869399070739746
2022-11-20 13:35:51,353:INFO: 		 ADE on zara1                     dataset:	 2.492572069168091
2022-11-20 13:35:52,036:INFO: 		 ADE on zara2                     dataset:	 1.8262230157852173
2022-11-20 13:35:52,036:INFO: Average training:	ADE  1.6202	FDE  2.8500
2022-11-20 13:35:52,053:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_409.pth.tar
2022-11-20 13:35:52,053:INFO: 
===> EPOCH: 410 (P4)
2022-11-20 13:35:52,054:INFO: - Computing loss (training)
2022-11-20 13:35:52,627:INFO: Dataset: hotel               Batch: 1/4	Loss 86.4865 (86.4865)
2022-11-20 13:35:53,060:INFO: Dataset: hotel               Batch: 2/4	Loss 87.4938 (86.9758)
2022-11-20 13:35:53,435:INFO: Dataset: hotel               Batch: 3/4	Loss 87.7588 (87.2360)
2022-11-20 13:35:53,767:INFO: Dataset: hotel               Batch: 4/4	Loss 54.7843 (81.8417)
2022-11-20 13:35:54,415:INFO: Dataset: univ                Batch:  1/15	Loss 86.9809 (86.9809)
2022-11-20 13:35:54,861:INFO: Dataset: univ                Batch:  2/15	Loss 88.5252 (87.7388)
2022-11-20 13:35:55,315:INFO: Dataset: univ                Batch:  3/15	Loss 87.9059 (87.7972)
2022-11-20 13:35:55,757:INFO: Dataset: univ                Batch:  4/15	Loss 88.1998 (87.8985)
2022-11-20 13:35:56,215:INFO: Dataset: univ                Batch:  5/15	Loss 85.3032 (87.3631)
2022-11-20 13:35:56,664:INFO: Dataset: univ                Batch:  6/15	Loss 86.9749 (87.3005)
2022-11-20 13:35:57,132:INFO: Dataset: univ                Batch:  7/15	Loss 85.8835 (87.0873)
2022-11-20 13:35:57,587:INFO: Dataset: univ                Batch:  8/15	Loss 87.9209 (87.1858)
2022-11-20 13:35:58,039:INFO: Dataset: univ                Batch:  9/15	Loss 88.2759 (87.3032)
2022-11-20 13:35:58,496:INFO: Dataset: univ                Batch: 10/15	Loss 88.6817 (87.4315)
2022-11-20 13:35:58,950:INFO: Dataset: univ                Batch: 11/15	Loss 88.1482 (87.4898)
2022-11-20 13:35:59,401:INFO: Dataset: univ                Batch: 12/15	Loss 87.2150 (87.4680)
2022-11-20 13:35:59,840:INFO: Dataset: univ                Batch: 13/15	Loss 87.5938 (87.4770)
2022-11-20 13:36:00,308:INFO: Dataset: univ                Batch: 14/15	Loss 85.6984 (87.3399)
2022-11-20 13:36:00,619:INFO: Dataset: univ                Batch: 15/15	Loss 16.1915 (86.2879)
2022-11-20 13:36:01,183:INFO: Dataset: zara1               Batch: 1/8	Loss 92.7819 (92.7819)
2022-11-20 13:36:01,571:INFO: Dataset: zara1               Batch: 2/8	Loss 93.8925 (93.3372)
2022-11-20 13:36:01,963:INFO: Dataset: zara1               Batch: 3/8	Loss 93.9103 (93.5325)
2022-11-20 13:36:02,370:INFO: Dataset: zara1               Batch: 4/8	Loss 92.7828 (93.3443)
2022-11-20 13:36:02,765:INFO: Dataset: zara1               Batch: 5/8	Loss 94.3929 (93.5678)
2022-11-20 13:36:03,157:INFO: Dataset: zara1               Batch: 6/8	Loss 93.5747 (93.5689)
2022-11-20 13:36:03,535:INFO: Dataset: zara1               Batch: 7/8	Loss 93.3822 (93.5409)
2022-11-20 13:36:03,899:INFO: Dataset: zara1               Batch: 8/8	Loss 81.2044 (92.2164)
2022-11-20 13:36:04,484:INFO: Dataset: zara2               Batch:  1/18	Loss 88.4187 (88.4187)
2022-11-20 13:36:04,866:INFO: Dataset: zara2               Batch:  2/18	Loss 88.3416 (88.3794)
2022-11-20 13:36:05,247:INFO: Dataset: zara2               Batch:  3/18	Loss 88.3140 (88.3574)
2022-11-20 13:36:05,645:INFO: Dataset: zara2               Batch:  4/18	Loss 90.4352 (88.8468)
2022-11-20 13:36:06,037:INFO: Dataset: zara2               Batch:  5/18	Loss 88.7331 (88.8243)
2022-11-20 13:36:06,424:INFO: Dataset: zara2               Batch:  6/18	Loss 87.8543 (88.6754)
2022-11-20 13:36:06,806:INFO: Dataset: zara2               Batch:  7/18	Loss 88.5309 (88.6561)
2022-11-20 13:36:07,181:INFO: Dataset: zara2               Batch:  8/18	Loss 88.3351 (88.6153)
2022-11-20 13:36:07,560:INFO: Dataset: zara2               Batch:  9/18	Loss 86.3230 (88.3474)
2022-11-20 13:36:07,939:INFO: Dataset: zara2               Batch: 10/18	Loss 87.9802 (88.3086)
2022-11-20 13:36:08,320:INFO: Dataset: zara2               Batch: 11/18	Loss 87.2104 (88.2158)
2022-11-20 13:36:08,699:INFO: Dataset: zara2               Batch: 12/18	Loss 88.0867 (88.2060)
2022-11-20 13:36:09,092:INFO: Dataset: zara2               Batch: 13/18	Loss 87.8941 (88.1816)
2022-11-20 13:36:09,520:INFO: Dataset: zara2               Batch: 14/18	Loss 88.0788 (88.1729)
2022-11-20 13:36:09,975:INFO: Dataset: zara2               Batch: 15/18	Loss 87.8216 (88.1484)
2022-11-20 13:36:10,436:INFO: Dataset: zara2               Batch: 16/18	Loss 87.0391 (88.0762)
2022-11-20 13:36:10,828:INFO: Dataset: zara2               Batch: 17/18	Loss 87.5445 (88.0447)
2022-11-20 13:36:11,202:INFO: Dataset: zara2               Batch: 18/18	Loss 75.3550 (87.4132)
2022-11-20 13:36:11,249:INFO: - Computing loss (validation)
2022-11-20 13:36:11,599:INFO: Dataset: hotel               Batch: 1/2	Loss 83.7966 (83.7966)
2022-11-20 13:36:11,756:INFO: Dataset: hotel               Batch: 2/2	Loss 6.4455 (78.7807)
2022-11-20 13:36:12,134:INFO: Dataset: univ                Batch: 1/3	Loss 88.9436 (88.9436)
2022-11-20 13:36:12,352:INFO: Dataset: univ                Batch: 2/3	Loss 88.5861 (88.7863)
2022-11-20 13:36:12,561:INFO: Dataset: univ                Batch: 3/3	Loss 80.9582 (86.3026)
2022-11-20 13:36:12,910:INFO: Dataset: zara1               Batch: 1/2	Loss 92.7799 (92.7799)
2022-11-20 13:36:13,077:INFO: Dataset: zara1               Batch: 2/2	Loss 30.3936 (77.3337)
2022-11-20 13:36:13,435:INFO: Dataset: zara2               Batch: 1/5	Loss 85.6677 (85.6677)
2022-11-20 13:36:13,638:INFO: Dataset: zara2               Batch: 2/5	Loss 85.3679 (85.5215)
2022-11-20 13:36:13,829:INFO: Dataset: zara2               Batch: 3/5	Loss 84.6989 (85.2365)
2022-11-20 13:36:14,020:INFO: Dataset: zara2               Batch: 4/5	Loss 85.8355 (85.4068)
2022-11-20 13:36:14,213:INFO: Dataset: zara2               Batch: 5/5	Loss 85.0719 (85.3390)
2022-11-20 13:36:14,238:INFO: - Computing ADE (validation)
2022-11-20 13:36:14,457:INFO: 		 ADE on hotel                     dataset:	 1.198767066001892
2022-11-20 13:36:14,730:INFO: 		 ADE on univ                      dataset:	 1.5907937288284302
2022-11-20 13:36:14,952:INFO: 		 ADE on zara1                     dataset:	 2.5445268154144287
2022-11-20 13:36:15,264:INFO: 		 ADE on zara2                     dataset:	 1.5666618347167969
2022-11-20 13:36:15,264:INFO: Average validation:	ADE  1.6159	FDE  2.8353
2022-11-20 13:36:15,265:INFO: - Computing ADE (training)
2022-11-20 13:36:15,548:INFO: 		 ADE on hotel                     dataset:	 1.5145620107650757
2022-11-20 13:36:16,423:INFO: 		 ADE on univ                      dataset:	 1.4854642152786255
2022-11-20 13:36:16,888:INFO: 		 ADE on zara1                     dataset:	 2.4723479747772217
2022-11-20 13:36:17,620:INFO: 		 ADE on zara2                     dataset:	 1.8284317255020142
2022-11-20 13:36:17,620:INFO: Average training:	ADE  1.6187	FDE  2.8521
2022-11-20 13:36:17,637:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_410.pth.tar
2022-11-20 13:36:17,637:INFO: 
===> EPOCH: 411 (P4)
2022-11-20 13:36:17,637:INFO: - Computing loss (training)
2022-11-20 13:36:18,160:INFO: Dataset: hotel               Batch: 1/4	Loss 87.1570 (87.1570)
2022-11-20 13:36:18,542:INFO: Dataset: hotel               Batch: 2/4	Loss 87.7895 (87.4829)
2022-11-20 13:36:18,929:INFO: Dataset: hotel               Batch: 3/4	Loss 87.6324 (87.5301)
2022-11-20 13:36:19,284:INFO: Dataset: hotel               Batch: 4/4	Loss 52.4023 (81.3201)
2022-11-20 13:36:19,974:INFO: Dataset: univ                Batch:  1/15	Loss 86.2261 (86.2261)
2022-11-20 13:36:20,414:INFO: Dataset: univ                Batch:  2/15	Loss 88.1755 (87.1239)
2022-11-20 13:36:20,855:INFO: Dataset: univ                Batch:  3/15	Loss 86.5338 (86.9266)
2022-11-20 13:36:21,299:INFO: Dataset: univ                Batch:  4/15	Loss 89.7934 (87.5525)
2022-11-20 13:36:21,747:INFO: Dataset: univ                Batch:  5/15	Loss 86.1482 (87.2716)
2022-11-20 13:36:22,223:INFO: Dataset: univ                Batch:  6/15	Loss 86.8426 (87.1970)
2022-11-20 13:36:22,671:INFO: Dataset: univ                Batch:  7/15	Loss 87.2800 (87.2083)
2022-11-20 13:36:23,114:INFO: Dataset: univ                Batch:  8/15	Loss 89.1088 (87.4479)
2022-11-20 13:36:23,574:INFO: Dataset: univ                Batch:  9/15	Loss 85.5744 (87.2166)
2022-11-20 13:36:24,009:INFO: Dataset: univ                Batch: 10/15	Loss 87.7495 (87.2688)
2022-11-20 13:36:24,470:INFO: Dataset: univ                Batch: 11/15	Loss 85.4149 (87.0793)
2022-11-20 13:36:24,911:INFO: Dataset: univ                Batch: 12/15	Loss 86.8960 (87.0636)
2022-11-20 13:36:25,359:INFO: Dataset: univ                Batch: 13/15	Loss 86.1515 (86.9905)
2022-11-20 13:36:25,813:INFO: Dataset: univ                Batch: 14/15	Loss 85.4899 (86.8745)
2022-11-20 13:36:26,114:INFO: Dataset: univ                Batch: 15/15	Loss 16.1949 (85.9099)
2022-11-20 13:36:26,687:INFO: Dataset: zara1               Batch: 1/8	Loss 92.2858 (92.2858)
2022-11-20 13:36:27,054:INFO: Dataset: zara1               Batch: 2/8	Loss 93.2034 (92.7531)
2022-11-20 13:36:27,501:INFO: Dataset: zara1               Batch: 3/8	Loss 92.4475 (92.6470)
2022-11-20 13:36:28,012:INFO: Dataset: zara1               Batch: 4/8	Loss 91.7254 (92.4157)
2022-11-20 13:36:28,394:INFO: Dataset: zara1               Batch: 5/8	Loss 93.0445 (92.5414)
2022-11-20 13:36:28,776:INFO: Dataset: zara1               Batch: 6/8	Loss 93.1176 (92.6319)
2022-11-20 13:36:29,155:INFO: Dataset: zara1               Batch: 7/8	Loss 93.4161 (92.7316)
2022-11-20 13:36:29,525:INFO: Dataset: zara1               Batch: 8/8	Loss 80.2642 (91.3667)
2022-11-20 13:36:30,102:INFO: Dataset: zara2               Batch:  1/18	Loss 86.6208 (86.6208)
2022-11-20 13:36:30,485:INFO: Dataset: zara2               Batch:  2/18	Loss 87.5742 (87.1453)
2022-11-20 13:36:30,892:INFO: Dataset: zara2               Batch:  3/18	Loss 87.2653 (87.1877)
2022-11-20 13:36:31,278:INFO: Dataset: zara2               Batch:  4/18	Loss 87.8197 (87.3348)
2022-11-20 13:36:31,668:INFO: Dataset: zara2               Batch:  5/18	Loss 87.6526 (87.3993)
2022-11-20 13:36:32,055:INFO: Dataset: zara2               Batch:  6/18	Loss 87.8714 (87.4830)
2022-11-20 13:36:32,438:INFO: Dataset: zara2               Batch:  7/18	Loss 87.7723 (87.5219)
2022-11-20 13:36:32,831:INFO: Dataset: zara2               Batch:  8/18	Loss 89.6369 (87.7542)
2022-11-20 13:36:33,222:INFO: Dataset: zara2               Batch:  9/18	Loss 87.7822 (87.7574)
2022-11-20 13:36:33,593:INFO: Dataset: zara2               Batch: 10/18	Loss 86.3855 (87.6203)
2022-11-20 13:36:33,986:INFO: Dataset: zara2               Batch: 11/18	Loss 86.6550 (87.5256)
2022-11-20 13:36:34,374:INFO: Dataset: zara2               Batch: 12/18	Loss 87.3551 (87.5120)
2022-11-20 13:36:34,766:INFO: Dataset: zara2               Batch: 13/18	Loss 87.8009 (87.5351)
2022-11-20 13:36:35,186:INFO: Dataset: zara2               Batch: 14/18	Loss 88.5561 (87.5971)
2022-11-20 13:36:35,587:INFO: Dataset: zara2               Batch: 15/18	Loss 87.3706 (87.5801)
2022-11-20 13:36:35,989:INFO: Dataset: zara2               Batch: 16/18	Loss 86.7913 (87.5340)
2022-11-20 13:36:36,409:INFO: Dataset: zara2               Batch: 17/18	Loss 87.1739 (87.5127)
2022-11-20 13:36:36,789:INFO: Dataset: zara2               Batch: 18/18	Loss 74.4872 (86.9076)
2022-11-20 13:36:36,826:INFO: - Computing loss (validation)
2022-11-20 13:36:37,184:INFO: Dataset: hotel               Batch: 1/2	Loss 83.6951 (83.6951)
2022-11-20 13:36:37,340:INFO: Dataset: hotel               Batch: 2/2	Loss 6.2144 (78.1419)
2022-11-20 13:36:37,709:INFO: Dataset: univ                Batch: 1/3	Loss 88.2151 (88.2151)
2022-11-20 13:36:37,912:INFO: Dataset: univ                Batch: 2/3	Loss 88.8940 (88.5453)
2022-11-20 13:36:38,110:INFO: Dataset: univ                Batch: 3/3	Loss 79.5046 (85.9042)
2022-11-20 13:36:38,477:INFO: Dataset: zara1               Batch: 1/2	Loss 91.9073 (91.9073)
2022-11-20 13:36:38,656:INFO: Dataset: zara1               Batch: 2/2	Loss 30.3497 (77.8540)
2022-11-20 13:36:39,005:INFO: Dataset: zara2               Batch: 1/5	Loss 86.5113 (86.5113)
2022-11-20 13:36:39,194:INFO: Dataset: zara2               Batch: 2/5	Loss 84.1759 (85.3996)
2022-11-20 13:36:39,383:INFO: Dataset: zara2               Batch: 3/5	Loss 85.2254 (85.3400)
2022-11-20 13:36:39,572:INFO: Dataset: zara2               Batch: 4/5	Loss 85.2058 (85.3062)
2022-11-20 13:36:39,760:INFO: Dataset: zara2               Batch: 5/5	Loss 83.6511 (84.9730)
2022-11-20 13:36:39,791:INFO: - Computing ADE (validation)
2022-11-20 13:36:40,005:INFO: 		 ADE on hotel                     dataset:	 1.225825309753418
2022-11-20 13:36:40,278:INFO: 		 ADE on univ                      dataset:	 1.595907211303711
2022-11-20 13:36:40,464:INFO: 		 ADE on zara1                     dataset:	 2.527482032775879
2022-11-20 13:36:40,785:INFO: 		 ADE on zara2                     dataset:	 1.5777924060821533
2022-11-20 13:36:40,786:INFO: Average validation:	ADE  1.6232	FDE  2.8477
2022-11-20 13:36:40,786:INFO: - Computing ADE (training)
2022-11-20 13:36:41,062:INFO: 		 ADE on hotel                     dataset:	 1.529779314994812
2022-11-20 13:36:41,749:INFO: 		 ADE on univ                      dataset:	 1.487761378288269
2022-11-20 13:36:42,158:INFO: 		 ADE on zara1                     dataset:	 2.4789037704467773
2022-11-20 13:36:42,873:INFO: 		 ADE on zara2                     dataset:	 1.822523593902588
2022-11-20 13:36:42,873:INFO: Average training:	ADE  1.6199	FDE  2.8568
2022-11-20 13:36:42,890:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_411.pth.tar
2022-11-20 13:36:42,890:INFO: 
===> EPOCH: 412 (P4)
2022-11-20 13:36:42,891:INFO: - Computing loss (training)
2022-11-20 13:36:43,469:INFO: Dataset: hotel               Batch: 1/4	Loss 87.4461 (87.4461)
2022-11-20 13:36:43,838:INFO: Dataset: hotel               Batch: 2/4	Loss 88.9866 (88.1747)
2022-11-20 13:36:44,203:INFO: Dataset: hotel               Batch: 3/4	Loss 84.9918 (87.1390)
2022-11-20 13:36:44,539:INFO: Dataset: hotel               Batch: 4/4	Loss 52.0254 (81.2095)
2022-11-20 13:36:45,184:INFO: Dataset: univ                Batch:  1/15	Loss 86.7076 (86.7076)
2022-11-20 13:36:45,624:INFO: Dataset: univ                Batch:  2/15	Loss 86.8735 (86.7868)
2022-11-20 13:36:46,072:INFO: Dataset: univ                Batch:  3/15	Loss 85.4429 (86.3598)
2022-11-20 13:36:46,538:INFO: Dataset: univ                Batch:  4/15	Loss 86.0150 (86.2670)
2022-11-20 13:36:46,975:INFO: Dataset: univ                Batch:  5/15	Loss 87.8620 (86.5491)
2022-11-20 13:36:47,417:INFO: Dataset: univ                Batch:  6/15	Loss 87.3216 (86.6682)
2022-11-20 13:36:47,849:INFO: Dataset: univ                Batch:  7/15	Loss 86.2359 (86.6111)
2022-11-20 13:36:48,286:INFO: Dataset: univ                Batch:  8/15	Loss 86.6939 (86.6215)
2022-11-20 13:36:48,719:INFO: Dataset: univ                Batch:  9/15	Loss 86.6542 (86.6251)
2022-11-20 13:36:49,169:INFO: Dataset: univ                Batch: 10/15	Loss 85.8641 (86.5434)
2022-11-20 13:36:49,611:INFO: Dataset: univ                Batch: 11/15	Loss 87.5515 (86.6384)
2022-11-20 13:36:50,046:INFO: Dataset: univ                Batch: 12/15	Loss 86.5340 (86.6298)
2022-11-20 13:36:50,487:INFO: Dataset: univ                Batch: 13/15	Loss 85.4656 (86.5379)
2022-11-20 13:36:50,944:INFO: Dataset: univ                Batch: 14/15	Loss 86.8094 (86.5580)
2022-11-20 13:36:51,251:INFO: Dataset: univ                Batch: 15/15	Loss 16.0953 (85.6298)
2022-11-20 13:36:51,794:INFO: Dataset: zara1               Batch: 1/8	Loss 91.8331 (91.8331)
2022-11-20 13:36:52,168:INFO: Dataset: zara1               Batch: 2/8	Loss 93.9580 (92.8765)
2022-11-20 13:36:52,543:INFO: Dataset: zara1               Batch: 3/8	Loss 93.6206 (93.1078)
2022-11-20 13:36:52,916:INFO: Dataset: zara1               Batch: 4/8	Loss 92.0874 (92.8609)
2022-11-20 13:36:53,290:INFO: Dataset: zara1               Batch: 5/8	Loss 92.4599 (92.7812)
2022-11-20 13:36:53,671:INFO: Dataset: zara1               Batch: 6/8	Loss 91.3900 (92.5545)
2022-11-20 13:36:54,043:INFO: Dataset: zara1               Batch: 7/8	Loss 90.4791 (92.2425)
2022-11-20 13:36:54,403:INFO: Dataset: zara1               Batch: 8/8	Loss 78.5957 (90.6839)
2022-11-20 13:36:54,955:INFO: Dataset: zara2               Batch:  1/18	Loss 88.0714 (88.0714)
2022-11-20 13:36:55,336:INFO: Dataset: zara2               Batch:  2/18	Loss 86.9756 (87.5212)
2022-11-20 13:36:55,747:INFO: Dataset: zara2               Batch:  3/18	Loss 87.5794 (87.5394)
2022-11-20 13:36:56,134:INFO: Dataset: zara2               Batch:  4/18	Loss 86.5743 (87.2864)
2022-11-20 13:36:56,526:INFO: Dataset: zara2               Batch:  5/18	Loss 87.7880 (87.3826)
2022-11-20 13:36:56,932:INFO: Dataset: zara2               Batch:  6/18	Loss 87.7715 (87.4444)
2022-11-20 13:36:57,330:INFO: Dataset: zara2               Batch:  7/18	Loss 85.0976 (87.1136)
2022-11-20 13:36:57,712:INFO: Dataset: zara2               Batch:  8/18	Loss 87.5612 (87.1627)
2022-11-20 13:36:58,093:INFO: Dataset: zara2               Batch:  9/18	Loss 87.6533 (87.2147)
2022-11-20 13:36:58,481:INFO: Dataset: zara2               Batch: 10/18	Loss 86.5335 (87.1479)
2022-11-20 13:36:58,883:INFO: Dataset: zara2               Batch: 11/18	Loss 86.3941 (87.0823)
2022-11-20 13:36:59,286:INFO: Dataset: zara2               Batch: 12/18	Loss 86.8143 (87.0593)
2022-11-20 13:36:59,678:INFO: Dataset: zara2               Batch: 13/18	Loss 87.3438 (87.0798)
2022-11-20 13:37:00,065:INFO: Dataset: zara2               Batch: 14/18	Loss 87.8603 (87.1348)
2022-11-20 13:37:00,472:INFO: Dataset: zara2               Batch: 15/18	Loss 86.9739 (87.1246)
2022-11-20 13:37:00,871:INFO: Dataset: zara2               Batch: 16/18	Loss 85.4429 (87.0225)
2022-11-20 13:37:01,252:INFO: Dataset: zara2               Batch: 17/18	Loss 86.1729 (86.9731)
2022-11-20 13:37:01,635:INFO: Dataset: zara2               Batch: 18/18	Loss 74.9665 (86.3776)
2022-11-20 13:37:01,674:INFO: - Computing loss (validation)
2022-11-20 13:37:02,013:INFO: Dataset: hotel               Batch: 1/2	Loss 83.1057 (83.1057)
2022-11-20 13:37:02,166:INFO: Dataset: hotel               Batch: 2/2	Loss 6.5109 (78.4002)
2022-11-20 13:37:02,537:INFO: Dataset: univ                Batch: 1/3	Loss 87.6952 (87.6952)
2022-11-20 13:37:02,738:INFO: Dataset: univ                Batch: 2/3	Loss 87.2241 (87.4431)
2022-11-20 13:37:02,933:INFO: Dataset: univ                Batch: 3/3	Loss 81.2415 (85.5022)
2022-11-20 13:37:03,263:INFO: Dataset: zara1               Batch: 1/2	Loss 90.9692 (90.9692)
2022-11-20 13:37:03,430:INFO: Dataset: zara1               Batch: 2/2	Loss 29.9491 (75.0765)
2022-11-20 13:37:03,785:INFO: Dataset: zara2               Batch: 1/5	Loss 83.8270 (83.8270)
2022-11-20 13:37:03,980:INFO: Dataset: zara2               Batch: 2/5	Loss 84.0304 (83.9313)
2022-11-20 13:37:04,170:INFO: Dataset: zara2               Batch: 3/5	Loss 84.6223 (84.1659)
2022-11-20 13:37:04,361:INFO: Dataset: zara2               Batch: 4/5	Loss 84.9508 (84.3634)
2022-11-20 13:37:04,552:INFO: Dataset: zara2               Batch: 5/5	Loss 84.6473 (84.4196)
2022-11-20 13:37:04,578:INFO: - Computing ADE (validation)
2022-11-20 13:37:04,764:INFO: 		 ADE on hotel                     dataset:	 1.203313946723938
2022-11-20 13:37:05,041:INFO: 		 ADE on univ                      dataset:	 1.5984182357788086
2022-11-20 13:37:05,242:INFO: 		 ADE on zara1                     dataset:	 2.498746871948242
2022-11-20 13:37:05,568:INFO: 		 ADE on zara2                     dataset:	 1.589998483657837
2022-11-20 13:37:05,568:INFO: Average validation:	ADE  1.6260	FDE  2.8543
2022-11-20 13:37:05,569:INFO: - Computing ADE (training)
2022-11-20 13:37:05,827:INFO: 		 ADE on hotel                     dataset:	 1.5344693660736084
2022-11-20 13:37:06,544:INFO: 		 ADE on univ                      dataset:	 1.4947046041488647
2022-11-20 13:37:06,928:INFO: 		 ADE on zara1                     dataset:	 2.479623794555664
2022-11-20 13:37:07,634:INFO: 		 ADE on zara2                     dataset:	 1.8234870433807373
2022-11-20 13:37:07,634:INFO: Average training:	ADE  1.6252	FDE  2.8683
2022-11-20 13:37:07,652:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_412.pth.tar
2022-11-20 13:37:07,652:INFO: 
===> EPOCH: 413 (P4)
2022-11-20 13:37:07,653:INFO: - Computing loss (training)
2022-11-20 13:37:08,173:INFO: Dataset: hotel               Batch: 1/4	Loss 87.7097 (87.7097)
2022-11-20 13:37:08,544:INFO: Dataset: hotel               Batch: 2/4	Loss 86.6953 (87.1828)
2022-11-20 13:37:08,911:INFO: Dataset: hotel               Batch: 3/4	Loss 87.1327 (87.1660)
2022-11-20 13:37:09,248:INFO: Dataset: hotel               Batch: 4/4	Loss 51.7397 (80.7164)
2022-11-20 13:37:09,856:INFO: Dataset: univ                Batch:  1/15	Loss 86.8866 (86.8866)
2022-11-20 13:37:10,315:INFO: Dataset: univ                Batch:  2/15	Loss 87.1623 (87.0309)
2022-11-20 13:37:10,779:INFO: Dataset: univ                Batch:  3/15	Loss 86.0224 (86.6822)
2022-11-20 13:37:11,236:INFO: Dataset: univ                Batch:  4/15	Loss 85.4607 (86.3580)
2022-11-20 13:37:11,692:INFO: Dataset: univ                Batch:  5/15	Loss 86.3604 (86.3585)
2022-11-20 13:37:12,258:INFO: Dataset: univ                Batch:  6/15	Loss 86.1199 (86.3176)
2022-11-20 13:37:12,739:INFO: Dataset: univ                Batch:  7/15	Loss 85.5949 (86.2135)
2022-11-20 13:37:13,291:INFO: Dataset: univ                Batch:  8/15	Loss 85.2247 (86.0755)
2022-11-20 13:37:13,747:INFO: Dataset: univ                Batch:  9/15	Loss 85.9919 (86.0665)
2022-11-20 13:37:14,231:INFO: Dataset: univ                Batch: 10/15	Loss 85.8960 (86.0500)
2022-11-20 13:37:14,673:INFO: Dataset: univ                Batch: 11/15	Loss 87.0368 (86.1280)
2022-11-20 13:37:15,138:INFO: Dataset: univ                Batch: 12/15	Loss 86.1263 (86.1278)
2022-11-20 13:37:15,577:INFO: Dataset: univ                Batch: 13/15	Loss 86.0615 (86.1228)
2022-11-20 13:37:16,014:INFO: Dataset: univ                Batch: 14/15	Loss 85.2125 (86.0598)
2022-11-20 13:37:16,324:INFO: Dataset: univ                Batch: 15/15	Loss 15.6188 (84.8847)
2022-11-20 13:37:16,876:INFO: Dataset: zara1               Batch: 1/8	Loss 90.8115 (90.8115)
2022-11-20 13:37:17,250:INFO: Dataset: zara1               Batch: 2/8	Loss 90.7043 (90.7572)
2022-11-20 13:37:17,620:INFO: Dataset: zara1               Batch: 3/8	Loss 93.3876 (91.6411)
2022-11-20 13:37:17,990:INFO: Dataset: zara1               Batch: 4/8	Loss 90.9984 (91.4766)
2022-11-20 13:37:18,368:INFO: Dataset: zara1               Batch: 5/8	Loss 91.6430 (91.5064)
2022-11-20 13:37:18,739:INFO: Dataset: zara1               Batch: 6/8	Loss 91.2564 (91.4640)
2022-11-20 13:37:19,113:INFO: Dataset: zara1               Batch: 7/8	Loss 91.4066 (91.4558)
2022-11-20 13:37:19,471:INFO: Dataset: zara1               Batch: 8/8	Loss 80.3954 (90.3207)
2022-11-20 13:37:20,019:INFO: Dataset: zara2               Batch:  1/18	Loss 85.2723 (85.2723)
2022-11-20 13:37:20,404:INFO: Dataset: zara2               Batch:  2/18	Loss 85.9142 (85.5841)
2022-11-20 13:37:20,786:INFO: Dataset: zara2               Batch:  3/18	Loss 86.7492 (85.9869)
2022-11-20 13:37:21,161:INFO: Dataset: zara2               Batch:  4/18	Loss 86.1723 (86.0309)
2022-11-20 13:37:21,534:INFO: Dataset: zara2               Batch:  5/18	Loss 87.7823 (86.3605)
2022-11-20 13:37:21,906:INFO: Dataset: zara2               Batch:  6/18	Loss 86.3949 (86.3664)
2022-11-20 13:37:22,275:INFO: Dataset: zara2               Batch:  7/18	Loss 86.1156 (86.3336)
2022-11-20 13:37:22,646:INFO: Dataset: zara2               Batch:  8/18	Loss 86.8445 (86.3940)
2022-11-20 13:37:23,015:INFO: Dataset: zara2               Batch:  9/18	Loss 86.1314 (86.3641)
2022-11-20 13:37:23,385:INFO: Dataset: zara2               Batch: 10/18	Loss 86.7616 (86.4031)
2022-11-20 13:37:23,753:INFO: Dataset: zara2               Batch: 11/18	Loss 86.7583 (86.4368)
2022-11-20 13:37:24,124:INFO: Dataset: zara2               Batch: 12/18	Loss 85.8891 (86.3929)
2022-11-20 13:37:24,492:INFO: Dataset: zara2               Batch: 13/18	Loss 85.7752 (86.3476)
2022-11-20 13:37:24,873:INFO: Dataset: zara2               Batch: 14/18	Loss 88.4548 (86.4895)
2022-11-20 13:37:25,244:INFO: Dataset: zara2               Batch: 15/18	Loss 86.8690 (86.5124)
2022-11-20 13:37:25,615:INFO: Dataset: zara2               Batch: 16/18	Loss 86.8397 (86.5331)
2022-11-20 13:37:25,986:INFO: Dataset: zara2               Batch: 17/18	Loss 86.0683 (86.5048)
2022-11-20 13:37:26,345:INFO: Dataset: zara2               Batch: 18/18	Loss 74.4109 (85.9390)
2022-11-20 13:37:26,385:INFO: - Computing loss (validation)
2022-11-20 13:37:26,723:INFO: Dataset: hotel               Batch: 1/2	Loss 83.1928 (83.1928)
2022-11-20 13:37:26,876:INFO: Dataset: hotel               Batch: 2/2	Loss 6.4708 (78.4795)
2022-11-20 13:37:27,244:INFO: Dataset: univ                Batch: 1/3	Loss 87.2971 (87.2971)
2022-11-20 13:37:27,447:INFO: Dataset: univ                Batch: 2/3	Loss 86.1352 (86.6503)
2022-11-20 13:37:27,638:INFO: Dataset: univ                Batch: 3/3	Loss 80.9385 (85.1007)
2022-11-20 13:37:27,987:INFO: Dataset: zara1               Batch: 1/2	Loss 90.3626 (90.3626)
2022-11-20 13:37:28,156:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1385 (76.0327)
2022-11-20 13:37:28,499:INFO: Dataset: zara2               Batch: 1/5	Loss 83.7112 (83.7112)
2022-11-20 13:37:28,688:INFO: Dataset: zara2               Batch: 2/5	Loss 83.2737 (83.4880)
2022-11-20 13:37:28,878:INFO: Dataset: zara2               Batch: 3/5	Loss 84.4431 (83.7792)
2022-11-20 13:37:29,065:INFO: Dataset: zara2               Batch: 4/5	Loss 86.1524 (84.3429)
2022-11-20 13:37:29,251:INFO: Dataset: zara2               Batch: 5/5	Loss 82.9006 (84.0643)
2022-11-20 13:37:29,277:INFO: - Computing ADE (validation)
2022-11-20 13:37:29,482:INFO: 		 ADE on hotel                     dataset:	 1.2718833684921265
2022-11-20 13:37:29,735:INFO: 		 ADE on univ                      dataset:	 1.61247718334198
2022-11-20 13:37:29,920:INFO: 		 ADE on zara1                     dataset:	 2.513550281524658
2022-11-20 13:37:30,244:INFO: 		 ADE on zara2                     dataset:	 1.5796911716461182
2022-11-20 13:37:30,252:INFO: Average validation:	ADE  1.6342	FDE  2.8785
2022-11-20 13:37:30,253:INFO: - Computing ADE (training)
2022-11-20 13:37:30,530:INFO: 		 ADE on hotel                     dataset:	 1.5532405376434326
2022-11-20 13:37:31,253:INFO: 		 ADE on univ                      dataset:	 1.5072590112686157
2022-11-20 13:37:31,648:INFO: 		 ADE on zara1                     dataset:	 2.463057279586792
2022-11-20 13:37:32,359:INFO: 		 ADE on zara2                     dataset:	 1.8328402042388916
2022-11-20 13:37:32,359:INFO: Average training:	ADE  1.6354	FDE  2.8878
2022-11-20 13:37:32,376:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_413.pth.tar
2022-11-20 13:37:32,376:INFO: 
===> EPOCH: 414 (P4)
2022-11-20 13:37:32,376:INFO: - Computing loss (training)
2022-11-20 13:37:32,870:INFO: Dataset: hotel               Batch: 1/4	Loss 85.2297 (85.2297)
2022-11-20 13:37:33,242:INFO: Dataset: hotel               Batch: 2/4	Loss 86.2640 (85.7343)
2022-11-20 13:37:33,611:INFO: Dataset: hotel               Batch: 3/4	Loss 88.0599 (86.5391)
2022-11-20 13:37:33,943:INFO: Dataset: hotel               Batch: 4/4	Loss 53.0344 (80.7487)
2022-11-20 13:37:34,574:INFO: Dataset: univ                Batch:  1/15	Loss 85.0656 (85.0656)
2022-11-20 13:37:35,003:INFO: Dataset: univ                Batch:  2/15	Loss 87.6404 (86.2739)
2022-11-20 13:37:35,436:INFO: Dataset: univ                Batch:  3/15	Loss 85.5294 (86.0241)
2022-11-20 13:37:35,871:INFO: Dataset: univ                Batch:  4/15	Loss 86.8323 (86.2309)
2022-11-20 13:37:36,307:INFO: Dataset: univ                Batch:  5/15	Loss 85.2788 (86.0312)
2022-11-20 13:37:36,746:INFO: Dataset: univ                Batch:  6/15	Loss 85.7021 (85.9754)
2022-11-20 13:37:37,197:INFO: Dataset: univ                Batch:  7/15	Loss 84.7552 (85.7847)
2022-11-20 13:37:37,716:INFO: Dataset: univ                Batch:  8/15	Loss 85.8714 (85.7957)
2022-11-20 13:37:38,148:INFO: Dataset: univ                Batch:  9/15	Loss 84.5552 (85.6599)
2022-11-20 13:37:38,587:INFO: Dataset: univ                Batch: 10/15	Loss 85.7113 (85.6653)
2022-11-20 13:37:39,016:INFO: Dataset: univ                Batch: 11/15	Loss 87.5706 (85.8320)
2022-11-20 13:37:39,470:INFO: Dataset: univ                Batch: 12/15	Loss 86.5700 (85.8985)
2022-11-20 13:37:39,920:INFO: Dataset: univ                Batch: 13/15	Loss 84.8967 (85.8178)
2022-11-20 13:37:40,375:INFO: Dataset: univ                Batch: 14/15	Loss 84.0235 (85.6796)
2022-11-20 13:37:40,676:INFO: Dataset: univ                Batch: 15/15	Loss 16.7401 (84.8106)
2022-11-20 13:37:41,237:INFO: Dataset: zara1               Batch: 1/8	Loss 89.9515 (89.9515)
2022-11-20 13:37:41,603:INFO: Dataset: zara1               Batch: 2/8	Loss 92.3439 (91.0704)
2022-11-20 13:37:41,971:INFO: Dataset: zara1               Batch: 3/8	Loss 90.4891 (90.8806)
2022-11-20 13:37:42,339:INFO: Dataset: zara1               Batch: 4/8	Loss 90.3474 (90.7458)
2022-11-20 13:37:42,706:INFO: Dataset: zara1               Batch: 5/8	Loss 90.7122 (90.7389)
2022-11-20 13:37:43,072:INFO: Dataset: zara1               Batch: 6/8	Loss 91.2998 (90.8376)
2022-11-20 13:37:43,439:INFO: Dataset: zara1               Batch: 7/8	Loss 90.1704 (90.7361)
2022-11-20 13:37:43,791:INFO: Dataset: zara1               Batch: 8/8	Loss 77.4998 (89.0780)
2022-11-20 13:37:44,342:INFO: Dataset: zara2               Batch:  1/18	Loss 86.9872 (86.9872)
2022-11-20 13:37:44,716:INFO: Dataset: zara2               Batch:  2/18	Loss 84.6995 (85.8485)
2022-11-20 13:37:45,088:INFO: Dataset: zara2               Batch:  3/18	Loss 86.2907 (85.9969)
2022-11-20 13:37:45,471:INFO: Dataset: zara2               Batch:  4/18	Loss 86.2316 (86.0570)
2022-11-20 13:37:45,840:INFO: Dataset: zara2               Batch:  5/18	Loss 86.1992 (86.0868)
2022-11-20 13:37:46,213:INFO: Dataset: zara2               Batch:  6/18	Loss 85.1803 (85.9344)
2022-11-20 13:37:46,581:INFO: Dataset: zara2               Batch:  7/18	Loss 87.9754 (86.1992)
2022-11-20 13:37:46,951:INFO: Dataset: zara2               Batch:  8/18	Loss 85.5663 (86.1152)
2022-11-20 13:37:47,318:INFO: Dataset: zara2               Batch:  9/18	Loss 86.7153 (86.1843)
2022-11-20 13:37:47,688:INFO: Dataset: zara2               Batch: 10/18	Loss 85.9259 (86.1596)
2022-11-20 13:37:48,056:INFO: Dataset: zara2               Batch: 11/18	Loss 84.5750 (86.0105)
2022-11-20 13:37:48,427:INFO: Dataset: zara2               Batch: 12/18	Loss 86.1041 (86.0173)
2022-11-20 13:37:48,796:INFO: Dataset: zara2               Batch: 13/18	Loss 85.6545 (85.9854)
2022-11-20 13:37:49,166:INFO: Dataset: zara2               Batch: 14/18	Loss 85.0811 (85.9214)
2022-11-20 13:37:49,536:INFO: Dataset: zara2               Batch: 15/18	Loss 86.2167 (85.9400)
2022-11-20 13:37:49,907:INFO: Dataset: zara2               Batch: 16/18	Loss 85.8874 (85.9365)
2022-11-20 13:37:50,275:INFO: Dataset: zara2               Batch: 17/18	Loss 85.7333 (85.9258)
2022-11-20 13:37:50,632:INFO: Dataset: zara2               Batch: 18/18	Loss 73.5700 (85.3334)
2022-11-20 13:37:50,672:INFO: - Computing loss (validation)
2022-11-20 13:37:51,008:INFO: Dataset: hotel               Batch: 1/2	Loss 83.1037 (83.1037)
2022-11-20 13:37:51,158:INFO: Dataset: hotel               Batch: 2/2	Loss 6.6410 (76.8406)
2022-11-20 13:37:51,507:INFO: Dataset: univ                Batch: 1/3	Loss 86.8047 (86.8047)
2022-11-20 13:37:51,707:INFO: Dataset: univ                Batch: 2/3	Loss 85.9322 (86.3428)
2022-11-20 13:37:51,898:INFO: Dataset: univ                Batch: 3/3	Loss 80.4177 (84.7290)
2022-11-20 13:37:52,258:INFO: Dataset: zara1               Batch: 1/2	Loss 89.9630 (89.9630)
2022-11-20 13:37:52,424:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1989 (74.3324)
2022-11-20 13:37:52,764:INFO: Dataset: zara2               Batch: 1/5	Loss 83.7860 (83.7860)
2022-11-20 13:37:52,959:INFO: Dataset: zara2               Batch: 2/5	Loss 83.9863 (83.8843)
2022-11-20 13:37:53,149:INFO: Dataset: zara2               Batch: 3/5	Loss 83.0545 (83.5890)
2022-11-20 13:37:53,337:INFO: Dataset: zara2               Batch: 4/5	Loss 85.3166 (84.0165)
2022-11-20 13:37:53,528:INFO: Dataset: zara2               Batch: 5/5	Loss 82.1258 (83.6407)
2022-11-20 13:37:53,562:INFO: - Computing ADE (validation)
2022-11-20 13:37:53,762:INFO: 		 ADE on hotel                     dataset:	 1.2683151960372925
2022-11-20 13:37:54,020:INFO: 		 ADE on univ                      dataset:	 1.623275876045227
2022-11-20 13:37:54,220:INFO: 		 ADE on zara1                     dataset:	 2.491248369216919
2022-11-20 13:37:54,537:INFO: 		 ADE on zara2                     dataset:	 1.5833501815795898
2022-11-20 13:37:54,537:INFO: Average validation:	ADE  1.6397	FDE  2.9004
2022-11-20 13:37:54,538:INFO: - Computing ADE (training)
2022-11-20 13:37:54,804:INFO: 		 ADE on hotel                     dataset:	 1.601935625076294
2022-11-20 13:37:55,496:INFO: 		 ADE on univ                      dataset:	 1.525085687637329
2022-11-20 13:37:55,888:INFO: 		 ADE on zara1                     dataset:	 2.447608232498169
2022-11-20 13:37:56,617:INFO: 		 ADE on zara2                     dataset:	 1.853652000427246
2022-11-20 13:37:56,617:INFO: Average training:	ADE  1.6525	FDE  2.9224
2022-11-20 13:37:56,633:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_414.pth.tar
2022-11-20 13:37:56,633:INFO: 
===> EPOCH: 415 (P4)
2022-11-20 13:37:56,634:INFO: - Computing loss (training)
2022-11-20 13:37:57,152:INFO: Dataset: hotel               Batch: 1/4	Loss 86.2447 (86.2447)
2022-11-20 13:37:57,530:INFO: Dataset: hotel               Batch: 2/4	Loss 86.9319 (86.5771)
2022-11-20 13:37:57,903:INFO: Dataset: hotel               Batch: 3/4	Loss 87.5960 (86.8995)
2022-11-20 13:37:58,237:INFO: Dataset: hotel               Batch: 4/4	Loss 51.4083 (80.8594)
2022-11-20 13:37:58,851:INFO: Dataset: univ                Batch:  1/15	Loss 86.4233 (86.4233)
2022-11-20 13:37:59,306:INFO: Dataset: univ                Batch:  2/15	Loss 85.2413 (85.7919)
2022-11-20 13:37:59,737:INFO: Dataset: univ                Batch:  3/15	Loss 85.5639 (85.7211)
2022-11-20 13:38:00,184:INFO: Dataset: univ                Batch:  4/15	Loss 84.2787 (85.3577)
2022-11-20 13:38:00,637:INFO: Dataset: univ                Batch:  5/15	Loss 85.6601 (85.4224)
2022-11-20 13:38:01,079:INFO: Dataset: univ                Batch:  6/15	Loss 84.7946 (85.3173)
2022-11-20 13:38:01,504:INFO: Dataset: univ                Batch:  7/15	Loss 86.6999 (85.4933)
2022-11-20 13:38:01,956:INFO: Dataset: univ                Batch:  8/15	Loss 84.1929 (85.3212)
2022-11-20 13:38:02,410:INFO: Dataset: univ                Batch:  9/15	Loss 85.4117 (85.3319)
2022-11-20 13:38:02,864:INFO: Dataset: univ                Batch: 10/15	Loss 84.8414 (85.2802)
2022-11-20 13:38:03,296:INFO: Dataset: univ                Batch: 11/15	Loss 84.9770 (85.2543)
2022-11-20 13:38:03,736:INFO: Dataset: univ                Batch: 12/15	Loss 85.1889 (85.2489)
2022-11-20 13:38:04,166:INFO: Dataset: univ                Batch: 13/15	Loss 86.3010 (85.3234)
2022-11-20 13:38:04,625:INFO: Dataset: univ                Batch: 14/15	Loss 85.8713 (85.3635)
2022-11-20 13:38:04,930:INFO: Dataset: univ                Batch: 15/15	Loss 15.8553 (84.3424)
2022-11-20 13:38:05,471:INFO: Dataset: zara1               Batch: 1/8	Loss 89.6539 (89.6539)
2022-11-20 13:38:05,848:INFO: Dataset: zara1               Batch: 2/8	Loss 91.6362 (90.6616)
2022-11-20 13:38:06,227:INFO: Dataset: zara1               Batch: 3/8	Loss 89.4950 (90.2599)
2022-11-20 13:38:06,606:INFO: Dataset: zara1               Batch: 4/8	Loss 90.3434 (90.2803)
2022-11-20 13:38:06,985:INFO: Dataset: zara1               Batch: 5/8	Loss 90.3919 (90.3015)
2022-11-20 13:38:07,363:INFO: Dataset: zara1               Batch: 6/8	Loss 89.6697 (90.1963)
2022-11-20 13:38:07,741:INFO: Dataset: zara1               Batch: 7/8	Loss 89.0892 (90.0259)
2022-11-20 13:38:08,105:INFO: Dataset: zara1               Batch: 8/8	Loss 77.6909 (88.6560)
2022-11-20 13:38:08,702:INFO: Dataset: zara2               Batch:  1/18	Loss 85.8818 (85.8818)
2022-11-20 13:38:09,077:INFO: Dataset: zara2               Batch:  2/18	Loss 85.1458 (85.4944)
2022-11-20 13:38:09,449:INFO: Dataset: zara2               Batch:  3/18	Loss 86.1965 (85.7558)
2022-11-20 13:38:09,819:INFO: Dataset: zara2               Batch:  4/18	Loss 86.3510 (85.9046)
2022-11-20 13:38:10,191:INFO: Dataset: zara2               Batch:  5/18	Loss 85.4318 (85.8058)
2022-11-20 13:38:10,565:INFO: Dataset: zara2               Batch:  6/18	Loss 86.0069 (85.8389)
2022-11-20 13:38:10,936:INFO: Dataset: zara2               Batch:  7/18	Loss 85.6833 (85.8159)
2022-11-20 13:38:11,305:INFO: Dataset: zara2               Batch:  8/18	Loss 84.5501 (85.6655)
2022-11-20 13:38:11,675:INFO: Dataset: zara2               Batch:  9/18	Loss 85.7930 (85.6808)
2022-11-20 13:38:12,046:INFO: Dataset: zara2               Batch: 10/18	Loss 85.3063 (85.6465)
2022-11-20 13:38:12,418:INFO: Dataset: zara2               Batch: 11/18	Loss 85.3519 (85.6183)
2022-11-20 13:38:12,791:INFO: Dataset: zara2               Batch: 12/18	Loss 85.6457 (85.6205)
2022-11-20 13:38:13,163:INFO: Dataset: zara2               Batch: 13/18	Loss 85.5567 (85.6159)
2022-11-20 13:38:13,535:INFO: Dataset: zara2               Batch: 14/18	Loss 84.4819 (85.5379)
2022-11-20 13:38:13,908:INFO: Dataset: zara2               Batch: 15/18	Loss 85.5557 (85.5391)
2022-11-20 13:38:14,280:INFO: Dataset: zara2               Batch: 16/18	Loss 84.7333 (85.4835)
2022-11-20 13:38:14,652:INFO: Dataset: zara2               Batch: 17/18	Loss 84.8102 (85.4389)
2022-11-20 13:38:15,011:INFO: Dataset: zara2               Batch: 18/18	Loss 73.4533 (84.9138)
2022-11-20 13:38:15,054:INFO: - Computing loss (validation)
2022-11-20 13:38:15,388:INFO: Dataset: hotel               Batch: 1/2	Loss 82.9752 (82.9752)
2022-11-20 13:38:15,542:INFO: Dataset: hotel               Batch: 2/2	Loss 6.8227 (75.1780)
2022-11-20 13:38:15,909:INFO: Dataset: univ                Batch: 1/3	Loss 86.1040 (86.1040)
2022-11-20 13:38:16,111:INFO: Dataset: univ                Batch: 2/3	Loss 85.6024 (85.8627)
2022-11-20 13:38:16,308:INFO: Dataset: univ                Batch: 3/3	Loss 80.1206 (84.0924)
2022-11-20 13:38:16,677:INFO: Dataset: zara1               Batch: 1/2	Loss 88.8093 (88.8093)
2022-11-20 13:38:16,847:INFO: Dataset: zara1               Batch: 2/2	Loss 29.3066 (75.0338)
2022-11-20 13:38:17,193:INFO: Dataset: zara2               Batch: 1/5	Loss 84.0255 (84.0255)
2022-11-20 13:38:17,385:INFO: Dataset: zara2               Batch: 2/5	Loss 83.3816 (83.7010)
2022-11-20 13:38:17,580:INFO: Dataset: zara2               Batch: 3/5	Loss 83.1406 (83.5064)
2022-11-20 13:38:17,774:INFO: Dataset: zara2               Batch: 4/5	Loss 84.5475 (83.7934)
2022-11-20 13:38:17,965:INFO: Dataset: zara2               Batch: 5/5	Loss 80.8081 (83.2091)
2022-11-20 13:38:17,990:INFO: - Computing ADE (validation)
2022-11-20 13:38:18,197:INFO: 		 ADE on hotel                     dataset:	 1.4004178047180176
2022-11-20 13:38:18,445:INFO: 		 ADE on univ                      dataset:	 1.6466306447982788
2022-11-20 13:38:18,638:INFO: 		 ADE on zara1                     dataset:	 2.4790167808532715
2022-11-20 13:38:18,949:INFO: 		 ADE on zara2                     dataset:	 1.6435343027114868
2022-11-20 13:38:18,950:INFO: Average validation:	ADE  1.6804	FDE  2.9833
2022-11-20 13:38:18,951:INFO: - Computing ADE (training)
2022-11-20 13:38:19,220:INFO: 		 ADE on hotel                     dataset:	 1.683427095413208
2022-11-20 13:38:19,928:INFO: 		 ADE on univ                      dataset:	 1.5615341663360596
2022-11-20 13:38:20,353:INFO: 		 ADE on zara1                     dataset:	 2.429852247238159
2022-11-20 13:38:21,068:INFO: 		 ADE on zara2                     dataset:	 1.8742550611495972
2022-11-20 13:38:21,068:INFO: Average training:	ADE  1.6834	FDE  2.9901
2022-11-20 13:38:21,084:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_415.pth.tar
2022-11-20 13:38:21,084:INFO: 
===> EPOCH: 416 (P4)
2022-11-20 13:38:21,085:INFO: - Computing loss (training)
2022-11-20 13:38:21,593:INFO: Dataset: hotel               Batch: 1/4	Loss 85.5914 (85.5914)
2022-11-20 13:38:21,975:INFO: Dataset: hotel               Batch: 2/4	Loss 88.2712 (86.9739)
2022-11-20 13:38:22,349:INFO: Dataset: hotel               Batch: 3/4	Loss 86.4401 (86.8065)
2022-11-20 13:38:22,688:INFO: Dataset: hotel               Batch: 4/4	Loss 52.9783 (81.5850)
2022-11-20 13:38:23,320:INFO: Dataset: univ                Batch:  1/15	Loss 85.3145 (85.3145)
2022-11-20 13:38:23,752:INFO: Dataset: univ                Batch:  2/15	Loss 87.0968 (86.1493)
2022-11-20 13:38:24,181:INFO: Dataset: univ                Batch:  3/15	Loss 85.2994 (85.8859)
2022-11-20 13:38:24,614:INFO: Dataset: univ                Batch:  4/15	Loss 85.7915 (85.8626)
2022-11-20 13:38:25,057:INFO: Dataset: univ                Batch:  5/15	Loss 84.8206 (85.6444)
2022-11-20 13:38:25,515:INFO: Dataset: univ                Batch:  6/15	Loss 83.5626 (85.2583)
2022-11-20 13:38:25,968:INFO: Dataset: univ                Batch:  7/15	Loss 86.3810 (85.4289)
2022-11-20 13:38:26,398:INFO: Dataset: univ                Batch:  8/15	Loss 85.4680 (85.4335)
2022-11-20 13:38:26,831:INFO: Dataset: univ                Batch:  9/15	Loss 85.4525 (85.4356)
2022-11-20 13:38:27,279:INFO: Dataset: univ                Batch: 10/15	Loss 84.0723 (85.2928)
2022-11-20 13:38:27,717:INFO: Dataset: univ                Batch: 11/15	Loss 84.4143 (85.2122)
2022-11-20 13:38:28,170:INFO: Dataset: univ                Batch: 12/15	Loss 84.5370 (85.1523)
2022-11-20 13:38:28,598:INFO: Dataset: univ                Batch: 13/15	Loss 85.0191 (85.1428)
2022-11-20 13:38:29,055:INFO: Dataset: univ                Batch: 14/15	Loss 83.9296 (85.0464)
2022-11-20 13:38:29,440:INFO: Dataset: univ                Batch: 15/15	Loss 15.7427 (84.1005)
2022-11-20 13:38:29,979:INFO: Dataset: zara1               Batch: 1/8	Loss 89.8483 (89.8483)
2022-11-20 13:38:30,353:INFO: Dataset: zara1               Batch: 2/8	Loss 88.7104 (89.3066)
2022-11-20 13:38:30,728:INFO: Dataset: zara1               Batch: 3/8	Loss 89.6252 (89.4153)
2022-11-20 13:38:31,100:INFO: Dataset: zara1               Batch: 4/8	Loss 89.4956 (89.4324)
2022-11-20 13:38:31,473:INFO: Dataset: zara1               Batch: 5/8	Loss 89.0838 (89.3620)
2022-11-20 13:38:31,847:INFO: Dataset: zara1               Batch: 6/8	Loss 89.0756 (89.3130)
2022-11-20 13:38:32,220:INFO: Dataset: zara1               Batch: 7/8	Loss 89.2960 (89.3107)
2022-11-20 13:38:32,582:INFO: Dataset: zara1               Batch: 8/8	Loss 75.7583 (87.8770)
2022-11-20 13:38:33,149:INFO: Dataset: zara2               Batch:  1/18	Loss 84.9332 (84.9332)
2022-11-20 13:38:33,529:INFO: Dataset: zara2               Batch:  2/18	Loss 85.3826 (85.1609)
2022-11-20 13:38:33,904:INFO: Dataset: zara2               Batch:  3/18	Loss 84.5934 (84.9632)
2022-11-20 13:38:34,277:INFO: Dataset: zara2               Batch:  4/18	Loss 84.2229 (84.7926)
2022-11-20 13:38:34,650:INFO: Dataset: zara2               Batch:  5/18	Loss 84.5281 (84.7396)
2022-11-20 13:38:35,028:INFO: Dataset: zara2               Batch:  6/18	Loss 86.1745 (84.9551)
2022-11-20 13:38:35,400:INFO: Dataset: zara2               Batch:  7/18	Loss 85.0757 (84.9737)
2022-11-20 13:38:35,772:INFO: Dataset: zara2               Batch:  8/18	Loss 84.9957 (84.9764)
2022-11-20 13:38:36,145:INFO: Dataset: zara2               Batch:  9/18	Loss 86.1587 (85.0948)
2022-11-20 13:38:36,519:INFO: Dataset: zara2               Batch: 10/18	Loss 83.7977 (84.9522)
2022-11-20 13:38:36,891:INFO: Dataset: zara2               Batch: 11/18	Loss 84.8237 (84.9397)
2022-11-20 13:38:37,265:INFO: Dataset: zara2               Batch: 12/18	Loss 83.6726 (84.8304)
2022-11-20 13:38:37,638:INFO: Dataset: zara2               Batch: 13/18	Loss 83.8896 (84.7591)
2022-11-20 13:38:38,014:INFO: Dataset: zara2               Batch: 14/18	Loss 85.6967 (84.8254)
2022-11-20 13:38:38,387:INFO: Dataset: zara2               Batch: 15/18	Loss 85.0811 (84.8434)
2022-11-20 13:38:38,762:INFO: Dataset: zara2               Batch: 16/18	Loss 84.8674 (84.8449)
2022-11-20 13:38:39,135:INFO: Dataset: zara2               Batch: 17/18	Loss 84.5322 (84.8283)
2022-11-20 13:38:39,497:INFO: Dataset: zara2               Batch: 18/18	Loss 72.8830 (84.2458)
2022-11-20 13:38:39,535:INFO: - Computing loss (validation)
2022-11-20 13:38:39,869:INFO: Dataset: hotel               Batch: 1/2	Loss 83.3864 (83.3864)
2022-11-20 13:38:40,020:INFO: Dataset: hotel               Batch: 2/2	Loss 6.6979 (79.4604)
2022-11-20 13:38:40,389:INFO: Dataset: univ                Batch: 1/3	Loss 85.4055 (85.4055)
2022-11-20 13:38:40,598:INFO: Dataset: univ                Batch: 2/3	Loss 84.8687 (85.0963)
2022-11-20 13:38:40,798:INFO: Dataset: univ                Batch: 3/3	Loss 79.0600 (83.3524)
2022-11-20 13:38:41,150:INFO: Dataset: zara1               Batch: 1/2	Loss 88.0458 (88.0458)
2022-11-20 13:38:41,322:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6111 (71.8016)
2022-11-20 13:38:41,691:INFO: Dataset: zara2               Batch: 1/5	Loss 83.5281 (83.5281)
2022-11-20 13:38:41,884:INFO: Dataset: zara2               Batch: 2/5	Loss 83.0068 (83.2665)
2022-11-20 13:38:42,077:INFO: Dataset: zara2               Batch: 3/5	Loss 82.6767 (83.0737)
2022-11-20 13:38:42,269:INFO: Dataset: zara2               Batch: 4/5	Loss 82.6950 (82.9815)
2022-11-20 13:38:42,462:INFO: Dataset: zara2               Batch: 5/5	Loss 82.2378 (82.8325)
2022-11-20 13:38:42,494:INFO: - Computing ADE (validation)
2022-11-20 13:38:42,705:INFO: 		 ADE on hotel                     dataset:	 1.4686650037765503
2022-11-20 13:38:42,966:INFO: 		 ADE on univ                      dataset:	 1.7008838653564453
2022-11-20 13:38:43,166:INFO: 		 ADE on zara1                     dataset:	 2.3757429122924805
2022-11-20 13:38:43,472:INFO: 		 ADE on zara2                     dataset:	 1.7145159244537354
2022-11-20 13:38:43,473:INFO: Average validation:	ADE  1.7324	FDE  3.1009
2022-11-20 13:38:43,474:INFO: - Computing ADE (training)
2022-11-20 13:38:43,748:INFO: 		 ADE on hotel                     dataset:	 1.7637914419174194
2022-11-20 13:38:44,444:INFO: 		 ADE on univ                      dataset:	 1.6129391193389893
2022-11-20 13:38:44,829:INFO: 		 ADE on zara1                     dataset:	 2.4248766899108887
2022-11-20 13:38:45,541:INFO: 		 ADE on zara2                     dataset:	 1.906644344329834
2022-11-20 13:38:45,542:INFO: Average training:	ADE  1.7281	FDE  3.0933
2022-11-20 13:38:45,558:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_416.pth.tar
2022-11-20 13:38:45,558:INFO: 
===> EPOCH: 417 (P4)
2022-11-20 13:38:45,559:INFO: - Computing loss (training)
2022-11-20 13:38:46,068:INFO: Dataset: hotel               Batch: 1/4	Loss 86.5816 (86.5816)
2022-11-20 13:38:46,444:INFO: Dataset: hotel               Batch: 2/4	Loss 87.1220 (86.8562)
2022-11-20 13:38:46,813:INFO: Dataset: hotel               Batch: 3/4	Loss 88.5454 (87.3970)
2022-11-20 13:38:47,148:INFO: Dataset: hotel               Batch: 4/4	Loss 51.8718 (81.4449)
2022-11-20 13:38:47,762:INFO: Dataset: univ                Batch:  1/15	Loss 85.0857 (85.0857)
2022-11-20 13:38:48,220:INFO: Dataset: univ                Batch:  2/15	Loss 83.0595 (83.9943)
2022-11-20 13:38:48,655:INFO: Dataset: univ                Batch:  3/15	Loss 85.2016 (84.3747)
2022-11-20 13:38:49,093:INFO: Dataset: univ                Batch:  4/15	Loss 85.2765 (84.6022)
2022-11-20 13:38:49,523:INFO: Dataset: univ                Batch:  5/15	Loss 84.8536 (84.6507)
2022-11-20 13:38:49,979:INFO: Dataset: univ                Batch:  6/15	Loss 84.8183 (84.6807)
2022-11-20 13:38:50,428:INFO: Dataset: univ                Batch:  7/15	Loss 84.6169 (84.6713)
2022-11-20 13:38:50,881:INFO: Dataset: univ                Batch:  8/15	Loss 84.8724 (84.6983)
2022-11-20 13:38:51,330:INFO: Dataset: univ                Batch:  9/15	Loss 84.5891 (84.6859)
2022-11-20 13:38:51,759:INFO: Dataset: univ                Batch: 10/15	Loss 84.3839 (84.6577)
2022-11-20 13:38:52,199:INFO: Dataset: univ                Batch: 11/15	Loss 84.4004 (84.6347)
2022-11-20 13:38:52,626:INFO: Dataset: univ                Batch: 12/15	Loss 85.3231 (84.6861)
2022-11-20 13:38:53,079:INFO: Dataset: univ                Batch: 13/15	Loss 83.7575 (84.6094)
2022-11-20 13:38:53,521:INFO: Dataset: univ                Batch: 14/15	Loss 84.5240 (84.6031)
2022-11-20 13:38:53,823:INFO: Dataset: univ                Batch: 15/15	Loss 15.6996 (83.7541)
2022-11-20 13:38:54,370:INFO: Dataset: zara1               Batch: 1/8	Loss 88.8908 (88.8908)
2022-11-20 13:38:54,738:INFO: Dataset: zara1               Batch: 2/8	Loss 88.0354 (88.4294)
2022-11-20 13:38:55,105:INFO: Dataset: zara1               Batch: 3/8	Loss 88.4312 (88.4300)
2022-11-20 13:38:55,474:INFO: Dataset: zara1               Batch: 4/8	Loss 87.3492 (88.1911)
2022-11-20 13:38:55,842:INFO: Dataset: zara1               Batch: 5/8	Loss 87.8289 (88.1198)
2022-11-20 13:38:56,215:INFO: Dataset: zara1               Batch: 6/8	Loss 86.6601 (87.8349)
2022-11-20 13:38:56,583:INFO: Dataset: zara1               Batch: 7/8	Loss 88.6283 (87.9617)
2022-11-20 13:38:56,942:INFO: Dataset: zara1               Batch: 8/8	Loss 76.2622 (86.6255)
2022-11-20 13:38:57,510:INFO: Dataset: zara2               Batch:  1/18	Loss 84.7849 (84.7849)
2022-11-20 13:38:57,889:INFO: Dataset: zara2               Batch:  2/18	Loss 83.5236 (84.1505)
2022-11-20 13:38:58,270:INFO: Dataset: zara2               Batch:  3/18	Loss 84.5276 (84.2672)
2022-11-20 13:38:58,646:INFO: Dataset: zara2               Batch:  4/18	Loss 85.1633 (84.5179)
2022-11-20 13:38:59,025:INFO: Dataset: zara2               Batch:  5/18	Loss 83.8882 (84.3919)
2022-11-20 13:38:59,404:INFO: Dataset: zara2               Batch:  6/18	Loss 84.4500 (84.4017)
2022-11-20 13:38:59,780:INFO: Dataset: zara2               Batch:  7/18	Loss 84.2393 (84.3781)
2022-11-20 13:39:00,161:INFO: Dataset: zara2               Batch:  8/18	Loss 83.8367 (84.3096)
2022-11-20 13:39:00,539:INFO: Dataset: zara2               Batch:  9/18	Loss 84.7726 (84.3614)
2022-11-20 13:39:00,915:INFO: Dataset: zara2               Batch: 10/18	Loss 84.2257 (84.3475)
2022-11-20 13:39:01,293:INFO: Dataset: zara2               Batch: 11/18	Loss 84.7276 (84.3787)
2022-11-20 13:39:01,669:INFO: Dataset: zara2               Batch: 12/18	Loss 83.8399 (84.3326)
2022-11-20 13:39:02,046:INFO: Dataset: zara2               Batch: 13/18	Loss 85.1607 (84.3966)
2022-11-20 13:39:02,423:INFO: Dataset: zara2               Batch: 14/18	Loss 83.6425 (84.3455)
2022-11-20 13:39:02,800:INFO: Dataset: zara2               Batch: 15/18	Loss 83.3512 (84.2800)
2022-11-20 13:39:03,177:INFO: Dataset: zara2               Batch: 16/18	Loss 84.1972 (84.2749)
2022-11-20 13:39:03,554:INFO: Dataset: zara2               Batch: 17/18	Loss 83.9600 (84.2550)
2022-11-20 13:39:03,918:INFO: Dataset: zara2               Batch: 18/18	Loss 72.4102 (83.6911)
2022-11-20 13:39:03,956:INFO: - Computing loss (validation)
2022-11-20 13:39:04,299:INFO: Dataset: hotel               Batch: 1/2	Loss 84.1286 (84.1286)
2022-11-20 13:39:04,458:INFO: Dataset: hotel               Batch: 2/2	Loss 6.6783 (77.7846)
2022-11-20 13:39:04,810:INFO: Dataset: univ                Batch: 1/3	Loss 85.6907 (85.6907)
2022-11-20 13:39:05,009:INFO: Dataset: univ                Batch: 2/3	Loss 85.8694 (85.7827)
2022-11-20 13:39:05,204:INFO: Dataset: univ                Batch: 3/3	Loss 77.7579 (83.1126)
2022-11-20 13:39:05,548:INFO: Dataset: zara1               Batch: 1/2	Loss 86.6563 (86.6563)
2022-11-20 13:39:05,721:INFO: Dataset: zara1               Batch: 2/2	Loss 28.8710 (74.0216)
2022-11-20 13:39:06,072:INFO: Dataset: zara2               Batch: 1/5	Loss 82.8609 (82.8609)
2022-11-20 13:39:06,269:INFO: Dataset: zara2               Batch: 2/5	Loss 82.1549 (82.5176)
2022-11-20 13:39:06,462:INFO: Dataset: zara2               Batch: 3/5	Loss 82.7708 (82.6002)
2022-11-20 13:39:06,653:INFO: Dataset: zara2               Batch: 4/5	Loss 82.9261 (82.6814)
2022-11-20 13:39:06,844:INFO: Dataset: zara2               Batch: 5/5	Loss 82.2440 (82.5956)
2022-11-20 13:39:06,877:INFO: - Computing ADE (validation)
2022-11-20 13:39:07,071:INFO: 		 ADE on hotel                     dataset:	 1.5775315761566162
2022-11-20 13:39:07,335:INFO: 		 ADE on univ                      dataset:	 1.7808558940887451
2022-11-20 13:39:07,551:INFO: 		 ADE on zara1                     dataset:	 2.509397506713867
2022-11-20 13:39:07,864:INFO: 		 ADE on zara2                     dataset:	 1.7404226064682007
2022-11-20 13:39:07,864:INFO: Average validation:	ADE  1.7972	FDE  3.2562
2022-11-20 13:39:07,865:INFO: - Computing ADE (training)
2022-11-20 13:39:08,137:INFO: 		 ADE on hotel                     dataset:	 1.8480660915374756
2022-11-20 13:39:08,827:INFO: 		 ADE on univ                      dataset:	 1.6697310209274292
2022-11-20 13:39:09,220:INFO: 		 ADE on zara1                     dataset:	 2.449841260910034
2022-11-20 13:39:09,933:INFO: 		 ADE on zara2                     dataset:	 1.963378667831421
2022-11-20 13:39:09,933:INFO: Average training:	ADE  1.7836	FDE  3.2323
2022-11-20 13:39:09,949:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_417.pth.tar
2022-11-20 13:39:09,949:INFO: 
===> EPOCH: 418 (P4)
2022-11-20 13:39:09,950:INFO: - Computing loss (training)
2022-11-20 13:39:10,451:INFO: Dataset: hotel               Batch: 1/4	Loss 87.6964 (87.6964)
2022-11-20 13:39:10,826:INFO: Dataset: hotel               Batch: 2/4	Loss 87.2282 (87.4730)
2022-11-20 13:39:11,195:INFO: Dataset: hotel               Batch: 3/4	Loss 86.2478 (87.0562)
2022-11-20 13:39:11,530:INFO: Dataset: hotel               Batch: 4/4	Loss 53.5103 (81.3472)
2022-11-20 13:39:12,141:INFO: Dataset: univ                Batch:  1/15	Loss 84.3444 (84.3444)
2022-11-20 13:39:12,603:INFO: Dataset: univ                Batch:  2/15	Loss 84.6382 (84.4960)
2022-11-20 13:39:13,042:INFO: Dataset: univ                Batch:  3/15	Loss 85.7215 (84.8725)
2022-11-20 13:39:13,468:INFO: Dataset: univ                Batch:  4/15	Loss 84.2807 (84.7429)
2022-11-20 13:39:13,904:INFO: Dataset: univ                Batch:  5/15	Loss 84.0362 (84.6117)
2022-11-20 13:39:14,347:INFO: Dataset: univ                Batch:  6/15	Loss 84.1104 (84.5267)
2022-11-20 13:39:14,800:INFO: Dataset: univ                Batch:  7/15	Loss 85.1388 (84.6124)
2022-11-20 13:39:15,240:INFO: Dataset: univ                Batch:  8/15	Loss 83.9823 (84.5358)
2022-11-20 13:39:15,718:INFO: Dataset: univ                Batch:  9/15	Loss 83.4071 (84.3983)
2022-11-20 13:39:16,188:INFO: Dataset: univ                Batch: 10/15	Loss 83.6084 (84.3191)
2022-11-20 13:39:16,703:INFO: Dataset: univ                Batch: 11/15	Loss 84.1448 (84.3040)
2022-11-20 13:39:17,206:INFO: Dataset: univ                Batch: 12/15	Loss 83.5994 (84.2418)
2022-11-20 13:39:17,664:INFO: Dataset: univ                Batch: 13/15	Loss 84.0563 (84.2276)
2022-11-20 13:39:18,122:INFO: Dataset: univ                Batch: 14/15	Loss 84.5854 (84.2499)
2022-11-20 13:39:18,444:INFO: Dataset: univ                Batch: 15/15	Loss 15.3427 (83.1462)
2022-11-20 13:39:19,009:INFO: Dataset: zara1               Batch: 1/8	Loss 86.6799 (86.6799)
2022-11-20 13:39:19,403:INFO: Dataset: zara1               Batch: 2/8	Loss 87.4651 (87.0136)
2022-11-20 13:39:19,786:INFO: Dataset: zara1               Batch: 3/8	Loss 87.2524 (87.0965)
2022-11-20 13:39:20,179:INFO: Dataset: zara1               Batch: 4/8	Loss 87.5951 (87.2208)
2022-11-20 13:39:20,575:INFO: Dataset: zara1               Batch: 5/8	Loss 86.7459 (87.1226)
2022-11-20 13:39:20,982:INFO: Dataset: zara1               Batch: 6/8	Loss 87.6020 (87.1985)
2022-11-20 13:39:21,380:INFO: Dataset: zara1               Batch: 7/8	Loss 87.5690 (87.2451)
2022-11-20 13:39:21,761:INFO: Dataset: zara1               Batch: 8/8	Loss 75.9046 (85.9141)
2022-11-20 13:39:22,579:INFO: Dataset: zara2               Batch:  1/18	Loss 83.5280 (83.5280)
2022-11-20 13:39:23,220:INFO: Dataset: zara2               Batch:  2/18	Loss 85.3097 (84.3895)
2022-11-20 13:39:23,617:INFO: Dataset: zara2               Batch:  3/18	Loss 83.0830 (83.9794)
2022-11-20 13:39:24,017:INFO: Dataset: zara2               Batch:  4/18	Loss 83.4679 (83.8545)
2022-11-20 13:39:24,420:INFO: Dataset: zara2               Batch:  5/18	Loss 83.6597 (83.8176)
2022-11-20 13:39:24,811:INFO: Dataset: zara2               Batch:  6/18	Loss 83.7534 (83.8080)
2022-11-20 13:39:25,201:INFO: Dataset: zara2               Batch:  7/18	Loss 84.0439 (83.8426)
2022-11-20 13:39:25,588:INFO: Dataset: zara2               Batch:  8/18	Loss 83.6071 (83.8166)
2022-11-20 13:39:25,975:INFO: Dataset: zara2               Batch:  9/18	Loss 84.5780 (83.9031)
2022-11-20 13:39:26,362:INFO: Dataset: zara2               Batch: 10/18	Loss 83.8912 (83.9018)
2022-11-20 13:39:26,750:INFO: Dataset: zara2               Batch: 11/18	Loss 83.0068 (83.8129)
2022-11-20 13:39:27,139:INFO: Dataset: zara2               Batch: 12/18	Loss 84.0399 (83.8322)
2022-11-20 13:39:27,527:INFO: Dataset: zara2               Batch: 13/18	Loss 83.8167 (83.8310)
2022-11-20 13:39:27,914:INFO: Dataset: zara2               Batch: 14/18	Loss 84.2349 (83.8573)
2022-11-20 13:39:28,307:INFO: Dataset: zara2               Batch: 15/18	Loss 84.2941 (83.8847)
2022-11-20 13:39:28,700:INFO: Dataset: zara2               Batch: 16/18	Loss 83.9588 (83.8891)
2022-11-20 13:39:29,087:INFO: Dataset: zara2               Batch: 17/18	Loss 83.8905 (83.8892)
2022-11-20 13:39:29,461:INFO: Dataset: zara2               Batch: 18/18	Loss 72.1142 (83.3188)
2022-11-20 13:39:29,510:INFO: - Computing loss (validation)
2022-11-20 13:39:29,846:INFO: Dataset: hotel               Batch: 1/2	Loss 84.3580 (84.3580)
2022-11-20 13:39:29,997:INFO: Dataset: hotel               Batch: 2/2	Loss 6.4563 (80.1040)
2022-11-20 13:39:30,364:INFO: Dataset: univ                Batch: 1/3	Loss 85.1711 (85.1711)
2022-11-20 13:39:30,564:INFO: Dataset: univ                Batch: 2/3	Loss 83.7468 (84.3982)
2022-11-20 13:39:30,762:INFO: Dataset: univ                Batch: 3/3	Loss 78.1018 (82.1832)
2022-11-20 13:39:31,115:INFO: Dataset: zara1               Batch: 1/2	Loss 86.7878 (86.7878)
2022-11-20 13:39:31,285:INFO: Dataset: zara1               Batch: 2/2	Loss 28.1626 (69.6338)
2022-11-20 13:39:31,642:INFO: Dataset: zara2               Batch: 1/5	Loss 82.4992 (82.4992)
2022-11-20 13:39:31,839:INFO: Dataset: zara2               Batch: 2/5	Loss 82.1217 (82.3158)
2022-11-20 13:39:32,035:INFO: Dataset: zara2               Batch: 3/5	Loss 82.3398 (82.3238)
2022-11-20 13:39:32,230:INFO: Dataset: zara2               Batch: 4/5	Loss 82.5029 (82.3711)
2022-11-20 13:39:32,422:INFO: Dataset: zara2               Batch: 5/5	Loss 81.7203 (82.2450)
2022-11-20 13:39:32,449:INFO: - Computing ADE (validation)
2022-11-20 13:39:32,658:INFO: 		 ADE on hotel                     dataset:	 1.702822208404541
2022-11-20 13:39:32,925:INFO: 		 ADE on univ                      dataset:	 1.802230715751648
2022-11-20 13:39:33,132:INFO: 		 ADE on zara1                     dataset:	 2.4071991443634033
2022-11-20 13:39:33,455:INFO: 		 ADE on zara2                     dataset:	 1.794166088104248
2022-11-20 13:39:33,456:INFO: Average validation:	ADE  1.8290	FDE  3.3550
2022-11-20 13:39:33,457:INFO: - Computing ADE (training)
2022-11-20 13:39:33,745:INFO: 		 ADE on hotel                     dataset:	 1.9206321239471436
2022-11-20 13:39:34,440:INFO: 		 ADE on univ                      dataset:	 1.7221184968948364
2022-11-20 13:39:34,830:INFO: 		 ADE on zara1                     dataset:	 2.3859145641326904
2022-11-20 13:39:35,537:INFO: 		 ADE on zara2                     dataset:	 1.9863768815994263
2022-11-20 13:39:35,538:INFO: Average training:	ADE  1.8231	FDE  3.3472
2022-11-20 13:39:35,555:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_418.pth.tar
2022-11-20 13:39:35,555:INFO: 
===> EPOCH: 419 (P4)
2022-11-20 13:39:35,555:INFO: - Computing loss (training)
2022-11-20 13:39:36,075:INFO: Dataset: hotel               Batch: 1/4	Loss 86.1585 (86.1585)
2022-11-20 13:39:36,451:INFO: Dataset: hotel               Batch: 2/4	Loss 89.7602 (87.8444)
2022-11-20 13:39:36,826:INFO: Dataset: hotel               Batch: 3/4	Loss 86.7421 (87.4672)
2022-11-20 13:39:37,161:INFO: Dataset: hotel               Batch: 4/4	Loss 53.4193 (82.3016)
2022-11-20 13:39:37,742:INFO: Dataset: univ                Batch:  1/15	Loss 84.9818 (84.9818)
2022-11-20 13:39:38,196:INFO: Dataset: univ                Batch:  2/15	Loss 84.3792 (84.6589)
2022-11-20 13:39:38,655:INFO: Dataset: univ                Batch:  3/15	Loss 84.1367 (84.4652)
2022-11-20 13:39:39,108:INFO: Dataset: univ                Batch:  4/15	Loss 83.2449 (84.1442)
2022-11-20 13:39:39,552:INFO: Dataset: univ                Batch:  5/15	Loss 84.2185 (84.1592)
2022-11-20 13:39:39,988:INFO: Dataset: univ                Batch:  6/15	Loss 84.2863 (84.1795)
2022-11-20 13:39:40,422:INFO: Dataset: univ                Batch:  7/15	Loss 85.6007 (84.3753)
2022-11-20 13:39:40,860:INFO: Dataset: univ                Batch:  8/15	Loss 84.0687 (84.3375)
2022-11-20 13:39:41,296:INFO: Dataset: univ                Batch:  9/15	Loss 84.4786 (84.3531)
2022-11-20 13:39:41,748:INFO: Dataset: univ                Batch: 10/15	Loss 84.2605 (84.3432)
2022-11-20 13:39:42,186:INFO: Dataset: univ                Batch: 11/15	Loss 84.5783 (84.3646)
2022-11-20 13:39:42,650:INFO: Dataset: univ                Batch: 12/15	Loss 83.2025 (84.2552)
2022-11-20 13:39:43,110:INFO: Dataset: univ                Batch: 13/15	Loss 85.1365 (84.3280)
2022-11-20 13:39:43,563:INFO: Dataset: univ                Batch: 14/15	Loss 83.9218 (84.2988)
2022-11-20 13:39:43,872:INFO: Dataset: univ                Batch: 15/15	Loss 15.6177 (83.2606)
2022-11-20 13:39:44,428:INFO: Dataset: zara1               Batch: 1/8	Loss 85.4221 (85.4221)
2022-11-20 13:39:44,804:INFO: Dataset: zara1               Batch: 2/8	Loss 86.6188 (86.0018)
2022-11-20 13:39:45,181:INFO: Dataset: zara1               Batch: 3/8	Loss 87.8002 (86.5938)
2022-11-20 13:39:45,559:INFO: Dataset: zara1               Batch: 4/8	Loss 86.9344 (86.6687)
2022-11-20 13:39:45,936:INFO: Dataset: zara1               Batch: 5/8	Loss 86.8251 (86.7020)
2022-11-20 13:39:46,317:INFO: Dataset: zara1               Batch: 6/8	Loss 87.3326 (86.8157)
2022-11-20 13:39:46,695:INFO: Dataset: zara1               Batch: 7/8	Loss 86.1509 (86.7233)
2022-11-20 13:39:47,062:INFO: Dataset: zara1               Batch: 8/8	Loss 74.5382 (85.1970)
2022-11-20 13:39:47,617:INFO: Dataset: zara2               Batch:  1/18	Loss 83.6835 (83.6835)
2022-11-20 13:39:47,995:INFO: Dataset: zara2               Batch:  2/18	Loss 83.5616 (83.6230)
2022-11-20 13:39:48,377:INFO: Dataset: zara2               Batch:  3/18	Loss 84.0417 (83.7608)
2022-11-20 13:39:48,751:INFO: Dataset: zara2               Batch:  4/18	Loss 83.9195 (83.7982)
2022-11-20 13:39:49,135:INFO: Dataset: zara2               Batch:  5/18	Loss 84.0970 (83.8620)
2022-11-20 13:39:49,511:INFO: Dataset: zara2               Batch:  6/18	Loss 83.5991 (83.8193)
2022-11-20 13:39:49,885:INFO: Dataset: zara2               Batch:  7/18	Loss 83.7096 (83.8049)
2022-11-20 13:39:50,259:INFO: Dataset: zara2               Batch:  8/18	Loss 83.7955 (83.8037)
2022-11-20 13:39:50,633:INFO: Dataset: zara2               Batch:  9/18	Loss 84.8988 (83.9185)
2022-11-20 13:39:51,008:INFO: Dataset: zara2               Batch: 10/18	Loss 83.2426 (83.8488)
2022-11-20 13:39:51,382:INFO: Dataset: zara2               Batch: 11/18	Loss 83.7925 (83.8432)
2022-11-20 13:39:51,756:INFO: Dataset: zara2               Batch: 12/18	Loss 83.2096 (83.7869)
2022-11-20 13:39:52,131:INFO: Dataset: zara2               Batch: 13/18	Loss 83.3561 (83.7554)
2022-11-20 13:39:52,506:INFO: Dataset: zara2               Batch: 14/18	Loss 84.1251 (83.7789)
2022-11-20 13:39:52,883:INFO: Dataset: zara2               Batch: 15/18	Loss 83.4143 (83.7540)
2022-11-20 13:39:53,272:INFO: Dataset: zara2               Batch: 16/18	Loss 84.4094 (83.7934)
2022-11-20 13:39:53,648:INFO: Dataset: zara2               Batch: 17/18	Loss 83.3454 (83.7697)
2022-11-20 13:39:54,011:INFO: Dataset: zara2               Batch: 18/18	Loss 71.1783 (83.1515)
2022-11-20 13:39:54,049:INFO: - Computing loss (validation)
2022-11-20 13:39:54,400:INFO: Dataset: hotel               Batch: 1/2	Loss 84.6748 (84.6748)
2022-11-20 13:39:54,558:INFO: Dataset: hotel               Batch: 2/2	Loss 6.4328 (79.8682)
2022-11-20 13:39:54,925:INFO: Dataset: univ                Batch: 1/3	Loss 84.5084 (84.5084)
2022-11-20 13:39:55,128:INFO: Dataset: univ                Batch: 2/3	Loss 83.8032 (84.1493)
2022-11-20 13:39:55,323:INFO: Dataset: univ                Batch: 3/3	Loss 79.2058 (82.8579)
2022-11-20 13:39:55,661:INFO: Dataset: zara1               Batch: 1/2	Loss 86.5110 (86.5110)
2022-11-20 13:39:55,836:INFO: Dataset: zara1               Batch: 2/2	Loss 28.4927 (72.3329)
2022-11-20 13:39:56,199:INFO: Dataset: zara2               Batch: 1/5	Loss 82.4607 (82.4607)
2022-11-20 13:39:56,390:INFO: Dataset: zara2               Batch: 2/5	Loss 82.2810 (82.3739)
2022-11-20 13:39:56,583:INFO: Dataset: zara2               Batch: 3/5	Loss 81.7548 (82.1677)
2022-11-20 13:39:56,774:INFO: Dataset: zara2               Batch: 4/5	Loss 82.3823 (82.2206)
2022-11-20 13:39:56,965:INFO: Dataset: zara2               Batch: 5/5	Loss 81.5396 (82.0818)
2022-11-20 13:39:56,996:INFO: - Computing ADE (validation)
2022-11-20 13:39:57,202:INFO: 		 ADE on hotel                     dataset:	 1.745237946510315
2022-11-20 13:39:57,453:INFO: 		 ADE on univ                      dataset:	 1.841394066810608
2022-11-20 13:39:57,657:INFO: 		 ADE on zara1                     dataset:	 2.4196863174438477
2022-11-20 13:39:57,982:INFO: 		 ADE on zara2                     dataset:	 1.8191943168640137
2022-11-20 13:39:57,982:INFO: Average validation:	ADE  1.8616	FDE  3.4299
2022-11-20 13:39:57,983:INFO: - Computing ADE (training)
2022-11-20 13:39:58,261:INFO: 		 ADE on hotel                     dataset:	 1.9515793323516846
2022-11-20 13:39:58,952:INFO: 		 ADE on univ                      dataset:	 1.7395479679107666
2022-11-20 13:39:59,351:INFO: 		 ADE on zara1                     dataset:	 2.4125120639801025
2022-11-20 13:40:00,069:INFO: 		 ADE on zara2                     dataset:	 2.0039639472961426
2022-11-20 13:40:00,069:INFO: Average training:	ADE  1.8415	FDE  3.3960
2022-11-20 13:40:00,086:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_419.pth.tar
2022-11-20 13:40:00,086:INFO: 
===> EPOCH: 420 (P4)
2022-11-20 13:40:00,087:INFO: - Computing loss (training)
2022-11-20 13:40:00,592:INFO: Dataset: hotel               Batch: 1/4	Loss 87.5998 (87.5998)
2022-11-20 13:40:00,970:INFO: Dataset: hotel               Batch: 2/4	Loss 86.6336 (87.1089)
2022-11-20 13:40:01,343:INFO: Dataset: hotel               Batch: 3/4	Loss 87.6464 (87.2775)
2022-11-20 13:40:01,678:INFO: Dataset: hotel               Batch: 4/4	Loss 54.9154 (81.8554)
2022-11-20 13:40:02,295:INFO: Dataset: univ                Batch:  1/15	Loss 84.5438 (84.5438)
2022-11-20 13:40:02,728:INFO: Dataset: univ                Batch:  2/15	Loss 85.1287 (84.8355)
2022-11-20 13:40:03,168:INFO: Dataset: univ                Batch:  3/15	Loss 84.3625 (84.6700)
2022-11-20 13:40:03,605:INFO: Dataset: univ                Batch:  4/15	Loss 87.2098 (85.3122)
2022-11-20 13:40:04,045:INFO: Dataset: univ                Batch:  5/15	Loss 83.3211 (84.8975)
2022-11-20 13:40:04,485:INFO: Dataset: univ                Batch:  6/15	Loss 83.5273 (84.6590)
2022-11-20 13:40:04,935:INFO: Dataset: univ                Batch:  7/15	Loss 84.1083 (84.5754)
2022-11-20 13:40:05,368:INFO: Dataset: univ                Batch:  8/15	Loss 84.8422 (84.6076)
2022-11-20 13:40:05,822:INFO: Dataset: univ                Batch:  9/15	Loss 83.4758 (84.4703)
2022-11-20 13:40:06,251:INFO: Dataset: univ                Batch: 10/15	Loss 84.6035 (84.4825)
2022-11-20 13:40:06,704:INFO: Dataset: univ                Batch: 11/15	Loss 82.8798 (84.3275)
2022-11-20 13:40:07,138:INFO: Dataset: univ                Batch: 12/15	Loss 84.1907 (84.3164)
2022-11-20 13:40:07,596:INFO: Dataset: univ                Batch: 13/15	Loss 83.2333 (84.2241)
2022-11-20 13:40:08,025:INFO: Dataset: univ                Batch: 14/15	Loss 85.1255 (84.2820)
2022-11-20 13:40:08,327:INFO: Dataset: univ                Batch: 15/15	Loss 16.3203 (83.5702)
2022-11-20 13:40:08,880:INFO: Dataset: zara1               Batch: 1/8	Loss 86.3950 (86.3950)
2022-11-20 13:40:09,271:INFO: Dataset: zara1               Batch: 2/8	Loss 87.4553 (86.8926)
2022-11-20 13:40:09,646:INFO: Dataset: zara1               Batch: 3/8	Loss 85.8939 (86.5817)
2022-11-20 13:40:10,020:INFO: Dataset: zara1               Batch: 4/8	Loss 87.0119 (86.6858)
2022-11-20 13:40:10,404:INFO: Dataset: zara1               Batch: 5/8	Loss 87.1868 (86.7831)
2022-11-20 13:40:10,794:INFO: Dataset: zara1               Batch: 6/8	Loss 87.3424 (86.8756)
2022-11-20 13:40:11,184:INFO: Dataset: zara1               Batch: 7/8	Loss 86.1455 (86.7811)
2022-11-20 13:40:11,566:INFO: Dataset: zara1               Batch: 8/8	Loss 73.7210 (85.4614)
2022-11-20 13:40:12,136:INFO: Dataset: zara2               Batch:  1/18	Loss 83.8332 (83.8332)
2022-11-20 13:40:12,797:INFO: Dataset: zara2               Batch:  2/18	Loss 83.2698 (83.5497)
2022-11-20 13:40:13,253:INFO: Dataset: zara2               Batch:  3/18	Loss 83.2175 (83.4343)
2022-11-20 13:40:13,664:INFO: Dataset: zara2               Batch:  4/18	Loss 83.3753 (83.4182)
2022-11-20 13:40:14,085:INFO: Dataset: zara2               Batch:  5/18	Loss 83.7800 (83.4967)
2022-11-20 13:40:14,478:INFO: Dataset: zara2               Batch:  6/18	Loss 84.5358 (83.6718)
2022-11-20 13:40:14,897:INFO: Dataset: zara2               Batch:  7/18	Loss 83.8505 (83.7002)
2022-11-20 13:40:15,322:INFO: Dataset: zara2               Batch:  8/18	Loss 83.2094 (83.6375)
2022-11-20 13:40:15,721:INFO: Dataset: zara2               Batch:  9/18	Loss 83.2177 (83.5936)
2022-11-20 13:40:16,141:INFO: Dataset: zara2               Batch: 10/18	Loss 83.0504 (83.5375)
2022-11-20 13:40:16,560:INFO: Dataset: zara2               Batch: 11/18	Loss 83.9207 (83.5723)
2022-11-20 13:40:16,991:INFO: Dataset: zara2               Batch: 12/18	Loss 84.4233 (83.6421)
2022-11-20 13:40:17,411:INFO: Dataset: zara2               Batch: 13/18	Loss 83.5632 (83.6361)
2022-11-20 13:40:17,815:INFO: Dataset: zara2               Batch: 14/18	Loss 84.1182 (83.6705)
2022-11-20 13:40:18,224:INFO: Dataset: zara2               Batch: 15/18	Loss 83.4390 (83.6556)
2022-11-20 13:40:18,644:INFO: Dataset: zara2               Batch: 16/18	Loss 83.5951 (83.6518)
2022-11-20 13:40:19,036:INFO: Dataset: zara2               Batch: 17/18	Loss 82.8859 (83.6084)
2022-11-20 13:40:19,443:INFO: Dataset: zara2               Batch: 18/18	Loss 72.0996 (83.1176)
2022-11-20 13:40:19,491:INFO: - Computing loss (validation)
2022-11-20 13:40:19,959:INFO: Dataset: hotel               Batch: 1/2	Loss 84.2858 (84.2858)
2022-11-20 13:40:20,298:INFO: Dataset: hotel               Batch: 2/2	Loss 6.7431 (77.4049)
2022-11-20 13:40:20,757:INFO: Dataset: univ                Batch: 1/3	Loss 83.8750 (83.8750)
2022-11-20 13:40:20,968:INFO: Dataset: univ                Batch: 2/3	Loss 84.2354 (84.0243)
2022-11-20 13:40:21,174:INFO: Dataset: univ                Batch: 3/3	Loss 77.7422 (81.9318)
2022-11-20 13:40:21,525:INFO: Dataset: zara1               Batch: 1/2	Loss 85.4642 (85.4642)
2022-11-20 13:40:21,703:INFO: Dataset: zara1               Batch: 2/2	Loss 28.9766 (73.4765)
2022-11-20 13:40:22,082:INFO: Dataset: zara2               Batch: 1/5	Loss 81.9791 (81.9791)
2022-11-20 13:40:22,285:INFO: Dataset: zara2               Batch: 2/5	Loss 81.9793 (81.9792)
2022-11-20 13:40:22,484:INFO: Dataset: zara2               Batch: 3/5	Loss 82.5897 (82.1867)
2022-11-20 13:40:22,683:INFO: Dataset: zara2               Batch: 4/5	Loss 82.2017 (82.1904)
2022-11-20 13:40:22,882:INFO: Dataset: zara2               Batch: 5/5	Loss 81.3756 (82.0152)
2022-11-20 13:40:22,917:INFO: - Computing ADE (validation)
2022-11-20 13:40:23,122:INFO: 		 ADE on hotel                     dataset:	 1.6429530382156372
2022-11-20 13:40:23,396:INFO: 		 ADE on univ                      dataset:	 1.8358979225158691
2022-11-20 13:40:23,591:INFO: 		 ADE on zara1                     dataset:	 2.4273459911346436
2022-11-20 13:40:23,914:INFO: 		 ADE on zara2                     dataset:	 1.811888575553894
2022-11-20 13:40:23,914:INFO: Average validation:	ADE  1.8509	FDE  3.4076
2022-11-20 13:40:23,915:INFO: - Computing ADE (training)
2022-11-20 13:40:24,192:INFO: 		 ADE on hotel                     dataset:	 1.9399446249008179
2022-11-20 13:40:24,924:INFO: 		 ADE on univ                      dataset:	 1.7347480058670044
2022-11-20 13:40:25,304:INFO: 		 ADE on zara1                     dataset:	 2.384734869003296
2022-11-20 13:40:26,062:INFO: 		 ADE on zara2                     dataset:	 1.9697556495666504
2022-11-20 13:40:26,063:INFO: Average training:	ADE  1.8291	FDE  3.3673
2022-11-20 13:40:26,079:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_420.pth.tar
2022-11-20 13:40:26,080:INFO: 
===> EPOCH: 421 (P4)
2022-11-20 13:40:26,080:INFO: - Computing loss (training)
2022-11-20 13:40:26,576:INFO: Dataset: hotel               Batch: 1/4	Loss 88.8418 (88.8418)
2022-11-20 13:40:26,956:INFO: Dataset: hotel               Batch: 2/4	Loss 87.5017 (88.1957)
2022-11-20 13:40:27,328:INFO: Dataset: hotel               Batch: 3/4	Loss 86.5525 (87.6471)
2022-11-20 13:40:27,666:INFO: Dataset: hotel               Batch: 4/4	Loss 53.8982 (81.9036)
2022-11-20 13:40:28,296:INFO: Dataset: univ                Batch:  1/15	Loss 84.6608 (84.6608)
2022-11-20 13:40:28,754:INFO: Dataset: univ                Batch:  2/15	Loss 83.4995 (84.0438)
2022-11-20 13:40:29,211:INFO: Dataset: univ                Batch:  3/15	Loss 83.2439 (83.7685)
2022-11-20 13:40:29,666:INFO: Dataset: univ                Batch:  4/15	Loss 84.1493 (83.8630)
2022-11-20 13:40:30,117:INFO: Dataset: univ                Batch:  5/15	Loss 84.6542 (84.0105)
2022-11-20 13:40:30,579:INFO: Dataset: univ                Batch:  6/15	Loss 84.5221 (84.0969)
2022-11-20 13:40:31,051:INFO: Dataset: univ                Batch:  7/15	Loss 83.6739 (84.0315)
2022-11-20 13:40:31,558:INFO: Dataset: univ                Batch:  8/15	Loss 83.2254 (83.9269)
2022-11-20 13:40:32,103:INFO: Dataset: univ                Batch:  9/15	Loss 84.2229 (83.9604)
2022-11-20 13:40:32,555:INFO: Dataset: univ                Batch: 10/15	Loss 83.4020 (83.9056)
2022-11-20 13:40:33,001:INFO: Dataset: univ                Batch: 11/15	Loss 83.8395 (83.8998)
2022-11-20 13:40:33,429:INFO: Dataset: univ                Batch: 12/15	Loss 84.8450 (83.9660)
2022-11-20 13:40:33,894:INFO: Dataset: univ                Batch: 13/15	Loss 83.2768 (83.9065)
2022-11-20 13:40:34,331:INFO: Dataset: univ                Batch: 14/15	Loss 83.9453 (83.9090)
2022-11-20 13:40:34,641:INFO: Dataset: univ                Batch: 15/15	Loss 15.6176 (82.8378)
2022-11-20 13:40:35,179:INFO: Dataset: zara1               Batch: 1/8	Loss 85.9636 (85.9636)
2022-11-20 13:40:35,555:INFO: Dataset: zara1               Batch: 2/8	Loss 85.9399 (85.9518)
2022-11-20 13:40:35,934:INFO: Dataset: zara1               Batch: 3/8	Loss 85.9416 (85.9486)
2022-11-20 13:40:36,312:INFO: Dataset: zara1               Batch: 4/8	Loss 87.2191 (86.2669)
2022-11-20 13:40:36,693:INFO: Dataset: zara1               Batch: 5/8	Loss 86.6744 (86.3532)
2022-11-20 13:40:37,074:INFO: Dataset: zara1               Batch: 6/8	Loss 86.8568 (86.4357)
2022-11-20 13:40:37,452:INFO: Dataset: zara1               Batch: 7/8	Loss 86.0053 (86.3622)
2022-11-20 13:40:37,818:INFO: Dataset: zara1               Batch: 8/8	Loss 74.1426 (85.0374)
2022-11-20 13:40:38,356:INFO: Dataset: zara2               Batch:  1/18	Loss 83.7818 (83.7818)
2022-11-20 13:40:38,735:INFO: Dataset: zara2               Batch:  2/18	Loss 83.1101 (83.4230)
2022-11-20 13:40:39,117:INFO: Dataset: zara2               Batch:  3/18	Loss 83.3071 (83.3847)
2022-11-20 13:40:39,493:INFO: Dataset: zara2               Batch:  4/18	Loss 83.8271 (83.4958)
2022-11-20 13:40:39,868:INFO: Dataset: zara2               Batch:  5/18	Loss 83.1610 (83.4317)
2022-11-20 13:40:40,249:INFO: Dataset: zara2               Batch:  6/18	Loss 82.7344 (83.3081)
2022-11-20 13:40:40,625:INFO: Dataset: zara2               Batch:  7/18	Loss 83.0137 (83.2656)
2022-11-20 13:40:41,002:INFO: Dataset: zara2               Batch:  8/18	Loss 83.4907 (83.2954)
2022-11-20 13:40:41,377:INFO: Dataset: zara2               Batch:  9/18	Loss 84.1147 (83.3839)
2022-11-20 13:40:41,751:INFO: Dataset: zara2               Batch: 10/18	Loss 83.4117 (83.3864)
2022-11-20 13:40:42,130:INFO: Dataset: zara2               Batch: 11/18	Loss 83.8436 (83.4250)
2022-11-20 13:40:42,505:INFO: Dataset: zara2               Batch: 12/18	Loss 84.1552 (83.4840)
2022-11-20 13:40:42,879:INFO: Dataset: zara2               Batch: 13/18	Loss 82.6333 (83.4208)
2022-11-20 13:40:43,254:INFO: Dataset: zara2               Batch: 14/18	Loss 83.7291 (83.4429)
2022-11-20 13:40:43,638:INFO: Dataset: zara2               Batch: 15/18	Loss 84.1197 (83.4883)
2022-11-20 13:40:44,016:INFO: Dataset: zara2               Batch: 16/18	Loss 82.6229 (83.4318)
2022-11-20 13:40:44,391:INFO: Dataset: zara2               Batch: 17/18	Loss 83.1483 (83.4146)
2022-11-20 13:40:44,756:INFO: Dataset: zara2               Batch: 18/18	Loss 72.0511 (82.8867)
2022-11-20 13:40:44,794:INFO: - Computing loss (validation)
2022-11-20 13:40:45,131:INFO: Dataset: hotel               Batch: 1/2	Loss 84.3036 (84.3036)
2022-11-20 13:40:45,286:INFO: Dataset: hotel               Batch: 2/2	Loss 6.6418 (78.4724)
2022-11-20 13:40:45,644:INFO: Dataset: univ                Batch: 1/3	Loss 85.0068 (85.0068)
2022-11-20 13:40:45,849:INFO: Dataset: univ                Batch: 2/3	Loss 83.7503 (84.4125)
2022-11-20 13:40:46,054:INFO: Dataset: univ                Batch: 3/3	Loss 77.2853 (81.8438)
2022-11-20 13:40:46,407:INFO: Dataset: zara1               Batch: 1/2	Loss 86.1963 (86.1963)
2022-11-20 13:40:46,578:INFO: Dataset: zara1               Batch: 2/2	Loss 28.0224 (74.0378)
2022-11-20 13:40:46,929:INFO: Dataset: zara2               Batch: 1/5	Loss 83.2840 (83.2840)
2022-11-20 13:40:47,125:INFO: Dataset: zara2               Batch: 2/5	Loss 81.8876 (82.5987)
2022-11-20 13:40:47,321:INFO: Dataset: zara2               Batch: 3/5	Loss 82.0267 (82.3979)
2022-11-20 13:40:47,527:INFO: Dataset: zara2               Batch: 4/5	Loss 82.1105 (82.3259)
2022-11-20 13:40:47,723:INFO: Dataset: zara2               Batch: 5/5	Loss 80.9346 (82.0429)
2022-11-20 13:40:47,756:INFO: - Computing ADE (validation)
2022-11-20 13:40:47,944:INFO: 		 ADE on hotel                     dataset:	 1.7833607196807861
2022-11-20 13:40:48,213:INFO: 		 ADE on univ                      dataset:	 1.8277735710144043
2022-11-20 13:40:48,412:INFO: 		 ADE on zara1                     dataset:	 2.32125186920166
2022-11-20 13:40:48,737:INFO: 		 ADE on zara2                     dataset:	 1.8001445531845093
2022-11-20 13:40:48,738:INFO: Average validation:	ADE  1.8439	FDE  3.3948
2022-11-20 13:40:48,739:INFO: - Computing ADE (training)
2022-11-20 13:40:49,038:INFO: 		 ADE on hotel                     dataset:	 1.9578133821487427
2022-11-20 13:40:49,785:INFO: 		 ADE on univ                      dataset:	 1.7362555265426636
2022-11-20 13:40:50,190:INFO: 		 ADE on zara1                     dataset:	 2.4057366847991943
2022-11-20 13:40:50,906:INFO: 		 ADE on zara2                     dataset:	 1.9520678520202637
2022-11-20 13:40:50,906:INFO: Average training:	ADE  1.8284	FDE  3.3644
2022-11-20 13:40:50,922:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_421.pth.tar
2022-11-20 13:40:50,922:INFO: 
===> EPOCH: 422 (P4)
2022-11-20 13:40:50,922:INFO: - Computing loss (training)
2022-11-20 13:40:51,433:INFO: Dataset: hotel               Batch: 1/4	Loss 87.0733 (87.0733)
2022-11-20 13:40:51,812:INFO: Dataset: hotel               Batch: 2/4	Loss 87.2298 (87.1512)
2022-11-20 13:40:52,187:INFO: Dataset: hotel               Batch: 3/4	Loss 88.4055 (87.5746)
2022-11-20 13:40:52,522:INFO: Dataset: hotel               Batch: 4/4	Loss 53.7411 (81.6381)
2022-11-20 13:40:53,183:INFO: Dataset: univ                Batch:  1/15	Loss 82.4887 (82.4887)
2022-11-20 13:40:53,625:INFO: Dataset: univ                Batch:  2/15	Loss 83.9928 (83.1365)
2022-11-20 13:40:54,086:INFO: Dataset: univ                Batch:  3/15	Loss 83.1652 (83.1460)
2022-11-20 13:40:54,523:INFO: Dataset: univ                Batch:  4/15	Loss 84.3685 (83.4201)
2022-11-20 13:40:54,972:INFO: Dataset: univ                Batch:  5/15	Loss 83.9282 (83.5217)
2022-11-20 13:40:55,427:INFO: Dataset: univ                Batch:  6/15	Loss 83.2845 (83.4817)
2022-11-20 13:40:55,865:INFO: Dataset: univ                Batch:  7/15	Loss 84.5225 (83.6285)
2022-11-20 13:40:56,321:INFO: Dataset: univ                Batch:  8/15	Loss 83.7506 (83.6447)
2022-11-20 13:40:56,759:INFO: Dataset: univ                Batch:  9/15	Loss 84.0415 (83.6875)
2022-11-20 13:40:57,203:INFO: Dataset: univ                Batch: 10/15	Loss 83.6928 (83.6880)
2022-11-20 13:40:57,661:INFO: Dataset: univ                Batch: 11/15	Loss 82.9809 (83.6198)
2022-11-20 13:40:58,082:INFO: Dataset: univ                Batch: 12/15	Loss 85.9294 (83.7753)
2022-11-20 13:40:58,513:INFO: Dataset: univ                Batch: 13/15	Loss 84.0172 (83.7922)
2022-11-20 13:40:58,955:INFO: Dataset: univ                Batch: 14/15	Loss 83.9123 (83.8006)
2022-11-20 13:40:59,266:INFO: Dataset: univ                Batch: 15/15	Loss 15.5576 (82.9048)
2022-11-20 13:40:59,837:INFO: Dataset: zara1               Batch: 1/8	Loss 85.9660 (85.9660)
2022-11-20 13:41:00,215:INFO: Dataset: zara1               Batch: 2/8	Loss 86.4333 (86.1992)
2022-11-20 13:41:00,609:INFO: Dataset: zara1               Batch: 3/8	Loss 85.6767 (86.0243)
2022-11-20 13:41:00,993:INFO: Dataset: zara1               Batch: 4/8	Loss 86.1057 (86.0436)
2022-11-20 13:41:01,366:INFO: Dataset: zara1               Batch: 5/8	Loss 85.3026 (85.8987)
2022-11-20 13:41:01,741:INFO: Dataset: zara1               Batch: 6/8	Loss 87.3439 (86.1149)
2022-11-20 13:41:02,128:INFO: Dataset: zara1               Batch: 7/8	Loss 85.8463 (86.0781)
2022-11-20 13:41:02,490:INFO: Dataset: zara1               Batch: 8/8	Loss 75.0672 (84.7163)
2022-11-20 13:41:03,075:INFO: Dataset: zara2               Batch:  1/18	Loss 83.0199 (83.0199)
2022-11-20 13:41:03,456:INFO: Dataset: zara2               Batch:  2/18	Loss 82.0491 (82.5125)
2022-11-20 13:41:03,839:INFO: Dataset: zara2               Batch:  3/18	Loss 83.8526 (82.9822)
2022-11-20 13:41:04,217:INFO: Dataset: zara2               Batch:  4/18	Loss 83.7138 (83.1627)
2022-11-20 13:41:04,597:INFO: Dataset: zara2               Batch:  5/18	Loss 83.2287 (83.1746)
2022-11-20 13:41:04,978:INFO: Dataset: zara2               Batch:  6/18	Loss 82.6849 (83.1012)
2022-11-20 13:41:05,358:INFO: Dataset: zara2               Batch:  7/18	Loss 84.3651 (83.2603)
2022-11-20 13:41:05,734:INFO: Dataset: zara2               Batch:  8/18	Loss 83.7488 (83.3220)
2022-11-20 13:41:06,110:INFO: Dataset: zara2               Batch:  9/18	Loss 83.0636 (83.2936)
2022-11-20 13:41:06,566:INFO: Dataset: zara2               Batch: 10/18	Loss 82.6081 (83.2251)
2022-11-20 13:41:06,945:INFO: Dataset: zara2               Batch: 11/18	Loss 83.6092 (83.2569)
2022-11-20 13:41:07,327:INFO: Dataset: zara2               Batch: 12/18	Loss 82.7845 (83.2175)
2022-11-20 13:41:07,709:INFO: Dataset: zara2               Batch: 13/18	Loss 84.5751 (83.3294)
2022-11-20 13:41:08,090:INFO: Dataset: zara2               Batch: 14/18	Loss 83.1882 (83.3196)
2022-11-20 13:41:08,473:INFO: Dataset: zara2               Batch: 15/18	Loss 83.6633 (83.3420)
2022-11-20 13:41:08,849:INFO: Dataset: zara2               Batch: 16/18	Loss 82.5711 (83.2898)
2022-11-20 13:41:09,225:INFO: Dataset: zara2               Batch: 17/18	Loss 83.8689 (83.3258)
2022-11-20 13:41:09,594:INFO: Dataset: zara2               Batch: 18/18	Loss 71.9017 (82.7762)
2022-11-20 13:41:09,632:INFO: - Computing loss (validation)
2022-11-20 13:41:09,968:INFO: Dataset: hotel               Batch: 1/2	Loss 84.2542 (84.2542)
2022-11-20 13:41:10,122:INFO: Dataset: hotel               Batch: 2/2	Loss 6.6653 (78.6932)
2022-11-20 13:41:10,484:INFO: Dataset: univ                Batch: 1/3	Loss 83.5140 (83.5140)
2022-11-20 13:41:10,686:INFO: Dataset: univ                Batch: 2/3	Loss 85.0334 (84.2156)
2022-11-20 13:41:10,887:INFO: Dataset: univ                Batch: 3/3	Loss 77.6204 (82.0496)
2022-11-20 13:41:11,229:INFO: Dataset: zara1               Batch: 1/2	Loss 85.2326 (85.2326)
2022-11-20 13:41:11,397:INFO: Dataset: zara1               Batch: 2/2	Loss 28.3730 (72.6174)
2022-11-20 13:41:11,755:INFO: Dataset: zara2               Batch: 1/5	Loss 81.7304 (81.7304)
2022-11-20 13:41:11,953:INFO: Dataset: zara2               Batch: 2/5	Loss 82.5989 (82.1430)
2022-11-20 13:41:12,151:INFO: Dataset: zara2               Batch: 3/5	Loss 82.3207 (82.2023)
2022-11-20 13:41:12,348:INFO: Dataset: zara2               Batch: 4/5	Loss 82.3536 (82.2398)
2022-11-20 13:41:12,547:INFO: Dataset: zara2               Batch: 5/5	Loss 80.7051 (81.9238)
2022-11-20 13:41:12,574:INFO: - Computing ADE (validation)
2022-11-20 13:41:12,777:INFO: 		 ADE on hotel                     dataset:	 1.6816685199737549
2022-11-20 13:41:13,045:INFO: 		 ADE on univ                      dataset:	 1.8163481950759888
2022-11-20 13:41:13,246:INFO: 		 ADE on zara1                     dataset:	 2.411508798599243
2022-11-20 13:41:13,587:INFO: 		 ADE on zara2                     dataset:	 1.8122215270996094
2022-11-20 13:41:13,588:INFO: Average validation:	ADE  1.8421	FDE  3.3871
2022-11-20 13:41:13,588:INFO: - Computing ADE (training)
2022-11-20 13:41:13,866:INFO: 		 ADE on hotel                     dataset:	 2.01102876663208
2022-11-20 13:41:14,599:INFO: 		 ADE on univ                      dataset:	 1.722468614578247
2022-11-20 13:41:14,998:INFO: 		 ADE on zara1                     dataset:	 2.3846144676208496
2022-11-20 13:41:15,722:INFO: 		 ADE on zara2                     dataset:	 1.9700521230697632
2022-11-20 13:41:15,722:INFO: Average training:	ADE  1.8223	FDE  3.3522
2022-11-20 13:41:15,739:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_422.pth.tar
2022-11-20 13:41:15,739:INFO: 
===> EPOCH: 423 (P4)
2022-11-20 13:41:15,740:INFO: - Computing loss (training)
2022-11-20 13:41:16,282:INFO: Dataset: hotel               Batch: 1/4	Loss 86.4698 (86.4698)
2022-11-20 13:41:16,665:INFO: Dataset: hotel               Batch: 2/4	Loss 85.7155 (86.1035)
2022-11-20 13:41:17,049:INFO: Dataset: hotel               Batch: 3/4	Loss 88.6345 (86.9445)
2022-11-20 13:41:17,395:INFO: Dataset: hotel               Batch: 4/4	Loss 54.5324 (81.3002)
2022-11-20 13:41:18,056:INFO: Dataset: univ                Batch:  1/15	Loss 82.3942 (82.3942)
2022-11-20 13:41:18,498:INFO: Dataset: univ                Batch:  2/15	Loss 83.7984 (83.0202)
2022-11-20 13:41:18,943:INFO: Dataset: univ                Batch:  3/15	Loss 83.6954 (83.2284)
2022-11-20 13:41:19,416:INFO: Dataset: univ                Batch:  4/15	Loss 82.8352 (83.1243)
2022-11-20 13:41:19,858:INFO: Dataset: univ                Batch:  5/15	Loss 84.3145 (83.3382)
2022-11-20 13:41:20,326:INFO: Dataset: univ                Batch:  6/15	Loss 82.9870 (83.2760)
2022-11-20 13:41:20,785:INFO: Dataset: univ                Batch:  7/15	Loss 83.2621 (83.2740)
2022-11-20 13:41:21,226:INFO: Dataset: univ                Batch:  8/15	Loss 83.5071 (83.3013)
2022-11-20 13:41:21,667:INFO: Dataset: univ                Batch:  9/15	Loss 83.5052 (83.3230)
2022-11-20 13:41:22,089:INFO: Dataset: univ                Batch: 10/15	Loss 84.6301 (83.4309)
2022-11-20 13:41:22,530:INFO: Dataset: univ                Batch: 11/15	Loss 83.4477 (83.4323)
2022-11-20 13:41:22,962:INFO: Dataset: univ                Batch: 12/15	Loss 83.7644 (83.4572)
2022-11-20 13:41:23,396:INFO: Dataset: univ                Batch: 13/15	Loss 83.6721 (83.4726)
2022-11-20 13:41:23,822:INFO: Dataset: univ                Batch: 14/15	Loss 84.5379 (83.5369)
2022-11-20 13:41:24,131:INFO: Dataset: univ                Batch: 15/15	Loss 15.7651 (82.7051)
2022-11-20 13:41:24,671:INFO: Dataset: zara1               Batch: 1/8	Loss 86.4108 (86.4108)
2022-11-20 13:41:25,041:INFO: Dataset: zara1               Batch: 2/8	Loss 86.3452 (86.3776)
2022-11-20 13:41:25,413:INFO: Dataset: zara1               Batch: 3/8	Loss 86.3595 (86.3716)
2022-11-20 13:41:25,784:INFO: Dataset: zara1               Batch: 4/8	Loss 86.6321 (86.4370)
2022-11-20 13:41:26,154:INFO: Dataset: zara1               Batch: 5/8	Loss 86.1901 (86.3858)
2022-11-20 13:41:26,524:INFO: Dataset: zara1               Batch: 6/8	Loss 85.2758 (86.1997)
2022-11-20 13:41:26,895:INFO: Dataset: zara1               Batch: 7/8	Loss 85.4986 (86.1043)
2022-11-20 13:41:27,254:INFO: Dataset: zara1               Batch: 8/8	Loss 73.3292 (84.6991)
2022-11-20 13:41:27,878:INFO: Dataset: zara2               Batch:  1/18	Loss 83.1260 (83.1260)
2022-11-20 13:41:28,255:INFO: Dataset: zara2               Batch:  2/18	Loss 83.5407 (83.3292)
2022-11-20 13:41:28,631:INFO: Dataset: zara2               Batch:  3/18	Loss 83.2293 (83.2951)
2022-11-20 13:41:29,005:INFO: Dataset: zara2               Batch:  4/18	Loss 83.1692 (83.2646)
2022-11-20 13:41:29,379:INFO: Dataset: zara2               Batch:  5/18	Loss 82.8624 (83.1783)
2022-11-20 13:41:29,762:INFO: Dataset: zara2               Batch:  6/18	Loss 83.1735 (83.1775)
2022-11-20 13:41:30,137:INFO: Dataset: zara2               Batch:  7/18	Loss 83.6886 (83.2480)
2022-11-20 13:41:30,512:INFO: Dataset: zara2               Batch:  8/18	Loss 83.1719 (83.2384)
2022-11-20 13:41:30,891:INFO: Dataset: zara2               Batch:  9/18	Loss 82.4736 (83.1422)
2022-11-20 13:41:31,272:INFO: Dataset: zara2               Batch: 10/18	Loss 82.5728 (83.0842)
2022-11-20 13:41:31,652:INFO: Dataset: zara2               Batch: 11/18	Loss 83.4131 (83.1165)
2022-11-20 13:41:32,035:INFO: Dataset: zara2               Batch: 12/18	Loss 83.6168 (83.1542)
2022-11-20 13:41:32,415:INFO: Dataset: zara2               Batch: 13/18	Loss 83.5886 (83.1833)
2022-11-20 13:41:32,792:INFO: Dataset: zara2               Batch: 14/18	Loss 83.8554 (83.2316)
2022-11-20 13:41:33,168:INFO: Dataset: zara2               Batch: 15/18	Loss 82.8568 (83.2074)
2022-11-20 13:41:33,546:INFO: Dataset: zara2               Batch: 16/18	Loss 82.9954 (83.1950)
2022-11-20 13:41:33,929:INFO: Dataset: zara2               Batch: 17/18	Loss 82.7458 (83.1689)
2022-11-20 13:41:34,297:INFO: Dataset: zara2               Batch: 18/18	Loss 71.2020 (82.5813)
2022-11-20 13:41:34,328:INFO: - Computing loss (validation)
2022-11-20 13:41:34,678:INFO: Dataset: hotel               Batch: 1/2	Loss 84.7082 (84.7082)
2022-11-20 13:41:34,831:INFO: Dataset: hotel               Batch: 2/2	Loss 6.3278 (79.0905)
2022-11-20 13:41:35,202:INFO: Dataset: univ                Batch: 1/3	Loss 83.8799 (83.8799)
2022-11-20 13:41:35,405:INFO: Dataset: univ                Batch: 2/3	Loss 84.2814 (84.0917)
2022-11-20 13:41:35,603:INFO: Dataset: univ                Batch: 3/3	Loss 77.8539 (82.1439)
2022-11-20 13:41:35,945:INFO: Dataset: zara1               Batch: 1/2	Loss 85.0055 (85.0055)
2022-11-20 13:41:36,113:INFO: Dataset: zara1               Batch: 2/2	Loss 28.1281 (70.0089)
2022-11-20 13:41:36,495:INFO: Dataset: zara2               Batch: 1/5	Loss 81.5604 (81.5604)
2022-11-20 13:41:36,687:INFO: Dataset: zara2               Batch: 2/5	Loss 82.7060 (82.1144)
2022-11-20 13:41:36,883:INFO: Dataset: zara2               Batch: 3/5	Loss 82.2923 (82.1749)
2022-11-20 13:41:37,076:INFO: Dataset: zara2               Batch: 4/5	Loss 81.8411 (82.0911)
2022-11-20 13:41:37,268:INFO: Dataset: zara2               Batch: 5/5	Loss 80.0059 (81.7043)
2022-11-20 13:41:37,294:INFO: - Computing ADE (validation)
2022-11-20 13:41:37,491:INFO: 		 ADE on hotel                     dataset:	 1.7089996337890625
2022-11-20 13:41:37,746:INFO: 		 ADE on univ                      dataset:	 1.8176255226135254
2022-11-20 13:41:37,956:INFO: 		 ADE on zara1                     dataset:	 2.302673816680908
2022-11-20 13:41:38,280:INFO: 		 ADE on zara2                     dataset:	 1.7931439876556396
2022-11-20 13:41:38,280:INFO: Average validation:	ADE  1.8309	FDE  3.3714
2022-11-20 13:41:38,281:INFO: - Computing ADE (training)
2022-11-20 13:41:38,553:INFO: 		 ADE on hotel                     dataset:	 1.921007513999939
2022-11-20 13:41:39,297:INFO: 		 ADE on univ                      dataset:	 1.7360012531280518
2022-11-20 13:41:39,965:INFO: 		 ADE on zara1                     dataset:	 2.38569974899292
2022-11-20 13:41:40,820:INFO: 		 ADE on zara2                     dataset:	 1.9680460691452026
2022-11-20 13:41:40,821:INFO: Average training:	ADE  1.8292	FDE  3.3679
2022-11-20 13:41:40,840:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_423.pth.tar
2022-11-20 13:41:40,840:INFO: 
===> EPOCH: 424 (P4)
2022-11-20 13:41:40,841:INFO: - Computing loss (training)
2022-11-20 13:41:41,493:INFO: Dataset: hotel               Batch: 1/4	Loss 87.4924 (87.4924)
2022-11-20 13:41:41,897:INFO: Dataset: hotel               Batch: 2/4	Loss 87.4261 (87.4602)
2022-11-20 13:41:42,280:INFO: Dataset: hotel               Batch: 3/4	Loss 87.1416 (87.3543)
2022-11-20 13:41:42,628:INFO: Dataset: hotel               Batch: 4/4	Loss 53.0239 (81.3759)
2022-11-20 13:41:43,249:INFO: Dataset: univ                Batch:  1/15	Loss 85.1684 (85.1684)
2022-11-20 13:41:43,712:INFO: Dataset: univ                Batch:  2/15	Loss 83.3448 (84.1966)
2022-11-20 13:41:44,161:INFO: Dataset: univ                Batch:  3/15	Loss 83.8315 (84.0787)
2022-11-20 13:41:44,601:INFO: Dataset: univ                Batch:  4/15	Loss 84.3802 (84.1525)
2022-11-20 13:41:45,059:INFO: Dataset: univ                Batch:  5/15	Loss 83.1027 (83.9257)
2022-11-20 13:41:45,503:INFO: Dataset: univ                Batch:  6/15	Loss 85.0087 (84.1063)
2022-11-20 13:41:45,944:INFO: Dataset: univ                Batch:  7/15	Loss 83.2487 (83.9817)
2022-11-20 13:41:46,396:INFO: Dataset: univ                Batch:  8/15	Loss 82.9938 (83.8528)
2022-11-20 13:41:46,839:INFO: Dataset: univ                Batch:  9/15	Loss 83.4576 (83.8097)
2022-11-20 13:41:47,284:INFO: Dataset: univ                Batch: 10/15	Loss 84.1008 (83.8372)
2022-11-20 13:41:47,732:INFO: Dataset: univ                Batch: 11/15	Loss 83.4262 (83.8003)
2022-11-20 13:41:48,177:INFO: Dataset: univ                Batch: 12/15	Loss 83.0465 (83.7395)
2022-11-20 13:41:48,623:INFO: Dataset: univ                Batch: 13/15	Loss 83.9953 (83.7588)
2022-11-20 13:41:49,087:INFO: Dataset: univ                Batch: 14/15	Loss 82.8842 (83.6892)
2022-11-20 13:41:49,396:INFO: Dataset: univ                Batch: 15/15	Loss 15.4340 (82.6930)
2022-11-20 13:41:49,946:INFO: Dataset: zara1               Batch: 1/8	Loss 85.6949 (85.6949)
2022-11-20 13:41:50,330:INFO: Dataset: zara1               Batch: 2/8	Loss 85.3183 (85.5017)
2022-11-20 13:41:50,712:INFO: Dataset: zara1               Batch: 3/8	Loss 85.8859 (85.6182)
2022-11-20 13:41:51,093:INFO: Dataset: zara1               Batch: 4/8	Loss 86.0049 (85.7012)
2022-11-20 13:41:51,475:INFO: Dataset: zara1               Batch: 5/8	Loss 85.5471 (85.6706)
2022-11-20 13:41:51,864:INFO: Dataset: zara1               Batch: 6/8	Loss 86.1754 (85.7553)
2022-11-20 13:41:52,252:INFO: Dataset: zara1               Batch: 7/8	Loss 86.7736 (85.9035)
2022-11-20 13:41:52,620:INFO: Dataset: zara1               Batch: 8/8	Loss 74.0319 (84.8039)

2022-11-11 22:41:48,348:INFO: Initializing Training Set
2022-11-11 22:41:52,538:INFO: Initializing Validation Set
2022-11-11 22:41:53,018:INFO: Initializing Validation O Set
2022-11-11 22:41:56,331:INFO: 
===> EPOCH: 1 (P1)
2022-11-11 22:41:56,331:INFO: - Computing loss (training)
2022-11-11 22:42:03,685:INFO: Dataset: hotel               Batch: 1/8	Loss 2.5641 (2.5641)
2022-11-11 22:42:04,064:INFO: Dataset: hotel               Batch: 2/8	Loss 1.4149 (1.8777)
2022-11-11 22:42:04,361:INFO: Dataset: hotel               Batch: 3/8	Loss 2.3284 (2.0329)
2022-11-11 22:42:04,777:INFO: Dataset: hotel               Batch: 4/8	Loss 1.9602 (2.0176)
2022-11-11 22:42:05,079:INFO: Dataset: hotel               Batch: 5/8	Loss 1.7563 (1.9656)
2022-11-11 22:42:05,282:INFO: Dataset: hotel               Batch: 6/8	Loss 4.4901 (2.3543)
2022-11-11 22:42:05,308:INFO: Dataset: hotel               Batch: 7/8	Loss 4.5914 (2.6230)
2022-11-11 22:42:05,319:INFO: Dataset: hotel               Batch: 8/8	Loss 0.1834 (2.4878)
2022-11-11 22:42:30,634:INFO: Dataset: univ                Batch:  1/29	Loss 1.7031 (1.7031)
2022-11-11 22:42:30,717:INFO: Dataset: univ                Batch:  2/29	Loss 1.8535 (1.7736)
2022-11-11 22:42:30,768:INFO: Dataset: univ                Batch:  3/29	Loss 1.7642 (1.7713)
2022-11-11 22:42:30,798:INFO: Dataset: univ                Batch:  4/29	Loss 2.0243 (1.8204)
2022-11-11 22:42:30,824:INFO: Dataset: univ                Batch:  5/29	Loss 1.6439 (1.7850)
2022-11-11 22:42:30,848:INFO: Dataset: univ                Batch:  6/29	Loss 1.5419 (1.7409)
2022-11-11 22:42:30,873:INFO: Dataset: univ                Batch:  7/29	Loss 1.3544 (1.6840)
2022-11-11 22:42:30,901:INFO: Dataset: univ                Batch:  8/29	Loss 1.4664 (1.6575)
2022-11-11 22:42:30,931:INFO: Dataset: univ                Batch:  9/29	Loss 1.5724 (1.6460)
2022-11-11 22:42:30,954:INFO: Dataset: univ                Batch: 10/29	Loss 1.4277 (1.6267)
2022-11-11 22:42:30,983:INFO: Dataset: univ                Batch: 11/29	Loss 2.2736 (1.6735)
2022-11-11 22:42:31,013:INFO: Dataset: univ                Batch: 12/29	Loss 2.7959 (1.7384)
2022-11-11 22:42:31,042:INFO: Dataset: univ                Batch: 13/29	Loss 2.4799 (1.7760)
2022-11-11 22:42:31,067:INFO: Dataset: univ                Batch: 14/29	Loss 2.9050 (1.8473)
2022-11-11 22:42:31,091:INFO: Dataset: univ                Batch: 15/29	Loss 2.5194 (1.8756)
2022-11-11 22:42:31,115:INFO: Dataset: univ                Batch: 16/29	Loss 1.8552 (1.8747)
2022-11-11 22:42:31,137:INFO: Dataset: univ                Batch: 17/29	Loss 2.6592 (1.9139)
2022-11-11 22:42:31,163:INFO: Dataset: univ                Batch: 18/29	Loss 2.8541 (1.9560)
2022-11-11 22:42:31,189:INFO: Dataset: univ                Batch: 19/29	Loss 2.1072 (1.9634)
2022-11-11 22:42:31,216:INFO: Dataset: univ                Batch: 20/29	Loss 2.0016 (1.9647)
2022-11-11 22:42:31,241:INFO: Dataset: univ                Batch: 21/29	Loss 1.7060 (1.9559)
2022-11-11 22:42:31,275:INFO: Dataset: univ                Batch: 22/29	Loss 1.6920 (1.9488)
2022-11-11 22:42:31,303:INFO: Dataset: univ                Batch: 23/29	Loss 2.5897 (1.9640)
2022-11-11 22:42:31,331:INFO: Dataset: univ                Batch: 24/29	Loss 6.2363 (2.0079)
2022-11-11 22:42:31,355:INFO: Dataset: univ                Batch: 25/29	Loss 4.4829 (2.0210)
2022-11-11 22:42:31,382:INFO: Dataset: univ                Batch: 26/29	Loss 6.0835 (2.0346)
2022-11-11 22:42:31,408:INFO: Dataset: univ                Batch: 27/29	Loss 5.5574 (2.0483)
2022-11-11 22:42:31,432:INFO: Dataset: univ                Batch: 28/29	Loss 7.5180 (2.0712)
2022-11-11 22:42:31,446:INFO: Dataset: univ                Batch: 29/29	Loss 3.1210 (2.0724)
2022-11-11 22:42:39,685:INFO: Dataset: zara1               Batch:  1/16	Loss 3.4031 (3.4031)
2022-11-11 22:42:40,045:INFO: Dataset: zara1               Batch:  2/16	Loss 2.7129 (3.0709)
2022-11-11 22:42:40,503:INFO: Dataset: zara1               Batch:  3/16	Loss 2.5862 (2.9069)
2022-11-11 22:42:40,747:INFO: Dataset: zara1               Batch:  4/16	Loss 4.2865 (3.1691)
2022-11-11 22:42:41,128:INFO: Dataset: zara1               Batch:  5/16	Loss 3.0655 (3.1512)
2022-11-11 22:42:41,267:INFO: Dataset: zara1               Batch:  6/16	Loss 2.5587 (3.0656)
2022-11-11 22:42:41,303:INFO: Dataset: zara1               Batch:  7/16	Loss 4.7997 (3.2480)
2022-11-11 22:42:41,346:INFO: Dataset: zara1               Batch:  8/16	Loss 6.3851 (3.5725)
2022-11-11 22:42:41,379:INFO: Dataset: zara1               Batch:  9/16	Loss 5.1601 (3.7713)
2022-11-11 22:42:41,430:INFO: Dataset: zara1               Batch: 10/16	Loss 2.4422 (3.6895)
2022-11-11 22:42:41,469:INFO: Dataset: zara1               Batch: 11/16	Loss 4.3455 (3.7334)
2022-11-11 22:42:41,510:INFO: Dataset: zara1               Batch: 12/16	Loss 2.8167 (3.5789)
2022-11-11 22:42:41,549:INFO: Dataset: zara1               Batch: 13/16	Loss 3.2604 (3.5299)
2022-11-11 22:42:41,576:INFO: Dataset: zara1               Batch: 14/16	Loss 4.8655 (3.5878)
2022-11-11 22:42:41,600:INFO: Dataset: zara1               Batch: 15/16	Loss 5.5431 (3.7021)
2022-11-11 22:42:41,619:INFO: Dataset: zara1               Batch: 16/16	Loss 3.4294 (3.6897)
2022-11-11 22:43:06,247:INFO: Dataset: zara2               Batch:  1/36	Loss 4.4517 (4.4517)
2022-11-11 22:43:06,277:INFO: Dataset: zara2               Batch:  2/36	Loss 4.3295 (4.3898)
2022-11-11 22:43:06,304:INFO: Dataset: zara2               Batch:  3/36	Loss 4.1187 (4.2719)
2022-11-11 22:43:06,334:INFO: Dataset: zara2               Batch:  4/36	Loss 2.2984 (3.7884)
2022-11-11 22:43:06,359:INFO: Dataset: zara2               Batch:  5/36	Loss 3.6231 (3.7524)
2022-11-11 22:43:06,386:INFO: Dataset: zara2               Batch:  6/36	Loss 2.4722 (3.5239)
2022-11-11 22:43:06,413:INFO: Dataset: zara2               Batch:  7/36	Loss 2.7687 (3.4550)
2022-11-11 22:43:06,444:INFO: Dataset: zara2               Batch:  8/36	Loss 3.1373 (3.4142)
2022-11-11 22:43:06,481:INFO: Dataset: zara2               Batch:  9/36	Loss 2.5539 (3.3612)
2022-11-11 22:43:06,517:INFO: Dataset: zara2               Batch: 10/36	Loss 1.6065 (3.1676)
2022-11-11 22:43:06,551:INFO: Dataset: zara2               Batch: 11/36	Loss 1.9265 (2.9826)
2022-11-11 22:43:06,586:INFO: Dataset: zara2               Batch: 12/36	Loss 1.4473 (2.7406)
2022-11-11 22:43:06,611:INFO: Dataset: zara2               Batch: 13/36	Loss 1.1221 (2.5351)
2022-11-11 22:43:06,636:INFO: Dataset: zara2               Batch: 14/36	Loss 1.6995 (2.4486)
2022-11-11 22:43:06,664:INFO: Dataset: zara2               Batch: 15/36	Loss 1.4957 (2.3404)
2022-11-11 22:43:06,696:INFO: Dataset: zara2               Batch: 16/36	Loss 1.3755 (2.2596)
2022-11-11 22:43:06,739:INFO: Dataset: zara2               Batch: 17/36	Loss 1.1377 (2.1794)
2022-11-11 22:43:06,767:INFO: Dataset: zara2               Batch: 18/36	Loss 1.0606 (2.1212)
2022-11-11 22:43:06,793:INFO: Dataset: zara2               Batch: 19/36	Loss 1.5212 (2.0781)
2022-11-11 22:43:06,820:INFO: Dataset: zara2               Batch: 20/36	Loss 1.5873 (2.0410)
2022-11-11 22:43:06,847:INFO: Dataset: zara2               Batch: 21/36	Loss 1.9168 (2.0302)
2022-11-11 22:43:06,872:INFO: Dataset: zara2               Batch: 22/36	Loss 1.4747 (2.0020)
2022-11-11 22:43:06,898:INFO: Dataset: zara2               Batch: 23/36	Loss 2.8128 (2.0234)
2022-11-11 22:43:06,925:INFO: Dataset: zara2               Batch: 24/36	Loss 3.5716 (2.0672)
2022-11-11 22:43:06,955:INFO: Dataset: zara2               Batch: 25/36	Loss 3.9698 (2.1180)
2022-11-11 22:43:06,979:INFO: Dataset: zara2               Batch: 26/36	Loss 0.9845 (2.1000)
2022-11-11 22:43:07,002:INFO: Dataset: zara2               Batch: 27/36	Loss 0.9218 (2.0788)
2022-11-11 22:43:07,035:INFO: Dataset: zara2               Batch: 28/36	Loss 1.7287 (2.0647)
2022-11-11 22:43:07,061:INFO: Dataset: zara2               Batch: 29/36	Loss 2.4781 (2.0795)
2022-11-11 22:43:07,091:INFO: Dataset: zara2               Batch: 30/36	Loss 2.5714 (2.0910)
2022-11-11 22:43:07,122:INFO: Dataset: zara2               Batch: 31/36	Loss 2.0839 (2.0908)
2022-11-11 22:43:07,157:INFO: Dataset: zara2               Batch: 32/36	Loss 2.6970 (2.1025)
2022-11-11 22:43:07,183:INFO: Dataset: zara2               Batch: 33/36	Loss 2.6274 (2.1131)
2022-11-11 22:43:07,212:INFO: Dataset: zara2               Batch: 34/36	Loss 3.5621 (2.1390)
2022-11-11 22:43:07,241:INFO: Dataset: zara2               Batch: 35/36	Loss 2.3516 (2.1425)
2022-11-11 22:43:07,263:INFO: Dataset: zara2               Batch: 36/36	Loss 1.5722 (2.1357)
2022-11-11 22:43:08,204:INFO:  --> Model Saved in ./models/E6//P1/CRMF_epoch_1.pth.tar
2022-11-11 22:43:08,204:INFO: 
===> EPOCH: 2 (P1)
2022-11-11 22:43:08,204:INFO: - Computing loss (training)
2022-11-11 22:43:15,386:INFO: Dataset: hotel               Batch: 1/8	Loss 2.4372 (2.4372)
2022-11-11 22:43:15,787:INFO: Dataset: hotel               Batch: 2/8	Loss 1.5817 (1.9263)
2022-11-11 22:43:16,101:INFO: Dataset: hotel               Batch: 3/8	Loss 2.4190 (2.0958)
2022-11-11 22:43:16,427:INFO: Dataset: hotel               Batch: 4/8	Loss 1.9769 (2.0708)
2022-11-11 22:43:16,684:INFO: Dataset: hotel               Batch: 5/8	Loss 1.8576 (2.0284)
2022-11-11 22:43:16,916:INFO: Dataset: hotel               Batch: 6/8	Loss 4.5359 (2.4145)
2022-11-11 22:43:16,941:INFO: Dataset: hotel               Batch: 7/8	Loss 4.5003 (2.6650)
2022-11-11 22:43:16,959:INFO: Dataset: hotel               Batch: 8/8	Loss 0.2314 (2.5301)
2022-11-11 22:43:41,183:INFO: Dataset: univ                Batch:  1/29	Loss 1.3198 (1.3198)
2022-11-11 22:43:41,216:INFO: Dataset: univ                Batch:  2/29	Loss 1.3365 (1.3276)
2022-11-11 22:43:41,251:INFO: Dataset: univ                Batch:  3/29	Loss 1.2979 (1.3204)
2022-11-11 22:43:41,283:INFO: Dataset: univ                Batch:  4/29	Loss 1.5160 (1.3584)
2022-11-11 22:43:41,317:INFO: Dataset: univ                Batch:  5/29	Loss 1.2769 (1.3420)
2022-11-11 22:43:41,359:INFO: Dataset: univ                Batch:  6/29	Loss 1.2096 (1.3180)
2022-11-11 22:43:41,388:INFO: Dataset: univ                Batch:  7/29	Loss 1.0973 (1.2855)
2022-11-11 22:43:41,419:INFO: Dataset: univ                Batch:  8/29	Loss 1.0241 (1.2537)
2022-11-11 22:43:41,450:INFO: Dataset: univ                Batch:  9/29	Loss 1.1370 (1.2379)
2022-11-11 22:43:41,485:INFO: Dataset: univ                Batch: 10/29	Loss 1.0983 (1.2256)
2022-11-11 22:43:41,518:INFO: Dataset: univ                Batch: 11/29	Loss 1.4293 (1.2403)
2022-11-11 22:43:41,556:INFO: Dataset: univ                Batch: 12/29	Loss 1.6503 (1.2640)
2022-11-11 22:43:41,587:INFO: Dataset: univ                Batch: 13/29	Loss 1.6477 (1.2835)
2022-11-11 22:43:41,619:INFO: Dataset: univ                Batch: 14/29	Loss 1.8457 (1.3190)
2022-11-11 22:43:41,652:INFO: Dataset: univ                Batch: 15/29	Loss 1.5308 (1.3279)
2022-11-11 22:43:41,684:INFO: Dataset: univ                Batch: 16/29	Loss 1.3037 (1.3268)
2022-11-11 22:43:41,717:INFO: Dataset: univ                Batch: 17/29	Loss 1.5104 (1.3360)
2022-11-11 22:43:41,750:INFO: Dataset: univ                Batch: 18/29	Loss 1.7641 (1.3552)
2022-11-11 22:43:41,778:INFO: Dataset: univ                Batch: 19/29	Loss 1.3850 (1.3566)
2022-11-11 22:43:41,805:INFO: Dataset: univ                Batch: 20/29	Loss 1.3492 (1.3564)
2022-11-11 22:43:41,844:INFO: Dataset: univ                Batch: 21/29	Loss 1.0674 (1.3465)
2022-11-11 22:43:41,877:INFO: Dataset: univ                Batch: 22/29	Loss 1.0771 (1.3393)
2022-11-11 22:43:41,915:INFO: Dataset: univ                Batch: 23/29	Loss 1.6048 (1.3456)
2022-11-11 22:43:41,938:INFO: Dataset: univ                Batch: 24/29	Loss 3.2020 (1.3647)
2022-11-11 22:43:41,967:INFO: Dataset: univ                Batch: 25/29	Loss 2.5589 (1.3710)
2022-11-11 22:43:41,993:INFO: Dataset: univ                Batch: 26/29	Loss 2.8690 (1.3760)
2022-11-11 22:43:42,021:INFO: Dataset: univ                Batch: 27/29	Loss 3.2326 (1.3832)
2022-11-11 22:43:42,046:INFO: Dataset: univ                Batch: 28/29	Loss 3.6501 (1.3927)
2022-11-11 22:43:42,061:INFO: Dataset: univ                Batch: 29/29	Loss 1.4734 (1.3928)
2022-11-11 22:43:50,218:INFO: Dataset: zara1               Batch:  1/16	Loss 1.9255 (1.9255)
2022-11-11 22:43:50,529:INFO: Dataset: zara1               Batch:  2/16	Loss 1.3909 (1.6682)
2022-11-11 22:43:50,911:INFO: Dataset: zara1               Batch:  3/16	Loss 1.6252 (1.6537)
2022-11-11 22:43:51,271:INFO: Dataset: zara1               Batch:  4/16	Loss 2.2513 (1.7672)
2022-11-11 22:43:51,516:INFO: Dataset: zara1               Batch:  5/16	Loss 1.6805 (1.7523)
2022-11-11 22:43:51,736:INFO: Dataset: zara1               Batch:  6/16	Loss 1.4218 (1.7045)
2022-11-11 22:43:51,781:INFO: Dataset: zara1               Batch:  7/16	Loss 2.0810 (1.7441)
2022-11-11 22:43:51,820:INFO: Dataset: zara1               Batch:  8/16	Loss 2.7789 (1.8511)
2022-11-11 22:43:51,853:INFO: Dataset: zara1               Batch:  9/16	Loss 2.5303 (1.9362)
2022-11-11 22:43:51,887:INFO: Dataset: zara1               Batch: 10/16	Loss 1.9085 (1.9345)
2022-11-11 22:43:51,918:INFO: Dataset: zara1               Batch: 11/16	Loss 2.0604 (1.9429)
2022-11-11 22:43:51,946:INFO: Dataset: zara1               Batch: 12/16	Loss 1.6784 (1.8983)
2022-11-11 22:43:51,971:INFO: Dataset: zara1               Batch: 13/16	Loss 1.6193 (1.8554)
2022-11-11 22:43:51,994:INFO: Dataset: zara1               Batch: 14/16	Loss 2.3910 (1.8787)
2022-11-11 22:43:52,022:INFO: Dataset: zara1               Batch: 15/16	Loss 2.3981 (1.9090)
2022-11-11 22:43:52,040:INFO: Dataset: zara1               Batch: 16/16	Loss 1.4207 (1.8867)
2022-11-11 22:44:15,470:INFO: Dataset: zara2               Batch:  1/36	Loss 2.0199 (2.0199)
2022-11-11 22:44:15,503:INFO: Dataset: zara2               Batch:  2/36	Loss 2.0094 (2.0145)
2022-11-11 22:44:15,536:INFO: Dataset: zara2               Batch:  3/36	Loss 2.0441 (2.0274)
2022-11-11 22:44:15,571:INFO: Dataset: zara2               Batch:  4/36	Loss 1.4329 (1.8818)
2022-11-11 22:44:15,600:INFO: Dataset: zara2               Batch:  5/36	Loss 1.9325 (1.8928)
2022-11-11 22:44:15,632:INFO: Dataset: zara2               Batch:  6/36	Loss 1.2117 (1.7712)
2022-11-11 22:44:15,662:INFO: Dataset: zara2               Batch:  7/36	Loss 1.7750 (1.7716)
2022-11-11 22:44:15,691:INFO: Dataset: zara2               Batch:  8/36	Loss 1.3071 (1.7119)
2022-11-11 22:44:15,727:INFO: Dataset: zara2               Batch:  9/36	Loss 1.5092 (1.6994)
2022-11-11 22:44:15,755:INFO: Dataset: zara2               Batch: 10/36	Loss 1.1045 (1.6338)
2022-11-11 22:44:15,787:INFO: Dataset: zara2               Batch: 11/36	Loss 1.1380 (1.5599)
2022-11-11 22:44:15,816:INFO: Dataset: zara2               Batch: 12/36	Loss 0.9946 (1.4708)
2022-11-11 22:44:15,848:INFO: Dataset: zara2               Batch: 13/36	Loss 0.9172 (1.4005)
2022-11-11 22:44:15,879:INFO: Dataset: zara2               Batch: 14/36	Loss 1.0016 (1.3592)
2022-11-11 22:44:15,904:INFO: Dataset: zara2               Batch: 15/36	Loss 1.0493 (1.3240)
2022-11-11 22:44:15,931:INFO: Dataset: zara2               Batch: 16/36	Loss 0.8057 (1.2806)
2022-11-11 22:44:15,957:INFO: Dataset: zara2               Batch: 17/36	Loss 0.8739 (1.2516)
2022-11-11 22:44:15,985:INFO: Dataset: zara2               Batch: 18/36	Loss 0.7975 (1.2279)
2022-11-11 22:44:16,009:INFO: Dataset: zara2               Batch: 19/36	Loss 0.9245 (1.2062)
2022-11-11 22:44:16,036:INFO: Dataset: zara2               Batch: 20/36	Loss 1.0141 (1.1916)
2022-11-11 22:44:16,063:INFO: Dataset: zara2               Batch: 21/36	Loss 1.1966 (1.1921)
2022-11-11 22:44:16,091:INFO: Dataset: zara2               Batch: 22/36	Loss 0.8328 (1.1738)
2022-11-11 22:44:16,116:INFO: Dataset: zara2               Batch: 23/36	Loss 1.5623 (1.1841)
2022-11-11 22:44:16,137:INFO: Dataset: zara2               Batch: 24/36	Loss 1.6958 (1.1985)
2022-11-11 22:44:16,164:INFO: Dataset: zara2               Batch: 25/36	Loss 1.9026 (1.2173)
2022-11-11 22:44:16,190:INFO: Dataset: zara2               Batch: 26/36	Loss 0.7569 (1.2100)
2022-11-11 22:44:16,215:INFO: Dataset: zara2               Batch: 27/36	Loss 0.6447 (1.1998)
2022-11-11 22:44:16,243:INFO: Dataset: zara2               Batch: 28/36	Loss 1.0518 (1.1939)
2022-11-11 22:44:16,268:INFO: Dataset: zara2               Batch: 29/36	Loss 1.1889 (1.1937)
2022-11-11 22:44:16,290:INFO: Dataset: zara2               Batch: 30/36	Loss 1.3764 (1.1980)
2022-11-11 22:44:16,314:INFO: Dataset: zara2               Batch: 31/36	Loss 1.3252 (1.2013)
2022-11-11 22:44:16,334:INFO: Dataset: zara2               Batch: 32/36	Loss 1.6752 (1.2104)
2022-11-11 22:44:16,355:INFO: Dataset: zara2               Batch: 33/36	Loss 1.2709 (1.2116)
2022-11-11 22:44:16,380:INFO: Dataset: zara2               Batch: 34/36	Loss 1.4699 (1.2162)
2022-11-11 22:44:16,408:INFO: Dataset: zara2               Batch: 35/36	Loss 1.4939 (1.2208)
2022-11-11 22:44:16,428:INFO: Dataset: zara2               Batch: 36/36	Loss 1.0959 (1.2194)
2022-11-11 22:44:17,366:INFO:  --> Model Saved in ./models/E6//P1/CRMF_epoch_2.pth.tar
2022-11-11 22:44:17,366:INFO: 
===> EPOCH: 3 (P2)
2022-11-11 22:44:17,367:INFO: - Computing loss (training)
2022-11-11 22:44:23,908:INFO: Dataset: hotel               Batch: 1/8	Loss 3.2048 (3.2048)
2022-11-11 22:44:24,291:INFO: Dataset: hotel               Batch: 2/8	Loss 1.7665 (2.3457)
2022-11-11 22:44:24,659:INFO: Dataset: hotel               Batch: 3/8	Loss 2.7052 (2.4695)
2022-11-11 22:44:24,985:INFO: Dataset: hotel               Batch: 4/8	Loss 2.2631 (2.4260)
2022-11-11 22:44:25,291:INFO: Dataset: hotel               Batch: 5/8	Loss 2.0689 (2.3550)
2022-11-11 22:44:25,542:INFO: Dataset: hotel               Batch: 6/8	Loss 5.6671 (2.8649)
2022-11-11 22:44:25,667:INFO: Dataset: hotel               Batch: 7/8	Loss 5.5621 (3.1889)
2022-11-11 22:44:25,720:INFO: Dataset: hotel               Batch: 8/8	Loss 0.2398 (3.0255)
2022-11-11 22:44:50,606:INFO: Dataset: univ                Batch:  1/29	Loss 2.0009 (2.0009)
2022-11-11 22:44:50,743:INFO: Dataset: univ                Batch:  2/29	Loss 2.1216 (2.0575)
2022-11-11 22:44:50,842:INFO: Dataset: univ                Batch:  3/29	Loss 2.0672 (2.0598)
2022-11-11 22:44:50,945:INFO: Dataset: univ                Batch:  4/29	Loss 2.2906 (2.1046)
2022-11-11 22:44:51,045:INFO: Dataset: univ                Batch:  5/29	Loss 1.9500 (2.0736)
2022-11-11 22:44:51,151:INFO: Dataset: univ                Batch:  6/29	Loss 1.7348 (2.0122)
2022-11-11 22:44:51,262:INFO: Dataset: univ                Batch:  7/29	Loss 1.5271 (1.9407)
2022-11-11 22:44:51,371:INFO: Dataset: univ                Batch:  8/29	Loss 1.6573 (1.9062)
2022-11-11 22:44:51,475:INFO: Dataset: univ                Batch:  9/29	Loss 1.6911 (1.8771)
2022-11-11 22:44:51,580:INFO: Dataset: univ                Batch: 10/29	Loss 1.4720 (1.8413)
2022-11-11 22:44:51,698:INFO: Dataset: univ                Batch: 11/29	Loss 2.3774 (1.8802)
2022-11-11 22:44:51,820:INFO: Dataset: univ                Batch: 12/29	Loss 3.0161 (1.9458)
2022-11-11 22:44:51,946:INFO: Dataset: univ                Batch: 13/29	Loss 2.5781 (1.9779)
2022-11-11 22:44:52,061:INFO: Dataset: univ                Batch: 14/29	Loss 3.0720 (2.0470)
2022-11-11 22:44:52,176:INFO: Dataset: univ                Batch: 15/29	Loss 2.3580 (2.0601)
2022-11-11 22:44:52,285:INFO: Dataset: univ                Batch: 16/29	Loss 1.8043 (2.0486)
2022-11-11 22:44:52,412:INFO: Dataset: univ                Batch: 17/29	Loss 2.2564 (2.0590)
2022-11-11 22:44:52,532:INFO: Dataset: univ                Batch: 18/29	Loss 2.4960 (2.0786)
2022-11-11 22:44:52,636:INFO: Dataset: univ                Batch: 19/29	Loss 1.8281 (2.0663)
2022-11-11 22:44:52,762:INFO: Dataset: univ                Batch: 20/29	Loss 2.0564 (2.0659)
2022-11-11 22:44:52,888:INFO: Dataset: univ                Batch: 21/29	Loss 1.5116 (2.0470)
2022-11-11 22:44:53,018:INFO: Dataset: univ                Batch: 22/29	Loss 1.6344 (2.0360)
2022-11-11 22:44:53,136:INFO: Dataset: univ                Batch: 23/29	Loss 2.3256 (2.0428)
2022-11-11 22:44:53,243:INFO: Dataset: univ                Batch: 24/29	Loss 6.1037 (2.0846)
2022-11-11 22:44:53,360:INFO: Dataset: univ                Batch: 25/29	Loss 4.0897 (2.0951)
2022-11-11 22:44:53,464:INFO: Dataset: univ                Batch: 26/29	Loss 5.6598 (2.1071)
2022-11-11 22:44:53,589:INFO: Dataset: univ                Batch: 27/29	Loss 5.6421 (2.1209)
2022-11-11 22:44:53,701:INFO: Dataset: univ                Batch: 28/29	Loss 6.6990 (2.1400)
2022-11-11 22:44:53,751:INFO: Dataset: univ                Batch: 29/29	Loss 2.5888 (2.1405)
2022-11-11 22:45:02,778:INFO: Dataset: zara1               Batch:  1/16	Loss 2.6043 (2.6043)
2022-11-11 22:45:03,282:INFO: Dataset: zara1               Batch:  2/16	Loss 2.0659 (2.3451)
2022-11-11 22:45:03,745:INFO: Dataset: zara1               Batch:  3/16	Loss 3.2657 (2.6565)
2022-11-11 22:45:04,002:INFO: Dataset: zara1               Batch:  4/16	Loss 2.8760 (2.6982)
2022-11-11 22:45:04,210:INFO: Dataset: zara1               Batch:  5/16	Loss 2.5773 (2.6774)
2022-11-11 22:45:04,506:INFO: Dataset: zara1               Batch:  6/16	Loss 2.0058 (2.5804)
2022-11-11 22:45:04,622:INFO: Dataset: zara1               Batch:  7/16	Loss 3.9568 (2.7252)
2022-11-11 22:45:04,743:INFO: Dataset: zara1               Batch:  8/16	Loss 3.9827 (2.8552)
2022-11-11 22:45:04,863:INFO: Dataset: zara1               Batch:  9/16	Loss 3.3009 (2.9110)
2022-11-11 22:45:04,979:INFO: Dataset: zara1               Batch: 10/16	Loss 2.6256 (2.8935)
2022-11-11 22:45:05,102:INFO: Dataset: zara1               Batch: 11/16	Loss 3.3083 (2.9213)
2022-11-11 22:45:05,209:INFO: Dataset: zara1               Batch: 12/16	Loss 2.2451 (2.8073)
2022-11-11 22:45:05,314:INFO: Dataset: zara1               Batch: 13/16	Loss 2.6728 (2.7866)
2022-11-11 22:45:05,433:INFO: Dataset: zara1               Batch: 14/16	Loss 2.8796 (2.7906)
2022-11-11 22:45:05,542:INFO: Dataset: zara1               Batch: 15/16	Loss 2.7098 (2.7859)
2022-11-11 22:45:05,626:INFO: Dataset: zara1               Batch: 16/16	Loss 1.6627 (2.7345)
2022-11-11 22:45:30,792:INFO: Dataset: zara2               Batch:  1/36	Loss 3.2997 (3.2997)
2022-11-11 22:45:30,902:INFO: Dataset: zara2               Batch:  2/36	Loss 2.6545 (2.9730)
2022-11-11 22:45:31,020:INFO: Dataset: zara2               Batch:  3/36	Loss 2.5624 (2.7943)
2022-11-11 22:45:31,128:INFO: Dataset: zara2               Batch:  4/36	Loss 1.6043 (2.5028)
2022-11-11 22:45:31,236:INFO: Dataset: zara2               Batch:  5/36	Loss 1.9798 (2.3891)
2022-11-11 22:45:31,350:INFO: Dataset: zara2               Batch:  6/36	Loss 1.5537 (2.2400)
2022-11-11 22:45:31,464:INFO: Dataset: zara2               Batch:  7/36	Loss 1.9992 (2.2180)
2022-11-11 22:45:31,577:INFO: Dataset: zara2               Batch:  8/36	Loss 1.8621 (2.1723)
2022-11-11 22:45:31,685:INFO: Dataset: zara2               Batch:  9/36	Loss 1.6498 (2.1401)
2022-11-11 22:45:31,799:INFO: Dataset: zara2               Batch: 10/36	Loss 1.1932 (2.0356)
2022-11-11 22:45:31,913:INFO: Dataset: zara2               Batch: 11/36	Loss 1.1953 (1.9104)
2022-11-11 22:45:32,028:INFO: Dataset: zara2               Batch: 12/36	Loss 1.2409 (1.8048)
2022-11-11 22:45:32,137:INFO: Dataset: zara2               Batch: 13/36	Loss 1.0238 (1.7057)
2022-11-11 22:45:32,259:INFO: Dataset: zara2               Batch: 14/36	Loss 1.1174 (1.6448)
2022-11-11 22:45:32,372:INFO: Dataset: zara2               Batch: 15/36	Loss 1.2443 (1.5993)
2022-11-11 22:45:32,478:INFO: Dataset: zara2               Batch: 16/36	Loss 0.9536 (1.5453)
2022-11-11 22:45:32,588:INFO: Dataset: zara2               Batch: 17/36	Loss 0.8328 (1.4943)
2022-11-11 22:45:32,703:INFO: Dataset: zara2               Batch: 18/36	Loss 0.9572 (1.4664)
2022-11-11 22:45:32,816:INFO: Dataset: zara2               Batch: 19/36	Loss 0.9607 (1.4301)
2022-11-11 22:45:32,922:INFO: Dataset: zara2               Batch: 20/36	Loss 1.2601 (1.4172)
2022-11-11 22:45:33,033:INFO: Dataset: zara2               Batch: 21/36	Loss 1.2034 (1.3987)
2022-11-11 22:45:33,143:INFO: Dataset: zara2               Batch: 22/36	Loss 1.0735 (1.3821)
2022-11-11 22:45:33,251:INFO: Dataset: zara2               Batch: 23/36	Loss 2.4160 (1.4095)
2022-11-11 22:45:33,361:INFO: Dataset: zara2               Batch: 24/36	Loss 1.9874 (1.4258)
2022-11-11 22:45:33,464:INFO: Dataset: zara2               Batch: 25/36	Loss 2.6667 (1.4590)
2022-11-11 22:45:33,572:INFO: Dataset: zara2               Batch: 26/36	Loss 1.0985 (1.4532)
2022-11-11 22:45:33,677:INFO: Dataset: zara2               Batch: 27/36	Loss 0.8078 (1.4416)
2022-11-11 22:45:33,785:INFO: Dataset: zara2               Batch: 28/36	Loss 1.2191 (1.4326)
2022-11-11 22:45:33,889:INFO: Dataset: zara2               Batch: 29/36	Loss 1.6549 (1.4406)
2022-11-11 22:45:34,013:INFO: Dataset: zara2               Batch: 30/36	Loss 1.6707 (1.4460)
2022-11-11 22:45:34,121:INFO: Dataset: zara2               Batch: 31/36	Loss 1.5974 (1.4499)
2022-11-11 22:45:34,231:INFO: Dataset: zara2               Batch: 32/36	Loss 2.0140 (1.4608)
2022-11-11 22:45:34,353:INFO: Dataset: zara2               Batch: 33/36	Loss 2.0501 (1.4727)
2022-11-11 22:45:34,460:INFO: Dataset: zara2               Batch: 34/36	Loss 1.9354 (1.4810)
2022-11-11 22:45:34,569:INFO: Dataset: zara2               Batch: 35/36	Loss 1.7051 (1.4847)
2022-11-11 22:45:34,647:INFO: Dataset: zara2               Batch: 36/36	Loss 1.2869 (1.4823)
2022-11-11 22:45:35,629:INFO:  --> Model Saved in ./models/E6//P2/CRMF_epoch_3.pth.tar
2022-11-11 22:45:35,629:INFO: 
===> EPOCH: 4 (P2)
2022-11-11 22:45:35,630:INFO: - Computing loss (training)
2022-11-11 22:45:42,821:INFO: Dataset: hotel               Batch: 1/8	Loss 2.5651 (2.5651)
2022-11-11 22:45:43,104:INFO: Dataset: hotel               Batch: 2/8	Loss 1.5328 (1.9485)
2022-11-11 22:45:43,549:INFO: Dataset: hotel               Batch: 3/8	Loss 2.2513 (2.0527)
2022-11-11 22:45:43,849:INFO: Dataset: hotel               Batch: 4/8	Loss 1.9691 (2.0351)
2022-11-11 22:45:44,117:INFO: Dataset: hotel               Batch: 5/8	Loss 1.8329 (1.9949)
2022-11-11 22:45:44,367:INFO: Dataset: hotel               Batch: 6/8	Loss 4.5066 (2.3816)
2022-11-11 22:45:44,481:INFO: Dataset: hotel               Batch: 7/8	Loss 4.5544 (2.6426)
2022-11-11 22:45:44,516:INFO: Dataset: hotel               Batch: 8/8	Loss 0.2504 (2.5100)
2022-11-11 22:46:10,742:INFO: Dataset: univ                Batch:  1/29	Loss 1.0716 (1.0716)
2022-11-11 22:46:10,853:INFO: Dataset: univ                Batch:  2/29	Loss 1.0255 (1.0500)
2022-11-11 22:46:10,965:INFO: Dataset: univ                Batch:  3/29	Loss 1.0713 (1.0551)
2022-11-11 22:46:11,071:INFO: Dataset: univ                Batch:  4/29	Loss 1.2113 (1.0855)
2022-11-11 22:46:11,192:INFO: Dataset: univ                Batch:  5/29	Loss 1.0924 (1.0869)
2022-11-11 22:46:11,306:INFO: Dataset: univ                Batch:  6/29	Loss 1.0316 (1.0768)
2022-11-11 22:46:11,414:INFO: Dataset: univ                Batch:  7/29	Loss 0.9071 (1.0518)
2022-11-11 22:46:11,539:INFO: Dataset: univ                Batch:  8/29	Loss 0.8547 (1.0278)
2022-11-11 22:46:11,651:INFO: Dataset: univ                Batch:  9/29	Loss 0.8886 (1.0090)
2022-11-11 22:46:11,752:INFO: Dataset: univ                Batch: 10/29	Loss 0.8869 (0.9982)
2022-11-11 22:46:11,854:INFO: Dataset: univ                Batch: 11/29	Loss 1.1165 (1.0068)
2022-11-11 22:46:11,965:INFO: Dataset: univ                Batch: 12/29	Loss 1.2025 (1.0181)
2022-11-11 22:46:12,077:INFO: Dataset: univ                Batch: 13/29	Loss 1.3116 (1.0330)
2022-11-11 22:46:12,197:INFO: Dataset: univ                Batch: 14/29	Loss 1.3419 (1.0525)
2022-11-11 22:46:12,318:INFO: Dataset: univ                Batch: 15/29	Loss 1.1058 (1.0547)
2022-11-11 22:46:12,423:INFO: Dataset: univ                Batch: 16/29	Loss 1.0080 (1.0526)
2022-11-11 22:46:12,524:INFO: Dataset: univ                Batch: 17/29	Loss 1.0002 (1.0500)
2022-11-11 22:46:12,630:INFO: Dataset: univ                Batch: 18/29	Loss 1.1829 (1.0560)
2022-11-11 22:46:12,754:INFO: Dataset: univ                Batch: 19/29	Loss 0.9573 (1.0511)
2022-11-11 22:46:12,856:INFO: Dataset: univ                Batch: 20/29	Loss 1.0965 (1.0526)
2022-11-11 22:46:12,968:INFO: Dataset: univ                Batch: 21/29	Loss 0.8013 (1.0440)
2022-11-11 22:46:13,072:INFO: Dataset: univ                Batch: 22/29	Loss 0.8156 (1.0379)
2022-11-11 22:46:13,178:INFO: Dataset: univ                Batch: 23/29	Loss 1.1926 (1.0416)
2022-11-11 22:46:13,285:INFO: Dataset: univ                Batch: 24/29	Loss 2.3424 (1.0550)
2022-11-11 22:46:13,395:INFO: Dataset: univ                Batch: 25/29	Loss 1.9406 (1.0596)
2022-11-11 22:46:13,496:INFO: Dataset: univ                Batch: 26/29	Loss 2.1247 (1.0632)
2022-11-11 22:46:13,599:INFO: Dataset: univ                Batch: 27/29	Loss 2.5568 (1.0690)
2022-11-11 22:46:13,708:INFO: Dataset: univ                Batch: 28/29	Loss 2.4315 (1.0747)
2022-11-11 22:46:13,764:INFO: Dataset: univ                Batch: 29/29	Loss 0.7569 (1.0744)
2022-11-11 22:46:21,928:INFO: Dataset: zara1               Batch:  1/16	Loss 1.1800 (1.1800)
2022-11-11 22:46:22,243:INFO: Dataset: zara1               Batch:  2/16	Loss 0.9391 (1.0641)
2022-11-11 22:46:22,697:INFO: Dataset: zara1               Batch:  3/16	Loss 1.1639 (1.0978)
2022-11-11 22:46:23,041:INFO: Dataset: zara1               Batch:  4/16	Loss 1.4714 (1.1688)
2022-11-11 22:46:23,343:INFO: Dataset: zara1               Batch:  5/16	Loss 1.0929 (1.1557)
2022-11-11 22:46:23,585:INFO: Dataset: zara1               Batch:  6/16	Loss 0.7966 (1.1039)
2022-11-11 22:46:23,691:INFO: Dataset: zara1               Batch:  7/16	Loss 1.4715 (1.1425)
2022-11-11 22:46:23,792:INFO: Dataset: zara1               Batch:  8/16	Loss 1.4346 (1.1727)
2022-11-11 22:46:23,908:INFO: Dataset: zara1               Batch:  9/16	Loss 1.4040 (1.2017)
2022-11-11 22:46:24,014:INFO: Dataset: zara1               Batch: 10/16	Loss 1.6766 (1.2309)
2022-11-11 22:46:24,119:INFO: Dataset: zara1               Batch: 11/16	Loss 1.4400 (1.2449)
2022-11-11 22:46:24,239:INFO: Dataset: zara1               Batch: 12/16	Loss 1.1175 (1.2235)
2022-11-11 22:46:24,342:INFO: Dataset: zara1               Batch: 13/16	Loss 1.1945 (1.2190)
2022-11-11 22:46:24,443:INFO: Dataset: zara1               Batch: 14/16	Loss 1.1785 (1.2173)
2022-11-11 22:46:24,548:INFO: Dataset: zara1               Batch: 15/16	Loss 1.3481 (1.2249)
2022-11-11 22:46:24,633:INFO: Dataset: zara1               Batch: 16/16	Loss 0.8154 (1.2062)
2022-11-11 22:46:49,030:INFO: Dataset: zara2               Batch:  1/36	Loss 1.6594 (1.6594)
2022-11-11 22:46:49,145:INFO: Dataset: zara2               Batch:  2/36	Loss 1.2646 (1.4595)
2022-11-11 22:46:49,254:INFO: Dataset: zara2               Batch:  3/36	Loss 1.2633 (1.3742)
2022-11-11 22:46:49,363:INFO: Dataset: zara2               Batch:  4/36	Loss 0.8430 (1.2440)
2022-11-11 22:46:49,463:INFO: Dataset: zara2               Batch:  5/36	Loss 1.2155 (1.2378)
2022-11-11 22:46:49,574:INFO: Dataset: zara2               Batch:  6/36	Loss 0.8224 (1.1637)
2022-11-11 22:46:49,676:INFO: Dataset: zara2               Batch:  7/36	Loss 1.4069 (1.1858)
2022-11-11 22:46:49,785:INFO: Dataset: zara2               Batch:  8/36	Loss 1.0969 (1.1744)
2022-11-11 22:46:49,895:INFO: Dataset: zara2               Batch:  9/36	Loss 0.8586 (1.1550)
2022-11-11 22:46:50,003:INFO: Dataset: zara2               Batch: 10/36	Loss 0.7428 (1.1095)
2022-11-11 22:46:50,115:INFO: Dataset: zara2               Batch: 11/36	Loss 0.7729 (1.0593)
2022-11-11 22:46:50,240:INFO: Dataset: zara2               Batch: 12/36	Loss 0.7430 (1.0095)
2022-11-11 22:46:50,357:INFO: Dataset: zara2               Batch: 13/36	Loss 0.6230 (0.9604)
2022-11-11 22:46:50,465:INFO: Dataset: zara2               Batch: 14/36	Loss 0.7547 (0.9391)
2022-11-11 22:46:50,580:INFO: Dataset: zara2               Batch: 15/36	Loss 0.7673 (0.9196)
2022-11-11 22:46:50,707:INFO: Dataset: zara2               Batch: 16/36	Loss 0.5521 (0.8888)
2022-11-11 22:46:50,825:INFO: Dataset: zara2               Batch: 17/36	Loss 0.6360 (0.8708)
2022-11-11 22:46:50,940:INFO: Dataset: zara2               Batch: 18/36	Loss 0.6439 (0.8590)
2022-11-11 22:46:51,053:INFO: Dataset: zara2               Batch: 19/36	Loss 0.6444 (0.8436)
2022-11-11 22:46:51,160:INFO: Dataset: zara2               Batch: 20/36	Loss 0.7710 (0.8381)
2022-11-11 22:46:51,270:INFO: Dataset: zara2               Batch: 21/36	Loss 0.7646 (0.8317)
2022-11-11 22:46:51,376:INFO: Dataset: zara2               Batch: 22/36	Loss 0.6720 (0.8236)
2022-11-11 22:46:51,482:INFO: Dataset: zara2               Batch: 23/36	Loss 1.3184 (0.8366)
2022-11-11 22:46:51,600:INFO: Dataset: zara2               Batch: 24/36	Loss 1.2159 (0.8474)
2022-11-11 22:46:51,711:INFO: Dataset: zara2               Batch: 25/36	Loss 1.4631 (0.8638)
2022-11-11 22:46:51,808:INFO: Dataset: zara2               Batch: 26/36	Loss 0.6495 (0.8604)
2022-11-11 22:46:51,922:INFO: Dataset: zara2               Batch: 27/36	Loss 0.4437 (0.8529)
2022-11-11 22:46:52,020:INFO: Dataset: zara2               Batch: 28/36	Loss 0.7463 (0.8486)
2022-11-11 22:46:52,129:INFO: Dataset: zara2               Batch: 29/36	Loss 1.0633 (0.8563)
2022-11-11 22:46:52,246:INFO: Dataset: zara2               Batch: 30/36	Loss 1.0937 (0.8618)
2022-11-11 22:46:52,350:INFO: Dataset: zara2               Batch: 31/36	Loss 0.8999 (0.8628)
2022-11-11 22:46:52,484:INFO: Dataset: zara2               Batch: 32/36	Loss 1.1199 (0.8678)
2022-11-11 22:46:52,598:INFO: Dataset: zara2               Batch: 33/36	Loss 1.0879 (0.8723)
2022-11-11 22:46:52,707:INFO: Dataset: zara2               Batch: 34/36	Loss 1.1325 (0.8769)
2022-11-11 22:46:52,818:INFO: Dataset: zara2               Batch: 35/36	Loss 1.0613 (0.8800)
2022-11-11 22:46:52,910:INFO: Dataset: zara2               Batch: 36/36	Loss 0.9941 (0.8813)
2022-11-11 22:46:53,843:INFO:  --> Model Saved in ./models/E6//P2/CRMF_epoch_4.pth.tar
2022-11-11 22:46:53,843:INFO: 
===> EPOCH: 5 (P3)
2022-11-11 22:46:53,843:INFO: - Computing loss (training)
2022-11-11 22:47:03,337:INFO: Dataset: hotel               Batch: 1/8	Loss 43.4834 (43.4834)
2022-11-11 22:47:04,717:INFO: Dataset: hotel               Batch: 2/8	Loss 39.5002 (41.1043)
2022-11-11 22:47:05,953:INFO: Dataset: hotel               Batch: 3/8	Loss 41.6983 (41.3088)
2022-11-11 22:47:07,129:INFO: Dataset: hotel               Batch: 4/8	Loss 40.6786 (41.1759)
2022-11-11 22:47:08,432:INFO: Dataset: hotel               Batch: 5/8	Loss 40.2472 (40.9912)
2022-11-11 22:47:09,797:INFO: Dataset: hotel               Batch: 6/8	Loss 45.8554 (41.7402)
2022-11-11 22:47:11,097:INFO: Dataset: hotel               Batch: 7/8	Loss 45.3102 (42.1690)
2022-11-11 22:47:12,363:INFO: Dataset: hotel               Batch: 8/8	Loss 8.3537 (40.2953)
2022-11-11 22:47:39,656:INFO: Dataset: univ                Batch:  1/29	Loss 40.5253 (40.5253)
2022-11-11 22:47:41,060:INFO: Dataset: univ                Batch:  2/29	Loss 40.5901 (40.5557)
2022-11-11 22:47:42,442:INFO: Dataset: univ                Batch:  3/29	Loss 40.3833 (40.5141)
2022-11-11 22:47:43,797:INFO: Dataset: univ                Batch:  4/29	Loss 40.5768 (40.5263)
2022-11-11 22:47:45,172:INFO: Dataset: univ                Batch:  5/29	Loss 39.7750 (40.3754)
2022-11-11 22:47:46,509:INFO: Dataset: univ                Batch:  6/29	Loss 39.3239 (40.1848)
2022-11-11 22:47:47,798:INFO: Dataset: univ                Batch:  7/29	Loss 39.1195 (40.0279)
2022-11-11 22:47:49,096:INFO: Dataset: univ                Batch:  8/29	Loss 39.3138 (39.9409)
2022-11-11 22:47:50,520:INFO: Dataset: univ                Batch:  9/29	Loss 40.0817 (39.9599)
2022-11-11 22:47:51,816:INFO: Dataset: univ                Batch: 10/29	Loss 39.3973 (39.9102)
2022-11-11 22:47:53,104:INFO: Dataset: univ                Batch: 11/29	Loss 41.0535 (39.9930)
2022-11-11 22:47:54,390:INFO: Dataset: univ                Batch: 12/29	Loss 42.1624 (40.1184)
2022-11-11 22:47:55,685:INFO: Dataset: univ                Batch: 13/29	Loss 41.6160 (40.1943)
2022-11-11 22:47:56,960:INFO: Dataset: univ                Batch: 14/29	Loss 42.4842 (40.3390)
2022-11-11 22:47:58,214:INFO: Dataset: univ                Batch: 15/29	Loss 41.7625 (40.3988)
2022-11-11 22:47:59,567:INFO: Dataset: univ                Batch: 16/29	Loss 40.6116 (40.4084)
2022-11-11 22:48:00,832:INFO: Dataset: univ                Batch: 17/29	Loss 41.7250 (40.4742)
2022-11-11 22:48:02,141:INFO: Dataset: univ                Batch: 18/29	Loss 41.9784 (40.5416)
2022-11-11 22:48:03,474:INFO: Dataset: univ                Batch: 19/29	Loss 40.6908 (40.5489)
2022-11-11 22:48:04,813:INFO: Dataset: univ                Batch: 20/29	Loss 40.4795 (40.5466)
2022-11-11 22:48:06,100:INFO: Dataset: univ                Batch: 21/29	Loss 40.1014 (40.5314)
2022-11-11 22:48:07,350:INFO: Dataset: univ                Batch: 22/29	Loss 40.0227 (40.5178)
2022-11-11 22:48:08,699:INFO: Dataset: univ                Batch: 23/29	Loss 42.1492 (40.5565)
2022-11-11 22:48:09,976:INFO: Dataset: univ                Batch: 24/29	Loss 49.3733 (40.6471)
2022-11-11 22:48:11,301:INFO: Dataset: univ                Batch: 25/29	Loss 46.5184 (40.6781)
2022-11-11 22:48:12,579:INFO: Dataset: univ                Batch: 26/29	Loss 50.3459 (40.7104)
2022-11-11 22:48:13,852:INFO: Dataset: univ                Batch: 27/29	Loss 48.9382 (40.7426)
2022-11-11 22:48:15,220:INFO: Dataset: univ                Batch: 28/29	Loss 53.2700 (40.7949)
2022-11-11 22:48:16,558:INFO: Dataset: univ                Batch: 29/29	Loss 19.8306 (40.7710)
2022-11-11 22:48:27,215:INFO: Dataset: zara1               Batch:  1/16	Loss 45.5613 (45.5613)
2022-11-11 22:48:28,566:INFO: Dataset: zara1               Batch:  2/16	Loss 43.2242 (44.4364)
2022-11-11 22:48:29,827:INFO: Dataset: zara1               Batch:  3/16	Loss 42.8023 (43.8836)
2022-11-11 22:48:31,023:INFO: Dataset: zara1               Batch:  4/16	Loss 46.4124 (44.3641)
2022-11-11 22:48:32,178:INFO: Dataset: zara1               Batch:  5/16	Loss 42.4502 (44.0346)
2022-11-11 22:48:33,379:INFO: Dataset: zara1               Batch:  6/16	Loss 43.3298 (43.9327)
2022-11-11 22:48:34,536:INFO: Dataset: zara1               Batch:  7/16	Loss 47.8455 (44.3443)
2022-11-11 22:48:36,078:INFO: Dataset: zara1               Batch:  8/16	Loss 51.0563 (45.0384)
2022-11-11 22:48:37,295:INFO: Dataset: zara1               Batch:  9/16	Loss 49.0647 (45.5427)
2022-11-11 22:48:38,610:INFO: Dataset: zara1               Batch: 10/16	Loss 43.6405 (45.4256)
2022-11-11 22:48:40,067:INFO: Dataset: zara1               Batch: 11/16	Loss 46.2884 (45.4834)
2022-11-11 22:48:41,319:INFO: Dataset: zara1               Batch: 12/16	Loss 43.8745 (45.2121)
2022-11-11 22:48:42,562:INFO: Dataset: zara1               Batch: 13/16	Loss 44.7908 (45.1474)
2022-11-11 22:48:43,786:INFO: Dataset: zara1               Batch: 14/16	Loss 48.3526 (45.2863)
2022-11-11 22:48:45,097:INFO: Dataset: zara1               Batch: 15/16	Loss 49.8851 (45.5552)
2022-11-11 22:48:46,329:INFO: Dataset: zara1               Batch: 16/16	Loss 34.8035 (45.0629)
2022-11-11 22:49:14,864:INFO: Dataset: zara2               Batch:  1/36	Loss 48.1897 (48.1897)
2022-11-11 22:49:16,089:INFO: Dataset: zara2               Batch:  2/36	Loss 48.6085 (48.4018)
2022-11-11 22:49:17,357:INFO: Dataset: zara2               Batch:  3/36	Loss 47.8548 (48.1638)
2022-11-11 22:49:18,588:INFO: Dataset: zara2               Batch:  4/36	Loss 43.0093 (46.9009)
2022-11-11 22:49:19,769:INFO: Dataset: zara2               Batch:  5/36	Loss 45.1971 (46.5306)
2022-11-11 22:49:20,960:INFO: Dataset: zara2               Batch:  6/36	Loss 43.2464 (45.9443)
2022-11-11 22:49:22,118:INFO: Dataset: zara2               Batch:  7/36	Loss 42.7594 (45.6538)
2022-11-11 22:49:23,305:INFO: Dataset: zara2               Batch:  8/36	Loss 45.8235 (45.6756)
2022-11-11 22:49:24,518:INFO: Dataset: zara2               Batch:  9/36	Loss 43.0979 (45.5169)
2022-11-11 22:49:25,662:INFO: Dataset: zara2               Batch: 10/36	Loss 39.5152 (44.8547)
2022-11-11 22:49:26,812:INFO: Dataset: zara2               Batch: 11/36	Loss 41.7577 (44.3931)
2022-11-11 22:49:27,954:INFO: Dataset: zara2               Batch: 12/36	Loss 39.7846 (43.6666)
2022-11-11 22:49:29,136:INFO: Dataset: zara2               Batch: 13/36	Loss 39.3608 (43.1198)
2022-11-11 22:49:30,286:INFO: Dataset: zara2               Batch: 14/36	Loss 41.4181 (42.9438)
2022-11-11 22:49:31,438:INFO: Dataset: zara2               Batch: 15/36	Loss 40.5527 (42.6722)
2022-11-11 22:49:32,599:INFO: Dataset: zara2               Batch: 16/36	Loss 40.4192 (42.4836)
2022-11-11 22:49:33,731:INFO: Dataset: zara2               Batch: 17/36	Loss 39.9179 (42.3002)
2022-11-11 22:49:34,878:INFO: Dataset: zara2               Batch: 18/36	Loss 39.0268 (42.1300)
2022-11-11 22:49:36,046:INFO: Dataset: zara2               Batch: 19/36	Loss 40.3008 (41.9986)
2022-11-11 22:49:37,149:INFO: Dataset: zara2               Batch: 20/36	Loss 40.6789 (41.8987)
2022-11-11 22:49:38,343:INFO: Dataset: zara2               Batch: 21/36	Loss 41.8814 (41.8972)
2022-11-11 22:49:39,498:INFO: Dataset: zara2               Batch: 22/36	Loss 40.3972 (41.8209)
2022-11-11 22:49:40,720:INFO: Dataset: zara2               Batch: 23/36	Loss 46.8251 (41.9532)
2022-11-11 22:49:41,941:INFO: Dataset: zara2               Batch: 24/36	Loss 47.8420 (42.1199)
2022-11-11 22:49:43,148:INFO: Dataset: zara2               Batch: 25/36	Loss 46.2282 (42.2296)
2022-11-11 22:49:44,359:INFO: Dataset: zara2               Batch: 26/36	Loss 39.5372 (42.1869)
2022-11-11 22:49:45,533:INFO: Dataset: zara2               Batch: 27/36	Loss 39.2392 (42.1338)
2022-11-11 22:49:46,726:INFO: Dataset: zara2               Batch: 28/36	Loss 42.6221 (42.1535)
2022-11-11 22:49:47,905:INFO: Dataset: zara2               Batch: 29/36	Loss 45.2637 (42.2649)
2022-11-11 22:49:49,042:INFO: Dataset: zara2               Batch: 30/36	Loss 44.3391 (42.3133)
2022-11-11 22:49:50,240:INFO: Dataset: zara2               Batch: 31/36	Loss 43.6317 (42.3475)
2022-11-11 22:49:51,377:INFO: Dataset: zara2               Batch: 32/36	Loss 47.5485 (42.4477)
2022-11-11 22:49:52,578:INFO: Dataset: zara2               Batch: 33/36	Loss 45.1950 (42.5034)
2022-11-11 22:49:53,724:INFO: Dataset: zara2               Batch: 34/36	Loss 47.9038 (42.5999)
2022-11-11 22:49:55,050:INFO: Dataset: zara2               Batch: 35/36	Loss 45.1965 (42.6429)
2022-11-11 22:49:56,297:INFO: Dataset: zara2               Batch: 36/36	Loss 30.7632 (42.5015)
2022-11-11 22:49:57,235:INFO: - Computing ADE (validation)
2022-11-11 22:50:06,091:INFO: 		 ADE on hotel                     dataset:	 0.9737710952758789
2022-11-11 22:50:15,413:INFO: 		 ADE on univ                      dataset:	 1.5143342018127441
2022-11-11 22:50:25,170:INFO: 		 ADE on zara1                     dataset:	 2.5589210987091064
2022-11-11 22:50:34,925:INFO: 		 ADE on zara2                     dataset:	 1.4368979930877686
2022-11-11 22:50:34,925:INFO: Average validation:	ADE  1.5171	FDE  2.7341
2022-11-11 22:50:34,926:INFO: - Computing ADE (training)
2022-11-11 22:50:44,101:INFO: 		 ADE on hotel                     dataset:	 1.313735842704773
2022-11-11 22:51:10,873:INFO: 		 ADE on univ                      dataset:	 1.3986254930496216
2022-11-11 22:51:20,957:INFO: 		 ADE on zara1                     dataset:	 2.4780282974243164
2022-11-11 22:51:46,739:INFO: 		 ADE on zara2                     dataset:	 1.705362319946289
2022-11-11 22:51:46,739:INFO: Average training:	ADE  1.5275	FDE  2.7631
2022-11-11 22:51:46,754:INFO:  --> Model Saved in ./models/E6//P3/CRMF_epoch_5.pth.tar
2022-11-11 22:51:46,754:INFO: 
===> EPOCH: 6 (P3)
2022-11-11 22:51:46,755:INFO: - Computing loss (training)
2022-11-11 22:51:55,093:INFO: Dataset: hotel               Batch: 1/8	Loss 42.9043 (42.9043)
2022-11-11 22:51:56,682:INFO: Dataset: hotel               Batch: 2/8	Loss 39.5218 (40.8839)
2022-11-11 22:51:58,161:INFO: Dataset: hotel               Batch: 3/8	Loss 41.7695 (41.1888)
2022-11-11 22:51:59,681:INFO: Dataset: hotel               Batch: 4/8	Loss 40.7135 (41.0886)
2022-11-11 22:52:01,158:INFO: Dataset: hotel               Batch: 5/8	Loss 40.2987 (40.9315)
2022-11-11 22:52:02,719:INFO: Dataset: hotel               Batch: 6/8	Loss 45.8891 (41.6948)
2022-11-11 22:52:04,267:INFO: Dataset: hotel               Batch: 7/8	Loss 45.4103 (42.1411)
2022-11-11 22:52:05,717:INFO: Dataset: hotel               Batch: 8/8	Loss 8.3635 (40.2695)
2022-11-11 22:52:32,077:INFO: Dataset: univ                Batch:  1/29	Loss 40.2675 (40.2675)
2022-11-11 22:52:33,634:INFO: Dataset: univ                Batch:  2/29	Loss 40.4670 (40.3610)
2022-11-11 22:52:35,065:INFO: Dataset: univ                Batch:  3/29	Loss 40.2905 (40.3440)
2022-11-11 22:52:36,544:INFO: Dataset: univ                Batch:  4/29	Loss 40.5306 (40.3802)
2022-11-11 22:52:38,069:INFO: Dataset: univ                Batch:  5/29	Loss 39.7187 (40.2474)
2022-11-11 22:52:39,541:INFO: Dataset: univ                Batch:  6/29	Loss 39.2883 (40.0735)
2022-11-11 22:52:41,006:INFO: Dataset: univ                Batch:  7/29	Loss 39.0766 (39.9267)
2022-11-11 22:52:42,527:INFO: Dataset: univ                Batch:  8/29	Loss 39.2707 (39.8468)
2022-11-11 22:52:44,114:INFO: Dataset: univ                Batch:  9/29	Loss 40.0473 (39.8739)
2022-11-11 22:52:45,612:INFO: Dataset: univ                Batch: 10/29	Loss 39.3589 (39.8284)
2022-11-11 22:52:47,088:INFO: Dataset: univ                Batch: 11/29	Loss 41.0197 (39.9147)
2022-11-11 22:52:48,568:INFO: Dataset: univ                Batch: 12/29	Loss 42.1024 (40.0411)
2022-11-11 22:52:50,083:INFO: Dataset: univ                Batch: 13/29	Loss 41.5813 (40.1192)
2022-11-11 22:52:51,589:INFO: Dataset: univ                Batch: 14/29	Loss 42.4354 (40.2655)
2022-11-11 22:52:53,078:INFO: Dataset: univ                Batch: 15/29	Loss 41.7387 (40.3275)
2022-11-11 22:52:54,549:INFO: Dataset: univ                Batch: 16/29	Loss 40.5830 (40.3389)
2022-11-11 22:52:56,061:INFO: Dataset: univ                Batch: 17/29	Loss 41.7001 (40.4070)
2022-11-11 22:52:57,519:INFO: Dataset: univ                Batch: 18/29	Loss 41.9474 (40.4760)
2022-11-11 22:52:58,988:INFO: Dataset: univ                Batch: 19/29	Loss 40.6591 (40.4850)
2022-11-11 22:53:00,438:INFO: Dataset: univ                Batch: 20/29	Loss 40.4515 (40.4839)
2022-11-11 22:53:01,983:INFO: Dataset: univ                Batch: 21/29	Loss 40.0724 (40.4698)
2022-11-11 22:53:03,559:INFO: Dataset: univ                Batch: 22/29	Loss 39.9828 (40.4568)
2022-11-11 22:53:05,064:INFO: Dataset: univ                Batch: 23/29	Loss 42.1229 (40.4963)
2022-11-11 22:53:06,690:INFO: Dataset: univ                Batch: 24/29	Loss 49.5222 (40.5890)
2022-11-11 22:53:08,231:INFO: Dataset: univ                Batch: 25/29	Loss 46.4574 (40.6200)
2022-11-11 22:53:09,767:INFO: Dataset: univ                Batch: 26/29	Loss 50.3220 (40.6525)
2022-11-11 22:53:11,271:INFO: Dataset: univ                Batch: 27/29	Loss 48.9317 (40.6848)
2022-11-11 22:53:12,906:INFO: Dataset: univ                Batch: 28/29	Loss 53.1370 (40.7368)
2022-11-11 22:53:14,419:INFO: Dataset: univ                Batch: 29/29	Loss 19.7607 (40.7129)
2022-11-11 22:53:25,823:INFO: Dataset: zara1               Batch:  1/16	Loss 43.7515 (43.7515)
2022-11-11 22:53:27,399:INFO: Dataset: zara1               Batch:  2/16	Loss 42.9022 (43.3427)
2022-11-11 22:53:28,862:INFO: Dataset: zara1               Batch:  3/16	Loss 42.7537 (43.1434)
2022-11-11 22:53:30,306:INFO: Dataset: zara1               Batch:  4/16	Loss 46.3835 (43.7591)
2022-11-11 22:53:31,794:INFO: Dataset: zara1               Batch:  5/16	Loss 42.3183 (43.5110)
2022-11-11 22:53:33,246:INFO: Dataset: zara1               Batch:  6/16	Loss 43.1701 (43.4617)
2022-11-11 22:53:34,716:INFO: Dataset: zara1               Batch:  7/16	Loss 47.6863 (43.9061)
2022-11-11 22:53:36,197:INFO: Dataset: zara1               Batch:  8/16	Loss 50.9093 (44.6303)
2022-11-11 22:53:37,671:INFO: Dataset: zara1               Batch:  9/16	Loss 48.9811 (45.1753)
2022-11-11 22:53:39,063:INFO: Dataset: zara1               Batch: 10/16	Loss 43.5513 (45.0753)
2022-11-11 22:53:40,518:INFO: Dataset: zara1               Batch: 11/16	Loss 46.3020 (45.1575)
2022-11-11 22:53:41,915:INFO: Dataset: zara1               Batch: 12/16	Loss 43.8366 (44.9348)
2022-11-11 22:53:43,371:INFO: Dataset: zara1               Batch: 13/16	Loss 44.7656 (44.9088)
2022-11-11 22:53:44,856:INFO: Dataset: zara1               Batch: 14/16	Loss 48.3131 (45.0564)
2022-11-11 22:53:46,786:INFO: Dataset: zara1               Batch: 15/16	Loss 49.9055 (45.3399)
2022-11-11 22:53:48,256:INFO: Dataset: zara1               Batch: 16/16	Loss 34.8269 (44.8585)
2022-11-11 22:54:13,771:INFO: Dataset: zara2               Batch:  1/36	Loss 47.9096 (47.9096)
2022-11-11 22:54:15,235:INFO: Dataset: zara2               Batch:  2/36	Loss 48.4097 (48.1628)
2022-11-11 22:54:16,717:INFO: Dataset: zara2               Batch:  3/36	Loss 47.7872 (47.9994)
2022-11-11 22:54:18,218:INFO: Dataset: zara2               Batch:  4/36	Loss 42.9619 (46.7652)
2022-11-11 22:54:19,669:INFO: Dataset: zara2               Batch:  5/36	Loss 45.0907 (46.4013)
2022-11-11 22:54:21,091:INFO: Dataset: zara2               Batch:  6/36	Loss 43.1905 (45.8280)
2022-11-11 22:54:22,567:INFO: Dataset: zara2               Batch:  7/36	Loss 42.7632 (45.5485)
2022-11-11 22:54:24,061:INFO: Dataset: zara2               Batch:  8/36	Loss 45.8112 (45.5823)
2022-11-11 22:54:25,519:INFO: Dataset: zara2               Batch:  9/36	Loss 43.0591 (45.4269)
2022-11-11 22:54:26,935:INFO: Dataset: zara2               Batch: 10/36	Loss 39.4666 (44.7693)
2022-11-11 22:54:28,366:INFO: Dataset: zara2               Batch: 11/36	Loss 41.7549 (44.3200)
2022-11-11 22:54:29,846:INFO: Dataset: zara2               Batch: 12/36	Loss 39.7693 (43.6026)
2022-11-11 22:54:31,305:INFO: Dataset: zara2               Batch: 13/36	Loss 39.3460 (43.0621)
2022-11-11 22:54:32,718:INFO: Dataset: zara2               Batch: 14/36	Loss 41.3881 (42.8889)
2022-11-11 22:54:34,324:INFO: Dataset: zara2               Batch: 15/36	Loss 40.5285 (42.6208)
2022-11-11 22:54:35,760:INFO: Dataset: zara2               Batch: 16/36	Loss 40.4066 (42.4355)
2022-11-11 22:54:37,284:INFO: Dataset: zara2               Batch: 17/36	Loss 39.9340 (42.2566)
2022-11-11 22:54:38,707:INFO: Dataset: zara2               Batch: 18/36	Loss 39.0288 (42.0888)
2022-11-11 22:54:40,252:INFO: Dataset: zara2               Batch: 19/36	Loss 40.3020 (41.9605)
2022-11-11 22:54:41,654:INFO: Dataset: zara2               Batch: 20/36	Loss 40.6151 (41.8587)
2022-11-11 22:54:43,153:INFO: Dataset: zara2               Batch: 21/36	Loss 41.8856 (41.8610)
2022-11-11 22:54:44,559:INFO: Dataset: zara2               Batch: 22/36	Loss 40.3922 (41.7863)
2022-11-11 22:54:46,007:INFO: Dataset: zara2               Batch: 23/36	Loss 46.8250 (41.9195)
2022-11-11 22:54:47,489:INFO: Dataset: zara2               Batch: 24/36	Loss 47.8237 (42.0866)
2022-11-11 22:54:48,951:INFO: Dataset: zara2               Batch: 25/36	Loss 46.2051 (42.1966)
2022-11-11 22:54:50,441:INFO: Dataset: zara2               Batch: 26/36	Loss 39.5141 (42.1541)
2022-11-11 22:54:51,880:INFO: Dataset: zara2               Batch: 27/36	Loss 39.2422 (42.1016)
2022-11-11 22:54:53,261:INFO: Dataset: zara2               Batch: 28/36	Loss 42.6343 (42.1231)
2022-11-11 22:54:54,663:INFO: Dataset: zara2               Batch: 29/36	Loss 45.2731 (42.2359)
2022-11-11 22:54:56,173:INFO: Dataset: zara2               Batch: 30/36	Loss 44.3384 (42.2849)
2022-11-11 22:54:57,626:INFO: Dataset: zara2               Batch: 31/36	Loss 43.6167 (42.3195)
2022-11-11 22:54:59,046:INFO: Dataset: zara2               Batch: 32/36	Loss 47.6652 (42.4225)
2022-11-11 22:55:00,501:INFO: Dataset: zara2               Batch: 33/36	Loss 45.1709 (42.4782)
2022-11-11 22:55:01,912:INFO: Dataset: zara2               Batch: 34/36	Loss 47.8839 (42.5748)
2022-11-11 22:55:03,375:INFO: Dataset: zara2               Batch: 35/36	Loss 45.2059 (42.6183)
2022-11-11 22:55:04,803:INFO: Dataset: zara2               Batch: 36/36	Loss 30.7369 (42.4769)
2022-11-11 22:55:05,766:INFO: - Computing ADE (validation)
2022-11-11 22:55:14,806:INFO: 		 ADE on hotel                     dataset:	 0.9578452110290527
2022-11-11 22:55:23,935:INFO: 		 ADE on univ                      dataset:	 1.5101951360702515
2022-11-11 22:55:32,658:INFO: 		 ADE on zara1                     dataset:	 2.5513594150543213
2022-11-11 22:55:42,121:INFO: 		 ADE on zara2                     dataset:	 1.4400235414505005
2022-11-11 22:55:42,121:INFO: Average validation:	ADE  1.5147	FDE  2.7318
2022-11-11 22:55:42,123:INFO: - Computing ADE (training)
2022-11-11 22:55:51,665:INFO: 		 ADE on hotel                     dataset:	 1.3226484060287476
2022-11-11 22:56:17,672:INFO: 		 ADE on univ                      dataset:	 1.3984297513961792
2022-11-11 22:56:27,341:INFO: 		 ADE on zara1                     dataset:	 2.478839874267578
2022-11-11 22:56:53,538:INFO: 		 ADE on zara2                     dataset:	 1.705090880393982
2022-11-11 22:56:53,538:INFO: Average training:	ADE  1.5276	FDE  2.7634
2022-11-11 22:56:53,555:INFO:  --> Model Saved in ./models/E6//P3/CRMF_epoch_6.pth.tar
2022-11-11 22:56:53,555:INFO: 
===> EPOCH: 7 (P4)
2022-11-11 22:56:53,556:INFO: - Computing loss (training)
2022-11-11 22:57:00,001:INFO: Dataset: hotel               Batch: 1/8	Loss 15.1120 (15.1120)
2022-11-11 22:57:00,417:INFO: Dataset: hotel               Batch: 2/8	Loss 14.5557 (14.7797)
2022-11-11 22:57:00,806:INFO: Dataset: hotel               Batch: 3/8	Loss 13.7772 (14.4347)
2022-11-11 22:57:01,174:INFO: Dataset: hotel               Batch: 4/8	Loss 12.8186 (14.0940)
2022-11-11 22:57:01,421:INFO: Dataset: hotel               Batch: 5/8	Loss 11.4837 (13.5749)
2022-11-11 22:57:01,594:INFO: Dataset: hotel               Batch: 6/8	Loss 8.2163 (12.7498)
2022-11-11 22:57:01,597:INFO: Dataset: hotel               Batch: 7/8	Loss 7.1708 (12.0797)
2022-11-11 22:57:01,601:INFO: Dataset: hotel               Batch: 8/8	Loss 6.8728 (11.7912)
2022-11-11 22:57:26,428:INFO: Dataset: univ                Batch:  1/29	Loss 14.5732 (14.5732)
2022-11-11 22:57:26,434:INFO: Dataset: univ                Batch:  2/29	Loss 14.6799 (14.6232)
2022-11-11 22:57:26,441:INFO: Dataset: univ                Batch:  3/29	Loss 13.5846 (14.3726)
2022-11-11 22:57:26,460:INFO: Dataset: univ                Batch:  4/29	Loss 12.4745 (14.0041)
2022-11-11 22:57:26,469:INFO: Dataset: univ                Batch:  5/29	Loss 10.6314 (13.3271)
2022-11-11 22:57:26,478:INFO: Dataset: univ                Batch:  6/29	Loss 9.2683 (12.5911)
2022-11-11 22:57:26,483:INFO: Dataset: univ                Batch:  7/29	Loss 7.9311 (11.9049)
2022-11-11 22:57:26,487:INFO: Dataset: univ                Batch:  8/29	Loss 7.0346 (11.3116)
2022-11-11 22:57:26,491:INFO: Dataset: univ                Batch:  9/29	Loss 6.4034 (10.6484)
2022-11-11 22:57:26,495:INFO: Dataset: univ                Batch: 10/29	Loss 5.8618 (10.2255)
2022-11-11 22:57:26,500:INFO: Dataset: univ                Batch: 11/29	Loss 5.4168 (9.8773)
2022-11-11 22:57:26,505:INFO: Dataset: univ                Batch: 12/29	Loss 5.0922 (9.6007)
2022-11-11 22:57:26,508:INFO: Dataset: univ                Batch: 13/29	Loss 4.6998 (9.3524)
2022-11-11 22:57:26,512:INFO: Dataset: univ                Batch: 14/29	Loss 4.3567 (9.0368)
2022-11-11 22:57:26,517:INFO: Dataset: univ                Batch: 15/29	Loss 4.0288 (8.8262)
2022-11-11 22:57:26,523:INFO: Dataset: univ                Batch: 16/29	Loss 3.7712 (8.5996)
2022-11-11 22:57:26,526:INFO: Dataset: univ                Batch: 17/29	Loss 3.2891 (8.3340)
2022-11-11 22:57:26,531:INFO: Dataset: univ                Batch: 18/29	Loss 2.8309 (8.0876)
2022-11-11 22:57:26,535:INFO: Dataset: univ                Batch: 19/29	Loss 2.5173 (7.8137)
2022-11-11 22:57:26,540:INFO: Dataset: univ                Batch: 20/29	Loss 2.3679 (7.6333)
2022-11-11 22:57:26,545:INFO: Dataset: univ                Batch: 21/29	Loss 2.1296 (7.4452)
2022-11-11 22:57:26,550:INFO: Dataset: univ                Batch: 22/29	Loss 1.8558 (7.2959)
2022-11-11 22:57:26,555:INFO: Dataset: univ                Batch: 23/29	Loss 1.5084 (7.1588)
2022-11-11 22:57:26,560:INFO: Dataset: univ                Batch: 24/29	Loss 1.9688 (7.1055)
2022-11-11 22:57:26,564:INFO: Dataset: univ                Batch: 25/29	Loss 1.4232 (7.0755)
2022-11-11 22:57:26,568:INFO: Dataset: univ                Batch: 26/29	Loss 1.7732 (7.0577)
2022-11-11 22:57:26,572:INFO: Dataset: univ                Batch: 27/29	Loss 0.9851 (7.0340)
2022-11-11 22:57:26,575:INFO: Dataset: univ                Batch: 28/29	Loss 0.6045 (7.0072)
2022-11-11 22:57:26,578:INFO: Dataset: univ                Batch: 29/29	Loss 0.3643 (6.9996)
2022-11-11 22:57:34,515:INFO: Dataset: zara1               Batch:  1/16	Loss 18.5432 (18.5432)
2022-11-11 22:57:34,976:INFO: Dataset: zara1               Batch:  2/16	Loss 16.9338 (17.7685)
2022-11-11 22:57:35,289:INFO: Dataset: zara1               Batch:  3/16	Loss 15.7392 (17.0821)
2022-11-11 22:57:35,606:INFO: Dataset: zara1               Batch:  4/16	Loss 13.6515 (16.4303)
2022-11-11 22:57:35,872:INFO: Dataset: zara1               Batch:  5/16	Loss 12.2220 (15.7056)
2022-11-11 22:57:36,052:INFO: Dataset: zara1               Batch:  6/16	Loss 10.9122 (15.0131)
2022-11-11 22:57:36,055:INFO: Dataset: zara1               Batch:  7/16	Loss 8.9363 (14.3738)
2022-11-11 22:57:36,062:INFO: Dataset: zara1               Batch:  8/16	Loss 6.8028 (13.5909)
2022-11-11 22:57:36,071:INFO: Dataset: zara1               Batch:  9/16	Loss 5.8363 (12.6197)
2022-11-11 22:57:36,075:INFO: Dataset: zara1               Batch: 10/16	Loss 5.3735 (12.1736)
2022-11-11 22:57:36,078:INFO: Dataset: zara1               Batch: 11/16	Loss 3.7321 (11.6079)
2022-11-11 22:57:36,088:INFO: Dataset: zara1               Batch: 12/16	Loss 3.5772 (10.2539)
2022-11-11 22:57:36,092:INFO: Dataset: zara1               Batch: 13/16	Loss 2.8828 (9.1209)
2022-11-11 22:57:36,095:INFO: Dataset: zara1               Batch: 14/16	Loss 4.5240 (8.9217)
2022-11-11 22:57:36,106:INFO: Dataset: zara1               Batch: 15/16	Loss 3.3424 (8.5955)
2022-11-11 22:57:36,110:INFO: Dataset: zara1               Batch: 16/16	Loss 1.6737 (8.2785)
2022-11-11 22:58:01,177:INFO: Dataset: zara2               Batch:  1/36	Loss 19.5281 (19.5281)
2022-11-11 22:58:01,182:INFO: Dataset: zara2               Batch:  2/36	Loss 15.7848 (17.6326)
2022-11-11 22:58:01,186:INFO: Dataset: zara2               Batch:  3/36	Loss 12.3469 (15.3328)
2022-11-11 22:58:01,190:INFO: Dataset: zara2               Batch:  4/36	Loss 9.4630 (13.8946)
2022-11-11 22:58:01,194:INFO: Dataset: zara2               Batch:  5/36	Loss 8.2501 (12.6679)
2022-11-11 22:58:01,200:INFO: Dataset: zara2               Batch:  6/36	Loss 7.0335 (11.6620)
2022-11-11 22:58:01,203:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5992 (11.2003)
2022-11-11 22:58:01,206:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0672 (10.5409)
2022-11-11 22:58:01,210:INFO: Dataset: zara2               Batch:  9/36	Loss 5.5804 (10.2354)
2022-11-11 22:58:01,215:INFO: Dataset: zara2               Batch: 10/36	Loss 5.4268 (9.7048)
2022-11-11 22:58:01,218:INFO: Dataset: zara2               Batch: 11/36	Loss 4.8120 (8.9756)
2022-11-11 22:58:01,222:INFO: Dataset: zara2               Batch: 12/36	Loss 4.4736 (8.2659)
2022-11-11 22:58:01,225:INFO: Dataset: zara2               Batch: 13/36	Loss 4.2495 (7.7558)
2022-11-11 22:58:01,229:INFO: Dataset: zara2               Batch: 14/36	Loss 3.5484 (7.3205)
2022-11-11 22:58:01,234:INFO: Dataset: zara2               Batch: 15/36	Loss 3.1345 (6.8451)
2022-11-11 22:58:01,237:INFO: Dataset: zara2               Batch: 16/36	Loss 2.8919 (6.5142)
2022-11-11 22:58:01,240:INFO: Dataset: zara2               Batch: 17/36	Loss 2.4464 (6.2234)
2022-11-11 22:58:01,243:INFO: Dataset: zara2               Batch: 18/36	Loss 2.1866 (6.0135)
2022-11-11 22:58:01,248:INFO: Dataset: zara2               Batch: 19/36	Loss 1.7910 (5.7102)
2022-11-11 22:58:01,254:INFO: Dataset: zara2               Batch: 20/36	Loss 1.5875 (5.3982)
2022-11-11 22:58:01,258:INFO: Dataset: zara2               Batch: 21/36	Loss 1.2113 (5.0348)
2022-11-11 22:58:01,264:INFO: Dataset: zara2               Batch: 22/36	Loss 1.0172 (4.8304)
2022-11-11 22:58:01,268:INFO: Dataset: zara2               Batch: 23/36	Loss 1.5166 (4.7428)
2022-11-11 22:58:01,273:INFO: Dataset: zara2               Batch: 24/36	Loss 1.1816 (4.6420)
2022-11-11 22:58:01,275:INFO: Dataset: zara2               Batch: 25/36	Loss 1.1985 (4.5501)
2022-11-11 22:58:01,281:INFO: Dataset: zara2               Batch: 26/36	Loss 0.6121 (4.4877)
2022-11-11 22:58:01,285:INFO: Dataset: zara2               Batch: 27/36	Loss 0.3346 (4.4128)
2022-11-11 22:58:01,290:INFO: Dataset: zara2               Batch: 28/36	Loss 0.4058 (4.2514)
2022-11-11 22:58:01,293:INFO: Dataset: zara2               Batch: 29/36	Loss 0.2876 (4.1094)
2022-11-11 22:58:01,298:INFO: Dataset: zara2               Batch: 30/36	Loss 0.1729 (4.0176)
2022-11-11 22:58:01,303:INFO: Dataset: zara2               Batch: 31/36	Loss 0.4257 (3.9244)
2022-11-11 22:58:01,309:INFO: Dataset: zara2               Batch: 32/36	Loss 0.1837 (3.8523)
2022-11-11 22:58:01,312:INFO: Dataset: zara2               Batch: 33/36	Loss 0.1891 (3.7780)
2022-11-11 22:58:01,317:INFO: Dataset: zara2               Batch: 34/36	Loss 0.2085 (3.7143)
2022-11-11 22:58:01,323:INFO: Dataset: zara2               Batch: 35/36	Loss 0.3130 (3.6579)
2022-11-11 22:58:01,326:INFO: Dataset: zara2               Batch: 36/36	Loss 0.2902 (3.6178)
2022-11-11 22:58:02,304:INFO: - Computing loss (validation)
2022-11-11 22:58:08,672:INFO: Dataset: hotel               Batch: 1/3	Loss 31.9682 (31.9682)
2022-11-11 22:58:09,010:INFO: Dataset: hotel               Batch: 2/3	Loss 29.5436 (30.5928)
2022-11-11 22:58:09,391:INFO: Dataset: hotel               Batch: 3/3	Loss 28.8957 (30.4885)
2022-11-11 22:58:16,955:INFO: Dataset: univ                Batch: 1/6	Loss 15.8844 (15.8844)
2022-11-11 22:58:17,372:INFO: Dataset: univ                Batch: 2/6	Loss 15.7753 (15.8419)
2022-11-11 22:58:17,742:INFO: Dataset: univ                Batch: 3/6	Loss 16.1134 (15.8780)
2022-11-11 22:58:18,038:INFO: Dataset: univ                Batch: 4/6	Loss 15.2402 (15.8090)
2022-11-11 22:58:18,275:INFO: Dataset: univ                Batch: 5/6	Loss 16.8441 (15.9581)
2022-11-11 22:58:18,470:INFO: Dataset: univ                Batch: 6/6	Loss 15.0352 (15.9375)
2022-11-11 22:58:25,522:INFO: Dataset: zara1               Batch: 1/3	Loss 15.9029 (15.9029)
2022-11-11 22:58:25,918:INFO: Dataset: zara1               Batch: 2/3	Loss 15.1706 (15.6146)
2022-11-11 22:58:26,225:INFO: Dataset: zara1               Batch: 3/3	Loss 16.0073 (15.7282)
2022-11-11 22:58:33,861:INFO: Dataset: zara2               Batch:  1/10	Loss 0.1379 (0.1379)
2022-11-11 22:58:34,273:INFO: Dataset: zara2               Batch:  2/10	Loss 0.0704 (0.1034)
2022-11-11 22:58:34,647:INFO: Dataset: zara2               Batch:  3/10	Loss 0.0854 (0.0963)
2022-11-11 22:58:34,926:INFO: Dataset: zara2               Batch:  4/10	Loss 0.0524 (0.0869)
2022-11-11 22:58:35,199:INFO: Dataset: zara2               Batch:  5/10	Loss 0.1573 (0.0966)
2022-11-11 22:58:35,450:INFO: Dataset: zara2               Batch:  6/10	Loss 0.2401 (0.1082)
2022-11-11 22:58:35,452:INFO: Dataset: zara2               Batch:  7/10	Loss 0.1133 (0.1087)
2022-11-11 22:58:35,456:INFO: Dataset: zara2               Batch:  8/10	Loss 0.1383 (0.1122)
2022-11-11 22:58:35,464:INFO: Dataset: zara2               Batch:  9/10	Loss 0.1200 (0.1130)
2022-11-11 22:58:35,466:INFO: Dataset: zara2               Batch: 10/10	Loss 0.1767 (0.1185)
2022-11-11 22:58:36,388:INFO: - Computing ADE (validation o)
2022-11-11 22:58:44,185:INFO: 		 ADE on eth                       dataset:	 3.467067241668701
2022-11-11 22:58:44,185:INFO: Average validation o:	ADE  3.4671	FDE  5.8624
2022-11-11 22:58:44,204:INFO:  --> Model Saved in ./models/E6//P4/CRMF_epoch_7.pth.tar
2022-11-11 22:58:44,204:INFO: 
===> EPOCH: 8 (P4)
2022-11-11 22:58:44,205:INFO: - Computing loss (training)
2022-11-11 22:58:50,483:INFO: Dataset: hotel               Batch: 1/8	Loss 33.5134 (33.5134)
2022-11-11 22:58:50,861:INFO: Dataset: hotel               Batch: 2/8	Loss 27.7783 (30.0879)
2022-11-11 22:58:51,178:INFO: Dataset: hotel               Batch: 3/8	Loss 26.0683 (28.7043)
2022-11-11 22:58:51,471:INFO: Dataset: hotel               Batch: 4/8	Loss 23.7124 (27.6521)
2022-11-11 22:58:51,669:INFO: Dataset: hotel               Batch: 5/8	Loss 21.1022 (26.3495)
2022-11-11 22:58:51,895:INFO: Dataset: hotel               Batch: 6/8	Loss 17.2959 (24.9556)
2022-11-11 22:58:51,901:INFO: Dataset: hotel               Batch: 7/8	Loss 15.7719 (23.8525)
2022-11-11 22:58:51,906:INFO: Dataset: hotel               Batch: 8/8	Loss 15.6220 (23.3965)
2022-11-11 22:59:15,200:INFO: Dataset: univ                Batch:  1/29	Loss 8.2491 (8.2491)
2022-11-11 22:59:15,205:INFO: Dataset: univ                Batch:  2/29	Loss 7.7262 (8.0040)
2022-11-11 22:59:15,213:INFO: Dataset: univ                Batch:  3/29	Loss 7.0711 (7.7790)
2022-11-11 22:59:15,220:INFO: Dataset: univ                Batch:  4/29	Loss 6.5315 (7.5367)
2022-11-11 22:59:15,226:INFO: Dataset: univ                Batch:  5/29	Loss 6.2918 (7.2868)
2022-11-11 22:59:15,234:INFO: Dataset: univ                Batch:  6/29	Loss 5.8720 (7.0303)
2022-11-11 22:59:15,239:INFO: Dataset: univ                Batch:  7/29	Loss 5.4686 (6.8003)
2022-11-11 22:59:15,243:INFO: Dataset: univ                Batch:  8/29	Loss 5.0940 (6.5925)
2022-11-11 22:59:15,247:INFO: Dataset: univ                Batch:  9/29	Loss 4.8622 (6.3587)
2022-11-11 22:59:15,254:INFO: Dataset: univ                Batch: 10/29	Loss 4.6101 (6.2042)
2022-11-11 22:59:15,259:INFO: Dataset: univ                Batch: 11/29	Loss 4.3475 (6.0697)
2022-11-11 22:59:15,265:INFO: Dataset: univ                Batch: 12/29	Loss 3.9384 (5.9465)
2022-11-11 22:59:15,269:INFO: Dataset: univ                Batch: 13/29	Loss 3.8073 (5.8382)
2022-11-11 22:59:15,276:INFO: Dataset: univ                Batch: 14/29	Loss 3.1944 (5.6711)
2022-11-11 22:59:15,279:INFO: Dataset: univ                Batch: 15/29	Loss 2.8134 (5.5509)
2022-11-11 22:59:15,284:INFO: Dataset: univ                Batch: 16/29	Loss 2.4736 (5.4130)
2022-11-11 22:59:15,289:INFO: Dataset: univ                Batch: 17/29	Loss 2.1697 (5.2508)
2022-11-11 22:59:15,294:INFO: Dataset: univ                Batch: 18/29	Loss 1.8277 (5.0975)
2022-11-11 22:59:15,298:INFO: Dataset: univ                Batch: 19/29	Loss 1.6057 (4.9258)
2022-11-11 22:59:15,302:INFO: Dataset: univ                Batch: 20/29	Loss 1.3308 (4.8067)
2022-11-11 22:59:15,306:INFO: Dataset: univ                Batch: 21/29	Loss 1.1489 (4.6817)
2022-11-11 22:59:15,311:INFO: Dataset: univ                Batch: 22/29	Loss 0.9636 (4.5824)
2022-11-11 22:59:15,315:INFO: Dataset: univ                Batch: 23/29	Loss 0.7916 (4.4926)
2022-11-11 22:59:15,319:INFO: Dataset: univ                Batch: 24/29	Loss 0.6864 (4.4535)
2022-11-11 22:59:15,322:INFO: Dataset: univ                Batch: 25/29	Loss 0.6201 (4.4333)
2022-11-11 22:59:15,325:INFO: Dataset: univ                Batch: 26/29	Loss 0.5325 (4.4202)
2022-11-11 22:59:15,330:INFO: Dataset: univ                Batch: 27/29	Loss 0.8059 (4.4061)
2022-11-11 22:59:15,334:INFO: Dataset: univ                Batch: 28/29	Loss 0.4790 (4.3897)
2022-11-11 22:59:15,337:INFO: Dataset: univ                Batch: 29/29	Loss 0.4094 (4.3852)
2022-11-11 22:59:22,248:INFO: Dataset: zara1               Batch:  1/16	Loss 20.4686 (20.4686)
2022-11-11 22:59:22,633:INFO: Dataset: zara1               Batch:  2/16	Loss 19.3099 (19.9108)
2022-11-11 22:59:22,988:INFO: Dataset: zara1               Batch:  3/16	Loss 16.5153 (18.7622)
2022-11-11 22:59:23,289:INFO: Dataset: zara1               Batch:  4/16	Loss 15.0655 (18.0598)
2022-11-11 22:59:23,563:INFO: Dataset: zara1               Batch:  5/16	Loss 13.5296 (17.2798)
2022-11-11 22:59:23,729:INFO: Dataset: zara1               Batch:  6/16	Loss 12.3649 (16.5697)
2022-11-11 22:59:23,732:INFO: Dataset: zara1               Batch:  7/16	Loss 11.0166 (15.9856)
2022-11-11 22:59:23,737:INFO: Dataset: zara1               Batch:  8/16	Loss 9.9683 (15.3633)
2022-11-11 22:59:23,741:INFO: Dataset: zara1               Batch:  9/16	Loss 8.9988 (14.5662)
2022-11-11 22:59:23,744:INFO: Dataset: zara1               Batch: 10/16	Loss 9.2029 (14.2360)
2022-11-11 22:59:23,747:INFO: Dataset: zara1               Batch: 11/16	Loss 6.4280 (13.7127)
2022-11-11 22:59:23,752:INFO: Dataset: zara1               Batch: 12/16	Loss 6.1818 (12.4430)
2022-11-11 22:59:23,758:INFO: Dataset: zara1               Batch: 13/16	Loss 4.5877 (11.2356)
2022-11-11 22:59:23,762:INFO: Dataset: zara1               Batch: 14/16	Loss 2.7533 (10.8679)
2022-11-11 22:59:23,766:INFO: Dataset: zara1               Batch: 15/16	Loss 2.9082 (10.4025)
2022-11-11 22:59:23,769:INFO: Dataset: zara1               Batch: 16/16	Loss 2.4214 (10.0371)
2022-11-11 22:59:46,722:INFO: Dataset: zara2               Batch:  1/36	Loss 21.4496 (21.4496)
2022-11-11 22:59:46,726:INFO: Dataset: zara2               Batch:  2/36	Loss 18.6663 (20.0402)
2022-11-11 22:59:46,731:INFO: Dataset: zara2               Batch:  3/36	Loss 14.5269 (17.6414)
2022-11-11 22:59:46,737:INFO: Dataset: zara2               Batch:  4/36	Loss 10.9946 (16.0129)
2022-11-11 22:59:46,741:INFO: Dataset: zara2               Batch:  5/36	Loss 9.5023 (14.5979)
2022-11-11 22:59:46,767:INFO: Dataset: zara2               Batch:  6/36	Loss 7.5736 (13.3439)
2022-11-11 22:59:46,771:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5713 (12.7262)
2022-11-11 22:59:46,774:INFO: Dataset: zara2               Batch:  8/36	Loss 5.3320 (11.7763)
2022-11-11 22:59:46,779:INFO: Dataset: zara2               Batch:  9/36	Loss 5.0187 (11.3602)
2022-11-11 22:59:46,782:INFO: Dataset: zara2               Batch: 10/36	Loss 5.2027 (10.6808)
2022-11-11 22:59:46,787:INFO: Dataset: zara2               Batch: 11/36	Loss 4.5294 (9.7639)
2022-11-11 22:59:46,791:INFO: Dataset: zara2               Batch: 12/36	Loss 4.4114 (8.9202)
2022-11-11 22:59:46,794:INFO: Dataset: zara2               Batch: 13/36	Loss 4.2922 (8.3325)
2022-11-11 22:59:46,798:INFO: Dataset: zara2               Batch: 14/36	Loss 3.7649 (7.8599)
2022-11-11 22:59:46,803:INFO: Dataset: zara2               Batch: 15/36	Loss 3.4425 (7.3582)
2022-11-11 22:59:46,806:INFO: Dataset: zara2               Batch: 16/36	Loss 3.3984 (7.0268)
2022-11-11 22:59:46,809:INFO: Dataset: zara2               Batch: 17/36	Loss 3.0628 (6.7434)
2022-11-11 22:59:46,813:INFO: Dataset: zara2               Batch: 18/36	Loss 2.7778 (6.5372)
2022-11-11 22:59:46,817:INFO: Dataset: zara2               Batch: 19/36	Loss 2.3257 (6.2347)
2022-11-11 22:59:46,820:INFO: Dataset: zara2               Batch: 20/36	Loss 1.9622 (5.9114)
2022-11-11 22:59:46,824:INFO: Dataset: zara2               Batch: 21/36	Loss 1.5419 (5.5321)
2022-11-11 22:59:46,829:INFO: Dataset: zara2               Batch: 22/36	Loss 1.3377 (5.3187)
2022-11-11 22:59:46,833:INFO: Dataset: zara2               Batch: 23/36	Loss 1.2552 (5.2113)
2022-11-11 22:59:46,837:INFO: Dataset: zara2               Batch: 24/36	Loss 1.0334 (5.0930)
2022-11-11 22:59:46,841:INFO: Dataset: zara2               Batch: 25/36	Loss 1.1940 (4.9889)
2022-11-11 22:59:46,845:INFO: Dataset: zara2               Batch: 26/36	Loss 0.7485 (4.9217)
2022-11-11 22:59:46,849:INFO: Dataset: zara2               Batch: 27/36	Loss 0.4729 (4.8416)
2022-11-11 22:59:46,855:INFO: Dataset: zara2               Batch: 28/36	Loss 0.3459 (4.6604)
2022-11-11 22:59:46,861:INFO: Dataset: zara2               Batch: 29/36	Loss 0.2486 (4.5024)
2022-11-11 22:59:46,865:INFO: Dataset: zara2               Batch: 30/36	Loss 0.3498 (4.4056)
2022-11-11 22:59:46,869:INFO: Dataset: zara2               Batch: 31/36	Loss 0.1431 (4.2949)
2022-11-11 22:59:46,873:INFO: Dataset: zara2               Batch: 32/36	Loss 0.1352 (4.2147)
2022-11-11 22:59:46,877:INFO: Dataset: zara2               Batch: 33/36	Loss 0.0752 (4.1309)
2022-11-11 22:59:46,880:INFO: Dataset: zara2               Batch: 34/36	Loss 0.1053 (4.0589)
2022-11-11 22:59:46,884:INFO: Dataset: zara2               Batch: 35/36	Loss 0.1497 (3.9942)
2022-11-11 22:59:46,887:INFO: Dataset: zara2               Batch: 36/36	Loss 0.1829 (3.9488)
2022-11-11 22:59:47,788:INFO: - Computing loss (validation)
2022-11-11 22:59:53,811:INFO: Dataset: hotel               Batch: 1/3	Loss 31.0764 (31.0764)
2022-11-11 22:59:54,261:INFO: Dataset: hotel               Batch: 2/3	Loss 30.0341 (30.4851)
2022-11-11 22:59:54,579:INFO: Dataset: hotel               Batch: 3/3	Loss 29.5274 (30.4263)
2022-11-11 23:00:02,386:INFO: Dataset: univ                Batch: 1/6	Loss 15.9048 (15.9048)
2022-11-11 23:00:02,803:INFO: Dataset: univ                Batch: 2/6	Loss 15.6994 (15.8247)
2022-11-11 23:00:03,099:INFO: Dataset: univ                Batch: 3/6	Loss 15.7248 (15.8114)
2022-11-11 23:00:03,393:INFO: Dataset: univ                Batch: 4/6	Loss 15.8216 (15.8125)
2022-11-11 23:00:03,635:INFO: Dataset: univ                Batch: 5/6	Loss 15.5520 (15.7750)
2022-11-11 23:00:03,848:INFO: Dataset: univ                Batch: 6/6	Loss 15.5242 (15.7694)
2022-11-11 23:00:10,842:INFO: Dataset: zara1               Batch: 1/3	Loss 15.9024 (15.9024)
2022-11-11 23:00:11,179:INFO: Dataset: zara1               Batch: 2/3	Loss 16.1008 (15.9805)
2022-11-11 23:00:11,559:INFO: Dataset: zara1               Batch: 3/3	Loss 15.8904 (15.9544)
2022-11-11 23:00:20,431:INFO: Dataset: zara2               Batch:  1/10	Loss 0.1013 (0.1013)
2022-11-11 23:00:20,781:INFO: Dataset: zara2               Batch:  2/10	Loss 0.0701 (0.0854)
2022-11-11 23:00:21,104:INFO: Dataset: zara2               Batch:  3/10	Loss 0.0764 (0.0818)
2022-11-11 23:00:21,417:INFO: Dataset: zara2               Batch:  4/10	Loss 0.0906 (0.0837)
2022-11-11 23:00:21,718:INFO: Dataset: zara2               Batch:  5/10	Loss 0.0909 (0.0847)
2022-11-11 23:00:21,912:INFO: Dataset: zara2               Batch:  6/10	Loss 0.0876 (0.0850)
2022-11-11 23:00:21,914:INFO: Dataset: zara2               Batch:  7/10	Loss 0.0828 (0.0847)
2022-11-11 23:00:21,916:INFO: Dataset: zara2               Batch:  8/10	Loss 0.0836 (0.0846)
2022-11-11 23:00:21,918:INFO: Dataset: zara2               Batch:  9/10	Loss 0.0749 (0.0836)
2022-11-11 23:00:21,920:INFO: Dataset: zara2               Batch: 10/10	Loss 0.1226 (0.0870)
2022-11-11 23:00:22,859:INFO: - Computing ADE (validation o)
2022-11-11 23:00:30,942:INFO: 		 ADE on eth                       dataset:	 3.468209981918335
2022-11-11 23:00:30,942:INFO: Average validation o:	ADE  3.4682	FDE  5.8714
2022-11-11 23:00:30,958:INFO:  --> Model Saved in ./models/E6//P4/CRMF_epoch_8.pth.tar
2022-11-11 23:00:30,958:INFO: 
===> EPOCH: 9 (P5)

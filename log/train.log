2022-10-29 20:25:28,946:INFO: Initializing Training Set
2022-10-29 20:25:30,734:INFO: Initializing Validation Set
2022-10-29 20:25:30,959:INFO: Initializing Validation O Set
2022-10-29 20:25:32,790:INFO: => loaded checkpoint './models/eth/pretrain/P3/5.0/SSE_data_eth_irm[5.0]_epoch_365.pth.tar' (epoch 366)
2022-10-29 20:25:32,790:INFO: 
===> EPOCH: 366 (P3)
2022-10-29 20:25:32,791:INFO: - Computing loss (training)
2022-10-29 20:25:34,510:INFO: Dataset: hotel               Batch: 1/4	Loss 5.4933 (5.4933)
2022-10-29 20:25:36,141:INFO: Dataset: hotel               Batch: 2/4	Loss 6.7426 (6.0846)
2022-10-29 20:25:37,801:INFO: Dataset: hotel               Batch: 3/4	Loss 6.3849 (6.1860)
2022-10-29 20:25:38,924:INFO: Dataset: hotel               Batch: 4/4	Loss 3.1760 (5.6459)
2022-10-29 20:25:41,218:INFO: Dataset: univ                Batch:  1/15	Loss 2.6664 (2.6664)
2022-10-29 20:25:43,050:INFO: Dataset: univ                Batch:  2/15	Loss 3.0347 (2.8583)
2022-10-29 20:25:44,940:INFO: Dataset: univ                Batch:  3/15	Loss 2.8337 (2.8512)
2022-10-29 20:25:46,730:INFO: Dataset: univ                Batch:  4/15	Loss 2.9803 (2.8821)
2022-10-29 20:25:48,545:INFO: Dataset: univ                Batch:  5/15	Loss 2.4698 (2.7966)
2022-10-29 20:25:50,354:INFO: Dataset: univ                Batch:  6/15	Loss 2.6667 (2.7744)
2022-10-29 20:25:52,116:INFO: Dataset: univ                Batch:  7/15	Loss 2.6443 (2.7573)
2022-10-29 20:25:53,935:INFO: Dataset: univ                Batch:  8/15	Loss 2.3848 (2.7098)
2022-10-29 20:25:55,766:INFO: Dataset: univ                Batch:  9/15	Loss 2.7838 (2.7181)
2022-10-29 20:25:57,585:INFO: Dataset: univ                Batch: 10/15	Loss 2.6237 (2.7088)
2022-10-29 20:25:59,515:INFO: Dataset: univ                Batch: 11/15	Loss 2.9308 (2.7275)
2022-10-29 20:26:01,334:INFO: Dataset: univ                Batch: 12/15	Loss 3.0999 (2.7595)
2022-10-29 20:26:03,144:INFO: Dataset: univ                Batch: 13/15	Loss 2.4372 (2.7349)
2022-10-29 20:26:05,001:INFO: Dataset: univ                Batch: 14/15	Loss 2.3302 (2.7058)
2022-10-29 20:26:05,545:INFO: Dataset: univ                Batch: 15/15	Loss 0.4260 (2.6723)
2022-10-29 20:26:07,570:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2812 (1.2812)

2022-11-11 21:55:07,863:INFO: Initializing Training Set
2022-11-11 21:55:11,955:INFO: Initializing Validation Set
2022-11-11 21:55:12,560:INFO: Initializing Validation O Set
2022-11-11 21:55:15,856:INFO: 
===> EPOCH: 5 (P3)
2022-11-11 21:55:15,856:INFO: - Computing loss (training)
2022-11-11 21:55:25,543:INFO: Dataset: hotel               Batch: 1/8	Loss 43.4755 (43.4755)
2022-11-11 21:55:26,734:INFO: Dataset: hotel               Batch: 2/8	Loss 39.4959 (41.0986)
2022-11-11 21:55:27,872:INFO: Dataset: hotel               Batch: 3/8	Loss 41.6936 (41.3034)
2022-11-11 21:55:28,996:INFO: Dataset: hotel               Batch: 4/8	Loss 40.6741 (41.1707)
2022-11-11 21:55:30,250:INFO: Dataset: hotel               Batch: 5/8	Loss 40.2449 (40.9866)
2022-11-11 21:55:31,357:INFO: Dataset: hotel               Batch: 6/8	Loss 45.8485 (41.7352)
2022-11-11 21:55:32,471:INFO: Dataset: hotel               Batch: 7/8	Loss 45.3005 (42.1634)
2022-11-11 21:55:33,507:INFO: Dataset: hotel               Batch: 8/8	Loss 8.3530 (40.2900)
2022-11-11 21:55:58,539:INFO: Dataset: univ                Batch:  1/29	Loss 40.5187 (40.5187)
2022-11-11 21:55:59,804:INFO: Dataset: univ                Batch:  2/29	Loss 40.5990 (40.5564)
2022-11-11 21:56:00,987:INFO: Dataset: univ                Batch:  3/29	Loss 40.3826 (40.5144)
2022-11-11 21:56:02,230:INFO: Dataset: univ                Batch:  4/29	Loss 40.5639 (40.5240)
2022-11-11 21:56:03,396:INFO: Dataset: univ                Batch:  5/29	Loss 39.7693 (40.3725)
2022-11-11 21:56:04,538:INFO: Dataset: univ                Batch:  6/29	Loss 39.3274 (40.1830)
2022-11-11 21:56:05,745:INFO: Dataset: univ                Batch:  7/29	Loss 39.1223 (40.0268)
2022-11-11 21:56:07,335:INFO: Dataset: univ                Batch:  8/29	Loss 39.3163 (39.9403)
2022-11-11 21:56:08,643:INFO: Dataset: univ                Batch:  9/29	Loss 40.0757 (39.9586)
2022-11-11 21:56:09,817:INFO: Dataset: univ                Batch: 10/29	Loss 39.3939 (39.9087)
2022-11-11 21:56:10,947:INFO: Dataset: univ                Batch: 11/29	Loss 41.0504 (39.9913)
2022-11-11 21:56:12,143:INFO: Dataset: univ                Batch: 12/29	Loss 42.1492 (40.1161)
2022-11-11 21:56:13,283:INFO: Dataset: univ                Batch: 13/29	Loss 41.6121 (40.1919)
2022-11-11 21:56:14,383:INFO: Dataset: univ                Batch: 14/29	Loss 42.4762 (40.3362)
2022-11-11 21:56:15,787:INFO: Dataset: univ                Batch: 15/29	Loss 41.7641 (40.3963)
2022-11-11 21:56:17,317:INFO: Dataset: univ                Batch: 16/29	Loss 40.6058 (40.4056)
2022-11-11 21:56:19,118:INFO: Dataset: univ                Batch: 17/29	Loss 41.7166 (40.4712)
2022-11-11 21:56:20,899:INFO: Dataset: univ                Batch: 18/29	Loss 41.9637 (40.5380)
2022-11-11 21:56:22,100:INFO: Dataset: univ                Batch: 19/29	Loss 40.6814 (40.5451)
2022-11-11 21:56:23,288:INFO: Dataset: univ                Batch: 20/29	Loss 40.4730 (40.5427)
2022-11-11 21:56:24,523:INFO: Dataset: univ                Batch: 21/29	Loss 40.0958 (40.5274)
2022-11-11 21:56:25,793:INFO: Dataset: univ                Batch: 22/29	Loss 40.0180 (40.5138)
2022-11-11 21:56:27,225:INFO: Dataset: univ                Batch: 23/29	Loss 42.1363 (40.5523)
2022-11-11 21:56:28,753:INFO: Dataset: univ                Batch: 24/29	Loss 49.3645 (40.6428)
2022-11-11 21:56:30,242:INFO: Dataset: univ                Batch: 25/29	Loss 46.4986 (40.6737)
2022-11-11 21:56:31,710:INFO: Dataset: univ                Batch: 26/29	Loss 50.3360 (40.7061)
2022-11-11 21:56:33,183:INFO: Dataset: univ                Batch: 27/29	Loss 48.9232 (40.7382)
2022-11-11 21:56:34,701:INFO: Dataset: univ                Batch: 28/29	Loss 53.2389 (40.7904)
2022-11-11 21:56:36,118:INFO: Dataset: univ                Batch: 29/29	Loss 19.8236 (40.7665)
2022-11-11 21:56:45,558:INFO: Dataset: zara1               Batch:  1/16	Loss 45.2559 (45.2559)
2022-11-11 21:56:47,050:INFO: Dataset: zara1               Batch:  2/16	Loss 43.1548 (44.2446)
2022-11-11 21:56:48,599:INFO: Dataset: zara1               Batch:  3/16	Loss 42.8262 (43.7648)
2022-11-11 21:56:50,096:INFO: Dataset: zara1               Batch:  4/16	Loss 46.4152 (44.2683)
2022-11-11 21:56:51,721:INFO: Dataset: zara1               Batch:  5/16	Loss 42.4251 (43.9510)
2022-11-11 21:56:53,161:INFO: Dataset: zara1               Batch:  6/16	Loss 43.3029 (43.8573)
2022-11-11 21:56:54,586:INFO: Dataset: zara1               Batch:  7/16	Loss 47.8376 (44.2760)
2022-11-11 21:56:56,155:INFO: Dataset: zara1               Batch:  8/16	Loss 51.0067 (44.9721)
2022-11-11 21:56:57,704:INFO: Dataset: zara1               Batch:  9/16	Loss 49.0275 (45.4800)
2022-11-11 21:56:59,249:INFO: Dataset: zara1               Batch: 10/16	Loss 43.6362 (45.3665)
2022-11-11 21:57:00,772:INFO: Dataset: zara1               Batch: 11/16	Loss 46.2859 (45.4281)
2022-11-11 21:57:02,310:INFO: Dataset: zara1               Batch: 12/16	Loss 43.8732 (45.1660)
2022-11-11 21:57:03,882:INFO: Dataset: zara1               Batch: 13/16	Loss 44.7951 (45.1089)
2022-11-11 21:57:05,342:INFO: Dataset: zara1               Batch: 14/16	Loss 48.3042 (45.2475)
2022-11-11 21:57:06,845:INFO: Dataset: zara1               Batch: 15/16	Loss 49.8488 (45.5165)
2022-11-11 21:57:08,367:INFO: Dataset: zara1               Batch: 16/16	Loss 34.7893 (45.0253)
2022-11-11 21:57:36,188:INFO: Dataset: zara2               Batch:  1/36	Loss 48.5218 (48.5218)
2022-11-11 21:57:37,427:INFO: Dataset: zara2               Batch:  2/36	Loss 48.7659 (48.6454)
2022-11-11 21:57:38,738:INFO: Dataset: zara2               Batch:  3/36	Loss 47.8900 (48.3167)
2022-11-11 21:57:40,203:INFO: Dataset: zara2               Batch:  4/36	Loss 43.0066 (47.0157)
2022-11-11 21:57:41,748:INFO: Dataset: zara2               Batch:  5/36	Loss 45.2038 (46.6219)
2022-11-11 21:57:43,371:INFO: Dataset: zara2               Batch:  6/36	Loss 43.2445 (46.0190)
2022-11-11 21:57:44,839:INFO: Dataset: zara2               Batch:  7/36	Loss 42.7512 (45.7209)
2022-11-11 21:57:46,441:INFO: Dataset: zara2               Batch:  8/36	Loss 45.7974 (45.7308)
2022-11-11 21:57:48,057:INFO: Dataset: zara2               Batch:  9/36	Loss 43.0798 (45.5675)
2022-11-11 21:57:49,645:INFO: Dataset: zara2               Batch: 10/36	Loss 39.4900 (44.8969)
2022-11-11 21:57:51,259:INFO: Dataset: zara2               Batch: 11/36	Loss 41.7552 (44.4287)
2022-11-11 21:57:52,690:INFO: Dataset: zara2               Batch: 12/36	Loss 39.7809 (43.6960)
2022-11-11 21:57:54,109:INFO: Dataset: zara2               Batch: 13/36	Loss 39.3595 (43.1453)
2022-11-11 21:57:55,558:INFO: Dataset: zara2               Batch: 14/36	Loss 41.4032 (42.9651)
2022-11-11 21:57:57,261:INFO: Dataset: zara2               Batch: 15/36	Loss 40.5438 (42.6901)
2022-11-11 21:57:58,869:INFO: Dataset: zara2               Batch: 16/36	Loss 40.4211 (42.5002)
2022-11-11 21:58:00,383:INFO: Dataset: zara2               Batch: 17/36	Loss 39.9186 (42.3156)
2022-11-11 21:58:01,891:INFO: Dataset: zara2               Batch: 18/36	Loss 39.0310 (42.1448)
2022-11-11 21:58:03,369:INFO: Dataset: zara2               Batch: 19/36	Loss 40.3024 (42.0125)
2022-11-11 21:58:04,826:INFO: Dataset: zara2               Batch: 20/36	Loss 40.6675 (41.9107)
2022-11-11 21:58:06,343:INFO: Dataset: zara2               Batch: 21/36	Loss 41.8738 (41.9075)
2022-11-11 21:58:07,936:INFO: Dataset: zara2               Batch: 22/36	Loss 40.3881 (41.8302)
2022-11-11 21:58:09,435:INFO: Dataset: zara2               Batch: 23/36	Loss 46.8040 (41.9617)
2022-11-11 21:58:11,051:INFO: Dataset: zara2               Batch: 24/36	Loss 47.8270 (42.1277)
2022-11-11 21:58:12,494:INFO: Dataset: zara2               Batch: 25/36	Loss 46.2077 (42.2367)
2022-11-11 21:58:13,970:INFO: Dataset: zara2               Batch: 26/36	Loss 39.5353 (42.1939)
2022-11-11 21:58:15,453:INFO: Dataset: zara2               Batch: 27/36	Loss 39.2327 (42.1405)
2022-11-11 21:58:17,078:INFO: Dataset: zara2               Batch: 28/36	Loss 42.6145 (42.1596)
2022-11-11 21:58:18,718:INFO: Dataset: zara2               Batch: 29/36	Loss 45.2413 (42.2700)
2022-11-11 21:58:20,323:INFO: Dataset: zara2               Batch: 30/36	Loss 44.3354 (42.3181)
2022-11-11 21:58:21,753:INFO: Dataset: zara2               Batch: 31/36	Loss 43.6295 (42.3522)
2022-11-11 21:58:23,164:INFO: Dataset: zara2               Batch: 32/36	Loss 47.5475 (42.4523)
2022-11-11 21:58:24,638:INFO: Dataset: zara2               Batch: 33/36	Loss 45.1857 (42.5077)
2022-11-11 21:58:26,139:INFO: Dataset: zara2               Batch: 34/36	Loss 47.8888 (42.6038)
2022-11-11 21:58:27,823:INFO: Dataset: zara2               Batch: 35/36	Loss 45.1830 (42.6465)
2022-11-11 21:58:29,408:INFO: Dataset: zara2               Batch: 36/36	Loss 30.7572 (42.5050)
2022-11-11 21:58:30,442:INFO: - Computing ADE (validation)
2022-11-11 21:58:40,072:INFO: 		 ADE on hotel                     dataset:	 0.9734169840812683
2022-11-11 21:58:51,350:INFO: 		 ADE on univ                      dataset:	 1.5141680240631104
2022-11-11 21:59:02,594:INFO: 		 ADE on zara1                     dataset:	 2.557817220687866
2022-11-11 21:59:12,153:INFO: 		 ADE on zara2                     dataset:	 1.4365038871765137
2022-11-11 21:59:12,153:INFO: Average validation:	ADE  1.5167	FDE  2.7338
2022-11-11 21:59:12,154:INFO: - Computing ADE (training)
2022-11-11 21:59:21,647:INFO: 		 ADE on hotel                     dataset:	 1.313422441482544
2022-11-11 21:59:47,840:INFO: 		 ADE on univ                      dataset:	 1.3984920978546143
2022-11-11 21:59:59,029:INFO: 		 ADE on zara1                     dataset:	 2.4767072200775146
2022-11-11 22:00:27,581:INFO: 		 ADE on zara2                     dataset:	 1.7049208879470825
2022-11-11 22:00:27,582:INFO: Average training:	ADE  1.5272	FDE  2.7628
2022-11-11 22:00:27,592:INFO:  --> Model Saved in ./models/E6//P3/CRMF_epoch_5.pth.tar
2022-11-11 22:00:27,592:INFO: 
===> EPOCH: 6 (P3)
2022-11-11 22:00:27,594:INFO: - Computing loss (training)
2022-11-11 22:00:36,787:INFO: Dataset: hotel               Batch: 1/8	Loss 42.8378 (42.8378)
2022-11-11 22:00:38,494:INFO: Dataset: hotel               Batch: 2/8	Loss 39.5129 (40.8519)
2022-11-11 22:00:40,090:INFO: Dataset: hotel               Batch: 3/8	Loss 41.7623 (41.1653)
2022-11-11 22:00:41,591:INFO: Dataset: hotel               Batch: 4/8	Loss 40.6955 (41.0662)
2022-11-11 22:00:43,128:INFO: Dataset: hotel               Batch: 5/8	Loss 40.2908 (40.9120)
2022-11-11 22:00:44,723:INFO: Dataset: hotel               Batch: 6/8	Loss 45.8837 (41.6775)
2022-11-11 22:00:46,174:INFO: Dataset: hotel               Batch: 7/8	Loss 45.4117 (42.1260)
2022-11-11 22:00:47,674:INFO: Dataset: hotel               Batch: 8/8	Loss 8.3631 (40.2553)
2022-11-11 22:01:15,940:INFO: Dataset: univ                Batch:  1/29	Loss 40.2812 (40.2812)
2022-11-11 22:01:17,534:INFO: Dataset: univ                Batch:  2/29	Loss 40.4807 (40.3748)
2022-11-11 22:01:19,032:INFO: Dataset: univ                Batch:  3/29	Loss 40.3100 (40.3591)
2022-11-11 22:01:20,637:INFO: Dataset: univ                Batch:  4/29	Loss 40.5594 (40.3980)
2022-11-11 22:01:22,206:INFO: Dataset: univ                Batch:  5/29	Loss 39.7512 (40.2682)
2022-11-11 22:01:23,752:INFO: Dataset: univ                Batch:  6/29	Loss 39.3205 (40.0963)
2022-11-11 22:01:25,335:INFO: Dataset: univ                Batch:  7/29	Loss 39.1060 (39.9505)
2022-11-11 22:01:26,804:INFO: Dataset: univ                Batch:  8/29	Loss 39.2954 (39.8707)
2022-11-11 22:01:28,296:INFO: Dataset: univ                Batch:  9/29	Loss 40.0601 (39.8963)
2022-11-11 22:01:29,903:INFO: Dataset: univ                Batch: 10/29	Loss 39.3718 (39.8499)
2022-11-11 22:01:31,480:INFO: Dataset: univ                Batch: 11/29	Loss 41.0193 (39.9346)
2022-11-11 22:01:33,032:INFO: Dataset: univ                Batch: 12/29	Loss 42.0967 (40.0596)
2022-11-11 22:01:34,784:INFO: Dataset: univ                Batch: 13/29	Loss 41.5778 (40.1365)
2022-11-11 22:01:36,975:INFO: Dataset: univ                Batch: 14/29	Loss 42.4337 (40.2816)
2022-11-11 22:01:38,489:INFO: Dataset: univ                Batch: 15/29	Loss 41.7328 (40.3427)
2022-11-11 22:01:40,093:INFO: Dataset: univ                Batch: 16/29	Loss 40.5779 (40.3532)
2022-11-11 22:01:41,896:INFO: Dataset: univ                Batch: 17/29	Loss 41.6889 (40.4200)
2022-11-11 22:01:43,400:INFO: Dataset: univ                Batch: 18/29	Loss 41.9433 (40.4882)
2022-11-11 22:01:44,987:INFO: Dataset: univ                Batch: 19/29	Loss 40.6509 (40.4962)
2022-11-11 22:01:46,084:INFO: Dataset: univ                Batch: 20/29	Loss 40.4463 (40.4946)
2022-11-11 22:01:47,242:INFO: Dataset: univ                Batch: 21/29	Loss 40.0653 (40.4799)
2022-11-11 22:01:48,661:INFO: Dataset: univ                Batch: 22/29	Loss 39.9802 (40.4666)
2022-11-11 22:01:49,916:INFO: Dataset: univ                Batch: 23/29	Loss 42.1147 (40.5056)
2022-11-11 22:01:51,235:INFO: Dataset: univ                Batch: 24/29	Loss 49.5029 (40.5981)
2022-11-11 22:01:52,390:INFO: Dataset: univ                Batch: 25/29	Loss 46.4492 (40.6289)
2022-11-11 22:01:53,577:INFO: Dataset: univ                Batch: 26/29	Loss 50.3116 (40.6614)
2022-11-11 22:01:55,171:INFO: Dataset: univ                Batch: 27/29	Loss 48.9072 (40.6936)
2022-11-11 22:01:56,535:INFO: Dataset: univ                Batch: 28/29	Loss 53.1104 (40.7454)
2022-11-11 22:01:57,889:INFO: Dataset: univ                Batch: 29/29	Loss 19.7582 (40.7216)
2022-11-11 22:02:07,836:INFO: Dataset: zara1               Batch:  1/16	Loss 43.7723 (43.7723)
2022-11-11 22:02:09,297:INFO: Dataset: zara1               Batch:  2/16	Loss 42.9195 (43.3619)
2022-11-11 22:02:10,807:INFO: Dataset: zara1               Batch:  3/16	Loss 42.7569 (43.1572)
2022-11-11 22:02:12,313:INFO: Dataset: zara1               Batch:  4/16	Loss 46.3787 (43.7693)
2022-11-11 22:02:13,789:INFO: Dataset: zara1               Batch:  5/16	Loss 42.3354 (43.5224)
2022-11-11 22:02:15,256:INFO: Dataset: zara1               Batch:  6/16	Loss 43.1843 (43.4736)
2022-11-11 22:02:16,709:INFO: Dataset: zara1               Batch:  7/16	Loss 47.7076 (43.9190)
2022-11-11 22:02:18,100:INFO: Dataset: zara1               Batch:  8/16	Loss 50.8762 (44.6384)
2022-11-11 22:02:19,698:INFO: Dataset: zara1               Batch:  9/16	Loss 48.9637 (45.1801)
2022-11-11 22:02:21,161:INFO: Dataset: zara1               Batch: 10/16	Loss 43.5485 (45.0797)
2022-11-11 22:02:22,774:INFO: Dataset: zara1               Batch: 11/16	Loss 46.2928 (45.1610)
2022-11-11 22:02:24,332:INFO: Dataset: zara1               Batch: 12/16	Loss 43.8106 (44.9333)
2022-11-11 22:02:25,922:INFO: Dataset: zara1               Batch: 13/16	Loss 44.7555 (44.9060)
2022-11-11 22:02:27,477:INFO: Dataset: zara1               Batch: 14/16	Loss 48.2879 (45.0526)
2022-11-11 22:02:29,100:INFO: Dataset: zara1               Batch: 15/16	Loss 49.8737 (45.3345)
2022-11-11 22:02:30,725:INFO: Dataset: zara1               Batch: 16/16	Loss 34.8090 (44.8525)
2022-11-11 22:02:59,989:INFO: Dataset: zara2               Batch:  1/36	Loss 47.9425 (47.9425)
2022-11-11 22:03:01,471:INFO: Dataset: zara2               Batch:  2/36	Loss 48.4546 (48.2018)
2022-11-11 22:03:03,005:INFO: Dataset: zara2               Batch:  3/36	Loss 47.8320 (48.0409)
2022-11-11 22:03:04,566:INFO: Dataset: zara2               Batch:  4/36	Loss 42.9831 (46.8017)
2022-11-11 22:03:06,083:INFO: Dataset: zara2               Batch:  5/36	Loss 45.1011 (46.4321)
2022-11-11 22:03:07,637:INFO: Dataset: zara2               Batch:  6/36	Loss 43.1982 (45.8547)
2022-11-11 22:03:09,125:INFO: Dataset: zara2               Batch:  7/36	Loss 42.7573 (45.5723)
2022-11-11 22:03:10,655:INFO: Dataset: zara2               Batch:  8/36	Loss 45.8008 (45.6016)
2022-11-11 22:03:12,218:INFO: Dataset: zara2               Batch:  9/36	Loss 43.0448 (45.4442)
2022-11-11 22:03:13,892:INFO: Dataset: zara2               Batch: 10/36	Loss 39.4688 (44.7849)
2022-11-11 22:03:15,505:INFO: Dataset: zara2               Batch: 11/36	Loss 41.7453 (44.3318)
2022-11-11 22:03:17,178:INFO: Dataset: zara2               Batch: 12/36	Loss 39.7727 (43.6132)
2022-11-11 22:03:18,704:INFO: Dataset: zara2               Batch: 13/36	Loss 39.3421 (43.0708)
2022-11-11 22:03:20,369:INFO: Dataset: zara2               Batch: 14/36	Loss 41.3867 (42.8965)
2022-11-11 22:03:21,917:INFO: Dataset: zara2               Batch: 15/36	Loss 40.5275 (42.6274)
2022-11-11 22:03:23,441:INFO: Dataset: zara2               Batch: 16/36	Loss 40.4061 (42.4416)
2022-11-11 22:03:25,001:INFO: Dataset: zara2               Batch: 17/36	Loss 39.9279 (42.2618)
2022-11-11 22:03:26,535:INFO: Dataset: zara2               Batch: 18/36	Loss 39.0319 (42.0939)
2022-11-11 22:03:27,980:INFO: Dataset: zara2               Batch: 19/36	Loss 40.3023 (41.9652)
2022-11-11 22:03:29,478:INFO: Dataset: zara2               Batch: 20/36	Loss 40.6306 (41.8642)
2022-11-11 22:03:31,008:INFO: Dataset: zara2               Batch: 21/36	Loss 41.8833 (41.8659)
2022-11-11 22:03:32,551:INFO: Dataset: zara2               Batch: 22/36	Loss 40.3911 (41.7909)
2022-11-11 22:03:34,012:INFO: Dataset: zara2               Batch: 23/36	Loss 46.8150 (41.9237)
2022-11-11 22:03:35,511:INFO: Dataset: zara2               Batch: 24/36	Loss 47.8154 (42.0905)
2022-11-11 22:03:37,051:INFO: Dataset: zara2               Batch: 25/36	Loss 46.1909 (42.1999)
2022-11-11 22:03:38,589:INFO: Dataset: zara2               Batch: 26/36	Loss 39.5201 (42.1575)
2022-11-11 22:03:40,141:INFO: Dataset: zara2               Batch: 27/36	Loss 39.2464 (42.1050)
2022-11-11 22:03:41,723:INFO: Dataset: zara2               Batch: 28/36	Loss 42.6335 (42.1263)
2022-11-11 22:03:43,304:INFO: Dataset: zara2               Batch: 29/36	Loss 45.2716 (42.2389)
2022-11-11 22:03:44,924:INFO: Dataset: zara2               Batch: 30/36	Loss 44.3266 (42.2876)
2022-11-11 22:03:46,821:INFO: Dataset: zara2               Batch: 31/36	Loss 43.6115 (42.3220)
2022-11-11 22:03:48,436:INFO: Dataset: zara2               Batch: 32/36	Loss 47.6499 (42.4247)
2022-11-11 22:03:50,080:INFO: Dataset: zara2               Batch: 33/36	Loss 45.1494 (42.4799)
2022-11-11 22:03:51,597:INFO: Dataset: zara2               Batch: 34/36	Loss 47.8745 (42.5763)
2022-11-11 22:03:53,138:INFO: Dataset: zara2               Batch: 35/36	Loss 45.2033 (42.6198)
2022-11-11 22:03:54,594:INFO: Dataset: zara2               Batch: 36/36	Loss 30.7412 (42.4784)
2022-11-11 22:03:55,573:INFO: - Computing ADE (validation)
2022-11-11 22:04:04,707:INFO: 		 ADE on hotel                     dataset:	 0.9576564431190491
2022-11-11 22:04:14,476:INFO: 		 ADE on univ                      dataset:	 1.5099765062332153
2022-11-11 22:04:25,338:INFO: 		 ADE on zara1                     dataset:	 2.549945592880249
2022-11-11 22:04:35,155:INFO: 		 ADE on zara2                     dataset:	 1.439832329750061
2022-11-11 22:04:35,155:INFO: Average validation:	ADE  1.5145	FDE  2.7315
2022-11-11 22:04:35,157:INFO: - Computing ADE (training)
2022-11-11 22:04:45,665:INFO: 		 ADE on hotel                     dataset:	 1.3223720788955688
2022-11-11 22:05:13,144:INFO: 		 ADE on univ                      dataset:	 1.398166298866272
2022-11-11 22:05:23,025:INFO: 		 ADE on zara1                     dataset:	 2.4776253700256348
2022-11-11 22:05:50,634:INFO: 		 ADE on zara2                     dataset:	 1.7048029899597168
2022-11-11 22:05:50,635:INFO: Average training:	ADE  1.5273	FDE  2.7630
2022-11-11 22:05:50,646:INFO:  --> Model Saved in ./models/E6//P3/CRMF_epoch_6.pth.tar
2022-11-11 22:05:50,646:INFO: 
===> EPOCH: 7 (P4)
2022-11-11 22:05:50,647:INFO: - Computing loss (training)
2022-11-11 22:05:57,319:INFO: Dataset: hotel               Batch: 1/8	Loss 15.0667 (15.0667)
2022-11-11 22:05:57,862:INFO: Dataset: hotel               Batch: 2/8	Loss 14.5190 (14.7396)
2022-11-11 22:05:58,178:INFO: Dataset: hotel               Batch: 3/8	Loss 13.7548 (14.4006)
2022-11-11 22:05:58,418:INFO: Dataset: hotel               Batch: 4/8	Loss 12.8070 (14.0647)
2022-11-11 22:05:58,701:INFO: Dataset: hotel               Batch: 5/8	Loss 11.4761 (13.5499)
2022-11-11 22:05:58,979:INFO: Dataset: hotel               Batch: 6/8	Loss 8.2261 (12.7302)
2022-11-11 22:05:58,983:INFO: Dataset: hotel               Batch: 7/8	Loss 7.1603 (12.0612)
2022-11-11 22:05:58,986:INFO: Dataset: hotel               Batch: 8/8	Loss 6.8692 (11.7735)
2022-11-11 22:06:25,556:INFO: Dataset: univ                Batch:  1/29	Loss 14.4925 (14.4925)
2022-11-11 22:06:25,564:INFO: Dataset: univ                Batch:  2/29	Loss 14.6860 (14.5832)
2022-11-11 22:06:25,572:INFO: Dataset: univ                Batch:  3/29	Loss 13.7064 (14.3717)
2022-11-11 22:06:25,581:INFO: Dataset: univ                Batch:  4/29	Loss 12.6369 (14.0349)
2022-11-11 22:06:25,589:INFO: Dataset: univ                Batch:  5/29	Loss 10.8092 (13.3873)
2022-11-11 22:06:25,609:INFO: Dataset: univ                Batch:  6/29	Loss 9.4528 (12.6739)
2022-11-11 22:06:25,614:INFO: Dataset: univ                Batch:  7/29	Loss 8.1000 (12.0004)
2022-11-11 22:06:25,618:INFO: Dataset: univ                Batch:  8/29	Loss 7.1926 (11.4147)
2022-11-11 22:06:25,621:INFO: Dataset: univ                Batch:  9/29	Loss 6.5635 (10.7592)
2022-11-11 22:06:25,629:INFO: Dataset: univ                Batch: 10/29	Loss 6.0282 (10.3412)
2022-11-11 22:06:25,633:INFO: Dataset: univ                Batch: 11/29	Loss 5.5857 (9.9969)
2022-11-11 22:06:25,639:INFO: Dataset: univ                Batch: 12/29	Loss 5.2568 (9.7228)
2022-11-11 22:06:25,646:INFO: Dataset: univ                Batch: 13/29	Loss 4.8650 (9.4768)
2022-11-11 22:06:25,653:INFO: Dataset: univ                Batch: 14/29	Loss 4.5322 (9.1644)
2022-11-11 22:06:25,656:INFO: Dataset: univ                Batch: 15/29	Loss 4.2095 (8.9560)
2022-11-11 22:06:25,663:INFO: Dataset: univ                Batch: 16/29	Loss 3.9547 (8.7318)
2022-11-11 22:06:25,669:INFO: Dataset: univ                Batch: 17/29	Loss 3.4752 (8.4689)
2022-11-11 22:06:25,674:INFO: Dataset: univ                Batch: 18/29	Loss 3.0125 (8.2246)
2022-11-11 22:06:25,680:INFO: Dataset: univ                Batch: 19/29	Loss 2.6797 (7.9520)
2022-11-11 22:06:25,683:INFO: Dataset: univ                Batch: 20/29	Loss 2.5281 (7.7722)
2022-11-11 22:06:25,688:INFO: Dataset: univ                Batch: 21/29	Loss 2.2676 (7.5842)
2022-11-11 22:06:25,695:INFO: Dataset: univ                Batch: 22/29	Loss 1.9941 (7.4348)
2022-11-11 22:06:25,698:INFO: Dataset: univ                Batch: 23/29	Loss 1.6175 (7.2970)
2022-11-11 22:06:25,703:INFO: Dataset: univ                Batch: 24/29	Loss 2.0654 (7.2433)
2022-11-11 22:06:25,711:INFO: Dataset: univ                Batch: 25/29	Loss 1.4596 (7.2127)
2022-11-11 22:06:25,714:INFO: Dataset: univ                Batch: 26/29	Loss 1.8051 (7.1946)
2022-11-11 22:06:25,719:INFO: Dataset: univ                Batch: 27/29	Loss 0.9966 (7.1704)
2022-11-11 22:06:25,723:INFO: Dataset: univ                Batch: 28/29	Loss 0.6147 (7.1430)
2022-11-11 22:06:25,732:INFO: Dataset: univ                Batch: 29/29	Loss 0.4059 (7.1354)
2022-11-11 22:06:33,730:INFO: Dataset: zara1               Batch:  1/16	Loss 17.9018 (17.9018)
2022-11-11 22:06:34,172:INFO: Dataset: zara1               Batch:  2/16	Loss 16.3828 (17.1707)
2022-11-11 22:06:34,563:INFO: Dataset: zara1               Batch:  3/16	Loss 15.2706 (16.5279)
2022-11-11 22:06:34,895:INFO: Dataset: zara1               Batch:  4/16	Loss 13.1413 (15.8845)
2022-11-11 22:06:35,196:INFO: Dataset: zara1               Batch:  5/16	Loss 11.7749 (15.1769)
2022-11-11 22:06:35,420:INFO: Dataset: zara1               Batch:  6/16	Loss 10.3923 (14.4856)
2022-11-11 22:06:35,425:INFO: Dataset: zara1               Batch:  7/16	Loss 8.4606 (13.8518)
2022-11-11 22:06:35,428:INFO: Dataset: zara1               Batch:  8/16	Loss 6.3794 (13.0791)
2022-11-11 22:06:35,431:INFO: Dataset: zara1               Batch:  9/16	Loss 5.3791 (12.1147)
2022-11-11 22:06:35,437:INFO: Dataset: zara1               Batch: 10/16	Loss 4.8044 (11.6646)
2022-11-11 22:06:35,442:INFO: Dataset: zara1               Batch: 11/16	Loss 3.2450 (11.1004)
2022-11-11 22:06:35,446:INFO: Dataset: zara1               Batch: 12/16	Loss 3.0478 (9.7427)
2022-11-11 22:06:35,451:INFO: Dataset: zara1               Batch: 13/16	Loss 2.4158 (8.6165)
2022-11-11 22:06:35,454:INFO: Dataset: zara1               Batch: 14/16	Loss 4.0502 (8.4186)
2022-11-11 22:06:35,458:INFO: Dataset: zara1               Batch: 15/16	Loss 2.9938 (8.1014)
2022-11-11 22:06:35,463:INFO: Dataset: zara1               Batch: 16/16	Loss 1.4502 (7.7969)
2022-11-11 22:07:00,034:INFO: Dataset: zara2               Batch:  1/36	Loss 20.7737 (20.7737)
2022-11-11 22:07:00,039:INFO: Dataset: zara2               Batch:  2/36	Loss 17.0393 (18.8827)
2022-11-11 22:07:00,044:INFO: Dataset: zara2               Batch:  3/36	Loss 13.2913 (16.4499)
2022-11-11 22:07:00,053:INFO: Dataset: zara2               Batch:  4/36	Loss 10.3571 (14.9571)
2022-11-11 22:07:00,057:INFO: Dataset: zara2               Batch:  5/36	Loss 9.1764 (13.7008)
2022-11-11 22:07:00,075:INFO: Dataset: zara2               Batch:  6/36	Loss 7.6368 (12.6182)
2022-11-11 22:07:00,079:INFO: Dataset: zara2               Batch:  7/36	Loss 6.9651 (12.1026)
2022-11-11 22:07:00,082:INFO: Dataset: zara2               Batch:  8/36	Loss 6.1978 (11.3441)
2022-11-11 22:07:00,087:INFO: Dataset: zara2               Batch:  9/36	Loss 5.8969 (11.0086)
2022-11-11 22:07:00,092:INFO: Dataset: zara2               Batch: 10/36	Loss 5.5957 (10.4114)
2022-11-11 22:07:00,095:INFO: Dataset: zara2               Batch: 11/36	Loss 5.1412 (9.6259)
2022-11-11 22:07:00,099:INFO: Dataset: zara2               Batch: 12/36	Loss 4.8033 (8.8657)
2022-11-11 22:07:00,104:INFO: Dataset: zara2               Batch: 13/36	Loss 4.5275 (8.3148)
2022-11-11 22:07:00,110:INFO: Dataset: zara2               Batch: 14/36	Loss 4.0672 (7.8753)
2022-11-11 22:07:00,114:INFO: Dataset: zara2               Batch: 15/36	Loss 3.7250 (7.4039)
2022-11-11 22:07:00,117:INFO: Dataset: zara2               Batch: 16/36	Loss 3.3960 (7.0685)
2022-11-11 22:07:00,120:INFO: Dataset: zara2               Batch: 17/36	Loss 3.0628 (6.7821)
2022-11-11 22:07:00,126:INFO: Dataset: zara2               Batch: 18/36	Loss 2.8015 (6.5751)
2022-11-11 22:07:00,132:INFO: Dataset: zara2               Batch: 19/36	Loss 2.4174 (6.2765)
2022-11-11 22:07:00,137:INFO: Dataset: zara2               Batch: 20/36	Loss 2.1850 (5.9669)
2022-11-11 22:07:00,142:INFO: Dataset: zara2               Batch: 21/36	Loss 1.6666 (5.5936)
2022-11-11 22:07:00,147:INFO: Dataset: zara2               Batch: 22/36	Loss 1.4743 (5.3840)
2022-11-11 22:07:00,152:INFO: Dataset: zara2               Batch: 23/36	Loss 1.2958 (5.2760)
2022-11-11 22:07:00,159:INFO: Dataset: zara2               Batch: 24/36	Loss 0.7670 (5.1483)
2022-11-11 22:07:00,163:INFO: Dataset: zara2               Batch: 25/36	Loss 0.8756 (5.0343)
2022-11-11 22:07:00,166:INFO: Dataset: zara2               Batch: 26/36	Loss 0.9774 (4.9699)
2022-11-11 22:07:00,170:INFO: Dataset: zara2               Batch: 27/36	Loss 0.6149 (4.8915)
2022-11-11 22:07:00,176:INFO: Dataset: zara2               Batch: 28/36	Loss 0.5586 (4.7169)
2022-11-11 22:07:00,182:INFO: Dataset: zara2               Batch: 29/36	Loss 0.3630 (4.5609)
2022-11-11 22:07:00,187:INFO: Dataset: zara2               Batch: 30/36	Loss 0.4469 (4.4650)
2022-11-11 22:07:00,192:INFO: Dataset: zara2               Batch: 31/36	Loss 0.4632 (4.3611)
2022-11-11 22:07:00,196:INFO: Dataset: zara2               Batch: 32/36	Loss 0.2637 (4.2822)
2022-11-11 22:07:00,200:INFO: Dataset: zara2               Batch: 33/36	Loss 0.1171 (4.1977)
2022-11-11 22:07:00,205:INFO: Dataset: zara2               Batch: 34/36	Loss 0.1662 (4.1257)
2022-11-11 22:07:00,209:INFO: Dataset: zara2               Batch: 35/36	Loss 0.2147 (4.0609)
2022-11-11 22:07:00,213:INFO: Dataset: zara2               Batch: 36/36	Loss 0.1970 (4.0150)

2022-11-08 17:29:59,803:INFO: Initializing Training Set
2022-11-08 17:30:03,931:INFO: Initializing Validation Set
2022-11-08 17:30:04,510:INFO: Initializing Validation O Set
2022-11-08 17:30:08,015:INFO: => loaded checkpoint './models/eth/E3/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 100)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1050.pth.tar' (epoch 1051)
2022-11-08 17:30:08,016:INFO: 
===> EPOCH: 1051 (P5)
2022-11-08 17:30:08,016:INFO: - Computing loss (training)
2022-11-08 17:30:14,263:INFO: Dataset: hotel               Batch: 1/8	Loss 14.6295 (14.6295)
2022-11-08 17:30:14,618:INFO: Dataset: hotel               Batch: 2/8	Loss 14.3501 (14.4898)
2022-11-08 17:30:14,971:INFO: Dataset: hotel               Batch: 3/8	Loss 13.7089 (14.2245)
2022-11-08 17:30:15,362:INFO: Dataset: hotel               Batch: 4/8	Loss 14.8521 (14.3947)
2022-11-08 17:30:15,624:INFO: Dataset: hotel               Batch: 5/8	Loss 13.1180 (14.1452)
2022-11-08 17:30:15,766:INFO: Dataset: hotel               Batch: 6/8	Loss 14.4793 (14.2033)
2022-11-08 17:30:15,773:INFO: Dataset: hotel               Batch: 7/8	Loss 12.8721 (14.0229)
2022-11-08 17:30:15,778:INFO: Dataset: hotel               Batch: 8/8	Loss 13.4203 (14.0070)
2022-11-08 17:30:39,198:INFO: Dataset: univ                Batch:  1/29	Loss 8.4728 (8.4728)
2022-11-08 17:30:39,203:INFO: Dataset: univ                Batch:  2/29	Loss 8.5580 (8.5160)
2022-11-08 17:30:39,211:INFO: Dataset: univ                Batch:  3/29	Loss 8.1430 (8.3790)
2022-11-08 17:30:39,220:INFO: Dataset: univ                Batch:  4/29	Loss 8.2164 (8.3364)
2022-11-08 17:30:39,231:INFO: Dataset: univ                Batch:  5/29	Loss 8.4198 (8.3527)
2022-11-08 17:30:39,240:INFO: Dataset: univ                Batch:  6/29	Loss 8.4624 (8.3707)
2022-11-08 17:30:39,243:INFO: Dataset: univ                Batch:  7/29	Loss 8.5769 (8.3974)
2022-11-08 17:30:39,250:INFO: Dataset: univ                Batch:  8/29	Loss 8.1984 (8.3704)
2022-11-08 17:30:39,254:INFO: Dataset: univ                Batch:  9/29	Loss 7.7322 (8.3018)
2022-11-08 17:30:39,258:INFO: Dataset: univ                Batch: 10/29	Loss 8.6093 (8.3278)
2022-11-08 17:30:39,265:INFO: Dataset: univ                Batch: 11/29	Loss 8.2577 (8.3217)
2022-11-08 17:30:39,283:INFO: Dataset: univ                Batch: 12/29	Loss 8.0785 (8.3003)
2022-11-08 17:30:39,291:INFO: Dataset: univ                Batch: 13/29	Loss 7.6971 (8.2489)
2022-11-08 17:30:39,299:INFO: Dataset: univ                Batch: 14/29	Loss 7.5341 (8.1917)
2022-11-08 17:30:39,319:INFO: Dataset: univ                Batch: 15/29	Loss 7.4311 (8.1392)
2022-11-08 17:30:39,323:INFO: Dataset: univ                Batch: 16/29	Loss 7.3801 (8.0906)
2022-11-08 17:30:39,329:INFO: Dataset: univ                Batch: 17/29	Loss 7.5312 (8.0581)
2022-11-08 17:30:39,351:INFO: Dataset: univ                Batch: 18/29	Loss 7.1260 (8.0135)
2022-11-08 17:30:39,359:INFO: Dataset: univ                Batch: 19/29	Loss 6.8865 (7.9463)
2022-11-08 17:30:39,368:INFO: Dataset: univ                Batch: 20/29	Loss 7.0959 (7.9069)
2022-11-08 17:30:39,392:INFO: Dataset: univ                Batch: 21/29	Loss 6.8672 (7.8597)
2022-11-08 17:30:39,404:INFO: Dataset: univ                Batch: 22/29	Loss 6.8402 (7.8182)
2022-11-08 17:30:39,411:INFO: Dataset: univ                Batch: 23/29	Loss 6.7441 (7.7691)
2022-11-08 17:30:39,416:INFO: Dataset: univ                Batch: 24/29	Loss 6.3072 (7.7025)
2022-11-08 17:30:39,420:INFO: Dataset: univ                Batch: 25/29	Loss 6.2850 (7.6475)
2022-11-08 17:30:39,427:INFO: Dataset: univ                Batch: 26/29	Loss 6.3415 (7.5962)
2022-11-08 17:30:39,430:INFO: Dataset: univ                Batch: 27/29	Loss 6.1667 (7.5372)
2022-11-08 17:30:39,434:INFO: Dataset: univ                Batch: 28/29	Loss 6.2565 (7.4971)
2022-11-08 17:30:39,439:INFO: Dataset: univ                Batch: 29/29	Loss 5.3084 (7.4640)
2022-11-08 17:30:47,067:INFO: Dataset: zara1               Batch:  1/16	Loss 13.1759 (13.1759)
2022-11-08 17:30:47,439:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2846 (13.2312)
2022-11-08 17:30:47,776:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4988 (13.3243)
2022-11-08 17:30:48,138:INFO: Dataset: zara1               Batch:  4/16	Loss 13.0530 (13.2559)
2022-11-08 17:30:48,305:INFO: Dataset: zara1               Batch:  5/16	Loss 13.3129 (13.2659)
2022-11-08 17:30:48,485:INFO: Dataset: zara1               Batch:  6/16	Loss 13.1021 (13.2312)
2022-11-08 17:30:48,488:INFO: Dataset: zara1               Batch:  7/16	Loss 12.9580 (13.1949)
2022-11-08 17:30:48,491:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8538 (13.1534)
2022-11-08 17:30:48,493:INFO: Dataset: zara1               Batch:  9/16	Loss 12.3994 (13.0772)
2022-11-08 17:30:48,496:INFO: Dataset: zara1               Batch: 10/16	Loss 12.7145 (13.0403)
2022-11-08 17:30:48,498:INFO: Dataset: zara1               Batch: 11/16	Loss 13.0414 (13.0404)
2022-11-08 17:30:48,502:INFO: Dataset: zara1               Batch: 12/16	Loss 12.5288 (12.9893)
2022-11-08 17:30:48,505:INFO: Dataset: zara1               Batch: 13/16	Loss 12.0817 (12.9141)
2022-11-08 17:30:48,509:INFO: Dataset: zara1               Batch: 14/16	Loss 12.5420 (12.8903)
2022-11-08 17:30:48,513:INFO: Dataset: zara1               Batch: 15/16	Loss 11.4567 (12.7972)
2022-11-08 17:30:48,518:INFO: Dataset: zara1               Batch: 16/16	Loss 11.1994 (12.7417)
2022-11-08 17:31:11,245:INFO: Dataset: zara2               Batch:  1/36	Loss 7.3156 (7.3156)
2022-11-08 17:31:11,249:INFO: Dataset: zara2               Batch:  2/36	Loss 7.4946 (7.4091)
2022-11-08 17:31:11,253:INFO: Dataset: zara2               Batch:  3/36	Loss 6.7900 (7.1940)
2022-11-08 17:31:11,257:INFO: Dataset: zara2               Batch:  4/36	Loss 7.2865 (7.2199)
2022-11-08 17:31:11,260:INFO: Dataset: zara2               Batch:  5/36	Loss 6.8445 (7.1489)
2022-11-08 17:31:11,286:INFO: Dataset: zara2               Batch:  6/36	Loss 6.8286 (7.0928)
2022-11-08 17:31:11,289:INFO: Dataset: zara2               Batch:  7/36	Loss 6.8580 (7.0618)
2022-11-08 17:31:11,292:INFO: Dataset: zara2               Batch:  8/36	Loss 6.7611 (7.0307)
2022-11-08 17:31:11,295:INFO: Dataset: zara2               Batch:  9/36	Loss 6.7523 (7.0027)
2022-11-08 17:31:11,299:INFO: Dataset: zara2               Batch: 10/36	Loss 6.7262 (6.9729)
2022-11-08 17:31:11,302:INFO: Dataset: zara2               Batch: 11/36	Loss 6.6042 (6.9364)
2022-11-08 17:31:11,305:INFO: Dataset: zara2               Batch: 12/36	Loss 6.7392 (6.9152)
2022-11-08 17:31:11,308:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0082 (6.8441)
2022-11-08 17:31:11,312:INFO: Dataset: zara2               Batch: 14/36	Loss 6.2855 (6.7995)
2022-11-08 17:31:11,315:INFO: Dataset: zara2               Batch: 15/36	Loss 6.1116 (6.7541)
2022-11-08 17:31:11,319:INFO: Dataset: zara2               Batch: 16/36	Loss 6.2632 (6.7240)
2022-11-08 17:31:11,323:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6635 (6.6578)
2022-11-08 17:31:11,329:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6690 (6.5981)
2022-11-08 17:31:11,331:INFO: Dataset: zara2               Batch: 19/36	Loss 5.8064 (6.5533)
2022-11-08 17:31:11,335:INFO: Dataset: zara2               Batch: 20/36	Loss 6.1007 (6.5320)
2022-11-08 17:31:11,338:INFO: Dataset: zara2               Batch: 21/36	Loss 5.7916 (6.4941)
2022-11-08 17:31:11,342:INFO: Dataset: zara2               Batch: 22/36	Loss 5.4893 (6.4388)
2022-11-08 17:31:11,346:INFO: Dataset: zara2               Batch: 23/36	Loss 5.4715 (6.4013)
2022-11-08 17:31:11,349:INFO: Dataset: zara2               Batch: 24/36	Loss 5.3739 (6.3566)
2022-11-08 17:31:11,352:INFO: Dataset: zara2               Batch: 25/36	Loss 5.4539 (6.3149)
2022-11-08 17:31:11,356:INFO: Dataset: zara2               Batch: 26/36	Loss 5.3399 (6.2791)
2022-11-08 17:31:11,359:INFO: Dataset: zara2               Batch: 27/36	Loss 5.0885 (6.2391)
2022-11-08 17:31:11,362:INFO: Dataset: zara2               Batch: 28/36	Loss 5.0375 (6.1945)
2022-11-08 17:31:11,366:INFO: Dataset: zara2               Batch: 29/36	Loss 5.1425 (6.1586)
2022-11-08 17:31:11,369:INFO: Dataset: zara2               Batch: 30/36	Loss 4.7397 (6.1035)
2022-11-08 17:31:11,371:INFO: Dataset: zara2               Batch: 31/36	Loss 4.9899 (6.0699)
2022-11-08 17:31:11,375:INFO: Dataset: zara2               Batch: 32/36	Loss 4.8082 (6.0270)
2022-11-08 17:31:11,378:INFO: Dataset: zara2               Batch: 33/36	Loss 4.9945 (5.9989)
2022-11-08 17:31:11,381:INFO: Dataset: zara2               Batch: 34/36	Loss 4.6817 (5.9643)
2022-11-08 17:31:11,386:INFO: Dataset: zara2               Batch: 35/36	Loss 4.7157 (5.9264)
2022-11-08 17:31:11,389:INFO: Dataset: zara2               Batch: 36/36	Loss 4.6778 (5.9012)
2022-11-08 17:31:12,273:INFO: - Computing loss (validation)
2022-11-08 17:31:18,163:INFO: Dataset: hotel               Batch: 1/3	Loss 16.7755 (16.7755)
2022-11-08 17:31:18,564:INFO: Dataset: hotel               Batch: 2/3	Loss 17.1857 (16.9762)
2022-11-08 17:31:18,891:INFO: Dataset: hotel               Batch: 3/3	Loss 20.9375 (17.1520)
2022-11-08 17:31:26,318:INFO: Dataset: univ                Batch: 1/6	Loss 9.1031 (9.1031)
2022-11-08 17:31:26,614:INFO: Dataset: univ                Batch: 2/6	Loss 8.8894 (8.9996)
2022-11-08 17:31:27,005:INFO: Dataset: univ                Batch: 3/6	Loss 9.2208 (9.0696)
2022-11-08 17:31:27,306:INFO: Dataset: univ                Batch: 4/6	Loss 8.5352 (8.9136)
2022-11-08 17:31:27,555:INFO: Dataset: univ                Batch: 5/6	Loss 8.9040 (8.9117)
2022-11-08 17:31:27,754:INFO: Dataset: univ                Batch: 6/6	Loss 9.7202 (9.0117)
2022-11-08 17:31:34,570:INFO: Dataset: zara1               Batch: 1/3	Loss 10.5969 (10.5969)
2022-11-08 17:31:35,012:INFO: Dataset: zara1               Batch: 2/3	Loss 10.6081 (10.6026)
2022-11-08 17:31:35,318:INFO: Dataset: zara1               Batch: 3/3	Loss 10.6542 (10.6150)
2022-11-08 17:31:43,220:INFO: Dataset: zara2               Batch:  1/10	Loss 4.5436 (4.5436)
2022-11-08 17:31:43,588:INFO: Dataset: zara2               Batch:  2/10	Loss 4.4429 (4.4873)
2022-11-08 17:31:43,984:INFO: Dataset: zara2               Batch:  3/10	Loss 4.9265 (4.6243)
2022-11-08 17:31:44,716:INFO: Dataset: zara2               Batch:  4/10	Loss 4.4611 (4.5838)
2022-11-08 17:31:45,066:INFO: Dataset: zara2               Batch:  5/10	Loss 4.5636 (4.5799)
2022-11-08 17:31:45,290:INFO: Dataset: zara2               Batch:  6/10	Loss 4.5199 (4.5704)
2022-11-08 17:31:45,291:INFO: Dataset: zara2               Batch:  7/10	Loss 4.6563 (4.5830)
2022-11-08 17:31:45,293:INFO: Dataset: zara2               Batch:  8/10	Loss 4.5397 (4.5776)
2022-11-08 17:31:45,295:INFO: Dataset: zara2               Batch:  9/10	Loss 4.4556 (4.5650)
2022-11-08 17:31:45,296:INFO: Dataset: zara2               Batch: 10/10	Loss 4.7114 (4.5791)
2022-11-08 17:31:46,281:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1051.pth.tar
2022-11-08 17:31:46,281:INFO: 
===> EPOCH: 1052 (P5)
2022-11-08 17:31:46,282:INFO: - Computing loss (training)
2022-11-08 17:31:52,735:INFO: Dataset: hotel               Batch: 1/8	Loss 13.7406 (13.7406)
2022-11-08 17:31:53,138:INFO: Dataset: hotel               Batch: 2/8	Loss 16.1566 (15.0045)
2022-11-08 17:31:53,460:INFO: Dataset: hotel               Batch: 3/8	Loss 15.3349 (15.1119)
2022-11-08 17:31:53,777:INFO: Dataset: hotel               Batch: 4/8	Loss 13.8725 (14.7863)
2022-11-08 17:31:54,000:INFO: Dataset: hotel               Batch: 5/8	Loss 15.0822 (14.8404)
2022-11-08 17:31:54,164:INFO: Dataset: hotel               Batch: 6/8	Loss 14.3311 (14.7543)
2022-11-08 17:31:54,170:INFO: Dataset: hotel               Batch: 7/8	Loss 13.2885 (14.5594)
2022-11-08 17:31:54,174:INFO: Dataset: hotel               Batch: 8/8	Loss 12.8248 (14.5113)
2022-11-08 17:32:18,250:INFO: Dataset: univ                Batch:  1/29	Loss 9.3573 (9.3573)
2022-11-08 17:32:18,256:INFO: Dataset: univ                Batch:  2/29	Loss 9.2369 (9.3002)
2022-11-08 17:32:18,275:INFO: Dataset: univ                Batch:  3/29	Loss 9.5836 (9.3783)
2022-11-08 17:32:18,291:INFO: Dataset: univ                Batch:  4/29	Loss 9.1707 (9.3276)
2022-11-08 17:32:18,297:INFO: Dataset: univ                Batch:  5/29	Loss 9.6869 (9.3926)
2022-11-08 17:32:18,305:INFO: Dataset: univ                Batch:  6/29	Loss 8.9367 (9.3135)
2022-11-08 17:32:18,312:INFO: Dataset: univ                Batch:  7/29	Loss 9.1300 (9.2869)
2022-11-08 17:32:18,318:INFO: Dataset: univ                Batch:  8/29	Loss 8.9492 (9.2426)
2022-11-08 17:32:18,324:INFO: Dataset: univ                Batch:  9/29	Loss 9.1450 (9.2317)
2022-11-08 17:32:18,329:INFO: Dataset: univ                Batch: 10/29	Loss 8.7224 (9.1792)
2022-11-08 17:32:18,336:INFO: Dataset: univ                Batch: 11/29	Loss 8.7899 (9.1439)
2022-11-08 17:32:18,343:INFO: Dataset: univ                Batch: 12/29	Loss 9.1397 (9.1436)
2022-11-08 17:32:18,350:INFO: Dataset: univ                Batch: 13/29	Loss 8.5097 (9.0986)
2022-11-08 17:32:18,357:INFO: Dataset: univ                Batch: 14/29	Loss 8.4063 (9.0435)
2022-11-08 17:32:18,363:INFO: Dataset: univ                Batch: 15/29	Loss 8.3715 (8.9985)
2022-11-08 17:32:18,369:INFO: Dataset: univ                Batch: 16/29	Loss 8.4396 (8.9644)
2022-11-08 17:32:18,376:INFO: Dataset: univ                Batch: 17/29	Loss 7.8482 (8.9032)
2022-11-08 17:32:18,384:INFO: Dataset: univ                Batch: 18/29	Loss 7.6608 (8.8334)
2022-11-08 17:32:18,391:INFO: Dataset: univ                Batch: 19/29	Loss 7.8460 (8.7855)
2022-11-08 17:32:18,395:INFO: Dataset: univ                Batch: 20/29	Loss 7.7264 (8.7391)
2022-11-08 17:32:18,402:INFO: Dataset: univ                Batch: 21/29	Loss 7.4761 (8.6791)
2022-11-08 17:32:18,406:INFO: Dataset: univ                Batch: 22/29	Loss 7.5048 (8.6297)
2022-11-08 17:32:18,410:INFO: Dataset: univ                Batch: 23/29	Loss 7.2025 (8.5736)
2022-11-08 17:32:18,416:INFO: Dataset: univ                Batch: 24/29	Loss 7.0383 (8.5096)
2022-11-08 17:32:18,423:INFO: Dataset: univ                Batch: 25/29	Loss 7.1108 (8.4567)
2022-11-08 17:32:18,428:INFO: Dataset: univ                Batch: 26/29	Loss 7.0946 (8.4050)
2022-11-08 17:32:18,435:INFO: Dataset: univ                Batch: 27/29	Loss 6.5313 (8.3297)
2022-11-08 17:32:18,440:INFO: Dataset: univ                Batch: 28/29	Loss 6.4681 (8.2648)
2022-11-08 17:32:18,446:INFO: Dataset: univ                Batch: 29/29	Loss 6.4481 (8.2447)
2022-11-08 17:32:25,529:INFO: Dataset: zara1               Batch:  1/16	Loss 13.1794 (13.1794)
2022-11-08 17:32:25,808:INFO: Dataset: zara1               Batch:  2/16	Loss 12.9652 (13.0715)
2022-11-08 17:32:26,167:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4444 (13.1831)
2022-11-08 17:32:26,428:INFO: Dataset: zara1               Batch:  4/16	Loss 13.7566 (13.3063)
2022-11-08 17:32:26,709:INFO: Dataset: zara1               Batch:  5/16	Loss 13.3709 (13.3185)
2022-11-08 17:32:26,967:INFO: Dataset: zara1               Batch:  6/16	Loss 13.2879 (13.3137)
2022-11-08 17:32:26,972:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0932 (13.2803)
2022-11-08 17:32:26,976:INFO: Dataset: zara1               Batch:  8/16	Loss 12.5426 (13.1800)
2022-11-08 17:32:26,980:INFO: Dataset: zara1               Batch:  9/16	Loss 12.7023 (13.1247)
2022-11-08 17:32:26,984:INFO: Dataset: zara1               Batch: 10/16	Loss 11.7051 (12.9949)
2022-11-08 17:32:26,986:INFO: Dataset: zara1               Batch: 11/16	Loss 12.7191 (12.9722)
2022-11-08 17:32:26,990:INFO: Dataset: zara1               Batch: 12/16	Loss 12.6633 (12.9460)
2022-11-08 17:32:26,993:INFO: Dataset: zara1               Batch: 13/16	Loss 12.7271 (12.9306)
2022-11-08 17:32:26,996:INFO: Dataset: zara1               Batch: 14/16	Loss 12.5077 (12.9013)
2022-11-08 17:32:26,999:INFO: Dataset: zara1               Batch: 15/16	Loss 11.5999 (12.8216)
2022-11-08 17:32:27,001:INFO: Dataset: zara1               Batch: 16/16	Loss 12.0446 (12.7856)
2022-11-08 17:32:50,180:INFO: Dataset: zara2               Batch:  1/36	Loss 6.9224 (6.9224)
2022-11-08 17:32:50,187:INFO: Dataset: zara2               Batch:  2/36	Loss 7.5490 (7.2047)
2022-11-08 17:32:50,191:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4910 (6.9820)
2022-11-08 17:32:50,194:INFO: Dataset: zara2               Batch:  4/36	Loss 6.2454 (6.8043)
2022-11-08 17:32:50,197:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3975 (6.7155)
2022-11-08 17:32:50,222:INFO: Dataset: zara2               Batch:  6/36	Loss 6.5237 (6.6793)
2022-11-08 17:32:50,227:INFO: Dataset: zara2               Batch:  7/36	Loss 6.3206 (6.6375)
2022-11-08 17:32:50,231:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0587 (6.5678)
2022-11-08 17:32:50,234:INFO: Dataset: zara2               Batch:  9/36	Loss 6.7549 (6.5869)
2022-11-08 17:32:50,237:INFO: Dataset: zara2               Batch: 10/36	Loss 6.4035 (6.5690)
2022-11-08 17:32:50,240:INFO: Dataset: zara2               Batch: 11/36	Loss 6.0960 (6.5181)
2022-11-08 17:32:50,245:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8659 (6.4728)
2022-11-08 17:32:50,248:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0213 (6.4378)
2022-11-08 17:32:50,251:INFO: Dataset: zara2               Batch: 14/36	Loss 6.2548 (6.4236)
2022-11-08 17:32:50,256:INFO: Dataset: zara2               Batch: 15/36	Loss 5.8215 (6.3812)
2022-11-08 17:32:50,258:INFO: Dataset: zara2               Batch: 16/36	Loss 5.7356 (6.3356)
2022-11-08 17:32:50,261:INFO: Dataset: zara2               Batch: 17/36	Loss 5.4839 (6.2894)
2022-11-08 17:32:50,265:INFO: Dataset: zara2               Batch: 18/36	Loss 5.7703 (6.2575)
2022-11-08 17:32:50,268:INFO: Dataset: zara2               Batch: 19/36	Loss 5.5269 (6.2228)
2022-11-08 17:32:50,272:INFO: Dataset: zara2               Batch: 20/36	Loss 5.2888 (6.1796)
2022-11-08 17:32:50,274:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4908 (6.1462)
2022-11-08 17:32:50,277:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0675 (6.0935)
2022-11-08 17:32:50,282:INFO: Dataset: zara2               Batch: 23/36	Loss 5.4714 (6.0711)
2022-11-08 17:32:50,286:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9971 (6.0357)
2022-11-08 17:32:50,290:INFO: Dataset: zara2               Batch: 25/36	Loss 5.2163 (6.0049)
2022-11-08 17:32:50,294:INFO: Dataset: zara2               Batch: 26/36	Loss 5.0062 (5.9657)
2022-11-08 17:32:50,297:INFO: Dataset: zara2               Batch: 27/36	Loss 4.8923 (5.9264)
2022-11-08 17:32:50,300:INFO: Dataset: zara2               Batch: 28/36	Loss 4.8728 (5.8904)
2022-11-08 17:32:50,303:INFO: Dataset: zara2               Batch: 29/36	Loss 4.9146 (5.8501)
2022-11-08 17:32:50,307:INFO: Dataset: zara2               Batch: 30/36	Loss 4.9485 (5.8114)
2022-11-08 17:32:50,311:INFO: Dataset: zara2               Batch: 31/36	Loss 4.7112 (5.7773)
2022-11-08 17:32:50,315:INFO: Dataset: zara2               Batch: 32/36	Loss 4.2568 (5.7223)
2022-11-08 17:32:50,319:INFO: Dataset: zara2               Batch: 33/36	Loss 4.5003 (5.6894)
2022-11-08 17:32:50,323:INFO: Dataset: zara2               Batch: 34/36	Loss 4.4693 (5.6510)
2022-11-08 17:32:50,327:INFO: Dataset: zara2               Batch: 35/36	Loss 4.4161 (5.6070)
2022-11-08 17:32:50,331:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1652 (5.5784)
2022-11-08 17:32:51,211:INFO: - Computing loss (validation)
2022-11-08 17:32:57,861:INFO: Dataset: hotel               Batch: 1/3	Loss 17.2070 (17.2070)
2022-11-08 17:32:58,282:INFO: Dataset: hotel               Batch: 2/3	Loss 17.7060 (17.4702)
2022-11-08 17:32:58,523:INFO: Dataset: hotel               Batch: 3/3	Loss 18.9606 (17.5719)
2022-11-08 17:33:06,808:INFO: Dataset: univ                Batch: 1/6	Loss 9.0713 (9.0713)
2022-11-08 17:33:07,160:INFO: Dataset: univ                Batch: 2/6	Loss 8.7915 (8.9140)
2022-11-08 17:33:07,524:INFO: Dataset: univ                Batch: 3/6	Loss 9.2541 (9.0274)
2022-11-08 17:33:07,892:INFO: Dataset: univ                Batch: 4/6	Loss 8.7519 (8.9424)
2022-11-08 17:33:08,175:INFO: Dataset: univ                Batch: 5/6	Loss 9.4516 (9.0312)
2022-11-08 17:33:08,350:INFO: Dataset: univ                Batch: 6/6	Loss 8.9024 (9.0123)
2022-11-08 17:33:16,055:INFO: Dataset: zara1               Batch: 1/3	Loss 10.8301 (10.8301)
2022-11-08 17:33:16,517:INFO: Dataset: zara1               Batch: 2/3	Loss 11.0273 (10.9341)
2022-11-08 17:33:16,770:INFO: Dataset: zara1               Batch: 3/3	Loss 10.9312 (10.9334)
2022-11-08 17:33:24,723:INFO: Dataset: zara2               Batch:  1/10	Loss 4.3809 (4.3809)
2022-11-08 17:33:25,106:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1582 (4.2682)
2022-11-08 17:33:25,595:INFO: Dataset: zara2               Batch:  3/10	Loss 4.4484 (4.3253)
2022-11-08 17:33:25,910:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3837 (4.3393)
2022-11-08 17:33:26,108:INFO: Dataset: zara2               Batch:  5/10	Loss 4.4204 (4.3566)
2022-11-08 17:33:26,337:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1564 (4.3231)
2022-11-08 17:33:26,338:INFO: Dataset: zara2               Batch:  7/10	Loss 4.4292 (4.3386)
2022-11-08 17:33:26,340:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2359 (4.3246)
2022-11-08 17:33:26,341:INFO: Dataset: zara2               Batch:  9/10	Loss 4.4737 (4.3405)
2022-11-08 17:33:26,342:INFO: Dataset: zara2               Batch: 10/10	Loss 4.3888 (4.3450)
2022-11-08 17:33:27,350:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1052.pth.tar
2022-11-08 17:33:27,350:INFO: 
===> EPOCH: 1053 (P5)
2022-11-08 17:33:27,351:INFO: - Computing loss (training)
2022-11-08 17:33:35,166:INFO: Dataset: hotel               Batch: 1/8	Loss 14.9715 (14.9715)
2022-11-08 17:33:35,728:INFO: Dataset: hotel               Batch: 2/8	Loss 14.2707 (14.6491)
2022-11-08 17:33:35,922:INFO: Dataset: hotel               Batch: 3/8	Loss 16.6415 (15.3602)
2022-11-08 17:33:36,084:INFO: Dataset: hotel               Batch: 4/8	Loss 14.8931 (15.2398)
2022-11-08 17:33:36,432:INFO: Dataset: hotel               Batch: 5/8	Loss 13.6145 (14.9446)
2022-11-08 17:33:36,740:INFO: Dataset: hotel               Batch: 6/8	Loss 13.6219 (14.7072)
2022-11-08 17:33:36,744:INFO: Dataset: hotel               Batch: 7/8	Loss 14.8146 (14.7235)
2022-11-08 17:33:36,751:INFO: Dataset: hotel               Batch: 8/8	Loss 16.1706 (14.7655)
2022-11-08 17:34:01,025:INFO: Dataset: univ                Batch:  1/29	Loss 9.9600 (9.9600)
2022-11-08 17:34:01,034:INFO: Dataset: univ                Batch:  2/29	Loss 9.0610 (9.4478)
2022-11-08 17:34:01,045:INFO: Dataset: univ                Batch:  3/29	Loss 9.4323 (9.4426)
2022-11-08 17:34:01,052:INFO: Dataset: univ                Batch:  4/29	Loss 9.6696 (9.5031)
2022-11-08 17:34:01,060:INFO: Dataset: univ                Batch:  5/29	Loss 9.5149 (9.5054)
2022-11-08 17:34:01,071:INFO: Dataset: univ                Batch:  6/29	Loss 9.2549 (9.4663)
2022-11-08 17:34:01,075:INFO: Dataset: univ                Batch:  7/29	Loss 9.3193 (9.4447)
2022-11-08 17:34:01,079:INFO: Dataset: univ                Batch:  8/29	Loss 9.3751 (9.4366)
2022-11-08 17:34:01,085:INFO: Dataset: univ                Batch:  9/29	Loss 9.3737 (9.4294)
2022-11-08 17:34:01,088:INFO: Dataset: univ                Batch: 10/29	Loss 9.1005 (9.3970)
2022-11-08 17:34:01,092:INFO: Dataset: univ                Batch: 11/29	Loss 9.4549 (9.4018)
2022-11-08 17:34:01,096:INFO: Dataset: univ                Batch: 12/29	Loss 9.2206 (9.3869)
2022-11-08 17:34:01,101:INFO: Dataset: univ                Batch: 13/29	Loss 8.5610 (9.3113)
2022-11-08 17:34:01,104:INFO: Dataset: univ                Batch: 14/29	Loss 8.7033 (9.2656)
2022-11-08 17:34:01,107:INFO: Dataset: univ                Batch: 15/29	Loss 8.4357 (9.2224)
2022-11-08 17:34:01,111:INFO: Dataset: univ                Batch: 16/29	Loss 8.3431 (9.1651)
2022-11-08 17:34:01,114:INFO: Dataset: univ                Batch: 17/29	Loss 8.1255 (9.1004)
2022-11-08 17:34:01,117:INFO: Dataset: univ                Batch: 18/29	Loss 8.2853 (9.0558)
2022-11-08 17:34:01,121:INFO: Dataset: univ                Batch: 19/29	Loss 7.7242 (8.9881)
2022-11-08 17:34:01,125:INFO: Dataset: univ                Batch: 20/29	Loss 7.8830 (8.9307)
2022-11-08 17:34:01,130:INFO: Dataset: univ                Batch: 21/29	Loss 7.4717 (8.8519)
2022-11-08 17:34:01,134:INFO: Dataset: univ                Batch: 22/29	Loss 7.6034 (8.7904)
2022-11-08 17:34:01,138:INFO: Dataset: univ                Batch: 23/29	Loss 7.1858 (8.7091)
2022-11-08 17:34:01,143:INFO: Dataset: univ                Batch: 24/29	Loss 7.4517 (8.6553)
2022-11-08 17:34:01,147:INFO: Dataset: univ                Batch: 25/29	Loss 7.2256 (8.5937)
2022-11-08 17:34:01,151:INFO: Dataset: univ                Batch: 26/29	Loss 6.7095 (8.5059)
2022-11-08 17:34:01,155:INFO: Dataset: univ                Batch: 27/29	Loss 6.6631 (8.4393)
2022-11-08 17:34:01,161:INFO: Dataset: univ                Batch: 28/29	Loss 6.7118 (8.3776)
2022-11-08 17:34:01,164:INFO: Dataset: univ                Batch: 29/29	Loss 6.9407 (8.3638)
2022-11-08 17:34:08,342:INFO: Dataset: zara1               Batch:  1/16	Loss 12.6300 (12.6300)
2022-11-08 17:34:08,730:INFO: Dataset: zara1               Batch:  2/16	Loss 13.1015 (12.8823)
2022-11-08 17:34:09,131:INFO: Dataset: zara1               Batch:  3/16	Loss 13.1207 (12.9795)
2022-11-08 17:34:09,383:INFO: Dataset: zara1               Batch:  4/16	Loss 13.6253 (13.1508)
2022-11-08 17:34:09,721:INFO: Dataset: zara1               Batch:  5/16	Loss 12.7893 (13.0872)
2022-11-08 17:34:09,976:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0023 (13.0729)
2022-11-08 17:34:09,981:INFO: Dataset: zara1               Batch:  7/16	Loss 12.8111 (13.0339)
2022-11-08 17:34:09,984:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8808 (13.0150)
2022-11-08 17:34:09,986:INFO: Dataset: zara1               Batch:  9/16	Loss 12.9308 (13.0077)
2022-11-08 17:34:09,989:INFO: Dataset: zara1               Batch: 10/16	Loss 13.1583 (13.0231)
2022-11-08 17:34:09,992:INFO: Dataset: zara1               Batch: 11/16	Loss 12.3119 (12.9688)
2022-11-08 17:34:09,996:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3929 (12.9240)
2022-11-08 17:34:09,999:INFO: Dataset: zara1               Batch: 13/16	Loss 12.4136 (12.8872)
2022-11-08 17:34:10,004:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4932 (12.8596)
2022-11-08 17:34:10,008:INFO: Dataset: zara1               Batch: 15/16	Loss 12.1656 (12.8166)
2022-11-08 17:34:10,011:INFO: Dataset: zara1               Batch: 16/16	Loss 11.8627 (12.7714)
2022-11-08 17:34:34,624:INFO: Dataset: zara2               Batch:  1/36	Loss 6.2490 (6.2490)
2022-11-08 17:34:34,629:INFO: Dataset: zara2               Batch:  2/36	Loss 5.6939 (5.9967)
2022-11-08 17:34:34,634:INFO: Dataset: zara2               Batch:  3/36	Loss 6.3906 (6.1285)
2022-11-08 17:34:34,638:INFO: Dataset: zara2               Batch:  4/36	Loss 6.4264 (6.1973)
2022-11-08 17:34:34,642:INFO: Dataset: zara2               Batch:  5/36	Loss 6.5725 (6.2755)
2022-11-08 17:34:34,647:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1980 (6.2643)
2022-11-08 17:34:34,650:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5225 (6.2994)
2022-11-08 17:34:34,654:INFO: Dataset: zara2               Batch:  8/36	Loss 6.4337 (6.3157)
2022-11-08 17:34:34,658:INFO: Dataset: zara2               Batch:  9/36	Loss 6.2721 (6.3110)
2022-11-08 17:34:34,663:INFO: Dataset: zara2               Batch: 10/36	Loss 6.2883 (6.3088)
2022-11-08 17:34:34,666:INFO: Dataset: zara2               Batch: 11/36	Loss 6.0931 (6.2878)
2022-11-08 17:34:34,671:INFO: Dataset: zara2               Batch: 12/36	Loss 6.1476 (6.2763)
2022-11-08 17:34:34,674:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0742 (6.2592)
2022-11-08 17:34:34,678:INFO: Dataset: zara2               Batch: 14/36	Loss 5.8759 (6.2342)
2022-11-08 17:34:34,681:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5338 (6.1920)
2022-11-08 17:34:34,684:INFO: Dataset: zara2               Batch: 16/36	Loss 5.7747 (6.1640)
2022-11-08 17:34:34,687:INFO: Dataset: zara2               Batch: 17/36	Loss 6.0217 (6.1556)
2022-11-08 17:34:34,690:INFO: Dataset: zara2               Batch: 18/36	Loss 5.7545 (6.1343)
2022-11-08 17:34:34,695:INFO: Dataset: zara2               Batch: 19/36	Loss 5.3135 (6.0876)
2022-11-08 17:34:34,698:INFO: Dataset: zara2               Batch: 20/36	Loss 5.2404 (6.0479)
2022-11-08 17:34:34,702:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4167 (6.0202)
2022-11-08 17:34:34,706:INFO: Dataset: zara2               Batch: 22/36	Loss 5.6224 (6.0008)
2022-11-08 17:34:34,709:INFO: Dataset: zara2               Batch: 23/36	Loss 5.5527 (5.9805)
2022-11-08 17:34:34,712:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9936 (5.9429)
2022-11-08 17:34:34,716:INFO: Dataset: zara2               Batch: 25/36	Loss 5.0755 (5.9043)
2022-11-08 17:34:34,721:INFO: Dataset: zara2               Batch: 26/36	Loss 5.1130 (5.8694)
2022-11-08 17:34:34,724:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7532 (5.8236)
2022-11-08 17:34:34,728:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7591 (5.7895)
2022-11-08 17:34:34,733:INFO: Dataset: zara2               Batch: 29/36	Loss 4.7400 (5.7517)
2022-11-08 17:34:34,736:INFO: Dataset: zara2               Batch: 30/36	Loss 4.8852 (5.7191)
2022-11-08 17:34:34,740:INFO: Dataset: zara2               Batch: 31/36	Loss 4.7263 (5.6847)
2022-11-08 17:34:34,743:INFO: Dataset: zara2               Batch: 32/36	Loss 4.9599 (5.6633)
2022-11-08 17:34:34,747:INFO: Dataset: zara2               Batch: 33/36	Loss 4.5091 (5.6270)
2022-11-08 17:34:34,751:INFO: Dataset: zara2               Batch: 34/36	Loss 4.6184 (5.5947)
2022-11-08 17:34:34,754:INFO: Dataset: zara2               Batch: 35/36	Loss 4.3968 (5.5633)
2022-11-08 17:34:34,757:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1776 (5.5374)
2022-11-08 17:34:35,725:INFO: - Computing loss (validation)
2022-11-08 17:34:41,996:INFO: Dataset: hotel               Batch: 1/3	Loss 17.0817 (17.0817)
2022-11-08 17:34:42,443:INFO: Dataset: hotel               Batch: 2/3	Loss 17.0844 (17.0830)
2022-11-08 17:34:42,858:INFO: Dataset: hotel               Batch: 3/3	Loss 15.8408 (17.0025)
2022-11-08 17:34:50,376:INFO: Dataset: univ                Batch: 1/6	Loss 9.2591 (9.2591)
2022-11-08 17:34:50,712:INFO: Dataset: univ                Batch: 2/6	Loss 8.8205 (9.0132)
2022-11-08 17:34:51,148:INFO: Dataset: univ                Batch: 3/6	Loss 8.8512 (8.9618)
2022-11-08 17:34:51,425:INFO: Dataset: univ                Batch: 4/6	Loss 8.3148 (8.7715)
2022-11-08 17:34:51,626:INFO: Dataset: univ                Batch: 5/6	Loss 9.1621 (8.8388)
2022-11-08 17:34:51,871:INFO: Dataset: univ                Batch: 6/6	Loss 9.0461 (8.8659)
2022-11-08 17:34:58,889:INFO: Dataset: zara1               Batch: 1/3	Loss 11.1498 (11.1498)
2022-11-08 17:34:59,217:INFO: Dataset: zara1               Batch: 2/3	Loss 10.9596 (11.0531)
2022-11-08 17:34:59,551:INFO: Dataset: zara1               Batch: 3/3	Loss 11.6315 (11.1889)
2022-11-08 17:35:07,674:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2795 (4.2795)
2022-11-08 17:35:08,035:INFO: Dataset: zara2               Batch:  2/10	Loss 4.3175 (4.2987)
2022-11-08 17:35:08,465:INFO: Dataset: zara2               Batch:  3/10	Loss 4.3015 (4.2996)
2022-11-08 17:35:08,736:INFO: Dataset: zara2               Batch:  4/10	Loss 4.4793 (4.3453)
2022-11-08 17:35:08,940:INFO: Dataset: zara2               Batch:  5/10	Loss 4.7337 (4.4239)
2022-11-08 17:35:09,174:INFO: Dataset: zara2               Batch:  6/10	Loss 4.4173 (4.4228)
2022-11-08 17:35:09,175:INFO: Dataset: zara2               Batch:  7/10	Loss 4.4488 (4.4266)
2022-11-08 17:35:09,176:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3400 (4.4141)
2022-11-08 17:35:09,177:INFO: Dataset: zara2               Batch:  9/10	Loss 4.5315 (4.4262)
2022-11-08 17:35:09,180:INFO: Dataset: zara2               Batch: 10/10	Loss 4.3382 (4.4178)
2022-11-08 17:35:10,151:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1053.pth.tar
2022-11-08 17:35:10,151:INFO: 
===> EPOCH: 1054 (P5)
2022-11-08 17:35:10,152:INFO: - Computing loss (training)
2022-11-08 17:35:17,470:INFO: Dataset: hotel               Batch: 1/8	Loss 14.9639 (14.9639)
2022-11-08 17:35:17,917:INFO: Dataset: hotel               Batch: 2/8	Loss 14.8811 (14.9221)
2022-11-08 17:35:18,190:INFO: Dataset: hotel               Batch: 3/8	Loss 14.8177 (14.8905)
2022-11-08 17:35:18,505:INFO: Dataset: hotel               Batch: 4/8	Loss 15.5123 (15.0467)
2022-11-08 17:35:18,707:INFO: Dataset: hotel               Batch: 5/8	Loss 14.0771 (14.8505)
2022-11-08 17:35:18,962:INFO: Dataset: hotel               Batch: 6/8	Loss 12.4467 (14.4300)
2022-11-08 17:35:18,969:INFO: Dataset: hotel               Batch: 7/8	Loss 13.9724 (14.3597)
2022-11-08 17:35:18,972:INFO: Dataset: hotel               Batch: 8/8	Loss 13.2211 (14.3267)
2022-11-08 17:35:44,692:INFO: Dataset: univ                Batch:  1/29	Loss 9.1161 (9.1161)
2022-11-08 17:35:44,697:INFO: Dataset: univ                Batch:  2/29	Loss 8.9878 (9.0521)
2022-11-08 17:35:44,703:INFO: Dataset: univ                Batch:  3/29	Loss 9.0129 (9.0393)
2022-11-08 17:35:44,707:INFO: Dataset: univ                Batch:  4/29	Loss 9.1955 (9.0752)
2022-11-08 17:35:44,710:INFO: Dataset: univ                Batch:  5/29	Loss 9.2703 (9.1160)
2022-11-08 17:35:44,734:INFO: Dataset: univ                Batch:  6/29	Loss 9.2464 (9.1376)
2022-11-08 17:35:44,736:INFO: Dataset: univ                Batch:  7/29	Loss 9.2637 (9.1546)
2022-11-08 17:35:44,739:INFO: Dataset: univ                Batch:  8/29	Loss 9.2844 (9.1688)
2022-11-08 17:35:44,743:INFO: Dataset: univ                Batch:  9/29	Loss 9.0751 (9.1585)
2022-11-08 17:35:44,745:INFO: Dataset: univ                Batch: 10/29	Loss 8.9604 (9.1383)
2022-11-08 17:35:44,747:INFO: Dataset: univ                Batch: 11/29	Loss 8.4314 (9.0695)
2022-11-08 17:35:44,751:INFO: Dataset: univ                Batch: 12/29	Loss 8.6255 (9.0331)
2022-11-08 17:35:44,754:INFO: Dataset: univ                Batch: 13/29	Loss 8.8718 (9.0211)
2022-11-08 17:35:44,758:INFO: Dataset: univ                Batch: 14/29	Loss 8.3023 (8.9717)
2022-11-08 17:35:44,760:INFO: Dataset: univ                Batch: 15/29	Loss 7.9670 (8.8976)
2022-11-08 17:35:44,763:INFO: Dataset: univ                Batch: 16/29	Loss 8.4544 (8.8723)
2022-11-08 17:35:44,765:INFO: Dataset: univ                Batch: 17/29	Loss 8.4054 (8.8467)
2022-11-08 17:35:44,769:INFO: Dataset: univ                Batch: 18/29	Loss 7.9214 (8.7946)
2022-11-08 17:35:44,774:INFO: Dataset: univ                Batch: 19/29	Loss 7.7787 (8.7426)
2022-11-08 17:35:44,778:INFO: Dataset: univ                Batch: 20/29	Loss 7.5250 (8.6821)
2022-11-08 17:35:44,781:INFO: Dataset: univ                Batch: 21/29	Loss 7.8112 (8.6477)
2022-11-08 17:35:44,784:INFO: Dataset: univ                Batch: 22/29	Loss 7.7855 (8.6153)
2022-11-08 17:35:44,786:INFO: Dataset: univ                Batch: 23/29	Loss 7.3888 (8.5634)
2022-11-08 17:35:44,789:INFO: Dataset: univ                Batch: 24/29	Loss 7.4036 (8.5148)
2022-11-08 17:35:44,793:INFO: Dataset: univ                Batch: 25/29	Loss 7.2362 (8.4617)
2022-11-08 17:35:44,796:INFO: Dataset: univ                Batch: 26/29	Loss 7.0700 (8.4036)
2022-11-08 17:35:44,799:INFO: Dataset: univ                Batch: 27/29	Loss 6.5895 (8.3345)
2022-11-08 17:35:44,802:INFO: Dataset: univ                Batch: 28/29	Loss 6.7607 (8.2793)
2022-11-08 17:35:44,805:INFO: Dataset: univ                Batch: 29/29	Loss 6.5601 (8.2539)
2022-11-08 17:35:52,473:INFO: Dataset: zara1               Batch:  1/16	Loss 13.0357 (13.0357)
2022-11-08 17:35:52,882:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2403 (13.1419)
2022-11-08 17:35:53,282:INFO: Dataset: zara1               Batch:  3/16	Loss 13.3464 (13.2150)
2022-11-08 17:35:53,635:INFO: Dataset: zara1               Batch:  4/16	Loss 13.1987 (13.2111)
2022-11-08 17:35:53,880:INFO: Dataset: zara1               Batch:  5/16	Loss 13.4700 (13.2644)
2022-11-08 17:35:54,072:INFO: Dataset: zara1               Batch:  6/16	Loss 13.2658 (13.2646)
2022-11-08 17:35:54,075:INFO: Dataset: zara1               Batch:  7/16	Loss 13.2515 (13.2627)
2022-11-08 17:35:54,078:INFO: Dataset: zara1               Batch:  8/16	Loss 13.2858 (13.2659)
2022-11-08 17:35:54,081:INFO: Dataset: zara1               Batch:  9/16	Loss 12.7870 (13.2106)
2022-11-08 17:35:54,084:INFO: Dataset: zara1               Batch: 10/16	Loss 12.4969 (13.1404)
2022-11-08 17:35:54,088:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5421 (13.0916)
2022-11-08 17:35:54,092:INFO: Dataset: zara1               Batch: 12/16	Loss 13.3391 (13.1087)
2022-11-08 17:35:54,096:INFO: Dataset: zara1               Batch: 13/16	Loss 12.3930 (13.0527)
2022-11-08 17:35:54,104:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3065 (13.0023)
2022-11-08 17:35:54,108:INFO: Dataset: zara1               Batch: 15/16	Loss 11.6856 (12.9142)
2022-11-08 17:35:54,112:INFO: Dataset: zara1               Batch: 16/16	Loss 12.2763 (12.8883)
2022-11-08 17:36:18,398:INFO: Dataset: zara2               Batch:  1/36	Loss 5.9067 (5.9067)
2022-11-08 17:36:18,402:INFO: Dataset: zara2               Batch:  2/36	Loss 6.7460 (6.3628)
2022-11-08 17:36:18,405:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4638 (6.3962)
2022-11-08 17:36:18,409:INFO: Dataset: zara2               Batch:  4/36	Loss 6.0376 (6.3127)
2022-11-08 17:36:18,412:INFO: Dataset: zara2               Batch:  5/36	Loss 6.2718 (6.3043)
2022-11-08 17:36:18,444:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1080 (6.2700)
2022-11-08 17:36:18,446:INFO: Dataset: zara2               Batch:  7/36	Loss 6.3678 (6.2819)
2022-11-08 17:36:18,449:INFO: Dataset: zara2               Batch:  8/36	Loss 6.5661 (6.3106)
2022-11-08 17:36:18,454:INFO: Dataset: zara2               Batch:  9/36	Loss 5.8800 (6.2620)
2022-11-08 17:36:18,456:INFO: Dataset: zara2               Batch: 10/36	Loss 5.8046 (6.2219)
2022-11-08 17:36:18,459:INFO: Dataset: zara2               Batch: 11/36	Loss 5.8553 (6.1929)
2022-11-08 17:36:18,464:INFO: Dataset: zara2               Batch: 12/36	Loss 6.5885 (6.2286)
2022-11-08 17:36:18,467:INFO: Dataset: zara2               Batch: 13/36	Loss 5.9509 (6.2057)
2022-11-08 17:36:18,471:INFO: Dataset: zara2               Batch: 14/36	Loss 5.7524 (6.1734)
2022-11-08 17:36:18,473:INFO: Dataset: zara2               Batch: 15/36	Loss 5.6205 (6.1346)
2022-11-08 17:36:18,476:INFO: Dataset: zara2               Batch: 16/36	Loss 5.6163 (6.1073)
2022-11-08 17:36:18,479:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6384 (6.0806)
2022-11-08 17:36:18,483:INFO: Dataset: zara2               Batch: 18/36	Loss 5.3711 (6.0436)
2022-11-08 17:36:18,486:INFO: Dataset: zara2               Batch: 19/36	Loss 5.6153 (6.0231)
2022-11-08 17:36:18,489:INFO: Dataset: zara2               Batch: 20/36	Loss 5.5455 (5.9999)
2022-11-08 17:36:18,492:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3559 (5.9678)
2022-11-08 17:36:18,495:INFO: Dataset: zara2               Batch: 22/36	Loss 5.4667 (5.9446)
2022-11-08 17:36:18,498:INFO: Dataset: zara2               Batch: 23/36	Loss 5.1301 (5.9079)
2022-11-08 17:36:18,501:INFO: Dataset: zara2               Batch: 24/36	Loss 5.0758 (5.8745)
2022-11-08 17:36:18,504:INFO: Dataset: zara2               Batch: 25/36	Loss 4.6721 (5.8249)
2022-11-08 17:36:18,508:INFO: Dataset: zara2               Batch: 26/36	Loss 4.4934 (5.7819)
2022-11-08 17:36:18,511:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9053 (5.7456)
2022-11-08 17:36:18,513:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7679 (5.7144)
2022-11-08 17:36:18,517:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6084 (5.6716)
2022-11-08 17:36:18,521:INFO: Dataset: zara2               Batch: 30/36	Loss 4.7605 (5.6436)
2022-11-08 17:36:18,523:INFO: Dataset: zara2               Batch: 31/36	Loss 4.7205 (5.6150)
2022-11-08 17:36:18,527:INFO: Dataset: zara2               Batch: 32/36	Loss 4.5158 (5.5824)
2022-11-08 17:36:18,530:INFO: Dataset: zara2               Batch: 33/36	Loss 4.4660 (5.5463)
2022-11-08 17:36:18,534:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3425 (5.5070)
2022-11-08 17:36:18,536:INFO: Dataset: zara2               Batch: 35/36	Loss 4.4155 (5.4709)
2022-11-08 17:36:18,539:INFO: Dataset: zara2               Batch: 36/36	Loss 4.4748 (5.4502)
2022-11-08 17:36:19,541:INFO: - Computing loss (validation)
2022-11-08 17:36:26,163:INFO: Dataset: hotel               Batch: 1/3	Loss 16.7821 (16.7821)
2022-11-08 17:36:26,563:INFO: Dataset: hotel               Batch: 2/3	Loss 17.5392 (17.1565)
2022-11-08 17:36:26,922:INFO: Dataset: hotel               Batch: 3/3	Loss 19.1416 (17.3055)
2022-11-08 17:36:34,894:INFO: Dataset: univ                Batch: 1/6	Loss 9.0298 (9.0298)
2022-11-08 17:36:35,358:INFO: Dataset: univ                Batch: 2/6	Loss 9.1356 (9.0800)
2022-11-08 17:36:35,724:INFO: Dataset: univ                Batch: 3/6	Loss 9.3540 (9.1630)
2022-11-08 17:36:36,028:INFO: Dataset: univ                Batch: 4/6	Loss 8.6635 (9.0312)
2022-11-08 17:36:36,272:INFO: Dataset: univ                Batch: 5/6	Loss 8.7321 (8.9753)
2022-11-08 17:36:36,519:INFO: Dataset: univ                Batch: 6/6	Loss 8.9478 (8.9710)
2022-11-08 17:36:44,585:INFO: Dataset: zara1               Batch: 1/3	Loss 11.3622 (11.3622)
2022-11-08 17:36:45,169:INFO: Dataset: zara1               Batch: 2/3	Loss 11.3198 (11.3417)
2022-11-08 17:36:45,621:INFO: Dataset: zara1               Batch: 3/3	Loss 11.1341 (11.2956)
2022-11-08 17:36:53,113:INFO: Dataset: zara2               Batch:  1/10	Loss 3.9928 (3.9928)
2022-11-08 17:36:53,508:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1563 (4.0737)
2022-11-08 17:36:53,840:INFO: Dataset: zara2               Batch:  3/10	Loss 4.3552 (4.1695)
2022-11-08 17:36:54,185:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3100 (4.2032)
2022-11-08 17:36:54,432:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3036 (4.2252)
2022-11-08 17:36:54,596:INFO: Dataset: zara2               Batch:  6/10	Loss 4.3015 (4.2373)
2022-11-08 17:36:54,598:INFO: Dataset: zara2               Batch:  7/10	Loss 4.5325 (4.2820)
2022-11-08 17:36:54,600:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3662 (4.2925)
2022-11-08 17:36:54,602:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2761 (4.2908)
2022-11-08 17:36:54,603:INFO: Dataset: zara2               Batch: 10/10	Loss 4.4765 (4.3102)
2022-11-08 17:36:55,531:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1054.pth.tar
2022-11-08 17:36:55,531:INFO: 
===> EPOCH: 1055 (P5)
2022-11-08 17:36:55,532:INFO: - Computing loss (training)
2022-11-08 17:37:03,276:INFO: Dataset: hotel               Batch: 1/8	Loss 14.4073 (14.4073)
2022-11-08 17:37:03,685:INFO: Dataset: hotel               Batch: 2/8	Loss 16.0171 (15.2394)
2022-11-08 17:37:04,067:INFO: Dataset: hotel               Batch: 3/8	Loss 14.1780 (14.8844)
2022-11-08 17:37:04,392:INFO: Dataset: hotel               Batch: 4/8	Loss 14.0421 (14.6734)
2022-11-08 17:37:04,689:INFO: Dataset: hotel               Batch: 5/8	Loss 14.2815 (14.5924)
2022-11-08 17:37:04,933:INFO: Dataset: hotel               Batch: 6/8	Loss 13.8358 (14.4580)
2022-11-08 17:37:04,936:INFO: Dataset: hotel               Batch: 7/8	Loss 15.2039 (14.5611)
2022-11-08 17:37:04,939:INFO: Dataset: hotel               Batch: 8/8	Loss 12.7378 (14.5130)
2022-11-08 17:37:27,873:INFO: Dataset: univ                Batch:  1/29	Loss 9.4719 (9.4719)
2022-11-08 17:37:27,879:INFO: Dataset: univ                Batch:  2/29	Loss 9.3844 (9.4255)
2022-11-08 17:37:27,885:INFO: Dataset: univ                Batch:  3/29	Loss 9.6463 (9.5001)
2022-11-08 17:37:27,890:INFO: Dataset: univ                Batch:  4/29	Loss 9.6064 (9.5275)
2022-11-08 17:37:27,894:INFO: Dataset: univ                Batch:  5/29	Loss 9.6237 (9.5445)
2022-11-08 17:37:27,914:INFO: Dataset: univ                Batch:  6/29	Loss 9.4620 (9.5318)
2022-11-08 17:37:27,919:INFO: Dataset: univ                Batch:  7/29	Loss 8.8941 (9.4249)
2022-11-08 17:37:27,926:INFO: Dataset: univ                Batch:  8/29	Loss 8.9521 (9.3616)
2022-11-08 17:37:27,931:INFO: Dataset: univ                Batch:  9/29	Loss 9.1499 (9.3330)
2022-11-08 17:37:27,936:INFO: Dataset: univ                Batch: 10/29	Loss 8.8753 (9.2868)
2022-11-08 17:37:27,939:INFO: Dataset: univ                Batch: 11/29	Loss 9.1695 (9.2779)
2022-11-08 17:37:27,945:INFO: Dataset: univ                Batch: 12/29	Loss 8.7898 (9.2382)
2022-11-08 17:37:27,950:INFO: Dataset: univ                Batch: 13/29	Loss 8.9118 (9.2123)
2022-11-08 17:37:27,955:INFO: Dataset: univ                Batch: 14/29	Loss 8.7278 (9.1781)
2022-11-08 17:37:27,961:INFO: Dataset: univ                Batch: 15/29	Loss 8.5346 (9.1371)
2022-11-08 17:37:27,966:INFO: Dataset: univ                Batch: 16/29	Loss 8.4829 (9.0965)
2022-11-08 17:37:27,972:INFO: Dataset: univ                Batch: 17/29	Loss 8.1184 (9.0396)
2022-11-08 17:37:27,978:INFO: Dataset: univ                Batch: 18/29	Loss 7.8988 (8.9676)
2022-11-08 17:37:27,982:INFO: Dataset: univ                Batch: 19/29	Loss 7.8612 (8.9074)
2022-11-08 17:37:27,989:INFO: Dataset: univ                Batch: 20/29	Loss 7.7880 (8.8502)
2022-11-08 17:37:27,993:INFO: Dataset: univ                Batch: 21/29	Loss 7.9060 (8.8088)
2022-11-08 17:37:27,998:INFO: Dataset: univ                Batch: 22/29	Loss 7.5384 (8.7514)
2022-11-08 17:37:28,005:INFO: Dataset: univ                Batch: 23/29	Loss 7.5794 (8.6993)
2022-11-08 17:37:28,011:INFO: Dataset: univ                Batch: 24/29	Loss 7.4121 (8.6496)
2022-11-08 17:37:28,017:INFO: Dataset: univ                Batch: 25/29	Loss 7.2435 (8.5913)
2022-11-08 17:37:28,023:INFO: Dataset: univ                Batch: 26/29	Loss 7.1504 (8.5402)
2022-11-08 17:37:28,030:INFO: Dataset: univ                Batch: 27/29	Loss 7.0343 (8.4876)
2022-11-08 17:37:28,034:INFO: Dataset: univ                Batch: 28/29	Loss 6.9032 (8.4360)
2022-11-08 17:37:28,038:INFO: Dataset: univ                Batch: 29/29	Loss 6.6831 (8.4100)
2022-11-08 17:37:34,906:INFO: Dataset: zara1               Batch:  1/16	Loss 13.3201 (13.3201)
2022-11-08 17:37:35,290:INFO: Dataset: zara1               Batch:  2/16	Loss 13.0307 (13.1905)
2022-11-08 17:37:35,648:INFO: Dataset: zara1               Batch:  3/16	Loss 13.2416 (13.2098)
2022-11-08 17:37:36,002:INFO: Dataset: zara1               Batch:  4/16	Loss 12.9026 (13.1256)
2022-11-08 17:37:36,180:INFO: Dataset: zara1               Batch:  5/16	Loss 13.1854 (13.1375)
2022-11-08 17:37:36,415:INFO: Dataset: zara1               Batch:  6/16	Loss 13.3818 (13.1852)
2022-11-08 17:37:36,419:INFO: Dataset: zara1               Batch:  7/16	Loss 13.3413 (13.2083)
2022-11-08 17:37:36,422:INFO: Dataset: zara1               Batch:  8/16	Loss 12.5613 (13.1360)
2022-11-08 17:37:36,425:INFO: Dataset: zara1               Batch:  9/16	Loss 12.7883 (13.0934)
2022-11-08 17:37:36,427:INFO: Dataset: zara1               Batch: 10/16	Loss 12.9479 (13.0793)
2022-11-08 17:37:36,430:INFO: Dataset: zara1               Batch: 11/16	Loss 12.2696 (13.0156)
2022-11-08 17:37:36,436:INFO: Dataset: zara1               Batch: 12/16	Loss 12.1433 (12.9525)
2022-11-08 17:37:36,440:INFO: Dataset: zara1               Batch: 13/16	Loss 12.5721 (12.9275)
2022-11-08 17:37:36,445:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4117 (12.8923)
2022-11-08 17:37:36,449:INFO: Dataset: zara1               Batch: 15/16	Loss 12.5057 (12.8662)
2022-11-08 17:37:36,453:INFO: Dataset: zara1               Batch: 16/16	Loss 11.8007 (12.8224)
2022-11-08 17:37:58,990:INFO: Dataset: zara2               Batch:  1/36	Loss 5.8580 (5.8580)
2022-11-08 17:37:58,994:INFO: Dataset: zara2               Batch:  2/36	Loss 6.0405 (5.9579)
2022-11-08 17:37:58,998:INFO: Dataset: zara2               Batch:  3/36	Loss 6.3129 (6.0885)
2022-11-08 17:37:59,001:INFO: Dataset: zara2               Batch:  4/36	Loss 6.1190 (6.0955)
2022-11-08 17:37:59,005:INFO: Dataset: zara2               Batch:  5/36	Loss 6.5398 (6.1742)
2022-11-08 17:37:59,027:INFO: Dataset: zara2               Batch:  6/36	Loss 5.9961 (6.1422)
2022-11-08 17:37:59,033:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0244 (6.1258)
2022-11-08 17:37:59,038:INFO: Dataset: zara2               Batch:  8/36	Loss 5.8777 (6.0890)
2022-11-08 17:37:59,042:INFO: Dataset: zara2               Batch:  9/36	Loss 6.1630 (6.0975)
2022-11-08 17:37:59,047:INFO: Dataset: zara2               Batch: 10/36	Loss 5.9575 (6.0866)
2022-11-08 17:37:59,051:INFO: Dataset: zara2               Batch: 11/36	Loss 5.3980 (6.0295)
2022-11-08 17:37:59,056:INFO: Dataset: zara2               Batch: 12/36	Loss 6.1707 (6.0410)
2022-11-08 17:37:59,060:INFO: Dataset: zara2               Batch: 13/36	Loss 5.9018 (6.0312)
2022-11-08 17:37:59,064:INFO: Dataset: zara2               Batch: 14/36	Loss 5.9822 (6.0275)
2022-11-08 17:37:59,069:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5266 (5.9929)
2022-11-08 17:37:59,074:INFO: Dataset: zara2               Batch: 16/36	Loss 5.4128 (5.9572)
2022-11-08 17:37:59,080:INFO: Dataset: zara2               Batch: 17/36	Loss 5.3019 (5.9156)
2022-11-08 17:37:59,085:INFO: Dataset: zara2               Batch: 18/36	Loss 5.4342 (5.8902)
2022-11-08 17:37:59,088:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4813 (5.8700)
2022-11-08 17:37:59,092:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4625 (5.8511)
2022-11-08 17:37:59,099:INFO: Dataset: zara2               Batch: 21/36	Loss 5.2800 (5.8228)
2022-11-08 17:37:59,105:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9662 (5.7821)
2022-11-08 17:37:59,110:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9967 (5.7429)
2022-11-08 17:37:59,114:INFO: Dataset: zara2               Batch: 24/36	Loss 5.1887 (5.7214)
2022-11-08 17:37:59,119:INFO: Dataset: zara2               Batch: 25/36	Loss 5.0383 (5.6968)
2022-11-08 17:37:59,124:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8873 (5.6671)
2022-11-08 17:37:59,130:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9796 (5.6417)
2022-11-08 17:37:59,135:INFO: Dataset: zara2               Batch: 28/36	Loss 4.9263 (5.6173)
2022-11-08 17:37:59,138:INFO: Dataset: zara2               Batch: 29/36	Loss 4.4947 (5.5836)
2022-11-08 17:37:59,141:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5368 (5.5409)
2022-11-08 17:37:59,146:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3835 (5.5100)
2022-11-08 17:37:59,149:INFO: Dataset: zara2               Batch: 32/36	Loss 4.5894 (5.4782)
2022-11-08 17:37:59,153:INFO: Dataset: zara2               Batch: 33/36	Loss 4.3622 (5.4424)
2022-11-08 17:37:59,158:INFO: Dataset: zara2               Batch: 34/36	Loss 4.4026 (5.4150)
2022-11-08 17:37:59,161:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0841 (5.3795)
2022-11-08 17:37:59,165:INFO: Dataset: zara2               Batch: 36/36	Loss 4.4589 (5.3603)
2022-11-08 17:38:00,078:INFO: - Computing loss (validation)
2022-11-08 17:38:06,057:INFO: Dataset: hotel               Batch: 1/3	Loss 16.6129 (16.6129)
2022-11-08 17:38:06,353:INFO: Dataset: hotel               Batch: 2/3	Loss 16.9305 (16.7848)
2022-11-08 17:38:06,709:INFO: Dataset: hotel               Batch: 3/3	Loss 20.7157 (16.9726)
2022-11-08 17:38:14,219:INFO: Dataset: univ                Batch: 1/6	Loss 8.7229 (8.7229)
2022-11-08 17:38:14,622:INFO: Dataset: univ                Batch: 2/6	Loss 9.4124 (9.0297)
2022-11-08 17:38:14,911:INFO: Dataset: univ                Batch: 3/6	Loss 9.2272 (9.0960)
2022-11-08 17:38:15,246:INFO: Dataset: univ                Batch: 4/6	Loss 8.6489 (8.9847)
2022-11-08 17:38:15,496:INFO: Dataset: univ                Batch: 5/6	Loss 8.9170 (8.9721)
2022-11-08 17:38:15,741:INFO: Dataset: univ                Batch: 6/6	Loss 9.4010 (9.0248)
2022-11-08 17:38:22,609:INFO: Dataset: zara1               Batch: 1/3	Loss 11.8460 (11.8460)
2022-11-08 17:38:22,978:INFO: Dataset: zara1               Batch: 2/3	Loss 11.2227 (11.5411)
2022-11-08 17:38:23,319:INFO: Dataset: zara1               Batch: 3/3	Loss 11.5010 (11.5308)
2022-11-08 17:38:30,796:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2073 (4.2073)
2022-11-08 17:38:31,179:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1490 (4.1788)
2022-11-08 17:38:31,546:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2256 (4.1929)
2022-11-08 17:38:31,863:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3565 (4.2357)
2022-11-08 17:38:32,107:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1708 (4.2219)
2022-11-08 17:38:32,276:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2223 (4.2219)
2022-11-08 17:38:32,278:INFO: Dataset: zara2               Batch:  7/10	Loss 4.4106 (4.2475)
2022-11-08 17:38:32,279:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2014 (4.2418)
2022-11-08 17:38:32,281:INFO: Dataset: zara2               Batch:  9/10	Loss 4.1458 (4.2310)
2022-11-08 17:38:32,283:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2955 (4.2376)
2022-11-08 17:38:33,224:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1055.pth.tar
2022-11-08 17:38:33,224:INFO: 
===> EPOCH: 1056 (P5)
2022-11-08 17:38:33,226:INFO: - Computing loss (training)
2022-11-08 17:38:41,397:INFO: Dataset: hotel               Batch: 1/8	Loss 15.4240 (15.4240)
2022-11-08 17:38:41,826:INFO: Dataset: hotel               Batch: 2/8	Loss 15.9753 (15.6902)
2022-11-08 17:38:42,256:INFO: Dataset: hotel               Batch: 3/8	Loss 13.4594 (14.9112)
2022-11-08 17:38:42,482:INFO: Dataset: hotel               Batch: 4/8	Loss 14.9950 (14.9312)
2022-11-08 17:38:42,838:INFO: Dataset: hotel               Batch: 5/8	Loss 14.9347 (14.9320)
2022-11-08 17:38:43,043:INFO: Dataset: hotel               Batch: 6/8	Loss 12.1108 (14.4759)
2022-11-08 17:38:43,045:INFO: Dataset: hotel               Batch: 7/8	Loss 12.8312 (14.2490)
2022-11-08 17:38:43,049:INFO: Dataset: hotel               Batch: 8/8	Loss 16.8063 (14.3367)
2022-11-08 17:39:06,792:INFO: Dataset: univ                Batch:  1/29	Loss 9.7568 (9.7568)
2022-11-08 17:39:06,796:INFO: Dataset: univ                Batch:  2/29	Loss 9.3675 (9.5436)
2022-11-08 17:39:06,802:INFO: Dataset: univ                Batch:  3/29	Loss 9.6020 (9.5636)
2022-11-08 17:39:06,807:INFO: Dataset: univ                Batch:  4/29	Loss 9.1474 (9.4611)
2022-11-08 17:39:06,819:INFO: Dataset: univ                Batch:  5/29	Loss 9.5302 (9.4771)
2022-11-08 17:39:06,830:INFO: Dataset: univ                Batch:  6/29	Loss 9.5343 (9.4865)
2022-11-08 17:39:06,836:INFO: Dataset: univ                Batch:  7/29	Loss 9.1237 (9.4311)
2022-11-08 17:39:06,841:INFO: Dataset: univ                Batch:  8/29	Loss 8.9737 (9.3751)
2022-11-08 17:39:06,846:INFO: Dataset: univ                Batch:  9/29	Loss 9.1255 (9.3465)
2022-11-08 17:39:06,852:INFO: Dataset: univ                Batch: 10/29	Loss 8.6907 (9.2718)
2022-11-08 17:39:06,857:INFO: Dataset: univ                Batch: 11/29	Loss 8.5861 (9.1995)
2022-11-08 17:39:06,862:INFO: Dataset: univ                Batch: 12/29	Loss 8.6841 (9.1659)
2022-11-08 17:39:06,868:INFO: Dataset: univ                Batch: 13/29	Loss 8.3931 (9.1106)
2022-11-08 17:39:06,875:INFO: Dataset: univ                Batch: 14/29	Loss 8.3125 (9.0544)
2022-11-08 17:39:06,879:INFO: Dataset: univ                Batch: 15/29	Loss 8.2710 (8.9960)
2022-11-08 17:39:06,885:INFO: Dataset: univ                Batch: 16/29	Loss 8.2416 (8.9441)
2022-11-08 17:39:06,891:INFO: Dataset: univ                Batch: 17/29	Loss 8.4976 (8.9206)
2022-11-08 17:39:06,897:INFO: Dataset: univ                Batch: 18/29	Loss 7.7724 (8.8553)
2022-11-08 17:39:06,901:INFO: Dataset: univ                Batch: 19/29	Loss 7.9364 (8.8073)
2022-11-08 17:39:06,907:INFO: Dataset: univ                Batch: 20/29	Loss 7.9055 (8.7628)
2022-11-08 17:39:06,913:INFO: Dataset: univ                Batch: 21/29	Loss 7.9062 (8.7254)
2022-11-08 17:39:06,920:INFO: Dataset: univ                Batch: 22/29	Loss 7.2773 (8.6529)
2022-11-08 17:39:06,925:INFO: Dataset: univ                Batch: 23/29	Loss 7.8035 (8.6225)
2022-11-08 17:39:06,934:INFO: Dataset: univ                Batch: 24/29	Loss 7.3080 (8.5692)
2022-11-08 17:39:06,939:INFO: Dataset: univ                Batch: 25/29	Loss 7.4786 (8.5289)
2022-11-08 17:39:06,944:INFO: Dataset: univ                Batch: 26/29	Loss 7.1329 (8.4756)
2022-11-08 17:39:06,950:INFO: Dataset: univ                Batch: 27/29	Loss 6.5839 (8.3935)
2022-11-08 17:39:06,954:INFO: Dataset: univ                Batch: 28/29	Loss 6.6427 (8.3366)
2022-11-08 17:39:06,959:INFO: Dataset: univ                Batch: 29/29	Loss 6.2692 (8.3043)
2022-11-08 17:39:13,883:INFO: Dataset: zara1               Batch:  1/16	Loss 13.4397 (13.4397)
2022-11-08 17:39:14,289:INFO: Dataset: zara1               Batch:  2/16	Loss 13.5644 (13.4975)
2022-11-08 17:39:14,601:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4847 (13.4935)
2022-11-08 17:39:14,914:INFO: Dataset: zara1               Batch:  4/16	Loss 13.2641 (13.4312)
2022-11-08 17:39:15,145:INFO: Dataset: zara1               Batch:  5/16	Loss 13.4572 (13.4367)
2022-11-08 17:39:15,347:INFO: Dataset: zara1               Batch:  6/16	Loss 13.2982 (13.4143)
2022-11-08 17:39:15,351:INFO: Dataset: zara1               Batch:  7/16	Loss 12.6049 (13.2829)
2022-11-08 17:39:15,356:INFO: Dataset: zara1               Batch:  8/16	Loss 12.7972 (13.2258)
2022-11-08 17:39:15,362:INFO: Dataset: zara1               Batch:  9/16	Loss 12.9782 (13.1928)
2022-11-08 17:39:15,367:INFO: Dataset: zara1               Batch: 10/16	Loss 13.2045 (13.1940)
2022-11-08 17:39:15,371:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5516 (13.1419)
2022-11-08 17:39:15,377:INFO: Dataset: zara1               Batch: 12/16	Loss 12.9756 (13.1272)
2022-11-08 17:39:15,382:INFO: Dataset: zara1               Batch: 13/16	Loss 12.2497 (13.0657)
2022-11-08 17:39:15,390:INFO: Dataset: zara1               Batch: 14/16	Loss 12.8293 (13.0502)
2022-11-08 17:39:15,396:INFO: Dataset: zara1               Batch: 15/16	Loss 12.3381 (12.9986)
2022-11-08 17:39:15,404:INFO: Dataset: zara1               Batch: 16/16	Loss 12.5314 (12.9765)
2022-11-08 17:39:39,486:INFO: Dataset: zara2               Batch:  1/36	Loss 6.6632 (6.6632)
2022-11-08 17:39:39,490:INFO: Dataset: zara2               Batch:  2/36	Loss 6.1211 (6.3864)
2022-11-08 17:39:39,502:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4983 (6.4233)
2022-11-08 17:39:39,515:INFO: Dataset: zara2               Batch:  4/36	Loss 6.5478 (6.4538)
2022-11-08 17:39:39,530:INFO: Dataset: zara2               Batch:  5/36	Loss 6.1975 (6.4027)
2022-11-08 17:39:39,545:INFO: Dataset: zara2               Batch:  6/36	Loss 6.0753 (6.3488)
2022-11-08 17:39:39,549:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4312 (6.3611)
2022-11-08 17:39:39,558:INFO: Dataset: zara2               Batch:  8/36	Loss 6.1218 (6.3275)
2022-11-08 17:39:39,565:INFO: Dataset: zara2               Batch:  9/36	Loss 5.7300 (6.2633)
2022-11-08 17:39:39,569:INFO: Dataset: zara2               Batch: 10/36	Loss 6.0827 (6.2435)
2022-11-08 17:39:39,572:INFO: Dataset: zara2               Batch: 11/36	Loss 5.8811 (6.2085)
2022-11-08 17:39:39,577:INFO: Dataset: zara2               Batch: 12/36	Loss 5.5786 (6.1561)
2022-11-08 17:39:39,580:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0264 (6.1456)
2022-11-08 17:39:39,592:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6119 (6.1107)
2022-11-08 17:39:39,598:INFO: Dataset: zara2               Batch: 15/36	Loss 5.4838 (6.0728)
2022-11-08 17:39:39,609:INFO: Dataset: zara2               Batch: 16/36	Loss 5.7176 (6.0501)
2022-11-08 17:39:39,613:INFO: Dataset: zara2               Batch: 17/36	Loss 5.4306 (6.0206)
2022-11-08 17:39:39,617:INFO: Dataset: zara2               Batch: 18/36	Loss 5.3194 (5.9768)
2022-11-08 17:39:39,623:INFO: Dataset: zara2               Batch: 19/36	Loss 5.1920 (5.9300)
2022-11-08 17:39:39,636:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0007 (5.8766)
2022-11-08 17:39:39,641:INFO: Dataset: zara2               Batch: 21/36	Loss 5.1016 (5.8343)
2022-11-08 17:39:39,647:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9389 (5.7950)
2022-11-08 17:39:39,652:INFO: Dataset: zara2               Batch: 23/36	Loss 4.7152 (5.7553)
2022-11-08 17:39:39,659:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9693 (5.7195)
2022-11-08 17:39:39,663:INFO: Dataset: zara2               Batch: 25/36	Loss 4.6459 (5.6803)
2022-11-08 17:39:39,667:INFO: Dataset: zara2               Batch: 26/36	Loss 4.9557 (5.6523)
2022-11-08 17:39:39,672:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9225 (5.6267)
2022-11-08 17:39:39,675:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5807 (5.5953)
2022-11-08 17:39:39,679:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6719 (5.5574)
2022-11-08 17:39:39,682:INFO: Dataset: zara2               Batch: 30/36	Loss 4.4577 (5.5221)
2022-11-08 17:39:39,687:INFO: Dataset: zara2               Batch: 31/36	Loss 4.2783 (5.4805)
2022-11-08 17:39:39,690:INFO: Dataset: zara2               Batch: 32/36	Loss 4.5879 (5.4541)
2022-11-08 17:39:39,693:INFO: Dataset: zara2               Batch: 33/36	Loss 4.1380 (5.4152)
2022-11-08 17:39:39,697:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3515 (5.3827)
2022-11-08 17:39:39,703:INFO: Dataset: zara2               Batch: 35/36	Loss 4.3979 (5.3464)
2022-11-08 17:39:39,708:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1425 (5.3257)
2022-11-08 17:39:40,655:INFO: - Computing loss (validation)
2022-11-08 17:39:47,074:INFO: Dataset: hotel               Batch: 1/3	Loss 17.7100 (17.7100)
2022-11-08 17:39:47,394:INFO: Dataset: hotel               Batch: 2/3	Loss 17.2390 (17.4728)
2022-11-08 17:39:47,760:INFO: Dataset: hotel               Batch: 3/3	Loss 15.3107 (17.3178)
2022-11-08 17:39:55,695:INFO: Dataset: univ                Batch: 1/6	Loss 9.4405 (9.4405)
2022-11-08 17:39:56,113:INFO: Dataset: univ                Batch: 2/6	Loss 9.0083 (9.1739)
2022-11-08 17:39:56,532:INFO: Dataset: univ                Batch: 3/6	Loss 8.4988 (8.9133)
2022-11-08 17:39:56,857:INFO: Dataset: univ                Batch: 4/6	Loss 8.9744 (8.9299)
2022-11-08 17:39:57,142:INFO: Dataset: univ                Batch: 5/6	Loss 9.3678 (9.0031)
2022-11-08 17:39:57,376:INFO: Dataset: univ                Batch: 6/6	Loss 9.0834 (9.0140)
2022-11-08 17:40:05,372:INFO: Dataset: zara1               Batch: 1/3	Loss 11.7435 (11.7435)
2022-11-08 17:40:05,719:INFO: Dataset: zara1               Batch: 2/3	Loss 11.4274 (11.5944)
2022-11-08 17:40:06,125:INFO: Dataset: zara1               Batch: 3/3	Loss 11.5830 (11.5920)
2022-11-08 17:40:14,494:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1029 (4.1029)
2022-11-08 17:40:14,809:INFO: Dataset: zara2               Batch:  2/10	Loss 4.0706 (4.0876)
2022-11-08 17:40:15,271:INFO: Dataset: zara2               Batch:  3/10	Loss 3.9778 (4.0510)
2022-11-08 17:40:15,571:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3276 (4.1292)
2022-11-08 17:40:15,779:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3427 (4.1756)
2022-11-08 17:40:15,999:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1869 (4.1772)
2022-11-08 17:40:16,001:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1704 (4.1762)
2022-11-08 17:40:16,001:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2121 (4.1811)
2022-11-08 17:40:16,002:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2033 (4.1835)
2022-11-08 17:40:16,003:INFO: Dataset: zara2               Batch: 10/10	Loss 4.0769 (4.1738)
2022-11-08 17:40:16,939:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1056.pth.tar
2022-11-08 17:40:16,939:INFO: 
===> EPOCH: 1057 (P5)
2022-11-08 17:40:16,940:INFO: - Computing loss (training)
2022-11-08 17:40:23,085:INFO: Dataset: hotel               Batch: 1/8	Loss 15.7451 (15.7451)
2022-11-08 17:40:23,523:INFO: Dataset: hotel               Batch: 2/8	Loss 14.6543 (15.2247)
2022-11-08 17:40:24,043:INFO: Dataset: hotel               Batch: 3/8	Loss 14.3151 (14.9160)
2022-11-08 17:40:24,421:INFO: Dataset: hotel               Batch: 4/8	Loss 15.2769 (15.0031)
2022-11-08 17:40:24,653:INFO: Dataset: hotel               Batch: 5/8	Loss 15.4536 (15.0845)
2022-11-08 17:40:24,863:INFO: Dataset: hotel               Batch: 6/8	Loss 13.6464 (14.8433)
2022-11-08 17:40:24,866:INFO: Dataset: hotel               Batch: 7/8	Loss 12.8133 (14.5778)
2022-11-08 17:40:24,870:INFO: Dataset: hotel               Batch: 8/8	Loss 13.6569 (14.5487)
2022-11-08 17:40:47,931:INFO: Dataset: univ                Batch:  1/29	Loss 9.4055 (9.4055)
2022-11-08 17:40:47,935:INFO: Dataset: univ                Batch:  2/29	Loss 9.3866 (9.3952)
2022-11-08 17:40:47,941:INFO: Dataset: univ                Batch:  3/29	Loss 9.9952 (9.5716)
2022-11-08 17:40:47,948:INFO: Dataset: univ                Batch:  4/29	Loss 9.8620 (9.6369)
2022-11-08 17:40:47,960:INFO: Dataset: univ                Batch:  5/29	Loss 9.1732 (9.5446)
2022-11-08 17:40:47,969:INFO: Dataset: univ                Batch:  6/29	Loss 8.8985 (9.4333)
2022-11-08 17:40:47,973:INFO: Dataset: univ                Batch:  7/29	Loss 9.3162 (9.4193)
2022-11-08 17:40:47,978:INFO: Dataset: univ                Batch:  8/29	Loss 9.1566 (9.3857)
2022-11-08 17:40:47,985:INFO: Dataset: univ                Batch:  9/29	Loss 8.7887 (9.3242)
2022-11-08 17:40:47,989:INFO: Dataset: univ                Batch: 10/29	Loss 9.2741 (9.3197)
2022-11-08 17:40:47,996:INFO: Dataset: univ                Batch: 11/29	Loss 9.1698 (9.3056)
2022-11-08 17:40:48,002:INFO: Dataset: univ                Batch: 12/29	Loss 8.6945 (9.2540)
2022-11-08 17:40:48,007:INFO: Dataset: univ                Batch: 13/29	Loss 8.8282 (9.2245)
2022-11-08 17:40:48,014:INFO: Dataset: univ                Batch: 14/29	Loss 8.2459 (9.1421)
2022-11-08 17:40:48,022:INFO: Dataset: univ                Batch: 15/29	Loss 8.6124 (9.1076)
2022-11-08 17:40:48,028:INFO: Dataset: univ                Batch: 16/29	Loss 8.1542 (9.0422)
2022-11-08 17:40:48,036:INFO: Dataset: univ                Batch: 17/29	Loss 7.9668 (8.9835)
2022-11-08 17:40:48,043:INFO: Dataset: univ                Batch: 18/29	Loss 7.7776 (8.9146)
2022-11-08 17:40:48,050:INFO: Dataset: univ                Batch: 19/29	Loss 8.0754 (8.8674)
2022-11-08 17:40:48,055:INFO: Dataset: univ                Batch: 20/29	Loss 7.9274 (8.8254)
2022-11-08 17:40:48,061:INFO: Dataset: univ                Batch: 21/29	Loss 7.8986 (8.7786)
2022-11-08 17:40:48,068:INFO: Dataset: univ                Batch: 22/29	Loss 7.8065 (8.7333)
2022-11-08 17:40:48,072:INFO: Dataset: univ                Batch: 23/29	Loss 7.3482 (8.6675)
2022-11-08 17:40:48,077:INFO: Dataset: univ                Batch: 24/29	Loss 7.5747 (8.6273)
2022-11-08 17:40:48,084:INFO: Dataset: univ                Batch: 25/29	Loss 7.3441 (8.5723)
2022-11-08 17:40:48,089:INFO: Dataset: univ                Batch: 26/29	Loss 7.5595 (8.5317)
2022-11-08 17:40:48,094:INFO: Dataset: univ                Batch: 27/29	Loss 7.2579 (8.4926)
2022-11-08 17:40:48,101:INFO: Dataset: univ                Batch: 28/29	Loss 6.7008 (8.4285)
2022-11-08 17:40:48,107:INFO: Dataset: univ                Batch: 29/29	Loss 6.6135 (8.4049)
2022-11-08 17:40:55,134:INFO: Dataset: zara1               Batch:  1/16	Loss 13.3154 (13.3154)
2022-11-08 17:40:55,507:INFO: Dataset: zara1               Batch:  2/16	Loss 13.4324 (13.3774)
2022-11-08 17:40:55,927:INFO: Dataset: zara1               Batch:  3/16	Loss 13.1673 (13.3008)
2022-11-08 17:40:56,117:INFO: Dataset: zara1               Batch:  4/16	Loss 13.4318 (13.3317)
2022-11-08 17:40:56,427:INFO: Dataset: zara1               Batch:  5/16	Loss 12.8029 (13.2376)
2022-11-08 17:40:56,555:INFO: Dataset: zara1               Batch:  6/16	Loss 13.7581 (13.3150)
2022-11-08 17:40:56,561:INFO: Dataset: zara1               Batch:  7/16	Loss 13.1148 (13.2871)
2022-11-08 17:40:56,567:INFO: Dataset: zara1               Batch:  8/16	Loss 13.2244 (13.2793)
2022-11-08 17:40:56,572:INFO: Dataset: zara1               Batch:  9/16	Loss 13.3328 (13.2850)
2022-11-08 17:40:56,577:INFO: Dataset: zara1               Batch: 10/16	Loss 12.8216 (13.2339)
2022-11-08 17:40:56,581:INFO: Dataset: zara1               Batch: 11/16	Loss 12.7547 (13.1924)
2022-11-08 17:40:56,586:INFO: Dataset: zara1               Batch: 12/16	Loss 12.2465 (13.1158)
2022-11-08 17:40:56,589:INFO: Dataset: zara1               Batch: 13/16	Loss 12.3570 (13.0581)
2022-11-08 17:40:56,593:INFO: Dataset: zara1               Batch: 14/16	Loss 12.1125 (12.9939)
2022-11-08 17:40:56,597:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0215 (12.9340)
2022-11-08 17:40:56,601:INFO: Dataset: zara1               Batch: 16/16	Loss 11.7849 (12.8850)
2022-11-08 17:41:19,865:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3890 (6.3890)
2022-11-08 17:41:19,869:INFO: Dataset: zara2               Batch:  2/36	Loss 5.9849 (6.1827)
2022-11-08 17:41:19,875:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1753 (6.1802)
2022-11-08 17:41:19,881:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3607 (6.2349)
2022-11-08 17:41:19,886:INFO: Dataset: zara2               Batch:  5/36	Loss 6.0629 (6.2040)
2022-11-08 17:41:19,905:INFO: Dataset: zara2               Batch:  6/36	Loss 6.4564 (6.2441)
2022-11-08 17:41:19,909:INFO: Dataset: zara2               Batch:  7/36	Loss 5.9322 (6.1998)
2022-11-08 17:41:19,915:INFO: Dataset: zara2               Batch:  8/36	Loss 5.8403 (6.1590)
2022-11-08 17:41:19,920:INFO: Dataset: zara2               Batch:  9/36	Loss 5.9967 (6.1389)
2022-11-08 17:41:19,924:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6827 (6.0943)
2022-11-08 17:41:19,927:INFO: Dataset: zara2               Batch: 11/36	Loss 6.0226 (6.0879)
2022-11-08 17:41:19,935:INFO: Dataset: zara2               Batch: 12/36	Loss 5.9543 (6.0776)
2022-11-08 17:41:19,938:INFO: Dataset: zara2               Batch: 13/36	Loss 5.6862 (6.0471)
2022-11-08 17:41:19,945:INFO: Dataset: zara2               Batch: 14/36	Loss 5.4834 (6.0104)
2022-11-08 17:41:19,948:INFO: Dataset: zara2               Batch: 15/36	Loss 5.3324 (5.9627)
2022-11-08 17:41:19,954:INFO: Dataset: zara2               Batch: 16/36	Loss 5.4907 (5.9351)
2022-11-08 17:41:19,957:INFO: Dataset: zara2               Batch: 17/36	Loss 5.2456 (5.8994)
2022-11-08 17:41:19,961:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6280 (5.8841)
2022-11-08 17:41:19,966:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2736 (5.8520)
2022-11-08 17:41:19,974:INFO: Dataset: zara2               Batch: 20/36	Loss 4.7050 (5.8009)
2022-11-08 17:41:19,980:INFO: Dataset: zara2               Batch: 21/36	Loss 5.2761 (5.7770)
2022-11-08 17:41:19,987:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1433 (5.7489)
2022-11-08 17:41:19,991:INFO: Dataset: zara2               Batch: 23/36	Loss 4.7471 (5.6999)
2022-11-08 17:41:19,996:INFO: Dataset: zara2               Batch: 24/36	Loss 4.8947 (5.6640)
2022-11-08 17:41:20,003:INFO: Dataset: zara2               Batch: 25/36	Loss 4.6929 (5.6261)
2022-11-08 17:41:20,009:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8019 (5.5919)
2022-11-08 17:41:20,013:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7445 (5.5601)
2022-11-08 17:41:20,020:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5909 (5.5307)
2022-11-08 17:41:20,024:INFO: Dataset: zara2               Batch: 29/36	Loss 4.4651 (5.4945)
2022-11-08 17:41:20,028:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3195 (5.4561)
2022-11-08 17:41:20,032:INFO: Dataset: zara2               Batch: 31/36	Loss 4.6352 (5.4299)
2022-11-08 17:41:20,039:INFO: Dataset: zara2               Batch: 32/36	Loss 4.3147 (5.3957)
2022-11-08 17:41:20,043:INFO: Dataset: zara2               Batch: 33/36	Loss 4.7807 (5.3804)
2022-11-08 17:41:20,049:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3355 (5.3514)
2022-11-08 17:41:20,055:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2325 (5.3214)
2022-11-08 17:41:20,059:INFO: Dataset: zara2               Batch: 36/36	Loss 4.0399 (5.2916)
2022-11-08 17:41:21,021:INFO: - Computing loss (validation)
2022-11-08 17:41:28,361:INFO: Dataset: hotel               Batch: 1/3	Loss 16.7559 (16.7559)
2022-11-08 17:41:28,648:INFO: Dataset: hotel               Batch: 2/3	Loss 17.4011 (17.0651)
2022-11-08 17:41:29,025:INFO: Dataset: hotel               Batch: 3/3	Loss 15.2338 (16.8901)
2022-11-08 17:41:37,320:INFO: Dataset: univ                Batch: 1/6	Loss 9.1807 (9.1807)
2022-11-08 17:41:37,851:INFO: Dataset: univ                Batch: 2/6	Loss 9.6649 (9.4032)
2022-11-08 17:41:38,182:INFO: Dataset: univ                Batch: 3/6	Loss 9.2366 (9.3466)
2022-11-08 17:41:38,501:INFO: Dataset: univ                Batch: 4/6	Loss 8.4637 (9.0523)
2022-11-08 17:41:38,707:INFO: Dataset: univ                Batch: 5/6	Loss 9.0175 (9.0447)
2022-11-08 17:41:38,909:INFO: Dataset: univ                Batch: 6/6	Loss 9.0122 (9.0397)
2022-11-08 17:41:45,850:INFO: Dataset: zara1               Batch: 1/3	Loss 11.8190 (11.8190)
2022-11-08 17:41:46,311:INFO: Dataset: zara1               Batch: 2/3	Loss 11.5005 (11.6610)
2022-11-08 17:41:46,624:INFO: Dataset: zara1               Batch: 3/3	Loss 12.2016 (11.7740)
2022-11-08 17:41:54,233:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2104 (4.2104)
2022-11-08 17:41:54,556:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1598 (4.1849)
2022-11-08 17:41:54,912:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1680 (4.1793)
2022-11-08 17:41:55,224:INFO: Dataset: zara2               Batch:  4/10	Loss 4.0930 (4.1575)
2022-11-08 17:41:55,474:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1296 (4.1520)
2022-11-08 17:41:55,647:INFO: Dataset: zara2               Batch:  6/10	Loss 4.3030 (4.1766)
2022-11-08 17:41:55,648:INFO: Dataset: zara2               Batch:  7/10	Loss 4.3183 (4.1954)
2022-11-08 17:41:55,649:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0624 (4.1771)
2022-11-08 17:41:55,650:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2687 (4.1870)
2022-11-08 17:41:55,651:INFO: Dataset: zara2               Batch: 10/10	Loss 3.9680 (4.1655)
2022-11-08 17:41:56,569:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1057.pth.tar
2022-11-08 17:41:56,569:INFO: 
===> EPOCH: 1058 (P5)
2022-11-08 17:41:56,569:INFO: - Computing loss (training)
2022-11-08 17:42:04,262:INFO: Dataset: hotel               Batch: 1/8	Loss 13.0324 (13.0324)
2022-11-08 17:42:04,635:INFO: Dataset: hotel               Batch: 2/8	Loss 14.5659 (13.8110)
2022-11-08 17:42:04,998:INFO: Dataset: hotel               Batch: 3/8	Loss 14.0983 (13.9195)
2022-11-08 17:42:05,357:INFO: Dataset: hotel               Batch: 4/8	Loss 14.2020 (13.9879)
2022-11-08 17:42:05,630:INFO: Dataset: hotel               Batch: 5/8	Loss 14.5609 (14.1040)
2022-11-08 17:42:05,737:INFO: Dataset: hotel               Batch: 6/8	Loss 14.7992 (14.2249)
2022-11-08 17:42:05,740:INFO: Dataset: hotel               Batch: 7/8	Loss 14.5945 (14.2827)
2022-11-08 17:42:05,743:INFO: Dataset: hotel               Batch: 8/8	Loss 13.3820 (14.2565)
2022-11-08 17:42:29,653:INFO: Dataset: univ                Batch:  1/29	Loss 9.3682 (9.3682)
2022-11-08 17:42:29,656:INFO: Dataset: univ                Batch:  2/29	Loss 9.0048 (9.1599)
2022-11-08 17:42:29,663:INFO: Dataset: univ                Batch:  3/29	Loss 9.5010 (9.2765)
2022-11-08 17:42:29,668:INFO: Dataset: univ                Batch:  4/29	Loss 9.5056 (9.3291)
2022-11-08 17:42:29,685:INFO: Dataset: univ                Batch:  5/29	Loss 9.1900 (9.3031)
2022-11-08 17:42:29,697:INFO: Dataset: univ                Batch:  6/29	Loss 9.2897 (9.3010)
2022-11-08 17:42:29,702:INFO: Dataset: univ                Batch:  7/29	Loss 9.1111 (9.2750)
2022-11-08 17:42:29,707:INFO: Dataset: univ                Batch:  8/29	Loss 9.2326 (9.2701)
2022-11-08 17:42:29,711:INFO: Dataset: univ                Batch:  9/29	Loss 9.0976 (9.2530)
2022-11-08 17:42:29,715:INFO: Dataset: univ                Batch: 10/29	Loss 9.3130 (9.2592)
2022-11-08 17:42:29,721:INFO: Dataset: univ                Batch: 11/29	Loss 8.8052 (9.2132)
2022-11-08 17:42:29,725:INFO: Dataset: univ                Batch: 12/29	Loss 9.0678 (9.2024)
2022-11-08 17:42:29,730:INFO: Dataset: univ                Batch: 13/29	Loss 8.5202 (9.1448)
2022-11-08 17:42:29,734:INFO: Dataset: univ                Batch: 14/29	Loss 8.9551 (9.1341)
2022-11-08 17:42:29,739:INFO: Dataset: univ                Batch: 15/29	Loss 7.8812 (9.0407)
2022-11-08 17:42:29,742:INFO: Dataset: univ                Batch: 16/29	Loss 8.3839 (8.9972)
2022-11-08 17:42:29,747:INFO: Dataset: univ                Batch: 17/29	Loss 7.7663 (8.9268)
2022-11-08 17:42:29,753:INFO: Dataset: univ                Batch: 18/29	Loss 8.0509 (8.8757)
2022-11-08 17:42:29,758:INFO: Dataset: univ                Batch: 19/29	Loss 7.7546 (8.8150)
2022-11-08 17:42:29,763:INFO: Dataset: univ                Batch: 20/29	Loss 7.6287 (8.7594)
2022-11-08 17:42:29,768:INFO: Dataset: univ                Batch: 21/29	Loss 7.7899 (8.7143)
2022-11-08 17:42:29,774:INFO: Dataset: univ                Batch: 22/29	Loss 7.3185 (8.6378)
2022-11-08 17:42:29,779:INFO: Dataset: univ                Batch: 23/29	Loss 7.2278 (8.5661)
2022-11-08 17:42:29,784:INFO: Dataset: univ                Batch: 24/29	Loss 7.3262 (8.5180)
2022-11-08 17:42:29,790:INFO: Dataset: univ                Batch: 25/29	Loss 7.4065 (8.4756)
2022-11-08 17:42:29,795:INFO: Dataset: univ                Batch: 26/29	Loss 7.1586 (8.4291)
2022-11-08 17:42:29,802:INFO: Dataset: univ                Batch: 27/29	Loss 7.3705 (8.3934)
2022-11-08 17:42:29,806:INFO: Dataset: univ                Batch: 28/29	Loss 6.7365 (8.3295)
2022-11-08 17:42:29,810:INFO: Dataset: univ                Batch: 29/29	Loss 6.7664 (8.3086)
2022-11-08 17:42:37,042:INFO: Dataset: zara1               Batch:  1/16	Loss 13.0947 (13.0947)
2022-11-08 17:42:37,399:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2275 (13.1550)
2022-11-08 17:42:37,882:INFO: Dataset: zara1               Batch:  3/16	Loss 13.0984 (13.1347)
2022-11-08 17:42:38,158:INFO: Dataset: zara1               Batch:  4/16	Loss 13.7803 (13.3041)
2022-11-08 17:42:38,438:INFO: Dataset: zara1               Batch:  5/16	Loss 13.4353 (13.3304)
2022-11-08 17:42:38,626:INFO: Dataset: zara1               Batch:  6/16	Loss 13.6032 (13.3834)
2022-11-08 17:42:38,628:INFO: Dataset: zara1               Batch:  7/16	Loss 12.7433 (13.2921)
2022-11-08 17:42:38,631:INFO: Dataset: zara1               Batch:  8/16	Loss 13.4109 (13.3044)
2022-11-08 17:42:38,633:INFO: Dataset: zara1               Batch:  9/16	Loss 12.7645 (13.2486)
2022-11-08 17:42:38,637:INFO: Dataset: zara1               Batch: 10/16	Loss 12.4843 (13.1842)
2022-11-08 17:42:38,641:INFO: Dataset: zara1               Batch: 11/16	Loss 13.2397 (13.1890)
2022-11-08 17:42:38,646:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3142 (13.1146)
2022-11-08 17:42:38,651:INFO: Dataset: zara1               Batch: 13/16	Loss 13.0376 (13.1090)
2022-11-08 17:42:38,665:INFO: Dataset: zara1               Batch: 14/16	Loss 12.5954 (13.0824)
2022-11-08 17:42:38,703:INFO: Dataset: zara1               Batch: 15/16	Loss 12.2847 (13.0288)
2022-11-08 17:42:38,707:INFO: Dataset: zara1               Batch: 16/16	Loss 12.5480 (13.0081)
2022-11-08 17:43:04,078:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3030 (6.3030)
2022-11-08 17:43:04,084:INFO: Dataset: zara2               Batch:  2/36	Loss 6.0806 (6.1881)
2022-11-08 17:43:04,087:INFO: Dataset: zara2               Batch:  3/36	Loss 7.0160 (6.4309)
2022-11-08 17:43:04,091:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3831 (6.4202)
2022-11-08 17:43:04,097:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3596 (6.4071)
2022-11-08 17:43:04,113:INFO: Dataset: zara2               Batch:  6/36	Loss 5.9464 (6.3273)
2022-11-08 17:43:04,117:INFO: Dataset: zara2               Batch:  7/36	Loss 5.7472 (6.2364)
2022-11-08 17:43:04,120:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0580 (6.2148)
2022-11-08 17:43:04,122:INFO: Dataset: zara2               Batch:  9/36	Loss 5.8063 (6.1704)
2022-11-08 17:43:04,125:INFO: Dataset: zara2               Batch: 10/36	Loss 6.2748 (6.1806)
2022-11-08 17:43:04,128:INFO: Dataset: zara2               Batch: 11/36	Loss 5.9406 (6.1613)
2022-11-08 17:43:04,133:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7686 (6.1299)
2022-11-08 17:43:04,136:INFO: Dataset: zara2               Batch: 13/36	Loss 5.8733 (6.1097)
2022-11-08 17:43:04,139:INFO: Dataset: zara2               Batch: 14/36	Loss 5.5417 (6.0652)
2022-11-08 17:43:04,144:INFO: Dataset: zara2               Batch: 15/36	Loss 5.6177 (6.0387)
2022-11-08 17:43:04,147:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3832 (5.9960)
2022-11-08 17:43:04,150:INFO: Dataset: zara2               Batch: 17/36	Loss 5.3481 (5.9544)
2022-11-08 17:43:04,154:INFO: Dataset: zara2               Batch: 18/36	Loss 4.9200 (5.8930)
2022-11-08 17:43:04,159:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4558 (5.8714)
2022-11-08 17:43:04,163:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0844 (5.8298)
2022-11-08 17:43:04,167:INFO: Dataset: zara2               Batch: 21/36	Loss 5.1128 (5.7976)
2022-11-08 17:43:04,170:INFO: Dataset: zara2               Batch: 22/36	Loss 4.7168 (5.7519)
2022-11-08 17:43:04,173:INFO: Dataset: zara2               Batch: 23/36	Loss 4.7702 (5.7038)
2022-11-08 17:43:04,177:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9007 (5.6760)
2022-11-08 17:43:04,181:INFO: Dataset: zara2               Batch: 25/36	Loss 4.7407 (5.6394)
2022-11-08 17:43:04,186:INFO: Dataset: zara2               Batch: 26/36	Loss 4.9371 (5.6119)
2022-11-08 17:43:04,190:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7946 (5.5785)
2022-11-08 17:43:04,194:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5534 (5.5437)
2022-11-08 17:43:04,198:INFO: Dataset: zara2               Batch: 29/36	Loss 4.3543 (5.5083)
2022-11-08 17:43:04,203:INFO: Dataset: zara2               Batch: 30/36	Loss 4.2834 (5.4680)
2022-11-08 17:43:04,206:INFO: Dataset: zara2               Batch: 31/36	Loss 4.6124 (5.4401)
2022-11-08 17:43:04,210:INFO: Dataset: zara2               Batch: 32/36	Loss 4.6439 (5.4170)
2022-11-08 17:43:04,213:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2490 (5.3798)
2022-11-08 17:43:04,216:INFO: Dataset: zara2               Batch: 34/36	Loss 3.9678 (5.3390)
2022-11-08 17:43:04,219:INFO: Dataset: zara2               Batch: 35/36	Loss 3.8137 (5.2925)
2022-11-08 17:43:04,223:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9681 (5.2666)
2022-11-08 17:43:05,116:INFO: - Computing loss (validation)
2022-11-08 17:43:11,181:INFO: Dataset: hotel               Batch: 1/3	Loss 16.4311 (16.4311)
2022-11-08 17:43:11,528:INFO: Dataset: hotel               Batch: 2/3	Loss 18.0857 (17.1980)
2022-11-08 17:43:11,891:INFO: Dataset: hotel               Batch: 3/3	Loss 19.3242 (17.3359)
2022-11-08 17:43:19,339:INFO: Dataset: univ                Batch: 1/6	Loss 9.0711 (9.0711)
2022-11-08 17:43:19,773:INFO: Dataset: univ                Batch: 2/6	Loss 8.6917 (8.8903)
2022-11-08 17:43:20,203:INFO: Dataset: univ                Batch: 3/6	Loss 9.0224 (8.9330)
2022-11-08 17:43:20,491:INFO: Dataset: univ                Batch: 4/6	Loss 8.9201 (8.9294)
2022-11-08 17:43:20,701:INFO: Dataset: univ                Batch: 5/6	Loss 9.7884 (9.0915)
2022-11-08 17:43:20,963:INFO: Dataset: univ                Batch: 6/6	Loss 8.7389 (9.0364)
2022-11-08 17:43:28,170:INFO: Dataset: zara1               Batch: 1/3	Loss 11.6244 (11.6244)
2022-11-08 17:43:28,573:INFO: Dataset: zara1               Batch: 2/3	Loss 11.9387 (11.7795)
2022-11-08 17:43:28,988:INFO: Dataset: zara1               Batch: 3/3	Loss 11.8010 (11.7851)
2022-11-08 17:43:36,575:INFO: Dataset: zara2               Batch:  1/10	Loss 4.3021 (4.3021)
2022-11-08 17:43:37,016:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2406 (4.2717)
2022-11-08 17:43:37,424:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1563 (4.2362)
2022-11-08 17:43:37,825:INFO: Dataset: zara2               Batch:  4/10	Loss 3.9857 (4.1713)
2022-11-08 17:43:38,061:INFO: Dataset: zara2               Batch:  5/10	Loss 4.0889 (4.1544)
2022-11-08 17:43:38,247:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1155 (4.1474)
2022-11-08 17:43:38,249:INFO: Dataset: zara2               Batch:  7/10	Loss 3.9905 (4.1232)
2022-11-08 17:43:38,250:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0900 (4.1194)
2022-11-08 17:43:38,251:INFO: Dataset: zara2               Batch:  9/10	Loss 3.9793 (4.1041)
2022-11-08 17:43:38,252:INFO: Dataset: zara2               Batch: 10/10	Loss 3.8699 (4.0813)
2022-11-08 17:43:39,184:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1058.pth.tar
2022-11-08 17:43:39,184:INFO: 
===> EPOCH: 1059 (P5)
2022-11-08 17:43:39,185:INFO: - Computing loss (training)
2022-11-08 17:43:45,297:INFO: Dataset: hotel               Batch: 1/8	Loss 15.0067 (15.0067)
2022-11-08 17:43:45,685:INFO: Dataset: hotel               Batch: 2/8	Loss 15.2685 (15.1406)
2022-11-08 17:43:46,029:INFO: Dataset: hotel               Batch: 3/8	Loss 13.8865 (14.7371)
2022-11-08 17:43:46,366:INFO: Dataset: hotel               Batch: 4/8	Loss 15.0122 (14.8011)
2022-11-08 17:43:46,599:INFO: Dataset: hotel               Batch: 5/8	Loss 14.3335 (14.7077)
2022-11-08 17:43:46,828:INFO: Dataset: hotel               Batch: 6/8	Loss 14.1526 (14.6095)
2022-11-08 17:43:46,837:INFO: Dataset: hotel               Batch: 7/8	Loss 14.3139 (14.5653)
2022-11-08 17:43:46,842:INFO: Dataset: hotel               Batch: 8/8	Loss 12.2372 (14.5008)
2022-11-08 17:44:10,232:INFO: Dataset: univ                Batch:  1/29	Loss 9.9924 (9.9924)
2022-11-08 17:44:10,237:INFO: Dataset: univ                Batch:  2/29	Loss 8.8818 (9.3845)
2022-11-08 17:44:10,243:INFO: Dataset: univ                Batch:  3/29	Loss 9.4965 (9.4213)
2022-11-08 17:44:10,259:INFO: Dataset: univ                Batch:  4/29	Loss 9.4022 (9.4167)
2022-11-08 17:44:10,266:INFO: Dataset: univ                Batch:  5/29	Loss 9.0811 (9.3456)
2022-11-08 17:44:10,277:INFO: Dataset: univ                Batch:  6/29	Loss 9.3020 (9.3395)
2022-11-08 17:44:10,283:INFO: Dataset: univ                Batch:  7/29	Loss 8.5649 (9.2079)
2022-11-08 17:44:10,286:INFO: Dataset: univ                Batch:  8/29	Loss 9.1937 (9.2060)
2022-11-08 17:44:10,290:INFO: Dataset: univ                Batch:  9/29	Loss 9.5372 (9.2393)
2022-11-08 17:44:10,294:INFO: Dataset: univ                Batch: 10/29	Loss 9.3461 (9.2481)
2022-11-08 17:44:10,299:INFO: Dataset: univ                Batch: 11/29	Loss 8.5681 (9.1965)
2022-11-08 17:44:10,303:INFO: Dataset: univ                Batch: 12/29	Loss 8.8540 (9.1679)
2022-11-08 17:44:10,305:INFO: Dataset: univ                Batch: 13/29	Loss 8.7474 (9.1354)
2022-11-08 17:44:10,308:INFO: Dataset: univ                Batch: 14/29	Loss 8.6466 (9.0973)
2022-11-08 17:44:10,311:INFO: Dataset: univ                Batch: 15/29	Loss 8.7447 (9.0713)
2022-11-08 17:44:10,314:INFO: Dataset: univ                Batch: 16/29	Loss 8.3389 (9.0219)
2022-11-08 17:44:10,317:INFO: Dataset: univ                Batch: 17/29	Loss 8.4721 (8.9931)
2022-11-08 17:44:10,320:INFO: Dataset: univ                Batch: 18/29	Loss 7.8250 (8.9254)
2022-11-08 17:44:10,324:INFO: Dataset: univ                Batch: 19/29	Loss 7.9059 (8.8744)
2022-11-08 17:44:10,327:INFO: Dataset: univ                Batch: 20/29	Loss 7.8913 (8.8232)
2022-11-08 17:44:10,329:INFO: Dataset: univ                Batch: 21/29	Loss 8.0585 (8.7894)
2022-11-08 17:44:10,333:INFO: Dataset: univ                Batch: 22/29	Loss 7.4519 (8.7297)
2022-11-08 17:44:10,336:INFO: Dataset: univ                Batch: 23/29	Loss 7.6379 (8.6858)
2022-11-08 17:44:10,340:INFO: Dataset: univ                Batch: 24/29	Loss 7.7234 (8.6453)
2022-11-08 17:44:10,344:INFO: Dataset: univ                Batch: 25/29	Loss 7.3805 (8.5982)
2022-11-08 17:44:10,348:INFO: Dataset: univ                Batch: 26/29	Loss 6.9271 (8.5287)
2022-11-08 17:44:10,351:INFO: Dataset: univ                Batch: 27/29	Loss 7.2598 (8.4820)
2022-11-08 17:44:10,353:INFO: Dataset: univ                Batch: 28/29	Loss 6.9787 (8.4357)
2022-11-08 17:44:10,356:INFO: Dataset: univ                Batch: 29/29	Loss 7.0132 (8.4189)
2022-11-08 17:44:17,760:INFO: Dataset: zara1               Batch:  1/16	Loss 13.1179 (13.1179)
2022-11-08 17:44:18,180:INFO: Dataset: zara1               Batch:  2/16	Loss 13.1427 (13.1325)
2022-11-08 17:44:18,588:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4183 (13.2210)
2022-11-08 17:44:18,847:INFO: Dataset: zara1               Batch:  4/16	Loss 13.4589 (13.2773)
2022-11-08 17:44:19,135:INFO: Dataset: zara1               Batch:  5/16	Loss 13.2206 (13.2660)
2022-11-08 17:44:19,286:INFO: Dataset: zara1               Batch:  6/16	Loss 12.8937 (13.2097)
2022-11-08 17:44:19,290:INFO: Dataset: zara1               Batch:  7/16	Loss 12.6810 (13.1359)
2022-11-08 17:44:19,294:INFO: Dataset: zara1               Batch:  8/16	Loss 12.7585 (13.0837)
2022-11-08 17:44:19,298:INFO: Dataset: zara1               Batch:  9/16	Loss 12.9726 (13.0720)
2022-11-08 17:44:19,301:INFO: Dataset: zara1               Batch: 10/16	Loss 13.0823 (13.0730)
2022-11-08 17:44:19,304:INFO: Dataset: zara1               Batch: 11/16	Loss 13.2419 (13.0870)
2022-11-08 17:44:19,307:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3674 (13.0293)
2022-11-08 17:44:19,310:INFO: Dataset: zara1               Batch: 13/16	Loss 12.9709 (13.0245)
2022-11-08 17:44:19,314:INFO: Dataset: zara1               Batch: 14/16	Loss 12.2987 (12.9804)
2022-11-08 17:44:19,317:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0818 (12.9214)
2022-11-08 17:44:19,320:INFO: Dataset: zara1               Batch: 16/16	Loss 12.5316 (12.9033)
2022-11-08 17:44:43,285:INFO: Dataset: zara2               Batch:  1/36	Loss 6.5122 (6.5122)
2022-11-08 17:44:43,289:INFO: Dataset: zara2               Batch:  2/36	Loss 6.2248 (6.3649)
2022-11-08 17:44:43,293:INFO: Dataset: zara2               Batch:  3/36	Loss 5.8318 (6.1749)
2022-11-08 17:44:43,300:INFO: Dataset: zara2               Batch:  4/36	Loss 6.0258 (6.1401)
2022-11-08 17:44:43,306:INFO: Dataset: zara2               Batch:  5/36	Loss 6.0659 (6.1259)
2022-11-08 17:44:43,327:INFO: Dataset: zara2               Batch:  6/36	Loss 6.5320 (6.1830)
2022-11-08 17:44:43,330:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0501 (6.1656)
2022-11-08 17:44:43,333:INFO: Dataset: zara2               Batch:  8/36	Loss 6.2790 (6.1779)
2022-11-08 17:44:43,336:INFO: Dataset: zara2               Batch:  9/36	Loss 5.5532 (6.1071)
2022-11-08 17:44:43,340:INFO: Dataset: zara2               Batch: 10/36	Loss 6.2367 (6.1212)
2022-11-08 17:44:43,343:INFO: Dataset: zara2               Batch: 11/36	Loss 5.2059 (6.0468)
2022-11-08 17:44:43,348:INFO: Dataset: zara2               Batch: 12/36	Loss 5.3134 (5.9858)
2022-11-08 17:44:43,351:INFO: Dataset: zara2               Batch: 13/36	Loss 6.3967 (6.0186)
2022-11-08 17:44:43,355:INFO: Dataset: zara2               Batch: 14/36	Loss 5.4083 (5.9752)
2022-11-08 17:44:43,357:INFO: Dataset: zara2               Batch: 15/36	Loss 5.3187 (5.9327)
2022-11-08 17:44:43,360:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3443 (5.8995)
2022-11-08 17:44:43,362:INFO: Dataset: zara2               Batch: 17/36	Loss 5.2982 (5.8636)
2022-11-08 17:44:43,366:INFO: Dataset: zara2               Batch: 18/36	Loss 4.8858 (5.8182)
2022-11-08 17:44:43,370:INFO: Dataset: zara2               Batch: 19/36	Loss 5.1618 (5.7826)
2022-11-08 17:44:43,374:INFO: Dataset: zara2               Batch: 20/36	Loss 4.8953 (5.7388)
2022-11-08 17:44:43,379:INFO: Dataset: zara2               Batch: 21/36	Loss 4.9003 (5.7031)
2022-11-08 17:44:43,383:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1402 (5.6820)
2022-11-08 17:44:43,387:INFO: Dataset: zara2               Batch: 23/36	Loss 5.1229 (5.6565)
2022-11-08 17:44:43,390:INFO: Dataset: zara2               Batch: 24/36	Loss 5.0342 (5.6298)
2022-11-08 17:44:43,394:INFO: Dataset: zara2               Batch: 25/36	Loss 4.6907 (5.5887)
2022-11-08 17:44:43,397:INFO: Dataset: zara2               Batch: 26/36	Loss 5.0447 (5.5681)
2022-11-08 17:44:43,402:INFO: Dataset: zara2               Batch: 27/36	Loss 4.3656 (5.5258)
2022-11-08 17:44:43,405:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7590 (5.4930)
2022-11-08 17:44:43,408:INFO: Dataset: zara2               Batch: 29/36	Loss 4.3678 (5.4586)
2022-11-08 17:44:43,413:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3179 (5.4195)
2022-11-08 17:44:43,416:INFO: Dataset: zara2               Batch: 31/36	Loss 3.9493 (5.3730)
2022-11-08 17:44:43,420:INFO: Dataset: zara2               Batch: 32/36	Loss 4.1231 (5.3306)
2022-11-08 17:44:43,425:INFO: Dataset: zara2               Batch: 33/36	Loss 4.4856 (5.3050)
2022-11-08 17:44:43,429:INFO: Dataset: zara2               Batch: 34/36	Loss 4.1748 (5.2696)
2022-11-08 17:44:43,434:INFO: Dataset: zara2               Batch: 35/36	Loss 4.1175 (5.2336)
2022-11-08 17:44:43,437:INFO: Dataset: zara2               Batch: 36/36	Loss 4.0407 (5.2096)
2022-11-08 17:44:44,339:INFO: - Computing loss (validation)
2022-11-08 17:44:50,285:INFO: Dataset: hotel               Batch: 1/3	Loss 16.6916 (16.6916)
2022-11-08 17:44:50,678:INFO: Dataset: hotel               Batch: 2/3	Loss 16.9075 (16.7920)
2022-11-08 17:44:50,984:INFO: Dataset: hotel               Batch: 3/3	Loss 17.6770 (16.8584)
2022-11-08 17:44:59,229:INFO: Dataset: univ                Batch: 1/6	Loss 9.6246 (9.6246)
2022-11-08 17:44:59,647:INFO: Dataset: univ                Batch: 2/6	Loss 8.6243 (9.0733)
2022-11-08 17:45:00,060:INFO: Dataset: univ                Batch: 3/6	Loss 8.4894 (8.8632)
2022-11-08 17:45:00,381:INFO: Dataset: univ                Batch: 4/6	Loss 8.8931 (8.8702)
2022-11-08 17:45:00,610:INFO: Dataset: univ                Batch: 5/6	Loss 9.6011 (8.9777)
2022-11-08 17:45:00,885:INFO: Dataset: univ                Batch: 6/6	Loss 9.5182 (9.0496)
2022-11-08 17:45:08,896:INFO: Dataset: zara1               Batch: 1/3	Loss 11.7642 (11.7642)
2022-11-08 17:45:09,275:INFO: Dataset: zara1               Batch: 2/3	Loss 12.1598 (11.9465)
2022-11-08 17:45:09,658:INFO: Dataset: zara1               Batch: 3/3	Loss 12.0482 (11.9688)
2022-11-08 17:45:17,712:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1192 (4.1192)
2022-11-08 17:45:18,137:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2623 (4.1992)
2022-11-08 17:45:18,528:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0076 (4.1273)
2022-11-08 17:45:18,863:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1944 (4.1432)
2022-11-08 17:45:19,165:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1813 (4.1502)
2022-11-08 17:45:19,373:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0153 (4.1285)
2022-11-08 17:45:19,374:INFO: Dataset: zara2               Batch:  7/10	Loss 3.9839 (4.1091)
2022-11-08 17:45:19,375:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0536 (4.1022)
2022-11-08 17:45:19,377:INFO: Dataset: zara2               Batch:  9/10	Loss 4.0146 (4.0932)
2022-11-08 17:45:19,378:INFO: Dataset: zara2               Batch: 10/10	Loss 4.0398 (4.0873)
2022-11-08 17:45:20,297:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1059.pth.tar
2022-11-08 17:45:20,299:INFO: 
===> EPOCH: 1060 (P5)
2022-11-08 17:45:20,299:INFO: - Computing loss (training)
2022-11-08 17:45:26,566:INFO: Dataset: hotel               Batch: 1/8	Loss 14.4334 (14.4334)
2022-11-08 17:45:26,920:INFO: Dataset: hotel               Batch: 2/8	Loss 15.0899 (14.7586)
2022-11-08 17:45:27,320:INFO: Dataset: hotel               Batch: 3/8	Loss 14.4028 (14.6506)
2022-11-08 17:45:27,658:INFO: Dataset: hotel               Batch: 4/8	Loss 13.9823 (14.4745)
2022-11-08 17:45:27,919:INFO: Dataset: hotel               Batch: 5/8	Loss 14.5012 (14.4799)
2022-11-08 17:45:28,088:INFO: Dataset: hotel               Batch: 6/8	Loss 13.0409 (14.2484)
2022-11-08 17:45:28,091:INFO: Dataset: hotel               Batch: 7/8	Loss 13.9819 (14.2121)
2022-11-08 17:45:28,093:INFO: Dataset: hotel               Batch: 8/8	Loss 14.2608 (14.2136)
2022-11-08 17:45:52,603:INFO: Dataset: univ                Batch:  1/29	Loss 9.6516 (9.6516)
2022-11-08 17:45:52,607:INFO: Dataset: univ                Batch:  2/29	Loss 9.0409 (9.3230)
2022-11-08 17:45:52,611:INFO: Dataset: univ                Batch:  3/29	Loss 9.3281 (9.3247)
2022-11-08 17:45:52,614:INFO: Dataset: univ                Batch:  4/29	Loss 9.4155 (9.3469)
2022-11-08 17:45:52,630:INFO: Dataset: univ                Batch:  5/29	Loss 9.1490 (9.3040)
2022-11-08 17:45:52,639:INFO: Dataset: univ                Batch:  6/29	Loss 9.1631 (9.2818)
2022-11-08 17:45:52,643:INFO: Dataset: univ                Batch:  7/29	Loss 9.0579 (9.2475)
2022-11-08 17:45:52,648:INFO: Dataset: univ                Batch:  8/29	Loss 9.0120 (9.2181)
2022-11-08 17:45:52,650:INFO: Dataset: univ                Batch:  9/29	Loss 8.8083 (9.1710)
2022-11-08 17:45:52,653:INFO: Dataset: univ                Batch: 10/29	Loss 9.1817 (9.1721)
2022-11-08 17:45:52,657:INFO: Dataset: univ                Batch: 11/29	Loss 9.1905 (9.1735)
2022-11-08 17:45:52,661:INFO: Dataset: univ                Batch: 12/29	Loss 8.8385 (9.1466)
2022-11-08 17:45:52,664:INFO: Dataset: univ                Batch: 13/29	Loss 8.4627 (9.0876)
2022-11-08 17:45:52,666:INFO: Dataset: univ                Batch: 14/29	Loss 8.4112 (9.0357)
2022-11-08 17:45:52,671:INFO: Dataset: univ                Batch: 15/29	Loss 8.3046 (8.9876)
2022-11-08 17:45:52,673:INFO: Dataset: univ                Batch: 16/29	Loss 8.0540 (8.9240)
2022-11-08 17:45:52,676:INFO: Dataset: univ                Batch: 17/29	Loss 8.3581 (8.8945)
2022-11-08 17:45:52,679:INFO: Dataset: univ                Batch: 18/29	Loss 7.8172 (8.8416)
2022-11-08 17:45:52,682:INFO: Dataset: univ                Batch: 19/29	Loss 7.9587 (8.7930)
2022-11-08 17:45:52,685:INFO: Dataset: univ                Batch: 20/29	Loss 8.0268 (8.7635)
2022-11-08 17:45:52,688:INFO: Dataset: univ                Batch: 21/29	Loss 7.9988 (8.7235)
2022-11-08 17:45:52,693:INFO: Dataset: univ                Batch: 22/29	Loss 7.5496 (8.6590)
2022-11-08 17:45:52,697:INFO: Dataset: univ                Batch: 23/29	Loss 7.6710 (8.6121)
2022-11-08 17:45:52,700:INFO: Dataset: univ                Batch: 24/29	Loss 6.9871 (8.5331)
2022-11-08 17:45:52,704:INFO: Dataset: univ                Batch: 25/29	Loss 7.2773 (8.4847)
2022-11-08 17:45:52,707:INFO: Dataset: univ                Batch: 26/29	Loss 6.7912 (8.4175)
2022-11-08 17:45:52,710:INFO: Dataset: univ                Batch: 27/29	Loss 6.8699 (8.3542)
2022-11-08 17:45:52,713:INFO: Dataset: univ                Batch: 28/29	Loss 6.6823 (8.2887)
2022-11-08 17:45:52,716:INFO: Dataset: univ                Batch: 29/29	Loss 6.7562 (8.2676)
2022-11-08 17:46:00,080:INFO: Dataset: zara1               Batch:  1/16	Loss 13.2452 (13.2452)
2022-11-08 17:46:00,506:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2388 (13.2419)
2022-11-08 17:46:00,848:INFO: Dataset: zara1               Batch:  3/16	Loss 13.2466 (13.2434)
2022-11-08 17:46:01,157:INFO: Dataset: zara1               Batch:  4/16	Loss 13.8496 (13.4054)
2022-11-08 17:46:01,367:INFO: Dataset: zara1               Batch:  5/16	Loss 13.2336 (13.3680)
2022-11-08 17:46:01,605:INFO: Dataset: zara1               Batch:  6/16	Loss 13.5520 (13.3961)
2022-11-08 17:46:01,610:INFO: Dataset: zara1               Batch:  7/16	Loss 13.5476 (13.4215)
2022-11-08 17:46:01,614:INFO: Dataset: zara1               Batch:  8/16	Loss 13.2742 (13.4037)
2022-11-08 17:46:01,617:INFO: Dataset: zara1               Batch:  9/16	Loss 12.6723 (13.3222)
2022-11-08 17:46:01,624:INFO: Dataset: zara1               Batch: 10/16	Loss 12.9971 (13.2920)
2022-11-08 17:46:01,627:INFO: Dataset: zara1               Batch: 11/16	Loss 12.1688 (13.1850)
2022-11-08 17:46:01,632:INFO: Dataset: zara1               Batch: 12/16	Loss 13.0713 (13.1738)
2022-11-08 17:46:01,638:INFO: Dataset: zara1               Batch: 13/16	Loss 12.6625 (13.1310)
2022-11-08 17:46:01,642:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3142 (13.0759)
2022-11-08 17:46:01,646:INFO: Dataset: zara1               Batch: 15/16	Loss 12.5887 (13.0458)
2022-11-08 17:46:01,650:INFO: Dataset: zara1               Batch: 16/16	Loss 12.3317 (13.0142)
2022-11-08 17:46:27,130:INFO: Dataset: zara2               Batch:  1/36	Loss 6.5220 (6.5220)
2022-11-08 17:46:27,134:INFO: Dataset: zara2               Batch:  2/36	Loss 6.5173 (6.5198)
2022-11-08 17:46:27,139:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1335 (6.3900)
2022-11-08 17:46:27,144:INFO: Dataset: zara2               Batch:  4/36	Loss 5.7839 (6.2410)
2022-11-08 17:46:27,159:INFO: Dataset: zara2               Batch:  5/36	Loss 6.9200 (6.3576)
2022-11-08 17:46:27,168:INFO: Dataset: zara2               Batch:  6/36	Loss 6.2305 (6.3363)
2022-11-08 17:46:27,172:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0481 (6.2884)
2022-11-08 17:46:27,176:INFO: Dataset: zara2               Batch:  8/36	Loss 5.6476 (6.2083)
2022-11-08 17:46:27,181:INFO: Dataset: zara2               Batch:  9/36	Loss 5.7295 (6.1507)
2022-11-08 17:46:27,185:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6625 (6.1053)
2022-11-08 17:46:27,191:INFO: Dataset: zara2               Batch: 11/36	Loss 6.0540 (6.1009)
2022-11-08 17:46:27,196:INFO: Dataset: zara2               Batch: 12/36	Loss 5.3826 (6.0354)
2022-11-08 17:46:27,200:INFO: Dataset: zara2               Batch: 13/36	Loss 5.5384 (5.9963)
2022-11-08 17:46:27,203:INFO: Dataset: zara2               Batch: 14/36	Loss 6.1366 (6.0050)
2022-11-08 17:46:27,206:INFO: Dataset: zara2               Batch: 15/36	Loss 5.1138 (5.9426)
2022-11-08 17:46:27,209:INFO: Dataset: zara2               Batch: 16/36	Loss 5.5257 (5.9167)
2022-11-08 17:46:27,213:INFO: Dataset: zara2               Batch: 17/36	Loss 5.7926 (5.9088)
2022-11-08 17:46:27,218:INFO: Dataset: zara2               Batch: 18/36	Loss 5.4235 (5.8790)
2022-11-08 17:46:27,221:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2870 (5.8426)
2022-11-08 17:46:27,225:INFO: Dataset: zara2               Batch: 20/36	Loss 5.1508 (5.8071)
2022-11-08 17:46:27,228:INFO: Dataset: zara2               Batch: 21/36	Loss 4.9196 (5.7590)
2022-11-08 17:46:27,232:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1031 (5.7274)
2022-11-08 17:46:27,236:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0935 (5.7029)
2022-11-08 17:46:27,241:INFO: Dataset: zara2               Batch: 24/36	Loss 4.8159 (5.6650)
2022-11-08 17:46:27,246:INFO: Dataset: zara2               Batch: 25/36	Loss 4.3888 (5.6126)
2022-11-08 17:46:27,251:INFO: Dataset: zara2               Batch: 26/36	Loss 4.5272 (5.5648)
2022-11-08 17:46:27,256:INFO: Dataset: zara2               Batch: 27/36	Loss 4.5037 (5.5243)
2022-11-08 17:46:27,259:INFO: Dataset: zara2               Batch: 28/36	Loss 4.1526 (5.4773)
2022-11-08 17:46:27,263:INFO: Dataset: zara2               Batch: 29/36	Loss 4.2063 (5.4301)
2022-11-08 17:46:27,266:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5339 (5.4044)
2022-11-08 17:46:27,270:INFO: Dataset: zara2               Batch: 31/36	Loss 4.6203 (5.3777)
2022-11-08 17:46:27,274:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4514 (5.3512)
2022-11-08 17:46:27,278:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2942 (5.3159)
2022-11-08 17:46:27,281:INFO: Dataset: zara2               Batch: 34/36	Loss 4.1118 (5.2727)
2022-11-08 17:46:27,285:INFO: Dataset: zara2               Batch: 35/36	Loss 4.1613 (5.2382)
2022-11-08 17:46:27,289:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9353 (5.2115)
2022-11-08 17:46:28,260:INFO: - Computing loss (validation)
2022-11-08 17:46:35,131:INFO: Dataset: hotel               Batch: 1/3	Loss 16.3790 (16.3790)
2022-11-08 17:46:35,462:INFO: Dataset: hotel               Batch: 2/3	Loss 17.9532 (17.1433)
2022-11-08 17:46:35,875:INFO: Dataset: hotel               Batch: 3/3	Loss 17.5257 (17.1655)
2022-11-08 17:46:44,046:INFO: Dataset: univ                Batch: 1/6	Loss 8.7830 (8.7830)
2022-11-08 17:46:44,503:INFO: Dataset: univ                Batch: 2/6	Loss 9.2334 (9.0180)
2022-11-08 17:46:44,818:INFO: Dataset: univ                Batch: 3/6	Loss 9.2883 (9.1018)
2022-11-08 17:46:45,230:INFO: Dataset: univ                Batch: 4/6	Loss 8.8633 (9.0378)
2022-11-08 17:46:45,516:INFO: Dataset: univ                Batch: 5/6	Loss 9.0291 (9.0359)
2022-11-08 17:46:45,685:INFO: Dataset: univ                Batch: 6/6	Loss 9.0160 (9.0330)
2022-11-08 17:46:52,592:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9248 (11.9248)
2022-11-08 17:46:53,028:INFO: Dataset: zara1               Batch: 2/3	Loss 11.9751 (11.9478)
2022-11-08 17:46:53,349:INFO: Dataset: zara1               Batch: 3/3	Loss 11.9697 (11.9532)
2022-11-08 17:47:01,635:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1719 (4.1719)
2022-11-08 17:47:02,018:INFO: Dataset: zara2               Batch:  2/10	Loss 4.0014 (4.0871)
2022-11-08 17:47:02,378:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0185 (4.0653)
2022-11-08 17:47:02,607:INFO: Dataset: zara2               Batch:  4/10	Loss 3.9568 (4.0390)
2022-11-08 17:47:02,880:INFO: Dataset: zara2               Batch:  5/10	Loss 4.0374 (4.0387)
2022-11-08 17:47:03,062:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0119 (4.0343)
2022-11-08 17:47:03,064:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1763 (4.0524)
2022-11-08 17:47:03,065:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0494 (4.0520)
2022-11-08 17:47:03,067:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2034 (4.0675)
2022-11-08 17:47:03,068:INFO: Dataset: zara2               Batch: 10/10	Loss 3.9563 (4.0561)
2022-11-08 17:47:04,005:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1060.pth.tar
2022-11-08 17:47:04,005:INFO: 
===> EPOCH: 1061 (P5)
2022-11-08 17:47:04,006:INFO: - Computing loss (training)
2022-11-08 17:47:10,309:INFO: Dataset: hotel               Batch: 1/8	Loss 13.4579 (13.4579)
2022-11-08 17:47:10,695:INFO: Dataset: hotel               Batch: 2/8	Loss 14.4953 (13.9637)
2022-11-08 17:47:11,063:INFO: Dataset: hotel               Batch: 3/8	Loss 14.4439 (14.1345)
2022-11-08 17:47:11,345:INFO: Dataset: hotel               Batch: 4/8	Loss 13.0103 (13.8700)
2022-11-08 17:47:11,651:INFO: Dataset: hotel               Batch: 5/8	Loss 13.8661 (13.8692)
2022-11-08 17:47:11,798:INFO: Dataset: hotel               Batch: 6/8	Loss 15.4283 (14.1353)
2022-11-08 17:47:11,804:INFO: Dataset: hotel               Batch: 7/8	Loss 15.5619 (14.3599)
2022-11-08 17:47:11,807:INFO: Dataset: hotel               Batch: 8/8	Loss 13.4361 (14.3343)
2022-11-08 17:47:34,634:INFO: Dataset: univ                Batch:  1/29	Loss 9.4598 (9.4598)
2022-11-08 17:47:34,639:INFO: Dataset: univ                Batch:  2/29	Loss 9.3814 (9.4217)
2022-11-08 17:47:34,644:INFO: Dataset: univ                Batch:  3/29	Loss 9.5031 (9.4503)
2022-11-08 17:47:34,652:INFO: Dataset: univ                Batch:  4/29	Loss 9.2013 (9.3812)
2022-11-08 17:47:34,657:INFO: Dataset: univ                Batch:  5/29	Loss 9.5407 (9.4119)
2022-11-08 17:47:34,663:INFO: Dataset: univ                Batch:  6/29	Loss 9.3579 (9.4022)
2022-11-08 17:47:34,667:INFO: Dataset: univ                Batch:  7/29	Loss 8.8286 (9.3168)
2022-11-08 17:47:34,672:INFO: Dataset: univ                Batch:  8/29	Loss 9.0441 (9.2842)
2022-11-08 17:47:34,677:INFO: Dataset: univ                Batch:  9/29	Loss 9.4262 (9.2997)
2022-11-08 17:47:34,682:INFO: Dataset: univ                Batch: 10/29	Loss 8.9413 (9.2639)
2022-11-08 17:47:34,686:INFO: Dataset: univ                Batch: 11/29	Loss 8.6882 (9.2143)
2022-11-08 17:47:34,691:INFO: Dataset: univ                Batch: 12/29	Loss 8.5585 (9.1550)
2022-11-08 17:47:34,696:INFO: Dataset: univ                Batch: 13/29	Loss 8.7598 (9.1252)
2022-11-08 17:47:34,702:INFO: Dataset: univ                Batch: 14/29	Loss 8.7143 (9.0976)
2022-11-08 17:47:34,708:INFO: Dataset: univ                Batch: 15/29	Loss 8.3837 (9.0501)
2022-11-08 17:47:34,713:INFO: Dataset: univ                Batch: 16/29	Loss 8.2793 (9.0054)
2022-11-08 17:47:34,719:INFO: Dataset: univ                Batch: 17/29	Loss 8.3398 (8.9671)
2022-11-08 17:47:34,723:INFO: Dataset: univ                Batch: 18/29	Loss 7.8720 (8.8976)
2022-11-08 17:47:34,726:INFO: Dataset: univ                Batch: 19/29	Loss 8.4170 (8.8770)
2022-11-08 17:47:34,731:INFO: Dataset: univ                Batch: 20/29	Loss 7.9830 (8.8306)
2022-11-08 17:47:34,736:INFO: Dataset: univ                Batch: 21/29	Loss 7.8952 (8.7895)
2022-11-08 17:47:34,741:INFO: Dataset: univ                Batch: 22/29	Loss 7.6235 (8.7364)
2022-11-08 17:47:34,746:INFO: Dataset: univ                Batch: 23/29	Loss 7.3224 (8.6748)
2022-11-08 17:47:34,751:INFO: Dataset: univ                Batch: 24/29	Loss 7.5797 (8.6315)
2022-11-08 17:47:34,756:INFO: Dataset: univ                Batch: 25/29	Loss 6.9376 (8.5564)
2022-11-08 17:47:34,761:INFO: Dataset: univ                Batch: 26/29	Loss 7.1793 (8.5073)
2022-11-08 17:47:34,768:INFO: Dataset: univ                Batch: 27/29	Loss 7.1844 (8.4613)
2022-11-08 17:47:34,773:INFO: Dataset: univ                Batch: 28/29	Loss 6.9042 (8.4070)
2022-11-08 17:47:34,777:INFO: Dataset: univ                Batch: 29/29	Loss 6.8323 (8.3913)
2022-11-08 17:47:41,658:INFO: Dataset: zara1               Batch:  1/16	Loss 13.3579 (13.3579)
2022-11-08 17:47:41,996:INFO: Dataset: zara1               Batch:  2/16	Loss 13.0492 (13.2103)
2022-11-08 17:47:42,431:INFO: Dataset: zara1               Batch:  3/16	Loss 13.1593 (13.1948)
2022-11-08 17:47:42,698:INFO: Dataset: zara1               Batch:  4/16	Loss 12.9635 (13.1384)
2022-11-08 17:47:42,987:INFO: Dataset: zara1               Batch:  5/16	Loss 13.2310 (13.1586)
2022-11-08 17:47:43,179:INFO: Dataset: zara1               Batch:  6/16	Loss 13.1196 (13.1515)
2022-11-08 17:47:43,182:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0086 (13.1307)
2022-11-08 17:47:43,185:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8858 (13.1048)
2022-11-08 17:47:43,189:INFO: Dataset: zara1               Batch:  9/16	Loss 13.0011 (13.0950)
2022-11-08 17:47:43,192:INFO: Dataset: zara1               Batch: 10/16	Loss 12.7406 (13.0622)
2022-11-08 17:47:43,196:INFO: Dataset: zara1               Batch: 11/16	Loss 12.8226 (13.0376)
2022-11-08 17:47:43,199:INFO: Dataset: zara1               Batch: 12/16	Loss 13.0243 (13.0363)
2022-11-08 17:47:43,202:INFO: Dataset: zara1               Batch: 13/16	Loss 12.4011 (12.9894)
2022-11-08 17:47:43,206:INFO: Dataset: zara1               Batch: 14/16	Loss 12.7498 (12.9752)
2022-11-08 17:47:43,208:INFO: Dataset: zara1               Batch: 15/16	Loss 12.3735 (12.9392)
2022-11-08 17:47:43,211:INFO: Dataset: zara1               Batch: 16/16	Loss 12.4272 (12.9136)
2022-11-08 17:48:05,915:INFO: Dataset: zara2               Batch:  1/36	Loss 6.0229 (6.0229)
2022-11-08 17:48:05,922:INFO: Dataset: zara2               Batch:  2/36	Loss 6.2470 (6.1427)
2022-11-08 17:48:05,928:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1753 (6.1542)
2022-11-08 17:48:05,932:INFO: Dataset: zara2               Batch:  4/36	Loss 6.1107 (6.1430)
2022-11-08 17:48:05,948:INFO: Dataset: zara2               Batch:  5/36	Loss 6.0036 (6.1129)
2022-11-08 17:48:05,956:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1814 (6.1244)
2022-11-08 17:48:05,959:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0114 (6.1083)
2022-11-08 17:48:05,964:INFO: Dataset: zara2               Batch:  8/36	Loss 5.9442 (6.0869)
2022-11-08 17:48:05,968:INFO: Dataset: zara2               Batch:  9/36	Loss 5.9182 (6.0649)
2022-11-08 17:48:05,972:INFO: Dataset: zara2               Batch: 10/36	Loss 5.7894 (6.0372)
2022-11-08 17:48:05,975:INFO: Dataset: zara2               Batch: 11/36	Loss 5.6701 (5.9950)
2022-11-08 17:48:05,980:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7642 (5.9739)
2022-11-08 17:48:05,986:INFO: Dataset: zara2               Batch: 13/36	Loss 5.4578 (5.9375)
2022-11-08 17:48:05,989:INFO: Dataset: zara2               Batch: 14/36	Loss 5.4876 (5.9062)
2022-11-08 17:48:05,993:INFO: Dataset: zara2               Batch: 15/36	Loss 5.0917 (5.8533)
2022-11-08 17:48:05,997:INFO: Dataset: zara2               Batch: 16/36	Loss 5.7224 (5.8459)
2022-11-08 17:48:06,000:INFO: Dataset: zara2               Batch: 17/36	Loss 5.7797 (5.8422)
2022-11-08 17:48:06,004:INFO: Dataset: zara2               Batch: 18/36	Loss 4.8511 (5.7908)
2022-11-08 17:48:06,008:INFO: Dataset: zara2               Batch: 19/36	Loss 5.1933 (5.7613)
2022-11-08 17:48:06,012:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0939 (5.7305)
2022-11-08 17:48:06,016:INFO: Dataset: zara2               Batch: 21/36	Loss 4.6686 (5.6857)
2022-11-08 17:48:06,020:INFO: Dataset: zara2               Batch: 22/36	Loss 4.8691 (5.6513)
2022-11-08 17:48:06,025:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0756 (5.6230)
2022-11-08 17:48:06,028:INFO: Dataset: zara2               Batch: 24/36	Loss 5.1461 (5.6030)
2022-11-08 17:48:06,031:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8218 (5.5720)
2022-11-08 17:48:06,034:INFO: Dataset: zara2               Batch: 26/36	Loss 4.6569 (5.5346)
2022-11-08 17:48:06,039:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7375 (5.5027)
2022-11-08 17:48:06,043:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5581 (5.4635)
2022-11-08 17:48:06,046:INFO: Dataset: zara2               Batch: 29/36	Loss 4.3168 (5.4258)
2022-11-08 17:48:06,049:INFO: Dataset: zara2               Batch: 30/36	Loss 4.2406 (5.3852)
2022-11-08 17:48:06,054:INFO: Dataset: zara2               Batch: 31/36	Loss 4.4254 (5.3516)
2022-11-08 17:48:06,058:INFO: Dataset: zara2               Batch: 32/36	Loss 4.1211 (5.3159)
2022-11-08 17:48:06,062:INFO: Dataset: zara2               Batch: 33/36	Loss 4.6293 (5.2937)
2022-11-08 17:48:06,066:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3318 (5.2660)
2022-11-08 17:48:06,071:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0556 (5.2304)
2022-11-08 17:48:06,074:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1220 (5.2102)
2022-11-08 17:48:06,964:INFO: - Computing loss (validation)
2022-11-08 17:48:12,881:INFO: Dataset: hotel               Batch: 1/3	Loss 17.0115 (17.0115)
2022-11-08 17:48:13,230:INFO: Dataset: hotel               Batch: 2/3	Loss 16.6656 (16.8373)
2022-11-08 17:48:13,565:INFO: Dataset: hotel               Batch: 3/3	Loss 15.7029 (16.7560)
2022-11-08 17:48:21,601:INFO: Dataset: univ                Batch: 1/6	Loss 9.1537 (9.1537)
2022-11-08 17:48:21,978:INFO: Dataset: univ                Batch: 2/6	Loss 8.9349 (9.0438)
2022-11-08 17:48:22,297:INFO: Dataset: univ                Batch: 3/6	Loss 8.8110 (8.9641)
2022-11-08 17:48:22,547:INFO: Dataset: univ                Batch: 4/6	Loss 9.9429 (9.1549)
2022-11-08 17:48:22,841:INFO: Dataset: univ                Batch: 5/6	Loss 8.6972 (9.0555)
2022-11-08 17:48:23,021:INFO: Dataset: univ                Batch: 6/6	Loss 8.6732 (8.9882)
2022-11-08 17:48:29,958:INFO: Dataset: zara1               Batch: 1/3	Loss 12.0055 (12.0055)
2022-11-08 17:48:30,332:INFO: Dataset: zara1               Batch: 2/3	Loss 12.0430 (12.0229)
2022-11-08 17:48:30,610:INFO: Dataset: zara1               Batch: 3/3	Loss 12.2333 (12.0730)
2022-11-08 17:48:38,239:INFO: Dataset: zara2               Batch:  1/10	Loss 3.9612 (3.9612)
2022-11-08 17:48:38,558:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1302 (4.0457)
2022-11-08 17:48:38,974:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0734 (4.0546)
2022-11-08 17:48:39,214:INFO: Dataset: zara2               Batch:  4/10	Loss 4.0633 (4.0570)
2022-11-08 17:48:39,469:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2839 (4.1064)
2022-11-08 17:48:39,656:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1175 (4.1083)
2022-11-08 17:48:39,658:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1600 (4.1168)
2022-11-08 17:48:39,659:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0861 (4.1130)
2022-11-08 17:48:39,660:INFO: Dataset: zara2               Batch:  9/10	Loss 3.9677 (4.0977)
2022-11-08 17:48:39,661:INFO: Dataset: zara2               Batch: 10/10	Loss 3.9712 (4.0862)
2022-11-08 17:48:40,587:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1061.pth.tar
2022-11-08 17:48:40,587:INFO: 
===> EPOCH: 1062 (P5)
2022-11-08 17:48:40,588:INFO: - Computing loss (training)
2022-11-08 17:48:46,594:INFO: Dataset: hotel               Batch: 1/8	Loss 13.2900 (13.2900)
2022-11-08 17:48:46,942:INFO: Dataset: hotel               Batch: 2/8	Loss 14.5138 (13.8441)
2022-11-08 17:48:47,335:INFO: Dataset: hotel               Batch: 3/8	Loss 13.6493 (13.7740)
2022-11-08 17:48:47,611:INFO: Dataset: hotel               Batch: 4/8	Loss 15.5889 (14.1989)
2022-11-08 17:48:47,879:INFO: Dataset: hotel               Batch: 5/8	Loss 14.7164 (14.3036)
2022-11-08 17:48:48,116:INFO: Dataset: hotel               Batch: 6/8	Loss 12.5214 (14.0182)
2022-11-08 17:48:48,120:INFO: Dataset: hotel               Batch: 7/8	Loss 14.3087 (14.0652)
2022-11-08 17:48:48,124:INFO: Dataset: hotel               Batch: 8/8	Loss 14.8751 (14.0951)
2022-11-08 17:49:11,022:INFO: Dataset: univ                Batch:  1/29	Loss 9.3609 (9.3609)
2022-11-08 17:49:11,028:INFO: Dataset: univ                Batch:  2/29	Loss 9.2544 (9.3112)
2022-11-08 17:49:11,031:INFO: Dataset: univ                Batch:  3/29	Loss 10.0372 (9.5274)
2022-11-08 17:49:11,044:INFO: Dataset: univ                Batch:  4/29	Loss 9.5359 (9.5295)
2022-11-08 17:49:11,049:INFO: Dataset: univ                Batch:  5/29	Loss 9.0630 (9.4330)
2022-11-08 17:49:11,054:INFO: Dataset: univ                Batch:  6/29	Loss 9.3371 (9.4178)
2022-11-08 17:49:11,058:INFO: Dataset: univ                Batch:  7/29	Loss 9.0488 (9.3661)
2022-11-08 17:49:11,064:INFO: Dataset: univ                Batch:  8/29	Loss 8.9511 (9.3172)
2022-11-08 17:49:11,070:INFO: Dataset: univ                Batch:  9/29	Loss 9.3334 (9.3191)
2022-11-08 17:49:11,076:INFO: Dataset: univ                Batch: 10/29	Loss 8.8596 (9.2695)
2022-11-08 17:49:11,080:INFO: Dataset: univ                Batch: 11/29	Loss 8.7235 (9.2148)
2022-11-08 17:49:11,087:INFO: Dataset: univ                Batch: 12/29	Loss 8.3229 (9.1302)
2022-11-08 17:49:11,093:INFO: Dataset: univ                Batch: 13/29	Loss 8.4496 (9.0705)
2022-11-08 17:49:11,096:INFO: Dataset: univ                Batch: 14/29	Loss 8.4675 (9.0274)
2022-11-08 17:49:11,102:INFO: Dataset: univ                Batch: 15/29	Loss 8.5613 (8.9994)
2022-11-08 17:49:11,109:INFO: Dataset: univ                Batch: 16/29	Loss 8.1580 (8.9469)
2022-11-08 17:49:11,116:INFO: Dataset: univ                Batch: 17/29	Loss 8.0681 (8.8977)
2022-11-08 17:49:11,123:INFO: Dataset: univ                Batch: 18/29	Loss 8.1109 (8.8529)
2022-11-08 17:49:11,131:INFO: Dataset: univ                Batch: 19/29	Loss 7.5945 (8.7850)
2022-11-08 17:49:11,135:INFO: Dataset: univ                Batch: 20/29	Loss 7.9011 (8.7444)
2022-11-08 17:49:11,141:INFO: Dataset: univ                Batch: 21/29	Loss 7.3003 (8.6748)
2022-11-08 17:49:11,147:INFO: Dataset: univ                Batch: 22/29	Loss 7.2058 (8.6080)
2022-11-08 17:49:11,153:INFO: Dataset: univ                Batch: 23/29	Loss 7.4890 (8.5605)
2022-11-08 17:49:11,160:INFO: Dataset: univ                Batch: 24/29	Loss 7.3509 (8.5102)
2022-11-08 17:49:11,166:INFO: Dataset: univ                Batch: 25/29	Loss 7.0415 (8.4545)
2022-11-08 17:49:11,171:INFO: Dataset: univ                Batch: 26/29	Loss 6.7482 (8.3809)
2022-11-08 17:49:11,176:INFO: Dataset: univ                Batch: 27/29	Loss 6.8521 (8.3211)
2022-11-08 17:49:11,182:INFO: Dataset: univ                Batch: 28/29	Loss 6.6803 (8.2504)
2022-11-08 17:49:11,189:INFO: Dataset: univ                Batch: 29/29	Loss 6.5579 (8.2269)
2022-11-08 17:49:18,007:INFO: Dataset: zara1               Batch:  1/16	Loss 12.9314 (12.9314)
2022-11-08 17:49:18,375:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2061 (13.0758)
2022-11-08 17:49:18,744:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4945 (13.2274)
2022-11-08 17:49:18,992:INFO: Dataset: zara1               Batch:  4/16	Loss 13.0533 (13.1773)
2022-11-08 17:49:19,303:INFO: Dataset: zara1               Batch:  5/16	Loss 13.4020 (13.2216)
2022-11-08 17:49:19,451:INFO: Dataset: zara1               Batch:  6/16	Loss 13.8292 (13.3335)
2022-11-08 17:49:19,454:INFO: Dataset: zara1               Batch:  7/16	Loss 13.2846 (13.3277)
2022-11-08 17:49:19,458:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8583 (13.2664)
2022-11-08 17:49:19,461:INFO: Dataset: zara1               Batch:  9/16	Loss 13.2363 (13.2635)
2022-11-08 17:49:19,465:INFO: Dataset: zara1               Batch: 10/16	Loss 13.2573 (13.2628)
2022-11-08 17:49:19,468:INFO: Dataset: zara1               Batch: 11/16	Loss 12.9667 (13.2364)
2022-11-08 17:49:19,471:INFO: Dataset: zara1               Batch: 12/16	Loss 13.0463 (13.2186)
2022-11-08 17:49:19,474:INFO: Dataset: zara1               Batch: 13/16	Loss 12.3804 (13.1439)
2022-11-08 17:49:19,476:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4174 (13.0844)
2022-11-08 17:49:19,479:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0522 (13.0143)
2022-11-08 17:49:19,482:INFO: Dataset: zara1               Batch: 16/16	Loss 12.5869 (12.9938)
2022-11-08 17:49:42,244:INFO: Dataset: zara2               Batch:  1/36	Loss 6.4247 (6.4247)
2022-11-08 17:49:42,248:INFO: Dataset: zara2               Batch:  2/36	Loss 5.9689 (6.1831)
2022-11-08 17:49:42,253:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4600 (6.2629)
2022-11-08 17:49:42,257:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3092 (6.2738)
2022-11-08 17:49:42,261:INFO: Dataset: zara2               Batch:  5/36	Loss 6.1743 (6.2537)
2022-11-08 17:49:42,280:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1848 (6.2426)
2022-11-08 17:49:42,284:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0028 (6.2087)
2022-11-08 17:49:42,288:INFO: Dataset: zara2               Batch:  8/36	Loss 5.8469 (6.1637)
2022-11-08 17:49:42,293:INFO: Dataset: zara2               Batch:  9/36	Loss 5.7908 (6.1224)
2022-11-08 17:49:42,296:INFO: Dataset: zara2               Batch: 10/36	Loss 6.1770 (6.1278)
2022-11-08 17:49:42,300:INFO: Dataset: zara2               Batch: 11/36	Loss 5.1036 (6.0518)
2022-11-08 17:49:42,303:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7006 (6.0205)
2022-11-08 17:49:42,307:INFO: Dataset: zara2               Batch: 13/36	Loss 5.4803 (5.9742)
2022-11-08 17:49:42,312:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6022 (5.9449)
2022-11-08 17:49:42,315:INFO: Dataset: zara2               Batch: 15/36	Loss 5.6540 (5.9275)
2022-11-08 17:49:42,320:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3935 (5.8952)
2022-11-08 17:49:42,323:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6110 (5.8778)
2022-11-08 17:49:42,326:INFO: Dataset: zara2               Batch: 18/36	Loss 5.1802 (5.8399)
2022-11-08 17:49:42,330:INFO: Dataset: zara2               Batch: 19/36	Loss 5.3057 (5.8103)
2022-11-08 17:49:42,334:INFO: Dataset: zara2               Batch: 20/36	Loss 4.9096 (5.7661)
2022-11-08 17:49:42,338:INFO: Dataset: zara2               Batch: 21/36	Loss 4.8884 (5.7330)
2022-11-08 17:49:42,343:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9854 (5.7002)
2022-11-08 17:49:42,347:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0679 (5.6762)
2022-11-08 17:49:42,351:INFO: Dataset: zara2               Batch: 24/36	Loss 5.2977 (5.6609)
2022-11-08 17:49:42,355:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8132 (5.6320)
2022-11-08 17:49:42,357:INFO: Dataset: zara2               Batch: 26/36	Loss 4.9221 (5.6058)
2022-11-08 17:49:42,360:INFO: Dataset: zara2               Batch: 27/36	Loss 4.3850 (5.5636)
2022-11-08 17:49:42,364:INFO: Dataset: zara2               Batch: 28/36	Loss 4.6422 (5.5285)
2022-11-08 17:49:42,367:INFO: Dataset: zara2               Batch: 29/36	Loss 4.7023 (5.4985)
2022-11-08 17:49:42,370:INFO: Dataset: zara2               Batch: 30/36	Loss 4.4913 (5.4646)
2022-11-08 17:49:42,374:INFO: Dataset: zara2               Batch: 31/36	Loss 4.1681 (5.4216)
2022-11-08 17:49:42,377:INFO: Dataset: zara2               Batch: 32/36	Loss 4.1925 (5.3863)
2022-11-08 17:49:42,380:INFO: Dataset: zara2               Batch: 33/36	Loss 4.1687 (5.3473)
2022-11-08 17:49:42,383:INFO: Dataset: zara2               Batch: 34/36	Loss 3.9208 (5.3124)
2022-11-08 17:49:42,387:INFO: Dataset: zara2               Batch: 35/36	Loss 3.9988 (5.2717)
2022-11-08 17:49:42,390:INFO: Dataset: zara2               Batch: 36/36	Loss 4.0227 (5.2492)
2022-11-08 17:49:43,305:INFO: - Computing loss (validation)
2022-11-08 17:49:49,167:INFO: Dataset: hotel               Batch: 1/3	Loss 17.8148 (17.8148)
2022-11-08 17:49:49,539:INFO: Dataset: hotel               Batch: 2/3	Loss 16.2521 (16.9703)
2022-11-08 17:49:49,932:INFO: Dataset: hotel               Batch: 3/3	Loss 19.3134 (17.1382)
2022-11-08 17:49:57,309:INFO: Dataset: univ                Batch: 1/6	Loss 9.1486 (9.1486)
2022-11-08 17:49:57,712:INFO: Dataset: univ                Batch: 2/6	Loss 8.9401 (9.0541)
2022-11-08 17:49:58,056:INFO: Dataset: univ                Batch: 3/6	Loss 8.9770 (9.0268)
2022-11-08 17:49:58,335:INFO: Dataset: univ                Batch: 4/6	Loss 8.6014 (8.9044)
2022-11-08 17:49:58,547:INFO: Dataset: univ                Batch: 5/6	Loss 9.7479 (9.0640)
2022-11-08 17:49:58,824:INFO: Dataset: univ                Batch: 6/6	Loss 8.4409 (8.9829)
2022-11-08 17:50:05,648:INFO: Dataset: zara1               Batch: 1/3	Loss 12.1286 (12.1286)
2022-11-08 17:50:06,013:INFO: Dataset: zara1               Batch: 2/3	Loss 11.7978 (11.9632)
2022-11-08 17:50:06,366:INFO: Dataset: zara1               Batch: 3/3	Loss 12.4145 (12.0749)
2022-11-08 17:50:13,846:INFO: Dataset: zara2               Batch:  1/10	Loss 3.9605 (3.9605)
2022-11-08 17:50:14,243:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2266 (4.0811)
2022-11-08 17:50:14,621:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2321 (4.1304)
2022-11-08 17:50:14,922:INFO: Dataset: zara2               Batch:  4/10	Loss 3.8972 (4.0742)
2022-11-08 17:50:15,151:INFO: Dataset: zara2               Batch:  5/10	Loss 3.9702 (4.0517)
2022-11-08 17:50:15,389:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0443 (4.0504)
2022-11-08 17:50:15,391:INFO: Dataset: zara2               Batch:  7/10	Loss 3.8581 (4.0213)
2022-11-08 17:50:15,393:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3023 (4.0601)
2022-11-08 17:50:15,394:INFO: Dataset: zara2               Batch:  9/10	Loss 3.7765 (4.0291)
2022-11-08 17:50:15,395:INFO: Dataset: zara2               Batch: 10/10	Loss 4.1516 (4.0425)
2022-11-08 17:50:16,303:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1062.pth.tar
2022-11-08 17:50:16,303:INFO: 
===> EPOCH: 1063 (P5)
2022-11-08 17:50:16,304:INFO: - Computing loss (training)
2022-11-08 17:50:22,248:INFO: Dataset: hotel               Batch: 1/8	Loss 14.9963 (14.9963)
2022-11-08 17:50:22,680:INFO: Dataset: hotel               Batch: 2/8	Loss 14.5996 (14.7970)
2022-11-08 17:50:23,043:INFO: Dataset: hotel               Batch: 3/8	Loss 13.4507 (14.3496)
2022-11-08 17:50:23,353:INFO: Dataset: hotel               Batch: 4/8	Loss 16.4112 (14.8750)
2022-11-08 17:50:23,553:INFO: Dataset: hotel               Batch: 5/8	Loss 14.6736 (14.8311)
2022-11-08 17:50:23,736:INFO: Dataset: hotel               Batch: 6/8	Loss 12.3034 (14.4078)
2022-11-08 17:50:23,744:INFO: Dataset: hotel               Batch: 7/8	Loss 13.5874 (14.2977)
2022-11-08 17:50:23,753:INFO: Dataset: hotel               Batch: 8/8	Loss 14.1159 (14.2929)
2022-11-08 17:50:46,174:INFO: Dataset: univ                Batch:  1/29	Loss 9.4708 (9.4708)
2022-11-08 17:50:46,178:INFO: Dataset: univ                Batch:  2/29	Loss 9.1218 (9.2909)
2022-11-08 17:50:46,183:INFO: Dataset: univ                Batch:  3/29	Loss 9.6209 (9.3944)
2022-11-08 17:50:46,193:INFO: Dataset: univ                Batch:  4/29	Loss 9.4537 (9.4083)
2022-11-08 17:50:46,199:INFO: Dataset: univ                Batch:  5/29	Loss 9.2404 (9.3763)
2022-11-08 17:50:46,204:INFO: Dataset: univ                Batch:  6/29	Loss 9.3848 (9.3775)
2022-11-08 17:50:46,209:INFO: Dataset: univ                Batch:  7/29	Loss 9.1146 (9.3347)
2022-11-08 17:50:46,214:INFO: Dataset: univ                Batch:  8/29	Loss 9.2153 (9.3194)
2022-11-08 17:50:46,219:INFO: Dataset: univ                Batch:  9/29	Loss 8.7813 (9.2588)
2022-11-08 17:50:46,223:INFO: Dataset: univ                Batch: 10/29	Loss 8.8636 (9.2153)
2022-11-08 17:50:46,229:INFO: Dataset: univ                Batch: 11/29	Loss 9.1189 (9.2066)
2022-11-08 17:50:46,234:INFO: Dataset: univ                Batch: 12/29	Loss 8.5289 (9.1482)
2022-11-08 17:50:46,240:INFO: Dataset: univ                Batch: 13/29	Loss 8.8180 (9.1196)
2022-11-08 17:50:46,247:INFO: Dataset: univ                Batch: 14/29	Loss 8.3489 (9.0605)
2022-11-08 17:50:46,252:INFO: Dataset: univ                Batch: 15/29	Loss 7.8389 (8.9708)
2022-11-08 17:50:46,256:INFO: Dataset: univ                Batch: 16/29	Loss 8.0895 (8.9136)
2022-11-08 17:50:46,262:INFO: Dataset: univ                Batch: 17/29	Loss 8.2868 (8.8772)
2022-11-08 17:50:46,269:INFO: Dataset: univ                Batch: 18/29	Loss 8.1385 (8.8314)
2022-11-08 17:50:46,275:INFO: Dataset: univ                Batch: 19/29	Loss 7.8794 (8.7847)
2022-11-08 17:50:46,283:INFO: Dataset: univ                Batch: 20/29	Loss 8.0127 (8.7489)
2022-11-08 17:50:46,288:INFO: Dataset: univ                Batch: 21/29	Loss 7.7708 (8.7029)
2022-11-08 17:50:46,294:INFO: Dataset: univ                Batch: 22/29	Loss 7.7399 (8.6598)
2022-11-08 17:50:46,299:INFO: Dataset: univ                Batch: 23/29	Loss 7.7568 (8.6219)
2022-11-08 17:50:46,303:INFO: Dataset: univ                Batch: 24/29	Loss 7.5376 (8.5762)
2022-11-08 17:50:46,308:INFO: Dataset: univ                Batch: 25/29	Loss 7.1692 (8.5125)
2022-11-08 17:50:46,316:INFO: Dataset: univ                Batch: 26/29	Loss 6.9860 (8.4526)
2022-11-08 17:50:46,322:INFO: Dataset: univ                Batch: 27/29	Loss 6.8957 (8.3949)
2022-11-08 17:50:46,327:INFO: Dataset: univ                Batch: 28/29	Loss 6.4167 (8.3381)
2022-11-08 17:50:46,330:INFO: Dataset: univ                Batch: 29/29	Loss 6.6979 (8.3113)
2022-11-08 17:50:53,169:INFO: Dataset: zara1               Batch:  1/16	Loss 13.4623 (13.4623)
2022-11-08 17:50:53,505:INFO: Dataset: zara1               Batch:  2/16	Loss 13.4130 (13.4361)
2022-11-08 17:50:53,875:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4993 (13.4581)
2022-11-08 17:50:54,170:INFO: Dataset: zara1               Batch:  4/16	Loss 13.1561 (13.3810)
2022-11-08 17:50:54,391:INFO: Dataset: zara1               Batch:  5/16	Loss 12.8820 (13.2878)
2022-11-08 17:50:54,622:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0101 (13.2424)
2022-11-08 17:50:54,626:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0135 (13.2083)
2022-11-08 17:50:54,630:INFO: Dataset: zara1               Batch:  8/16	Loss 13.1481 (13.1993)
2022-11-08 17:50:54,633:INFO: Dataset: zara1               Batch:  9/16	Loss 13.1101 (13.1881)
2022-11-08 17:50:54,636:INFO: Dataset: zara1               Batch: 10/16	Loss 12.8248 (13.1519)
2022-11-08 17:50:54,639:INFO: Dataset: zara1               Batch: 11/16	Loss 12.3835 (13.0827)
2022-11-08 17:50:54,642:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3329 (13.0260)
2022-11-08 17:50:54,645:INFO: Dataset: zara1               Batch: 13/16	Loss 12.5141 (12.9821)
2022-11-08 17:50:54,647:INFO: Dataset: zara1               Batch: 14/16	Loss 12.5628 (12.9481)
2022-11-08 17:50:54,650:INFO: Dataset: zara1               Batch: 15/16	Loss 11.9377 (12.8763)
2022-11-08 17:50:54,654:INFO: Dataset: zara1               Batch: 16/16	Loss 12.3465 (12.8571)
2022-11-08 17:51:17,268:INFO: Dataset: zara2               Batch:  1/36	Loss 6.2027 (6.2027)
2022-11-08 17:51:17,272:INFO: Dataset: zara2               Batch:  2/36	Loss 6.3342 (6.2705)
2022-11-08 17:51:17,276:INFO: Dataset: zara2               Batch:  3/36	Loss 6.5385 (6.3517)
2022-11-08 17:51:17,280:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3078 (6.3412)
2022-11-08 17:51:17,284:INFO: Dataset: zara2               Batch:  5/36	Loss 5.6316 (6.2021)
2022-11-08 17:51:17,304:INFO: Dataset: zara2               Batch:  6/36	Loss 6.4580 (6.2421)
2022-11-08 17:51:17,309:INFO: Dataset: zara2               Batch:  7/36	Loss 5.8738 (6.1891)
2022-11-08 17:51:17,314:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0583 (6.1733)
2022-11-08 17:51:17,319:INFO: Dataset: zara2               Batch:  9/36	Loss 5.5837 (6.1144)
2022-11-08 17:51:17,322:INFO: Dataset: zara2               Batch: 10/36	Loss 5.8855 (6.0914)
2022-11-08 17:51:17,327:INFO: Dataset: zara2               Batch: 11/36	Loss 5.6991 (6.0487)
2022-11-08 17:51:17,333:INFO: Dataset: zara2               Batch: 12/36	Loss 5.5377 (6.0123)
2022-11-08 17:51:17,336:INFO: Dataset: zara2               Batch: 13/36	Loss 5.8739 (6.0008)
2022-11-08 17:51:17,340:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6247 (5.9732)
2022-11-08 17:51:17,343:INFO: Dataset: zara2               Batch: 15/36	Loss 5.4658 (5.9377)
2022-11-08 17:51:17,347:INFO: Dataset: zara2               Batch: 16/36	Loss 5.4734 (5.9103)
2022-11-08 17:51:17,351:INFO: Dataset: zara2               Batch: 17/36	Loss 5.4262 (5.8825)
2022-11-08 17:51:17,356:INFO: Dataset: zara2               Batch: 18/36	Loss 5.2096 (5.8471)
2022-11-08 17:51:17,361:INFO: Dataset: zara2               Batch: 19/36	Loss 5.3141 (5.8202)
2022-11-08 17:51:17,366:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0109 (5.7882)
2022-11-08 17:51:17,369:INFO: Dataset: zara2               Batch: 21/36	Loss 5.1084 (5.7485)
2022-11-08 17:51:17,374:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0456 (5.7164)
2022-11-08 17:51:17,377:INFO: Dataset: zara2               Batch: 23/36	Loss 4.8434 (5.6809)
2022-11-08 17:51:17,383:INFO: Dataset: zara2               Batch: 24/36	Loss 5.1227 (5.6567)
2022-11-08 17:51:17,386:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8954 (5.6169)
2022-11-08 17:51:17,390:INFO: Dataset: zara2               Batch: 26/36	Loss 4.4752 (5.5715)
2022-11-08 17:51:17,395:INFO: Dataset: zara2               Batch: 27/36	Loss 4.3811 (5.5258)
2022-11-08 17:51:17,400:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5860 (5.4895)
2022-11-08 17:51:17,406:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5379 (5.4590)
2022-11-08 17:51:17,410:INFO: Dataset: zara2               Batch: 30/36	Loss 4.2828 (5.4130)
2022-11-08 17:51:17,414:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3137 (5.3765)
2022-11-08 17:51:17,418:INFO: Dataset: zara2               Batch: 32/36	Loss 4.2149 (5.3369)
2022-11-08 17:51:17,422:INFO: Dataset: zara2               Batch: 33/36	Loss 3.9878 (5.3029)
2022-11-08 17:51:17,425:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3827 (5.2756)
2022-11-08 17:51:17,429:INFO: Dataset: zara2               Batch: 35/36	Loss 4.5901 (5.2582)
2022-11-08 17:51:17,433:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9160 (5.2290)
2022-11-08 17:51:18,326:INFO: - Computing loss (validation)
2022-11-08 17:51:24,237:INFO: Dataset: hotel               Batch: 1/3	Loss 16.6135 (16.6135)
2022-11-08 17:51:24,594:INFO: Dataset: hotel               Batch: 2/3	Loss 16.9355 (16.7769)
2022-11-08 17:51:24,911:INFO: Dataset: hotel               Batch: 3/3	Loss 14.9170 (16.6309)
2022-11-08 17:51:32,322:INFO: Dataset: univ                Batch: 1/6	Loss 8.7799 (8.7799)
2022-11-08 17:51:32,743:INFO: Dataset: univ                Batch: 2/6	Loss 9.2734 (9.0221)
2022-11-08 17:51:33,053:INFO: Dataset: univ                Batch: 3/6	Loss 9.0377 (9.0273)
2022-11-08 17:51:33,338:INFO: Dataset: univ                Batch: 4/6	Loss 9.1042 (9.0473)
2022-11-08 17:51:33,530:INFO: Dataset: univ                Batch: 5/6	Loss 8.4583 (8.9227)
2022-11-08 17:51:33,759:INFO: Dataset: univ                Batch: 6/6	Loss 9.0023 (8.9346)
2022-11-08 17:51:40,543:INFO: Dataset: zara1               Batch: 1/3	Loss 12.6983 (12.6983)
2022-11-08 17:51:40,954:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8892 (12.2887)
2022-11-08 17:51:41,364:INFO: Dataset: zara1               Batch: 3/3	Loss 12.0257 (12.2278)
2022-11-08 17:51:48,732:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2796 (4.2796)
2022-11-08 17:51:49,141:INFO: Dataset: zara2               Batch:  2/10	Loss 4.0606 (4.1785)
2022-11-08 17:51:49,518:INFO: Dataset: zara2               Batch:  3/10	Loss 3.8462 (4.0702)
2022-11-08 17:51:49,800:INFO: Dataset: zara2               Batch:  4/10	Loss 4.0497 (4.0652)
2022-11-08 17:51:50,025:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2026 (4.0939)
2022-11-08 17:51:50,251:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0722 (4.0902)
2022-11-08 17:51:50,253:INFO: Dataset: zara2               Batch:  7/10	Loss 3.8826 (4.0633)
2022-11-08 17:51:50,255:INFO: Dataset: zara2               Batch:  8/10	Loss 3.9892 (4.0535)
2022-11-08 17:51:50,256:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2868 (4.0778)
2022-11-08 17:51:50,258:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2939 (4.0982)
2022-11-08 17:51:51,160:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1063.pth.tar
2022-11-08 17:51:51,160:INFO: 
===> EPOCH: 1064 (P5)
2022-11-08 17:51:51,160:INFO: - Computing loss (training)
2022-11-08 17:51:57,105:INFO: Dataset: hotel               Batch: 1/8	Loss 13.3634 (13.3634)
2022-11-08 17:51:57,483:INFO: Dataset: hotel               Batch: 2/8	Loss 15.3542 (14.3637)
2022-11-08 17:51:57,800:INFO: Dataset: hotel               Batch: 3/8	Loss 15.3909 (14.7352)
2022-11-08 17:51:58,076:INFO: Dataset: hotel               Batch: 4/8	Loss 14.3554 (14.6430)
2022-11-08 17:51:58,369:INFO: Dataset: hotel               Batch: 5/8	Loss 14.2603 (14.5688)
2022-11-08 17:51:58,573:INFO: Dataset: hotel               Batch: 6/8	Loss 12.5863 (14.2469)
2022-11-08 17:51:58,579:INFO: Dataset: hotel               Batch: 7/8	Loss 12.6523 (13.9962)
2022-11-08 17:51:58,585:INFO: Dataset: hotel               Batch: 8/8	Loss 13.7742 (13.9904)
2022-11-08 17:52:21,316:INFO: Dataset: univ                Batch:  1/29	Loss 9.3747 (9.3747)
2022-11-08 17:52:21,322:INFO: Dataset: univ                Batch:  2/29	Loss 9.4518 (9.4114)
2022-11-08 17:52:21,328:INFO: Dataset: univ                Batch:  3/29	Loss 9.6982 (9.5133)
2022-11-08 17:52:21,331:INFO: Dataset: univ                Batch:  4/29	Loss 9.2493 (9.4500)
2022-11-08 17:52:21,338:INFO: Dataset: univ                Batch:  5/29	Loss 9.5897 (9.4784)
2022-11-08 17:52:21,356:INFO: Dataset: univ                Batch:  6/29	Loss 9.0881 (9.4104)
2022-11-08 17:52:21,363:INFO: Dataset: univ                Batch:  7/29	Loss 9.1923 (9.3768)
2022-11-08 17:52:21,367:INFO: Dataset: univ                Batch:  8/29	Loss 8.8622 (9.3090)
2022-11-08 17:52:21,371:INFO: Dataset: univ                Batch:  9/29	Loss 8.8718 (9.2593)
2022-11-08 17:52:21,376:INFO: Dataset: univ                Batch: 10/29	Loss 8.9267 (9.2280)
2022-11-08 17:52:21,380:INFO: Dataset: univ                Batch: 11/29	Loss 8.4575 (9.1499)
2022-11-08 17:52:21,385:INFO: Dataset: univ                Batch: 12/29	Loss 8.5961 (9.1096)
2022-11-08 17:52:21,390:INFO: Dataset: univ                Batch: 13/29	Loss 8.2089 (9.0305)
2022-11-08 17:52:21,394:INFO: Dataset: univ                Batch: 14/29	Loss 8.2524 (8.9702)
2022-11-08 17:52:21,398:INFO: Dataset: univ                Batch: 15/29	Loss 8.0641 (8.9075)
2022-11-08 17:52:21,403:INFO: Dataset: univ                Batch: 16/29	Loss 7.7867 (8.8293)
2022-11-08 17:52:21,407:INFO: Dataset: univ                Batch: 17/29	Loss 7.8352 (8.7694)
2022-11-08 17:52:21,412:INFO: Dataset: univ                Batch: 18/29	Loss 8.0023 (8.7307)
2022-11-08 17:52:21,416:INFO: Dataset: univ                Batch: 19/29	Loss 8.1659 (8.7014)
2022-11-08 17:52:21,420:INFO: Dataset: univ                Batch: 20/29	Loss 7.6705 (8.6512)
2022-11-08 17:52:21,423:INFO: Dataset: univ                Batch: 21/29	Loss 7.6825 (8.6032)
2022-11-08 17:52:21,427:INFO: Dataset: univ                Batch: 22/29	Loss 7.5130 (8.5544)
2022-11-08 17:52:21,431:INFO: Dataset: univ                Batch: 23/29	Loss 7.2260 (8.5008)
2022-11-08 17:52:21,434:INFO: Dataset: univ                Batch: 24/29	Loss 7.0093 (8.4326)
2022-11-08 17:52:21,437:INFO: Dataset: univ                Batch: 25/29	Loss 7.1039 (8.3884)
2022-11-08 17:52:21,440:INFO: Dataset: univ                Batch: 26/29	Loss 6.9164 (8.3373)
2022-11-08 17:52:21,445:INFO: Dataset: univ                Batch: 27/29	Loss 6.9008 (8.2827)
2022-11-08 17:52:21,449:INFO: Dataset: univ                Batch: 28/29	Loss 6.4213 (8.2119)
2022-11-08 17:52:21,451:INFO: Dataset: univ                Batch: 29/29	Loss 6.7731 (8.1923)
2022-11-08 17:52:28,332:INFO: Dataset: zara1               Batch:  1/16	Loss 13.3963 (13.3963)
2022-11-08 17:52:28,695:INFO: Dataset: zara1               Batch:  2/16	Loss 13.4016 (13.3991)
2022-11-08 17:52:29,172:INFO: Dataset: zara1               Batch:  3/16	Loss 13.4406 (13.4126)
2022-11-08 17:52:29,389:INFO: Dataset: zara1               Batch:  4/16	Loss 13.6109 (13.4657)
2022-11-08 17:52:29,606:INFO: Dataset: zara1               Batch:  5/16	Loss 13.6515 (13.5040)
2022-11-08 17:52:29,797:INFO: Dataset: zara1               Batch:  6/16	Loss 13.3488 (13.4806)
2022-11-08 17:52:29,801:INFO: Dataset: zara1               Batch:  7/16	Loss 13.7972 (13.5205)
2022-11-08 17:52:29,805:INFO: Dataset: zara1               Batch:  8/16	Loss 12.7532 (13.4297)
2022-11-08 17:52:29,808:INFO: Dataset: zara1               Batch:  9/16	Loss 12.6902 (13.3483)
2022-11-08 17:52:29,811:INFO: Dataset: zara1               Batch: 10/16	Loss 12.8976 (13.2997)
2022-11-08 17:52:29,814:INFO: Dataset: zara1               Batch: 11/16	Loss 12.8003 (13.2637)
2022-11-08 17:52:29,817:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3999 (13.1928)
2022-11-08 17:52:29,822:INFO: Dataset: zara1               Batch: 13/16	Loss 12.4037 (13.1343)
2022-11-08 17:52:29,827:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4236 (13.0797)
2022-11-08 17:52:29,831:INFO: Dataset: zara1               Batch: 15/16	Loss 12.2017 (13.0098)
2022-11-08 17:52:29,834:INFO: Dataset: zara1               Batch: 16/16	Loss 11.9641 (12.9603)
2022-11-08 17:52:52,240:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3012 (6.3012)
2022-11-08 17:52:52,244:INFO: Dataset: zara2               Batch:  2/36	Loss 6.5047 (6.4017)
2022-11-08 17:52:52,248:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4873 (6.4272)
2022-11-08 17:52:52,253:INFO: Dataset: zara2               Batch:  4/36	Loss 7.0479 (6.5871)
2022-11-08 17:52:52,258:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4575 (6.5596)
2022-11-08 17:52:52,263:INFO: Dataset: zara2               Batch:  6/36	Loss 5.9628 (6.4810)
2022-11-08 17:52:52,266:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5499 (6.4916)
2022-11-08 17:52:52,270:INFO: Dataset: zara2               Batch:  8/36	Loss 6.5179 (6.4952)
2022-11-08 17:52:52,274:INFO: Dataset: zara2               Batch:  9/36	Loss 5.7869 (6.4267)
2022-11-08 17:52:52,277:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6952 (6.3528)
2022-11-08 17:52:52,281:INFO: Dataset: zara2               Batch: 11/36	Loss 5.9427 (6.3164)
2022-11-08 17:52:52,285:INFO: Dataset: zara2               Batch: 12/36	Loss 5.9436 (6.2850)
2022-11-08 17:52:52,291:INFO: Dataset: zara2               Batch: 13/36	Loss 5.8582 (6.2524)
2022-11-08 17:52:52,295:INFO: Dataset: zara2               Batch: 14/36	Loss 5.8093 (6.2200)
2022-11-08 17:52:52,299:INFO: Dataset: zara2               Batch: 15/36	Loss 5.3317 (6.1612)
2022-11-08 17:52:52,306:INFO: Dataset: zara2               Batch: 16/36	Loss 5.1652 (6.0954)
2022-11-08 17:52:52,312:INFO: Dataset: zara2               Batch: 17/36	Loss 5.5299 (6.0574)
2022-11-08 17:52:52,318:INFO: Dataset: zara2               Batch: 18/36	Loss 5.1674 (6.0083)
2022-11-08 17:52:52,321:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2900 (5.9656)
2022-11-08 17:52:52,326:INFO: Dataset: zara2               Batch: 20/36	Loss 4.9274 (5.9117)
2022-11-08 17:52:52,330:INFO: Dataset: zara2               Batch: 21/36	Loss 5.2544 (5.8796)
2022-11-08 17:52:52,335:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0237 (5.8387)
2022-11-08 17:52:52,339:INFO: Dataset: zara2               Batch: 23/36	Loss 4.7230 (5.7859)
2022-11-08 17:52:52,342:INFO: Dataset: zara2               Batch: 24/36	Loss 5.0813 (5.7590)
2022-11-08 17:52:52,346:INFO: Dataset: zara2               Batch: 25/36	Loss 4.5339 (5.7037)
2022-11-08 17:52:52,350:INFO: Dataset: zara2               Batch: 26/36	Loss 4.4567 (5.6476)
2022-11-08 17:52:52,355:INFO: Dataset: zara2               Batch: 27/36	Loss 4.4385 (5.6027)
2022-11-08 17:52:52,358:INFO: Dataset: zara2               Batch: 28/36	Loss 4.3662 (5.5561)
2022-11-08 17:52:52,362:INFO: Dataset: zara2               Batch: 29/36	Loss 3.8737 (5.5077)
2022-11-08 17:52:52,367:INFO: Dataset: zara2               Batch: 30/36	Loss 4.6957 (5.4808)
2022-11-08 17:52:52,370:INFO: Dataset: zara2               Batch: 31/36	Loss 4.2359 (5.4385)
2022-11-08 17:52:52,374:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4309 (5.4061)
2022-11-08 17:52:52,382:INFO: Dataset: zara2               Batch: 33/36	Loss 4.7075 (5.3847)
2022-11-08 17:52:52,387:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0561 (5.3372)
2022-11-08 17:52:52,392:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0789 (5.3009)
2022-11-08 17:52:52,395:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9427 (5.2744)
2022-11-08 17:52:53,277:INFO: - Computing loss (validation)
2022-11-08 17:52:59,360:INFO: Dataset: hotel               Batch: 1/3	Loss 16.7113 (16.7113)
2022-11-08 17:52:59,791:INFO: Dataset: hotel               Batch: 2/3	Loss 17.2913 (16.9756)
2022-11-08 17:53:00,146:INFO: Dataset: hotel               Batch: 3/3	Loss 17.2893 (17.0002)
2022-11-08 17:53:07,595:INFO: Dataset: univ                Batch: 1/6	Loss 9.3773 (9.3773)
2022-11-08 17:53:08,046:INFO: Dataset: univ                Batch: 2/6	Loss 8.8156 (9.0624)
2022-11-08 17:53:08,341:INFO: Dataset: univ                Batch: 3/6	Loss 8.9650 (9.0349)
2022-11-08 17:53:08,645:INFO: Dataset: univ                Batch: 4/6	Loss 8.6118 (8.9192)
2022-11-08 17:53:08,862:INFO: Dataset: univ                Batch: 5/6	Loss 8.9715 (8.9316)
2022-11-08 17:53:09,052:INFO: Dataset: univ                Batch: 6/6	Loss 8.8524 (8.9183)
2022-11-08 17:53:15,854:INFO: Dataset: zara1               Batch: 1/3	Loss 12.1341 (12.1341)
2022-11-08 17:53:16,219:INFO: Dataset: zara1               Batch: 2/3	Loss 12.1718 (12.1514)
2022-11-08 17:53:16,611:INFO: Dataset: zara1               Batch: 3/3	Loss 12.1457 (12.1501)
2022-11-08 17:53:24,057:INFO: Dataset: zara2               Batch:  1/10	Loss 3.9824 (3.9824)
2022-11-08 17:53:24,399:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1756 (4.0785)
2022-11-08 17:53:24,819:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0376 (4.0646)
2022-11-08 17:53:25,069:INFO: Dataset: zara2               Batch:  4/10	Loss 4.0076 (4.0500)
2022-11-08 17:53:25,329:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2603 (4.0886)
2022-11-08 17:53:25,581:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1060 (4.0916)
2022-11-08 17:53:25,584:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1138 (4.0950)
2022-11-08 17:53:25,587:INFO: Dataset: zara2               Batch:  8/10	Loss 3.8927 (4.0667)
2022-11-08 17:53:25,590:INFO: Dataset: zara2               Batch:  9/10	Loss 3.9289 (4.0517)
2022-11-08 17:53:25,591:INFO: Dataset: zara2               Batch: 10/10	Loss 4.1902 (4.0644)
2022-11-08 17:53:26,508:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1064.pth.tar
2022-11-08 17:53:26,510:INFO: 
===> EPOCH: 1065 (P5)
2022-11-08 17:53:26,510:INFO: - Computing loss (training)
2022-11-08 17:53:32,449:INFO: Dataset: hotel               Batch: 1/8	Loss 15.6162 (15.6162)
2022-11-08 17:53:32,780:INFO: Dataset: hotel               Batch: 2/8	Loss 16.7154 (16.1925)
2022-11-08 17:53:33,206:INFO: Dataset: hotel               Batch: 3/8	Loss 14.4385 (15.5966)
2022-11-08 17:53:33,457:INFO: Dataset: hotel               Batch: 4/8	Loss 12.6366 (14.8673)
2022-11-08 17:53:33,717:INFO: Dataset: hotel               Batch: 5/8	Loss 13.4713 (14.5785)
2022-11-08 17:53:33,880:INFO: Dataset: hotel               Batch: 6/8	Loss 13.1164 (14.3375)
2022-11-08 17:53:33,882:INFO: Dataset: hotel               Batch: 7/8	Loss 13.1770 (14.1665)
2022-11-08 17:53:33,886:INFO: Dataset: hotel               Batch: 8/8	Loss 14.1851 (14.1672)
2022-11-08 17:53:56,661:INFO: Dataset: univ                Batch:  1/29	Loss 8.9322 (8.9322)
2022-11-08 17:53:56,665:INFO: Dataset: univ                Batch:  2/29	Loss 9.6158 (9.2526)
2022-11-08 17:53:56,671:INFO: Dataset: univ                Batch:  3/29	Loss 9.8823 (9.4522)
2022-11-08 17:53:56,677:INFO: Dataset: univ                Batch:  4/29	Loss 9.1820 (9.3833)
2022-11-08 17:53:56,683:INFO: Dataset: univ                Batch:  5/29	Loss 9.3650 (9.3799)
2022-11-08 17:53:56,691:INFO: Dataset: univ                Batch:  6/29	Loss 9.1715 (9.3437)
2022-11-08 17:53:56,696:INFO: Dataset: univ                Batch:  7/29	Loss 9.3368 (9.3427)
2022-11-08 17:53:56,701:INFO: Dataset: univ                Batch:  8/29	Loss 8.8856 (9.2795)
2022-11-08 17:53:56,707:INFO: Dataset: univ                Batch:  9/29	Loss 8.6858 (9.2077)
2022-11-08 17:53:56,713:INFO: Dataset: univ                Batch: 10/29	Loss 8.7040 (9.1569)
2022-11-08 17:53:56,720:INFO: Dataset: univ                Batch: 11/29	Loss 9.2571 (9.1646)
2022-11-08 17:53:56,726:INFO: Dataset: univ                Batch: 12/29	Loss 8.5039 (9.1174)
2022-11-08 17:53:56,730:INFO: Dataset: univ                Batch: 13/29	Loss 8.5167 (9.0678)
2022-11-08 17:53:56,736:INFO: Dataset: univ                Batch: 14/29	Loss 8.3369 (9.0160)
2022-11-08 17:53:56,744:INFO: Dataset: univ                Batch: 15/29	Loss 8.3401 (8.9724)
2022-11-08 17:53:56,750:INFO: Dataset: univ                Batch: 16/29	Loss 8.0082 (8.9150)
2022-11-08 17:53:56,756:INFO: Dataset: univ                Batch: 17/29	Loss 8.2429 (8.8829)
2022-11-08 17:53:56,762:INFO: Dataset: univ                Batch: 18/29	Loss 7.8694 (8.8206)
2022-11-08 17:53:56,770:INFO: Dataset: univ                Batch: 19/29	Loss 7.9492 (8.7756)
2022-11-08 17:53:56,777:INFO: Dataset: univ                Batch: 20/29	Loss 8.0447 (8.7358)
2022-11-08 17:53:56,782:INFO: Dataset: univ                Batch: 21/29	Loss 7.5497 (8.6824)
2022-11-08 17:53:56,788:INFO: Dataset: univ                Batch: 22/29	Loss 7.2566 (8.6199)
2022-11-08 17:53:56,792:INFO: Dataset: univ                Batch: 23/29	Loss 7.4811 (8.5783)
2022-11-08 17:53:56,798:INFO: Dataset: univ                Batch: 24/29	Loss 7.3379 (8.5189)
2022-11-08 17:53:56,802:INFO: Dataset: univ                Batch: 25/29	Loss 7.2380 (8.4762)
2022-11-08 17:53:56,806:INFO: Dataset: univ                Batch: 26/29	Loss 6.5526 (8.3931)
2022-11-08 17:53:56,811:INFO: Dataset: univ                Batch: 27/29	Loss 6.8354 (8.3281)
2022-11-08 17:53:56,815:INFO: Dataset: univ                Batch: 28/29	Loss 6.7182 (8.2692)
2022-11-08 17:53:56,819:INFO: Dataset: univ                Batch: 29/29	Loss 6.9041 (8.2529)
2022-11-08 17:54:03,613:INFO: Dataset: zara1               Batch:  1/16	Loss 13.1239 (13.1239)
2022-11-08 17:54:03,942:INFO: Dataset: zara1               Batch:  2/16	Loss 13.3313 (13.2248)
2022-11-08 17:54:04,386:INFO: Dataset: zara1               Batch:  3/16	Loss 13.5337 (13.3259)
2022-11-08 17:54:04,646:INFO: Dataset: zara1               Batch:  4/16	Loss 13.3912 (13.3401)
2022-11-08 17:54:04,867:INFO: Dataset: zara1               Batch:  5/16	Loss 13.3676 (13.3453)
2022-11-08 17:54:05,036:INFO: Dataset: zara1               Batch:  6/16	Loss 12.8707 (13.2546)
2022-11-08 17:54:05,039:INFO: Dataset: zara1               Batch:  7/16	Loss 13.1853 (13.2444)
2022-11-08 17:54:05,043:INFO: Dataset: zara1               Batch:  8/16	Loss 13.0716 (13.2211)
2022-11-08 17:54:05,046:INFO: Dataset: zara1               Batch:  9/16	Loss 13.1625 (13.2153)
2022-11-08 17:54:05,048:INFO: Dataset: zara1               Batch: 10/16	Loss 12.6889 (13.1718)
2022-11-08 17:54:05,052:INFO: Dataset: zara1               Batch: 11/16	Loss 12.9103 (13.1473)
2022-11-08 17:54:05,055:INFO: Dataset: zara1               Batch: 12/16	Loss 12.2124 (13.0691)
2022-11-08 17:54:05,059:INFO: Dataset: zara1               Batch: 13/16	Loss 12.3044 (13.0150)
2022-11-08 17:54:05,061:INFO: Dataset: zara1               Batch: 14/16	Loss 12.2410 (12.9508)
2022-11-08 17:54:05,064:INFO: Dataset: zara1               Batch: 15/16	Loss 12.4315 (12.9170)
2022-11-08 17:54:05,066:INFO: Dataset: zara1               Batch: 16/16	Loss 11.7420 (12.8737)
2022-11-08 17:54:27,720:INFO: Dataset: zara2               Batch:  1/36	Loss 6.0548 (6.0548)
2022-11-08 17:54:27,726:INFO: Dataset: zara2               Batch:  2/36	Loss 6.1450 (6.1021)
2022-11-08 17:54:27,729:INFO: Dataset: zara2               Batch:  3/36	Loss 6.5803 (6.2615)
2022-11-08 17:54:27,734:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3882 (6.2938)
2022-11-08 17:54:27,737:INFO: Dataset: zara2               Batch:  5/36	Loss 5.9446 (6.2202)
2022-11-08 17:54:27,759:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1501 (6.2085)
2022-11-08 17:54:27,762:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0556 (6.1884)
2022-11-08 17:54:27,766:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0628 (6.1725)
2022-11-08 17:54:27,769:INFO: Dataset: zara2               Batch:  9/36	Loss 6.3318 (6.1911)
2022-11-08 17:54:27,773:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6857 (6.1437)
2022-11-08 17:54:27,777:INFO: Dataset: zara2               Batch: 11/36	Loss 6.2430 (6.1526)
2022-11-08 17:54:27,780:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8350 (6.1259)
2022-11-08 17:54:27,784:INFO: Dataset: zara2               Batch: 13/36	Loss 5.6669 (6.0910)
2022-11-08 17:54:27,788:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6800 (6.0638)
2022-11-08 17:54:27,791:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5197 (6.0263)
2022-11-08 17:54:27,795:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3928 (5.9789)
2022-11-08 17:54:27,799:INFO: Dataset: zara2               Batch: 17/36	Loss 5.9621 (5.9779)
2022-11-08 17:54:27,803:INFO: Dataset: zara2               Batch: 18/36	Loss 4.8505 (5.9241)
2022-11-08 17:54:27,806:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4494 (5.8947)
2022-11-08 17:54:27,810:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0039 (5.8508)
2022-11-08 17:54:27,814:INFO: Dataset: zara2               Batch: 21/36	Loss 4.8706 (5.8027)
2022-11-08 17:54:27,818:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9936 (5.7604)
2022-11-08 17:54:27,823:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9476 (5.7250)
2022-11-08 17:54:27,826:INFO: Dataset: zara2               Batch: 24/36	Loss 4.8983 (5.6928)
2022-11-08 17:54:27,831:INFO: Dataset: zara2               Batch: 25/36	Loss 5.0990 (5.6646)
2022-11-08 17:54:27,834:INFO: Dataset: zara2               Batch: 26/36	Loss 4.6038 (5.6254)
2022-11-08 17:54:27,840:INFO: Dataset: zara2               Batch: 27/36	Loss 4.5343 (5.5821)
2022-11-08 17:54:27,846:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5092 (5.5396)
2022-11-08 17:54:27,852:INFO: Dataset: zara2               Batch: 29/36	Loss 4.3996 (5.4982)
2022-11-08 17:54:27,857:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3346 (5.4599)
2022-11-08 17:54:27,860:INFO: Dataset: zara2               Batch: 31/36	Loss 4.2455 (5.4254)
2022-11-08 17:54:27,864:INFO: Dataset: zara2               Batch: 32/36	Loss 4.3079 (5.3930)
2022-11-08 17:54:27,870:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2711 (5.3500)
2022-11-08 17:54:27,874:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3480 (5.3177)
2022-11-08 17:54:27,878:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2179 (5.2859)
2022-11-08 17:54:27,883:INFO: Dataset: zara2               Batch: 36/36	Loss 4.2058 (5.2610)
2022-11-08 17:54:28,766:INFO: - Computing loss (validation)
2022-11-08 17:54:34,594:INFO: Dataset: hotel               Batch: 1/3	Loss 16.9744 (16.9744)
2022-11-08 17:54:34,996:INFO: Dataset: hotel               Batch: 2/3	Loss 16.0628 (16.5152)
2022-11-08 17:54:35,332:INFO: Dataset: hotel               Batch: 3/3	Loss 17.3332 (16.5794)
2022-11-08 17:54:42,761:INFO: Dataset: univ                Batch: 1/6	Loss 8.7607 (8.7607)
2022-11-08 17:54:43,014:INFO: Dataset: univ                Batch: 2/6	Loss 8.5956 (8.6710)
2022-11-08 17:54:43,465:INFO: Dataset: univ                Batch: 3/6	Loss 8.8288 (8.7234)
2022-11-08 17:54:43,782:INFO: Dataset: univ                Batch: 4/6	Loss 8.6269 (8.6936)
2022-11-08 17:54:44,000:INFO: Dataset: univ                Batch: 5/6	Loss 9.1268 (8.7682)
2022-11-08 17:54:44,230:INFO: Dataset: univ                Batch: 6/6	Loss 9.1814 (8.8166)
2022-11-08 17:54:50,983:INFO: Dataset: zara1               Batch: 1/3	Loss 12.3673 (12.3673)
2022-11-08 17:54:51,402:INFO: Dataset: zara1               Batch: 2/3	Loss 12.2342 (12.2982)
2022-11-08 17:54:51,776:INFO: Dataset: zara1               Batch: 3/3	Loss 12.0636 (12.2439)
2022-11-08 17:54:59,188:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1059 (4.1059)
2022-11-08 17:54:59,526:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1276 (4.1170)
2022-11-08 17:54:59,840:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0998 (4.1109)
2022-11-08 17:55:00,151:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1622 (4.1229)
2022-11-08 17:55:00,398:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1563 (4.1297)
2022-11-08 17:55:00,597:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1044 (4.1255)
2022-11-08 17:55:00,598:INFO: Dataset: zara2               Batch:  7/10	Loss 3.9217 (4.0941)
2022-11-08 17:55:00,600:INFO: Dataset: zara2               Batch:  8/10	Loss 4.1750 (4.1036)
2022-11-08 17:55:00,601:INFO: Dataset: zara2               Batch:  9/10	Loss 4.1836 (4.1128)
2022-11-08 17:55:00,602:INFO: Dataset: zara2               Batch: 10/10	Loss 4.0960 (4.1112)
2022-11-08 17:55:01,538:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1065.pth.tar
2022-11-08 17:55:01,538:INFO: 
===> EPOCH: 1066 (P5)
2022-11-08 17:55:01,538:INFO: - Computing loss (training)
2022-11-08 17:55:07,518:INFO: Dataset: hotel               Batch: 1/8	Loss 12.4800 (12.4800)
2022-11-08 17:55:07,854:INFO: Dataset: hotel               Batch: 2/8	Loss 13.3931 (12.9205)
2022-11-08 17:55:08,242:INFO: Dataset: hotel               Batch: 3/8	Loss 14.2835 (13.4169)
2022-11-08 17:55:08,550:INFO: Dataset: hotel               Batch: 4/8	Loss 14.8006 (13.7545)
2022-11-08 17:55:08,755:INFO: Dataset: hotel               Batch: 5/8	Loss 13.9023 (13.7842)
2022-11-08 17:55:08,919:INFO: Dataset: hotel               Batch: 6/8	Loss 13.5463 (13.7463)
2022-11-08 17:55:08,923:INFO: Dataset: hotel               Batch: 7/8	Loss 14.7876 (13.9173)
2022-11-08 17:55:08,927:INFO: Dataset: hotel               Batch: 8/8	Loss 13.6999 (13.9112)
2022-11-08 17:55:31,397:INFO: Dataset: univ                Batch:  1/29	Loss 9.0973 (9.0973)
2022-11-08 17:55:31,400:INFO: Dataset: univ                Batch:  2/29	Loss 9.6226 (9.3667)
2022-11-08 17:55:31,407:INFO: Dataset: univ                Batch:  3/29	Loss 9.6595 (9.4679)
2022-11-08 17:55:31,412:INFO: Dataset: univ                Batch:  4/29	Loss 9.0216 (9.3587)
2022-11-08 17:55:31,418:INFO: Dataset: univ                Batch:  5/29	Loss 9.5415 (9.3938)
2022-11-08 17:55:31,427:INFO: Dataset: univ                Batch:  6/29	Loss 9.1067 (9.3467)
2022-11-08 17:55:31,433:INFO: Dataset: univ                Batch:  7/29	Loss 8.7872 (9.2641)
2022-11-08 17:55:31,440:INFO: Dataset: univ                Batch:  8/29	Loss 8.8516 (9.2047)
2022-11-08 17:55:31,447:INFO: Dataset: univ                Batch:  9/29	Loss 9.0063 (9.1816)
2022-11-08 17:55:31,452:INFO: Dataset: univ                Batch: 10/29	Loss 8.2709 (9.0807)
2022-11-08 17:55:31,458:INFO: Dataset: univ                Batch: 11/29	Loss 9.2661 (9.0966)
2022-11-08 17:55:31,462:INFO: Dataset: univ                Batch: 12/29	Loss 8.5497 (9.0615)
2022-11-08 17:55:31,467:INFO: Dataset: univ                Batch: 13/29	Loss 8.5050 (9.0243)
2022-11-08 17:55:31,473:INFO: Dataset: univ                Batch: 14/29	Loss 7.9317 (8.9395)
2022-11-08 17:55:31,479:INFO: Dataset: univ                Batch: 15/29	Loss 8.3046 (8.9009)
2022-11-08 17:55:31,486:INFO: Dataset: univ                Batch: 16/29	Loss 8.0881 (8.8539)
2022-11-08 17:55:31,494:INFO: Dataset: univ                Batch: 17/29	Loss 8.0913 (8.8054)
2022-11-08 17:55:31,500:INFO: Dataset: univ                Batch: 18/29	Loss 7.7775 (8.7397)
2022-11-08 17:55:31,506:INFO: Dataset: univ                Batch: 19/29	Loss 7.4645 (8.6722)
2022-11-08 17:55:31,511:INFO: Dataset: univ                Batch: 20/29	Loss 7.6996 (8.6197)
2022-11-08 17:55:31,518:INFO: Dataset: univ                Batch: 21/29	Loss 7.4385 (8.5664)
2022-11-08 17:55:31,523:INFO: Dataset: univ                Batch: 22/29	Loss 7.3186 (8.5159)
2022-11-08 17:55:31,532:INFO: Dataset: univ                Batch: 23/29	Loss 6.9672 (8.4432)
2022-11-08 17:55:31,540:INFO: Dataset: univ                Batch: 24/29	Loss 7.3572 (8.3980)
2022-11-08 17:55:31,546:INFO: Dataset: univ                Batch: 25/29	Loss 6.7936 (8.3347)
2022-11-08 17:55:31,552:INFO: Dataset: univ                Batch: 26/29	Loss 7.1631 (8.2905)
2022-11-08 17:55:31,557:INFO: Dataset: univ                Batch: 27/29	Loss 6.7023 (8.2220)
2022-11-08 17:55:31,564:INFO: Dataset: univ                Batch: 28/29	Loss 6.2784 (8.1498)
2022-11-08 17:55:31,570:INFO: Dataset: univ                Batch: 29/29	Loss 6.2312 (8.1167)
2022-11-08 17:55:38,379:INFO: Dataset: zara1               Batch:  1/16	Loss 13.3400 (13.3400)
2022-11-08 17:55:38,715:INFO: Dataset: zara1               Batch:  2/16	Loss 13.1762 (13.2540)
2022-11-08 17:55:39,049:INFO: Dataset: zara1               Batch:  3/16	Loss 13.1296 (13.2167)
2022-11-08 17:55:39,416:INFO: Dataset: zara1               Batch:  4/16	Loss 12.7516 (13.1122)
2022-11-08 17:55:39,623:INFO: Dataset: zara1               Batch:  5/16	Loss 13.6036 (13.2097)
2022-11-08 17:55:39,854:INFO: Dataset: zara1               Batch:  6/16	Loss 13.5512 (13.2678)
2022-11-08 17:55:39,857:INFO: Dataset: zara1               Batch:  7/16	Loss 12.6823 (13.1936)
2022-11-08 17:55:39,861:INFO: Dataset: zara1               Batch:  8/16	Loss 13.5700 (13.2480)
2022-11-08 17:55:39,866:INFO: Dataset: zara1               Batch:  9/16	Loss 13.1465 (13.2360)
2022-11-08 17:55:39,870:INFO: Dataset: zara1               Batch: 10/16	Loss 12.7833 (13.1884)
2022-11-08 17:55:39,875:INFO: Dataset: zara1               Batch: 11/16	Loss 12.6067 (13.1389)
2022-11-08 17:55:39,880:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3234 (13.0761)
2022-11-08 17:55:39,884:INFO: Dataset: zara1               Batch: 13/16	Loss 12.3811 (13.0227)
2022-11-08 17:55:39,888:INFO: Dataset: zara1               Batch: 14/16	Loss 13.0269 (13.0230)
2022-11-08 17:55:39,891:INFO: Dataset: zara1               Batch: 15/16	Loss 12.2238 (12.9705)
2022-11-08 17:55:39,894:INFO: Dataset: zara1               Batch: 16/16	Loss 11.7670 (12.9154)
2022-11-08 17:56:02,510:INFO: Dataset: zara2               Batch:  1/36	Loss 6.8238 (6.8238)
2022-11-08 17:56:02,514:INFO: Dataset: zara2               Batch:  2/36	Loss 6.6585 (6.7352)
2022-11-08 17:56:02,517:INFO: Dataset: zara2               Batch:  3/36	Loss 6.0203 (6.5054)
2022-11-08 17:56:02,521:INFO: Dataset: zara2               Batch:  4/36	Loss 6.4136 (6.4799)
2022-11-08 17:56:02,524:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3681 (6.4553)
2022-11-08 17:56:02,546:INFO: Dataset: zara2               Batch:  6/36	Loss 6.4430 (6.4533)
2022-11-08 17:56:02,549:INFO: Dataset: zara2               Batch:  7/36	Loss 6.1687 (6.4147)
2022-11-08 17:56:02,552:INFO: Dataset: zara2               Batch:  8/36	Loss 6.2417 (6.3955)
2022-11-08 17:56:02,557:INFO: Dataset: zara2               Batch:  9/36	Loss 5.8152 (6.3285)
2022-11-08 17:56:02,561:INFO: Dataset: zara2               Batch: 10/36	Loss 6.2453 (6.3196)
2022-11-08 17:56:02,565:INFO: Dataset: zara2               Batch: 11/36	Loss 5.6184 (6.2512)
2022-11-08 17:56:02,571:INFO: Dataset: zara2               Batch: 12/36	Loss 6.0722 (6.2355)
2022-11-08 17:56:02,574:INFO: Dataset: zara2               Batch: 13/36	Loss 5.8882 (6.2063)
2022-11-08 17:56:02,579:INFO: Dataset: zara2               Batch: 14/36	Loss 5.7560 (6.1755)
2022-11-08 17:56:02,583:INFO: Dataset: zara2               Batch: 15/36	Loss 5.4971 (6.1368)
2022-11-08 17:56:02,588:INFO: Dataset: zara2               Batch: 16/36	Loss 6.1713 (6.1388)
2022-11-08 17:56:02,592:INFO: Dataset: zara2               Batch: 17/36	Loss 5.5623 (6.1038)
2022-11-08 17:56:02,596:INFO: Dataset: zara2               Batch: 18/36	Loss 5.3131 (6.0558)
2022-11-08 17:56:02,602:INFO: Dataset: zara2               Batch: 19/36	Loss 4.9741 (6.0016)
2022-11-08 17:56:02,607:INFO: Dataset: zara2               Batch: 20/36	Loss 5.3449 (5.9666)
2022-11-08 17:56:02,611:INFO: Dataset: zara2               Batch: 21/36	Loss 4.9741 (5.9104)
2022-11-08 17:56:02,616:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9654 (5.8620)
2022-11-08 17:56:02,620:INFO: Dataset: zara2               Batch: 23/36	Loss 5.1410 (5.8310)
2022-11-08 17:56:02,624:INFO: Dataset: zara2               Batch: 24/36	Loss 4.8223 (5.7890)
2022-11-08 17:56:02,630:INFO: Dataset: zara2               Batch: 25/36	Loss 5.0323 (5.7600)
2022-11-08 17:56:02,634:INFO: Dataset: zara2               Batch: 26/36	Loss 4.4504 (5.7148)
2022-11-08 17:56:02,640:INFO: Dataset: zara2               Batch: 27/36	Loss 4.4870 (5.6680)
2022-11-08 17:56:02,644:INFO: Dataset: zara2               Batch: 28/36	Loss 4.4600 (5.6208)
2022-11-08 17:56:02,647:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5699 (5.5844)
2022-11-08 17:56:02,651:INFO: Dataset: zara2               Batch: 30/36	Loss 4.4759 (5.5478)
2022-11-08 17:56:02,653:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3235 (5.5129)
2022-11-08 17:56:02,659:INFO: Dataset: zara2               Batch: 32/36	Loss 4.1636 (5.4729)
2022-11-08 17:56:02,665:INFO: Dataset: zara2               Batch: 33/36	Loss 4.1943 (5.4360)
2022-11-08 17:56:02,671:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0648 (5.3900)
2022-11-08 17:56:02,676:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0608 (5.3526)
2022-11-08 17:56:02,680:INFO: Dataset: zara2               Batch: 36/36	Loss 4.3227 (5.3322)
2022-11-08 17:56:03,572:INFO: - Computing loss (validation)
2022-11-08 17:56:09,459:INFO: Dataset: hotel               Batch: 1/3	Loss 16.6265 (16.6265)
2022-11-08 17:56:09,856:INFO: Dataset: hotel               Batch: 2/3	Loss 16.9682 (16.8067)
2022-11-08 17:56:10,167:INFO: Dataset: hotel               Batch: 3/3	Loss 18.6706 (16.9340)
2022-11-08 17:56:17,571:INFO: Dataset: univ                Batch: 1/6	Loss 8.9232 (8.9232)
2022-11-08 17:56:17,917:INFO: Dataset: univ                Batch: 2/6	Loss 8.9017 (8.9125)
2022-11-08 17:56:18,383:INFO: Dataset: univ                Batch: 3/6	Loss 8.8430 (8.8853)
2022-11-08 17:56:18,629:INFO: Dataset: univ                Batch: 4/6	Loss 8.5438 (8.7881)
2022-11-08 17:56:18,855:INFO: Dataset: univ                Batch: 5/6	Loss 8.7832 (8.7871)
2022-11-08 17:56:19,047:INFO: Dataset: univ                Batch: 6/6	Loss 8.8255 (8.7925)
2022-11-08 17:56:25,839:INFO: Dataset: zara1               Batch: 1/3	Loss 12.1958 (12.1958)
2022-11-08 17:56:26,195:INFO: Dataset: zara1               Batch: 2/3	Loss 12.2024 (12.1989)
2022-11-08 17:56:26,563:INFO: Dataset: zara1               Batch: 3/3	Loss 11.9623 (12.1456)
2022-11-08 17:56:34,148:INFO: Dataset: zara2               Batch:  1/10	Loss 4.0218 (4.0218)
2022-11-08 17:56:34,480:INFO: Dataset: zara2               Batch:  2/10	Loss 3.9911 (4.0070)
2022-11-08 17:56:34,863:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1371 (4.0526)
2022-11-08 17:56:35,154:INFO: Dataset: zara2               Batch:  4/10	Loss 4.2891 (4.1180)
2022-11-08 17:56:35,381:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1497 (4.1242)
2022-11-08 17:56:35,597:INFO: Dataset: zara2               Batch:  6/10	Loss 3.7540 (4.0653)
2022-11-08 17:56:35,600:INFO: Dataset: zara2               Batch:  7/10	Loss 3.8710 (4.0323)
2022-11-08 17:56:35,602:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0869 (4.0395)
2022-11-08 17:56:35,605:INFO: Dataset: zara2               Batch:  9/10	Loss 4.3095 (4.0701)
2022-11-08 17:56:35,606:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2051 (4.0833)
2022-11-08 17:56:36,520:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1066.pth.tar
2022-11-08 17:56:36,520:INFO: 
===> EPOCH: 1067 (P5)
2022-11-08 17:56:36,522:INFO: - Computing loss (training)
2022-11-08 17:56:42,502:INFO: Dataset: hotel               Batch: 1/8	Loss 15.6707 (15.6707)
2022-11-08 17:56:42,805:INFO: Dataset: hotel               Batch: 2/8	Loss 13.1219 (14.4558)
2022-11-08 17:56:43,269:INFO: Dataset: hotel               Batch: 3/8	Loss 15.6296 (14.8495)
2022-11-08 17:56:43,545:INFO: Dataset: hotel               Batch: 4/8	Loss 12.8793 (14.4191)
2022-11-08 17:56:43,732:INFO: Dataset: hotel               Batch: 5/8	Loss 13.6138 (14.2470)
2022-11-08 17:56:43,956:INFO: Dataset: hotel               Batch: 6/8	Loss 14.1766 (14.2359)
2022-11-08 17:56:43,960:INFO: Dataset: hotel               Batch: 7/8	Loss 13.7334 (14.1586)
2022-11-08 17:56:43,963:INFO: Dataset: hotel               Batch: 8/8	Loss 12.1245 (14.0969)
2022-11-08 17:57:07,168:INFO: Dataset: univ                Batch:  1/29	Loss 9.3388 (9.3388)
2022-11-08 17:57:07,172:INFO: Dataset: univ                Batch:  2/29	Loss 9.1324 (9.2331)
2022-11-08 17:57:07,177:INFO: Dataset: univ                Batch:  3/29	Loss 9.1426 (9.2070)
2022-11-08 17:57:07,182:INFO: Dataset: univ                Batch:  4/29	Loss 9.1405 (9.1894)
2022-11-08 17:57:07,188:INFO: Dataset: univ                Batch:  5/29	Loss 9.2288 (9.1975)
2022-11-08 17:57:07,194:INFO: Dataset: univ                Batch:  6/29	Loss 9.2374 (9.2044)
2022-11-08 17:57:07,199:INFO: Dataset: univ                Batch:  7/29	Loss 8.9144 (9.1569)
2022-11-08 17:57:07,204:INFO: Dataset: univ                Batch:  8/29	Loss 8.6171 (9.0899)
2022-11-08 17:57:07,208:INFO: Dataset: univ                Batch:  9/29	Loss 8.6664 (9.0343)
2022-11-08 17:57:07,213:INFO: Dataset: univ                Batch: 10/29	Loss 8.6226 (8.9895)
2022-11-08 17:57:07,217:INFO: Dataset: univ                Batch: 11/29	Loss 9.2483 (9.0143)
2022-11-08 17:57:07,223:INFO: Dataset: univ                Batch: 12/29	Loss 8.4777 (8.9731)
2022-11-08 17:57:07,227:INFO: Dataset: univ                Batch: 13/29	Loss 8.7685 (8.9595)
2022-11-08 17:57:07,230:INFO: Dataset: univ                Batch: 14/29	Loss 8.6726 (8.9411)
2022-11-08 17:57:07,234:INFO: Dataset: univ                Batch: 15/29	Loss 8.0459 (8.8759)
2022-11-08 17:57:07,239:INFO: Dataset: univ                Batch: 16/29	Loss 8.1572 (8.8335)
2022-11-08 17:57:07,245:INFO: Dataset: univ                Batch: 17/29	Loss 8.1806 (8.7920)
2022-11-08 17:57:07,251:INFO: Dataset: univ                Batch: 18/29	Loss 8.1330 (8.7570)
2022-11-08 17:57:07,257:INFO: Dataset: univ                Batch: 19/29	Loss 8.0731 (8.7234)
2022-11-08 17:57:07,261:INFO: Dataset: univ                Batch: 20/29	Loss 7.8906 (8.6864)
2022-11-08 17:57:07,265:INFO: Dataset: univ                Batch: 21/29	Loss 7.5717 (8.6319)
2022-11-08 17:57:07,269:INFO: Dataset: univ                Batch: 22/29	Loss 7.5130 (8.5864)
2022-11-08 17:57:07,275:INFO: Dataset: univ                Batch: 23/29	Loss 7.3504 (8.5312)
2022-11-08 17:57:07,281:INFO: Dataset: univ                Batch: 24/29	Loss 7.1948 (8.4760)
2022-11-08 17:57:07,286:INFO: Dataset: univ                Batch: 25/29	Loss 6.9098 (8.4124)
2022-11-08 17:57:07,293:INFO: Dataset: univ                Batch: 26/29	Loss 7.0048 (8.3486)
2022-11-08 17:57:07,297:INFO: Dataset: univ                Batch: 27/29	Loss 7.0639 (8.2998)
2022-11-08 17:57:07,303:INFO: Dataset: univ                Batch: 28/29	Loss 6.5631 (8.2325)
2022-11-08 17:57:07,307:INFO: Dataset: univ                Batch: 29/29	Loss 6.6918 (8.2139)
2022-11-08 17:57:14,555:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8118 (12.8118)
2022-11-08 17:57:15,021:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2886 (13.0300)
2022-11-08 17:57:15,288:INFO: Dataset: zara1               Batch:  3/16	Loss 12.9876 (13.0162)
2022-11-08 17:57:15,565:INFO: Dataset: zara1               Batch:  4/16	Loss 13.0299 (13.0202)
2022-11-08 17:57:15,846:INFO: Dataset: zara1               Batch:  5/16	Loss 13.6825 (13.1590)
2022-11-08 17:57:16,031:INFO: Dataset: zara1               Batch:  6/16	Loss 12.6163 (13.0829)
2022-11-08 17:57:16,035:INFO: Dataset: zara1               Batch:  7/16	Loss 12.9990 (13.0718)
2022-11-08 17:57:16,039:INFO: Dataset: zara1               Batch:  8/16	Loss 13.0649 (13.0709)
2022-11-08 17:57:16,042:INFO: Dataset: zara1               Batch:  9/16	Loss 12.6619 (13.0259)
2022-11-08 17:57:16,045:INFO: Dataset: zara1               Batch: 10/16	Loss 12.6321 (12.9863)
2022-11-08 17:57:16,050:INFO: Dataset: zara1               Batch: 11/16	Loss 12.7028 (12.9602)
2022-11-08 17:57:16,054:INFO: Dataset: zara1               Batch: 12/16	Loss 12.9455 (12.9590)
2022-11-08 17:57:16,058:INFO: Dataset: zara1               Batch: 13/16	Loss 12.7126 (12.9408)
2022-11-08 17:57:16,062:INFO: Dataset: zara1               Batch: 14/16	Loss 12.6645 (12.9174)
2022-11-08 17:57:16,065:INFO: Dataset: zara1               Batch: 15/16	Loss 11.7605 (12.8488)
2022-11-08 17:57:16,069:INFO: Dataset: zara1               Batch: 16/16	Loss 12.0619 (12.8094)
2022-11-08 17:57:39,365:INFO: Dataset: zara2               Batch:  1/36	Loss 6.8016 (6.8016)
2022-11-08 17:57:39,369:INFO: Dataset: zara2               Batch:  2/36	Loss 6.1591 (6.4555)
2022-11-08 17:57:39,376:INFO: Dataset: zara2               Batch:  3/36	Loss 6.7045 (6.5397)
2022-11-08 17:57:39,380:INFO: Dataset: zara2               Batch:  4/36	Loss 6.6697 (6.5679)
2022-11-08 17:57:39,385:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4081 (6.5327)
2022-11-08 17:57:39,392:INFO: Dataset: zara2               Batch:  6/36	Loss 5.9741 (6.4263)
2022-11-08 17:57:39,396:INFO: Dataset: zara2               Batch:  7/36	Loss 6.0094 (6.3697)
2022-11-08 17:57:39,401:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0256 (6.3237)
2022-11-08 17:57:39,406:INFO: Dataset: zara2               Batch:  9/36	Loss 6.2299 (6.3135)
2022-11-08 17:57:39,410:INFO: Dataset: zara2               Batch: 10/36	Loss 6.4870 (6.3316)
2022-11-08 17:57:39,415:INFO: Dataset: zara2               Batch: 11/36	Loss 5.5093 (6.2582)
2022-11-08 17:57:39,417:INFO: Dataset: zara2               Batch: 12/36	Loss 6.1338 (6.2486)
2022-11-08 17:57:39,422:INFO: Dataset: zara2               Batch: 13/36	Loss 5.8134 (6.2118)
2022-11-08 17:57:39,426:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6053 (6.1642)
2022-11-08 17:57:39,430:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5861 (6.1240)
2022-11-08 17:57:39,434:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3867 (6.0821)
2022-11-08 17:57:39,439:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6747 (6.0578)
2022-11-08 17:57:39,443:INFO: Dataset: zara2               Batch: 18/36	Loss 5.8893 (6.0474)
2022-11-08 17:57:39,449:INFO: Dataset: zara2               Batch: 19/36	Loss 5.0919 (5.9938)
2022-11-08 17:57:39,455:INFO: Dataset: zara2               Batch: 20/36	Loss 5.2726 (5.9547)
2022-11-08 17:57:39,460:INFO: Dataset: zara2               Batch: 21/36	Loss 5.6104 (5.9387)
2022-11-08 17:57:39,466:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1757 (5.9004)
2022-11-08 17:57:39,470:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9880 (5.8534)
2022-11-08 17:57:39,476:INFO: Dataset: zara2               Batch: 24/36	Loss 4.8008 (5.8083)
2022-11-08 17:57:39,479:INFO: Dataset: zara2               Batch: 25/36	Loss 4.7035 (5.7648)
2022-11-08 17:57:39,483:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8066 (5.7321)
2022-11-08 17:57:39,486:INFO: Dataset: zara2               Batch: 27/36	Loss 4.5042 (5.6846)
2022-11-08 17:57:39,490:INFO: Dataset: zara2               Batch: 28/36	Loss 4.4986 (5.6464)
2022-11-08 17:57:39,494:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5027 (5.6049)
2022-11-08 17:57:39,499:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5061 (5.5662)
2022-11-08 17:57:39,505:INFO: Dataset: zara2               Batch: 31/36	Loss 4.0866 (5.5210)
2022-11-08 17:57:39,511:INFO: Dataset: zara2               Batch: 32/36	Loss 4.2551 (5.4839)
2022-11-08 17:57:39,515:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2184 (5.4455)
2022-11-08 17:57:39,520:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0018 (5.4035)
2022-11-08 17:57:39,524:INFO: Dataset: zara2               Batch: 35/36	Loss 4.1322 (5.3646)
2022-11-08 17:57:39,528:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1381 (5.3447)
2022-11-08 17:57:40,421:INFO: - Computing loss (validation)
2022-11-08 17:57:46,326:INFO: Dataset: hotel               Batch: 1/3	Loss 16.8619 (16.8619)
2022-11-08 17:57:46,722:INFO: Dataset: hotel               Batch: 2/3	Loss 15.7172 (16.2832)
2022-11-08 17:57:47,033:INFO: Dataset: hotel               Batch: 3/3	Loss 18.7317 (16.4504)
2022-11-08 17:57:54,392:INFO: Dataset: univ                Batch: 1/6	Loss 8.4481 (8.4481)
2022-11-08 17:57:54,782:INFO: Dataset: univ                Batch: 2/6	Loss 8.1905 (8.3185)
2022-11-08 17:57:55,178:INFO: Dataset: univ                Batch: 3/6	Loss 8.9132 (8.4798)
2022-11-08 17:57:55,478:INFO: Dataset: univ                Batch: 4/6	Loss 9.3817 (8.6763)
2022-11-08 17:57:55,718:INFO: Dataset: univ                Batch: 5/6	Loss 8.7620 (8.6942)
2022-11-08 17:57:55,920:INFO: Dataset: univ                Batch: 6/6	Loss 8.7743 (8.7041)
2022-11-08 17:58:03,746:INFO: Dataset: zara1               Batch: 1/3	Loss 12.4425 (12.4425)
2022-11-08 17:58:04,150:INFO: Dataset: zara1               Batch: 2/3	Loss 12.2152 (12.3259)
2022-11-08 17:58:04,481:INFO: Dataset: zara1               Batch: 3/3	Loss 12.0012 (12.2413)
2022-11-08 17:58:12,522:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2774 (4.2774)
2022-11-08 17:58:13,189:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2010 (4.2400)
2022-11-08 17:58:13,579:INFO: Dataset: zara2               Batch:  3/10	Loss 3.8966 (4.1185)
2022-11-08 17:58:14,144:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1752 (4.1308)
2022-11-08 17:58:14,381:INFO: Dataset: zara2               Batch:  5/10	Loss 4.4055 (4.1855)
2022-11-08 17:58:14,577:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2744 (4.2001)
2022-11-08 17:58:14,579:INFO: Dataset: zara2               Batch:  7/10	Loss 3.9136 (4.1622)
2022-11-08 17:58:14,580:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2058 (4.1675)
2022-11-08 17:58:14,582:INFO: Dataset: zara2               Batch:  9/10	Loss 4.1877 (4.1698)
2022-11-08 17:58:14,583:INFO: Dataset: zara2               Batch: 10/10	Loss 3.8451 (4.1402)
2022-11-08 17:58:15,669:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1067.pth.tar
2022-11-08 17:58:15,669:INFO: 
===> EPOCH: 1068 (P5)
2022-11-08 17:58:15,670:INFO: - Computing loss (training)
2022-11-08 17:58:27,586:INFO: Dataset: hotel               Batch: 1/8	Loss 14.1445 (14.1445)
2022-11-08 17:58:28,031:INFO: Dataset: hotel               Batch: 2/8	Loss 15.3333 (14.7735)
2022-11-08 17:58:28,662:INFO: Dataset: hotel               Batch: 3/8	Loss 15.0369 (14.8584)
2022-11-08 17:58:29,189:INFO: Dataset: hotel               Batch: 4/8	Loss 13.2798 (14.4710)
2022-11-08 17:58:29,583:INFO: Dataset: hotel               Batch: 5/8	Loss 13.9729 (14.3780)
2022-11-08 17:58:29,874:INFO: Dataset: hotel               Batch: 6/8	Loss 12.3942 (14.0661)
2022-11-08 17:58:29,881:INFO: Dataset: hotel               Batch: 7/8	Loss 11.7535 (13.7492)
2022-11-08 17:58:29,886:INFO: Dataset: hotel               Batch: 8/8	Loss 14.7666 (13.7774)
2022-11-08 17:58:55,816:INFO: Dataset: univ                Batch:  1/29	Loss 9.1961 (9.1961)
2022-11-08 17:58:55,821:INFO: Dataset: univ                Batch:  2/29	Loss 9.6197 (9.3877)
2022-11-08 17:58:55,825:INFO: Dataset: univ                Batch:  3/29	Loss 9.0377 (9.2795)
2022-11-08 17:58:55,830:INFO: Dataset: univ                Batch:  4/29	Loss 9.0325 (9.2114)
2022-11-08 17:58:55,848:INFO: Dataset: univ                Batch:  5/29	Loss 8.9670 (9.1606)
2022-11-08 17:58:55,855:INFO: Dataset: univ                Batch:  6/29	Loss 9.1405 (9.1577)
2022-11-08 17:58:55,861:INFO: Dataset: univ                Batch:  7/29	Loss 9.1948 (9.1630)
2022-11-08 17:58:55,868:INFO: Dataset: univ                Batch:  8/29	Loss 8.8593 (9.1237)
2022-11-08 17:58:55,874:INFO: Dataset: univ                Batch:  9/29	Loss 8.7336 (9.0820)
2022-11-08 17:58:55,881:INFO: Dataset: univ                Batch: 10/29	Loss 8.3036 (8.9912)
2022-11-08 17:58:55,886:INFO: Dataset: univ                Batch: 11/29	Loss 8.6683 (8.9598)
2022-11-08 17:58:55,891:INFO: Dataset: univ                Batch: 12/29	Loss 8.9310 (8.9577)
2022-11-08 17:58:55,897:INFO: Dataset: univ                Batch: 13/29	Loss 8.0514 (8.8822)
2022-11-08 17:58:55,902:INFO: Dataset: univ                Batch: 14/29	Loss 7.7281 (8.7937)
2022-11-08 17:58:55,907:INFO: Dataset: univ                Batch: 15/29	Loss 8.3091 (8.7638)
2022-11-08 17:58:55,912:INFO: Dataset: univ                Batch: 16/29	Loss 7.8377 (8.6974)
2022-11-08 17:58:55,917:INFO: Dataset: univ                Batch: 17/29	Loss 7.7299 (8.6419)
2022-11-08 17:58:55,922:INFO: Dataset: univ                Batch: 18/29	Loss 8.0324 (8.6128)
2022-11-08 17:58:55,928:INFO: Dataset: univ                Batch: 19/29	Loss 7.6287 (8.5583)
2022-11-08 17:58:55,932:INFO: Dataset: univ                Batch: 20/29	Loss 7.6893 (8.5196)
2022-11-08 17:58:55,939:INFO: Dataset: univ                Batch: 21/29	Loss 7.3311 (8.4542)
2022-11-08 17:58:55,944:INFO: Dataset: univ                Batch: 22/29	Loss 7.2411 (8.3977)
2022-11-08 17:58:55,952:INFO: Dataset: univ                Batch: 23/29	Loss 7.1166 (8.3415)
2022-11-08 17:58:55,958:INFO: Dataset: univ                Batch: 24/29	Loss 6.9121 (8.2833)
2022-11-08 17:58:55,964:INFO: Dataset: univ                Batch: 25/29	Loss 7.0255 (8.2310)
2022-11-08 17:58:55,971:INFO: Dataset: univ                Batch: 26/29	Loss 6.9330 (8.1897)
2022-11-08 17:58:55,977:INFO: Dataset: univ                Batch: 27/29	Loss 6.7576 (8.1425)
2022-11-08 17:58:55,985:INFO: Dataset: univ                Batch: 28/29	Loss 6.6004 (8.0892)
2022-11-08 17:58:55,992:INFO: Dataset: univ                Batch: 29/29	Loss 6.4891 (8.0677)
2022-11-08 17:59:03,026:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8991 (12.8991)
2022-11-08 17:59:03,464:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2731 (13.0915)
2022-11-08 17:59:03,723:INFO: Dataset: zara1               Batch:  3/16	Loss 13.5462 (13.2451)
2022-11-08 17:59:04,085:INFO: Dataset: zara1               Batch:  4/16	Loss 13.2815 (13.2539)
2022-11-08 17:59:04,302:INFO: Dataset: zara1               Batch:  5/16	Loss 13.3141 (13.2677)
2022-11-08 17:59:04,501:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0628 (13.2369)
2022-11-08 17:59:04,505:INFO: Dataset: zara1               Batch:  7/16	Loss 13.3209 (13.2485)
2022-11-08 17:59:04,508:INFO: Dataset: zara1               Batch:  8/16	Loss 13.0177 (13.2181)
2022-11-08 17:59:04,511:INFO: Dataset: zara1               Batch:  9/16	Loss 12.8527 (13.1830)
2022-11-08 17:59:04,514:INFO: Dataset: zara1               Batch: 10/16	Loss 12.6791 (13.1264)
2022-11-08 17:59:04,518:INFO: Dataset: zara1               Batch: 11/16	Loss 13.0116 (13.1177)
2022-11-08 17:59:04,522:INFO: Dataset: zara1               Batch: 12/16	Loss 12.8188 (13.0926)
2022-11-08 17:59:04,527:INFO: Dataset: zara1               Batch: 13/16	Loss 12.5995 (13.0524)
2022-11-08 17:59:04,533:INFO: Dataset: zara1               Batch: 14/16	Loss 12.5475 (13.0215)
2022-11-08 17:59:04,539:INFO: Dataset: zara1               Batch: 15/16	Loss 11.7372 (12.9422)
2022-11-08 17:59:04,544:INFO: Dataset: zara1               Batch: 16/16	Loss 12.3676 (12.9107)
2022-11-08 17:59:28,496:INFO: Dataset: zara2               Batch:  1/36	Loss 6.4512 (6.4512)
2022-11-08 17:59:28,500:INFO: Dataset: zara2               Batch:  2/36	Loss 6.7006 (6.5741)
2022-11-08 17:59:28,503:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4620 (6.5328)
2022-11-08 17:59:28,507:INFO: Dataset: zara2               Batch:  4/36	Loss 6.7140 (6.5761)
2022-11-08 17:59:28,516:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4560 (6.5527)
2022-11-08 17:59:28,524:INFO: Dataset: zara2               Batch:  6/36	Loss 6.4543 (6.5367)
2022-11-08 17:59:28,527:INFO: Dataset: zara2               Batch:  7/36	Loss 6.2728 (6.5008)
2022-11-08 17:59:28,531:INFO: Dataset: zara2               Batch:  8/36	Loss 6.4289 (6.4904)
2022-11-08 17:59:28,537:INFO: Dataset: zara2               Batch:  9/36	Loss 6.2094 (6.4611)
2022-11-08 17:59:28,542:INFO: Dataset: zara2               Batch: 10/36	Loss 5.9973 (6.4175)
2022-11-08 17:59:28,546:INFO: Dataset: zara2               Batch: 11/36	Loss 5.9862 (6.3793)
2022-11-08 17:59:28,551:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7180 (6.3283)
2022-11-08 17:59:28,555:INFO: Dataset: zara2               Batch: 13/36	Loss 5.4730 (6.2570)
2022-11-08 17:59:28,559:INFO: Dataset: zara2               Batch: 14/36	Loss 5.4739 (6.2035)
2022-11-08 17:59:28,562:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5834 (6.1672)
2022-11-08 17:59:28,567:INFO: Dataset: zara2               Batch: 16/36	Loss 5.9337 (6.1522)
2022-11-08 17:59:28,571:INFO: Dataset: zara2               Batch: 17/36	Loss 4.9081 (6.0888)
2022-11-08 17:59:28,575:INFO: Dataset: zara2               Batch: 18/36	Loss 5.1766 (6.0427)
2022-11-08 17:59:28,579:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4286 (6.0149)
2022-11-08 17:59:28,584:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4255 (5.9862)
2022-11-08 17:59:28,587:INFO: Dataset: zara2               Batch: 21/36	Loss 4.9942 (5.9461)
2022-11-08 17:59:28,592:INFO: Dataset: zara2               Batch: 22/36	Loss 4.8515 (5.9007)
2022-11-08 17:59:28,596:INFO: Dataset: zara2               Batch: 23/36	Loss 5.1212 (5.8627)
2022-11-08 17:59:28,600:INFO: Dataset: zara2               Batch: 24/36	Loss 5.0461 (5.8300)
2022-11-08 17:59:28,603:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8340 (5.7866)
2022-11-08 17:59:28,606:INFO: Dataset: zara2               Batch: 26/36	Loss 4.6906 (5.7411)
2022-11-08 17:59:28,611:INFO: Dataset: zara2               Batch: 27/36	Loss 4.4693 (5.6973)
2022-11-08 17:59:28,616:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7564 (5.6588)
2022-11-08 17:59:28,620:INFO: Dataset: zara2               Batch: 29/36	Loss 4.9536 (5.6345)
2022-11-08 17:59:28,623:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5832 (5.6018)
2022-11-08 17:59:28,628:INFO: Dataset: zara2               Batch: 31/36	Loss 4.1388 (5.5585)
2022-11-08 17:59:28,634:INFO: Dataset: zara2               Batch: 32/36	Loss 4.5790 (5.5299)
2022-11-08 17:59:28,640:INFO: Dataset: zara2               Batch: 33/36	Loss 4.3048 (5.4931)
2022-11-08 17:59:28,643:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0843 (5.4473)
2022-11-08 17:59:28,649:INFO: Dataset: zara2               Batch: 35/36	Loss 4.1809 (5.4139)
2022-11-08 17:59:28,655:INFO: Dataset: zara2               Batch: 36/36	Loss 4.3819 (5.3902)
2022-11-08 17:59:29,582:INFO: - Computing loss (validation)
2022-11-08 17:59:36,211:INFO: Dataset: hotel               Batch: 1/3	Loss 16.8235 (16.8235)
2022-11-08 17:59:36,561:INFO: Dataset: hotel               Batch: 2/3	Loss 17.1666 (16.9957)
2022-11-08 17:59:36,936:INFO: Dataset: hotel               Batch: 3/3	Loss 14.1738 (16.8031)
2022-11-08 17:59:44,675:INFO: Dataset: univ                Batch: 1/6	Loss 8.6946 (8.6946)
2022-11-08 17:59:45,056:INFO: Dataset: univ                Batch: 2/6	Loss 8.5496 (8.6223)
2022-11-08 17:59:45,465:INFO: Dataset: univ                Batch: 3/6	Loss 8.7631 (8.6705)
2022-11-08 17:59:45,762:INFO: Dataset: univ                Batch: 4/6	Loss 8.9769 (8.7380)
2022-11-08 17:59:46,029:INFO: Dataset: univ                Batch: 5/6	Loss 8.3495 (8.6603)
2022-11-08 17:59:46,260:INFO: Dataset: univ                Batch: 6/6	Loss 8.7884 (8.6761)
2022-11-08 17:59:53,318:INFO: Dataset: zara1               Batch: 1/3	Loss 12.0303 (12.0303)
2022-11-08 17:59:53,677:INFO: Dataset: zara1               Batch: 2/3	Loss 12.0639 (12.0464)
2022-11-08 17:59:54,110:INFO: Dataset: zara1               Batch: 3/3	Loss 12.4093 (12.1362)
2022-11-08 18:00:02,040:INFO: Dataset: zara2               Batch:  1/10	Loss 4.0089 (4.0089)
2022-11-08 18:00:02,365:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1232 (4.0661)
2022-11-08 18:00:02,786:INFO: Dataset: zara2               Batch:  3/10	Loss 3.9324 (4.0266)
2022-11-08 18:00:03,019:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1797 (4.0633)
2022-11-08 18:00:03,255:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1756 (4.0845)
2022-11-08 18:00:03,514:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2921 (4.1187)
2022-11-08 18:00:03,515:INFO: Dataset: zara2               Batch:  7/10	Loss 4.2699 (4.1406)
2022-11-08 18:00:03,516:INFO: Dataset: zara2               Batch:  8/10	Loss 3.9997 (4.1231)
2022-11-08 18:00:03,519:INFO: Dataset: zara2               Batch:  9/10	Loss 4.1327 (4.1242)
2022-11-08 18:00:03,519:INFO: Dataset: zara2               Batch: 10/10	Loss 4.0439 (4.1156)
2022-11-08 18:00:04,488:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1068.pth.tar
2022-11-08 18:00:04,488:INFO: 
===> EPOCH: 1069 (P5)
2022-11-08 18:00:04,488:INFO: - Computing loss (training)
2022-11-08 18:00:10,808:INFO: Dataset: hotel               Batch: 1/8	Loss 15.3394 (15.3394)
2022-11-08 18:00:11,207:INFO: Dataset: hotel               Batch: 2/8	Loss 14.1785 (14.7617)
2022-11-08 18:00:11,540:INFO: Dataset: hotel               Batch: 3/8	Loss 13.9528 (14.4912)
2022-11-08 18:00:11,852:INFO: Dataset: hotel               Batch: 4/8	Loss 13.9127 (14.3412)
2022-11-08 18:00:12,119:INFO: Dataset: hotel               Batch: 5/8	Loss 15.1729 (14.5161)
2022-11-08 18:00:12,305:INFO: Dataset: hotel               Batch: 6/8	Loss 12.3564 (14.1911)
2022-11-08 18:00:12,311:INFO: Dataset: hotel               Batch: 7/8	Loss 13.7296 (14.1286)
2022-11-08 18:00:12,314:INFO: Dataset: hotel               Batch: 8/8	Loss 7.7151 (13.9594)
2022-11-08 18:00:35,745:INFO: Dataset: univ                Batch:  1/29	Loss 9.0977 (9.0977)
2022-11-08 18:00:35,751:INFO: Dataset: univ                Batch:  2/29	Loss 9.3427 (9.2092)
2022-11-08 18:00:35,758:INFO: Dataset: univ                Batch:  3/29	Loss 9.1163 (9.1794)
2022-11-08 18:00:35,762:INFO: Dataset: univ                Batch:  4/29	Loss 9.6376 (9.2761)
2022-11-08 18:00:35,766:INFO: Dataset: univ                Batch:  5/29	Loss 8.9196 (9.2013)
2022-11-08 18:00:35,772:INFO: Dataset: univ                Batch:  6/29	Loss 9.2165 (9.2040)
2022-11-08 18:00:35,777:INFO: Dataset: univ                Batch:  7/29	Loss 8.4574 (9.0835)
2022-11-08 18:00:35,781:INFO: Dataset: univ                Batch:  8/29	Loss 8.4875 (9.0179)
2022-11-08 18:00:35,786:INFO: Dataset: univ                Batch:  9/29	Loss 8.8291 (8.9964)
2022-11-08 18:00:35,791:INFO: Dataset: univ                Batch: 10/29	Loss 8.5468 (8.9502)
2022-11-08 18:00:35,796:INFO: Dataset: univ                Batch: 11/29	Loss 8.7808 (8.9339)
2022-11-08 18:00:35,801:INFO: Dataset: univ                Batch: 12/29	Loss 8.7016 (8.9147)
2022-11-08 18:00:35,805:INFO: Dataset: univ                Batch: 13/29	Loss 8.1600 (8.8548)
2022-11-08 18:00:35,810:INFO: Dataset: univ                Batch: 14/29	Loss 8.6686 (8.8402)
2022-11-08 18:00:35,816:INFO: Dataset: univ                Batch: 15/29	Loss 7.9776 (8.7814)
2022-11-08 18:00:35,822:INFO: Dataset: univ                Batch: 16/29	Loss 7.9474 (8.7268)
2022-11-08 18:00:35,827:INFO: Dataset: univ                Batch: 17/29	Loss 8.2542 (8.7000)
2022-11-08 18:00:35,832:INFO: Dataset: univ                Batch: 18/29	Loss 7.8439 (8.6541)
2022-11-08 18:00:35,840:INFO: Dataset: univ                Batch: 19/29	Loss 7.7141 (8.6058)
2022-11-08 18:00:35,844:INFO: Dataset: univ                Batch: 20/29	Loss 7.5031 (8.5508)
2022-11-08 18:00:35,849:INFO: Dataset: univ                Batch: 21/29	Loss 7.4088 (8.4985)
2022-11-08 18:00:35,854:INFO: Dataset: univ                Batch: 22/29	Loss 7.6351 (8.4558)
2022-11-08 18:00:35,859:INFO: Dataset: univ                Batch: 23/29	Loss 7.4529 (8.4130)
2022-11-08 18:00:35,864:INFO: Dataset: univ                Batch: 24/29	Loss 6.8611 (8.3398)
2022-11-08 18:00:35,870:INFO: Dataset: univ                Batch: 25/29	Loss 7.0465 (8.2912)
2022-11-08 18:00:35,876:INFO: Dataset: univ                Batch: 26/29	Loss 6.9432 (8.2330)
2022-11-08 18:00:35,881:INFO: Dataset: univ                Batch: 27/29	Loss 6.5470 (8.1718)
2022-11-08 18:00:35,884:INFO: Dataset: univ                Batch: 28/29	Loss 6.5099 (8.1115)
2022-11-08 18:00:35,888:INFO: Dataset: univ                Batch: 29/29	Loss 6.6384 (8.0939)
2022-11-08 18:00:42,809:INFO: Dataset: zara1               Batch:  1/16	Loss 12.4598 (12.4598)
2022-11-08 18:00:43,200:INFO: Dataset: zara1               Batch:  2/16	Loss 13.7070 (13.0834)
2022-11-08 18:00:43,582:INFO: Dataset: zara1               Batch:  3/16	Loss 13.3619 (13.1774)
2022-11-08 18:00:43,842:INFO: Dataset: zara1               Batch:  4/16	Loss 13.5080 (13.2671)
2022-11-08 18:00:44,088:INFO: Dataset: zara1               Batch:  5/16	Loss 13.3963 (13.2952)
2022-11-08 18:00:44,263:INFO: Dataset: zara1               Batch:  6/16	Loss 13.2685 (13.2909)
2022-11-08 18:00:44,267:INFO: Dataset: zara1               Batch:  7/16	Loss 13.1057 (13.2675)
2022-11-08 18:00:44,272:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8359 (13.2022)
2022-11-08 18:00:44,276:INFO: Dataset: zara1               Batch:  9/16	Loss 12.6504 (13.1242)
2022-11-08 18:00:44,280:INFO: Dataset: zara1               Batch: 10/16	Loss 12.5759 (13.0575)
2022-11-08 18:00:44,284:INFO: Dataset: zara1               Batch: 11/16	Loss 12.7047 (13.0251)
2022-11-08 18:00:44,287:INFO: Dataset: zara1               Batch: 12/16	Loss 12.6600 (12.9962)
2022-11-08 18:00:44,291:INFO: Dataset: zara1               Batch: 13/16	Loss 11.9028 (12.8985)
2022-11-08 18:00:44,295:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3174 (12.8520)
2022-11-08 18:00:44,297:INFO: Dataset: zara1               Batch: 15/16	Loss 12.5268 (12.8334)
2022-11-08 18:00:44,303:INFO: Dataset: zara1               Batch: 16/16	Loss 11.8187 (12.7762)
2022-11-08 18:01:07,659:INFO: Dataset: zara2               Batch:  1/36	Loss 5.8163 (5.8163)
2022-11-08 18:01:07,665:INFO: Dataset: zara2               Batch:  2/36	Loss 6.2690 (6.0533)
2022-11-08 18:01:07,670:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1685 (6.0912)
2022-11-08 18:01:07,676:INFO: Dataset: zara2               Batch:  4/36	Loss 6.2954 (6.1443)
2022-11-08 18:01:07,682:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4464 (6.2021)
2022-11-08 18:01:07,686:INFO: Dataset: zara2               Batch:  6/36	Loss 6.0230 (6.1710)
2022-11-08 18:01:07,689:INFO: Dataset: zara2               Batch:  7/36	Loss 6.3690 (6.1972)
2022-11-08 18:01:07,693:INFO: Dataset: zara2               Batch:  8/36	Loss 6.3116 (6.2084)
2022-11-08 18:01:07,698:INFO: Dataset: zara2               Batch:  9/36	Loss 6.7131 (6.2625)
2022-11-08 18:01:07,702:INFO: Dataset: zara2               Batch: 10/36	Loss 6.1931 (6.2561)
2022-11-08 18:01:07,706:INFO: Dataset: zara2               Batch: 11/36	Loss 5.9589 (6.2310)
2022-11-08 18:01:07,709:INFO: Dataset: zara2               Batch: 12/36	Loss 5.9164 (6.2061)
2022-11-08 18:01:07,712:INFO: Dataset: zara2               Batch: 13/36	Loss 5.9511 (6.1869)
2022-11-08 18:01:07,717:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6381 (6.1484)
2022-11-08 18:01:07,721:INFO: Dataset: zara2               Batch: 15/36	Loss 5.7028 (6.1170)
2022-11-08 18:01:07,729:INFO: Dataset: zara2               Batch: 16/36	Loss 5.4441 (6.0703)
2022-11-08 18:01:07,734:INFO: Dataset: zara2               Batch: 17/36	Loss 5.5517 (6.0399)
2022-11-08 18:01:07,740:INFO: Dataset: zara2               Batch: 18/36	Loss 5.7167 (6.0206)
2022-11-08 18:01:07,744:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2365 (5.9797)
2022-11-08 18:01:07,749:INFO: Dataset: zara2               Batch: 20/36	Loss 5.3549 (5.9485)
2022-11-08 18:01:07,752:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3678 (5.9214)
2022-11-08 18:01:07,755:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0282 (5.8821)
2022-11-08 18:01:07,759:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9812 (5.8366)
2022-11-08 18:01:07,762:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7980 (5.7934)
2022-11-08 18:01:07,766:INFO: Dataset: zara2               Batch: 25/36	Loss 4.9117 (5.7600)
2022-11-08 18:01:07,770:INFO: Dataset: zara2               Batch: 26/36	Loss 5.2948 (5.7424)
2022-11-08 18:01:07,773:INFO: Dataset: zara2               Batch: 27/36	Loss 4.6981 (5.7074)
2022-11-08 18:01:07,776:INFO: Dataset: zara2               Batch: 28/36	Loss 4.6325 (5.6689)
2022-11-08 18:01:07,780:INFO: Dataset: zara2               Batch: 29/36	Loss 4.3303 (5.6225)
2022-11-08 18:01:07,785:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3335 (5.5875)
2022-11-08 18:01:07,788:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3989 (5.5466)
2022-11-08 18:01:07,792:INFO: Dataset: zara2               Batch: 32/36	Loss 4.2524 (5.5077)
2022-11-08 18:01:07,796:INFO: Dataset: zara2               Batch: 33/36	Loss 4.1911 (5.4642)
2022-11-08 18:01:07,800:INFO: Dataset: zara2               Batch: 34/36	Loss 4.2443 (5.4295)
2022-11-08 18:01:07,803:INFO: Dataset: zara2               Batch: 35/36	Loss 4.4023 (5.4037)
2022-11-08 18:01:07,806:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9131 (5.3716)
2022-11-08 18:01:08,714:INFO: - Computing loss (validation)
2022-11-08 18:01:15,831:INFO: Dataset: hotel               Batch: 1/3	Loss 16.7492 (16.7492)
2022-11-08 18:01:16,007:INFO: Dataset: hotel               Batch: 2/3	Loss 15.9227 (16.3252)
2022-11-08 18:01:16,490:INFO: Dataset: hotel               Batch: 3/3	Loss 17.1510 (16.3928)
2022-11-08 18:01:24,219:INFO: Dataset: univ                Batch: 1/6	Loss 7.9542 (7.9542)
2022-11-08 18:01:24,627:INFO: Dataset: univ                Batch: 2/6	Loss 8.6771 (8.2195)
2022-11-08 18:01:25,011:INFO: Dataset: univ                Batch: 3/6	Loss 8.8652 (8.4242)
2022-11-08 18:01:25,301:INFO: Dataset: univ                Batch: 4/6	Loss 8.6473 (8.4754)
2022-11-08 18:01:25,556:INFO: Dataset: univ                Batch: 5/6	Loss 8.2931 (8.4353)
2022-11-08 18:01:25,713:INFO: Dataset: univ                Batch: 6/6	Loss 9.9250 (8.5964)
2022-11-08 18:01:33,032:INFO: Dataset: zara1               Batch: 1/3	Loss 12.0246 (12.0246)
2022-11-08 18:01:33,526:INFO: Dataset: zara1               Batch: 2/3	Loss 12.4631 (12.2411)
2022-11-08 18:01:33,905:INFO: Dataset: zara1               Batch: 3/3	Loss 11.9669 (12.1741)
2022-11-08 18:01:41,810:INFO: Dataset: zara2               Batch:  1/10	Loss 4.0376 (4.0376)
2022-11-08 18:01:42,179:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1631 (4.1039)
2022-11-08 18:01:42,562:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2788 (4.1587)
2022-11-08 18:01:42,838:INFO: Dataset: zara2               Batch:  4/10	Loss 4.2602 (4.1835)
2022-11-08 18:01:43,091:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2213 (4.1907)
2022-11-08 18:01:43,273:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2983 (4.2090)
2022-11-08 18:01:43,275:INFO: Dataset: zara2               Batch:  7/10	Loss 4.0599 (4.1895)
2022-11-08 18:01:43,276:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0767 (4.1767)
2022-11-08 18:01:43,277:INFO: Dataset: zara2               Batch:  9/10	Loss 4.3293 (4.1922)
2022-11-08 18:01:43,279:INFO: Dataset: zara2               Batch: 10/10	Loss 4.0849 (4.1823)
2022-11-08 18:01:44,276:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1069.pth.tar
2022-11-08 18:01:44,277:INFO: 
===> EPOCH: 1070 (P5)
2022-11-08 18:01:44,277:INFO: - Computing loss (training)
2022-11-08 18:01:50,712:INFO: Dataset: hotel               Batch: 1/8	Loss 13.3728 (13.3728)
2022-11-08 18:01:51,087:INFO: Dataset: hotel               Batch: 2/8	Loss 13.5525 (13.4676)
2022-11-08 18:01:51,474:INFO: Dataset: hotel               Batch: 3/8	Loss 12.9506 (13.2879)
2022-11-08 18:01:51,765:INFO: Dataset: hotel               Batch: 4/8	Loss 15.4329 (13.8565)
2022-11-08 18:01:51,960:INFO: Dataset: hotel               Batch: 5/8	Loss 14.2530 (13.9413)
2022-11-08 18:01:52,164:INFO: Dataset: hotel               Batch: 6/8	Loss 13.3875 (13.8517)
2022-11-08 18:01:52,169:INFO: Dataset: hotel               Batch: 7/8	Loss 13.3877 (13.7843)
2022-11-08 18:01:52,175:INFO: Dataset: hotel               Batch: 8/8	Loss 11.8008 (13.7294)
2022-11-08 18:02:15,619:INFO: Dataset: univ                Batch:  1/29	Loss 9.2089 (9.2089)
2022-11-08 18:02:15,626:INFO: Dataset: univ                Batch:  2/29	Loss 9.3256 (9.2692)
2022-11-08 18:02:15,632:INFO: Dataset: univ                Batch:  3/29	Loss 8.9025 (9.1600)
2022-11-08 18:02:15,642:INFO: Dataset: univ                Batch:  4/29	Loss 8.8789 (9.0839)
2022-11-08 18:02:15,651:INFO: Dataset: univ                Batch:  5/29	Loss 8.7718 (9.0215)
2022-11-08 18:02:15,662:INFO: Dataset: univ                Batch:  6/29	Loss 8.6661 (8.9523)
2022-11-08 18:02:15,667:INFO: Dataset: univ                Batch:  7/29	Loss 9.0626 (8.9689)
2022-11-08 18:02:15,672:INFO: Dataset: univ                Batch:  8/29	Loss 8.7257 (8.9393)
2022-11-08 18:02:15,677:INFO: Dataset: univ                Batch:  9/29	Loss 8.6224 (8.9036)
2022-11-08 18:02:15,681:INFO: Dataset: univ                Batch: 10/29	Loss 8.6530 (8.8797)
2022-11-08 18:02:15,686:INFO: Dataset: univ                Batch: 11/29	Loss 8.1353 (8.8042)
2022-11-08 18:02:15,691:INFO: Dataset: univ                Batch: 12/29	Loss 8.7459 (8.7994)
2022-11-08 18:02:15,694:INFO: Dataset: univ                Batch: 13/29	Loss 8.4046 (8.7716)
2022-11-08 18:02:15,699:INFO: Dataset: univ                Batch: 14/29	Loss 8.0275 (8.7172)
2022-11-08 18:02:15,705:INFO: Dataset: univ                Batch: 15/29	Loss 7.6995 (8.6494)
2022-11-08 18:02:15,710:INFO: Dataset: univ                Batch: 16/29	Loss 8.1788 (8.6191)
2022-11-08 18:02:15,714:INFO: Dataset: univ                Batch: 17/29	Loss 8.1514 (8.5956)
2022-11-08 18:02:15,720:INFO: Dataset: univ                Batch: 18/29	Loss 7.7196 (8.5482)
2022-11-08 18:02:15,724:INFO: Dataset: univ                Batch: 19/29	Loss 7.9736 (8.5194)
2022-11-08 18:02:15,728:INFO: Dataset: univ                Batch: 20/29	Loss 7.7755 (8.4794)
2022-11-08 18:02:15,732:INFO: Dataset: univ                Batch: 21/29	Loss 7.4768 (8.4330)
2022-11-08 18:02:15,737:INFO: Dataset: univ                Batch: 22/29	Loss 7.1040 (8.3658)
2022-11-08 18:02:15,742:INFO: Dataset: univ                Batch: 23/29	Loss 6.9607 (8.3043)
2022-11-08 18:02:15,746:INFO: Dataset: univ                Batch: 24/29	Loss 6.9137 (8.2481)
2022-11-08 18:02:15,750:INFO: Dataset: univ                Batch: 25/29	Loss 7.0259 (8.1976)
2022-11-08 18:02:15,755:INFO: Dataset: univ                Batch: 26/29	Loss 6.7214 (8.1364)
2022-11-08 18:02:15,759:INFO: Dataset: univ                Batch: 27/29	Loss 6.3379 (8.0594)
2022-11-08 18:02:15,764:INFO: Dataset: univ                Batch: 28/29	Loss 6.6424 (8.0080)
2022-11-08 18:02:15,767:INFO: Dataset: univ                Batch: 29/29	Loss 6.5619 (7.9901)
2022-11-08 18:02:23,062:INFO: Dataset: zara1               Batch:  1/16	Loss 12.9784 (12.9784)
2022-11-08 18:02:23,381:INFO: Dataset: zara1               Batch:  2/16	Loss 13.5776 (13.2709)
2022-11-08 18:02:23,723:INFO: Dataset: zara1               Batch:  3/16	Loss 13.3933 (13.3081)
2022-11-08 18:02:24,142:INFO: Dataset: zara1               Batch:  4/16	Loss 13.4149 (13.3382)
2022-11-08 18:02:24,423:INFO: Dataset: zara1               Batch:  5/16	Loss 12.9727 (13.2766)
2022-11-08 18:02:24,702:INFO: Dataset: zara1               Batch:  6/16	Loss 12.5423 (13.1706)
2022-11-08 18:02:24,707:INFO: Dataset: zara1               Batch:  7/16	Loss 12.8160 (13.1160)
2022-11-08 18:02:24,712:INFO: Dataset: zara1               Batch:  8/16	Loss 13.3494 (13.1492)
2022-11-08 18:02:24,716:INFO: Dataset: zara1               Batch:  9/16	Loss 13.0706 (13.1412)
2022-11-08 18:02:24,720:INFO: Dataset: zara1               Batch: 10/16	Loss 12.2968 (13.0616)
2022-11-08 18:02:24,723:INFO: Dataset: zara1               Batch: 11/16	Loss 12.7465 (13.0294)
2022-11-08 18:02:24,727:INFO: Dataset: zara1               Batch: 12/16	Loss 12.6343 (12.9966)
2022-11-08 18:02:24,730:INFO: Dataset: zara1               Batch: 13/16	Loss 12.7121 (12.9733)
2022-11-08 18:02:24,734:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3485 (12.9359)
2022-11-08 18:02:24,738:INFO: Dataset: zara1               Batch: 15/16	Loss 12.5935 (12.9144)
2022-11-08 18:02:24,741:INFO: Dataset: zara1               Batch: 16/16	Loss 11.8515 (12.8680)
2022-11-08 18:02:48,078:INFO: Dataset: zara2               Batch:  1/36	Loss 6.4891 (6.4891)
2022-11-08 18:02:48,082:INFO: Dataset: zara2               Batch:  2/36	Loss 6.5655 (6.5282)
2022-11-08 18:02:48,087:INFO: Dataset: zara2               Batch:  3/36	Loss 6.3464 (6.4701)
2022-11-08 18:02:48,092:INFO: Dataset: zara2               Batch:  4/36	Loss 6.0925 (6.3807)
2022-11-08 18:02:48,099:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3682 (6.3783)
2022-11-08 18:02:48,117:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1510 (6.3400)
2022-11-08 18:02:48,122:INFO: Dataset: zara2               Batch:  7/36	Loss 6.2163 (6.3243)
2022-11-08 18:02:48,126:INFO: Dataset: zara2               Batch:  8/36	Loss 6.4383 (6.3379)
2022-11-08 18:02:48,130:INFO: Dataset: zara2               Batch:  9/36	Loss 5.9218 (6.2877)
2022-11-08 18:02:48,135:INFO: Dataset: zara2               Batch: 10/36	Loss 5.8365 (6.2483)
2022-11-08 18:02:48,139:INFO: Dataset: zara2               Batch: 11/36	Loss 6.2927 (6.2523)
2022-11-08 18:02:48,143:INFO: Dataset: zara2               Batch: 12/36	Loss 5.9834 (6.2340)
2022-11-08 18:02:48,147:INFO: Dataset: zara2               Batch: 13/36	Loss 6.5391 (6.2581)
2022-11-08 18:02:48,151:INFO: Dataset: zara2               Batch: 14/36	Loss 6.1077 (6.2469)
2022-11-08 18:02:48,154:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5817 (6.2040)
2022-11-08 18:02:48,158:INFO: Dataset: zara2               Batch: 16/36	Loss 5.9922 (6.1924)
2022-11-08 18:02:48,161:INFO: Dataset: zara2               Batch: 17/36	Loss 6.0890 (6.1866)
2022-11-08 18:02:48,165:INFO: Dataset: zara2               Batch: 18/36	Loss 5.8824 (6.1684)
2022-11-08 18:02:48,170:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2747 (6.1190)
2022-11-08 18:02:48,174:INFO: Dataset: zara2               Batch: 20/36	Loss 4.9482 (6.0566)
2022-11-08 18:02:48,179:INFO: Dataset: zara2               Batch: 21/36	Loss 5.0245 (6.0131)
2022-11-08 18:02:48,183:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0760 (5.9681)
2022-11-08 18:02:48,188:INFO: Dataset: zara2               Batch: 23/36	Loss 4.8253 (5.9157)
2022-11-08 18:02:48,193:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7632 (5.8654)
2022-11-08 18:02:48,197:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8042 (5.8284)
2022-11-08 18:02:48,204:INFO: Dataset: zara2               Batch: 26/36	Loss 4.7506 (5.7919)
2022-11-08 18:02:48,209:INFO: Dataset: zara2               Batch: 27/36	Loss 4.5441 (5.7455)
2022-11-08 18:02:48,213:INFO: Dataset: zara2               Batch: 28/36	Loss 4.4826 (5.7048)
2022-11-08 18:02:48,217:INFO: Dataset: zara2               Batch: 29/36	Loss 4.7395 (5.6752)
2022-11-08 18:02:48,219:INFO: Dataset: zara2               Batch: 30/36	Loss 4.6497 (5.6397)
2022-11-08 18:02:48,223:INFO: Dataset: zara2               Batch: 31/36	Loss 4.2450 (5.5940)
2022-11-08 18:02:48,229:INFO: Dataset: zara2               Batch: 32/36	Loss 4.1231 (5.5395)
2022-11-08 18:02:48,236:INFO: Dataset: zara2               Batch: 33/36	Loss 4.1376 (5.4985)
2022-11-08 18:02:48,243:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0705 (5.4561)
2022-11-08 18:02:48,248:INFO: Dataset: zara2               Batch: 35/36	Loss 4.3916 (5.4285)
2022-11-08 18:02:48,252:INFO: Dataset: zara2               Batch: 36/36	Loss 4.0913 (5.4044)
2022-11-08 18:02:49,169:INFO: - Computing loss (validation)
2022-11-08 18:02:56,066:INFO: Dataset: hotel               Batch: 1/3	Loss 17.0208 (17.0208)
2022-11-08 18:02:56,445:INFO: Dataset: hotel               Batch: 2/3	Loss 16.4638 (16.7392)
2022-11-08 18:02:56,874:INFO: Dataset: hotel               Batch: 3/3	Loss 17.7005 (16.8048)
2022-11-08 18:03:04,246:INFO: Dataset: univ                Batch: 1/6	Loss 9.3721 (9.3721)
2022-11-08 18:03:04,653:INFO: Dataset: univ                Batch: 2/6	Loss 8.2566 (8.7330)
2022-11-08 18:03:04,993:INFO: Dataset: univ                Batch: 3/6	Loss 8.5139 (8.6500)
2022-11-08 18:03:05,287:INFO: Dataset: univ                Batch: 4/6	Loss 8.0216 (8.4569)
2022-11-08 18:03:05,490:INFO: Dataset: univ                Batch: 5/6	Loss 8.5884 (8.4808)
2022-11-08 18:03:05,727:INFO: Dataset: univ                Batch: 6/6	Loss 8.7667 (8.5228)
2022-11-08 18:03:12,588:INFO: Dataset: zara1               Batch: 1/3	Loss 12.1711 (12.1711)
2022-11-08 18:03:12,953:INFO: Dataset: zara1               Batch: 2/3	Loss 12.0330 (12.0934)
2022-11-08 18:03:13,356:INFO: Dataset: zara1               Batch: 3/3	Loss 11.9011 (12.0439)
2022-11-08 18:03:20,847:INFO: Dataset: zara2               Batch:  1/10	Loss 3.8684 (3.8684)
2022-11-08 18:03:21,198:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1791 (4.0257)
2022-11-08 18:03:21,579:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0151 (4.0222)
2022-11-08 18:03:21,917:INFO: Dataset: zara2               Batch:  4/10	Loss 4.0970 (4.0427)
2022-11-08 18:03:22,073:INFO: Dataset: zara2               Batch:  5/10	Loss 4.0541 (4.0449)
2022-11-08 18:03:22,304:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2032 (4.0697)
2022-11-08 18:03:22,307:INFO: Dataset: zara2               Batch:  7/10	Loss 4.0107 (4.0617)
2022-11-08 18:03:22,309:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2589 (4.0853)
2022-11-08 18:03:22,311:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2580 (4.1048)
2022-11-08 18:03:22,313:INFO: Dataset: zara2               Batch: 10/10	Loss 4.4089 (4.1353)
2022-11-08 18:03:23,227:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1070.pth.tar
2022-11-08 18:03:23,227:INFO: 
===> EPOCH: 1071 (P5)
2022-11-08 18:03:23,228:INFO: - Computing loss (training)
2022-11-08 18:03:29,270:INFO: Dataset: hotel               Batch: 1/8	Loss 14.8981 (14.8981)
2022-11-08 18:03:29,623:INFO: Dataset: hotel               Batch: 2/8	Loss 15.7464 (15.2937)
2022-11-08 18:03:29,961:INFO: Dataset: hotel               Batch: 3/8	Loss 14.1956 (14.9300)
2022-11-08 18:03:30,313:INFO: Dataset: hotel               Batch: 4/8	Loss 12.9891 (14.4298)
2022-11-08 18:03:30,526:INFO: Dataset: hotel               Batch: 5/8	Loss 14.1428 (14.3727)
2022-11-08 18:03:30,747:INFO: Dataset: hotel               Batch: 6/8	Loss 13.6372 (14.2478)
2022-11-08 18:03:30,751:INFO: Dataset: hotel               Batch: 7/8	Loss 11.0607 (13.8037)
2022-11-08 18:03:30,754:INFO: Dataset: hotel               Batch: 8/8	Loss 15.7884 (13.8718)
2022-11-08 18:03:55,148:INFO: Dataset: univ                Batch:  1/29	Loss 8.5113 (8.5113)
2022-11-08 18:03:55,154:INFO: Dataset: univ                Batch:  2/29	Loss 9.2558 (8.8957)
2022-11-08 18:03:55,158:INFO: Dataset: univ                Batch:  3/29	Loss 9.0631 (8.9498)
2022-11-08 18:03:55,163:INFO: Dataset: univ                Batch:  4/29	Loss 9.2294 (9.0127)
2022-11-08 18:03:55,167:INFO: Dataset: univ                Batch:  5/29	Loss 8.9703 (9.0039)
2022-11-08 18:03:55,188:INFO: Dataset: univ                Batch:  6/29	Loss 9.0763 (9.0143)
2022-11-08 18:03:55,194:INFO: Dataset: univ                Batch:  7/29	Loss 8.5413 (8.9400)
2022-11-08 18:03:55,200:INFO: Dataset: univ                Batch:  8/29	Loss 8.9955 (8.9468)
2022-11-08 18:03:55,204:INFO: Dataset: univ                Batch:  9/29	Loss 8.9814 (8.9507)
2022-11-08 18:03:55,209:INFO: Dataset: univ                Batch: 10/29	Loss 8.5854 (8.9128)
2022-11-08 18:03:55,214:INFO: Dataset: univ                Batch: 11/29	Loss 8.2061 (8.8440)
2022-11-08 18:03:55,218:INFO: Dataset: univ                Batch: 12/29	Loss 8.0892 (8.7737)
2022-11-08 18:03:55,222:INFO: Dataset: univ                Batch: 13/29	Loss 8.3810 (8.7495)
2022-11-08 18:03:55,225:INFO: Dataset: univ                Batch: 14/29	Loss 8.6238 (8.7421)
2022-11-08 18:03:55,230:INFO: Dataset: univ                Batch: 15/29	Loss 8.0323 (8.6986)
2022-11-08 18:03:55,234:INFO: Dataset: univ                Batch: 16/29	Loss 8.2146 (8.6705)
2022-11-08 18:03:55,240:INFO: Dataset: univ                Batch: 17/29	Loss 8.2261 (8.6420)
2022-11-08 18:03:55,245:INFO: Dataset: univ                Batch: 18/29	Loss 7.6957 (8.5970)
2022-11-08 18:03:55,251:INFO: Dataset: univ                Batch: 19/29	Loss 7.7485 (8.5495)
2022-11-08 18:03:55,255:INFO: Dataset: univ                Batch: 20/29	Loss 7.5659 (8.4965)
2022-11-08 18:03:55,260:INFO: Dataset: univ                Batch: 21/29	Loss 7.4644 (8.4443)
2022-11-08 18:03:55,264:INFO: Dataset: univ                Batch: 22/29	Loss 7.0644 (8.3796)
2022-11-08 18:03:55,269:INFO: Dataset: univ                Batch: 23/29	Loss 7.2606 (8.3264)
2022-11-08 18:03:55,274:INFO: Dataset: univ                Batch: 24/29	Loss 7.1680 (8.2765)
2022-11-08 18:03:55,278:INFO: Dataset: univ                Batch: 25/29	Loss 7.1256 (8.2239)
2022-11-08 18:03:55,281:INFO: Dataset: univ                Batch: 26/29	Loss 6.7730 (8.1672)
2022-11-08 18:03:55,287:INFO: Dataset: univ                Batch: 27/29	Loss 6.4863 (8.1078)
2022-11-08 18:03:55,292:INFO: Dataset: univ                Batch: 28/29	Loss 6.9017 (8.0646)
2022-11-08 18:03:55,296:INFO: Dataset: univ                Batch: 29/29	Loss 6.5360 (8.0421)
2022-11-08 18:04:02,526:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8963 (12.8963)
2022-11-08 18:04:02,987:INFO: Dataset: zara1               Batch:  2/16	Loss 12.8055 (12.8496)
2022-11-08 18:04:03,328:INFO: Dataset: zara1               Batch:  3/16	Loss 12.5083 (12.7444)
2022-11-08 18:04:03,613:INFO: Dataset: zara1               Batch:  4/16	Loss 13.2646 (12.8872)
2022-11-08 18:04:03,899:INFO: Dataset: zara1               Batch:  5/16	Loss 13.7533 (13.0604)
2022-11-08 18:04:04,043:INFO: Dataset: zara1               Batch:  6/16	Loss 13.1002 (13.0664)
2022-11-08 18:04:04,046:INFO: Dataset: zara1               Batch:  7/16	Loss 13.1795 (13.0811)
2022-11-08 18:04:04,048:INFO: Dataset: zara1               Batch:  8/16	Loss 12.7760 (13.0437)
2022-11-08 18:04:04,051:INFO: Dataset: zara1               Batch:  9/16	Loss 13.1962 (13.0619)
2022-11-08 18:04:04,054:INFO: Dataset: zara1               Batch: 10/16	Loss 12.4737 (12.9920)
2022-11-08 18:04:04,057:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5751 (12.9558)
2022-11-08 18:04:04,060:INFO: Dataset: zara1               Batch: 12/16	Loss 12.2494 (12.8963)
2022-11-08 18:04:04,063:INFO: Dataset: zara1               Batch: 13/16	Loss 12.1603 (12.8514)
2022-11-08 18:04:04,066:INFO: Dataset: zara1               Batch: 14/16	Loss 12.2316 (12.8066)
2022-11-08 18:04:04,069:INFO: Dataset: zara1               Batch: 15/16	Loss 12.3963 (12.7771)
2022-11-08 18:04:04,071:INFO: Dataset: zara1               Batch: 16/16	Loss 11.7016 (12.7239)
2022-11-08 18:04:27,028:INFO: Dataset: zara2               Batch:  1/36	Loss 5.8313 (5.8313)
2022-11-08 18:04:27,033:INFO: Dataset: zara2               Batch:  2/36	Loss 6.3034 (6.0789)
2022-11-08 18:04:27,037:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1612 (6.1063)
2022-11-08 18:04:27,041:INFO: Dataset: zara2               Batch:  4/36	Loss 6.7828 (6.2788)
2022-11-08 18:04:27,045:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3423 (6.2910)
2022-11-08 18:04:27,049:INFO: Dataset: zara2               Batch:  6/36	Loss 6.0797 (6.2630)
2022-11-08 18:04:27,052:INFO: Dataset: zara2               Batch:  7/36	Loss 5.8727 (6.2107)
2022-11-08 18:04:27,055:INFO: Dataset: zara2               Batch:  8/36	Loss 6.5942 (6.2611)
2022-11-08 18:04:27,058:INFO: Dataset: zara2               Batch:  9/36	Loss 6.0371 (6.2355)
2022-11-08 18:04:27,062:INFO: Dataset: zara2               Batch: 10/36	Loss 6.1487 (6.2272)
2022-11-08 18:04:27,065:INFO: Dataset: zara2               Batch: 11/36	Loss 6.3668 (6.2430)
2022-11-08 18:04:27,068:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7953 (6.2029)
2022-11-08 18:04:27,070:INFO: Dataset: zara2               Batch: 13/36	Loss 6.4983 (6.2253)
2022-11-08 18:04:27,073:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6379 (6.1860)
2022-11-08 18:04:27,079:INFO: Dataset: zara2               Batch: 15/36	Loss 6.0421 (6.1762)
2022-11-08 18:04:27,084:INFO: Dataset: zara2               Batch: 16/36	Loss 5.5070 (6.1311)
2022-11-08 18:04:27,089:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6825 (6.1069)
2022-11-08 18:04:27,095:INFO: Dataset: zara2               Batch: 18/36	Loss 5.2116 (6.0541)
2022-11-08 18:04:27,097:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2794 (6.0090)
2022-11-08 18:04:27,102:INFO: Dataset: zara2               Batch: 20/36	Loss 5.3550 (5.9806)
2022-11-08 18:04:27,106:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4717 (5.9560)
2022-11-08 18:04:27,111:INFO: Dataset: zara2               Batch: 22/36	Loss 5.4346 (5.9345)
2022-11-08 18:04:27,113:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9786 (5.8933)
2022-11-08 18:04:27,117:INFO: Dataset: zara2               Batch: 24/36	Loss 4.8211 (5.8453)
2022-11-08 18:04:27,120:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8786 (5.8078)
2022-11-08 18:04:27,123:INFO: Dataset: zara2               Batch: 26/36	Loss 4.6912 (5.7598)
2022-11-08 18:04:27,127:INFO: Dataset: zara2               Batch: 27/36	Loss 4.8187 (5.7231)
2022-11-08 18:04:27,130:INFO: Dataset: zara2               Batch: 28/36	Loss 4.4783 (5.6782)
2022-11-08 18:04:27,133:INFO: Dataset: zara2               Batch: 29/36	Loss 4.4772 (5.6430)
2022-11-08 18:04:27,136:INFO: Dataset: zara2               Batch: 30/36	Loss 4.2467 (5.5947)
2022-11-08 18:04:27,140:INFO: Dataset: zara2               Batch: 31/36	Loss 4.1444 (5.5528)
2022-11-08 18:04:27,144:INFO: Dataset: zara2               Batch: 32/36	Loss 4.3221 (5.5152)
2022-11-08 18:04:27,149:INFO: Dataset: zara2               Batch: 33/36	Loss 4.9232 (5.4992)
2022-11-08 18:04:27,152:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3009 (5.4582)
2022-11-08 18:04:27,155:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0354 (5.4239)
2022-11-08 18:04:27,159:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1286 (5.3978)
2022-11-08 18:04:28,038:INFO: - Computing loss (validation)
2022-11-08 18:04:35,341:INFO: Dataset: hotel               Batch: 1/3	Loss 16.6085 (16.6085)
2022-11-08 18:04:35,831:INFO: Dataset: hotel               Batch: 2/3	Loss 15.8165 (16.2010)
2022-11-08 18:04:36,220:INFO: Dataset: hotel               Batch: 3/3	Loss 16.8445 (16.2427)
2022-11-08 18:04:44,030:INFO: Dataset: univ                Batch: 1/6	Loss 8.1987 (8.1987)
2022-11-08 18:04:44,372:INFO: Dataset: univ                Batch: 2/6	Loss 8.7837 (8.4605)
2022-11-08 18:04:44,764:INFO: Dataset: univ                Batch: 3/6	Loss 8.7771 (8.5573)
2022-11-08 18:04:45,083:INFO: Dataset: univ                Batch: 4/6	Loss 8.6409 (8.5761)
2022-11-08 18:04:45,340:INFO: Dataset: univ                Batch: 5/6	Loss 7.9908 (8.4373)
2022-11-08 18:04:45,576:INFO: Dataset: univ                Batch: 6/6	Loss 8.7201 (8.4703)
2022-11-08 18:04:52,483:INFO: Dataset: zara1               Batch: 1/3	Loss 12.0159 (12.0159)
2022-11-08 18:04:52,896:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8965 (11.9572)
2022-11-08 18:04:53,242:INFO: Dataset: zara1               Batch: 3/3	Loss 12.4529 (12.0704)
2022-11-08 18:05:00,762:INFO: Dataset: zara2               Batch:  1/10	Loss 4.3274 (4.3274)
2022-11-08 18:05:01,186:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1710 (4.2488)
2022-11-08 18:05:01,573:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1416 (4.2141)
2022-11-08 18:05:01,863:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3665 (4.2490)
2022-11-08 18:05:02,066:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3359 (4.2648)
2022-11-08 18:05:02,333:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0944 (4.2378)
2022-11-08 18:05:02,335:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1027 (4.2189)
2022-11-08 18:05:02,336:INFO: Dataset: zara2               Batch:  8/10	Loss 4.1720 (4.2128)
2022-11-08 18:05:02,338:INFO: Dataset: zara2               Batch:  9/10	Loss 4.3397 (4.2272)
2022-11-08 18:05:02,341:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2825 (4.2328)
2022-11-08 18:05:03,263:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1071.pth.tar
2022-11-08 18:05:03,263:INFO: 
===> EPOCH: 1072 (P5)
2022-11-08 18:05:03,263:INFO: - Computing loss (training)
2022-11-08 18:05:09,235:INFO: Dataset: hotel               Batch: 1/8	Loss 15.8496 (15.8496)
2022-11-08 18:05:09,573:INFO: Dataset: hotel               Batch: 2/8	Loss 15.1585 (15.5091)
2022-11-08 18:05:09,988:INFO: Dataset: hotel               Batch: 3/8	Loss 13.8689 (14.9466)
2022-11-08 18:05:10,265:INFO: Dataset: hotel               Batch: 4/8	Loss 11.4148 (14.0510)
2022-11-08 18:05:10,484:INFO: Dataset: hotel               Batch: 5/8	Loss 11.9502 (13.6389)
2022-11-08 18:05:10,681:INFO: Dataset: hotel               Batch: 6/8	Loss 13.1749 (13.5610)
2022-11-08 18:05:10,686:INFO: Dataset: hotel               Batch: 7/8	Loss 13.4922 (13.5504)
2022-11-08 18:05:10,690:INFO: Dataset: hotel               Batch: 8/8	Loss 14.6064 (13.5783)
2022-11-08 18:05:34,306:INFO: Dataset: univ                Batch:  1/29	Loss 8.7858 (8.7858)
2022-11-08 18:05:34,310:INFO: Dataset: univ                Batch:  2/29	Loss 9.4084 (9.0826)
2022-11-08 18:05:34,316:INFO: Dataset: univ                Batch:  3/29	Loss 8.7318 (8.9608)
2022-11-08 18:05:34,322:INFO: Dataset: univ                Batch:  4/29	Loss 9.2534 (9.0260)
2022-11-08 18:05:34,328:INFO: Dataset: univ                Batch:  5/29	Loss 8.7068 (8.9568)
2022-11-08 18:05:34,335:INFO: Dataset: univ                Batch:  6/29	Loss 8.6119 (8.8998)
2022-11-08 18:05:34,340:INFO: Dataset: univ                Batch:  7/29	Loss 8.5891 (8.8578)
2022-11-08 18:05:34,344:INFO: Dataset: univ                Batch:  8/29	Loss 8.6175 (8.8306)
2022-11-08 18:05:34,349:INFO: Dataset: univ                Batch:  9/29	Loss 9.1859 (8.8650)
2022-11-08 18:05:34,355:INFO: Dataset: univ                Batch: 10/29	Loss 8.5840 (8.8333)
2022-11-08 18:05:34,361:INFO: Dataset: univ                Batch: 11/29	Loss 8.6120 (8.8142)
2022-11-08 18:05:34,368:INFO: Dataset: univ                Batch: 12/29	Loss 8.1975 (8.7639)
2022-11-08 18:05:34,374:INFO: Dataset: univ                Batch: 13/29	Loss 8.1468 (8.7134)
2022-11-08 18:05:34,378:INFO: Dataset: univ                Batch: 14/29	Loss 8.4608 (8.6948)
2022-11-08 18:05:34,383:INFO: Dataset: univ                Batch: 15/29	Loss 8.2612 (8.6654)
2022-11-08 18:05:34,390:INFO: Dataset: univ                Batch: 16/29	Loss 7.7153 (8.6045)
2022-11-08 18:05:34,394:INFO: Dataset: univ                Batch: 17/29	Loss 7.4189 (8.5289)
2022-11-08 18:05:34,401:INFO: Dataset: univ                Batch: 18/29	Loss 7.5345 (8.4697)
2022-11-08 18:05:34,406:INFO: Dataset: univ                Batch: 19/29	Loss 7.6593 (8.4297)
2022-11-08 18:05:34,410:INFO: Dataset: univ                Batch: 20/29	Loss 7.9053 (8.4071)
2022-11-08 18:05:34,415:INFO: Dataset: univ                Batch: 21/29	Loss 7.0923 (8.3442)
2022-11-08 18:05:34,420:INFO: Dataset: univ                Batch: 22/29	Loss 6.9502 (8.2762)
2022-11-08 18:05:34,425:INFO: Dataset: univ                Batch: 23/29	Loss 7.0446 (8.2197)
2022-11-08 18:05:34,430:INFO: Dataset: univ                Batch: 24/29	Loss 7.2031 (8.1751)
2022-11-08 18:05:34,435:INFO: Dataset: univ                Batch: 25/29	Loss 6.6252 (8.1032)
2022-11-08 18:05:34,441:INFO: Dataset: univ                Batch: 26/29	Loss 6.7473 (8.0504)
2022-11-08 18:05:34,447:INFO: Dataset: univ                Batch: 27/29	Loss 6.5678 (7.9906)
2022-11-08 18:05:34,452:INFO: Dataset: univ                Batch: 28/29	Loss 6.5368 (7.9390)
2022-11-08 18:05:34,457:INFO: Dataset: univ                Batch: 29/29	Loss 6.4478 (7.9194)
2022-11-08 18:05:41,774:INFO: Dataset: zara1               Batch:  1/16	Loss 13.2340 (13.2340)
2022-11-08 18:05:42,111:INFO: Dataset: zara1               Batch:  2/16	Loss 13.5609 (13.4032)
2022-11-08 18:05:42,501:INFO: Dataset: zara1               Batch:  3/16	Loss 13.6741 (13.4841)
2022-11-08 18:05:42,806:INFO: Dataset: zara1               Batch:  4/16	Loss 13.3872 (13.4583)
2022-11-08 18:05:43,048:INFO: Dataset: zara1               Batch:  5/16	Loss 12.8233 (13.3338)
2022-11-08 18:05:43,256:INFO: Dataset: zara1               Batch:  6/16	Loss 13.1027 (13.2914)
2022-11-08 18:05:43,259:INFO: Dataset: zara1               Batch:  7/16	Loss 13.2857 (13.2905)
2022-11-08 18:05:43,264:INFO: Dataset: zara1               Batch:  8/16	Loss 13.5922 (13.3317)
2022-11-08 18:05:43,267:INFO: Dataset: zara1               Batch:  9/16	Loss 12.5748 (13.2426)
2022-11-08 18:05:43,275:INFO: Dataset: zara1               Batch: 10/16	Loss 12.9912 (13.2158)
2022-11-08 18:05:43,281:INFO: Dataset: zara1               Batch: 11/16	Loss 12.3290 (13.1212)
2022-11-08 18:05:43,290:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3623 (13.0608)
2022-11-08 18:05:43,294:INFO: Dataset: zara1               Batch: 13/16	Loss 12.4488 (13.0132)
2022-11-08 18:05:43,297:INFO: Dataset: zara1               Batch: 14/16	Loss 11.8794 (12.9320)
2022-11-08 18:05:43,302:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0278 (12.8678)
2022-11-08 18:05:43,305:INFO: Dataset: zara1               Batch: 16/16	Loss 11.3229 (12.8011)
2022-11-08 18:06:07,026:INFO: Dataset: zara2               Batch:  1/36	Loss 6.5821 (6.5821)
2022-11-08 18:06:07,042:INFO: Dataset: zara2               Batch:  2/36	Loss 6.2238 (6.4255)
2022-11-08 18:06:07,068:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4674 (6.4398)
2022-11-08 18:06:07,072:INFO: Dataset: zara2               Batch:  4/36	Loss 6.2418 (6.3890)
2022-11-08 18:06:07,098:INFO: Dataset: zara2               Batch:  5/36	Loss 6.6800 (6.4501)
2022-11-08 18:06:07,113:INFO: Dataset: zara2               Batch:  6/36	Loss 6.5616 (6.4697)
2022-11-08 18:06:07,129:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5051 (6.4752)
2022-11-08 18:06:07,144:INFO: Dataset: zara2               Batch:  8/36	Loss 6.2189 (6.4428)
2022-11-08 18:06:07,160:INFO: Dataset: zara2               Batch:  9/36	Loss 6.0445 (6.4003)
2022-11-08 18:06:07,171:INFO: Dataset: zara2               Batch: 10/36	Loss 5.8256 (6.3463)
2022-11-08 18:06:07,175:INFO: Dataset: zara2               Batch: 11/36	Loss 5.9657 (6.3135)
2022-11-08 18:06:07,179:INFO: Dataset: zara2               Batch: 12/36	Loss 5.9627 (6.2857)
2022-11-08 18:06:07,183:INFO: Dataset: zara2               Batch: 13/36	Loss 5.6538 (6.2419)
2022-11-08 18:06:07,185:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6550 (6.1982)
2022-11-08 18:06:07,189:INFO: Dataset: zara2               Batch: 15/36	Loss 5.9858 (6.1835)
2022-11-08 18:06:07,193:INFO: Dataset: zara2               Batch: 16/36	Loss 5.9702 (6.1724)
2022-11-08 18:06:07,198:INFO: Dataset: zara2               Batch: 17/36	Loss 5.5557 (6.1337)
2022-11-08 18:06:07,201:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6912 (6.1088)
2022-11-08 18:06:07,206:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2676 (6.0621)
2022-11-08 18:06:07,210:INFO: Dataset: zara2               Batch: 20/36	Loss 5.3383 (6.0273)
2022-11-08 18:06:07,215:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4261 (6.0004)
2022-11-08 18:06:07,219:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1065 (5.9598)
2022-11-08 18:06:07,223:INFO: Dataset: zara2               Batch: 23/36	Loss 4.8410 (5.9067)
2022-11-08 18:06:07,226:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9114 (5.8642)
2022-11-08 18:06:07,230:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8297 (5.8219)
2022-11-08 18:06:07,234:INFO: Dataset: zara2               Batch: 26/36	Loss 5.0651 (5.7870)
2022-11-08 18:06:07,238:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7686 (5.7548)
2022-11-08 18:06:07,242:INFO: Dataset: zara2               Batch: 28/36	Loss 4.9602 (5.7250)
2022-11-08 18:06:07,246:INFO: Dataset: zara2               Batch: 29/36	Loss 4.3544 (5.6769)
2022-11-08 18:06:07,251:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5978 (5.6413)
2022-11-08 18:06:07,256:INFO: Dataset: zara2               Batch: 31/36	Loss 4.5776 (5.6058)
2022-11-08 18:06:07,260:INFO: Dataset: zara2               Batch: 32/36	Loss 4.1084 (5.5582)
2022-11-08 18:06:07,264:INFO: Dataset: zara2               Batch: 33/36	Loss 4.5326 (5.5229)
2022-11-08 18:06:07,269:INFO: Dataset: zara2               Batch: 34/36	Loss 4.2456 (5.4857)
2022-11-08 18:06:07,274:INFO: Dataset: zara2               Batch: 35/36	Loss 4.1099 (5.4432)
2022-11-08 18:06:07,279:INFO: Dataset: zara2               Batch: 36/36	Loss 4.2524 (5.4251)
2022-11-08 18:06:08,308:INFO: - Computing loss (validation)
2022-11-08 18:06:15,568:INFO: Dataset: hotel               Batch: 1/3	Loss 16.9175 (16.9175)
2022-11-08 18:06:16,016:INFO: Dataset: hotel               Batch: 2/3	Loss 15.9952 (16.4581)
2022-11-08 18:06:16,543:INFO: Dataset: hotel               Batch: 3/3	Loss 15.8531 (16.4044)
2022-11-08 18:06:26,570:INFO: Dataset: univ                Batch: 1/6	Loss 8.2551 (8.2551)
2022-11-08 18:06:28,704:INFO: Dataset: univ                Batch: 2/6	Loss 8.7669 (8.4898)
2022-11-08 18:06:28,727:INFO: Dataset: univ                Batch: 3/6	Loss 8.6817 (8.5498)
2022-11-08 18:06:28,729:INFO: Dataset: univ                Batch: 4/6	Loss 8.1523 (8.4380)
2022-11-08 18:06:28,731:INFO: Dataset: univ                Batch: 5/6	Loss 8.6482 (8.4798)
2022-11-08 18:06:28,733:INFO: Dataset: univ                Batch: 6/6	Loss 8.3490 (8.4595)
2022-11-08 18:06:35,693:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9803 (11.9803)
2022-11-08 18:06:36,042:INFO: Dataset: zara1               Batch: 2/3	Loss 12.1573 (12.0722)
2022-11-08 18:06:36,414:INFO: Dataset: zara1               Batch: 3/3	Loss 11.4908 (11.9264)
2022-11-08 18:06:43,986:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1896 (4.1896)
2022-11-08 18:06:44,346:INFO: Dataset: zara2               Batch:  2/10	Loss 4.4977 (4.3357)
2022-11-08 18:06:44,740:INFO: Dataset: zara2               Batch:  3/10	Loss 4.4941 (4.3848)
2022-11-08 18:06:45,070:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1600 (4.3232)
2022-11-08 18:06:45,247:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1428 (4.2827)
2022-11-08 18:06:45,436:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0061 (4.2340)
2022-11-08 18:06:45,438:INFO: Dataset: zara2               Batch:  7/10	Loss 4.2311 (4.2335)
2022-11-08 18:06:45,439:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2904 (4.2408)
2022-11-08 18:06:45,440:INFO: Dataset: zara2               Batch:  9/10	Loss 4.0949 (4.2232)
2022-11-08 18:06:45,442:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2015 (4.2211)
2022-11-08 18:06:46,365:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1072.pth.tar
2022-11-08 18:06:46,365:INFO: 
===> EPOCH: 1073 (P5)
2022-11-08 18:06:46,366:INFO: - Computing loss (training)
2022-11-08 18:06:52,378:INFO: Dataset: hotel               Batch: 1/8	Loss 14.0614 (14.0614)
2022-11-08 18:06:52,771:INFO: Dataset: hotel               Batch: 2/8	Loss 14.8614 (14.4670)
2022-11-08 18:06:53,164:INFO: Dataset: hotel               Batch: 3/8	Loss 14.4231 (14.4527)
2022-11-08 18:06:53,497:INFO: Dataset: hotel               Batch: 4/8	Loss 13.0690 (14.1151)
2022-11-08 18:06:53,694:INFO: Dataset: hotel               Batch: 5/8	Loss 13.4347 (13.9816)
2022-11-08 18:06:53,841:INFO: Dataset: hotel               Batch: 6/8	Loss 12.6842 (13.7689)
2022-11-08 18:06:53,845:INFO: Dataset: hotel               Batch: 7/8	Loss 12.6386 (13.5925)
2022-11-08 18:06:53,848:INFO: Dataset: hotel               Batch: 8/8	Loss 12.1533 (13.5526)
2022-11-08 18:07:18,063:INFO: Dataset: univ                Batch:  1/29	Loss 9.3849 (9.3849)
2022-11-08 18:07:18,069:INFO: Dataset: univ                Batch:  2/29	Loss 9.1948 (9.2889)
2022-11-08 18:07:18,077:INFO: Dataset: univ                Batch:  3/29	Loss 8.5217 (8.9861)
2022-11-08 18:07:18,085:INFO: Dataset: univ                Batch:  4/29	Loss 9.0500 (9.0034)
2022-11-08 18:07:18,091:INFO: Dataset: univ                Batch:  5/29	Loss 8.9600 (8.9930)
2022-11-08 18:07:18,099:INFO: Dataset: univ                Batch:  6/29	Loss 8.8004 (8.9601)
2022-11-08 18:07:18,103:INFO: Dataset: univ                Batch:  7/29	Loss 8.9381 (8.9571)
2022-11-08 18:07:18,111:INFO: Dataset: univ                Batch:  8/29	Loss 8.6917 (8.9235)
2022-11-08 18:07:18,116:INFO: Dataset: univ                Batch:  9/29	Loss 9.0306 (8.9349)
2022-11-08 18:07:18,120:INFO: Dataset: univ                Batch: 10/29	Loss 8.4739 (8.8913)
2022-11-08 18:07:18,122:INFO: Dataset: univ                Batch: 11/29	Loss 8.2946 (8.8419)
2022-11-08 18:07:18,127:INFO: Dataset: univ                Batch: 12/29	Loss 8.1198 (8.7813)
2022-11-08 18:07:18,132:INFO: Dataset: univ                Batch: 13/29	Loss 8.5708 (8.7671)
2022-11-08 18:07:18,136:INFO: Dataset: univ                Batch: 14/29	Loss 8.1364 (8.7239)
2022-11-08 18:07:18,140:INFO: Dataset: univ                Batch: 15/29	Loss 7.9989 (8.6749)
2022-11-08 18:07:18,145:INFO: Dataset: univ                Batch: 16/29	Loss 7.9719 (8.6273)
2022-11-08 18:07:18,149:INFO: Dataset: univ                Batch: 17/29	Loss 7.5080 (8.5603)
2022-11-08 18:07:18,152:INFO: Dataset: univ                Batch: 18/29	Loss 7.5950 (8.5128)
2022-11-08 18:07:18,158:INFO: Dataset: univ                Batch: 19/29	Loss 7.7155 (8.4760)
2022-11-08 18:07:18,162:INFO: Dataset: univ                Batch: 20/29	Loss 7.5273 (8.4258)
2022-11-08 18:07:18,167:INFO: Dataset: univ                Batch: 21/29	Loss 7.4079 (8.3811)
2022-11-08 18:07:18,171:INFO: Dataset: univ                Batch: 22/29	Loss 6.9486 (8.3142)
2022-11-08 18:07:18,176:INFO: Dataset: univ                Batch: 23/29	Loss 7.3043 (8.2718)
2022-11-08 18:07:18,180:INFO: Dataset: univ                Batch: 24/29	Loss 7.2829 (8.2335)
2022-11-08 18:07:18,184:INFO: Dataset: univ                Batch: 25/29	Loss 6.6666 (8.1674)
2022-11-08 18:07:18,188:INFO: Dataset: univ                Batch: 26/29	Loss 6.5731 (8.0977)
2022-11-08 18:07:18,193:INFO: Dataset: univ                Batch: 27/29	Loss 6.6938 (8.0480)
2022-11-08 18:07:18,197:INFO: Dataset: univ                Batch: 28/29	Loss 6.8708 (8.0077)
2022-11-08 18:07:18,200:INFO: Dataset: univ                Batch: 29/29	Loss 6.3025 (7.9828)
2022-11-08 18:07:25,156:INFO: Dataset: zara1               Batch:  1/16	Loss 13.0826 (13.0826)
2022-11-08 18:07:25,598:INFO: Dataset: zara1               Batch:  2/16	Loss 12.9502 (13.0189)
2022-11-08 18:07:25,923:INFO: Dataset: zara1               Batch:  3/16	Loss 13.2114 (13.0769)
2022-11-08 18:07:26,312:INFO: Dataset: zara1               Batch:  4/16	Loss 13.0482 (13.0702)
2022-11-08 18:07:26,531:INFO: Dataset: zara1               Batch:  5/16	Loss 12.6708 (12.9926)
2022-11-08 18:07:26,779:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0405 (13.0018)
2022-11-08 18:07:26,784:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0309 (13.0053)
2022-11-08 18:07:26,787:INFO: Dataset: zara1               Batch:  8/16	Loss 12.9566 (12.9999)
2022-11-08 18:07:26,791:INFO: Dataset: zara1               Batch:  9/16	Loss 12.5021 (12.9468)
2022-11-08 18:07:26,794:INFO: Dataset: zara1               Batch: 10/16	Loss 12.5581 (12.9123)
2022-11-08 18:07:26,796:INFO: Dataset: zara1               Batch: 11/16	Loss 12.7107 (12.8936)
2022-11-08 18:07:26,799:INFO: Dataset: zara1               Batch: 12/16	Loss 12.4019 (12.8525)
2022-11-08 18:07:26,802:INFO: Dataset: zara1               Batch: 13/16	Loss 12.3605 (12.8128)
2022-11-08 18:07:26,805:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3983 (12.7795)
2022-11-08 18:07:26,808:INFO: Dataset: zara1               Batch: 15/16	Loss 12.6719 (12.7716)
2022-11-08 18:07:26,810:INFO: Dataset: zara1               Batch: 16/16	Loss 11.6723 (12.7230)
2022-11-08 18:07:50,723:INFO: Dataset: zara2               Batch:  1/36	Loss 6.1194 (6.1194)
2022-11-08 18:07:50,728:INFO: Dataset: zara2               Batch:  2/36	Loss 6.4804 (6.3122)
2022-11-08 18:07:50,734:INFO: Dataset: zara2               Batch:  3/36	Loss 5.9198 (6.1708)
2022-11-08 18:07:50,738:INFO: Dataset: zara2               Batch:  4/36	Loss 6.0530 (6.1418)
2022-11-08 18:07:50,742:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4134 (6.2039)
2022-11-08 18:07:50,746:INFO: Dataset: zara2               Batch:  6/36	Loss 6.3185 (6.2241)
2022-11-08 18:07:50,749:INFO: Dataset: zara2               Batch:  7/36	Loss 6.6202 (6.2818)
2022-11-08 18:07:50,752:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0593 (6.2525)
2022-11-08 18:07:50,757:INFO: Dataset: zara2               Batch:  9/36	Loss 6.3286 (6.2627)
2022-11-08 18:07:50,761:INFO: Dataset: zara2               Batch: 10/36	Loss 6.0404 (6.2386)
2022-11-08 18:07:50,766:INFO: Dataset: zara2               Batch: 11/36	Loss 6.2422 (6.2389)
2022-11-08 18:07:50,770:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8322 (6.2038)
2022-11-08 18:07:50,775:INFO: Dataset: zara2               Batch: 13/36	Loss 5.7173 (6.1652)
2022-11-08 18:07:50,778:INFO: Dataset: zara2               Batch: 14/36	Loss 5.5718 (6.1235)
2022-11-08 18:07:50,780:INFO: Dataset: zara2               Batch: 15/36	Loss 5.9545 (6.1124)
2022-11-08 18:07:50,784:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3341 (6.0625)
2022-11-08 18:07:50,787:INFO: Dataset: zara2               Batch: 17/36	Loss 5.7726 (6.0470)
2022-11-08 18:07:50,792:INFO: Dataset: zara2               Batch: 18/36	Loss 5.8195 (6.0352)
2022-11-08 18:07:50,795:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2319 (5.9901)
2022-11-08 18:07:50,798:INFO: Dataset: zara2               Batch: 20/36	Loss 5.7680 (5.9797)
2022-11-08 18:07:50,801:INFO: Dataset: zara2               Batch: 21/36	Loss 5.2360 (5.9493)
2022-11-08 18:07:50,804:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0429 (5.9138)
2022-11-08 18:07:50,808:INFO: Dataset: zara2               Batch: 23/36	Loss 5.3073 (5.8854)
2022-11-08 18:07:50,812:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7012 (5.8369)
2022-11-08 18:07:50,815:INFO: Dataset: zara2               Batch: 25/36	Loss 4.6956 (5.7945)
2022-11-08 18:07:50,818:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8186 (5.7577)
2022-11-08 18:07:50,821:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9851 (5.7315)
2022-11-08 18:07:50,824:INFO: Dataset: zara2               Batch: 28/36	Loss 4.9094 (5.7048)
2022-11-08 18:07:50,828:INFO: Dataset: zara2               Batch: 29/36	Loss 4.8823 (5.6751)
2022-11-08 18:07:50,832:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3342 (5.6308)
2022-11-08 18:07:50,835:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3971 (5.5885)
2022-11-08 18:07:50,838:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4244 (5.5498)
2022-11-08 18:07:50,841:INFO: Dataset: zara2               Batch: 33/36	Loss 4.3561 (5.5107)
2022-11-08 18:07:50,845:INFO: Dataset: zara2               Batch: 34/36	Loss 4.1375 (5.4716)
2022-11-08 18:07:50,848:INFO: Dataset: zara2               Batch: 35/36	Loss 4.6973 (5.4539)
2022-11-08 18:07:50,852:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9202 (5.4220)
2022-11-08 18:07:51,734:INFO: - Computing loss (validation)
2022-11-08 18:07:57,923:INFO: Dataset: hotel               Batch: 1/3	Loss 16.5148 (16.5148)
2022-11-08 18:07:58,293:INFO: Dataset: hotel               Batch: 2/3	Loss 15.8544 (16.2004)
2022-11-08 18:07:58,679:INFO: Dataset: hotel               Batch: 3/3	Loss 16.2397 (16.2034)
2022-11-08 18:08:06,637:INFO: Dataset: univ                Batch: 1/6	Loss 8.5978 (8.5978)
2022-11-08 18:08:07,020:INFO: Dataset: univ                Batch: 2/6	Loss 8.1970 (8.3886)
2022-11-08 18:08:07,431:INFO: Dataset: univ                Batch: 3/6	Loss 8.5406 (8.4410)
2022-11-08 18:08:07,767:INFO: Dataset: univ                Batch: 4/6	Loss 8.6417 (8.4876)
2022-11-08 18:08:07,993:INFO: Dataset: univ                Batch: 5/6	Loss 7.8649 (8.3428)
2022-11-08 18:08:08,256:INFO: Dataset: univ                Batch: 6/6	Loss 8.6831 (8.3870)
2022-11-08 18:08:15,437:INFO: Dataset: zara1               Batch: 1/3	Loss 11.4189 (11.4189)
2022-11-08 18:08:15,805:INFO: Dataset: zara1               Batch: 2/3	Loss 12.1067 (11.7744)
2022-11-08 18:08:16,208:INFO: Dataset: zara1               Batch: 3/3	Loss 12.5099 (11.9518)
2022-11-08 18:08:25,154:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1527 (4.1527)
2022-11-08 18:08:25,559:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1937 (4.1715)
2022-11-08 18:08:25,960:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1892 (4.1775)
2022-11-08 18:08:26,269:INFO: Dataset: zara2               Batch:  4/10	Loss 4.4881 (4.2533)
2022-11-08 18:08:26,540:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3243 (4.2675)
2022-11-08 18:08:26,828:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0129 (4.2289)
2022-11-08 18:08:26,830:INFO: Dataset: zara2               Batch:  7/10	Loss 4.0871 (4.2082)
2022-11-08 18:08:26,831:INFO: Dataset: zara2               Batch:  8/10	Loss 4.4780 (4.2409)
2022-11-08 18:08:26,839:INFO: Dataset: zara2               Batch:  9/10	Loss 4.3306 (4.2506)
2022-11-08 18:08:26,842:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2962 (4.2550)
2022-11-08 18:08:27,913:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1073.pth.tar
2022-11-08 18:08:27,914:INFO: 
===> EPOCH: 1074 (P5)
2022-11-08 18:08:27,914:INFO: - Computing loss (training)
2022-11-08 18:08:34,861:INFO: Dataset: hotel               Batch: 1/8	Loss 14.2042 (14.2042)
2022-11-08 18:08:35,361:INFO: Dataset: hotel               Batch: 2/8	Loss 13.7777 (13.9861)
2022-11-08 18:08:35,758:INFO: Dataset: hotel               Batch: 3/8	Loss 13.3367 (13.7882)
2022-11-08 18:08:36,078:INFO: Dataset: hotel               Batch: 4/8	Loss 13.2404 (13.6552)
2022-11-08 18:08:36,359:INFO: Dataset: hotel               Batch: 5/8	Loss 12.7732 (13.4815)
2022-11-08 18:08:36,544:INFO: Dataset: hotel               Batch: 6/8	Loss 13.8647 (13.5506)
2022-11-08 18:08:36,548:INFO: Dataset: hotel               Batch: 7/8	Loss 12.6650 (13.4296)
2022-11-08 18:08:36,560:INFO: Dataset: hotel               Batch: 8/8	Loss 13.1689 (13.4207)
2022-11-08 18:09:03,243:INFO: Dataset: univ                Batch:  1/29	Loss 8.9449 (8.9449)
2022-11-08 18:09:03,258:INFO: Dataset: univ                Batch:  2/29	Loss 8.2911 (8.6087)
2022-11-08 18:09:03,265:INFO: Dataset: univ                Batch:  3/29	Loss 9.0277 (8.7487)
2022-11-08 18:09:03,275:INFO: Dataset: univ                Batch:  4/29	Loss 8.7510 (8.7493)
2022-11-08 18:09:03,281:INFO: Dataset: univ                Batch:  5/29	Loss 8.8445 (8.7683)
2022-11-08 18:09:03,290:INFO: Dataset: univ                Batch:  6/29	Loss 8.9897 (8.8080)
2022-11-08 18:09:03,295:INFO: Dataset: univ                Batch:  7/29	Loss 8.7272 (8.7972)
2022-11-08 18:09:03,305:INFO: Dataset: univ                Batch:  8/29	Loss 8.8143 (8.7993)
2022-11-08 18:09:03,310:INFO: Dataset: univ                Batch:  9/29	Loss 8.3244 (8.7458)
2022-11-08 18:09:03,320:INFO: Dataset: univ                Batch: 10/29	Loss 9.0085 (8.7695)
2022-11-08 18:09:03,326:INFO: Dataset: univ                Batch: 11/29	Loss 8.4113 (8.7363)
2022-11-08 18:09:03,331:INFO: Dataset: univ                Batch: 12/29	Loss 8.4649 (8.7170)
2022-11-08 18:09:03,339:INFO: Dataset: univ                Batch: 13/29	Loss 7.8477 (8.6484)
2022-11-08 18:09:03,347:INFO: Dataset: univ                Batch: 14/29	Loss 7.9915 (8.6017)
2022-11-08 18:09:03,356:INFO: Dataset: univ                Batch: 15/29	Loss 7.9958 (8.5649)
2022-11-08 18:09:03,364:INFO: Dataset: univ                Batch: 16/29	Loss 7.6052 (8.5037)
2022-11-08 18:09:03,373:INFO: Dataset: univ                Batch: 17/29	Loss 8.3886 (8.4978)
2022-11-08 18:09:03,379:INFO: Dataset: univ                Batch: 18/29	Loss 7.5834 (8.4480)
2022-11-08 18:09:03,383:INFO: Dataset: univ                Batch: 19/29	Loss 7.3751 (8.3882)
2022-11-08 18:09:03,392:INFO: Dataset: univ                Batch: 20/29	Loss 7.4457 (8.3455)
2022-11-08 18:09:03,396:INFO: Dataset: univ                Batch: 21/29	Loss 7.5776 (8.3147)
2022-11-08 18:09:03,410:INFO: Dataset: univ                Batch: 22/29	Loss 6.9364 (8.2472)
2022-11-08 18:09:03,416:INFO: Dataset: univ                Batch: 23/29	Loss 7.1565 (8.2079)
2022-11-08 18:09:03,427:INFO: Dataset: univ                Batch: 24/29	Loss 7.0211 (8.1565)
2022-11-08 18:09:03,438:INFO: Dataset: univ                Batch: 25/29	Loss 6.8647 (8.1033)
2022-11-08 18:09:03,447:INFO: Dataset: univ                Batch: 26/29	Loss 6.6698 (8.0476)
2022-11-08 18:09:03,459:INFO: Dataset: univ                Batch: 27/29	Loss 6.6627 (7.9911)
2022-11-08 18:09:03,464:INFO: Dataset: univ                Batch: 28/29	Loss 6.6025 (7.9428)
2022-11-08 18:09:03,476:INFO: Dataset: univ                Batch: 29/29	Loss 5.9770 (7.9155)
2022-11-08 18:09:11,609:INFO: Dataset: zara1               Batch:  1/16	Loss 13.2028 (13.2028)
2022-11-08 18:09:11,966:INFO: Dataset: zara1               Batch:  2/16	Loss 13.7145 (13.4460)
2022-11-08 18:09:12,547:INFO: Dataset: zara1               Batch:  3/16	Loss 13.3393 (13.4141)
2022-11-08 18:09:12,881:INFO: Dataset: zara1               Batch:  4/16	Loss 13.0334 (13.3153)
2022-11-08 18:09:13,088:INFO: Dataset: zara1               Batch:  5/16	Loss 13.0718 (13.2612)
2022-11-08 18:09:13,347:INFO: Dataset: zara1               Batch:  6/16	Loss 12.8891 (13.2022)
2022-11-08 18:09:13,356:INFO: Dataset: zara1               Batch:  7/16	Loss 12.4487 (13.1069)
2022-11-08 18:09:13,360:INFO: Dataset: zara1               Batch:  8/16	Loss 13.0857 (13.1043)
2022-11-08 18:09:13,365:INFO: Dataset: zara1               Batch:  9/16	Loss 12.7068 (13.0616)
2022-11-08 18:09:13,373:INFO: Dataset: zara1               Batch: 10/16	Loss 12.8863 (13.0398)
2022-11-08 18:09:13,376:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5083 (12.9799)
2022-11-08 18:09:13,383:INFO: Dataset: zara1               Batch: 12/16	Loss 12.5995 (12.9485)
2022-11-08 18:09:13,391:INFO: Dataset: zara1               Batch: 13/16	Loss 12.0885 (12.8883)
2022-11-08 18:09:13,395:INFO: Dataset: zara1               Batch: 14/16	Loss 11.9877 (12.8274)
2022-11-08 18:09:13,400:INFO: Dataset: zara1               Batch: 15/16	Loss 12.3864 (12.7995)
2022-11-08 18:09:13,409:INFO: Dataset: zara1               Batch: 16/16	Loss 11.8584 (12.7499)
2022-11-08 18:09:39,347:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3778 (6.3778)
2022-11-08 18:09:39,363:INFO: Dataset: zara2               Batch:  2/36	Loss 6.2896 (6.3302)
2022-11-08 18:09:39,378:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1387 (6.2770)
2022-11-08 18:09:39,398:INFO: Dataset: zara2               Batch:  4/36	Loss 6.4619 (6.3259)
2022-11-08 18:09:39,413:INFO: Dataset: zara2               Batch:  5/36	Loss 6.7782 (6.4136)
2022-11-08 18:09:39,425:INFO: Dataset: zara2               Batch:  6/36	Loss 6.7271 (6.4690)
2022-11-08 18:09:39,434:INFO: Dataset: zara2               Batch:  7/36	Loss 6.1639 (6.4274)
2022-11-08 18:09:39,448:INFO: Dataset: zara2               Batch:  8/36	Loss 6.0057 (6.3745)
2022-11-08 18:09:39,462:INFO: Dataset: zara2               Batch:  9/36	Loss 6.0385 (6.3409)
2022-11-08 18:09:39,475:INFO: Dataset: zara2               Batch: 10/36	Loss 5.9678 (6.3040)
2022-11-08 18:09:39,483:INFO: Dataset: zara2               Batch: 11/36	Loss 6.0338 (6.2761)
2022-11-08 18:09:39,498:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7112 (6.2298)
2022-11-08 18:09:39,507:INFO: Dataset: zara2               Batch: 13/36	Loss 6.2882 (6.2337)
2022-11-08 18:09:39,512:INFO: Dataset: zara2               Batch: 14/36	Loss 5.9807 (6.2158)
2022-11-08 18:09:39,519:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5804 (6.1697)
2022-11-08 18:09:39,526:INFO: Dataset: zara2               Batch: 16/36	Loss 5.6112 (6.1402)
2022-11-08 18:09:39,534:INFO: Dataset: zara2               Batch: 17/36	Loss 5.3036 (6.0962)
2022-11-08 18:09:39,543:INFO: Dataset: zara2               Batch: 18/36	Loss 5.2687 (6.0531)
2022-11-08 18:09:39,547:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4651 (6.0219)
2022-11-08 18:09:39,554:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0896 (5.9789)
2022-11-08 18:09:39,558:INFO: Dataset: zara2               Batch: 21/36	Loss 5.2743 (5.9370)
2022-11-08 18:09:39,564:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0004 (5.8908)
2022-11-08 18:09:39,573:INFO: Dataset: zara2               Batch: 23/36	Loss 5.2217 (5.8620)
2022-11-08 18:09:39,577:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7199 (5.8124)
2022-11-08 18:09:39,580:INFO: Dataset: zara2               Batch: 25/36	Loss 5.0653 (5.7863)
2022-11-08 18:09:39,586:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8906 (5.7516)
2022-11-08 18:09:39,592:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7616 (5.7164)
2022-11-08 18:09:39,597:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7205 (5.6752)
2022-11-08 18:09:39,609:INFO: Dataset: zara2               Batch: 29/36	Loss 4.7489 (5.6454)
2022-11-08 18:09:39,613:INFO: Dataset: zara2               Batch: 30/36	Loss 4.6713 (5.6137)
2022-11-08 18:09:39,622:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3205 (5.5720)
2022-11-08 18:09:39,627:INFO: Dataset: zara2               Batch: 32/36	Loss 4.6706 (5.5421)
2022-11-08 18:09:39,631:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2346 (5.5056)
2022-11-08 18:09:39,639:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3966 (5.4788)
2022-11-08 18:09:39,643:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2452 (5.4368)
2022-11-08 18:09:39,652:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9524 (5.4142)
2022-11-08 18:09:40,710:INFO: - Computing loss (validation)
2022-11-08 18:09:47,920:INFO: Dataset: hotel               Batch: 1/3	Loss 16.4757 (16.4757)
2022-11-08 18:09:48,509:INFO: Dataset: hotel               Batch: 2/3	Loss 16.6370 (16.5534)
2022-11-08 18:09:48,884:INFO: Dataset: hotel               Batch: 3/3	Loss 17.1340 (16.5989)
2022-11-08 18:09:57,774:INFO: Dataset: univ                Batch: 1/6	Loss 7.9310 (7.9310)
2022-11-08 18:09:58,274:INFO: Dataset: univ                Batch: 2/6	Loss 8.6101 (8.2459)
2022-11-08 18:09:58,679:INFO: Dataset: univ                Batch: 3/6	Loss 8.6135 (8.3502)
2022-11-08 18:09:59,028:INFO: Dataset: univ                Batch: 4/6	Loss 8.5249 (8.3877)
2022-11-08 18:09:59,271:INFO: Dataset: univ                Batch: 5/6	Loss 8.2956 (8.3700)
2022-11-08 18:09:59,444:INFO: Dataset: univ                Batch: 6/6	Loss 8.1916 (8.3412)
2022-11-08 18:10:06,786:INFO: Dataset: zara1               Batch: 1/3	Loss 11.5474 (11.5474)
2022-11-08 18:10:07,229:INFO: Dataset: zara1               Batch: 2/3	Loss 11.7414 (11.6468)
2022-11-08 18:10:07,569:INFO: Dataset: zara1               Batch: 3/3	Loss 12.5654 (11.8625)
2022-11-08 18:10:15,732:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1265 (4.1265)
2022-11-08 18:10:16,186:INFO: Dataset: zara2               Batch:  2/10	Loss 4.3106 (4.2195)
2022-11-08 18:10:16,542:INFO: Dataset: zara2               Batch:  3/10	Loss 4.3064 (4.2451)
2022-11-08 18:10:16,962:INFO: Dataset: zara2               Batch:  4/10	Loss 3.9856 (4.1818)
2022-11-08 18:10:17,209:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1010 (4.1653)
2022-11-08 18:10:17,451:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0354 (4.1416)
2022-11-08 18:10:17,455:INFO: Dataset: zara2               Batch:  7/10	Loss 4.3682 (4.1764)
2022-11-08 18:10:17,458:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3117 (4.1934)
2022-11-08 18:10:17,459:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2634 (4.2015)
2022-11-08 18:10:17,460:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2118 (4.2025)
2022-11-08 18:10:18,536:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1074.pth.tar
2022-11-08 18:10:18,537:INFO: 
===> EPOCH: 1075 (P5)
2022-11-08 18:10:18,538:INFO: - Computing loss (training)
2022-11-08 18:10:25,878:INFO: Dataset: hotel               Batch: 1/8	Loss 15.6224 (15.6224)
2022-11-08 18:10:26,219:INFO: Dataset: hotel               Batch: 2/8	Loss 11.8369 (13.7553)
2022-11-08 18:10:26,597:INFO: Dataset: hotel               Batch: 3/8	Loss 13.2098 (13.5796)
2022-11-08 18:10:26,892:INFO: Dataset: hotel               Batch: 4/8	Loss 13.0102 (13.4480)
2022-11-08 18:10:27,177:INFO: Dataset: hotel               Batch: 5/8	Loss 12.8001 (13.3314)
2022-11-08 18:10:27,332:INFO: Dataset: hotel               Batch: 6/8	Loss 14.0666 (13.4565)
2022-11-08 18:10:27,344:INFO: Dataset: hotel               Batch: 7/8	Loss 13.9952 (13.5373)
2022-11-08 18:10:27,349:INFO: Dataset: hotel               Batch: 8/8	Loss 16.6361 (13.6395)
2022-11-08 18:10:53,193:INFO: Dataset: univ                Batch:  1/29	Loss 8.8284 (8.8284)
2022-11-08 18:10:53,199:INFO: Dataset: univ                Batch:  2/29	Loss 8.4828 (8.6525)
2022-11-08 18:10:53,214:INFO: Dataset: univ                Batch:  3/29	Loss 8.7960 (8.6995)
2022-11-08 18:10:53,244:INFO: Dataset: univ                Batch:  4/29	Loss 9.0880 (8.7962)
2022-11-08 18:10:53,255:INFO: Dataset: univ                Batch:  5/29	Loss 9.0792 (8.8487)
2022-11-08 18:10:53,267:INFO: Dataset: univ                Batch:  6/29	Loss 9.3319 (8.9252)
2022-11-08 18:10:53,283:INFO: Dataset: univ                Batch:  7/29	Loss 8.6309 (8.8810)
2022-11-08 18:10:53,298:INFO: Dataset: univ                Batch:  8/29	Loss 9.1522 (8.9104)
2022-11-08 18:10:53,312:INFO: Dataset: univ                Batch:  9/29	Loss 8.6806 (8.8841)
2022-11-08 18:10:53,325:INFO: Dataset: univ                Batch: 10/29	Loss 8.6263 (8.8609)
2022-11-08 18:10:53,332:INFO: Dataset: univ                Batch: 11/29	Loss 8.5563 (8.8365)
2022-11-08 18:10:53,346:INFO: Dataset: univ                Batch: 12/29	Loss 7.9203 (8.7626)
2022-11-08 18:10:53,361:INFO: Dataset: univ                Batch: 13/29	Loss 8.3845 (8.7336)
2022-11-08 18:10:53,368:INFO: Dataset: univ                Batch: 14/29	Loss 7.7749 (8.6583)
2022-11-08 18:10:53,379:INFO: Dataset: univ                Batch: 15/29	Loss 7.4201 (8.5719)
2022-11-08 18:10:53,392:INFO: Dataset: univ                Batch: 16/29	Loss 7.7731 (8.5182)
2022-11-08 18:10:53,401:INFO: Dataset: univ                Batch: 17/29	Loss 7.8158 (8.4788)
2022-11-08 18:10:53,415:INFO: Dataset: univ                Batch: 18/29	Loss 8.1454 (8.4600)
2022-11-08 18:10:53,429:INFO: Dataset: univ                Batch: 19/29	Loss 7.7680 (8.4259)
2022-11-08 18:10:53,444:INFO: Dataset: univ                Batch: 20/29	Loss 7.4884 (8.3768)
2022-11-08 18:10:53,450:INFO: Dataset: univ                Batch: 21/29	Loss 7.5274 (8.3363)
2022-11-08 18:10:53,471:INFO: Dataset: univ                Batch: 22/29	Loss 7.1708 (8.2772)
2022-11-08 18:10:53,485:INFO: Dataset: univ                Batch: 23/29	Loss 7.1000 (8.2318)
2022-11-08 18:10:53,490:INFO: Dataset: univ                Batch: 24/29	Loss 7.0662 (8.1857)
2022-11-08 18:10:53,492:INFO: Dataset: univ                Batch: 25/29	Loss 6.9745 (8.1343)
2022-11-08 18:10:53,496:INFO: Dataset: univ                Batch: 26/29	Loss 6.8176 (8.0828)
2022-11-08 18:10:53,502:INFO: Dataset: univ                Batch: 27/29	Loss 6.5979 (8.0228)
2022-11-08 18:10:53,506:INFO: Dataset: univ                Batch: 28/29	Loss 6.2010 (7.9550)
2022-11-08 18:10:53,509:INFO: Dataset: univ                Batch: 29/29	Loss 6.3020 (7.9262)
2022-11-08 18:11:01,579:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8864 (12.8864)
2022-11-08 18:11:01,990:INFO: Dataset: zara1               Batch:  2/16	Loss 12.9488 (12.9150)
2022-11-08 18:11:02,341:INFO: Dataset: zara1               Batch:  3/16	Loss 13.0685 (12.9695)
2022-11-08 18:11:02,647:INFO: Dataset: zara1               Batch:  4/16	Loss 12.7625 (12.9268)
2022-11-08 18:11:02,991:INFO: Dataset: zara1               Batch:  5/16	Loss 13.2175 (12.9880)
2022-11-08 18:11:03,243:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0891 (13.0021)
2022-11-08 18:11:03,261:INFO: Dataset: zara1               Batch:  7/16	Loss 12.5658 (12.9348)
2022-11-08 18:11:03,267:INFO: Dataset: zara1               Batch:  8/16	Loss 12.7143 (12.9045)
2022-11-08 18:11:03,279:INFO: Dataset: zara1               Batch:  9/16	Loss 12.9951 (12.9171)
2022-11-08 18:11:03,284:INFO: Dataset: zara1               Batch: 10/16	Loss 13.1513 (12.9380)
2022-11-08 18:11:03,299:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5230 (12.8985)
2022-11-08 18:11:03,312:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3505 (12.8450)
2022-11-08 18:11:03,317:INFO: Dataset: zara1               Batch: 13/16	Loss 11.9122 (12.7777)
2022-11-08 18:11:03,329:INFO: Dataset: zara1               Batch: 14/16	Loss 12.2620 (12.7422)
2022-11-08 18:11:03,342:INFO: Dataset: zara1               Batch: 15/16	Loss 12.4563 (12.7224)
2022-11-08 18:11:03,350:INFO: Dataset: zara1               Batch: 16/16	Loss 12.2790 (12.7044)
2022-11-08 18:11:29,679:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3151 (6.3151)
2022-11-08 18:11:29,685:INFO: Dataset: zara2               Batch:  2/36	Loss 6.3069 (6.3108)
2022-11-08 18:11:29,697:INFO: Dataset: zara2               Batch:  3/36	Loss 6.6427 (6.4287)
2022-11-08 18:11:29,707:INFO: Dataset: zara2               Batch:  4/36	Loss 6.0481 (6.3330)
2022-11-08 18:11:29,716:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4337 (6.3510)
2022-11-08 18:11:29,730:INFO: Dataset: zara2               Batch:  6/36	Loss 6.3214 (6.3463)
2022-11-08 18:11:29,734:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5580 (6.3765)
2022-11-08 18:11:29,744:INFO: Dataset: zara2               Batch:  8/36	Loss 6.1691 (6.3518)
2022-11-08 18:11:29,747:INFO: Dataset: zara2               Batch:  9/36	Loss 5.8443 (6.3000)
2022-11-08 18:11:29,761:INFO: Dataset: zara2               Batch: 10/36	Loss 6.3285 (6.3031)
2022-11-08 18:11:29,767:INFO: Dataset: zara2               Batch: 11/36	Loss 6.2388 (6.2972)
2022-11-08 18:11:29,779:INFO: Dataset: zara2               Batch: 12/36	Loss 6.1279 (6.2812)
2022-11-08 18:11:29,790:INFO: Dataset: zara2               Batch: 13/36	Loss 5.6120 (6.2332)
2022-11-08 18:11:29,797:INFO: Dataset: zara2               Batch: 14/36	Loss 5.8815 (6.2068)
2022-11-08 18:11:29,809:INFO: Dataset: zara2               Batch: 15/36	Loss 5.7114 (6.1725)
2022-11-08 18:11:29,814:INFO: Dataset: zara2               Batch: 16/36	Loss 5.4056 (6.1341)
2022-11-08 18:11:29,825:INFO: Dataset: zara2               Batch: 17/36	Loss 5.3197 (6.0844)
2022-11-08 18:11:29,830:INFO: Dataset: zara2               Batch: 18/36	Loss 5.1791 (6.0273)
2022-11-08 18:11:29,839:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4234 (5.9925)
2022-11-08 18:11:29,844:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4471 (5.9695)
2022-11-08 18:11:29,852:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4818 (5.9445)
2022-11-08 18:11:29,863:INFO: Dataset: zara2               Batch: 22/36	Loss 5.4115 (5.9221)
2022-11-08 18:11:29,866:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0585 (5.8771)
2022-11-08 18:11:29,876:INFO: Dataset: zara2               Batch: 24/36	Loss 5.1003 (5.8406)
2022-11-08 18:11:29,881:INFO: Dataset: zara2               Batch: 25/36	Loss 5.2289 (5.8160)
2022-11-08 18:11:29,892:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8657 (5.7754)
2022-11-08 18:11:29,898:INFO: Dataset: zara2               Batch: 27/36	Loss 4.8248 (5.7380)
2022-11-08 18:11:29,908:INFO: Dataset: zara2               Batch: 28/36	Loss 4.2179 (5.6902)
2022-11-08 18:11:29,915:INFO: Dataset: zara2               Batch: 29/36	Loss 4.7406 (5.6612)
2022-11-08 18:11:29,924:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5386 (5.6272)
2022-11-08 18:11:29,928:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3395 (5.5862)
2022-11-08 18:11:29,934:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4218 (5.5523)
2022-11-08 18:11:29,942:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2901 (5.5154)
2022-11-08 18:11:29,947:INFO: Dataset: zara2               Batch: 34/36	Loss 4.8223 (5.4956)
2022-11-08 18:11:29,968:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0750 (5.4575)
2022-11-08 18:11:29,976:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1102 (5.4292)
2022-11-08 18:11:30,968:INFO: - Computing loss (validation)
2022-11-08 18:11:38,190:INFO: Dataset: hotel               Batch: 1/3	Loss 15.9781 (15.9781)
2022-11-08 18:11:38,558:INFO: Dataset: hotel               Batch: 2/3	Loss 16.0744 (16.0250)
2022-11-08 18:11:38,951:INFO: Dataset: hotel               Batch: 3/3	Loss 15.4749 (15.9874)
2022-11-08 18:11:47,816:INFO: Dataset: univ                Batch: 1/6	Loss 8.3373 (8.3373)
2022-11-08 18:11:48,208:INFO: Dataset: univ                Batch: 2/6	Loss 7.7971 (8.0359)
2022-11-08 18:11:48,626:INFO: Dataset: univ                Batch: 3/6	Loss 8.6486 (8.2000)
2022-11-08 18:11:48,900:INFO: Dataset: univ                Batch: 4/6	Loss 8.8871 (8.3552)
2022-11-08 18:11:49,205:INFO: Dataset: univ                Batch: 5/6	Loss 8.4373 (8.3706)
2022-11-08 18:11:49,399:INFO: Dataset: univ                Batch: 6/6	Loss 7.8304 (8.2774)
2022-11-08 18:11:57,399:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9204 (11.9204)
2022-11-08 18:11:57,893:INFO: Dataset: zara1               Batch: 2/3	Loss 11.9298 (11.9250)
2022-11-08 18:11:58,243:INFO: Dataset: zara1               Batch: 3/3	Loss 11.7845 (11.8893)
2022-11-08 18:12:07,119:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2477 (4.2477)
2022-11-08 18:12:07,632:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2295 (4.2385)
2022-11-08 18:12:07,883:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1073 (4.1919)
2022-11-08 18:12:08,281:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3145 (4.2247)
2022-11-08 18:12:08,526:INFO: Dataset: zara2               Batch:  5/10	Loss 4.5078 (4.2786)
2022-11-08 18:12:08,765:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2053 (4.2667)
2022-11-08 18:12:08,768:INFO: Dataset: zara2               Batch:  7/10	Loss 4.4439 (4.2897)
2022-11-08 18:12:08,775:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3125 (4.2927)
2022-11-08 18:12:08,779:INFO: Dataset: zara2               Batch:  9/10	Loss 4.4531 (4.3094)
2022-11-08 18:12:08,780:INFO: Dataset: zara2               Batch: 10/10	Loss 4.3139 (4.3099)
2022-11-08 18:12:09,826:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1075.pth.tar
2022-11-08 18:12:09,826:INFO: 
===> EPOCH: 1076 (P5)
2022-11-08 18:12:09,827:INFO: - Computing loss (training)
2022-11-08 18:12:16,915:INFO: Dataset: hotel               Batch: 1/8	Loss 12.2048 (12.2048)
2022-11-08 18:12:17,517:INFO: Dataset: hotel               Batch: 2/8	Loss 13.5386 (12.9437)
2022-11-08 18:12:17,865:INFO: Dataset: hotel               Batch: 3/8	Loss 12.2935 (12.7312)
2022-11-08 18:12:18,198:INFO: Dataset: hotel               Batch: 4/8	Loss 14.3351 (13.1584)
2022-11-08 18:12:18,411:INFO: Dataset: hotel               Batch: 5/8	Loss 13.9637 (13.3302)
2022-11-08 18:12:18,684:INFO: Dataset: hotel               Batch: 6/8	Loss 14.1831 (13.4644)
2022-11-08 18:12:18,694:INFO: Dataset: hotel               Batch: 7/8	Loss 12.0535 (13.2543)
2022-11-08 18:12:18,697:INFO: Dataset: hotel               Batch: 8/8	Loss 13.3172 (13.2564)
2022-11-08 18:12:44,599:INFO: Dataset: univ                Batch:  1/29	Loss 9.0933 (9.0933)
2022-11-08 18:12:44,615:INFO: Dataset: univ                Batch:  2/29	Loss 8.9275 (9.0085)
2022-11-08 18:12:44,636:INFO: Dataset: univ                Batch:  3/29	Loss 8.5027 (8.8170)
2022-11-08 18:12:44,651:INFO: Dataset: univ                Batch:  4/29	Loss 8.8837 (8.8331)
2022-11-08 18:12:44,667:INFO: Dataset: univ                Batch:  5/29	Loss 9.1156 (8.8897)
2022-11-08 18:12:44,680:INFO: Dataset: univ                Batch:  6/29	Loss 8.5767 (8.8366)
2022-11-08 18:12:44,690:INFO: Dataset: univ                Batch:  7/29	Loss 8.5524 (8.7957)
2022-11-08 18:12:44,702:INFO: Dataset: univ                Batch:  8/29	Loss 8.5617 (8.7670)
2022-11-08 18:12:44,716:INFO: Dataset: univ                Batch:  9/29	Loss 8.5827 (8.7457)
2022-11-08 18:12:44,732:INFO: Dataset: univ                Batch: 10/29	Loss 8.1321 (8.6760)
2022-11-08 18:12:44,748:INFO: Dataset: univ                Batch: 11/29	Loss 8.2162 (8.6355)
2022-11-08 18:12:44,765:INFO: Dataset: univ                Batch: 12/29	Loss 8.1350 (8.5970)
2022-11-08 18:12:44,781:INFO: Dataset: univ                Batch: 13/29	Loss 8.1370 (8.5569)
2022-11-08 18:12:44,799:INFO: Dataset: univ                Batch: 14/29	Loss 7.9292 (8.5120)
2022-11-08 18:12:44,814:INFO: Dataset: univ                Batch: 15/29	Loss 8.1703 (8.4906)
2022-11-08 18:12:44,828:INFO: Dataset: univ                Batch: 16/29	Loss 7.6794 (8.4413)
2022-11-08 18:12:44,835:INFO: Dataset: univ                Batch: 17/29	Loss 7.6687 (8.3903)
2022-11-08 18:12:44,849:INFO: Dataset: univ                Batch: 18/29	Loss 7.6204 (8.3504)
2022-11-08 18:12:44,854:INFO: Dataset: univ                Batch: 19/29	Loss 7.6570 (8.3202)
2022-11-08 18:12:44,860:INFO: Dataset: univ                Batch: 20/29	Loss 7.1726 (8.2696)
2022-11-08 18:12:44,864:INFO: Dataset: univ                Batch: 21/29	Loss 7.6869 (8.2487)
2022-11-08 18:12:44,867:INFO: Dataset: univ                Batch: 22/29	Loss 6.7550 (8.1732)
2022-11-08 18:12:44,871:INFO: Dataset: univ                Batch: 23/29	Loss 6.9293 (8.1199)
2022-11-08 18:12:44,875:INFO: Dataset: univ                Batch: 24/29	Loss 7.0307 (8.0796)
2022-11-08 18:12:44,880:INFO: Dataset: univ                Batch: 25/29	Loss 6.7981 (8.0305)
2022-11-08 18:12:44,885:INFO: Dataset: univ                Batch: 26/29	Loss 6.1822 (7.9608)
2022-11-08 18:12:44,892:INFO: Dataset: univ                Batch: 27/29	Loss 6.6143 (7.9098)
2022-11-08 18:12:44,896:INFO: Dataset: univ                Batch: 28/29	Loss 6.5215 (7.8605)
2022-11-08 18:12:44,899:INFO: Dataset: univ                Batch: 29/29	Loss 6.4349 (7.8387)
2022-11-08 18:12:52,961:INFO: Dataset: zara1               Batch:  1/16	Loss 12.6787 (12.6787)
2022-11-08 18:12:53,366:INFO: Dataset: zara1               Batch:  2/16	Loss 13.1371 (12.9274)
2022-11-08 18:12:53,780:INFO: Dataset: zara1               Batch:  3/16	Loss 13.2945 (13.0546)
2022-11-08 18:12:54,084:INFO: Dataset: zara1               Batch:  4/16	Loss 13.2362 (13.0970)
2022-11-08 18:12:54,367:INFO: Dataset: zara1               Batch:  5/16	Loss 12.8236 (13.0445)
2022-11-08 18:12:54,596:INFO: Dataset: zara1               Batch:  6/16	Loss 12.9331 (13.0268)
2022-11-08 18:12:54,603:INFO: Dataset: zara1               Batch:  7/16	Loss 12.8803 (13.0054)
2022-11-08 18:12:54,630:INFO: Dataset: zara1               Batch:  8/16	Loss 12.5812 (12.9518)
2022-11-08 18:12:54,644:INFO: Dataset: zara1               Batch:  9/16	Loss 13.0352 (12.9626)
2022-11-08 18:12:54,649:INFO: Dataset: zara1               Batch: 10/16	Loss 12.8275 (12.9504)
2022-11-08 18:12:54,659:INFO: Dataset: zara1               Batch: 11/16	Loss 12.9117 (12.9472)
2022-11-08 18:12:54,667:INFO: Dataset: zara1               Batch: 12/16	Loss 12.8325 (12.9365)
2022-11-08 18:12:54,677:INFO: Dataset: zara1               Batch: 13/16	Loss 12.1029 (12.8762)
2022-11-08 18:12:54,684:INFO: Dataset: zara1               Batch: 14/16	Loss 12.2033 (12.8349)
2022-11-08 18:12:54,697:INFO: Dataset: zara1               Batch: 15/16	Loss 12.3268 (12.8000)
2022-11-08 18:12:54,711:INFO: Dataset: zara1               Batch: 16/16	Loss 12.1081 (12.7705)
2022-11-08 18:13:20,640:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3390 (6.3390)
2022-11-08 18:13:20,645:INFO: Dataset: zara2               Batch:  2/36	Loss 6.3459 (6.3426)
2022-11-08 18:13:20,648:INFO: Dataset: zara2               Batch:  3/36	Loss 6.6910 (6.4692)
2022-11-08 18:13:20,653:INFO: Dataset: zara2               Batch:  4/36	Loss 6.2342 (6.4100)
2022-11-08 18:13:20,676:INFO: Dataset: zara2               Batch:  5/36	Loss 6.2766 (6.3833)
2022-11-08 18:13:20,681:INFO: Dataset: zara2               Batch:  6/36	Loss 5.8627 (6.3007)
2022-11-08 18:13:20,695:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4570 (6.3238)
2022-11-08 18:13:20,702:INFO: Dataset: zara2               Batch:  8/36	Loss 6.2569 (6.3146)
2022-11-08 18:13:20,714:INFO: Dataset: zara2               Batch:  9/36	Loss 6.0249 (6.2841)
2022-11-08 18:13:20,719:INFO: Dataset: zara2               Batch: 10/36	Loss 5.9637 (6.2473)
2022-11-08 18:13:20,729:INFO: Dataset: zara2               Batch: 11/36	Loss 5.9903 (6.2237)
2022-11-08 18:13:20,733:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8466 (6.1929)
2022-11-08 18:13:20,743:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0323 (6.1801)
2022-11-08 18:13:20,748:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6926 (6.1463)
2022-11-08 18:13:20,759:INFO: Dataset: zara2               Batch: 15/36	Loss 6.0790 (6.1411)
2022-11-08 18:13:20,765:INFO: Dataset: zara2               Batch: 16/36	Loss 5.7580 (6.1151)
2022-11-08 18:13:20,772:INFO: Dataset: zara2               Batch: 17/36	Loss 5.3512 (6.0645)
2022-11-08 18:13:20,777:INFO: Dataset: zara2               Batch: 18/36	Loss 5.7918 (6.0522)
2022-11-08 18:13:20,784:INFO: Dataset: zara2               Batch: 19/36	Loss 5.5262 (6.0193)
2022-11-08 18:13:20,796:INFO: Dataset: zara2               Batch: 20/36	Loss 5.6888 (6.0027)
2022-11-08 18:13:20,801:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3599 (5.9651)
2022-11-08 18:13:20,812:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1969 (5.9335)
2022-11-08 18:13:20,816:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0998 (5.8935)
2022-11-08 18:13:20,828:INFO: Dataset: zara2               Batch: 24/36	Loss 5.1456 (5.8607)
2022-11-08 18:13:20,832:INFO: Dataset: zara2               Batch: 25/36	Loss 5.1494 (5.8330)
2022-11-08 18:13:20,839:INFO: Dataset: zara2               Batch: 26/36	Loss 5.0363 (5.8018)
2022-11-08 18:13:20,845:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7313 (5.7651)
2022-11-08 18:13:20,851:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7908 (5.7276)
2022-11-08 18:13:20,858:INFO: Dataset: zara2               Batch: 29/36	Loss 4.2802 (5.6831)
2022-11-08 18:13:20,863:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3949 (5.6372)
2022-11-08 18:13:20,867:INFO: Dataset: zara2               Batch: 31/36	Loss 4.5562 (5.6009)
2022-11-08 18:13:20,874:INFO: Dataset: zara2               Batch: 32/36	Loss 4.7645 (5.5783)
2022-11-08 18:13:20,877:INFO: Dataset: zara2               Batch: 33/36	Loss 4.4731 (5.5458)
2022-11-08 18:13:20,881:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0847 (5.5019)
2022-11-08 18:13:20,886:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0506 (5.4630)
2022-11-08 18:13:20,893:INFO: Dataset: zara2               Batch: 36/36	Loss 4.2512 (5.4346)
2022-11-08 18:13:21,918:INFO: - Computing loss (validation)
2022-11-08 18:13:29,049:INFO: Dataset: hotel               Batch: 1/3	Loss 15.9129 (15.9129)
2022-11-08 18:13:29,537:INFO: Dataset: hotel               Batch: 2/3	Loss 16.4110 (16.1783)
2022-11-08 18:13:29,901:INFO: Dataset: hotel               Batch: 3/3	Loss 17.2188 (16.2458)
2022-11-08 18:13:38,685:INFO: Dataset: univ                Batch: 1/6	Loss 8.2822 (8.2822)
2022-11-08 18:13:39,150:INFO: Dataset: univ                Batch: 2/6	Loss 9.5452 (8.8305)
2022-11-08 18:13:39,585:INFO: Dataset: univ                Batch: 3/6	Loss 7.8550 (8.4411)
2022-11-08 18:13:39,875:INFO: Dataset: univ                Batch: 4/6	Loss 7.6083 (8.2013)
2022-11-08 18:13:40,161:INFO: Dataset: univ                Batch: 5/6	Loss 8.7402 (8.2977)
2022-11-08 18:13:40,362:INFO: Dataset: univ                Batch: 6/6	Loss 8.3484 (8.3059)
2022-11-08 18:13:48,398:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9401 (11.9401)
2022-11-08 18:13:48,799:INFO: Dataset: zara1               Batch: 2/3	Loss 11.5910 (11.7663)
2022-11-08 18:13:49,311:INFO: Dataset: zara1               Batch: 3/3	Loss 11.8416 (11.7857)
2022-11-08 18:13:58,119:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2993 (4.2993)
2022-11-08 18:13:58,597:INFO: Dataset: zara2               Batch:  2/10	Loss 4.0640 (4.1744)
2022-11-08 18:13:58,982:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2216 (4.1900)
2022-11-08 18:13:59,352:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1411 (4.1768)
2022-11-08 18:13:59,648:INFO: Dataset: zara2               Batch:  5/10	Loss 4.0957 (4.1603)
2022-11-08 18:13:59,861:INFO: Dataset: zara2               Batch:  6/10	Loss 4.3811 (4.1962)
2022-11-08 18:13:59,863:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1962 (4.1962)
2022-11-08 18:13:59,865:INFO: Dataset: zara2               Batch:  8/10	Loss 4.7161 (4.2569)
2022-11-08 18:13:59,867:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2901 (4.2604)
2022-11-08 18:13:59,868:INFO: Dataset: zara2               Batch: 10/10	Loss 4.2209 (4.2566)
2022-11-08 18:14:00,944:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1076.pth.tar
2022-11-08 18:14:00,944:INFO: 
===> EPOCH: 1077 (P5)
2022-11-08 18:14:00,945:INFO: - Computing loss (training)
2022-11-08 18:14:08,379:INFO: Dataset: hotel               Batch: 1/8	Loss 13.2410 (13.2410)
2022-11-08 18:14:08,836:INFO: Dataset: hotel               Batch: 2/8	Loss 15.2793 (14.3222)
2022-11-08 18:14:09,249:INFO: Dataset: hotel               Batch: 3/8	Loss 12.6920 (13.8710)
2022-11-08 18:14:09,649:INFO: Dataset: hotel               Batch: 4/8	Loss 13.3486 (13.7377)
2022-11-08 18:14:09,884:INFO: Dataset: hotel               Batch: 5/8	Loss 13.3562 (13.6659)
2022-11-08 18:14:10,130:INFO: Dataset: hotel               Batch: 6/8	Loss 12.4516 (13.4638)
2022-11-08 18:14:10,135:INFO: Dataset: hotel               Batch: 7/8	Loss 12.3931 (13.3148)
2022-11-08 18:14:10,147:INFO: Dataset: hotel               Batch: 8/8	Loss 13.6736 (13.3267)
2022-11-08 18:14:36,302:INFO: Dataset: univ                Batch:  1/29	Loss 8.9561 (8.9561)
2022-11-08 18:14:36,307:INFO: Dataset: univ                Batch:  2/29	Loss 9.2445 (9.0973)
2022-11-08 18:14:36,314:INFO: Dataset: univ                Batch:  3/29	Loss 8.4582 (8.8618)
2022-11-08 18:14:36,319:INFO: Dataset: univ                Batch:  4/29	Loss 8.5116 (8.7740)
2022-11-08 18:14:36,324:INFO: Dataset: univ                Batch:  5/29	Loss 8.8051 (8.7799)
2022-11-08 18:14:36,331:INFO: Dataset: univ                Batch:  6/29	Loss 9.0931 (8.8215)
2022-11-08 18:14:36,336:INFO: Dataset: univ                Batch:  7/29	Loss 8.6801 (8.8018)
2022-11-08 18:14:36,341:INFO: Dataset: univ                Batch:  8/29	Loss 8.4622 (8.7570)
2022-11-08 18:14:36,345:INFO: Dataset: univ                Batch:  9/29	Loss 9.0434 (8.7904)
2022-11-08 18:14:36,349:INFO: Dataset: univ                Batch: 10/29	Loss 8.4273 (8.7500)
2022-11-08 18:14:36,354:INFO: Dataset: univ                Batch: 11/29	Loss 8.1765 (8.6947)
2022-11-08 18:14:36,359:INFO: Dataset: univ                Batch: 12/29	Loss 8.2455 (8.6572)
2022-11-08 18:14:36,366:INFO: Dataset: univ                Batch: 13/29	Loss 8.0718 (8.6128)
2022-11-08 18:14:36,372:INFO: Dataset: univ                Batch: 14/29	Loss 8.1699 (8.5826)
2022-11-08 18:14:36,379:INFO: Dataset: univ                Batch: 15/29	Loss 7.9170 (8.5386)
2022-11-08 18:14:36,383:INFO: Dataset: univ                Batch: 16/29	Loss 7.9435 (8.5067)
2022-11-08 18:14:36,389:INFO: Dataset: univ                Batch: 17/29	Loss 7.7429 (8.4636)
2022-11-08 18:14:36,394:INFO: Dataset: univ                Batch: 18/29	Loss 7.5822 (8.4136)
2022-11-08 18:14:36,398:INFO: Dataset: univ                Batch: 19/29	Loss 8.0637 (8.3965)
2022-11-08 18:14:36,402:INFO: Dataset: univ                Batch: 20/29	Loss 7.3865 (8.3445)
2022-11-08 18:14:36,408:INFO: Dataset: univ                Batch: 21/29	Loss 7.0837 (8.2842)
2022-11-08 18:14:36,414:INFO: Dataset: univ                Batch: 22/29	Loss 7.0984 (8.2296)
2022-11-08 18:14:36,435:INFO: Dataset: univ                Batch: 23/29	Loss 7.1761 (8.1876)
2022-11-08 18:14:36,448:INFO: Dataset: univ                Batch: 24/29	Loss 7.1234 (8.1444)
2022-11-08 18:14:36,466:INFO: Dataset: univ                Batch: 25/29	Loss 6.5344 (8.0751)
2022-11-08 18:14:36,482:INFO: Dataset: univ                Batch: 26/29	Loss 6.6158 (8.0168)
2022-11-08 18:14:36,494:INFO: Dataset: univ                Batch: 27/29	Loss 6.5176 (7.9645)
2022-11-08 18:14:36,497:INFO: Dataset: univ                Batch: 28/29	Loss 6.7760 (7.9287)
2022-11-08 18:14:36,500:INFO: Dataset: univ                Batch: 29/29	Loss 6.5979 (7.9116)
2022-11-08 18:14:44,009:INFO: Dataset: zara1               Batch:  1/16	Loss 12.9445 (12.9445)
2022-11-08 18:14:44,381:INFO: Dataset: zara1               Batch:  2/16	Loss 12.8825 (12.9117)
2022-11-08 18:14:44,783:INFO: Dataset: zara1               Batch:  3/16	Loss 12.6182 (12.8095)
2022-11-08 18:14:45,159:INFO: Dataset: zara1               Batch:  4/16	Loss 13.3195 (12.9300)
2022-11-08 18:14:45,437:INFO: Dataset: zara1               Batch:  5/16	Loss 13.0327 (12.9517)
2022-11-08 18:14:45,601:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0663 (12.9747)
2022-11-08 18:14:45,605:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0478 (12.9858)
2022-11-08 18:14:45,608:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8660 (12.9732)
2022-11-08 18:14:45,612:INFO: Dataset: zara1               Batch:  9/16	Loss 12.8081 (12.9545)
2022-11-08 18:14:45,615:INFO: Dataset: zara1               Batch: 10/16	Loss 12.4443 (12.9118)
2022-11-08 18:14:45,618:INFO: Dataset: zara1               Batch: 11/16	Loss 12.4395 (12.8682)
2022-11-08 18:14:45,622:INFO: Dataset: zara1               Batch: 12/16	Loss 12.3099 (12.8127)
2022-11-08 18:14:45,626:INFO: Dataset: zara1               Batch: 13/16	Loss 12.2579 (12.7765)
2022-11-08 18:14:45,630:INFO: Dataset: zara1               Batch: 14/16	Loss 11.6013 (12.7084)
2022-11-08 18:14:45,634:INFO: Dataset: zara1               Batch: 15/16	Loss 12.8292 (12.7155)
2022-11-08 18:14:45,637:INFO: Dataset: zara1               Batch: 16/16	Loss 11.7902 (12.6653)
2022-11-08 18:15:12,311:INFO: Dataset: zara2               Batch:  1/36	Loss 6.7270 (6.7270)
2022-11-08 18:15:12,317:INFO: Dataset: zara2               Batch:  2/36	Loss 6.1095 (6.4049)
2022-11-08 18:15:12,325:INFO: Dataset: zara2               Batch:  3/36	Loss 6.4535 (6.4223)
2022-11-08 18:15:12,331:INFO: Dataset: zara2               Batch:  4/36	Loss 6.2868 (6.3872)
2022-11-08 18:15:12,348:INFO: Dataset: zara2               Batch:  5/36	Loss 5.9648 (6.3009)
2022-11-08 18:15:12,364:INFO: Dataset: zara2               Batch:  6/36	Loss 6.5509 (6.3421)
2022-11-08 18:15:12,369:INFO: Dataset: zara2               Batch:  7/36	Loss 5.9557 (6.2880)
2022-11-08 18:15:12,375:INFO: Dataset: zara2               Batch:  8/36	Loss 5.8500 (6.2370)
2022-11-08 18:15:12,378:INFO: Dataset: zara2               Batch:  9/36	Loss 5.6698 (6.1769)
2022-11-08 18:15:12,385:INFO: Dataset: zara2               Batch: 10/36	Loss 5.8759 (6.1454)
2022-11-08 18:15:12,396:INFO: Dataset: zara2               Batch: 11/36	Loss 6.5263 (6.1797)
2022-11-08 18:15:12,400:INFO: Dataset: zara2               Batch: 12/36	Loss 6.0827 (6.1723)
2022-11-08 18:15:12,411:INFO: Dataset: zara2               Batch: 13/36	Loss 5.8381 (6.1469)
2022-11-08 18:15:12,415:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6116 (6.1085)
2022-11-08 18:15:12,427:INFO: Dataset: zara2               Batch: 15/36	Loss 6.1160 (6.1090)
2022-11-08 18:15:12,432:INFO: Dataset: zara2               Batch: 16/36	Loss 5.4364 (6.0711)
2022-11-08 18:15:12,444:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6783 (6.0437)
2022-11-08 18:15:12,452:INFO: Dataset: zara2               Batch: 18/36	Loss 5.8236 (6.0310)
2022-11-08 18:15:12,461:INFO: Dataset: zara2               Batch: 19/36	Loss 5.4668 (6.0010)
2022-11-08 18:15:12,468:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4705 (5.9730)
2022-11-08 18:15:12,478:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3744 (5.9428)
2022-11-08 18:15:12,484:INFO: Dataset: zara2               Batch: 22/36	Loss 4.8970 (5.9042)
2022-11-08 18:15:12,494:INFO: Dataset: zara2               Batch: 23/36	Loss 4.7689 (5.8509)
2022-11-08 18:15:12,500:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9154 (5.8118)
2022-11-08 18:15:12,504:INFO: Dataset: zara2               Batch: 25/36	Loss 4.7535 (5.7636)
2022-11-08 18:15:12,515:INFO: Dataset: zara2               Batch: 26/36	Loss 5.3514 (5.7487)
2022-11-08 18:15:12,519:INFO: Dataset: zara2               Batch: 27/36	Loss 4.6798 (5.7091)
2022-11-08 18:15:12,529:INFO: Dataset: zara2               Batch: 28/36	Loss 5.2033 (5.6918)
2022-11-08 18:15:12,535:INFO: Dataset: zara2               Batch: 29/36	Loss 4.7573 (5.6615)
2022-11-08 18:15:12,547:INFO: Dataset: zara2               Batch: 30/36	Loss 4.4231 (5.6173)
2022-11-08 18:15:12,557:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3152 (5.5773)
2022-11-08 18:15:12,565:INFO: Dataset: zara2               Batch: 32/36	Loss 4.6586 (5.5463)
2022-11-08 18:15:12,576:INFO: Dataset: zara2               Batch: 33/36	Loss 4.6430 (5.5230)
2022-11-08 18:15:12,582:INFO: Dataset: zara2               Batch: 34/36	Loss 4.2347 (5.4875)
2022-11-08 18:15:12,603:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2743 (5.4484)
2022-11-08 18:15:12,611:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1130 (5.4221)
2022-11-08 18:15:13,629:INFO: - Computing loss (validation)
2022-11-08 18:15:20,902:INFO: Dataset: hotel               Batch: 1/3	Loss 16.0680 (16.0680)
2022-11-08 18:15:21,315:INFO: Dataset: hotel               Batch: 2/3	Loss 15.8368 (15.9609)
2022-11-08 18:15:21,708:INFO: Dataset: hotel               Batch: 3/3	Loss 15.5174 (15.9291)
2022-11-08 18:15:30,802:INFO: Dataset: univ                Batch: 1/6	Loss 8.2074 (8.2074)
2022-11-08 18:15:31,151:INFO: Dataset: univ                Batch: 2/6	Loss 8.2794 (8.2427)
2022-11-08 18:15:31,516:INFO: Dataset: univ                Batch: 3/6	Loss 7.9192 (8.1325)
2022-11-08 18:15:31,929:INFO: Dataset: univ                Batch: 4/6	Loss 7.9683 (8.0872)
2022-11-08 18:15:32,198:INFO: Dataset: univ                Batch: 5/6	Loss 8.5167 (8.1608)
2022-11-08 18:15:32,485:INFO: Dataset: univ                Batch: 6/6	Loss 8.6178 (8.2191)
2022-11-08 18:15:40,684:INFO: Dataset: zara1               Batch: 1/3	Loss 11.8282 (11.8282)
2022-11-08 18:15:41,115:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8341 (11.8312)
2022-11-08 18:15:41,578:INFO: Dataset: zara1               Batch: 3/3	Loss 11.8138 (11.8274)
2022-11-08 18:15:50,517:INFO: Dataset: zara2               Batch:  1/10	Loss 4.5007 (4.5007)
2022-11-08 18:15:51,015:INFO: Dataset: zara2               Batch:  2/10	Loss 4.4885 (4.4946)
2022-11-08 18:15:51,365:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2888 (4.4257)
2022-11-08 18:15:51,719:INFO: Dataset: zara2               Batch:  4/10	Loss 4.4227 (4.4249)
2022-11-08 18:15:52,011:INFO: Dataset: zara2               Batch:  5/10	Loss 4.5356 (4.4485)
2022-11-08 18:15:52,285:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0157 (4.3765)
2022-11-08 18:15:52,286:INFO: Dataset: zara2               Batch:  7/10	Loss 4.2860 (4.3642)
2022-11-08 18:15:52,295:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0853 (4.3255)
2022-11-08 18:15:52,298:INFO: Dataset: zara2               Batch:  9/10	Loss 4.4754 (4.3415)
2022-11-08 18:15:52,299:INFO: Dataset: zara2               Batch: 10/10	Loss 4.1357 (4.3217)
2022-11-08 18:15:53,325:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1077.pth.tar
2022-11-08 18:15:53,325:INFO: 
===> EPOCH: 1078 (P5)
2022-11-08 18:15:53,325:INFO: - Computing loss (training)
2022-11-08 18:16:00,537:INFO: Dataset: hotel               Batch: 1/8	Loss 12.4842 (12.4842)
2022-11-08 18:16:00,980:INFO: Dataset: hotel               Batch: 2/8	Loss 13.3503 (12.9110)
2022-11-08 18:16:01,333:INFO: Dataset: hotel               Batch: 3/8	Loss 13.5591 (13.1359)
2022-11-08 18:16:01,690:INFO: Dataset: hotel               Batch: 4/8	Loss 13.9177 (13.3205)
2022-11-08 18:16:01,948:INFO: Dataset: hotel               Batch: 5/8	Loss 13.4082 (13.3384)
2022-11-08 18:16:02,210:INFO: Dataset: hotel               Batch: 6/8	Loss 12.7307 (13.2340)
2022-11-08 18:16:02,212:INFO: Dataset: hotel               Batch: 7/8	Loss 13.5086 (13.2733)
2022-11-08 18:16:02,216:INFO: Dataset: hotel               Batch: 8/8	Loss 9.7808 (13.1627)
2022-11-08 18:16:28,119:INFO: Dataset: univ                Batch:  1/29	Loss 8.1722 (8.1722)
2022-11-08 18:16:28,127:INFO: Dataset: univ                Batch:  2/29	Loss 9.4449 (8.8335)
2022-11-08 18:16:28,136:INFO: Dataset: univ                Batch:  3/29	Loss 8.7778 (8.8156)
2022-11-08 18:16:28,147:INFO: Dataset: univ                Batch:  4/29	Loss 8.6671 (8.7807)
2022-11-08 18:16:28,155:INFO: Dataset: univ                Batch:  5/29	Loss 8.5133 (8.7287)
2022-11-08 18:16:28,162:INFO: Dataset: univ                Batch:  6/29	Loss 9.3909 (8.8355)
2022-11-08 18:16:28,167:INFO: Dataset: univ                Batch:  7/29	Loss 8.1264 (8.7133)
2022-11-08 18:16:28,178:INFO: Dataset: univ                Batch:  8/29	Loss 8.4631 (8.6833)
2022-11-08 18:16:28,187:INFO: Dataset: univ                Batch:  9/29	Loss 8.4285 (8.6540)
2022-11-08 18:16:28,197:INFO: Dataset: univ                Batch: 10/29	Loss 8.6069 (8.6491)
2022-11-08 18:16:28,204:INFO: Dataset: univ                Batch: 11/29	Loss 8.2426 (8.6122)
2022-11-08 18:16:28,212:INFO: Dataset: univ                Batch: 12/29	Loss 7.8175 (8.5490)
2022-11-08 18:16:28,219:INFO: Dataset: univ                Batch: 13/29	Loss 8.1379 (8.5176)
2022-11-08 18:16:28,231:INFO: Dataset: univ                Batch: 14/29	Loss 7.7255 (8.4622)
2022-11-08 18:16:28,238:INFO: Dataset: univ                Batch: 15/29	Loss 7.5966 (8.4031)
2022-11-08 18:16:28,250:INFO: Dataset: univ                Batch: 16/29	Loss 7.9528 (8.3727)
2022-11-08 18:16:28,255:INFO: Dataset: univ                Batch: 17/29	Loss 8.1400 (8.3582)
2022-11-08 18:16:28,262:INFO: Dataset: univ                Batch: 18/29	Loss 7.5693 (8.3133)
2022-11-08 18:16:28,269:INFO: Dataset: univ                Batch: 19/29	Loss 7.5656 (8.2740)
2022-11-08 18:16:28,278:INFO: Dataset: univ                Batch: 20/29	Loss 7.4193 (8.2319)
2022-11-08 18:16:28,284:INFO: Dataset: univ                Batch: 21/29	Loss 6.9633 (8.1664)
2022-11-08 18:16:28,292:INFO: Dataset: univ                Batch: 22/29	Loss 6.8633 (8.1035)
2022-11-08 18:16:28,301:INFO: Dataset: univ                Batch: 23/29	Loss 7.1923 (8.0666)
2022-11-08 18:16:28,311:INFO: Dataset: univ                Batch: 24/29	Loss 6.6688 (8.0032)
2022-11-08 18:16:28,319:INFO: Dataset: univ                Batch: 25/29	Loss 6.7520 (7.9549)
2022-11-08 18:16:28,328:INFO: Dataset: univ                Batch: 26/29	Loss 6.4666 (7.8875)
2022-11-08 18:16:28,334:INFO: Dataset: univ                Batch: 27/29	Loss 6.6756 (7.8548)
2022-11-08 18:16:28,342:INFO: Dataset: univ                Batch: 28/29	Loss 6.4509 (7.8070)
2022-11-08 18:16:28,348:INFO: Dataset: univ                Batch: 29/29	Loss 6.6883 (7.7931)
2022-11-08 18:16:37,019:INFO: Dataset: zara1               Batch:  1/16	Loss 12.7556 (12.7556)
2022-11-08 18:16:37,449:INFO: Dataset: zara1               Batch:  2/16	Loss 12.9877 (12.8637)
2022-11-08 18:16:37,796:INFO: Dataset: zara1               Batch:  3/16	Loss 12.8938 (12.8731)
2022-11-08 18:16:38,201:INFO: Dataset: zara1               Batch:  4/16	Loss 12.9021 (12.8801)
2022-11-08 18:16:38,447:INFO: Dataset: zara1               Batch:  5/16	Loss 13.1742 (12.9431)
2022-11-08 18:16:38,634:INFO: Dataset: zara1               Batch:  6/16	Loss 12.6618 (12.9013)
2022-11-08 18:16:38,638:INFO: Dataset: zara1               Batch:  7/16	Loss 13.6134 (13.0146)
2022-11-08 18:16:38,648:INFO: Dataset: zara1               Batch:  8/16	Loss 12.5153 (12.9587)
2022-11-08 18:16:38,653:INFO: Dataset: zara1               Batch:  9/16	Loss 12.6244 (12.9245)
2022-11-08 18:16:38,664:INFO: Dataset: zara1               Batch: 10/16	Loss 12.8376 (12.9166)
2022-11-08 18:16:38,669:INFO: Dataset: zara1               Batch: 11/16	Loss 12.6820 (12.8951)
2022-11-08 18:16:38,682:INFO: Dataset: zara1               Batch: 12/16	Loss 12.5418 (12.8636)
2022-11-08 18:16:38,688:INFO: Dataset: zara1               Batch: 13/16	Loss 12.2157 (12.8049)
2022-11-08 18:16:38,703:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3015 (12.7655)
2022-11-08 18:16:38,718:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0270 (12.7161)
2022-11-08 18:16:38,735:INFO: Dataset: zara1               Batch: 16/16	Loss 12.4853 (12.7049)
2022-11-08 18:17:05,548:INFO: Dataset: zara2               Batch:  1/36	Loss 6.5071 (6.5071)
2022-11-08 18:17:05,554:INFO: Dataset: zara2               Batch:  2/36	Loss 6.6046 (6.5563)
2022-11-08 18:17:05,565:INFO: Dataset: zara2               Batch:  3/36	Loss 6.3764 (6.4976)
2022-11-08 18:17:05,578:INFO: Dataset: zara2               Batch:  4/36	Loss 6.5117 (6.5013)
2022-11-08 18:17:05,584:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4394 (6.4896)
2022-11-08 18:17:05,596:INFO: Dataset: zara2               Batch:  6/36	Loss 6.4109 (6.4773)
2022-11-08 18:17:05,602:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4972 (6.4800)
2022-11-08 18:17:05,612:INFO: Dataset: zara2               Batch:  8/36	Loss 6.4809 (6.4801)
2022-11-08 18:17:05,619:INFO: Dataset: zara2               Batch:  9/36	Loss 6.1378 (6.4459)
2022-11-08 18:17:05,631:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6172 (6.3591)
2022-11-08 18:17:05,636:INFO: Dataset: zara2               Batch: 11/36	Loss 5.8863 (6.3179)
2022-11-08 18:17:05,645:INFO: Dataset: zara2               Batch: 12/36	Loss 6.1370 (6.3043)
2022-11-08 18:17:05,651:INFO: Dataset: zara2               Batch: 13/36	Loss 5.4400 (6.2344)
2022-11-08 18:17:05,655:INFO: Dataset: zara2               Batch: 14/36	Loss 5.7256 (6.1932)
2022-11-08 18:17:05,665:INFO: Dataset: zara2               Batch: 15/36	Loss 5.7029 (6.1627)
2022-11-08 18:17:05,670:INFO: Dataset: zara2               Batch: 16/36	Loss 5.3957 (6.1112)
2022-11-08 18:17:05,682:INFO: Dataset: zara2               Batch: 17/36	Loss 5.8188 (6.0915)
2022-11-08 18:17:05,686:INFO: Dataset: zara2               Batch: 18/36	Loss 5.3716 (6.0504)
2022-11-08 18:17:05,699:INFO: Dataset: zara2               Batch: 19/36	Loss 5.7871 (6.0375)
2022-11-08 18:17:05,704:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4798 (6.0142)
2022-11-08 18:17:05,710:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3638 (5.9850)
2022-11-08 18:17:05,714:INFO: Dataset: zara2               Batch: 22/36	Loss 4.8715 (5.9328)
2022-11-08 18:17:05,719:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0156 (5.8924)
2022-11-08 18:17:05,729:INFO: Dataset: zara2               Batch: 24/36	Loss 5.3827 (5.8725)
2022-11-08 18:17:05,733:INFO: Dataset: zara2               Batch: 25/36	Loss 4.5786 (5.8132)
2022-11-08 18:17:05,745:INFO: Dataset: zara2               Batch: 26/36	Loss 4.7377 (5.7776)
2022-11-08 18:17:05,749:INFO: Dataset: zara2               Batch: 27/36	Loss 4.6405 (5.7397)
2022-11-08 18:17:05,753:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7809 (5.7068)
2022-11-08 18:17:05,768:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6929 (5.6739)
2022-11-08 18:17:05,772:INFO: Dataset: zara2               Batch: 30/36	Loss 4.4557 (5.6308)
2022-11-08 18:17:05,778:INFO: Dataset: zara2               Batch: 31/36	Loss 4.8642 (5.6020)
2022-11-08 18:17:05,783:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4325 (5.5574)
2022-11-08 18:17:05,793:INFO: Dataset: zara2               Batch: 33/36	Loss 4.4209 (5.5241)
2022-11-08 18:17:05,799:INFO: Dataset: zara2               Batch: 34/36	Loss 4.2410 (5.4895)
2022-11-08 18:17:05,802:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2526 (5.4503)
2022-11-08 18:17:05,809:INFO: Dataset: zara2               Batch: 36/36	Loss 4.6607 (5.4356)
2022-11-08 18:17:06,831:INFO: - Computing loss (validation)
2022-11-08 18:17:14,168:INFO: Dataset: hotel               Batch: 1/3	Loss 16.5261 (16.5261)
2022-11-08 18:17:14,578:INFO: Dataset: hotel               Batch: 2/3	Loss 15.4439 (15.9910)
2022-11-08 18:17:15,082:INFO: Dataset: hotel               Batch: 3/3	Loss 19.1591 (16.2072)
2022-11-08 18:17:24,166:INFO: Dataset: univ                Batch: 1/6	Loss 7.9420 (7.9420)
2022-11-08 18:17:24,564:INFO: Dataset: univ                Batch: 2/6	Loss 8.2946 (8.0982)
2022-11-08 18:17:24,899:INFO: Dataset: univ                Batch: 3/6	Loss 8.3778 (8.1981)
2022-11-08 18:17:25,299:INFO: Dataset: univ                Batch: 4/6	Loss 8.3060 (8.2226)
2022-11-08 18:17:25,530:INFO: Dataset: univ                Batch: 5/6	Loss 8.7006 (8.3080)
2022-11-08 18:17:25,769:INFO: Dataset: univ                Batch: 6/6	Loss 7.9249 (8.2477)
2022-11-08 18:17:34,086:INFO: Dataset: zara1               Batch: 1/3	Loss 11.5927 (11.5927)
2022-11-08 18:17:34,585:INFO: Dataset: zara1               Batch: 2/3	Loss 12.0632 (11.8139)
2022-11-08 18:17:34,866:INFO: Dataset: zara1               Batch: 3/3	Loss 11.5976 (11.7603)
2022-11-08 18:17:43,931:INFO: Dataset: zara2               Batch:  1/10	Loss 4.3464 (4.3464)
2022-11-08 18:17:44,356:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2639 (4.3074)
2022-11-08 18:17:44,818:INFO: Dataset: zara2               Batch:  3/10	Loss 4.3139 (4.3096)
2022-11-08 18:17:45,133:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1086 (4.2589)
2022-11-08 18:17:45,462:INFO: Dataset: zara2               Batch:  5/10	Loss 4.4167 (4.2918)
2022-11-08 18:17:45,675:INFO: Dataset: zara2               Batch:  6/10	Loss 4.0514 (4.2545)
2022-11-08 18:17:45,678:INFO: Dataset: zara2               Batch:  7/10	Loss 4.2315 (4.2511)
2022-11-08 18:17:45,679:INFO: Dataset: zara2               Batch:  8/10	Loss 4.0523 (4.2288)
2022-11-08 18:17:45,681:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2542 (4.2317)
2022-11-08 18:17:45,684:INFO: Dataset: zara2               Batch: 10/10	Loss 4.6785 (4.2716)
2022-11-08 18:17:46,733:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1078.pth.tar
2022-11-08 18:17:46,733:INFO: 
===> EPOCH: 1079 (P5)
2022-11-08 18:17:46,734:INFO: - Computing loss (training)
2022-11-08 18:17:53,936:INFO: Dataset: hotel               Batch: 1/8	Loss 12.9101 (12.9101)
2022-11-08 18:17:54,361:INFO: Dataset: hotel               Batch: 2/8	Loss 13.8625 (13.3730)
2022-11-08 18:17:54,827:INFO: Dataset: hotel               Batch: 3/8	Loss 13.4097 (13.3851)
2022-11-08 18:17:55,255:INFO: Dataset: hotel               Batch: 4/8	Loss 15.0845 (13.8354)
2022-11-08 18:17:55,538:INFO: Dataset: hotel               Batch: 5/8	Loss 13.0587 (13.6876)
2022-11-08 18:17:55,718:INFO: Dataset: hotel               Batch: 6/8	Loss 13.0249 (13.5851)
2022-11-08 18:17:55,722:INFO: Dataset: hotel               Batch: 7/8	Loss 11.8730 (13.3479)
2022-11-08 18:17:55,732:INFO: Dataset: hotel               Batch: 8/8	Loss 10.3524 (13.2609)
2022-11-08 18:18:21,950:INFO: Dataset: univ                Batch:  1/29	Loss 8.7163 (8.7163)
2022-11-08 18:18:21,983:INFO: Dataset: univ                Batch:  2/29	Loss 8.9042 (8.8104)
2022-11-08 18:18:22,002:INFO: Dataset: univ                Batch:  3/29	Loss 8.7390 (8.7851)
2022-11-08 18:18:22,020:INFO: Dataset: univ                Batch:  4/29	Loss 8.5289 (8.7143)
2022-11-08 18:18:22,038:INFO: Dataset: univ                Batch:  5/29	Loss 8.8395 (8.7369)
2022-11-08 18:18:22,056:INFO: Dataset: univ                Batch:  6/29	Loss 8.8439 (8.7519)
2022-11-08 18:18:22,071:INFO: Dataset: univ                Batch:  7/29	Loss 8.9603 (8.7803)
2022-11-08 18:18:22,088:INFO: Dataset: univ                Batch:  8/29	Loss 8.9355 (8.7963)
2022-11-08 18:18:22,105:INFO: Dataset: univ                Batch:  9/29	Loss 8.1468 (8.7240)
2022-11-08 18:18:22,122:INFO: Dataset: univ                Batch: 10/29	Loss 8.6078 (8.7117)
2022-11-08 18:18:22,138:INFO: Dataset: univ                Batch: 11/29	Loss 8.3464 (8.6730)
2022-11-08 18:18:22,143:INFO: Dataset: univ                Batch: 12/29	Loss 8.2335 (8.6375)
2022-11-08 18:18:22,151:INFO: Dataset: univ                Batch: 13/29	Loss 7.9666 (8.5850)
2022-11-08 18:18:22,156:INFO: Dataset: univ                Batch: 14/29	Loss 8.2814 (8.5621)
2022-11-08 18:18:22,161:INFO: Dataset: univ                Batch: 15/29	Loss 7.9760 (8.5217)
2022-11-08 18:18:22,166:INFO: Dataset: univ                Batch: 16/29	Loss 7.6972 (8.4715)
2022-11-08 18:18:22,169:INFO: Dataset: univ                Batch: 17/29	Loss 8.3301 (8.4642)
2022-11-08 18:18:22,176:INFO: Dataset: univ                Batch: 18/29	Loss 7.7628 (8.4211)
2022-11-08 18:18:22,180:INFO: Dataset: univ                Batch: 19/29	Loss 7.5063 (8.3709)
2022-11-08 18:18:22,184:INFO: Dataset: univ                Batch: 20/29	Loss 7.3976 (8.3232)
2022-11-08 18:18:22,189:INFO: Dataset: univ                Batch: 21/29	Loss 7.2552 (8.2767)
2022-11-08 18:18:22,193:INFO: Dataset: univ                Batch: 22/29	Loss 7.1039 (8.2234)
2022-11-08 18:18:22,196:INFO: Dataset: univ                Batch: 23/29	Loss 6.9958 (8.1752)
2022-11-08 18:18:22,200:INFO: Dataset: univ                Batch: 24/29	Loss 6.8072 (8.1085)
2022-11-08 18:18:22,205:INFO: Dataset: univ                Batch: 25/29	Loss 6.6504 (8.0486)
2022-11-08 18:18:22,208:INFO: Dataset: univ                Batch: 26/29	Loss 6.9581 (8.0091)
2022-11-08 18:18:22,213:INFO: Dataset: univ                Batch: 27/29	Loss 6.6692 (7.9608)
2022-11-08 18:18:22,218:INFO: Dataset: univ                Batch: 28/29	Loss 6.4192 (7.9016)
2022-11-08 18:18:22,221:INFO: Dataset: univ                Batch: 29/29	Loss 5.9904 (7.8748)
2022-11-08 18:18:30,422:INFO: Dataset: zara1               Batch:  1/16	Loss 12.6815 (12.6815)
2022-11-08 18:18:30,980:INFO: Dataset: zara1               Batch:  2/16	Loss 12.8871 (12.7843)
2022-11-08 18:18:31,372:INFO: Dataset: zara1               Batch:  3/16	Loss 12.9207 (12.8298)
2022-11-08 18:18:31,705:INFO: Dataset: zara1               Batch:  4/16	Loss 13.1624 (12.9155)
2022-11-08 18:18:31,968:INFO: Dataset: zara1               Batch:  5/16	Loss 13.0281 (12.9355)
2022-11-08 18:18:32,239:INFO: Dataset: zara1               Batch:  6/16	Loss 12.7931 (12.9103)
2022-11-08 18:18:32,253:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0253 (12.9286)
2022-11-08 18:18:32,263:INFO: Dataset: zara1               Batch:  8/16	Loss 13.1054 (12.9492)
2022-11-08 18:18:32,270:INFO: Dataset: zara1               Batch:  9/16	Loss 12.0002 (12.8575)
2022-11-08 18:18:32,278:INFO: Dataset: zara1               Batch: 10/16	Loss 12.1125 (12.7708)
2022-11-08 18:18:32,289:INFO: Dataset: zara1               Batch: 11/16	Loss 12.4959 (12.7452)
2022-11-08 18:18:32,303:INFO: Dataset: zara1               Batch: 12/16	Loss 12.6546 (12.7379)
2022-11-08 18:18:32,314:INFO: Dataset: zara1               Batch: 13/16	Loss 12.0877 (12.6856)
2022-11-08 18:18:32,321:INFO: Dataset: zara1               Batch: 14/16	Loss 12.1219 (12.6429)
2022-11-08 18:18:32,334:INFO: Dataset: zara1               Batch: 15/16	Loss 11.9876 (12.6045)
2022-11-08 18:18:32,341:INFO: Dataset: zara1               Batch: 16/16	Loss 12.6905 (12.6079)
2022-11-08 18:18:58,626:INFO: Dataset: zara2               Batch:  1/36	Loss 5.9873 (5.9873)
2022-11-08 18:18:58,631:INFO: Dataset: zara2               Batch:  2/36	Loss 6.7270 (6.3267)
2022-11-08 18:18:58,650:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1912 (6.2774)
2022-11-08 18:18:58,654:INFO: Dataset: zara2               Batch:  4/36	Loss 5.8813 (6.1809)
2022-11-08 18:18:58,666:INFO: Dataset: zara2               Batch:  5/36	Loss 5.9370 (6.1346)
2022-11-08 18:18:58,672:INFO: Dataset: zara2               Batch:  6/36	Loss 6.2666 (6.1574)
2022-11-08 18:18:58,681:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4534 (6.2039)
2022-11-08 18:18:58,686:INFO: Dataset: zara2               Batch:  8/36	Loss 6.2228 (6.2064)
2022-11-08 18:18:58,689:INFO: Dataset: zara2               Batch:  9/36	Loss 6.0753 (6.1925)
2022-11-08 18:18:58,701:INFO: Dataset: zara2               Batch: 10/36	Loss 5.9352 (6.1664)
2022-11-08 18:18:58,706:INFO: Dataset: zara2               Batch: 11/36	Loss 6.1052 (6.1602)
2022-11-08 18:18:58,718:INFO: Dataset: zara2               Batch: 12/36	Loss 5.9254 (6.1398)
2022-11-08 18:18:58,721:INFO: Dataset: zara2               Batch: 13/36	Loss 5.4825 (6.0905)
2022-11-08 18:18:58,732:INFO: Dataset: zara2               Batch: 14/36	Loss 5.8597 (6.0719)
2022-11-08 18:18:58,736:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5558 (6.0419)
2022-11-08 18:18:58,747:INFO: Dataset: zara2               Batch: 16/36	Loss 5.6161 (6.0153)
2022-11-08 18:18:58,753:INFO: Dataset: zara2               Batch: 17/36	Loss 5.4545 (5.9784)
2022-11-08 18:18:58,765:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6411 (5.9588)
2022-11-08 18:18:58,770:INFO: Dataset: zara2               Batch: 19/36	Loss 5.2141 (5.9216)
2022-11-08 18:18:58,780:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4290 (5.9020)
2022-11-08 18:18:58,785:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3476 (5.8724)
2022-11-08 18:18:58,795:INFO: Dataset: zara2               Batch: 22/36	Loss 5.4783 (5.8547)
2022-11-08 18:18:58,798:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9586 (5.8177)
2022-11-08 18:18:58,803:INFO: Dataset: zara2               Batch: 24/36	Loss 5.2103 (5.7909)
2022-11-08 18:18:58,811:INFO: Dataset: zara2               Batch: 25/36	Loss 5.2338 (5.7697)
2022-11-08 18:18:58,816:INFO: Dataset: zara2               Batch: 26/36	Loss 4.7518 (5.7309)
2022-11-08 18:18:58,820:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9487 (5.6997)
2022-11-08 18:18:58,830:INFO: Dataset: zara2               Batch: 28/36	Loss 4.6577 (5.6657)
2022-11-08 18:18:58,834:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6833 (5.6313)
2022-11-08 18:18:58,838:INFO: Dataset: zara2               Batch: 30/36	Loss 4.7683 (5.5992)
2022-11-08 18:18:58,846:INFO: Dataset: zara2               Batch: 31/36	Loss 4.4766 (5.5649)
2022-11-08 18:18:58,854:INFO: Dataset: zara2               Batch: 32/36	Loss 4.5554 (5.5300)
2022-11-08 18:18:58,868:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2691 (5.4902)
2022-11-08 18:18:58,885:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3796 (5.4612)
2022-11-08 18:18:58,896:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0474 (5.4178)
2022-11-08 18:18:58,903:INFO: Dataset: zara2               Batch: 36/36	Loss 4.5497 (5.4010)
2022-11-08 18:18:59,932:INFO: - Computing loss (validation)
2022-11-08 18:19:07,388:INFO: Dataset: hotel               Batch: 1/3	Loss 16.3065 (16.3065)
2022-11-08 18:19:07,745:INFO: Dataset: hotel               Batch: 2/3	Loss 15.5073 (15.8850)
2022-11-08 18:19:08,136:INFO: Dataset: hotel               Batch: 3/3	Loss 16.8666 (15.9520)
2022-11-08 18:19:17,583:INFO: Dataset: univ                Batch: 1/6	Loss 7.9407 (7.9407)
2022-11-08 18:19:18,000:INFO: Dataset: univ                Batch: 2/6	Loss 7.8762 (7.9111)
2022-11-08 18:19:18,334:INFO: Dataset: univ                Batch: 3/6	Loss 7.7710 (7.8624)
2022-11-08 18:19:18,688:INFO: Dataset: univ                Batch: 4/6	Loss 8.7958 (8.0557)
2022-11-08 18:19:18,967:INFO: Dataset: univ                Batch: 5/6	Loss 8.2510 (8.0899)
2022-11-08 18:19:19,189:INFO: Dataset: univ                Batch: 6/6	Loss 8.5631 (8.1484)
2022-11-08 18:19:27,898:INFO: Dataset: zara1               Batch: 1/3	Loss 11.5569 (11.5569)
2022-11-08 18:19:28,245:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8140 (11.6943)
2022-11-08 18:19:28,660:INFO: Dataset: zara1               Batch: 3/3	Loss 12.0815 (11.7927)
2022-11-08 18:19:37,838:INFO: Dataset: zara2               Batch:  1/10	Loss 4.3895 (4.3895)
2022-11-08 18:19:38,274:INFO: Dataset: zara2               Batch:  2/10	Loss 4.3393 (4.3662)
2022-11-08 18:19:38,690:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2722 (4.3364)
2022-11-08 18:19:39,048:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3546 (4.3405)
2022-11-08 18:19:39,344:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2785 (4.3291)
2022-11-08 18:19:39,533:INFO: Dataset: zara2               Batch:  6/10	Loss 4.4043 (4.3412)
2022-11-08 18:19:39,535:INFO: Dataset: zara2               Batch:  7/10	Loss 4.2684 (4.3317)
2022-11-08 18:19:39,538:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3614 (4.3356)
2022-11-08 18:19:39,539:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2545 (4.3267)
2022-11-08 18:19:39,549:INFO: Dataset: zara2               Batch: 10/10	Loss 4.3084 (4.3250)
2022-11-08 18:19:40,928:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1079.pth.tar
2022-11-08 18:19:40,929:INFO: 
===> EPOCH: 1080 (P5)
2022-11-08 18:19:40,929:INFO: - Computing loss (training)
2022-11-08 18:19:48,269:INFO: Dataset: hotel               Batch: 1/8	Loss 12.8170 (12.8170)
2022-11-08 18:19:48,857:INFO: Dataset: hotel               Batch: 2/8	Loss 11.6163 (12.2395)
2022-11-08 18:19:49,197:INFO: Dataset: hotel               Batch: 3/8	Loss 12.4008 (12.2908)
2022-11-08 18:19:49,523:INFO: Dataset: hotel               Batch: 4/8	Loss 15.0725 (13.0031)
2022-11-08 18:19:49,771:INFO: Dataset: hotel               Batch: 5/8	Loss 13.7326 (13.1551)
2022-11-08 18:19:49,966:INFO: Dataset: hotel               Batch: 6/8	Loss 13.4221 (13.2015)
2022-11-08 18:19:49,970:INFO: Dataset: hotel               Batch: 7/8	Loss 12.7389 (13.1357)
2022-11-08 18:19:49,979:INFO: Dataset: hotel               Batch: 8/8	Loss 12.8628 (13.1285)
2022-11-08 18:20:16,282:INFO: Dataset: univ                Batch:  1/29	Loss 8.5718 (8.5718)
2022-11-08 18:20:16,288:INFO: Dataset: univ                Batch:  2/29	Loss 8.3081 (8.4472)
2022-11-08 18:20:16,305:INFO: Dataset: univ                Batch:  3/29	Loss 8.4570 (8.4507)
2022-11-08 18:20:16,318:INFO: Dataset: univ                Batch:  4/29	Loss 8.6310 (8.4937)
2022-11-08 18:20:16,333:INFO: Dataset: univ                Batch:  5/29	Loss 8.8381 (8.5577)
2022-11-08 18:20:16,350:INFO: Dataset: univ                Batch:  6/29	Loss 8.5585 (8.5578)
2022-11-08 18:20:16,357:INFO: Dataset: univ                Batch:  7/29	Loss 8.6904 (8.5768)
2022-11-08 18:20:16,369:INFO: Dataset: univ                Batch:  8/29	Loss 8.3633 (8.5520)
2022-11-08 18:20:16,382:INFO: Dataset: univ                Batch:  9/29	Loss 8.4708 (8.5428)
2022-11-08 18:20:16,388:INFO: Dataset: univ                Batch: 10/29	Loss 8.1860 (8.5074)
2022-11-08 18:20:16,404:INFO: Dataset: univ                Batch: 11/29	Loss 8.6436 (8.5204)
2022-11-08 18:20:16,420:INFO: Dataset: univ                Batch: 12/29	Loss 8.2791 (8.5016)
2022-11-08 18:20:16,437:INFO: Dataset: univ                Batch: 13/29	Loss 8.0651 (8.4690)
2022-11-08 18:20:16,454:INFO: Dataset: univ                Batch: 14/29	Loss 8.0687 (8.4406)
2022-11-08 18:20:16,468:INFO: Dataset: univ                Batch: 15/29	Loss 7.7179 (8.3968)
2022-11-08 18:20:16,482:INFO: Dataset: univ                Batch: 16/29	Loss 7.9701 (8.3738)
2022-11-08 18:20:16,502:INFO: Dataset: univ                Batch: 17/29	Loss 7.5910 (8.3267)
2022-11-08 18:20:16,507:INFO: Dataset: univ                Batch: 18/29	Loss 7.6634 (8.2919)
2022-11-08 18:20:16,514:INFO: Dataset: univ                Batch: 19/29	Loss 7.3137 (8.2393)
2022-11-08 18:20:16,518:INFO: Dataset: univ                Batch: 20/29	Loss 7.2357 (8.1909)
2022-11-08 18:20:16,522:INFO: Dataset: univ                Batch: 21/29	Loss 7.1625 (8.1429)
2022-11-08 18:20:16,526:INFO: Dataset: univ                Batch: 22/29	Loss 6.7805 (8.0808)
2022-11-08 18:20:16,530:INFO: Dataset: univ                Batch: 23/29	Loss 7.2228 (8.0456)
2022-11-08 18:20:16,536:INFO: Dataset: univ                Batch: 24/29	Loss 6.9347 (7.9994)
2022-11-08 18:20:16,540:INFO: Dataset: univ                Batch: 25/29	Loss 7.1016 (7.9662)
2022-11-08 18:20:16,544:INFO: Dataset: univ                Batch: 26/29	Loss 6.6015 (7.9095)
2022-11-08 18:20:16,548:INFO: Dataset: univ                Batch: 27/29	Loss 6.6538 (7.8693)
2022-11-08 18:20:16,550:INFO: Dataset: univ                Batch: 28/29	Loss 6.1785 (7.8075)
2022-11-08 18:20:16,554:INFO: Dataset: univ                Batch: 29/29	Loss 6.5279 (7.7921)
2022-11-08 18:20:24,640:INFO: Dataset: zara1               Batch:  1/16	Loss 13.0197 (13.0197)
2022-11-08 18:20:25,095:INFO: Dataset: zara1               Batch:  2/16	Loss 12.2803 (12.6792)
2022-11-08 18:20:25,554:INFO: Dataset: zara1               Batch:  3/16	Loss 13.0773 (12.8180)
2022-11-08 18:20:25,937:INFO: Dataset: zara1               Batch:  4/16	Loss 13.1289 (12.9095)
2022-11-08 18:20:26,165:INFO: Dataset: zara1               Batch:  5/16	Loss 12.9376 (12.9144)
2022-11-08 18:20:26,382:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0897 (12.9421)
2022-11-08 18:20:26,388:INFO: Dataset: zara1               Batch:  7/16	Loss 12.8329 (12.9283)
2022-11-08 18:20:26,394:INFO: Dataset: zara1               Batch:  8/16	Loss 13.0509 (12.9438)
2022-11-08 18:20:26,400:INFO: Dataset: zara1               Batch:  9/16	Loss 12.4014 (12.8774)
2022-11-08 18:20:26,407:INFO: Dataset: zara1               Batch: 10/16	Loss 12.0057 (12.8001)
2022-11-08 18:20:26,419:INFO: Dataset: zara1               Batch: 11/16	Loss 12.9257 (12.8100)
2022-11-08 18:20:26,424:INFO: Dataset: zara1               Batch: 12/16	Loss 12.6285 (12.7933)
2022-11-08 18:20:26,433:INFO: Dataset: zara1               Batch: 13/16	Loss 12.4029 (12.7570)
2022-11-08 18:20:26,437:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4649 (12.7321)
2022-11-08 18:20:26,444:INFO: Dataset: zara1               Batch: 15/16	Loss 12.1247 (12.6909)
2022-11-08 18:20:26,451:INFO: Dataset: zara1               Batch: 16/16	Loss 11.5454 (12.6391)
2022-11-08 18:20:52,774:INFO: Dataset: zara2               Batch:  1/36	Loss 6.1414 (6.1414)
2022-11-08 18:20:52,780:INFO: Dataset: zara2               Batch:  2/36	Loss 6.7942 (6.4872)
2022-11-08 18:20:52,784:INFO: Dataset: zara2               Batch:  3/36	Loss 7.1553 (6.6924)
2022-11-08 18:20:52,789:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3027 (6.6008)
2022-11-08 18:20:52,794:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3297 (6.5492)
2022-11-08 18:20:52,817:INFO: Dataset: zara2               Batch:  6/36	Loss 6.9983 (6.6239)
2022-11-08 18:20:52,820:INFO: Dataset: zara2               Batch:  7/36	Loss 6.8024 (6.6513)
2022-11-08 18:20:52,824:INFO: Dataset: zara2               Batch:  8/36	Loss 5.8806 (6.5571)
2022-11-08 18:20:52,826:INFO: Dataset: zara2               Batch:  9/36	Loss 5.6938 (6.4766)
2022-11-08 18:20:52,831:INFO: Dataset: zara2               Batch: 10/36	Loss 6.9124 (6.5214)
2022-11-08 18:20:52,836:INFO: Dataset: zara2               Batch: 11/36	Loss 6.3809 (6.5099)
2022-11-08 18:20:52,841:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7125 (6.4443)
2022-11-08 18:20:52,844:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0513 (6.4143)
2022-11-08 18:20:52,847:INFO: Dataset: zara2               Batch: 14/36	Loss 5.7267 (6.3701)
2022-11-08 18:20:52,852:INFO: Dataset: zara2               Batch: 15/36	Loss 5.3957 (6.3098)
2022-11-08 18:20:52,857:INFO: Dataset: zara2               Batch: 16/36	Loss 5.2771 (6.2532)
2022-11-08 18:20:52,864:INFO: Dataset: zara2               Batch: 17/36	Loss 5.4981 (6.2105)
2022-11-08 18:20:52,871:INFO: Dataset: zara2               Batch: 18/36	Loss 5.4544 (6.1704)
2022-11-08 18:20:52,876:INFO: Dataset: zara2               Batch: 19/36	Loss 4.9939 (6.0961)
2022-11-08 18:20:52,883:INFO: Dataset: zara2               Batch: 20/36	Loss 5.1702 (6.0465)
2022-11-08 18:20:52,890:INFO: Dataset: zara2               Batch: 21/36	Loss 5.2780 (6.0119)
2022-11-08 18:20:52,897:INFO: Dataset: zara2               Batch: 22/36	Loss 5.2027 (5.9713)
2022-11-08 18:20:52,901:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9229 (5.9212)
2022-11-08 18:20:52,904:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9147 (5.8836)
2022-11-08 18:20:52,909:INFO: Dataset: zara2               Batch: 25/36	Loss 5.3931 (5.8647)
2022-11-08 18:20:52,918:INFO: Dataset: zara2               Batch: 26/36	Loss 4.2925 (5.8124)
2022-11-08 18:20:52,923:INFO: Dataset: zara2               Batch: 27/36	Loss 4.5020 (5.7643)
2022-11-08 18:20:52,933:INFO: Dataset: zara2               Batch: 28/36	Loss 4.6808 (5.7170)
2022-11-08 18:20:52,936:INFO: Dataset: zara2               Batch: 29/36	Loss 4.4189 (5.6748)
2022-11-08 18:20:52,942:INFO: Dataset: zara2               Batch: 30/36	Loss 4.8228 (5.6423)
2022-11-08 18:20:52,947:INFO: Dataset: zara2               Batch: 31/36	Loss 4.4258 (5.5989)
2022-11-08 18:20:52,952:INFO: Dataset: zara2               Batch: 32/36	Loss 4.5981 (5.5647)
2022-11-08 18:20:52,954:INFO: Dataset: zara2               Batch: 33/36	Loss 4.1635 (5.5231)
2022-11-08 18:20:52,962:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0821 (5.4805)
2022-11-08 18:20:52,966:INFO: Dataset: zara2               Batch: 35/36	Loss 4.3628 (5.4506)
2022-11-08 18:20:52,969:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1608 (5.4216)
2022-11-08 18:20:54,039:INFO: - Computing loss (validation)
2022-11-08 18:21:01,941:INFO: Dataset: hotel               Batch: 1/3	Loss 16.4894 (16.4894)
2022-11-08 18:21:02,422:INFO: Dataset: hotel               Batch: 2/3	Loss 15.6664 (16.0295)
2022-11-08 18:21:02,868:INFO: Dataset: hotel               Batch: 3/3	Loss 18.7402 (16.2238)
2022-11-08 18:21:13,020:INFO: Dataset: univ                Batch: 1/6	Loss 7.9564 (7.9564)
2022-11-08 18:21:13,449:INFO: Dataset: univ                Batch: 2/6	Loss 8.0245 (7.9879)
2022-11-08 18:21:13,868:INFO: Dataset: univ                Batch: 3/6	Loss 7.9858 (7.9873)
2022-11-08 18:21:14,223:INFO: Dataset: univ                Batch: 4/6	Loss 7.8688 (7.9564)
2022-11-08 18:21:14,649:INFO: Dataset: univ                Batch: 5/6	Loss 8.7914 (8.1084)
2022-11-08 18:21:14,950:INFO: Dataset: univ                Batch: 6/6	Loss 8.8698 (8.2176)
2022-11-08 18:21:23,420:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9984 (11.9984)
2022-11-08 18:21:23,927:INFO: Dataset: zara1               Batch: 2/3	Loss 11.6790 (11.8358)
2022-11-08 18:21:24,295:INFO: Dataset: zara1               Batch: 3/3	Loss 11.5143 (11.7480)
2022-11-08 18:21:33,049:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1102 (4.1102)
2022-11-08 18:21:33,543:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2154 (4.1593)
2022-11-08 18:21:33,952:INFO: Dataset: zara2               Batch:  3/10	Loss 4.5017 (4.2656)
2022-11-08 18:21:34,207:INFO: Dataset: zara2               Batch:  4/10	Loss 4.6676 (4.3582)
2022-11-08 18:21:34,498:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2555 (4.3368)
2022-11-08 18:21:34,700:INFO: Dataset: zara2               Batch:  6/10	Loss 4.1442 (4.3055)
2022-11-08 18:21:34,703:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1109 (4.2795)
2022-11-08 18:21:34,707:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3536 (4.2886)
2022-11-08 18:21:34,711:INFO: Dataset: zara2               Batch:  9/10	Loss 4.3149 (4.2916)
2022-11-08 18:21:34,713:INFO: Dataset: zara2               Batch: 10/10	Loss 3.9622 (4.2614)
2022-11-08 18:21:35,690:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1080.pth.tar
2022-11-08 18:21:35,690:INFO: 
===> EPOCH: 1081 (P5)
2022-11-08 18:21:35,691:INFO: - Computing loss (training)
2022-11-08 18:21:43,138:INFO: Dataset: hotel               Batch: 1/8	Loss 14.8812 (14.8812)
2022-11-08 18:21:43,612:INFO: Dataset: hotel               Batch: 2/8	Loss 12.1336 (13.5201)
2022-11-08 18:21:43,936:INFO: Dataset: hotel               Batch: 3/8	Loss 15.3016 (14.0991)
2022-11-08 18:21:44,246:INFO: Dataset: hotel               Batch: 4/8	Loss 13.5025 (13.9560)
2022-11-08 18:21:44,513:INFO: Dataset: hotel               Batch: 5/8	Loss 12.1896 (13.6007)
2022-11-08 18:21:44,627:INFO: Dataset: hotel               Batch: 6/8	Loss 11.8235 (13.3031)
2022-11-08 18:21:44,630:INFO: Dataset: hotel               Batch: 7/8	Loss 12.6940 (13.2178)
2022-11-08 18:21:44,635:INFO: Dataset: hotel               Batch: 8/8	Loss 14.7174 (13.2614)
2022-11-08 18:22:11,405:INFO: Dataset: univ                Batch:  1/29	Loss 8.7061 (8.7061)
2022-11-08 18:22:11,424:INFO: Dataset: univ                Batch:  2/29	Loss 8.5864 (8.6483)
2022-11-08 18:22:11,433:INFO: Dataset: univ                Batch:  3/29	Loss 8.9716 (8.7533)
2022-11-08 18:22:11,441:INFO: Dataset: univ                Batch:  4/29	Loss 8.9444 (8.7980)
2022-11-08 18:22:11,450:INFO: Dataset: univ                Batch:  5/29	Loss 8.6278 (8.7657)
2022-11-08 18:22:11,456:INFO: Dataset: univ                Batch:  6/29	Loss 8.7179 (8.7565)
2022-11-08 18:22:11,467:INFO: Dataset: univ                Batch:  7/29	Loss 8.2332 (8.6629)
2022-11-08 18:22:11,473:INFO: Dataset: univ                Batch:  8/29	Loss 8.5508 (8.6477)
2022-11-08 18:22:11,484:INFO: Dataset: univ                Batch:  9/29	Loss 8.3620 (8.6104)
2022-11-08 18:22:11,490:INFO: Dataset: univ                Batch: 10/29	Loss 8.6000 (8.6092)
2022-11-08 18:22:11,502:INFO: Dataset: univ                Batch: 11/29	Loss 8.2208 (8.5726)
2022-11-08 18:22:11,507:INFO: Dataset: univ                Batch: 12/29	Loss 7.9234 (8.5181)
2022-11-08 18:22:11,520:INFO: Dataset: univ                Batch: 13/29	Loss 8.5257 (8.5186)
2022-11-08 18:22:11,525:INFO: Dataset: univ                Batch: 14/29	Loss 8.0215 (8.4819)
2022-11-08 18:22:11,534:INFO: Dataset: univ                Batch: 15/29	Loss 7.6186 (8.4214)
2022-11-08 18:22:11,540:INFO: Dataset: univ                Batch: 16/29	Loss 7.7275 (8.3749)
2022-11-08 18:22:11,552:INFO: Dataset: univ                Batch: 17/29	Loss 7.7963 (8.3449)
2022-11-08 18:22:11,557:INFO: Dataset: univ                Batch: 18/29	Loss 7.5469 (8.3019)
2022-11-08 18:22:11,569:INFO: Dataset: univ                Batch: 19/29	Loss 7.4721 (8.2612)
2022-11-08 18:22:11,581:INFO: Dataset: univ                Batch: 20/29	Loss 7.6918 (8.2341)
2022-11-08 18:22:11,589:INFO: Dataset: univ                Batch: 21/29	Loss 6.9631 (8.1749)
2022-11-08 18:22:11,600:INFO: Dataset: univ                Batch: 22/29	Loss 7.2925 (8.1281)
2022-11-08 18:22:11,604:INFO: Dataset: univ                Batch: 23/29	Loss 7.5191 (8.1044)
2022-11-08 18:22:11,616:INFO: Dataset: univ                Batch: 24/29	Loss 6.7424 (8.0552)
2022-11-08 18:22:11,624:INFO: Dataset: univ                Batch: 25/29	Loss 6.6436 (7.9966)
2022-11-08 18:22:11,640:INFO: Dataset: univ                Batch: 26/29	Loss 6.9191 (7.9540)
2022-11-08 18:22:11,662:INFO: Dataset: univ                Batch: 27/29	Loss 6.6522 (7.9057)
2022-11-08 18:22:11,668:INFO: Dataset: univ                Batch: 28/29	Loss 6.6495 (7.8718)
2022-11-08 18:22:11,678:INFO: Dataset: univ                Batch: 29/29	Loss 6.6660 (7.8538)
2022-11-08 18:22:20,182:INFO: Dataset: zara1               Batch:  1/16	Loss 12.9121 (12.9121)
2022-11-08 18:22:20,542:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2867 (13.0643)
2022-11-08 18:22:20,931:INFO: Dataset: zara1               Batch:  3/16	Loss 12.9810 (13.0345)
2022-11-08 18:22:21,282:INFO: Dataset: zara1               Batch:  4/16	Loss 12.4211 (12.8671)
2022-11-08 18:22:21,539:INFO: Dataset: zara1               Batch:  5/16	Loss 12.3352 (12.7530)
2022-11-08 18:22:21,906:INFO: Dataset: zara1               Batch:  6/16	Loss 12.6742 (12.7392)
2022-11-08 18:22:21,918:INFO: Dataset: zara1               Batch:  7/16	Loss 13.0221 (12.7814)
2022-11-08 18:22:21,921:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8470 (12.7892)
2022-11-08 18:22:21,927:INFO: Dataset: zara1               Batch:  9/16	Loss 12.7920 (12.7896)
2022-11-08 18:22:21,936:INFO: Dataset: zara1               Batch: 10/16	Loss 12.4108 (12.7527)
2022-11-08 18:22:21,940:INFO: Dataset: zara1               Batch: 11/16	Loss 12.6054 (12.7395)
2022-11-08 18:22:21,950:INFO: Dataset: zara1               Batch: 12/16	Loss 12.6105 (12.7284)
2022-11-08 18:22:21,953:INFO: Dataset: zara1               Batch: 13/16	Loss 11.7073 (12.6549)
2022-11-08 18:22:21,957:INFO: Dataset: zara1               Batch: 14/16	Loss 12.0374 (12.6063)
2022-11-08 18:22:21,968:INFO: Dataset: zara1               Batch: 15/16	Loss 11.8647 (12.5594)
2022-11-08 18:22:21,972:INFO: Dataset: zara1               Batch: 16/16	Loss 11.9562 (12.5334)
2022-11-08 18:22:48,274:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3277 (6.3277)
2022-11-08 18:22:48,290:INFO: Dataset: zara2               Batch:  2/36	Loss 6.5208 (6.4366)
2022-11-08 18:22:48,304:INFO: Dataset: zara2               Batch:  3/36	Loss 6.2083 (6.3574)
2022-11-08 18:22:48,308:INFO: Dataset: zara2               Batch:  4/36	Loss 6.1619 (6.3020)
2022-11-08 18:22:48,323:INFO: Dataset: zara2               Batch:  5/36	Loss 5.8606 (6.2082)
2022-11-08 18:22:48,340:INFO: Dataset: zara2               Batch:  6/36	Loss 6.1018 (6.1908)
2022-11-08 18:22:48,354:INFO: Dataset: zara2               Batch:  7/36	Loss 6.2728 (6.2019)
2022-11-08 18:22:48,363:INFO: Dataset: zara2               Batch:  8/36	Loss 6.1753 (6.1984)
2022-11-08 18:22:48,375:INFO: Dataset: zara2               Batch:  9/36	Loss 6.0464 (6.1804)
2022-11-08 18:22:48,390:INFO: Dataset: zara2               Batch: 10/36	Loss 6.2348 (6.1853)
2022-11-08 18:22:48,407:INFO: Dataset: zara2               Batch: 11/36	Loss 6.1177 (6.1777)
2022-11-08 18:22:48,422:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8456 (6.1489)
2022-11-08 18:22:48,438:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0494 (6.1409)
2022-11-08 18:22:48,450:INFO: Dataset: zara2               Batch: 14/36	Loss 5.6719 (6.1036)
2022-11-08 18:22:48,459:INFO: Dataset: zara2               Batch: 15/36	Loss 5.9189 (6.0919)
2022-11-08 18:22:48,465:INFO: Dataset: zara2               Batch: 16/36	Loss 5.2219 (6.0298)
2022-11-08 18:22:48,470:INFO: Dataset: zara2               Batch: 17/36	Loss 5.3937 (5.9942)
2022-11-08 18:22:48,473:INFO: Dataset: zara2               Batch: 18/36	Loss 5.3128 (5.9540)
2022-11-08 18:22:48,477:INFO: Dataset: zara2               Batch: 19/36	Loss 4.8636 (5.8991)
2022-11-08 18:22:48,482:INFO: Dataset: zara2               Batch: 20/36	Loss 5.6331 (5.8877)
2022-11-08 18:22:48,485:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4626 (5.8713)
2022-11-08 18:22:48,488:INFO: Dataset: zara2               Batch: 22/36	Loss 5.1369 (5.8327)
2022-11-08 18:22:48,493:INFO: Dataset: zara2               Batch: 23/36	Loss 5.1923 (5.8075)
2022-11-08 18:22:48,499:INFO: Dataset: zara2               Batch: 24/36	Loss 4.6802 (5.7599)
2022-11-08 18:22:48,503:INFO: Dataset: zara2               Batch: 25/36	Loss 4.7427 (5.7202)
2022-11-08 18:22:48,506:INFO: Dataset: zara2               Batch: 26/36	Loss 5.2946 (5.7058)
2022-11-08 18:22:48,511:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9141 (5.6756)
2022-11-08 18:22:48,516:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5709 (5.6378)
2022-11-08 18:22:48,520:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5626 (5.6063)
2022-11-08 18:22:48,524:INFO: Dataset: zara2               Batch: 30/36	Loss 4.5489 (5.5678)
2022-11-08 18:22:48,530:INFO: Dataset: zara2               Batch: 31/36	Loss 4.7583 (5.5389)
2022-11-08 18:22:48,534:INFO: Dataset: zara2               Batch: 32/36	Loss 4.3858 (5.5058)
2022-11-08 18:22:48,538:INFO: Dataset: zara2               Batch: 33/36	Loss 4.3915 (5.4689)
2022-11-08 18:22:48,541:INFO: Dataset: zara2               Batch: 34/36	Loss 4.6457 (5.4426)
2022-11-08 18:22:48,546:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2189 (5.4044)
2022-11-08 18:22:48,550:INFO: Dataset: zara2               Batch: 36/36	Loss 4.1684 (5.3780)
2022-11-08 18:22:49,663:INFO: - Computing loss (validation)
2022-11-08 18:22:56,723:INFO: Dataset: hotel               Batch: 1/3	Loss 16.8486 (16.8486)
2022-11-08 18:22:57,188:INFO: Dataset: hotel               Batch: 2/3	Loss 15.3305 (16.0282)
2022-11-08 18:22:57,568:INFO: Dataset: hotel               Batch: 3/3	Loss 13.6070 (15.8546)
2022-11-08 18:23:06,607:INFO: Dataset: univ                Batch: 1/6	Loss 8.5658 (8.5658)
2022-11-08 18:23:07,072:INFO: Dataset: univ                Batch: 2/6	Loss 7.7960 (8.1385)
2022-11-08 18:23:07,443:INFO: Dataset: univ                Batch: 3/6	Loss 8.3243 (8.1997)
2022-11-08 18:23:07,922:INFO: Dataset: univ                Batch: 4/6	Loss 8.9059 (8.3310)
2022-11-08 18:23:08,135:INFO: Dataset: univ                Batch: 5/6	Loss 7.9770 (8.2572)
2022-11-08 18:23:08,354:INFO: Dataset: univ                Batch: 6/6	Loss 7.7810 (8.1727)
2022-11-08 18:23:16,491:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9560 (11.9560)
2022-11-08 18:23:16,906:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8311 (11.8930)
2022-11-08 18:23:17,303:INFO: Dataset: zara1               Batch: 3/3	Loss 11.6055 (11.8255)
2022-11-08 18:23:26,357:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2964 (4.2964)
2022-11-08 18:23:26,821:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1617 (4.2292)
2022-11-08 18:23:27,252:INFO: Dataset: zara2               Batch:  3/10	Loss 4.3748 (4.2764)
2022-11-08 18:23:27,587:INFO: Dataset: zara2               Batch:  4/10	Loss 4.5032 (4.3287)
2022-11-08 18:23:27,900:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1652 (4.2970)
2022-11-08 18:23:28,152:INFO: Dataset: zara2               Batch:  6/10	Loss 4.3042 (4.2984)
2022-11-08 18:23:28,154:INFO: Dataset: zara2               Batch:  7/10	Loss 4.2681 (4.2940)
2022-11-08 18:23:28,156:INFO: Dataset: zara2               Batch:  8/10	Loss 4.2972 (4.2944)
2022-11-08 18:23:28,159:INFO: Dataset: zara2               Batch:  9/10	Loss 4.3536 (4.3015)
2022-11-08 18:23:28,174:INFO: Dataset: zara2               Batch: 10/10	Loss 4.3962 (4.3104)
2022-11-08 18:23:29,280:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1081.pth.tar
2022-11-08 18:23:29,280:INFO: 
===> EPOCH: 1082 (P5)
2022-11-08 18:23:29,280:INFO: - Computing loss (training)
2022-11-08 18:23:36,375:INFO: Dataset: hotel               Batch: 1/8	Loss 12.9801 (12.9801)
2022-11-08 18:23:36,846:INFO: Dataset: hotel               Batch: 2/8	Loss 13.0733 (13.0242)
2022-11-08 18:23:37,224:INFO: Dataset: hotel               Batch: 3/8	Loss 14.4242 (13.5308)
2022-11-08 18:23:37,572:INFO: Dataset: hotel               Batch: 4/8	Loss 13.2570 (13.4634)
2022-11-08 18:23:37,935:INFO: Dataset: hotel               Batch: 5/8	Loss 12.7769 (13.3352)
2022-11-08 18:23:38,075:INFO: Dataset: hotel               Batch: 6/8	Loss 11.8792 (13.0709)
2022-11-08 18:23:38,090:INFO: Dataset: hotel               Batch: 7/8	Loss 13.3133 (13.1064)
2022-11-08 18:23:38,103:INFO: Dataset: hotel               Batch: 8/8	Loss 10.2261 (13.0228)
2022-11-08 18:24:04,119:INFO: Dataset: univ                Batch:  1/29	Loss 8.8977 (8.8977)
2022-11-08 18:24:04,124:INFO: Dataset: univ                Batch:  2/29	Loss 9.1596 (9.0159)
2022-11-08 18:24:04,138:INFO: Dataset: univ                Batch:  3/29	Loss 8.4323 (8.8106)
2022-11-08 18:24:04,156:INFO: Dataset: univ                Batch:  4/29	Loss 8.4237 (8.7194)
2022-11-08 18:24:04,170:INFO: Dataset: univ                Batch:  5/29	Loss 8.9177 (8.7604)
2022-11-08 18:24:04,180:INFO: Dataset: univ                Batch:  6/29	Loss 8.6090 (8.7357)
2022-11-08 18:24:04,185:INFO: Dataset: univ                Batch:  7/29	Loss 8.5341 (8.7083)
2022-11-08 18:24:04,197:INFO: Dataset: univ                Batch:  8/29	Loss 8.7070 (8.7081)
2022-11-08 18:24:04,202:INFO: Dataset: univ                Batch:  9/29	Loss 8.3471 (8.6655)
2022-11-08 18:24:04,207:INFO: Dataset: univ                Batch: 10/29	Loss 8.4327 (8.6421)
2022-11-08 18:24:04,211:INFO: Dataset: univ                Batch: 11/29	Loss 8.5376 (8.6336)
2022-11-08 18:24:04,216:INFO: Dataset: univ                Batch: 12/29	Loss 8.3601 (8.6112)
2022-11-08 18:24:04,219:INFO: Dataset: univ                Batch: 13/29	Loss 8.6083 (8.6110)
2022-11-08 18:24:04,225:INFO: Dataset: univ                Batch: 14/29	Loss 7.9274 (8.5616)
2022-11-08 18:24:04,232:INFO: Dataset: univ                Batch: 15/29	Loss 7.7559 (8.5050)
2022-11-08 18:24:04,236:INFO: Dataset: univ                Batch: 16/29	Loss 8.0033 (8.4757)
2022-11-08 18:24:04,241:INFO: Dataset: univ                Batch: 17/29	Loss 7.3342 (8.4097)
2022-11-08 18:24:04,244:INFO: Dataset: univ                Batch: 18/29	Loss 7.6247 (8.3721)
2022-11-08 18:24:04,251:INFO: Dataset: univ                Batch: 19/29	Loss 7.4293 (8.3158)
2022-11-08 18:24:04,256:INFO: Dataset: univ                Batch: 20/29	Loss 7.6440 (8.2875)
2022-11-08 18:24:04,262:INFO: Dataset: univ                Batch: 21/29	Loss 7.1309 (8.2346)
2022-11-08 18:24:04,267:INFO: Dataset: univ                Batch: 22/29	Loss 6.6023 (8.1619)
2022-11-08 18:24:04,272:INFO: Dataset: univ                Batch: 23/29	Loss 6.9981 (8.1099)
2022-11-08 18:24:04,277:INFO: Dataset: univ                Batch: 24/29	Loss 6.6364 (8.0533)
2022-11-08 18:24:04,283:INFO: Dataset: univ                Batch: 25/29	Loss 6.6628 (7.9951)
2022-11-08 18:24:04,289:INFO: Dataset: univ                Batch: 26/29	Loss 6.2346 (7.9157)
2022-11-08 18:24:04,294:INFO: Dataset: univ                Batch: 27/29	Loss 6.6943 (7.8731)
2022-11-08 18:24:04,298:INFO: Dataset: univ                Batch: 28/29	Loss 6.2998 (7.8152)
2022-11-08 18:24:04,304:INFO: Dataset: univ                Batch: 29/29	Loss 6.4948 (7.7997)
2022-11-08 18:24:12,469:INFO: Dataset: zara1               Batch:  1/16	Loss 13.0254 (13.0254)
2022-11-08 18:24:12,957:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2481 (13.1377)
2022-11-08 18:24:13,323:INFO: Dataset: zara1               Batch:  3/16	Loss 13.8144 (13.3327)
2022-11-08 18:24:13,701:INFO: Dataset: zara1               Batch:  4/16	Loss 12.7783 (13.1908)
2022-11-08 18:24:13,984:INFO: Dataset: zara1               Batch:  5/16	Loss 12.4774 (13.0444)
2022-11-08 18:24:14,238:INFO: Dataset: zara1               Batch:  6/16	Loss 13.1219 (13.0570)
2022-11-08 18:24:14,242:INFO: Dataset: zara1               Batch:  7/16	Loss 12.9080 (13.0347)
2022-11-08 18:24:14,251:INFO: Dataset: zara1               Batch:  8/16	Loss 12.4495 (12.9585)
2022-11-08 18:24:14,256:INFO: Dataset: zara1               Batch:  9/16	Loss 12.4395 (12.8996)
2022-11-08 18:24:14,265:INFO: Dataset: zara1               Batch: 10/16	Loss 11.9860 (12.8029)
2022-11-08 18:24:14,271:INFO: Dataset: zara1               Batch: 11/16	Loss 12.6595 (12.7898)
2022-11-08 18:24:14,275:INFO: Dataset: zara1               Batch: 12/16	Loss 12.0317 (12.7302)
2022-11-08 18:24:14,286:INFO: Dataset: zara1               Batch: 13/16	Loss 11.8090 (12.6558)
2022-11-08 18:24:14,289:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3657 (12.6287)
2022-11-08 18:24:14,292:INFO: Dataset: zara1               Batch: 15/16	Loss 12.3736 (12.6093)
2022-11-08 18:24:14,302:INFO: Dataset: zara1               Batch: 16/16	Loss 12.0943 (12.5871)
2022-11-08 18:24:40,331:INFO: Dataset: zara2               Batch:  1/36	Loss 6.7093 (6.7093)
2022-11-08 18:24:40,338:INFO: Dataset: zara2               Batch:  2/36	Loss 6.5842 (6.6503)
2022-11-08 18:24:40,358:INFO: Dataset: zara2               Batch:  3/36	Loss 6.6028 (6.6357)
2022-11-08 18:24:40,373:INFO: Dataset: zara2               Batch:  4/36	Loss 6.5557 (6.6148)
2022-11-08 18:24:40,392:INFO: Dataset: zara2               Batch:  5/36	Loss 6.2886 (6.5553)
2022-11-08 18:24:40,409:INFO: Dataset: zara2               Batch:  6/36	Loss 6.6055 (6.5630)
2022-11-08 18:24:40,424:INFO: Dataset: zara2               Batch:  7/36	Loss 6.1157 (6.4976)
2022-11-08 18:24:40,433:INFO: Dataset: zara2               Batch:  8/36	Loss 5.9622 (6.4242)
2022-11-08 18:24:40,438:INFO: Dataset: zara2               Batch:  9/36	Loss 5.8885 (6.3736)
2022-11-08 18:24:40,449:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6383 (6.2997)
2022-11-08 18:24:40,453:INFO: Dataset: zara2               Batch: 11/36	Loss 6.5393 (6.3215)
2022-11-08 18:24:40,458:INFO: Dataset: zara2               Batch: 12/36	Loss 6.2313 (6.3144)
2022-11-08 18:24:40,466:INFO: Dataset: zara2               Batch: 13/36	Loss 5.4554 (6.2536)
2022-11-08 18:24:40,471:INFO: Dataset: zara2               Batch: 14/36	Loss 5.5412 (6.2094)
2022-11-08 18:24:40,475:INFO: Dataset: zara2               Batch: 15/36	Loss 5.2111 (6.1503)
2022-11-08 18:24:40,483:INFO: Dataset: zara2               Batch: 16/36	Loss 5.6996 (6.1194)
2022-11-08 18:24:40,488:INFO: Dataset: zara2               Batch: 17/36	Loss 5.2951 (6.0677)
2022-11-08 18:24:40,493:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6560 (6.0433)
2022-11-08 18:24:40,501:INFO: Dataset: zara2               Batch: 19/36	Loss 5.1066 (5.9961)
2022-11-08 18:24:40,505:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0833 (5.9498)
2022-11-08 18:24:40,509:INFO: Dataset: zara2               Batch: 21/36	Loss 4.7871 (5.8857)
2022-11-08 18:24:40,519:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0726 (5.8463)
2022-11-08 18:24:40,525:INFO: Dataset: zara2               Batch: 23/36	Loss 5.0034 (5.8076)
2022-11-08 18:24:40,534:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7289 (5.7706)
2022-11-08 18:24:40,538:INFO: Dataset: zara2               Batch: 25/36	Loss 4.7811 (5.7302)
2022-11-08 18:24:40,546:INFO: Dataset: zara2               Batch: 26/36	Loss 4.7996 (5.6849)
2022-11-08 18:24:40,552:INFO: Dataset: zara2               Batch: 27/36	Loss 4.9895 (5.6588)
2022-11-08 18:24:40,556:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7925 (5.6272)
2022-11-08 18:24:40,561:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6659 (5.5892)
2022-11-08 18:24:40,570:INFO: Dataset: zara2               Batch: 30/36	Loss 4.6149 (5.5526)
2022-11-08 18:24:40,577:INFO: Dataset: zara2               Batch: 31/36	Loss 4.5751 (5.5229)
2022-11-08 18:24:40,588:INFO: Dataset: zara2               Batch: 32/36	Loss 4.2349 (5.4803)
2022-11-08 18:24:40,601:INFO: Dataset: zara2               Batch: 33/36	Loss 4.3639 (5.4479)
2022-11-08 18:24:40,605:INFO: Dataset: zara2               Batch: 34/36	Loss 4.5010 (5.4227)
2022-11-08 18:24:40,612:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2336 (5.3935)
2022-11-08 18:24:40,620:INFO: Dataset: zara2               Batch: 36/36	Loss 4.3338 (5.3685)
2022-11-08 18:24:41,681:INFO: - Computing loss (validation)
2022-11-08 18:24:48,603:INFO: Dataset: hotel               Batch: 1/3	Loss 16.3002 (16.3002)
2022-11-08 18:24:49,159:INFO: Dataset: hotel               Batch: 2/3	Loss 16.4479 (16.3694)
2022-11-08 18:24:49,581:INFO: Dataset: hotel               Batch: 3/3	Loss 14.8196 (16.2425)
2022-11-08 18:24:58,358:INFO: Dataset: univ                Batch: 1/6	Loss 7.9923 (7.9923)
2022-11-08 18:24:58,845:INFO: Dataset: univ                Batch: 2/6	Loss 8.3706 (8.1673)
2022-11-08 18:24:59,236:INFO: Dataset: univ                Batch: 3/6	Loss 8.4220 (8.2475)
2022-11-08 18:24:59,519:INFO: Dataset: univ                Batch: 4/6	Loss 7.6560 (8.0657)
2022-11-08 18:24:59,820:INFO: Dataset: univ                Batch: 5/6	Loss 7.9774 (8.0472)
2022-11-08 18:25:00,170:INFO: Dataset: univ                Batch: 6/6	Loss 9.0043 (8.1700)
2022-11-08 18:25:08,104:INFO: Dataset: zara1               Batch: 1/3	Loss 11.9885 (11.9885)
2022-11-08 18:25:08,604:INFO: Dataset: zara1               Batch: 2/3	Loss 11.3611 (11.6867)
2022-11-08 18:25:09,090:INFO: Dataset: zara1               Batch: 3/3	Loss 12.1996 (11.8087)
2022-11-08 18:25:17,853:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2235 (4.2235)
2022-11-08 18:25:18,268:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2667 (4.2457)
2022-11-08 18:25:18,659:INFO: Dataset: zara2               Batch:  3/10	Loss 4.2319 (4.2415)
2022-11-08 18:25:19,007:INFO: Dataset: zara2               Batch:  4/10	Loss 4.3224 (4.2615)
2022-11-08 18:25:19,357:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3477 (4.2790)
2022-11-08 18:25:19,573:INFO: Dataset: zara2               Batch:  6/10	Loss 4.4066 (4.3012)
2022-11-08 18:25:19,574:INFO: Dataset: zara2               Batch:  7/10	Loss 4.0677 (4.2698)
2022-11-08 18:25:19,576:INFO: Dataset: zara2               Batch:  8/10	Loss 4.1203 (4.2525)
2022-11-08 18:25:19,583:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2958 (4.2573)
2022-11-08 18:25:19,592:INFO: Dataset: zara2               Batch: 10/10	Loss 4.0311 (4.2357)
2022-11-08 18:25:20,722:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1082.pth.tar
2022-11-08 18:25:20,722:INFO: 
===> EPOCH: 1083 (P5)
2022-11-08 18:25:20,723:INFO: - Computing loss (training)
2022-11-08 18:25:27,789:INFO: Dataset: hotel               Batch: 1/8	Loss 13.6839 (13.6839)
2022-11-08 18:25:28,192:INFO: Dataset: hotel               Batch: 2/8	Loss 13.9818 (13.8198)
2022-11-08 18:25:28,557:INFO: Dataset: hotel               Batch: 3/8	Loss 14.1067 (13.9136)
2022-11-08 18:25:28,925:INFO: Dataset: hotel               Batch: 4/8	Loss 13.1324 (13.7112)
2022-11-08 18:25:29,156:INFO: Dataset: hotel               Batch: 5/8	Loss 14.4526 (13.8649)
2022-11-08 18:25:29,421:INFO: Dataset: hotel               Batch: 6/8	Loss 11.9668 (13.5541)
2022-11-08 18:25:29,435:INFO: Dataset: hotel               Batch: 7/8	Loss 10.6577 (13.1195)
2022-11-08 18:25:29,443:INFO: Dataset: hotel               Batch: 8/8	Loss 15.2473 (13.1896)
2022-11-08 18:25:57,159:INFO: Dataset: univ                Batch:  1/29	Loss 9.0171 (9.0171)
2022-11-08 18:25:57,163:INFO: Dataset: univ                Batch:  2/29	Loss 9.2137 (9.1155)
2022-11-08 18:25:57,169:INFO: Dataset: univ                Batch:  3/29	Loss 8.8276 (9.0151)
2022-11-08 18:25:57,174:INFO: Dataset: univ                Batch:  4/29	Loss 9.1314 (9.0435)
2022-11-08 18:25:57,183:INFO: Dataset: univ                Batch:  5/29	Loss 8.7248 (8.9770)
2022-11-08 18:25:57,204:INFO: Dataset: univ                Batch:  6/29	Loss 8.2945 (8.8578)
2022-11-08 18:25:57,209:INFO: Dataset: univ                Batch:  7/29	Loss 8.6925 (8.8331)
2022-11-08 18:25:57,215:INFO: Dataset: univ                Batch:  8/29	Loss 8.4228 (8.7802)
2022-11-08 18:25:57,220:INFO: Dataset: univ                Batch:  9/29	Loss 8.6135 (8.7642)
2022-11-08 18:25:57,224:INFO: Dataset: univ                Batch: 10/29	Loss 8.6862 (8.7560)
2022-11-08 18:25:57,236:INFO: Dataset: univ                Batch: 11/29	Loss 8.8760 (8.7665)
2022-11-08 18:25:57,242:INFO: Dataset: univ                Batch: 12/29	Loss 8.1007 (8.7078)
2022-11-08 18:25:57,250:INFO: Dataset: univ                Batch: 13/29	Loss 8.8046 (8.7140)
2022-11-08 18:25:57,256:INFO: Dataset: univ                Batch: 14/29	Loss 8.0954 (8.6655)
2022-11-08 18:25:57,267:INFO: Dataset: univ                Batch: 15/29	Loss 7.8641 (8.6082)
2022-11-08 18:25:57,273:INFO: Dataset: univ                Batch: 16/29	Loss 7.3992 (8.5171)
2022-11-08 18:25:57,284:INFO: Dataset: univ                Batch: 17/29	Loss 7.4803 (8.4436)
2022-11-08 18:25:57,288:INFO: Dataset: univ                Batch: 18/29	Loss 7.4756 (8.3851)
2022-11-08 18:25:57,292:INFO: Dataset: univ                Batch: 19/29	Loss 7.3207 (8.3202)
2022-11-08 18:25:57,301:INFO: Dataset: univ                Batch: 20/29	Loss 7.1800 (8.2522)
2022-11-08 18:25:57,305:INFO: Dataset: univ                Batch: 21/29	Loss 7.4530 (8.2181)
2022-11-08 18:25:57,310:INFO: Dataset: univ                Batch: 22/29	Loss 7.0642 (8.1631)
2022-11-08 18:25:57,317:INFO: Dataset: univ                Batch: 23/29	Loss 6.8271 (8.1026)
2022-11-08 18:25:57,322:INFO: Dataset: univ                Batch: 24/29	Loss 6.9383 (8.0527)
2022-11-08 18:25:57,326:INFO: Dataset: univ                Batch: 25/29	Loss 6.7871 (7.9925)
2022-11-08 18:25:57,334:INFO: Dataset: univ                Batch: 26/29	Loss 6.4913 (7.9326)
2022-11-08 18:25:57,339:INFO: Dataset: univ                Batch: 27/29	Loss 6.6343 (7.8915)
2022-11-08 18:25:57,348:INFO: Dataset: univ                Batch: 28/29	Loss 6.2184 (7.8285)
2022-11-08 18:25:57,352:INFO: Dataset: univ                Batch: 29/29	Loss 6.1828 (7.8053)
2022-11-08 18:26:04,938:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8513 (12.8513)
2022-11-08 18:26:05,305:INFO: Dataset: zara1               Batch:  2/16	Loss 13.5540 (13.1610)
2022-11-08 18:26:05,630:INFO: Dataset: zara1               Batch:  3/16	Loss 13.1849 (13.1695)
2022-11-08 18:26:06,037:INFO: Dataset: zara1               Batch:  4/16	Loss 12.8410 (13.1059)
2022-11-08 18:26:06,247:INFO: Dataset: zara1               Batch:  5/16	Loss 12.4853 (12.9814)
2022-11-08 18:26:06,489:INFO: Dataset: zara1               Batch:  6/16	Loss 12.4774 (12.8924)
2022-11-08 18:26:06,492:INFO: Dataset: zara1               Batch:  7/16	Loss 12.5931 (12.8500)
2022-11-08 18:26:06,497:INFO: Dataset: zara1               Batch:  8/16	Loss 12.2317 (12.7681)
2022-11-08 18:26:06,501:INFO: Dataset: zara1               Batch:  9/16	Loss 12.4254 (12.7234)
2022-11-08 18:26:06,504:INFO: Dataset: zara1               Batch: 10/16	Loss 12.2885 (12.6829)
2022-11-08 18:26:06,508:INFO: Dataset: zara1               Batch: 11/16	Loss 12.3446 (12.6516)
2022-11-08 18:26:06,512:INFO: Dataset: zara1               Batch: 12/16	Loss 12.0526 (12.6058)
2022-11-08 18:26:06,516:INFO: Dataset: zara1               Batch: 13/16	Loss 12.4506 (12.5938)
2022-11-08 18:26:06,519:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4661 (12.5835)
2022-11-08 18:26:06,522:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0453 (12.5566)
2022-11-08 18:26:06,525:INFO: Dataset: zara1               Batch: 16/16	Loss 11.8766 (12.5208)
2022-11-08 18:26:31,777:INFO: Dataset: zara2               Batch:  1/36	Loss 6.0465 (6.0465)
2022-11-08 18:26:31,780:INFO: Dataset: zara2               Batch:  2/36	Loss 5.9270 (5.9826)
2022-11-08 18:26:31,784:INFO: Dataset: zara2               Batch:  3/36	Loss 6.5535 (6.1704)
2022-11-08 18:26:31,787:INFO: Dataset: zara2               Batch:  4/36	Loss 6.1440 (6.1639)
2022-11-08 18:26:31,790:INFO: Dataset: zara2               Batch:  5/36	Loss 6.0996 (6.1527)
2022-11-08 18:26:31,815:INFO: Dataset: zara2               Batch:  6/36	Loss 6.4229 (6.1976)
2022-11-08 18:26:31,819:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4169 (6.2288)
2022-11-08 18:26:31,822:INFO: Dataset: zara2               Batch:  8/36	Loss 6.3648 (6.2438)
2022-11-08 18:26:31,825:INFO: Dataset: zara2               Batch:  9/36	Loss 6.2834 (6.2481)
2022-11-08 18:26:31,829:INFO: Dataset: zara2               Batch: 10/36	Loss 5.7161 (6.1963)
2022-11-08 18:26:31,832:INFO: Dataset: zara2               Batch: 11/36	Loss 5.8334 (6.1609)
2022-11-08 18:26:31,836:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8962 (6.1396)
2022-11-08 18:26:31,838:INFO: Dataset: zara2               Batch: 13/36	Loss 6.3121 (6.1549)
2022-11-08 18:26:31,842:INFO: Dataset: zara2               Batch: 14/36	Loss 5.9104 (6.1365)
2022-11-08 18:26:31,844:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5075 (6.0932)
2022-11-08 18:26:31,846:INFO: Dataset: zara2               Batch: 16/36	Loss 5.2796 (6.0464)
2022-11-08 18:26:31,850:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6953 (6.0267)
2022-11-08 18:26:31,853:INFO: Dataset: zara2               Batch: 18/36	Loss 5.5662 (6.0020)
2022-11-08 18:26:31,856:INFO: Dataset: zara2               Batch: 19/36	Loss 5.3031 (5.9596)
2022-11-08 18:26:31,859:INFO: Dataset: zara2               Batch: 20/36	Loss 5.3736 (5.9290)
2022-11-08 18:26:31,862:INFO: Dataset: zara2               Batch: 21/36	Loss 5.0058 (5.8790)
2022-11-08 18:26:31,866:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9518 (5.8365)
2022-11-08 18:26:31,870:INFO: Dataset: zara2               Batch: 23/36	Loss 4.8445 (5.7899)
2022-11-08 18:26:31,872:INFO: Dataset: zara2               Batch: 24/36	Loss 5.0933 (5.7642)
2022-11-08 18:26:31,874:INFO: Dataset: zara2               Batch: 25/36	Loss 4.7654 (5.7237)
2022-11-08 18:26:31,878:INFO: Dataset: zara2               Batch: 26/36	Loss 5.0449 (5.6977)
2022-11-08 18:26:31,881:INFO: Dataset: zara2               Batch: 27/36	Loss 4.7164 (5.6567)
2022-11-08 18:26:31,884:INFO: Dataset: zara2               Batch: 28/36	Loss 4.8788 (5.6284)
2022-11-08 18:26:31,887:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5526 (5.5966)
2022-11-08 18:26:31,889:INFO: Dataset: zara2               Batch: 30/36	Loss 4.2549 (5.5541)
2022-11-08 18:26:31,891:INFO: Dataset: zara2               Batch: 31/36	Loss 4.1727 (5.5155)
2022-11-08 18:26:31,895:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4055 (5.4791)
2022-11-08 18:26:31,898:INFO: Dataset: zara2               Batch: 33/36	Loss 4.4818 (5.4487)
2022-11-08 18:26:31,900:INFO: Dataset: zara2               Batch: 34/36	Loss 4.3578 (5.4153)
2022-11-08 18:26:31,904:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2178 (5.3833)
2022-11-08 18:26:31,909:INFO: Dataset: zara2               Batch: 36/36	Loss 3.9386 (5.3577)
2022-11-08 18:26:32,954:INFO: - Computing loss (validation)
2022-11-08 18:26:40,228:INFO: Dataset: hotel               Batch: 1/3	Loss 15.2616 (15.2616)
2022-11-08 18:26:40,574:INFO: Dataset: hotel               Batch: 2/3	Loss 16.2963 (15.7282)
2022-11-08 18:26:40,925:INFO: Dataset: hotel               Batch: 3/3	Loss 17.4234 (15.8323)
2022-11-08 18:26:53,807:INFO: Dataset: univ                Batch: 1/6	Loss 8.1792 (8.1792)
2022-11-08 18:26:54,160:INFO: Dataset: univ                Batch: 2/6	Loss 8.5339 (8.3639)
2022-11-08 18:26:54,873:INFO: Dataset: univ                Batch: 3/6	Loss 7.9834 (8.2307)
2022-11-08 18:26:55,035:INFO: Dataset: univ                Batch: 4/6	Loss 7.8095 (8.1099)
2022-11-08 18:26:55,355:INFO: Dataset: univ                Batch: 5/6	Loss 7.9550 (8.0771)
2022-11-08 18:26:55,664:INFO: Dataset: univ                Batch: 6/6	Loss 8.8083 (8.1633)
2022-11-08 18:27:05,800:INFO: Dataset: zara1               Batch: 1/3	Loss 11.7378 (11.7378)
2022-11-08 18:27:06,013:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8976 (11.8174)
2022-11-08 18:27:06,410:INFO: Dataset: zara1               Batch: 3/3	Loss 12.1198 (11.8991)
2022-11-08 18:27:15,035:INFO: Dataset: zara2               Batch:  1/10	Loss 4.1520 (4.1520)
2022-11-08 18:27:15,537:INFO: Dataset: zara2               Batch:  2/10	Loss 4.1677 (4.1596)
2022-11-08 18:27:15,915:INFO: Dataset: zara2               Batch:  3/10	Loss 4.0810 (4.1335)
2022-11-08 18:27:16,253:INFO: Dataset: zara2               Batch:  4/10	Loss 4.5644 (4.2299)
2022-11-08 18:27:16,570:INFO: Dataset: zara2               Batch:  5/10	Loss 4.1416 (4.2147)
2022-11-08 18:27:16,657:INFO: Dataset: zara2               Batch:  6/10	Loss 4.3705 (4.2374)
2022-11-08 18:27:16,659:INFO: Dataset: zara2               Batch:  7/10	Loss 4.4753 (4.2687)
2022-11-08 18:27:16,662:INFO: Dataset: zara2               Batch:  8/10	Loss 4.1096 (4.2462)
2022-11-08 18:27:16,664:INFO: Dataset: zara2               Batch:  9/10	Loss 4.2219 (4.2433)
2022-11-08 18:27:16,666:INFO: Dataset: zara2               Batch: 10/10	Loss 4.5545 (4.2739)
2022-11-08 18:27:17,713:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1083.pth.tar
2022-11-08 18:27:17,713:INFO: 
===> EPOCH: 1084 (P5)
2022-11-08 18:27:17,713:INFO: - Computing loss (training)
2022-11-08 18:27:24,915:INFO: Dataset: hotel               Batch: 1/8	Loss 11.7646 (11.7646)
2022-11-08 18:27:25,401:INFO: Dataset: hotel               Batch: 2/8	Loss 13.4055 (12.5697)
2022-11-08 18:27:25,779:INFO: Dataset: hotel               Batch: 3/8	Loss 14.2397 (13.1333)
2022-11-08 18:27:26,094:INFO: Dataset: hotel               Batch: 4/8	Loss 10.7959 (12.5977)
2022-11-08 18:27:26,397:INFO: Dataset: hotel               Batch: 5/8	Loss 12.9641 (12.6723)
2022-11-08 18:27:26,697:INFO: Dataset: hotel               Batch: 6/8	Loss 12.9612 (12.7223)
2022-11-08 18:27:26,702:INFO: Dataset: hotel               Batch: 7/8	Loss 14.5441 (12.9698)
2022-11-08 18:27:26,707:INFO: Dataset: hotel               Batch: 8/8	Loss 13.8455 (12.9952)
2022-11-08 18:27:52,695:INFO: Dataset: univ                Batch:  1/29	Loss 8.4650 (8.4650)
2022-11-08 18:27:52,704:INFO: Dataset: univ                Batch:  2/29	Loss 8.8722 (8.6571)
2022-11-08 18:27:52,710:INFO: Dataset: univ                Batch:  3/29	Loss 9.1036 (8.8006)
2022-11-08 18:27:52,721:INFO: Dataset: univ                Batch:  4/29	Loss 8.6343 (8.7543)
2022-11-08 18:27:52,729:INFO: Dataset: univ                Batch:  5/29	Loss 8.5722 (8.7187)
2022-11-08 18:27:52,735:INFO: Dataset: univ                Batch:  6/29	Loss 8.3691 (8.6680)
2022-11-08 18:27:52,739:INFO: Dataset: univ                Batch:  7/29	Loss 8.7934 (8.6865)
2022-11-08 18:27:52,745:INFO: Dataset: univ                Batch:  8/29	Loss 8.5469 (8.6690)
2022-11-08 18:27:52,753:INFO: Dataset: univ                Batch:  9/29	Loss 8.1547 (8.6105)
2022-11-08 18:27:52,757:INFO: Dataset: univ                Batch: 10/29	Loss 8.3180 (8.5792)
2022-11-08 18:27:52,760:INFO: Dataset: univ                Batch: 11/29	Loss 8.0467 (8.5281)
2022-11-08 18:27:52,769:INFO: Dataset: univ                Batch: 12/29	Loss 8.1875 (8.4991)
2022-11-08 18:27:52,773:INFO: Dataset: univ                Batch: 13/29	Loss 8.0714 (8.4647)
2022-11-08 18:27:52,776:INFO: Dataset: univ                Batch: 14/29	Loss 8.1647 (8.4446)
2022-11-08 18:27:52,784:INFO: Dataset: univ                Batch: 15/29	Loss 7.6896 (8.3987)
2022-11-08 18:27:52,788:INFO: Dataset: univ                Batch: 16/29	Loss 8.0039 (8.3758)
2022-11-08 18:27:52,792:INFO: Dataset: univ                Batch: 17/29	Loss 7.4550 (8.3209)
2022-11-08 18:27:52,799:INFO: Dataset: univ                Batch: 18/29	Loss 7.3323 (8.2642)
2022-11-08 18:27:52,806:INFO: Dataset: univ                Batch: 19/29	Loss 7.3410 (8.2121)
2022-11-08 18:27:52,810:INFO: Dataset: univ                Batch: 20/29	Loss 7.4305 (8.1747)
2022-11-08 18:27:52,816:INFO: Dataset: univ                Batch: 21/29	Loss 6.9447 (8.1156)
2022-11-08 18:27:52,822:INFO: Dataset: univ                Batch: 22/29	Loss 6.8089 (8.0553)
2022-11-08 18:27:52,826:INFO: Dataset: univ                Batch: 23/29	Loss 7.0477 (8.0175)
2022-11-08 18:27:52,833:INFO: Dataset: univ                Batch: 24/29	Loss 7.2502 (7.9889)
2022-11-08 18:27:52,836:INFO: Dataset: univ                Batch: 25/29	Loss 6.5515 (7.9358)
2022-11-08 18:27:52,840:INFO: Dataset: univ                Batch: 26/29	Loss 6.7873 (7.8996)
2022-11-08 18:27:52,844:INFO: Dataset: univ                Batch: 27/29	Loss 6.6611 (7.8572)
2022-11-08 18:27:52,852:INFO: Dataset: univ                Batch: 28/29	Loss 6.4322 (7.8041)
2022-11-08 18:27:52,858:INFO: Dataset: univ                Batch: 29/29	Loss 5.8528 (7.7731)
2022-11-08 18:28:00,575:INFO: Dataset: zara1               Batch:  1/16	Loss 12.4876 (12.4876)
2022-11-08 18:28:00,923:INFO: Dataset: zara1               Batch:  2/16	Loss 12.8703 (12.6781)
2022-11-08 18:28:01,243:INFO: Dataset: zara1               Batch:  3/16	Loss 13.1519 (12.8328)
2022-11-08 18:28:01,530:INFO: Dataset: zara1               Batch:  4/16	Loss 13.4453 (12.9846)
2022-11-08 18:28:01,853:INFO: Dataset: zara1               Batch:  5/16	Loss 12.7733 (12.9460)
2022-11-08 18:28:02,069:INFO: Dataset: zara1               Batch:  6/16	Loss 12.7696 (12.9152)
2022-11-08 18:28:02,072:INFO: Dataset: zara1               Batch:  7/16	Loss 12.4839 (12.8488)
2022-11-08 18:28:02,075:INFO: Dataset: zara1               Batch:  8/16	Loss 12.6049 (12.8129)
2022-11-08 18:28:02,081:INFO: Dataset: zara1               Batch:  9/16	Loss 12.6871 (12.7976)
2022-11-08 18:28:02,086:INFO: Dataset: zara1               Batch: 10/16	Loss 12.3958 (12.7576)
2022-11-08 18:28:02,092:INFO: Dataset: zara1               Batch: 11/16	Loss 12.9622 (12.7763)
2022-11-08 18:28:02,100:INFO: Dataset: zara1               Batch: 12/16	Loss 12.1451 (12.7239)
2022-11-08 18:28:02,103:INFO: Dataset: zara1               Batch: 13/16	Loss 11.9968 (12.6625)
2022-11-08 18:28:02,107:INFO: Dataset: zara1               Batch: 14/16	Loss 11.8762 (12.5957)
2022-11-08 18:28:02,112:INFO: Dataset: zara1               Batch: 15/16	Loss 12.0066 (12.5580)
2022-11-08 18:28:02,120:INFO: Dataset: zara1               Batch: 16/16	Loss 11.3627 (12.5027)
2022-11-08 18:28:27,638:INFO: Dataset: zara2               Batch:  1/36	Loss 6.4827 (6.4827)
2022-11-08 18:28:27,645:INFO: Dataset: zara2               Batch:  2/36	Loss 7.0613 (6.7728)
2022-11-08 18:28:27,655:INFO: Dataset: zara2               Batch:  3/36	Loss 7.0154 (6.8595)
2022-11-08 18:28:27,660:INFO: Dataset: zara2               Batch:  4/36	Loss 6.4135 (6.7576)
2022-11-08 18:28:27,670:INFO: Dataset: zara2               Batch:  5/36	Loss 6.7734 (6.7604)
2022-11-08 18:28:27,674:INFO: Dataset: zara2               Batch:  6/36	Loss 6.7303 (6.7562)
2022-11-08 18:28:27,678:INFO: Dataset: zara2               Batch:  7/36	Loss 6.3446 (6.6962)
2022-11-08 18:28:27,687:INFO: Dataset: zara2               Batch:  8/36	Loss 6.4379 (6.6587)
2022-11-08 18:28:27,691:INFO: Dataset: zara2               Batch:  9/36	Loss 6.1845 (6.6045)
2022-11-08 18:28:27,694:INFO: Dataset: zara2               Batch: 10/36	Loss 5.6776 (6.5100)
2022-11-08 18:28:27,704:INFO: Dataset: zara2               Batch: 11/36	Loss 5.7570 (6.4414)
2022-11-08 18:28:27,708:INFO: Dataset: zara2               Batch: 12/36	Loss 6.0372 (6.4097)
2022-11-08 18:28:27,711:INFO: Dataset: zara2               Batch: 13/36	Loss 5.7526 (6.3621)
2022-11-08 18:28:27,722:INFO: Dataset: zara2               Batch: 14/36	Loss 5.3753 (6.2900)
2022-11-08 18:28:27,725:INFO: Dataset: zara2               Batch: 15/36	Loss 5.2830 (6.2149)
2022-11-08 18:28:27,737:INFO: Dataset: zara2               Batch: 16/36	Loss 5.5725 (6.1709)
2022-11-08 18:28:27,742:INFO: Dataset: zara2               Batch: 17/36	Loss 5.6110 (6.1380)
2022-11-08 18:28:27,752:INFO: Dataset: zara2               Batch: 18/36	Loss 5.1238 (6.0766)
2022-11-08 18:28:27,756:INFO: Dataset: zara2               Batch: 19/36	Loss 5.0353 (6.0364)
2022-11-08 18:28:27,760:INFO: Dataset: zara2               Batch: 20/36	Loss 5.4953 (6.0089)
2022-11-08 18:28:27,766:INFO: Dataset: zara2               Batch: 21/36	Loss 4.7286 (5.9521)
2022-11-08 18:28:27,771:INFO: Dataset: zara2               Batch: 22/36	Loss 5.3918 (5.9278)
2022-11-08 18:28:27,775:INFO: Dataset: zara2               Batch: 23/36	Loss 4.7625 (5.8835)
2022-11-08 18:28:27,781:INFO: Dataset: zara2               Batch: 24/36	Loss 4.6286 (5.8287)
2022-11-08 18:28:27,785:INFO: Dataset: zara2               Batch: 25/36	Loss 4.8077 (5.7903)
2022-11-08 18:28:27,789:INFO: Dataset: zara2               Batch: 26/36	Loss 4.5986 (5.7436)
2022-11-08 18:28:27,793:INFO: Dataset: zara2               Batch: 27/36	Loss 4.6208 (5.7030)
2022-11-08 18:28:27,799:INFO: Dataset: zara2               Batch: 28/36	Loss 4.1363 (5.6510)
2022-11-08 18:28:27,803:INFO: Dataset: zara2               Batch: 29/36	Loss 4.4268 (5.6156)
2022-11-08 18:28:27,807:INFO: Dataset: zara2               Batch: 30/36	Loss 4.6502 (5.5836)
2022-11-08 18:28:27,811:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3596 (5.5350)
2022-11-08 18:28:27,816:INFO: Dataset: zara2               Batch: 32/36	Loss 4.7081 (5.5059)
2022-11-08 18:28:27,820:INFO: Dataset: zara2               Batch: 33/36	Loss 4.0987 (5.4615)
2022-11-08 18:28:27,824:INFO: Dataset: zara2               Batch: 34/36	Loss 4.0093 (5.4132)
2022-11-08 18:28:27,827:INFO: Dataset: zara2               Batch: 35/36	Loss 3.7827 (5.3650)
2022-11-08 18:28:27,834:INFO: Dataset: zara2               Batch: 36/36	Loss 3.6583 (5.3362)
2022-11-08 18:28:28,818:INFO: - Computing loss (validation)
2022-11-08 18:28:35,739:INFO: Dataset: hotel               Batch: 1/3	Loss 16.2088 (16.2088)
2022-11-08 18:28:36,142:INFO: Dataset: hotel               Batch: 2/3	Loss 15.8616 (16.0332)
2022-11-08 18:28:36,509:INFO: Dataset: hotel               Batch: 3/3	Loss 15.7866 (16.0114)
2022-11-08 18:28:45,226:INFO: Dataset: univ                Batch: 1/6	Loss 9.2194 (9.2194)
2022-11-08 18:28:45,723:INFO: Dataset: univ                Batch: 2/6	Loss 9.8743 (9.4993)
2022-11-08 18:28:46,042:INFO: Dataset: univ                Batch: 3/6	Loss 8.6051 (9.1557)
2022-11-08 18:28:46,328:INFO: Dataset: univ                Batch: 4/6	Loss 9.2309 (9.1752)
2022-11-08 18:28:46,623:INFO: Dataset: univ                Batch: 5/6	Loss 9.0927 (9.1583)
2022-11-08 18:28:46,852:INFO: Dataset: univ                Batch: 6/6	Loss 9.0252 (9.1377)
2022-11-08 18:28:54,892:INFO: Dataset: zara1               Batch: 1/3	Loss 11.7100 (11.7100)
2022-11-08 18:28:55,398:INFO: Dataset: zara1               Batch: 2/3	Loss 11.6415 (11.6764)
2022-11-08 18:28:55,814:INFO: Dataset: zara1               Batch: 3/3	Loss 12.1848 (11.8088)
2022-11-08 18:29:04,942:INFO: Dataset: zara2               Batch:  1/10	Loss 4.0846 (4.0846)
2022-11-08 18:29:05,294:INFO: Dataset: zara2               Batch:  2/10	Loss 3.9136 (3.9916)
2022-11-08 18:29:05,784:INFO: Dataset: zara2               Batch:  3/10	Loss 3.7770 (3.9246)
2022-11-08 18:29:06,119:INFO: Dataset: zara2               Batch:  4/10	Loss 3.9079 (3.9204)
2022-11-08 18:29:06,342:INFO: Dataset: zara2               Batch:  5/10	Loss 3.8932 (3.9151)
2022-11-08 18:29:06,608:INFO: Dataset: zara2               Batch:  6/10	Loss 3.9450 (3.9199)
2022-11-08 18:29:06,609:INFO: Dataset: zara2               Batch:  7/10	Loss 3.7603 (3.8965)
2022-11-08 18:29:06,611:INFO: Dataset: zara2               Batch:  8/10	Loss 3.8751 (3.8936)
2022-11-08 18:29:06,614:INFO: Dataset: zara2               Batch:  9/10	Loss 3.8368 (3.8869)
2022-11-08 18:29:06,621:INFO: Dataset: zara2               Batch: 10/10	Loss 3.8043 (3.8792)
2022-11-08 18:29:07,660:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1084.pth.tar
2022-11-08 18:29:07,660:INFO: 
===> EPOCH: 1085 (P5)
2022-11-08 18:29:07,660:INFO: - Computing loss (training)
2022-11-08 18:29:14,547:INFO: Dataset: hotel               Batch: 1/8	Loss 14.0286 (14.0286)
2022-11-08 18:29:14,967:INFO: Dataset: hotel               Batch: 2/8	Loss 13.4902 (13.7581)
2022-11-08 18:29:15,312:INFO: Dataset: hotel               Batch: 3/8	Loss 13.1469 (13.5681)
2022-11-08 18:29:15,723:INFO: Dataset: hotel               Batch: 4/8	Loss 12.8963 (13.3977)
2022-11-08 18:29:15,978:INFO: Dataset: hotel               Batch: 5/8	Loss 11.8590 (13.0749)
2022-11-08 18:29:16,202:INFO: Dataset: hotel               Batch: 6/8	Loss 13.7740 (13.1943)
2022-11-08 18:29:16,212:INFO: Dataset: hotel               Batch: 7/8	Loss 10.5785 (12.8429)
2022-11-08 18:29:16,226:INFO: Dataset: hotel               Batch: 8/8	Loss 16.9183 (12.9558)
2022-11-08 18:29:41,253:INFO: Dataset: univ                Batch:  1/29	Loss 9.9548 (9.9548)
2022-11-08 18:29:41,258:INFO: Dataset: univ                Batch:  2/29	Loss 10.6362 (10.2960)
2022-11-08 18:29:41,263:INFO: Dataset: univ                Batch:  3/29	Loss 10.4671 (10.3420)
2022-11-08 18:29:41,271:INFO: Dataset: univ                Batch:  4/29	Loss 10.2389 (10.3190)
2022-11-08 18:29:41,291:INFO: Dataset: univ                Batch:  5/29	Loss 9.6560 (10.1672)
2022-11-08 18:29:41,308:INFO: Dataset: univ                Batch:  6/29	Loss 9.7839 (10.0944)
2022-11-08 18:29:41,321:INFO: Dataset: univ                Batch:  7/29	Loss 9.3673 (9.9833)
2022-11-08 18:29:41,334:INFO: Dataset: univ                Batch:  8/29	Loss 9.4143 (9.9075)
2022-11-08 18:29:41,341:INFO: Dataset: univ                Batch:  9/29	Loss 9.0751 (9.8114)
2022-11-08 18:29:41,356:INFO: Dataset: univ                Batch: 10/29	Loss 9.4604 (9.7745)
2022-11-08 18:29:41,363:INFO: Dataset: univ                Batch: 11/29	Loss 8.4394 (9.6617)
2022-11-08 18:29:41,373:INFO: Dataset: univ                Batch: 12/29	Loss 8.3424 (9.5420)
2022-11-08 18:29:41,380:INFO: Dataset: univ                Batch: 13/29	Loss 8.7062 (9.4837)
2022-11-08 18:29:41,392:INFO: Dataset: univ                Batch: 14/29	Loss 7.9168 (9.3665)
2022-11-08 18:29:41,404:INFO: Dataset: univ                Batch: 15/29	Loss 8.0947 (9.2768)
2022-11-08 18:29:41,411:INFO: Dataset: univ                Batch: 16/29	Loss 7.2745 (9.1207)
2022-11-08 18:29:41,421:INFO: Dataset: univ                Batch: 17/29	Loss 6.9378 (8.9931)
2022-11-08 18:29:41,426:INFO: Dataset: univ                Batch: 18/29	Loss 7.1399 (8.8879)
2022-11-08 18:29:41,440:INFO: Dataset: univ                Batch: 19/29	Loss 7.2156 (8.7964)
2022-11-08 18:29:41,455:INFO: Dataset: univ                Batch: 20/29	Loss 7.1231 (8.7113)
2022-11-08 18:29:41,463:INFO: Dataset: univ                Batch: 21/29	Loss 6.8858 (8.6271)
2022-11-08 18:29:41,474:INFO: Dataset: univ                Batch: 22/29	Loss 6.8420 (8.5467)
2022-11-08 18:29:41,479:INFO: Dataset: univ                Batch: 23/29	Loss 6.8376 (8.4755)
2022-11-08 18:29:41,491:INFO: Dataset: univ                Batch: 24/29	Loss 6.5474 (8.3913)
2022-11-08 18:29:41,502:INFO: Dataset: univ                Batch: 25/29	Loss 6.3595 (8.3133)
2022-11-08 18:29:41,508:INFO: Dataset: univ                Batch: 26/29	Loss 6.2118 (8.2319)
2022-11-08 18:29:41,521:INFO: Dataset: univ                Batch: 27/29	Loss 5.8718 (8.1367)
2022-11-08 18:29:41,531:INFO: Dataset: univ                Batch: 28/29	Loss 6.2476 (8.0666)
2022-11-08 18:29:41,539:INFO: Dataset: univ                Batch: 29/29	Loss 5.5852 (8.0325)
2022-11-08 18:29:49,337:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8883 (12.8883)
2022-11-08 18:29:49,824:INFO: Dataset: zara1               Batch:  2/16	Loss 13.3088 (13.1143)
2022-11-08 18:29:50,193:INFO: Dataset: zara1               Batch:  3/16	Loss 12.6581 (12.9654)
2022-11-08 18:29:50,507:INFO: Dataset: zara1               Batch:  4/16	Loss 13.2943 (13.0570)
2022-11-08 18:29:50,723:INFO: Dataset: zara1               Batch:  5/16	Loss 13.3111 (13.1126)
2022-11-08 18:29:50,958:INFO: Dataset: zara1               Batch:  6/16	Loss 13.2084 (13.1280)
2022-11-08 18:29:50,963:INFO: Dataset: zara1               Batch:  7/16	Loss 12.6859 (13.0721)
2022-11-08 18:29:50,975:INFO: Dataset: zara1               Batch:  8/16	Loss 12.5930 (12.9997)
2022-11-08 18:29:50,979:INFO: Dataset: zara1               Batch:  9/16	Loss 13.4232 (13.0425)
2022-11-08 18:29:50,991:INFO: Dataset: zara1               Batch: 10/16	Loss 13.0933 (13.0481)
2022-11-08 18:29:51,000:INFO: Dataset: zara1               Batch: 11/16	Loss 12.4093 (12.9817)
2022-11-08 18:29:51,007:INFO: Dataset: zara1               Batch: 12/16	Loss 12.5966 (12.9522)
2022-11-08 18:29:51,013:INFO: Dataset: zara1               Batch: 13/16	Loss 12.2675 (12.9019)
2022-11-08 18:29:51,026:INFO: Dataset: zara1               Batch: 14/16	Loss 12.3372 (12.8629)
2022-11-08 18:29:51,036:INFO: Dataset: zara1               Batch: 15/16	Loss 12.1342 (12.8166)
2022-11-08 18:29:51,041:INFO: Dataset: zara1               Batch: 16/16	Loss 11.9865 (12.7703)
2022-11-08 18:30:16,096:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3056 (6.3056)
2022-11-08 18:30:16,118:INFO: Dataset: zara2               Batch:  2/36	Loss 5.9848 (6.1536)
2022-11-08 18:30:16,135:INFO: Dataset: zara2               Batch:  3/36	Loss 6.5128 (6.2611)
2022-11-08 18:30:16,146:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3255 (6.2766)
2022-11-08 18:30:16,159:INFO: Dataset: zara2               Batch:  5/36	Loss 6.4871 (6.3208)
2022-11-08 18:30:16,178:INFO: Dataset: zara2               Batch:  6/36	Loss 6.2276 (6.3080)
2022-11-08 18:30:16,192:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4926 (6.3406)
2022-11-08 18:30:16,203:INFO: Dataset: zara2               Batch:  8/36	Loss 6.1282 (6.3114)
2022-11-08 18:30:16,217:INFO: Dataset: zara2               Batch:  9/36	Loss 6.5844 (6.3439)
2022-11-08 18:30:16,236:INFO: Dataset: zara2               Batch: 10/36	Loss 6.6229 (6.3726)
2022-11-08 18:30:16,251:INFO: Dataset: zara2               Batch: 11/36	Loss 6.2917 (6.3649)
2022-11-08 18:30:16,271:INFO: Dataset: zara2               Batch: 12/36	Loss 5.7858 (6.3172)
2022-11-08 18:30:16,284:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0813 (6.2964)
2022-11-08 18:30:16,294:INFO: Dataset: zara2               Batch: 14/36	Loss 6.4500 (6.3070)
2022-11-08 18:30:16,308:INFO: Dataset: zara2               Batch: 15/36	Loss 5.9718 (6.2840)
2022-11-08 18:30:16,323:INFO: Dataset: zara2               Batch: 16/36	Loss 5.7841 (6.2495)
2022-11-08 18:30:16,330:INFO: Dataset: zara2               Batch: 17/36	Loss 5.5029 (6.2070)
2022-11-08 18:30:16,334:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6341 (6.1781)
2022-11-08 18:30:16,337:INFO: Dataset: zara2               Batch: 19/36	Loss 5.8175 (6.1631)
2022-11-08 18:30:16,341:INFO: Dataset: zara2               Batch: 20/36	Loss 5.5442 (6.1345)
2022-11-08 18:30:16,344:INFO: Dataset: zara2               Batch: 21/36	Loss 5.0666 (6.0761)
2022-11-08 18:30:16,348:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0073 (6.0306)
2022-11-08 18:30:16,353:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9522 (5.9874)
2022-11-08 18:30:16,357:INFO: Dataset: zara2               Batch: 24/36	Loss 5.1890 (5.9582)
2022-11-08 18:30:16,360:INFO: Dataset: zara2               Batch: 25/36	Loss 5.1002 (5.9277)
2022-11-08 18:30:16,362:INFO: Dataset: zara2               Batch: 26/36	Loss 4.7869 (5.8893)
2022-11-08 18:30:16,365:INFO: Dataset: zara2               Batch: 27/36	Loss 4.5890 (5.8408)
2022-11-08 18:30:16,369:INFO: Dataset: zara2               Batch: 28/36	Loss 4.7411 (5.7979)
2022-11-08 18:30:16,375:INFO: Dataset: zara2               Batch: 29/36	Loss 5.0983 (5.7694)
2022-11-08 18:30:16,379:INFO: Dataset: zara2               Batch: 30/36	Loss 4.0855 (5.7141)
2022-11-08 18:30:16,383:INFO: Dataset: zara2               Batch: 31/36	Loss 4.5032 (5.6711)
2022-11-08 18:30:16,386:INFO: Dataset: zara2               Batch: 32/36	Loss 4.9502 (5.6503)
2022-11-08 18:30:16,389:INFO: Dataset: zara2               Batch: 33/36	Loss 4.5751 (5.6158)
2022-11-08 18:30:16,391:INFO: Dataset: zara2               Batch: 34/36	Loss 4.2395 (5.5781)
2022-11-08 18:30:16,395:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2804 (5.5403)
2022-11-08 18:30:16,399:INFO: Dataset: zara2               Batch: 36/36	Loss 4.3728 (5.5123)
2022-11-08 18:30:17,380:INFO: - Computing loss (validation)
2022-11-08 18:30:24,039:INFO: Dataset: hotel               Batch: 1/3	Loss 15.0486 (15.0486)
2022-11-08 18:30:24,468:INFO: Dataset: hotel               Batch: 2/3	Loss 17.2774 (16.0742)
2022-11-08 18:30:25,008:INFO: Dataset: hotel               Batch: 3/3	Loss 16.4437 (16.0957)
2022-11-08 18:30:33,436:INFO: Dataset: univ                Batch: 1/6	Loss 7.7067 (7.7067)
2022-11-08 18:30:33,841:INFO: Dataset: univ                Batch: 2/6	Loss 8.4065 (8.0343)
2022-11-08 18:30:34,258:INFO: Dataset: univ                Batch: 3/6	Loss 8.1388 (8.0691)
2022-11-08 18:30:34,546:INFO: Dataset: univ                Batch: 4/6	Loss 8.2641 (8.1178)
2022-11-08 18:30:34,785:INFO: Dataset: univ                Batch: 5/6	Loss 7.5619 (8.0025)
2022-11-08 18:30:35,064:INFO: Dataset: univ                Batch: 6/6	Loss 7.3435 (7.8879)
2022-11-08 18:30:42,889:INFO: Dataset: zara1               Batch: 1/3	Loss 11.6156 (11.6156)
2022-11-08 18:30:43,327:INFO: Dataset: zara1               Batch: 2/3	Loss 12.1933 (11.9108)
2022-11-08 18:30:43,663:INFO: Dataset: zara1               Batch: 3/3	Loss 11.9724 (11.9275)
2022-11-08 18:30:52,257:INFO: Dataset: zara2               Batch:  1/10	Loss 4.4421 (4.4421)
2022-11-08 18:30:52,708:INFO: Dataset: zara2               Batch:  2/10	Loss 4.4448 (4.4434)
2022-11-08 18:30:53,024:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1342 (4.3418)
2022-11-08 18:30:53,356:INFO: Dataset: zara2               Batch:  4/10	Loss 4.7330 (4.4378)
2022-11-08 18:30:53,583:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3932 (4.4286)
2022-11-08 18:30:53,830:INFO: Dataset: zara2               Batch:  6/10	Loss 4.3210 (4.4096)
2022-11-08 18:30:53,838:INFO: Dataset: zara2               Batch:  7/10	Loss 4.3091 (4.3953)
2022-11-08 18:30:53,841:INFO: Dataset: zara2               Batch:  8/10	Loss 4.5287 (4.4119)
2022-11-08 18:30:53,845:INFO: Dataset: zara2               Batch:  9/10	Loss 4.1954 (4.3873)
2022-11-08 18:30:53,851:INFO: Dataset: zara2               Batch: 10/10	Loss 4.1068 (4.3600)
2022-11-08 18:30:54,850:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1085.pth.tar
2022-11-08 18:30:54,851:INFO: 
===> EPOCH: 1086 (P5)
2022-11-08 18:30:54,851:INFO: - Computing loss (training)
2022-11-08 18:31:01,721:INFO: Dataset: hotel               Batch: 1/8	Loss 14.4574 (14.4574)
2022-11-08 18:31:02,190:INFO: Dataset: hotel               Batch: 2/8	Loss 11.5956 (13.0900)
2022-11-08 18:31:02,609:INFO: Dataset: hotel               Batch: 3/8	Loss 13.0610 (13.0807)
2022-11-08 18:31:02,870:INFO: Dataset: hotel               Batch: 4/8	Loss 13.1766 (13.1074)
2022-11-08 18:31:03,125:INFO: Dataset: hotel               Batch: 5/8	Loss 12.1214 (12.9137)
2022-11-08 18:31:03,325:INFO: Dataset: hotel               Batch: 6/8	Loss 14.1069 (13.1271)
2022-11-08 18:31:03,334:INFO: Dataset: hotel               Batch: 7/8	Loss 13.2949 (13.1518)
2022-11-08 18:31:03,341:INFO: Dataset: hotel               Batch: 8/8	Loss 13.6565 (13.1678)
2022-11-08 18:31:28,279:INFO: Dataset: univ                Batch:  1/29	Loss 8.0964 (8.0964)
2022-11-08 18:31:28,298:INFO: Dataset: univ                Batch:  2/29	Loss 8.1580 (8.1238)
2022-11-08 18:31:28,312:INFO: Dataset: univ                Batch:  3/29	Loss 9.0375 (8.4300)
2022-11-08 18:31:28,329:INFO: Dataset: univ                Batch:  4/29	Loss 8.5547 (8.4607)
2022-11-08 18:31:28,345:INFO: Dataset: univ                Batch:  5/29	Loss 8.6832 (8.5060)
2022-11-08 18:31:28,362:INFO: Dataset: univ                Batch:  6/29	Loss 8.8629 (8.5582)
2022-11-08 18:31:28,378:INFO: Dataset: univ                Batch:  7/29	Loss 8.2793 (8.5152)
2022-11-08 18:31:28,394:INFO: Dataset: univ                Batch:  8/29	Loss 8.2983 (8.4868)
2022-11-08 18:31:28,409:INFO: Dataset: univ                Batch:  9/29	Loss 7.9982 (8.4308)
2022-11-08 18:31:28,422:INFO: Dataset: univ                Batch: 10/29	Loss 8.1959 (8.4054)
2022-11-08 18:31:28,429:INFO: Dataset: univ                Batch: 11/29	Loss 8.1451 (8.3846)
2022-11-08 18:31:28,442:INFO: Dataset: univ                Batch: 12/29	Loss 8.5363 (8.3960)
2022-11-08 18:31:28,447:INFO: Dataset: univ                Batch: 13/29	Loss 7.6531 (8.3455)
2022-11-08 18:31:28,461:INFO: Dataset: univ                Batch: 14/29	Loss 8.2605 (8.3394)
2022-11-08 18:31:28,477:INFO: Dataset: univ                Batch: 15/29	Loss 7.7296 (8.2925)
2022-11-08 18:31:28,493:INFO: Dataset: univ                Batch: 16/29	Loss 8.0093 (8.2744)
2022-11-08 18:31:28,499:INFO: Dataset: univ                Batch: 17/29	Loss 7.2729 (8.2146)
2022-11-08 18:31:28,506:INFO: Dataset: univ                Batch: 18/29	Loss 7.4760 (8.1774)
2022-11-08 18:31:28,510:INFO: Dataset: univ                Batch: 19/29	Loss 7.1319 (8.1142)
2022-11-08 18:31:28,513:INFO: Dataset: univ                Batch: 20/29	Loss 7.2429 (8.0726)
2022-11-08 18:31:28,518:INFO: Dataset: univ                Batch: 21/29	Loss 6.8891 (8.0138)
2022-11-08 18:31:28,521:INFO: Dataset: univ                Batch: 22/29	Loss 7.2204 (7.9800)
2022-11-08 18:31:28,523:INFO: Dataset: univ                Batch: 23/29	Loss 7.0208 (7.9414)
2022-11-08 18:31:28,528:INFO: Dataset: univ                Batch: 24/29	Loss 7.0794 (7.9081)
2022-11-08 18:31:28,532:INFO: Dataset: univ                Batch: 25/29	Loss 6.8201 (7.8737)
2022-11-08 18:31:28,536:INFO: Dataset: univ                Batch: 26/29	Loss 6.4285 (7.8184)
2022-11-08 18:31:28,541:INFO: Dataset: univ                Batch: 27/29	Loss 6.5509 (7.7729)
2022-11-08 18:31:28,545:INFO: Dataset: univ                Batch: 28/29	Loss 5.8883 (7.6939)
2022-11-08 18:31:28,550:INFO: Dataset: univ                Batch: 29/29	Loss 6.6632 (7.6818)
2022-11-08 18:31:36,275:INFO: Dataset: zara1               Batch:  1/16	Loss 12.9185 (12.9185)
2022-11-08 18:31:36,679:INFO: Dataset: zara1               Batch:  2/16	Loss 13.2720 (13.0810)
2022-11-08 18:31:37,079:INFO: Dataset: zara1               Batch:  3/16	Loss 13.0926 (13.0846)
2022-11-08 18:31:37,395:INFO: Dataset: zara1               Batch:  4/16	Loss 12.9210 (13.0457)
2022-11-08 18:31:37,728:INFO: Dataset: zara1               Batch:  5/16	Loss 12.9766 (13.0308)
2022-11-08 18:31:37,875:INFO: Dataset: zara1               Batch:  6/16	Loss 12.8356 (12.9952)
2022-11-08 18:31:37,881:INFO: Dataset: zara1               Batch:  7/16	Loss 12.6007 (12.9332)
2022-11-08 18:31:37,896:INFO: Dataset: zara1               Batch:  8/16	Loss 12.7658 (12.9121)
2022-11-08 18:31:37,909:INFO: Dataset: zara1               Batch:  9/16	Loss 12.5262 (12.8680)
2022-11-08 18:31:37,912:INFO: Dataset: zara1               Batch: 10/16	Loss 12.6496 (12.8444)
2022-11-08 18:31:37,928:INFO: Dataset: zara1               Batch: 11/16	Loss 12.6684 (12.8274)
2022-11-08 18:31:37,937:INFO: Dataset: zara1               Batch: 12/16	Loss 12.5390 (12.8026)
2022-11-08 18:31:37,947:INFO: Dataset: zara1               Batch: 13/16	Loss 12.2529 (12.7596)
2022-11-08 18:31:37,961:INFO: Dataset: zara1               Batch: 14/16	Loss 12.1603 (12.7161)
2022-11-08 18:31:37,967:INFO: Dataset: zara1               Batch: 15/16	Loss 11.8014 (12.6598)
2022-11-08 18:31:37,977:INFO: Dataset: zara1               Batch: 16/16	Loss 11.2015 (12.5992)
2022-11-08 18:32:02,978:INFO: Dataset: zara2               Batch:  1/36	Loss 6.6816 (6.6816)
2022-11-08 18:32:02,985:INFO: Dataset: zara2               Batch:  2/36	Loss 6.5673 (6.6236)
2022-11-08 18:32:02,991:INFO: Dataset: zara2               Batch:  3/36	Loss 6.1970 (6.4662)
2022-11-08 18:32:03,005:INFO: Dataset: zara2               Batch:  4/36	Loss 6.7216 (6.5237)
2022-11-08 18:32:03,011:INFO: Dataset: zara2               Batch:  5/36	Loss 6.0213 (6.4105)
2022-11-08 18:32:03,024:INFO: Dataset: zara2               Batch:  6/36	Loss 5.8478 (6.3168)
2022-11-08 18:32:03,027:INFO: Dataset: zara2               Batch:  7/36	Loss 6.4516 (6.3359)
2022-11-08 18:32:03,039:INFO: Dataset: zara2               Batch:  8/36	Loss 6.1778 (6.3181)
2022-11-08 18:32:03,046:INFO: Dataset: zara2               Batch:  9/36	Loss 6.3322 (6.3197)
2022-11-08 18:32:03,058:INFO: Dataset: zara2               Batch: 10/36	Loss 6.0233 (6.2920)
2022-11-08 18:32:03,063:INFO: Dataset: zara2               Batch: 11/36	Loss 6.4501 (6.3046)
2022-11-08 18:32:03,073:INFO: Dataset: zara2               Batch: 12/36	Loss 5.8828 (6.2613)
2022-11-08 18:32:03,076:INFO: Dataset: zara2               Batch: 13/36	Loss 5.5351 (6.2071)
2022-11-08 18:32:03,088:INFO: Dataset: zara2               Batch: 14/36	Loss 5.5450 (6.1576)
2022-11-08 18:32:03,095:INFO: Dataset: zara2               Batch: 15/36	Loss 5.9001 (6.1385)
2022-11-08 18:32:03,103:INFO: Dataset: zara2               Batch: 16/36	Loss 5.6492 (6.1032)
2022-11-08 18:32:03,108:INFO: Dataset: zara2               Batch: 17/36	Loss 5.4517 (6.0685)
2022-11-08 18:32:03,114:INFO: Dataset: zara2               Batch: 18/36	Loss 5.2612 (6.0209)
2022-11-08 18:32:03,122:INFO: Dataset: zara2               Batch: 19/36	Loss 5.7890 (6.0081)
2022-11-08 18:32:03,127:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0100 (5.9575)
2022-11-08 18:32:03,134:INFO: Dataset: zara2               Batch: 21/36	Loss 5.1068 (5.9123)
2022-11-08 18:32:03,138:INFO: Dataset: zara2               Batch: 22/36	Loss 4.9904 (5.8662)
2022-11-08 18:32:03,144:INFO: Dataset: zara2               Batch: 23/36	Loss 4.8407 (5.8250)
2022-11-08 18:32:03,154:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7336 (5.7793)
2022-11-08 18:32:03,158:INFO: Dataset: zara2               Batch: 25/36	Loss 4.9726 (5.7438)
2022-11-08 18:32:03,163:INFO: Dataset: zara2               Batch: 26/36	Loss 4.7538 (5.7098)
2022-11-08 18:32:03,172:INFO: Dataset: zara2               Batch: 27/36	Loss 4.6395 (5.6768)
2022-11-08 18:32:03,175:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5236 (5.6399)
2022-11-08 18:32:03,178:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6835 (5.6064)
2022-11-08 18:32:03,189:INFO: Dataset: zara2               Batch: 30/36	Loss 4.9632 (5.5822)
2022-11-08 18:32:03,193:INFO: Dataset: zara2               Batch: 31/36	Loss 4.3125 (5.5379)
2022-11-08 18:32:03,196:INFO: Dataset: zara2               Batch: 32/36	Loss 4.6581 (5.5079)
2022-11-08 18:32:03,206:INFO: Dataset: zara2               Batch: 33/36	Loss 4.2439 (5.4647)
2022-11-08 18:32:03,213:INFO: Dataset: zara2               Batch: 34/36	Loss 4.1812 (5.4212)
2022-11-08 18:32:03,221:INFO: Dataset: zara2               Batch: 35/36	Loss 4.4907 (5.3900)
2022-11-08 18:32:03,225:INFO: Dataset: zara2               Batch: 36/36	Loss 4.3025 (5.3673)
2022-11-08 18:32:04,198:INFO: - Computing loss (validation)
2022-11-08 18:32:11,225:INFO: Dataset: hotel               Batch: 1/3	Loss 15.5587 (15.5587)
2022-11-08 18:32:11,633:INFO: Dataset: hotel               Batch: 2/3	Loss 15.6150 (15.5866)
2022-11-08 18:32:12,092:INFO: Dataset: hotel               Batch: 3/3	Loss 17.5779 (15.7021)
2022-11-08 18:32:20,039:INFO: Dataset: univ                Batch: 1/6	Loss 8.5092 (8.5092)
2022-11-08 18:32:20,433:INFO: Dataset: univ                Batch: 2/6	Loss 8.4033 (8.4541)
2022-11-08 18:32:20,831:INFO: Dataset: univ                Batch: 3/6	Loss 8.0681 (8.3256)
2022-11-08 18:32:21,094:INFO: Dataset: univ                Batch: 4/6	Loss 7.6057 (8.1171)
2022-11-08 18:32:21,319:INFO: Dataset: univ                Batch: 5/6	Loss 7.5616 (7.9915)
2022-11-08 18:32:21,538:INFO: Dataset: univ                Batch: 6/6	Loss 8.0799 (8.0028)
2022-11-08 18:32:28,763:INFO: Dataset: zara1               Batch: 1/3	Loss 11.7000 (11.7000)
2022-11-08 18:32:29,103:INFO: Dataset: zara1               Batch: 2/3	Loss 12.0399 (11.8706)
2022-11-08 18:32:29,510:INFO: Dataset: zara1               Batch: 3/3	Loss 11.9038 (11.8785)
2022-11-08 18:32:37,547:INFO: Dataset: zara2               Batch:  1/10	Loss 4.2020 (4.2020)
2022-11-08 18:32:37,964:INFO: Dataset: zara2               Batch:  2/10	Loss 3.9079 (4.0657)
2022-11-08 18:32:38,469:INFO: Dataset: zara2               Batch:  3/10	Loss 4.4768 (4.1964)
2022-11-08 18:32:38,745:INFO: Dataset: zara2               Batch:  4/10	Loss 4.1402 (4.1830)
2022-11-08 18:32:39,074:INFO: Dataset: zara2               Batch:  5/10	Loss 4.3768 (4.2237)
2022-11-08 18:32:39,338:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2667 (4.2306)
2022-11-08 18:32:39,339:INFO: Dataset: zara2               Batch:  7/10	Loss 4.3950 (4.2544)
2022-11-08 18:32:39,340:INFO: Dataset: zara2               Batch:  8/10	Loss 4.5641 (4.2924)
2022-11-08 18:32:39,341:INFO: Dataset: zara2               Batch:  9/10	Loss 4.5687 (4.3223)
2022-11-08 18:32:39,341:INFO: Dataset: zara2               Batch: 10/10	Loss 4.5570 (4.3458)
2022-11-08 18:32:40,385:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1086.pth.tar
2022-11-08 18:32:40,385:INFO: 
===> EPOCH: 1087 (P5)
2022-11-08 18:32:40,386:INFO: - Computing loss (training)
2022-11-08 18:32:47,125:INFO: Dataset: hotel               Batch: 1/8	Loss 11.8085 (11.8085)
2022-11-08 18:32:47,563:INFO: Dataset: hotel               Batch: 2/8	Loss 12.7959 (12.3069)
2022-11-08 18:32:47,963:INFO: Dataset: hotel               Batch: 3/8	Loss 13.4110 (12.6656)
2022-11-08 18:32:48,314:INFO: Dataset: hotel               Batch: 4/8	Loss 13.8267 (12.9648)
2022-11-08 18:32:48,632:INFO: Dataset: hotel               Batch: 5/8	Loss 13.5441 (13.0764)
2022-11-08 18:32:48,874:INFO: Dataset: hotel               Batch: 6/8	Loss 13.8726 (13.2177)
2022-11-08 18:32:48,878:INFO: Dataset: hotel               Batch: 7/8	Loss 12.1840 (13.0738)
2022-11-08 18:32:48,885:INFO: Dataset: hotel               Batch: 8/8	Loss 6.3219 (12.9134)
2022-11-08 18:33:13,867:INFO: Dataset: univ                Batch:  1/29	Loss 8.3397 (8.3397)
2022-11-08 18:33:13,873:INFO: Dataset: univ                Batch:  2/29	Loss 8.5636 (8.4534)
2022-11-08 18:33:13,879:INFO: Dataset: univ                Batch:  3/29	Loss 8.0149 (8.2945)
2022-11-08 18:33:13,884:INFO: Dataset: univ                Batch:  4/29	Loss 8.1380 (8.2544)
2022-11-08 18:33:13,890:INFO: Dataset: univ                Batch:  5/29	Loss 7.7609 (8.1484)
2022-11-08 18:33:13,897:INFO: Dataset: univ                Batch:  6/29	Loss 8.4297 (8.1966)
2022-11-08 18:33:13,901:INFO: Dataset: univ                Batch:  7/29	Loss 8.7322 (8.2555)
2022-11-08 18:33:13,906:INFO: Dataset: univ                Batch:  8/29	Loss 8.3815 (8.2722)
2022-11-08 18:33:13,914:INFO: Dataset: univ                Batch:  9/29	Loss 8.2947 (8.2746)
2022-11-08 18:33:13,920:INFO: Dataset: univ                Batch: 10/29	Loss 8.3468 (8.2814)
2022-11-08 18:33:13,925:INFO: Dataset: univ                Batch: 11/29	Loss 8.2337 (8.2773)
2022-11-08 18:33:13,931:INFO: Dataset: univ                Batch: 12/29	Loss 7.8417 (8.2412)
2022-11-08 18:33:13,935:INFO: Dataset: univ                Batch: 13/29	Loss 8.4053 (8.2522)
2022-11-08 18:33:13,939:INFO: Dataset: univ                Batch: 14/29	Loss 7.5922 (8.2077)
2022-11-08 18:33:13,945:INFO: Dataset: univ                Batch: 15/29	Loss 7.7912 (8.1803)
2022-11-08 18:33:13,951:INFO: Dataset: univ                Batch: 16/29	Loss 7.7580 (8.1527)
2022-11-08 18:33:13,956:INFO: Dataset: univ                Batch: 17/29	Loss 7.6477 (8.1224)
2022-11-08 18:33:13,960:INFO: Dataset: univ                Batch: 18/29	Loss 7.3576 (8.0795)
2022-11-08 18:33:13,965:INFO: Dataset: univ                Batch: 19/29	Loss 6.8068 (8.0040)
2022-11-08 18:33:13,972:INFO: Dataset: univ                Batch: 20/29	Loss 7.7435 (7.9938)
2022-11-08 18:33:13,980:INFO: Dataset: univ                Batch: 21/29	Loss 7.2029 (7.9564)
2022-11-08 18:33:13,984:INFO: Dataset: univ                Batch: 22/29	Loss 7.1074 (7.9174)
2022-11-08 18:33:13,989:INFO: Dataset: univ                Batch: 23/29	Loss 6.8468 (7.8739)
2022-11-08 18:33:13,994:INFO: Dataset: univ                Batch: 24/29	Loss 6.4697 (7.8102)
2022-11-08 18:33:14,000:INFO: Dataset: univ                Batch: 25/29	Loss 6.5037 (7.7502)
2022-11-08 18:33:14,003:INFO: Dataset: univ                Batch: 26/29	Loss 6.6496 (7.7134)
2022-11-08 18:33:14,007:INFO: Dataset: univ                Batch: 27/29	Loss 6.4943 (7.6722)
2022-11-08 18:33:14,012:INFO: Dataset: univ                Batch: 28/29	Loss 6.3941 (7.6303)
2022-11-08 18:33:14,017:INFO: Dataset: univ                Batch: 29/29	Loss 6.4983 (7.6138)
2022-11-08 18:33:21,197:INFO: Dataset: zara1               Batch:  1/16	Loss 12.8902 (12.8902)
2022-11-08 18:33:21,578:INFO: Dataset: zara1               Batch:  2/16	Loss 12.7931 (12.8401)
2022-11-08 18:33:21,936:INFO: Dataset: zara1               Batch:  3/16	Loss 13.5816 (13.0594)
2022-11-08 18:33:22,269:INFO: Dataset: zara1               Batch:  4/16	Loss 13.0686 (13.0617)
2022-11-08 18:33:22,587:INFO: Dataset: zara1               Batch:  5/16	Loss 12.9014 (13.0298)
2022-11-08 18:33:22,954:INFO: Dataset: zara1               Batch:  6/16	Loss 13.0904 (13.0424)
2022-11-08 18:33:22,958:INFO: Dataset: zara1               Batch:  7/16	Loss 12.8547 (13.0121)
2022-11-08 18:33:22,961:INFO: Dataset: zara1               Batch:  8/16	Loss 12.8663 (12.9950)
2022-11-08 18:33:22,965:INFO: Dataset: zara1               Batch:  9/16	Loss 12.4471 (12.9275)
2022-11-08 18:33:22,970:INFO: Dataset: zara1               Batch: 10/16	Loss 12.9928 (12.9337)
2022-11-08 18:33:22,974:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5450 (12.8995)
2022-11-08 18:33:22,978:INFO: Dataset: zara1               Batch: 12/16	Loss 12.0248 (12.8380)
2022-11-08 18:33:22,981:INFO: Dataset: zara1               Batch: 13/16	Loss 12.5392 (12.8144)
2022-11-08 18:33:22,984:INFO: Dataset: zara1               Batch: 14/16	Loss 12.4663 (12.7943)
2022-11-08 18:33:22,990:INFO: Dataset: zara1               Batch: 15/16	Loss 11.6798 (12.7313)
2022-11-08 18:33:22,993:INFO: Dataset: zara1               Batch: 16/16	Loss 12.2663 (12.7122)
2022-11-08 18:33:48,412:INFO: Dataset: zara2               Batch:  1/36	Loss 6.3879 (6.3879)
2022-11-08 18:33:48,426:INFO: Dataset: zara2               Batch:  2/36	Loss 6.3834 (6.3858)
2022-11-08 18:33:48,443:INFO: Dataset: zara2               Batch:  3/36	Loss 6.7180 (6.4883)
2022-11-08 18:33:48,463:INFO: Dataset: zara2               Batch:  4/36	Loss 6.6394 (6.5248)
2022-11-08 18:33:48,479:INFO: Dataset: zara2               Batch:  5/36	Loss 6.5151 (6.5228)
2022-11-08 18:33:48,496:INFO: Dataset: zara2               Batch:  6/36	Loss 6.2744 (6.4872)
2022-11-08 18:33:48,510:INFO: Dataset: zara2               Batch:  7/36	Loss 6.3033 (6.4595)
2022-11-08 18:33:48,520:INFO: Dataset: zara2               Batch:  8/36	Loss 6.9246 (6.5157)
2022-11-08 18:33:48,540:INFO: Dataset: zara2               Batch:  9/36	Loss 5.7874 (6.4508)
2022-11-08 18:33:48,548:INFO: Dataset: zara2               Batch: 10/36	Loss 5.9018 (6.3878)
2022-11-08 18:33:48,564:INFO: Dataset: zara2               Batch: 11/36	Loss 6.1270 (6.3661)
2022-11-08 18:33:48,578:INFO: Dataset: zara2               Batch: 12/36	Loss 5.3181 (6.2658)
2022-11-08 18:33:48,594:INFO: Dataset: zara2               Batch: 13/36	Loss 6.0213 (6.2451)
2022-11-08 18:33:48,611:INFO: Dataset: zara2               Batch: 14/36	Loss 5.7086 (6.2031)
2022-11-08 18:33:48,626:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5665 (6.1659)
2022-11-08 18:33:48,646:INFO: Dataset: zara2               Batch: 16/36	Loss 5.5018 (6.1178)
2022-11-08 18:33:48,658:INFO: Dataset: zara2               Batch: 17/36	Loss 5.7030 (6.0913)
2022-11-08 18:33:48,662:INFO: Dataset: zara2               Batch: 18/36	Loss 5.2504 (6.0466)
2022-11-08 18:33:48,665:INFO: Dataset: zara2               Batch: 19/36	Loss 5.1386 (5.9970)
2022-11-08 18:33:48,671:INFO: Dataset: zara2               Batch: 20/36	Loss 5.3306 (5.9591)
2022-11-08 18:33:48,675:INFO: Dataset: zara2               Batch: 21/36	Loss 5.1163 (5.9207)
2022-11-08 18:33:48,677:INFO: Dataset: zara2               Batch: 22/36	Loss 5.0434 (5.8801)
2022-11-08 18:33:48,681:INFO: Dataset: zara2               Batch: 23/36	Loss 5.4905 (5.8626)
2022-11-08 18:33:48,686:INFO: Dataset: zara2               Batch: 24/36	Loss 4.7054 (5.8097)
2022-11-08 18:33:48,691:INFO: Dataset: zara2               Batch: 25/36	Loss 4.6428 (5.7599)
2022-11-08 18:33:48,695:INFO: Dataset: zara2               Batch: 26/36	Loss 4.5454 (5.7146)
2022-11-08 18:33:48,699:INFO: Dataset: zara2               Batch: 27/36	Loss 4.6183 (5.6742)
2022-11-08 18:33:48,702:INFO: Dataset: zara2               Batch: 28/36	Loss 4.2911 (5.6236)
2022-11-08 18:33:48,705:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5660 (5.5869)
2022-11-08 18:33:48,708:INFO: Dataset: zara2               Batch: 30/36	Loss 4.6632 (5.5542)
2022-11-08 18:33:48,711:INFO: Dataset: zara2               Batch: 31/36	Loss 4.6772 (5.5209)
2022-11-08 18:33:48,713:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4994 (5.4948)
2022-11-08 18:33:48,718:INFO: Dataset: zara2               Batch: 33/36	Loss 4.8452 (5.4748)
2022-11-08 18:33:48,722:INFO: Dataset: zara2               Batch: 34/36	Loss 4.2236 (5.4294)
2022-11-08 18:33:48,727:INFO: Dataset: zara2               Batch: 35/36	Loss 4.2594 (5.3946)
2022-11-08 18:33:48,731:INFO: Dataset: zara2               Batch: 36/36	Loss 4.4327 (5.3781)
2022-11-08 18:33:49,733:INFO: - Computing loss (validation)
2022-11-08 18:33:56,574:INFO: Dataset: hotel               Batch: 1/3	Loss 14.9871 (14.9871)
2022-11-08 18:33:56,981:INFO: Dataset: hotel               Batch: 2/3	Loss 17.4055 (16.1387)
2022-11-08 18:33:57,404:INFO: Dataset: hotel               Batch: 3/3	Loss 16.0714 (16.1341)
2022-11-08 18:34:05,980:INFO: Dataset: univ                Batch: 1/6	Loss 7.8720 (7.8720)
2022-11-08 18:34:06,369:INFO: Dataset: univ                Batch: 2/6	Loss 8.7576 (8.2590)
2022-11-08 18:34:06,862:INFO: Dataset: univ                Batch: 3/6	Loss 7.9562 (8.1402)
2022-11-08 18:34:07,181:INFO: Dataset: univ                Batch: 4/6	Loss 8.2605 (8.1683)
2022-11-08 18:34:07,392:INFO: Dataset: univ                Batch: 5/6	Loss 7.8522 (8.0973)
2022-11-08 18:34:07,579:INFO: Dataset: univ                Batch: 6/6	Loss 7.7689 (8.0413)
2022-11-08 18:34:15,344:INFO: Dataset: zara1               Batch: 1/3	Loss 11.7655 (11.7655)
2022-11-08 18:34:15,746:INFO: Dataset: zara1               Batch: 2/3	Loss 11.8588 (11.8090)
2022-11-08 18:34:16,131:INFO: Dataset: zara1               Batch: 3/3	Loss 11.7588 (11.7972)
2022-11-08 18:34:24,859:INFO: Dataset: zara2               Batch:  1/10	Loss 4.0042 (4.0042)
2022-11-08 18:34:25,330:INFO: Dataset: zara2               Batch:  2/10	Loss 4.2326 (4.1169)
2022-11-08 18:34:25,713:INFO: Dataset: zara2               Batch:  3/10	Loss 4.1523 (4.1283)
2022-11-08 18:34:25,987:INFO: Dataset: zara2               Batch:  4/10	Loss 4.9057 (4.3229)
2022-11-08 18:34:26,181:INFO: Dataset: zara2               Batch:  5/10	Loss 4.2894 (4.3166)
2022-11-08 18:34:26,490:INFO: Dataset: zara2               Batch:  6/10	Loss 4.2692 (4.3083)
2022-11-08 18:34:26,492:INFO: Dataset: zara2               Batch:  7/10	Loss 4.1631 (4.2832)
2022-11-08 18:34:26,494:INFO: Dataset: zara2               Batch:  8/10	Loss 4.3787 (4.2963)
2022-11-08 18:34:26,496:INFO: Dataset: zara2               Batch:  9/10	Loss 4.0999 (4.2762)
2022-11-08 18:34:26,497:INFO: Dataset: zara2               Batch: 10/10	Loss 3.9536 (4.2476)
2022-11-08 18:34:27,514:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P5/CRMF_epoch_1087.pth.tar
2022-11-08 18:34:27,515:INFO: 
===> EPOCH: 1088 (P5)
2022-11-08 18:34:27,515:INFO: - Computing loss (training)
2022-11-08 18:34:34,393:INFO: Dataset: hotel               Batch: 1/8	Loss 13.2957 (13.2957)
2022-11-08 18:34:34,847:INFO: Dataset: hotel               Batch: 2/8	Loss 12.4688 (12.8436)
2022-11-08 18:34:35,273:INFO: Dataset: hotel               Batch: 3/8	Loss 13.9088 (13.2179)
2022-11-08 18:34:35,574:INFO: Dataset: hotel               Batch: 4/8	Loss 11.9587 (12.8839)
2022-11-08 18:34:35,849:INFO: Dataset: hotel               Batch: 5/8	Loss 14.0734 (13.1191)
2022-11-08 18:34:36,048:INFO: Dataset: hotel               Batch: 6/8	Loss 13.0484 (13.1078)
2022-11-08 18:34:36,059:INFO: Dataset: hotel               Batch: 7/8	Loss 13.7481 (13.1968)
2022-11-08 18:34:36,064:INFO: Dataset: hotel               Batch: 8/8	Loss 10.9348 (13.1252)

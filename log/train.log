2022-11-01 00:07:30,382:INFO: Initializing Training Set
2022-11-01 00:07:34,367:INFO: Initializing Validation Set
2022-11-01 00:07:34,906:INFO: Initializing Validation O Set
2022-11-01 00:07:37,505:INFO: 
===> EPOCH: 451 (P4)
2022-11-01 00:07:37,506:INFO: - Computing loss (training)
2022-11-01 00:07:45,484:INFO: Dataset: hotel               Batch: 1/8	Loss -952.5187 (-952.5187)
2022-11-01 00:07:46,284:INFO: Dataset: hotel               Batch: 2/8	Loss -1423.7811 (-1185.9270)
2022-11-01 00:07:46,946:INFO: Dataset: hotel               Batch: 3/8	Loss -1368.4583 (-1249.0242)
2022-11-01 00:07:47,589:INFO: Dataset: hotel               Batch: 4/8	Loss -1264.5905 (-1252.8615)
2022-11-01 00:07:48,175:INFO: Dataset: hotel               Batch: 5/8	Loss -1631.2828 (-1324.2617)
2022-11-01 00:07:48,768:INFO: Dataset: hotel               Batch: 6/8	Loss -1039.5420 (-1277.5569)
2022-11-01 00:07:49,386:INFO: Dataset: hotel               Batch: 7/8	Loss -1163.1376 (-1261.1671)
2022-11-01 00:07:49,904:INFO: Dataset: hotel               Batch: 8/8	Loss -697.6891 (-1247.7864)
2022-11-01 00:08:17,805:INFO: Dataset: univ                Batch:  1/29	Loss 6265.1562 (6265.1562)
2022-11-01 00:08:18,661:INFO: Dataset: univ                Batch:  2/29	Loss -2732.8184 (1941.8888)
2022-11-01 00:08:19,414:INFO: Dataset: univ                Batch:  3/29	Loss -2103.3804 (681.5190)
2022-11-01 00:08:19,992:INFO: Dataset: univ                Batch:  4/29	Loss -1606.8495 (178.5809)
2022-11-01 00:08:20,581:INFO: Dataset: univ                Batch:  5/29	Loss -4869.9224 (-782.3906)
2022-11-01 00:08:21,195:INFO: Dataset: univ                Batch:  6/29	Loss -2208.8604 (-997.8793)
2022-11-01 00:08:21,819:INFO: Dataset: univ                Batch:  7/29	Loss -2060.3228 (-1140.2404)
2022-11-01 00:08:22,422:INFO: Dataset: univ                Batch:  8/29	Loss -326.8777 (-1023.0797)
2022-11-01 00:08:23,054:INFO: Dataset: univ                Batch:  9/29	Loss -1795.4769 (-1088.8947)
2022-11-01 00:08:23,674:INFO: Dataset: univ                Batch: 10/29	Loss -2249.1060 (-1207.5509)
2022-11-01 00:08:24,347:INFO: Dataset: univ                Batch: 11/29	Loss -3118.2134 (-1399.8143)
2022-11-01 00:08:25,131:INFO: Dataset: univ                Batch: 12/29	Loss -3171.7544 (-1545.3359)
2022-11-01 00:08:25,802:INFO: Dataset: univ                Batch: 13/29	Loss -1932.8453 (-1575.0874)
2022-11-01 00:08:26,461:INFO: Dataset: univ                Batch: 14/29	Loss -1419.5354 (-1564.5827)
2022-11-01 00:08:27,104:INFO: Dataset: univ                Batch: 15/29	Loss -556.3624 (-1486.9919)
2022-11-01 00:08:27,745:INFO: Dataset: univ                Batch: 16/29	Loss 10900.1504 (-759.1514)
2022-11-01 00:08:28,478:INFO: Dataset: univ                Batch: 17/29	Loss -1070.5969 (-779.0415)
2022-11-01 00:08:29,207:INFO: Dataset: univ                Batch: 18/29	Loss -1531.0135 (-823.0572)
2022-11-01 00:08:29,967:INFO: Dataset: univ                Batch: 19/29	Loss -897.0621 (-826.7059)
2022-11-01 00:08:30,686:INFO: Dataset: univ                Batch: 20/29	Loss -1901.9181 (-878.5634)
2022-11-01 00:08:31,414:INFO: Dataset: univ                Batch: 21/29	Loss -726.8936 (-870.3177)
2022-11-01 00:08:32,095:INFO: Dataset: univ                Batch: 22/29	Loss -1700.2405 (-909.1682)
2022-11-01 00:08:32,712:INFO: Dataset: univ                Batch: 23/29	Loss -3287.2048 (-1018.3258)
2022-11-01 00:08:33,669:INFO: Dataset: univ                Batch: 24/29	Loss -2094.2556 (-1063.9768)
2022-11-01 00:08:34,366:INFO: Dataset: univ                Batch: 25/29	Loss -278.3280 (-1030.0028)
2022-11-01 00:08:35,012:INFO: Dataset: univ                Batch: 26/29	Loss -2465.8870 (-1080.6934)
2022-11-01 00:08:35,685:INFO: Dataset: univ                Batch: 27/29	Loss -1072.2438 (-1080.3713)
2022-11-01 00:08:36,384:INFO: Dataset: univ                Batch: 28/29	Loss -963.4294 (-1075.6921)
2022-11-01 00:08:36,902:INFO: Dataset: univ                Batch: 29/29	Loss -221.7572 (-1062.1761)
2022-11-01 00:08:46,560:INFO: Dataset: zara1               Batch:  1/16	Loss -1326.5144 (-1326.5144)
2022-11-01 00:08:47,542:INFO: Dataset: zara1               Batch:  2/16	Loss -1303.9432 (-1315.0939)
2022-11-01 00:08:48,262:INFO: Dataset: zara1               Batch:  3/16	Loss -1478.9259 (-1365.3297)
2022-11-01 00:08:48,914:INFO: Dataset: zara1               Batch:  4/16	Loss -1362.9708 (-1364.7954)
2022-11-01 00:08:49,608:INFO: Dataset: zara1               Batch:  5/16	Loss -1394.9176 (-1371.2249)
2022-11-01 00:08:50,291:INFO: Dataset: zara1               Batch:  6/16	Loss -1623.0454 (-1407.4580)
2022-11-01 00:08:50,941:INFO: Dataset: zara1               Batch:  7/16	Loss -6475.7642 (-2153.7117)
2022-11-01 00:08:51,576:INFO: Dataset: zara1               Batch:  8/16	Loss -1789.1677 (-2106.5858)
2022-11-01 00:08:52,195:INFO: Dataset: zara1               Batch:  9/16	Loss -1282.1371 (-2012.2071)
2022-11-01 00:08:52,857:INFO: Dataset: zara1               Batch: 10/16	Loss -916.3713 (-1875.2276)
2022-11-01 00:08:53,541:INFO: Dataset: zara1               Batch: 11/16	Loss -1343.5992 (-1822.1440)
2022-11-01 00:08:54,210:INFO: Dataset: zara1               Batch: 12/16	Loss 691.1617 (-1615.8535)
2022-11-01 00:08:54,856:INFO: Dataset: zara1               Batch: 13/16	Loss -1420.9318 (-1601.6392)
2022-11-01 00:08:55,506:INFO: Dataset: zara1               Batch: 14/16	Loss -1089.9506 (-1561.0148)
2022-11-01 00:08:56,141:INFO: Dataset: zara1               Batch: 15/16	Loss -1236.0149 (-1541.0696)
2022-11-01 00:08:56,778:INFO: Dataset: zara1               Batch: 16/16	Loss -1392.0564 (-1535.1875)

2022-11-13 13:38:39,818:INFO: Initializing Training Set
2022-11-13 13:38:43,577:INFO: Initializing Validation Set
2022-11-13 13:38:44,034:INFO: Initializing Validation O Set
2022-11-13 13:38:47,230:INFO: 
===> EPOCH: 351 (P4)
2022-11-13 13:38:47,231:INFO: - Computing loss (training)
2022-11-13 13:38:56,593:INFO: Dataset: hotel               Batch: 1/8	Loss 50.5341 (50.5341)
2022-11-13 13:38:58,007:INFO: Dataset: hotel               Batch: 2/8	Loss 39.9208 (45.1759)
2022-11-13 13:38:59,428:INFO: Dataset: hotel               Batch: 3/8	Loss 34.4299 (41.4352)
2022-11-13 13:39:00,813:INFO: Dataset: hotel               Batch: 4/8	Loss 27.6568 (38.2504)
2022-11-13 13:39:02,165:INFO: Dataset: hotel               Batch: 5/8	Loss 22.4094 (35.2509)
2022-11-13 13:39:03,550:INFO: Dataset: hotel               Batch: 6/8	Loss 16.2977 (31.8467)
2022-11-13 13:39:04,912:INFO: Dataset: hotel               Batch: 7/8	Loss 8.8346 (28.1836)
2022-11-13 13:39:06,195:INFO: Dataset: hotel               Batch: 8/8	Loss 0.9497 (27.3572)
2022-11-13 13:39:31,433:INFO: Dataset: univ                Batch:  1/29	Loss 77.8018 (77.8018)
2022-11-13 13:39:32,985:INFO: Dataset: univ                Batch:  2/29	Loss 60.7190 (68.1763)
2022-11-13 13:39:34,424:INFO: Dataset: univ                Batch:  3/29	Loss 48.5543 (61.5246)
2022-11-13 13:39:35,769:INFO: Dataset: univ                Batch:  4/29	Loss 38.5298 (55.4820)
2022-11-13 13:39:37,150:INFO: Dataset: univ                Batch:  5/29	Loss 30.2484 (49.6915)
2022-11-13 13:39:38,506:INFO: Dataset: univ                Batch:  6/29	Loss 26.8087 (45.9922)
2022-11-13 13:39:39,907:INFO: Dataset: univ                Batch:  7/29	Loss 20.0861 (41.7245)
2022-11-13 13:39:41,269:INFO: Dataset: univ                Batch:  8/29	Loss 17.5203 (38.6760)
2022-11-13 13:39:42,613:INFO: Dataset: univ                Batch:  9/29	Loss 15.1489 (36.1007)
2022-11-13 13:39:43,960:INFO: Dataset: univ                Batch: 10/29	Loss 10.8535 (33.7618)
2022-11-13 13:39:45,350:INFO: Dataset: univ                Batch: 11/29	Loss 7.2073 (31.5649)
2022-11-13 13:39:46,691:INFO: Dataset: univ                Batch: 12/29	Loss 3.0483 (29.0944)
2022-11-13 13:39:48,027:INFO: Dataset: univ                Batch: 13/29	Loss -1.4249 (26.7026)
2022-11-13 13:39:49,406:INFO: Dataset: univ                Batch: 14/29	Loss -4.3252 (24.6910)
2022-11-13 13:39:50,841:INFO: Dataset: univ                Batch: 15/29	Loss -8.4034 (22.2625)
2022-11-13 13:39:52,179:INFO: Dataset: univ                Batch: 16/29	Loss -11.4629 (20.1799)
2022-11-13 13:39:53,518:INFO: Dataset: univ                Batch: 17/29	Loss -12.3266 (18.2008)
2022-11-13 13:39:54,953:INFO: Dataset: univ                Batch: 18/29	Loss -14.7369 (16.3293)
2022-11-13 13:39:56,329:INFO: Dataset: univ                Batch: 19/29	Loss -16.5014 (14.7074)
2022-11-13 13:39:57,902:INFO: Dataset: univ                Batch: 20/29	Loss -21.9181 (12.6159)
2022-11-13 13:39:59,325:INFO: Dataset: univ                Batch: 21/29	Loss -23.5327 (11.0227)
2022-11-13 13:40:00,706:INFO: Dataset: univ                Batch: 22/29	Loss -25.1147 (8.9967)
2022-11-13 13:40:02,035:INFO: Dataset: univ                Batch: 23/29	Loss -28.0822 (7.5640)
2022-11-13 13:40:03,399:INFO: Dataset: univ                Batch: 24/29	Loss -28.9292 (6.0200)
2022-11-13 13:40:04,760:INFO: Dataset: univ                Batch: 25/29	Loss -32.4016 (4.2719)
2022-11-13 13:40:06,108:INFO: Dataset: univ                Batch: 26/29	Loss -33.4536 (2.7663)
2022-11-13 13:40:07,479:INFO: Dataset: univ                Batch: 27/29	Loss -34.8466 (1.2815)
2022-11-13 13:40:08,834:INFO: Dataset: univ                Batch: 28/29	Loss -37.5248 (-0.0490)
2022-11-13 13:40:10,100:INFO: Dataset: univ                Batch: 29/29	Loss -13.8192 (-0.2109)
2022-11-13 13:40:19,456:INFO: Dataset: zara1               Batch:  1/16	Loss 2454.7646 (2454.7646)
2022-11-13 13:40:20,811:INFO: Dataset: zara1               Batch:  2/16	Loss 2073.3845 (2289.6460)
2022-11-13 13:40:22,142:INFO: Dataset: zara1               Batch:  3/16	Loss 780.3323 (1825.2418)
2022-11-13 13:40:23,514:INFO: Dataset: zara1               Batch:  4/16	Loss 755.1765 (1585.2478)
2022-11-13 13:40:24,871:INFO: Dataset: zara1               Batch:  5/16	Loss 401.5376 (1371.6609)
2022-11-13 13:40:26,316:INFO: Dataset: zara1               Batch:  6/16	Loss 142.5650 (1170.5677)
2022-11-13 13:40:27,668:INFO: Dataset: zara1               Batch:  7/16	Loss 75.8506 (1000.9452)
2022-11-13 13:40:29,036:INFO: Dataset: zara1               Batch:  8/16	Loss 62.1675 (869.9978)
2022-11-13 13:40:30,371:INFO: Dataset: zara1               Batch:  9/16	Loss 63.5837 (777.7106)
2022-11-13 13:40:31,741:INFO: Dataset: zara1               Batch: 10/16	Loss 65.2485 (706.6392)
2022-11-13 13:40:33,086:INFO: Dataset: zara1               Batch: 11/16	Loss 64.4038 (654.0839)
2022-11-13 13:40:34,465:INFO: Dataset: zara1               Batch: 12/16	Loss 62.3857 (601.1020)
2022-11-13 13:40:35,796:INFO: Dataset: zara1               Batch: 13/16	Loss 60.2584 (560.7354)
2022-11-13 13:40:37,199:INFO: Dataset: zara1               Batch: 14/16	Loss 54.6800 (525.3116)
2022-11-13 13:40:38,933:INFO: Dataset: zara1               Batch: 15/16	Loss 50.6239 (498.9400)
2022-11-13 13:40:40,476:INFO: Dataset: zara1               Batch: 16/16	Loss 31.5732 (474.3418)
2022-11-13 13:41:05,547:INFO: Dataset: zara2               Batch:  1/36	Loss 116.9157 (116.9157)
2022-11-13 13:41:06,902:INFO: Dataset: zara2               Batch:  2/36	Loss 76.1523 (99.1471)
2022-11-13 13:41:08,267:INFO: Dataset: zara2               Batch:  3/36	Loss 60.3294 (85.8444)
2022-11-13 13:41:09,611:INFO: Dataset: zara2               Batch:  4/36	Loss 56.9830 (78.4482)
2022-11-13 13:41:10,974:INFO: Dataset: zara2               Batch:  5/36	Loss 47.0630 (71.6640)
2022-11-13 13:41:12,297:INFO: Dataset: zara2               Batch:  6/36	Loss 41.9243 (67.2889)
2022-11-13 13:41:13,651:INFO: Dataset: zara2               Batch:  7/36	Loss 38.3809 (62.7507)
2022-11-13 13:41:15,009:INFO: Dataset: zara2               Batch:  8/36	Loss 38.0338 (59.9929)
2022-11-13 13:41:16,433:INFO: Dataset: zara2               Batch:  9/36	Loss 35.6962 (57.6229)
2022-11-13 13:41:17,790:INFO: Dataset: zara2               Batch: 10/36	Loss 34.2803 (55.3310)
2022-11-13 13:41:19,176:INFO: Dataset: zara2               Batch: 11/36	Loss 33.4019 (53.3704)
2022-11-13 13:41:20,518:INFO: Dataset: zara2               Batch: 12/36	Loss 32.0949 (51.6615)
2022-11-13 13:41:21,876:INFO: Dataset: zara2               Batch: 13/36	Loss 31.2412 (50.1599)
2022-11-13 13:41:23,222:INFO: Dataset: zara2               Batch: 14/36	Loss 30.5537 (48.6978)
2022-11-13 13:41:24,566:INFO: Dataset: zara2               Batch: 15/36	Loss 29.3268 (47.5890)
2022-11-13 13:41:25,938:INFO: Dataset: zara2               Batch: 16/36	Loss 28.5566 (46.4609)
2022-11-13 13:41:27,295:INFO: Dataset: zara2               Batch: 17/36	Loss 26.6291 (45.3321)
2022-11-13 13:41:28,659:INFO: Dataset: zara2               Batch: 18/36	Loss 24.6715 (44.1880)
2022-11-13 13:41:30,016:INFO: Dataset: zara2               Batch: 19/36	Loss 22.1395 (43.0694)
2022-11-13 13:41:31,746:INFO: Dataset: zara2               Batch: 20/36	Loss 19.1659 (41.9617)
2022-11-13 13:41:33,446:INFO: Dataset: zara2               Batch: 21/36	Loss 16.5299 (40.7745)
2022-11-13 13:41:34,829:INFO: Dataset: zara2               Batch: 22/36	Loss 14.4017 (39.9373)
2022-11-13 13:41:36,415:INFO: Dataset: zara2               Batch: 23/36	Loss 10.8401 (38.6433)
2022-11-13 13:41:37,969:INFO: Dataset: zara2               Batch: 24/36	Loss 7.5626 (37.3859)
2022-11-13 13:41:39,713:INFO: Dataset: zara2               Batch: 25/36	Loss 3.4260 (35.8809)
2022-11-13 13:41:41,245:INFO: Dataset: zara2               Batch: 26/36	Loss -0.1887 (34.4675)
2022-11-13 13:41:42,902:INFO: Dataset: zara2               Batch: 27/36	Loss -3.2317 (33.1093)
2022-11-13 13:41:44,636:INFO: Dataset: zara2               Batch: 28/36	Loss -5.0225 (31.9626)
2022-11-13 13:41:46,364:INFO: Dataset: zara2               Batch: 29/36	Loss -6.3582 (30.3757)
2022-11-13 13:41:47,725:INFO: Dataset: zara2               Batch: 30/36	Loss -7.0068 (29.2638)
2022-11-13 13:41:49,134:INFO: Dataset: zara2               Batch: 31/36	Loss -9.8841 (27.9882)
2022-11-13 13:41:50,506:INFO: Dataset: zara2               Batch: 32/36	Loss -11.2553 (26.5684)
2022-11-13 13:41:52,038:INFO: Dataset: zara2               Batch: 33/36	Loss -13.9632 (25.3133)
2022-11-13 13:41:53,529:INFO: Dataset: zara2               Batch: 34/36	Loss -16.6419 (24.0250)
2022-11-13 13:41:54,950:INFO: Dataset: zara2               Batch: 35/36	Loss -17.9441 (22.9046)
2022-11-13 13:41:56,691:INFO: Dataset: zara2               Batch: 36/36	Loss -11.4512 (22.3764)
2022-11-13 13:41:57,677:INFO: - Computing ADE (validation)
2022-11-13 13:42:06,818:INFO: 		 ADE on hotel                     dataset:	 0.9691547155380249
2022-11-13 13:42:16,085:INFO: 		 ADE on univ                      dataset:	 1.5033971071243286
2022-11-13 13:42:26,509:INFO: 		 ADE on zara1                     dataset:	 2.5260491371154785
2022-11-13 13:42:36,640:INFO: 		 ADE on zara2                     dataset:	 1.440130352973938
2022-11-13 13:42:36,640:INFO: Average validation:	ADE  1.5104	FDE  2.7282
2022-11-13 13:42:36,644:INFO: - Computing ADE (training)
2022-11-13 13:42:45,828:INFO: 		 ADE on hotel                     dataset:	 1.3164658546447754
2022-11-13 13:43:10,679:INFO: 		 ADE on univ                      dataset:	 1.394852876663208
2022-11-13 13:43:19,946:INFO: 		 ADE on zara1                     dataset:	 2.4696972370147705
2022-11-13 13:43:44,416:INFO: 		 ADE on zara2                     dataset:	 1.703295111656189
2022-11-13 13:43:44,416:INFO: Average training:	ADE  1.5240	FDE  2.7611
2022-11-13 13:43:44,451:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_351.pth.tar
2022-11-13 13:43:44,451:INFO: 
===> EPOCH: 352 (P4)
2022-11-13 13:43:44,452:INFO: - Computing loss (training)
2022-11-13 13:43:52,826:INFO: Dataset: hotel               Batch: 1/8	Loss 469.1190 (469.1190)
2022-11-13 13:43:55,162:INFO: Dataset: hotel               Batch: 2/8	Loss 306.5677 (387.8433)
2022-11-13 13:43:56,621:INFO: Dataset: hotel               Batch: 3/8	Loss 303.4749 (360.4236)
2022-11-13 13:43:58,853:INFO: Dataset: hotel               Batch: 4/8	Loss 205.7856 (323.3252)
2022-11-13 13:44:00,409:INFO: Dataset: hotel               Batch: 5/8	Loss 119.7133 (283.6150)
2022-11-13 13:44:01,816:INFO: Dataset: hotel               Batch: 6/8	Loss 67.8020 (245.5504)
2022-11-13 13:44:03,345:INFO: Dataset: hotel               Batch: 7/8	Loss 103.5434 (226.0630)
2022-11-13 13:44:04,719:INFO: Dataset: hotel               Batch: 8/8	Loss 7.8004 (219.7282)
2022-11-13 13:44:31,463:INFO: Dataset: univ                Batch:  1/29	Loss 70.9267 (70.9267)
2022-11-13 13:44:33,178:INFO: Dataset: univ                Batch:  2/29	Loss 35.1035 (51.6830)
2022-11-13 13:44:35,059:INFO: Dataset: univ                Batch:  3/29	Loss 20.7413 (41.0544)
2022-11-13 13:44:36,581:INFO: Dataset: univ                Batch:  4/29	Loss 17.8371 (35.8123)
2022-11-13 13:44:38,558:INFO: Dataset: univ                Batch:  5/29	Loss 19.4137 (32.7991)
2022-11-13 13:44:40,024:INFO: Dataset: univ                Batch:  6/29	Loss 19.6003 (30.3670)
2022-11-13 13:44:41,465:INFO: Dataset: univ                Batch:  7/29	Loss 21.0869 (28.9784)
2022-11-13 13:44:42,919:INFO: Dataset: univ                Batch:  8/29	Loss 20.3996 (27.8905)
2022-11-13 13:44:44,938:INFO: Dataset: univ                Batch:  9/29	Loss 19.2717 (26.9638)
2022-11-13 13:44:46,723:INFO: Dataset: univ                Batch: 10/29	Loss 17.0210 (25.8959)
2022-11-13 13:44:48,317:INFO: Dataset: univ                Batch: 11/29	Loss 14.2033 (24.8513)
2022-11-13 13:44:50,110:INFO: Dataset: univ                Batch: 12/29	Loss 12.5381 (23.8869)
2022-11-13 13:44:51,963:INFO: Dataset: univ                Batch: 13/29	Loss 10.6010 (22.6954)
2022-11-13 13:44:53,769:INFO: Dataset: univ                Batch: 14/29	Loss 10.3569 (21.8470)
2022-11-13 13:44:55,642:INFO: Dataset: univ                Batch: 15/29	Loss 9.1847 (21.1563)
2022-11-13 13:44:57,383:INFO: Dataset: univ                Batch: 16/29	Loss 6.8823 (20.1856)
2022-11-13 13:44:58,748:INFO: Dataset: univ                Batch: 17/29	Loss 7.2412 (19.4721)
2022-11-13 13:45:00,222:INFO: Dataset: univ                Batch: 18/29	Loss 6.0421 (18.8000)
2022-11-13 13:45:01,778:INFO: Dataset: univ                Batch: 19/29	Loss 4.2667 (17.9949)
2022-11-13 13:45:03,445:INFO: Dataset: univ                Batch: 20/29	Loss 2.6068 (17.1511)
2022-11-13 13:45:05,090:INFO: Dataset: univ                Batch: 21/29	Loss 1.8503 (16.3778)
2022-11-13 13:45:06,717:INFO: Dataset: univ                Batch: 22/29	Loss 0.0696 (15.5512)
2022-11-13 13:45:08,189:INFO: Dataset: univ                Batch: 23/29	Loss -0.7364 (14.9334)
2022-11-13 13:45:09,987:INFO: Dataset: univ                Batch: 24/29	Loss -2.4722 (14.1931)
2022-11-13 13:45:11,605:INFO: Dataset: univ                Batch: 25/29	Loss -4.5300 (13.4935)
2022-11-13 13:45:13,145:INFO: Dataset: univ                Batch: 26/29	Loss -5.5162 (12.8798)
2022-11-13 13:45:14,563:INFO: Dataset: univ                Batch: 27/29	Loss -7.2565 (12.1810)
2022-11-13 13:45:16,028:INFO: Dataset: univ                Batch: 28/29	Loss -9.3417 (11.4251)
2022-11-13 13:45:17,445:INFO: Dataset: univ                Batch: 29/29	Loss -4.2417 (11.1897)
2022-11-13 13:45:27,436:INFO: Dataset: zara1               Batch:  1/16	Loss 44.1792 (44.1792)

2022-11-08 23:37:21,493:INFO: Initializing Training Set
2022-11-08 23:37:23,065:INFO: Initializing Validation Set
2022-11-08 23:37:23,245:INFO: Initializing Validation O Set
2022-11-08 23:37:25,231:INFO: 
===> EPOCH: 1 (P1)
2022-11-08 23:37:25,232:INFO: - Computing loss (training)
2022-11-08 23:37:25,382:INFO: Dataset: hotel               Batch: 1/8	Loss 6.2753 (6.2753)
2022-11-08 23:37:25,396:INFO: Dataset: hotel               Batch: 2/8	Loss 7.2511 (6.7924)
2022-11-08 23:37:25,410:INFO: Dataset: hotel               Batch: 3/8	Loss 6.6706 (6.7556)
2022-11-08 23:37:25,425:INFO: Dataset: hotel               Batch: 4/8	Loss 4.2745 (6.1750)
2022-11-08 23:37:25,439:INFO: Dataset: hotel               Batch: 5/8	Loss 4.7082 (5.8691)
2022-11-08 23:37:25,456:INFO: Dataset: hotel               Batch: 6/8	Loss 5.5714 (5.8165)
2022-11-08 23:37:25,470:INFO: Dataset: hotel               Batch: 7/8	Loss 4.0082 (5.5368)
2022-11-08 23:37:25,477:INFO: Dataset: hotel               Batch: 8/8	Loss 1.7044 (5.4306)
2022-11-08 23:37:25,665:INFO: Dataset: univ                Batch:  1/29	Loss 6.5540 (6.5540)
2022-11-08 23:37:25,682:INFO: Dataset: univ                Batch:  2/29	Loss 7.0803 (6.8350)
2022-11-08 23:37:25,700:INFO: Dataset: univ                Batch:  3/29	Loss 6.3119 (6.6515)
2022-11-08 23:37:25,715:INFO: Dataset: univ                Batch:  4/29	Loss 8.2232 (7.0385)
2022-11-08 23:37:25,730:INFO: Dataset: univ                Batch:  5/29	Loss 6.3529 (6.8854)
2022-11-08 23:37:25,750:INFO: Dataset: univ                Batch:  6/29	Loss 8.6696 (7.1988)
2022-11-08 23:37:25,766:INFO: Dataset: univ                Batch:  7/29	Loss 4.8243 (6.7884)
2022-11-08 23:37:25,780:INFO: Dataset: univ                Batch:  8/29	Loss 6.9318 (6.8046)
2022-11-08 23:37:25,795:INFO: Dataset: univ                Batch:  9/29	Loss 6.9906 (6.8253)
2022-11-08 23:37:25,810:INFO: Dataset: univ                Batch: 10/29	Loss 6.0619 (6.7484)
2022-11-08 23:37:25,825:INFO: Dataset: univ                Batch: 11/29	Loss 5.2140 (6.6111)
2022-11-08 23:37:25,841:INFO: Dataset: univ                Batch: 12/29	Loss 7.0417 (6.6454)
2022-11-08 23:37:25,855:INFO: Dataset: univ                Batch: 13/29	Loss 5.9317 (6.5893)
2022-11-08 23:37:25,870:INFO: Dataset: univ                Batch: 14/29	Loss 6.7079 (6.5972)
2022-11-08 23:37:25,886:INFO: Dataset: univ                Batch: 15/29	Loss 6.6711 (6.6016)
2022-11-08 23:37:25,902:INFO: Dataset: univ                Batch: 16/29	Loss 5.1596 (6.5025)
2022-11-08 23:37:25,919:INFO: Dataset: univ                Batch: 17/29	Loss 6.2918 (6.4907)
2022-11-08 23:37:25,935:INFO: Dataset: univ                Batch: 18/29	Loss 5.5784 (6.4337)
2022-11-08 23:37:25,951:INFO: Dataset: univ                Batch: 19/29	Loss 6.6271 (6.4446)
2022-11-08 23:37:25,967:INFO: Dataset: univ                Batch: 20/29	Loss 5.0538 (6.3732)
2022-11-08 23:37:25,982:INFO: Dataset: univ                Batch: 21/29	Loss 5.7116 (6.3455)
2022-11-08 23:37:25,998:INFO: Dataset: univ                Batch: 22/29	Loss 6.6846 (6.3614)
2022-11-08 23:37:26,013:INFO: Dataset: univ                Batch: 23/29	Loss 5.1425 (6.3066)
2022-11-08 23:37:26,029:INFO: Dataset: univ                Batch: 24/29	Loss 5.5484 (6.2765)
2022-11-08 23:37:26,044:INFO: Dataset: univ                Batch: 25/29	Loss 4.0638 (6.1765)
2022-11-08 23:37:26,060:INFO: Dataset: univ                Batch: 26/29	Loss 4.9510 (6.1308)
2022-11-08 23:37:26,076:INFO: Dataset: univ                Batch: 27/29	Loss 6.7932 (6.1532)
2022-11-08 23:37:26,092:INFO: Dataset: univ                Batch: 28/29	Loss 7.8163 (6.2089)
2022-11-08 23:37:26,101:INFO: Dataset: univ                Batch: 29/29	Loss 2.1578 (6.1460)
2022-11-08 23:37:26,296:INFO: Dataset: zara1               Batch:  1/16	Loss 9.2696 (9.2696)
2022-11-08 23:37:26,311:INFO: Dataset: zara1               Batch:  2/16	Loss 9.1303 (9.2063)
2022-11-08 23:37:26,328:INFO: Dataset: zara1               Batch:  3/16	Loss 9.2166 (9.2095)
2022-11-08 23:37:26,345:INFO: Dataset: zara1               Batch:  4/16	Loss 10.2284 (9.4612)
2022-11-08 23:37:26,359:INFO: Dataset: zara1               Batch:  5/16	Loss 9.3453 (9.4383)
2022-11-08 23:37:26,377:INFO: Dataset: zara1               Batch:  6/16	Loss 10.5906 (9.6250)
2022-11-08 23:37:26,390:INFO: Dataset: zara1               Batch:  7/16	Loss 9.8017 (9.6467)
2022-11-08 23:37:26,404:INFO: Dataset: zara1               Batch:  8/16	Loss 8.5810 (9.5112)
2022-11-08 23:37:26,418:INFO: Dataset: zara1               Batch:  9/16	Loss 9.1566 (9.4737)
2022-11-08 23:37:26,432:INFO: Dataset: zara1               Batch: 10/16	Loss 9.0034 (9.4244)
2022-11-08 23:37:26,446:INFO: Dataset: zara1               Batch: 11/16	Loss 8.6957 (9.3561)
2022-11-08 23:37:26,460:INFO: Dataset: zara1               Batch: 12/16	Loss 8.7995 (9.3167)
2022-11-08 23:37:26,474:INFO: Dataset: zara1               Batch: 13/16	Loss 9.0026 (9.2948)
2022-11-08 23:37:26,488:INFO: Dataset: zara1               Batch: 14/16	Loss 9.0627 (9.2793)
2022-11-08 23:37:26,502:INFO: Dataset: zara1               Batch: 15/16	Loss 8.4579 (9.2254)
2022-11-08 23:37:26,515:INFO: Dataset: zara1               Batch: 16/16	Loss 7.0279 (9.1236)
2022-11-08 23:37:26,707:INFO: Dataset: zara2               Batch:  1/36	Loss 5.7666 (5.7666)
2022-11-08 23:37:26,722:INFO: Dataset: zara2               Batch:  2/36	Loss 8.3096 (7.0762)
2022-11-08 23:37:26,737:INFO: Dataset: zara2               Batch:  3/36	Loss 7.1729 (7.1053)
2022-11-08 23:37:26,754:INFO: Dataset: zara2               Batch:  4/36	Loss 6.1119 (6.8800)
2022-11-08 23:37:26,770:INFO: Dataset: zara2               Batch:  5/36	Loss 6.0572 (6.7168)
2022-11-08 23:37:26,787:INFO: Dataset: zara2               Batch:  6/36	Loss 6.8831 (6.7451)
2022-11-08 23:37:26,801:INFO: Dataset: zara2               Batch:  7/36	Loss 6.5886 (6.7224)
2022-11-08 23:37:26,815:INFO: Dataset: zara2               Batch:  8/36	Loss 6.5321 (6.6980)
2022-11-08 23:37:26,829:INFO: Dataset: zara2               Batch:  9/36	Loss 6.1552 (6.6313)
2022-11-08 23:37:26,844:INFO: Dataset: zara2               Batch: 10/36	Loss 6.9689 (6.6616)
2022-11-08 23:37:26,858:INFO: Dataset: zara2               Batch: 11/36	Loss 5.6816 (6.5579)
2022-11-08 23:37:26,872:INFO: Dataset: zara2               Batch: 12/36	Loss 5.2536 (6.4523)
2022-11-08 23:37:26,886:INFO: Dataset: zara2               Batch: 13/36	Loss 7.1052 (6.4928)
2022-11-08 23:37:26,901:INFO: Dataset: zara2               Batch: 14/36	Loss 6.7974 (6.5167)
2022-11-08 23:37:26,915:INFO: Dataset: zara2               Batch: 15/36	Loss 5.5603 (6.4593)
2022-11-08 23:37:26,931:INFO: Dataset: zara2               Batch: 16/36	Loss 6.0071 (6.4320)
2022-11-08 23:37:26,947:INFO: Dataset: zara2               Batch: 17/36	Loss 6.1501 (6.4162)
2022-11-08 23:37:26,963:INFO: Dataset: zara2               Batch: 18/36	Loss 5.6538 (6.3758)
2022-11-08 23:37:26,979:INFO: Dataset: zara2               Batch: 19/36	Loss 5.6172 (6.3421)
2022-11-08 23:37:26,995:INFO: Dataset: zara2               Batch: 20/36	Loss 5.1669 (6.2760)
2022-11-08 23:37:27,011:INFO: Dataset: zara2               Batch: 21/36	Loss 5.4293 (6.2337)
2022-11-08 23:37:27,026:INFO: Dataset: zara2               Batch: 22/36	Loss 6.6617 (6.2507)
2022-11-08 23:37:27,041:INFO: Dataset: zara2               Batch: 23/36	Loss 4.9124 (6.2008)
2022-11-08 23:37:27,056:INFO: Dataset: zara2               Batch: 24/36	Loss 8.1722 (6.2853)
2022-11-08 23:37:27,071:INFO: Dataset: zara2               Batch: 25/36	Loss 6.2005 (6.2815)
2022-11-08 23:37:27,085:INFO: Dataset: zara2               Batch: 26/36	Loss 5.1057 (6.2288)
2022-11-08 23:37:27,100:INFO: Dataset: zara2               Batch: 27/36	Loss 6.0669 (6.2218)
2022-11-08 23:37:27,115:INFO: Dataset: zara2               Batch: 28/36	Loss 5.2147 (6.1905)
2022-11-08 23:37:27,131:INFO: Dataset: zara2               Batch: 29/36	Loss 5.8987 (6.1810)
2022-11-08 23:37:27,146:INFO: Dataset: zara2               Batch: 30/36	Loss 5.6821 (6.1639)
2022-11-08 23:37:27,162:INFO: Dataset: zara2               Batch: 31/36	Loss 5.3612 (6.1385)
2022-11-08 23:37:27,177:INFO: Dataset: zara2               Batch: 32/36	Loss 6.0426 (6.1356)
2022-11-08 23:37:27,193:INFO: Dataset: zara2               Batch: 33/36	Loss 5.9427 (6.1304)
2022-11-08 23:37:27,208:INFO: Dataset: zara2               Batch: 34/36	Loss 5.1116 (6.0983)
2022-11-08 23:37:27,224:INFO: Dataset: zara2               Batch: 35/36	Loss 5.6372 (6.0846)
2022-11-08 23:37:27,237:INFO: Dataset: zara2               Batch: 36/36	Loss 3.7999 (6.0476)
2022-11-08 23:37:27,275:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 2, 2, 2)_shuffle_true_seed_72/pretrain/P1/CRMF_epoch_1.pth.tar
2022-11-08 23:37:27,275:INFO: 
===> EPOCH: 2 (P1)
2022-11-08 23:37:27,276:INFO: - Computing loss (training)
2022-11-08 23:37:27,433:INFO: Dataset: hotel               Batch: 1/8	Loss 6.2103 (6.2103)
2022-11-08 23:37:27,447:INFO: Dataset: hotel               Batch: 2/8	Loss 3.6007 (4.9181)
2022-11-08 23:37:27,464:INFO: Dataset: hotel               Batch: 3/8	Loss 3.6417 (4.5093)
2022-11-08 23:37:27,478:INFO: Dataset: hotel               Batch: 4/8	Loss 5.9956 (4.9075)
2022-11-08 23:37:27,493:INFO: Dataset: hotel               Batch: 5/8	Loss 5.4616 (5.0213)
2022-11-08 23:37:27,508:INFO: Dataset: hotel               Batch: 6/8	Loss 6.4761 (5.2634)
2022-11-08 23:37:27,522:INFO: Dataset: hotel               Batch: 7/8	Loss 7.5431 (5.5806)
2022-11-08 23:37:27,528:INFO: Dataset: hotel               Batch: 8/8	Loss 0.3274 (5.4073)
2022-11-08 23:37:27,729:INFO: Dataset: univ                Batch:  1/29	Loss 4.5545 (4.5545)
2022-11-08 23:37:27,746:INFO: Dataset: univ                Batch:  2/29	Loss 4.5941 (4.5757)
2022-11-08 23:37:27,762:INFO: Dataset: univ                Batch:  3/29	Loss 3.9565 (4.3702)
2022-11-08 23:37:27,777:INFO: Dataset: univ                Batch:  4/29	Loss 4.3299 (4.3594)
2022-11-08 23:37:27,795:INFO: Dataset: univ                Batch:  5/29	Loss 3.8003 (4.2534)
2022-11-08 23:37:27,813:INFO: Dataset: univ                Batch:  6/29	Loss 3.8398 (4.1773)
2022-11-08 23:37:27,827:INFO: Dataset: univ                Batch:  7/29	Loss 5.3525 (4.3483)
2022-11-08 23:37:27,842:INFO: Dataset: univ                Batch:  8/29	Loss 4.5382 (4.3710)
2022-11-08 23:37:27,857:INFO: Dataset: univ                Batch:  9/29	Loss 3.9850 (4.3292)
2022-11-08 23:37:27,872:INFO: Dataset: univ                Batch: 10/29	Loss 4.0872 (4.3034)
2022-11-08 23:37:27,887:INFO: Dataset: univ                Batch: 11/29	Loss 4.8807 (4.3583)
2022-11-08 23:37:27,902:INFO: Dataset: univ                Batch: 12/29	Loss 6.0387 (4.4849)
2022-11-08 23:37:27,917:INFO: Dataset: univ                Batch: 13/29	Loss 6.6799 (4.6166)
2022-11-08 23:37:27,932:INFO: Dataset: univ                Batch: 14/29	Loss 3.8478 (4.5658)
2022-11-08 23:37:27,949:INFO: Dataset: univ                Batch: 15/29	Loss 3.9023 (4.5174)
2022-11-08 23:37:27,965:INFO: Dataset: univ                Batch: 16/29	Loss 3.9929 (4.4870)
2022-11-08 23:37:27,981:INFO: Dataset: univ                Batch: 17/29	Loss 4.5606 (4.4908)
2022-11-08 23:37:27,998:INFO: Dataset: univ                Batch: 18/29	Loss 5.3095 (4.5297)
2022-11-08 23:37:28,014:INFO: Dataset: univ                Batch: 19/29	Loss 4.0048 (4.5008)
2022-11-08 23:37:28,030:INFO: Dataset: univ                Batch: 20/29	Loss 5.0630 (4.5321)
2022-11-08 23:37:28,046:INFO: Dataset: univ                Batch: 21/29	Loss 4.5942 (4.5349)
2022-11-08 23:37:28,062:INFO: Dataset: univ                Batch: 22/29	Loss 3.8679 (4.5022)
2022-11-08 23:37:28,078:INFO: Dataset: univ                Batch: 23/29	Loss 4.6077 (4.5068)
2022-11-08 23:37:28,093:INFO: Dataset: univ                Batch: 24/29	Loss 3.8683 (4.4828)
2022-11-08 23:37:28,109:INFO: Dataset: univ                Batch: 25/29	Loss 3.7688 (4.4511)
2022-11-08 23:37:28,125:INFO: Dataset: univ                Batch: 26/29	Loss 4.0577 (4.4355)
2022-11-08 23:37:28,140:INFO: Dataset: univ                Batch: 27/29	Loss 4.8094 (4.4474)
2022-11-08 23:37:28,156:INFO: Dataset: univ                Batch: 28/29	Loss 4.8449 (4.4607)
2022-11-08 23:37:28,166:INFO: Dataset: univ                Batch: 29/29	Loss 1.3962 (4.4154)
2022-11-08 23:37:28,346:INFO: Dataset: zara1               Batch:  1/16	Loss 6.7515 (6.7515)
2022-11-08 23:37:28,361:INFO: Dataset: zara1               Batch:  2/16	Loss 7.5560 (7.1371)
2022-11-08 23:37:28,379:INFO: Dataset: zara1               Batch:  3/16	Loss 7.2094 (7.1617)
2022-11-08 23:37:28,394:INFO: Dataset: zara1               Batch:  4/16	Loss 6.6036 (7.0444)
2022-11-08 23:37:28,410:INFO: Dataset: zara1               Batch:  5/16	Loss 6.8037 (6.9994)
2022-11-08 23:37:28,426:INFO: Dataset: zara1               Batch:  6/16	Loss 5.3006 (6.7091)
2022-11-08 23:37:28,440:INFO: Dataset: zara1               Batch:  7/16	Loss 7.3075 (6.7834)
2022-11-08 23:37:28,454:INFO: Dataset: zara1               Batch:  8/16	Loss 6.2803 (6.7210)
2022-11-08 23:37:28,468:INFO: Dataset: zara1               Batch:  9/16	Loss 6.3407 (6.6773)
2022-11-08 23:37:28,481:INFO: Dataset: zara1               Batch: 10/16	Loss 5.9981 (6.6111)
2022-11-08 23:37:28,497:INFO: Dataset: zara1               Batch: 11/16	Loss 6.6029 (6.6104)
2022-11-08 23:37:28,512:INFO: Dataset: zara1               Batch: 12/16	Loss 5.7996 (6.5363)
2022-11-08 23:37:28,525:INFO: Dataset: zara1               Batch: 13/16	Loss 6.7332 (6.5505)
2022-11-08 23:37:28,539:INFO: Dataset: zara1               Batch: 14/16	Loss 6.3634 (6.5380)
2022-11-08 23:37:28,553:INFO: Dataset: zara1               Batch: 15/16	Loss 6.9584 (6.5641)
2022-11-08 23:37:28,566:INFO: Dataset: zara1               Batch: 16/16	Loss 4.8698 (6.4910)
2022-11-08 23:37:28,757:INFO: Dataset: zara2               Batch:  1/36	Loss 4.2707 (4.2707)
2022-11-08 23:37:28,805:INFO: Dataset: zara2               Batch:  2/36	Loss 4.6303 (4.4511)
2022-11-08 23:37:28,825:INFO: Dataset: zara2               Batch:  3/36	Loss 4.7214 (4.5459)
2022-11-08 23:37:28,840:INFO: Dataset: zara2               Batch:  4/36	Loss 5.1844 (4.7058)
2022-11-08 23:37:28,854:INFO: Dataset: zara2               Batch:  5/36	Loss 4.3883 (4.6390)
2022-11-08 23:37:28,871:INFO: Dataset: zara2               Batch:  6/36	Loss 5.0854 (4.7200)
2022-11-08 23:37:28,885:INFO: Dataset: zara2               Batch:  7/36	Loss 4.2741 (4.6606)
2022-11-08 23:37:28,899:INFO: Dataset: zara2               Batch:  8/36	Loss 4.4566 (4.6327)
2022-11-08 23:37:28,913:INFO: Dataset: zara2               Batch:  9/36	Loss 4.2676 (4.5891)
2022-11-08 23:37:28,928:INFO: Dataset: zara2               Batch: 10/36	Loss 6.0355 (4.7245)
2022-11-08 23:37:28,943:INFO: Dataset: zara2               Batch: 11/36	Loss 4.4878 (4.7038)
2022-11-08 23:37:28,959:INFO: Dataset: zara2               Batch: 12/36	Loss 4.9053 (4.7226)
2022-11-08 23:37:28,976:INFO: Dataset: zara2               Batch: 13/36	Loss 5.3033 (4.7661)
2022-11-08 23:37:28,992:INFO: Dataset: zara2               Batch: 14/36	Loss 4.7714 (4.7666)
2022-11-08 23:37:29,008:INFO: Dataset: zara2               Batch: 15/36	Loss 3.3889 (4.6776)
2022-11-08 23:37:29,024:INFO: Dataset: zara2               Batch: 16/36	Loss 4.0176 (4.6392)
2022-11-08 23:37:29,042:INFO: Dataset: zara2               Batch: 17/36	Loss 3.3738 (4.5536)
2022-11-08 23:37:29,059:INFO: Dataset: zara2               Batch: 18/36	Loss 3.9758 (4.5217)
2022-11-08 23:37:29,074:INFO: Dataset: zara2               Batch: 19/36	Loss 4.2902 (4.5090)
2022-11-08 23:37:29,092:INFO: Dataset: zara2               Batch: 20/36	Loss 4.3672 (4.5019)
2022-11-08 23:37:29,108:INFO: Dataset: zara2               Batch: 21/36	Loss 4.2398 (4.4897)
2022-11-08 23:37:29,124:INFO: Dataset: zara2               Batch: 22/36	Loss 4.2913 (4.4796)
2022-11-08 23:37:29,139:INFO: Dataset: zara2               Batch: 23/36	Loss 3.6136 (4.4364)
2022-11-08 23:37:29,155:INFO: Dataset: zara2               Batch: 24/36	Loss 4.9311 (4.4604)
2022-11-08 23:37:29,171:INFO: Dataset: zara2               Batch: 25/36	Loss 4.4314 (4.4595)
2022-11-08 23:37:29,187:INFO: Dataset: zara2               Batch: 26/36	Loss 3.8159 (4.4374)
2022-11-08 23:37:29,202:INFO: Dataset: zara2               Batch: 27/36	Loss 3.7909 (4.4140)
2022-11-08 23:37:29,218:INFO: Dataset: zara2               Batch: 28/36	Loss 4.5190 (4.4178)
2022-11-08 23:37:29,233:INFO: Dataset: zara2               Batch: 29/36	Loss 4.5058 (4.4211)
2022-11-08 23:37:29,248:INFO: Dataset: zara2               Batch: 30/36	Loss 4.8415 (4.4340)
2022-11-08 23:37:29,264:INFO: Dataset: zara2               Batch: 31/36	Loss 4.1284 (4.4230)
2022-11-08 23:37:29,281:INFO: Dataset: zara2               Batch: 32/36	Loss 4.6625 (4.4307)
2022-11-08 23:37:29,296:INFO: Dataset: zara2               Batch: 33/36	Loss 4.0612 (4.4181)
2022-11-08 23:37:29,311:INFO: Dataset: zara2               Batch: 34/36	Loss 4.4025 (4.4176)
2022-11-08 23:37:29,326:INFO: Dataset: zara2               Batch: 35/36	Loss 4.0467 (4.4050)
2022-11-08 23:37:29,338:INFO: Dataset: zara2               Batch: 36/36	Loss 2.4493 (4.3672)
2022-11-08 23:37:29,370:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 2, 2, 2)_shuffle_true_seed_72/pretrain/P1/CRMF_epoch_2.pth.tar
2022-11-08 23:37:29,370:INFO: 
===> EPOCH: 3 (P2)
2022-11-08 23:37:29,370:INFO: - Computing loss (training)
2022-11-08 23:37:29,591:INFO: Dataset: hotel               Batch: 1/8	Loss 8.4700 (8.4700)
2022-11-08 23:37:29,672:INFO: Dataset: hotel               Batch: 2/8	Loss 7.3330 (7.9288)
2022-11-08 23:37:29,748:INFO: Dataset: hotel               Batch: 3/8	Loss 7.8801 (7.9124)
2022-11-08 23:37:29,824:INFO: Dataset: hotel               Batch: 4/8	Loss 6.3367 (7.4954)
2022-11-08 23:37:29,901:INFO: Dataset: hotel               Batch: 5/8	Loss 6.9430 (7.3847)
2022-11-08 23:37:29,980:INFO: Dataset: hotel               Batch: 6/8	Loss 7.1034 (7.3374)
2022-11-08 23:37:30,057:INFO: Dataset: hotel               Batch: 7/8	Loss 5.7888 (7.1378)
2022-11-08 23:37:30,081:INFO: Dataset: hotel               Batch: 8/8	Loss 1.4427 (6.9800)
2022-11-08 23:37:30,339:INFO: Dataset: univ                Batch:  1/29	Loss 8.1310 (8.1310)
2022-11-08 23:37:30,429:INFO: Dataset: univ                Batch:  2/29	Loss 7.1298 (7.6270)
2022-11-08 23:37:30,512:INFO: Dataset: univ                Batch:  3/29	Loss 7.4478 (7.5625)
2022-11-08 23:37:30,597:INFO: Dataset: univ                Batch:  4/29	Loss 8.0666 (7.6927)
2022-11-08 23:37:30,678:INFO: Dataset: univ                Batch:  5/29	Loss 7.3762 (7.6276)
2022-11-08 23:37:30,763:INFO: Dataset: univ                Batch:  6/29	Loss 7.1903 (7.5554)
2022-11-08 23:37:30,846:INFO: Dataset: univ                Batch:  7/29	Loss 6.2217 (7.3544)
2022-11-08 23:37:30,930:INFO: Dataset: univ                Batch:  8/29	Loss 5.3304 (7.0655)
2022-11-08 23:37:31,011:INFO: Dataset: univ                Batch:  9/29	Loss 7.0901 (7.0680)
2022-11-08 23:37:31,094:INFO: Dataset: univ                Batch: 10/29	Loss 6.0694 (6.9663)
2022-11-08 23:37:31,176:INFO: Dataset: univ                Batch: 11/29	Loss 5.3791 (6.8319)
2022-11-08 23:37:31,258:INFO: Dataset: univ                Batch: 12/29	Loss 5.0227 (6.6755)
2022-11-08 23:37:31,343:INFO: Dataset: univ                Batch: 13/29	Loss 6.4917 (6.6617)
2022-11-08 23:37:31,425:INFO: Dataset: univ                Batch: 14/29	Loss 5.5393 (6.5722)
2022-11-08 23:37:31,506:INFO: Dataset: univ                Batch: 15/29	Loss 5.1726 (6.4753)
2022-11-08 23:37:31,587:INFO: Dataset: univ                Batch: 16/29	Loss 4.6392 (6.3587)
2022-11-08 23:37:31,670:INFO: Dataset: univ                Batch: 17/29	Loss 4.3713 (6.2293)
2022-11-08 23:37:31,750:INFO: Dataset: univ                Batch: 18/29	Loss 5.3114 (6.1790)
2022-11-08 23:37:31,831:INFO: Dataset: univ                Batch: 19/29	Loss 5.0976 (6.1232)
2022-11-08 23:37:31,914:INFO: Dataset: univ                Batch: 20/29	Loss 5.2323 (6.0708)
2022-11-08 23:37:31,997:INFO: Dataset: univ                Batch: 21/29	Loss 4.7004 (6.0076)
2022-11-08 23:37:32,086:INFO: Dataset: univ                Batch: 22/29	Loss 4.8681 (5.9577)
2022-11-08 23:37:32,173:INFO: Dataset: univ                Batch: 23/29	Loss 5.3097 (5.9297)
2022-11-08 23:37:32,264:INFO: Dataset: univ                Batch: 24/29	Loss 4.5775 (5.8727)
2022-11-08 23:37:32,356:INFO: Dataset: univ                Batch: 25/29	Loss 4.5391 (5.8111)
2022-11-08 23:37:32,444:INFO: Dataset: univ                Batch: 26/29	Loss 4.0980 (5.7415)
2022-11-08 23:37:32,532:INFO: Dataset: univ                Batch: 27/29	Loss 5.6037 (5.7366)
2022-11-08 23:37:32,622:INFO: Dataset: univ                Batch: 28/29	Loss 3.7041 (5.6526)
2022-11-08 23:37:32,661:INFO: Dataset: univ                Batch: 29/29	Loss 1.4482 (5.5869)
2022-11-08 23:37:32,915:INFO: Dataset: zara1               Batch:  1/16	Loss 7.7971 (7.7971)
2022-11-08 23:37:32,996:INFO: Dataset: zara1               Batch:  2/16	Loss 7.8966 (7.8484)
2022-11-08 23:37:33,078:INFO: Dataset: zara1               Batch:  3/16	Loss 6.5767 (7.4258)
2022-11-08 23:37:33,157:INFO: Dataset: zara1               Batch:  4/16	Loss 5.7928 (6.9483)
2022-11-08 23:37:33,238:INFO: Dataset: zara1               Batch:  5/16	Loss 7.9912 (7.1396)
2022-11-08 23:37:33,317:INFO: Dataset: zara1               Batch:  6/16	Loss 7.0046 (7.1110)
2022-11-08 23:37:33,396:INFO: Dataset: zara1               Batch:  7/16	Loss 6.6673 (7.0468)
2022-11-08 23:37:33,476:INFO: Dataset: zara1               Batch:  8/16	Loss 5.8569 (6.9050)
2022-11-08 23:37:33,555:INFO: Dataset: zara1               Batch:  9/16	Loss 7.6307 (6.9799)
2022-11-08 23:37:33,635:INFO: Dataset: zara1               Batch: 10/16	Loss 6.1694 (6.8879)
2022-11-08 23:37:33,716:INFO: Dataset: zara1               Batch: 11/16	Loss 6.6273 (6.8667)
2022-11-08 23:37:33,794:INFO: Dataset: zara1               Batch: 12/16	Loss 5.8275 (6.7763)
2022-11-08 23:37:33,875:INFO: Dataset: zara1               Batch: 13/16	Loss 6.0713 (6.7215)
2022-11-08 23:37:33,955:INFO: Dataset: zara1               Batch: 14/16	Loss 5.7709 (6.6565)
2022-11-08 23:37:34,034:INFO: Dataset: zara1               Batch: 15/16	Loss 6.1453 (6.6225)
2022-11-08 23:37:34,096:INFO: Dataset: zara1               Batch: 16/16	Loss 4.7142 (6.5412)
2022-11-08 23:37:34,361:INFO: Dataset: zara2               Batch:  1/36	Loss 4.8746 (4.8746)
2022-11-08 23:37:34,441:INFO: Dataset: zara2               Batch:  2/36	Loss 5.0149 (4.9441)
2022-11-08 23:37:34,521:INFO: Dataset: zara2               Batch:  3/36	Loss 4.3223 (4.7330)
2022-11-08 23:37:34,603:INFO: Dataset: zara2               Batch:  4/36	Loss 6.3344 (5.0976)
2022-11-08 23:37:34,683:INFO: Dataset: zara2               Batch:  5/36	Loss 4.8651 (5.0446)
2022-11-08 23:37:34,765:INFO: Dataset: zara2               Batch:  6/36	Loss 4.9682 (5.0317)
2022-11-08 23:37:34,844:INFO: Dataset: zara2               Batch:  7/36	Loss 4.1686 (4.8888)
2022-11-08 23:37:34,926:INFO: Dataset: zara2               Batch:  8/36	Loss 7.3036 (5.1922)
2022-11-08 23:37:35,005:INFO: Dataset: zara2               Batch:  9/36	Loss 5.2698 (5.1999)
2022-11-08 23:37:35,086:INFO: Dataset: zara2               Batch: 10/36	Loss 3.9728 (5.0706)
2022-11-08 23:37:35,168:INFO: Dataset: zara2               Batch: 11/36	Loss 4.2451 (4.9915)
2022-11-08 23:37:35,251:INFO: Dataset: zara2               Batch: 12/36	Loss 4.1631 (4.9135)
2022-11-08 23:37:35,333:INFO: Dataset: zara2               Batch: 13/36	Loss 4.6620 (4.8952)
2022-11-08 23:37:35,415:INFO: Dataset: zara2               Batch: 14/36	Loss 4.5238 (4.8703)
2022-11-08 23:37:35,497:INFO: Dataset: zara2               Batch: 15/36	Loss 5.0859 (4.8861)
2022-11-08 23:37:35,580:INFO: Dataset: zara2               Batch: 16/36	Loss 3.8477 (4.8340)
2022-11-08 23:37:35,664:INFO: Dataset: zara2               Batch: 17/36	Loss 4.5688 (4.8183)
2022-11-08 23:37:35,746:INFO: Dataset: zara2               Batch: 18/36	Loss 4.4803 (4.7984)
2022-11-08 23:37:35,827:INFO: Dataset: zara2               Batch: 19/36	Loss 4.0169 (4.7601)
2022-11-08 23:37:35,908:INFO: Dataset: zara2               Batch: 20/36	Loss 3.9705 (4.7187)
2022-11-08 23:37:35,988:INFO: Dataset: zara2               Batch: 21/36	Loss 3.9111 (4.6849)
2022-11-08 23:37:36,069:INFO: Dataset: zara2               Batch: 22/36	Loss 3.6742 (4.6413)
2022-11-08 23:37:36,152:INFO: Dataset: zara2               Batch: 23/36	Loss 3.8839 (4.6047)
2022-11-08 23:37:36,233:INFO: Dataset: zara2               Batch: 24/36	Loss 4.2544 (4.5898)
2022-11-08 23:37:36,314:INFO: Dataset: zara2               Batch: 25/36	Loss 3.9312 (4.5653)
2022-11-08 23:37:36,396:INFO: Dataset: zara2               Batch: 26/36	Loss 4.4093 (4.5595)
2022-11-08 23:37:36,475:INFO: Dataset: zara2               Batch: 27/36	Loss 4.8917 (4.5719)
2022-11-08 23:37:36,555:INFO: Dataset: zara2               Batch: 28/36	Loss 4.2189 (4.5615)
2022-11-08 23:37:36,636:INFO: Dataset: zara2               Batch: 29/36	Loss 4.8095 (4.5714)
2022-11-08 23:37:36,715:INFO: Dataset: zara2               Batch: 30/36	Loss 4.2172 (4.5603)
2022-11-08 23:37:36,795:INFO: Dataset: zara2               Batch: 31/36	Loss 4.1306 (4.5471)
2022-11-08 23:37:36,876:INFO: Dataset: zara2               Batch: 32/36	Loss 4.8477 (4.5575)
2022-11-08 23:37:36,957:INFO: Dataset: zara2               Batch: 33/36	Loss 3.8687 (4.5357)
2022-11-08 23:37:37,038:INFO: Dataset: zara2               Batch: 34/36	Loss 4.5152 (4.5350)
2022-11-08 23:37:37,119:INFO: Dataset: zara2               Batch: 35/36	Loss 4.8452 (4.5442)
2022-11-08 23:37:37,179:INFO: Dataset: zara2               Batch: 36/36	Loss 3.0861 (4.5146)
2022-11-08 23:37:37,225:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 2, 2, 2)_shuffle_true_seed_72/pretrain/P2/CRMF_epoch_3.pth.tar
2022-11-08 23:37:37,225:INFO: 
===> EPOCH: 4 (P2)
2022-11-08 23:37:37,226:INFO: - Computing loss (training)
2022-11-08 23:37:37,433:INFO: Dataset: hotel               Batch: 1/8	Loss 4.1393 (4.1393)
2022-11-08 23:37:37,520:INFO: Dataset: hotel               Batch: 2/8	Loss 4.6275 (4.3930)
2022-11-08 23:37:37,601:INFO: Dataset: hotel               Batch: 3/8	Loss 5.0833 (4.6391)
2022-11-08 23:37:37,681:INFO: Dataset: hotel               Batch: 4/8	Loss 5.3957 (4.8205)
2022-11-08 23:37:37,762:INFO: Dataset: hotel               Batch: 5/8	Loss 4.5385 (4.7625)
2022-11-08 23:37:37,841:INFO: Dataset: hotel               Batch: 6/8	Loss 3.5626 (4.5377)
2022-11-08 23:37:37,919:INFO: Dataset: hotel               Batch: 7/8	Loss 5.3304 (4.6403)
2022-11-08 23:37:37,945:INFO: Dataset: hotel               Batch: 8/8	Loss 0.8102 (4.5594)
2022-11-08 23:37:38,183:INFO: Dataset: univ                Batch:  1/29	Loss 4.1209 (4.1209)
2022-11-08 23:37:38,269:INFO: Dataset: univ                Batch:  2/29	Loss 4.4237 (4.2643)
2022-11-08 23:37:38,350:INFO: Dataset: univ                Batch:  3/29	Loss 3.7493 (4.0980)
2022-11-08 23:37:38,430:INFO: Dataset: univ                Batch:  4/29	Loss 3.1780 (3.8436)
2022-11-08 23:37:38,511:INFO: Dataset: univ                Batch:  5/29	Loss 3.5839 (3.7901)
2022-11-08 23:37:38,596:INFO: Dataset: univ                Batch:  6/29	Loss 4.6713 (3.9280)
2022-11-08 23:37:38,680:INFO: Dataset: univ                Batch:  7/29	Loss 2.9163 (3.7622)
2022-11-08 23:37:38,761:INFO: Dataset: univ                Batch:  8/29	Loss 3.1616 (3.6819)
2022-11-08 23:37:38,840:INFO: Dataset: univ                Batch:  9/29	Loss 3.2044 (3.6305)
2022-11-08 23:37:38,922:INFO: Dataset: univ                Batch: 10/29	Loss 3.1938 (3.5853)
2022-11-08 23:37:39,002:INFO: Dataset: univ                Batch: 11/29	Loss 3.2463 (3.5567)
2022-11-08 23:37:39,082:INFO: Dataset: univ                Batch: 12/29	Loss 3.7449 (3.5711)
2022-11-08 23:37:39,161:INFO: Dataset: univ                Batch: 13/29	Loss 4.0654 (3.6061)
2022-11-08 23:37:39,243:INFO: Dataset: univ                Batch: 14/29	Loss 3.7479 (3.6158)
2022-11-08 23:37:39,324:INFO: Dataset: univ                Batch: 15/29	Loss 2.9763 (3.5708)
2022-11-08 23:37:39,405:INFO: Dataset: univ                Batch: 16/29	Loss 4.2577 (3.6084)
2022-11-08 23:37:39,487:INFO: Dataset: univ                Batch: 17/29	Loss 3.0132 (3.5735)
2022-11-08 23:37:39,567:INFO: Dataset: univ                Batch: 18/29	Loss 3.3846 (3.5624)
2022-11-08 23:37:39,654:INFO: Dataset: univ                Batch: 19/29	Loss 3.2893 (3.5461)
2022-11-08 23:37:39,739:INFO: Dataset: univ                Batch: 20/29	Loss 3.2578 (3.5318)
2022-11-08 23:37:39,821:INFO: Dataset: univ                Batch: 21/29	Loss 3.1598 (3.5145)
2022-11-08 23:37:39,904:INFO: Dataset: univ                Batch: 22/29	Loss 3.0719 (3.4951)
2022-11-08 23:37:39,987:INFO: Dataset: univ                Batch: 23/29	Loss 4.7250 (3.5437)
2022-11-08 23:37:40,069:INFO: Dataset: univ                Batch: 24/29	Loss 2.9061 (3.5144)
2022-11-08 23:37:40,152:INFO: Dataset: univ                Batch: 25/29	Loss 3.2501 (3.5037)
2022-11-08 23:37:40,235:INFO: Dataset: univ                Batch: 26/29	Loss 3.1715 (3.4903)
2022-11-08 23:37:40,317:INFO: Dataset: univ                Batch: 27/29	Loss 3.2646 (3.4827)
2022-11-08 23:37:40,399:INFO: Dataset: univ                Batch: 28/29	Loss 3.2334 (3.4738)
2022-11-08 23:37:40,437:INFO: Dataset: univ                Batch: 29/29	Loss 1.0041 (3.4381)
2022-11-08 23:37:40,685:INFO: Dataset: zara1               Batch:  1/16	Loss 4.4080 (4.4080)
2022-11-08 23:37:40,771:INFO: Dataset: zara1               Batch:  2/16	Loss 5.3264 (4.8637)
2022-11-08 23:37:40,849:INFO: Dataset: zara1               Batch:  3/16	Loss 6.3977 (5.3182)
2022-11-08 23:37:40,931:INFO: Dataset: zara1               Batch:  4/16	Loss 5.9997 (5.4835)
2022-11-08 23:37:41,012:INFO: Dataset: zara1               Batch:  5/16	Loss 6.5440 (5.7093)
2022-11-08 23:37:41,094:INFO: Dataset: zara1               Batch:  6/16	Loss 5.9575 (5.7448)
2022-11-08 23:37:41,175:INFO: Dataset: zara1               Batch:  7/16	Loss 4.3648 (5.5620)
2022-11-08 23:37:41,256:INFO: Dataset: zara1               Batch:  8/16	Loss 5.3233 (5.5294)
2022-11-08 23:37:41,337:INFO: Dataset: zara1               Batch:  9/16	Loss 4.6807 (5.4571)
2022-11-08 23:37:41,416:INFO: Dataset: zara1               Batch: 10/16	Loss 4.9724 (5.4108)
2022-11-08 23:37:41,495:INFO: Dataset: zara1               Batch: 11/16	Loss 4.7906 (5.3525)
2022-11-08 23:37:41,573:INFO: Dataset: zara1               Batch: 12/16	Loss 5.0862 (5.3315)
2022-11-08 23:37:41,652:INFO: Dataset: zara1               Batch: 13/16	Loss 5.1190 (5.3133)
2022-11-08 23:37:41,734:INFO: Dataset: zara1               Batch: 14/16	Loss 4.4973 (5.2463)
2022-11-08 23:37:41,815:INFO: Dataset: zara1               Batch: 15/16	Loss 4.3218 (5.1864)
2022-11-08 23:37:41,882:INFO: Dataset: zara1               Batch: 16/16	Loss 3.5806 (5.1214)
2022-11-08 23:37:42,140:INFO: Dataset: zara2               Batch:  1/36	Loss 3.6541 (3.6541)
2022-11-08 23:37:42,220:INFO: Dataset: zara2               Batch:  2/36	Loss 4.7529 (4.2229)
2022-11-08 23:37:42,304:INFO: Dataset: zara2               Batch:  3/36	Loss 4.3605 (4.2663)
2022-11-08 23:37:42,385:INFO: Dataset: zara2               Batch:  4/36	Loss 3.1567 (3.9908)
2022-11-08 23:37:42,468:INFO: Dataset: zara2               Batch:  5/36	Loss 3.3808 (3.8706)
2022-11-08 23:37:42,549:INFO: Dataset: zara2               Batch:  6/36	Loss 3.0122 (3.7320)
2022-11-08 23:37:42,628:INFO: Dataset: zara2               Batch:  7/36	Loss 3.1474 (3.6467)
2022-11-08 23:37:42,707:INFO: Dataset: zara2               Batch:  8/36	Loss 3.9837 (3.6854)
2022-11-08 23:37:42,785:INFO: Dataset: zara2               Batch:  9/36	Loss 4.3391 (3.7584)
2022-11-08 23:37:42,865:INFO: Dataset: zara2               Batch: 10/36	Loss 3.8487 (3.7678)
2022-11-08 23:37:42,945:INFO: Dataset: zara2               Batch: 11/36	Loss 3.8414 (3.7752)
2022-11-08 23:37:43,024:INFO: Dataset: zara2               Batch: 12/36	Loss 3.5646 (3.7582)
2022-11-08 23:37:43,106:INFO: Dataset: zara2               Batch: 13/36	Loss 3.5061 (3.7393)
2022-11-08 23:37:43,187:INFO: Dataset: zara2               Batch: 14/36	Loss 3.6136 (3.7303)
2022-11-08 23:37:43,268:INFO: Dataset: zara2               Batch: 15/36	Loss 3.2216 (3.6971)
2022-11-08 23:37:43,351:INFO: Dataset: zara2               Batch: 16/36	Loss 3.3505 (3.6799)
2022-11-08 23:37:43,432:INFO: Dataset: zara2               Batch: 17/36	Loss 3.8564 (3.6900)
2022-11-08 23:37:43,513:INFO: Dataset: zara2               Batch: 18/36	Loss 3.1708 (3.6603)
2022-11-08 23:37:43,595:INFO: Dataset: zara2               Batch: 19/36	Loss 3.3415 (3.6441)
2022-11-08 23:37:43,676:INFO: Dataset: zara2               Batch: 20/36	Loss 4.5923 (3.6894)
2022-11-08 23:37:43,756:INFO: Dataset: zara2               Batch: 21/36	Loss 4.2659 (3.7136)
2022-11-08 23:37:43,837:INFO: Dataset: zara2               Batch: 22/36	Loss 3.7746 (3.7159)
2022-11-08 23:37:43,918:INFO: Dataset: zara2               Batch: 23/36	Loss 3.2133 (3.6930)
2022-11-08 23:37:43,997:INFO: Dataset: zara2               Batch: 24/36	Loss 3.2246 (3.6744)
2022-11-08 23:37:44,079:INFO: Dataset: zara2               Batch: 25/36	Loss 3.0805 (3.6523)
2022-11-08 23:37:44,161:INFO: Dataset: zara2               Batch: 26/36	Loss 3.2804 (3.6383)
2022-11-08 23:37:44,242:INFO: Dataset: zara2               Batch: 27/36	Loss 2.9720 (3.6148)
2022-11-08 23:37:44,325:INFO: Dataset: zara2               Batch: 28/36	Loss 3.0087 (3.5928)
2022-11-08 23:37:44,408:INFO: Dataset: zara2               Batch: 29/36	Loss 3.6835 (3.5956)
2022-11-08 23:37:44,488:INFO: Dataset: zara2               Batch: 30/36	Loss 3.2030 (3.5813)
2022-11-08 23:37:44,569:INFO: Dataset: zara2               Batch: 31/36	Loss 3.0929 (3.5678)
2022-11-08 23:37:44,650:INFO: Dataset: zara2               Batch: 32/36	Loss 3.3662 (3.5618)
2022-11-08 23:37:44,729:INFO: Dataset: zara2               Batch: 33/36	Loss 3.8419 (3.5698)
2022-11-08 23:37:44,810:INFO: Dataset: zara2               Batch: 34/36	Loss 3.6052 (3.5708)
2022-11-08 23:37:44,891:INFO: Dataset: zara2               Batch: 35/36	Loss 3.4704 (3.5682)
2022-11-08 23:37:44,952:INFO: Dataset: zara2               Batch: 36/36	Loss 2.5078 (3.5475)
2022-11-08 23:37:44,990:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 2, 2, 2)_shuffle_true_seed_72/pretrain/P2/CRMF_epoch_4.pth.tar
2022-11-08 23:37:44,990:INFO: 
===> EPOCH: 5 (P3)
2022-11-08 23:37:44,990:INFO: - Computing loss (training)
2022-11-08 23:37:45,772:INFO: Dataset: hotel               Batch: 1/8	Loss 74.7378 (74.7378)
2022-11-08 23:37:46,429:INFO: Dataset: hotel               Batch: 2/8	Loss 75.4165 (75.0847)
2022-11-08 23:37:47,061:INFO: Dataset: hotel               Batch: 3/8	Loss 74.6434 (74.9510)
2022-11-08 23:37:47,706:INFO: Dataset: hotel               Batch: 4/8	Loss 72.8510 (74.4434)
2022-11-08 23:37:48,403:INFO: Dataset: hotel               Batch: 5/8	Loss 71.5818 (73.8646)
2022-11-08 23:37:49,038:INFO: Dataset: hotel               Batch: 6/8	Loss 72.8334 (73.7076)
2022-11-08 23:37:49,672:INFO: Dataset: hotel               Batch: 7/8	Loss 74.7777 (73.8655)
2022-11-08 23:37:50,270:INFO: Dataset: hotel               Batch: 8/8	Loss 15.1462 (71.8514)
2022-11-08 23:37:51,236:INFO: Dataset: univ                Batch:  1/29	Loss 71.3615 (71.3615)
2022-11-08 23:37:52,075:INFO: Dataset: univ                Batch:  2/29	Loss 70.3352 (70.8357)
2022-11-08 23:37:52,960:INFO: Dataset: univ                Batch:  3/29	Loss 69.3832 (70.3578)
2022-11-08 23:37:53,665:INFO: Dataset: univ                Batch:  4/29	Loss 68.6089 (69.9139)
2022-11-08 23:37:54,408:INFO: Dataset: univ                Batch:  5/29	Loss 65.7331 (69.0777)
2022-11-08 23:37:55,110:INFO: Dataset: univ                Batch:  6/29	Loss 64.9806 (68.3972)
2022-11-08 23:37:55,859:INFO: Dataset: univ                Batch:  7/29	Loss 62.6787 (67.5539)
2022-11-08 23:37:56,593:INFO: Dataset: univ                Batch:  8/29	Loss 63.8921 (67.1805)
2022-11-08 23:37:57,327:INFO: Dataset: univ                Batch:  9/29	Loss 59.0028 (66.1661)
2022-11-08 23:37:58,064:INFO: Dataset: univ                Batch: 10/29	Loss 61.4631 (65.7562)
2022-11-08 23:37:58,788:INFO: Dataset: univ                Batch: 11/29	Loss 56.2429 (64.8474)
2022-11-08 23:37:59,532:INFO: Dataset: univ                Batch: 12/29	Loss 57.0913 (64.2141)
2022-11-08 23:38:00,273:INFO: Dataset: univ                Batch: 13/29	Loss 54.2362 (63.2983)
2022-11-08 23:38:01,035:INFO: Dataset: univ                Batch: 14/29	Loss 53.3188 (62.5184)
2022-11-08 23:38:01,784:INFO: Dataset: univ                Batch: 15/29	Loss 51.6574 (61.8618)
2022-11-08 23:38:02,496:INFO: Dataset: univ                Batch: 16/29	Loss 50.7582 (61.1575)
2022-11-08 23:38:03,224:INFO: Dataset: univ                Batch: 17/29	Loss 49.4088 (60.4255)
2022-11-08 23:38:03,950:INFO: Dataset: univ                Batch: 18/29	Loss 47.9629 (59.6290)
2022-11-08 23:38:04,699:INFO: Dataset: univ                Batch: 19/29	Loss 47.1756 (58.8294)
2022-11-08 23:38:05,424:INFO: Dataset: univ                Batch: 20/29	Loss 47.9295 (58.2425)
2022-11-08 23:38:06,154:INFO: Dataset: univ                Batch: 21/29	Loss 46.3728 (57.6388)
2022-11-08 23:38:06,878:INFO: Dataset: univ                Batch: 22/29	Loss 48.9052 (57.3056)
2022-11-08 23:38:07,611:INFO: Dataset: univ                Batch: 23/29	Loss 45.0940 (56.7395)
2022-11-08 23:38:08,366:INFO: Dataset: univ                Batch: 24/29	Loss 44.0076 (56.0940)
2022-11-08 23:38:09,091:INFO: Dataset: univ                Batch: 25/29	Loss 45.5823 (55.6777)
2022-11-08 23:38:09,813:INFO: Dataset: univ                Batch: 26/29	Loss 44.3982 (55.2397)
2022-11-08 23:38:10,554:INFO: Dataset: univ                Batch: 27/29	Loss 44.9594 (54.8920)
2022-11-08 23:38:11,279:INFO: Dataset: univ                Batch: 28/29	Loss 45.5791 (54.6302)
2022-11-08 23:38:11,949:INFO: Dataset: univ                Batch: 29/29	Loss 16.3667 (54.1587)
2022-11-08 23:38:12,848:INFO: Dataset: zara1               Batch:  1/16	Loss 49.6767 (49.6767)
2022-11-08 23:38:13,543:INFO: Dataset: zara1               Batch:  2/16	Loss 48.7242 (49.1839)
2022-11-08 23:38:14,247:INFO: Dataset: zara1               Batch:  3/16	Loss 47.0452 (48.3569)
2022-11-08 23:38:14,935:INFO: Dataset: zara1               Batch:  4/16	Loss 48.6951 (48.4235)
2022-11-08 23:38:15,651:INFO: Dataset: zara1               Batch:  5/16	Loss 48.2915 (48.3950)
2022-11-08 23:38:16,375:INFO: Dataset: zara1               Batch:  6/16	Loss 48.9176 (48.4972)
2022-11-08 23:38:17,050:INFO: Dataset: zara1               Batch:  7/16	Loss 47.1159 (48.3131)
2022-11-08 23:38:17,798:INFO: Dataset: zara1               Batch:  8/16	Loss 46.6313 (48.1152)
2022-11-08 23:38:18,484:INFO: Dataset: zara1               Batch:  9/16	Loss 47.1170 (48.0085)
2022-11-08 23:38:19,154:INFO: Dataset: zara1               Batch: 10/16	Loss 46.2009 (47.8339)
2022-11-08 23:38:19,853:INFO: Dataset: zara1               Batch: 11/16	Loss 45.1290 (47.6013)
2022-11-08 23:38:20,556:INFO: Dataset: zara1               Batch: 12/16	Loss 48.5575 (47.6795)
2022-11-08 23:38:21,252:INFO: Dataset: zara1               Batch: 13/16	Loss 45.3315 (47.4895)
2022-11-08 23:38:21,949:INFO: Dataset: zara1               Batch: 14/16	Loss 45.8220 (47.3528)
2022-11-08 23:38:22,621:INFO: Dataset: zara1               Batch: 15/16	Loss 45.7172 (47.2549)
2022-11-08 23:38:23,301:INFO: Dataset: zara1               Batch: 16/16	Loss 32.8622 (46.5277)
2022-11-08 23:38:24,233:INFO: Dataset: zara2               Batch:  1/36	Loss 43.7996 (43.7996)
2022-11-08 23:38:24,943:INFO: Dataset: zara2               Batch:  2/36	Loss 42.9040 (43.3144)
2022-11-08 23:38:25,661:INFO: Dataset: zara2               Batch:  3/36	Loss 43.6978 (43.4435)
2022-11-08 23:38:26,352:INFO: Dataset: zara2               Batch:  4/36	Loss 43.8288 (43.5370)
2022-11-08 23:38:27,058:INFO: Dataset: zara2               Batch:  5/36	Loss 42.1852 (43.2772)
2022-11-08 23:38:27,782:INFO: Dataset: zara2               Batch:  6/36	Loss 42.6958 (43.1675)
2022-11-08 23:38:28,480:INFO: Dataset: zara2               Batch:  7/36	Loss 43.7004 (43.2443)
2022-11-08 23:38:29,174:INFO: Dataset: zara2               Batch:  8/36	Loss 43.2369 (43.2434)
2022-11-08 23:38:29,884:INFO: Dataset: zara2               Batch:  9/36	Loss 42.8650 (43.1974)
2022-11-08 23:38:30,629:INFO: Dataset: zara2               Batch: 10/36	Loss 43.5524 (43.2404)
2022-11-08 23:38:31,369:INFO: Dataset: zara2               Batch: 11/36	Loss 42.8111 (43.2006)
2022-11-08 23:38:32,126:INFO: Dataset: zara2               Batch: 12/36	Loss 43.3667 (43.2142)
2022-11-08 23:38:32,848:INFO: Dataset: zara2               Batch: 13/36	Loss 42.9745 (43.1941)
2022-11-08 23:38:33,567:INFO: Dataset: zara2               Batch: 14/36	Loss 42.1149 (43.1152)
2022-11-08 23:38:34,291:INFO: Dataset: zara2               Batch: 15/36	Loss 42.8312 (43.0983)
2022-11-08 23:38:35,012:INFO: Dataset: zara2               Batch: 16/36	Loss 43.3007 (43.1116)
2022-11-08 23:38:35,735:INFO: Dataset: zara2               Batch: 17/36	Loss 41.8791 (43.0408)
2022-11-08 23:38:36,457:INFO: Dataset: zara2               Batch: 18/36	Loss 42.3488 (42.9970)
2022-11-08 23:38:37,197:INFO: Dataset: zara2               Batch: 19/36	Loss 43.0635 (43.0004)
2022-11-08 23:38:37,930:INFO: Dataset: zara2               Batch: 20/36	Loss 43.2785 (43.0132)
2022-11-08 23:38:38,644:INFO: Dataset: zara2               Batch: 21/36	Loss 44.2270 (43.0637)
2022-11-08 23:38:39,358:INFO: Dataset: zara2               Batch: 22/36	Loss 42.3113 (43.0293)
2022-11-08 23:38:40,074:INFO: Dataset: zara2               Batch: 23/36	Loss 42.4934 (43.0063)

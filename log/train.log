2022-11-08 16:33:37,905:INFO: Initializing Training Set
2022-11-08 16:33:41,722:INFO: Initializing Validation Set
2022-11-08 16:33:42,149:INFO: Initializing Validation O Set
2022-11-08 16:33:44,539:INFO: 
===> EPOCH: 251 (P3)
2022-11-08 16:33:44,539:INFO: - Computing loss (training)
2022-11-08 16:33:53,280:INFO: Dataset: hotel               Batch: 1/8	Loss 74.0936 (74.0936)
2022-11-08 16:33:55,090:INFO: Dataset: hotel               Batch: 2/8	Loss 74.5566 (74.3390)
2022-11-08 16:33:56,979:INFO: Dataset: hotel               Batch: 3/8	Loss 75.9195 (74.8167)
2022-11-08 16:33:58,730:INFO: Dataset: hotel               Batch: 4/8	Loss 72.4676 (74.2670)
2022-11-08 16:34:00,554:INFO: Dataset: hotel               Batch: 5/8	Loss 73.5462 (74.1167)
2022-11-08 16:34:02,336:INFO: Dataset: hotel               Batch: 6/8	Loss 73.3607 (73.9832)
2022-11-08 16:34:04,072:INFO: Dataset: hotel               Batch: 7/8	Loss 71.1658 (73.5474)
2022-11-08 16:34:05,748:INFO: Dataset: hotel               Batch: 8/8	Loss 16.2313 (71.9595)
2022-11-08 16:34:31,449:INFO: Dataset: univ                Batch:  1/29	Loss 72.1638 (72.1638)
2022-11-08 16:34:33,444:INFO: Dataset: univ                Batch:  2/29	Loss 70.6396 (71.4265)
2022-11-08 16:34:35,391:INFO: Dataset: univ                Batch:  3/29	Loss 70.3745 (71.0519)
2022-11-08 16:34:37,211:INFO: Dataset: univ                Batch:  4/29	Loss 69.6281 (70.7351)
2022-11-08 16:34:39,053:INFO: Dataset: univ                Batch:  5/29	Loss 68.1456 (70.2360)
2022-11-08 16:34:40,849:INFO: Dataset: univ                Batch:  6/29	Loss 65.3433 (69.2883)
2022-11-08 16:34:42,720:INFO: Dataset: univ                Batch:  7/29	Loss 65.9061 (68.8483)
2022-11-08 16:34:44,567:INFO: Dataset: univ                Batch:  8/29	Loss 65.0517 (68.3540)
2022-11-08 16:34:46,510:INFO: Dataset: univ                Batch:  9/29	Loss 64.3335 (67.9049)
2022-11-08 16:34:49,871:INFO: Dataset: univ                Batch: 10/29	Loss 61.9744 (67.3162)
2022-11-08 16:34:51,934:INFO: Dataset: univ                Batch: 11/29	Loss 61.1298 (66.7603)
2022-11-08 16:34:53,914:INFO: Dataset: univ                Batch: 12/29	Loss 58.9594 (66.0590)
2022-11-08 16:34:56,011:INFO: Dataset: univ                Batch: 13/29	Loss 57.7098 (65.3568)
2022-11-08 16:34:57,974:INFO: Dataset: univ                Batch: 14/29	Loss 55.5217 (64.5783)
2022-11-08 16:35:00,326:INFO: Dataset: univ                Batch: 15/29	Loss 56.7107 (64.0858)
2022-11-08 16:35:02,469:INFO: Dataset: univ                Batch: 16/29	Loss 55.4383 (63.6011)
2022-11-08 16:35:04,367:INFO: Dataset: univ                Batch: 17/29	Loss 51.3229 (62.7659)
2022-11-08 16:35:06,223:INFO: Dataset: univ                Batch: 18/29	Loss 53.7372 (62.3151)
2022-11-08 16:35:08,066:INFO: Dataset: univ                Batch: 19/29	Loss 49.6095 (61.5396)
2022-11-08 16:35:10,012:INFO: Dataset: univ                Batch: 20/29	Loss 48.5047 (60.8920)
2022-11-08 16:35:11,916:INFO: Dataset: univ                Batch: 21/29	Loss 48.5913 (60.3636)
2022-11-08 16:35:13,786:INFO: Dataset: univ                Batch: 22/29	Loss 46.7350 (59.7704)
2022-11-08 16:35:15,643:INFO: Dataset: univ                Batch: 23/29	Loss 47.9284 (59.3291)
2022-11-08 16:35:17,613:INFO: Dataset: univ                Batch: 24/29	Loss 46.5559 (58.8002)
2022-11-08 16:35:19,536:INFO: Dataset: univ                Batch: 25/29	Loss 47.0294 (58.3825)
2022-11-08 16:35:21,549:INFO: Dataset: univ                Batch: 26/29	Loss 43.9835 (57.7899)
2022-11-08 16:35:23,562:INFO: Dataset: univ                Batch: 27/29	Loss 45.8978 (57.3090)
2022-11-08 16:35:25,598:INFO: Dataset: univ                Batch: 28/29	Loss 43.9527 (56.8226)
2022-11-08 16:35:27,445:INFO: Dataset: univ                Batch: 29/29	Loss 15.8723 (56.2113)
2022-11-08 16:35:39,885:INFO: Dataset: zara1               Batch:  1/16	Loss 50.7132 (50.7132)
2022-11-08 16:35:41,679:INFO: Dataset: zara1               Batch:  2/16	Loss 48.0937 (49.3369)
2022-11-08 16:35:43,520:INFO: Dataset: zara1               Batch:  3/16	Loss 47.2802 (48.6322)
2022-11-08 16:35:45,331:INFO: Dataset: zara1               Batch:  4/16	Loss 48.3486 (48.5585)
2022-11-08 16:35:47,130:INFO: Dataset: zara1               Batch:  5/16	Loss 48.6682 (48.5823)
2022-11-08 16:35:48,979:INFO: Dataset: zara1               Batch:  6/16	Loss 47.9838 (48.4885)
2022-11-08 16:35:50,742:INFO: Dataset: zara1               Batch:  7/16	Loss 46.7908 (48.2482)
2022-11-08 16:35:52,632:INFO: Dataset: zara1               Batch:  8/16	Loss 47.6714 (48.1701)
2022-11-08 16:35:54,374:INFO: Dataset: zara1               Batch:  9/16	Loss 46.9345 (48.0566)
2022-11-08 16:35:56,232:INFO: Dataset: zara1               Batch: 10/16	Loss 46.6303 (47.9193)
2022-11-08 16:35:58,037:INFO: Dataset: zara1               Batch: 11/16	Loss 45.8053 (47.7235)
2022-11-08 16:35:59,926:INFO: Dataset: zara1               Batch: 12/16	Loss 47.2910 (47.6820)
2022-11-08 16:36:01,737:INFO: Dataset: zara1               Batch: 13/16	Loss 46.6755 (47.6095)
2022-11-08 16:36:03,552:INFO: Dataset: zara1               Batch: 14/16	Loss 46.1723 (47.5082)
2022-11-08 16:36:05,394:INFO: Dataset: zara1               Batch: 15/16	Loss 46.8320 (47.4637)
2022-11-08 16:36:07,185:INFO: Dataset: zara1               Batch: 16/16	Loss 34.3002 (46.9303)
2022-11-08 16:36:32,491:INFO: Dataset: zara2               Batch:  1/36	Loss 44.1030 (44.1030)
2022-11-08 16:36:34,337:INFO: Dataset: zara2               Batch:  2/36	Loss 42.4972 (43.3644)
2022-11-08 16:36:36,156:INFO: Dataset: zara2               Batch:  3/36	Loss 43.0679 (43.2662)
2022-11-08 16:36:38,023:INFO: Dataset: zara2               Batch:  4/36	Loss 43.4803 (43.3173)
2022-11-08 16:36:39,828:INFO: Dataset: zara2               Batch:  5/36	Loss 42.3516 (43.1037)
2022-11-08 16:36:41,711:INFO: Dataset: zara2               Batch:  6/36	Loss 43.8046 (43.2141)
2022-11-08 16:36:43,520:INFO: Dataset: zara2               Batch:  7/36	Loss 44.4813 (43.4290)
2022-11-08 16:36:45,359:INFO: Dataset: zara2               Batch:  8/36	Loss 43.8893 (43.4878)
2022-11-08 16:36:47,227:INFO: Dataset: zara2               Batch:  9/36	Loss 43.8177 (43.5283)
2022-11-08 16:36:49,045:INFO: Dataset: zara2               Batch: 10/36	Loss 44.2770 (43.5961)
2022-11-08 16:36:50,897:INFO: Dataset: zara2               Batch: 11/36	Loss 43.1992 (43.5597)
2022-11-08 16:36:52,786:INFO: Dataset: zara2               Batch: 12/36	Loss 43.0076 (43.5084)
2022-11-08 16:36:54,572:INFO: Dataset: zara2               Batch: 13/36	Loss 43.1505 (43.4859)
2022-11-08 16:36:56,490:INFO: Dataset: zara2               Batch: 14/36	Loss 42.9333 (43.4526)
2022-11-08 16:36:59,028:INFO: Dataset: zara2               Batch: 15/36	Loss 42.1887 (43.3784)
2022-11-08 16:37:01,203:INFO: Dataset: zara2               Batch: 16/36	Loss 44.0597 (43.4143)
2022-11-08 16:37:03,088:INFO: Dataset: zara2               Batch: 17/36	Loss 43.1374 (43.3952)
2022-11-08 16:37:04,876:INFO: Dataset: zara2               Batch: 18/36	Loss 42.6078 (43.3569)
2022-11-08 16:37:06,742:INFO: Dataset: zara2               Batch: 19/36	Loss 43.5250 (43.3661)
2022-11-08 16:37:08,644:INFO: Dataset: zara2               Batch: 20/36	Loss 43.0406 (43.3495)
2022-11-08 16:37:10,468:INFO: Dataset: zara2               Batch: 21/36	Loss 42.2025 (43.2869)
2022-11-08 16:37:12,434:INFO: Dataset: zara2               Batch: 22/36	Loss 42.5046 (43.2527)
2022-11-08 16:37:14,276:INFO: Dataset: zara2               Batch: 23/36	Loss 41.8621 (43.1973)
2022-11-08 16:37:16,168:INFO: Dataset: zara2               Batch: 24/36	Loss 43.9409 (43.2270)
2022-11-08 16:37:17,996:INFO: Dataset: zara2               Batch: 25/36	Loss 42.8503 (43.2131)
2022-11-08 16:37:19,831:INFO: Dataset: zara2               Batch: 26/36	Loss 42.5910 (43.1901)
2022-11-08 16:37:21,700:INFO: Dataset: zara2               Batch: 27/36	Loss 42.4305 (43.1666)
2022-11-08 16:37:23,589:INFO: Dataset: zara2               Batch: 28/36	Loss 43.7782 (43.1895)
2022-11-08 16:37:25,382:INFO: Dataset: zara2               Batch: 29/36	Loss 41.7223 (43.1354)
2022-11-08 16:37:27,280:INFO: Dataset: zara2               Batch: 30/36	Loss 42.4065 (43.1061)
2022-11-08 16:37:29,180:INFO: Dataset: zara2               Batch: 31/36	Loss 42.7934 (43.0957)
2022-11-08 16:37:31,033:INFO: Dataset: zara2               Batch: 32/36	Loss 43.1267 (43.0966)
2022-11-08 16:37:32,963:INFO: Dataset: zara2               Batch: 33/36	Loss 43.4299 (43.1070)
2022-11-08 16:37:34,808:INFO: Dataset: zara2               Batch: 34/36	Loss 42.2481 (43.0826)
2022-11-08 16:37:36,679:INFO: Dataset: zara2               Batch: 35/36	Loss 43.2181 (43.0863)
2022-11-08 16:37:38,555:INFO: Dataset: zara2               Batch: 36/36	Loss 30.8506 (42.8658)
2022-11-08 16:37:39,495:INFO: - Computing ADE (validation o)
2022-11-08 16:37:48,443:INFO: 		 ADE on eth                       dataset:	 2.890359401702881
2022-11-08 16:37:48,444:INFO: Average validation o:	ADE  2.8904	FDE  4.8563
2022-11-08 16:37:48,445:INFO: - Computing ADE (validation)
2022-11-08 16:37:57,191:INFO: 		 ADE on hotel                     dataset:	 1.0468430519104004
2022-11-08 16:38:06,158:INFO: 		 ADE on univ                      dataset:	 1.5669435262680054
2022-11-08 16:38:15,244:INFO: 		 ADE on zara1                     dataset:	 2.5956547260284424
2022-11-08 16:38:24,632:INFO: 		 ADE on zara2                     dataset:	 1.5041944980621338
2022-11-08 16:38:24,632:INFO: Average validation:	ADE  1.5752	FDE  2.8088
2022-11-08 16:38:24,636:INFO: - Computing ADE (training)
2022-11-08 16:38:33,540:INFO: 		 ADE on hotel                     dataset:	 1.3880194425582886
2022-11-08 16:38:57,744:INFO: 		 ADE on univ                      dataset:	 1.446062684059143
2022-11-08 16:39:06,889:INFO: 		 ADE on zara1                     dataset:	 2.5028414726257324
2022-11-08 16:39:32,287:INFO: 		 ADE on zara2                     dataset:	 1.7593460083007812
2022-11-08 16:39:32,287:INFO: Average training:	ADE  1.5755	FDE  2.8173
2022-11-08 16:39:32,304:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 400, 300, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_251.pth.tar
2022-11-08 16:39:32,304:INFO: 
===> EPOCH: 252 (P3)
2022-11-08 16:39:32,305:INFO: - Computing loss (training)
2022-11-08 16:39:41,474:INFO: Dataset: hotel               Batch: 1/8	Loss 42.4205 (42.4205)
2022-11-08 16:39:43,267:INFO: Dataset: hotel               Batch: 2/8	Loss 42.7457 (42.5955)
2022-11-08 16:39:45,056:INFO: Dataset: hotel               Batch: 3/8	Loss 43.0515 (42.7485)
2022-11-08 16:39:46,812:INFO: Dataset: hotel               Batch: 4/8	Loss 43.5490 (42.9350)
2022-11-08 16:39:48,697:INFO: Dataset: hotel               Batch: 5/8	Loss 40.9569 (42.5333)
2022-11-08 16:39:50,510:INFO: Dataset: hotel               Batch: 6/8	Loss 41.6990 (42.3913)
2022-11-08 16:39:52,394:INFO: Dataset: hotel               Batch: 7/8	Loss 41.9814 (42.3298)
2022-11-08 16:39:54,078:INFO: Dataset: hotel               Batch: 8/8	Loss 8.7640 (41.2227)
2022-11-08 16:40:20,705:INFO: Dataset: univ                Batch:  1/29	Loss 42.3705 (42.3705)
2022-11-08 16:40:22,550:INFO: Dataset: univ                Batch:  2/29	Loss 41.7075 (42.0365)
2022-11-08 16:40:24,418:INFO: Dataset: univ                Batch:  3/29	Loss 42.0353 (42.0361)
2022-11-08 16:40:26,458:INFO: Dataset: univ                Batch:  4/29	Loss 41.9514 (42.0153)
2022-11-08 16:40:28,570:INFO: Dataset: univ                Batch:  5/29	Loss 41.8436 (41.9801)
2022-11-08 16:40:30,448:INFO: Dataset: univ                Batch:  6/29	Loss 40.8581 (41.7672)
2022-11-08 16:40:32,400:INFO: Dataset: univ                Batch:  7/29	Loss 41.9784 (41.7981)
2022-11-08 16:40:34,279:INFO: Dataset: univ                Batch:  8/29	Loss 41.1334 (41.7092)
2022-11-08 16:40:36,252:INFO: Dataset: univ                Batch:  9/29	Loss 41.8736 (41.7281)
2022-11-08 16:40:38,090:INFO: Dataset: univ                Batch: 10/29	Loss 41.9193 (41.7467)
2022-11-08 16:40:40,013:INFO: Dataset: univ                Batch: 11/29	Loss 42.0047 (41.7702)
2022-11-08 16:40:41,883:INFO: Dataset: univ                Batch: 12/29	Loss 41.9535 (41.7860)
2022-11-08 16:40:43,830:INFO: Dataset: univ                Batch: 13/29	Loss 41.7422 (41.7829)
2022-11-08 16:40:45,734:INFO: Dataset: univ                Batch: 14/29	Loss 43.5433 (41.8901)
2022-11-08 16:40:47,646:INFO: Dataset: univ                Batch: 15/29	Loss 41.8881 (41.8899)
2022-11-08 16:40:49,489:INFO: Dataset: univ                Batch: 16/29	Loss 42.6055 (41.9352)
2022-11-08 16:40:51,371:INFO: Dataset: univ                Batch: 17/29	Loss 41.8726 (41.9316)
2022-11-08 16:40:53,261:INFO: Dataset: univ                Batch: 18/29	Loss 41.8721 (41.9283)
2022-11-08 16:40:55,144:INFO: Dataset: univ                Batch: 19/29	Loss 41.9665 (41.9302)
2022-11-08 16:40:57,165:INFO: Dataset: univ                Batch: 20/29	Loss 41.5273 (41.9065)
2022-11-08 16:40:59,023:INFO: Dataset: univ                Batch: 21/29	Loss 42.2371 (41.9235)
2022-11-08 16:41:00,895:INFO: Dataset: univ                Batch: 22/29	Loss 43.5787 (41.9907)
2022-11-08 16:41:02,780:INFO: Dataset: univ                Batch: 23/29	Loss 42.2026 (41.9999)
2022-11-08 16:41:04,658:INFO: Dataset: univ                Batch: 24/29	Loss 42.8600 (42.0335)
2022-11-08 16:41:06,569:INFO: Dataset: univ                Batch: 25/29	Loss 41.5123 (42.0093)
2022-11-08 16:41:08,488:INFO: Dataset: univ                Batch: 26/29	Loss 42.0425 (42.0105)
2022-11-08 16:41:10,478:INFO: Dataset: univ                Batch: 27/29	Loss 41.6353 (41.9964)
2022-11-08 16:41:12,386:INFO: Dataset: univ                Batch: 28/29	Loss 41.6179 (41.9807)
2022-11-08 16:41:14,215:INFO: Dataset: univ                Batch: 29/29	Loss 15.6283 (41.5886)
2022-11-08 16:41:24,200:INFO: Dataset: zara1               Batch:  1/16	Loss 46.8583 (46.8583)
2022-11-08 16:41:26,143:INFO: Dataset: zara1               Batch:  2/16	Loss 46.8688 (46.8634)
2022-11-08 16:41:27,973:INFO: Dataset: zara1               Batch:  3/16	Loss 47.5196 (47.0739)
2022-11-08 16:41:29,846:INFO: Dataset: zara1               Batch:  4/16	Loss 45.9884 (46.7498)
2022-11-08 16:41:31,711:INFO: Dataset: zara1               Batch:  5/16	Loss 44.5620 (46.2601)
2022-11-08 16:41:33,554:INFO: Dataset: zara1               Batch:  6/16	Loss 45.8426 (46.2044)
2022-11-08 16:41:35,432:INFO: Dataset: zara1               Batch:  7/16	Loss 45.2114 (46.0509)
2022-11-08 16:41:37,267:INFO: Dataset: zara1               Batch:  8/16	Loss 44.4518 (45.8754)
2022-11-08 16:41:39,441:INFO: Dataset: zara1               Batch:  9/16	Loss 44.5113 (45.7180)
2022-11-08 16:41:41,285:INFO: Dataset: zara1               Batch: 10/16	Loss 45.9990 (45.7491)
2022-11-08 16:41:43,120:INFO: Dataset: zara1               Batch: 11/16	Loss 44.9007 (45.6747)
2022-11-08 16:41:44,933:INFO: Dataset: zara1               Batch: 12/16	Loss 44.6944 (45.6001)
2022-11-08 16:41:46,792:INFO: Dataset: zara1               Batch: 13/16	Loss 44.8606 (45.5382)
2022-11-08 16:41:48,733:INFO: Dataset: zara1               Batch: 14/16	Loss 44.7019 (45.4804)
2022-11-08 16:41:50,541:INFO: Dataset: zara1               Batch: 15/16	Loss 44.8799 (45.4438)
2022-11-08 16:41:52,306:INFO: Dataset: zara1               Batch: 16/16	Loss 32.9806 (44.9125)
2022-11-08 16:42:18,088:INFO: Dataset: zara2               Batch:  1/36	Loss 41.8174 (41.8174)
2022-11-08 16:42:19,895:INFO: Dataset: zara2               Batch:  2/36	Loss 43.3499 (42.4931)
2022-11-08 16:42:21,765:INFO: Dataset: zara2               Batch:  3/36	Loss 43.6536 (42.8434)
2022-11-08 16:42:23,591:INFO: Dataset: zara2               Batch:  4/36	Loss 42.8692 (42.8507)
2022-11-08 16:42:25,452:INFO: Dataset: zara2               Batch:  5/36	Loss 42.7700 (42.8368)
2022-11-08 16:42:27,329:INFO: Dataset: zara2               Batch:  6/36	Loss 42.7245 (42.8180)
2022-11-08 16:42:29,264:INFO: Dataset: zara2               Batch:  7/36	Loss 42.1801 (42.7252)
2022-11-08 16:42:31,189:INFO: Dataset: zara2               Batch:  8/36	Loss 42.6076 (42.7124)
2022-11-08 16:42:33,191:INFO: Dataset: zara2               Batch:  9/36	Loss 41.7316 (42.6015)
2022-11-08 16:42:35,013:INFO: Dataset: zara2               Batch: 10/36	Loss 42.5349 (42.5953)
2022-11-08 16:42:36,939:INFO: Dataset: zara2               Batch: 11/36	Loss 43.6306 (42.7018)
2022-11-08 16:42:38,760:INFO: Dataset: zara2               Batch: 12/36	Loss 43.2408 (42.7525)
2022-11-08 16:42:40,647:INFO: Dataset: zara2               Batch: 13/36	Loss 43.3469 (42.7992)
2022-11-08 16:42:42,485:INFO: Dataset: zara2               Batch: 14/36	Loss 42.3502 (42.7651)
2022-11-08 16:42:44,271:INFO: Dataset: zara2               Batch: 15/36	Loss 41.9898 (42.7155)
2022-11-08 16:42:46,129:INFO: Dataset: zara2               Batch: 16/36	Loss 42.9184 (42.7275)
2022-11-08 16:42:47,998:INFO: Dataset: zara2               Batch: 17/36	Loss 42.6051 (42.7196)
2022-11-08 16:42:49,846:INFO: Dataset: zara2               Batch: 18/36	Loss 42.5316 (42.7099)
2022-11-08 16:42:51,655:INFO: Dataset: zara2               Batch: 19/36	Loss 41.8809 (42.6599)
2022-11-08 16:42:53,605:INFO: Dataset: zara2               Batch: 20/36	Loss 42.3494 (42.6450)
2022-11-08 16:42:55,515:INFO: Dataset: zara2               Batch: 21/36	Loss 41.9074 (42.6117)
2022-11-08 16:42:57,425:INFO: Dataset: zara2               Batch: 22/36	Loss 42.5292 (42.6080)
2022-11-08 16:42:59,315:INFO: Dataset: zara2               Batch: 23/36	Loss 42.3630 (42.5965)
2022-11-08 16:43:01,185:INFO: Dataset: zara2               Batch: 24/36	Loss 41.8097 (42.5651)
2022-11-08 16:43:03,106:INFO: Dataset: zara2               Batch: 25/36	Loss 42.0838 (42.5476)
2022-11-08 16:43:05,047:INFO: Dataset: zara2               Batch: 26/36	Loss 41.0248 (42.4837)
2022-11-08 16:43:06,948:INFO: Dataset: zara2               Batch: 27/36	Loss 44.5173 (42.5494)
2022-11-08 16:43:08,833:INFO: Dataset: zara2               Batch: 28/36	Loss 42.8179 (42.5591)
2022-11-08 16:43:10,666:INFO: Dataset: zara2               Batch: 29/36	Loss 42.7101 (42.5643)
2022-11-08 16:43:12,674:INFO: Dataset: zara2               Batch: 30/36	Loss 41.4302 (42.5316)
2022-11-08 16:43:14,782:INFO: Dataset: zara2               Batch: 31/36	Loss 42.1705 (42.5192)
2022-11-08 16:43:16,648:INFO: Dataset: zara2               Batch: 32/36	Loss 42.4032 (42.5155)
2022-11-08 16:43:18,466:INFO: Dataset: zara2               Batch: 33/36	Loss 42.1643 (42.5041)
2022-11-08 16:43:20,327:INFO: Dataset: zara2               Batch: 34/36	Loss 42.4734 (42.5032)
2022-11-08 16:43:22,161:INFO: Dataset: zara2               Batch: 35/36	Loss 41.6478 (42.4805)
2022-11-08 16:43:23,965:INFO: Dataset: zara2               Batch: 36/36	Loss 30.3558 (42.2500)
2022-11-08 16:43:24,902:INFO: - Computing ADE (validation o)
2022-11-08 16:43:33,639:INFO: 		 ADE on eth                       dataset:	 2.8899011611938477
2022-11-08 16:43:33,639:INFO: Average validation o:	ADE  2.8899	FDE  4.8454
2022-11-08 16:43:33,640:INFO: - Computing ADE (validation)
2022-11-08 16:43:42,602:INFO: 		 ADE on hotel                     dataset:	 1.0251294374465942
2022-11-08 16:43:52,272:INFO: 		 ADE on univ                      dataset:	 1.5383732318878174
2022-11-08 16:44:01,394:INFO: 		 ADE on zara1                     dataset:	 2.6011641025543213
2022-11-08 16:44:11,005:INFO: 		 ADE on zara2                     dataset:	 1.4935752153396606
2022-11-08 16:44:11,006:INFO: Average validation:	ADE  1.5556	FDE  2.7799
2022-11-08 16:44:11,007:INFO: - Computing ADE (training)

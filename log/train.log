2022-10-28 14:17:50,805:INFO: Initializing Training Set
2022-10-28 14:17:54,382:INFO: Initializing Validation Set
2022-10-28 14:17:54,803:INFO: Initializing Validation O Set
2022-10-28 14:17:57,140:INFO: => loaded checkpoint './models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_243.pth.tar' (epoch 244)
2022-10-28 14:17:57,141:INFO: 
===> EPOCH: 244 (P2)
2022-10-28 14:17:57,141:INFO: - Computing loss (training)
2022-10-28 14:18:04,390:INFO: Dataset: hotel               Batch: 1/8	Loss 35.5455 (35.5455)
2022-10-28 14:18:06,095:INFO: Dataset: hotel               Batch: 2/8	Loss 35.6256 (35.5863)
2022-10-28 14:18:07,762:INFO: Dataset: hotel               Batch: 3/8	Loss 35.7277 (35.6303)
2022-10-28 14:18:09,454:INFO: Dataset: hotel               Batch: 4/8	Loss 35.6930 (35.6470)
2022-10-28 14:18:11,172:INFO: Dataset: hotel               Batch: 5/8	Loss 35.5790 (35.6316)
2022-10-28 14:18:12,906:INFO: Dataset: hotel               Batch: 6/8	Loss 35.6689 (35.6375)
2022-10-28 14:18:14,633:INFO: Dataset: hotel               Batch: 7/8	Loss 35.6217 (35.6354)
2022-10-28 14:18:15,698:INFO: Dataset: hotel               Batch: 8/8	Loss 7.8347 (34.8652)
2022-10-28 14:18:40,197:INFO: Dataset: univ                Batch:  1/29	Loss 35.4759 (35.4759)
2022-10-28 14:18:42,987:INFO: Dataset: univ                Batch:  2/29	Loss 35.3425 (35.4081)
2022-10-28 14:18:45,768:INFO: Dataset: univ                Batch:  3/29	Loss 35.4377 (35.4178)
2022-10-28 14:18:48,204:INFO: Dataset: univ                Batch:  4/29	Loss 35.3862 (35.4088)
2022-10-28 14:18:50,633:INFO: Dataset: univ                Batch:  5/29	Loss 35.3942 (35.4059)
2022-10-28 14:18:53,263:INFO: Dataset: univ                Batch:  6/29	Loss 35.4194 (35.4081)
2022-10-28 14:18:55,862:INFO: Dataset: univ                Batch:  7/29	Loss 35.3826 (35.4045)
2022-10-28 14:18:58,142:INFO: Dataset: univ                Batch:  8/29	Loss 35.3959 (35.4035)
2022-10-28 14:19:00,432:INFO: Dataset: univ                Batch:  9/29	Loss 35.3928 (35.4025)
2022-10-28 14:19:03,106:INFO: Dataset: univ                Batch: 10/29	Loss 35.4316 (35.4056)
2022-10-28 14:19:05,804:INFO: Dataset: univ                Batch: 11/29	Loss 35.3676 (35.4022)
2022-10-28 14:19:08,332:INFO: Dataset: univ                Batch: 12/29	Loss 35.4286 (35.4045)
2022-10-28 14:19:10,679:INFO: Dataset: univ                Batch: 13/29	Loss 35.3625 (35.4008)
2022-10-28 14:19:13,426:INFO: Dataset: univ                Batch: 14/29	Loss 35.3922 (35.4002)
2022-10-28 14:19:16,140:INFO: Dataset: univ                Batch: 15/29	Loss 35.4019 (35.4003)
2022-10-28 14:19:18,599:INFO: Dataset: univ                Batch: 16/29	Loss 35.4438 (35.4031)
2022-10-28 14:19:20,840:INFO: Dataset: univ                Batch: 17/29	Loss 35.3794 (35.4017)
2022-10-28 14:19:23,565:INFO: Dataset: univ                Batch: 18/29	Loss 35.3938 (35.4012)
2022-10-28 14:19:26,095:INFO: Dataset: univ                Batch: 19/29	Loss 35.4003 (35.4012)
2022-10-28 14:19:28,455:INFO: Dataset: univ                Batch: 20/29	Loss 35.3660 (35.3992)
2022-10-28 14:19:30,770:INFO: Dataset: univ                Batch: 21/29	Loss 35.4108 (35.3997)
2022-10-28 14:19:33,293:INFO: Dataset: univ                Batch: 22/29	Loss 35.4118 (35.4003)
2022-10-28 14:19:35,825:INFO: Dataset: univ                Batch: 23/29	Loss 35.4065 (35.4005)
2022-10-28 14:19:38,158:INFO: Dataset: univ                Batch: 24/29	Loss 35.4891 (35.4039)
2022-10-28 14:19:40,478:INFO: Dataset: univ                Batch: 25/29	Loss 35.4020 (35.4038)
2022-10-28 14:19:42,964:INFO: Dataset: univ                Batch: 26/29	Loss 35.3961 (35.4035)
2022-10-28 14:19:45,474:INFO: Dataset: univ                Batch: 27/29	Loss 35.3795 (35.4026)
2022-10-28 14:19:47,958:INFO: Dataset: univ                Batch: 28/29	Loss 35.3686 (35.4013)
2022-10-28 14:19:49,493:INFO: Dataset: univ                Batch: 29/29	Loss 13.3364 (35.0844)
2022-10-28 14:19:59,457:INFO: Dataset: zara1               Batch:  1/16	Loss 35.2318 (35.2318)
2022-10-28 14:20:01,550:INFO: Dataset: zara1               Batch:  2/16	Loss 35.2115 (35.2222)
2022-10-28 14:20:03,483:INFO: Dataset: zara1               Batch:  3/16	Loss 35.1928 (35.2117)
2022-10-28 14:20:05,171:INFO: Dataset: zara1               Batch:  4/16	Loss 35.1855 (35.2049)
2022-10-28 14:20:07,207:INFO: Dataset: zara1               Batch:  5/16	Loss 35.2563 (35.2150)
2022-10-28 14:20:09,430:INFO: Dataset: zara1               Batch:  6/16	Loss 35.1505 (35.2044)
2022-10-28 14:20:11,573:INFO: Dataset: zara1               Batch:  7/16	Loss 35.2176 (35.2061)
2022-10-28 14:20:13,703:INFO: Dataset: zara1               Batch:  8/16	Loss 35.1904 (35.2044)
2022-10-28 14:20:15,879:INFO: Dataset: zara1               Batch:  9/16	Loss 35.1563 (35.1995)
2022-10-28 14:20:18,107:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1640 (35.1958)
2022-10-28 14:20:20,269:INFO: Dataset: zara1               Batch: 11/16	Loss 35.1741 (35.1939)
2022-10-28 14:20:22,536:INFO: Dataset: zara1               Batch: 12/16	Loss 35.1911 (35.1937)
2022-10-28 14:20:24,717:INFO: Dataset: zara1               Batch: 13/16	Loss 35.2407 (35.1971)
2022-10-28 14:20:26,981:INFO: Dataset: zara1               Batch: 14/16	Loss 35.2711 (35.2023)
2022-10-28 14:20:29,187:INFO: Dataset: zara1               Batch: 15/16	Loss 35.1981 (35.2020)
2022-10-28 14:20:31,047:INFO: Dataset: zara1               Batch: 16/16	Loss 25.3417 (34.7142)
2022-10-28 14:20:52,554:INFO: Dataset: zara2               Batch:  1/36	Loss 35.1261 (35.1261)
2022-10-28 14:20:54,883:INFO: Dataset: zara2               Batch:  2/36	Loss 35.2439 (35.1786)
2022-10-28 14:20:57,174:INFO: Dataset: zara2               Batch:  3/36	Loss 35.2859 (35.2122)
2022-10-28 14:20:59,412:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1627 (35.2010)
2022-10-28 14:21:01,631:INFO: Dataset: zara2               Batch:  5/36	Loss 35.1335 (35.1874)
2022-10-28 14:21:03,817:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1289 (35.1774)
2022-10-28 14:21:06,106:INFO: Dataset: zara2               Batch:  7/36	Loss 35.2151 (35.1823)
2022-10-28 14:21:08,369:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1710 (35.1808)
2022-10-28 14:21:10,644:INFO: Dataset: zara2               Batch:  9/36	Loss 35.2475 (35.1871)
2022-10-28 14:21:12,934:INFO: Dataset: zara2               Batch: 10/36	Loss 35.2249 (35.1908)
2022-10-28 14:21:15,195:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1580 (35.1875)
2022-10-28 14:21:17,440:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1414 (35.1833)
2022-10-28 14:21:19,668:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1248 (35.1783)
2022-10-28 14:21:21,879:INFO: Dataset: zara2               Batch: 14/36	Loss 35.1530 (35.1763)
2022-10-28 14:21:24,087:INFO: Dataset: zara2               Batch: 15/36	Loss 35.0993 (35.1715)
2022-10-28 14:21:26,308:INFO: Dataset: zara2               Batch: 16/36	Loss 35.1720 (35.1715)
2022-10-28 14:21:28,517:INFO: Dataset: zara2               Batch: 17/36	Loss 35.1322 (35.1694)
2022-10-28 14:21:30,757:INFO: Dataset: zara2               Batch: 18/36	Loss 35.2162 (35.1722)
2022-10-28 14:21:33,070:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1257 (35.1694)
2022-10-28 14:21:35,306:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1664 (35.1692)
2022-10-28 14:21:37,564:INFO: Dataset: zara2               Batch: 21/36	Loss 35.2104 (35.1712)
2022-10-28 14:21:39,732:INFO: Dataset: zara2               Batch: 22/36	Loss 35.2105 (35.1732)
2022-10-28 14:21:41,896:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1156 (35.1706)
2022-10-28 14:21:44,346:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1840 (35.1712)
2022-10-28 14:21:46,713:INFO: Dataset: zara2               Batch: 25/36	Loss 35.2296 (35.1734)
2022-10-28 14:21:48,795:INFO: Dataset: zara2               Batch: 26/36	Loss 35.1301 (35.1717)
2022-10-28 14:21:50,956:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1938 (35.1725)
2022-10-28 14:21:53,097:INFO: Dataset: zara2               Batch: 28/36	Loss 35.1751 (35.1725)
2022-10-28 14:21:55,354:INFO: Dataset: zara2               Batch: 29/36	Loss 35.1694 (35.1724)
2022-10-28 14:21:57,551:INFO: Dataset: zara2               Batch: 30/36	Loss 35.1145 (35.1706)
2022-10-28 14:21:59,754:INFO: Dataset: zara2               Batch: 31/36	Loss 35.2995 (35.1742)
2022-10-28 14:22:01,904:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1604 (35.1738)
2022-10-28 14:22:04,155:INFO: Dataset: zara2               Batch: 33/36	Loss 35.2069 (35.1748)
2022-10-28 14:22:06,267:INFO: Dataset: zara2               Batch: 34/36	Loss 35.1911 (35.1753)
2022-10-28 14:22:08,426:INFO: Dataset: zara2               Batch: 35/36	Loss 35.1189 (35.1738)
2022-10-28 14:22:10,251:INFO: Dataset: zara2               Batch: 36/36	Loss 25.3036 (34.9519)
2022-10-28 14:22:11,035:INFO: - Computing ADE (validation o)
2022-10-28 14:22:18,091:INFO: 		 ADE on eth                       dataset:	 1.1558083295822144
2022-10-28 14:22:18,091:INFO: Average validation o:	ADE  1.1558	FDE  2.0291
2022-10-28 14:22:18,092:INFO: - Computing ADE (validation)
2022-10-28 14:22:25,021:INFO: 		 ADE on hotel                     dataset:	 1.0733124017715454
2022-10-28 14:22:32,118:INFO: 		 ADE on univ                      dataset:	 0.9421373605728149
2022-10-28 14:22:39,105:INFO: 		 ADE on zara1                     dataset:	 0.6386573314666748
2022-10-28 14:22:46,779:INFO: 		 ADE on zara2                     dataset:	 0.5558803677558899
2022-10-28 14:22:46,779:INFO: Average validation:	ADE  0.7900	FDE  1.4837
2022-10-28 14:22:46,780:INFO: - Computing ADE (training)
2022-10-28 14:22:55,114:INFO: 		 ADE on hotel                     dataset:	 1.3756043910980225

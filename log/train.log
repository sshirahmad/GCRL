2022-10-29 23:56:06,190:INFO: Initializing Training Set
2022-10-29 23:56:08,064:INFO: Initializing Validation Set
2022-10-29 23:56:08,298:INFO: Initializing Validation O Set
2022-10-29 23:56:10,194:INFO: => loaded checkpoint './models/eth/pretrain/P3/5.0/SSE_data_eth_irm[5.0]_epoch_400.pth.tar' (epoch 401)
2022-10-29 23:56:10,194:INFO: 
===> EPOCH: 401 (P4)
2022-10-29 23:56:10,195:INFO: - Computing loss (training)
2022-10-29 23:56:11,418:INFO: Dataset: hotel               Batch: 1/4	Loss 159.2441 (159.2441)
2022-10-29 23:56:12,395:INFO: Dataset: hotel               Batch: 2/4	Loss 87.6935 (125.3792)
2022-10-29 23:56:13,373:INFO: Dataset: hotel               Batch: 3/4	Loss 84.4244 (111.5520)
2022-10-29 23:56:14,088:INFO: Dataset: hotel               Batch: 4/4	Loss 48.7775 (100.2890)
2022-10-29 23:56:15,488:INFO: Dataset: univ                Batch:  1/15	Loss 4229.2246 (4229.2246)
2022-10-29 23:56:16,539:INFO: Dataset: univ                Batch:  2/15	Loss 12349.9053 (8105.3189)
2022-10-29 23:56:17,595:INFO: Dataset: univ                Batch:  3/15	Loss 24774.3086 (13657.8561)
2022-10-29 23:56:18,649:INFO: Dataset: univ                Batch:  4/15	Loss 16128.8105 (14318.2329)
2022-10-29 23:56:19,690:INFO: Dataset: univ                Batch:  5/15	Loss 12631.6494 (14002.7700)
2022-10-29 23:56:20,741:INFO: Dataset: univ                Batch:  6/15	Loss 109562.4844 (30371.2485)
2022-10-29 23:56:21,781:INFO: Dataset: univ                Batch:  7/15	Loss 10329.3398 (27710.3171)
2022-10-29 23:56:22,828:INFO: Dataset: univ                Batch:  8/15	Loss 27702.4824 (27709.3475)
2022-10-29 23:56:23,871:INFO: Dataset: univ                Batch:  9/15	Loss 34089.8672 (28364.2109)
2022-10-29 23:56:24,919:INFO: Dataset: univ                Batch: 10/15	Loss 41156.3672 (29688.7489)
2022-10-29 23:56:25,971:INFO: Dataset: univ                Batch: 11/15	Loss 13010.9922 (28105.1289)
2022-10-29 23:56:27,036:INFO: Dataset: univ                Batch: 12/15	Loss 68171.3281 (31790.2607)
2022-10-29 23:56:28,085:INFO: Dataset: univ                Batch: 13/15	Loss 13498.1143 (30365.4549)
2022-10-29 23:56:29,142:INFO: Dataset: univ                Batch: 14/15	Loss 16015.6123 (29265.1739)
2022-10-29 23:56:29,589:INFO: Dataset: univ                Batch: 15/15	Loss 546.1479 (28874.5777)
2022-10-29 23:56:30,879:INFO: Dataset: zara1               Batch: 1/8	Loss 1986.6440 (1986.6440)
2022-10-29 23:56:31,886:INFO: Dataset: zara1               Batch: 2/8	Loss 2239.2593 (2102.5089)
2022-10-29 23:56:32,897:INFO: Dataset: zara1               Batch: 3/8	Loss 5214.0562 (3139.6913)
2022-10-29 23:56:33,908:INFO: Dataset: zara1               Batch: 4/8	Loss 9303.3184 (4578.2844)
2022-10-29 23:56:34,924:INFO: Dataset: zara1               Batch: 5/8	Loss 12266.9287 (6068.6217)
2022-10-29 23:56:35,921:INFO: Dataset: zara1               Batch: 6/8	Loss 2575.7502 (5479.8032)
2022-10-29 23:56:36,931:INFO: Dataset: zara1               Batch: 7/8	Loss 9357.4600 (5980.7336)
2022-10-29 23:56:37,842:INFO: Dataset: zara1               Batch: 8/8	Loss 5454.2847 (5926.1492)
2022-10-29 23:56:39,186:INFO: Dataset: zara2               Batch:  1/18	Loss 152356.6562 (152356.6562)
2022-10-29 23:56:40,319:INFO: Dataset: zara2               Batch:  2/18	Loss 17094.4258 (86181.0170)
2022-10-29 23:56:41,333:INFO: Dataset: zara2               Batch:  3/18	Loss 28343.1523 (67904.4788)
2022-10-29 23:56:42,345:INFO: Dataset: zara2               Batch:  4/18	Loss 99956.2891 (76063.9740)
2022-10-29 23:56:43,356:INFO: Dataset: zara2               Batch:  5/18	Loss 37391.6133 (68580.4729)
2022-10-29 23:56:44,369:INFO: Dataset: zara2               Batch:  6/18	Loss 149131.5625 (82235.8757)
2022-10-29 23:56:45,378:INFO: Dataset: zara2               Batch:  7/18	Loss 67178.4453 (79935.6954)
2022-10-29 23:56:46,385:INFO: Dataset: zara2               Batch:  8/18	Loss 103829.7422 (82868.0675)
2022-10-29 23:56:47,395:INFO: Dataset: zara2               Batch:  9/18	Loss 64778.1055 (80927.3368)
2022-10-29 23:56:48,477:INFO: Dataset: zara2               Batch: 10/18	Loss 44827.6172 (77410.5801)
2022-10-29 23:56:49,548:INFO: Dataset: zara2               Batch: 11/18	Loss 16668.0332 (72301.0888)
2022-10-29 23:56:50,589:INFO: Dataset: zara2               Batch: 12/18	Loss 44091.0156 (69939.2506)
2022-10-29 23:56:51,688:INFO: Dataset: zara2               Batch: 13/18	Loss 14843.4033 (66043.4573)
2022-10-29 23:56:52,710:INFO: Dataset: zara2               Batch: 14/18	Loss 20124.1465 (62647.4328)
2022-10-29 23:56:53,714:INFO: Dataset: zara2               Batch: 15/18	Loss 30477.7441 (60426.2075)
2022-10-29 23:56:54,720:INFO: Dataset: zara2               Batch: 16/18	Loss 23296.4941 (58195.5331)
2022-10-29 23:56:55,794:INFO: Dataset: zara2               Batch: 17/18	Loss 16000.4697 (55665.4474)
2022-10-29 23:56:56,721:INFO: Dataset: zara2               Batch: 18/18	Loss 110077.8516 (58471.9724)
2022-10-29 23:56:56,786:INFO: - Computing ADE (validation o)
2022-10-29 23:56:57,138:INFO: 		 ADE on eth                       dataset:	 2.894493818283081
2022-10-29 23:56:57,139:INFO: Average validation o:	ADE  2.8945	FDE  4.8954
2022-10-29 23:56:57,139:INFO: - Computing ADE (validation)
2022-10-29 23:56:57,464:INFO: 		 ADE on hotel                     dataset:	 0.9939405918121338
2022-10-29 23:56:57,890:INFO: 		 ADE on univ                      dataset:	 1.4841927289962769
2022-10-29 23:56:58,228:INFO: 		 ADE on zara1                     dataset:	 2.558382511138916
2022-10-29 23:56:58,771:INFO: 		 ADE on zara2                     dataset:	 1.4520694017410278
2022-10-29 23:56:58,771:INFO: Average validation:	ADE  1.5080	FDE  2.7359
2022-10-29 23:56:58,772:INFO: - Computing ADE (training)
2022-10-29 23:56:59,220:INFO: 		 ADE on hotel                     dataset:	 1.3203344345092773
2022-10-29 23:57:00,232:INFO: 		 ADE on univ                      dataset:	 1.3974227905273438
2022-10-29 23:57:00,883:INFO: 		 ADE on zara1                     dataset:	 2.4901230335235596
2022-10-29 23:57:02,015:INFO: 		 ADE on zara2                     dataset:	 1.719648003578186
2022-10-29 23:57:02,016:INFO: Average training:	ADE  1.5305	FDE  2.7836
2022-10-29 23:57:02,026:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_401.pth.tar
2022-10-29 23:57:02,026:INFO: 
===> EPOCH: 402 (P4)
2022-10-29 23:57:02,027:INFO: - Computing loss (training)
2022-10-29 23:57:03,256:INFO: Dataset: hotel               Batch: 1/4	Loss 84.8071 (84.8071)
2022-10-29 23:57:04,263:INFO: Dataset: hotel               Batch: 2/4	Loss 94.2825 (89.6141)
2022-10-29 23:57:05,265:INFO: Dataset: hotel               Batch: 3/4	Loss 134.0526 (104.3787)
2022-10-29 23:57:05,997:INFO: Dataset: hotel               Batch: 4/4	Loss 49.7033 (93.9918)
2022-10-29 23:57:07,370:INFO: Dataset: univ                Batch:  1/15	Loss 8242.8379 (8242.8379)
2022-10-29 23:57:08,446:INFO: Dataset: univ                Batch:  2/15	Loss 16560.9121 (12466.6671)
2022-10-29 23:57:09,519:INFO: Dataset: univ                Batch:  3/15	Loss 16271.0098 (13800.1575)
2022-10-29 23:57:10,592:INFO: Dataset: univ                Batch:  4/15	Loss 33890.1719 (18982.4762)
2022-10-29 23:57:11,665:INFO: Dataset: univ                Batch:  5/15	Loss 4943.8579 (16148.3137)
2022-10-29 23:57:12,813:INFO: Dataset: univ                Batch:  6/15	Loss 36608.3242 (19735.2552)
2022-10-29 23:57:13,946:INFO: Dataset: univ                Batch:  7/15	Loss 4453.4053 (17384.5525)
2022-10-29 23:57:15,055:INFO: Dataset: univ                Batch:  8/15	Loss 6886.7085 (15910.2725)
2022-10-29 23:57:16,127:INFO: Dataset: univ                Batch:  9/15	Loss 29154.9316 (17341.7996)
2022-10-29 23:57:17,252:INFO: Dataset: univ                Batch: 10/15	Loss 31654.2793 (18836.7391)
2022-10-29 23:57:18,361:INFO: Dataset: univ                Batch: 11/15	Loss 45783.1719 (21487.3712)
2022-10-29 23:57:19,519:INFO: Dataset: univ                Batch: 12/15	Loss 7605.9409 (20292.5971)
2022-10-29 23:57:20,619:INFO: Dataset: univ                Batch: 13/15	Loss 15597.4951 (19918.7220)
2022-10-29 23:57:21,719:INFO: Dataset: univ                Batch: 14/15	Loss 23939.9043 (20216.9233)
2022-10-29 23:57:22,194:INFO: Dataset: univ                Batch: 15/15	Loss 2593.0146 (19993.0958)
2022-10-29 23:57:23,527:INFO: Dataset: zara1               Batch: 1/8	Loss 23359.8359 (23359.8359)
2022-10-29 23:57:24,542:INFO: Dataset: zara1               Batch: 2/8	Loss 785.6168 (13015.3330)
2022-10-29 23:57:25,572:INFO: Dataset: zara1               Batch: 3/8	Loss 20672.7441 (15794.1479)
2022-10-29 23:57:26,585:INFO: Dataset: zara1               Batch: 4/8	Loss 3124.2051 (12581.5046)
2022-10-29 23:57:27,596:INFO: Dataset: zara1               Batch: 5/8	Loss 2130.2258 (10508.3121)
2022-10-29 23:57:28,635:INFO: Dataset: zara1               Batch: 6/8	Loss 2126.3171 (9144.7340)
2022-10-29 23:57:29,652:INFO: Dataset: zara1               Batch: 7/8	Loss 4774.0015 (8504.6792)
2022-10-29 23:57:30,575:INFO: Dataset: zara1               Batch: 8/8	Loss 2369.2725 (7904.0552)
2022-10-29 23:57:31,937:INFO: Dataset: zara2               Batch:  1/18	Loss 10823.3955 (10823.3955)
2022-10-29 23:57:32,957:INFO: Dataset: zara2               Batch:  2/18	Loss 15649.5605 (13236.4780)
2022-10-29 23:57:33,993:INFO: Dataset: zara2               Batch:  3/18	Loss 9247.9316 (11964.3705)
2022-10-29 23:57:35,030:INFO: Dataset: zara2               Batch:  4/18	Loss 14006.2520 (12419.1603)
2022-10-29 23:57:36,102:INFO: Dataset: zara2               Batch:  5/18	Loss 28536.7480 (15660.3571)
2022-10-29 23:57:37,111:INFO: Dataset: zara2               Batch:  6/18	Loss 46879.9883 (21225.7996)
2022-10-29 23:57:38,143:INFO: Dataset: zara2               Batch:  7/18	Loss 22824.4004 (21476.2561)
2022-10-29 23:57:39,195:INFO: Dataset: zara2               Batch:  8/18	Loss 50167.0352 (25728.2708)
2022-10-29 23:57:40,248:INFO: Dataset: zara2               Batch:  9/18	Loss 47942.8125 (28281.4225)
2022-10-29 23:57:41,283:INFO: Dataset: zara2               Batch: 10/18	Loss 26916.5273 (28143.1756)
2022-10-29 23:57:42,300:INFO: Dataset: zara2               Batch: 11/18	Loss 97039.3047 (33354.5729)
2022-10-29 23:57:43,346:INFO: Dataset: zara2               Batch: 12/18	Loss 17336.9844 (32137.2048)
2022-10-29 23:57:44,357:INFO: Dataset: zara2               Batch: 13/18	Loss 53943.4180 (33659.0296)
2022-10-29 23:57:45,387:INFO: Dataset: zara2               Batch: 14/18	Loss 4517.0093 (31654.1277)
2022-10-29 23:57:46,436:INFO: Dataset: zara2               Batch: 15/18	Loss 42725.1445 (32478.3806)
2022-10-29 23:57:47,444:INFO: Dataset: zara2               Batch: 16/18	Loss 33962.2930 (32561.5367)
2022-10-29 23:57:48,479:INFO: Dataset: zara2               Batch: 17/18	Loss 18008.1602 (31624.7214)
2022-10-29 23:57:49,385:INFO: Dataset: zara2               Batch: 18/18	Loss 9425.5908 (30633.8526)
2022-10-29 23:57:49,448:INFO: - Computing ADE (validation o)
2022-10-29 23:57:49,784:INFO: 		 ADE on eth                       dataset:	 2.9289183616638184
2022-10-29 23:57:49,785:INFO: Average validation o:	ADE  2.9289	FDE  4.9549
2022-10-29 23:57:49,785:INFO: - Computing ADE (validation)
2022-10-29 23:57:50,119:INFO: 		 ADE on hotel                     dataset:	 1.0083949565887451
2022-10-29 23:57:50,545:INFO: 		 ADE on univ                      dataset:	 1.492173194885254
2022-10-29 23:57:50,879:INFO: 		 ADE on zara1                     dataset:	 2.5613350868225098
2022-10-29 23:57:51,401:INFO: 		 ADE on zara2                     dataset:	 1.4762051105499268
2022-10-29 23:57:51,402:INFO: Average validation:	ADE  1.5220	FDE  2.7507
2022-10-29 23:57:51,402:INFO: - Computing ADE (training)
2022-10-29 23:57:51,856:INFO: 		 ADE on hotel                     dataset:	 1.3277862071990967
2022-10-29 23:57:52,864:INFO: 		 ADE on univ                      dataset:	 1.4088188409805298
2022-10-29 23:57:53,515:INFO: 		 ADE on zara1                     dataset:	 2.5200681686401367
2022-10-29 23:57:54,671:INFO: 		 ADE on zara2                     dataset:	 1.7495781183242798
2022-10-29 23:57:54,672:INFO: Average training:	ADE  1.5467	FDE  2.8049
2022-10-29 23:57:54,682:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_402.pth.tar
2022-10-29 23:57:54,682:INFO: 
===> EPOCH: 403 (P4)
2022-10-29 23:57:54,683:INFO: - Computing loss (training)
2022-10-29 23:57:55,919:INFO: Dataset: hotel               Batch: 1/4	Loss 81.6786 (81.6786)
2022-10-29 23:57:56,932:INFO: Dataset: hotel               Batch: 2/4	Loss 107.2250 (94.7262)
2022-10-29 23:57:57,942:INFO: Dataset: hotel               Batch: 3/4	Loss 82.1846 (90.4470)
2022-10-29 23:57:58,679:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7050 (84.0506)
2022-10-29 23:58:00,074:INFO: Dataset: univ                Batch:  1/15	Loss 7500.3452 (7500.3452)
2022-10-29 23:58:01,137:INFO: Dataset: univ                Batch:  2/15	Loss 10684.4932 (8886.9259)
2022-10-29 23:58:02,212:INFO: Dataset: univ                Batch:  3/15	Loss 25695.2891 (14376.2395)
2022-10-29 23:58:03,278:INFO: Dataset: univ                Batch:  4/15	Loss 9176.4150 (13186.6929)
2022-10-29 23:58:04,337:INFO: Dataset: univ                Batch:  5/15	Loss 102420.9062 (29771.0884)
2022-10-29 23:58:05,398:INFO: Dataset: univ                Batch:  6/15	Loss 7650.8086 (26268.9368)
2022-10-29 23:58:06,460:INFO: Dataset: univ                Batch:  7/15	Loss 8343.5234 (23734.3939)
2022-10-29 23:58:07,529:INFO: Dataset: univ                Batch:  8/15	Loss 6732.2598 (21711.1155)
2022-10-29 23:58:08,601:INFO: Dataset: univ                Batch:  9/15	Loss 13326.3203 (20720.2305)
2022-10-29 23:58:09,660:INFO: Dataset: univ                Batch: 10/15	Loss 6279.9712 (19324.2256)
2022-10-29 23:58:10,729:INFO: Dataset: univ                Batch: 11/15	Loss 5772.5039 (18106.1864)
2022-10-29 23:58:11,799:INFO: Dataset: univ                Batch: 12/15	Loss 16127.2568 (17931.9515)
2022-10-29 23:58:12,871:INFO: Dataset: univ                Batch: 13/15	Loss 8316.2979 (17146.1561)
2022-10-29 23:58:13,923:INFO: Dataset: univ                Batch: 14/15	Loss 53004.2188 (19332.3290)
2022-10-29 23:58:14,374:INFO: Dataset: univ                Batch: 15/15	Loss 6573.9141 (19148.5285)
2022-10-29 23:58:15,669:INFO: Dataset: zara1               Batch: 1/8	Loss 7305.2798 (7305.2798)
2022-10-29 23:58:16,669:INFO: Dataset: zara1               Batch: 2/8	Loss 4013.0190 (5753.3068)
2022-10-29 23:58:17,671:INFO: Dataset: zara1               Batch: 3/8	Loss 16025.2441 (8881.3865)
2022-10-29 23:58:18,674:INFO: Dataset: zara1               Batch: 4/8	Loss 2293.3298 (7164.2155)
2022-10-29 23:58:19,673:INFO: Dataset: zara1               Batch: 5/8	Loss 3301.7188 (6438.8043)
2022-10-29 23:58:20,673:INFO: Dataset: zara1               Batch: 6/8	Loss 7867.9473 (6673.0577)
2022-10-29 23:58:21,672:INFO: Dataset: zara1               Batch: 7/8	Loss 1573.2528 (5975.7003)
2022-10-29 23:58:22,579:INFO: Dataset: zara1               Batch: 8/8	Loss 26317.6504 (8309.6715)
2022-10-29 23:58:23,899:INFO: Dataset: zara2               Batch:  1/18	Loss 8745.1240 (8745.1240)
2022-10-29 23:58:24,905:INFO: Dataset: zara2               Batch:  2/18	Loss 18636.6875 (13690.9058)
2022-10-29 23:58:25,911:INFO: Dataset: zara2               Batch:  3/18	Loss 22468.3066 (16274.4426)
2022-10-29 23:58:26,914:INFO: Dataset: zara2               Batch:  4/18	Loss 12244.1992 (15308.8032)
2022-10-29 23:58:27,923:INFO: Dataset: zara2               Batch:  5/18	Loss 10595.4854 (14474.0952)
2022-10-29 23:58:28,936:INFO: Dataset: zara2               Batch:  6/18	Loss 13583.3135 (14332.1891)
2022-10-29 23:58:29,945:INFO: Dataset: zara2               Batch:  7/18	Loss 120116.4297 (29179.8918)
2022-10-29 23:58:30,946:INFO: Dataset: zara2               Batch:  8/18	Loss 7782.7563 (26560.3248)
2022-10-29 23:58:31,951:INFO: Dataset: zara2               Batch:  9/18	Loss 14546.3945 (25189.5812)
2022-10-29 23:58:32,951:INFO: Dataset: zara2               Batch: 10/18	Loss 5697.6567 (23340.4524)
2022-10-29 23:58:33,950:INFO: Dataset: zara2               Batch: 11/18	Loss 36164.4414 (24569.2439)
2022-10-29 23:58:34,951:INFO: Dataset: zara2               Batch: 12/18	Loss 32062.2637 (25204.0563)
2022-10-29 23:58:35,949:INFO: Dataset: zara2               Batch: 13/18	Loss 83660.1797 (29596.3248)
2022-10-29 23:58:36,949:INFO: Dataset: zara2               Batch: 14/18	Loss 19735.8965 (28870.1204)
2022-10-29 23:58:37,946:INFO: Dataset: zara2               Batch: 15/18	Loss 38042.6250 (29536.4215)
2022-10-29 23:58:38,944:INFO: Dataset: zara2               Batch: 16/18	Loss 10952.7236 (28429.0581)
2022-10-29 23:58:39,943:INFO: Dataset: zara2               Batch: 17/18	Loss 15471.0000 (27600.4629)
2022-10-29 23:58:40,850:INFO: Dataset: zara2               Batch: 18/18	Loss 3520.6211 (26430.1085)
2022-10-29 23:58:40,912:INFO: - Computing ADE (validation o)
2022-10-29 23:58:41,262:INFO: 		 ADE on eth                       dataset:	 2.9762604236602783
2022-10-29 23:58:41,262:INFO: Average validation o:	ADE  2.9763	FDE  5.0259
2022-10-29 23:58:41,263:INFO: - Computing ADE (validation)
2022-10-29 23:58:41,586:INFO: 		 ADE on hotel                     dataset:	 1.0102895498275757
2022-10-29 23:58:42,005:INFO: 		 ADE on univ                      dataset:	 1.5019980669021606
2022-10-29 23:58:42,350:INFO: 		 ADE on zara1                     dataset:	 2.542703151702881
2022-10-29 23:58:42,877:INFO: 		 ADE on zara2                     dataset:	 1.4797841310501099
2022-10-29 23:58:42,877:INFO: Average validation:	ADE  1.5274	FDE  2.7533
2022-10-29 23:58:42,877:INFO: - Computing ADE (training)
2022-10-29 23:58:43,331:INFO: 		 ADE on hotel                     dataset:	 1.3350703716278076
2022-10-29 23:58:44,339:INFO: 		 ADE on univ                      dataset:	 1.4144395589828491
2022-10-29 23:58:44,996:INFO: 		 ADE on zara1                     dataset:	 2.539005756378174
2022-10-29 23:58:46,164:INFO: 		 ADE on zara2                     dataset:	 1.775505542755127
2022-10-29 23:58:46,164:INFO: Average training:	ADE  1.5574	FDE  2.8163
2022-10-29 23:58:46,174:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_403.pth.tar
2022-10-29 23:58:46,174:INFO: 
===> EPOCH: 404 (P4)
2022-10-29 23:58:46,175:INFO: - Computing loss (training)
2022-10-29 23:58:47,405:INFO: Dataset: hotel               Batch: 1/4	Loss 84.6230 (84.6230)
2022-10-29 23:58:48,417:INFO: Dataset: hotel               Batch: 2/4	Loss 81.4381 (82.9973)
2022-10-29 23:58:49,419:INFO: Dataset: hotel               Batch: 3/4	Loss 85.2846 (83.6894)
2022-10-29 23:58:50,154:INFO: Dataset: hotel               Batch: 4/4	Loss 49.9478 (77.4574)
2022-10-29 23:58:51,531:INFO: Dataset: univ                Batch:  1/15	Loss 14113.7373 (14113.7373)
2022-10-29 23:58:52,596:INFO: Dataset: univ                Batch:  2/15	Loss 9050.3848 (11615.8387)
2022-10-29 23:58:53,650:INFO: Dataset: univ                Batch:  3/15	Loss 8437.1221 (10652.1325)
2022-10-29 23:58:54,709:INFO: Dataset: univ                Batch:  4/15	Loss 87517.0312 (29711.0621)
2022-10-29 23:58:55,784:INFO: Dataset: univ                Batch:  5/15	Loss 10010.2812 (25531.6273)
2022-10-29 23:58:56,845:INFO: Dataset: univ                Batch:  6/15	Loss 36746.0312 (27431.3556)
2022-10-29 23:58:57,911:INFO: Dataset: univ                Batch:  7/15	Loss 24457.7227 (26979.4152)
2022-10-29 23:58:58,969:INFO: Dataset: univ                Batch:  8/15	Loss 23294.5156 (26537.4850)
2022-10-29 23:59:00,028:INFO: Dataset: univ                Batch:  9/15	Loss 21158.3379 (25934.1009)
2022-10-29 23:59:01,091:INFO: Dataset: univ                Batch: 10/15	Loss 6296.3398 (23904.3145)
2022-10-29 23:59:02,146:INFO: Dataset: univ                Batch: 11/15	Loss 36006.9102 (24977.7608)
2022-10-29 23:59:03,279:INFO: Dataset: univ                Batch: 12/15	Loss 7847.7788 (23654.1194)
2022-10-29 23:59:04,335:INFO: Dataset: univ                Batch: 13/15	Loss 5506.6836 (22322.8512)
2022-10-29 23:59:05,402:INFO: Dataset: univ                Batch: 14/15	Loss 36535.7578 (23386.0000)
2022-10-29 23:59:05,847:INFO: Dataset: univ                Batch: 15/15	Loss 564314.9375 (28000.1223)
2022-10-29 23:59:07,160:INFO: Dataset: zara1               Batch: 1/8	Loss 8208.1572 (8208.1572)
2022-10-29 23:59:08,164:INFO: Dataset: zara1               Batch: 2/8	Loss 5724.1455 (6943.0084)
2022-10-29 23:59:09,172:INFO: Dataset: zara1               Batch: 3/8	Loss 1724.4333 (5182.0078)
2022-10-29 23:59:10,180:INFO: Dataset: zara1               Batch: 4/8	Loss 2627.0974 (4589.7209)
2022-10-29 23:59:11,186:INFO: Dataset: zara1               Batch: 5/8	Loss 2245.2322 (4111.7756)
2022-10-29 23:59:12,188:INFO: Dataset: zara1               Batch: 6/8	Loss 2383.5137 (3810.1676)
2022-10-29 23:59:13,192:INFO: Dataset: zara1               Batch: 7/8	Loss 22325.2441 (6570.5178)
2022-10-29 23:59:14,105:INFO: Dataset: zara1               Batch: 8/8	Loss 29426.1660 (9012.4634)
2022-10-29 23:59:15,431:INFO: Dataset: zara2               Batch:  1/18	Loss 79099.1484 (79099.1484)
2022-10-29 23:59:16,449:INFO: Dataset: zara2               Batch:  2/18	Loss 13650.7559 (45854.0170)
2022-10-29 23:59:17,465:INFO: Dataset: zara2               Batch:  3/18	Loss 31749.4785 (41157.0423)
2022-10-29 23:59:18,484:INFO: Dataset: zara2               Batch:  4/18	Loss 10884.5586 (33334.3032)
2022-10-29 23:59:19,503:INFO: Dataset: zara2               Batch:  5/18	Loss 2759.6794 (27463.4095)
2022-10-29 23:59:20,519:INFO: Dataset: zara2               Batch:  6/18	Loss 492055.7500 (102869.6276)
2022-10-29 23:59:21,533:INFO: Dataset: zara2               Batch:  7/18	Loss 6517.2583 (88559.8698)
2022-10-29 23:59:22,547:INFO: Dataset: zara2               Batch:  8/18	Loss 36092.4688 (81660.7167)
2022-10-29 23:59:23,565:INFO: Dataset: zara2               Batch:  9/18	Loss 11507.9141 (74464.9791)
2022-10-29 23:59:24,584:INFO: Dataset: zara2               Batch: 10/18	Loss 10282.7627 (67657.4948)
2022-10-29 23:59:25,602:INFO: Dataset: zara2               Batch: 11/18	Loss 8901.5029 (62272.8073)
2022-10-29 23:59:26,621:INFO: Dataset: zara2               Batch: 12/18	Loss 14028.0127 (58542.0005)
2022-10-29 23:59:27,642:INFO: Dataset: zara2               Batch: 13/18	Loss 2181.7185 (54496.4615)
2022-10-29 23:59:28,659:INFO: Dataset: zara2               Batch: 14/18	Loss 12175.5781 (51432.7842)
2022-10-29 23:59:29,672:INFO: Dataset: zara2               Batch: 15/18	Loss 4663.3042 (48667.1416)
2022-10-29 23:59:30,690:INFO: Dataset: zara2               Batch: 16/18	Loss 742706.2500 (91854.0554)
2022-10-29 23:59:31,707:INFO: Dataset: zara2               Batch: 17/18	Loss 15711.3008 (87840.9837)
2022-10-29 23:59:32,630:INFO: Dataset: zara2               Batch: 18/18	Loss 17374.7539 (84567.5483)
2022-10-29 23:59:32,692:INFO: - Computing ADE (validation o)
2022-10-29 23:59:33,020:INFO: 		 ADE on eth                       dataset:	 3.144853353500366
2022-10-29 23:59:33,020:INFO: Average validation o:	ADE  3.1449	FDE  5.3256
2022-10-29 23:59:33,021:INFO: - Computing ADE (validation)
2022-10-29 23:59:33,356:INFO: 		 ADE on hotel                     dataset:	 1.1401172876358032
2022-10-29 23:59:33,782:INFO: 		 ADE on univ                      dataset:	 1.534590482711792
2022-10-29 23:59:34,126:INFO: 		 ADE on zara1                     dataset:	 2.5024685859680176
2022-10-29 23:59:34,652:INFO: 		 ADE on zara2                     dataset:	 1.576271653175354
2022-10-29 23:59:34,652:INFO: Average validation:	ADE  1.5845	FDE  2.8429
2022-10-29 23:59:34,652:INFO: - Computing ADE (training)
2022-10-29 23:59:35,118:INFO: 		 ADE on hotel                     dataset:	 1.4246646165847778
2022-10-29 23:59:36,121:INFO: 		 ADE on univ                      dataset:	 1.4625555276870728
2022-10-29 23:59:36,794:INFO: 		 ADE on zara1                     dataset:	 2.6105058193206787
2022-10-29 23:59:37,950:INFO: 		 ADE on zara2                     dataset:	 1.9004477262496948
2022-10-29 23:59:37,950:INFO: Average training:	ADE  1.6236	FDE  2.9237
2022-10-29 23:59:37,960:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_404.pth.tar
2022-10-29 23:59:37,960:INFO: 
===> EPOCH: 405 (P4)
2022-10-29 23:59:37,961:INFO: - Computing loss (training)
2022-10-29 23:59:39,198:INFO: Dataset: hotel               Batch: 1/4	Loss 96.0496 (96.0496)
2022-10-29 23:59:40,198:INFO: Dataset: hotel               Batch: 2/4	Loss 90.8076 (93.3772)
2022-10-29 23:59:41,203:INFO: Dataset: hotel               Batch: 3/4	Loss 165.9952 (118.2848)
2022-10-29 23:59:41,937:INFO: Dataset: hotel               Batch: 4/4	Loss 52.1731 (106.3359)
2022-10-29 23:59:43,321:INFO: Dataset: univ                Batch:  1/15	Loss 15808.5381 (15808.5381)
2022-10-29 23:59:44,398:INFO: Dataset: univ                Batch:  2/15	Loss 7527.5356 (11682.2921)
2022-10-29 23:59:45,473:INFO: Dataset: univ                Batch:  3/15	Loss 14585.8584 (12624.1552)
2022-10-29 23:59:46,537:INFO: Dataset: univ                Batch:  4/15	Loss 5015.2349 (10912.0858)
2022-10-29 23:59:47,611:INFO: Dataset: univ                Batch:  5/15	Loss 4835.6802 (9709.2642)
2022-10-29 23:59:48,678:INFO: Dataset: univ                Batch:  6/15	Loss 3808.5415 (8791.0829)
2022-10-29 23:59:49,736:INFO: Dataset: univ                Batch:  7/15	Loss 12856.5996 (9316.4135)
2022-10-29 23:59:50,799:INFO: Dataset: univ                Batch:  8/15	Loss 4549.0864 (8749.6769)
2022-10-29 23:59:51,859:INFO: Dataset: univ                Batch:  9/15	Loss 21882.8945 (10152.2050)
2022-10-29 23:59:52,941:INFO: Dataset: univ                Batch: 10/15	Loss 2106.2036 (9195.5931)
2022-10-29 23:59:54,012:INFO: Dataset: univ                Batch: 11/15	Loss 15197.8691 (9719.5256)
2022-10-29 23:59:55,069:INFO: Dataset: univ                Batch: 12/15	Loss 4591.1338 (9318.1832)
2022-10-29 23:59:56,143:INFO: Dataset: univ                Batch: 13/15	Loss 4508.0410 (8944.7087)
2022-10-29 23:59:57,218:INFO: Dataset: univ                Batch: 14/15	Loss 5078.7407 (8654.7797)
2022-10-29 23:59:57,671:INFO: Dataset: univ                Batch: 15/15	Loss 593.4954 (8534.8269)
2022-10-29 23:59:58,978:INFO: Dataset: zara1               Batch: 1/8	Loss 2143.4629 (2143.4629)
2022-10-29 23:59:59,984:INFO: Dataset: zara1               Batch: 2/8	Loss 3053.3257 (2571.9554)
2022-10-30 00:00:00,990:INFO: Dataset: zara1               Batch: 3/8	Loss 3314.6489 (2821.8244)
2022-10-30 00:00:01,996:INFO: Dataset: zara1               Batch: 4/8	Loss 8205.5781 (4086.9791)
2022-10-30 00:00:03,002:INFO: Dataset: zara1               Batch: 5/8	Loss 729.8030 (3417.1842)
2022-10-30 00:00:04,010:INFO: Dataset: zara1               Batch: 6/8	Loss 3833.9634 (3483.4121)
2022-10-30 00:00:05,020:INFO: Dataset: zara1               Batch: 7/8	Loss 533.1235 (3118.6105)
2022-10-30 00:00:05,952:INFO: Dataset: zara1               Batch: 8/8	Loss 2611.2588 (3056.1262)
2022-10-30 00:00:07,261:INFO: Dataset: zara2               Batch:  1/18	Loss 2916.8403 (2916.8403)
2022-10-30 00:00:08,266:INFO: Dataset: zara2               Batch:  2/18	Loss 15202.2188 (8978.3486)
2022-10-30 00:00:09,266:INFO: Dataset: zara2               Batch:  3/18	Loss 10619.9033 (9543.9661)
2022-10-30 00:00:10,265:INFO: Dataset: zara2               Batch:  4/18	Loss 50642.5039 (19244.4298)
2022-10-30 00:00:11,268:INFO: Dataset: zara2               Batch:  5/18	Loss 7101.1250 (16966.6534)
2022-10-30 00:00:12,269:INFO: Dataset: zara2               Batch:  6/18	Loss 1243.8741 (14351.4102)
2022-10-30 00:00:13,265:INFO: Dataset: zara2               Batch:  7/18	Loss 10572.2051 (13857.3323)
2022-10-30 00:00:14,266:INFO: Dataset: zara2               Batch:  8/18	Loss 62857.7539 (19572.4867)
2022-10-30 00:00:15,268:INFO: Dataset: zara2               Batch:  9/18	Loss 7030.9883 (17959.3516)
2022-10-30 00:00:16,266:INFO: Dataset: zara2               Batch: 10/18	Loss 70541.4297 (23605.4343)
2022-10-30 00:00:17,259:INFO: Dataset: zara2               Batch: 11/18	Loss 79594.8516 (28993.6632)
2022-10-30 00:00:18,257:INFO: Dataset: zara2               Batch: 12/18	Loss 4807.4131 (26946.4019)
2022-10-30 00:00:19,260:INFO: Dataset: zara2               Batch: 13/18	Loss 5949.9634 (25321.0249)
2022-10-30 00:00:20,260:INFO: Dataset: zara2               Batch: 14/18	Loss 29510.7734 (25649.7186)
2022-10-30 00:00:21,259:INFO: Dataset: zara2               Batch: 15/18	Loss 2411.8582 (24183.1190)
2022-10-30 00:00:22,260:INFO: Dataset: zara2               Batch: 16/18	Loss 9501.9727 (23326.8543)
2022-10-30 00:00:23,260:INFO: Dataset: zara2               Batch: 17/18	Loss 8075.5806 (22453.7606)
2022-10-30 00:00:24,163:INFO: Dataset: zara2               Batch: 18/18	Loss 1974.9431 (21431.3432)
2022-10-30 00:00:24,228:INFO: - Computing ADE (validation o)
2022-10-30 00:00:24,572:INFO: 		 ADE on eth                       dataset:	 3.3807053565979004
2022-10-30 00:00:24,572:INFO: Average validation o:	ADE  3.3807	FDE  5.7560
2022-10-30 00:00:24,573:INFO: - Computing ADE (validation)
2022-10-30 00:00:24,901:INFO: 		 ADE on hotel                     dataset:	 1.2510616779327393
2022-10-30 00:00:25,327:INFO: 		 ADE on univ                      dataset:	 1.6129125356674194
2022-10-30 00:00:25,660:INFO: 		 ADE on zara1                     dataset:	 2.420950412750244
2022-10-30 00:00:26,182:INFO: 		 ADE on zara2                     dataset:	 1.6814274787902832
2022-10-30 00:00:26,182:INFO: Average validation:	ADE  1.6652	FDE  2.9853
2022-10-30 00:00:26,183:INFO: - Computing ADE (training)
2022-10-30 00:00:26,633:INFO: 		 ADE on hotel                     dataset:	 1.5418556928634644
2022-10-30 00:00:27,649:INFO: 		 ADE on univ                      dataset:	 1.5289416313171387
2022-10-30 00:00:28,314:INFO: 		 ADE on zara1                     dataset:	 2.676240921020508
2022-10-30 00:00:29,487:INFO: 		 ADE on zara2                     dataset:	 2.031723737716675
2022-10-30 00:00:29,487:INFO: Average training:	ADE  1.7044	FDE  3.0684
2022-10-30 00:00:29,498:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_405.pth.tar
2022-10-30 00:00:29,498:INFO: 
===> EPOCH: 406 (P4)
2022-10-30 00:00:29,498:INFO: - Computing loss (training)
2022-10-30 00:00:30,719:INFO: Dataset: hotel               Batch: 1/4	Loss 83.3918 (83.3918)
2022-10-30 00:00:31,719:INFO: Dataset: hotel               Batch: 2/4	Loss 109.1681 (96.2186)
2022-10-30 00:00:32,718:INFO: Dataset: hotel               Batch: 3/4	Loss 82.4934 (91.7167)
2022-10-30 00:00:33,446:INFO: Dataset: hotel               Batch: 4/4	Loss 50.8122 (84.5396)
2022-10-30 00:00:34,819:INFO: Dataset: univ                Batch:  1/15	Loss 35655.7656 (35655.7656)
2022-10-30 00:00:35,898:INFO: Dataset: univ                Batch:  2/15	Loss 11270.0928 (22451.8577)
2022-10-30 00:00:36,970:INFO: Dataset: univ                Batch:  3/15	Loss 6243.0376 (16904.0734)
2022-10-30 00:00:38,053:INFO: Dataset: univ                Batch:  4/15	Loss 14629.1045 (16281.3016)
2022-10-30 00:00:39,128:INFO: Dataset: univ                Batch:  5/15	Loss 4231.5371 (13786.0350)
2022-10-30 00:00:40,201:INFO: Dataset: univ                Batch:  6/15	Loss 5253.6694 (12415.4428)
2022-10-30 00:00:41,268:INFO: Dataset: univ                Batch:  7/15	Loss 19106.5410 (13283.7830)
2022-10-30 00:00:42,335:INFO: Dataset: univ                Batch:  8/15	Loss 7140.4604 (12538.1679)
2022-10-30 00:00:43,418:INFO: Dataset: univ                Batch:  9/15	Loss 8635.3721 (12076.6986)
2022-10-30 00:00:44,492:INFO: Dataset: univ                Batch: 10/15	Loss 3935.4556 (11243.4210)
2022-10-30 00:00:45,584:INFO: Dataset: univ                Batch: 11/15	Loss 3854.3242 (10534.4073)
2022-10-30 00:00:46,685:INFO: Dataset: univ                Batch: 12/15	Loss 7581.1143 (10280.9568)
2022-10-30 00:00:47,860:INFO: Dataset: univ                Batch: 13/15	Loss 6752.7031 (10006.2805)
2022-10-30 00:00:48,967:INFO: Dataset: univ                Batch: 14/15	Loss 11449.7861 (10109.0674)
2022-10-30 00:00:49,489:INFO: Dataset: univ                Batch: 15/15	Loss 597.8129 (10016.6682)
2022-10-30 00:00:50,874:INFO: Dataset: zara1               Batch: 1/8	Loss 546.1536 (546.1536)
2022-10-30 00:00:51,892:INFO: Dataset: zara1               Batch: 2/8	Loss 11815.6895 (6058.9568)
2022-10-30 00:00:52,928:INFO: Dataset: zara1               Batch: 3/8	Loss 454.0140 (3958.0513)
2022-10-30 00:00:53,956:INFO: Dataset: zara1               Batch: 4/8	Loss 1448.9144 (3392.5750)
2022-10-30 00:00:54,976:INFO: Dataset: zara1               Batch: 5/8	Loss 428.5658 (2786.9184)
2022-10-30 00:00:55,984:INFO: Dataset: zara1               Batch: 6/8	Loss 740.2675 (2432.6361)
2022-10-30 00:00:57,024:INFO: Dataset: zara1               Batch: 7/8	Loss 2821.5491 (2482.8561)
2022-10-30 00:00:57,958:INFO: Dataset: zara1               Batch: 8/8	Loss 805.2381 (2275.3612)
2022-10-30 00:00:59,339:INFO: Dataset: zara2               Batch:  1/18	Loss 2679.7637 (2679.7637)
2022-10-30 00:01:00,394:INFO: Dataset: zara2               Batch:  2/18	Loss 6427.0503 (4583.0618)
2022-10-30 00:01:01,426:INFO: Dataset: zara2               Batch:  3/18	Loss 5533.7402 (4884.7014)
2022-10-30 00:01:02,469:INFO: Dataset: zara2               Batch:  4/18	Loss 2396.8596 (4298.2282)
2022-10-30 00:01:03,512:INFO: Dataset: zara2               Batch:  5/18	Loss 1617.3090 (3741.6304)
2022-10-30 00:01:04,588:INFO: Dataset: zara2               Batch:  6/18	Loss 1510.7411 (3382.0610)
2022-10-30 00:01:05,675:INFO: Dataset: zara2               Batch:  7/18	Loss 14417.8340 (4994.8731)
2022-10-30 00:01:06,754:INFO: Dataset: zara2               Batch:  8/18	Loss 80985.4062 (14758.2316)
2022-10-30 00:01:07,832:INFO: Dataset: zara2               Batch:  9/18	Loss 8355.9893 (14001.4505)
2022-10-30 00:01:08,889:INFO: Dataset: zara2               Batch: 10/18	Loss 12377.7188 (13828.1068)
2022-10-30 00:01:09,984:INFO: Dataset: zara2               Batch: 11/18	Loss 203747.2344 (30545.4480)
2022-10-30 00:01:11,025:INFO: Dataset: zara2               Batch: 12/18	Loss 7632.1992 (28671.0848)
2022-10-30 00:01:12,067:INFO: Dataset: zara2               Batch: 13/18	Loss 15525.8125 (27657.8529)
2022-10-30 00:01:13,130:INFO: Dataset: zara2               Batch: 14/18	Loss 7813.9390 (26187.6257)
2022-10-30 00:01:14,227:INFO: Dataset: zara2               Batch: 15/18	Loss 11891.4678 (25313.9560)
2022-10-30 00:01:15,290:INFO: Dataset: zara2               Batch: 16/18	Loss 3114.1843 (24062.5689)
2022-10-30 00:01:16,375:INFO: Dataset: zara2               Batch: 17/18	Loss 3924.9343 (22835.3058)
2022-10-30 00:01:17,327:INFO: Dataset: zara2               Batch: 18/18	Loss 14196.9688 (22398.3193)
2022-10-30 00:01:17,391:INFO: - Computing ADE (validation o)
2022-10-30 00:01:17,784:INFO: 		 ADE on eth                       dataset:	 3.447516679763794
2022-10-30 00:01:17,784:INFO: Average validation o:	ADE  3.4475	FDE  5.8976
2022-10-30 00:01:17,785:INFO: - Computing ADE (validation)
2022-10-30 00:01:18,118:INFO: 		 ADE on hotel                     dataset:	 1.3367944955825806
2022-10-30 00:01:18,563:INFO: 		 ADE on univ                      dataset:	 1.6349761486053467
2022-10-30 00:01:18,905:INFO: 		 ADE on zara1                     dataset:	 2.3808677196502686
2022-10-30 00:01:19,459:INFO: 		 ADE on zara2                     dataset:	 1.7343003749847412
2022-10-30 00:01:19,459:INFO: Average validation:	ADE  1.6984	FDE  3.0539
2022-10-30 00:01:19,460:INFO: - Computing ADE (training)
2022-10-30 00:01:19,904:INFO: 		 ADE on hotel                     dataset:	 1.600955605506897
2022-10-30 00:01:20,960:INFO: 		 ADE on univ                      dataset:	 1.5651098489761353
2022-10-30 00:01:21,662:INFO: 		 ADE on zara1                     dataset:	 2.6956145763397217
2022-10-30 00:01:22,827:INFO: 		 ADE on zara2                     dataset:	 2.0927371978759766
2022-10-30 00:01:22,827:INFO: Average training:	ADE  1.7451	FDE  3.1471
2022-10-30 00:01:22,838:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_406.pth.tar
2022-10-30 00:01:22,838:INFO: 
===> EPOCH: 407 (P4)
2022-10-30 00:01:22,838:INFO: - Computing loss (training)
2022-10-30 00:01:24,066:INFO: Dataset: hotel               Batch: 1/4	Loss 86.3409 (86.3409)
2022-10-30 00:01:25,082:INFO: Dataset: hotel               Batch: 2/4	Loss 86.8418 (86.5955)
2022-10-30 00:01:26,091:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8629 (85.3611)
2022-10-30 00:01:26,822:INFO: Dataset: hotel               Batch: 4/4	Loss 54.3256 (80.2022)
2022-10-30 00:01:28,173:INFO: Dataset: univ                Batch:  1/15	Loss 5760.7041 (5760.7041)
2022-10-30 00:01:29,245:INFO: Dataset: univ                Batch:  2/15	Loss 6301.0312 (6026.9523)
2022-10-30 00:01:30,306:INFO: Dataset: univ                Batch:  3/15	Loss 48063.3398 (19942.0445)
2022-10-30 00:01:31,365:INFO: Dataset: univ                Batch:  4/15	Loss 1413.1790 (15261.8756)
2022-10-30 00:01:32,432:INFO: Dataset: univ                Batch:  5/15	Loss 11733.4531 (14541.8487)
2022-10-30 00:01:33,510:INFO: Dataset: univ                Batch:  6/15	Loss 10149.2256 (13868.2212)
2022-10-30 00:01:34,614:INFO: Dataset: univ                Batch:  7/15	Loss 3839.3643 (12428.2519)
2022-10-30 00:01:35,761:INFO: Dataset: univ                Batch:  8/15	Loss 56385.4102 (18341.4822)
2022-10-30 00:01:36,879:INFO: Dataset: univ                Batch:  9/15	Loss 2901.3250 (16567.8074)
2022-10-30 00:01:37,979:INFO: Dataset: univ                Batch: 10/15	Loss 8557.4531 (15734.0912)
2022-10-30 00:01:39,082:INFO: Dataset: univ                Batch: 11/15	Loss 3092.6230 (14416.9330)
2022-10-30 00:01:40,170:INFO: Dataset: univ                Batch: 12/15	Loss 8371.5010 (13939.7069)
2022-10-30 00:01:41,245:INFO: Dataset: univ                Batch: 13/15	Loss 10961.8369 (13705.7723)
2022-10-30 00:01:42,330:INFO: Dataset: univ                Batch: 14/15	Loss 10511.2930 (13465.8409)
2022-10-30 00:01:42,796:INFO: Dataset: univ                Batch: 15/15	Loss 2049.6465 (13307.8688)
2022-10-30 00:01:44,165:INFO: Dataset: zara1               Batch: 1/8	Loss 1934.0929 (1934.0929)
2022-10-30 00:01:45,185:INFO: Dataset: zara1               Batch: 2/8	Loss 2050.8970 (1993.3703)
2022-10-30 00:01:46,197:INFO: Dataset: zara1               Batch: 3/8	Loss 1279.9266 (1762.0948)
2022-10-30 00:01:47,215:INFO: Dataset: zara1               Batch: 4/8	Loss 2947.6904 (2108.2985)
2022-10-30 00:01:48,221:INFO: Dataset: zara1               Batch: 5/8	Loss 1587.3661 (1998.7172)
2022-10-30 00:01:49,221:INFO: Dataset: zara1               Batch: 6/8	Loss 3360.8882 (2217.1088)
2022-10-30 00:01:50,220:INFO: Dataset: zara1               Batch: 7/8	Loss 3367.7776 (2369.6724)
2022-10-30 00:01:51,127:INFO: Dataset: zara1               Batch: 8/8	Loss 50158.9688 (7475.5814)
2022-10-30 00:01:52,445:INFO: Dataset: zara2               Batch:  1/18	Loss 6400.5435 (6400.5435)
2022-10-30 00:01:53,481:INFO: Dataset: zara2               Batch:  2/18	Loss 3050.2610 (4625.0481)
2022-10-30 00:01:54,522:INFO: Dataset: zara2               Batch:  3/18	Loss 27311.7402 (12483.3903)
2022-10-30 00:01:55,541:INFO: Dataset: zara2               Batch:  4/18	Loss 27683.2383 (16170.6538)
2022-10-30 00:01:56,569:INFO: Dataset: zara2               Batch:  5/18	Loss 1049.8667 (13188.9601)
2022-10-30 00:01:57,648:INFO: Dataset: zara2               Batch:  6/18	Loss 7567.0874 (12282.2065)
2022-10-30 00:01:58,686:INFO: Dataset: zara2               Batch:  7/18	Loss 2880.0735 (10815.4087)
2022-10-30 00:01:59,712:INFO: Dataset: zara2               Batch:  8/18	Loss 25523.3672 (12694.7898)
2022-10-30 00:02:00,733:INFO: Dataset: zara2               Batch:  9/18	Loss 20786.1406 (13570.5595)
2022-10-30 00:02:01,753:INFO: Dataset: zara2               Batch: 10/18	Loss 38313.6797 (16353.4224)
2022-10-30 00:02:02,767:INFO: Dataset: zara2               Batch: 11/18	Loss 4341.8081 (15321.6093)
2022-10-30 00:02:03,778:INFO: Dataset: zara2               Batch: 12/18	Loss 13954.5840 (15196.3184)
2022-10-30 00:02:04,793:INFO: Dataset: zara2               Batch: 13/18	Loss 26787.8047 (16091.8435)
2022-10-30 00:02:05,807:INFO: Dataset: zara2               Batch: 14/18	Loss 34409.0430 (17484.4923)
2022-10-30 00:02:06,927:INFO: Dataset: zara2               Batch: 15/18	Loss 3463.0688 (16491.2164)
2022-10-30 00:02:07,961:INFO: Dataset: zara2               Batch: 16/18	Loss 2435.1553 (15626.6262)
2022-10-30 00:02:09,012:INFO: Dataset: zara2               Batch: 17/18	Loss 1091.7854 (14848.8336)
2022-10-30 00:02:09,973:INFO: Dataset: zara2               Batch: 18/18	Loss 4181.4785 (14298.6246)
2022-10-30 00:02:10,035:INFO: - Computing ADE (validation o)
2022-10-30 00:02:10,385:INFO: 		 ADE on eth                       dataset:	 3.5349369049072266
2022-10-30 00:02:10,385:INFO: Average validation o:	ADE  3.5349	FDE  6.0848
2022-10-30 00:02:10,386:INFO: - Computing ADE (validation)
2022-10-30 00:02:10,742:INFO: 		 ADE on hotel                     dataset:	 1.431258201599121
2022-10-30 00:02:11,169:INFO: 		 ADE on univ                      dataset:	 1.6817514896392822
2022-10-30 00:02:11,498:INFO: 		 ADE on zara1                     dataset:	 2.351958990097046
2022-10-30 00:02:12,045:INFO: 		 ADE on zara2                     dataset:	 1.8025555610656738
2022-10-30 00:02:12,045:INFO: Average validation:	ADE  1.7513	FDE  3.1536
2022-10-30 00:02:12,046:INFO: - Computing ADE (training)
2022-10-30 00:02:12,523:INFO: 		 ADE on hotel                     dataset:	 1.6508768796920776
2022-10-30 00:02:13,544:INFO: 		 ADE on univ                      dataset:	 1.6117647886276245
2022-10-30 00:02:14,202:INFO: 		 ADE on zara1                     dataset:	 2.741086483001709
2022-10-30 00:02:15,383:INFO: 		 ADE on zara2                     dataset:	 2.1616361141204834
2022-10-30 00:02:15,383:INFO: Average training:	ADE  1.7963	FDE  3.2454
2022-10-30 00:02:15,394:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_407.pth.tar
2022-10-30 00:02:15,394:INFO: 
===> EPOCH: 408 (P4)
2022-10-30 00:02:15,394:INFO: - Computing loss (training)
2022-10-30 00:02:16,662:INFO: Dataset: hotel               Batch: 1/4	Loss 91.6711 (91.6711)
2022-10-30 00:02:17,687:INFO: Dataset: hotel               Batch: 2/4	Loss 83.1171 (87.2651)
2022-10-30 00:02:18,696:INFO: Dataset: hotel               Batch: 3/4	Loss 84.2119 (86.2648)
2022-10-30 00:02:19,482:INFO: Dataset: hotel               Batch: 4/4	Loss 51.6112 (80.9159)
2022-10-30 00:02:20,950:INFO: Dataset: univ                Batch:  1/15	Loss 3789.5840 (3789.5840)
2022-10-30 00:02:22,073:INFO: Dataset: univ                Batch:  2/15	Loss 32235.2305 (18223.7681)
2022-10-30 00:02:23,190:INFO: Dataset: univ                Batch:  3/15	Loss 6183.7559 (14459.8230)
2022-10-30 00:02:24,304:INFO: Dataset: univ                Batch:  4/15	Loss 3916.6802 (11954.0306)
2022-10-30 00:02:25,431:INFO: Dataset: univ                Batch:  5/15	Loss 2720.3694 (10029.5113)
2022-10-30 00:02:26,556:INFO: Dataset: univ                Batch:  6/15	Loss 4619.7368 (9156.2341)
2022-10-30 00:02:27,676:INFO: Dataset: univ                Batch:  7/15	Loss 4158.2461 (8528.4265)
2022-10-30 00:02:28,799:INFO: Dataset: univ                Batch:  8/15	Loss 61949.8828 (15297.8093)
2022-10-30 00:02:29,905:INFO: Dataset: univ                Batch:  9/15	Loss 1454.8191 (13761.6108)
2022-10-30 00:02:31,008:INFO: Dataset: univ                Batch: 10/15	Loss 3323.8591 (12775.6005)
2022-10-30 00:02:32,123:INFO: Dataset: univ                Batch: 11/15	Loss 67383.6328 (17818.7145)
2022-10-30 00:02:33,231:INFO: Dataset: univ                Batch: 12/15	Loss 1359.7679 (16533.2154)
2022-10-30 00:02:34,334:INFO: Dataset: univ                Batch: 13/15	Loss 2885.5798 (15587.5835)
2022-10-30 00:02:35,437:INFO: Dataset: univ                Batch: 14/15	Loss 745.6251 (14585.7995)
2022-10-30 00:02:35,916:INFO: Dataset: univ                Batch: 15/15	Loss 442.3244 (14384.0562)
2022-10-30 00:02:37,337:INFO: Dataset: zara1               Batch: 1/8	Loss 4336.2031 (4336.2031)
2022-10-30 00:02:38,384:INFO: Dataset: zara1               Batch: 2/8	Loss 422.4814 (2427.2654)
2022-10-30 00:02:39,437:INFO: Dataset: zara1               Batch: 3/8	Loss 1646.3982 (2155.2079)
2022-10-30 00:02:40,482:INFO: Dataset: zara1               Batch: 4/8	Loss 581.3328 (1797.7296)
2022-10-30 00:02:41,528:INFO: Dataset: zara1               Batch: 5/8	Loss 3009.0081 (2034.9800)
2022-10-30 00:02:42,579:INFO: Dataset: zara1               Batch: 6/8	Loss 22084.2129 (5637.0456)
2022-10-30 00:02:43,627:INFO: Dataset: zara1               Batch: 7/8	Loss 1862.6750 (5120.2505)
2022-10-30 00:02:44,568:INFO: Dataset: zara1               Batch: 8/8	Loss 291.1638 (4634.8002)
2022-10-30 00:02:45,928:INFO: Dataset: zara2               Batch:  1/18	Loss 5687.9282 (5687.9282)
2022-10-30 00:02:46,975:INFO: Dataset: zara2               Batch:  2/18	Loss 12491.0156 (9045.1682)
2022-10-30 00:02:48,024:INFO: Dataset: zara2               Batch:  3/18	Loss 2518.9543 (7037.6054)
2022-10-30 00:02:49,081:INFO: Dataset: zara2               Batch:  4/18	Loss 1649.8651 (5717.1607)
2022-10-30 00:02:50,131:INFO: Dataset: zara2               Batch:  5/18	Loss -1565.7073 (4230.7717)
2022-10-30 00:02:51,177:INFO: Dataset: zara2               Batch:  6/18	Loss 2921.9839 (4023.2486)
2022-10-30 00:02:52,223:INFO: Dataset: zara2               Batch:  7/18	Loss 19454.8652 (6250.6991)
2022-10-30 00:02:53,273:INFO: Dataset: zara2               Batch:  8/18	Loss 6479.1821 (6281.7625)
2022-10-30 00:02:54,319:INFO: Dataset: zara2               Batch:  9/18	Loss 1567.5941 (5728.5295)
2022-10-30 00:02:55,377:INFO: Dataset: zara2               Batch: 10/18	Loss 28886.6504 (8136.2607)
2022-10-30 00:02:56,428:INFO: Dataset: zara2               Batch: 11/18	Loss 6095.2393 (7964.0997)
2022-10-30 00:02:57,478:INFO: Dataset: zara2               Batch: 12/18	Loss 73808.7969 (13613.0620)
2022-10-30 00:02:58,523:INFO: Dataset: zara2               Batch: 13/18	Loss 82598.7109 (18551.9517)
2022-10-30 00:02:59,572:INFO: Dataset: zara2               Batch: 14/18	Loss 399637.3438 (46289.4370)
2022-10-30 00:03:00,632:INFO: Dataset: zara2               Batch: 15/18	Loss 50849.1484 (46600.4913)
2022-10-30 00:03:01,688:INFO: Dataset: zara2               Batch: 16/18	Loss 3203.1831 (43700.8939)
2022-10-30 00:03:02,741:INFO: Dataset: zara2               Batch: 17/18	Loss 4199.6089 (41330.2655)
2022-10-30 00:03:03,703:INFO: Dataset: zara2               Batch: 18/18	Loss 7021.6367 (39532.3096)
2022-10-30 00:03:03,770:INFO: - Computing ADE (validation o)
2022-10-30 00:03:04,120:INFO: 		 ADE on eth                       dataset:	 4.05841588973999
2022-10-30 00:03:04,120:INFO: Average validation o:	ADE  4.0584	FDE  7.1412
2022-10-30 00:03:04,121:INFO: - Computing ADE (validation)
2022-10-30 00:03:04,483:INFO: 		 ADE on hotel                     dataset:	 1.8369015455245972
2022-10-30 00:03:04,932:INFO: 		 ADE on univ                      dataset:	 1.9547795057296753
2022-10-30 00:03:05,296:INFO: 		 ADE on zara1                     dataset:	 2.19704008102417
2022-10-30 00:03:05,835:INFO: 		 ADE on zara2                     dataset:	 2.1181986331939697
2022-10-30 00:03:05,835:INFO: Average validation:	ADE  2.0223	FDE  3.7262
2022-10-30 00:03:05,836:INFO: - Computing ADE (training)
2022-10-30 00:03:06,309:INFO: 		 ADE on hotel                     dataset:	 1.9929773807525635
2022-10-30 00:03:07,372:INFO: 		 ADE on univ                      dataset:	 1.8892290592193604
2022-10-30 00:03:08,051:INFO: 		 ADE on zara1                     dataset:	 2.933563709259033
2022-10-30 00:03:09,260:INFO: 		 ADE on zara2                     dataset:	 2.5302932262420654
2022-10-30 00:03:09,261:INFO: Average training:	ADE  2.0885	FDE  3.8520
2022-10-30 00:03:09,271:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_408.pth.tar
2022-10-30 00:03:09,271:INFO: 
===> EPOCH: 409 (P4)
2022-10-30 00:03:09,272:INFO: - Computing loss (training)
2022-10-30 00:03:10,584:INFO: Dataset: hotel               Batch: 1/4	Loss 325.2613 (325.2613)
2022-10-30 00:03:11,642:INFO: Dataset: hotel               Batch: 2/4	Loss 88.6240 (204.4604)
2022-10-30 00:03:12,694:INFO: Dataset: hotel               Batch: 3/4	Loss 86.7901 (166.4124)
2022-10-30 00:03:13,468:INFO: Dataset: hotel               Batch: 4/4	Loss 53.3115 (147.9104)
2022-10-30 00:03:14,893:INFO: Dataset: univ                Batch:  1/15	Loss 2551.5828 (2551.5828)
2022-10-30 00:03:16,019:INFO: Dataset: univ                Batch:  2/15	Loss 5146.7114 (3876.3075)
2022-10-30 00:03:17,143:INFO: Dataset: univ                Batch:  3/15	Loss 2874.5730 (3540.7467)
2022-10-30 00:03:18,264:INFO: Dataset: univ                Batch:  4/15	Loss 3682.2170 (3575.9771)
2022-10-30 00:03:19,384:INFO: Dataset: univ                Batch:  5/15	Loss 1408.9010 (3158.3128)
2022-10-30 00:03:20,511:INFO: Dataset: univ                Batch:  6/15	Loss 9739.1660 (4302.9698)
2022-10-30 00:03:21,624:INFO: Dataset: univ                Batch:  7/15	Loss 2471.1709 (4067.3323)
2022-10-30 00:03:22,753:INFO: Dataset: univ                Batch:  8/15	Loss 4683.7847 (4150.0559)
2022-10-30 00:03:23,870:INFO: Dataset: univ                Batch:  9/15	Loss 18605.1934 (5774.9256)
2022-10-30 00:03:24,993:INFO: Dataset: univ                Batch: 10/15	Loss 1929.5714 (5386.1222)
2022-10-30 00:03:26,111:INFO: Dataset: univ                Batch: 11/15	Loss 4967.0088 (5351.3934)
2022-10-30 00:03:27,238:INFO: Dataset: univ                Batch: 12/15	Loss 1196.4202 (4959.0402)
2022-10-30 00:03:28,360:INFO: Dataset: univ                Batch: 13/15	Loss 1460.8163 (4680.8547)
2022-10-30 00:03:29,481:INFO: Dataset: univ                Batch: 14/15	Loss 2069.4258 (4498.8490)
2022-10-30 00:03:29,961:INFO: Dataset: univ                Batch: 15/15	Loss 337.4854 (4434.5611)
2022-10-30 00:03:31,313:INFO: Dataset: zara1               Batch: 1/8	Loss 288.4807 (288.4807)
2022-10-30 00:03:32,357:INFO: Dataset: zara1               Batch: 2/8	Loss 181.4713 (233.2430)
2022-10-30 00:03:33,404:INFO: Dataset: zara1               Batch: 3/8	Loss 12649.1367 (4338.2269)
2022-10-30 00:03:34,449:INFO: Dataset: zara1               Batch: 4/8	Loss 205.3291 (3314.4815)
2022-10-30 00:03:35,494:INFO: Dataset: zara1               Batch: 5/8	Loss 240.4975 (2666.5461)
2022-10-30 00:03:36,541:INFO: Dataset: zara1               Batch: 6/8	Loss 21148.0254 (5636.5610)
2022-10-30 00:03:37,586:INFO: Dataset: zara1               Batch: 7/8	Loss 1335.8732 (5051.9284)
2022-10-30 00:03:38,533:INFO: Dataset: zara1               Batch: 8/8	Loss 224.0982 (4579.3092)
2022-10-30 00:03:39,932:INFO: Dataset: zara2               Batch:  1/18	Loss 1786.1616 (1786.1616)
2022-10-30 00:03:41,016:INFO: Dataset: zara2               Batch:  2/18	Loss 37903.0859 (19299.0509)
2022-10-30 00:03:42,109:INFO: Dataset: zara2               Batch:  3/18	Loss 1049.3014 (13203.5611)
2022-10-30 00:03:43,190:INFO: Dataset: zara2               Batch:  4/18	Loss 805.9501 (10043.7502)
2022-10-30 00:03:44,269:INFO: Dataset: zara2               Batch:  5/18	Loss 507.5776 (8259.7131)
2022-10-30 00:03:45,344:INFO: Dataset: zara2               Batch:  6/18	Loss 8923.9209 (8378.1197)
2022-10-30 00:03:46,421:INFO: Dataset: zara2               Batch:  7/18	Loss 7647.9072 (8267.9048)
2022-10-30 00:03:47,491:INFO: Dataset: zara2               Batch:  8/18	Loss 2675.9829 (7474.3048)
2022-10-30 00:03:48,556:INFO: Dataset: zara2               Batch:  9/18	Loss 9896.8809 (7719.7395)
2022-10-30 00:03:49,629:INFO: Dataset: zara2               Batch: 10/18	Loss 3809.1912 (7379.1321)
2022-10-30 00:03:50,701:INFO: Dataset: zara2               Batch: 11/18	Loss 1225.3394 (6847.3851)
2022-10-30 00:03:51,772:INFO: Dataset: zara2               Batch: 12/18	Loss -156.6063 (6235.6065)
2022-10-30 00:03:52,844:INFO: Dataset: zara2               Batch: 13/18	Loss 3631.9871 (6011.7095)
2022-10-30 00:03:53,908:INFO: Dataset: zara2               Batch: 14/18	Loss 1951.0476 (5706.7315)
2022-10-30 00:03:54,972:INFO: Dataset: zara2               Batch: 15/18	Loss 27412.0156 (7111.4934)
2022-10-30 00:03:56,040:INFO: Dataset: zara2               Batch: 16/18	Loss 1610.4951 (6765.6466)
2022-10-30 00:03:57,110:INFO: Dataset: zara2               Batch: 17/18	Loss 1339.0740 (6453.5572)
2022-10-30 00:03:58,074:INFO: Dataset: zara2               Batch: 18/18	Loss 1085.2051 (6177.5517)
2022-10-30 00:03:58,139:INFO: - Computing ADE (validation o)
2022-10-30 00:03:58,502:INFO: 		 ADE on eth                       dataset:	 4.566749572753906
2022-10-30 00:03:58,502:INFO: Average validation o:	ADE  4.5667	FDE  8.1887
2022-10-30 00:03:58,503:INFO: - Computing ADE (validation)
2022-10-30 00:03:58,850:INFO: 		 ADE on hotel                     dataset:	 2.1966142654418945
2022-10-30 00:03:59,303:INFO: 		 ADE on univ                      dataset:	 2.2696986198425293
2022-10-30 00:03:59,659:INFO: 		 ADE on zara1                     dataset:	 2.166525363922119
2022-10-30 00:04:00,205:INFO: 		 ADE on zara2                     dataset:	 2.4823644161224365
2022-10-30 00:04:00,205:INFO: Average validation:	ADE  2.3377	FDE  4.4304
2022-10-30 00:04:00,206:INFO: - Computing ADE (training)
2022-10-30 00:04:00,684:INFO: 		 ADE on hotel                     dataset:	 2.3479175567626953
2022-10-30 00:04:01,740:INFO: 		 ADE on univ                      dataset:	 2.205003023147583
2022-10-30 00:04:02,423:INFO: 		 ADE on zara1                     dataset:	 3.157569646835327
2022-10-30 00:04:03,649:INFO: 		 ADE on zara2                     dataset:	 2.871440887451172
2022-10-30 00:04:03,649:INFO: Average training:	ADE  2.4046	FDE  4.5679
2022-10-30 00:04:03,660:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_409.pth.tar
2022-10-30 00:04:03,660:INFO: 
===> EPOCH: 410 (P4)
2022-10-30 00:04:03,661:INFO: - Computing loss (training)
2022-10-30 00:04:04,958:INFO: Dataset: hotel               Batch: 1/4	Loss 94.1016 (94.1016)
2022-10-30 00:04:06,019:INFO: Dataset: hotel               Batch: 2/4	Loss 107.3578 (100.5506)
2022-10-30 00:04:07,070:INFO: Dataset: hotel               Batch: 3/4	Loss 99.1578 (100.0519)
2022-10-30 00:04:07,844:INFO: Dataset: hotel               Batch: 4/4	Loss 56.7662 (92.9709)
2022-10-30 00:04:09,279:INFO: Dataset: univ                Batch:  1/15	Loss 7359.1123 (7359.1123)
2022-10-30 00:04:10,413:INFO: Dataset: univ                Batch:  2/15	Loss 2018.7659 (4792.0555)
2022-10-30 00:04:11,549:INFO: Dataset: univ                Batch:  3/15	Loss 2753.4758 (4122.1106)
2022-10-30 00:04:12,684:INFO: Dataset: univ                Batch:  4/15	Loss 753.2872 (3276.7483)
2022-10-30 00:04:13,816:INFO: Dataset: univ                Batch:  5/15	Loss 1346.4230 (2891.6309)
2022-10-30 00:04:14,951:INFO: Dataset: univ                Batch:  6/15	Loss 1717.4679 (2691.4653)
2022-10-30 00:04:16,075:INFO: Dataset: univ                Batch:  7/15	Loss 2114.7937 (2610.4453)
2022-10-30 00:04:17,200:INFO: Dataset: univ                Batch:  8/15	Loss 2372.2178 (2580.0002)
2022-10-30 00:04:18,323:INFO: Dataset: univ                Batch:  9/15	Loss 3707.0693 (2702.7318)
2022-10-30 00:04:19,441:INFO: Dataset: univ                Batch: 10/15	Loss 1745.7152 (2602.1567)
2022-10-30 00:04:20,558:INFO: Dataset: univ                Batch: 11/15	Loss 2282.9983 (2572.8162)
2022-10-30 00:04:21,683:INFO: Dataset: univ                Batch: 12/15	Loss 863.3562 (2419.7916)
2022-10-30 00:04:22,790:INFO: Dataset: univ                Batch: 13/15	Loss 13484.4209 (3247.9790)
2022-10-30 00:04:23,906:INFO: Dataset: univ                Batch: 14/15	Loss 3222.8843 (3246.1653)
2022-10-30 00:04:24,394:INFO: Dataset: univ                Batch: 15/15	Loss 184.2451 (3207.8587)
2022-10-30 00:04:25,838:INFO: Dataset: zara1               Batch: 1/8	Loss 543.9478 (543.9478)
2022-10-30 00:04:26,887:INFO: Dataset: zara1               Batch: 2/8	Loss 311.9399 (434.5726)
2022-10-30 00:04:27,935:INFO: Dataset: zara1               Batch: 3/8	Loss 1036.5095 (627.4262)
2022-10-30 00:04:28,986:INFO: Dataset: zara1               Batch: 4/8	Loss 246.4035 (529.0345)
2022-10-30 00:04:30,038:INFO: Dataset: zara1               Batch: 5/8	Loss 886.0406 (595.4126)
2022-10-30 00:04:31,091:INFO: Dataset: zara1               Batch: 6/8	Loss 238.4475 (535.0489)
2022-10-30 00:04:32,138:INFO: Dataset: zara1               Batch: 7/8	Loss 563.3767 (539.1752)
2022-10-30 00:04:33,094:INFO: Dataset: zara1               Batch: 8/8	Loss 248.5084 (505.8250)
2022-10-30 00:04:34,470:INFO: Dataset: zara2               Batch:  1/18	Loss 2886.5776 (2886.5776)
2022-10-30 00:04:35,538:INFO: Dataset: zara2               Batch:  2/18	Loss 7423.2319 (5269.7983)
2022-10-30 00:04:36,606:INFO: Dataset: zara2               Batch:  3/18	Loss 2168.2036 (4226.9951)
2022-10-30 00:04:37,676:INFO: Dataset: zara2               Batch:  4/18	Loss 599.2802 (3245.7071)
2022-10-30 00:04:38,740:INFO: Dataset: zara2               Batch:  5/18	Loss 2490.8677 (3094.3164)
2022-10-30 00:04:39,815:INFO: Dataset: zara2               Batch:  6/18	Loss 346.3708 (2669.9534)
2022-10-30 00:04:40,890:INFO: Dataset: zara2               Batch:  7/18	Loss 27884.9590 (6395.7071)
2022-10-30 00:04:41,950:INFO: Dataset: zara2               Batch:  8/18	Loss 966.0179 (5760.7916)
2022-10-30 00:04:43,013:INFO: Dataset: zara2               Batch:  9/18	Loss 900.6957 (5210.7080)
2022-10-30 00:04:44,085:INFO: Dataset: zara2               Batch: 10/18	Loss 4812.3809 (5172.7667)
2022-10-30 00:04:45,153:INFO: Dataset: zara2               Batch: 11/18	Loss 1915.9753 (4912.2234)
2022-10-30 00:04:46,222:INFO: Dataset: zara2               Batch: 12/18	Loss 6041.9497 (5000.2220)
2022-10-30 00:04:47,279:INFO: Dataset: zara2               Batch: 13/18	Loss 3343.1450 (4859.9851)
2022-10-30 00:04:48,338:INFO: Dataset: zara2               Batch: 14/18	Loss 1466.7908 (4632.1784)
2022-10-30 00:04:49,397:INFO: Dataset: zara2               Batch: 15/18	Loss 8691.5215 (4895.6700)
2022-10-30 00:04:50,469:INFO: Dataset: zara2               Batch: 16/18	Loss 1430.6367 (4692.7740)
2022-10-30 00:04:51,531:INFO: Dataset: zara2               Batch: 17/18	Loss 3908.5645 (4648.5020)
2022-10-30 00:04:52,492:INFO: Dataset: zara2               Batch: 18/18	Loss 722.0043 (4493.3634)
2022-10-30 00:04:52,558:INFO: - Computing ADE (validation o)
2022-10-30 00:04:52,900:INFO: 		 ADE on eth                       dataset:	 4.714322566986084
2022-10-30 00:04:52,901:INFO: Average validation o:	ADE  4.7143	FDE  8.4961
2022-10-30 00:04:52,901:INFO: - Computing ADE (validation)
2022-10-30 00:04:53,253:INFO: 		 ADE on hotel                     dataset:	 2.344550132751465
2022-10-30 00:04:53,706:INFO: 		 ADE on univ                      dataset:	 2.341371774673462
2022-10-30 00:04:54,063:INFO: 		 ADE on zara1                     dataset:	 2.129561424255371
2022-10-30 00:04:54,617:INFO: 		 ADE on zara2                     dataset:	 2.5550005435943604
2022-10-30 00:04:54,617:INFO: Average validation:	ADE  2.4076	FDE  4.6119
2022-10-30 00:04:54,618:INFO: - Computing ADE (training)
2022-10-30 00:04:55,078:INFO: 		 ADE on hotel                     dataset:	 2.444202184677124
2022-10-30 00:04:56,158:INFO: 		 ADE on univ                      dataset:	 2.2873637676239014
2022-10-30 00:04:56,857:INFO: 		 ADE on zara1                     dataset:	 3.2159297466278076
2022-10-30 00:04:58,053:INFO: 		 ADE on zara2                     dataset:	 2.9608588218688965
2022-10-30 00:04:58,054:INFO: Average training:	ADE  2.4872	FDE  4.7695
2022-10-30 00:04:58,065:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_410.pth.tar
2022-10-30 00:04:58,065:INFO: 
===> EPOCH: 411 (P4)
2022-10-30 00:04:58,065:INFO: - Computing loss (training)
2022-10-30 00:04:59,348:INFO: Dataset: hotel               Batch: 1/4	Loss 91.9312 (91.9312)
2022-10-30 00:05:00,399:INFO: Dataset: hotel               Batch: 2/4	Loss 91.1806 (91.5710)
2022-10-30 00:05:01,450:INFO: Dataset: hotel               Batch: 3/4	Loss 228.5518 (137.1592)
2022-10-30 00:05:02,217:INFO: Dataset: hotel               Batch: 4/4	Loss 55.2906 (123.7665)
2022-10-30 00:05:03,665:INFO: Dataset: univ                Batch:  1/15	Loss 1433.3170 (1433.3170)
2022-10-30 00:05:04,807:INFO: Dataset: univ                Batch:  2/15	Loss 7975.1719 (4810.1724)
2022-10-30 00:05:05,941:INFO: Dataset: univ                Batch:  3/15	Loss 1110.9578 (3686.3390)
2022-10-30 00:05:07,074:INFO: Dataset: univ                Batch:  4/15	Loss 9315.7842 (5034.7150)
2022-10-30 00:05:08,221:INFO: Dataset: univ                Batch:  5/15	Loss 1437.4590 (4268.3921)
2022-10-30 00:05:09,360:INFO: Dataset: univ                Batch:  6/15	Loss 34881.4375 (9354.9101)
2022-10-30 00:05:10,502:INFO: Dataset: univ                Batch:  7/15	Loss 1530.3478 (8109.8895)
2022-10-30 00:05:11,630:INFO: Dataset: univ                Batch:  8/15	Loss 3572.2712 (7550.2904)
2022-10-30 00:05:12,762:INFO: Dataset: univ                Batch:  9/15	Loss 1944.8372 (6914.8780)
2022-10-30 00:05:13,891:INFO: Dataset: univ                Batch: 10/15	Loss 1807.2688 (6422.8716)
2022-10-30 00:05:15,006:INFO: Dataset: univ                Batch: 11/15	Loss 723.9815 (5917.0551)
2022-10-30 00:05:16,127:INFO: Dataset: univ                Batch: 12/15	Loss 2160.4182 (5601.5287)
2022-10-30 00:05:17,241:INFO: Dataset: univ                Batch: 13/15	Loss 805.9069 (5241.8695)
2022-10-30 00:05:18,363:INFO: Dataset: univ                Batch: 14/15	Loss -550.1544 (4815.0244)
2022-10-30 00:05:18,844:INFO: Dataset: univ                Batch: 15/15	Loss 594.7320 (4760.4258)
2022-10-30 00:05:20,185:INFO: Dataset: zara1               Batch: 1/8	Loss 683.5543 (683.5543)
2022-10-30 00:05:21,248:INFO: Dataset: zara1               Batch: 2/8	Loss 2358.7659 (1448.7744)
2022-10-30 00:05:22,308:INFO: Dataset: zara1               Batch: 3/8	Loss 426.9820 (1126.4061)
2022-10-30 00:05:23,372:INFO: Dataset: zara1               Batch: 4/8	Loss 548.0842 (970.0868)
2022-10-30 00:05:24,430:INFO: Dataset: zara1               Batch: 5/8	Loss 295.8082 (842.0805)
2022-10-30 00:05:25,503:INFO: Dataset: zara1               Batch: 6/8	Loss 320.9252 (745.2693)
2022-10-30 00:05:26,562:INFO: Dataset: zara1               Batch: 7/8	Loss 436.2953 (703.4329)
2022-10-30 00:05:27,516:INFO: Dataset: zara1               Batch: 8/8	Loss 249.0456 (657.0375)
2022-10-30 00:05:28,882:INFO: Dataset: zara2               Batch:  1/18	Loss 988.4626 (988.4626)
2022-10-30 00:05:29,950:INFO: Dataset: zara2               Batch:  2/18	Loss 470.1646 (733.4510)
2022-10-30 00:05:31,021:INFO: Dataset: zara2               Batch:  3/18	Loss 14934.9941 (4980.9115)
2022-10-30 00:05:32,077:INFO: Dataset: zara2               Batch:  4/18	Loss 622.1292 (3836.4450)
2022-10-30 00:05:33,139:INFO: Dataset: zara2               Batch:  5/18	Loss 2643.7773 (3604.3973)
2022-10-30 00:05:34,207:INFO: Dataset: zara2               Batch:  6/18	Loss 7251.2563 (4251.4791)
2022-10-30 00:05:35,272:INFO: Dataset: zara2               Batch:  7/18	Loss 1702.0782 (3838.4358)
2022-10-30 00:05:36,331:INFO: Dataset: zara2               Batch:  8/18	Loss 5580.8853 (4037.6831)
2022-10-30 00:05:37,389:INFO: Dataset: zara2               Batch:  9/18	Loss 365.7801 (3597.7701)
2022-10-30 00:05:38,450:INFO: Dataset: zara2               Batch: 10/18	Loss 869.0204 (3338.0418)
2022-10-30 00:05:39,510:INFO: Dataset: zara2               Batch: 11/18	Loss 216.6929 (3066.7658)
2022-10-30 00:05:40,571:INFO: Dataset: zara2               Batch: 12/18	Loss 336.1500 (2818.4069)
2022-10-30 00:05:41,635:INFO: Dataset: zara2               Batch: 13/18	Loss 1495.5834 (2712.4742)
2022-10-30 00:05:42,701:INFO: Dataset: zara2               Batch: 14/18	Loss 297.9451 (2547.4688)
2022-10-30 00:05:43,780:INFO: Dataset: zara2               Batch: 15/18	Loss 3606.7461 (2615.0337)
2022-10-30 00:05:44,860:INFO: Dataset: zara2               Batch: 16/18	Loss 1371.1144 (2538.3021)
2022-10-30 00:05:45,918:INFO: Dataset: zara2               Batch: 17/18	Loss 29012.9355 (4024.6617)
2022-10-30 00:05:46,879:INFO: Dataset: zara2               Batch: 18/18	Loss 1303.8964 (3899.6208)
2022-10-30 00:05:46,946:INFO: - Computing ADE (validation o)
2022-10-30 00:05:47,298:INFO: 		 ADE on eth                       dataset:	 5.099941253662109
2022-10-30 00:05:47,299:INFO: Average validation o:	ADE  5.0999	FDE  9.3491
2022-10-30 00:05:47,299:INFO: - Computing ADE (validation)
2022-10-30 00:05:47,652:INFO: 		 ADE on hotel                     dataset:	 2.5919039249420166
2022-10-30 00:05:48,097:INFO: 		 ADE on univ                      dataset:	 2.5783698558807373
2022-10-30 00:05:48,453:INFO: 		 ADE on zara1                     dataset:	 2.150864839553833
2022-10-30 00:05:49,018:INFO: 		 ADE on zara2                     dataset:	 2.802558422088623
2022-10-30 00:05:49,019:INFO: Average validation:	ADE  2.6365	FDE  5.1818
2022-10-30 00:05:49,020:INFO: - Computing ADE (training)
2022-10-30 00:05:49,472:INFO: 		 ADE on hotel                     dataset:	 2.676417827606201
2022-10-30 00:05:50,527:INFO: 		 ADE on univ                      dataset:	 2.5312135219573975
2022-10-30 00:05:51,224:INFO: 		 ADE on zara1                     dataset:	 3.402733087539673
2022-10-30 00:05:52,429:INFO: 		 ADE on zara2                     dataset:	 3.2294414043426514
2022-10-30 00:05:52,429:INFO: Average training:	ADE  2.7321	FDE  5.3716
2022-10-30 00:05:52,441:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_411.pth.tar
2022-10-30 00:05:52,441:INFO: 
===> EPOCH: 412 (P4)
2022-10-30 00:05:52,441:INFO: - Computing loss (training)
2022-10-30 00:05:53,718:INFO: Dataset: hotel               Batch: 1/4	Loss 92.9967 (92.9967)
2022-10-30 00:05:54,787:INFO: Dataset: hotel               Batch: 2/4	Loss 99.8683 (96.5806)
2022-10-30 00:05:55,848:INFO: Dataset: hotel               Batch: 3/4	Loss 126.3641 (105.7447)
2022-10-30 00:05:56,628:INFO: Dataset: hotel               Batch: 4/4	Loss 58.8544 (98.2596)
2022-10-30 00:05:58,067:INFO: Dataset: univ                Batch:  1/15	Loss 1811.6912 (1811.6912)
2022-10-30 00:05:59,192:INFO: Dataset: univ                Batch:  2/15	Loss 4054.7180 (2923.8130)
2022-10-30 00:06:00,314:INFO: Dataset: univ                Batch:  3/15	Loss 1724.8457 (2520.6401)
2022-10-30 00:06:01,437:INFO: Dataset: univ                Batch:  4/15	Loss 6114.7534 (3432.0600)
2022-10-30 00:06:02,565:INFO: Dataset: univ                Batch:  5/15	Loss 3551.1560 (3457.0986)
2022-10-30 00:06:03,692:INFO: Dataset: univ                Batch:  6/15	Loss 1841.4199 (3179.1442)
2022-10-30 00:06:04,809:INFO: Dataset: univ                Batch:  7/15	Loss 816.1309 (2848.9565)
2022-10-30 00:06:05,928:INFO: Dataset: univ                Batch:  8/15	Loss 558.1234 (2563.6506)
2022-10-30 00:06:07,051:INFO: Dataset: univ                Batch:  9/15	Loss 2690.2964 (2577.8723)
2022-10-30 00:06:08,176:INFO: Dataset: univ                Batch: 10/15	Loss 5197.2598 (2858.8428)
2022-10-30 00:06:09,296:INFO: Dataset: univ                Batch: 11/15	Loss 3555.7729 (2926.3192)
2022-10-30 00:06:10,412:INFO: Dataset: univ                Batch: 12/15	Loss 905.7320 (2771.6940)
2022-10-30 00:06:11,528:INFO: Dataset: univ                Batch: 13/15	Loss 3423.7217 (2819.1494)
2022-10-30 00:06:12,665:INFO: Dataset: univ                Batch: 14/15	Loss 17006.5332 (3927.3362)
2022-10-30 00:06:13,151:INFO: Dataset: univ                Batch: 15/15	Loss 8941.5078 (4001.2348)
2022-10-30 00:06:14,528:INFO: Dataset: zara1               Batch: 1/8	Loss 1928.4440 (1928.4440)
2022-10-30 00:06:15,602:INFO: Dataset: zara1               Batch: 2/8	Loss 155.8966 (979.2735)
2022-10-30 00:06:16,760:INFO: Dataset: zara1               Batch: 3/8	Loss 1661.8450 (1198.4935)
2022-10-30 00:06:17,814:INFO: Dataset: zara1               Batch: 4/8	Loss 1778.2285 (1354.8613)
2022-10-30 00:06:18,864:INFO: Dataset: zara1               Batch: 5/8	Loss 141.7359 (1072.9238)
2022-10-30 00:06:19,906:INFO: Dataset: zara1               Batch: 6/8	Loss 6230.4634 (1931.3410)
2022-10-30 00:06:20,954:INFO: Dataset: zara1               Batch: 7/8	Loss 546.2487 (1733.0081)
2022-10-30 00:06:21,894:INFO: Dataset: zara1               Batch: 8/8	Loss 1175.4302 (1677.5438)
2022-10-30 00:06:23,267:INFO: Dataset: zara2               Batch:  1/18	Loss 338.6921 (338.6921)
2022-10-30 00:06:24,340:INFO: Dataset: zara2               Batch:  2/18	Loss 741.5950 (562.9435)
2022-10-30 00:06:25,415:INFO: Dataset: zara2               Batch:  3/18	Loss 1709.1284 (937.4277)
2022-10-30 00:06:26,483:INFO: Dataset: zara2               Batch:  4/18	Loss 465.2333 (812.1369)
2022-10-30 00:06:27,552:INFO: Dataset: zara2               Batch:  5/18	Loss 215.5402 (693.1105)
2022-10-30 00:06:28,617:INFO: Dataset: zara2               Batch:  6/18	Loss 377.2785 (644.1303)
2022-10-30 00:06:29,682:INFO: Dataset: zara2               Batch:  7/18	Loss 1146.7916 (720.8000)
2022-10-30 00:06:30,752:INFO: Dataset: zara2               Batch:  8/18	Loss 612.3477 (707.0613)
2022-10-30 00:06:31,819:INFO: Dataset: zara2               Batch:  9/18	Loss 554.6113 (690.1976)
2022-10-30 00:06:32,876:INFO: Dataset: zara2               Batch: 10/18	Loss -6027.2856 (-59.5852)
2022-10-30 00:06:33,946:INFO: Dataset: zara2               Batch: 11/18	Loss 1539.8270 (111.9196)
2022-10-30 00:06:35,013:INFO: Dataset: zara2               Batch: 12/18	Loss 678.6175 (166.6764)
2022-10-30 00:06:36,068:INFO: Dataset: zara2               Batch: 13/18	Loss 290.1504 (175.4057)
2022-10-30 00:06:37,132:INFO: Dataset: zara2               Batch: 14/18	Loss 132.4734 (172.6306)
2022-10-30 00:06:38,192:INFO: Dataset: zara2               Batch: 15/18	Loss 1680.2656 (282.0389)
2022-10-30 00:06:39,250:INFO: Dataset: zara2               Batch: 16/18	Loss 189.6519 (275.9945)
2022-10-30 00:06:40,316:INFO: Dataset: zara2               Batch: 17/18	Loss 27835.8926 (1847.0140)
2022-10-30 00:06:41,291:INFO: Dataset: zara2               Batch: 18/18	Loss 5683.9009 (2034.1328)
2022-10-30 00:06:41,358:INFO: - Computing ADE (validation o)
2022-10-30 00:06:41,713:INFO: 		 ADE on eth                       dataset:	 5.462400436401367
2022-10-30 00:06:41,713:INFO: Average validation o:	ADE  5.4624	FDE  10.2425
2022-10-30 00:06:41,714:INFO: - Computing ADE (validation)
2022-10-30 00:06:42,051:INFO: 		 ADE on hotel                     dataset:	 2.882281541824341
2022-10-30 00:06:42,499:INFO: 		 ADE on univ                      dataset:	 2.8214218616485596
2022-10-30 00:06:42,850:INFO: 		 ADE on zara1                     dataset:	 2.2214086055755615
2022-10-30 00:06:43,408:INFO: 		 ADE on zara2                     dataset:	 3.079606294631958
2022-10-30 00:06:43,408:INFO: Average validation:	ADE  2.8846	FDE  5.8376
2022-10-30 00:06:43,409:INFO: - Computing ADE (training)
2022-10-30 00:06:43,882:INFO: 		 ADE on hotel                     dataset:	 2.8734161853790283
2022-10-30 00:06:44,951:INFO: 		 ADE on univ                      dataset:	 2.7836084365844727
2022-10-30 00:06:45,657:INFO: 		 ADE on zara1                     dataset:	 3.6199042797088623
2022-10-30 00:06:46,882:INFO: 		 ADE on zara2                     dataset:	 3.4949262142181396
2022-10-30 00:06:46,882:INFO: Average training:	ADE  2.9835	FDE  6.0425
2022-10-30 00:06:46,893:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_412.pth.tar
2022-10-30 00:06:46,893:INFO: 
===> EPOCH: 413 (P4)
2022-10-30 00:06:46,893:INFO: - Computing loss (training)
2022-10-30 00:06:48,179:INFO: Dataset: hotel               Batch: 1/4	Loss 94.6741 (94.6741)
2022-10-30 00:06:49,242:INFO: Dataset: hotel               Batch: 2/4	Loss 94.6521 (94.6627)
2022-10-30 00:06:50,307:INFO: Dataset: hotel               Batch: 3/4	Loss 92.1519 (93.8139)
2022-10-30 00:06:51,077:INFO: Dataset: hotel               Batch: 4/4	Loss 56.6676 (87.8352)
2022-10-30 00:06:52,497:INFO: Dataset: univ                Batch:  1/15	Loss 2864.3604 (2864.3604)
2022-10-30 00:06:53,614:INFO: Dataset: univ                Batch:  2/15	Loss 1359.2428 (2102.6181)
2022-10-30 00:06:54,722:INFO: Dataset: univ                Batch:  3/15	Loss 1378.2634 (1872.5962)
2022-10-30 00:06:55,820:INFO: Dataset: univ                Batch:  4/15	Loss 3132.6565 (2163.8856)
2022-10-30 00:06:56,919:INFO: Dataset: univ                Batch:  5/15	Loss 374.4200 (1823.7211)
2022-10-30 00:06:58,029:INFO: Dataset: univ                Batch:  6/15	Loss 1432.9796 (1755.7929)
2022-10-30 00:06:59,131:INFO: Dataset: univ                Batch:  7/15	Loss 803.5835 (1624.6292)
2022-10-30 00:07:00,238:INFO: Dataset: univ                Batch:  8/15	Loss 5481.8027 (2113.2023)
2022-10-30 00:07:01,341:INFO: Dataset: univ                Batch:  9/15	Loss 7610.0259 (2718.6544)
2022-10-30 00:07:02,453:INFO: Dataset: univ                Batch: 10/15	Loss 1984.7119 (2640.8781)
2022-10-30 00:07:03,563:INFO: Dataset: univ                Batch: 11/15	Loss 48133.1641 (7135.0031)
2022-10-30 00:07:04,681:INFO: Dataset: univ                Batch: 12/15	Loss 14480.1826 (7789.8265)
2022-10-30 00:07:05,786:INFO: Dataset: univ                Batch: 13/15	Loss 745.0875 (7276.0269)
2022-10-30 00:07:06,891:INFO: Dataset: univ                Batch: 14/15	Loss 9625.1670 (7431.7193)
2022-10-30 00:07:07,381:INFO: Dataset: univ                Batch: 15/15	Loss 98.1043 (7339.6234)
2022-10-30 00:07:08,730:INFO: Dataset: zara1               Batch: 1/8	Loss 255.7319 (255.7319)
2022-10-30 00:07:09,775:INFO: Dataset: zara1               Batch: 2/8	Loss 2220.0774 (1249.9312)
2022-10-30 00:07:10,816:INFO: Dataset: zara1               Batch: 3/8	Loss 9457.1582 (4066.3523)
2022-10-30 00:07:11,856:INFO: Dataset: zara1               Batch: 4/8	Loss 180.0127 (3153.5607)
2022-10-30 00:07:12,899:INFO: Dataset: zara1               Batch: 5/8	Loss 137.0838 (2594.1139)
2022-10-30 00:07:13,940:INFO: Dataset: zara1               Batch: 6/8	Loss 131.8809 (2192.9249)
2022-10-30 00:07:14,979:INFO: Dataset: zara1               Batch: 7/8	Loss 136.8680 (1880.7359)
2022-10-30 00:07:15,925:INFO: Dataset: zara1               Batch: 8/8	Loss 729.9509 (1751.1212)
2022-10-30 00:07:17,319:INFO: Dataset: zara2               Batch:  1/18	Loss 382.9252 (382.9252)
2022-10-30 00:07:18,388:INFO: Dataset: zara2               Batch:  2/18	Loss 3335.1836 (1854.4984)
2022-10-30 00:07:19,456:INFO: Dataset: zara2               Batch:  3/18	Loss 2441.5354 (2050.5796)
2022-10-30 00:07:20,528:INFO: Dataset: zara2               Batch:  4/18	Loss 349.9891 (1661.1417)
2022-10-30 00:07:21,592:INFO: Dataset: zara2               Batch:  5/18	Loss 5141.3643 (2463.2906)
2022-10-30 00:07:22,669:INFO: Dataset: zara2               Batch:  6/18	Loss 2013.0479 (2375.0077)
2022-10-30 00:07:23,748:INFO: Dataset: zara2               Batch:  7/18	Loss 1937.8741 (2314.7771)
2022-10-30 00:07:24,829:INFO: Dataset: zara2               Batch:  8/18	Loss 42820.8789 (7482.5910)
2022-10-30 00:07:25,909:INFO: Dataset: zara2               Batch:  9/18	Loss 272.3097 (6656.3394)
2022-10-30 00:07:26,982:INFO: Dataset: zara2               Batch: 10/18	Loss 543.4961 (6050.4487)
2022-10-30 00:07:28,047:INFO: Dataset: zara2               Batch: 11/18	Loss 2306.2737 (5700.0874)
2022-10-30 00:07:29,114:INFO: Dataset: zara2               Batch: 12/18	Loss 288.3478 (5241.8459)
2022-10-30 00:07:30,183:INFO: Dataset: zara2               Batch: 13/18	Loss 13751.2900 (5870.6794)
2022-10-30 00:07:31,250:INFO: Dataset: zara2               Batch: 14/18	Loss 1605.7576 (5568.8569)
2022-10-30 00:07:32,319:INFO: Dataset: zara2               Batch: 15/18	Loss 242.5372 (5210.9866)
2022-10-30 00:07:33,388:INFO: Dataset: zara2               Batch: 16/18	Loss 609.5800 (4919.7049)
2022-10-30 00:07:34,454:INFO: Dataset: zara2               Batch: 17/18	Loss 377.3313 (4664.8159)
2022-10-30 00:07:35,418:INFO: Dataset: zara2               Batch: 18/18	Loss 857.7517 (4491.7390)
2022-10-30 00:07:35,487:INFO: - Computing ADE (validation o)
2022-10-30 00:07:35,837:INFO: 		 ADE on eth                       dataset:	 4.7594146728515625
2022-10-30 00:07:35,837:INFO: Average validation o:	ADE  4.7594	FDE  8.5983
2022-10-30 00:07:35,837:INFO: - Computing ADE (validation)
2022-10-30 00:07:36,179:INFO: 		 ADE on hotel                     dataset:	 2.3298592567443848
2022-10-30 00:07:36,631:INFO: 		 ADE on univ                      dataset:	 2.3628334999084473
2022-10-30 00:07:36,990:INFO: 		 ADE on zara1                     dataset:	 2.0832321643829346
2022-10-30 00:07:37,546:INFO: 		 ADE on zara2                     dataset:	 2.592890977859497
2022-10-30 00:07:37,547:INFO: Average validation:	ADE  2.4292	FDE  4.6554
2022-10-30 00:07:37,547:INFO: - Computing ADE (training)
2022-10-30 00:07:38,012:INFO: 		 ADE on hotel                     dataset:	 2.419863700866699
2022-10-30 00:07:39,084:INFO: 		 ADE on univ                      dataset:	 2.302509307861328
2022-10-30 00:07:39,793:INFO: 		 ADE on zara1                     dataset:	 3.2230825424194336
2022-10-30 00:07:41,016:INFO: 		 ADE on zara2                     dataset:	 2.9710519313812256
2022-10-30 00:07:41,017:INFO: Average training:	ADE  2.4998	FDE  4.8035
2022-10-30 00:07:41,028:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_413.pth.tar
2022-10-30 00:07:41,028:INFO: 
===> EPOCH: 414 (P4)
2022-10-30 00:07:41,028:INFO: - Computing loss (training)
2022-10-30 00:07:42,315:INFO: Dataset: hotel               Batch: 1/4	Loss 89.2922 (89.2922)
2022-10-30 00:07:43,385:INFO: Dataset: hotel               Batch: 2/4	Loss 120.9194 (105.3725)
2022-10-30 00:07:44,452:INFO: Dataset: hotel               Batch: 3/4	Loss 88.2014 (99.6213)
2022-10-30 00:07:45,247:INFO: Dataset: hotel               Batch: 4/4	Loss 52.6420 (91.3162)
2022-10-30 00:07:46,690:INFO: Dataset: univ                Batch:  1/15	Loss 75940.3750 (75940.3750)
2022-10-30 00:07:47,807:INFO: Dataset: univ                Batch:  2/15	Loss 2370.3523 (40616.1403)
2022-10-30 00:07:48,919:INFO: Dataset: univ                Batch:  3/15	Loss 8837.6729 (30407.9536)
2022-10-30 00:07:50,044:INFO: Dataset: univ                Batch:  4/15	Loss 1456.2089 (22602.3823)
2022-10-30 00:07:51,153:INFO: Dataset: univ                Batch:  5/15	Loss 992.8514 (18781.7848)
2022-10-30 00:07:52,270:INFO: Dataset: univ                Batch:  6/15	Loss 4486.6196 (16414.1681)
2022-10-30 00:07:53,376:INFO: Dataset: univ                Batch:  7/15	Loss 953.6854 (14360.0619)
2022-10-30 00:07:54,489:INFO: Dataset: univ                Batch:  8/15	Loss 2130.8894 (12846.9926)
2022-10-30 00:07:55,610:INFO: Dataset: univ                Batch:  9/15	Loss 2872.9238 (11649.9851)
2022-10-30 00:07:56,732:INFO: Dataset: univ                Batch: 10/15	Loss 1374.1938 (10568.5398)
2022-10-30 00:07:57,828:INFO: Dataset: univ                Batch: 11/15	Loss 1604.3441 (9820.3332)
2022-10-30 00:07:58,946:INFO: Dataset: univ                Batch: 12/15	Loss 1651.5361 (9091.7254)
2022-10-30 00:08:00,052:INFO: Dataset: univ                Batch: 13/15	Loss 16251.3008 (9624.7082)
2022-10-30 00:08:01,161:INFO: Dataset: univ                Batch: 14/15	Loss 906.9116 (9027.3288)
2022-10-30 00:08:01,642:INFO: Dataset: univ                Batch: 15/15	Loss 104.8581 (8891.6017)
2022-10-30 00:08:03,004:INFO: Dataset: zara1               Batch: 1/8	Loss 2889.3364 (2889.3364)
2022-10-30 00:08:04,060:INFO: Dataset: zara1               Batch: 2/8	Loss 4027.1296 (3420.1573)
2022-10-30 00:08:05,117:INFO: Dataset: zara1               Batch: 3/8	Loss 37038.6562 (13449.9857)
2022-10-30 00:08:06,171:INFO: Dataset: zara1               Batch: 4/8	Loss 237.0510 (10233.0434)
2022-10-30 00:08:07,300:INFO: Dataset: zara1               Batch: 5/8	Loss 295.9285 (8327.8702)
2022-10-30 00:08:08,359:INFO: Dataset: zara1               Batch: 6/8	Loss 231.2183 (6860.8420)
2022-10-30 00:08:09,402:INFO: Dataset: zara1               Batch: 7/8	Loss 463.1328 (5966.4567)
2022-10-30 00:08:10,353:INFO: Dataset: zara1               Batch: 8/8	Loss 35024.9844 (9315.8344)
2022-10-30 00:08:11,732:INFO: Dataset: zara2               Batch:  1/18	Loss 5849.6152 (5849.6152)
2022-10-30 00:08:12,811:INFO: Dataset: zara2               Batch:  2/18	Loss 3755.4124 (4895.6609)
2022-10-30 00:08:13,895:INFO: Dataset: zara2               Batch:  3/18	Loss 647.4534 (3425.6191)
2022-10-30 00:08:14,978:INFO: Dataset: zara2               Batch:  4/18	Loss 913.6519 (2772.9179)
2022-10-30 00:08:16,055:INFO: Dataset: zara2               Batch:  5/18	Loss 632.4106 (2333.4509)
2022-10-30 00:08:17,135:INFO: Dataset: zara2               Batch:  6/18	Loss 756.2747 (2084.9594)
2022-10-30 00:08:18,212:INFO: Dataset: zara2               Batch:  7/18	Loss 3875.6050 (2358.0065)
2022-10-30 00:08:19,292:INFO: Dataset: zara2               Batch:  8/18	Loss 1792.8413 (2287.5692)
2022-10-30 00:08:20,367:INFO: Dataset: zara2               Batch:  9/18	Loss 352.7811 (2074.8505)
2022-10-30 00:08:21,445:INFO: Dataset: zara2               Batch: 10/18	Loss 10840.7705 (3069.5286)
2022-10-30 00:08:22,522:INFO: Dataset: zara2               Batch: 11/18	Loss 861.3441 (2870.7511)
2022-10-30 00:08:23,595:INFO: Dataset: zara2               Batch: 12/18	Loss 167.0024 (2674.2597)
2022-10-30 00:08:24,671:INFO: Dataset: zara2               Batch: 13/18	Loss 879.3436 (2537.0972)
2022-10-30 00:08:25,748:INFO: Dataset: zara2               Batch: 14/18	Loss 966.0436 (2429.8818)
2022-10-30 00:08:26,811:INFO: Dataset: zara2               Batch: 15/18	Loss 3909.3872 (2521.3795)
2022-10-30 00:08:27,882:INFO: Dataset: zara2               Batch: 16/18	Loss 546.0536 (2384.3711)
2022-10-30 00:08:28,951:INFO: Dataset: zara2               Batch: 17/18	Loss 12776.3633 (3006.1604)
2022-10-30 00:08:29,919:INFO: Dataset: zara2               Batch: 18/18	Loss 172.5535 (2873.5912)
2022-10-30 00:08:29,986:INFO: - Computing ADE (validation o)
2022-10-30 00:08:30,343:INFO: 		 ADE on eth                       dataset:	 4.448502063751221
2022-10-30 00:08:30,344:INFO: Average validation o:	ADE  4.4485	FDE  7.9465
2022-10-30 00:08:30,344:INFO: - Computing ADE (validation)
2022-10-30 00:08:30,679:INFO: 		 ADE on hotel                     dataset:	 2.198463201522827
2022-10-30 00:08:31,130:INFO: 		 ADE on univ                      dataset:	 2.216890573501587
2022-10-30 00:08:31,488:INFO: 		 ADE on zara1                     dataset:	 2.102198362350464
2022-10-30 00:08:32,029:INFO: 		 ADE on zara2                     dataset:	 2.4068403244018555
2022-10-30 00:08:32,029:INFO: Average validation:	ADE  2.2789	FDE  4.2879
2022-10-30 00:08:32,029:INFO: - Computing ADE (training)
2022-10-30 00:08:32,495:INFO: 		 ADE on hotel                     dataset:	 2.3099417686462402
2022-10-30 00:08:33,553:INFO: 		 ADE on univ                      dataset:	 2.14741849899292
2022-10-30 00:08:34,262:INFO: 		 ADE on zara1                     dataset:	 3.1149916648864746
2022-10-30 00:08:35,457:INFO: 		 ADE on zara2                     dataset:	 2.810779333114624
2022-10-30 00:08:35,458:INFO: Average training:	ADE  2.3478	FDE  4.4209
2022-10-30 00:08:35,469:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_414.pth.tar
2022-10-30 00:08:35,469:INFO: 
===> EPOCH: 415 (P4)
2022-10-30 00:08:35,469:INFO: - Computing loss (training)
2022-10-30 00:08:36,754:INFO: Dataset: hotel               Batch: 1/4	Loss 88.1686 (88.1686)
2022-10-30 00:08:37,814:INFO: Dataset: hotel               Batch: 2/4	Loss 88.2170 (88.1943)
2022-10-30 00:08:38,878:INFO: Dataset: hotel               Batch: 3/4	Loss 85.7613 (87.3606)
2022-10-30 00:08:39,653:INFO: Dataset: hotel               Batch: 4/4	Loss 54.8444 (82.5132)
2022-10-30 00:08:41,102:INFO: Dataset: univ                Batch:  1/15	Loss 3301.0493 (3301.0493)
2022-10-30 00:08:42,233:INFO: Dataset: univ                Batch:  2/15	Loss 2554.3970 (2931.7374)
2022-10-30 00:08:43,361:INFO: Dataset: univ                Batch:  3/15	Loss 480.5879 (2066.8200)
2022-10-30 00:08:44,494:INFO: Dataset: univ                Batch:  4/15	Loss 17546.8281 (5693.3671)
2022-10-30 00:08:45,627:INFO: Dataset: univ                Batch:  5/15	Loss 610.4297 (4574.5758)
2022-10-30 00:08:46,760:INFO: Dataset: univ                Batch:  6/15	Loss 1286.1573 (4032.0272)
2022-10-30 00:08:47,892:INFO: Dataset: univ                Batch:  7/15	Loss 2042.9148 (3737.9482)
2022-10-30 00:08:49,020:INFO: Dataset: univ                Batch:  8/15	Loss 1518.8785 (3467.8378)
2022-10-30 00:08:50,144:INFO: Dataset: univ                Batch:  9/15	Loss 849.0818 (3185.9354)
2022-10-30 00:08:51,274:INFO: Dataset: univ                Batch: 10/15	Loss 1219.1707 (2975.4732)
2022-10-30 00:08:52,400:INFO: Dataset: univ                Batch: 11/15	Loss 3062.5884 (2983.1420)
2022-10-30 00:08:53,527:INFO: Dataset: univ                Batch: 12/15	Loss 2844.7163 (2970.5690)
2022-10-30 00:08:54,653:INFO: Dataset: univ                Batch: 13/15	Loss 7397.4980 (3277.9345)
2022-10-30 00:08:55,781:INFO: Dataset: univ                Batch: 14/15	Loss 656.6700 (3101.2780)
2022-10-30 00:08:56,264:INFO: Dataset: univ                Batch: 15/15	Loss 299.7989 (3063.5745)
2022-10-30 00:08:57,621:INFO: Dataset: zara1               Batch: 1/8	Loss 166.9766 (166.9766)
2022-10-30 00:08:58,676:INFO: Dataset: zara1               Batch: 2/8	Loss 273.7692 (223.9626)
2022-10-30 00:08:59,727:INFO: Dataset: zara1               Batch: 3/8	Loss 215.6661 (221.1893)
2022-10-30 00:09:00,774:INFO: Dataset: zara1               Batch: 4/8	Loss 445.2061 (275.8911)
2022-10-30 00:09:01,823:INFO: Dataset: zara1               Batch: 5/8	Loss 215.0946 (263.2634)
2022-10-30 00:09:02,879:INFO: Dataset: zara1               Batch: 6/8	Loss 161.3680 (245.3895)
2022-10-30 00:09:03,933:INFO: Dataset: zara1               Batch: 7/8	Loss 3509.2239 (721.0043)
2022-10-30 00:09:04,881:INFO: Dataset: zara1               Batch: 8/8	Loss 77.7297 (651.5983)
2022-10-30 00:09:06,262:INFO: Dataset: zara2               Batch:  1/18	Loss 918.5854 (918.5854)
2022-10-30 00:09:07,336:INFO: Dataset: zara2               Batch:  2/18	Loss 427.5224 (658.0825)
2022-10-30 00:09:08,401:INFO: Dataset: zara2               Batch:  3/18	Loss -96.4005 (403.0353)
2022-10-30 00:09:09,469:INFO: Dataset: zara2               Batch:  4/18	Loss 13576.4424 (3782.8591)
2022-10-30 00:09:10,542:INFO: Dataset: zara2               Batch:  5/18	Loss 2005.4900 (3425.8935)
2022-10-30 00:09:11,609:INFO: Dataset: zara2               Batch:  6/18	Loss 2995.4814 (3350.1886)
2022-10-30 00:09:12,681:INFO: Dataset: zara2               Batch:  7/18	Loss 330.4668 (2888.8247)
2022-10-30 00:09:13,753:INFO: Dataset: zara2               Batch:  8/18	Loss 190.6045 (2521.0629)
2022-10-30 00:09:14,814:INFO: Dataset: zara2               Batch:  9/18	Loss 1220.6881 (2386.9776)
2022-10-30 00:09:15,881:INFO: Dataset: zara2               Batch: 10/18	Loss 268.8370 (2199.1992)
2022-10-30 00:09:16,948:INFO: Dataset: zara2               Batch: 11/18	Loss 9896.9844 (2967.1346)
2022-10-30 00:09:18,026:INFO: Dataset: zara2               Batch: 12/18	Loss 981.4247 (2799.3213)
2022-10-30 00:09:19,108:INFO: Dataset: zara2               Batch: 13/18	Loss 4824.7905 (2958.4134)
2022-10-30 00:09:20,191:INFO: Dataset: zara2               Batch: 14/18	Loss 367.4702 (2775.7395)
2022-10-30 00:09:21,266:INFO: Dataset: zara2               Batch: 15/18	Loss 2877.2229 (2782.1268)
2022-10-30 00:09:22,340:INFO: Dataset: zara2               Batch: 16/18	Loss 613.5126 (2649.9719)
2022-10-30 00:09:23,406:INFO: Dataset: zara2               Batch: 17/18	Loss 6580.3286 (2860.9291)
2022-10-30 00:09:24,379:INFO: Dataset: zara2               Batch: 18/18	Loss 4276.3408 (2929.2544)
2022-10-30 00:09:24,447:INFO: - Computing ADE (validation o)
2022-10-30 00:09:24,801:INFO: 		 ADE on eth                       dataset:	 4.728626728057861
2022-10-30 00:09:24,801:INFO: Average validation o:	ADE  4.7286	FDE  8.4732
2022-10-30 00:09:24,801:INFO: - Computing ADE (validation)
2022-10-30 00:09:25,166:INFO: 		 ADE on hotel                     dataset:	 2.3100063800811768
2022-10-30 00:09:25,625:INFO: 		 ADE on univ                      dataset:	 2.336045503616333
2022-10-30 00:09:25,984:INFO: 		 ADE on zara1                     dataset:	 2.1532371044158936
2022-10-30 00:09:26,535:INFO: 		 ADE on zara2                     dataset:	 2.5516421794891357
2022-10-30 00:09:26,536:INFO: Average validation:	ADE  2.4031	FDE  4.5643
2022-10-30 00:09:26,536:INFO: - Computing ADE (training)
2022-10-30 00:09:27,012:INFO: 		 ADE on hotel                     dataset:	 2.3938961029052734
2022-10-30 00:09:28,075:INFO: 		 ADE on univ                      dataset:	 2.2673120498657227
2022-10-30 00:09:28,770:INFO: 		 ADE on zara1                     dataset:	 3.1789515018463135
2022-10-30 00:09:29,990:INFO: 		 ADE on zara2                     dataset:	 2.9255905151367188
2022-10-30 00:09:29,991:INFO: Average training:	ADE  2.4622	FDE  4.6882
2022-10-30 00:09:30,002:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_415.pth.tar
2022-10-30 00:09:30,002:INFO: 
===> EPOCH: 416 (P4)
2022-10-30 00:09:30,002:INFO: - Computing loss (training)
2022-10-30 00:09:31,290:INFO: Dataset: hotel               Batch: 1/4	Loss 89.9276 (89.9276)
2022-10-30 00:09:32,360:INFO: Dataset: hotel               Batch: 2/4	Loss 87.6634 (88.7463)
2022-10-30 00:09:33,421:INFO: Dataset: hotel               Batch: 3/4	Loss 90.2038 (89.2383)
2022-10-30 00:09:34,205:INFO: Dataset: hotel               Batch: 4/4	Loss 52.0061 (82.7055)
2022-10-30 00:09:35,649:INFO: Dataset: univ                Batch:  1/15	Loss 3941.1733 (3941.1733)
2022-10-30 00:09:36,763:INFO: Dataset: univ                Batch:  2/15	Loss 655.1025 (2384.6135)
2022-10-30 00:09:37,877:INFO: Dataset: univ                Batch:  3/15	Loss 520.9279 (1784.2368)
2022-10-30 00:09:38,987:INFO: Dataset: univ                Batch:  4/15	Loss 1046.7058 (1611.4786)
2022-10-30 00:09:40,088:INFO: Dataset: univ                Batch:  5/15	Loss 658.2310 (1434.7374)
2022-10-30 00:09:41,195:INFO: Dataset: univ                Batch:  6/15	Loss -171.3681 (1175.8941)
2022-10-30 00:09:42,293:INFO: Dataset: univ                Batch:  7/15	Loss 3492.5608 (1471.2825)
2022-10-30 00:09:43,403:INFO: Dataset: univ                Batch:  8/15	Loss 262.4194 (1308.8167)
2022-10-30 00:09:44,518:INFO: Dataset: univ                Batch:  9/15	Loss 1734.6384 (1357.4901)
2022-10-30 00:09:45,642:INFO: Dataset: univ                Batch: 10/15	Loss 76.2338 (1228.8109)
2022-10-30 00:09:46,757:INFO: Dataset: univ                Batch: 11/15	Loss 679.9465 (1180.3201)
2022-10-30 00:09:47,863:INFO: Dataset: univ                Batch: 12/15	Loss 1264.6411 (1186.7006)
2022-10-30 00:09:48,970:INFO: Dataset: univ                Batch: 13/15	Loss 974.9060 (1170.1849)
2022-10-30 00:09:50,078:INFO: Dataset: univ                Batch: 14/15	Loss 2584.5068 (1268.7131)
2022-10-30 00:09:50,557:INFO: Dataset: univ                Batch: 15/15	Loss 477.3881 (1257.9131)
2022-10-30 00:09:51,928:INFO: Dataset: zara1               Batch: 1/8	Loss 163.8882 (163.8882)
2022-10-30 00:09:52,997:INFO: Dataset: zara1               Batch: 2/8	Loss 271.6647 (214.6339)
2022-10-30 00:09:54,059:INFO: Dataset: zara1               Batch: 3/8	Loss 210.8719 (213.3426)
2022-10-30 00:09:55,116:INFO: Dataset: zara1               Batch: 4/8	Loss 935.2908 (386.8280)
2022-10-30 00:09:56,175:INFO: Dataset: zara1               Batch: 5/8	Loss 8667.8789 (1991.2366)
2022-10-30 00:09:57,318:INFO: Dataset: zara1               Batch: 6/8	Loss 853.3324 (1786.1869)
2022-10-30 00:09:58,373:INFO: Dataset: zara1               Batch: 7/8	Loss 468.7883 (1565.1888)
2022-10-30 00:09:59,332:INFO: Dataset: zara1               Batch: 8/8	Loss 5019.1021 (1952.3907)
2022-10-30 00:10:00,718:INFO: Dataset: zara2               Batch:  1/18	Loss 530.3580 (530.3580)
2022-10-30 00:10:01,787:INFO: Dataset: zara2               Batch:  2/18	Loss 958.9322 (749.8211)
2022-10-30 00:10:02,860:INFO: Dataset: zara2               Batch:  3/18	Loss 380.7929 (624.8467)
2022-10-30 00:10:03,932:INFO: Dataset: zara2               Batch:  4/18	Loss 510.5871 (595.1338)
2022-10-30 00:10:05,000:INFO: Dataset: zara2               Batch:  5/18	Loss 495.4848 (574.4202)
2022-10-30 00:10:06,066:INFO: Dataset: zara2               Batch:  6/18	Loss -174.7066 (450.6718)
2022-10-30 00:10:07,137:INFO: Dataset: zara2               Batch:  7/18	Loss 775.6487 (500.0718)
2022-10-30 00:10:08,202:INFO: Dataset: zara2               Batch:  8/18	Loss 1493.6794 (638.0728)
2022-10-30 00:10:09,270:INFO: Dataset: zara2               Batch:  9/18	Loss 152.5824 (582.8923)
2022-10-30 00:10:10,332:INFO: Dataset: zara2               Batch: 10/18	Loss 918.3785 (616.4002)
2022-10-30 00:10:11,387:INFO: Dataset: zara2               Batch: 11/18	Loss 1255.0796 (672.7906)
2022-10-30 00:10:12,451:INFO: Dataset: zara2               Batch: 12/18	Loss 309.5417 (639.3779)
2022-10-30 00:10:13,509:INFO: Dataset: zara2               Batch: 13/18	Loss 2334.6663 (778.5989)
2022-10-30 00:10:14,568:INFO: Dataset: zara2               Batch: 14/18	Loss 2002.8932 (870.0616)
2022-10-30 00:10:15,635:INFO: Dataset: zara2               Batch: 15/18	Loss 4888.3872 (1164.1824)
2022-10-30 00:10:16,699:INFO: Dataset: zara2               Batch: 16/18	Loss 601.8397 (1128.2549)
2022-10-30 00:10:17,774:INFO: Dataset: zara2               Batch: 17/18	Loss 1130.2119 (1128.3680)
2022-10-30 00:10:18,747:INFO: Dataset: zara2               Batch: 18/18	Loss 1171.9979 (1130.6616)
2022-10-30 00:10:18,814:INFO: - Computing ADE (validation o)
2022-10-30 00:10:19,168:INFO: 		 ADE on eth                       dataset:	 4.8006696701049805
2022-10-30 00:10:19,168:INFO: Average validation o:	ADE  4.8007	FDE  8.6683
2022-10-30 00:10:19,169:INFO: - Computing ADE (validation)
2022-10-30 00:10:19,502:INFO: 		 ADE on hotel                     dataset:	 2.423971176147461
2022-10-30 00:10:19,942:INFO: 		 ADE on univ                      dataset:	 2.432283878326416
2022-10-30 00:10:20,300:INFO: 		 ADE on zara1                     dataset:	 2.0862064361572266
2022-10-30 00:10:20,854:INFO: 		 ADE on zara2                     dataset:	 2.6407458782196045
2022-10-30 00:10:20,854:INFO: Average validation:	ADE  2.4882	FDE  4.7667
2022-10-30 00:10:20,855:INFO: - Computing ADE (training)
2022-10-30 00:10:21,328:INFO: 		 ADE on hotel                     dataset:	 2.5022313594818115
2022-10-30 00:10:22,415:INFO: 		 ADE on univ                      dataset:	 2.362699508666992
2022-10-30 00:10:23,121:INFO: 		 ADE on zara1                     dataset:	 3.2460134029388428
2022-10-30 00:10:24,323:INFO: 		 ADE on zara2                     dataset:	 3.044569730758667
2022-10-30 00:10:24,324:INFO: Average training:	ADE  2.5609	FDE  4.9138
2022-10-30 00:10:24,335:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_416.pth.tar
2022-10-30 00:10:24,335:INFO: 
===> EPOCH: 417 (P4)
2022-10-30 00:10:24,335:INFO: - Computing loss (training)
2022-10-30 00:10:25,640:INFO: Dataset: hotel               Batch: 1/4	Loss 92.6383 (92.6383)
2022-10-30 00:10:26,722:INFO: Dataset: hotel               Batch: 2/4	Loss 108.9983 (101.4750)
2022-10-30 00:10:27,789:INFO: Dataset: hotel               Batch: 3/4	Loss 85.9384 (96.2046)
2022-10-30 00:10:28,570:INFO: Dataset: hotel               Batch: 4/4	Loss 53.2679 (88.5009)
2022-10-30 00:10:30,021:INFO: Dataset: univ                Batch:  1/15	Loss 1215.8635 (1215.8635)
2022-10-30 00:10:31,142:INFO: Dataset: univ                Batch:  2/15	Loss 638.7303 (939.9108)
2022-10-30 00:10:32,261:INFO: Dataset: univ                Batch:  3/15	Loss 813.6295 (901.4799)
2022-10-30 00:10:33,382:INFO: Dataset: univ                Batch:  4/15	Loss 653.7010 (834.8631)
2022-10-30 00:10:34,510:INFO: Dataset: univ                Batch:  5/15	Loss 420.2662 (748.9197)
2022-10-30 00:10:35,654:INFO: Dataset: univ                Batch:  6/15	Loss 289.9960 (675.1827)
2022-10-30 00:10:36,800:INFO: Dataset: univ                Batch:  7/15	Loss 813.0039 (695.8579)
2022-10-30 00:10:37,927:INFO: Dataset: univ                Batch:  8/15	Loss 415.6338 (658.2428)
2022-10-30 00:10:39,054:INFO: Dataset: univ                Batch:  9/15	Loss 606.0953 (651.9731)
2022-10-30 00:10:40,173:INFO: Dataset: univ                Batch: 10/15	Loss 579.6666 (645.5516)
2022-10-30 00:10:41,288:INFO: Dataset: univ                Batch: 11/15	Loss 1500.0070 (716.1643)
2022-10-30 00:10:42,414:INFO: Dataset: univ                Batch: 12/15	Loss 1922.4426 (810.2667)
2022-10-30 00:10:43,547:INFO: Dataset: univ                Batch: 13/15	Loss 295.7854 (764.2767)
2022-10-30 00:10:44,674:INFO: Dataset: univ                Batch: 14/15	Loss 2118.7361 (863.5038)
2022-10-30 00:10:45,152:INFO: Dataset: univ                Batch: 15/15	Loss 269.0247 (856.9962)
2022-10-30 00:10:46,518:INFO: Dataset: zara1               Batch: 1/8	Loss 104.4629 (104.4629)
2022-10-30 00:10:47,577:INFO: Dataset: zara1               Batch: 2/8	Loss 173.9236 (137.2043)
2022-10-30 00:10:48,620:INFO: Dataset: zara1               Batch: 3/8	Loss 117.3874 (130.2400)
2022-10-30 00:10:49,676:INFO: Dataset: zara1               Batch: 4/8	Loss 302.2228 (173.4657)
2022-10-30 00:10:50,735:INFO: Dataset: zara1               Batch: 5/8	Loss 559.2607 (258.9124)
2022-10-30 00:10:51,799:INFO: Dataset: zara1               Batch: 6/8	Loss 138.8221 (238.8418)
2022-10-30 00:10:52,868:INFO: Dataset: zara1               Batch: 7/8	Loss 441.7221 (269.5306)
2022-10-30 00:10:53,829:INFO: Dataset: zara1               Batch: 8/8	Loss 110.7065 (252.7287)
2022-10-30 00:10:55,216:INFO: Dataset: zara2               Batch:  1/18	Loss 818.4844 (818.4844)
2022-10-30 00:10:56,296:INFO: Dataset: zara2               Batch:  2/18	Loss 345.9847 (582.9076)
2022-10-30 00:10:57,352:INFO: Dataset: zara2               Batch:  3/18	Loss 358.2785 (507.1828)
2022-10-30 00:10:58,401:INFO: Dataset: zara2               Batch:  4/18	Loss 3633.5007 (1334.3544)
2022-10-30 00:10:59,454:INFO: Dataset: zara2               Batch:  5/18	Loss 406.2922 (1167.7791)
2022-10-30 00:11:00,508:INFO: Dataset: zara2               Batch:  6/18	Loss 424.9517 (1046.0390)
2022-10-30 00:11:01,557:INFO: Dataset: zara2               Batch:  7/18	Loss 578.9658 (985.0911)
2022-10-30 00:11:02,612:INFO: Dataset: zara2               Batch:  8/18	Loss 1044.7782 (992.7245)
2022-10-30 00:11:03,676:INFO: Dataset: zara2               Batch:  9/18	Loss 271.0055 (914.1859)
2022-10-30 00:11:04,757:INFO: Dataset: zara2               Batch: 10/18	Loss 753.4675 (898.8771)
2022-10-30 00:11:05,829:INFO: Dataset: zara2               Batch: 11/18	Loss 2767.5925 (1060.0333)
2022-10-30 00:11:06,897:INFO: Dataset: zara2               Batch: 12/18	Loss 375.3313 (1003.5182)
2022-10-30 00:11:07,970:INFO: Dataset: zara2               Batch: 13/18	Loss 473.0122 (963.1797)
2022-10-30 00:11:09,037:INFO: Dataset: zara2               Batch: 14/18	Loss 290.2463 (916.0207)
2022-10-30 00:11:10,105:INFO: Dataset: zara2               Batch: 15/18	Loss 4684.1943 (1160.0351)
2022-10-30 00:11:11,174:INFO: Dataset: zara2               Batch: 16/18	Loss 282.3468 (1107.7204)
2022-10-30 00:11:12,240:INFO: Dataset: zara2               Batch: 17/18	Loss 763.1849 (1086.0342)
2022-10-30 00:11:13,202:INFO: Dataset: zara2               Batch: 18/18	Loss 3137.4856 (1176.2452)
2022-10-30 00:11:13,267:INFO: - Computing ADE (validation o)
2022-10-30 00:11:13,630:INFO: 		 ADE on eth                       dataset:	 4.928125858306885
2022-10-30 00:11:13,630:INFO: Average validation o:	ADE  4.9281	FDE  8.9347
2022-10-30 00:11:13,631:INFO: - Computing ADE (validation)
2022-10-30 00:11:13,976:INFO: 		 ADE on hotel                     dataset:	 2.4749627113342285
2022-10-30 00:11:14,420:INFO: 		 ADE on univ                      dataset:	 2.519796848297119
2022-10-30 00:11:14,770:INFO: 		 ADE on zara1                     dataset:	 2.0935916900634766
2022-10-30 00:11:15,319:INFO: 		 ADE on zara2                     dataset:	 2.710423707962036
2022-10-30 00:11:15,319:INFO: Average validation:	ADE  2.5625	FDE  4.9410
2022-10-30 00:11:15,320:INFO: - Computing ADE (training)
2022-10-30 00:11:15,781:INFO: 		 ADE on hotel                     dataset:	 2.5995163917541504
2022-10-30 00:11:16,835:INFO: 		 ADE on univ                      dataset:	 2.437741279602051
2022-10-30 00:11:17,534:INFO: 		 ADE on zara1                     dataset:	 3.322545051574707
2022-10-30 00:11:18,747:INFO: 		 ADE on zara2                     dataset:	 3.1114437580108643
2022-10-30 00:11:18,748:INFO: Average training:	ADE  2.6350	FDE  5.0898
2022-10-30 00:11:18,759:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_417.pth.tar
2022-10-30 00:11:18,759:INFO: 
===> EPOCH: 418 (P4)
2022-10-30 00:11:18,759:INFO: - Computing loss (training)
2022-10-30 00:11:20,049:INFO: Dataset: hotel               Batch: 1/4	Loss 92.0278 (92.0278)
2022-10-30 00:11:21,110:INFO: Dataset: hotel               Batch: 2/4	Loss 87.9447 (89.9863)
2022-10-30 00:11:22,167:INFO: Dataset: hotel               Batch: 3/4	Loss 89.6086 (89.8600)
2022-10-30 00:11:22,937:INFO: Dataset: hotel               Batch: 4/4	Loss 55.4724 (83.6902)
2022-10-30 00:11:24,381:INFO: Dataset: univ                Batch:  1/15	Loss 181.5636 (181.5636)
2022-10-30 00:11:25,505:INFO: Dataset: univ                Batch:  2/15	Loss 1926.5487 (1016.7896)
2022-10-30 00:11:26,629:INFO: Dataset: univ                Batch:  3/15	Loss 382.3512 (813.3196)
2022-10-30 00:11:27,755:INFO: Dataset: univ                Batch:  4/15	Loss 1975.2992 (1103.9593)
2022-10-30 00:11:28,873:INFO: Dataset: univ                Batch:  5/15	Loss 14404.3682 (3289.6198)
2022-10-30 00:11:30,002:INFO: Dataset: univ                Batch:  6/15	Loss 11269.5898 (4679.0713)
2022-10-30 00:11:31,121:INFO: Dataset: univ                Batch:  7/15	Loss 836.1743 (4106.6292)
2022-10-30 00:11:32,242:INFO: Dataset: univ                Batch:  8/15	Loss 180.7282 (3596.4820)
2022-10-30 00:11:33,361:INFO: Dataset: univ                Batch:  9/15	Loss 304.7426 (3227.5908)
2022-10-30 00:11:34,472:INFO: Dataset: univ                Batch: 10/15	Loss 672.9825 (3001.7197)
2022-10-30 00:11:35,593:INFO: Dataset: univ                Batch: 11/15	Loss 386.3344 (2769.2555)
2022-10-30 00:11:36,729:INFO: Dataset: univ                Batch: 12/15	Loss 565.3842 (2564.3082)
2022-10-30 00:11:37,861:INFO: Dataset: univ                Batch: 13/15	Loss 488.3745 (2393.7206)
2022-10-30 00:11:38,998:INFO: Dataset: univ                Batch: 14/15	Loss 684.6814 (2261.2134)
2022-10-30 00:11:39,480:INFO: Dataset: univ                Batch: 15/15	Loss 366.4655 (2234.4560)
2022-10-30 00:11:40,837:INFO: Dataset: zara1               Batch: 1/8	Loss 951.5180 (951.5180)
2022-10-30 00:11:41,885:INFO: Dataset: zara1               Batch: 2/8	Loss 234.2177 (583.2546)
2022-10-30 00:11:42,935:INFO: Dataset: zara1               Batch: 3/8	Loss 115.3309 (430.5295)
2022-10-30 00:11:43,972:INFO: Dataset: zara1               Batch: 4/8	Loss 176.9424 (367.3310)
2022-10-30 00:11:45,018:INFO: Dataset: zara1               Batch: 5/8	Loss 193.2516 (332.8354)
2022-10-30 00:11:46,148:INFO: Dataset: zara1               Batch: 6/8	Loss 313.0158 (329.4770)
2022-10-30 00:11:47,191:INFO: Dataset: zara1               Batch: 7/8	Loss 247.3878 (317.0862)
2022-10-30 00:11:48,143:INFO: Dataset: zara1               Batch: 8/8	Loss 129.7455 (296.9717)
2022-10-30 00:11:49,510:INFO: Dataset: zara2               Batch:  1/18	Loss 163.9733 (163.9733)
2022-10-30 00:11:50,572:INFO: Dataset: zara2               Batch:  2/18	Loss 670.3965 (406.2739)
2022-10-30 00:11:51,621:INFO: Dataset: zara2               Batch:  3/18	Loss 295.8167 (371.4400)
2022-10-30 00:11:52,676:INFO: Dataset: zara2               Batch:  4/18	Loss 571.9639 (420.2202)
2022-10-30 00:11:53,726:INFO: Dataset: zara2               Batch:  5/18	Loss 924.2293 (522.8143)
2022-10-30 00:11:54,783:INFO: Dataset: zara2               Batch:  6/18	Loss 5982.7900 (1411.3217)
2022-10-30 00:11:55,839:INFO: Dataset: zara2               Batch:  7/18	Loss 149.2422 (1239.4998)
2022-10-30 00:11:56,889:INFO: Dataset: zara2               Batch:  8/18	Loss 754.2626 (1172.2190)
2022-10-30 00:11:57,940:INFO: Dataset: zara2               Batch:  9/18	Loss 1020.9066 (1152.8639)
2022-10-30 00:11:58,991:INFO: Dataset: zara2               Batch: 10/18	Loss 160.5917 (1046.6123)
2022-10-30 00:12:00,047:INFO: Dataset: zara2               Batch: 11/18	Loss 368.3241 (983.1542)
2022-10-30 00:12:01,099:INFO: Dataset: zara2               Batch: 12/18	Loss 1278.2633 (1007.3327)
2022-10-30 00:12:02,150:INFO: Dataset: zara2               Batch: 13/18	Loss 268.2032 (945.6268)
2022-10-30 00:12:03,198:INFO: Dataset: zara2               Batch: 14/18	Loss 1300.2405 (972.1949)
2022-10-30 00:12:04,249:INFO: Dataset: zara2               Batch: 15/18	Loss 204.5991 (925.0234)
2022-10-30 00:12:05,354:INFO: Dataset: zara2               Batch: 16/18	Loss 940.0933 (925.9614)
2022-10-30 00:12:06,427:INFO: Dataset: zara2               Batch: 17/18	Loss 676.9901 (911.5760)
2022-10-30 00:12:07,379:INFO: Dataset: zara2               Batch: 18/18	Loss 1655.5830 (948.8440)
2022-10-30 00:12:07,444:INFO: - Computing ADE (validation o)
2022-10-30 00:12:07,815:INFO: 		 ADE on eth                       dataset:	 4.937688827514648
2022-10-30 00:12:07,815:INFO: Average validation o:	ADE  4.9377	FDE  8.9665
2022-10-30 00:12:07,816:INFO: - Computing ADE (validation)
2022-10-30 00:12:08,160:INFO: 		 ADE on hotel                     dataset:	 2.4678242206573486
2022-10-30 00:12:08,599:INFO: 		 ADE on univ                      dataset:	 2.5019397735595703
2022-10-30 00:12:08,965:INFO: 		 ADE on zara1                     dataset:	 2.0843124389648438
2022-10-30 00:12:09,537:INFO: 		 ADE on zara2                     dataset:	 2.7122440338134766
2022-10-30 00:12:09,537:INFO: Average validation:	ADE  2.5529	FDE  4.9307
2022-10-30 00:12:09,538:INFO: - Computing ADE (training)
2022-10-30 00:12:10,010:INFO: 		 ADE on hotel                     dataset:	 2.5916690826416016
2022-10-30 00:12:11,061:INFO: 		 ADE on univ                      dataset:	 2.4276888370513916
2022-10-30 00:12:11,745:INFO: 		 ADE on zara1                     dataset:	 3.309473991394043
2022-10-30 00:12:12,965:INFO: 		 ADE on zara2                     dataset:	 3.1115903854370117
2022-10-30 00:12:12,965:INFO: Average training:	ADE  2.6268	FDE  5.0805
2022-10-30 00:12:12,976:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_418.pth.tar
2022-10-30 00:12:12,976:INFO: 
===> EPOCH: 419 (P4)
2022-10-30 00:12:12,976:INFO: - Computing loss (training)
2022-10-30 00:12:14,266:INFO: Dataset: hotel               Batch: 1/4	Loss 93.2066 (93.2066)
2022-10-30 00:12:15,324:INFO: Dataset: hotel               Batch: 2/4	Loss 89.2232 (91.2439)
2022-10-30 00:12:16,384:INFO: Dataset: hotel               Batch: 3/4	Loss 87.5627 (90.0050)
2022-10-30 00:12:17,155:INFO: Dataset: hotel               Batch: 4/4	Loss 56.5779 (83.9634)
2022-10-30 00:12:18,591:INFO: Dataset: univ                Batch:  1/15	Loss 31492.6543 (31492.6543)
2022-10-30 00:12:19,693:INFO: Dataset: univ                Batch:  2/15	Loss 987.6952 (16327.9811)
2022-10-30 00:12:20,815:INFO: Dataset: univ                Batch:  3/15	Loss 304.9217 (10810.2713)
2022-10-30 00:12:21,922:INFO: Dataset: univ                Batch:  4/15	Loss 608.8273 (8395.1861)
2022-10-30 00:12:23,031:INFO: Dataset: univ                Batch:  5/15	Loss 1785.9836 (7012.5856)
2022-10-30 00:12:24,161:INFO: Dataset: univ                Batch:  6/15	Loss 1299.0027 (6052.0412)
2022-10-30 00:12:25,287:INFO: Dataset: univ                Batch:  7/15	Loss 828.1345 (5235.4373)
2022-10-30 00:12:26,408:INFO: Dataset: univ                Batch:  8/15	Loss 300.5403 (4622.7930)
2022-10-30 00:12:27,531:INFO: Dataset: univ                Batch:  9/15	Loss -304.9184 (4102.8252)
2022-10-30 00:12:28,650:INFO: Dataset: univ                Batch: 10/15	Loss 198.8657 (3733.7359)
2022-10-30 00:12:29,774:INFO: Dataset: univ                Batch: 11/15	Loss 11104.5459 (4372.4496)
2022-10-30 00:12:30,893:INFO: Dataset: univ                Batch: 12/15	Loss 2099.0332 (4189.7507)
2022-10-30 00:12:32,011:INFO: Dataset: univ                Batch: 13/15	Loss 179.2715 (3874.5617)
2022-10-30 00:12:33,132:INFO: Dataset: univ                Batch: 14/15	Loss 847.4800 (3664.9195)
2022-10-30 00:12:33,611:INFO: Dataset: univ                Batch: 15/15	Loss 213.2092 (3616.6656)
2022-10-30 00:12:34,960:INFO: Dataset: zara1               Batch: 1/8	Loss 292.9302 (292.9302)
2022-10-30 00:12:36,003:INFO: Dataset: zara1               Batch: 2/8	Loss 139.2621 (217.3985)
2022-10-30 00:12:37,047:INFO: Dataset: zara1               Batch: 3/8	Loss 582.1318 (345.0049)
2022-10-30 00:12:38,090:INFO: Dataset: zara1               Batch: 4/8	Loss 1275.6255 (571.1098)
2022-10-30 00:12:39,134:INFO: Dataset: zara1               Batch: 5/8	Loss 242.5738 (509.3528)
2022-10-30 00:12:40,183:INFO: Dataset: zara1               Batch: 6/8	Loss 194.0432 (454.0856)
2022-10-30 00:12:41,236:INFO: Dataset: zara1               Batch: 7/8	Loss 213.4268 (418.3158)
2022-10-30 00:12:42,187:INFO: Dataset: zara1               Batch: 8/8	Loss 714.0184 (452.2438)
2022-10-30 00:12:43,545:INFO: Dataset: zara2               Batch:  1/18	Loss 159.4896 (159.4896)
2022-10-30 00:12:44,600:INFO: Dataset: zara2               Batch:  2/18	Loss 476.3104 (337.9342)
2022-10-30 00:12:45,659:INFO: Dataset: zara2               Batch:  3/18	Loss 1278.7971 (636.4409)
2022-10-30 00:12:46,719:INFO: Dataset: zara2               Batch:  4/18	Loss 1163.8398 (772.1252)
2022-10-30 00:12:47,779:INFO: Dataset: zara2               Batch:  5/18	Loss 2351.6719 (1096.0868)
2022-10-30 00:12:48,842:INFO: Dataset: zara2               Batch:  6/18	Loss 8076.0039 (2309.5396)
2022-10-30 00:12:49,904:INFO: Dataset: zara2               Batch:  7/18	Loss 111.4904 (2002.1460)
2022-10-30 00:12:50,956:INFO: Dataset: zara2               Batch:  8/18	Loss 498.4120 (1804.1415)
2022-10-30 00:12:52,012:INFO: Dataset: zara2               Batch:  9/18	Loss 191.9031 (1643.2893)
2022-10-30 00:12:53,069:INFO: Dataset: zara2               Batch: 10/18	Loss 149.7256 (1499.3099)
2022-10-30 00:12:54,126:INFO: Dataset: zara2               Batch: 11/18	Loss 321.2829 (1385.6240)
2022-10-30 00:12:55,189:INFO: Dataset: zara2               Batch: 12/18	Loss 269.3199 (1291.6368)
2022-10-30 00:12:56,239:INFO: Dataset: zara2               Batch: 13/18	Loss 5865.5093 (1645.8704)
2022-10-30 00:12:57,291:INFO: Dataset: zara2               Batch: 14/18	Loss 346.8461 (1553.7682)
2022-10-30 00:12:58,345:INFO: Dataset: zara2               Batch: 15/18	Loss 526.3185 (1486.6901)
2022-10-30 00:12:59,396:INFO: Dataset: zara2               Batch: 16/18	Loss 321.7487 (1417.3252)
2022-10-30 00:13:00,450:INFO: Dataset: zara2               Batch: 17/18	Loss 3408.7356 (1546.1934)
2022-10-30 00:13:01,397:INFO: Dataset: zara2               Batch: 18/18	Loss 19011.3320 (2369.0673)
2022-10-30 00:13:01,465:INFO: - Computing ADE (validation o)
2022-10-30 00:13:01,822:INFO: 		 ADE on eth                       dataset:	 4.513742446899414
2022-10-30 00:13:01,822:INFO: Average validation o:	ADE  4.5137	FDE  7.9996
2022-10-30 00:13:01,823:INFO: - Computing ADE (validation)
2022-10-30 00:13:02,179:INFO: 		 ADE on hotel                     dataset:	 2.1510565280914307
2022-10-30 00:13:02,634:INFO: 		 ADE on univ                      dataset:	 2.210503578186035
2022-10-30 00:13:02,996:INFO: 		 ADE on zara1                     dataset:	 2.10003662109375
2022-10-30 00:13:03,537:INFO: 		 ADE on zara2                     dataset:	 2.4055466651916504
2022-10-30 00:13:03,537:INFO: Average validation:	ADE  2.2724	FDE  4.2521
2022-10-30 00:13:03,538:INFO: - Computing ADE (training)
2022-10-30 00:13:04,001:INFO: 		 ADE on hotel                     dataset:	 2.271414279937744
2022-10-30 00:13:05,107:INFO: 		 ADE on univ                      dataset:	 2.1286559104919434
2022-10-30 00:13:05,797:INFO: 		 ADE on zara1                     dataset:	 3.0982162952423096
2022-10-30 00:13:06,996:INFO: 		 ADE on zara2                     dataset:	 2.787954092025757
2022-10-30 00:13:06,996:INFO: Average training:	ADE  2.3279	FDE  4.3644
2022-10-30 00:13:07,007:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_419.pth.tar
2022-10-30 00:13:07,007:INFO: 
===> EPOCH: 420 (P4)
2022-10-30 00:13:07,007:INFO: - Computing loss (training)
2022-10-30 00:13:08,293:INFO: Dataset: hotel               Batch: 1/4	Loss 85.4865 (85.4865)
2022-10-30 00:13:09,342:INFO: Dataset: hotel               Batch: 2/4	Loss 85.9365 (85.7137)
2022-10-30 00:13:10,387:INFO: Dataset: hotel               Batch: 3/4	Loss 85.2666 (85.5553)
2022-10-30 00:13:11,152:INFO: Dataset: hotel               Batch: 4/4	Loss 79.3681 (84.5513)
2022-10-30 00:13:12,614:INFO: Dataset: univ                Batch:  1/15	Loss 227.1568 (227.1568)
2022-10-30 00:13:13,729:INFO: Dataset: univ                Batch:  2/15	Loss 461.6453 (351.1288)
2022-10-30 00:13:14,836:INFO: Dataset: univ                Batch:  3/15	Loss 450.0099 (384.5927)
2022-10-30 00:13:15,950:INFO: Dataset: univ                Batch:  4/15	Loss 477.1852 (410.2733)
2022-10-30 00:13:17,056:INFO: Dataset: univ                Batch:  5/15	Loss 170.6814 (361.2709)
2022-10-30 00:13:18,168:INFO: Dataset: univ                Batch:  6/15	Loss 272.4388 (347.1783)
2022-10-30 00:13:19,290:INFO: Dataset: univ                Batch:  7/15	Loss 1220.0341 (479.3588)
2022-10-30 00:13:20,399:INFO: Dataset: univ                Batch:  8/15	Loss 1109.8997 (552.6775)
2022-10-30 00:13:21,519:INFO: Dataset: univ                Batch:  9/15	Loss 639.4769 (562.7748)
2022-10-30 00:13:22,645:INFO: Dataset: univ                Batch: 10/15	Loss 523.1105 (558.7220)
2022-10-30 00:13:23,774:INFO: Dataset: univ                Batch: 11/15	Loss 309.4350 (533.9698)
2022-10-30 00:13:24,886:INFO: Dataset: univ                Batch: 12/15	Loss 1066.4918 (576.0466)
2022-10-30 00:13:25,998:INFO: Dataset: univ                Batch: 13/15	Loss 470.7946 (568.8463)
2022-10-30 00:13:27,104:INFO: Dataset: univ                Batch: 14/15	Loss 1148.6466 (606.7018)
2022-10-30 00:13:27,582:INFO: Dataset: univ                Batch: 15/15	Loss 66.3827 (598.5594)
2022-10-30 00:13:28,960:INFO: Dataset: zara1               Batch: 1/8	Loss 173.6630 (173.6630)
2022-10-30 00:13:30,030:INFO: Dataset: zara1               Batch: 2/8	Loss 157.9623 (166.1212)
2022-10-30 00:13:31,093:INFO: Dataset: zara1               Batch: 3/8	Loss 253.4725 (195.4074)
2022-10-30 00:13:32,161:INFO: Dataset: zara1               Batch: 4/8	Loss 289.6800 (222.6482)
2022-10-30 00:13:33,226:INFO: Dataset: zara1               Batch: 5/8	Loss 319.5536 (240.9092)
2022-10-30 00:13:34,290:INFO: Dataset: zara1               Batch: 6/8	Loss 87.0678 (213.5738)
2022-10-30 00:13:35,340:INFO: Dataset: zara1               Batch: 7/8	Loss 210.8631 (213.1743)
2022-10-30 00:13:36,301:INFO: Dataset: zara1               Batch: 8/8	Loss 131.7972 (204.7367)
2022-10-30 00:13:37,681:INFO: Dataset: zara2               Batch:  1/18	Loss 110.9437 (110.9437)
2022-10-30 00:13:38,840:INFO: Dataset: zara2               Batch:  2/18	Loss 399.9796 (247.0734)
2022-10-30 00:13:39,927:INFO: Dataset: zara2               Batch:  3/18	Loss 245.1250 (246.3669)
2022-10-30 00:13:41,012:INFO: Dataset: zara2               Batch:  4/18	Loss 498.7379 (312.6561)
2022-10-30 00:13:42,095:INFO: Dataset: zara2               Batch:  5/18	Loss 254.5630 (300.2610)
2022-10-30 00:13:43,175:INFO: Dataset: zara2               Batch:  6/18	Loss 679.7938 (366.6840)
2022-10-30 00:13:44,255:INFO: Dataset: zara2               Batch:  7/18	Loss 560.1273 (393.5605)
2022-10-30 00:13:45,330:INFO: Dataset: zara2               Batch:  8/18	Loss 310.6768 (382.2993)
2022-10-30 00:13:46,407:INFO: Dataset: zara2               Batch:  9/18	Loss 1239.1112 (487.2786)
2022-10-30 00:13:47,480:INFO: Dataset: zara2               Batch: 10/18	Loss 464.3250 (484.9553)
2022-10-30 00:13:48,563:INFO: Dataset: zara2               Batch: 11/18	Loss 643.5664 (499.3784)
2022-10-30 00:13:49,612:INFO: Dataset: zara2               Batch: 12/18	Loss 347.3874 (487.6460)
2022-10-30 00:13:50,662:INFO: Dataset: zara2               Batch: 13/18	Loss 307.0080 (473.1271)
2022-10-30 00:13:51,712:INFO: Dataset: zara2               Batch: 14/18	Loss 201.9562 (454.7211)
2022-10-30 00:13:52,764:INFO: Dataset: zara2               Batch: 15/18	Loss 163.0310 (434.3369)
2022-10-30 00:13:53,816:INFO: Dataset: zara2               Batch: 16/18	Loss 566.0076 (443.1881)
2022-10-30 00:13:54,869:INFO: Dataset: zara2               Batch: 17/18	Loss 514.1177 (447.5287)
2022-10-30 00:13:55,821:INFO: Dataset: zara2               Batch: 18/18	Loss 88.8210 (429.9165)
2022-10-30 00:13:55,889:INFO: - Computing ADE (validation o)
2022-10-30 00:13:56,260:INFO: 		 ADE on eth                       dataset:	 4.723294258117676
2022-10-30 00:13:56,261:INFO: Average validation o:	ADE  4.7233	FDE  8.4692
2022-10-30 00:13:56,261:INFO: - Computing ADE (validation)
2022-10-30 00:13:56,595:INFO: 		 ADE on hotel                     dataset:	 2.2465906143188477
2022-10-30 00:13:57,043:INFO: 		 ADE on univ                      dataset:	 2.3607397079467773
2022-10-30 00:13:57,400:INFO: 		 ADE on zara1                     dataset:	 2.1335642337799072
2022-10-30 00:13:57,956:INFO: 		 ADE on zara2                     dataset:	 2.5380375385284424
2022-10-30 00:13:57,956:INFO: Average validation:	ADE  2.4063	FDE  4.5750
2022-10-30 00:13:57,957:INFO: - Computing ADE (training)
2022-10-30 00:13:58,434:INFO: 		 ADE on hotel                     dataset:	 2.4121904373168945
2022-10-30 00:13:59,491:INFO: 		 ADE on univ                      dataset:	 2.2938220500946045
2022-10-30 00:14:00,201:INFO: 		 ADE on zara1                     dataset:	 3.2068819999694824
2022-10-30 00:14:01,405:INFO: 		 ADE on zara2                     dataset:	 2.954285144805908
2022-10-30 00:14:01,405:INFO: Average training:	ADE  2.4891	FDE  4.7344
2022-10-30 00:14:01,416:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_420.pth.tar
2022-10-30 00:14:01,416:INFO: 
===> EPOCH: 421 (P4)
2022-10-30 00:14:01,416:INFO: - Computing loss (training)
2022-10-30 00:14:02,685:INFO: Dataset: hotel               Batch: 1/4	Loss 90.8305 (90.8305)
2022-10-30 00:14:03,735:INFO: Dataset: hotel               Batch: 2/4	Loss 86.2123 (88.4827)
2022-10-30 00:14:04,777:INFO: Dataset: hotel               Batch: 3/4	Loss 90.4752 (89.1563)
2022-10-30 00:14:05,548:INFO: Dataset: hotel               Batch: 4/4	Loss 58.5727 (83.9918)
2022-10-30 00:14:06,997:INFO: Dataset: univ                Batch:  1/15	Loss 194.8242 (194.8242)
2022-10-30 00:14:08,108:INFO: Dataset: univ                Batch:  2/15	Loss 269.4589 (230.6478)
2022-10-30 00:14:09,233:INFO: Dataset: univ                Batch:  3/15	Loss 456.1880 (312.6138)
2022-10-30 00:14:10,350:INFO: Dataset: univ                Batch:  4/15	Loss 1352.3008 (576.9722)
2022-10-30 00:14:11,451:INFO: Dataset: univ                Batch:  5/15	Loss 327.9575 (529.3092)
2022-10-30 00:14:12,556:INFO: Dataset: univ                Batch:  6/15	Loss 212.6077 (480.6262)
2022-10-30 00:14:13,655:INFO: Dataset: univ                Batch:  7/15	Loss 485.4406 (481.2624)
2022-10-30 00:14:14,767:INFO: Dataset: univ                Batch:  8/15	Loss 269.2235 (453.1090)
2022-10-30 00:14:15,875:INFO: Dataset: univ                Batch:  9/15	Loss 318.9575 (438.6909)
2022-10-30 00:14:16,982:INFO: Dataset: univ                Batch: 10/15	Loss 275.5093 (422.8844)
2022-10-30 00:14:18,098:INFO: Dataset: univ                Batch: 11/15	Loss 279.2446 (409.9366)
2022-10-30 00:14:19,209:INFO: Dataset: univ                Batch: 12/15	Loss 328.4578 (403.8471)
2022-10-30 00:14:20,325:INFO: Dataset: univ                Batch: 13/15	Loss 216.7596 (389.3370)
2022-10-30 00:14:21,433:INFO: Dataset: univ                Batch: 14/15	Loss 584.5684 (403.5459)
2022-10-30 00:14:21,909:INFO: Dataset: univ                Batch: 15/15	Loss 53.3230 (398.1188)
2022-10-30 00:14:23,280:INFO: Dataset: zara1               Batch: 1/8	Loss 124.5651 (124.5651)
2022-10-30 00:14:24,348:INFO: Dataset: zara1               Batch: 2/8	Loss 153.5236 (138.5919)
2022-10-30 00:14:25,405:INFO: Dataset: zara1               Batch: 3/8	Loss 123.7575 (133.4594)
2022-10-30 00:14:26,456:INFO: Dataset: zara1               Batch: 4/8	Loss 135.7379 (134.0451)
2022-10-30 00:14:27,503:INFO: Dataset: zara1               Batch: 5/8	Loss 142.2587 (135.8246)
2022-10-30 00:14:28,547:INFO: Dataset: zara1               Batch: 6/8	Loss 101.0070 (129.5249)
2022-10-30 00:14:29,590:INFO: Dataset: zara1               Batch: 7/8	Loss 183.7907 (137.4293)
2022-10-30 00:14:30,539:INFO: Dataset: zara1               Batch: 8/8	Loss 158.9033 (139.8932)
2022-10-30 00:14:31,927:INFO: Dataset: zara2               Batch:  1/18	Loss 131.0555 (131.0555)
2022-10-30 00:14:32,992:INFO: Dataset: zara2               Batch:  2/18	Loss 319.3579 (221.6021)
2022-10-30 00:14:34,068:INFO: Dataset: zara2               Batch:  3/18	Loss 510.2926 (315.1426)
2022-10-30 00:14:35,115:INFO: Dataset: zara2               Batch:  4/18	Loss 554.0005 (379.0674)
2022-10-30 00:14:36,166:INFO: Dataset: zara2               Batch:  5/18	Loss 761.8490 (453.2372)
2022-10-30 00:14:37,214:INFO: Dataset: zara2               Batch:  6/18	Loss 222.4906 (409.3725)
2022-10-30 00:14:38,260:INFO: Dataset: zara2               Batch:  7/18	Loss 595.1599 (436.5418)
2022-10-30 00:14:39,305:INFO: Dataset: zara2               Batch:  8/18	Loss 185.4284 (403.3793)
2022-10-30 00:14:40,355:INFO: Dataset: zara2               Batch:  9/18	Loss 353.8703 (397.6606)
2022-10-30 00:14:41,414:INFO: Dataset: zara2               Batch: 10/18	Loss 651.9990 (423.5983)
2022-10-30 00:14:42,463:INFO: Dataset: zara2               Batch: 11/18	Loss 189.4826 (403.6882)
2022-10-30 00:14:43,513:INFO: Dataset: zara2               Batch: 12/18	Loss 383.0457 (402.1031)
2022-10-30 00:14:44,569:INFO: Dataset: zara2               Batch: 13/18	Loss 345.7313 (397.8684)
2022-10-30 00:14:45,624:INFO: Dataset: zara2               Batch: 14/18	Loss 241.1860 (386.0315)
2022-10-30 00:14:46,677:INFO: Dataset: zara2               Batch: 15/18	Loss 228.3177 (375.7614)
2022-10-30 00:14:47,732:INFO: Dataset: zara2               Batch: 16/18	Loss 607.1902 (389.9505)
2022-10-30 00:14:48,792:INFO: Dataset: zara2               Batch: 17/18	Loss 320.2058 (385.5011)
2022-10-30 00:14:49,745:INFO: Dataset: zara2               Batch: 18/18	Loss -298.8929 (355.5185)
2022-10-30 00:14:49,811:INFO: - Computing ADE (validation o)
2022-10-30 00:14:50,172:INFO: 		 ADE on eth                       dataset:	 4.739045143127441
2022-10-30 00:14:50,173:INFO: Average validation o:	ADE  4.7390	FDE  8.5404
2022-10-30 00:14:50,173:INFO: - Computing ADE (validation)
2022-10-30 00:14:50,526:INFO: 		 ADE on hotel                     dataset:	 2.330587387084961
2022-10-30 00:14:50,972:INFO: 		 ADE on univ                      dataset:	 2.3896005153656006
2022-10-30 00:14:51,315:INFO: 		 ADE on zara1                     dataset:	 2.105361223220825
2022-10-30 00:14:51,857:INFO: 		 ADE on zara2                     dataset:	 2.5797617435455322
2022-10-30 00:14:51,857:INFO: Average validation:	ADE  2.4396	FDE  4.6494
2022-10-30 00:14:51,858:INFO: - Computing ADE (training)
2022-10-30 00:14:52,324:INFO: 		 ADE on hotel                     dataset:	 2.440704107284546
2022-10-30 00:14:53,383:INFO: 		 ADE on univ                      dataset:	 2.31681752204895
2022-10-30 00:14:54,082:INFO: 		 ADE on zara1                     dataset:	 3.2395339012145996
2022-10-30 00:14:55,287:INFO: 		 ADE on zara2                     dataset:	 3.0015830993652344
2022-10-30 00:14:55,288:INFO: Average training:	ADE  2.5177	FDE  4.8051
2022-10-30 00:14:55,298:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_421.pth.tar
2022-10-30 00:14:55,299:INFO: 
===> EPOCH: 422 (P4)
2022-10-30 00:14:55,299:INFO: - Computing loss (training)
2022-10-30 00:14:56,609:INFO: Dataset: hotel               Batch: 1/4	Loss 86.3251 (86.3251)
2022-10-30 00:14:57,670:INFO: Dataset: hotel               Batch: 2/4	Loss 92.0208 (89.0705)
2022-10-30 00:14:58,726:INFO: Dataset: hotel               Batch: 3/4	Loss 124.1086 (100.5806)
2022-10-30 00:14:59,504:INFO: Dataset: hotel               Batch: 4/4	Loss 53.1461 (92.0074)
2022-10-30 00:15:00,922:INFO: Dataset: univ                Batch:  1/15	Loss 599.5842 (599.5842)
2022-10-30 00:15:02,042:INFO: Dataset: univ                Batch:  2/15	Loss 349.3890 (467.7683)
2022-10-30 00:15:03,160:INFO: Dataset: univ                Batch:  3/15	Loss 519.5823 (485.1099)
2022-10-30 00:15:04,265:INFO: Dataset: univ                Batch:  4/15	Loss 422.4579 (470.3816)
2022-10-30 00:15:05,386:INFO: Dataset: univ                Batch:  5/15	Loss 400.0331 (454.7024)
2022-10-30 00:15:06,501:INFO: Dataset: univ                Batch:  6/15	Loss 198.3436 (414.8819)
2022-10-30 00:15:07,620:INFO: Dataset: univ                Batch:  7/15	Loss 2927.9995 (782.2868)
2022-10-30 00:15:08,741:INFO: Dataset: univ                Batch:  8/15	Loss 20377.8848 (3449.1919)
2022-10-30 00:15:09,857:INFO: Dataset: univ                Batch:  9/15	Loss 386.8033 (3114.3074)
2022-10-30 00:15:10,975:INFO: Dataset: univ                Batch: 10/15	Loss 209.8578 (2791.7192)
2022-10-30 00:15:12,078:INFO: Dataset: univ                Batch: 11/15	Loss 553.5229 (2602.4629)
2022-10-30 00:15:13,182:INFO: Dataset: univ                Batch: 12/15	Loss 196.2778 (2405.2214)
2022-10-30 00:15:14,289:INFO: Dataset: univ                Batch: 13/15	Loss 393.1920 (2255.7594)
2022-10-30 00:15:15,400:INFO: Dataset: univ                Batch: 14/15	Loss 1096.9719 (2177.0842)
2022-10-30 00:15:15,876:INFO: Dataset: univ                Batch: 15/15	Loss 27.3860 (2146.0133)
2022-10-30 00:15:17,247:INFO: Dataset: zara1               Batch: 1/8	Loss 117.2093 (117.2093)
2022-10-30 00:15:18,316:INFO: Dataset: zara1               Batch: 2/8	Loss 3226.7942 (1541.2891)
2022-10-30 00:15:19,378:INFO: Dataset: zara1               Batch: 3/8	Loss 351.3812 (1124.1375)
2022-10-30 00:15:20,443:INFO: Dataset: zara1               Batch: 4/8	Loss 122.3900 (862.5346)
2022-10-30 00:15:21,502:INFO: Dataset: zara1               Batch: 5/8	Loss 131.2071 (710.6104)
2022-10-30 00:15:22,559:INFO: Dataset: zara1               Batch: 6/8	Loss 131.7325 (608.0549)
2022-10-30 00:15:23,624:INFO: Dataset: zara1               Batch: 7/8	Loss 394.9081 (577.3708)
2022-10-30 00:15:24,591:INFO: Dataset: zara1               Batch: 8/8	Loss 122.2129 (526.5848)
2022-10-30 00:15:25,949:INFO: Dataset: zara2               Batch:  1/18	Loss 1951.5720 (1951.5720)
2022-10-30 00:15:27,006:INFO: Dataset: zara2               Batch:  2/18	Loss 146.0244 (1032.8199)
2022-10-30 00:15:28,057:INFO: Dataset: zara2               Batch:  3/18	Loss 223.6683 (785.1880)
2022-10-30 00:15:29,193:INFO: Dataset: zara2               Batch:  4/18	Loss 645.6763 (747.5470)
2022-10-30 00:15:30,239:INFO: Dataset: zara2               Batch:  5/18	Loss 149.5931 (622.4432)
2022-10-30 00:15:31,312:INFO: Dataset: zara2               Batch:  6/18	Loss 285.2242 (570.5114)
2022-10-30 00:15:32,381:INFO: Dataset: zara2               Batch:  7/18	Loss 1211.9054 (657.3062)
2022-10-30 00:15:33,449:INFO: Dataset: zara2               Batch:  8/18	Loss 2317.9900 (859.6902)
2022-10-30 00:15:34,509:INFO: Dataset: zara2               Batch:  9/18	Loss 209.8626 (780.9824)
2022-10-30 00:15:35,571:INFO: Dataset: zara2               Batch: 10/18	Loss 216.2314 (722.2280)
2022-10-30 00:15:36,639:INFO: Dataset: zara2               Batch: 11/18	Loss 128.6279 (671.7330)
2022-10-30 00:15:37,710:INFO: Dataset: zara2               Batch: 12/18	Loss 283.7034 (635.1990)
2022-10-30 00:15:38,782:INFO: Dataset: zara2               Batch: 13/18	Loss 11.1686 (586.0572)
2022-10-30 00:15:39,846:INFO: Dataset: zara2               Batch: 14/18	Loss 180.1436 (560.1238)
2022-10-30 00:15:40,921:INFO: Dataset: zara2               Batch: 15/18	Loss 909.0220 (584.4655)
2022-10-30 00:15:41,991:INFO: Dataset: zara2               Batch: 16/18	Loss 202.3650 (558.6192)
2022-10-30 00:15:43,057:INFO: Dataset: zara2               Batch: 17/18	Loss 616.3685 (562.4838)
2022-10-30 00:15:44,022:INFO: Dataset: zara2               Batch: 18/18	Loss 2630.8916 (653.4404)
2022-10-30 00:15:44,090:INFO: - Computing ADE (validation o)
2022-10-30 00:15:44,468:INFO: 		 ADE on eth                       dataset:	 4.683687686920166
2022-10-30 00:15:44,468:INFO: Average validation o:	ADE  4.6837	FDE  8.4140
2022-10-30 00:15:44,469:INFO: - Computing ADE (validation)
2022-10-30 00:15:44,812:INFO: 		 ADE on hotel                     dataset:	 2.322303295135498
2022-10-30 00:15:45,264:INFO: 		 ADE on univ                      dataset:	 2.344364881515503
2022-10-30 00:15:45,628:INFO: 		 ADE on zara1                     dataset:	 2.1049442291259766
2022-10-30 00:15:46,188:INFO: 		 ADE on zara2                     dataset:	 2.5497353076934814
2022-10-30 00:15:46,188:INFO: Average validation:	ADE  2.4046	FDE  4.5717
2022-10-30 00:15:46,189:INFO: - Computing ADE (training)
2022-10-30 00:15:46,663:INFO: 		 ADE on hotel                     dataset:	 2.433903694152832
2022-10-30 00:15:47,749:INFO: 		 ADE on univ                      dataset:	 2.2864181995391846
2022-10-30 00:15:48,445:INFO: 		 ADE on zara1                     dataset:	 3.202185869216919
2022-10-30 00:15:49,679:INFO: 		 ADE on zara2                     dataset:	 2.962599754333496
2022-10-30 00:15:49,679:INFO: Average training:	ADE  2.4858	FDE  4.7259
2022-10-30 00:15:49,690:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_422.pth.tar
2022-10-30 00:15:49,691:INFO: 
===> EPOCH: 423 (P4)
2022-10-30 00:15:49,691:INFO: - Computing loss (training)
2022-10-30 00:15:50,967:INFO: Dataset: hotel               Batch: 1/4	Loss 194.9584 (194.9584)
2022-10-30 00:15:52,016:INFO: Dataset: hotel               Batch: 2/4	Loss 87.5536 (141.6499)
2022-10-30 00:15:53,058:INFO: Dataset: hotel               Batch: 3/4	Loss 87.0402 (122.6061)
2022-10-30 00:15:53,824:INFO: Dataset: hotel               Batch: 4/4	Loss 52.0477 (110.5051)
2022-10-30 00:15:55,281:INFO: Dataset: univ                Batch:  1/15	Loss 266.2775 (266.2775)
2022-10-30 00:15:56,406:INFO: Dataset: univ                Batch:  2/15	Loss 169.0371 (219.1909)
2022-10-30 00:15:57,540:INFO: Dataset: univ                Batch:  3/15	Loss 527.0239 (328.1663)
2022-10-30 00:15:58,670:INFO: Dataset: univ                Batch:  4/15	Loss 320.6165 (326.2379)
2022-10-30 00:15:59,791:INFO: Dataset: univ                Batch:  5/15	Loss 349.0525 (330.8069)
2022-10-30 00:16:00,918:INFO: Dataset: univ                Batch:  6/15	Loss 431.5191 (347.2552)
2022-10-30 00:16:02,057:INFO: Dataset: univ                Batch:  7/15	Loss 627.4103 (393.1681)
2022-10-30 00:16:03,186:INFO: Dataset: univ                Batch:  8/15	Loss 231.0649 (372.9929)
2022-10-30 00:16:04,304:INFO: Dataset: univ                Batch:  9/15	Loss 357.0489 (371.4765)
2022-10-30 00:16:05,428:INFO: Dataset: univ                Batch: 10/15	Loss 487.1443 (382.7339)
2022-10-30 00:16:06,571:INFO: Dataset: univ                Batch: 11/15	Loss 551.2740 (399.0334)
2022-10-30 00:16:07,706:INFO: Dataset: univ                Batch: 12/15	Loss 482.2638 (405.9747)
2022-10-30 00:16:08,836:INFO: Dataset: univ                Batch: 13/15	Loss 436.6820 (407.9250)
2022-10-30 00:16:09,973:INFO: Dataset: univ                Batch: 14/15	Loss 909.0163 (443.7791)
2022-10-30 00:16:10,458:INFO: Dataset: univ                Batch: 15/15	Loss 22.4762 (438.2289)
2022-10-30 00:16:11,800:INFO: Dataset: zara1               Batch: 1/8	Loss 261.3659 (261.3659)
2022-10-30 00:16:12,854:INFO: Dataset: zara1               Batch: 2/8	Loss 828.6165 (526.1638)
2022-10-30 00:16:13,906:INFO: Dataset: zara1               Batch: 3/8	Loss 102.7366 (376.2366)
2022-10-30 00:16:14,959:INFO: Dataset: zara1               Batch: 4/8	Loss 117.9071 (310.6545)
2022-10-30 00:16:16,014:INFO: Dataset: zara1               Batch: 5/8	Loss 230.1715 (295.1615)
2022-10-30 00:16:17,066:INFO: Dataset: zara1               Batch: 6/8	Loss 114.4899 (263.8053)
2022-10-30 00:16:18,122:INFO: Dataset: zara1               Batch: 7/8	Loss 183.6231 (251.7076)
2022-10-30 00:16:19,079:INFO: Dataset: zara1               Batch: 8/8	Loss 128.0970 (239.3465)
2022-10-30 00:16:20,459:INFO: Dataset: zara2               Batch:  1/18	Loss 261.9100 (261.9100)
2022-10-30 00:16:21,532:INFO: Dataset: zara2               Batch:  2/18	Loss 255.2775 (258.3887)
2022-10-30 00:16:22,608:INFO: Dataset: zara2               Batch:  3/18	Loss 411.7070 (311.3614)
2022-10-30 00:16:23,686:INFO: Dataset: zara2               Batch:  4/18	Loss 553.7355 (372.4025)
2022-10-30 00:16:24,752:INFO: Dataset: zara2               Batch:  5/18	Loss 133.9888 (327.1738)
2022-10-30 00:16:25,824:INFO: Dataset: zara2               Batch:  6/18	Loss 340.8618 (329.6442)
2022-10-30 00:16:26,895:INFO: Dataset: zara2               Batch:  7/18	Loss 1473.2067 (489.7719)
2022-10-30 00:16:27,950:INFO: Dataset: zara2               Batch:  8/18	Loss 217.8120 (457.4318)
2022-10-30 00:16:29,005:INFO: Dataset: zara2               Batch:  9/18	Loss 415.1528 (452.6892)
2022-10-30 00:16:30,056:INFO: Dataset: zara2               Batch: 10/18	Loss 179.0984 (425.8267)
2022-10-30 00:16:31,113:INFO: Dataset: zara2               Batch: 11/18	Loss 186.8401 (402.8793)
2022-10-30 00:16:32,178:INFO: Dataset: zara2               Batch: 12/18	Loss 307.7600 (395.0819)
2022-10-30 00:16:33,238:INFO: Dataset: zara2               Batch: 13/18	Loss 198.4646 (379.8541)
2022-10-30 00:16:34,296:INFO: Dataset: zara2               Batch: 14/18	Loss 7360.1611 (858.2414)
2022-10-30 00:16:35,354:INFO: Dataset: zara2               Batch: 15/18	Loss 316.4658 (826.9393)
2022-10-30 00:16:36,411:INFO: Dataset: zara2               Batch: 16/18	Loss 293.2780 (789.8133)
2022-10-30 00:16:37,479:INFO: Dataset: zara2               Batch: 17/18	Loss 222.8548 (753.3053)
2022-10-30 00:16:38,436:INFO: Dataset: zara2               Batch: 18/18	Loss 1115.6685 (771.4565)
2022-10-30 00:16:38,503:INFO: - Computing ADE (validation o)
2022-10-30 00:16:38,856:INFO: 		 ADE on eth                       dataset:	 4.721090316772461
2022-10-30 00:16:38,856:INFO: Average validation o:	ADE  4.7211	FDE  8.4979
2022-10-30 00:16:38,856:INFO: - Computing ADE (validation)
2022-10-30 00:16:39,211:INFO: 		 ADE on hotel                     dataset:	 2.3251519203186035
2022-10-30 00:16:39,648:INFO: 		 ADE on univ                      dataset:	 2.377410411834717
2022-10-30 00:16:40,013:INFO: 		 ADE on zara1                     dataset:	 2.126418113708496
2022-10-30 00:16:40,553:INFO: 		 ADE on zara2                     dataset:	 2.570699691772461
2022-10-30 00:16:40,554:INFO: Average validation:	ADE  2.4309	FDE  4.6298
2022-10-30 00:16:40,554:INFO: - Computing ADE (training)
2022-10-30 00:16:41,028:INFO: 		 ADE on hotel                     dataset:	 2.4861948490142822
2022-10-30 00:16:42,107:INFO: 		 ADE on univ                      dataset:	 2.307124376296997
2022-10-30 00:16:42,789:INFO: 		 ADE on zara1                     dataset:	 3.2193942070007324
2022-10-30 00:16:44,002:INFO: 		 ADE on zara2                     dataset:	 2.988037586212158
2022-10-30 00:16:44,002:INFO: Average training:	ADE  2.5080	FDE  4.7770
2022-10-30 00:16:44,012:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_423.pth.tar
2022-10-30 00:16:44,012:INFO: 
===> EPOCH: 424 (P4)
2022-10-30 00:16:44,013:INFO: - Computing loss (training)
2022-10-30 00:16:45,293:INFO: Dataset: hotel               Batch: 1/4	Loss 90.2201 (90.2201)
2022-10-30 00:16:46,352:INFO: Dataset: hotel               Batch: 2/4	Loss 88.4295 (89.2913)
2022-10-30 00:16:47,397:INFO: Dataset: hotel               Batch: 3/4	Loss 88.7180 (89.1062)
2022-10-30 00:16:48,172:INFO: Dataset: hotel               Batch: 4/4	Loss 53.0586 (83.1142)
2022-10-30 00:16:49,635:INFO: Dataset: univ                Batch:  1/15	Loss 170.5996 (170.5996)
2022-10-30 00:16:50,769:INFO: Dataset: univ                Batch:  2/15	Loss 1702.4489 (938.1177)
2022-10-30 00:16:51,914:INFO: Dataset: univ                Batch:  3/15	Loss 4366.4346 (2205.2836)
2022-10-30 00:16:53,034:INFO: Dataset: univ                Batch:  4/15	Loss 162.5246 (1721.1674)
2022-10-30 00:16:54,167:INFO: Dataset: univ                Batch:  5/15	Loss 10227.4209 (3490.7288)
2022-10-30 00:16:55,313:INFO: Dataset: univ                Batch:  6/15	Loss 304.6859 (2944.3806)
2022-10-30 00:16:56,442:INFO: Dataset: univ                Batch:  7/15	Loss 261.1729 (2564.7385)
2022-10-30 00:16:57,569:INFO: Dataset: univ                Batch:  8/15	Loss 205.9268 (2284.8530)
2022-10-30 00:16:58,698:INFO: Dataset: univ                Batch:  9/15	Loss 905.0465 (2129.8847)
2022-10-30 00:16:59,819:INFO: Dataset: univ                Batch: 10/15	Loss 371.5317 (1972.2271)
2022-10-30 00:17:00,942:INFO: Dataset: univ                Batch: 11/15	Loss 310.7868 (1838.4246)
2022-10-30 00:17:02,081:INFO: Dataset: univ                Batch: 12/15	Loss 236.6907 (1684.5042)
2022-10-30 00:17:03,196:INFO: Dataset: univ                Batch: 13/15	Loss 608.3667 (1607.2365)
2022-10-30 00:17:04,323:INFO: Dataset: univ                Batch: 14/15	Loss 292.1494 (1516.1313)
2022-10-30 00:17:04,806:INFO: Dataset: univ                Batch: 15/15	Loss 28.9174 (1495.9043)
2022-10-30 00:17:06,193:INFO: Dataset: zara1               Batch: 1/8	Loss 116.2348 (116.2348)
2022-10-30 00:17:07,281:INFO: Dataset: zara1               Batch: 2/8	Loss 119.2044 (117.7515)
2022-10-30 00:17:08,351:INFO: Dataset: zara1               Batch: 3/8	Loss 546.6387 (263.5488)
2022-10-30 00:17:09,412:INFO: Dataset: zara1               Batch: 4/8	Loss 110.8156 (227.8945)
2022-10-30 00:17:10,481:INFO: Dataset: zara1               Batch: 5/8	Loss 190.5223 (219.6176)
2022-10-30 00:17:11,547:INFO: Dataset: zara1               Batch: 6/8	Loss 166.4156 (209.8211)
2022-10-30 00:17:12,606:INFO: Dataset: zara1               Batch: 7/8	Loss 102.4687 (194.1959)
2022-10-30 00:17:13,570:INFO: Dataset: zara1               Batch: 8/8	Loss 188.9141 (193.6315)
2022-10-30 00:17:14,947:INFO: Dataset: zara2               Batch:  1/18	Loss 3346.7803 (3346.7803)
2022-10-30 00:17:16,011:INFO: Dataset: zara2               Batch:  2/18	Loss 245.5401 (1879.2715)
2022-10-30 00:17:17,085:INFO: Dataset: zara2               Batch:  3/18	Loss 117.4056 (1313.3499)
2022-10-30 00:17:18,148:INFO: Dataset: zara2               Batch:  4/18	Loss 374.3895 (1081.1813)
2022-10-30 00:17:19,212:INFO: Dataset: zara2               Batch:  5/18	Loss 143.5846 (897.2001)
2022-10-30 00:17:20,370:INFO: Dataset: zara2               Batch:  6/18	Loss 1194.4224 (946.7371)
2022-10-30 00:17:21,432:INFO: Dataset: zara2               Batch:  7/18	Loss 1480.7935 (1024.0568)
2022-10-30 00:17:22,506:INFO: Dataset: zara2               Batch:  8/18	Loss 105.6682 (903.8470)
2022-10-30 00:17:23,570:INFO: Dataset: zara2               Batch:  9/18	Loss 136.3451 (804.4283)
2022-10-30 00:17:24,634:INFO: Dataset: zara2               Batch: 10/18	Loss 124.7628 (731.9526)
2022-10-30 00:17:25,699:INFO: Dataset: zara2               Batch: 11/18	Loss 458.8508 (706.8588)
2022-10-30 00:17:26,765:INFO: Dataset: zara2               Batch: 12/18	Loss 366.0016 (675.9881)
2022-10-30 00:17:27,834:INFO: Dataset: zara2               Batch: 13/18	Loss 115.3526 (633.1114)
2022-10-30 00:17:28,907:INFO: Dataset: zara2               Batch: 14/18	Loss 115.5633 (596.5485)
2022-10-30 00:17:29,974:INFO: Dataset: zara2               Batch: 15/18	Loss 365.3577 (579.4045)
2022-10-30 00:17:31,043:INFO: Dataset: zara2               Batch: 16/18	Loss 526.7122 (576.0591)
2022-10-30 00:17:32,116:INFO: Dataset: zara2               Batch: 17/18	Loss 844.7386 (592.8926)
2022-10-30 00:17:33,082:INFO: Dataset: zara2               Batch: 18/18	Loss 42.9092 (563.8887)
2022-10-30 00:17:33,147:INFO: - Computing ADE (validation o)
2022-10-30 00:17:33,490:INFO: 		 ADE on eth                       dataset:	 4.916197776794434
2022-10-30 00:17:33,491:INFO: Average validation o:	ADE  4.9162	FDE  8.9146
2022-10-30 00:17:33,491:INFO: - Computing ADE (validation)
2022-10-30 00:17:33,847:INFO: 		 ADE on hotel                     dataset:	 2.4986207485198975
2022-10-30 00:17:34,289:INFO: 		 ADE on univ                      dataset:	 2.501960515975952
2022-10-30 00:17:34,640:INFO: 		 ADE on zara1                     dataset:	 2.0736637115478516
2022-10-30 00:17:35,186:INFO: 		 ADE on zara2                     dataset:	 2.677564859390259
2022-10-30 00:17:35,186:INFO: Average validation:	ADE  2.5413	FDE  4.9003
2022-10-30 00:17:35,187:INFO: - Computing ADE (training)
2022-10-30 00:17:35,673:INFO: 		 ADE on hotel                     dataset:	 2.602084159851074
2022-10-30 00:17:36,751:INFO: 		 ADE on univ                      dataset:	 2.428129196166992
2022-10-30 00:17:37,439:INFO: 		 ADE on zara1                     dataset:	 3.2943294048309326
2022-10-30 00:17:38,654:INFO: 		 ADE on zara2                     dataset:	 3.097367525100708
2022-10-30 00:17:38,655:INFO: Average training:	ADE  2.6236	FDE  5.0626
2022-10-30 00:17:38,665:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_424.pth.tar
2022-10-30 00:17:38,666:INFO: 
===> EPOCH: 425 (P4)
2022-10-30 00:17:38,666:INFO: - Computing loss (training)
2022-10-30 00:17:39,969:INFO: Dataset: hotel               Batch: 1/4	Loss 89.7929 (89.7929)
2022-10-30 00:17:41,047:INFO: Dataset: hotel               Batch: 2/4	Loss 100.2517 (94.7379)
2022-10-30 00:17:42,109:INFO: Dataset: hotel               Batch: 3/4	Loss 89.3618 (92.9945)
2022-10-30 00:17:42,891:INFO: Dataset: hotel               Batch: 4/4	Loss 54.0024 (86.2044)
2022-10-30 00:17:44,338:INFO: Dataset: univ                Batch:  1/15	Loss 340.9424 (340.9424)
2022-10-30 00:17:45,457:INFO: Dataset: univ                Batch:  2/15	Loss 221.1899 (284.6587)
2022-10-30 00:17:46,578:INFO: Dataset: univ                Batch:  3/15	Loss 194.0717 (254.5975)
2022-10-30 00:17:47,698:INFO: Dataset: univ                Batch:  4/15	Loss 298.9390 (266.1611)
2022-10-30 00:17:48,816:INFO: Dataset: univ                Batch:  5/15	Loss 206.0549 (253.9035)
2022-10-30 00:17:49,934:INFO: Dataset: univ                Batch:  6/15	Loss 376.1301 (273.3655)
2022-10-30 00:17:51,036:INFO: Dataset: univ                Batch:  7/15	Loss 155.1888 (257.7178)
2022-10-30 00:17:52,150:INFO: Dataset: univ                Batch:  8/15	Loss 374.8113 (272.6852)
2022-10-30 00:17:53,258:INFO: Dataset: univ                Batch:  9/15	Loss 262.9290 (271.5936)
2022-10-30 00:17:54,358:INFO: Dataset: univ                Batch: 10/15	Loss 164.5672 (261.5922)
2022-10-30 00:17:55,469:INFO: Dataset: univ                Batch: 11/15	Loss 355.7083 (270.2546)
2022-10-30 00:17:56,576:INFO: Dataset: univ                Batch: 12/15	Loss 190.8626 (263.8157)
2022-10-30 00:17:57,692:INFO: Dataset: univ                Batch: 13/15	Loss 196.0050 (258.3251)
2022-10-30 00:17:58,801:INFO: Dataset: univ                Batch: 14/15	Loss 286.0428 (260.2663)
2022-10-30 00:17:59,276:INFO: Dataset: univ                Batch: 15/15	Loss 36.4783 (258.0180)
2022-10-30 00:18:00,675:INFO: Dataset: zara1               Batch: 1/8	Loss 109.9352 (109.9352)
2022-10-30 00:18:01,738:INFO: Dataset: zara1               Batch: 2/8	Loss 121.6527 (116.4741)
2022-10-30 00:18:02,804:INFO: Dataset: zara1               Batch: 3/8	Loss 100.1900 (111.0240)
2022-10-30 00:18:03,868:INFO: Dataset: zara1               Batch: 4/8	Loss 215.6131 (139.0389)
2022-10-30 00:18:04,931:INFO: Dataset: zara1               Batch: 5/8	Loss 103.6769 (132.2385)
2022-10-30 00:18:05,994:INFO: Dataset: zara1               Batch: 6/8	Loss 214.3095 (144.7748)
2022-10-30 00:18:07,052:INFO: Dataset: zara1               Batch: 7/8	Loss 103.2998 (139.0265)
2022-10-30 00:18:08,014:INFO: Dataset: zara1               Batch: 8/8	Loss 109.0603 (136.0299)
2022-10-30 00:18:09,389:INFO: Dataset: zara2               Batch:  1/18	Loss 362.7052 (362.7052)
2022-10-30 00:18:10,447:INFO: Dataset: zara2               Batch:  2/18	Loss 160.2628 (259.6380)
2022-10-30 00:18:11,499:INFO: Dataset: zara2               Batch:  3/18	Loss 183.4456 (231.2840)
2022-10-30 00:18:12,553:INFO: Dataset: zara2               Batch:  4/18	Loss 294.5322 (247.0167)
2022-10-30 00:18:13,602:INFO: Dataset: zara2               Batch:  5/18	Loss 391.6146 (276.9392)
2022-10-30 00:18:14,653:INFO: Dataset: zara2               Batch:  6/18	Loss 149.9044 (257.7491)
2022-10-30 00:18:15,706:INFO: Dataset: zara2               Batch:  7/18	Loss 275.9501 (260.0309)
2022-10-30 00:18:16,763:INFO: Dataset: zara2               Batch:  8/18	Loss 1486.4252 (413.1603)
2022-10-30 00:18:17,813:INFO: Dataset: zara2               Batch:  9/18	Loss 110.7854 (377.6680)
2022-10-30 00:18:18,868:INFO: Dataset: zara2               Batch: 10/18	Loss 114.7698 (353.1111)
2022-10-30 00:18:19,920:INFO: Dataset: zara2               Batch: 11/18	Loss 1277.2920 (441.9984)
2022-10-30 00:18:20,972:INFO: Dataset: zara2               Batch: 12/18	Loss 170.8888 (420.5231)
2022-10-30 00:18:22,021:INFO: Dataset: zara2               Batch: 13/18	Loss 163.7949 (399.4769)
2022-10-30 00:18:23,069:INFO: Dataset: zara2               Batch: 14/18	Loss 148.0749 (382.2373)
2022-10-30 00:18:24,121:INFO: Dataset: zara2               Batch: 15/18	Loss 213.7171 (371.1130)
2022-10-30 00:18:25,173:INFO: Dataset: zara2               Batch: 16/18	Loss 273.3190 (364.6867)
2022-10-30 00:18:26,227:INFO: Dataset: zara2               Batch: 17/18	Loss 227.7096 (356.7965)
2022-10-30 00:18:27,188:INFO: Dataset: zara2               Batch: 18/18	Loss 130.3855 (346.7654)
2022-10-30 00:18:27,256:INFO: - Computing ADE (validation o)
2022-10-30 00:18:27,617:INFO: 		 ADE on eth                       dataset:	 4.940462589263916
2022-10-30 00:18:27,617:INFO: Average validation o:	ADE  4.9405	FDE  8.9509
2022-10-30 00:18:27,618:INFO: - Computing ADE (validation)
2022-10-30 00:18:27,968:INFO: 		 ADE on hotel                     dataset:	 2.502915620803833
2022-10-30 00:18:28,403:INFO: 		 ADE on univ                      dataset:	 2.5071568489074707
2022-10-30 00:18:28,764:INFO: 		 ADE on zara1                     dataset:	 2.067988395690918
2022-10-30 00:18:29,309:INFO: 		 ADE on zara2                     dataset:	 2.698941230773926
2022-10-30 00:18:29,309:INFO: Average validation:	ADE  2.5517	FDE  4.9160
2022-10-30 00:18:29,310:INFO: - Computing ADE (training)
2022-10-30 00:18:29,801:INFO: 		 ADE on hotel                     dataset:	 2.5716466903686523
2022-10-30 00:18:30,874:INFO: 		 ADE on univ                      dataset:	 2.429525375366211
2022-10-30 00:18:31,562:INFO: 		 ADE on zara1                     dataset:	 3.3051769733428955
2022-10-30 00:18:32,778:INFO: 		 ADE on zara2                     dataset:	 3.0988986492156982
2022-10-30 00:18:32,778:INFO: Average training:	ADE  2.6248	FDE  5.0652
2022-10-30 00:18:32,789:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_425.pth.tar
2022-10-30 00:18:32,790:INFO: 
===> EPOCH: 426 (P4)
2022-10-30 00:18:32,790:INFO: - Computing loss (training)
2022-10-30 00:18:34,093:INFO: Dataset: hotel               Batch: 1/4	Loss 87.4639 (87.4639)
2022-10-30 00:18:35,174:INFO: Dataset: hotel               Batch: 2/4	Loss 89.6988 (88.5565)
2022-10-30 00:18:36,245:INFO: Dataset: hotel               Batch: 3/4	Loss 88.7853 (88.6346)
2022-10-30 00:18:37,013:INFO: Dataset: hotel               Batch: 4/4	Loss 55.3024 (82.3464)
2022-10-30 00:18:38,450:INFO: Dataset: univ                Batch:  1/15	Loss 7160.6982 (7160.6982)
2022-10-30 00:18:39,581:INFO: Dataset: univ                Batch:  2/15	Loss 242.7246 (3679.3802)
2022-10-30 00:18:40,709:INFO: Dataset: univ                Batch:  3/15	Loss 296.1411 (2643.9064)
2022-10-30 00:18:41,846:INFO: Dataset: univ                Batch:  4/15	Loss 861.7816 (2161.7457)
2022-10-30 00:18:42,971:INFO: Dataset: univ                Batch:  5/15	Loss 336.9952 (1797.4990)
2022-10-30 00:18:44,109:INFO: Dataset: univ                Batch:  6/15	Loss 249.5328 (1515.0845)
2022-10-30 00:18:45,243:INFO: Dataset: univ                Batch:  7/15	Loss 381.7310 (1339.8514)
2022-10-30 00:18:46,367:INFO: Dataset: univ                Batch:  8/15	Loss 299.5447 (1212.5059)
2022-10-30 00:18:47,491:INFO: Dataset: univ                Batch:  9/15	Loss 318.9578 (1121.6230)
2022-10-30 00:18:48,611:INFO: Dataset: univ                Batch: 10/15	Loss 183.2531 (1035.5146)
2022-10-30 00:18:49,747:INFO: Dataset: univ                Batch: 11/15	Loss 534.4528 (987.3522)
2022-10-30 00:18:50,878:INFO: Dataset: univ                Batch: 12/15	Loss 180.2073 (918.0230)
2022-10-30 00:18:52,007:INFO: Dataset: univ                Batch: 13/15	Loss 230.5907 (869.1081)
2022-10-30 00:18:53,132:INFO: Dataset: univ                Batch: 14/15	Loss 404.9681 (831.7966)
2022-10-30 00:18:53,611:INFO: Dataset: univ                Batch: 15/15	Loss 24.2539 (820.2012)
2022-10-30 00:18:54,991:INFO: Dataset: zara1               Batch: 1/8	Loss 117.0350 (117.0350)
2022-10-30 00:18:56,054:INFO: Dataset: zara1               Batch: 2/8	Loss 104.5446 (110.8867)
2022-10-30 00:18:57,117:INFO: Dataset: zara1               Batch: 3/8	Loss 144.4818 (123.2018)
2022-10-30 00:18:58,175:INFO: Dataset: zara1               Batch: 4/8	Loss 193.9508 (140.5893)
2022-10-30 00:18:59,231:INFO: Dataset: zara1               Batch: 5/8	Loss 645.2036 (241.5121)
2022-10-30 00:19:00,288:INFO: Dataset: zara1               Batch: 6/8	Loss 94.4878 (215.7238)
2022-10-30 00:19:01,349:INFO: Dataset: zara1               Batch: 7/8	Loss 104.2919 (200.2937)
2022-10-30 00:19:02,305:INFO: Dataset: zara1               Batch: 8/8	Loss 86.6800 (186.0023)
2022-10-30 00:19:03,674:INFO: Dataset: zara2               Batch:  1/18	Loss 279.4351 (279.4351)
2022-10-30 00:19:04,735:INFO: Dataset: zara2               Batch:  2/18	Loss 221.4896 (250.6338)
2022-10-30 00:19:05,792:INFO: Dataset: zara2               Batch:  3/18	Loss 107.3088 (200.7395)
2022-10-30 00:19:06,852:INFO: Dataset: zara2               Batch:  4/18	Loss 239.5382 (210.3407)
2022-10-30 00:19:07,908:INFO: Dataset: zara2               Batch:  5/18	Loss 134.5597 (195.6634)
2022-10-30 00:19:08,970:INFO: Dataset: zara2               Batch:  6/18	Loss 197.9549 (196.0501)
2022-10-30 00:19:10,030:INFO: Dataset: zara2               Batch:  7/18	Loss 120.4603 (185.0810)
2022-10-30 00:19:11,090:INFO: Dataset: zara2               Batch:  8/18	Loss 31.2000 (167.1103)
2022-10-30 00:19:12,147:INFO: Dataset: zara2               Batch:  9/18	Loss 212.1524 (171.8204)
2022-10-30 00:19:13,207:INFO: Dataset: zara2               Batch: 10/18	Loss 259.9720 (180.7085)
2022-10-30 00:19:14,260:INFO: Dataset: zara2               Batch: 11/18	Loss 307.3070 (192.7042)
2022-10-30 00:19:15,313:INFO: Dataset: zara2               Batch: 12/18	Loss 145.1447 (188.6943)
2022-10-30 00:19:16,452:INFO: Dataset: zara2               Batch: 13/18	Loss 97.8899 (181.9375)
2022-10-30 00:19:17,503:INFO: Dataset: zara2               Batch: 14/18	Loss 276.0966 (188.4402)
2022-10-30 00:19:18,551:INFO: Dataset: zara2               Batch: 15/18	Loss 155.7948 (186.0741)
2022-10-30 00:19:19,594:INFO: Dataset: zara2               Batch: 16/18	Loss 148.6719 (183.8938)
2022-10-30 00:19:20,646:INFO: Dataset: zara2               Batch: 17/18	Loss 117.5719 (179.8932)
2022-10-30 00:19:21,599:INFO: Dataset: zara2               Batch: 18/18	Loss 503.0313 (194.8508)
2022-10-30 00:19:21,665:INFO: - Computing ADE (validation o)
2022-10-30 00:19:22,007:INFO: 		 ADE on eth                       dataset:	 4.756176471710205
2022-10-30 00:19:22,007:INFO: Average validation o:	ADE  4.7562	FDE  8.6152
2022-10-30 00:19:22,008:INFO: - Computing ADE (validation)
2022-10-30 00:19:22,348:INFO: 		 ADE on hotel                     dataset:	 2.3815395832061768
2022-10-30 00:19:22,789:INFO: 		 ADE on univ                      dataset:	 2.409623384475708
2022-10-30 00:19:23,153:INFO: 		 ADE on zara1                     dataset:	 2.047328472137451
2022-10-30 00:19:23,710:INFO: 		 ADE on zara2                     dataset:	 2.6185994148254395
2022-10-30 00:19:23,710:INFO: Average validation:	ADE  2.4637	FDE  4.7191
2022-10-30 00:19:23,711:INFO: - Computing ADE (training)
2022-10-30 00:19:24,184:INFO: 		 ADE on hotel                     dataset:	 2.5254995822906494
2022-10-30 00:19:25,249:INFO: 		 ADE on univ                      dataset:	 2.354367733001709
2022-10-30 00:19:25,955:INFO: 		 ADE on zara1                     dataset:	 3.261233329772949
2022-10-30 00:19:27,180:INFO: 		 ADE on zara2                     dataset:	 3.035539150238037
2022-10-30 00:19:27,180:INFO: Average training:	ADE  2.5547	FDE  4.8941
2022-10-30 00:19:27,190:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_426.pth.tar
2022-10-30 00:19:27,190:INFO: 
===> EPOCH: 427 (P4)
2022-10-30 00:19:27,191:INFO: - Computing loss (training)
2022-10-30 00:19:28,501:INFO: Dataset: hotel               Batch: 1/4	Loss 87.3915 (87.3915)
2022-10-30 00:19:29,566:INFO: Dataset: hotel               Batch: 2/4	Loss 88.3220 (87.8200)
2022-10-30 00:19:30,633:INFO: Dataset: hotel               Batch: 3/4	Loss 86.7908 (87.4888)
2022-10-30 00:19:31,426:INFO: Dataset: hotel               Batch: 4/4	Loss 54.7101 (82.1266)
2022-10-30 00:19:32,875:INFO: Dataset: univ                Batch:  1/15	Loss 216.3568 (216.3568)
2022-10-30 00:19:33,990:INFO: Dataset: univ                Batch:  2/15	Loss 35953.0430 (16960.5326)
2022-10-30 00:19:35,099:INFO: Dataset: univ                Batch:  3/15	Loss 131.2363 (11408.0224)
2022-10-30 00:19:36,217:INFO: Dataset: univ                Batch:  4/15	Loss 141.2724 (8533.1485)
2022-10-30 00:19:37,322:INFO: Dataset: univ                Batch:  5/15	Loss 1159.2307 (7143.6160)
2022-10-30 00:19:38,430:INFO: Dataset: univ                Batch:  6/15	Loss 557.2188 (6025.2265)
2022-10-30 00:19:39,532:INFO: Dataset: univ                Batch:  7/15	Loss 137.1128 (5221.3498)
2022-10-30 00:19:40,649:INFO: Dataset: univ                Batch:  8/15	Loss 1804.3961 (4765.3005)
2022-10-30 00:19:41,765:INFO: Dataset: univ                Batch:  9/15	Loss 143.6185 (4259.5147)
2022-10-30 00:19:42,881:INFO: Dataset: univ                Batch: 10/15	Loss 205.8126 (3831.5208)
2022-10-30 00:19:43,981:INFO: Dataset: univ                Batch: 11/15	Loss 212.2535 (3538.1920)
2022-10-30 00:19:45,089:INFO: Dataset: univ                Batch: 12/15	Loss 505.5503 (3307.6722)
2022-10-30 00:19:46,207:INFO: Dataset: univ                Batch: 13/15	Loss 203.2050 (3053.4368)
2022-10-30 00:19:47,324:INFO: Dataset: univ                Batch: 14/15	Loss 167.6193 (2847.4258)
2022-10-30 00:19:47,801:INFO: Dataset: univ                Batch: 15/15	Loss 25.1377 (2809.1747)
2022-10-30 00:19:49,155:INFO: Dataset: zara1               Batch: 1/8	Loss 212.8896 (212.8896)
2022-10-30 00:19:50,199:INFO: Dataset: zara1               Batch: 2/8	Loss 135.0792 (174.1382)
2022-10-30 00:19:51,246:INFO: Dataset: zara1               Batch: 3/8	Loss 130.0187 (161.2417)
2022-10-30 00:19:52,293:INFO: Dataset: zara1               Batch: 4/8	Loss 174.1179 (164.4064)
2022-10-30 00:19:53,350:INFO: Dataset: zara1               Batch: 5/8	Loss 458.3837 (223.2019)
2022-10-30 00:19:54,402:INFO: Dataset: zara1               Batch: 6/8	Loss 118.1658 (206.3782)
2022-10-30 00:19:55,458:INFO: Dataset: zara1               Batch: 7/8	Loss 97.0049 (189.0847)
2022-10-30 00:19:56,409:INFO: Dataset: zara1               Batch: 8/8	Loss 129.1280 (182.0161)
2022-10-30 00:19:57,765:INFO: Dataset: zara2               Batch:  1/18	Loss 351.2602 (351.2602)
2022-10-30 00:19:58,819:INFO: Dataset: zara2               Batch:  2/18	Loss 547.8939 (441.3577)
2022-10-30 00:19:59,871:INFO: Dataset: zara2               Batch:  3/18	Loss 111.3827 (319.3305)
2022-10-30 00:20:00,925:INFO: Dataset: zara2               Batch:  4/18	Loss 1259.2288 (568.4770)
2022-10-30 00:20:01,981:INFO: Dataset: zara2               Batch:  5/18	Loss 140.4657 (482.0101)
2022-10-30 00:20:03,038:INFO: Dataset: zara2               Batch:  6/18	Loss 169.1834 (426.8864)
2022-10-30 00:20:04,097:INFO: Dataset: zara2               Batch:  7/18	Loss 228.5936 (398.4520)
2022-10-30 00:20:05,154:INFO: Dataset: zara2               Batch:  8/18	Loss 104.2913 (361.5605)
2022-10-30 00:20:06,213:INFO: Dataset: zara2               Batch:  9/18	Loss 401.1803 (365.7331)
2022-10-30 00:20:07,263:INFO: Dataset: zara2               Batch: 10/18	Loss 152.0317 (346.1783)
2022-10-30 00:20:08,323:INFO: Dataset: zara2               Batch: 11/18	Loss 105.9445 (324.6648)
2022-10-30 00:20:09,381:INFO: Dataset: zara2               Batch: 12/18	Loss 290.9169 (321.7834)
2022-10-30 00:20:10,456:INFO: Dataset: zara2               Batch: 13/18	Loss 265.5541 (316.8325)
2022-10-30 00:20:11,502:INFO: Dataset: zara2               Batch: 14/18	Loss 652.7491 (338.4586)
2022-10-30 00:20:12,570:INFO: Dataset: zara2               Batch: 15/18	Loss 262.4604 (333.1167)
2022-10-30 00:20:13,639:INFO: Dataset: zara2               Batch: 16/18	Loss 251.5128 (327.7575)
2022-10-30 00:20:14,703:INFO: Dataset: zara2               Batch: 17/18	Loss 125.8974 (316.0398)
2022-10-30 00:20:15,671:INFO: Dataset: zara2               Batch: 18/18	Loss 132.1060 (307.5866)
2022-10-30 00:20:15,738:INFO: - Computing ADE (validation o)
2022-10-30 00:20:16,089:INFO: 		 ADE on eth                       dataset:	 4.495948314666748
2022-10-30 00:20:16,089:INFO: Average validation o:	ADE  4.4959	FDE  7.9630
2022-10-30 00:20:16,090:INFO: - Computing ADE (validation)
2022-10-30 00:20:16,446:INFO: 		 ADE on hotel                     dataset:	 2.1195454597473145
2022-10-30 00:20:16,893:INFO: 		 ADE on univ                      dataset:	 2.192366123199463
2022-10-30 00:20:17,241:INFO: 		 ADE on zara1                     dataset:	 2.101815938949585
2022-10-30 00:20:17,786:INFO: 		 ADE on zara2                     dataset:	 2.396960496902466
2022-10-30 00:20:17,786:INFO: Average validation:	ADE  2.2582	FDE  4.2182
2022-10-30 00:20:17,787:INFO: - Computing ADE (training)
2022-10-30 00:20:18,266:INFO: 		 ADE on hotel                     dataset:	 2.2647316455841064
2022-10-30 00:20:19,321:INFO: 		 ADE on univ                      dataset:	 2.117616891860962
2022-10-30 00:20:20,028:INFO: 		 ADE on zara1                     dataset:	 3.086961507797241
2022-10-30 00:20:21,245:INFO: 		 ADE on zara2                     dataset:	 2.7907638549804688
2022-10-30 00:20:21,245:INFO: Average training:	ADE  2.3197	FDE  4.3401
2022-10-30 00:20:21,256:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_427.pth.tar
2022-10-30 00:20:21,256:INFO: 
===> EPOCH: 428 (P4)
2022-10-30 00:20:21,257:INFO: - Computing loss (training)
2022-10-30 00:20:22,528:INFO: Dataset: hotel               Batch: 1/4	Loss 84.5927 (84.5927)
2022-10-30 00:20:23,590:INFO: Dataset: hotel               Batch: 2/4	Loss 87.3306 (85.9220)
2022-10-30 00:20:24,637:INFO: Dataset: hotel               Batch: 3/4	Loss 84.9331 (85.5839)
2022-10-30 00:20:25,406:INFO: Dataset: hotel               Batch: 4/4	Loss 51.8060 (79.8355)
2022-10-30 00:20:26,868:INFO: Dataset: univ                Batch:  1/15	Loss 147.6508 (147.6508)
2022-10-30 00:20:28,002:INFO: Dataset: univ                Batch:  2/15	Loss 195.0708 (172.7600)
2022-10-30 00:20:29,121:INFO: Dataset: univ                Batch:  3/15	Loss 195.5383 (180.2391)
2022-10-30 00:20:30,235:INFO: Dataset: univ                Batch:  4/15	Loss 368.5910 (225.7782)
2022-10-30 00:20:31,354:INFO: Dataset: univ                Batch:  5/15	Loss 353.8088 (250.4559)
2022-10-30 00:20:32,481:INFO: Dataset: univ                Batch:  6/15	Loss 858.8232 (359.4343)
2022-10-30 00:20:33,590:INFO: Dataset: univ                Batch:  7/15	Loss 300.4950 (351.8092)
2022-10-30 00:20:34,695:INFO: Dataset: univ                Batch:  8/15	Loss 175.5804 (330.7616)
2022-10-30 00:20:35,811:INFO: Dataset: univ                Batch:  9/15	Loss 135.2921 (308.2165)
2022-10-30 00:20:36,928:INFO: Dataset: univ                Batch: 10/15	Loss 148.6048 (291.6627)
2022-10-30 00:20:38,044:INFO: Dataset: univ                Batch: 11/15	Loss 227.7076 (285.9512)
2022-10-30 00:20:39,153:INFO: Dataset: univ                Batch: 12/15	Loss 164.0340 (276.5125)
2022-10-30 00:20:40,275:INFO: Dataset: univ                Batch: 13/15	Loss 191.1790 (269.5058)
2022-10-30 00:20:41,401:INFO: Dataset: univ                Batch: 14/15	Loss 836.4822 (310.5108)
2022-10-30 00:20:41,880:INFO: Dataset: univ                Batch: 15/15	Loss 203.9946 (308.8955)
2022-10-30 00:20:43,253:INFO: Dataset: zara1               Batch: 1/8	Loss 123.3008 (123.3008)
2022-10-30 00:20:44,310:INFO: Dataset: zara1               Batch: 2/8	Loss 100.3708 (112.0984)
2022-10-30 00:20:45,365:INFO: Dataset: zara1               Batch: 3/8	Loss 207.3090 (142.8512)
2022-10-30 00:20:46,418:INFO: Dataset: zara1               Batch: 4/8	Loss 92.9486 (131.6506)
2022-10-30 00:20:47,471:INFO: Dataset: zara1               Batch: 5/8	Loss 109.7273 (127.1889)
2022-10-30 00:20:48,519:INFO: Dataset: zara1               Batch: 6/8	Loss 109.0244 (124.2894)
2022-10-30 00:20:49,562:INFO: Dataset: zara1               Batch: 7/8	Loss 115.5577 (123.1623)
2022-10-30 00:20:50,514:INFO: Dataset: zara1               Batch: 8/8	Loss 120.2846 (122.8775)
2022-10-30 00:20:51,896:INFO: Dataset: zara2               Batch:  1/18	Loss 104.1823 (104.1823)
2022-10-30 00:20:52,977:INFO: Dataset: zara2               Batch:  2/18	Loss 109.8708 (107.1939)
2022-10-30 00:20:54,052:INFO: Dataset: zara2               Batch:  3/18	Loss 136.8464 (116.5083)
2022-10-30 00:20:55,113:INFO: Dataset: zara2               Batch:  4/18	Loss 146.5221 (123.9955)
2022-10-30 00:20:56,178:INFO: Dataset: zara2               Batch:  5/18	Loss 127.8783 (124.7141)
2022-10-30 00:20:57,246:INFO: Dataset: zara2               Batch:  6/18	Loss 332.7212 (159.7715)
2022-10-30 00:20:58,313:INFO: Dataset: zara2               Batch:  7/18	Loss 128.9258 (155.1799)
2022-10-30 00:20:59,377:INFO: Dataset: zara2               Batch:  8/18	Loss 494.4033 (199.4531)
2022-10-30 00:21:00,437:INFO: Dataset: zara2               Batch:  9/18	Loss 145.4136 (193.6308)
2022-10-30 00:21:01,591:INFO: Dataset: zara2               Batch: 10/18	Loss 469.9903 (219.4157)
2022-10-30 00:21:02,650:INFO: Dataset: zara2               Batch: 11/18	Loss 241.1108 (221.4513)
2022-10-30 00:21:03,721:INFO: Dataset: zara2               Batch: 12/18	Loss 131.1681 (213.8674)
2022-10-30 00:21:04,782:INFO: Dataset: zara2               Batch: 13/18	Loss 145.5202 (208.9975)
2022-10-30 00:21:05,851:INFO: Dataset: zara2               Batch: 14/18	Loss 144.6168 (204.3258)
2022-10-30 00:21:06,918:INFO: Dataset: zara2               Batch: 15/18	Loss 836.0015 (247.5205)
2022-10-30 00:21:07,990:INFO: Dataset: zara2               Batch: 16/18	Loss 160.0365 (242.7659)
2022-10-30 00:21:09,062:INFO: Dataset: zara2               Batch: 17/18	Loss 183.1516 (239.4961)
2022-10-30 00:21:10,037:INFO: Dataset: zara2               Batch: 18/18	Loss 128.5742 (233.8849)
2022-10-30 00:21:10,105:INFO: - Computing ADE (validation o)
2022-10-30 00:21:10,496:INFO: 		 ADE on eth                       dataset:	 4.427951812744141
2022-10-30 00:21:10,496:INFO: Average validation o:	ADE  4.4280	FDE  7.8737
2022-10-30 00:21:10,497:INFO: - Computing ADE (validation)
2022-10-30 00:21:10,847:INFO: 		 ADE on hotel                     dataset:	 2.0881431102752686
2022-10-30 00:21:11,298:INFO: 		 ADE on univ                      dataset:	 2.1997034549713135
2022-10-30 00:21:11,653:INFO: 		 ADE on zara1                     dataset:	 2.0937955379486084
2022-10-30 00:21:12,192:INFO: 		 ADE on zara2                     dataset:	 2.3802011013031006
2022-10-30 00:21:12,192:INFO: Average validation:	ADE  2.2536	FDE  4.2143
2022-10-30 00:21:12,193:INFO: - Computing ADE (training)
2022-10-30 00:21:12,658:INFO: 		 ADE on hotel                     dataset:	 2.2769296169281006
2022-10-30 00:21:13,724:INFO: 		 ADE on univ                      dataset:	 2.123302698135376
2022-10-30 00:21:14,416:INFO: 		 ADE on zara1                     dataset:	 3.0872609615325928
2022-10-30 00:21:15,673:INFO: 		 ADE on zara2                     dataset:	 2.795239210128784
2022-10-30 00:21:15,673:INFO: Average training:	ADE  2.3250	FDE  4.3491
2022-10-30 00:21:15,684:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_428.pth.tar
2022-10-30 00:21:15,684:INFO: 
===> EPOCH: 429 (P4)
2022-10-30 00:21:15,685:INFO: - Computing loss (training)
2022-10-30 00:21:17,005:INFO: Dataset: hotel               Batch: 1/4	Loss 85.8170 (85.8170)
2022-10-30 00:21:18,078:INFO: Dataset: hotel               Batch: 2/4	Loss 86.0759 (85.9452)
2022-10-30 00:21:19,148:INFO: Dataset: hotel               Batch: 3/4	Loss 84.0110 (85.2882)
2022-10-30 00:21:19,929:INFO: Dataset: hotel               Batch: 4/4	Loss 51.7847 (79.6306)
2022-10-30 00:21:21,381:INFO: Dataset: univ                Batch:  1/15	Loss 118.0580 (118.0580)
2022-10-30 00:21:22,524:INFO: Dataset: univ                Batch:  2/15	Loss 280.4673 (202.8070)
2022-10-30 00:21:23,653:INFO: Dataset: univ                Batch:  3/15	Loss 231.1503 (212.4223)
2022-10-30 00:21:24,780:INFO: Dataset: univ                Batch:  4/15	Loss 156.9651 (198.4538)
2022-10-30 00:21:25,920:INFO: Dataset: univ                Batch:  5/15	Loss 159.3081 (191.1551)
2022-10-30 00:21:27,052:INFO: Dataset: univ                Batch:  6/15	Loss 158.2911 (185.9420)
2022-10-30 00:21:28,191:INFO: Dataset: univ                Batch:  7/15	Loss 183.5880 (185.5982)
2022-10-30 00:21:29,318:INFO: Dataset: univ                Batch:  8/15	Loss 260.3102 (194.5927)
2022-10-30 00:21:30,437:INFO: Dataset: univ                Batch:  9/15	Loss 175.0391 (192.5765)
2022-10-30 00:21:31,554:INFO: Dataset: univ                Batch: 10/15	Loss 284.3059 (201.4992)
2022-10-30 00:21:32,668:INFO: Dataset: univ                Batch: 11/15	Loss 164.2099 (198.0519)
2022-10-30 00:21:33,787:INFO: Dataset: univ                Batch: 12/15	Loss 173.0111 (196.1736)
2022-10-30 00:21:34,902:INFO: Dataset: univ                Batch: 13/15	Loss 60.6065 (185.1035)
2022-10-30 00:21:36,022:INFO: Dataset: univ                Batch: 14/15	Loss 171.8311 (184.2291)
2022-10-30 00:21:36,504:INFO: Dataset: univ                Batch: 15/15	Loss 19.1422 (182.4219)
2022-10-30 00:21:37,867:INFO: Dataset: zara1               Batch: 1/8	Loss 254.7188 (254.7188)
2022-10-30 00:21:38,920:INFO: Dataset: zara1               Batch: 2/8	Loss 119.7785 (190.8452)
2022-10-30 00:21:39,964:INFO: Dataset: zara1               Batch: 3/8	Loss 95.0976 (160.7491)
2022-10-30 00:21:41,009:INFO: Dataset: zara1               Batch: 4/8	Loss 59.0237 (135.0676)
2022-10-30 00:21:42,057:INFO: Dataset: zara1               Batch: 5/8	Loss 153.9907 (138.9997)
2022-10-30 00:21:43,109:INFO: Dataset: zara1               Batch: 6/8	Loss 104.6074 (132.6406)
2022-10-30 00:21:44,158:INFO: Dataset: zara1               Batch: 7/8	Loss 240.6310 (148.2262)
2022-10-30 00:21:45,100:INFO: Dataset: zara1               Batch: 8/8	Loss 85.6938 (140.1957)
2022-10-30 00:21:46,483:INFO: Dataset: zara2               Batch:  1/18	Loss 686.3455 (686.3455)
2022-10-30 00:21:47,551:INFO: Dataset: zara2               Batch:  2/18	Loss 298.7375 (492.8294)
2022-10-30 00:21:48,620:INFO: Dataset: zara2               Batch:  3/18	Loss 109.1791 (367.8932)
2022-10-30 00:21:49,691:INFO: Dataset: zara2               Batch:  4/18	Loss 126.4036 (304.9270)
2022-10-30 00:21:50,764:INFO: Dataset: zara2               Batch:  5/18	Loss 162.9316 (275.1645)
2022-10-30 00:21:51,877:INFO: Dataset: zara2               Batch:  6/18	Loss 139.1348 (253.5874)
2022-10-30 00:21:52,984:INFO: Dataset: zara2               Batch:  7/18	Loss 113.7350 (232.4721)
2022-10-30 00:21:54,065:INFO: Dataset: zara2               Batch:  8/18	Loss 102.7439 (216.9066)
2022-10-30 00:21:55,125:INFO: Dataset: zara2               Batch:  9/18	Loss 135.0009 (206.9498)
2022-10-30 00:21:56,196:INFO: Dataset: zara2               Batch: 10/18	Loss 111.9306 (197.8646)
2022-10-30 00:21:57,269:INFO: Dataset: zara2               Batch: 11/18	Loss 148.1239 (193.1512)
2022-10-30 00:21:58,351:INFO: Dataset: zara2               Batch: 12/18	Loss 1731.9314 (314.6931)
2022-10-30 00:21:59,419:INFO: Dataset: zara2               Batch: 13/18	Loss 157.4931 (303.1525)
2022-10-30 00:22:00,493:INFO: Dataset: zara2               Batch: 14/18	Loss -17.8191 (281.2635)
2022-10-30 00:22:01,564:INFO: Dataset: zara2               Batch: 15/18	Loss 525.3194 (297.3388)
2022-10-30 00:22:02,629:INFO: Dataset: zara2               Batch: 16/18	Loss 382.3260 (302.6788)
2022-10-30 00:22:03,693:INFO: Dataset: zara2               Batch: 17/18	Loss 154.5267 (293.3906)
2022-10-30 00:22:04,660:INFO: Dataset: zara2               Batch: 18/18	Loss 83.3870 (284.3988)
2022-10-30 00:22:04,724:INFO: - Computing ADE (validation o)
2022-10-30 00:22:05,097:INFO: 		 ADE on eth                       dataset:	 4.562376976013184
2022-10-30 00:22:05,097:INFO: Average validation o:	ADE  4.5624	FDE  8.0648
2022-10-30 00:22:05,097:INFO: - Computing ADE (validation)
2022-10-30 00:22:05,453:INFO: 		 ADE on hotel                     dataset:	 2.122676134109497
2022-10-30 00:22:05,909:INFO: 		 ADE on univ                      dataset:	 2.2083709239959717
2022-10-30 00:22:06,268:INFO: 		 ADE on zara1                     dataset:	 2.054105281829834
2022-10-30 00:22:06,833:INFO: 		 ADE on zara2                     dataset:	 2.393284559249878
2022-10-30 00:22:06,834:INFO: Average validation:	ADE  2.2625	FDE  4.2330
2022-10-30 00:22:06,834:INFO: - Computing ADE (training)
2022-10-30 00:22:07,313:INFO: 		 ADE on hotel                     dataset:	 2.2532100677490234
2022-10-30 00:22:08,379:INFO: 		 ADE on univ                      dataset:	 2.1281440258026123
2022-10-30 00:22:09,057:INFO: 		 ADE on zara1                     dataset:	 3.0992205142974854
2022-10-30 00:22:10,288:INFO: 		 ADE on zara2                     dataset:	 2.7966468334198
2022-10-30 00:22:10,288:INFO: Average training:	ADE  2.3289	FDE  4.3609
2022-10-30 00:22:10,299:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_429.pth.tar
2022-10-30 00:22:10,299:INFO: 
===> EPOCH: 430 (P4)
2022-10-30 00:22:10,299:INFO: - Computing loss (training)
2022-10-30 00:22:11,563:INFO: Dataset: hotel               Batch: 1/4	Loss 86.2147 (86.2147)
2022-10-30 00:22:12,608:INFO: Dataset: hotel               Batch: 2/4	Loss 85.9603 (86.0917)
2022-10-30 00:22:13,652:INFO: Dataset: hotel               Batch: 3/4	Loss 85.0249 (85.7474)
2022-10-30 00:22:14,417:INFO: Dataset: hotel               Batch: 4/4	Loss 52.4512 (80.0809)
2022-10-30 00:22:15,833:INFO: Dataset: univ                Batch:  1/15	Loss 161.1875 (161.1875)
2022-10-30 00:22:16,971:INFO: Dataset: univ                Batch:  2/15	Loss 113.1530 (136.0365)
2022-10-30 00:22:18,098:INFO: Dataset: univ                Batch:  3/15	Loss 136.6626 (136.2537)
2022-10-30 00:22:19,220:INFO: Dataset: univ                Batch:  4/15	Loss 220.6666 (156.4649)
2022-10-30 00:22:20,341:INFO: Dataset: univ                Batch:  5/15	Loss 344.8114 (192.0557)
2022-10-30 00:22:21,468:INFO: Dataset: univ                Batch:  6/15	Loss 395.8560 (226.2042)
2022-10-30 00:22:22,585:INFO: Dataset: univ                Batch:  7/15	Loss 182.1956 (220.7574)
2022-10-30 00:22:23,715:INFO: Dataset: univ                Batch:  8/15	Loss 239.2456 (223.0193)
2022-10-30 00:22:24,837:INFO: Dataset: univ                Batch:  9/15	Loss 189.7025 (219.2881)
2022-10-30 00:22:25,969:INFO: Dataset: univ                Batch: 10/15	Loss 117.1265 (208.3435)
2022-10-30 00:22:27,091:INFO: Dataset: univ                Batch: 11/15	Loss 108.1128 (199.7190)
2022-10-30 00:22:28,219:INFO: Dataset: univ                Batch: 12/15	Loss 146.4184 (194.8396)
2022-10-30 00:22:29,339:INFO: Dataset: univ                Batch: 13/15	Loss 213.4697 (196.2399)
2022-10-30 00:22:30,471:INFO: Dataset: univ                Batch: 14/15	Loss 199.5475 (196.4943)
2022-10-30 00:22:30,957:INFO: Dataset: univ                Batch: 15/15	Loss 31.5674 (193.7744)
2022-10-30 00:22:32,343:INFO: Dataset: zara1               Batch: 1/8	Loss 153.1057 (153.1057)
2022-10-30 00:22:33,401:INFO: Dataset: zara1               Batch: 2/8	Loss 262.4995 (205.7343)
2022-10-30 00:22:34,447:INFO: Dataset: zara1               Batch: 3/8	Loss 138.0983 (181.3708)
2022-10-30 00:22:35,490:INFO: Dataset: zara1               Batch: 4/8	Loss 97.2464 (162.7156)
2022-10-30 00:22:36,534:INFO: Dataset: zara1               Batch: 5/8	Loss 160.5925 (162.2867)
2022-10-30 00:22:37,573:INFO: Dataset: zara1               Batch: 6/8	Loss 132.1562 (157.2405)
2022-10-30 00:22:38,615:INFO: Dataset: zara1               Batch: 7/8	Loss 108.6014 (150.3128)
2022-10-30 00:22:39,563:INFO: Dataset: zara1               Batch: 8/8	Loss 87.0126 (142.9167)
2022-10-30 00:22:40,953:INFO: Dataset: zara2               Batch:  1/18	Loss 138.1777 (138.1777)
2022-10-30 00:22:42,023:INFO: Dataset: zara2               Batch:  2/18	Loss 131.2652 (134.8006)
2022-10-30 00:22:43,091:INFO: Dataset: zara2               Batch:  3/18	Loss 351.4514 (211.5603)
2022-10-30 00:22:44,150:INFO: Dataset: zara2               Batch:  4/18	Loss 133.1906 (193.7465)
2022-10-30 00:22:45,220:INFO: Dataset: zara2               Batch:  5/18	Loss 161.3186 (187.4515)
2022-10-30 00:22:46,287:INFO: Dataset: zara2               Batch:  6/18	Loss 151.5775 (181.3609)
2022-10-30 00:22:47,349:INFO: Dataset: zara2               Batch:  7/18	Loss 131.4642 (174.7923)
2022-10-30 00:22:48,421:INFO: Dataset: zara2               Batch:  8/18	Loss 108.2637 (167.0962)
2022-10-30 00:22:49,493:INFO: Dataset: zara2               Batch:  9/18	Loss 166.9539 (167.0799)
2022-10-30 00:22:50,563:INFO: Dataset: zara2               Batch: 10/18	Loss 118.0175 (162.1952)
2022-10-30 00:22:51,629:INFO: Dataset: zara2               Batch: 11/18	Loss 120.0350 (158.1443)
2022-10-30 00:22:52,778:INFO: Dataset: zara2               Batch: 12/18	Loss 239.1929 (165.0880)
2022-10-30 00:22:53,846:INFO: Dataset: zara2               Batch: 13/18	Loss 115.1690 (161.2104)
2022-10-30 00:22:54,913:INFO: Dataset: zara2               Batch: 14/18	Loss 115.1918 (158.0799)
2022-10-30 00:22:55,972:INFO: Dataset: zara2               Batch: 15/18	Loss 106.6973 (154.8729)
2022-10-30 00:22:57,041:INFO: Dataset: zara2               Batch: 16/18	Loss 137.2161 (153.8845)
2022-10-30 00:22:58,107:INFO: Dataset: zara2               Batch: 17/18	Loss 222.9621 (157.7282)
2022-10-30 00:22:59,069:INFO: Dataset: zara2               Batch: 18/18	Loss 58.0123 (152.8158)
2022-10-30 00:22:59,135:INFO: - Computing ADE (validation o)
2022-10-30 00:22:59,480:INFO: 		 ADE on eth                       dataset:	 4.487786293029785
2022-10-30 00:22:59,480:INFO: Average validation o:	ADE  4.4878	FDE  7.9765
2022-10-30 00:22:59,481:INFO: - Computing ADE (validation)
2022-10-30 00:22:59,808:INFO: 		 ADE on hotel                     dataset:	 2.1586906909942627
2022-10-30 00:23:00,241:INFO: 		 ADE on univ                      dataset:	 2.210895299911499
2022-10-30 00:23:00,597:INFO: 		 ADE on zara1                     dataset:	 2.139671802520752
2022-10-30 00:23:01,127:INFO: 		 ADE on zara2                     dataset:	 2.396594285964966
2022-10-30 00:23:01,128:INFO: Average validation:	ADE  2.2720	FDE  4.2556
2022-10-30 00:23:01,128:INFO: - Computing ADE (training)
2022-10-30 00:23:01,611:INFO: 		 ADE on hotel                     dataset:	 2.285435199737549
2022-10-30 00:23:02,664:INFO: 		 ADE on univ                      dataset:	 2.138336181640625
2022-10-30 00:23:03,364:INFO: 		 ADE on zara1                     dataset:	 3.089221239089966
2022-10-30 00:23:04,573:INFO: 		 ADE on zara2                     dataset:	 2.801265239715576
2022-10-30 00:23:04,573:INFO: Average training:	ADE  2.3372	FDE  4.3800
2022-10-30 00:23:04,585:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_430.pth.tar
2022-10-30 00:23:04,585:INFO: 
===> EPOCH: 431 (P4)
2022-10-30 00:23:04,585:INFO: - Computing loss (training)
2022-10-30 00:23:05,864:INFO: Dataset: hotel               Batch: 1/4	Loss 85.9068 (85.9068)
2022-10-30 00:23:06,912:INFO: Dataset: hotel               Batch: 2/4	Loss 86.6618 (86.2898)
2022-10-30 00:23:07,959:INFO: Dataset: hotel               Batch: 3/4	Loss 85.2770 (85.9446)
2022-10-30 00:23:08,733:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9207 (79.9379)
2022-10-30 00:23:10,191:INFO: Dataset: univ                Batch:  1/15	Loss 123.5502 (123.5502)
2022-10-30 00:23:11,332:INFO: Dataset: univ                Batch:  2/15	Loss 117.5007 (120.4294)
2022-10-30 00:23:12,457:INFO: Dataset: univ                Batch:  3/15	Loss 220.6666 (151.7763)
2022-10-30 00:23:13,584:INFO: Dataset: univ                Batch:  4/15	Loss 247.5579 (176.7040)
2022-10-30 00:23:14,710:INFO: Dataset: univ                Batch:  5/15	Loss 134.3574 (168.5474)
2022-10-30 00:23:15,828:INFO: Dataset: univ                Batch:  6/15	Loss 202.6503 (173.9900)
2022-10-30 00:23:16,945:INFO: Dataset: univ                Batch:  7/15	Loss 154.6845 (171.2612)
2022-10-30 00:23:18,067:INFO: Dataset: univ                Batch:  8/15	Loss 171.4214 (171.2811)
2022-10-30 00:23:19,192:INFO: Dataset: univ                Batch:  9/15	Loss 394.9644 (196.6943)
2022-10-30 00:23:20,314:INFO: Dataset: univ                Batch: 10/15	Loss 154.3772 (192.3485)
2022-10-30 00:23:21,462:INFO: Dataset: univ                Batch: 11/15	Loss 203.5835 (193.4431)
2022-10-30 00:23:22,596:INFO: Dataset: univ                Batch: 12/15	Loss 201.5204 (194.1291)
2022-10-30 00:23:23,728:INFO: Dataset: univ                Batch: 13/15	Loss 168.9610 (192.0224)
2022-10-30 00:23:24,856:INFO: Dataset: univ                Batch: 14/15	Loss 171.2912 (190.5375)
2022-10-30 00:23:25,332:INFO: Dataset: univ                Batch: 15/15	Loss 17.2322 (188.5418)
2022-10-30 00:23:26,701:INFO: Dataset: zara1               Batch: 1/8	Loss 112.5965 (112.5965)
2022-10-30 00:23:27,749:INFO: Dataset: zara1               Batch: 2/8	Loss 140.9826 (126.3041)
2022-10-30 00:23:28,804:INFO: Dataset: zara1               Batch: 3/8	Loss 92.0639 (114.5736)
2022-10-30 00:23:29,853:INFO: Dataset: zara1               Batch: 4/8	Loss 100.4181 (111.0664)
2022-10-30 00:23:30,906:INFO: Dataset: zara1               Batch: 5/8	Loss 107.1880 (110.3188)
2022-10-30 00:23:31,956:INFO: Dataset: zara1               Batch: 6/8	Loss 143.8149 (115.8265)
2022-10-30 00:23:33,008:INFO: Dataset: zara1               Batch: 7/8	Loss 110.2624 (115.1078)
2022-10-30 00:23:33,963:INFO: Dataset: zara1               Batch: 8/8	Loss 79.0433 (111.5204)
2022-10-30 00:23:35,352:INFO: Dataset: zara2               Batch:  1/18	Loss 106.2104 (106.2104)
2022-10-30 00:23:36,423:INFO: Dataset: zara2               Batch:  2/18	Loss 226.8582 (165.9842)
2022-10-30 00:23:37,476:INFO: Dataset: zara2               Batch:  3/18	Loss 389.2697 (239.1963)
2022-10-30 00:23:38,523:INFO: Dataset: zara2               Batch:  4/18	Loss 167.7018 (220.0919)
2022-10-30 00:23:39,567:INFO: Dataset: zara2               Batch:  5/18	Loss 359.2515 (248.9148)
2022-10-30 00:23:40,634:INFO: Dataset: zara2               Batch:  6/18	Loss 391.7039 (272.6541)
2022-10-30 00:23:41,691:INFO: Dataset: zara2               Batch:  7/18	Loss 765.9291 (344.2249)
2022-10-30 00:23:42,731:INFO: Dataset: zara2               Batch:  8/18	Loss 370.0980 (347.6936)
2022-10-30 00:23:43,783:INFO: Dataset: zara2               Batch:  9/18	Loss 110.2174 (320.0265)
2022-10-30 00:23:44,834:INFO: Dataset: zara2               Batch: 10/18	Loss 3351.6162 (602.0969)
2022-10-30 00:23:45,887:INFO: Dataset: zara2               Batch: 11/18	Loss 131.9620 (563.2355)
2022-10-30 00:23:46,935:INFO: Dataset: zara2               Batch: 12/18	Loss 1234.0804 (626.0965)
2022-10-30 00:23:47,985:INFO: Dataset: zara2               Batch: 13/18	Loss 1164.4611 (665.0908)
2022-10-30 00:23:49,040:INFO: Dataset: zara2               Batch: 14/18	Loss 115.9824 (628.6537)
2022-10-30 00:23:50,087:INFO: Dataset: zara2               Batch: 15/18	Loss 129.8355 (593.9346)
2022-10-30 00:23:51,140:INFO: Dataset: zara2               Batch: 16/18	Loss 250.1975 (572.8713)
2022-10-30 00:23:52,192:INFO: Dataset: zara2               Batch: 17/18	Loss 395.3951 (562.6531)
2022-10-30 00:23:53,154:INFO: Dataset: zara2               Batch: 18/18	Loss 126.9641 (541.0451)
2022-10-30 00:23:53,221:INFO: - Computing ADE (validation o)
2022-10-30 00:23:53,574:INFO: 		 ADE on eth                       dataset:	 4.496312141418457
2022-10-30 00:23:53,575:INFO: Average validation o:	ADE  4.4963	FDE  8.0076
2022-10-30 00:23:53,575:INFO: - Computing ADE (validation)
2022-10-30 00:23:53,927:INFO: 		 ADE on hotel                     dataset:	 2.1413497924804688
2022-10-30 00:23:54,367:INFO: 		 ADE on univ                      dataset:	 2.2265448570251465
2022-10-30 00:23:54,733:INFO: 		 ADE on zara1                     dataset:	 2.1008880138397217
2022-10-30 00:23:55,292:INFO: 		 ADE on zara2                     dataset:	 2.4217939376831055
2022-10-30 00:23:55,292:INFO: Average validation:	ADE  2.2862	FDE  4.2879
2022-10-30 00:23:55,293:INFO: - Computing ADE (training)
2022-10-30 00:23:55,761:INFO: 		 ADE on hotel                     dataset:	 2.2920806407928467
2022-10-30 00:23:56,836:INFO: 		 ADE on univ                      dataset:	 2.152036428451538
2022-10-30 00:23:57,508:INFO: 		 ADE on zara1                     dataset:	 3.1106536388397217
2022-10-30 00:23:58,697:INFO: 		 ADE on zara2                     dataset:	 2.8378384113311768
2022-10-30 00:23:58,698:INFO: Average training:	ADE  2.3559	FDE  4.4186
2022-10-30 00:23:58,708:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_431.pth.tar
2022-10-30 00:23:58,708:INFO: 
===> EPOCH: 432 (P4)
2022-10-30 00:23:58,709:INFO: - Computing loss (training)
2022-10-30 00:24:00,019:INFO: Dataset: hotel               Batch: 1/4	Loss 86.9249 (86.9249)
2022-10-30 00:24:01,074:INFO: Dataset: hotel               Batch: 2/4	Loss 85.7390 (86.3379)
2022-10-30 00:24:02,123:INFO: Dataset: hotel               Batch: 3/4	Loss 84.8407 (85.7989)
2022-10-30 00:24:02,894:INFO: Dataset: hotel               Batch: 4/4	Loss 52.9160 (80.0292)
2022-10-30 00:24:04,365:INFO: Dataset: univ                Batch:  1/15	Loss 106.2171 (106.2171)
2022-10-30 00:24:05,519:INFO: Dataset: univ                Batch:  2/15	Loss 139.5188 (123.6928)
2022-10-30 00:24:06,637:INFO: Dataset: univ                Batch:  3/15	Loss 130.2495 (125.8600)
2022-10-30 00:24:07,759:INFO: Dataset: univ                Batch:  4/15	Loss 147.2808 (131.2294)
2022-10-30 00:24:08,876:INFO: Dataset: univ                Batch:  5/15	Loss 207.6361 (145.9961)
2022-10-30 00:24:10,006:INFO: Dataset: univ                Batch:  6/15	Loss 174.7112 (150.9055)
2022-10-30 00:24:11,120:INFO: Dataset: univ                Batch:  7/15	Loss 101.4949 (144.0821)
2022-10-30 00:24:12,235:INFO: Dataset: univ                Batch:  8/15	Loss 138.2804 (143.3043)
2022-10-30 00:24:13,349:INFO: Dataset: univ                Batch:  9/15	Loss 141.1349 (143.0917)
2022-10-30 00:24:14,464:INFO: Dataset: univ                Batch: 10/15	Loss 180.2090 (146.9851)
2022-10-30 00:24:15,580:INFO: Dataset: univ                Batch: 11/15	Loss 129.6659 (145.3646)
2022-10-30 00:24:16,688:INFO: Dataset: univ                Batch: 12/15	Loss 134.7710 (144.4964)
2022-10-30 00:24:17,791:INFO: Dataset: univ                Batch: 13/15	Loss 115.9315 (142.5446)
2022-10-30 00:24:18,903:INFO: Dataset: univ                Batch: 14/15	Loss 137.5880 (142.1905)
2022-10-30 00:24:19,382:INFO: Dataset: univ                Batch: 15/15	Loss 32.5174 (140.6417)
2022-10-30 00:24:20,763:INFO: Dataset: zara1               Batch: 1/8	Loss 119.7223 (119.7223)
2022-10-30 00:24:21,823:INFO: Dataset: zara1               Batch: 2/8	Loss 241.2548 (178.7664)
2022-10-30 00:24:22,888:INFO: Dataset: zara1               Batch: 3/8	Loss 111.0686 (157.2602)
2022-10-30 00:24:23,955:INFO: Dataset: zara1               Batch: 4/8	Loss 92.7954 (140.3234)
2022-10-30 00:24:25,019:INFO: Dataset: zara1               Batch: 5/8	Loss 98.4979 (132.7823)
2022-10-30 00:24:26,079:INFO: Dataset: zara1               Batch: 6/8	Loss 102.7389 (127.8379)
2022-10-30 00:24:27,139:INFO: Dataset: zara1               Batch: 7/8	Loss 97.5822 (123.2396)
2022-10-30 00:24:28,100:INFO: Dataset: zara1               Batch: 8/8	Loss 101.1174 (120.8062)
2022-10-30 00:24:29,500:INFO: Dataset: zara2               Batch:  1/18	Loss 136.6313 (136.6313)
2022-10-30 00:24:30,568:INFO: Dataset: zara2               Batch:  2/18	Loss 680.2197 (400.4080)
2022-10-30 00:24:31,623:INFO: Dataset: zara2               Batch:  3/18	Loss 119.5420 (302.6450)
2022-10-30 00:24:32,677:INFO: Dataset: zara2               Batch:  4/18	Loss 117.5161 (257.2360)
2022-10-30 00:24:33,729:INFO: Dataset: zara2               Batch:  5/18	Loss 133.5676 (233.3422)
2022-10-30 00:24:34,785:INFO: Dataset: zara2               Batch:  6/18	Loss 117.6367 (212.9696)
2022-10-30 00:24:35,829:INFO: Dataset: zara2               Batch:  7/18	Loss 119.6434 (199.4887)
2022-10-30 00:24:36,898:INFO: Dataset: zara2               Batch:  8/18	Loss 154.8290 (194.3075)
2022-10-30 00:24:37,958:INFO: Dataset: zara2               Batch:  9/18	Loss 132.4334 (187.3902)
2022-10-30 00:24:39,004:INFO: Dataset: zara2               Batch: 10/18	Loss 150.6381 (184.1142)
2022-10-30 00:24:40,053:INFO: Dataset: zara2               Batch: 11/18	Loss 163.0211 (182.1290)
2022-10-30 00:24:41,204:INFO: Dataset: zara2               Batch: 12/18	Loss 183.8218 (182.2716)
2022-10-30 00:24:42,271:INFO: Dataset: zara2               Batch: 13/18	Loss 622.2140 (213.6817)
2022-10-30 00:24:43,320:INFO: Dataset: zara2               Batch: 14/18	Loss 170.2455 (210.5647)
2022-10-30 00:24:44,371:INFO: Dataset: zara2               Batch: 15/18	Loss 106.7100 (203.9558)
2022-10-30 00:24:45,427:INFO: Dataset: zara2               Batch: 16/18	Loss 109.9048 (197.9363)
2022-10-30 00:24:46,498:INFO: Dataset: zara2               Batch: 17/18	Loss 207.5436 (198.5000)
2022-10-30 00:24:47,477:INFO: Dataset: zara2               Batch: 18/18	Loss 118.1458 (194.4352)
2022-10-30 00:24:47,545:INFO: - Computing ADE (validation o)
2022-10-30 00:24:47,908:INFO: 		 ADE on eth                       dataset:	 4.563477516174316
2022-10-30 00:24:47,909:INFO: Average validation o:	ADE  4.5635	FDE  8.1064
2022-10-30 00:24:47,909:INFO: - Computing ADE (validation)
2022-10-30 00:24:48,261:INFO: 		 ADE on hotel                     dataset:	 2.123492479324341
2022-10-30 00:24:48,704:INFO: 		 ADE on univ                      dataset:	 2.227853775024414
2022-10-30 00:24:49,073:INFO: 		 ADE on zara1                     dataset:	 2.1333534717559814
2022-10-30 00:24:49,631:INFO: 		 ADE on zara2                     dataset:	 2.4118690490722656
2022-10-30 00:24:49,631:INFO: Average validation:	ADE  2.2841	FDE  4.2918
2022-10-30 00:24:49,632:INFO: - Computing ADE (training)
2022-10-30 00:24:50,115:INFO: 		 ADE on hotel                     dataset:	 2.3003695011138916
2022-10-30 00:24:51,172:INFO: 		 ADE on univ                      dataset:	 2.161996364593506
2022-10-30 00:24:51,870:INFO: 		 ADE on zara1                     dataset:	 3.1233508586883545
2022-10-30 00:24:53,122:INFO: 		 ADE on zara2                     dataset:	 2.8251984119415283
2022-10-30 00:24:53,122:INFO: Average training:	ADE  2.3614	FDE  4.4346
2022-10-30 00:24:53,134:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_432.pth.tar
2022-10-30 00:24:53,134:INFO: 
===> EPOCH: 433 (P4)
2022-10-30 00:24:53,134:INFO: - Computing loss (training)
2022-10-30 00:24:54,450:INFO: Dataset: hotel               Batch: 1/4	Loss 85.7921 (85.7921)
2022-10-30 00:24:55,532:INFO: Dataset: hotel               Batch: 2/4	Loss 87.8228 (86.8362)
2022-10-30 00:24:56,616:INFO: Dataset: hotel               Batch: 3/4	Loss 85.0710 (86.2704)
2022-10-30 00:24:57,377:INFO: Dataset: hotel               Batch: 4/4	Loss 50.3796 (79.9256)
2022-10-30 00:24:58,818:INFO: Dataset: univ                Batch:  1/15	Loss 225.2624 (225.2624)
2022-10-30 00:24:59,953:INFO: Dataset: univ                Batch:  2/15	Loss 463.6669 (338.0326)
2022-10-30 00:25:01,096:INFO: Dataset: univ                Batch:  3/15	Loss 118.4930 (258.2890)
2022-10-30 00:25:02,236:INFO: Dataset: univ                Batch:  4/15	Loss 131.9231 (228.1958)
2022-10-30 00:25:03,379:INFO: Dataset: univ                Batch:  5/15	Loss 124.0330 (205.7819)
2022-10-30 00:25:04,513:INFO: Dataset: univ                Batch:  6/15	Loss 155.0645 (197.5757)
2022-10-30 00:25:05,652:INFO: Dataset: univ                Batch:  7/15	Loss 4623.8291 (872.8593)
2022-10-30 00:25:06,779:INFO: Dataset: univ                Batch:  8/15	Loss 139.3365 (789.0105)
2022-10-30 00:25:07,905:INFO: Dataset: univ                Batch:  9/15	Loss 149.7700 (721.0256)
2022-10-30 00:25:09,038:INFO: Dataset: univ                Batch: 10/15	Loss 182.4027 (663.9274)
2022-10-30 00:25:10,170:INFO: Dataset: univ                Batch: 11/15	Loss 132.2850 (616.0504)
2022-10-30 00:25:11,299:INFO: Dataset: univ                Batch: 12/15	Loss 146.5807 (578.6679)
2022-10-30 00:25:12,430:INFO: Dataset: univ                Batch: 13/15	Loss 216.1511 (551.7387)
2022-10-30 00:25:13,577:INFO: Dataset: univ                Batch: 14/15	Loss 116.9771 (518.6247)
2022-10-30 00:25:14,064:INFO: Dataset: univ                Batch: 15/15	Loss 18.5617 (512.0132)
2022-10-30 00:25:15,421:INFO: Dataset: zara1               Batch: 1/8	Loss 111.4660 (111.4660)
2022-10-30 00:25:16,465:INFO: Dataset: zara1               Batch: 2/8	Loss 99.1137 (105.6884)
2022-10-30 00:25:17,506:INFO: Dataset: zara1               Batch: 3/8	Loss 170.1696 (127.7522)
2022-10-30 00:25:18,551:INFO: Dataset: zara1               Batch: 4/8	Loss 98.7775 (120.5374)
2022-10-30 00:25:19,599:INFO: Dataset: zara1               Batch: 5/8	Loss 103.0970 (117.0716)
2022-10-30 00:25:20,641:INFO: Dataset: zara1               Batch: 6/8	Loss 128.2705 (118.9219)
2022-10-30 00:25:21,691:INFO: Dataset: zara1               Batch: 7/8	Loss 99.9617 (116.5271)
2022-10-30 00:25:22,644:INFO: Dataset: zara1               Batch: 8/8	Loss 145.7508 (119.3264)
2022-10-30 00:25:24,023:INFO: Dataset: zara2               Batch:  1/18	Loss 136.2426 (136.2426)
2022-10-30 00:25:25,105:INFO: Dataset: zara2               Batch:  2/18	Loss 124.4207 (130.2511)
2022-10-30 00:25:26,189:INFO: Dataset: zara2               Batch:  3/18	Loss 152.5610 (136.8002)
2022-10-30 00:25:27,265:INFO: Dataset: zara2               Batch:  4/18	Loss 157.8163 (141.6417)
2022-10-30 00:25:28,338:INFO: Dataset: zara2               Batch:  5/18	Loss 127.3557 (138.7743)
2022-10-30 00:25:29,419:INFO: Dataset: zara2               Batch:  6/18	Loss 139.6053 (138.9176)
2022-10-30 00:25:30,500:INFO: Dataset: zara2               Batch:  7/18	Loss 111.3430 (134.9801)
2022-10-30 00:25:31,578:INFO: Dataset: zara2               Batch:  8/18	Loss 953.2436 (230.0493)
2022-10-30 00:25:32,648:INFO: Dataset: zara2               Batch:  9/18	Loss 115.8722 (216.3272)
2022-10-30 00:25:33,719:INFO: Dataset: zara2               Batch: 10/18	Loss 121.5268 (206.8778)
2022-10-30 00:25:34,794:INFO: Dataset: zara2               Batch: 11/18	Loss 123.4934 (198.9967)
2022-10-30 00:25:35,864:INFO: Dataset: zara2               Batch: 12/18	Loss 347.4646 (210.3162)
2022-10-30 00:25:36,936:INFO: Dataset: zara2               Batch: 13/18	Loss 693.7291 (246.8084)
2022-10-30 00:25:38,010:INFO: Dataset: zara2               Batch: 14/18	Loss 142.5079 (239.5081)
2022-10-30 00:25:39,092:INFO: Dataset: zara2               Batch: 15/18	Loss 109.9880 (230.6064)
2022-10-30 00:25:40,171:INFO: Dataset: zara2               Batch: 16/18	Loss 220.8535 (230.0385)
2022-10-30 00:25:41,248:INFO: Dataset: zara2               Batch: 17/18	Loss 101.1137 (221.9695)
2022-10-30 00:25:42,218:INFO: Dataset: zara2               Batch: 18/18	Loss 92.4510 (215.6102)
2022-10-30 00:25:42,286:INFO: - Computing ADE (validation o)
2022-10-30 00:25:42,643:INFO: 		 ADE on eth                       dataset:	 4.437799453735352
2022-10-30 00:25:42,643:INFO: Average validation o:	ADE  4.4378	FDE  7.9044
2022-10-30 00:25:42,644:INFO: - Computing ADE (validation)
2022-10-30 00:25:42,993:INFO: 		 ADE on hotel                     dataset:	 2.1425886154174805
2022-10-30 00:25:43,425:INFO: 		 ADE on univ                      dataset:	 2.212822437286377
2022-10-30 00:25:43,789:INFO: 		 ADE on zara1                     dataset:	 2.129429578781128
2022-10-30 00:25:44,328:INFO: 		 ADE on zara2                     dataset:	 2.391885995864868
2022-10-30 00:25:44,329:INFO: Average validation:	ADE  2.2698	FDE  4.2466
2022-10-30 00:25:44,329:INFO: - Computing ADE (training)
2022-10-30 00:25:44,804:INFO: 		 ADE on hotel                     dataset:	 2.2807464599609375
2022-10-30 00:25:45,856:INFO: 		 ADE on univ                      dataset:	 2.1314127445220947
2022-10-30 00:25:46,554:INFO: 		 ADE on zara1                     dataset:	 3.0942373275756836
2022-10-30 00:25:47,767:INFO: 		 ADE on zara2                     dataset:	 2.811758041381836
2022-10-30 00:25:47,767:INFO: Average training:	ADE  2.3346	FDE  4.3740
2022-10-30 00:25:47,778:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_433.pth.tar
2022-10-30 00:25:47,778:INFO: 
===> EPOCH: 434 (P4)
2022-10-30 00:25:47,778:INFO: - Computing loss (training)
2022-10-30 00:25:49,082:INFO: Dataset: hotel               Batch: 1/4	Loss 85.0687 (85.0687)
2022-10-30 00:25:50,155:INFO: Dataset: hotel               Batch: 2/4	Loss 85.5594 (85.3087)
2022-10-30 00:25:51,219:INFO: Dataset: hotel               Batch: 3/4	Loss 85.4986 (85.3721)
2022-10-30 00:25:52,003:INFO: Dataset: hotel               Batch: 4/4	Loss 52.4445 (79.5077)
2022-10-30 00:25:53,479:INFO: Dataset: univ                Batch:  1/15	Loss 809.1420 (809.1420)
2022-10-30 00:25:54,616:INFO: Dataset: univ                Batch:  2/15	Loss 137.2676 (479.5819)
2022-10-30 00:25:55,767:INFO: Dataset: univ                Batch:  3/15	Loss 126.0171 (353.5016)
2022-10-30 00:25:56,906:INFO: Dataset: univ                Batch:  4/15	Loss 183.0549 (312.6565)
2022-10-30 00:25:58,040:INFO: Dataset: univ                Batch:  5/15	Loss 121.0026 (274.4173)
2022-10-30 00:25:59,183:INFO: Dataset: univ                Batch:  6/15	Loss 451.3779 (303.7049)
2022-10-30 00:26:00,325:INFO: Dataset: univ                Batch:  7/15	Loss 226.0320 (292.6120)
2022-10-30 00:26:01,462:INFO: Dataset: univ                Batch:  8/15	Loss 771.0225 (351.7364)
2022-10-30 00:26:02,596:INFO: Dataset: univ                Batch:  9/15	Loss 197.9552 (336.4113)
2022-10-30 00:26:03,731:INFO: Dataset: univ                Batch: 10/15	Loss 122.9830 (315.4044)
2022-10-30 00:26:04,868:INFO: Dataset: univ                Batch: 11/15	Loss 139.0537 (300.2566)
2022-10-30 00:26:06,009:INFO: Dataset: univ                Batch: 12/15	Loss 114.1669 (284.3559)
2022-10-30 00:26:07,155:INFO: Dataset: univ                Batch: 13/15	Loss 120.5762 (271.3847)
2022-10-30 00:26:08,303:INFO: Dataset: univ                Batch: 14/15	Loss 157.3891 (262.9172)
2022-10-30 00:26:08,797:INFO: Dataset: univ                Batch: 15/15	Loss 17.5130 (259.0911)
2022-10-30 00:26:10,164:INFO: Dataset: zara1               Batch: 1/8	Loss 99.5395 (99.5395)
2022-10-30 00:26:11,236:INFO: Dataset: zara1               Batch: 2/8	Loss 99.6460 (99.5920)
2022-10-30 00:26:12,297:INFO: Dataset: zara1               Batch: 3/8	Loss 104.7552 (101.2820)
2022-10-30 00:26:13,362:INFO: Dataset: zara1               Batch: 4/8	Loss 110.2107 (103.5877)
2022-10-30 00:26:14,424:INFO: Dataset: zara1               Batch: 5/8	Loss 94.5504 (101.8645)
2022-10-30 00:26:15,483:INFO: Dataset: zara1               Batch: 6/8	Loss 97.3426 (101.0803)
2022-10-30 00:26:16,546:INFO: Dataset: zara1               Batch: 7/8	Loss 111.8794 (102.6925)
2022-10-30 00:26:17,504:INFO: Dataset: zara1               Batch: 8/8	Loss 81.1234 (100.5129)
2022-10-30 00:26:18,883:INFO: Dataset: zara2               Batch:  1/18	Loss 95.4507 (95.4507)
2022-10-30 00:26:19,935:INFO: Dataset: zara2               Batch:  2/18	Loss 93.9179 (94.7544)
2022-10-30 00:26:20,982:INFO: Dataset: zara2               Batch:  3/18	Loss 160.0449 (117.2042)
2022-10-30 00:26:22,028:INFO: Dataset: zara2               Batch:  4/18	Loss 102.2232 (113.2212)
2022-10-30 00:26:23,078:INFO: Dataset: zara2               Batch:  5/18	Loss 126.4176 (115.9682)
2022-10-30 00:26:24,126:INFO: Dataset: zara2               Batch:  6/18	Loss 123.2373 (117.2674)
2022-10-30 00:26:25,176:INFO: Dataset: zara2               Batch:  7/18	Loss 147.3733 (121.8007)
2022-10-30 00:26:26,221:INFO: Dataset: zara2               Batch:  8/18	Loss 114.5429 (120.9895)
2022-10-30 00:26:27,270:INFO: Dataset: zara2               Batch:  9/18	Loss 97.9443 (118.4699)
2022-10-30 00:26:28,317:INFO: Dataset: zara2               Batch: 10/18	Loss 205.4698 (127.0675)
2022-10-30 00:26:29,373:INFO: Dataset: zara2               Batch: 11/18	Loss 115.7035 (126.0510)
2022-10-30 00:26:30,419:INFO: Dataset: zara2               Batch: 12/18	Loss 90.7910 (123.2678)
2022-10-30 00:26:31,463:INFO: Dataset: zara2               Batch: 13/18	Loss 178.0209 (127.6649)
2022-10-30 00:26:32,520:INFO: Dataset: zara2               Batch: 14/18	Loss 211.0552 (133.4894)
2022-10-30 00:26:33,579:INFO: Dataset: zara2               Batch: 15/18	Loss 93.9023 (130.8394)
2022-10-30 00:26:34,718:INFO: Dataset: zara2               Batch: 16/18	Loss 159.8062 (132.6818)
2022-10-30 00:26:35,773:INFO: Dataset: zara2               Batch: 17/18	Loss 136.8771 (132.9266)
2022-10-30 00:26:36,725:INFO: Dataset: zara2               Batch: 18/18	Loss 190.6714 (135.6854)
2022-10-30 00:26:36,791:INFO: - Computing ADE (validation o)
2022-10-30 00:26:37,151:INFO: 		 ADE on eth                       dataset:	 4.478308200836182
2022-10-30 00:26:37,151:INFO: Average validation o:	ADE  4.4783	FDE  7.9524
2022-10-30 00:26:37,152:INFO: - Computing ADE (validation)
2022-10-30 00:26:37,508:INFO: 		 ADE on hotel                     dataset:	 2.093616008758545
2022-10-30 00:26:37,959:INFO: 		 ADE on univ                      dataset:	 2.203603744506836
2022-10-30 00:26:38,317:INFO: 		 ADE on zara1                     dataset:	 2.1487369537353516
2022-10-30 00:26:38,855:INFO: 		 ADE on zara2                     dataset:	 2.3967766761779785
2022-10-30 00:26:38,855:INFO: Average validation:	ADE  2.2652	FDE  4.2400
2022-10-30 00:26:38,855:INFO: - Computing ADE (training)
2022-10-30 00:26:39,319:INFO: 		 ADE on hotel                     dataset:	 2.265209913253784
2022-10-30 00:26:40,412:INFO: 		 ADE on univ                      dataset:	 2.1343984603881836
2022-10-30 00:26:41,098:INFO: 		 ADE on zara1                     dataset:	 3.0792934894561768
2022-10-30 00:26:42,279:INFO: 		 ADE on zara2                     dataset:	 2.7959847450256348
2022-10-30 00:26:42,280:INFO: Average training:	ADE  2.3322	FDE  4.3687
2022-10-30 00:26:42,291:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_434.pth.tar
2022-10-30 00:26:42,291:INFO: 
===> EPOCH: 435 (P4)
2022-10-30 00:26:42,291:INFO: - Computing loss (training)
2022-10-30 00:26:43,568:INFO: Dataset: hotel               Batch: 1/4	Loss 85.1960 (85.1960)
2022-10-30 00:26:44,617:INFO: Dataset: hotel               Batch: 2/4	Loss 86.2870 (85.7389)
2022-10-30 00:26:45,661:INFO: Dataset: hotel               Batch: 3/4	Loss 83.8345 (85.1102)
2022-10-30 00:26:46,423:INFO: Dataset: hotel               Batch: 4/4	Loss 51.9441 (79.2471)
2022-10-30 00:26:47,870:INFO: Dataset: univ                Batch:  1/15	Loss 134.5167 (134.5167)
2022-10-30 00:26:48,988:INFO: Dataset: univ                Batch:  2/15	Loss 610.1476 (356.0303)
2022-10-30 00:26:50,096:INFO: Dataset: univ                Batch:  3/15	Loss 124.2984 (280.1479)
2022-10-30 00:26:51,206:INFO: Dataset: univ                Batch:  4/15	Loss 125.8722 (241.9725)
2022-10-30 00:26:52,338:INFO: Dataset: univ                Batch:  5/15	Loss 142.6933 (221.1032)
2022-10-30 00:26:53,451:INFO: Dataset: univ                Batch:  6/15	Loss 153.4443 (209.9850)
2022-10-30 00:26:54,561:INFO: Dataset: univ                Batch:  7/15	Loss 154.2905 (202.0003)
2022-10-30 00:26:55,676:INFO: Dataset: univ                Batch:  8/15	Loss 114.4159 (190.7998)
2022-10-30 00:26:56,795:INFO: Dataset: univ                Batch:  9/15	Loss 163.1656 (187.5451)
2022-10-30 00:26:57,900:INFO: Dataset: univ                Batch: 10/15	Loss 195.5277 (188.3209)
2022-10-30 00:26:59,013:INFO: Dataset: univ                Batch: 11/15	Loss 109.9992 (181.6672)
2022-10-30 00:27:00,133:INFO: Dataset: univ                Batch: 12/15	Loss 174.0219 (181.0354)
2022-10-30 00:27:01,257:INFO: Dataset: univ                Batch: 13/15	Loss 110.2348 (175.5565)
2022-10-30 00:27:02,379:INFO: Dataset: univ                Batch: 14/15	Loss 1534.9490 (271.4375)
2022-10-30 00:27:02,862:INFO: Dataset: univ                Batch: 15/15	Loss 19.3693 (267.8181)
2022-10-30 00:27:04,234:INFO: Dataset: zara1               Batch: 1/8	Loss 283.3177 (283.3177)
2022-10-30 00:27:05,304:INFO: Dataset: zara1               Batch: 2/8	Loss 95.2457 (186.2996)
2022-10-30 00:27:06,363:INFO: Dataset: zara1               Batch: 3/8	Loss 120.5340 (163.2636)
2022-10-30 00:27:07,419:INFO: Dataset: zara1               Batch: 4/8	Loss 112.0537 (150.2904)
2022-10-30 00:27:08,478:INFO: Dataset: zara1               Batch: 5/8	Loss 92.4596 (139.3300)
2022-10-30 00:27:09,544:INFO: Dataset: zara1               Batch: 6/8	Loss 140.0425 (139.4489)
2022-10-30 00:27:10,601:INFO: Dataset: zara1               Batch: 7/8	Loss 97.1486 (133.3773)
2022-10-30 00:27:11,560:INFO: Dataset: zara1               Batch: 8/8	Loss 165.8742 (137.0375)
2022-10-30 00:27:12,935:INFO: Dataset: zara2               Batch:  1/18	Loss 107.1452 (107.1452)
2022-10-30 00:27:14,004:INFO: Dataset: zara2               Batch:  2/18	Loss 50.7346 (78.7316)
2022-10-30 00:27:15,073:INFO: Dataset: zara2               Batch:  3/18	Loss 174.7698 (109.8817)
2022-10-30 00:27:16,138:INFO: Dataset: zara2               Batch:  4/18	Loss 244.9139 (142.3345)
2022-10-30 00:27:17,193:INFO: Dataset: zara2               Batch:  5/18	Loss 292.6915 (175.6189)
2022-10-30 00:27:18,248:INFO: Dataset: zara2               Batch:  6/18	Loss 169.2742 (174.5299)
2022-10-30 00:27:19,301:INFO: Dataset: zara2               Batch:  7/18	Loss 114.4041 (166.7486)
2022-10-30 00:27:20,365:INFO: Dataset: zara2               Batch:  8/18	Loss 119.3908 (160.9306)
2022-10-30 00:27:21,421:INFO: Dataset: zara2               Batch:  9/18	Loss 153.0028 (160.0844)
2022-10-30 00:27:22,477:INFO: Dataset: zara2               Batch: 10/18	Loss 128.7957 (156.8554)
2022-10-30 00:27:23,536:INFO: Dataset: zara2               Batch: 11/18	Loss 642.6964 (206.4443)
2022-10-30 00:27:24,596:INFO: Dataset: zara2               Batch: 12/18	Loss 108.3455 (198.4125)
2022-10-30 00:27:25,657:INFO: Dataset: zara2               Batch: 13/18	Loss 233.7000 (201.3485)
2022-10-30 00:27:26,722:INFO: Dataset: zara2               Batch: 14/18	Loss 188.3938 (200.3013)
2022-10-30 00:27:27,779:INFO: Dataset: zara2               Batch: 15/18	Loss 93.7360 (193.6176)
2022-10-30 00:27:28,844:INFO: Dataset: zara2               Batch: 16/18	Loss 135.3391 (190.1882)
2022-10-30 00:27:29,907:INFO: Dataset: zara2               Batch: 17/18	Loss 529.5250 (208.4909)
2022-10-30 00:27:30,872:INFO: Dataset: zara2               Batch: 18/18	Loss 88.9209 (202.8969)
2022-10-30 00:27:30,938:INFO: - Computing ADE (validation o)
2022-10-30 00:27:31,298:INFO: 		 ADE on eth                       dataset:	 4.59234094619751
2022-10-30 00:27:31,299:INFO: Average validation o:	ADE  4.5923	FDE  8.1282
2022-10-30 00:27:31,299:INFO: - Computing ADE (validation)
2022-10-30 00:27:31,647:INFO: 		 ADE on hotel                     dataset:	 2.127963066101074
2022-10-30 00:27:32,092:INFO: 		 ADE on univ                      dataset:	 2.2250776290893555
2022-10-30 00:27:32,445:INFO: 		 ADE on zara1                     dataset:	 2.127814292907715
2022-10-30 00:27:33,003:INFO: 		 ADE on zara2                     dataset:	 2.4095160961151123
2022-10-30 00:27:33,003:INFO: Average validation:	ADE  2.2818	FDE  4.2704
2022-10-30 00:27:33,003:INFO: - Computing ADE (training)
2022-10-30 00:27:33,472:INFO: 		 ADE on hotel                     dataset:	 2.274197816848755
2022-10-30 00:27:34,533:INFO: 		 ADE on univ                      dataset:	 2.145322799682617
2022-10-30 00:27:35,221:INFO: 		 ADE on zara1                     dataset:	 3.1079578399658203
2022-10-30 00:27:36,413:INFO: 		 ADE on zara2                     dataset:	 2.808875560760498
2022-10-30 00:27:36,414:INFO: Average training:	ADE  2.3446	FDE  4.3933
2022-10-30 00:27:36,425:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_435.pth.tar
2022-10-30 00:27:36,425:INFO: 
===> EPOCH: 436 (P4)
2022-10-30 00:27:36,425:INFO: - Computing loss (training)
2022-10-30 00:27:37,691:INFO: Dataset: hotel               Batch: 1/4	Loss 86.5128 (86.5128)
2022-10-30 00:27:38,747:INFO: Dataset: hotel               Batch: 2/4	Loss 86.0922 (86.3025)
2022-10-30 00:27:39,791:INFO: Dataset: hotel               Batch: 3/4	Loss 85.4331 (86.0073)
2022-10-30 00:27:40,557:INFO: Dataset: hotel               Batch: 4/4	Loss 53.4693 (80.8991)
2022-10-30 00:27:41,967:INFO: Dataset: univ                Batch:  1/15	Loss 104.8436 (104.8436)
2022-10-30 00:27:43,099:INFO: Dataset: univ                Batch:  2/15	Loss 99.2780 (101.8208)
2022-10-30 00:27:44,228:INFO: Dataset: univ                Batch:  3/15	Loss 109.7470 (104.6071)
2022-10-30 00:27:45,347:INFO: Dataset: univ                Batch:  4/15	Loss 123.6411 (109.4659)
2022-10-30 00:27:46,466:INFO: Dataset: univ                Batch:  5/15	Loss 127.6517 (113.0473)
2022-10-30 00:27:47,587:INFO: Dataset: univ                Batch:  6/15	Loss 138.6595 (117.1139)
2022-10-30 00:27:48,711:INFO: Dataset: univ                Batch:  7/15	Loss 102.2085 (114.6953)
2022-10-30 00:27:49,829:INFO: Dataset: univ                Batch:  8/15	Loss 156.7563 (119.8490)
2022-10-30 00:27:50,943:INFO: Dataset: univ                Batch:  9/15	Loss 200.2742 (128.5959)
2022-10-30 00:27:52,059:INFO: Dataset: univ                Batch: 10/15	Loss 938.2429 (208.5503)
2022-10-30 00:27:53,179:INFO: Dataset: univ                Batch: 11/15	Loss 499.3382 (235.3107)
2022-10-30 00:27:54,298:INFO: Dataset: univ                Batch: 12/15	Loss 104.8641 (225.0982)
2022-10-30 00:27:55,409:INFO: Dataset: univ                Batch: 13/15	Loss 114.8342 (217.1349)
2022-10-30 00:27:56,545:INFO: Dataset: univ                Batch: 14/15	Loss 139.0355 (211.6394)
2022-10-30 00:27:57,036:INFO: Dataset: univ                Batch: 15/15	Loss 22.7529 (208.6049)
2022-10-30 00:27:58,438:INFO: Dataset: zara1               Batch: 1/8	Loss 106.3618 (106.3618)
2022-10-30 00:27:59,526:INFO: Dataset: zara1               Batch: 2/8	Loss 127.2434 (117.2858)
2022-10-30 00:28:00,604:INFO: Dataset: zara1               Batch: 3/8	Loss 134.8832 (122.9840)
2022-10-30 00:28:01,680:INFO: Dataset: zara1               Batch: 4/8	Loss 95.4241 (116.0310)
2022-10-30 00:28:02,756:INFO: Dataset: zara1               Batch: 5/8	Loss 92.0294 (111.1645)
2022-10-30 00:28:03,833:INFO: Dataset: zara1               Batch: 6/8	Loss 107.0636 (110.5010)
2022-10-30 00:28:04,906:INFO: Dataset: zara1               Batch: 7/8	Loss 140.6230 (114.2483)
2022-10-30 00:28:05,865:INFO: Dataset: zara1               Batch: 8/8	Loss 90.3821 (111.4848)
2022-10-30 00:28:07,236:INFO: Dataset: zara2               Batch:  1/18	Loss 113.7876 (113.7876)
2022-10-30 00:28:08,299:INFO: Dataset: zara2               Batch:  2/18	Loss 119.7040 (116.7947)
2022-10-30 00:28:09,365:INFO: Dataset: zara2               Batch:  3/18	Loss 104.9314 (113.0478)
2022-10-30 00:28:10,432:INFO: Dataset: zara2               Batch:  4/18	Loss 297.5473 (162.4052)
2022-10-30 00:28:11,490:INFO: Dataset: zara2               Batch:  5/18	Loss 114.6234 (153.1216)
2022-10-30 00:28:12,551:INFO: Dataset: zara2               Batch:  6/18	Loss 146.2616 (151.9794)
2022-10-30 00:28:13,607:INFO: Dataset: zara2               Batch:  7/18	Loss 119.8973 (147.1754)
2022-10-30 00:28:14,665:INFO: Dataset: zara2               Batch:  8/18	Loss 100.7568 (141.3730)
2022-10-30 00:28:15,720:INFO: Dataset: zara2               Batch:  9/18	Loss 131.0999 (140.1378)
2022-10-30 00:28:16,779:INFO: Dataset: zara2               Batch: 10/18	Loss 127.5162 (138.9763)
2022-10-30 00:28:17,835:INFO: Dataset: zara2               Batch: 11/18	Loss 215.6222 (146.4453)
2022-10-30 00:28:18,904:INFO: Dataset: zara2               Batch: 12/18	Loss 350.6652 (163.1412)
2022-10-30 00:28:19,971:INFO: Dataset: zara2               Batch: 13/18	Loss 235.7846 (168.9853)
2022-10-30 00:28:21,023:INFO: Dataset: zara2               Batch: 14/18	Loss 111.0743 (164.6620)
2022-10-30 00:28:22,082:INFO: Dataset: zara2               Batch: 15/18	Loss 317.3369 (174.0237)
2022-10-30 00:28:23,143:INFO: Dataset: zara2               Batch: 16/18	Loss 104.5550 (169.3176)
2022-10-30 00:28:24,204:INFO: Dataset: zara2               Batch: 17/18	Loss 197.8780 (171.1064)
2022-10-30 00:28:25,253:INFO: Dataset: zara2               Batch: 18/18	Loss 110.4930 (168.0902)
2022-10-30 00:28:25,320:INFO: - Computing ADE (validation o)
2022-10-30 00:28:25,688:INFO: 		 ADE on eth                       dataset:	 4.548740386962891
2022-10-30 00:28:25,688:INFO: Average validation o:	ADE  4.5487	FDE  8.0705
2022-10-30 00:28:25,689:INFO: - Computing ADE (validation)
2022-10-30 00:28:26,049:INFO: 		 ADE on hotel                     dataset:	 2.1355690956115723
2022-10-30 00:28:26,495:INFO: 		 ADE on univ                      dataset:	 2.2088074684143066
2022-10-30 00:28:26,850:INFO: 		 ADE on zara1                     dataset:	 2.1326804161071777
2022-10-30 00:28:27,380:INFO: 		 ADE on zara2                     dataset:	 2.4022576808929443
2022-10-30 00:28:27,380:INFO: Average validation:	ADE  2.2713	FDE  4.2572
2022-10-30 00:28:27,381:INFO: - Computing ADE (training)
2022-10-30 00:28:27,854:INFO: 		 ADE on hotel                     dataset:	 2.291741371154785
2022-10-30 00:28:28,928:INFO: 		 ADE on univ                      dataset:	 2.143612861633301
2022-10-30 00:28:29,619:INFO: 		 ADE on zara1                     dataset:	 3.093647003173828
2022-10-30 00:28:30,846:INFO: 		 ADE on zara2                     dataset:	 2.8229198455810547
2022-10-30 00:28:30,846:INFO: Average training:	ADE  2.3458	FDE  4.3967
2022-10-30 00:28:30,857:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_436.pth.tar
2022-10-30 00:28:30,857:INFO: 
===> EPOCH: 437 (P4)
2022-10-30 00:28:30,858:INFO: - Computing loss (training)
2022-10-30 00:28:32,135:INFO: Dataset: hotel               Batch: 1/4	Loss 84.7925 (84.7925)
2022-10-30 00:28:33,183:INFO: Dataset: hotel               Batch: 2/4	Loss 85.3555 (85.0787)
2022-10-30 00:28:34,227:INFO: Dataset: hotel               Batch: 3/4	Loss 85.7075 (85.2893)
2022-10-30 00:28:34,994:INFO: Dataset: hotel               Batch: 4/4	Loss 52.3949 (79.7346)
2022-10-30 00:28:36,417:INFO: Dataset: univ                Batch:  1/15	Loss 170.1018 (170.1018)
2022-10-30 00:28:37,537:INFO: Dataset: univ                Batch:  2/15	Loss 107.4541 (138.2621)
2022-10-30 00:28:38,651:INFO: Dataset: univ                Batch:  3/15	Loss 176.7030 (151.1934)
2022-10-30 00:28:39,756:INFO: Dataset: univ                Batch:  4/15	Loss 106.0034 (141.3322)
2022-10-30 00:28:40,868:INFO: Dataset: univ                Batch:  5/15	Loss 167.2650 (146.6147)
2022-10-30 00:28:41,977:INFO: Dataset: univ                Batch:  6/15	Loss 126.1943 (143.2807)
2022-10-30 00:28:43,092:INFO: Dataset: univ                Batch:  7/15	Loss 115.7776 (139.1325)
2022-10-30 00:28:44,199:INFO: Dataset: univ                Batch:  8/15	Loss 140.0015 (139.2390)
2022-10-30 00:28:45,309:INFO: Dataset: univ                Batch:  9/15	Loss 111.4466 (136.1277)
2022-10-30 00:28:46,427:INFO: Dataset: univ                Batch: 10/15	Loss 205.7267 (143.3840)
2022-10-30 00:28:47,544:INFO: Dataset: univ                Batch: 11/15	Loss 123.0880 (141.4992)
2022-10-30 00:28:48,652:INFO: Dataset: univ                Batch: 12/15	Loss 116.2764 (139.4991)
2022-10-30 00:28:49,774:INFO: Dataset: univ                Batch: 13/15	Loss 106.3391 (136.8445)
2022-10-30 00:28:50,890:INFO: Dataset: univ                Batch: 14/15	Loss 236.6614 (143.9849)
2022-10-30 00:28:51,370:INFO: Dataset: univ                Batch: 15/15	Loss 26.6531 (142.1556)
2022-10-30 00:28:52,704:INFO: Dataset: zara1               Batch: 1/8	Loss 100.6146 (100.6146)
2022-10-30 00:28:53,755:INFO: Dataset: zara1               Batch: 2/8	Loss 92.6647 (96.7727)
2022-10-30 00:28:54,810:INFO: Dataset: zara1               Batch: 3/8	Loss 92.4834 (95.3752)
2022-10-30 00:28:55,864:INFO: Dataset: zara1               Batch: 4/8	Loss 119.8256 (101.8242)
2022-10-30 00:28:56,919:INFO: Dataset: zara1               Batch: 5/8	Loss 94.9512 (100.4212)
2022-10-30 00:28:57,968:INFO: Dataset: zara1               Batch: 6/8	Loss 232.1729 (123.8716)
2022-10-30 00:28:59,018:INFO: Dataset: zara1               Batch: 7/8	Loss 211.6390 (135.6874)
2022-10-30 00:28:59,973:INFO: Dataset: zara1               Batch: 8/8	Loss 85.5823 (130.4396)
2022-10-30 00:29:01,337:INFO: Dataset: zara2               Batch:  1/18	Loss 99.4376 (99.4376)
2022-10-30 00:29:02,393:INFO: Dataset: zara2               Batch:  2/18	Loss 106.9129 (103.3410)
2022-10-30 00:29:03,452:INFO: Dataset: zara2               Batch:  3/18	Loss 97.6541 (101.4801)
2022-10-30 00:29:04,523:INFO: Dataset: zara2               Batch:  4/18	Loss 97.2843 (100.4433)
2022-10-30 00:29:05,590:INFO: Dataset: zara2               Batch:  5/18	Loss 109.5923 (102.2261)
2022-10-30 00:29:06,646:INFO: Dataset: zara2               Batch:  6/18	Loss 125.2769 (105.7663)
2022-10-30 00:29:07,716:INFO: Dataset: zara2               Batch:  7/18	Loss 89.1755 (103.3571)
2022-10-30 00:29:08,783:INFO: Dataset: zara2               Batch:  8/18	Loss 128.5425 (106.4201)
2022-10-30 00:29:09,843:INFO: Dataset: zara2               Batch:  9/18	Loss 111.3218 (106.9674)
2022-10-30 00:29:10,914:INFO: Dataset: zara2               Batch: 10/18	Loss 167.2269 (113.2759)
2022-10-30 00:29:11,981:INFO: Dataset: zara2               Batch: 11/18	Loss 261.8606 (127.5004)
2022-10-30 00:29:13,042:INFO: Dataset: zara2               Batch: 12/18	Loss 175.7309 (131.1831)
2022-10-30 00:29:14,105:INFO: Dataset: zara2               Batch: 13/18	Loss 92.9396 (128.4783)
2022-10-30 00:29:15,173:INFO: Dataset: zara2               Batch: 14/18	Loss 234.2786 (135.9502)
2022-10-30 00:29:16,228:INFO: Dataset: zara2               Batch: 15/18	Loss 241.9359 (143.9346)
2022-10-30 00:29:17,290:INFO: Dataset: zara2               Batch: 16/18	Loss 148.0959 (144.1803)
2022-10-30 00:29:18,340:INFO: Dataset: zara2               Batch: 17/18	Loss 364.9755 (157.3604)
2022-10-30 00:29:19,294:INFO: Dataset: zara2               Batch: 18/18	Loss 101.7730 (154.5759)
2022-10-30 00:29:19,359:INFO: - Computing ADE (validation o)
2022-10-30 00:29:19,712:INFO: 		 ADE on eth                       dataset:	 4.492092609405518
2022-10-30 00:29:19,713:INFO: Average validation o:	ADE  4.4921	FDE  7.9857
2022-10-30 00:29:19,714:INFO: - Computing ADE (validation)
2022-10-30 00:29:20,072:INFO: 		 ADE on hotel                     dataset:	 2.1478896141052246
2022-10-30 00:29:20,507:INFO: 		 ADE on univ                      dataset:	 2.2117013931274414
2022-10-30 00:29:20,859:INFO: 		 ADE on zara1                     dataset:	 2.1281402111053467
2022-10-30 00:29:21,404:INFO: 		 ADE on zara2                     dataset:	 2.421419858932495
2022-10-30 00:29:21,405:INFO: Average validation:	ADE  2.2803	FDE  4.2670
2022-10-30 00:29:21,405:INFO: - Computing ADE (training)
2022-10-30 00:29:21,870:INFO: 		 ADE on hotel                     dataset:	 2.2738583087921143
2022-10-30 00:29:22,941:INFO: 		 ADE on univ                      dataset:	 2.134526491165161
2022-10-30 00:29:23,635:INFO: 		 ADE on zara1                     dataset:	 3.1247639656066895
2022-10-30 00:29:24,815:INFO: 		 ADE on zara2                     dataset:	 2.8053858280181885
2022-10-30 00:29:24,815:INFO: Average training:	ADE  2.3373	FDE  4.3844
2022-10-30 00:29:24,826:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_437.pth.tar
2022-10-30 00:29:24,826:INFO: 
===> EPOCH: 438 (P4)
2022-10-30 00:29:24,826:INFO: - Computing loss (training)
2022-10-30 00:29:26,109:INFO: Dataset: hotel               Batch: 1/4	Loss 84.7793 (84.7793)
2022-10-30 00:29:27,181:INFO: Dataset: hotel               Batch: 2/4	Loss 85.7860 (85.2827)
2022-10-30 00:29:28,263:INFO: Dataset: hotel               Batch: 3/4	Loss 85.8917 (85.4927)
2022-10-30 00:29:29,047:INFO: Dataset: hotel               Batch: 4/4	Loss 51.4056 (80.0963)
2022-10-30 00:29:30,491:INFO: Dataset: univ                Batch:  1/15	Loss 102.0644 (102.0644)
2022-10-30 00:29:31,637:INFO: Dataset: univ                Batch:  2/15	Loss 235.8839 (172.4112)
2022-10-30 00:29:32,773:INFO: Dataset: univ                Batch:  3/15	Loss 241.1387 (195.4929)
2022-10-30 00:29:33,890:INFO: Dataset: univ                Batch:  4/15	Loss 121.9573 (179.2970)
2022-10-30 00:29:35,018:INFO: Dataset: univ                Batch:  5/15	Loss 219.8745 (187.6475)
2022-10-30 00:29:36,146:INFO: Dataset: univ                Batch:  6/15	Loss 123.8472 (177.2326)
2022-10-30 00:29:37,278:INFO: Dataset: univ                Batch:  7/15	Loss 105.8390 (166.4717)
2022-10-30 00:29:38,398:INFO: Dataset: univ                Batch:  8/15	Loss 105.5148 (159.1002)
2022-10-30 00:29:39,529:INFO: Dataset: univ                Batch:  9/15	Loss 109.3673 (153.2020)
2022-10-30 00:29:40,658:INFO: Dataset: univ                Batch: 10/15	Loss 96.5933 (147.3922)
2022-10-30 00:29:41,775:INFO: Dataset: univ                Batch: 11/15	Loss 599.5248 (187.8051)
2022-10-30 00:29:42,906:INFO: Dataset: univ                Batch: 12/15	Loss 134.7012 (183.0697)
2022-10-30 00:29:44,024:INFO: Dataset: univ                Batch: 13/15	Loss 124.7033 (178.7713)
2022-10-30 00:29:45,155:INFO: Dataset: univ                Batch: 14/15	Loss 178.5418 (178.7538)
2022-10-30 00:29:45,645:INFO: Dataset: univ                Batch: 15/15	Loss 105.2061 (177.6420)
2022-10-30 00:29:46,996:INFO: Dataset: zara1               Batch: 1/8	Loss 92.0856 (92.0856)
2022-10-30 00:29:48,067:INFO: Dataset: zara1               Batch: 2/8	Loss 104.0369 (97.9859)
2022-10-30 00:29:49,138:INFO: Dataset: zara1               Batch: 3/8	Loss 110.4637 (101.9909)
2022-10-30 00:29:50,221:INFO: Dataset: zara1               Batch: 4/8	Loss 94.0770 (99.7092)
2022-10-30 00:29:51,296:INFO: Dataset: zara1               Batch: 5/8	Loss 99.6472 (99.6976)
2022-10-30 00:29:52,366:INFO: Dataset: zara1               Batch: 6/8	Loss 98.3852 (99.4806)
2022-10-30 00:29:53,440:INFO: Dataset: zara1               Batch: 7/8	Loss 143.6475 (105.7902)
2022-10-30 00:29:54,417:INFO: Dataset: zara1               Batch: 8/8	Loss 98.4604 (104.9955)
2022-10-30 00:29:55,786:INFO: Dataset: zara2               Batch:  1/18	Loss 95.0493 (95.0493)
2022-10-30 00:29:56,842:INFO: Dataset: zara2               Batch:  2/18	Loss 186.1460 (140.4584)
2022-10-30 00:29:57,902:INFO: Dataset: zara2               Batch:  3/18	Loss 110.2073 (130.3541)
2022-10-30 00:29:58,963:INFO: Dataset: zara2               Batch:  4/18	Loss 135.8947 (131.8008)
2022-10-30 00:30:00,026:INFO: Dataset: zara2               Batch:  5/18	Loss 360.0111 (177.0850)
2022-10-30 00:30:01,086:INFO: Dataset: zara2               Batch:  6/18	Loss 172.2313 (176.2834)
2022-10-30 00:30:02,148:INFO: Dataset: zara2               Batch:  7/18	Loss 103.6730 (165.6696)
2022-10-30 00:30:03,207:INFO: Dataset: zara2               Batch:  8/18	Loss 189.0381 (168.7336)
2022-10-30 00:30:04,265:INFO: Dataset: zara2               Batch:  9/18	Loss 94.3971 (160.9074)
2022-10-30 00:30:05,329:INFO: Dataset: zara2               Batch: 10/18	Loss 118.0406 (156.2808)
2022-10-30 00:30:06,388:INFO: Dataset: zara2               Batch: 11/18	Loss 98.7338 (150.9135)
2022-10-30 00:30:07,443:INFO: Dataset: zara2               Batch: 12/18	Loss 100.1939 (147.0500)
2022-10-30 00:30:08,484:INFO: Dataset: zara2               Batch: 13/18	Loss 132.7822 (145.8400)
2022-10-30 00:30:09,530:INFO: Dataset: zara2               Batch: 14/18	Loss 175.9701 (148.0187)
2022-10-30 00:30:10,675:INFO: Dataset: zara2               Batch: 15/18	Loss 137.6349 (147.3492)
2022-10-30 00:30:11,732:INFO: Dataset: zara2               Batch: 16/18	Loss 132.4102 (146.3535)
2022-10-30 00:30:12,791:INFO: Dataset: zara2               Batch: 17/18	Loss 182.8905 (148.5173)
2022-10-30 00:30:13,750:INFO: Dataset: zara2               Batch: 18/18	Loss 89.3174 (145.5030)
2022-10-30 00:30:13,817:INFO: - Computing ADE (validation o)
2022-10-30 00:30:14,171:INFO: 		 ADE on eth                       dataset:	 4.508112907409668
2022-10-30 00:30:14,171:INFO: Average validation o:	ADE  4.5081	FDE  8.0143
2022-10-30 00:30:14,172:INFO: - Computing ADE (validation)
2022-10-30 00:30:14,518:INFO: 		 ADE on hotel                     dataset:	 2.1217920780181885
2022-10-30 00:30:14,956:INFO: 		 ADE on univ                      dataset:	 2.2160372734069824
2022-10-30 00:30:15,307:INFO: 		 ADE on zara1                     dataset:	 2.1581904888153076
2022-10-30 00:30:15,848:INFO: 		 ADE on zara2                     dataset:	 2.4354259967803955
2022-10-30 00:30:15,848:INFO: Average validation:	ADE  2.2880	FDE  4.2803
2022-10-30 00:30:15,849:INFO: - Computing ADE (training)
2022-10-30 00:30:16,328:INFO: 		 ADE on hotel                     dataset:	 2.2582995891571045
2022-10-30 00:30:17,400:INFO: 		 ADE on univ                      dataset:	 2.1443731784820557
2022-10-30 00:30:18,104:INFO: 		 ADE on zara1                     dataset:	 3.118682622909546
2022-10-30 00:30:19,343:INFO: 		 ADE on zara2                     dataset:	 2.813992977142334
2022-10-30 00:30:19,343:INFO: Average training:	ADE  2.3453	FDE  4.3966
2022-10-30 00:30:19,354:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_438.pth.tar
2022-10-30 00:30:19,354:INFO: 
===> EPOCH: 439 (P4)
2022-10-30 00:30:19,354:INFO: - Computing loss (training)
2022-10-30 00:30:20,650:INFO: Dataset: hotel               Batch: 1/4	Loss 86.9094 (86.9094)
2022-10-30 00:30:21,720:INFO: Dataset: hotel               Batch: 2/4	Loss 83.7847 (85.3395)
2022-10-30 00:30:22,788:INFO: Dataset: hotel               Batch: 3/4	Loss 84.8862 (85.1860)
2022-10-30 00:30:23,579:INFO: Dataset: hotel               Batch: 4/4	Loss 53.4791 (79.6645)
2022-10-30 00:30:25,027:INFO: Dataset: univ                Batch:  1/15	Loss 95.3102 (95.3102)
2022-10-30 00:30:26,165:INFO: Dataset: univ                Batch:  2/15	Loss 144.2257 (118.0407)
2022-10-30 00:30:27,296:INFO: Dataset: univ                Batch:  3/15	Loss 112.6717 (116.3478)
2022-10-30 00:30:28,417:INFO: Dataset: univ                Batch:  4/15	Loss 110.7164 (115.0327)
2022-10-30 00:30:29,546:INFO: Dataset: univ                Batch:  5/15	Loss 123.3632 (116.7664)
2022-10-30 00:30:30,678:INFO: Dataset: univ                Batch:  6/15	Loss 390.7679 (162.4231)
2022-10-30 00:30:31,799:INFO: Dataset: univ                Batch:  7/15	Loss 143.7986 (159.8274)
2022-10-30 00:30:32,929:INFO: Dataset: univ                Batch:  8/15	Loss 177.3766 (162.0940)
2022-10-30 00:30:34,052:INFO: Dataset: univ                Batch:  9/15	Loss 137.7169 (159.4329)
2022-10-30 00:30:35,190:INFO: Dataset: univ                Batch: 10/15	Loss 116.3670 (154.9711)
2022-10-30 00:30:36,316:INFO: Dataset: univ                Batch: 11/15	Loss 143.7788 (154.0166)
2022-10-30 00:30:37,451:INFO: Dataset: univ                Batch: 12/15	Loss 108.8607 (150.1700)
2022-10-30 00:30:38,578:INFO: Dataset: univ                Batch: 13/15	Loss 96.8143 (146.2007)
2022-10-30 00:30:39,692:INFO: Dataset: univ                Batch: 14/15	Loss 100.9493 (143.0653)
2022-10-30 00:30:40,176:INFO: Dataset: univ                Batch: 15/15	Loss 17.1927 (140.9895)
2022-10-30 00:30:41,557:INFO: Dataset: zara1               Batch: 1/8	Loss 121.5315 (121.5315)
2022-10-30 00:30:42,632:INFO: Dataset: zara1               Batch: 2/8	Loss 220.3601 (172.6360)
2022-10-30 00:30:43,691:INFO: Dataset: zara1               Batch: 3/8	Loss 114.9688 (153.3878)
2022-10-30 00:30:44,753:INFO: Dataset: zara1               Batch: 4/8	Loss 96.1491 (140.1245)
2022-10-30 00:30:45,815:INFO: Dataset: zara1               Batch: 5/8	Loss 96.6402 (131.0801)
2022-10-30 00:30:46,876:INFO: Dataset: zara1               Batch: 6/8	Loss 90.4045 (124.8677)
2022-10-30 00:30:47,936:INFO: Dataset: zara1               Batch: 7/8	Loss 94.9429 (120.6715)
2022-10-30 00:30:48,903:INFO: Dataset: zara1               Batch: 8/8	Loss 80.1367 (116.0420)
2022-10-30 00:30:50,286:INFO: Dataset: zara2               Batch:  1/18	Loss 100.9700 (100.9700)
2022-10-30 00:30:51,346:INFO: Dataset: zara2               Batch:  2/18	Loss 962.0923 (527.5567)
2022-10-30 00:30:52,393:INFO: Dataset: zara2               Batch:  3/18	Loss 104.0038 (389.2985)
2022-10-30 00:30:53,453:INFO: Dataset: zara2               Batch:  4/18	Loss 97.1767 (308.0200)
2022-10-30 00:30:54,513:INFO: Dataset: zara2               Batch:  5/18	Loss 96.6371 (265.7688)
2022-10-30 00:30:55,577:INFO: Dataset: zara2               Batch:  6/18	Loss 336.2519 (277.3043)
2022-10-30 00:30:56,639:INFO: Dataset: zara2               Batch:  7/18	Loss 92.8983 (249.8830)
2022-10-30 00:30:57,699:INFO: Dataset: zara2               Batch:  8/18	Loss 177.3080 (241.3673)
2022-10-30 00:30:58,759:INFO: Dataset: zara2               Batch:  9/18	Loss 183.4836 (234.8906)
2022-10-30 00:30:59,816:INFO: Dataset: zara2               Batch: 10/18	Loss 216.1320 (232.9273)
2022-10-30 00:31:00,869:INFO: Dataset: zara2               Batch: 11/18	Loss 623.9470 (271.4072)
2022-10-30 00:31:01,926:INFO: Dataset: zara2               Batch: 12/18	Loss 97.6174 (254.5275)
2022-10-30 00:31:02,976:INFO: Dataset: zara2               Batch: 13/18	Loss 102.5765 (242.5977)
2022-10-30 00:31:04,029:INFO: Dataset: zara2               Batch: 14/18	Loss 106.8791 (233.7471)
2022-10-30 00:31:05,098:INFO: Dataset: zara2               Batch: 15/18	Loss 98.3136 (225.0815)
2022-10-30 00:31:06,159:INFO: Dataset: zara2               Batch: 16/18	Loss 111.8786 (218.4114)
2022-10-30 00:31:07,223:INFO: Dataset: zara2               Batch: 17/18	Loss 104.5302 (211.5335)
2022-10-30 00:31:08,175:INFO: Dataset: zara2               Batch: 18/18	Loss 101.4841 (206.3121)
2022-10-30 00:31:08,241:INFO: - Computing ADE (validation o)
2022-10-30 00:31:08,588:INFO: 		 ADE on eth                       dataset:	 4.525676727294922
2022-10-30 00:31:08,589:INFO: Average validation o:	ADE  4.5257	FDE  8.0436
2022-10-30 00:31:08,589:INFO: - Computing ADE (validation)
2022-10-30 00:31:08,942:INFO: 		 ADE on hotel                     dataset:	 2.124208688735962
2022-10-30 00:31:09,387:INFO: 		 ADE on univ                      dataset:	 2.22444486618042
2022-10-30 00:31:09,744:INFO: 		 ADE on zara1                     dataset:	 2.1669957637786865
2022-10-30 00:31:10,280:INFO: 		 ADE on zara2                     dataset:	 2.4028844833374023
2022-10-30 00:31:10,280:INFO: Average validation:	ADE  2.2811	FDE  4.2750
2022-10-30 00:31:10,280:INFO: - Computing ADE (training)
2022-10-30 00:31:10,753:INFO: 		 ADE on hotel                     dataset:	 2.2703614234924316
2022-10-30 00:31:11,823:INFO: 		 ADE on univ                      dataset:	 2.1546080112457275
2022-10-30 00:31:12,522:INFO: 		 ADE on zara1                     dataset:	 3.0866174697875977
2022-10-30 00:31:13,716:INFO: 		 ADE on zara2                     dataset:	 2.8164894580841064
2022-10-30 00:31:13,717:INFO: Average training:	ADE  2.3513	FDE  4.4078
2022-10-30 00:31:13,728:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_439.pth.tar
2022-10-30 00:31:13,728:INFO: 
===> EPOCH: 440 (P4)
2022-10-30 00:31:13,728:INFO: - Computing loss (training)
2022-10-30 00:31:15,039:INFO: Dataset: hotel               Batch: 1/4	Loss 86.0581 (86.0581)
2022-10-30 00:31:16,118:INFO: Dataset: hotel               Batch: 2/4	Loss 86.2753 (86.1649)
2022-10-30 00:31:17,191:INFO: Dataset: hotel               Batch: 3/4	Loss 84.5202 (85.6123)
2022-10-30 00:31:17,977:INFO: Dataset: hotel               Batch: 4/4	Loss 51.3194 (79.7309)
2022-10-30 00:31:19,433:INFO: Dataset: univ                Batch:  1/15	Loss 110.2468 (110.2468)
2022-10-30 00:31:20,550:INFO: Dataset: univ                Batch:  2/15	Loss 6179.4165 (2930.2840)
2022-10-30 00:31:21,673:INFO: Dataset: univ                Batch:  3/15	Loss 108.0255 (2020.8305)
2022-10-30 00:31:22,791:INFO: Dataset: univ                Batch:  4/15	Loss 108.7331 (1558.0453)
2022-10-30 00:31:23,907:INFO: Dataset: univ                Batch:  5/15	Loss 182.5518 (1290.5736)
2022-10-30 00:31:25,022:INFO: Dataset: univ                Batch:  6/15	Loss 120.2496 (1102.9903)
2022-10-30 00:31:26,127:INFO: Dataset: univ                Batch:  7/15	Loss 110.4484 (975.3155)
2022-10-30 00:31:27,235:INFO: Dataset: univ                Batch:  8/15	Loss 115.1919 (875.3953)
2022-10-30 00:31:28,347:INFO: Dataset: univ                Batch:  9/15	Loss 105.4213 (789.0330)
2022-10-30 00:31:29,466:INFO: Dataset: univ                Batch: 10/15	Loss 148.0888 (721.9330)
2022-10-30 00:31:30,581:INFO: Dataset: univ                Batch: 11/15	Loss 207.8639 (679.3545)
2022-10-30 00:31:31,701:INFO: Dataset: univ                Batch: 12/15	Loss 173.5175 (640.0138)
2022-10-30 00:31:32,811:INFO: Dataset: univ                Batch: 13/15	Loss 107.7717 (602.6013)
2022-10-30 00:31:33,932:INFO: Dataset: univ                Batch: 14/15	Loss 154.6164 (571.1894)
2022-10-30 00:31:34,424:INFO: Dataset: univ                Batch: 15/15	Loss 16.6605 (563.8315)
2022-10-30 00:31:35,808:INFO: Dataset: zara1               Batch: 1/8	Loss 95.6631 (95.6631)
2022-10-30 00:31:36,893:INFO: Dataset: zara1               Batch: 2/8	Loss 98.1973 (96.8040)
2022-10-30 00:31:37,968:INFO: Dataset: zara1               Batch: 3/8	Loss 92.7913 (95.4121)
2022-10-30 00:31:39,037:INFO: Dataset: zara1               Batch: 4/8	Loss 95.4843 (95.4294)
2022-10-30 00:31:40,108:INFO: Dataset: zara1               Batch: 5/8	Loss 125.0155 (101.2733)
2022-10-30 00:31:41,182:INFO: Dataset: zara1               Batch: 6/8	Loss 92.2456 (99.7687)
2022-10-30 00:31:42,249:INFO: Dataset: zara1               Batch: 7/8	Loss 93.7315 (98.8910)
2022-10-30 00:31:43,220:INFO: Dataset: zara1               Batch: 8/8	Loss 89.5800 (97.9060)
2022-10-30 00:31:44,622:INFO: Dataset: zara2               Batch:  1/18	Loss 116.2981 (116.2981)
2022-10-30 00:31:45,691:INFO: Dataset: zara2               Batch:  2/18	Loss 96.1054 (106.7398)
2022-10-30 00:31:46,770:INFO: Dataset: zara2               Batch:  3/18	Loss 94.6985 (102.5296)
2022-10-30 00:31:47,843:INFO: Dataset: zara2               Batch:  4/18	Loss 207.4609 (132.2141)
2022-10-30 00:31:48,913:INFO: Dataset: zara2               Batch:  5/18	Loss 96.5224 (124.8603)
2022-10-30 00:31:49,987:INFO: Dataset: zara2               Batch:  6/18	Loss 132.4660 (126.1566)
2022-10-30 00:31:51,058:INFO: Dataset: zara2               Batch:  7/18	Loss 97.6370 (121.9636)
2022-10-30 00:31:52,136:INFO: Dataset: zara2               Batch:  8/18	Loss 108.4251 (120.3091)
2022-10-30 00:31:53,212:INFO: Dataset: zara2               Batch:  9/18	Loss 111.2728 (119.2003)
2022-10-30 00:31:54,290:INFO: Dataset: zara2               Batch: 10/18	Loss 96.2237 (116.9577)
2022-10-30 00:31:55,366:INFO: Dataset: zara2               Batch: 11/18	Loss 298.9724 (134.0010)
2022-10-30 00:31:56,456:INFO: Dataset: zara2               Batch: 12/18	Loss 281.6507 (144.9943)
2022-10-30 00:31:57,539:INFO: Dataset: zara2               Batch: 13/18	Loss 115.2925 (142.6436)
2022-10-30 00:31:58,616:INFO: Dataset: zara2               Batch: 14/18	Loss 84.9059 (138.9947)
2022-10-30 00:31:59,678:INFO: Dataset: zara2               Batch: 15/18	Loss 343.5860 (151.6032)
2022-10-30 00:32:00,748:INFO: Dataset: zara2               Batch: 16/18	Loss 396.8257 (165.1194)
2022-10-30 00:32:01,830:INFO: Dataset: zara2               Batch: 17/18	Loss 134.2075 (163.2926)
2022-10-30 00:32:02,797:INFO: Dataset: zara2               Batch: 18/18	Loss 94.1727 (160.4931)
2022-10-30 00:32:02,863:INFO: - Computing ADE (validation o)
2022-10-30 00:32:03,210:INFO: 		 ADE on eth                       dataset:	 4.342319488525391
2022-10-30 00:32:03,211:INFO: Average validation o:	ADE  4.3423	FDE  7.6617
2022-10-30 00:32:03,211:INFO: - Computing ADE (validation)
2022-10-30 00:32:03,545:INFO: 		 ADE on hotel                     dataset:	 1.9887632131576538
2022-10-30 00:32:04,004:INFO: 		 ADE on univ                      dataset:	 2.1161086559295654
2022-10-30 00:32:04,361:INFO: 		 ADE on zara1                     dataset:	 2.1555135250091553
2022-10-30 00:32:04,925:INFO: 		 ADE on zara2                     dataset:	 2.296579360961914
2022-10-30 00:32:04,925:INFO: Average validation:	ADE  2.1776	FDE  4.0443
2022-10-30 00:32:04,925:INFO: - Computing ADE (training)
2022-10-30 00:32:05,400:INFO: 		 ADE on hotel                     dataset:	 2.172924518585205
2022-10-30 00:32:06,457:INFO: 		 ADE on univ                      dataset:	 2.0511715412139893
2022-10-30 00:32:07,179:INFO: 		 ADE on zara1                     dataset:	 3.033292531967163
2022-10-30 00:32:08,378:INFO: 		 ADE on zara2                     dataset:	 2.709097385406494
2022-10-30 00:32:08,379:INFO: Average training:	ADE  2.2504	FDE  4.1807
2022-10-30 00:32:08,390:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_440.pth.tar
2022-10-30 00:32:08,390:INFO: 
===> EPOCH: 441 (P4)
2022-10-30 00:32:08,390:INFO: - Computing loss (training)
2022-10-30 00:32:09,687:INFO: Dataset: hotel               Batch: 1/4	Loss 85.1959 (85.1959)
2022-10-30 00:32:10,852:INFO: Dataset: hotel               Batch: 2/4	Loss 84.0270 (84.5911)
2022-10-30 00:32:11,907:INFO: Dataset: hotel               Batch: 3/4	Loss 85.0061 (84.7292)
2022-10-30 00:32:12,683:INFO: Dataset: hotel               Batch: 4/4	Loss 51.0356 (79.7507)
2022-10-30 00:32:14,098:INFO: Dataset: univ                Batch:  1/15	Loss 129.6126 (129.6126)
2022-10-30 00:32:15,208:INFO: Dataset: univ                Batch:  2/15	Loss 120.4583 (124.8717)
2022-10-30 00:32:16,323:INFO: Dataset: univ                Batch:  3/15	Loss 377.5854 (212.0606)
2022-10-30 00:32:17,435:INFO: Dataset: univ                Batch:  4/15	Loss 357.6180 (249.4172)
2022-10-30 00:32:18,551:INFO: Dataset: univ                Batch:  5/15	Loss 97.7791 (215.6352)
2022-10-30 00:32:19,672:INFO: Dataset: univ                Batch:  6/15	Loss 164.7061 (206.3373)
2022-10-30 00:32:20,776:INFO: Dataset: univ                Batch:  7/15	Loss 130.7837 (195.3902)
2022-10-30 00:32:21,896:INFO: Dataset: univ                Batch:  8/15	Loss 92.0873 (181.6905)
2022-10-30 00:32:23,015:INFO: Dataset: univ                Batch:  9/15	Loss 224.4389 (186.7240)
2022-10-30 00:32:24,140:INFO: Dataset: univ                Batch: 10/15	Loss 12325.0146 (1407.8079)
2022-10-30 00:32:25,267:INFO: Dataset: univ                Batch: 11/15	Loss 107.2962 (1297.7996)
2022-10-30 00:32:26,396:INFO: Dataset: univ                Batch: 12/15	Loss 118.2827 (1196.3365)
2022-10-30 00:32:27,527:INFO: Dataset: univ                Batch: 13/15	Loss 109.8934 (1115.1187)
2022-10-30 00:32:28,669:INFO: Dataset: univ                Batch: 14/15	Loss 194.6661 (1045.9896)
2022-10-30 00:32:29,152:INFO: Dataset: univ                Batch: 15/15	Loss 21.2988 (1031.2277)
2022-10-30 00:32:30,536:INFO: Dataset: zara1               Batch: 1/8	Loss 96.6913 (96.6913)
2022-10-30 00:32:31,616:INFO: Dataset: zara1               Batch: 2/8	Loss 107.5197 (102.5061)
2022-10-30 00:32:32,705:INFO: Dataset: zara1               Batch: 3/8	Loss 92.1326 (98.6575)
2022-10-30 00:32:33,796:INFO: Dataset: zara1               Batch: 4/8	Loss 91.3517 (97.0214)
2022-10-30 00:32:34,874:INFO: Dataset: zara1               Batch: 5/8	Loss 95.2265 (96.6475)
2022-10-30 00:32:35,949:INFO: Dataset: zara1               Batch: 6/8	Loss 98.2868 (96.9246)
2022-10-30 00:32:37,022:INFO: Dataset: zara1               Batch: 7/8	Loss 93.5894 (96.4590)
2022-10-30 00:32:37,996:INFO: Dataset: zara1               Batch: 8/8	Loss 83.7099 (95.1975)
2022-10-30 00:32:39,386:INFO: Dataset: zara2               Batch:  1/18	Loss 126.6590 (126.6590)
2022-10-30 00:32:40,452:INFO: Dataset: zara2               Batch:  2/18	Loss 159.8865 (143.5017)
2022-10-30 00:32:41,523:INFO: Dataset: zara2               Batch:  3/18	Loss 116.7787 (134.5306)
2022-10-30 00:32:42,598:INFO: Dataset: zara2               Batch:  4/18	Loss 232.5533 (159.8316)
2022-10-30 00:32:43,662:INFO: Dataset: zara2               Batch:  5/18	Loss 146.9851 (157.3263)
2022-10-30 00:32:44,742:INFO: Dataset: zara2               Batch:  6/18	Loss 91.4989 (147.4658)
2022-10-30 00:32:45,817:INFO: Dataset: zara2               Batch:  7/18	Loss 122.5564 (143.9136)
2022-10-30 00:32:46,891:INFO: Dataset: zara2               Batch:  8/18	Loss 257.7398 (158.9241)
2022-10-30 00:32:47,963:INFO: Dataset: zara2               Batch:  9/18	Loss 127.4100 (155.1207)
2022-10-30 00:32:49,034:INFO: Dataset: zara2               Batch: 10/18	Loss 90.0831 (148.8973)
2022-10-30 00:32:50,101:INFO: Dataset: zara2               Batch: 11/18	Loss 173.0545 (151.2021)
2022-10-30 00:32:51,174:INFO: Dataset: zara2               Batch: 12/18	Loss 114.1664 (147.5382)
2022-10-30 00:32:52,245:INFO: Dataset: zara2               Batch: 13/18	Loss 116.9100 (145.1654)
2022-10-30 00:32:53,315:INFO: Dataset: zara2               Batch: 14/18	Loss 110.9834 (142.5394)
2022-10-30 00:32:54,383:INFO: Dataset: zara2               Batch: 15/18	Loss 116.7169 (140.8206)
2022-10-30 00:32:55,461:INFO: Dataset: zara2               Batch: 16/18	Loss 113.6607 (139.1446)
2022-10-30 00:32:56,536:INFO: Dataset: zara2               Batch: 17/18	Loss 166.5052 (140.8695)
2022-10-30 00:32:57,506:INFO: Dataset: zara2               Batch: 18/18	Loss 138.9327 (140.7712)
2022-10-30 00:32:57,571:INFO: - Computing ADE (validation o)
2022-10-30 00:32:57,935:INFO: 		 ADE on eth                       dataset:	 4.52978515625
2022-10-30 00:32:57,935:INFO: Average validation o:	ADE  4.5298	FDE  8.0357
2022-10-30 00:32:57,936:INFO: - Computing ADE (validation)
2022-10-30 00:32:58,286:INFO: 		 ADE on hotel                     dataset:	 2.1375439167022705
2022-10-30 00:32:58,730:INFO: 		 ADE on univ                      dataset:	 2.2158632278442383
2022-10-30 00:32:59,090:INFO: 		 ADE on zara1                     dataset:	 2.1227941513061523
2022-10-30 00:32:59,629:INFO: 		 ADE on zara2                     dataset:	 2.4206743240356445
2022-10-30 00:32:59,629:INFO: Average validation:	ADE  2.2813	FDE  4.2629
2022-10-30 00:32:59,630:INFO: - Computing ADE (training)
2022-10-30 00:33:00,104:INFO: 		 ADE on hotel                     dataset:	 2.287416934967041
2022-10-30 00:33:01,168:INFO: 		 ADE on univ                      dataset:	 2.1355209350585938
2022-10-30 00:33:01,883:INFO: 		 ADE on zara1                     dataset:	 3.120767831802368
2022-10-30 00:33:03,093:INFO: 		 ADE on zara2                     dataset:	 2.807565927505493
2022-10-30 00:33:03,093:INFO: Average training:	ADE  2.3386	FDE  4.3776
2022-10-30 00:33:03,104:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_441.pth.tar
2022-10-30 00:33:03,104:INFO: 
===> EPOCH: 442 (P4)
2022-10-30 00:33:03,104:INFO: - Computing loss (training)
2022-10-30 00:33:04,389:INFO: Dataset: hotel               Batch: 1/4	Loss 83.7107 (83.7107)
2022-10-30 00:33:05,443:INFO: Dataset: hotel               Batch: 2/4	Loss 83.5663 (83.6390)
2022-10-30 00:33:06,492:INFO: Dataset: hotel               Batch: 3/4	Loss 86.0346 (84.4638)
2022-10-30 00:33:07,260:INFO: Dataset: hotel               Batch: 4/4	Loss 54.0541 (79.6897)
2022-10-30 00:33:08,680:INFO: Dataset: univ                Batch:  1/15	Loss 101.4765 (101.4765)
2022-10-30 00:33:09,792:INFO: Dataset: univ                Batch:  2/15	Loss 115.2801 (108.2420)
2022-10-30 00:33:10,913:INFO: Dataset: univ                Batch:  3/15	Loss 97.4878 (104.6864)
2022-10-30 00:33:12,037:INFO: Dataset: univ                Batch:  4/15	Loss 131.6213 (111.2298)
2022-10-30 00:33:13,166:INFO: Dataset: univ                Batch:  5/15	Loss 234.7065 (138.0659)
2022-10-30 00:33:14,296:INFO: Dataset: univ                Batch:  6/15	Loss 104.0981 (132.0078)
2022-10-30 00:33:15,413:INFO: Dataset: univ                Batch:  7/15	Loss 113.3341 (129.4031)
2022-10-30 00:33:16,530:INFO: Dataset: univ                Batch:  8/15	Loss 99.1939 (125.6285)
2022-10-30 00:33:17,646:INFO: Dataset: univ                Batch:  9/15	Loss 129.5313 (126.0491)
2022-10-30 00:33:18,777:INFO: Dataset: univ                Batch: 10/15	Loss 113.2218 (124.7100)
2022-10-30 00:33:19,910:INFO: Dataset: univ                Batch: 11/15	Loss 102.4446 (122.6103)
2022-10-30 00:33:21,027:INFO: Dataset: univ                Batch: 12/15	Loss 184.4667 (127.7656)
2022-10-30 00:33:22,137:INFO: Dataset: univ                Batch: 13/15	Loss 248.8673 (136.7084)
2022-10-30 00:33:23,276:INFO: Dataset: univ                Batch: 14/15	Loss 95.8434 (133.4592)
2022-10-30 00:33:23,760:INFO: Dataset: univ                Batch: 15/15	Loss 315.0715 (136.0411)
2022-10-30 00:33:25,132:INFO: Dataset: zara1               Batch: 1/8	Loss 102.7898 (102.7898)
2022-10-30 00:33:26,199:INFO: Dataset: zara1               Batch: 2/8	Loss 101.8610 (102.3109)
2022-10-30 00:33:27,269:INFO: Dataset: zara1               Batch: 3/8	Loss 91.7379 (98.7817)
2022-10-30 00:33:28,335:INFO: Dataset: zara1               Batch: 4/8	Loss 96.2455 (98.1083)
2022-10-30 00:33:29,400:INFO: Dataset: zara1               Batch: 5/8	Loss 98.4817 (98.1806)
2022-10-30 00:33:30,471:INFO: Dataset: zara1               Batch: 6/8	Loss 96.4457 (97.8962)
2022-10-30 00:33:31,538:INFO: Dataset: zara1               Batch: 7/8	Loss 109.1221 (99.4715)
2022-10-30 00:33:32,512:INFO: Dataset: zara1               Batch: 8/8	Loss 102.1664 (99.7609)
2022-10-30 00:33:33,882:INFO: Dataset: zara2               Batch:  1/18	Loss 100.5026 (100.5026)
2022-10-30 00:33:34,945:INFO: Dataset: zara2               Batch:  2/18	Loss 102.5714 (101.5201)
2022-10-30 00:33:36,008:INFO: Dataset: zara2               Batch:  3/18	Loss 114.1878 (105.6539)
2022-10-30 00:33:37,073:INFO: Dataset: zara2               Batch:  4/18	Loss 98.3817 (103.7393)
2022-10-30 00:33:38,132:INFO: Dataset: zara2               Batch:  5/18	Loss 103.5829 (103.7070)
2022-10-30 00:33:39,196:INFO: Dataset: zara2               Batch:  6/18	Loss 90.8571 (101.6591)
2022-10-30 00:33:40,247:INFO: Dataset: zara2               Batch:  7/18	Loss 108.4064 (102.6038)
2022-10-30 00:33:41,314:INFO: Dataset: zara2               Batch:  8/18	Loss 112.0533 (103.8737)
2022-10-30 00:33:42,375:INFO: Dataset: zara2               Batch:  9/18	Loss 107.1843 (104.2364)
2022-10-30 00:33:43,444:INFO: Dataset: zara2               Batch: 10/18	Loss 178.3322 (111.6503)
2022-10-30 00:33:44,510:INFO: Dataset: zara2               Batch: 11/18	Loss 101.1179 (110.6847)
2022-10-30 00:33:45,561:INFO: Dataset: zara2               Batch: 12/18	Loss 117.5874 (111.2302)
2022-10-30 00:33:46,625:INFO: Dataset: zara2               Batch: 13/18	Loss 103.0894 (110.6185)
2022-10-30 00:33:47,686:INFO: Dataset: zara2               Batch: 14/18	Loss 118.2768 (111.1882)
2022-10-30 00:33:48,759:INFO: Dataset: zara2               Batch: 15/18	Loss 206.2763 (117.2142)
2022-10-30 00:33:49,831:INFO: Dataset: zara2               Batch: 16/18	Loss 137.5885 (118.5413)
2022-10-30 00:33:50,904:INFO: Dataset: zara2               Batch: 17/18	Loss 117.4365 (118.4789)
2022-10-30 00:33:51,872:INFO: Dataset: zara2               Batch: 18/18	Loss 143.7263 (119.6852)
2022-10-30 00:33:51,938:INFO: - Computing ADE (validation o)
2022-10-30 00:33:52,289:INFO: 		 ADE on eth                       dataset:	 4.51283597946167
2022-10-30 00:33:52,290:INFO: Average validation o:	ADE  4.5128	FDE  8.0161
2022-10-30 00:33:52,290:INFO: - Computing ADE (validation)
2022-10-30 00:33:52,643:INFO: 		 ADE on hotel                     dataset:	 2.1532487869262695
2022-10-30 00:33:53,081:INFO: 		 ADE on univ                      dataset:	 2.209042549133301
2022-10-30 00:33:53,448:INFO: 		 ADE on zara1                     dataset:	 2.152421712875366
2022-10-30 00:33:53,995:INFO: 		 ADE on zara2                     dataset:	 2.4139227867126465
2022-10-30 00:33:53,995:INFO: Average validation:	ADE  2.2778	FDE  4.2609
2022-10-30 00:33:53,996:INFO: - Computing ADE (training)
2022-10-30 00:33:54,472:INFO: 		 ADE on hotel                     dataset:	 2.296388864517212
2022-10-30 00:33:55,529:INFO: 		 ADE on univ                      dataset:	 2.1443634033203125
2022-10-30 00:33:56,224:INFO: 		 ADE on zara1                     dataset:	 3.1026594638824463
2022-10-30 00:33:57,429:INFO: 		 ADE on zara2                     dataset:	 2.805769205093384
2022-10-30 00:33:57,430:INFO: Average training:	ADE  2.3435	FDE  4.3887
2022-10-30 00:33:57,440:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_442.pth.tar
2022-10-30 00:33:57,440:INFO: 
===> EPOCH: 443 (P4)
2022-10-30 00:33:57,440:INFO: - Computing loss (training)
2022-10-30 00:33:58,804:INFO: Dataset: hotel               Batch: 1/4	Loss 84.8703 (84.8703)
2022-10-30 00:33:59,851:INFO: Dataset: hotel               Batch: 2/4	Loss 85.8019 (85.3253)
2022-10-30 00:34:00,892:INFO: Dataset: hotel               Batch: 3/4	Loss 85.0626 (85.2394)
2022-10-30 00:34:01,655:INFO: Dataset: hotel               Batch: 4/4	Loss 52.5862 (80.2423)
2022-10-30 00:34:03,093:INFO: Dataset: univ                Batch:  1/15	Loss 90.0406 (90.0406)
2022-10-30 00:34:04,206:INFO: Dataset: univ                Batch:  2/15	Loss 95.2956 (92.7914)
2022-10-30 00:34:05,322:INFO: Dataset: univ                Batch:  3/15	Loss 162.3602 (116.6532)
2022-10-30 00:34:06,440:INFO: Dataset: univ                Batch:  4/15	Loss 96.1172 (111.2258)
2022-10-30 00:34:07,560:INFO: Dataset: univ                Batch:  5/15	Loss 120.0504 (113.0507)
2022-10-30 00:34:08,684:INFO: Dataset: univ                Batch:  6/15	Loss 96.0183 (110.0690)
2022-10-30 00:34:09,802:INFO: Dataset: univ                Batch:  7/15	Loss 96.6764 (107.9900)
2022-10-30 00:34:10,914:INFO: Dataset: univ                Batch:  8/15	Loss 194.9126 (118.4279)
2022-10-30 00:34:12,022:INFO: Dataset: univ                Batch:  9/15	Loss 107.7224 (117.2562)
2022-10-30 00:34:13,128:INFO: Dataset: univ                Batch: 10/15	Loss 114.8638 (117.0271)
2022-10-30 00:34:14,236:INFO: Dataset: univ                Batch: 11/15	Loss 96.9590 (115.2956)
2022-10-30 00:34:15,372:INFO: Dataset: univ                Batch: 12/15	Loss 96.8742 (113.6095)
2022-10-30 00:34:16,497:INFO: Dataset: univ                Batch: 13/15	Loss 108.8283 (113.2513)
2022-10-30 00:34:17,633:INFO: Dataset: univ                Batch: 14/15	Loss 100.5359 (112.3763)
2022-10-30 00:34:18,119:INFO: Dataset: univ                Batch: 15/15	Loss 17.8889 (111.5927)
2022-10-30 00:34:19,493:INFO: Dataset: zara1               Batch: 1/8	Loss 96.9308 (96.9308)
2022-10-30 00:34:20,548:INFO: Dataset: zara1               Batch: 2/8	Loss 111.1471 (104.2117)
2022-10-30 00:34:21,587:INFO: Dataset: zara1               Batch: 3/8	Loss 200.1303 (134.2555)
2022-10-30 00:34:22,631:INFO: Dataset: zara1               Batch: 4/8	Loss 116.9730 (130.3425)
2022-10-30 00:34:23,679:INFO: Dataset: zara1               Batch: 5/8	Loss 94.6149 (124.0325)
2022-10-30 00:34:24,724:INFO: Dataset: zara1               Batch: 6/8	Loss 99.2706 (120.1848)
2022-10-30 00:34:25,774:INFO: Dataset: zara1               Batch: 7/8	Loss 110.7728 (118.9287)
2022-10-30 00:34:26,719:INFO: Dataset: zara1               Batch: 8/8	Loss 90.1858 (115.4645)
2022-10-30 00:34:28,117:INFO: Dataset: zara2               Batch:  1/18	Loss 90.9547 (90.9547)
2022-10-30 00:34:29,190:INFO: Dataset: zara2               Batch:  2/18	Loss 452.6026 (268.6155)
2022-10-30 00:34:30,287:INFO: Dataset: zara2               Batch:  3/18	Loss 145.3388 (225.1198)
2022-10-30 00:34:31,367:INFO: Dataset: zara2               Batch:  4/18	Loss 106.4186 (197.9930)
2022-10-30 00:34:32,442:INFO: Dataset: zara2               Batch:  5/18	Loss 124.4464 (182.8586)
2022-10-30 00:34:33,519:INFO: Dataset: zara2               Batch:  6/18	Loss 93.4495 (169.0865)
2022-10-30 00:34:34,594:INFO: Dataset: zara2               Batch:  7/18	Loss 101.9614 (159.7353)
2022-10-30 00:34:35,670:INFO: Dataset: zara2               Batch:  8/18	Loss 94.9338 (151.2500)
2022-10-30 00:34:36,736:INFO: Dataset: zara2               Batch:  9/18	Loss 92.5722 (144.5233)
2022-10-30 00:34:37,808:INFO: Dataset: zara2               Batch: 10/18	Loss 98.8011 (139.8686)
2022-10-30 00:34:38,889:INFO: Dataset: zara2               Batch: 11/18	Loss 95.3985 (135.9740)
2022-10-30 00:34:39,966:INFO: Dataset: zara2               Batch: 12/18	Loss 141.3842 (136.3950)
2022-10-30 00:34:41,034:INFO: Dataset: zara2               Batch: 13/18	Loss 236.4772 (142.8564)
2022-10-30 00:34:42,102:INFO: Dataset: zara2               Batch: 14/18	Loss 133.8707 (142.2039)
2022-10-30 00:34:43,171:INFO: Dataset: zara2               Batch: 15/18	Loss 97.2418 (139.2600)
2022-10-30 00:34:44,236:INFO: Dataset: zara2               Batch: 16/18	Loss 111.5809 (137.4099)
2022-10-30 00:34:45,309:INFO: Dataset: zara2               Batch: 17/18	Loss 163.6389 (138.9590)
2022-10-30 00:34:46,279:INFO: Dataset: zara2               Batch: 18/18	Loss 92.9920 (136.6109)
2022-10-30 00:34:46,343:INFO: - Computing ADE (validation o)
2022-10-30 00:34:46,683:INFO: 		 ADE on eth                       dataset:	 4.454913139343262
2022-10-30 00:34:46,683:INFO: Average validation o:	ADE  4.4549	FDE  7.9309
2022-10-30 00:34:46,684:INFO: - Computing ADE (validation)
2022-10-30 00:34:47,031:INFO: 		 ADE on hotel                     dataset:	 2.1242244243621826
2022-10-30 00:34:47,472:INFO: 		 ADE on univ                      dataset:	 2.2259418964385986
2022-10-30 00:34:47,836:INFO: 		 ADE on zara1                     dataset:	 2.1753056049346924
2022-10-30 00:34:48,395:INFO: 		 ADE on zara2                     dataset:	 2.393846273422241
2022-10-30 00:34:48,395:INFO: Average validation:	ADE  2.2790	FDE  4.2672
2022-10-30 00:34:48,396:INFO: - Computing ADE (training)
2022-10-30 00:34:48,878:INFO: 		 ADE on hotel                     dataset:	 2.2809970378875732
2022-10-30 00:34:49,986:INFO: 		 ADE on univ                      dataset:	 2.1428911685943604
2022-10-30 00:34:50,691:INFO: 		 ADE on zara1                     dataset:	 3.1157028675079346
2022-10-30 00:34:51,915:INFO: 		 ADE on zara2                     dataset:	 2.809091567993164
2022-10-30 00:34:51,915:INFO: Average training:	ADE  2.3436	FDE  4.3917
2022-10-30 00:34:51,927:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_443.pth.tar
2022-10-30 00:34:51,927:INFO: 
===> EPOCH: 444 (P4)
2022-10-30 00:34:51,927:INFO: - Computing loss (training)
2022-10-30 00:34:53,218:INFO: Dataset: hotel               Batch: 1/4	Loss 84.9362 (84.9362)
2022-10-30 00:34:54,285:INFO: Dataset: hotel               Batch: 2/4	Loss 86.1707 (85.5549)
2022-10-30 00:34:55,346:INFO: Dataset: hotel               Batch: 3/4	Loss 85.5094 (85.5401)
2022-10-30 00:34:56,126:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9372 (79.6969)
2022-10-30 00:34:57,571:INFO: Dataset: univ                Batch:  1/15	Loss 125.0712 (125.0712)
2022-10-30 00:34:58,709:INFO: Dataset: univ                Batch:  2/15	Loss 100.4773 (112.4603)
2022-10-30 00:34:59,819:INFO: Dataset: univ                Batch:  3/15	Loss 115.3064 (113.4359)
2022-10-30 00:35:00,936:INFO: Dataset: univ                Batch:  4/15	Loss 117.1020 (114.3881)
2022-10-30 00:35:02,041:INFO: Dataset: univ                Batch:  5/15	Loss 104.3462 (112.4825)
2022-10-30 00:35:03,154:INFO: Dataset: univ                Batch:  6/15	Loss 172.0156 (121.9782)
2022-10-30 00:35:04,267:INFO: Dataset: univ                Batch:  7/15	Loss 150.8999 (126.3532)
2022-10-30 00:35:05,380:INFO: Dataset: univ                Batch:  8/15	Loss 119.4928 (125.4977)
2022-10-30 00:35:06,487:INFO: Dataset: univ                Batch:  9/15	Loss 116.2481 (124.5308)
2022-10-30 00:35:07,596:INFO: Dataset: univ                Batch: 10/15	Loss 141.9900 (126.2505)
2022-10-30 00:35:08,714:INFO: Dataset: univ                Batch: 11/15	Loss 341.3252 (146.2802)
2022-10-30 00:35:09,824:INFO: Dataset: univ                Batch: 12/15	Loss 201.1155 (150.5717)
2022-10-30 00:35:10,931:INFO: Dataset: univ                Batch: 13/15	Loss 102.3319 (146.7836)
2022-10-30 00:35:12,048:INFO: Dataset: univ                Batch: 14/15	Loss 96.5991 (142.9394)
2022-10-30 00:35:12,528:INFO: Dataset: univ                Batch: 15/15	Loss 18.3247 (141.0556)
2022-10-30 00:35:13,905:INFO: Dataset: zara1               Batch: 1/8	Loss 94.4297 (94.4297)
2022-10-30 00:35:14,982:INFO: Dataset: zara1               Batch: 2/8	Loss 171.1059 (132.7678)
2022-10-30 00:35:16,058:INFO: Dataset: zara1               Batch: 3/8	Loss 91.3591 (118.2464)
2022-10-30 00:35:17,131:INFO: Dataset: zara1               Batch: 4/8	Loss 101.2263 (113.8402)
2022-10-30 00:35:18,210:INFO: Dataset: zara1               Batch: 5/8	Loss 116.1346 (114.2897)
2022-10-30 00:35:19,282:INFO: Dataset: zara1               Batch: 6/8	Loss 96.7636 (111.3588)
2022-10-30 00:35:20,356:INFO: Dataset: zara1               Batch: 7/8	Loss 107.8082 (110.8715)
2022-10-30 00:35:21,323:INFO: Dataset: zara1               Batch: 8/8	Loss 111.3694 (110.9226)
2022-10-30 00:35:22,698:INFO: Dataset: zara2               Batch:  1/18	Loss 114.5203 (114.5203)
2022-10-30 00:35:23,758:INFO: Dataset: zara2               Batch:  2/18	Loss 99.0497 (107.0741)
2022-10-30 00:35:24,826:INFO: Dataset: zara2               Batch:  3/18	Loss 95.1423 (103.1247)
2022-10-30 00:35:25,885:INFO: Dataset: zara2               Batch:  4/18	Loss 92.4618 (100.4490)
2022-10-30 00:35:26,941:INFO: Dataset: zara2               Batch:  5/18	Loss 94.9943 (99.3237)
2022-10-30 00:35:27,998:INFO: Dataset: zara2               Batch:  6/18	Loss 558.8034 (173.3446)
2022-10-30 00:35:29,057:INFO: Dataset: zara2               Batch:  7/18	Loss 88.0701 (160.8256)
2022-10-30 00:35:30,116:INFO: Dataset: zara2               Batch:  8/18	Loss 98.7586 (152.8801)
2022-10-30 00:35:31,163:INFO: Dataset: zara2               Batch:  9/18	Loss 132.4590 (150.3915)
2022-10-30 00:35:32,221:INFO: Dataset: zara2               Batch: 10/18	Loss 97.6934 (145.2472)
2022-10-30 00:35:33,279:INFO: Dataset: zara2               Batch: 11/18	Loss 137.3654 (144.5558)
2022-10-30 00:35:34,345:INFO: Dataset: zara2               Batch: 12/18	Loss 99.0231 (140.7605)
2022-10-30 00:35:35,407:INFO: Dataset: zara2               Batch: 13/18	Loss 98.9903 (137.5730)
2022-10-30 00:35:36,465:INFO: Dataset: zara2               Batch: 14/18	Loss 148.9910 (138.3466)
2022-10-30 00:35:37,519:INFO: Dataset: zara2               Batch: 15/18	Loss 2920.2820 (336.9000)
2022-10-30 00:35:38,587:INFO: Dataset: zara2               Batch: 16/18	Loss 100.5571 (323.1158)
2022-10-30 00:35:39,645:INFO: Dataset: zara2               Batch: 17/18	Loss 124.3051 (311.2765)
2022-10-30 00:35:40,614:INFO: Dataset: zara2               Batch: 18/18	Loss 93.3998 (300.2548)
2022-10-30 00:35:40,679:INFO: - Computing ADE (validation o)
2022-10-30 00:35:41,033:INFO: 		 ADE on eth                       dataset:	 4.536066055297852
2022-10-30 00:35:41,033:INFO: Average validation o:	ADE  4.5361	FDE  8.0518
2022-10-30 00:35:41,034:INFO: - Computing ADE (validation)
2022-10-30 00:35:41,388:INFO: 		 ADE on hotel                     dataset:	 2.1248176097869873
2022-10-30 00:35:41,843:INFO: 		 ADE on univ                      dataset:	 2.230823040008545
2022-10-30 00:35:42,200:INFO: 		 ADE on zara1                     dataset:	 2.117245674133301
2022-10-30 00:35:42,743:INFO: 		 ADE on zara2                     dataset:	 2.41692852973938
2022-10-30 00:35:42,744:INFO: Average validation:	ADE  2.2867	FDE  4.2769
2022-10-30 00:35:42,744:INFO: - Computing ADE (training)
2022-10-30 00:35:43,216:INFO: 		 ADE on hotel                     dataset:	 2.2764878273010254
2022-10-30 00:35:44,292:INFO: 		 ADE on univ                      dataset:	 2.1518795490264893
2022-10-30 00:35:45,006:INFO: 		 ADE on zara1                     dataset:	 3.118124008178711
2022-10-30 00:35:46,198:INFO: 		 ADE on zara2                     dataset:	 2.8234004974365234
2022-10-30 00:35:46,199:INFO: Average training:	ADE  2.3529	FDE  4.4080
2022-10-30 00:35:46,210:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_444.pth.tar
2022-10-30 00:35:46,210:INFO: 
===> EPOCH: 445 (P4)
2022-10-30 00:35:46,210:INFO: - Computing loss (training)
2022-10-30 00:35:47,485:INFO: Dataset: hotel               Batch: 1/4	Loss 84.8989 (84.8989)
2022-10-30 00:35:48,540:INFO: Dataset: hotel               Batch: 2/4	Loss 85.0364 (84.9665)
2022-10-30 00:35:49,592:INFO: Dataset: hotel               Batch: 3/4	Loss 86.0798 (85.3323)
2022-10-30 00:35:50,366:INFO: Dataset: hotel               Batch: 4/4	Loss 51.8264 (79.8069)
2022-10-30 00:35:51,814:INFO: Dataset: univ                Batch:  1/15	Loss 157.5794 (157.5794)
2022-10-30 00:35:52,961:INFO: Dataset: univ                Batch:  2/15	Loss 93.3397 (123.3073)
2022-10-30 00:35:54,104:INFO: Dataset: univ                Batch:  3/15	Loss 119.5697 (122.0881)
2022-10-30 00:35:55,267:INFO: Dataset: univ                Batch:  4/15	Loss 122.2025 (122.1189)
2022-10-30 00:35:56,424:INFO: Dataset: univ                Batch:  5/15	Loss 93.6100 (116.3553)
2022-10-30 00:35:57,650:INFO: Dataset: univ                Batch:  6/15	Loss 117.2924 (116.5063)
2022-10-30 00:35:58,788:INFO: Dataset: univ                Batch:  7/15	Loss 120.0074 (116.9252)
2022-10-30 00:35:59,924:INFO: Dataset: univ                Batch:  8/15	Loss 93.2824 (114.2457)
2022-10-30 00:36:01,061:INFO: Dataset: univ                Batch:  9/15	Loss 261.3962 (130.4566)
2022-10-30 00:36:02,182:INFO: Dataset: univ                Batch: 10/15	Loss 105.6225 (128.2722)
2022-10-30 00:36:03,306:INFO: Dataset: univ                Batch: 11/15	Loss 91.0638 (124.9059)
2022-10-30 00:36:04,419:INFO: Dataset: univ                Batch: 12/15	Loss 99.9195 (122.9383)
2022-10-30 00:36:05,533:INFO: Dataset: univ                Batch: 13/15	Loss 96.0873 (120.8315)
2022-10-30 00:36:06,645:INFO: Dataset: univ                Batch: 14/15	Loss 99.9849 (119.3725)
2022-10-30 00:36:07,128:INFO: Dataset: univ                Batch: 15/15	Loss 16.6763 (117.9173)
2022-10-30 00:36:08,489:INFO: Dataset: zara1               Batch: 1/8	Loss 118.8373 (118.8373)
2022-10-30 00:36:09,531:INFO: Dataset: zara1               Batch: 2/8	Loss 105.7626 (112.8315)
2022-10-30 00:36:10,584:INFO: Dataset: zara1               Batch: 3/8	Loss 97.9548 (107.8726)
2022-10-30 00:36:11,632:INFO: Dataset: zara1               Batch: 4/8	Loss 95.2002 (104.9522)
2022-10-30 00:36:12,682:INFO: Dataset: zara1               Batch: 5/8	Loss 115.0008 (107.0825)
2022-10-30 00:36:13,732:INFO: Dataset: zara1               Batch: 6/8	Loss 92.0591 (104.6945)
2022-10-30 00:36:14,789:INFO: Dataset: zara1               Batch: 7/8	Loss 96.5714 (103.4978)
2022-10-30 00:36:15,754:INFO: Dataset: zara1               Batch: 8/8	Loss 78.1637 (100.7911)
2022-10-30 00:36:17,126:INFO: Dataset: zara2               Batch:  1/18	Loss 92.6349 (92.6349)
2022-10-30 00:36:18,190:INFO: Dataset: zara2               Batch:  2/18	Loss 94.5224 (93.4949)
2022-10-30 00:36:19,253:INFO: Dataset: zara2               Batch:  3/18	Loss 103.6595 (97.0013)
2022-10-30 00:36:20,321:INFO: Dataset: zara2               Batch:  4/18	Loss 104.8503 (99.1660)
2022-10-30 00:36:21,375:INFO: Dataset: zara2               Batch:  5/18	Loss 93.6307 (98.1875)
2022-10-30 00:36:22,428:INFO: Dataset: zara2               Batch:  6/18	Loss 92.1509 (97.1363)
2022-10-30 00:36:23,482:INFO: Dataset: zara2               Batch:  7/18	Loss 144.5079 (103.1367)
2022-10-30 00:36:24,550:INFO: Dataset: zara2               Batch:  8/18	Loss 428.1802 (144.5058)
2022-10-30 00:36:25,606:INFO: Dataset: zara2               Batch:  9/18	Loss 100.9997 (139.8193)
2022-10-30 00:36:26,663:INFO: Dataset: zara2               Batch: 10/18	Loss 93.2254 (135.4489)
2022-10-30 00:36:27,738:INFO: Dataset: zara2               Batch: 11/18	Loss 124.3196 (134.5551)
2022-10-30 00:36:28,811:INFO: Dataset: zara2               Batch: 12/18	Loss 108.8027 (132.3692)
2022-10-30 00:36:29,890:INFO: Dataset: zara2               Batch: 13/18	Loss 102.5636 (129.9997)
2022-10-30 00:36:30,956:INFO: Dataset: zara2               Batch: 14/18	Loss 106.9429 (128.2702)
2022-10-30 00:36:32,022:INFO: Dataset: zara2               Batch: 15/18	Loss 94.2844 (126.1360)
2022-10-30 00:36:33,097:INFO: Dataset: zara2               Batch: 16/18	Loss 89.7282 (123.9658)
2022-10-30 00:36:34,166:INFO: Dataset: zara2               Batch: 17/18	Loss 95.3269 (122.2744)
2022-10-30 00:36:35,141:INFO: Dataset: zara2               Batch: 18/18	Loss 88.3553 (120.4464)
2022-10-30 00:36:35,208:INFO: - Computing ADE (validation o)
2022-10-30 00:36:35,561:INFO: 		 ADE on eth                       dataset:	 4.477963924407959
2022-10-30 00:36:35,561:INFO: Average validation o:	ADE  4.4780	FDE  7.9645
2022-10-30 00:36:35,562:INFO: - Computing ADE (validation)
2022-10-30 00:36:35,916:INFO: 		 ADE on hotel                     dataset:	 2.1396238803863525
2022-10-30 00:36:36,366:INFO: 		 ADE on univ                      dataset:	 2.226278305053711
2022-10-30 00:36:36,721:INFO: 		 ADE on zara1                     dataset:	 2.1743266582489014
2022-10-30 00:36:37,260:INFO: 		 ADE on zara2                     dataset:	 2.415010452270508
2022-10-30 00:36:37,260:INFO: Average validation:	ADE  2.2877	FDE  4.2809
2022-10-30 00:36:37,261:INFO: - Computing ADE (training)
2022-10-30 00:36:37,727:INFO: 		 ADE on hotel                     dataset:	 2.276393175125122
2022-10-30 00:36:38,820:INFO: 		 ADE on univ                      dataset:	 2.1480562686920166
2022-10-30 00:36:39,492:INFO: 		 ADE on zara1                     dataset:	 3.113030195236206
2022-10-30 00:36:40,722:INFO: 		 ADE on zara2                     dataset:	 2.8224034309387207
2022-10-30 00:36:40,722:INFO: Average training:	ADE  2.3497	FDE  4.4008
2022-10-30 00:36:40,734:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_445.pth.tar
2022-10-30 00:36:40,734:INFO: 
===> EPOCH: 446 (P4)
2022-10-30 00:36:40,735:INFO: - Computing loss (training)
2022-10-30 00:36:42,039:INFO: Dataset: hotel               Batch: 1/4	Loss 85.4055 (85.4055)
2022-10-30 00:36:43,118:INFO: Dataset: hotel               Batch: 2/4	Loss 86.0260 (85.7202)
2022-10-30 00:36:44,191:INFO: Dataset: hotel               Batch: 3/4	Loss 84.3196 (85.2430)
2022-10-30 00:36:44,976:INFO: Dataset: hotel               Batch: 4/4	Loss 52.1304 (79.6951)
2022-10-30 00:36:46,457:INFO: Dataset: univ                Batch:  1/15	Loss 132.1941 (132.1941)
2022-10-30 00:36:47,599:INFO: Dataset: univ                Batch:  2/15	Loss 105.7749 (118.9801)
2022-10-30 00:36:48,743:INFO: Dataset: univ                Batch:  3/15	Loss 99.6094 (112.1681)
2022-10-30 00:36:49,889:INFO: Dataset: univ                Batch:  4/15	Loss 92.4908 (107.2036)
2022-10-30 00:36:51,028:INFO: Dataset: univ                Batch:  5/15	Loss 89.3513 (103.9645)
2022-10-30 00:36:52,180:INFO: Dataset: univ                Batch:  6/15	Loss 347.2109 (145.5781)
2022-10-30 00:36:53,307:INFO: Dataset: univ                Batch:  7/15	Loss 96.7894 (138.7897)
2022-10-30 00:36:54,428:INFO: Dataset: univ                Batch:  8/15	Loss 119.5855 (136.3862)
2022-10-30 00:36:55,549:INFO: Dataset: univ                Batch:  9/15	Loss 101.8126 (132.3586)
2022-10-30 00:36:56,683:INFO: Dataset: univ                Batch: 10/15	Loss 97.7152 (129.0365)
2022-10-30 00:36:57,822:INFO: Dataset: univ                Batch: 11/15	Loss 94.1801 (126.0097)
2022-10-30 00:36:58,951:INFO: Dataset: univ                Batch: 12/15	Loss 98.6174 (123.9636)
2022-10-30 00:37:00,092:INFO: Dataset: univ                Batch: 13/15	Loss 94.3487 (121.7010)
2022-10-30 00:37:01,232:INFO: Dataset: univ                Batch: 14/15	Loss 128.9568 (122.2717)
2022-10-30 00:37:01,722:INFO: Dataset: univ                Batch: 15/15	Loss 32.3389 (120.9761)
2022-10-30 00:37:03,083:INFO: Dataset: zara1               Batch: 1/8	Loss 92.4131 (92.4131)
2022-10-30 00:37:04,145:INFO: Dataset: zara1               Batch: 2/8	Loss 90.2100 (91.3208)
2022-10-30 00:37:05,202:INFO: Dataset: zara1               Batch: 3/8	Loss 95.6684 (92.7081)
2022-10-30 00:37:06,262:INFO: Dataset: zara1               Batch: 4/8	Loss 112.3697 (97.6706)
2022-10-30 00:37:07,334:INFO: Dataset: zara1               Batch: 5/8	Loss 92.9930 (96.6964)
2022-10-30 00:37:08,402:INFO: Dataset: zara1               Batch: 6/8	Loss 103.3844 (97.8376)
2022-10-30 00:37:09,475:INFO: Dataset: zara1               Batch: 7/8	Loss 92.5520 (97.0007)
2022-10-30 00:37:10,445:INFO: Dataset: zara1               Batch: 8/8	Loss 79.4212 (95.1410)
2022-10-30 00:37:11,829:INFO: Dataset: zara2               Batch:  1/18	Loss 120.0543 (120.0543)
2022-10-30 00:37:12,911:INFO: Dataset: zara2               Batch:  2/18	Loss 97.0832 (109.3076)
2022-10-30 00:37:13,969:INFO: Dataset: zara2               Batch:  3/18	Loss 235.3795 (149.7457)
2022-10-30 00:37:15,026:INFO: Dataset: zara2               Batch:  4/18	Loss 113.1592 (140.4703)
2022-10-30 00:37:16,092:INFO: Dataset: zara2               Batch:  5/18	Loss 121.0004 (136.6249)
2022-10-30 00:37:17,154:INFO: Dataset: zara2               Batch:  6/18	Loss 91.9496 (129.1459)
2022-10-30 00:37:18,210:INFO: Dataset: zara2               Batch:  7/18	Loss 92.2870 (123.9139)
2022-10-30 00:37:19,259:INFO: Dataset: zara2               Batch:  8/18	Loss 115.9325 (122.8881)
2022-10-30 00:37:20,315:INFO: Dataset: zara2               Batch:  9/18	Loss 98.0772 (120.0987)
2022-10-30 00:37:21,371:INFO: Dataset: zara2               Batch: 10/18	Loss 93.4251 (117.7043)
2022-10-30 00:37:22,423:INFO: Dataset: zara2               Batch: 11/18	Loss 139.1476 (119.6580)
2022-10-30 00:37:23,483:INFO: Dataset: zara2               Batch: 12/18	Loss 115.5962 (119.3506)
2022-10-30 00:37:24,543:INFO: Dataset: zara2               Batch: 13/18	Loss 77.6207 (116.0470)
2022-10-30 00:37:25,594:INFO: Dataset: zara2               Batch: 14/18	Loss 126.1531 (116.8044)
2022-10-30 00:37:26,646:INFO: Dataset: zara2               Batch: 15/18	Loss 165.5847 (120.4929)
2022-10-30 00:37:27,699:INFO: Dataset: zara2               Batch: 16/18	Loss 90.5492 (118.6778)
2022-10-30 00:37:28,751:INFO: Dataset: zara2               Batch: 17/18	Loss 131.6494 (119.4789)
2022-10-30 00:37:29,708:INFO: Dataset: zara2               Batch: 18/18	Loss 90.8251 (117.9773)
2022-10-30 00:37:29,775:INFO: - Computing ADE (validation o)
2022-10-30 00:37:30,140:INFO: 		 ADE on eth                       dataset:	 4.487946510314941
2022-10-30 00:37:30,140:INFO: Average validation o:	ADE  4.4879	FDE  7.9826
2022-10-30 00:37:30,141:INFO: - Computing ADE (validation)
2022-10-30 00:37:30,482:INFO: 		 ADE on hotel                     dataset:	 2.1695075035095215
2022-10-30 00:37:30,926:INFO: 		 ADE on univ                      dataset:	 2.2245683670043945
2022-10-30 00:37:31,289:INFO: 		 ADE on zara1                     dataset:	 2.1684486865997314
2022-10-30 00:37:31,841:INFO: 		 ADE on zara2                     dataset:	 2.3973236083984375
2022-10-30 00:37:31,841:INFO: Average validation:	ADE  2.2817	FDE  4.2664
2022-10-30 00:37:31,842:INFO: - Computing ADE (training)
2022-10-30 00:37:32,325:INFO: 		 ADE on hotel                     dataset:	 2.2532176971435547
2022-10-30 00:37:33,445:INFO: 		 ADE on univ                      dataset:	 2.1461894512176514
2022-10-30 00:37:34,152:INFO: 		 ADE on zara1                     dataset:	 3.103139877319336
2022-10-30 00:37:35,374:INFO: 		 ADE on zara2                     dataset:	 2.8118083477020264
2022-10-30 00:37:35,375:INFO: Average training:	ADE  2.3450	FDE  4.3938
2022-10-30 00:37:35,386:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_446.pth.tar
2022-10-30 00:37:35,386:INFO: 
===> EPOCH: 447 (P4)
2022-10-30 00:37:35,387:INFO: - Computing loss (training)
2022-10-30 00:37:36,666:INFO: Dataset: hotel               Batch: 1/4	Loss 85.1986 (85.1986)
2022-10-30 00:37:37,715:INFO: Dataset: hotel               Batch: 2/4	Loss 84.8867 (85.0430)
2022-10-30 00:37:38,767:INFO: Dataset: hotel               Batch: 3/4	Loss 86.0457 (85.3767)
2022-10-30 00:37:39,539:INFO: Dataset: hotel               Batch: 4/4	Loss 51.1752 (79.7817)
2022-10-30 00:37:41,002:INFO: Dataset: univ                Batch:  1/15	Loss 100.8565 (100.8565)
2022-10-30 00:37:42,135:INFO: Dataset: univ                Batch:  2/15	Loss 92.1560 (96.4522)
2022-10-30 00:37:43,268:INFO: Dataset: univ                Batch:  3/15	Loss 97.7029 (96.8817)
2022-10-30 00:37:44,409:INFO: Dataset: univ                Batch:  4/15	Loss 104.6830 (98.9576)
2022-10-30 00:37:45,548:INFO: Dataset: univ                Batch:  5/15	Loss 95.9517 (98.3840)
2022-10-30 00:37:46,691:INFO: Dataset: univ                Batch:  6/15	Loss 92.2551 (97.3480)
2022-10-30 00:37:47,835:INFO: Dataset: univ                Batch:  7/15	Loss 101.0962 (97.8750)
2022-10-30 00:37:49,061:INFO: Dataset: univ                Batch:  8/15	Loss 97.6577 (97.8476)
2022-10-30 00:37:50,190:INFO: Dataset: univ                Batch:  9/15	Loss 92.1860 (97.2483)
2022-10-30 00:37:51,320:INFO: Dataset: univ                Batch: 10/15	Loss 100.4662 (97.5640)
2022-10-30 00:37:52,455:INFO: Dataset: univ                Batch: 11/15	Loss 103.5595 (98.1003)
2022-10-30 00:37:53,584:INFO: Dataset: univ                Batch: 12/15	Loss 99.9044 (98.2483)
2022-10-30 00:37:54,714:INFO: Dataset: univ                Batch: 13/15	Loss 117.4207 (99.5366)
2022-10-30 00:37:55,843:INFO: Dataset: univ                Batch: 14/15	Loss 120.9347 (100.9599)
2022-10-30 00:37:56,327:INFO: Dataset: univ                Batch: 15/15	Loss 19.6472 (99.9966)
2022-10-30 00:37:57,731:INFO: Dataset: zara1               Batch: 1/8	Loss 95.2125 (95.2125)
2022-10-30 00:37:58,815:INFO: Dataset: zara1               Batch: 2/8	Loss 103.9313 (99.5896)
2022-10-30 00:37:59,892:INFO: Dataset: zara1               Batch: 3/8	Loss 95.2970 (98.1941)
2022-10-30 00:38:00,972:INFO: Dataset: zara1               Batch: 4/8	Loss 94.2733 (97.1958)
2022-10-30 00:38:02,043:INFO: Dataset: zara1               Batch: 5/8	Loss 110.5365 (99.9891)
2022-10-30 00:38:03,111:INFO: Dataset: zara1               Batch: 6/8	Loss 101.4630 (100.2202)
2022-10-30 00:38:04,183:INFO: Dataset: zara1               Batch: 7/8	Loss 91.8658 (99.0162)
2022-10-30 00:38:05,134:INFO: Dataset: zara1               Batch: 8/8	Loss 82.0456 (97.3549)
2022-10-30 00:38:06,521:INFO: Dataset: zara2               Batch:  1/18	Loss 209.6702 (209.6702)
2022-10-30 00:38:07,603:INFO: Dataset: zara2               Batch:  2/18	Loss 133.5345 (170.5610)
2022-10-30 00:38:08,683:INFO: Dataset: zara2               Batch:  3/18	Loss 91.4829 (142.7979)
2022-10-30 00:38:09,748:INFO: Dataset: zara2               Batch:  4/18	Loss 89.7660 (129.8064)
2022-10-30 00:38:10,819:INFO: Dataset: zara2               Batch:  5/18	Loss 108.7510 (125.6735)
2022-10-30 00:38:11,892:INFO: Dataset: zara2               Batch:  6/18	Loss 112.5416 (123.2771)
2022-10-30 00:38:12,970:INFO: Dataset: zara2               Batch:  7/18	Loss 92.2305 (119.1542)
2022-10-30 00:38:14,054:INFO: Dataset: zara2               Batch:  8/18	Loss 103.4932 (117.1596)
2022-10-30 00:38:15,128:INFO: Dataset: zara2               Batch:  9/18	Loss 108.9501 (116.2394)
2022-10-30 00:38:16,205:INFO: Dataset: zara2               Batch: 10/18	Loss 92.3503 (113.9207)
2022-10-30 00:38:17,278:INFO: Dataset: zara2               Batch: 11/18	Loss 124.7704 (114.8886)
2022-10-30 00:38:18,360:INFO: Dataset: zara2               Batch: 12/18	Loss 104.3732 (114.0154)
2022-10-30 00:38:19,434:INFO: Dataset: zara2               Batch: 13/18	Loss 99.5642 (112.9012)
2022-10-30 00:38:20,501:INFO: Dataset: zara2               Batch: 14/18	Loss 91.3673 (111.3428)
2022-10-30 00:38:21,555:INFO: Dataset: zara2               Batch: 15/18	Loss 137.1643 (113.1708)
2022-10-30 00:38:22,603:INFO: Dataset: zara2               Batch: 16/18	Loss 99.3587 (112.3582)
2022-10-30 00:38:23,652:INFO: Dataset: zara2               Batch: 17/18	Loss 102.0046 (111.7318)
2022-10-30 00:38:24,614:INFO: Dataset: zara2               Batch: 18/18	Loss 78.8137 (110.0230)
2022-10-30 00:38:24,681:INFO: - Computing ADE (validation o)
2022-10-30 00:38:25,051:INFO: 		 ADE on eth                       dataset:	 4.502846717834473
2022-10-30 00:38:25,051:INFO: Average validation o:	ADE  4.5028	FDE  8.0044
2022-10-30 00:38:25,052:INFO: - Computing ADE (validation)
2022-10-30 00:38:25,414:INFO: 		 ADE on hotel                     dataset:	 2.1295406818389893
2022-10-30 00:38:25,866:INFO: 		 ADE on univ                      dataset:	 2.2227795124053955
2022-10-30 00:38:26,218:INFO: 		 ADE on zara1                     dataset:	 2.130152940750122
2022-10-30 00:38:26,764:INFO: 		 ADE on zara2                     dataset:	 2.416750192642212
2022-10-30 00:38:26,765:INFO: Average validation:	ADE  2.2834	FDE  4.2686
2022-10-30 00:38:26,765:INFO: - Computing ADE (training)
2022-10-30 00:38:27,243:INFO: 		 ADE on hotel                     dataset:	 2.243595838546753
2022-10-30 00:38:28,308:INFO: 		 ADE on univ                      dataset:	 2.148000955581665
2022-10-30 00:38:29,013:INFO: 		 ADE on zara1                     dataset:	 3.1157612800598145
2022-10-30 00:38:30,196:INFO: 		 ADE on zara2                     dataset:	 2.813157796859741
2022-10-30 00:38:30,196:INFO: Average training:	ADE  2.3471	FDE  4.3956
2022-10-30 00:38:30,207:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_447.pth.tar
2022-10-30 00:38:30,207:INFO: 
===> EPOCH: 448 (P4)
2022-10-30 00:38:30,207:INFO: - Computing loss (training)
2022-10-30 00:38:31,487:INFO: Dataset: hotel               Batch: 1/4	Loss 86.4500 (86.4500)
2022-10-30 00:38:32,545:INFO: Dataset: hotel               Batch: 2/4	Loss 86.0328 (86.2295)
2022-10-30 00:38:33,586:INFO: Dataset: hotel               Batch: 3/4	Loss 84.8915 (85.7723)
2022-10-30 00:38:34,354:INFO: Dataset: hotel               Batch: 4/4	Loss 52.6180 (80.5236)
2022-10-30 00:38:35,784:INFO: Dataset: univ                Batch:  1/15	Loss 87.5237 (87.5237)
2022-10-30 00:38:36,904:INFO: Dataset: univ                Batch:  2/15	Loss 138.3092 (113.4912)
2022-10-30 00:38:38,029:INFO: Dataset: univ                Batch:  3/15	Loss 101.3404 (109.3187)
2022-10-30 00:38:39,156:INFO: Dataset: univ                Batch:  4/15	Loss 92.7399 (104.8555)
2022-10-30 00:38:40,284:INFO: Dataset: univ                Batch:  5/15	Loss 97.4291 (103.3830)
2022-10-30 00:38:41,410:INFO: Dataset: univ                Batch:  6/15	Loss 98.4334 (102.5270)
2022-10-30 00:38:42,523:INFO: Dataset: univ                Batch:  7/15	Loss 98.9780 (102.0754)
2022-10-30 00:38:43,643:INFO: Dataset: univ                Batch:  8/15	Loss 199.6734 (114.1388)
2022-10-30 00:38:44,757:INFO: Dataset: univ                Batch:  9/15	Loss 100.0964 (112.6126)
2022-10-30 00:38:45,873:INFO: Dataset: univ                Batch: 10/15	Loss 456.8850 (145.6089)
2022-10-30 00:38:46,995:INFO: Dataset: univ                Batch: 11/15	Loss 101.3380 (141.3635)
2022-10-30 00:38:48,114:INFO: Dataset: univ                Batch: 12/15	Loss 107.8631 (138.6164)
2022-10-30 00:38:49,235:INFO: Dataset: univ                Batch: 13/15	Loss 129.9915 (137.9096)
2022-10-30 00:38:50,350:INFO: Dataset: univ                Batch: 14/15	Loss 114.1958 (136.4060)
2022-10-30 00:38:50,834:INFO: Dataset: univ                Batch: 15/15	Loss 16.8682 (134.8878)
2022-10-30 00:38:52,191:INFO: Dataset: zara1               Batch: 1/8	Loss 94.0479 (94.0479)
2022-10-30 00:38:53,240:INFO: Dataset: zara1               Batch: 2/8	Loss 104.3879 (98.9685)
2022-10-30 00:38:54,286:INFO: Dataset: zara1               Batch: 3/8	Loss 89.2688 (95.5605)
2022-10-30 00:38:55,342:INFO: Dataset: zara1               Batch: 4/8	Loss 94.1920 (95.2231)
2022-10-30 00:38:56,396:INFO: Dataset: zara1               Batch: 5/8	Loss 91.8997 (94.4901)
2022-10-30 00:38:57,446:INFO: Dataset: zara1               Batch: 6/8	Loss 144.9303 (103.2623)
2022-10-30 00:38:58,500:INFO: Dataset: zara1               Batch: 7/8	Loss 99.6421 (102.7946)
2022-10-30 00:38:59,454:INFO: Dataset: zara1               Batch: 8/8	Loss 79.7673 (99.9343)
2022-10-30 00:39:00,823:INFO: Dataset: zara2               Batch:  1/18	Loss 96.9983 (96.9983)
2022-10-30 00:39:01,905:INFO: Dataset: zara2               Batch:  2/18	Loss 92.9049 (95.0392)
2022-10-30 00:39:02,981:INFO: Dataset: zara2               Batch:  3/18	Loss 118.0975 (103.0775)
2022-10-30 00:39:04,060:INFO: Dataset: zara2               Batch:  4/18	Loss 98.7309 (101.9610)
2022-10-30 00:39:05,135:INFO: Dataset: zara2               Batch:  5/18	Loss 94.2154 (100.4740)
2022-10-30 00:39:06,210:INFO: Dataset: zara2               Batch:  6/18	Loss 121.4568 (103.8624)
2022-10-30 00:39:07,276:INFO: Dataset: zara2               Batch:  7/18	Loss 99.4111 (103.3109)
2022-10-30 00:39:08,354:INFO: Dataset: zara2               Batch:  8/18	Loss 99.5297 (102.8194)
2022-10-30 00:39:09,422:INFO: Dataset: zara2               Batch:  9/18	Loss 90.4351 (101.3876)
2022-10-30 00:39:10,498:INFO: Dataset: zara2               Batch: 10/18	Loss 103.6044 (101.6323)
2022-10-30 00:39:11,568:INFO: Dataset: zara2               Batch: 11/18	Loss 147.8540 (106.2043)
2022-10-30 00:39:12,640:INFO: Dataset: zara2               Batch: 12/18	Loss 104.6410 (106.0682)
2022-10-30 00:39:13,707:INFO: Dataset: zara2               Batch: 13/18	Loss 223.9921 (115.3914)
2022-10-30 00:39:14,779:INFO: Dataset: zara2               Batch: 14/18	Loss 90.7819 (113.5307)
2022-10-30 00:39:15,855:INFO: Dataset: zara2               Batch: 15/18	Loss 289.8199 (125.2463)
2022-10-30 00:39:16,933:INFO: Dataset: zara2               Batch: 16/18	Loss 204.4476 (130.1818)
2022-10-30 00:39:18,004:INFO: Dataset: zara2               Batch: 17/18	Loss 94.5863 (128.0531)
2022-10-30 00:39:18,975:INFO: Dataset: zara2               Batch: 18/18	Loss 99.1684 (126.7161)
2022-10-30 00:39:19,042:INFO: - Computing ADE (validation o)
2022-10-30 00:39:19,407:INFO: 		 ADE on eth                       dataset:	 4.4550700187683105
2022-10-30 00:39:19,407:INFO: Average validation o:	ADE  4.4551	FDE  7.9336
2022-10-30 00:39:19,408:INFO: - Computing ADE (validation)
2022-10-30 00:39:19,768:INFO: 		 ADE on hotel                     dataset:	 2.1614651679992676
2022-10-30 00:39:20,205:INFO: 		 ADE on univ                      dataset:	 2.2080554962158203
2022-10-30 00:39:20,555:INFO: 		 ADE on zara1                     dataset:	 2.159121513366699
2022-10-30 00:39:21,105:INFO: 		 ADE on zara2                     dataset:	 2.390882730484009
2022-10-30 00:39:21,106:INFO: Average validation:	ADE  2.2697	FDE  4.2495
2022-10-30 00:39:21,106:INFO: - Computing ADE (training)
2022-10-30 00:39:21,583:INFO: 		 ADE on hotel                     dataset:	 2.2723522186279297
2022-10-30 00:39:22,648:INFO: 		 ADE on univ                      dataset:	 2.1433897018432617
2022-10-30 00:39:23,351:INFO: 		 ADE on zara1                     dataset:	 3.1043601036071777
2022-10-30 00:39:24,543:INFO: 		 ADE on zara2                     dataset:	 2.8181896209716797
2022-10-30 00:39:24,543:INFO: Average training:	ADE  2.3449	FDE  4.3894
2022-10-30 00:39:24,554:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_448.pth.tar
2022-10-30 00:39:24,555:INFO: 
===> EPOCH: 449 (P4)
2022-10-30 00:39:24,555:INFO: - Computing loss (training)
2022-10-30 00:39:25,830:INFO: Dataset: hotel               Batch: 1/4	Loss 85.7401 (85.7401)
2022-10-30 00:39:26,878:INFO: Dataset: hotel               Batch: 2/4	Loss 83.6268 (84.6378)
2022-10-30 00:39:27,928:INFO: Dataset: hotel               Batch: 3/4	Loss 86.2003 (85.1750)
2022-10-30 00:39:28,690:INFO: Dataset: hotel               Batch: 4/4	Loss 51.8712 (79.7269)
2022-10-30 00:39:30,099:INFO: Dataset: univ                Batch:  1/15	Loss 103.6977 (103.6977)
2022-10-30 00:39:31,211:INFO: Dataset: univ                Batch:  2/15	Loss 112.6906 (108.2147)
2022-10-30 00:39:32,330:INFO: Dataset: univ                Batch:  3/15	Loss 112.7862 (109.7072)
2022-10-30 00:39:33,441:INFO: Dataset: univ                Batch:  4/15	Loss 96.1513 (106.0294)
2022-10-30 00:39:34,554:INFO: Dataset: univ                Batch:  5/15	Loss 96.0003 (104.0294)
2022-10-30 00:39:35,662:INFO: Dataset: univ                Batch:  6/15	Loss 102.4574 (103.7852)
2022-10-30 00:39:36,771:INFO: Dataset: univ                Batch:  7/15	Loss 89.9266 (101.9710)
2022-10-30 00:39:37,888:INFO: Dataset: univ                Batch:  8/15	Loss 114.4343 (103.6848)
2022-10-30 00:39:38,996:INFO: Dataset: univ                Batch:  9/15	Loss 94.6452 (102.6839)
2022-10-30 00:39:40,202:INFO: Dataset: univ                Batch: 10/15	Loss 95.6381 (101.9306)
2022-10-30 00:39:41,328:INFO: Dataset: univ                Batch: 11/15	Loss 90.3799 (100.6865)
2022-10-30 00:39:42,441:INFO: Dataset: univ                Batch: 12/15	Loss 94.6310 (100.1271)
2022-10-30 00:39:43,549:INFO: Dataset: univ                Batch: 13/15	Loss 91.3261 (99.4522)
2022-10-30 00:39:44,665:INFO: Dataset: univ                Batch: 14/15	Loss 96.5169 (99.2303)
2022-10-30 00:39:45,134:INFO: Dataset: univ                Batch: 15/15	Loss 16.5062 (98.3483)
2022-10-30 00:39:46,528:INFO: Dataset: zara1               Batch: 1/8	Loss 95.2444 (95.2444)
2022-10-30 00:39:47,597:INFO: Dataset: zara1               Batch: 2/8	Loss 93.5674 (94.4293)
2022-10-30 00:39:48,664:INFO: Dataset: zara1               Batch: 3/8	Loss 95.9241 (94.9424)
2022-10-30 00:39:49,736:INFO: Dataset: zara1               Batch: 4/8	Loss 92.8106 (94.3730)
2022-10-30 00:39:50,805:INFO: Dataset: zara1               Batch: 5/8	Loss 91.5812 (93.8686)
2022-10-30 00:39:51,862:INFO: Dataset: zara1               Batch: 6/8	Loss 105.4898 (95.9485)
2022-10-30 00:39:52,914:INFO: Dataset: zara1               Batch: 7/8	Loss 94.7266 (95.7611)
2022-10-30 00:39:53,871:INFO: Dataset: zara1               Batch: 8/8	Loss 80.6147 (94.1349)
2022-10-30 00:39:55,260:INFO: Dataset: zara2               Batch:  1/18	Loss 97.3157 (97.3157)
2022-10-30 00:39:56,341:INFO: Dataset: zara2               Batch:  2/18	Loss 100.4307 (98.8876)
2022-10-30 00:39:57,411:INFO: Dataset: zara2               Batch:  3/18	Loss 115.2386 (104.5777)
2022-10-30 00:39:58,493:INFO: Dataset: zara2               Batch:  4/18	Loss 111.0981 (106.2358)
2022-10-30 00:39:59,570:INFO: Dataset: zara2               Batch:  5/18	Loss 107.9908 (106.6120)
2022-10-30 00:40:00,647:INFO: Dataset: zara2               Batch:  6/18	Loss 106.2134 (106.5458)
2022-10-30 00:40:01,719:INFO: Dataset: zara2               Batch:  7/18	Loss 105.3569 (106.3697)
2022-10-30 00:40:02,793:INFO: Dataset: zara2               Batch:  8/18	Loss 92.5107 (104.6785)
2022-10-30 00:40:03,869:INFO: Dataset: zara2               Batch:  9/18	Loss 90.8784 (103.1099)
2022-10-30 00:40:04,963:INFO: Dataset: zara2               Batch: 10/18	Loss 125.4507 (105.2432)
2022-10-30 00:40:06,040:INFO: Dataset: zara2               Batch: 11/18	Loss 92.1984 (104.1412)
2022-10-30 00:40:07,105:INFO: Dataset: zara2               Batch: 12/18	Loss 148.9606 (108.3158)
2022-10-30 00:40:08,171:INFO: Dataset: zara2               Batch: 13/18	Loss 95.1325 (107.3612)
2022-10-30 00:40:09,242:INFO: Dataset: zara2               Batch: 14/18	Loss 102.8652 (107.0576)
2022-10-30 00:40:10,313:INFO: Dataset: zara2               Batch: 15/18	Loss 94.4329 (106.2521)
2022-10-30 00:40:11,386:INFO: Dataset: zara2               Batch: 16/18	Loss 95.3935 (105.5202)
2022-10-30 00:40:12,466:INFO: Dataset: zara2               Batch: 17/18	Loss 118.9374 (106.2891)
2022-10-30 00:40:13,438:INFO: Dataset: zara2               Batch: 18/18	Loss 162.5342 (108.8275)
2022-10-30 00:40:13,502:INFO: - Computing ADE (validation o)
2022-10-30 00:40:13,861:INFO: 		 ADE on eth                       dataset:	 4.530498027801514
2022-10-30 00:40:13,861:INFO: Average validation o:	ADE  4.5305	FDE  8.0357
2022-10-30 00:40:13,862:INFO: - Computing ADE (validation)
2022-10-30 00:40:14,212:INFO: 		 ADE on hotel                     dataset:	 2.113462448120117
2022-10-30 00:40:14,645:INFO: 		 ADE on univ                      dataset:	 2.2139017581939697
2022-10-30 00:40:14,996:INFO: 		 ADE on zara1                     dataset:	 2.177694320678711
2022-10-30 00:40:15,549:INFO: 		 ADE on zara2                     dataset:	 2.4054317474365234
2022-10-30 00:40:15,549:INFO: Average validation:	ADE  2.2765	FDE  4.2566
2022-10-30 00:40:15,550:INFO: - Computing ADE (training)
2022-10-30 00:40:16,007:INFO: 		 ADE on hotel                     dataset:	 2.267777442932129
2022-10-30 00:40:17,081:INFO: 		 ADE on univ                      dataset:	 2.140824317932129
2022-10-30 00:40:17,787:INFO: 		 ADE on zara1                     dataset:	 3.123321294784546
2022-10-30 00:40:18,986:INFO: 		 ADE on zara2                     dataset:	 2.8156280517578125
2022-10-30 00:40:18,986:INFO: Average training:	ADE  2.3436	FDE  4.3857
2022-10-30 00:40:18,997:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_449.pth.tar
2022-10-30 00:40:18,998:INFO: 
===> EPOCH: 450 (P4)
2022-10-30 00:40:18,998:INFO: - Computing loss (training)
2022-10-30 00:40:20,262:INFO: Dataset: hotel               Batch: 1/4	Loss 84.1111 (84.1111)
2022-10-30 00:40:21,320:INFO: Dataset: hotel               Batch: 2/4	Loss 85.1048 (84.6091)
2022-10-30 00:40:22,373:INFO: Dataset: hotel               Batch: 3/4	Loss 84.8542 (84.6871)
2022-10-30 00:40:23,137:INFO: Dataset: hotel               Batch: 4/4	Loss 53.5471 (79.6340)
2022-10-30 00:40:24,580:INFO: Dataset: univ                Batch:  1/15	Loss 93.2935 (93.2935)
2022-10-30 00:40:25,706:INFO: Dataset: univ                Batch:  2/15	Loss 98.9377 (95.9671)
2022-10-30 00:40:26,816:INFO: Dataset: univ                Batch:  3/15	Loss 188.8940 (124.0936)
2022-10-30 00:40:27,940:INFO: Dataset: univ                Batch:  4/15	Loss 101.0152 (117.8919)
2022-10-30 00:40:29,061:INFO: Dataset: univ                Batch:  5/15	Loss 159.5299 (126.2988)
2022-10-30 00:40:30,178:INFO: Dataset: univ                Batch:  6/15	Loss 89.8935 (120.5031)
2022-10-30 00:40:31,291:INFO: Dataset: univ                Batch:  7/15	Loss 98.1728 (117.3572)
2022-10-30 00:40:32,402:INFO: Dataset: univ                Batch:  8/15	Loss 96.6574 (114.9204)
2022-10-30 00:40:33,521:INFO: Dataset: univ                Batch:  9/15	Loss 108.2002 (114.1401)
2022-10-30 00:40:34,634:INFO: Dataset: univ                Batch: 10/15	Loss 97.5083 (112.5746)
2022-10-30 00:40:35,761:INFO: Dataset: univ                Batch: 11/15	Loss 98.0226 (111.1768)
2022-10-30 00:40:36,882:INFO: Dataset: univ                Batch: 12/15	Loss 100.9076 (110.3493)
2022-10-30 00:40:38,008:INFO: Dataset: univ                Batch: 13/15	Loss 90.8746 (108.7293)
2022-10-30 00:40:39,130:INFO: Dataset: univ                Batch: 14/15	Loss 97.0732 (107.9608)
2022-10-30 00:40:39,611:INFO: Dataset: univ                Batch: 15/15	Loss 16.7269 (106.7588)
2022-10-30 00:40:40,973:INFO: Dataset: zara1               Batch: 1/8	Loss 306.3157 (306.3157)
2022-10-30 00:40:42,024:INFO: Dataset: zara1               Batch: 2/8	Loss 93.0943 (193.8663)
2022-10-30 00:40:43,087:INFO: Dataset: zara1               Batch: 3/8	Loss 97.6156 (164.9228)
2022-10-30 00:40:44,155:INFO: Dataset: zara1               Batch: 4/8	Loss 92.2809 (144.8066)
2022-10-30 00:40:45,212:INFO: Dataset: zara1               Batch: 5/8	Loss 96.6443 (135.5492)
2022-10-30 00:40:46,274:INFO: Dataset: zara1               Batch: 6/8	Loss 94.1012 (128.8186)
2022-10-30 00:40:47,343:INFO: Dataset: zara1               Batch: 7/8	Loss 96.3510 (123.7432)
2022-10-30 00:40:48,312:INFO: Dataset: zara1               Batch: 8/8	Loss 82.0503 (119.5300)
2022-10-30 00:40:49,693:INFO: Dataset: zara2               Batch:  1/18	Loss 108.7389 (108.7389)
2022-10-30 00:40:50,763:INFO: Dataset: zara2               Batch:  2/18	Loss 105.2564 (107.0401)
2022-10-30 00:40:51,823:INFO: Dataset: zara2               Batch:  3/18	Loss 91.7928 (101.8051)
2022-10-30 00:40:52,888:INFO: Dataset: zara2               Batch:  4/18	Loss 129.4790 (109.0306)
2022-10-30 00:40:53,954:INFO: Dataset: zara2               Batch:  5/18	Loss 97.7298 (106.6751)
2022-10-30 00:40:55,027:INFO: Dataset: zara2               Batch:  6/18	Loss 104.8811 (106.3936)
2022-10-30 00:40:56,103:INFO: Dataset: zara2               Batch:  7/18	Loss 111.2356 (107.1103)
2022-10-30 00:40:57,164:INFO: Dataset: zara2               Batch:  8/18	Loss 89.3344 (104.8013)
2022-10-30 00:40:58,228:INFO: Dataset: zara2               Batch:  9/18	Loss 317.0703 (129.1432)
2022-10-30 00:40:59,299:INFO: Dataset: zara2               Batch: 10/18	Loss 108.0619 (127.0074)
2022-10-30 00:41:00,373:INFO: Dataset: zara2               Batch: 11/18	Loss 91.5201 (123.8626)
2022-10-30 00:41:01,438:INFO: Dataset: zara2               Batch: 12/18	Loss 87.0869 (121.0095)
2022-10-30 00:41:02,514:INFO: Dataset: zara2               Batch: 13/18	Loss 126.7116 (121.4308)
2022-10-30 00:41:03,583:INFO: Dataset: zara2               Batch: 14/18	Loss 103.1446 (120.0442)
2022-10-30 00:41:04,653:INFO: Dataset: zara2               Batch: 15/18	Loss 87.0553 (117.9355)
2022-10-30 00:41:05,720:INFO: Dataset: zara2               Batch: 16/18	Loss 121.4556 (118.1409)
2022-10-30 00:41:06,798:INFO: Dataset: zara2               Batch: 17/18	Loss 94.4729 (116.7700)
2022-10-30 00:41:07,762:INFO: Dataset: zara2               Batch: 18/18	Loss 444.7685 (133.0370)
2022-10-30 00:41:07,831:INFO: - Computing ADE (validation o)
2022-10-30 00:41:08,193:INFO: 		 ADE on eth                       dataset:	 4.501168727874756
2022-10-30 00:41:08,193:INFO: Average validation o:	ADE  4.5012	FDE  7.9917
2022-10-30 00:41:08,194:INFO: - Computing ADE (validation)
2022-10-30 00:41:08,548:INFO: 		 ADE on hotel                     dataset:	 2.1287307739257812
2022-10-30 00:41:08,992:INFO: 		 ADE on univ                      dataset:	 2.2276241779327393
2022-10-30 00:41:09,348:INFO: 		 ADE on zara1                     dataset:	 2.18584942817688
2022-10-30 00:41:09,892:INFO: 		 ADE on zara2                     dataset:	 2.396467924118042
2022-10-30 00:41:09,892:INFO: Average validation:	ADE  2.2817	FDE  4.2587
2022-10-30 00:41:09,893:INFO: - Computing ADE (training)
2022-10-30 00:41:10,372:INFO: 		 ADE on hotel                     dataset:	 2.2424890995025635
2022-10-30 00:41:11,459:INFO: 		 ADE on univ                      dataset:	 2.138711929321289
2022-10-30 00:41:12,130:INFO: 		 ADE on zara1                     dataset:	 3.111490488052368
2022-10-30 00:41:13,399:INFO: 		 ADE on zara2                     dataset:	 2.8005168437957764
2022-10-30 00:41:13,399:INFO: Average training:	ADE  2.3377	FDE  4.3736
2022-10-30 00:41:13,410:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_450.pth.tar
2022-10-30 00:41:13,410:INFO: 
===> EPOCH: 451 (P4)
2022-10-30 00:41:13,411:INFO: - Computing loss (training)
2022-10-30 00:41:14,747:INFO: Dataset: hotel               Batch: 1/4	Loss 85.4643 (85.4643)
2022-10-30 00:41:15,824:INFO: Dataset: hotel               Batch: 2/4	Loss 85.3259 (85.3951)
2022-10-30 00:41:16,902:INFO: Dataset: hotel               Batch: 3/4	Loss 85.6875 (85.4913)
2022-10-30 00:41:17,686:INFO: Dataset: hotel               Batch: 4/4	Loss 51.1489 (79.2390)
2022-10-30 00:41:19,127:INFO: Dataset: univ                Batch:  1/15	Loss 1820.6353 (1820.6353)
2022-10-30 00:41:20,251:INFO: Dataset: univ                Batch:  2/15	Loss 102.9829 (906.0848)
2022-10-30 00:41:21,364:INFO: Dataset: univ                Batch:  3/15	Loss 93.8896 (644.1442)
2022-10-30 00:41:22,481:INFO: Dataset: univ                Batch:  4/15	Loss 103.2192 (507.9073)
2022-10-30 00:41:23,594:INFO: Dataset: univ                Batch:  5/15	Loss 102.3324 (429.3758)
2022-10-30 00:41:24,699:INFO: Dataset: univ                Batch:  6/15	Loss 96.9623 (377.6138)
2022-10-30 00:41:25,799:INFO: Dataset: univ                Batch:  7/15	Loss 105.9924 (341.9587)
2022-10-30 00:41:26,908:INFO: Dataset: univ                Batch:  8/15	Loss 96.8317 (311.4752)
2022-10-30 00:41:28,025:INFO: Dataset: univ                Batch:  9/15	Loss 89.1695 (285.4975)
2022-10-30 00:41:29,141:INFO: Dataset: univ                Batch: 10/15	Loss 105.6984 (266.5547)
2022-10-30 00:41:30,253:INFO: Dataset: univ                Batch: 11/15	Loss 97.0973 (250.7791)
2022-10-30 00:41:31,457:INFO: Dataset: univ                Batch: 12/15	Loss 91.9797 (237.1815)
2022-10-30 00:41:32,574:INFO: Dataset: univ                Batch: 13/15	Loss 93.5847 (225.6372)
2022-10-30 00:41:33,692:INFO: Dataset: univ                Batch: 14/15	Loss 101.9229 (217.3070)
2022-10-30 00:41:34,173:INFO: Dataset: univ                Batch: 15/15	Loss 19.2293 (214.3972)
2022-10-30 00:41:35,559:INFO: Dataset: zara1               Batch: 1/8	Loss 92.7001 (92.7001)
2022-10-30 00:41:36,618:INFO: Dataset: zara1               Batch: 2/8	Loss 94.5052 (93.5348)
2022-10-30 00:41:37,679:INFO: Dataset: zara1               Batch: 3/8	Loss 92.7565 (93.2738)
2022-10-30 00:41:38,766:INFO: Dataset: zara1               Batch: 4/8	Loss 95.5752 (93.9209)
2022-10-30 00:41:39,837:INFO: Dataset: zara1               Batch: 5/8	Loss 191.2442 (114.4571)
2022-10-30 00:41:40,904:INFO: Dataset: zara1               Batch: 6/8	Loss 94.4043 (110.7491)
2022-10-30 00:41:41,965:INFO: Dataset: zara1               Batch: 7/8	Loss 95.7828 (108.7790)
2022-10-30 00:41:42,931:INFO: Dataset: zara1               Batch: 8/8	Loss 82.3903 (105.9179)
2022-10-30 00:41:44,347:INFO: Dataset: zara2               Batch:  1/18	Loss 91.0736 (91.0736)
2022-10-30 00:41:45,396:INFO: Dataset: zara2               Batch:  2/18	Loss 92.2862 (91.6791)
2022-10-30 00:41:46,467:INFO: Dataset: zara2               Batch:  3/18	Loss 98.6681 (93.9934)
2022-10-30 00:41:47,527:INFO: Dataset: zara2               Batch:  4/18	Loss 96.2826 (94.5677)
2022-10-30 00:41:48,580:INFO: Dataset: zara2               Batch:  5/18	Loss 90.9908 (93.8535)
2022-10-30 00:41:49,641:INFO: Dataset: zara2               Batch:  6/18	Loss 146.4030 (102.1614)
2022-10-30 00:41:50,710:INFO: Dataset: zara2               Batch:  7/18	Loss 91.4697 (100.5745)
2022-10-30 00:41:51,757:INFO: Dataset: zara2               Batch:  8/18	Loss 103.1884 (100.8723)
2022-10-30 00:41:52,810:INFO: Dataset: zara2               Batch:  9/18	Loss 109.1080 (101.7900)
2022-10-30 00:41:53,866:INFO: Dataset: zara2               Batch: 10/18	Loss 93.3011 (101.0120)
2022-10-30 00:41:54,933:INFO: Dataset: zara2               Batch: 11/18	Loss 102.3961 (101.1365)
2022-10-30 00:41:55,990:INFO: Dataset: zara2               Batch: 12/18	Loss 435.2688 (129.1694)
2022-10-30 00:41:57,072:INFO: Dataset: zara2               Batch: 13/18	Loss 116.3286 (128.3443)
2022-10-30 00:41:58,225:INFO: Dataset: zara2               Batch: 14/18	Loss 101.9068 (126.5232)
2022-10-30 00:41:59,290:INFO: Dataset: zara2               Batch: 15/18	Loss 94.9216 (124.3589)
2022-10-30 00:42:00,348:INFO: Dataset: zara2               Batch: 16/18	Loss 105.7189 (123.1706)
2022-10-30 00:42:01,406:INFO: Dataset: zara2               Batch: 17/18	Loss 106.3343 (122.2385)
2022-10-30 00:42:02,369:INFO: Dataset: zara2               Batch: 18/18	Loss 102.1931 (121.2874)
2022-10-30 00:42:02,434:INFO: - Computing ADE (validation o)
2022-10-30 00:42:02,783:INFO: 		 ADE on eth                       dataset:	 4.502345085144043
2022-10-30 00:42:02,783:INFO: Average validation o:	ADE  4.5023	FDE  7.9808
2022-10-30 00:42:02,783:INFO: - Computing ADE (validation)
2022-10-30 00:42:03,133:INFO: 		 ADE on hotel                     dataset:	 2.131002426147461
2022-10-30 00:42:03,570:INFO: 		 ADE on univ                      dataset:	 2.212998151779175
2022-10-30 00:42:03,927:INFO: 		 ADE on zara1                     dataset:	 2.1667072772979736
2022-10-30 00:42:04,475:INFO: 		 ADE on zara2                     dataset:	 2.4021525382995605
2022-10-30 00:42:04,476:INFO: Average validation:	ADE  2.2752	FDE  4.2449
2022-10-30 00:42:04,476:INFO: - Computing ADE (training)
2022-10-30 00:42:04,941:INFO: 		 ADE on hotel                     dataset:	 2.2777938842773438
2022-10-30 00:42:06,013:INFO: 		 ADE on univ                      dataset:	 2.1320106983184814
2022-10-30 00:42:06,715:INFO: 		 ADE on zara1                     dataset:	 3.092958688735962
2022-10-30 00:42:07,931:INFO: 		 ADE on zara2                     dataset:	 2.812941312789917
2022-10-30 00:42:07,931:INFO: Average training:	ADE  2.3351	FDE  4.3641
2022-10-30 00:42:07,942:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_451.pth.tar
2022-10-30 00:42:07,942:INFO: 
===> EPOCH: 452 (P4)
2022-10-30 00:42:07,942:INFO: - Computing loss (training)
2022-10-30 00:42:09,217:INFO: Dataset: hotel               Batch: 1/4	Loss 85.0025 (85.0025)
2022-10-30 00:42:10,271:INFO: Dataset: hotel               Batch: 2/4	Loss 84.6179 (84.8169)
2022-10-30 00:42:11,316:INFO: Dataset: hotel               Batch: 3/4	Loss 86.4765 (85.3428)
2022-10-30 00:42:12,082:INFO: Dataset: hotel               Batch: 4/4	Loss 50.8560 (79.4282)
2022-10-30 00:42:13,549:INFO: Dataset: univ                Batch:  1/15	Loss 117.6627 (117.6627)
2022-10-30 00:42:14,702:INFO: Dataset: univ                Batch:  2/15	Loss 149.3050 (133.7320)
2022-10-30 00:42:15,841:INFO: Dataset: univ                Batch:  3/15	Loss 91.5741 (120.8933)
2022-10-30 00:42:16,984:INFO: Dataset: univ                Batch:  4/15	Loss 114.8418 (119.3451)
2022-10-30 00:42:18,118:INFO: Dataset: univ                Batch:  5/15	Loss 92.5344 (114.1968)
2022-10-30 00:42:19,260:INFO: Dataset: univ                Batch:  6/15	Loss 111.6170 (113.8067)
2022-10-30 00:42:20,402:INFO: Dataset: univ                Batch:  7/15	Loss 99.9977 (111.7622)
2022-10-30 00:42:21,541:INFO: Dataset: univ                Batch:  8/15	Loss 93.9015 (109.7962)
2022-10-30 00:42:22,696:INFO: Dataset: univ                Batch:  9/15	Loss 90.8561 (107.6626)
2022-10-30 00:42:23,835:INFO: Dataset: univ                Batch: 10/15	Loss 108.5094 (107.7428)
2022-10-30 00:42:24,960:INFO: Dataset: univ                Batch: 11/15	Loss 88.7078 (106.0292)
2022-10-30 00:42:26,088:INFO: Dataset: univ                Batch: 12/15	Loss 92.9012 (104.8954)
2022-10-30 00:42:27,229:INFO: Dataset: univ                Batch: 13/15	Loss 89.6744 (103.6419)
2022-10-30 00:42:28,365:INFO: Dataset: univ                Batch: 14/15	Loss 88.6850 (102.5337)
2022-10-30 00:42:28,847:INFO: Dataset: univ                Batch: 15/15	Loss 17.4115 (101.6301)
2022-10-30 00:42:30,217:INFO: Dataset: zara1               Batch: 1/8	Loss 104.0607 (104.0607)
2022-10-30 00:42:31,280:INFO: Dataset: zara1               Batch: 2/8	Loss 105.6494 (104.8152)
2022-10-30 00:42:32,346:INFO: Dataset: zara1               Batch: 3/8	Loss 97.9156 (102.5282)
2022-10-30 00:42:33,413:INFO: Dataset: zara1               Batch: 4/8	Loss 101.1977 (102.2043)
2022-10-30 00:42:34,486:INFO: Dataset: zara1               Batch: 5/8	Loss 91.9400 (100.2517)
2022-10-30 00:42:35,548:INFO: Dataset: zara1               Batch: 6/8	Loss 94.2569 (99.2755)
2022-10-30 00:42:36,614:INFO: Dataset: zara1               Batch: 7/8	Loss 95.7976 (98.7200)
2022-10-30 00:42:37,569:INFO: Dataset: zara1               Batch: 8/8	Loss 82.4464 (96.6558)
2022-10-30 00:42:38,943:INFO: Dataset: zara2               Batch:  1/18	Loss 87.5243 (87.5243)
2022-10-30 00:42:40,000:INFO: Dataset: zara2               Batch:  2/18	Loss 101.3463 (93.6183)
2022-10-30 00:42:41,059:INFO: Dataset: zara2               Batch:  3/18	Loss 91.9404 (93.0036)
2022-10-30 00:42:42,124:INFO: Dataset: zara2               Batch:  4/18	Loss 122.9410 (100.2757)
2022-10-30 00:42:43,189:INFO: Dataset: zara2               Batch:  5/18	Loss 102.9136 (100.8056)
2022-10-30 00:42:44,239:INFO: Dataset: zara2               Batch:  6/18	Loss 110.2153 (102.5819)
2022-10-30 00:42:45,292:INFO: Dataset: zara2               Batch:  7/18	Loss 99.1515 (102.1025)
2022-10-30 00:42:46,349:INFO: Dataset: zara2               Batch:  8/18	Loss 96.9967 (101.4664)
2022-10-30 00:42:47,400:INFO: Dataset: zara2               Batch:  9/18	Loss 91.2422 (100.2351)
2022-10-30 00:42:48,450:INFO: Dataset: zara2               Batch: 10/18	Loss 92.8876 (99.5095)
2022-10-30 00:42:49,502:INFO: Dataset: zara2               Batch: 11/18	Loss 107.2036 (100.2236)
2022-10-30 00:42:50,557:INFO: Dataset: zara2               Batch: 12/18	Loss 141.9420 (103.3200)
2022-10-30 00:42:51,618:INFO: Dataset: zara2               Batch: 13/18	Loss 91.2177 (102.3242)
2022-10-30 00:42:52,677:INFO: Dataset: zara2               Batch: 14/18	Loss 96.8770 (101.9583)
2022-10-30 00:42:53,741:INFO: Dataset: zara2               Batch: 15/18	Loss 97.7611 (101.6808)
2022-10-30 00:42:54,803:INFO: Dataset: zara2               Batch: 16/18	Loss 114.3320 (102.5029)
2022-10-30 00:42:55,862:INFO: Dataset: zara2               Batch: 17/18	Loss 102.1149 (102.4769)
2022-10-30 00:42:56,820:INFO: Dataset: zara2               Batch: 18/18	Loss 145.7344 (104.5722)
2022-10-30 00:42:56,885:INFO: - Computing ADE (validation o)
2022-10-30 00:42:57,244:INFO: 		 ADE on eth                       dataset:	 4.507535457611084
2022-10-30 00:42:57,244:INFO: Average validation o:	ADE  4.5075	FDE  7.9838
2022-10-30 00:42:57,245:INFO: - Computing ADE (validation)
2022-10-30 00:42:57,599:INFO: 		 ADE on hotel                     dataset:	 2.1422364711761475
2022-10-30 00:42:58,053:INFO: 		 ADE on univ                      dataset:	 2.1995081901550293
2022-10-30 00:42:58,415:INFO: 		 ADE on zara1                     dataset:	 2.1501288414001465
2022-10-30 00:42:58,977:INFO: 		 ADE on zara2                     dataset:	 2.39559006690979
2022-10-30 00:42:58,977:INFO: Average validation:	ADE  2.2654	FDE  4.2255
2022-10-30 00:42:58,978:INFO: - Computing ADE (training)
2022-10-30 00:42:59,456:INFO: 		 ADE on hotel                     dataset:	 2.2553627490997314
2022-10-30 00:43:00,528:INFO: 		 ADE on univ                      dataset:	 2.124115228652954
2022-10-30 00:43:01,235:INFO: 		 ADE on zara1                     dataset:	 3.108085870742798
2022-10-30 00:43:02,431:INFO: 		 ADE on zara2                     dataset:	 2.802663564682007
2022-10-30 00:43:02,432:INFO: Average training:	ADE  2.3279	FDE  4.3503
2022-10-30 00:43:02,443:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_452.pth.tar
2022-10-30 00:43:02,443:INFO: 
===> EPOCH: 453 (P4)
2022-10-30 00:43:02,443:INFO: - Computing loss (training)
2022-10-30 00:43:03,738:INFO: Dataset: hotel               Batch: 1/4	Loss 86.3640 (86.3640)
2022-10-30 00:43:04,797:INFO: Dataset: hotel               Batch: 2/4	Loss 84.6557 (85.5159)
2022-10-30 00:43:05,869:INFO: Dataset: hotel               Batch: 3/4	Loss 84.6545 (85.2229)
2022-10-30 00:43:06,651:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9724 (79.8007)
2022-10-30 00:43:08,056:INFO: Dataset: univ                Batch:  1/15	Loss 102.3195 (102.3195)
2022-10-30 00:43:09,175:INFO: Dataset: univ                Batch:  2/15	Loss 94.6780 (98.3339)
2022-10-30 00:43:10,292:INFO: Dataset: univ                Batch:  3/15	Loss 83.9751 (93.4793)
2022-10-30 00:43:11,409:INFO: Dataset: univ                Batch:  4/15	Loss 113.6572 (98.5625)
2022-10-30 00:43:12,528:INFO: Dataset: univ                Batch:  5/15	Loss 142.8328 (107.8256)
2022-10-30 00:43:13,645:INFO: Dataset: univ                Batch:  6/15	Loss 90.1808 (105.0105)
2022-10-30 00:43:14,761:INFO: Dataset: univ                Batch:  7/15	Loss 100.5491 (104.3601)
2022-10-30 00:43:15,879:INFO: Dataset: univ                Batch:  8/15	Loss 134.0012 (108.1973)
2022-10-30 00:43:16,993:INFO: Dataset: univ                Batch:  9/15	Loss 96.1044 (106.9115)
2022-10-30 00:43:18,106:INFO: Dataset: univ                Batch: 10/15	Loss 96.8530 (105.9315)
2022-10-30 00:43:19,216:INFO: Dataset: univ                Batch: 11/15	Loss 97.6343 (105.2624)
2022-10-30 00:43:20,337:INFO: Dataset: univ                Batch: 12/15	Loss 197.8575 (112.8842)
2022-10-30 00:43:21,451:INFO: Dataset: univ                Batch: 13/15	Loss 96.6598 (111.6995)
2022-10-30 00:43:22,657:INFO: Dataset: univ                Batch: 14/15	Loss 96.0690 (110.4930)
2022-10-30 00:43:23,140:INFO: Dataset: univ                Batch: 15/15	Loss 16.5119 (109.2371)
2022-10-30 00:43:24,490:INFO: Dataset: zara1               Batch: 1/8	Loss 144.1554 (144.1554)
2022-10-30 00:43:25,538:INFO: Dataset: zara1               Batch: 2/8	Loss 98.6416 (122.4202)
2022-10-30 00:43:26,588:INFO: Dataset: zara1               Batch: 3/8	Loss 96.3023 (113.9059)
2022-10-30 00:43:27,628:INFO: Dataset: zara1               Batch: 4/8	Loss 96.0630 (109.5612)
2022-10-30 00:43:28,679:INFO: Dataset: zara1               Batch: 5/8	Loss 95.9118 (106.6989)
2022-10-30 00:43:29,724:INFO: Dataset: zara1               Batch: 6/8	Loss 92.0333 (104.4262)
2022-10-30 00:43:30,771:INFO: Dataset: zara1               Batch: 7/8	Loss 103.9470 (104.3557)
2022-10-30 00:43:31,723:INFO: Dataset: zara1               Batch: 8/8	Loss 79.4883 (101.5679)
2022-10-30 00:43:33,122:INFO: Dataset: zara2               Batch:  1/18	Loss 91.8957 (91.8957)
2022-10-30 00:43:34,196:INFO: Dataset: zara2               Batch:  2/18	Loss 94.5632 (93.2430)
2022-10-30 00:43:35,274:INFO: Dataset: zara2               Batch:  3/18	Loss 140.3571 (109.7595)
2022-10-30 00:43:36,348:INFO: Dataset: zara2               Batch:  4/18	Loss 101.3139 (107.6690)
2022-10-30 00:43:37,419:INFO: Dataset: zara2               Batch:  5/18	Loss 136.2099 (113.1225)
2022-10-30 00:43:38,491:INFO: Dataset: zara2               Batch:  6/18	Loss 103.8200 (111.6527)
2022-10-30 00:43:39,566:INFO: Dataset: zara2               Batch:  7/18	Loss 106.0181 (110.8779)
2022-10-30 00:43:40,642:INFO: Dataset: zara2               Batch:  8/18	Loss 93.4428 (108.7255)
2022-10-30 00:43:41,707:INFO: Dataset: zara2               Batch:  9/18	Loss 116.9735 (109.6579)
2022-10-30 00:43:42,778:INFO: Dataset: zara2               Batch: 10/18	Loss 97.0656 (108.4917)
2022-10-30 00:43:43,843:INFO: Dataset: zara2               Batch: 11/18	Loss 92.4471 (107.1081)
2022-10-30 00:43:44,910:INFO: Dataset: zara2               Batch: 12/18	Loss 124.2508 (108.6726)
2022-10-30 00:43:45,965:INFO: Dataset: zara2               Batch: 13/18	Loss 164.5258 (113.1028)
2022-10-30 00:43:47,038:INFO: Dataset: zara2               Batch: 14/18	Loss 126.1542 (113.9300)
2022-10-30 00:43:48,108:INFO: Dataset: zara2               Batch: 15/18	Loss 98.3215 (112.8562)
2022-10-30 00:43:49,168:INFO: Dataset: zara2               Batch: 16/18	Loss 248.9103 (121.0050)
2022-10-30 00:43:50,234:INFO: Dataset: zara2               Batch: 17/18	Loss 92.6179 (119.4427)
2022-10-30 00:43:51,250:INFO: Dataset: zara2               Batch: 18/18	Loss 114.2001 (119.1922)
2022-10-30 00:43:51,315:INFO: - Computing ADE (validation o)
2022-10-30 00:43:51,667:INFO: 		 ADE on eth                       dataset:	 4.509212970733643
2022-10-30 00:43:51,668:INFO: Average validation o:	ADE  4.5092	FDE  7.9884
2022-10-30 00:43:51,668:INFO: - Computing ADE (validation)
2022-10-30 00:43:52,016:INFO: 		 ADE on hotel                     dataset:	 2.1056973934173584
2022-10-30 00:43:52,472:INFO: 		 ADE on univ                      dataset:	 2.2109313011169434
2022-10-30 00:43:52,835:INFO: 		 ADE on zara1                     dataset:	 2.1842410564422607
2022-10-30 00:43:53,388:INFO: 		 ADE on zara2                     dataset:	 2.3875508308410645
2022-10-30 00:43:53,388:INFO: Average validation:	ADE  2.2684	FDE  4.2281
2022-10-30 00:43:53,389:INFO: - Computing ADE (training)
2022-10-30 00:43:53,875:INFO: 		 ADE on hotel                     dataset:	 2.244441032409668
2022-10-30 00:43:54,937:INFO: 		 ADE on univ                      dataset:	 2.122879981994629
2022-10-30 00:43:55,636:INFO: 		 ADE on zara1                     dataset:	 3.1056289672851562
2022-10-30 00:43:56,833:INFO: 		 ADE on zara2                     dataset:	 2.796082019805908
2022-10-30 00:43:56,834:INFO: Average training:	ADE  2.3252	FDE  4.3443
2022-10-30 00:43:56,845:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_453.pth.tar
2022-10-30 00:43:56,845:INFO: 
===> EPOCH: 454 (P4)
2022-10-30 00:43:56,845:INFO: - Computing loss (training)
2022-10-30 00:43:58,147:INFO: Dataset: hotel               Batch: 1/4	Loss 84.9520 (84.9520)
2022-10-30 00:43:59,217:INFO: Dataset: hotel               Batch: 2/4	Loss 85.5281 (85.2366)
2022-10-30 00:44:00,283:INFO: Dataset: hotel               Batch: 3/4	Loss 85.2750 (85.2496)
2022-10-30 00:44:01,059:INFO: Dataset: hotel               Batch: 4/4	Loss 50.8469 (79.2586)
2022-10-30 00:44:02,501:INFO: Dataset: univ                Batch:  1/15	Loss 90.0060 (90.0060)
2022-10-30 00:44:03,631:INFO: Dataset: univ                Batch:  2/15	Loss 98.7273 (94.4417)
2022-10-30 00:44:04,758:INFO: Dataset: univ                Batch:  3/15	Loss 101.0783 (96.7924)
2022-10-30 00:44:05,886:INFO: Dataset: univ                Batch:  4/15	Loss 69.8103 (90.0579)
2022-10-30 00:44:07,004:INFO: Dataset: univ                Batch:  5/15	Loss 94.7100 (90.9140)
2022-10-30 00:44:08,121:INFO: Dataset: univ                Batch:  6/15	Loss 121.8736 (96.1609)
2022-10-30 00:44:09,237:INFO: Dataset: univ                Batch:  7/15	Loss 95.2607 (96.0245)
2022-10-30 00:44:10,349:INFO: Dataset: univ                Batch:  8/15	Loss 98.9820 (96.3755)
2022-10-30 00:44:11,453:INFO: Dataset: univ                Batch:  9/15	Loss 105.3699 (97.2836)
2022-10-30 00:44:12,565:INFO: Dataset: univ                Batch: 10/15	Loss 124.8664 (99.9548)
2022-10-30 00:44:13,681:INFO: Dataset: univ                Batch: 11/15	Loss 96.5124 (99.6392)
2022-10-30 00:44:14,805:INFO: Dataset: univ                Batch: 12/15	Loss 90.9454 (98.8716)
2022-10-30 00:44:15,916:INFO: Dataset: univ                Batch: 13/15	Loss 98.8269 (98.8686)
2022-10-30 00:44:17,035:INFO: Dataset: univ                Batch: 14/15	Loss 138.2165 (101.7503)
2022-10-30 00:44:17,521:INFO: Dataset: univ                Batch: 15/15	Loss 16.9268 (100.4078)
2022-10-30 00:44:18,890:INFO: Dataset: zara1               Batch: 1/8	Loss 100.1477 (100.1477)
2022-10-30 00:44:19,938:INFO: Dataset: zara1               Batch: 2/8	Loss 96.3086 (98.2281)
2022-10-30 00:44:20,989:INFO: Dataset: zara1               Batch: 3/8	Loss 97.9431 (98.1272)
2022-10-30 00:44:22,041:INFO: Dataset: zara1               Batch: 4/8	Loss 94.4742 (97.1991)
2022-10-30 00:44:23,096:INFO: Dataset: zara1               Batch: 5/8	Loss 94.6640 (96.6987)
2022-10-30 00:44:24,152:INFO: Dataset: zara1               Batch: 6/8	Loss 95.4008 (96.4996)
2022-10-30 00:44:25,204:INFO: Dataset: zara1               Batch: 7/8	Loss 93.8981 (96.1193)
2022-10-30 00:44:26,167:INFO: Dataset: zara1               Batch: 8/8	Loss 81.9622 (94.6514)
2022-10-30 00:44:27,543:INFO: Dataset: zara2               Batch:  1/18	Loss 99.8483 (99.8483)
2022-10-30 00:44:28,601:INFO: Dataset: zara2               Batch:  2/18	Loss 187.3592 (142.3244)
2022-10-30 00:44:29,657:INFO: Dataset: zara2               Batch:  3/18	Loss 95.5716 (125.9698)
2022-10-30 00:44:30,716:INFO: Dataset: zara2               Batch:  4/18	Loss 91.5236 (117.8207)
2022-10-30 00:44:31,787:INFO: Dataset: zara2               Batch:  5/18	Loss 106.3829 (115.4987)
2022-10-30 00:44:32,845:INFO: Dataset: zara2               Batch:  6/18	Loss 109.6756 (114.5091)
2022-10-30 00:44:33,896:INFO: Dataset: zara2               Batch:  7/18	Loss 96.1013 (111.8805)
2022-10-30 00:44:34,955:INFO: Dataset: zara2               Batch:  8/18	Loss 91.7076 (109.3662)
2022-10-30 00:44:36,024:INFO: Dataset: zara2               Batch:  9/18	Loss 101.9256 (108.5778)
2022-10-30 00:44:37,089:INFO: Dataset: zara2               Batch: 10/18	Loss 90.2678 (106.8332)
2022-10-30 00:44:38,153:INFO: Dataset: zara2               Batch: 11/18	Loss 132.6300 (109.2329)
2022-10-30 00:44:39,213:INFO: Dataset: zara2               Batch: 12/18	Loss 119.8446 (110.1172)
2022-10-30 00:44:40,282:INFO: Dataset: zara2               Batch: 13/18	Loss 94.1141 (108.9860)
2022-10-30 00:44:41,363:INFO: Dataset: zara2               Batch: 14/18	Loss 116.0212 (109.5310)
2022-10-30 00:44:42,445:INFO: Dataset: zara2               Batch: 15/18	Loss 78.1533 (107.6733)
2022-10-30 00:44:43,533:INFO: Dataset: zara2               Batch: 16/18	Loss 306.4300 (119.8495)
2022-10-30 00:44:44,615:INFO: Dataset: zara2               Batch: 17/18	Loss 106.9961 (119.1139)
2022-10-30 00:44:45,583:INFO: Dataset: zara2               Batch: 18/18	Loss 82.5102 (117.5042)
2022-10-30 00:44:45,649:INFO: - Computing ADE (validation o)
2022-10-30 00:44:46,026:INFO: 		 ADE on eth                       dataset:	 4.4768900871276855
2022-10-30 00:44:46,026:INFO: Average validation o:	ADE  4.4769	FDE  7.9497
2022-10-30 00:44:46,027:INFO: - Computing ADE (validation)
2022-10-30 00:44:46,385:INFO: 		 ADE on hotel                     dataset:	 2.1058216094970703
2022-10-30 00:44:46,847:INFO: 		 ADE on univ                      dataset:	 2.2024199962615967
2022-10-30 00:44:47,202:INFO: 		 ADE on zara1                     dataset:	 2.167815685272217
2022-10-30 00:44:47,770:INFO: 		 ADE on zara2                     dataset:	 2.369239330291748
2022-10-30 00:44:47,770:INFO: Average validation:	ADE  2.2563	FDE  4.2121
2022-10-30 00:44:47,771:INFO: - Computing ADE (training)
2022-10-30 00:44:48,244:INFO: 		 ADE on hotel                     dataset:	 2.258622884750366
2022-10-30 00:44:49,297:INFO: 		 ADE on univ                      dataset:	 2.1290109157562256
2022-10-30 00:44:49,993:INFO: 		 ADE on zara1                     dataset:	 3.123171806335449
2022-10-30 00:44:51,220:INFO: 		 ADE on zara2                     dataset:	 2.8066699504852295
2022-10-30 00:44:51,220:INFO: Average training:	ADE  2.3332	FDE  4.3578
2022-10-30 00:44:51,231:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_454.pth.tar
2022-10-30 00:44:51,231:INFO: 
===> EPOCH: 455 (P4)
2022-10-30 00:44:51,231:INFO: - Computing loss (training)
2022-10-30 00:44:52,500:INFO: Dataset: hotel               Batch: 1/4	Loss 84.9904 (84.9904)
2022-10-30 00:44:53,547:INFO: Dataset: hotel               Batch: 2/4	Loss 85.3737 (85.1696)
2022-10-30 00:44:54,590:INFO: Dataset: hotel               Batch: 3/4	Loss 85.1452 (85.1613)
2022-10-30 00:44:55,361:INFO: Dataset: hotel               Batch: 4/4	Loss 51.1978 (79.2916)
2022-10-30 00:44:56,806:INFO: Dataset: univ                Batch:  1/15	Loss 104.7509 (104.7509)
2022-10-30 00:44:57,944:INFO: Dataset: univ                Batch:  2/15	Loss 90.6502 (97.5082)
2022-10-30 00:44:59,077:INFO: Dataset: univ                Batch:  3/15	Loss 99.3542 (98.1153)
2022-10-30 00:45:00,195:INFO: Dataset: univ                Batch:  4/15	Loss 98.6906 (98.2665)
2022-10-30 00:45:01,310:INFO: Dataset: univ                Batch:  5/15	Loss 88.0891 (96.2382)
2022-10-30 00:45:02,427:INFO: Dataset: univ                Batch:  6/15	Loss 104.4295 (97.5591)
2022-10-30 00:45:03,550:INFO: Dataset: univ                Batch:  7/15	Loss 94.4733 (97.1098)
2022-10-30 00:45:04,665:INFO: Dataset: univ                Batch:  8/15	Loss 87.1348 (95.9055)
2022-10-30 00:45:05,772:INFO: Dataset: univ                Batch:  9/15	Loss 106.1543 (96.9847)
2022-10-30 00:45:06,896:INFO: Dataset: univ                Batch: 10/15	Loss 184.2878 (105.8595)
2022-10-30 00:45:08,003:INFO: Dataset: univ                Batch: 11/15	Loss 180.8375 (112.2858)
2022-10-30 00:45:09,113:INFO: Dataset: univ                Batch: 12/15	Loss 171.3394 (117.2211)
2022-10-30 00:45:10,217:INFO: Dataset: univ                Batch: 13/15	Loss 99.1438 (115.9476)
2022-10-30 00:45:11,406:INFO: Dataset: univ                Batch: 14/15	Loss 91.9342 (114.3125)
2022-10-30 00:45:11,883:INFO: Dataset: univ                Batch: 15/15	Loss 17.3355 (113.3796)
2022-10-30 00:45:13,270:INFO: Dataset: zara1               Batch: 1/8	Loss 91.4536 (91.4536)
2022-10-30 00:45:14,344:INFO: Dataset: zara1               Batch: 2/8	Loss 93.3139 (92.4162)
2022-10-30 00:45:15,417:INFO: Dataset: zara1               Batch: 3/8	Loss 95.4386 (93.4056)
2022-10-30 00:45:16,491:INFO: Dataset: zara1               Batch: 4/8	Loss 97.0286 (94.2704)
2022-10-30 00:45:17,567:INFO: Dataset: zara1               Batch: 5/8	Loss 96.5410 (94.7579)
2022-10-30 00:45:18,637:INFO: Dataset: zara1               Batch: 6/8	Loss 93.6277 (94.5639)
2022-10-30 00:45:19,705:INFO: Dataset: zara1               Batch: 7/8	Loss 93.0404 (94.3483)
2022-10-30 00:45:20,682:INFO: Dataset: zara1               Batch: 8/8	Loss 77.7764 (92.6300)
2022-10-30 00:45:22,089:INFO: Dataset: zara2               Batch:  1/18	Loss 104.6827 (104.6827)
2022-10-30 00:45:23,156:INFO: Dataset: zara2               Batch:  2/18	Loss 89.9167 (97.4729)
2022-10-30 00:45:24,231:INFO: Dataset: zara2               Batch:  3/18	Loss 93.8978 (96.3024)
2022-10-30 00:45:25,302:INFO: Dataset: zara2               Batch:  4/18	Loss 99.7162 (97.1577)
2022-10-30 00:45:26,368:INFO: Dataset: zara2               Batch:  5/18	Loss 83.4020 (94.5266)
2022-10-30 00:45:27,463:INFO: Dataset: zara2               Batch:  6/18	Loss 95.5685 (94.7043)
2022-10-30 00:45:28,555:INFO: Dataset: zara2               Batch:  7/18	Loss 112.3698 (97.2590)
2022-10-30 00:45:29,633:INFO: Dataset: zara2               Batch:  8/18	Loss 90.3426 (96.4992)
2022-10-30 00:45:30,713:INFO: Dataset: zara2               Batch:  9/18	Loss 98.0441 (96.6726)
2022-10-30 00:45:31,784:INFO: Dataset: zara2               Batch: 10/18	Loss 97.3306 (96.7435)
2022-10-30 00:45:32,860:INFO: Dataset: zara2               Batch: 11/18	Loss 90.8531 (96.1998)
2022-10-30 00:45:33,937:INFO: Dataset: zara2               Batch: 12/18	Loss 93.1329 (95.9234)
2022-10-30 00:45:35,008:INFO: Dataset: zara2               Batch: 13/18	Loss 117.3035 (97.7002)
2022-10-30 00:45:36,074:INFO: Dataset: zara2               Batch: 14/18	Loss 113.5041 (98.8731)
2022-10-30 00:45:37,147:INFO: Dataset: zara2               Batch: 15/18	Loss 106.5975 (99.3544)
2022-10-30 00:45:38,220:INFO: Dataset: zara2               Batch: 16/18	Loss 87.9966 (98.6921)
2022-10-30 00:45:39,296:INFO: Dataset: zara2               Batch: 17/18	Loss 108.8104 (99.3012)
2022-10-30 00:45:40,268:INFO: Dataset: zara2               Batch: 18/18	Loss 91.1913 (98.8977)
2022-10-30 00:45:40,335:INFO: - Computing ADE (validation o)
2022-10-30 00:45:40,705:INFO: 		 ADE on eth                       dataset:	 4.515202045440674
2022-10-30 00:45:40,705:INFO: Average validation o:	ADE  4.5152	FDE  7.9967
2022-10-30 00:45:40,706:INFO: - Computing ADE (validation)
2022-10-30 00:45:41,060:INFO: 		 ADE on hotel                     dataset:	 2.1095385551452637
2022-10-30 00:45:41,497:INFO: 		 ADE on univ                      dataset:	 2.2098071575164795
2022-10-30 00:45:41,860:INFO: 		 ADE on zara1                     dataset:	 2.152226448059082
2022-10-30 00:45:42,400:INFO: 		 ADE on zara2                     dataset:	 2.3796849250793457
2022-10-30 00:45:42,400:INFO: Average validation:	ADE  2.2633	FDE  4.2221
2022-10-30 00:45:42,401:INFO: - Computing ADE (training)
2022-10-30 00:45:42,879:INFO: 		 ADE on hotel                     dataset:	 2.2456893920898438
2022-10-30 00:45:43,979:INFO: 		 ADE on univ                      dataset:	 2.1225764751434326
2022-10-30 00:45:44,668:INFO: 		 ADE on zara1                     dataset:	 3.1148669719696045
2022-10-30 00:45:45,879:INFO: 		 ADE on zara2                     dataset:	 2.797097682952881
2022-10-30 00:45:45,879:INFO: Average training:	ADE  2.3258	FDE  4.3431
2022-10-30 00:45:45,890:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_455.pth.tar
2022-10-30 00:45:45,890:INFO: 
===> EPOCH: 456 (P4)
2022-10-30 00:45:45,891:INFO: - Computing loss (training)
2022-10-30 00:45:47,208:INFO: Dataset: hotel               Batch: 1/4	Loss 84.8439 (84.8439)
2022-10-30 00:45:48,273:INFO: Dataset: hotel               Batch: 2/4	Loss 84.9663 (84.9061)
2022-10-30 00:45:49,330:INFO: Dataset: hotel               Batch: 3/4	Loss 84.5610 (84.7912)
2022-10-30 00:45:50,107:INFO: Dataset: hotel               Batch: 4/4	Loss 51.8599 (79.6647)
2022-10-30 00:45:51,552:INFO: Dataset: univ                Batch:  1/15	Loss 89.4684 (89.4684)
2022-10-30 00:45:52,684:INFO: Dataset: univ                Batch:  2/15	Loss 88.4937 (88.9692)
2022-10-30 00:45:53,816:INFO: Dataset: univ                Batch:  3/15	Loss 92.2133 (90.0736)
2022-10-30 00:45:54,948:INFO: Dataset: univ                Batch:  4/15	Loss 93.3489 (90.8899)
2022-10-30 00:45:56,087:INFO: Dataset: univ                Batch:  5/15	Loss 91.5005 (91.0136)
2022-10-30 00:45:57,221:INFO: Dataset: univ                Batch:  6/15	Loss 219.2464 (111.3698)
2022-10-30 00:45:58,366:INFO: Dataset: univ                Batch:  7/15	Loss 125.6070 (113.4469)
2022-10-30 00:45:59,495:INFO: Dataset: univ                Batch:  8/15	Loss 100.4769 (111.9201)
2022-10-30 00:46:00,621:INFO: Dataset: univ                Batch:  9/15	Loss 112.2025 (111.9488)
2022-10-30 00:46:01,737:INFO: Dataset: univ                Batch: 10/15	Loss 163.3846 (116.2958)
2022-10-30 00:46:02,879:INFO: Dataset: univ                Batch: 11/15	Loss 260.1655 (130.5566)
2022-10-30 00:46:03,996:INFO: Dataset: univ                Batch: 12/15	Loss 100.4480 (128.1691)
2022-10-30 00:46:05,131:INFO: Dataset: univ                Batch: 13/15	Loss 99.6758 (125.7794)
2022-10-30 00:46:06,256:INFO: Dataset: univ                Batch: 14/15	Loss 94.4690 (123.5719)
2022-10-30 00:46:06,734:INFO: Dataset: univ                Batch: 15/15	Loss 16.2693 (122.0718)
2022-10-30 00:46:08,099:INFO: Dataset: zara1               Batch: 1/8	Loss 96.0894 (96.0894)
2022-10-30 00:46:09,164:INFO: Dataset: zara1               Batch: 2/8	Loss 96.1535 (96.1217)
2022-10-30 00:46:10,232:INFO: Dataset: zara1               Batch: 3/8	Loss 94.2218 (95.5115)
2022-10-30 00:46:11,311:INFO: Dataset: zara1               Batch: 4/8	Loss 93.1547 (94.8835)
2022-10-30 00:46:12,377:INFO: Dataset: zara1               Batch: 5/8	Loss 96.8867 (95.2815)
2022-10-30 00:46:13,442:INFO: Dataset: zara1               Batch: 6/8	Loss 94.5830 (95.1706)
2022-10-30 00:46:14,489:INFO: Dataset: zara1               Batch: 7/8	Loss 102.6956 (96.2481)
2022-10-30 00:46:15,443:INFO: Dataset: zara1               Batch: 8/8	Loss 79.7464 (94.3635)
2022-10-30 00:46:16,799:INFO: Dataset: zara2               Batch:  1/18	Loss 90.8083 (90.8083)
2022-10-30 00:46:17,863:INFO: Dataset: zara2               Batch:  2/18	Loss 100.4102 (95.5296)
2022-10-30 00:46:18,921:INFO: Dataset: zara2               Batch:  3/18	Loss 109.0079 (99.7954)
2022-10-30 00:46:19,984:INFO: Dataset: zara2               Batch:  4/18	Loss 108.9299 (102.1403)
2022-10-30 00:46:21,040:INFO: Dataset: zara2               Batch:  5/18	Loss 92.7781 (100.2461)
2022-10-30 00:46:22,090:INFO: Dataset: zara2               Batch:  6/18	Loss -43.9965 (73.8172)
2022-10-30 00:46:23,142:INFO: Dataset: zara2               Batch:  7/18	Loss 170.1745 (86.9833)
2022-10-30 00:46:24,190:INFO: Dataset: zara2               Batch:  8/18	Loss 93.7482 (87.9203)
2022-10-30 00:46:25,246:INFO: Dataset: zara2               Batch:  9/18	Loss 100.5695 (89.3715)
2022-10-30 00:46:26,303:INFO: Dataset: zara2               Batch: 10/18	Loss 98.4893 (90.2881)
2022-10-30 00:46:27,356:INFO: Dataset: zara2               Batch: 11/18	Loss 93.6706 (90.6045)
2022-10-30 00:46:28,413:INFO: Dataset: zara2               Batch: 12/18	Loss 94.3323 (90.9242)
2022-10-30 00:46:29,465:INFO: Dataset: zara2               Batch: 13/18	Loss 203.0685 (100.2465)
2022-10-30 00:46:30,525:INFO: Dataset: zara2               Batch: 14/18	Loss 89.2833 (99.4898)
2022-10-30 00:46:31,579:INFO: Dataset: zara2               Batch: 15/18	Loss 115.4582 (100.4768)
2022-10-30 00:46:32,635:INFO: Dataset: zara2               Batch: 16/18	Loss 89.7316 (99.8122)
2022-10-30 00:46:33,692:INFO: Dataset: zara2               Batch: 17/18	Loss 93.7299 (99.4858)
2022-10-30 00:46:34,644:INFO: Dataset: zara2               Batch: 18/18	Loss 80.7709 (98.5298)
2022-10-30 00:46:34,711:INFO: - Computing ADE (validation o)
2022-10-30 00:46:35,055:INFO: 		 ADE on eth                       dataset:	 4.482240676879883
2022-10-30 00:46:35,055:INFO: Average validation o:	ADE  4.4822	FDE  7.9536
2022-10-30 00:46:35,056:INFO: - Computing ADE (validation)
2022-10-30 00:46:35,399:INFO: 		 ADE on hotel                     dataset:	 2.066756010055542
2022-10-30 00:46:35,857:INFO: 		 ADE on univ                      dataset:	 2.193363666534424
2022-10-30 00:46:36,222:INFO: 		 ADE on zara1                     dataset:	 2.167158365249634
2022-10-30 00:46:36,787:INFO: 		 ADE on zara2                     dataset:	 2.3735382556915283
2022-10-30 00:46:36,787:INFO: Average validation:	ADE  2.2510	FDE  4.2014
2022-10-30 00:46:36,788:INFO: - Computing ADE (training)
2022-10-30 00:46:37,265:INFO: 		 ADE on hotel                     dataset:	 2.2532129287719727
2022-10-30 00:46:38,301:INFO: 		 ADE on univ                      dataset:	 2.12213397026062
2022-10-30 00:46:38,979:INFO: 		 ADE on zara1                     dataset:	 3.1124958992004395
2022-10-30 00:46:40,197:INFO: 		 ADE on zara2                     dataset:	 2.801607847213745
2022-10-30 00:46:40,197:INFO: Average training:	ADE  2.3265	FDE  4.3420
2022-10-30 00:46:40,208:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_456.pth.tar
2022-10-30 00:46:40,208:INFO: 
===> EPOCH: 457 (P4)
2022-10-30 00:46:40,209:INFO: - Computing loss (training)
2022-10-30 00:46:41,509:INFO: Dataset: hotel               Batch: 1/4	Loss 85.1223 (85.1223)
2022-10-30 00:46:42,572:INFO: Dataset: hotel               Batch: 2/4	Loss 85.5197 (85.3272)
2022-10-30 00:46:43,638:INFO: Dataset: hotel               Batch: 3/4	Loss 84.1004 (84.9176)
2022-10-30 00:46:44,419:INFO: Dataset: hotel               Batch: 4/4	Loss 51.5607 (79.3728)
2022-10-30 00:46:45,851:INFO: Dataset: univ                Batch:  1/15	Loss 88.0551 (88.0551)
2022-10-30 00:46:46,963:INFO: Dataset: univ                Batch:  2/15	Loss 170.7058 (130.8812)
2022-10-30 00:46:48,075:INFO: Dataset: univ                Batch:  3/15	Loss 89.8121 (117.5522)
2022-10-30 00:46:49,201:INFO: Dataset: univ                Batch:  4/15	Loss 90.8187 (110.6682)
2022-10-30 00:46:50,310:INFO: Dataset: univ                Batch:  5/15	Loss 96.9017 (108.0161)
2022-10-30 00:46:51,436:INFO: Dataset: univ                Batch:  6/15	Loss 95.4994 (105.7372)
2022-10-30 00:46:52,556:INFO: Dataset: univ                Batch:  7/15	Loss 104.1822 (105.5386)
2022-10-30 00:46:53,674:INFO: Dataset: univ                Batch:  8/15	Loss 93.0081 (103.8041)
2022-10-30 00:46:54,786:INFO: Dataset: univ                Batch:  9/15	Loss 88.9850 (102.1425)
2022-10-30 00:46:55,896:INFO: Dataset: univ                Batch: 10/15	Loss 106.0721 (102.5270)
2022-10-30 00:46:57,015:INFO: Dataset: univ                Batch: 11/15	Loss 105.7366 (102.8107)
2022-10-30 00:46:58,142:INFO: Dataset: univ                Batch: 12/15	Loss 167.3750 (108.2320)
2022-10-30 00:46:59,259:INFO: Dataset: univ                Batch: 13/15	Loss 94.1704 (107.2080)
2022-10-30 00:47:00,464:INFO: Dataset: univ                Batch: 14/15	Loss 96.2444 (106.3805)
2022-10-30 00:47:00,944:INFO: Dataset: univ                Batch: 15/15	Loss 18.6452 (105.2912)
2022-10-30 00:47:02,317:INFO: Dataset: zara1               Batch: 1/8	Loss 100.0330 (100.0330)
2022-10-30 00:47:03,375:INFO: Dataset: zara1               Batch: 2/8	Loss 99.3920 (99.6958)
2022-10-30 00:47:04,438:INFO: Dataset: zara1               Batch: 3/8	Loss 91.4095 (96.8626)
2022-10-30 00:47:05,497:INFO: Dataset: zara1               Batch: 4/8	Loss 91.8531 (95.6142)
2022-10-30 00:47:06,540:INFO: Dataset: zara1               Batch: 5/8	Loss 96.5752 (95.8118)
2022-10-30 00:47:07,579:INFO: Dataset: zara1               Batch: 6/8	Loss 92.9352 (95.3212)
2022-10-30 00:47:08,634:INFO: Dataset: zara1               Batch: 7/8	Loss 93.5273 (95.0298)
2022-10-30 00:47:09,584:INFO: Dataset: zara1               Batch: 8/8	Loss 81.1480 (93.4736)
2022-10-30 00:47:10,988:INFO: Dataset: zara2               Batch:  1/18	Loss 94.5100 (94.5100)
2022-10-30 00:47:12,066:INFO: Dataset: zara2               Batch:  2/18	Loss 90.5654 (92.7442)
2022-10-30 00:47:13,130:INFO: Dataset: zara2               Batch:  3/18	Loss 111.0042 (98.8247)
2022-10-30 00:47:14,209:INFO: Dataset: zara2               Batch:  4/18	Loss 164.3772 (115.5342)
2022-10-30 00:47:15,282:INFO: Dataset: zara2               Batch:  5/18	Loss 92.7096 (111.5080)
2022-10-30 00:47:16,357:INFO: Dataset: zara2               Batch:  6/18	Loss 97.1748 (108.9846)
2022-10-30 00:47:17,425:INFO: Dataset: zara2               Batch:  7/18	Loss 90.6605 (106.2551)
2022-10-30 00:47:18,493:INFO: Dataset: zara2               Batch:  8/18	Loss 89.3415 (104.0960)
2022-10-30 00:47:19,581:INFO: Dataset: zara2               Batch:  9/18	Loss 90.1783 (102.5370)
2022-10-30 00:47:20,659:INFO: Dataset: zara2               Batch: 10/18	Loss 96.8079 (101.9537)
2022-10-30 00:47:21,735:INFO: Dataset: zara2               Batch: 11/18	Loss 106.3853 (102.3852)
2022-10-30 00:47:22,813:INFO: Dataset: zara2               Batch: 12/18	Loss 91.7510 (101.4368)
2022-10-30 00:47:23,884:INFO: Dataset: zara2               Batch: 13/18	Loss 161.4346 (106.3371)
2022-10-30 00:47:24,969:INFO: Dataset: zara2               Batch: 14/18	Loss 93.0833 (105.3831)
2022-10-30 00:47:26,049:INFO: Dataset: zara2               Batch: 15/18	Loss 88.3022 (104.2331)
2022-10-30 00:47:27,120:INFO: Dataset: zara2               Batch: 16/18	Loss 97.9802 (103.8355)
2022-10-30 00:47:28,203:INFO: Dataset: zara2               Batch: 17/18	Loss 97.0983 (103.4348)
2022-10-30 00:47:29,176:INFO: Dataset: zara2               Batch: 18/18	Loss 78.5983 (102.1414)
2022-10-30 00:47:29,243:INFO: - Computing ADE (validation o)
2022-10-30 00:47:29,616:INFO: 		 ADE on eth                       dataset:	 4.554937362670898
2022-10-30 00:47:29,617:INFO: Average validation o:	ADE  4.5549	FDE  8.0475
2022-10-30 00:47:29,617:INFO: - Computing ADE (validation)
2022-10-30 00:47:29,960:INFO: 		 ADE on hotel                     dataset:	 2.064852476119995
2022-10-30 00:47:30,404:INFO: 		 ADE on univ                      dataset:	 2.189548969268799
2022-10-30 00:47:30,750:INFO: 		 ADE on zara1                     dataset:	 2.1452691555023193
2022-10-30 00:47:31,341:INFO: 		 ADE on zara2                     dataset:	 2.3755688667297363
2022-10-30 00:47:31,341:INFO: Average validation:	ADE  2.2484	FDE  4.1962
2022-10-30 00:47:31,342:INFO: - Computing ADE (training)
2022-10-30 00:47:31,811:INFO: 		 ADE on hotel                     dataset:	 2.25118350982666
2022-10-30 00:47:32,885:INFO: 		 ADE on univ                      dataset:	 2.1211941242218018
2022-10-30 00:47:33,600:INFO: 		 ADE on zara1                     dataset:	 3.089836835861206
2022-10-30 00:47:34,823:INFO: 		 ADE on zara2                     dataset:	 2.802168607711792
2022-10-30 00:47:34,824:INFO: Average training:	ADE  2.3244	FDE  4.3358
2022-10-30 00:47:34,834:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_457.pth.tar
2022-10-30 00:47:34,835:INFO: 
===> EPOCH: 458 (P4)
2022-10-30 00:47:34,835:INFO: - Computing loss (training)
2022-10-30 00:47:36,120:INFO: Dataset: hotel               Batch: 1/4	Loss 83.7417 (83.7417)
2022-10-30 00:47:37,191:INFO: Dataset: hotel               Batch: 2/4	Loss 84.9797 (84.3427)
2022-10-30 00:47:38,264:INFO: Dataset: hotel               Batch: 3/4	Loss 84.9657 (84.5563)
2022-10-30 00:47:39,044:INFO: Dataset: hotel               Batch: 4/4	Loss 52.3284 (79.1141)
2022-10-30 00:47:40,505:INFO: Dataset: univ                Batch:  1/15	Loss 128.6315 (128.6315)
2022-10-30 00:47:41,634:INFO: Dataset: univ                Batch:  2/15	Loss 89.2929 (109.3343)
2022-10-30 00:47:42,766:INFO: Dataset: univ                Batch:  3/15	Loss 89.9571 (103.0201)
2022-10-30 00:47:43,896:INFO: Dataset: univ                Batch:  4/15	Loss 101.0145 (102.5444)
2022-10-30 00:47:45,025:INFO: Dataset: univ                Batch:  5/15	Loss 89.4613 (99.9089)
2022-10-30 00:47:46,151:INFO: Dataset: univ                Batch:  6/15	Loss 107.4066 (101.1070)
2022-10-30 00:47:47,277:INFO: Dataset: univ                Batch:  7/15	Loss 93.6500 (100.1210)
2022-10-30 00:47:48,408:INFO: Dataset: univ                Batch:  8/15	Loss 134.0047 (104.4251)
2022-10-30 00:47:49,540:INFO: Dataset: univ                Batch:  9/15	Loss 106.8470 (104.7031)
2022-10-30 00:47:50,673:INFO: Dataset: univ                Batch: 10/15	Loss 98.3687 (103.9938)
2022-10-30 00:47:51,799:INFO: Dataset: univ                Batch: 11/15	Loss 101.4350 (103.7659)
2022-10-30 00:47:52,922:INFO: Dataset: univ                Batch: 12/15	Loss 94.5569 (103.0141)
2022-10-30 00:47:54,047:INFO: Dataset: univ                Batch: 13/15	Loss 93.2936 (102.2880)
2022-10-30 00:47:55,186:INFO: Dataset: univ                Batch: 14/15	Loss 88.8753 (101.2743)
2022-10-30 00:47:55,668:INFO: Dataset: univ                Batch: 15/15	Loss 16.8937 (100.2786)
2022-10-30 00:47:57,039:INFO: Dataset: zara1               Batch: 1/8	Loss 93.0612 (93.0612)
2022-10-30 00:47:58,108:INFO: Dataset: zara1               Batch: 2/8	Loss 95.1761 (94.1897)
2022-10-30 00:47:59,174:INFO: Dataset: zara1               Batch: 3/8	Loss 95.2133 (94.5077)
2022-10-30 00:48:00,244:INFO: Dataset: zara1               Batch: 4/8	Loss 95.6226 (94.7985)
2022-10-30 00:48:01,314:INFO: Dataset: zara1               Batch: 5/8	Loss 94.4163 (94.7307)
2022-10-30 00:48:02,396:INFO: Dataset: zara1               Batch: 6/8	Loss 97.2770 (95.1820)
2022-10-30 00:48:03,472:INFO: Dataset: zara1               Batch: 7/8	Loss 94.8713 (95.1420)
2022-10-30 00:48:04,434:INFO: Dataset: zara1               Batch: 8/8	Loss 85.0357 (93.9930)
2022-10-30 00:48:05,835:INFO: Dataset: zara2               Batch:  1/18	Loss 104.4422 (104.4422)
2022-10-30 00:48:06,911:INFO: Dataset: zara2               Batch:  2/18	Loss 100.3615 (102.5726)
2022-10-30 00:48:07,995:INFO: Dataset: zara2               Batch:  3/18	Loss 90.7741 (98.7294)
2022-10-30 00:48:09,081:INFO: Dataset: zara2               Batch:  4/18	Loss 100.5684 (99.1697)
2022-10-30 00:48:10,166:INFO: Dataset: zara2               Batch:  5/18	Loss 110.8540 (101.3983)
2022-10-30 00:48:11,243:INFO: Dataset: zara2               Batch:  6/18	Loss 105.2209 (102.0939)
2022-10-30 00:48:12,326:INFO: Dataset: zara2               Batch:  7/18	Loss 101.6791 (102.0303)
2022-10-30 00:48:13,399:INFO: Dataset: zara2               Batch:  8/18	Loss 107.4816 (102.6665)
2022-10-30 00:48:14,483:INFO: Dataset: zara2               Batch:  9/18	Loss 88.3730 (100.9502)
2022-10-30 00:48:15,564:INFO: Dataset: zara2               Batch: 10/18	Loss 105.9597 (101.5171)
2022-10-30 00:48:16,647:INFO: Dataset: zara2               Batch: 11/18	Loss 93.3998 (100.7923)
2022-10-30 00:48:17,729:INFO: Dataset: zara2               Batch: 12/18	Loss 115.0699 (101.9755)
2022-10-30 00:48:18,821:INFO: Dataset: zara2               Batch: 13/18	Loss 89.3632 (100.9920)
2022-10-30 00:48:19,904:INFO: Dataset: zara2               Batch: 14/18	Loss 93.1260 (100.4443)
2022-10-30 00:48:20,985:INFO: Dataset: zara2               Batch: 15/18	Loss 89.9699 (99.7663)
2022-10-30 00:48:22,072:INFO: Dataset: zara2               Batch: 16/18	Loss 91.3463 (99.2689)
2022-10-30 00:48:23,146:INFO: Dataset: zara2               Batch: 17/18	Loss 89.3848 (98.7306)
2022-10-30 00:48:24,118:INFO: Dataset: zara2               Batch: 18/18	Loss 79.6463 (97.8346)
2022-10-30 00:48:24,184:INFO: - Computing ADE (validation o)
2022-10-30 00:48:24,536:INFO: 		 ADE on eth                       dataset:	 4.443591594696045
2022-10-30 00:48:24,537:INFO: Average validation o:	ADE  4.4436	FDE  7.8799
2022-10-30 00:48:24,537:INFO: - Computing ADE (validation)
2022-10-30 00:48:24,888:INFO: 		 ADE on hotel                     dataset:	 2.088256597518921
2022-10-30 00:48:25,337:INFO: 		 ADE on univ                      dataset:	 2.201239585876465
2022-10-30 00:48:25,697:INFO: 		 ADE on zara1                     dataset:	 2.189959764480591
2022-10-30 00:48:26,248:INFO: 		 ADE on zara2                     dataset:	 2.3700459003448486
2022-10-30 00:48:26,248:INFO: Average validation:	ADE  2.2563	FDE  4.2047
2022-10-30 00:48:26,249:INFO: - Computing ADE (training)
2022-10-30 00:48:26,725:INFO: 		 ADE on hotel                     dataset:	 2.243257522583008
2022-10-30 00:48:27,777:INFO: 		 ADE on univ                      dataset:	 2.118999719619751
2022-10-30 00:48:28,479:INFO: 		 ADE on zara1                     dataset:	 3.108304262161255
2022-10-30 00:48:29,661:INFO: 		 ADE on zara2                     dataset:	 2.7994396686553955
2022-10-30 00:48:29,661:INFO: Average training:	ADE  2.3233	FDE  4.3325
2022-10-30 00:48:29,673:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_458.pth.tar
2022-10-30 00:48:29,673:INFO: 
===> EPOCH: 459 (P4)
2022-10-30 00:48:29,673:INFO: - Computing loss (training)
2022-10-30 00:48:30,952:INFO: Dataset: hotel               Batch: 1/4	Loss 85.1172 (85.1172)
2022-10-30 00:48:32,011:INFO: Dataset: hotel               Batch: 2/4	Loss 85.1994 (85.1578)
2022-10-30 00:48:33,065:INFO: Dataset: hotel               Batch: 3/4	Loss 84.9515 (85.0935)
2022-10-30 00:48:33,833:INFO: Dataset: hotel               Batch: 4/4	Loss 51.0864 (79.3060)
2022-10-30 00:48:35,271:INFO: Dataset: univ                Batch:  1/15	Loss 102.3886 (102.3886)
2022-10-30 00:48:36,421:INFO: Dataset: univ                Batch:  2/15	Loss 99.9428 (101.1482)
2022-10-30 00:48:37,557:INFO: Dataset: univ                Batch:  3/15	Loss 106.6950 (103.0493)
2022-10-30 00:48:38,700:INFO: Dataset: univ                Batch:  4/15	Loss 96.6620 (101.4236)
2022-10-30 00:48:39,847:INFO: Dataset: univ                Batch:  5/15	Loss 91.3327 (99.3254)
2022-10-30 00:48:40,985:INFO: Dataset: univ                Batch:  6/15	Loss 100.5888 (99.5312)
2022-10-30 00:48:42,122:INFO: Dataset: univ                Batch:  7/15	Loss 292.0796 (127.5030)
2022-10-30 00:48:43,243:INFO: Dataset: univ                Batch:  8/15	Loss 97.8813 (124.3060)
2022-10-30 00:48:44,382:INFO: Dataset: univ                Batch:  9/15	Loss 92.0886 (120.6432)
2022-10-30 00:48:45,522:INFO: Dataset: univ                Batch: 10/15	Loss 92.8832 (117.6453)
2022-10-30 00:48:46,647:INFO: Dataset: univ                Batch: 11/15	Loss 93.7526 (115.7736)
2022-10-30 00:48:47,769:INFO: Dataset: univ                Batch: 12/15	Loss 112.8036 (115.5683)
2022-10-30 00:48:48,896:INFO: Dataset: univ                Batch: 13/15	Loss 128.3606 (116.4650)
2022-10-30 00:48:50,021:INFO: Dataset: univ                Batch: 14/15	Loss 92.2516 (114.6074)
2022-10-30 00:48:50,501:INFO: Dataset: univ                Batch: 15/15	Loss 17.3863 (113.4142)
2022-10-30 00:48:51,897:INFO: Dataset: zara1               Batch: 1/8	Loss 93.5191 (93.5191)
2022-10-30 00:48:52,963:INFO: Dataset: zara1               Batch: 2/8	Loss 91.5331 (92.5199)
2022-10-30 00:48:54,113:INFO: Dataset: zara1               Batch: 3/8	Loss 93.1477 (92.7352)
2022-10-30 00:48:55,177:INFO: Dataset: zara1               Batch: 4/8	Loss 104.0652 (95.3156)
2022-10-30 00:48:56,263:INFO: Dataset: zara1               Batch: 5/8	Loss 100.7447 (96.4198)
2022-10-30 00:48:57,332:INFO: Dataset: zara1               Batch: 6/8	Loss 173.0930 (109.1086)
2022-10-30 00:48:58,410:INFO: Dataset: zara1               Batch: 7/8	Loss 93.4366 (106.6753)
2022-10-30 00:48:59,383:INFO: Dataset: zara1               Batch: 8/8	Loss 79.9155 (103.5909)
2022-10-30 00:49:00,749:INFO: Dataset: zara2               Batch:  1/18	Loss 97.2282 (97.2282)
2022-10-30 00:49:01,816:INFO: Dataset: zara2               Batch:  2/18	Loss 106.5528 (102.0192)
2022-10-30 00:49:02,876:INFO: Dataset: zara2               Batch:  3/18	Loss 98.1021 (100.6767)
2022-10-30 00:49:03,950:INFO: Dataset: zara2               Batch:  4/18	Loss 91.5845 (98.3255)
2022-10-30 00:49:05,010:INFO: Dataset: zara2               Batch:  5/18	Loss 145.3382 (108.3147)
2022-10-30 00:49:06,087:INFO: Dataset: zara2               Batch:  6/18	Loss 95.8387 (106.1034)
2022-10-30 00:49:07,161:INFO: Dataset: zara2               Batch:  7/18	Loss 100.8405 (105.3954)
2022-10-30 00:49:08,215:INFO: Dataset: zara2               Batch:  8/18	Loss 132.1302 (108.7984)
2022-10-30 00:49:09,271:INFO: Dataset: zara2               Batch:  9/18	Loss 135.2349 (111.7300)
2022-10-30 00:49:10,326:INFO: Dataset: zara2               Batch: 10/18	Loss 93.8918 (110.0249)
2022-10-30 00:49:11,386:INFO: Dataset: zara2               Batch: 11/18	Loss 88.4071 (108.0702)
2022-10-30 00:49:12,437:INFO: Dataset: zara2               Batch: 12/18	Loss 92.7703 (106.7774)
2022-10-30 00:49:13,487:INFO: Dataset: zara2               Batch: 13/18	Loss 102.4428 (106.4181)
2022-10-30 00:49:14,533:INFO: Dataset: zara2               Batch: 14/18	Loss 274.7498 (118.4594)
2022-10-30 00:49:15,584:INFO: Dataset: zara2               Batch: 15/18	Loss 90.4162 (116.4706)
2022-10-30 00:49:16,634:INFO: Dataset: zara2               Batch: 16/18	Loss 94.2443 (115.1959)
2022-10-30 00:49:17,712:INFO: Dataset: zara2               Batch: 17/18	Loss 97.6492 (114.2585)
2022-10-30 00:49:18,694:INFO: Dataset: zara2               Batch: 18/18	Loss 76.9322 (112.6233)
2022-10-30 00:49:18,760:INFO: - Computing ADE (validation o)
2022-10-30 00:49:19,108:INFO: 		 ADE on eth                       dataset:	 4.469641208648682
2022-10-30 00:49:19,108:INFO: Average validation o:	ADE  4.4696	FDE  7.9220
2022-10-30 00:49:19,109:INFO: - Computing ADE (validation)
2022-10-30 00:49:19,445:INFO: 		 ADE on hotel                     dataset:	 2.11220121383667
2022-10-30 00:49:19,906:INFO: 		 ADE on univ                      dataset:	 2.1962673664093018
2022-10-30 00:49:20,259:INFO: 		 ADE on zara1                     dataset:	 2.1871562004089355
2022-10-30 00:49:20,819:INFO: 		 ADE on zara2                     dataset:	 2.381349563598633
2022-10-30 00:49:20,819:INFO: Average validation:	ADE  2.2590	FDE  4.2055
2022-10-30 00:49:20,820:INFO: - Computing ADE (training)
2022-10-30 00:49:21,291:INFO: 		 ADE on hotel                     dataset:	 2.2245137691497803
2022-10-30 00:49:22,363:INFO: 		 ADE on univ                      dataset:	 2.1110925674438477
2022-10-30 00:49:23,077:INFO: 		 ADE on zara1                     dataset:	 3.11741304397583
2022-10-30 00:49:24,339:INFO: 		 ADE on zara2                     dataset:	 2.8031117916107178
2022-10-30 00:49:24,339:INFO: Average training:	ADE  2.3185	FDE  4.3219
2022-10-30 00:49:24,349:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_459.pth.tar
2022-10-30 00:49:24,350:INFO: 
===> EPOCH: 460 (P4)
2022-10-30 00:49:24,350:INFO: - Computing loss (training)
2022-10-30 00:49:25,655:INFO: Dataset: hotel               Batch: 1/4	Loss 83.4586 (83.4586)
2022-10-30 00:49:26,723:INFO: Dataset: hotel               Batch: 2/4	Loss 85.7521 (84.5834)
2022-10-30 00:49:27,783:INFO: Dataset: hotel               Batch: 3/4	Loss 84.4186 (84.5269)
2022-10-30 00:49:28,556:INFO: Dataset: hotel               Batch: 4/4	Loss 52.6320 (79.3934)
2022-10-30 00:49:30,005:INFO: Dataset: univ                Batch:  1/15	Loss 96.7622 (96.7622)
2022-10-30 00:49:31,121:INFO: Dataset: univ                Batch:  2/15	Loss 90.5874 (93.7330)
2022-10-30 00:49:32,237:INFO: Dataset: univ                Batch:  3/15	Loss 138.3929 (109.2148)
2022-10-30 00:49:33,344:INFO: Dataset: univ                Batch:  4/15	Loss 98.9347 (106.7118)
2022-10-30 00:49:34,451:INFO: Dataset: univ                Batch:  5/15	Loss 96.9146 (104.8626)
2022-10-30 00:49:35,567:INFO: Dataset: univ                Batch:  6/15	Loss 96.8406 (103.5751)
2022-10-30 00:49:36,676:INFO: Dataset: univ                Batch:  7/15	Loss 91.2325 (101.7542)
2022-10-30 00:49:37,783:INFO: Dataset: univ                Batch:  8/15	Loss 91.4037 (100.5108)
2022-10-30 00:49:38,906:INFO: Dataset: univ                Batch:  9/15	Loss 89.3206 (99.1829)
2022-10-30 00:49:40,022:INFO: Dataset: univ                Batch: 10/15	Loss 93.0297 (98.5412)
2022-10-30 00:49:41,142:INFO: Dataset: univ                Batch: 11/15	Loss 89.9758 (97.7413)
2022-10-30 00:49:42,261:INFO: Dataset: univ                Batch: 12/15	Loss 99.3157 (97.8745)
2022-10-30 00:49:43,381:INFO: Dataset: univ                Batch: 13/15	Loss 91.0653 (97.4035)
2022-10-30 00:49:44,495:INFO: Dataset: univ                Batch: 14/15	Loss 108.1877 (98.1561)
2022-10-30 00:49:44,980:INFO: Dataset: univ                Batch: 15/15	Loss 16.0498 (96.8409)
2022-10-30 00:49:46,339:INFO: Dataset: zara1               Batch: 1/8	Loss 282.2861 (282.2861)
2022-10-30 00:49:47,379:INFO: Dataset: zara1               Batch: 2/8	Loss 96.1505 (196.6046)
2022-10-30 00:49:48,426:INFO: Dataset: zara1               Batch: 3/8	Loss 118.3176 (170.8664)
2022-10-30 00:49:49,488:INFO: Dataset: zara1               Batch: 4/8	Loss 93.9441 (149.1769)
2022-10-30 00:49:50,529:INFO: Dataset: zara1               Batch: 5/8	Loss 95.7013 (138.2863)
2022-10-30 00:49:51,579:INFO: Dataset: zara1               Batch: 6/8	Loss 88.7121 (128.9670)
2022-10-30 00:49:52,627:INFO: Dataset: zara1               Batch: 7/8	Loss 93.9865 (123.5725)
2022-10-30 00:49:53,578:INFO: Dataset: zara1               Batch: 8/8	Loss 141.9382 (125.7668)
2022-10-30 00:49:54,989:INFO: Dataset: zara2               Batch:  1/18	Loss 108.0966 (108.0966)
2022-10-30 00:49:56,068:INFO: Dataset: zara2               Batch:  2/18	Loss 99.0723 (102.7966)
2022-10-30 00:49:57,152:INFO: Dataset: zara2               Batch:  3/18	Loss 96.0884 (100.6416)
2022-10-30 00:49:58,233:INFO: Dataset: zara2               Batch:  4/18	Loss 92.1883 (98.6340)
2022-10-30 00:49:59,313:INFO: Dataset: zara2               Batch:  5/18	Loss 96.7181 (98.2379)
2022-10-30 00:50:00,396:INFO: Dataset: zara2               Batch:  6/18	Loss 93.1066 (97.3421)
2022-10-30 00:50:01,487:INFO: Dataset: zara2               Batch:  7/18	Loss 98.3953 (97.5049)
2022-10-30 00:50:02,580:INFO: Dataset: zara2               Batch:  8/18	Loss 94.0795 (97.0904)
2022-10-30 00:50:03,675:INFO: Dataset: zara2               Batch:  9/18	Loss 88.3111 (96.1555)
2022-10-30 00:50:04,775:INFO: Dataset: zara2               Batch: 10/18	Loss 90.3216 (95.5663)
2022-10-30 00:50:05,851:INFO: Dataset: zara2               Batch: 11/18	Loss 101.6195 (96.1462)
2022-10-30 00:50:06,927:INFO: Dataset: zara2               Batch: 12/18	Loss 116.7721 (97.9631)
2022-10-30 00:50:07,999:INFO: Dataset: zara2               Batch: 13/18	Loss 129.6991 (100.1731)
2022-10-30 00:50:09,066:INFO: Dataset: zara2               Batch: 14/18	Loss 104.7998 (100.5120)
2022-10-30 00:50:10,139:INFO: Dataset: zara2               Batch: 15/18	Loss 89.1171 (99.7900)
2022-10-30 00:50:11,217:INFO: Dataset: zara2               Batch: 16/18	Loss 91.4525 (99.2405)
2022-10-30 00:50:12,279:INFO: Dataset: zara2               Batch: 17/18	Loss 126.9840 (100.6373)
2022-10-30 00:50:13,238:INFO: Dataset: zara2               Batch: 18/18	Loss 80.7836 (99.8200)
2022-10-30 00:50:13,306:INFO: - Computing ADE (validation o)
2022-10-30 00:50:13,655:INFO: 		 ADE on eth                       dataset:	 4.4541215896606445
2022-10-30 00:50:13,656:INFO: Average validation o:	ADE  4.4541	FDE  7.9031
2022-10-30 00:50:13,656:INFO: - Computing ADE (validation)
2022-10-30 00:50:14,007:INFO: 		 ADE on hotel                     dataset:	 2.112581729888916
2022-10-30 00:50:14,449:INFO: 		 ADE on univ                      dataset:	 2.180971145629883
2022-10-30 00:50:14,819:INFO: 		 ADE on zara1                     dataset:	 2.12256121635437
2022-10-30 00:50:15,373:INFO: 		 ADE on zara2                     dataset:	 2.399545431137085
2022-10-30 00:50:15,373:INFO: Average validation:	ADE  2.2540	FDE  4.1984
2022-10-30 00:50:15,374:INFO: - Computing ADE (training)
2022-10-30 00:50:15,840:INFO: 		 ADE on hotel                     dataset:	 2.222480058670044
2022-10-30 00:50:16,928:INFO: 		 ADE on univ                      dataset:	 2.114408254623413
2022-10-30 00:50:17,621:INFO: 		 ADE on zara1                     dataset:	 3.1162631511688232
2022-10-30 00:50:18,847:INFO: 		 ADE on zara2                     dataset:	 2.794989585876465
2022-10-30 00:50:18,847:INFO: Average training:	ADE  2.3191	FDE  4.3230
2022-10-30 00:50:18,858:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_460.pth.tar
2022-10-30 00:50:18,858:INFO: 
===> EPOCH: 461 (P4)
2022-10-30 00:50:18,859:INFO: - Computing loss (training)
2022-10-30 00:50:20,137:INFO: Dataset: hotel               Batch: 1/4	Loss 83.7066 (83.7066)
2022-10-30 00:50:21,189:INFO: Dataset: hotel               Batch: 2/4	Loss 84.8503 (84.2798)
2022-10-30 00:50:22,235:INFO: Dataset: hotel               Batch: 3/4	Loss 87.7891 (85.4697)
2022-10-30 00:50:23,004:INFO: Dataset: hotel               Batch: 4/4	Loss 50.5241 (79.8914)
2022-10-30 00:50:24,426:INFO: Dataset: univ                Batch:  1/15	Loss 97.2424 (97.2424)
2022-10-30 00:50:25,534:INFO: Dataset: univ                Batch:  2/15	Loss 88.2244 (92.9666)
2022-10-30 00:50:26,648:INFO: Dataset: univ                Batch:  3/15	Loss 92.6827 (92.8724)
2022-10-30 00:50:27,762:INFO: Dataset: univ                Batch:  4/15	Loss 93.3037 (92.9859)
2022-10-30 00:50:28,878:INFO: Dataset: univ                Batch:  5/15	Loss 97.6202 (93.8886)
2022-10-30 00:50:30,003:INFO: Dataset: univ                Batch:  6/15	Loss 91.1926 (93.3769)
2022-10-30 00:50:31,120:INFO: Dataset: univ                Batch:  7/15	Loss 100.9306 (94.5090)
2022-10-30 00:50:32,243:INFO: Dataset: univ                Batch:  8/15	Loss 88.0799 (93.6481)
2022-10-30 00:50:33,358:INFO: Dataset: univ                Batch:  9/15	Loss 134.7314 (98.1882)
2022-10-30 00:50:34,476:INFO: Dataset: univ                Batch: 10/15	Loss 111.7889 (99.6416)
2022-10-30 00:50:35,600:INFO: Dataset: univ                Batch: 11/15	Loss 91.2970 (98.8573)
2022-10-30 00:50:36,729:INFO: Dataset: univ                Batch: 12/15	Loss 122.2582 (101.0563)
2022-10-30 00:50:37,845:INFO: Dataset: univ                Batch: 13/15	Loss 89.7446 (100.2112)
2022-10-30 00:50:38,968:INFO: Dataset: univ                Batch: 14/15	Loss 102.7840 (100.3874)
2022-10-30 00:50:39,449:INFO: Dataset: univ                Batch: 15/15	Loss 16.4183 (99.1538)
2022-10-30 00:50:40,823:INFO: Dataset: zara1               Batch: 1/8	Loss 99.3676 (99.3676)
2022-10-30 00:50:41,891:INFO: Dataset: zara1               Batch: 2/8	Loss 98.4748 (98.9395)
2022-10-30 00:50:42,960:INFO: Dataset: zara1               Batch: 3/8	Loss 97.5075 (98.4849)
2022-10-30 00:50:44,034:INFO: Dataset: zara1               Batch: 4/8	Loss 89.3004 (96.0288)
2022-10-30 00:50:45,184:INFO: Dataset: zara1               Batch: 5/8	Loss 95.1337 (95.8486)
2022-10-30 00:50:46,246:INFO: Dataset: zara1               Batch: 6/8	Loss 93.2304 (95.4400)
2022-10-30 00:50:47,310:INFO: Dataset: zara1               Batch: 7/8	Loss 93.3266 (95.1109)
2022-10-30 00:50:48,268:INFO: Dataset: zara1               Batch: 8/8	Loss 79.0614 (93.5482)
2022-10-30 00:50:49,636:INFO: Dataset: zara2               Batch:  1/18	Loss 107.8022 (107.8022)
2022-10-30 00:50:50,691:INFO: Dataset: zara2               Batch:  2/18	Loss 91.8331 (100.1865)
2022-10-30 00:50:51,735:INFO: Dataset: zara2               Batch:  3/18	Loss 98.4989 (99.5994)
2022-10-30 00:50:52,788:INFO: Dataset: zara2               Batch:  4/18	Loss 155.5086 (114.1792)
2022-10-30 00:50:53,841:INFO: Dataset: zara2               Batch:  5/18	Loss 93.0453 (110.1789)
2022-10-30 00:50:54,895:INFO: Dataset: zara2               Batch:  6/18	Loss 89.6826 (106.7413)
2022-10-30 00:50:55,946:INFO: Dataset: zara2               Batch:  7/18	Loss 100.5692 (105.8592)
2022-10-30 00:50:56,996:INFO: Dataset: zara2               Batch:  8/18	Loss 130.0423 (108.9292)
2022-10-30 00:50:58,045:INFO: Dataset: zara2               Batch:  9/18	Loss 123.1229 (110.5403)
2022-10-30 00:50:59,100:INFO: Dataset: zara2               Batch: 10/18	Loss 94.9863 (108.9831)
2022-10-30 00:51:00,160:INFO: Dataset: zara2               Batch: 11/18	Loss 99.8220 (108.0821)
2022-10-30 00:51:01,216:INFO: Dataset: zara2               Batch: 12/18	Loss 97.2381 (107.2249)
2022-10-30 00:51:02,271:INFO: Dataset: zara2               Batch: 13/18	Loss 90.3458 (105.9709)
2022-10-30 00:51:03,318:INFO: Dataset: zara2               Batch: 14/18	Loss 100.7564 (105.6255)
2022-10-30 00:51:04,372:INFO: Dataset: zara2               Batch: 15/18	Loss 95.7208 (105.0046)
2022-10-30 00:51:05,427:INFO: Dataset: zara2               Batch: 16/18	Loss 91.5960 (104.2253)
2022-10-30 00:51:06,475:INFO: Dataset: zara2               Batch: 17/18	Loss 93.4875 (103.5917)
2022-10-30 00:51:07,423:INFO: Dataset: zara2               Batch: 18/18	Loss 79.3553 (102.5780)
2022-10-30 00:51:07,489:INFO: - Computing ADE (validation o)
2022-10-30 00:51:07,846:INFO: 		 ADE on eth                       dataset:	 4.4835429191589355
2022-10-30 00:51:07,846:INFO: Average validation o:	ADE  4.4835	FDE  7.9385
2022-10-30 00:51:07,847:INFO: - Computing ADE (validation)
2022-10-30 00:51:08,185:INFO: 		 ADE on hotel                     dataset:	 2.093317747116089
2022-10-30 00:51:08,627:INFO: 		 ADE on univ                      dataset:	 2.1921401023864746
2022-10-30 00:51:08,996:INFO: 		 ADE on zara1                     dataset:	 2.1765248775482178
2022-10-30 00:51:09,563:INFO: 		 ADE on zara2                     dataset:	 2.3740270137786865
2022-10-30 00:51:09,563:INFO: Average validation:	ADE  2.2525	FDE  4.1925
2022-10-30 00:51:09,564:INFO: - Computing ADE (training)
2022-10-30 00:51:10,040:INFO: 		 ADE on hotel                     dataset:	 2.244575262069702
2022-10-30 00:51:11,117:INFO: 		 ADE on univ                      dataset:	 2.1142055988311768
2022-10-30 00:51:11,794:INFO: 		 ADE on zara1                     dataset:	 3.1009576320648193
2022-10-30 00:51:12,984:INFO: 		 ADE on zara2                     dataset:	 2.7850799560546875
2022-10-30 00:51:12,984:INFO: Average training:	ADE  2.3166	FDE  4.3173
2022-10-30 00:51:12,995:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_461.pth.tar
2022-10-30 00:51:12,995:INFO: 
===> EPOCH: 462 (P4)
2022-10-30 00:51:12,995:INFO: - Computing loss (training)
2022-10-30 00:51:14,284:INFO: Dataset: hotel               Batch: 1/4	Loss 87.1331 (87.1331)
2022-10-30 00:51:15,341:INFO: Dataset: hotel               Batch: 2/4	Loss 82.8138 (84.9630)
2022-10-30 00:51:16,392:INFO: Dataset: hotel               Batch: 3/4	Loss 84.2796 (84.7287)
2022-10-30 00:51:17,159:INFO: Dataset: hotel               Batch: 4/4	Loss 52.3939 (79.2685)
2022-10-30 00:51:18,607:INFO: Dataset: univ                Batch:  1/15	Loss 91.0514 (91.0514)
2022-10-30 00:51:19,754:INFO: Dataset: univ                Batch:  2/15	Loss 103.1362 (97.4827)
2022-10-30 00:51:20,906:INFO: Dataset: univ                Batch:  3/15	Loss 105.3379 (100.2963)
2022-10-30 00:51:22,045:INFO: Dataset: univ                Batch:  4/15	Loss 92.0443 (98.2017)
2022-10-30 00:51:23,183:INFO: Dataset: univ                Batch:  5/15	Loss 112.8506 (100.8542)
2022-10-30 00:51:24,329:INFO: Dataset: univ                Batch:  6/15	Loss 211.8556 (121.7972)
2022-10-30 00:51:25,465:INFO: Dataset: univ                Batch:  7/15	Loss 113.4514 (120.7310)
2022-10-30 00:51:26,598:INFO: Dataset: univ                Batch:  8/15	Loss 94.1238 (117.4202)
2022-10-30 00:51:27,734:INFO: Dataset: univ                Batch:  9/15	Loss 95.3221 (114.8022)
2022-10-30 00:51:28,875:INFO: Dataset: univ                Batch: 10/15	Loss 157.2990 (119.4626)
2022-10-30 00:51:30,013:INFO: Dataset: univ                Batch: 11/15	Loss 149.8022 (122.3948)
2022-10-30 00:51:31,143:INFO: Dataset: univ                Batch: 12/15	Loss 88.2861 (119.7389)
2022-10-30 00:51:32,294:INFO: Dataset: univ                Batch: 13/15	Loss 89.3831 (117.1500)
2022-10-30 00:51:33,445:INFO: Dataset: univ                Batch: 14/15	Loss 85.3724 (114.5391)
2022-10-30 00:51:33,928:INFO: Dataset: univ                Batch: 15/15	Loss 16.9705 (113.3693)
2022-10-30 00:51:35,301:INFO: Dataset: zara1               Batch: 1/8	Loss 101.7513 (101.7513)
2022-10-30 00:51:36,381:INFO: Dataset: zara1               Batch: 2/8	Loss 96.8620 (99.1822)
2022-10-30 00:51:37,447:INFO: Dataset: zara1               Batch: 3/8	Loss 91.7313 (96.7496)
2022-10-30 00:51:38,520:INFO: Dataset: zara1               Batch: 4/8	Loss 92.8808 (95.7705)
2022-10-30 00:51:39,580:INFO: Dataset: zara1               Batch: 5/8	Loss 93.9908 (95.4264)
2022-10-30 00:51:40,651:INFO: Dataset: zara1               Batch: 6/8	Loss 90.9168 (94.6363)
2022-10-30 00:51:41,719:INFO: Dataset: zara1               Batch: 7/8	Loss 98.4557 (95.1618)
2022-10-30 00:51:42,690:INFO: Dataset: zara1               Batch: 8/8	Loss 79.8866 (93.5619)
2022-10-30 00:51:44,103:INFO: Dataset: zara2               Batch:  1/18	Loss 107.7578 (107.7578)
2022-10-30 00:51:45,161:INFO: Dataset: zara2               Batch:  2/18	Loss 97.7615 (102.4981)
2022-10-30 00:51:46,216:INFO: Dataset: zara2               Batch:  3/18	Loss 375.4039 (184.3976)
2022-10-30 00:51:47,294:INFO: Dataset: zara2               Batch:  4/18	Loss 128.4592 (169.7721)
2022-10-30 00:51:48,363:INFO: Dataset: zara2               Batch:  5/18	Loss 93.9258 (153.3439)
2022-10-30 00:51:49,437:INFO: Dataset: zara2               Batch:  6/18	Loss 92.9065 (144.2481)
2022-10-30 00:51:50,521:INFO: Dataset: zara2               Batch:  7/18	Loss 92.3218 (136.8746)
2022-10-30 00:51:51,598:INFO: Dataset: zara2               Batch:  8/18	Loss 90.7221 (131.0299)
2022-10-30 00:51:52,670:INFO: Dataset: zara2               Batch:  9/18	Loss 94.0374 (126.9484)
2022-10-30 00:51:53,746:INFO: Dataset: zara2               Batch: 10/18	Loss 97.8670 (124.1058)
2022-10-30 00:51:54,820:INFO: Dataset: zara2               Batch: 11/18	Loss 89.4289 (120.9750)
2022-10-30 00:51:55,893:INFO: Dataset: zara2               Batch: 12/18	Loss 106.3083 (119.8248)
2022-10-30 00:51:56,980:INFO: Dataset: zara2               Batch: 13/18	Loss 90.4832 (117.5018)
2022-10-30 00:51:58,053:INFO: Dataset: zara2               Batch: 14/18	Loss 89.7736 (115.4178)
2022-10-30 00:51:59,125:INFO: Dataset: zara2               Batch: 15/18	Loss 101.7795 (114.4442)
2022-10-30 00:52:00,217:INFO: Dataset: zara2               Batch: 16/18	Loss 107.2727 (113.9839)
2022-10-30 00:52:01,294:INFO: Dataset: zara2               Batch: 17/18	Loss 92.2492 (112.6478)
2022-10-30 00:52:02,265:INFO: Dataset: zara2               Batch: 18/18	Loss 94.1997 (111.6140)
2022-10-30 00:52:02,328:INFO: - Computing ADE (validation o)
2022-10-30 00:52:02,682:INFO: 		 ADE on eth                       dataset:	 4.46784782409668
2022-10-30 00:52:02,682:INFO: Average validation o:	ADE  4.4678	FDE  7.9140
2022-10-30 00:52:02,683:INFO: - Computing ADE (validation)
2022-10-30 00:52:03,035:INFO: 		 ADE on hotel                     dataset:	 2.076361656188965
2022-10-30 00:52:03,496:INFO: 		 ADE on univ                      dataset:	 2.1725738048553467
2022-10-30 00:52:03,867:INFO: 		 ADE on zara1                     dataset:	 2.1551172733306885
2022-10-30 00:52:04,427:INFO: 		 ADE on zara2                     dataset:	 2.3711507320404053
2022-10-30 00:52:04,427:INFO: Average validation:	ADE  2.2391	FDE  4.1682
2022-10-30 00:52:04,428:INFO: - Computing ADE (training)
2022-10-30 00:52:04,904:INFO: 		 ADE on hotel                     dataset:	 2.214582681655884
2022-10-30 00:52:05,982:INFO: 		 ADE on univ                      dataset:	 2.1032931804656982
2022-10-30 00:52:06,680:INFO: 		 ADE on zara1                     dataset:	 3.0990233421325684
2022-10-30 00:52:07,917:INFO: 		 ADE on zara2                     dataset:	 2.7919981479644775
2022-10-30 00:52:07,917:INFO: Average training:	ADE  2.3093	FDE  4.3035
2022-10-30 00:52:07,928:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_462.pth.tar
2022-10-30 00:52:07,929:INFO: 
===> EPOCH: 463 (P4)
2022-10-30 00:52:07,929:INFO: - Computing loss (training)
2022-10-30 00:52:09,244:INFO: Dataset: hotel               Batch: 1/4	Loss 84.7493 (84.7493)
2022-10-30 00:52:10,322:INFO: Dataset: hotel               Batch: 2/4	Loss 86.2557 (85.4718)
2022-10-30 00:52:11,388:INFO: Dataset: hotel               Batch: 3/4	Loss 83.9582 (84.9665)
2022-10-30 00:52:12,171:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7705 (79.0115)
2022-10-30 00:52:13,612:INFO: Dataset: univ                Batch:  1/15	Loss 105.9771 (105.9771)
2022-10-30 00:52:14,741:INFO: Dataset: univ                Batch:  2/15	Loss 96.9502 (101.2760)
2022-10-30 00:52:15,873:INFO: Dataset: univ                Batch:  3/15	Loss 204.2730 (136.1294)
2022-10-30 00:52:16,998:INFO: Dataset: univ                Batch:  4/15	Loss 89.9357 (124.6964)
2022-10-30 00:52:18,131:INFO: Dataset: univ                Batch:  5/15	Loss 90.2653 (117.1126)
2022-10-30 00:52:19,281:INFO: Dataset: univ                Batch:  6/15	Loss 94.8873 (113.1258)
2022-10-30 00:52:20,420:INFO: Dataset: univ                Batch:  7/15	Loss 87.5810 (109.5965)
2022-10-30 00:52:21,552:INFO: Dataset: univ                Batch:  8/15	Loss 93.6825 (107.5568)
2022-10-30 00:52:22,682:INFO: Dataset: univ                Batch:  9/15	Loss 95.2931 (106.2426)
2022-10-30 00:52:23,817:INFO: Dataset: univ                Batch: 10/15	Loss 116.1605 (107.3205)
2022-10-30 00:52:24,959:INFO: Dataset: univ                Batch: 11/15	Loss 103.0740 (106.9462)
2022-10-30 00:52:26,104:INFO: Dataset: univ                Batch: 12/15	Loss 97.9859 (106.1600)
2022-10-30 00:52:27,234:INFO: Dataset: univ                Batch: 13/15	Loss 93.5066 (105.1096)
2022-10-30 00:52:28,363:INFO: Dataset: univ                Batch: 14/15	Loss 93.1280 (104.1997)
2022-10-30 00:52:28,846:INFO: Dataset: univ                Batch: 15/15	Loss 16.9325 (102.8681)
2022-10-30 00:52:30,205:INFO: Dataset: zara1               Batch: 1/8	Loss 90.4237 (90.4237)
2022-10-30 00:52:31,274:INFO: Dataset: zara1               Batch: 2/8	Loss 90.5827 (90.5008)
2022-10-30 00:52:32,324:INFO: Dataset: zara1               Batch: 3/8	Loss 197.7444 (123.0073)
2022-10-30 00:52:33,374:INFO: Dataset: zara1               Batch: 4/8	Loss 93.8871 (115.9487)
2022-10-30 00:52:34,428:INFO: Dataset: zara1               Batch: 5/8	Loss 91.2004 (111.5543)
2022-10-30 00:52:35,483:INFO: Dataset: zara1               Batch: 6/8	Loss 92.8241 (108.5362)
2022-10-30 00:52:36,621:INFO: Dataset: zara1               Batch: 7/8	Loss 94.2669 (106.7010)
2022-10-30 00:52:37,569:INFO: Dataset: zara1               Batch: 8/8	Loss 86.5635 (104.5282)
2022-10-30 00:52:38,960:INFO: Dataset: zara2               Batch:  1/18	Loss 90.8707 (90.8707)
2022-10-30 00:52:40,043:INFO: Dataset: zara2               Batch:  2/18	Loss 94.7527 (92.7096)
2022-10-30 00:52:41,124:INFO: Dataset: zara2               Batch:  3/18	Loss 91.7087 (92.3836)
2022-10-30 00:52:42,209:INFO: Dataset: zara2               Batch:  4/18	Loss 144.9429 (106.8265)
2022-10-30 00:52:43,293:INFO: Dataset: zara2               Batch:  5/18	Loss 100.4381 (105.5713)
2022-10-30 00:52:44,379:INFO: Dataset: zara2               Batch:  6/18	Loss 97.2564 (104.1050)
2022-10-30 00:52:45,463:INFO: Dataset: zara2               Batch:  7/18	Loss 207.1250 (118.9105)
2022-10-30 00:52:46,548:INFO: Dataset: zara2               Batch:  8/18	Loss 89.7141 (115.3090)
2022-10-30 00:52:47,645:INFO: Dataset: zara2               Batch:  9/18	Loss 103.2000 (113.8439)
2022-10-30 00:52:48,722:INFO: Dataset: zara2               Batch: 10/18	Loss 100.4495 (112.4159)
2022-10-30 00:52:49,788:INFO: Dataset: zara2               Batch: 11/18	Loss 90.1102 (110.3316)
2022-10-30 00:52:50,866:INFO: Dataset: zara2               Batch: 12/18	Loss 88.6088 (108.6124)
2022-10-30 00:52:51,941:INFO: Dataset: zara2               Batch: 13/18	Loss 90.7504 (107.1637)
2022-10-30 00:52:53,011:INFO: Dataset: zara2               Batch: 14/18	Loss 124.1994 (108.2816)
2022-10-30 00:52:54,086:INFO: Dataset: zara2               Batch: 15/18	Loss 91.1111 (107.1161)
2022-10-30 00:52:55,167:INFO: Dataset: zara2               Batch: 16/18	Loss 115.9107 (107.6630)
2022-10-30 00:52:56,243:INFO: Dataset: zara2               Batch: 17/18	Loss 92.1364 (106.7439)
2022-10-30 00:52:57,221:INFO: Dataset: zara2               Batch: 18/18	Loss 79.4733 (105.3689)
2022-10-30 00:52:57,287:INFO: - Computing ADE (validation o)
2022-10-30 00:52:57,650:INFO: 		 ADE on eth                       dataset:	 4.471017837524414
2022-10-30 00:52:57,650:INFO: Average validation o:	ADE  4.4710	FDE  7.9156
2022-10-30 00:52:57,651:INFO: - Computing ADE (validation)
2022-10-30 00:52:57,992:INFO: 		 ADE on hotel                     dataset:	 2.0751430988311768
2022-10-30 00:52:58,430:INFO: 		 ADE on univ                      dataset:	 2.1750659942626953
2022-10-30 00:52:58,779:INFO: 		 ADE on zara1                     dataset:	 2.1572585105895996
2022-10-30 00:52:59,337:INFO: 		 ADE on zara2                     dataset:	 2.3920860290527344
2022-10-30 00:52:59,337:INFO: Average validation:	ADE  2.2482	FDE  4.1810
2022-10-30 00:52:59,338:INFO: - Computing ADE (training)
2022-10-30 00:52:59,811:INFO: 		 ADE on hotel                     dataset:	 2.220209836959839
2022-10-30 00:53:00,896:INFO: 		 ADE on univ                      dataset:	 2.1060781478881836
2022-10-30 00:53:01,600:INFO: 		 ADE on zara1                     dataset:	 3.11643648147583
2022-10-30 00:53:02,821:INFO: 		 ADE on zara2                     dataset:	 2.7847394943237305
2022-10-30 00:53:02,821:INFO: Average training:	ADE  2.3111	FDE  4.3022
2022-10-30 00:53:02,832:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_463.pth.tar
2022-10-30 00:53:02,832:INFO: 
===> EPOCH: 464 (P4)
2022-10-30 00:53:02,832:INFO: - Computing loss (training)
2022-10-30 00:53:04,139:INFO: Dataset: hotel               Batch: 1/4	Loss 83.3957 (83.3957)
2022-10-30 00:53:05,212:INFO: Dataset: hotel               Batch: 2/4	Loss 84.5699 (83.9800)
2022-10-30 00:53:06,290:INFO: Dataset: hotel               Batch: 3/4	Loss 86.4922 (84.7985)
2022-10-30 00:53:07,073:INFO: Dataset: hotel               Batch: 4/4	Loss 51.6194 (78.7580)
2022-10-30 00:53:08,493:INFO: Dataset: univ                Batch:  1/15	Loss 95.1260 (95.1260)
2022-10-30 00:53:09,602:INFO: Dataset: univ                Batch:  2/15	Loss 89.0768 (92.0256)
2022-10-30 00:53:10,739:INFO: Dataset: univ                Batch:  3/15	Loss 92.2149 (92.0901)
2022-10-30 00:53:11,875:INFO: Dataset: univ                Batch:  4/15	Loss 87.1733 (90.8364)
2022-10-30 00:53:12,987:INFO: Dataset: univ                Batch:  5/15	Loss 109.1585 (94.7278)
2022-10-30 00:53:14,101:INFO: Dataset: univ                Batch:  6/15	Loss 93.7766 (94.5767)
2022-10-30 00:53:15,211:INFO: Dataset: univ                Batch:  7/15	Loss 87.4364 (93.6080)
2022-10-30 00:53:16,323:INFO: Dataset: univ                Batch:  8/15	Loss 123.8564 (97.2802)
2022-10-30 00:53:17,436:INFO: Dataset: univ                Batch:  9/15	Loss 138.4960 (101.9910)
2022-10-30 00:53:18,551:INFO: Dataset: univ                Batch: 10/15	Loss 115.3355 (103.3492)
2022-10-30 00:53:19,676:INFO: Dataset: univ                Batch: 11/15	Loss 101.4716 (103.1678)
2022-10-30 00:53:20,796:INFO: Dataset: univ                Batch: 12/15	Loss 100.8292 (102.9803)
2022-10-30 00:53:21,919:INFO: Dataset: univ                Batch: 13/15	Loss 90.7228 (102.0332)
2022-10-30 00:53:23,043:INFO: Dataset: univ                Batch: 14/15	Loss 98.5845 (101.7951)
2022-10-30 00:53:23,524:INFO: Dataset: univ                Batch: 15/15	Loss 15.1903 (100.3176)
2022-10-30 00:53:24,901:INFO: Dataset: zara1               Batch: 1/8	Loss 99.1589 (99.1589)
2022-10-30 00:53:25,965:INFO: Dataset: zara1               Batch: 2/8	Loss 95.5575 (97.3546)
2022-10-30 00:53:27,016:INFO: Dataset: zara1               Batch: 3/8	Loss 105.1735 (99.9781)
2022-10-30 00:53:28,075:INFO: Dataset: zara1               Batch: 4/8	Loss 93.5740 (98.4660)
2022-10-30 00:53:29,128:INFO: Dataset: zara1               Batch: 5/8	Loss 92.5040 (97.3114)
2022-10-30 00:53:30,174:INFO: Dataset: zara1               Batch: 6/8	Loss 98.0250 (97.4259)
2022-10-30 00:53:31,214:INFO: Dataset: zara1               Batch: 7/8	Loss 90.2670 (96.3961)
2022-10-30 00:53:32,159:INFO: Dataset: zara1               Batch: 8/8	Loss 79.0857 (94.6650)
2022-10-30 00:53:33,542:INFO: Dataset: zara2               Batch:  1/18	Loss 88.7936 (88.7936)
2022-10-30 00:53:34,612:INFO: Dataset: zara2               Batch:  2/18	Loss 101.2406 (94.8175)
2022-10-30 00:53:35,683:INFO: Dataset: zara2               Batch:  3/18	Loss 89.9298 (93.3689)
2022-10-30 00:53:36,755:INFO: Dataset: zara2               Batch:  4/18	Loss 171.8329 (111.8518)
2022-10-30 00:53:37,827:INFO: Dataset: zara2               Batch:  5/18	Loss 117.8417 (113.0805)
2022-10-30 00:53:38,903:INFO: Dataset: zara2               Batch:  6/18	Loss 98.8995 (110.5003)
2022-10-30 00:53:39,976:INFO: Dataset: zara2               Batch:  7/18	Loss 93.1214 (107.9782)
2022-10-30 00:53:41,051:INFO: Dataset: zara2               Batch:  8/18	Loss 131.7351 (110.8367)
2022-10-30 00:53:42,131:INFO: Dataset: zara2               Batch:  9/18	Loss 91.9127 (108.7108)
2022-10-30 00:53:43,205:INFO: Dataset: zara2               Batch: 10/18	Loss 89.8127 (106.7773)
2022-10-30 00:53:44,278:INFO: Dataset: zara2               Batch: 11/18	Loss 89.3292 (105.3879)
2022-10-30 00:53:45,355:INFO: Dataset: zara2               Batch: 12/18	Loss 113.4575 (106.0696)
2022-10-30 00:53:46,432:INFO: Dataset: zara2               Batch: 13/18	Loss 104.2066 (105.9221)
2022-10-30 00:53:47,506:INFO: Dataset: zara2               Batch: 14/18	Loss 94.7212 (105.1642)
2022-10-30 00:53:48,576:INFO: Dataset: zara2               Batch: 15/18	Loss 97.7248 (104.6955)
2022-10-30 00:53:49,638:INFO: Dataset: zara2               Batch: 16/18	Loss 102.6757 (104.5648)
2022-10-30 00:53:50,708:INFO: Dataset: zara2               Batch: 17/18	Loss 92.0630 (103.7890)
2022-10-30 00:53:51,674:INFO: Dataset: zara2               Batch: 18/18	Loss 83.5324 (102.7978)
2022-10-30 00:53:51,742:INFO: - Computing ADE (validation o)
2022-10-30 00:53:52,110:INFO: 		 ADE on eth                       dataset:	 4.446582794189453
2022-10-30 00:53:52,111:INFO: Average validation o:	ADE  4.4466	FDE  7.8594
2022-10-30 00:53:52,111:INFO: - Computing ADE (validation)
2022-10-30 00:53:52,470:INFO: 		 ADE on hotel                     dataset:	 2.0803866386413574
2022-10-30 00:53:52,908:INFO: 		 ADE on univ                      dataset:	 2.1765120029449463
2022-10-30 00:53:53,280:INFO: 		 ADE on zara1                     dataset:	 2.1637473106384277
2022-10-30 00:53:53,832:INFO: 		 ADE on zara2                     dataset:	 2.363564968109131
2022-10-30 00:53:53,832:INFO: Average validation:	ADE  2.2391	FDE  4.1641
2022-10-30 00:53:53,833:INFO: - Computing ADE (training)
2022-10-30 00:53:54,310:INFO: 		 ADE on hotel                     dataset:	 2.20162296295166
2022-10-30 00:53:55,388:INFO: 		 ADE on univ                      dataset:	 2.098439931869507
2022-10-30 00:53:56,099:INFO: 		 ADE on zara1                     dataset:	 3.107752799987793
2022-10-30 00:53:57,307:INFO: 		 ADE on zara2                     dataset:	 2.779078245162964
2022-10-30 00:53:57,307:INFO: Average training:	ADE  2.3035	FDE  4.2899
2022-10-30 00:53:57,318:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_464.pth.tar
2022-10-30 00:53:57,319:INFO: 
===> EPOCH: 465 (P4)
2022-10-30 00:53:57,319:INFO: - Computing loss (training)
2022-10-30 00:53:58,601:INFO: Dataset: hotel               Batch: 1/4	Loss 84.2812 (84.2812)
2022-10-30 00:53:59,651:INFO: Dataset: hotel               Batch: 2/4	Loss 85.1366 (84.6848)
2022-10-30 00:54:00,700:INFO: Dataset: hotel               Batch: 3/4	Loss 93.9682 (87.7598)
2022-10-30 00:54:01,466:INFO: Dataset: hotel               Batch: 4/4	Loss 51.3078 (81.9410)
2022-10-30 00:54:02,930:INFO: Dataset: univ                Batch:  1/15	Loss 89.4005 (89.4005)
2022-10-30 00:54:04,054:INFO: Dataset: univ                Batch:  2/15	Loss 89.6378 (89.5109)
2022-10-30 00:54:05,179:INFO: Dataset: univ                Batch:  3/15	Loss 113.2137 (97.3720)
2022-10-30 00:54:06,311:INFO: Dataset: univ                Batch:  4/15	Loss 111.8438 (100.9888)
2022-10-30 00:54:07,435:INFO: Dataset: univ                Batch:  5/15	Loss 146.4062 (109.8293)
2022-10-30 00:54:08,551:INFO: Dataset: univ                Batch:  6/15	Loss 93.8107 (107.5140)
2022-10-30 00:54:09,680:INFO: Dataset: univ                Batch:  7/15	Loss 103.7361 (106.9497)
2022-10-30 00:54:10,805:INFO: Dataset: univ                Batch:  8/15	Loss 87.9557 (104.5693)
2022-10-30 00:54:11,936:INFO: Dataset: univ                Batch:  9/15	Loss 94.6865 (103.3995)
2022-10-30 00:54:13,057:INFO: Dataset: univ                Batch: 10/15	Loss 88.7478 (101.9975)
2022-10-30 00:54:14,192:INFO: Dataset: univ                Batch: 11/15	Loss 571.7869 (146.8291)
2022-10-30 00:54:15,323:INFO: Dataset: univ                Batch: 12/15	Loss 280.7391 (157.5928)
2022-10-30 00:54:16,448:INFO: Dataset: univ                Batch: 13/15	Loss 90.8034 (153.0520)
2022-10-30 00:54:17,589:INFO: Dataset: univ                Batch: 14/15	Loss 87.8289 (148.1468)
2022-10-30 00:54:18,071:INFO: Dataset: univ                Batch: 15/15	Loss 15.8954 (145.9783)
2022-10-30 00:54:19,435:INFO: Dataset: zara1               Batch: 1/8	Loss 100.1943 (100.1943)
2022-10-30 00:54:20,504:INFO: Dataset: zara1               Batch: 2/8	Loss 123.6918 (111.6282)
2022-10-30 00:54:21,567:INFO: Dataset: zara1               Batch: 3/8	Loss 93.9160 (105.2341)
2022-10-30 00:54:22,620:INFO: Dataset: zara1               Batch: 4/8	Loss 95.7219 (102.8914)
2022-10-30 00:54:23,677:INFO: Dataset: zara1               Batch: 5/8	Loss 91.5697 (100.8995)
2022-10-30 00:54:24,722:INFO: Dataset: zara1               Batch: 6/8	Loss 102.9857 (101.2299)
2022-10-30 00:54:25,772:INFO: Dataset: zara1               Batch: 7/8	Loss 92.2100 (99.9919)
2022-10-30 00:54:26,727:INFO: Dataset: zara1               Batch: 8/8	Loss 80.8882 (97.8101)
2022-10-30 00:54:28,216:INFO: Dataset: zara2               Batch:  1/18	Loss 118.5038 (118.5038)
2022-10-30 00:54:29,298:INFO: Dataset: zara2               Batch:  2/18	Loss 100.6806 (109.9694)
2022-10-30 00:54:30,376:INFO: Dataset: zara2               Batch:  3/18	Loss 90.9470 (103.5978)
2022-10-30 00:54:31,457:INFO: Dataset: zara2               Batch:  4/18	Loss 111.4091 (105.5789)
2022-10-30 00:54:32,537:INFO: Dataset: zara2               Batch:  5/18	Loss 89.4203 (102.3173)
2022-10-30 00:54:33,604:INFO: Dataset: zara2               Batch:  6/18	Loss 94.4553 (100.9969)
2022-10-30 00:54:34,655:INFO: Dataset: zara2               Batch:  7/18	Loss 89.6698 (99.3841)
2022-10-30 00:54:35,714:INFO: Dataset: zara2               Batch:  8/18	Loss 124.2465 (102.2042)
2022-10-30 00:54:36,778:INFO: Dataset: zara2               Batch:  9/18	Loss 91.0618 (101.0559)
2022-10-30 00:54:37,846:INFO: Dataset: zara2               Batch: 10/18	Loss 89.3716 (99.7392)
2022-10-30 00:54:38,915:INFO: Dataset: zara2               Batch: 11/18	Loss 89.0821 (98.7714)
2022-10-30 00:54:39,984:INFO: Dataset: zara2               Batch: 12/18	Loss 98.0449 (98.7107)
2022-10-30 00:54:41,053:INFO: Dataset: zara2               Batch: 13/18	Loss 101.8138 (98.9594)
2022-10-30 00:54:42,129:INFO: Dataset: zara2               Batch: 14/18	Loss 94.3204 (98.5975)
2022-10-30 00:54:43,198:INFO: Dataset: zara2               Batch: 15/18	Loss 90.7441 (98.1323)
2022-10-30 00:54:44,262:INFO: Dataset: zara2               Batch: 16/18	Loss 96.5291 (98.0393)
2022-10-30 00:54:45,334:INFO: Dataset: zara2               Batch: 17/18	Loss 89.5975 (97.6000)
2022-10-30 00:54:46,317:INFO: Dataset: zara2               Batch: 18/18	Loss 101.1002 (97.7533)
2022-10-30 00:54:46,384:INFO: - Computing ADE (validation o)
2022-10-30 00:54:46,723:INFO: 		 ADE on eth                       dataset:	 4.429347991943359
2022-10-30 00:54:46,724:INFO: Average validation o:	ADE  4.4293	FDE  7.8548
2022-10-30 00:54:46,724:INFO: - Computing ADE (validation)
2022-10-30 00:54:47,080:INFO: 		 ADE on hotel                     dataset:	 2.0905802249908447
2022-10-30 00:54:47,532:INFO: 		 ADE on univ                      dataset:	 2.1859474182128906
2022-10-30 00:54:47,901:INFO: 		 ADE on zara1                     dataset:	 2.1923458576202393
2022-10-30 00:54:48,463:INFO: 		 ADE on zara2                     dataset:	 2.3637752532958984
2022-10-30 00:54:48,464:INFO: Average validation:	ADE  2.2463	FDE  4.1762
2022-10-30 00:54:48,464:INFO: - Computing ADE (training)
2022-10-30 00:54:48,943:INFO: 		 ADE on hotel                     dataset:	 2.218104362487793
2022-10-30 00:54:49,996:INFO: 		 ADE on univ                      dataset:	 2.1033270359039307
2022-10-30 00:54:50,690:INFO: 		 ADE on zara1                     dataset:	 3.1087095737457275
2022-10-30 00:54:51,868:INFO: 		 ADE on zara2                     dataset:	 2.7913644313812256
2022-10-30 00:54:51,868:INFO: Average training:	ADE  2.3099	FDE  4.2987
2022-10-30 00:54:51,880:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_465.pth.tar
2022-10-30 00:54:51,880:INFO: 
===> EPOCH: 466 (P4)
2022-10-30 00:54:51,881:INFO: - Computing loss (training)
2022-10-30 00:54:53,150:INFO: Dataset: hotel               Batch: 1/4	Loss 84.9165 (84.9165)
2022-10-30 00:54:54,204:INFO: Dataset: hotel               Batch: 2/4	Loss 85.1099 (85.0139)
2022-10-30 00:54:55,253:INFO: Dataset: hotel               Batch: 3/4	Loss 83.3236 (84.4695)
2022-10-30 00:54:56,026:INFO: Dataset: hotel               Batch: 4/4	Loss 52.0410 (78.6084)
2022-10-30 00:54:57,469:INFO: Dataset: univ                Batch:  1/15	Loss 92.7595 (92.7595)
2022-10-30 00:54:58,604:INFO: Dataset: univ                Batch:  2/15	Loss 102.8576 (97.8219)
2022-10-30 00:54:59,731:INFO: Dataset: univ                Batch:  3/15	Loss 91.5819 (95.8278)
2022-10-30 00:55:00,874:INFO: Dataset: univ                Batch:  4/15	Loss 90.8747 (94.5234)
2022-10-30 00:55:02,014:INFO: Dataset: univ                Batch:  5/15	Loss 96.9063 (94.9774)
2022-10-30 00:55:03,155:INFO: Dataset: univ                Batch:  6/15	Loss 88.8222 (93.8719)
2022-10-30 00:55:04,292:INFO: Dataset: univ                Batch:  7/15	Loss 100.5259 (94.7649)
2022-10-30 00:55:05,419:INFO: Dataset: univ                Batch:  8/15	Loss 89.2827 (94.1390)
2022-10-30 00:55:06,552:INFO: Dataset: univ                Batch:  9/15	Loss 88.0322 (93.4449)
2022-10-30 00:55:07,681:INFO: Dataset: univ                Batch: 10/15	Loss 90.9239 (93.2185)
2022-10-30 00:55:08,804:INFO: Dataset: univ                Batch: 11/15	Loss 88.9122 (92.8141)
2022-10-30 00:55:09,916:INFO: Dataset: univ                Batch: 12/15	Loss 95.1682 (92.9938)
2022-10-30 00:55:11,034:INFO: Dataset: univ                Batch: 13/15	Loss 93.9008 (93.0655)
2022-10-30 00:55:12,164:INFO: Dataset: univ                Batch: 14/15	Loss 91.5858 (92.9440)
2022-10-30 00:55:12,646:INFO: Dataset: univ                Batch: 15/15	Loss 16.0938 (91.8296)
2022-10-30 00:55:13,985:INFO: Dataset: zara1               Batch: 1/8	Loss 98.1854 (98.1854)
2022-10-30 00:55:15,028:INFO: Dataset: zara1               Batch: 2/8	Loss 92.4172 (95.2586)
2022-10-30 00:55:16,078:INFO: Dataset: zara1               Batch: 3/8	Loss 95.9626 (95.5126)
2022-10-30 00:55:17,126:INFO: Dataset: zara1               Batch: 4/8	Loss 107.3920 (98.2919)
2022-10-30 00:55:18,178:INFO: Dataset: zara1               Batch: 5/8	Loss 93.7684 (97.4528)
2022-10-30 00:55:19,227:INFO: Dataset: zara1               Batch: 6/8	Loss 92.6806 (96.5759)
2022-10-30 00:55:20,275:INFO: Dataset: zara1               Batch: 7/8	Loss 99.8071 (97.0240)
2022-10-30 00:55:21,225:INFO: Dataset: zara1               Batch: 8/8	Loss 81.2516 (95.2559)
2022-10-30 00:55:22,609:INFO: Dataset: zara2               Batch:  1/18	Loss 101.2287 (101.2287)
2022-10-30 00:55:23,678:INFO: Dataset: zara2               Batch:  2/18	Loss 89.3329 (95.2633)
2022-10-30 00:55:24,756:INFO: Dataset: zara2               Batch:  3/18	Loss 99.1521 (96.5236)
2022-10-30 00:55:25,836:INFO: Dataset: zara2               Batch:  4/18	Loss 88.6849 (94.5393)
2022-10-30 00:55:26,897:INFO: Dataset: zara2               Batch:  5/18	Loss 90.6375 (93.7256)
2022-10-30 00:55:27,956:INFO: Dataset: zara2               Batch:  6/18	Loss 89.6059 (93.0565)
2022-10-30 00:55:29,017:INFO: Dataset: zara2               Batch:  7/18	Loss 91.9266 (92.9023)
2022-10-30 00:55:30,071:INFO: Dataset: zara2               Batch:  8/18	Loss 87.9761 (92.2598)
2022-10-30 00:55:31,127:INFO: Dataset: zara2               Batch:  9/18	Loss 97.1768 (92.7769)
2022-10-30 00:55:32,189:INFO: Dataset: zara2               Batch: 10/18	Loss 93.4787 (92.8431)
2022-10-30 00:55:33,250:INFO: Dataset: zara2               Batch: 11/18	Loss 103.3597 (93.8639)
2022-10-30 00:55:34,337:INFO: Dataset: zara2               Batch: 12/18	Loss 92.2090 (93.7190)
2022-10-30 00:55:35,387:INFO: Dataset: zara2               Batch: 13/18	Loss 90.2690 (93.4333)
2022-10-30 00:55:36,444:INFO: Dataset: zara2               Batch: 14/18	Loss 158.3770 (97.8321)
2022-10-30 00:55:37,503:INFO: Dataset: zara2               Batch: 15/18	Loss 111.6014 (98.7587)
2022-10-30 00:55:38,555:INFO: Dataset: zara2               Batch: 16/18	Loss 90.0452 (98.2319)
2022-10-30 00:55:39,610:INFO: Dataset: zara2               Batch: 17/18	Loss 1086.9119 (154.7622)
2022-10-30 00:55:40,562:INFO: Dataset: zara2               Batch: 18/18	Loss 79.7254 (151.1028)
2022-10-30 00:55:40,628:INFO: - Computing ADE (validation o)
2022-10-30 00:55:40,986:INFO: 		 ADE on eth                       dataset:	 4.459905624389648
2022-10-30 00:55:40,987:INFO: Average validation o:	ADE  4.4599	FDE  7.8917
2022-10-30 00:55:40,987:INFO: - Computing ADE (validation)
2022-10-30 00:55:41,337:INFO: 		 ADE on hotel                     dataset:	 2.071331739425659
2022-10-30 00:55:41,779:INFO: 		 ADE on univ                      dataset:	 2.1724743843078613
2022-10-30 00:55:42,133:INFO: 		 ADE on zara1                     dataset:	 2.1737966537475586
2022-10-30 00:55:42,693:INFO: 		 ADE on zara2                     dataset:	 2.3657679557800293
2022-10-30 00:55:42,693:INFO: Average validation:	ADE  2.2379	FDE  4.1558
2022-10-30 00:55:42,693:INFO: - Computing ADE (training)
2022-10-30 00:55:43,185:INFO: 		 ADE on hotel                     dataset:	 2.201265335083008
2022-10-30 00:55:44,259:INFO: 		 ADE on univ                      dataset:	 2.0992326736450195
2022-10-30 00:55:44,949:INFO: 		 ADE on zara1                     dataset:	 3.109123945236206
2022-10-30 00:55:46,180:INFO: 		 ADE on zara2                     dataset:	 2.7653708457946777
2022-10-30 00:55:46,180:INFO: Average training:	ADE  2.3014	FDE  4.2802
2022-10-30 00:55:46,191:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_466.pth.tar
2022-10-30 00:55:46,191:INFO: 
===> EPOCH: 467 (P4)
2022-10-30 00:55:46,191:INFO: - Computing loss (training)
2022-10-30 00:55:47,491:INFO: Dataset: hotel               Batch: 1/4	Loss 86.8134 (86.8134)
2022-10-30 00:55:48,555:INFO: Dataset: hotel               Batch: 2/4	Loss 83.1400 (84.9591)
2022-10-30 00:55:49,620:INFO: Dataset: hotel               Batch: 3/4	Loss 83.6687 (84.5195)
2022-10-30 00:55:50,396:INFO: Dataset: hotel               Batch: 4/4	Loss 51.4301 (79.1065)
2022-10-30 00:55:51,834:INFO: Dataset: univ                Batch:  1/15	Loss 92.8204 (92.8204)
2022-10-30 00:55:52,940:INFO: Dataset: univ                Batch:  2/15	Loss 88.6250 (90.8220)
2022-10-30 00:55:54,050:INFO: Dataset: univ                Batch:  3/15	Loss 92.8378 (91.4633)
2022-10-30 00:55:55,161:INFO: Dataset: univ                Batch:  4/15	Loss 89.0364 (90.8738)
2022-10-30 00:55:56,278:INFO: Dataset: univ                Batch:  5/15	Loss 93.8586 (91.4733)
2022-10-30 00:55:57,383:INFO: Dataset: univ                Batch:  6/15	Loss 87.5471 (90.8718)
2022-10-30 00:55:58,488:INFO: Dataset: univ                Batch:  7/15	Loss 92.1603 (91.0546)
2022-10-30 00:55:59,609:INFO: Dataset: univ                Batch:  8/15	Loss 90.5623 (90.9872)
2022-10-30 00:56:00,718:INFO: Dataset: univ                Batch:  9/15	Loss 91.4856 (91.0416)
2022-10-30 00:56:01,820:INFO: Dataset: univ                Batch: 10/15	Loss 94.0074 (91.3154)
2022-10-30 00:56:02,937:INFO: Dataset: univ                Batch: 11/15	Loss 89.1584 (91.1160)
2022-10-30 00:56:04,071:INFO: Dataset: univ                Batch: 12/15	Loss 108.3006 (92.6662)
2022-10-30 00:56:05,195:INFO: Dataset: univ                Batch: 13/15	Loss 91.0438 (92.5329)
2022-10-30 00:56:06,306:INFO: Dataset: univ                Batch: 14/15	Loss 90.1602 (92.3608)
2022-10-30 00:56:06,790:INFO: Dataset: univ                Batch: 15/15	Loss 16.1081 (91.3996)
2022-10-30 00:56:08,183:INFO: Dataset: zara1               Batch: 1/8	Loss 97.5091 (97.5091)
2022-10-30 00:56:09,264:INFO: Dataset: zara1               Batch: 2/8	Loss 92.1847 (94.8290)
2022-10-30 00:56:10,333:INFO: Dataset: zara1               Batch: 3/8	Loss 90.3898 (93.2994)
2022-10-30 00:56:11,400:INFO: Dataset: zara1               Batch: 4/8	Loss 92.4875 (93.0803)
2022-10-30 00:56:12,477:INFO: Dataset: zara1               Batch: 5/8	Loss 98.7176 (94.2856)
2022-10-30 00:56:13,550:INFO: Dataset: zara1               Batch: 6/8	Loss 94.8895 (94.3895)
2022-10-30 00:56:14,622:INFO: Dataset: zara1               Batch: 7/8	Loss 196.7675 (109.2676)
2022-10-30 00:56:15,591:INFO: Dataset: zara1               Batch: 8/8	Loss 80.2768 (105.8955)
2022-10-30 00:56:16,992:INFO: Dataset: zara2               Batch:  1/18	Loss 91.9142 (91.9142)
2022-10-30 00:56:18,074:INFO: Dataset: zara2               Batch:  2/18	Loss 88.4528 (90.0982)
2022-10-30 00:56:19,159:INFO: Dataset: zara2               Batch:  3/18	Loss 92.2361 (90.8497)
2022-10-30 00:56:20,252:INFO: Dataset: zara2               Batch:  4/18	Loss 93.9681 (91.5807)
2022-10-30 00:56:21,332:INFO: Dataset: zara2               Batch:  5/18	Loss 89.6912 (91.1978)
2022-10-30 00:56:22,416:INFO: Dataset: zara2               Batch:  6/18	Loss 90.8946 (91.1446)
2022-10-30 00:56:23,493:INFO: Dataset: zara2               Batch:  7/18	Loss 92.9189 (91.4324)
2022-10-30 00:56:24,649:INFO: Dataset: zara2               Batch:  8/18	Loss 88.0026 (90.9571)
2022-10-30 00:56:25,727:INFO: Dataset: zara2               Batch:  9/18	Loss 92.0397 (91.0876)
2022-10-30 00:56:26,811:INFO: Dataset: zara2               Batch: 10/18	Loss 98.0006 (91.8131)
2022-10-30 00:56:27,907:INFO: Dataset: zara2               Batch: 11/18	Loss 131.4197 (95.3828)
2022-10-30 00:56:28,995:INFO: Dataset: zara2               Batch: 12/18	Loss 124.9485 (97.9204)
2022-10-30 00:56:30,076:INFO: Dataset: zara2               Batch: 13/18	Loss 112.5976 (98.9167)
2022-10-30 00:56:31,149:INFO: Dataset: zara2               Batch: 14/18	Loss 90.7138 (98.2867)
2022-10-30 00:56:32,219:INFO: Dataset: zara2               Batch: 15/18	Loss 99.3329 (98.3556)
2022-10-30 00:56:33,286:INFO: Dataset: zara2               Batch: 16/18	Loss 91.8642 (97.9568)
2022-10-30 00:56:34,350:INFO: Dataset: zara2               Batch: 17/18	Loss 105.0011 (98.4081)
2022-10-30 00:56:35,317:INFO: Dataset: zara2               Batch: 18/18	Loss 81.6009 (97.6468)
2022-10-30 00:56:35,383:INFO: - Computing ADE (validation o)
2022-10-30 00:56:35,734:INFO: 		 ADE on eth                       dataset:	 4.465832710266113
2022-10-30 00:56:35,735:INFO: Average validation o:	ADE  4.4658	FDE  7.8834
2022-10-30 00:56:35,736:INFO: - Computing ADE (validation)
2022-10-30 00:56:36,081:INFO: 		 ADE on hotel                     dataset:	 2.0441694259643555
2022-10-30 00:56:36,536:INFO: 		 ADE on univ                      dataset:	 2.1621925830841064
2022-10-30 00:56:36,898:INFO: 		 ADE on zara1                     dataset:	 2.2033450603485107
2022-10-30 00:56:37,437:INFO: 		 ADE on zara2                     dataset:	 2.3546292781829834
2022-10-30 00:56:37,437:INFO: Average validation:	ADE  2.2287	FDE  4.1333
2022-10-30 00:56:37,438:INFO: - Computing ADE (training)
2022-10-30 00:56:37,923:INFO: 		 ADE on hotel                     dataset:	 2.212238073348999
2022-10-30 00:56:38,985:INFO: 		 ADE on univ                      dataset:	 2.086190938949585
2022-10-30 00:56:39,691:INFO: 		 ADE on zara1                     dataset:	 3.1021173000335693
2022-10-30 00:56:40,896:INFO: 		 ADE on zara2                     dataset:	 2.7638111114501953
2022-10-30 00:56:40,897:INFO: Average training:	ADE  2.2917	FDE  4.2571
2022-10-30 00:56:40,908:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_467.pth.tar
2022-10-30 00:56:40,908:INFO: 
===> EPOCH: 468 (P4)
2022-10-30 00:56:40,908:INFO: - Computing loss (training)
2022-10-30 00:56:42,195:INFO: Dataset: hotel               Batch: 1/4	Loss 84.3541 (84.3541)
2022-10-30 00:56:43,249:INFO: Dataset: hotel               Batch: 2/4	Loss 85.9638 (85.1551)
2022-10-30 00:56:44,290:INFO: Dataset: hotel               Batch: 3/4	Loss 84.0704 (84.8004)
2022-10-30 00:56:45,052:INFO: Dataset: hotel               Batch: 4/4	Loss 50.6878 (79.0400)
2022-10-30 00:56:46,487:INFO: Dataset: univ                Batch:  1/15	Loss 107.4097 (107.4097)
2022-10-30 00:56:47,597:INFO: Dataset: univ                Batch:  2/15	Loss 93.2423 (100.5827)
2022-10-30 00:56:48,708:INFO: Dataset: univ                Batch:  3/15	Loss 87.2794 (96.0774)
2022-10-30 00:56:49,816:INFO: Dataset: univ                Batch:  4/15	Loss 96.8403 (96.2693)
2022-10-30 00:56:50,918:INFO: Dataset: univ                Batch:  5/15	Loss 102.1127 (97.3312)
2022-10-30 00:56:52,033:INFO: Dataset: univ                Batch:  6/15	Loss 100.6021 (97.8839)
2022-10-30 00:56:53,144:INFO: Dataset: univ                Batch:  7/15	Loss 95.1250 (97.4763)
2022-10-30 00:56:54,262:INFO: Dataset: univ                Batch:  8/15	Loss 87.6879 (96.2132)
2022-10-30 00:56:55,368:INFO: Dataset: univ                Batch:  9/15	Loss 88.1728 (95.3113)
2022-10-30 00:56:56,486:INFO: Dataset: univ                Batch: 10/15	Loss 90.4630 (94.8103)
2022-10-30 00:56:57,592:INFO: Dataset: univ                Batch: 11/15	Loss 125.7983 (97.6612)
2022-10-30 00:56:58,702:INFO: Dataset: univ                Batch: 12/15	Loss 87.0435 (96.7753)
2022-10-30 00:56:59,817:INFO: Dataset: univ                Batch: 13/15	Loss 87.4181 (96.0899)
2022-10-30 00:57:00,936:INFO: Dataset: univ                Batch: 14/15	Loss 95.3687 (96.0400)
2022-10-30 00:57:01,412:INFO: Dataset: univ                Batch: 15/15	Loss 18.5921 (95.1775)
2022-10-30 00:57:02,755:INFO: Dataset: zara1               Batch: 1/8	Loss 94.6615 (94.6615)
2022-10-30 00:57:03,797:INFO: Dataset: zara1               Batch: 2/8	Loss 94.8831 (94.7755)
2022-10-30 00:57:04,843:INFO: Dataset: zara1               Batch: 3/8	Loss 92.3427 (94.0240)
2022-10-30 00:57:05,883:INFO: Dataset: zara1               Batch: 4/8	Loss 91.4081 (93.3666)
2022-10-30 00:57:06,940:INFO: Dataset: zara1               Batch: 5/8	Loss 94.9713 (93.7308)
2022-10-30 00:57:07,982:INFO: Dataset: zara1               Batch: 6/8	Loss 109.5723 (96.0207)
2022-10-30 00:57:09,031:INFO: Dataset: zara1               Batch: 7/8	Loss 91.6648 (95.3799)
2022-10-30 00:57:09,987:INFO: Dataset: zara1               Batch: 8/8	Loss 79.0995 (93.4862)
2022-10-30 00:57:11,351:INFO: Dataset: zara2               Batch:  1/18	Loss 108.5094 (108.5094)
2022-10-30 00:57:12,413:INFO: Dataset: zara2               Batch:  2/18	Loss 93.9713 (101.1102)
2022-10-30 00:57:13,463:INFO: Dataset: zara2               Batch:  3/18	Loss 89.1916 (96.9899)
2022-10-30 00:57:14,524:INFO: Dataset: zara2               Batch:  4/18	Loss 95.5793 (96.6469)
2022-10-30 00:57:15,583:INFO: Dataset: zara2               Batch:  5/18	Loss 94.2281 (96.1577)
2022-10-30 00:57:16,649:INFO: Dataset: zara2               Batch:  6/18	Loss 94.8934 (95.9430)
2022-10-30 00:57:17,703:INFO: Dataset: zara2               Batch:  7/18	Loss 105.9965 (97.2897)
2022-10-30 00:57:18,768:INFO: Dataset: zara2               Batch:  8/18	Loss 92.6364 (96.6975)
2022-10-30 00:57:19,829:INFO: Dataset: zara2               Batch:  9/18	Loss 89.8040 (95.9293)
2022-10-30 00:57:20,888:INFO: Dataset: zara2               Batch: 10/18	Loss 92.3805 (95.5556)
2022-10-30 00:57:21,941:INFO: Dataset: zara2               Batch: 11/18	Loss 92.4509 (95.2546)
2022-10-30 00:57:22,999:INFO: Dataset: zara2               Batch: 12/18	Loss 110.8964 (96.6714)
2022-10-30 00:57:24,057:INFO: Dataset: zara2               Batch: 13/18	Loss 98.4266 (96.8083)
2022-10-30 00:57:25,125:INFO: Dataset: zara2               Batch: 14/18	Loss 88.4288 (96.2655)
2022-10-30 00:57:26,195:INFO: Dataset: zara2               Batch: 15/18	Loss 141.2029 (98.9415)
2022-10-30 00:57:27,269:INFO: Dataset: zara2               Batch: 16/18	Loss 98.2173 (98.8947)
2022-10-30 00:57:28,334:INFO: Dataset: zara2               Batch: 17/18	Loss 88.4850 (98.3506)
2022-10-30 00:57:29,296:INFO: Dataset: zara2               Batch: 18/18	Loss 92.9230 (98.0895)
2022-10-30 00:57:29,362:INFO: - Computing ADE (validation o)
2022-10-30 00:57:29,728:INFO: 		 ADE on eth                       dataset:	 4.4777116775512695
2022-10-30 00:57:29,728:INFO: Average validation o:	ADE  4.4777	FDE  7.8995
2022-10-30 00:57:29,729:INFO: - Computing ADE (validation)
2022-10-30 00:57:30,089:INFO: 		 ADE on hotel                     dataset:	 2.0525312423706055
2022-10-30 00:57:30,536:INFO: 		 ADE on univ                      dataset:	 2.1650166511535645
2022-10-30 00:57:30,890:INFO: 		 ADE on zara1                     dataset:	 2.224026918411255
2022-10-30 00:57:31,432:INFO: 		 ADE on zara2                     dataset:	 2.3509902954101562
2022-10-30 00:57:31,433:INFO: Average validation:	ADE  2.2305	FDE  4.1344
2022-10-30 00:57:31,434:INFO: - Computing ADE (training)
2022-10-30 00:57:31,908:INFO: 		 ADE on hotel                     dataset:	 2.194800615310669
2022-10-30 00:57:32,981:INFO: 		 ADE on univ                      dataset:	 2.074185848236084
2022-10-30 00:57:33,676:INFO: 		 ADE on zara1                     dataset:	 3.08323073387146
2022-10-30 00:57:34,884:INFO: 		 ADE on zara2                     dataset:	 2.7668116092681885
2022-10-30 00:57:34,885:INFO: Average training:	ADE  2.2821	FDE  4.2389
2022-10-30 00:57:34,896:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_468.pth.tar
2022-10-30 00:57:34,896:INFO: 
===> EPOCH: 469 (P4)
2022-10-30 00:57:34,896:INFO: - Computing loss (training)
2022-10-30 00:57:36,173:INFO: Dataset: hotel               Batch: 1/4	Loss 85.3579 (85.3579)
2022-10-30 00:57:37,221:INFO: Dataset: hotel               Batch: 2/4	Loss 84.1979 (84.7738)
2022-10-30 00:57:38,272:INFO: Dataset: hotel               Batch: 3/4	Loss 83.7562 (84.4362)
2022-10-30 00:57:39,043:INFO: Dataset: hotel               Batch: 4/4	Loss 51.3091 (78.8422)
2022-10-30 00:57:40,474:INFO: Dataset: univ                Batch:  1/15	Loss 87.5505 (87.5505)
2022-10-30 00:57:41,603:INFO: Dataset: univ                Batch:  2/15	Loss 90.1775 (88.8418)
2022-10-30 00:57:42,731:INFO: Dataset: univ                Batch:  3/15	Loss 87.8620 (88.5395)
2022-10-30 00:57:43,853:INFO: Dataset: univ                Batch:  4/15	Loss 92.4863 (89.5342)
2022-10-30 00:57:44,967:INFO: Dataset: univ                Batch:  5/15	Loss 90.3793 (89.6783)
2022-10-30 00:57:46,086:INFO: Dataset: univ                Batch:  6/15	Loss 89.0860 (89.5827)
2022-10-30 00:57:47,193:INFO: Dataset: univ                Batch:  7/15	Loss 89.3840 (89.5563)
2022-10-30 00:57:48,301:INFO: Dataset: univ                Batch:  8/15	Loss 91.5112 (89.7884)
2022-10-30 00:57:49,412:INFO: Dataset: univ                Batch:  9/15	Loss 87.7389 (89.5566)
2022-10-30 00:57:50,523:INFO: Dataset: univ                Batch: 10/15	Loss 92.2569 (89.8268)
2022-10-30 00:57:51,626:INFO: Dataset: univ                Batch: 11/15	Loss 87.6055 (89.6322)
2022-10-30 00:57:52,731:INFO: Dataset: univ                Batch: 12/15	Loss 91.0872 (89.7492)
2022-10-30 00:57:53,843:INFO: Dataset: univ                Batch: 13/15	Loss 95.5895 (90.2070)
2022-10-30 00:57:54,954:INFO: Dataset: univ                Batch: 14/15	Loss 94.8898 (90.5041)
2022-10-30 00:57:55,431:INFO: Dataset: univ                Batch: 15/15	Loss 20.9448 (89.6371)
2022-10-30 00:57:56,808:INFO: Dataset: zara1               Batch: 1/8	Loss 160.9156 (160.9156)
2022-10-30 00:57:57,879:INFO: Dataset: zara1               Batch: 2/8	Loss 94.8906 (131.1652)
2022-10-30 00:57:58,951:INFO: Dataset: zara1               Batch: 3/8	Loss 90.8933 (117.8478)
2022-10-30 00:58:00,022:INFO: Dataset: zara1               Batch: 4/8	Loss 94.9300 (112.8657)
2022-10-30 00:58:01,087:INFO: Dataset: zara1               Batch: 5/8	Loss 96.5872 (109.6587)
2022-10-30 00:58:02,155:INFO: Dataset: zara1               Batch: 6/8	Loss 93.8654 (106.8876)
2022-10-30 00:58:03,228:INFO: Dataset: zara1               Batch: 7/8	Loss 93.6666 (105.0334)
2022-10-30 00:58:04,205:INFO: Dataset: zara1               Batch: 8/8	Loss 86.0785 (103.0082)
2022-10-30 00:58:05,610:INFO: Dataset: zara2               Batch:  1/18	Loss 88.6276 (88.6276)
2022-10-30 00:58:06,688:INFO: Dataset: zara2               Batch:  2/18	Loss 91.0527 (89.8861)
2022-10-30 00:58:07,749:INFO: Dataset: zara2               Batch:  3/18	Loss 89.3199 (89.6919)
2022-10-30 00:58:08,814:INFO: Dataset: zara2               Batch:  4/18	Loss 90.6551 (89.9129)
2022-10-30 00:58:09,868:INFO: Dataset: zara2               Batch:  5/18	Loss 97.5873 (91.4652)
2022-10-30 00:58:10,922:INFO: Dataset: zara2               Batch:  6/18	Loss 117.0464 (95.8687)
2022-10-30 00:58:11,978:INFO: Dataset: zara2               Batch:  7/18	Loss 116.5496 (98.7563)
2022-10-30 00:58:13,036:INFO: Dataset: zara2               Batch:  8/18	Loss 90.8750 (97.8725)
2022-10-30 00:58:14,087:INFO: Dataset: zara2               Batch:  9/18	Loss 100.1559 (98.1178)
2022-10-30 00:58:15,139:INFO: Dataset: zara2               Batch: 10/18	Loss 431.6175 (128.7266)
2022-10-30 00:58:16,190:INFO: Dataset: zara2               Batch: 11/18	Loss 90.8277 (125.4301)
2022-10-30 00:58:17,242:INFO: Dataset: zara2               Batch: 12/18	Loss 87.8208 (122.2109)
2022-10-30 00:58:18,294:INFO: Dataset: zara2               Batch: 13/18	Loss 104.2046 (121.0241)
2022-10-30 00:58:19,356:INFO: Dataset: zara2               Batch: 14/18	Loss 86.6556 (118.6541)
2022-10-30 00:58:20,413:INFO: Dataset: zara2               Batch: 15/18	Loss 126.6770 (119.2114)
2022-10-30 00:58:21,466:INFO: Dataset: zara2               Batch: 16/18	Loss 89.6019 (117.2273)
2022-10-30 00:58:22,608:INFO: Dataset: zara2               Batch: 17/18	Loss 91.3401 (115.8497)
2022-10-30 00:58:23,565:INFO: Dataset: zara2               Batch: 18/18	Loss 81.0646 (114.2396)
2022-10-30 00:58:23,631:INFO: - Computing ADE (validation o)
2022-10-30 00:58:23,990:INFO: 		 ADE on eth                       dataset:	 4.412211894989014
2022-10-30 00:58:23,991:INFO: Average validation o:	ADE  4.4122	FDE  7.7944
2022-10-30 00:58:23,991:INFO: - Computing ADE (validation)
2022-10-30 00:58:24,333:INFO: 		 ADE on hotel                     dataset:	 2.0186145305633545
2022-10-30 00:58:24,775:INFO: 		 ADE on univ                      dataset:	 2.1559031009674072
2022-10-30 00:58:25,130:INFO: 		 ADE on zara1                     dataset:	 2.1888694763183594
2022-10-30 00:58:25,666:INFO: 		 ADE on zara2                     dataset:	 2.3346173763275146
2022-10-30 00:58:25,666:INFO: Average validation:	ADE  2.2159	FDE  4.1073
2022-10-30 00:58:25,667:INFO: - Computing ADE (training)
2022-10-30 00:58:26,131:INFO: 		 ADE on hotel                     dataset:	 2.186990976333618
2022-10-30 00:58:27,200:INFO: 		 ADE on univ                      dataset:	 2.0684854984283447
2022-10-30 00:58:27,897:INFO: 		 ADE on zara1                     dataset:	 3.090156078338623
2022-10-30 00:58:29,121:INFO: 		 ADE on zara2                     dataset:	 2.7573211193084717
2022-10-30 00:58:29,121:INFO: Average training:	ADE  2.2764	FDE  4.2267
2022-10-30 00:58:29,132:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_469.pth.tar
2022-10-30 00:58:29,132:INFO: 
===> EPOCH: 470 (P4)
2022-10-30 00:58:29,132:INFO: - Computing loss (training)
2022-10-30 00:58:30,431:INFO: Dataset: hotel               Batch: 1/4	Loss 83.5777 (83.5777)
2022-10-30 00:58:31,503:INFO: Dataset: hotel               Batch: 2/4	Loss 84.8492 (84.2012)
2022-10-30 00:58:32,570:INFO: Dataset: hotel               Batch: 3/4	Loss 84.1940 (84.1988)
2022-10-30 00:58:33,350:INFO: Dataset: hotel               Batch: 4/4	Loss 51.7626 (78.9782)
2022-10-30 00:58:34,795:INFO: Dataset: univ                Batch:  1/15	Loss 92.4162 (92.4162)
2022-10-30 00:58:35,907:INFO: Dataset: univ                Batch:  2/15	Loss 86.7376 (89.6408)
2022-10-30 00:58:37,015:INFO: Dataset: univ                Batch:  3/15	Loss 89.9215 (89.7361)
2022-10-30 00:58:38,136:INFO: Dataset: univ                Batch:  4/15	Loss 85.6930 (88.6049)
2022-10-30 00:58:39,255:INFO: Dataset: univ                Batch:  5/15	Loss 92.7186 (89.3983)
2022-10-30 00:58:40,368:INFO: Dataset: univ                Batch:  6/15	Loss 90.6912 (89.6004)
2022-10-30 00:58:41,485:INFO: Dataset: univ                Batch:  7/15	Loss 195.0411 (104.0188)
2022-10-30 00:58:42,606:INFO: Dataset: univ                Batch:  8/15	Loss 114.0632 (105.2367)
2022-10-30 00:58:43,736:INFO: Dataset: univ                Batch:  9/15	Loss 94.2451 (104.0349)
2022-10-30 00:58:44,864:INFO: Dataset: univ                Batch: 10/15	Loss 98.5692 (103.4870)
2022-10-30 00:58:45,998:INFO: Dataset: univ                Batch: 11/15	Loss 361.5800 (128.0016)
2022-10-30 00:58:47,128:INFO: Dataset: univ                Batch: 12/15	Loss 95.6363 (125.5005)
2022-10-30 00:58:48,261:INFO: Dataset: univ                Batch: 13/15	Loss 154.7701 (127.7854)
2022-10-30 00:58:49,385:INFO: Dataset: univ                Batch: 14/15	Loss 91.8234 (125.1303)
2022-10-30 00:58:49,872:INFO: Dataset: univ                Batch: 15/15	Loss 17.0308 (124.0904)
2022-10-30 00:58:51,251:INFO: Dataset: zara1               Batch: 1/8	Loss 90.0940 (90.0940)
2022-10-30 00:58:52,319:INFO: Dataset: zara1               Batch: 2/8	Loss 144.3744 (115.9731)
2022-10-30 00:58:53,383:INFO: Dataset: zara1               Batch: 3/8	Loss 91.0174 (107.8015)
2022-10-30 00:58:54,446:INFO: Dataset: zara1               Batch: 4/8	Loss 94.6353 (104.6563)
2022-10-30 00:58:55,512:INFO: Dataset: zara1               Batch: 5/8	Loss 92.3908 (102.2995)
2022-10-30 00:58:56,589:INFO: Dataset: zara1               Batch: 6/8	Loss 91.6797 (100.4465)
2022-10-30 00:58:57,653:INFO: Dataset: zara1               Batch: 7/8	Loss 96.7738 (99.9046)
2022-10-30 00:58:58,626:INFO: Dataset: zara1               Batch: 8/8	Loss 82.7755 (98.1105)
2022-10-30 00:58:59,971:INFO: Dataset: zara2               Batch:  1/18	Loss 91.6493 (91.6493)
2022-10-30 00:59:01,018:INFO: Dataset: zara2               Batch:  2/18	Loss 110.5321 (101.2947)
2022-10-30 00:59:02,060:INFO: Dataset: zara2               Batch:  3/18	Loss 101.2805 (101.2895)
2022-10-30 00:59:03,125:INFO: Dataset: zara2               Batch:  4/18	Loss 91.2754 (98.8248)
2022-10-30 00:59:04,188:INFO: Dataset: zara2               Batch:  5/18	Loss 92.4456 (97.5467)
2022-10-30 00:59:05,248:INFO: Dataset: zara2               Batch:  6/18	Loss 93.0282 (96.7778)
2022-10-30 00:59:06,306:INFO: Dataset: zara2               Batch:  7/18	Loss 114.9799 (99.1732)
2022-10-30 00:59:07,353:INFO: Dataset: zara2               Batch:  8/18	Loss 94.7388 (98.6575)
2022-10-30 00:59:08,403:INFO: Dataset: zara2               Batch:  9/18	Loss 103.9773 (99.2767)
2022-10-30 00:59:09,476:INFO: Dataset: zara2               Batch: 10/18	Loss 117.8267 (101.2748)
2022-10-30 00:59:10,539:INFO: Dataset: zara2               Batch: 11/18	Loss 91.1423 (100.3215)
2022-10-30 00:59:11,583:INFO: Dataset: zara2               Batch: 12/18	Loss 89.5789 (99.4388)
2022-10-30 00:59:12,649:INFO: Dataset: zara2               Batch: 13/18	Loss 91.4441 (98.8033)
2022-10-30 00:59:13,692:INFO: Dataset: zara2               Batch: 14/18	Loss 86.8779 (98.0142)
2022-10-30 00:59:14,742:INFO: Dataset: zara2               Batch: 15/18	Loss 94.8278 (97.7760)
2022-10-30 00:59:15,800:INFO: Dataset: zara2               Batch: 16/18	Loss 101.7484 (98.0272)
2022-10-30 00:59:16,847:INFO: Dataset: zara2               Batch: 17/18	Loss 90.9178 (97.6252)
2022-10-30 00:59:17,800:INFO: Dataset: zara2               Batch: 18/18	Loss 77.1272 (96.6662)
2022-10-30 00:59:17,866:INFO: - Computing ADE (validation o)
2022-10-30 00:59:18,225:INFO: 		 ADE on eth                       dataset:	 4.389294624328613
2022-10-30 00:59:18,225:INFO: Average validation o:	ADE  4.3893	FDE  7.7478
2022-10-30 00:59:18,226:INFO: - Computing ADE (validation)
2022-10-30 00:59:18,567:INFO: 		 ADE on hotel                     dataset:	 1.9997559785842896
2022-10-30 00:59:19,026:INFO: 		 ADE on univ                      dataset:	 2.140580892562866
2022-10-30 00:59:19,389:INFO: 		 ADE on zara1                     dataset:	 2.218005418777466
2022-10-30 00:59:19,952:INFO: 		 ADE on zara2                     dataset:	 2.3356239795684814
2022-10-30 00:59:19,952:INFO: Average validation:	ADE  2.2089	FDE  4.0861
2022-10-30 00:59:19,953:INFO: - Computing ADE (training)
2022-10-30 00:59:20,424:INFO: 		 ADE on hotel                     dataset:	 2.160301685333252
2022-10-30 00:59:21,553:INFO: 		 ADE on univ                      dataset:	 2.0565555095672607
2022-10-30 00:59:22,256:INFO: 		 ADE on zara1                     dataset:	 3.0663018226623535
2022-10-30 00:59:23,490:INFO: 		 ADE on zara2                     dataset:	 2.7364144325256348
2022-10-30 00:59:23,490:INFO: Average training:	ADE  2.2615	FDE  4.1947
2022-10-30 00:59:23,503:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_470.pth.tar
2022-10-30 00:59:23,503:INFO: 
===> EPOCH: 471 (P4)
2022-10-30 00:59:23,503:INFO: - Computing loss (training)
2022-10-30 00:59:24,812:INFO: Dataset: hotel               Batch: 1/4	Loss 85.6889 (85.6889)
2022-10-30 00:59:25,870:INFO: Dataset: hotel               Batch: 2/4	Loss 83.4995 (84.5766)
2022-10-30 00:59:26,933:INFO: Dataset: hotel               Batch: 3/4	Loss 83.6201 (84.2826)
2022-10-30 00:59:27,712:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9090 (78.5589)
2022-10-30 00:59:29,190:INFO: Dataset: univ                Batch:  1/15	Loss 90.2607 (90.2607)
2022-10-30 00:59:30,333:INFO: Dataset: univ                Batch:  2/15	Loss 88.9915 (89.6134)
2022-10-30 00:59:31,456:INFO: Dataset: univ                Batch:  3/15	Loss 90.3021 (89.8389)
2022-10-30 00:59:32,594:INFO: Dataset: univ                Batch:  4/15	Loss 154.4534 (107.4961)
2022-10-30 00:59:33,736:INFO: Dataset: univ                Batch:  5/15	Loss 86.5731 (103.4920)
2022-10-30 00:59:34,879:INFO: Dataset: univ                Batch:  6/15	Loss 87.2403 (100.6005)
2022-10-30 00:59:36,012:INFO: Dataset: univ                Batch:  7/15	Loss 98.8598 (100.3542)
2022-10-30 00:59:37,149:INFO: Dataset: univ                Batch:  8/15	Loss 86.4876 (98.6304)
2022-10-30 00:59:38,297:INFO: Dataset: univ                Batch:  9/15	Loss 90.2050 (97.7387)
2022-10-30 00:59:39,428:INFO: Dataset: univ                Batch: 10/15	Loss 87.1853 (96.7210)
2022-10-30 00:59:40,570:INFO: Dataset: univ                Batch: 11/15	Loss 87.1517 (95.7826)
2022-10-30 00:59:41,704:INFO: Dataset: univ                Batch: 12/15	Loss 90.2930 (95.3408)
2022-10-30 00:59:42,836:INFO: Dataset: univ                Batch: 13/15	Loss 86.5507 (94.7193)
2022-10-30 00:59:43,960:INFO: Dataset: univ                Batch: 14/15	Loss 88.7502 (94.3194)
2022-10-30 00:59:44,437:INFO: Dataset: univ                Batch: 15/15	Loss 16.9348 (93.1973)
2022-10-30 00:59:45,794:INFO: Dataset: zara1               Batch: 1/8	Loss 91.1828 (91.1828)
2022-10-30 00:59:46,850:INFO: Dataset: zara1               Batch: 2/8	Loss 91.4654 (91.3263)
2022-10-30 00:59:47,903:INFO: Dataset: zara1               Batch: 3/8	Loss 90.2994 (90.9998)
2022-10-30 00:59:48,956:INFO: Dataset: zara1               Batch: 4/8	Loss 92.8372 (91.4167)
2022-10-30 00:59:50,012:INFO: Dataset: zara1               Batch: 5/8	Loss 93.1713 (91.8066)
2022-10-30 00:59:51,062:INFO: Dataset: zara1               Batch: 6/8	Loss 91.0781 (91.6835)
2022-10-30 00:59:52,096:INFO: Dataset: zara1               Batch: 7/8	Loss 91.6585 (91.6799)
2022-10-30 00:59:53,040:INFO: Dataset: zara1               Batch: 8/8	Loss 81.1389 (90.6813)
2022-10-30 00:59:54,422:INFO: Dataset: zara2               Batch:  1/18	Loss 100.8065 (100.8065)
2022-10-30 00:59:55,501:INFO: Dataset: zara2               Batch:  2/18	Loss 91.1025 (96.4397)
2022-10-30 00:59:56,576:INFO: Dataset: zara2               Batch:  3/18	Loss 102.4938 (98.4124)
2022-10-30 00:59:57,656:INFO: Dataset: zara2               Batch:  4/18	Loss 89.5009 (96.3926)
2022-10-30 00:59:58,735:INFO: Dataset: zara2               Batch:  5/18	Loss 94.0849 (95.9303)
2022-10-30 00:59:59,821:INFO: Dataset: zara2               Batch:  6/18	Loss 99.0155 (96.4427)
2022-10-30 01:00:00,893:INFO: Dataset: zara2               Batch:  7/18	Loss 90.3165 (95.5832)
2022-10-30 01:00:01,973:INFO: Dataset: zara2               Batch:  8/18	Loss 96.8916 (95.7425)
2022-10-30 01:00:03,056:INFO: Dataset: zara2               Batch:  9/18	Loss 89.7117 (95.0844)
2022-10-30 01:00:04,136:INFO: Dataset: zara2               Batch: 10/18	Loss 95.0867 (95.0846)
2022-10-30 01:00:05,214:INFO: Dataset: zara2               Batch: 11/18	Loss 92.9134 (94.8874)
2022-10-30 01:00:06,295:INFO: Dataset: zara2               Batch: 12/18	Loss 92.4334 (94.6830)
2022-10-30 01:00:07,372:INFO: Dataset: zara2               Batch: 13/18	Loss 86.8523 (94.1054)
2022-10-30 01:00:08,452:INFO: Dataset: zara2               Batch: 14/18	Loss 86.0867 (93.5204)
2022-10-30 01:00:09,528:INFO: Dataset: zara2               Batch: 15/18	Loss 89.7309 (93.2875)
2022-10-30 01:00:10,607:INFO: Dataset: zara2               Batch: 16/18	Loss 90.3493 (93.1007)
2022-10-30 01:00:11,772:INFO: Dataset: zara2               Batch: 17/18	Loss 88.3631 (92.8391)
2022-10-30 01:00:12,737:INFO: Dataset: zara2               Batch: 18/18	Loss 79.4229 (92.2735)
2022-10-30 01:00:12,803:INFO: - Computing ADE (validation o)
2022-10-30 01:00:13,168:INFO: 		 ADE on eth                       dataset:	 4.38663911819458
2022-10-30 01:00:13,168:INFO: Average validation o:	ADE  4.3866	FDE  7.7296
2022-10-30 01:00:13,169:INFO: - Computing ADE (validation)
2022-10-30 01:00:13,513:INFO: 		 ADE on hotel                     dataset:	 1.9981441497802734
2022-10-30 01:00:13,944:INFO: 		 ADE on univ                      dataset:	 2.1286604404449463
2022-10-30 01:00:14,302:INFO: 		 ADE on zara1                     dataset:	 2.1753854751586914
2022-10-30 01:00:14,847:INFO: 		 ADE on zara2                     dataset:	 2.3271636962890625
2022-10-30 01:00:14,848:INFO: Average validation:	ADE  2.1970	FDE  4.0673
2022-10-30 01:00:14,848:INFO: - Computing ADE (training)
2022-10-30 01:00:15,335:INFO: 		 ADE on hotel                     dataset:	 2.156045913696289
2022-10-30 01:00:16,401:INFO: 		 ADE on univ                      dataset:	 2.053941011428833
2022-10-30 01:00:17,089:INFO: 		 ADE on zara1                     dataset:	 3.086503744125366
2022-10-30 01:00:18,297:INFO: 		 ADE on zara2                     dataset:	 2.7434263229370117
2022-10-30 01:00:18,297:INFO: Average training:	ADE  2.2623	FDE  4.1902
2022-10-30 01:00:18,308:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_471.pth.tar
2022-10-30 01:00:18,308:INFO: 
===> EPOCH: 472 (P4)
2022-10-30 01:00:18,308:INFO: - Computing loss (training)
2022-10-30 01:00:19,594:INFO: Dataset: hotel               Batch: 1/4	Loss 84.4114 (84.4114)
2022-10-30 01:00:20,643:INFO: Dataset: hotel               Batch: 2/4	Loss 84.7720 (84.5929)
2022-10-30 01:00:21,691:INFO: Dataset: hotel               Batch: 3/4	Loss 84.8192 (84.6642)
2022-10-30 01:00:22,456:INFO: Dataset: hotel               Batch: 4/4	Loss 49.7063 (79.1300)
2022-10-30 01:00:23,912:INFO: Dataset: univ                Batch:  1/15	Loss 87.5233 (87.5233)
2022-10-30 01:00:25,032:INFO: Dataset: univ                Batch:  2/15	Loss 96.0601 (91.6498)
2022-10-30 01:00:26,137:INFO: Dataset: univ                Batch:  3/15	Loss 93.3096 (92.1935)
2022-10-30 01:00:27,248:INFO: Dataset: univ                Batch:  4/15	Loss 97.0996 (93.3805)
2022-10-30 01:00:28,371:INFO: Dataset: univ                Batch:  5/15	Loss 87.4338 (92.1416)
2022-10-30 01:00:29,482:INFO: Dataset: univ                Batch:  6/15	Loss 94.8790 (92.5933)
2022-10-30 01:00:30,607:INFO: Dataset: univ                Batch:  7/15	Loss 105.4336 (94.6508)
2022-10-30 01:00:31,723:INFO: Dataset: univ                Batch:  8/15	Loss 89.0660 (93.9160)
2022-10-30 01:00:32,836:INFO: Dataset: univ                Batch:  9/15	Loss 106.9397 (95.3865)
2022-10-30 01:00:33,941:INFO: Dataset: univ                Batch: 10/15	Loss 92.5166 (95.1437)
2022-10-30 01:00:35,055:INFO: Dataset: univ                Batch: 11/15	Loss 86.8469 (94.4389)
2022-10-30 01:00:36,170:INFO: Dataset: univ                Batch: 12/15	Loss 85.3012 (93.6233)
2022-10-30 01:00:37,281:INFO: Dataset: univ                Batch: 13/15	Loss 91.2352 (93.4419)
2022-10-30 01:00:38,393:INFO: Dataset: univ                Batch: 14/15	Loss 90.7707 (93.2540)
2022-10-30 01:00:38,867:INFO: Dataset: univ                Batch: 15/15	Loss 15.9498 (92.3418)
2022-10-30 01:00:40,251:INFO: Dataset: zara1               Batch: 1/8	Loss 91.0598 (91.0598)
2022-10-30 01:00:41,323:INFO: Dataset: zara1               Batch: 2/8	Loss 93.6958 (92.3193)
2022-10-30 01:00:42,392:INFO: Dataset: zara1               Batch: 3/8	Loss 91.5803 (92.0811)
2022-10-30 01:00:43,467:INFO: Dataset: zara1               Batch: 4/8	Loss 98.3488 (93.5832)
2022-10-30 01:00:44,540:INFO: Dataset: zara1               Batch: 5/8	Loss 91.6959 (93.1679)
2022-10-30 01:00:45,613:INFO: Dataset: zara1               Batch: 6/8	Loss 90.4642 (92.6679)
2022-10-30 01:00:46,681:INFO: Dataset: zara1               Batch: 7/8	Loss 93.6082 (92.7918)
2022-10-30 01:00:47,647:INFO: Dataset: zara1               Batch: 8/8	Loss 79.5330 (91.1379)
2022-10-30 01:00:49,040:INFO: Dataset: zara2               Batch:  1/18	Loss 104.4454 (104.4454)
2022-10-30 01:00:50,123:INFO: Dataset: zara2               Batch:  2/18	Loss 97.7188 (100.8617)
2022-10-30 01:00:51,217:INFO: Dataset: zara2               Batch:  3/18	Loss 92.5109 (98.0993)
2022-10-30 01:00:52,294:INFO: Dataset: zara2               Batch:  4/18	Loss 99.4666 (98.4104)
2022-10-30 01:00:53,374:INFO: Dataset: zara2               Batch:  5/18	Loss 92.4873 (97.2440)
2022-10-30 01:00:54,460:INFO: Dataset: zara2               Batch:  6/18	Loss 94.5512 (96.7833)
2022-10-30 01:00:55,548:INFO: Dataset: zara2               Batch:  7/18	Loss 89.2486 (95.7315)
2022-10-30 01:00:56,625:INFO: Dataset: zara2               Batch:  8/18	Loss 94.6371 (95.6075)
2022-10-30 01:00:57,696:INFO: Dataset: zara2               Batch:  9/18	Loss 129.3348 (99.2223)
2022-10-30 01:00:58,774:INFO: Dataset: zara2               Batch: 10/18	Loss 98.0858 (99.1007)
2022-10-30 01:00:59,852:INFO: Dataset: zara2               Batch: 11/18	Loss 87.2148 (98.0625)
2022-10-30 01:01:00,928:INFO: Dataset: zara2               Batch: 12/18	Loss 87.6329 (97.0711)
2022-10-30 01:01:02,000:INFO: Dataset: zara2               Batch: 13/18	Loss 89.6839 (96.5020)
2022-10-30 01:01:03,075:INFO: Dataset: zara2               Batch: 14/18	Loss 90.0742 (96.0371)
2022-10-30 01:01:04,161:INFO: Dataset: zara2               Batch: 15/18	Loss 91.6879 (95.7865)
2022-10-30 01:01:05,243:INFO: Dataset: zara2               Batch: 16/18	Loss 89.1612 (95.3512)
2022-10-30 01:01:06,325:INFO: Dataset: zara2               Batch: 17/18	Loss 92.7624 (95.1949)
2022-10-30 01:01:07,286:INFO: Dataset: zara2               Batch: 18/18	Loss 97.9618 (95.3395)
2022-10-30 01:01:07,353:INFO: - Computing ADE (validation o)
2022-10-30 01:01:07,724:INFO: 		 ADE on eth                       dataset:	 4.395855903625488
2022-10-30 01:01:07,724:INFO: Average validation o:	ADE  4.3959	FDE  7.7505
2022-10-30 01:01:07,725:INFO: - Computing ADE (validation)
2022-10-30 01:01:08,071:INFO: 		 ADE on hotel                     dataset:	 1.9840306043624878
2022-10-30 01:01:08,525:INFO: 		 ADE on univ                      dataset:	 2.129092216491699
2022-10-30 01:01:08,903:INFO: 		 ADE on zara1                     dataset:	 2.202443838119507
2022-10-30 01:01:09,469:INFO: 		 ADE on zara2                     dataset:	 2.304842948913574
2022-10-30 01:01:09,469:INFO: Average validation:	ADE  2.1899	FDE  4.0519
2022-10-30 01:01:09,470:INFO: - Computing ADE (training)
2022-10-30 01:01:09,952:INFO: 		 ADE on hotel                     dataset:	 2.1486544609069824
2022-10-30 01:01:11,013:INFO: 		 ADE on univ                      dataset:	 2.05088472366333
2022-10-30 01:01:11,732:INFO: 		 ADE on zara1                     dataset:	 3.069293737411499
2022-10-30 01:01:12,940:INFO: 		 ADE on zara2                     dataset:	 2.7358453273773193
2022-10-30 01:01:12,940:INFO: Average training:	ADE  2.2573	FDE  4.1797
2022-10-30 01:01:12,951:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_472.pth.tar
2022-10-30 01:01:12,951:INFO: 
===> EPOCH: 473 (P4)
2022-10-30 01:01:12,951:INFO: - Computing loss (training)
2022-10-30 01:01:14,236:INFO: Dataset: hotel               Batch: 1/4	Loss 83.7974 (83.7974)
2022-10-30 01:01:15,286:INFO: Dataset: hotel               Batch: 2/4	Loss 83.2500 (83.5458)
2022-10-30 01:01:16,338:INFO: Dataset: hotel               Batch: 3/4	Loss 85.6785 (84.2522)
2022-10-30 01:01:17,118:INFO: Dataset: hotel               Batch: 4/4	Loss 51.5694 (78.6470)
2022-10-30 01:01:18,566:INFO: Dataset: univ                Batch:  1/15	Loss 87.2923 (87.2923)
2022-10-30 01:01:19,676:INFO: Dataset: univ                Batch:  2/15	Loss 91.8034 (89.5471)
2022-10-30 01:01:20,798:INFO: Dataset: univ                Batch:  3/15	Loss 95.9735 (91.8709)
2022-10-30 01:01:21,914:INFO: Dataset: univ                Batch:  4/15	Loss 90.7850 (91.6058)
2022-10-30 01:01:23,042:INFO: Dataset: univ                Batch:  5/15	Loss 92.1875 (91.7247)
2022-10-30 01:01:24,158:INFO: Dataset: univ                Batch:  6/15	Loss 88.7074 (91.2727)
2022-10-30 01:01:25,273:INFO: Dataset: univ                Batch:  7/15	Loss 87.7690 (90.8239)
2022-10-30 01:01:26,391:INFO: Dataset: univ                Batch:  8/15	Loss 95.3855 (91.3807)
2022-10-30 01:01:27,522:INFO: Dataset: univ                Batch:  9/15	Loss 89.3918 (91.1469)
2022-10-30 01:01:28,639:INFO: Dataset: univ                Batch: 10/15	Loss 88.4402 (90.8492)
2022-10-30 01:01:29,764:INFO: Dataset: univ                Batch: 11/15	Loss 87.6725 (90.5699)
2022-10-30 01:01:30,879:INFO: Dataset: univ                Batch: 12/15	Loss 90.6840 (90.5797)
2022-10-30 01:01:31,991:INFO: Dataset: univ                Batch: 13/15	Loss 90.1136 (90.5446)
2022-10-30 01:01:33,101:INFO: Dataset: univ                Batch: 14/15	Loss 91.9123 (90.6457)
2022-10-30 01:01:33,575:INFO: Dataset: univ                Batch: 15/15	Loss 16.7440 (89.5425)
2022-10-30 01:01:34,951:INFO: Dataset: zara1               Batch: 1/8	Loss 94.4881 (94.4881)
2022-10-30 01:01:35,989:INFO: Dataset: zara1               Batch: 2/8	Loss 90.5115 (92.4870)
2022-10-30 01:01:37,032:INFO: Dataset: zara1               Batch: 3/8	Loss 222.7481 (132.7740)
2022-10-30 01:01:38,076:INFO: Dataset: zara1               Batch: 4/8	Loss 89.5235 (120.8986)
2022-10-30 01:01:39,121:INFO: Dataset: zara1               Batch: 5/8	Loss 97.1926 (115.8386)
2022-10-30 01:01:40,168:INFO: Dataset: zara1               Batch: 6/8	Loss 95.1507 (112.0128)
2022-10-30 01:01:41,213:INFO: Dataset: zara1               Batch: 7/8	Loss 90.6112 (109.0783)
2022-10-30 01:01:42,160:INFO: Dataset: zara1               Batch: 8/8	Loss 78.0253 (105.6788)
2022-10-30 01:01:43,550:INFO: Dataset: zara2               Batch:  1/18	Loss 90.8268 (90.8268)
2022-10-30 01:01:44,636:INFO: Dataset: zara2               Batch:  2/18	Loss 93.5303 (92.1647)
2022-10-30 01:01:45,725:INFO: Dataset: zara2               Batch:  3/18	Loss 98.9980 (94.3679)
2022-10-30 01:01:46,803:INFO: Dataset: zara2               Batch:  4/18	Loss 104.2316 (96.9636)
2022-10-30 01:01:47,880:INFO: Dataset: zara2               Batch:  5/18	Loss 95.2001 (96.6336)
2022-10-30 01:01:48,970:INFO: Dataset: zara2               Batch:  6/18	Loss 104.0750 (97.8812)
2022-10-30 01:01:50,043:INFO: Dataset: zara2               Batch:  7/18	Loss 85.7113 (96.2861)
2022-10-30 01:01:51,112:INFO: Dataset: zara2               Batch:  8/18	Loss 96.3394 (96.2931)
2022-10-30 01:01:52,167:INFO: Dataset: zara2               Batch:  9/18	Loss 108.9369 (97.7600)
2022-10-30 01:01:53,219:INFO: Dataset: zara2               Batch: 10/18	Loss 310.5288 (118.4608)
2022-10-30 01:01:54,274:INFO: Dataset: zara2               Batch: 11/18	Loss 89.8411 (115.8316)
2022-10-30 01:01:55,331:INFO: Dataset: zara2               Batch: 12/18	Loss 88.4532 (113.6152)
2022-10-30 01:01:56,389:INFO: Dataset: zara2               Batch: 13/18	Loss 127.4288 (114.7603)
2022-10-30 01:01:57,453:INFO: Dataset: zara2               Batch: 14/18	Loss 94.2654 (113.3926)
2022-10-30 01:01:58,509:INFO: Dataset: zara2               Batch: 15/18	Loss 94.3815 (112.1607)
2022-10-30 01:01:59,565:INFO: Dataset: zara2               Batch: 16/18	Loss 145.9178 (114.4441)
2022-10-30 01:02:00,629:INFO: Dataset: zara2               Batch: 17/18	Loss 95.2458 (113.2873)
2022-10-30 01:02:01,589:INFO: Dataset: zara2               Batch: 18/18	Loss 76.3315 (111.4117)
2022-10-30 01:02:01,657:INFO: - Computing ADE (validation o)
2022-10-30 01:02:02,015:INFO: 		 ADE on eth                       dataset:	 4.332863807678223
2022-10-30 01:02:02,015:INFO: Average validation o:	ADE  4.3329	FDE  7.6502
2022-10-30 01:02:02,016:INFO: - Computing ADE (validation)
2022-10-30 01:02:02,362:INFO: 		 ADE on hotel                     dataset:	 2.0160701274871826
2022-10-30 01:02:02,818:INFO: 		 ADE on univ                      dataset:	 2.1176514625549316
2022-10-30 01:02:03,178:INFO: 		 ADE on zara1                     dataset:	 2.1977665424346924
2022-10-30 01:02:03,718:INFO: 		 ADE on zara2                     dataset:	 2.3160603046417236
2022-10-30 01:02:03,718:INFO: Average validation:	ADE  2.1895	FDE  4.0473
2022-10-30 01:02:03,719:INFO: - Computing ADE (training)
2022-10-30 01:02:04,204:INFO: 		 ADE on hotel                     dataset:	 2.133549213409424
2022-10-30 01:02:05,302:INFO: 		 ADE on univ                      dataset:	 2.043241500854492
2022-10-30 01:02:06,013:INFO: 		 ADE on zara1                     dataset:	 3.0780980587005615
2022-10-30 01:02:07,238:INFO: 		 ADE on zara2                     dataset:	 2.718864679336548
2022-10-30 01:02:07,239:INFO: Average training:	ADE  2.2486	FDE  4.1629
2022-10-30 01:02:07,250:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_473.pth.tar
2022-10-30 01:02:07,250:INFO: 
===> EPOCH: 474 (P4)
2022-10-30 01:02:07,251:INFO: - Computing loss (training)
2022-10-30 01:02:08,528:INFO: Dataset: hotel               Batch: 1/4	Loss 81.4605 (81.4605)
2022-10-30 01:02:09,664:INFO: Dataset: hotel               Batch: 2/4	Loss 85.3783 (83.4773)
2022-10-30 01:02:10,712:INFO: Dataset: hotel               Batch: 3/4	Loss 85.0296 (84.0164)
2022-10-30 01:02:11,485:INFO: Dataset: hotel               Batch: 4/4	Loss 51.7876 (78.2339)
2022-10-30 01:02:12,971:INFO: Dataset: univ                Batch:  1/15	Loss 87.5753 (87.5753)
2022-10-30 01:02:14,125:INFO: Dataset: univ                Batch:  2/15	Loss 89.6498 (88.6543)
2022-10-30 01:02:15,267:INFO: Dataset: univ                Batch:  3/15	Loss 88.1257 (88.4920)
2022-10-30 01:02:16,415:INFO: Dataset: univ                Batch:  4/15	Loss 87.2022 (88.1645)
2022-10-30 01:02:17,539:INFO: Dataset: univ                Batch:  5/15	Loss 89.9833 (88.4810)
2022-10-30 01:02:18,654:INFO: Dataset: univ                Batch:  6/15	Loss 97.1429 (89.8771)
2022-10-30 01:02:19,770:INFO: Dataset: univ                Batch:  7/15	Loss 89.5314 (89.8251)
2022-10-30 01:02:20,875:INFO: Dataset: univ                Batch:  8/15	Loss 91.8259 (90.0652)
2022-10-30 01:02:22,000:INFO: Dataset: univ                Batch:  9/15	Loss 96.4245 (90.8024)
2022-10-30 01:02:23,108:INFO: Dataset: univ                Batch: 10/15	Loss 96.1559 (91.2899)
2022-10-30 01:02:24,213:INFO: Dataset: univ                Batch: 11/15	Loss 184.1336 (98.8346)
2022-10-30 01:02:25,339:INFO: Dataset: univ                Batch: 12/15	Loss 89.0240 (97.9618)
2022-10-30 01:02:26,457:INFO: Dataset: univ                Batch: 13/15	Loss 88.7135 (97.2524)
2022-10-30 01:02:27,575:INFO: Dataset: univ                Batch: 14/15	Loss 89.1022 (96.6550)
2022-10-30 01:02:28,053:INFO: Dataset: univ                Batch: 15/15	Loss 16.5012 (95.5345)
2022-10-30 01:02:29,427:INFO: Dataset: zara1               Batch: 1/8	Loss 91.8721 (91.8721)
2022-10-30 01:02:30,482:INFO: Dataset: zara1               Batch: 2/8	Loss 94.6756 (93.2443)
2022-10-30 01:02:31,541:INFO: Dataset: zara1               Batch: 3/8	Loss 97.1796 (94.5924)
2022-10-30 01:02:32,606:INFO: Dataset: zara1               Batch: 4/8	Loss 94.6321 (94.6033)
2022-10-30 01:02:33,658:INFO: Dataset: zara1               Batch: 5/8	Loss 90.2549 (93.7805)
2022-10-30 01:02:34,720:INFO: Dataset: zara1               Batch: 6/8	Loss 95.5066 (94.0541)
2022-10-30 01:02:35,782:INFO: Dataset: zara1               Batch: 7/8	Loss 96.4486 (94.3952)
2022-10-30 01:02:36,733:INFO: Dataset: zara1               Batch: 8/8	Loss 78.7524 (92.7404)
2022-10-30 01:02:38,123:INFO: Dataset: zara2               Batch:  1/18	Loss 113.4342 (113.4342)
2022-10-30 01:02:39,196:INFO: Dataset: zara2               Batch:  2/18	Loss 92.9764 (103.9130)
2022-10-30 01:02:40,278:INFO: Dataset: zara2               Batch:  3/18	Loss 130.8927 (113.4546)
2022-10-30 01:02:41,359:INFO: Dataset: zara2               Batch:  4/18	Loss 86.1594 (106.2030)
2022-10-30 01:02:42,443:INFO: Dataset: zara2               Batch:  5/18	Loss 94.4896 (103.9166)
2022-10-30 01:02:43,517:INFO: Dataset: zara2               Batch:  6/18	Loss 99.4903 (103.1225)
2022-10-30 01:02:44,600:INFO: Dataset: zara2               Batch:  7/18	Loss 88.4939 (100.6634)
2022-10-30 01:02:45,672:INFO: Dataset: zara2               Batch:  8/18	Loss 90.9375 (99.4889)
2022-10-30 01:02:46,747:INFO: Dataset: zara2               Batch:  9/18	Loss 92.8456 (98.7465)
2022-10-30 01:02:47,832:INFO: Dataset: zara2               Batch: 10/18	Loss 91.4130 (98.0533)
2022-10-30 01:02:48,916:INFO: Dataset: zara2               Batch: 11/18	Loss 87.7749 (97.2587)
2022-10-30 01:02:50,006:INFO: Dataset: zara2               Batch: 12/18	Loss 90.5191 (96.6968)
2022-10-30 01:02:51,079:INFO: Dataset: zara2               Batch: 13/18	Loss 89.1156 (96.0661)
2022-10-30 01:02:52,152:INFO: Dataset: zara2               Batch: 14/18	Loss 91.4943 (95.7221)
2022-10-30 01:02:53,225:INFO: Dataset: zara2               Batch: 15/18	Loss 90.0470 (95.3546)
2022-10-30 01:02:54,300:INFO: Dataset: zara2               Batch: 16/18	Loss 87.6435 (94.8685)
2022-10-30 01:02:55,370:INFO: Dataset: zara2               Batch: 17/18	Loss 92.8844 (94.7634)
2022-10-30 01:02:56,327:INFO: Dataset: zara2               Batch: 18/18	Loss 89.1059 (94.5268)
2022-10-30 01:02:56,393:INFO: - Computing ADE (validation o)
2022-10-30 01:02:56,747:INFO: 		 ADE on eth                       dataset:	 4.442169189453125
2022-10-30 01:02:56,747:INFO: Average validation o:	ADE  4.4422	FDE  7.8034
2022-10-30 01:02:56,748:INFO: - Computing ADE (validation)
2022-10-30 01:02:57,100:INFO: 		 ADE on hotel                     dataset:	 1.9452322721481323
2022-10-30 01:02:57,542:INFO: 		 ADE on univ                      dataset:	 2.1079206466674805
2022-10-30 01:02:57,896:INFO: 		 ADE on zara1                     dataset:	 2.241140842437744
2022-10-30 01:02:58,445:INFO: 		 ADE on zara2                     dataset:	 2.300900459289551
2022-10-30 01:02:58,446:INFO: Average validation:	ADE  2.1775	FDE  4.0255
2022-10-30 01:02:58,446:INFO: - Computing ADE (training)
2022-10-30 01:02:58,913:INFO: 		 ADE on hotel                     dataset:	 2.1471428871154785
2022-10-30 01:03:00,004:INFO: 		 ADE on univ                      dataset:	 2.0407755374908447
2022-10-30 01:03:00,687:INFO: 		 ADE on zara1                     dataset:	 3.08278489112854
2022-10-30 01:03:01,913:INFO: 		 ADE on zara2                     dataset:	 2.7230710983276367
2022-10-30 01:03:01,913:INFO: Average training:	ADE  2.2484	FDE  4.1595
2022-10-30 01:03:01,924:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_474.pth.tar
2022-10-30 01:03:01,924:INFO: 
===> EPOCH: 475 (P4)
2022-10-30 01:03:01,924:INFO: - Computing loss (training)
2022-10-30 01:03:03,204:INFO: Dataset: hotel               Batch: 1/4	Loss 84.2709 (84.2709)
2022-10-30 01:03:04,255:INFO: Dataset: hotel               Batch: 2/4	Loss 83.0180 (83.6385)
2022-10-30 01:03:05,303:INFO: Dataset: hotel               Batch: 3/4	Loss 90.0330 (85.8034)
2022-10-30 01:03:06,073:INFO: Dataset: hotel               Batch: 4/4	Loss 50.8006 (80.2621)
2022-10-30 01:03:07,519:INFO: Dataset: univ                Batch:  1/15	Loss 86.1131 (86.1131)
2022-10-30 01:03:08,647:INFO: Dataset: univ                Batch:  2/15	Loss 102.9779 (93.9610)
2022-10-30 01:03:09,786:INFO: Dataset: univ                Batch:  3/15	Loss 90.2441 (92.7403)
2022-10-30 01:03:10,922:INFO: Dataset: univ                Batch:  4/15	Loss 86.3633 (90.9757)
2022-10-30 01:03:12,051:INFO: Dataset: univ                Batch:  5/15	Loss 96.2154 (92.0680)
2022-10-30 01:03:13,170:INFO: Dataset: univ                Batch:  6/15	Loss 89.6792 (91.7080)
2022-10-30 01:03:14,278:INFO: Dataset: univ                Batch:  7/15	Loss 93.0629 (91.8885)
2022-10-30 01:03:15,403:INFO: Dataset: univ                Batch:  8/15	Loss 87.9771 (91.3446)
2022-10-30 01:03:16,521:INFO: Dataset: univ                Batch:  9/15	Loss 102.6720 (92.5838)
2022-10-30 01:03:17,639:INFO: Dataset: univ                Batch: 10/15	Loss 100.9445 (93.4635)
2022-10-30 01:03:18,761:INFO: Dataset: univ                Batch: 11/15	Loss 86.4671 (92.8270)
2022-10-30 01:03:19,879:INFO: Dataset: univ                Batch: 12/15	Loss 89.6272 (92.5708)
2022-10-30 01:03:20,998:INFO: Dataset: univ                Batch: 13/15	Loss 89.3430 (92.3341)
2022-10-30 01:03:22,121:INFO: Dataset: univ                Batch: 14/15	Loss 87.3818 (91.9898)
2022-10-30 01:03:22,601:INFO: Dataset: univ                Batch: 15/15	Loss 16.4093 (90.9547)
2022-10-30 01:03:23,965:INFO: Dataset: zara1               Batch: 1/8	Loss 92.5185 (92.5185)
2022-10-30 01:03:25,028:INFO: Dataset: zara1               Batch: 2/8	Loss 93.9381 (93.2006)
2022-10-30 01:03:26,085:INFO: Dataset: zara1               Batch: 3/8	Loss 90.4749 (92.2809)
2022-10-30 01:03:27,143:INFO: Dataset: zara1               Batch: 4/8	Loss 90.1916 (91.7780)
2022-10-30 01:03:28,198:INFO: Dataset: zara1               Batch: 5/8	Loss 96.5471 (92.7816)
2022-10-30 01:03:29,255:INFO: Dataset: zara1               Batch: 6/8	Loss 89.9460 (92.3110)
2022-10-30 01:03:30,325:INFO: Dataset: zara1               Batch: 7/8	Loss 96.7302 (92.8862)
2022-10-30 01:03:31,284:INFO: Dataset: zara1               Batch: 8/8	Loss 112.7123 (95.0775)
2022-10-30 01:03:32,680:INFO: Dataset: zara2               Batch:  1/18	Loss 95.6132 (95.6132)
2022-10-30 01:03:33,737:INFO: Dataset: zara2               Batch:  2/18	Loss 125.5135 (110.4291)
2022-10-30 01:03:34,790:INFO: Dataset: zara2               Batch:  3/18	Loss 90.2007 (103.8774)
2022-10-30 01:03:35,851:INFO: Dataset: zara2               Batch:  4/18	Loss 86.2853 (99.6553)
2022-10-30 01:03:36,906:INFO: Dataset: zara2               Batch:  5/18	Loss 97.3376 (99.1450)
2022-10-30 01:03:37,963:INFO: Dataset: zara2               Batch:  6/18	Loss 90.3957 (97.7139)
2022-10-30 01:03:39,010:INFO: Dataset: zara2               Batch:  7/18	Loss 92.7153 (96.8818)
2022-10-30 01:03:40,064:INFO: Dataset: zara2               Batch:  8/18	Loss 88.4173 (95.8525)
2022-10-30 01:03:41,119:INFO: Dataset: zara2               Batch:  9/18	Loss 87.2940 (94.9271)
2022-10-30 01:03:42,172:INFO: Dataset: zara2               Batch: 10/18	Loss 89.1610 (94.3446)
2022-10-30 01:03:43,222:INFO: Dataset: zara2               Batch: 11/18	Loss 111.9929 (95.9080)
2022-10-30 01:03:44,274:INFO: Dataset: zara2               Batch: 12/18	Loss 87.3383 (95.2281)
2022-10-30 01:03:45,332:INFO: Dataset: zara2               Batch: 13/18	Loss 95.2392 (95.2290)
2022-10-30 01:03:46,385:INFO: Dataset: zara2               Batch: 14/18	Loss 91.6090 (94.9450)
2022-10-30 01:03:47,441:INFO: Dataset: zara2               Batch: 15/18	Loss 100.8955 (95.3530)
2022-10-30 01:03:48,488:INFO: Dataset: zara2               Batch: 16/18	Loss 91.5567 (95.1330)
2022-10-30 01:03:49,537:INFO: Dataset: zara2               Batch: 17/18	Loss 99.9570 (95.4288)
2022-10-30 01:03:50,491:INFO: Dataset: zara2               Batch: 18/18	Loss 76.1704 (94.5470)
2022-10-30 01:03:50,559:INFO: - Computing ADE (validation o)
2022-10-30 01:03:50,928:INFO: 		 ADE on eth                       dataset:	 4.345881462097168
2022-10-30 01:03:50,929:INFO: Average validation o:	ADE  4.3459	FDE  7.6567
2022-10-30 01:03:50,929:INFO: - Computing ADE (validation)
2022-10-30 01:03:51,264:INFO: 		 ADE on hotel                     dataset:	 1.9888548851013184
2022-10-30 01:03:51,707:INFO: 		 ADE on univ                      dataset:	 2.112360954284668
2022-10-30 01:03:52,063:INFO: 		 ADE on zara1                     dataset:	 2.244708776473999
2022-10-30 01:03:52,599:INFO: 		 ADE on zara2                     dataset:	 2.3169779777526855
2022-10-30 01:03:52,599:INFO: Average validation:	ADE  2.1883	FDE  4.0372
2022-10-30 01:03:52,600:INFO: - Computing ADE (training)
2022-10-30 01:03:53,083:INFO: 		 ADE on hotel                     dataset:	 2.1459202766418457
2022-10-30 01:03:54,127:INFO: 		 ADE on univ                      dataset:	 2.0361433029174805
2022-10-30 01:03:54,799:INFO: 		 ADE on zara1                     dataset:	 3.062406301498413
2022-10-30 01:03:56,008:INFO: 		 ADE on zara2                     dataset:	 2.7096433639526367
2022-10-30 01:03:56,009:INFO: Average training:	ADE  2.2410	FDE  4.1445
2022-10-30 01:03:56,020:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_475.pth.tar
2022-10-30 01:03:56,020:INFO: 
===> EPOCH: 476 (P4)
2022-10-30 01:03:56,020:INFO: - Computing loss (training)
2022-10-30 01:03:57,324:INFO: Dataset: hotel               Batch: 1/4	Loss 82.5739 (82.5739)
2022-10-30 01:03:58,395:INFO: Dataset: hotel               Batch: 2/4	Loss 84.7579 (83.6712)
2022-10-30 01:03:59,470:INFO: Dataset: hotel               Batch: 3/4	Loss 84.3129 (83.8918)
2022-10-30 01:04:00,249:INFO: Dataset: hotel               Batch: 4/4	Loss 51.2240 (78.5478)
2022-10-30 01:04:01,677:INFO: Dataset: univ                Batch:  1/15	Loss 90.8876 (90.8876)
2022-10-30 01:04:02,883:INFO: Dataset: univ                Batch:  2/15	Loss 94.4756 (92.7095)
2022-10-30 01:04:04,013:INFO: Dataset: univ                Batch:  3/15	Loss 87.6963 (91.0345)
2022-10-30 01:04:05,140:INFO: Dataset: univ                Batch:  4/15	Loss 86.9802 (90.0681)
2022-10-30 01:04:06,256:INFO: Dataset: univ                Batch:  5/15	Loss 93.3067 (90.6577)
2022-10-30 01:04:07,386:INFO: Dataset: univ                Batch:  6/15	Loss 107.3547 (93.2906)
2022-10-30 01:04:08,501:INFO: Dataset: univ                Batch:  7/15	Loss 97.1105 (93.8137)
2022-10-30 01:04:09,622:INFO: Dataset: univ                Batch:  8/15	Loss 86.6580 (92.9428)
2022-10-30 01:04:10,753:INFO: Dataset: univ                Batch:  9/15	Loss 81.7373 (91.5628)
2022-10-30 01:04:11,868:INFO: Dataset: univ                Batch: 10/15	Loss 109.0405 (93.2260)
2022-10-30 01:04:12,987:INFO: Dataset: univ                Batch: 11/15	Loss 89.4525 (92.8881)
2022-10-30 01:04:14,098:INFO: Dataset: univ                Batch: 12/15	Loss 85.9645 (92.3073)
2022-10-30 01:04:15,205:INFO: Dataset: univ                Batch: 13/15	Loss 89.5320 (92.0991)
2022-10-30 01:04:16,318:INFO: Dataset: univ                Batch: 14/15	Loss 86.9488 (91.7409)
2022-10-30 01:04:16,796:INFO: Dataset: univ                Batch: 15/15	Loss 19.4257 (91.0109)
2022-10-30 01:04:18,174:INFO: Dataset: zara1               Batch: 1/8	Loss 89.7652 (89.7652)
2022-10-30 01:04:19,240:INFO: Dataset: zara1               Batch: 2/8	Loss 92.0545 (90.9214)
2022-10-30 01:04:20,306:INFO: Dataset: zara1               Batch: 3/8	Loss 93.8367 (91.8488)
2022-10-30 01:04:21,367:INFO: Dataset: zara1               Batch: 4/8	Loss 93.0598 (92.1478)
2022-10-30 01:04:22,427:INFO: Dataset: zara1               Batch: 5/8	Loss 90.8109 (91.8603)
2022-10-30 01:04:23,490:INFO: Dataset: zara1               Batch: 6/8	Loss 90.5588 (91.6498)
2022-10-30 01:04:24,554:INFO: Dataset: zara1               Batch: 7/8	Loss 131.6144 (97.0920)
2022-10-30 01:04:25,511:INFO: Dataset: zara1               Batch: 8/8	Loss 80.0788 (95.2026)
2022-10-30 01:04:26,916:INFO: Dataset: zara2               Batch:  1/18	Loss 147.6098 (147.6098)
2022-10-30 01:04:28,005:INFO: Dataset: zara2               Batch:  2/18	Loss 93.8706 (121.0620)
2022-10-30 01:04:29,075:INFO: Dataset: zara2               Batch:  3/18	Loss 97.7607 (112.7417)
2022-10-30 01:04:30,156:INFO: Dataset: zara2               Batch:  4/18	Loss 88.2997 (106.9727)
2022-10-30 01:04:31,227:INFO: Dataset: zara2               Batch:  5/18	Loss 89.8169 (103.2569)
2022-10-30 01:04:32,306:INFO: Dataset: zara2               Batch:  6/18	Loss 90.9302 (101.2431)
2022-10-30 01:04:33,377:INFO: Dataset: zara2               Batch:  7/18	Loss 97.7716 (100.7433)
2022-10-30 01:04:34,458:INFO: Dataset: zara2               Batch:  8/18	Loss 91.9715 (99.5816)
2022-10-30 01:04:35,525:INFO: Dataset: zara2               Batch:  9/18	Loss 93.2783 (98.9375)
2022-10-30 01:04:36,595:INFO: Dataset: zara2               Batch: 10/18	Loss 89.2565 (98.0298)
2022-10-30 01:04:37,666:INFO: Dataset: zara2               Batch: 11/18	Loss 89.2394 (97.2847)
2022-10-30 01:04:38,750:INFO: Dataset: zara2               Batch: 12/18	Loss 89.7274 (96.6551)
2022-10-30 01:04:39,819:INFO: Dataset: zara2               Batch: 13/18	Loss 91.6423 (96.3170)
2022-10-30 01:04:40,891:INFO: Dataset: zara2               Batch: 14/18	Loss 91.0568 (95.9450)
2022-10-30 01:04:41,965:INFO: Dataset: zara2               Batch: 15/18	Loss 95.1710 (95.8938)
2022-10-30 01:04:43,030:INFO: Dataset: zara2               Batch: 16/18	Loss 96.2604 (95.9199)
2022-10-30 01:04:44,106:INFO: Dataset: zara2               Batch: 17/18	Loss 88.4512 (95.4774)
2022-10-30 01:04:45,076:INFO: Dataset: zara2               Batch: 18/18	Loss 80.4721 (94.8324)
2022-10-30 01:04:45,143:INFO: - Computing ADE (validation o)
2022-10-30 01:04:45,515:INFO: 		 ADE on eth                       dataset:	 4.338133335113525
2022-10-30 01:04:45,515:INFO: Average validation o:	ADE  4.3381	FDE  7.6511
2022-10-30 01:04:45,516:INFO: - Computing ADE (validation)
2022-10-30 01:04:45,876:INFO: 		 ADE on hotel                     dataset:	 1.9314773082733154
2022-10-30 01:04:46,332:INFO: 		 ADE on univ                      dataset:	 2.1134989261627197
2022-10-30 01:04:46,692:INFO: 		 ADE on zara1                     dataset:	 2.259288787841797
2022-10-30 01:04:47,243:INFO: 		 ADE on zara2                     dataset:	 2.305172920227051
2022-10-30 01:04:47,243:INFO: Average validation:	ADE  2.1823	FDE  4.0250
2022-10-30 01:04:47,244:INFO: - Computing ADE (training)
2022-10-30 01:04:47,727:INFO: 		 ADE on hotel                     dataset:	 2.135502338409424
2022-10-30 01:04:48,820:INFO: 		 ADE on univ                      dataset:	 2.033162832260132
2022-10-30 01:04:49,527:INFO: 		 ADE on zara1                     dataset:	 3.0806500911712646
2022-10-30 01:04:50,770:INFO: 		 ADE on zara2                     dataset:	 2.7125022411346436
2022-10-30 01:04:50,770:INFO: Average training:	ADE  2.2404	FDE  4.1399
2022-10-30 01:04:50,782:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_476.pth.tar
2022-10-30 01:04:50,782:INFO: 
===> EPOCH: 477 (P4)
2022-10-30 01:04:50,783:INFO: - Computing loss (training)
2022-10-30 01:04:52,093:INFO: Dataset: hotel               Batch: 1/4	Loss 82.8565 (82.8565)
2022-10-30 01:04:53,171:INFO: Dataset: hotel               Batch: 2/4	Loss 85.6336 (84.2418)
2022-10-30 01:04:54,235:INFO: Dataset: hotel               Batch: 3/4	Loss 84.1558 (84.2134)
2022-10-30 01:04:55,020:INFO: Dataset: hotel               Batch: 4/4	Loss 50.3018 (78.4421)
2022-10-30 01:04:56,450:INFO: Dataset: univ                Batch:  1/15	Loss 90.3138 (90.3138)
2022-10-30 01:04:57,553:INFO: Dataset: univ                Batch:  2/15	Loss 89.8708 (90.0865)
2022-10-30 01:04:58,684:INFO: Dataset: univ                Batch:  3/15	Loss 86.1971 (88.6472)
2022-10-30 01:04:59,793:INFO: Dataset: univ                Batch:  4/15	Loss 91.4511 (89.3484)
2022-10-30 01:05:00,901:INFO: Dataset: univ                Batch:  5/15	Loss 87.3182 (88.9558)
2022-10-30 01:05:02,011:INFO: Dataset: univ                Batch:  6/15	Loss 102.7438 (91.0915)
2022-10-30 01:05:03,123:INFO: Dataset: univ                Batch:  7/15	Loss 88.8909 (90.7809)
2022-10-30 01:05:04,237:INFO: Dataset: univ                Batch:  8/15	Loss 87.6275 (90.3632)
2022-10-30 01:05:05,349:INFO: Dataset: univ                Batch:  9/15	Loss 129.4898 (94.7624)
2022-10-30 01:05:06,466:INFO: Dataset: univ                Batch: 10/15	Loss 87.2547 (93.9658)
2022-10-30 01:05:07,582:INFO: Dataset: univ                Batch: 11/15	Loss 90.2852 (93.6208)
2022-10-30 01:05:08,698:INFO: Dataset: univ                Batch: 12/15	Loss 176.3346 (100.6201)
2022-10-30 01:05:09,818:INFO: Dataset: univ                Batch: 13/15	Loss 87.5955 (99.5559)
2022-10-30 01:05:10,940:INFO: Dataset: univ                Batch: 14/15	Loss 89.3015 (98.7927)
2022-10-30 01:05:11,421:INFO: Dataset: univ                Batch: 15/15	Loss 16.6618 (97.5628)
2022-10-30 01:05:12,777:INFO: Dataset: zara1               Batch: 1/8	Loss 93.4305 (93.4305)
2022-10-30 01:05:13,839:INFO: Dataset: zara1               Batch: 2/8	Loss 94.5994 (94.0686)
2022-10-30 01:05:14,895:INFO: Dataset: zara1               Batch: 3/8	Loss 89.5047 (92.4893)
2022-10-30 01:05:15,949:INFO: Dataset: zara1               Batch: 4/8	Loss 91.9598 (92.3531)
2022-10-30 01:05:17,005:INFO: Dataset: zara1               Batch: 5/8	Loss 96.4492 (93.1074)
2022-10-30 01:05:18,056:INFO: Dataset: zara1               Batch: 6/8	Loss 91.4064 (92.8163)
2022-10-30 01:05:19,108:INFO: Dataset: zara1               Batch: 7/8	Loss 91.7150 (92.6593)
2022-10-30 01:05:20,063:INFO: Dataset: zara1               Batch: 8/8	Loss 79.9359 (91.3468)
2022-10-30 01:05:21,420:INFO: Dataset: zara2               Batch:  1/18	Loss 99.5086 (99.5086)
2022-10-30 01:05:22,490:INFO: Dataset: zara2               Batch:  2/18	Loss 88.9449 (94.2653)
2022-10-30 01:05:23,548:INFO: Dataset: zara2               Batch:  3/18	Loss 91.9149 (93.5282)
2022-10-30 01:05:24,605:INFO: Dataset: zara2               Batch:  4/18	Loss 87.4038 (91.9959)
2022-10-30 01:05:25,658:INFO: Dataset: zara2               Batch:  5/18	Loss 86.0236 (90.8036)
2022-10-30 01:05:26,712:INFO: Dataset: zara2               Batch:  6/18	Loss 97.3878 (91.9294)
2022-10-30 01:05:27,763:INFO: Dataset: zara2               Batch:  7/18	Loss 118.1048 (95.6271)
2022-10-30 01:05:28,818:INFO: Dataset: zara2               Batch:  8/18	Loss 92.5421 (95.2083)
2022-10-30 01:05:29,869:INFO: Dataset: zara2               Batch:  9/18	Loss 147.7212 (101.1522)
2022-10-30 01:05:30,928:INFO: Dataset: zara2               Batch: 10/18	Loss 87.2111 (99.7568)
2022-10-30 01:05:31,987:INFO: Dataset: zara2               Batch: 11/18	Loss 91.6904 (99.0486)
2022-10-30 01:05:33,047:INFO: Dataset: zara2               Batch: 12/18	Loss 80.9176 (97.4664)
2022-10-30 01:05:34,104:INFO: Dataset: zara2               Batch: 13/18	Loss 90.9299 (96.9787)
2022-10-30 01:05:35,147:INFO: Dataset: zara2               Batch: 14/18	Loss 89.7384 (96.4518)
2022-10-30 01:05:36,191:INFO: Dataset: zara2               Batch: 15/18	Loss 89.5108 (95.9632)
2022-10-30 01:05:37,239:INFO: Dataset: zara2               Batch: 16/18	Loss 90.7753 (95.6569)
2022-10-30 01:05:38,294:INFO: Dataset: zara2               Batch: 17/18	Loss 139.9993 (98.2381)
2022-10-30 01:05:39,244:INFO: Dataset: zara2               Batch: 18/18	Loss 85.4637 (97.6172)
2022-10-30 01:05:39,311:INFO: - Computing ADE (validation o)
2022-10-30 01:05:39,661:INFO: 		 ADE on eth                       dataset:	 4.309361457824707
2022-10-30 01:05:39,661:INFO: Average validation o:	ADE  4.3094	FDE  7.5956
2022-10-30 01:05:39,662:INFO: - Computing ADE (validation)
2022-10-30 01:05:40,009:INFO: 		 ADE on hotel                     dataset:	 2.0008862018585205
2022-10-30 01:05:40,474:INFO: 		 ADE on univ                      dataset:	 2.1029155254364014
2022-10-30 01:05:40,837:INFO: 		 ADE on zara1                     dataset:	 2.2248682975769043
2022-10-30 01:05:41,384:INFO: 		 ADE on zara2                     dataset:	 2.2796835899353027
2022-10-30 01:05:41,384:INFO: Average validation:	ADE  2.1693	FDE  4.0009
2022-10-30 01:05:41,385:INFO: - Computing ADE (training)
2022-10-30 01:05:41,852:INFO: 		 ADE on hotel                     dataset:	 2.138324022293091
2022-10-30 01:05:42,908:INFO: 		 ADE on univ                      dataset:	 2.028102397918701
2022-10-30 01:05:43,585:INFO: 		 ADE on zara1                     dataset:	 3.0664384365081787
2022-10-30 01:05:44,763:INFO: 		 ADE on zara2                     dataset:	 2.709303617477417
2022-10-30 01:05:44,763:INFO: Average training:	ADE  2.2353	FDE  4.1272
2022-10-30 01:05:44,774:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_477.pth.tar
2022-10-30 01:05:44,774:INFO: 
===> EPOCH: 478 (P4)
2022-10-30 01:05:44,774:INFO: - Computing loss (training)
2022-10-30 01:05:46,070:INFO: Dataset: hotel               Batch: 1/4	Loss 84.5063 (84.5063)
2022-10-30 01:05:47,135:INFO: Dataset: hotel               Batch: 2/4	Loss 84.3351 (84.4207)
2022-10-30 01:05:48,197:INFO: Dataset: hotel               Batch: 3/4	Loss 83.0261 (83.9332)
2022-10-30 01:05:48,977:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9676 (77.8446)
2022-10-30 01:05:50,418:INFO: Dataset: univ                Batch:  1/15	Loss 95.6297 (95.6297)
2022-10-30 01:05:51,534:INFO: Dataset: univ                Batch:  2/15	Loss 85.7235 (90.4401)
2022-10-30 01:05:52,657:INFO: Dataset: univ                Batch:  3/15	Loss 88.3882 (89.6899)
2022-10-30 01:05:53,848:INFO: Dataset: univ                Batch:  4/15	Loss 102.0102 (92.4248)
2022-10-30 01:05:54,958:INFO: Dataset: univ                Batch:  5/15	Loss 89.4952 (91.8878)
2022-10-30 01:05:56,065:INFO: Dataset: univ                Batch:  6/15	Loss 102.2858 (93.4886)
2022-10-30 01:05:57,176:INFO: Dataset: univ                Batch:  7/15	Loss 104.3646 (95.0861)
2022-10-30 01:05:58,283:INFO: Dataset: univ                Batch:  8/15	Loss 90.8404 (94.5014)
2022-10-30 01:05:59,396:INFO: Dataset: univ                Batch:  9/15	Loss 86.0072 (93.5191)
2022-10-30 01:06:00,523:INFO: Dataset: univ                Batch: 10/15	Loss 89.1082 (93.0661)
2022-10-30 01:06:01,619:INFO: Dataset: univ                Batch: 11/15	Loss 107.5315 (94.1939)
2022-10-30 01:06:02,723:INFO: Dataset: univ                Batch: 12/15	Loss 89.0466 (93.7650)
2022-10-30 01:06:03,837:INFO: Dataset: univ                Batch: 13/15	Loss 86.4474 (93.1851)
2022-10-30 01:06:04,953:INFO: Dataset: univ                Batch: 14/15	Loss 87.9980 (92.8173)
2022-10-30 01:06:05,434:INFO: Dataset: univ                Batch: 15/15	Loss 15.8660 (91.7379)
2022-10-30 01:06:06,790:INFO: Dataset: zara1               Batch: 1/8	Loss 92.1065 (92.1065)
2022-10-30 01:06:07,843:INFO: Dataset: zara1               Batch: 2/8	Loss 90.1757 (91.2151)
2022-10-30 01:06:08,899:INFO: Dataset: zara1               Batch: 3/8	Loss 89.5945 (90.6480)
2022-10-30 01:06:09,950:INFO: Dataset: zara1               Batch: 4/8	Loss 89.1220 (90.2801)
2022-10-30 01:06:10,999:INFO: Dataset: zara1               Batch: 5/8	Loss 93.0228 (90.8255)
2022-10-30 01:06:12,046:INFO: Dataset: zara1               Batch: 6/8	Loss 95.1726 (91.5015)
2022-10-30 01:06:13,093:INFO: Dataset: zara1               Batch: 7/8	Loss 93.1954 (91.7561)
2022-10-30 01:06:14,042:INFO: Dataset: zara1               Batch: 8/8	Loss 79.8007 (90.5165)
2022-10-30 01:06:15,445:INFO: Dataset: zara2               Batch:  1/18	Loss 100.2583 (100.2583)
2022-10-30 01:06:16,523:INFO: Dataset: zara2               Batch:  2/18	Loss 93.1763 (96.4577)
2022-10-30 01:06:17,592:INFO: Dataset: zara2               Batch:  3/18	Loss 187.2775 (127.1425)
2022-10-30 01:06:18,659:INFO: Dataset: zara2               Batch:  4/18	Loss 89.6129 (117.7260)
2022-10-30 01:06:19,739:INFO: Dataset: zara2               Batch:  5/18	Loss 92.5345 (112.6380)
2022-10-30 01:06:20,809:INFO: Dataset: zara2               Batch:  6/18	Loss 93.6582 (109.3728)
2022-10-30 01:06:21,877:INFO: Dataset: zara2               Batch:  7/18	Loss 87.8317 (106.1359)
2022-10-30 01:06:22,958:INFO: Dataset: zara2               Batch:  8/18	Loss 98.9246 (105.2636)
2022-10-30 01:06:24,035:INFO: Dataset: zara2               Batch:  9/18	Loss 94.9102 (104.1228)
2022-10-30 01:06:25,116:INFO: Dataset: zara2               Batch: 10/18	Loss 95.7180 (103.3148)
2022-10-30 01:06:26,196:INFO: Dataset: zara2               Batch: 11/18	Loss 153.3668 (107.6924)
2022-10-30 01:06:27,282:INFO: Dataset: zara2               Batch: 12/18	Loss 96.0413 (106.7320)
2022-10-30 01:06:28,364:INFO: Dataset: zara2               Batch: 13/18	Loss 89.1504 (105.4041)
2022-10-30 01:06:29,446:INFO: Dataset: zara2               Batch: 14/18	Loss 105.4046 (105.4041)
2022-10-30 01:06:30,524:INFO: Dataset: zara2               Batch: 15/18	Loss 90.8381 (104.4231)
2022-10-30 01:06:31,607:INFO: Dataset: zara2               Batch: 16/18	Loss 90.8587 (103.6392)
2022-10-30 01:06:32,681:INFO: Dataset: zara2               Batch: 17/18	Loss 92.4022 (103.0033)
2022-10-30 01:06:33,644:INFO: Dataset: zara2               Batch: 18/18	Loss 78.8877 (101.9946)
2022-10-30 01:06:33,710:INFO: - Computing ADE (validation o)
2022-10-30 01:06:34,063:INFO: 		 ADE on eth                       dataset:	 4.352941989898682
2022-10-30 01:06:34,064:INFO: Average validation o:	ADE  4.3529	FDE  7.6561
2022-10-30 01:06:34,064:INFO: - Computing ADE (validation)
2022-10-30 01:06:34,420:INFO: 		 ADE on hotel                     dataset:	 1.964402675628662
2022-10-30 01:06:34,863:INFO: 		 ADE on univ                      dataset:	 2.09384822845459
2022-10-30 01:06:35,223:INFO: 		 ADE on zara1                     dataset:	 2.208091974258423
2022-10-30 01:06:35,762:INFO: 		 ADE on zara2                     dataset:	 2.3044817447662354
2022-10-30 01:06:35,762:INFO: Average validation:	ADE  2.1707	FDE  4.0018
2022-10-30 01:06:35,763:INFO: - Computing ADE (training)
2022-10-30 01:06:36,241:INFO: 		 ADE on hotel                     dataset:	 2.1220171451568604
2022-10-30 01:06:37,284:INFO: 		 ADE on univ                      dataset:	 2.033687114715576
2022-10-30 01:06:37,959:INFO: 		 ADE on zara1                     dataset:	 3.0664947032928467
2022-10-30 01:06:39,177:INFO: 		 ADE on zara2                     dataset:	 2.7102208137512207
2022-10-30 01:06:39,177:INFO: Average training:	ADE  2.2390	FDE  4.1294
2022-10-30 01:06:39,189:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_478.pth.tar
2022-10-30 01:06:39,189:INFO: 
===> EPOCH: 479 (P4)
2022-10-30 01:06:39,189:INFO: - Computing loss (training)
2022-10-30 01:06:40,468:INFO: Dataset: hotel               Batch: 1/4	Loss 84.0203 (84.0203)
2022-10-30 01:06:41,511:INFO: Dataset: hotel               Batch: 2/4	Loss 83.9508 (83.9867)
2022-10-30 01:06:42,558:INFO: Dataset: hotel               Batch: 3/4	Loss 83.3881 (83.8029)
2022-10-30 01:06:43,321:INFO: Dataset: hotel               Batch: 4/4	Loss 51.1421 (78.5030)
2022-10-30 01:06:44,771:INFO: Dataset: univ                Batch:  1/15	Loss 86.4594 (86.4594)
2022-10-30 01:06:45,886:INFO: Dataset: univ                Batch:  2/15	Loss 85.8894 (86.1612)
2022-10-30 01:06:47,006:INFO: Dataset: univ                Batch:  3/15	Loss 89.4605 (87.3521)
2022-10-30 01:06:48,119:INFO: Dataset: univ                Batch:  4/15	Loss 86.7142 (87.1939)
2022-10-30 01:06:49,230:INFO: Dataset: univ                Batch:  5/15	Loss 94.3204 (88.5744)
2022-10-30 01:06:50,350:INFO: Dataset: univ                Batch:  6/15	Loss 97.4585 (90.1195)
2022-10-30 01:06:51,457:INFO: Dataset: univ                Batch:  7/15	Loss 118.4738 (93.9302)
2022-10-30 01:06:52,572:INFO: Dataset: univ                Batch:  8/15	Loss 87.6238 (93.0274)
2022-10-30 01:06:53,675:INFO: Dataset: univ                Batch:  9/15	Loss 89.9985 (92.6947)
2022-10-30 01:06:54,774:INFO: Dataset: univ                Batch: 10/15	Loss 88.0054 (92.2607)
2022-10-30 01:06:55,877:INFO: Dataset: univ                Batch: 11/15	Loss 85.5633 (91.6560)
2022-10-30 01:06:56,988:INFO: Dataset: univ                Batch: 12/15	Loss 86.9893 (91.2644)
2022-10-30 01:06:58,093:INFO: Dataset: univ                Batch: 13/15	Loss 87.6851 (91.0035)
2022-10-30 01:06:59,208:INFO: Dataset: univ                Batch: 14/15	Loss 85.9012 (90.5960)
2022-10-30 01:06:59,692:INFO: Dataset: univ                Batch: 15/15	Loss 15.8020 (89.3909)
2022-10-30 01:07:01,037:INFO: Dataset: zara1               Batch: 1/8	Loss 92.8340 (92.8340)
2022-10-30 01:07:02,082:INFO: Dataset: zara1               Batch: 2/8	Loss 92.3248 (92.6053)
2022-10-30 01:07:03,127:INFO: Dataset: zara1               Batch: 3/8	Loss 90.5508 (91.9705)
2022-10-30 01:07:04,191:INFO: Dataset: zara1               Batch: 4/8	Loss 97.5169 (93.3151)
2022-10-30 01:07:05,251:INFO: Dataset: zara1               Batch: 5/8	Loss 92.3614 (93.1271)
2022-10-30 01:07:06,308:INFO: Dataset: zara1               Batch: 6/8	Loss 97.9859 (93.8431)
2022-10-30 01:07:07,364:INFO: Dataset: zara1               Batch: 7/8	Loss 93.3584 (93.7703)
2022-10-30 01:07:08,322:INFO: Dataset: zara1               Batch: 8/8	Loss 77.9965 (91.6533)
2022-10-30 01:07:09,688:INFO: Dataset: zara2               Batch:  1/18	Loss 105.1456 (105.1456)
2022-10-30 01:07:10,746:INFO: Dataset: zara2               Batch:  2/18	Loss 88.4553 (97.0848)
2022-10-30 01:07:11,793:INFO: Dataset: zara2               Batch:  3/18	Loss 97.1547 (97.1080)
2022-10-30 01:07:12,847:INFO: Dataset: zara2               Batch:  4/18	Loss 91.2741 (95.6291)
2022-10-30 01:07:13,899:INFO: Dataset: zara2               Batch:  5/18	Loss 81.7704 (92.9984)
2022-10-30 01:07:14,960:INFO: Dataset: zara2               Batch:  6/18	Loss 93.1877 (93.0289)
2022-10-30 01:07:16,017:INFO: Dataset: zara2               Batch:  7/18	Loss 88.4177 (92.3665)
2022-10-30 01:07:17,067:INFO: Dataset: zara2               Batch:  8/18	Loss 89.1913 (92.0004)
2022-10-30 01:07:18,127:INFO: Dataset: zara2               Batch:  9/18	Loss 89.2439 (91.6970)
2022-10-30 01:07:19,179:INFO: Dataset: zara2               Batch: 10/18	Loss 95.1578 (92.0487)
2022-10-30 01:07:20,227:INFO: Dataset: zara2               Batch: 11/18	Loss 94.8495 (92.3259)
2022-10-30 01:07:21,278:INFO: Dataset: zara2               Batch: 12/18	Loss 91.4469 (92.2475)
2022-10-30 01:07:22,330:INFO: Dataset: zara2               Batch: 13/18	Loss 166.1120 (97.7442)
2022-10-30 01:07:23,385:INFO: Dataset: zara2               Batch: 14/18	Loss 84.2155 (96.7078)
2022-10-30 01:07:24,437:INFO: Dataset: zara2               Batch: 15/18	Loss 99.6291 (96.9173)
2022-10-30 01:07:25,496:INFO: Dataset: zara2               Batch: 16/18	Loss 92.0191 (96.6142)
2022-10-30 01:07:26,547:INFO: Dataset: zara2               Batch: 17/18	Loss 89.6972 (96.1838)
2022-10-30 01:07:27,500:INFO: Dataset: zara2               Batch: 18/18	Loss 78.1570 (95.2570)
2022-10-30 01:07:27,569:INFO: - Computing ADE (validation o)
2022-10-30 01:07:27,938:INFO: 		 ADE on eth                       dataset:	 4.350881576538086
2022-10-30 01:07:27,938:INFO: Average validation o:	ADE  4.3509	FDE  7.6551
2022-10-30 01:07:27,939:INFO: - Computing ADE (validation)
2022-10-30 01:07:28,292:INFO: 		 ADE on hotel                     dataset:	 2.0015294551849365
2022-10-30 01:07:28,731:INFO: 		 ADE on univ                      dataset:	 2.091395378112793
2022-10-30 01:07:29,079:INFO: 		 ADE on zara1                     dataset:	 2.218585729598999
2022-10-30 01:07:29,625:INFO: 		 ADE on zara2                     dataset:	 2.280837059020996
2022-10-30 01:07:29,625:INFO: Average validation:	ADE  2.1634	FDE  3.9875
2022-10-30 01:07:29,626:INFO: - Computing ADE (training)
2022-10-30 01:07:30,109:INFO: 		 ADE on hotel                     dataset:	 2.1079657077789307
2022-10-30 01:07:31,188:INFO: 		 ADE on univ                      dataset:	 2.0215277671813965
2022-10-30 01:07:31,882:INFO: 		 ADE on zara1                     dataset:	 3.0677430629730225
2022-10-30 01:07:33,084:INFO: 		 ADE on zara2                     dataset:	 2.7009875774383545
2022-10-30 01:07:33,084:INFO: Average training:	ADE  2.2283	FDE  4.1095
2022-10-30 01:07:33,095:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_479.pth.tar
2022-10-30 01:07:33,095:INFO: 
===> EPOCH: 480 (P4)
2022-10-30 01:07:33,096:INFO: - Computing loss (training)
2022-10-30 01:07:34,399:INFO: Dataset: hotel               Batch: 1/4	Loss 82.8295 (82.8295)
2022-10-30 01:07:35,467:INFO: Dataset: hotel               Batch: 2/4	Loss 85.1655 (84.0166)
2022-10-30 01:07:36,528:INFO: Dataset: hotel               Batch: 3/4	Loss 83.5999 (83.8775)
2022-10-30 01:07:37,300:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7270 (78.7606)
2022-10-30 01:07:38,777:INFO: Dataset: univ                Batch:  1/15	Loss 91.2777 (91.2777)
2022-10-30 01:07:39,912:INFO: Dataset: univ                Batch:  2/15	Loss 95.7844 (93.4971)
2022-10-30 01:07:41,055:INFO: Dataset: univ                Batch:  3/15	Loss 87.0996 (91.3188)
2022-10-30 01:07:42,278:INFO: Dataset: univ                Batch:  4/15	Loss 87.2961 (90.2719)
2022-10-30 01:07:43,408:INFO: Dataset: univ                Batch:  5/15	Loss 89.0699 (90.0257)
2022-10-30 01:07:44,539:INFO: Dataset: univ                Batch:  6/15	Loss 85.9496 (89.3525)
2022-10-30 01:07:45,653:INFO: Dataset: univ                Batch:  7/15	Loss 106.8878 (91.9416)
2022-10-30 01:07:46,782:INFO: Dataset: univ                Batch:  8/15	Loss 88.9953 (91.5470)
2022-10-30 01:07:47,893:INFO: Dataset: univ                Batch:  9/15	Loss 87.9685 (91.1553)
2022-10-30 01:07:49,001:INFO: Dataset: univ                Batch: 10/15	Loss 88.7768 (90.9323)
2022-10-30 01:07:50,135:INFO: Dataset: univ                Batch: 11/15	Loss 90.3969 (90.8826)
2022-10-30 01:07:51,274:INFO: Dataset: univ                Batch: 12/15	Loss 95.4819 (91.2566)
2022-10-30 01:07:52,417:INFO: Dataset: univ                Batch: 13/15	Loss 88.0247 (91.0126)
2022-10-30 01:07:53,564:INFO: Dataset: univ                Batch: 14/15	Loss 108.5138 (92.3321)
2022-10-30 01:07:54,054:INFO: Dataset: univ                Batch: 15/15	Loss 15.4645 (91.1264)
2022-10-30 01:07:55,419:INFO: Dataset: zara1               Batch: 1/8	Loss 92.0382 (92.0382)
2022-10-30 01:07:56,470:INFO: Dataset: zara1               Batch: 2/8	Loss 100.1498 (95.7238)
2022-10-30 01:07:57,517:INFO: Dataset: zara1               Batch: 3/8	Loss 90.4407 (94.0196)
2022-10-30 01:07:58,568:INFO: Dataset: zara1               Batch: 4/8	Loss 94.7420 (94.1902)
2022-10-30 01:07:59,621:INFO: Dataset: zara1               Batch: 5/8	Loss 91.0397 (93.5673)
2022-10-30 01:08:00,678:INFO: Dataset: zara1               Batch: 6/8	Loss 92.0347 (93.3039)
2022-10-30 01:08:01,738:INFO: Dataset: zara1               Batch: 7/8	Loss 91.9303 (93.1120)
2022-10-30 01:08:02,699:INFO: Dataset: zara1               Batch: 8/8	Loss 78.3387 (91.5880)
2022-10-30 01:08:04,081:INFO: Dataset: zara2               Batch:  1/18	Loss 87.8656 (87.8656)
2022-10-30 01:08:05,129:INFO: Dataset: zara2               Batch:  2/18	Loss 105.2852 (96.7496)
2022-10-30 01:08:06,186:INFO: Dataset: zara2               Batch:  3/18	Loss 103.6619 (99.0361)
2022-10-30 01:08:07,246:INFO: Dataset: zara2               Batch:  4/18	Loss 85.7982 (95.7171)
2022-10-30 01:08:08,302:INFO: Dataset: zara2               Batch:  5/18	Loss 89.4617 (94.5447)
2022-10-30 01:08:09,364:INFO: Dataset: zara2               Batch:  6/18	Loss 87.8419 (93.5006)
2022-10-30 01:08:10,422:INFO: Dataset: zara2               Batch:  7/18	Loss 97.7241 (94.1279)
2022-10-30 01:08:11,487:INFO: Dataset: zara2               Batch:  8/18	Loss 109.6028 (96.0303)
2022-10-30 01:08:12,544:INFO: Dataset: zara2               Batch:  9/18	Loss 99.4221 (96.3798)
2022-10-30 01:08:13,604:INFO: Dataset: zara2               Batch: 10/18	Loss 91.3802 (95.9033)
2022-10-30 01:08:14,664:INFO: Dataset: zara2               Batch: 11/18	Loss 87.8817 (95.1364)
2022-10-30 01:08:15,718:INFO: Dataset: zara2               Batch: 12/18	Loss 100.7393 (95.6178)
2022-10-30 01:08:16,778:INFO: Dataset: zara2               Batch: 13/18	Loss 90.0798 (95.2122)
2022-10-30 01:08:17,832:INFO: Dataset: zara2               Batch: 14/18	Loss 90.1939 (94.8519)
2022-10-30 01:08:18,892:INFO: Dataset: zara2               Batch: 15/18	Loss 93.3446 (94.7383)
2022-10-30 01:08:19,948:INFO: Dataset: zara2               Batch: 16/18	Loss 99.2503 (95.0075)
2022-10-30 01:08:21,001:INFO: Dataset: zara2               Batch: 17/18	Loss 93.1765 (94.9047)
2022-10-30 01:08:21,955:INFO: Dataset: zara2               Batch: 18/18	Loss 82.6041 (94.3048)
2022-10-30 01:08:22,019:INFO: - Computing ADE (validation o)
2022-10-30 01:08:22,373:INFO: 		 ADE on eth                       dataset:	 4.307870388031006
2022-10-30 01:08:22,373:INFO: Average validation o:	ADE  4.3079	FDE  7.5865
2022-10-30 01:08:22,374:INFO: - Computing ADE (validation)
2022-10-30 01:08:22,705:INFO: 		 ADE on hotel                     dataset:	 1.9598602056503296
2022-10-30 01:08:23,155:INFO: 		 ADE on univ                      dataset:	 2.0987131595611572
2022-10-30 01:08:23,509:INFO: 		 ADE on zara1                     dataset:	 2.2399091720581055
2022-10-30 01:08:24,084:INFO: 		 ADE on zara2                     dataset:	 2.2685604095458984
2022-10-30 01:08:24,084:INFO: Average validation:	ADE  2.1616	FDE  3.9785
2022-10-30 01:08:24,084:INFO: - Computing ADE (training)
2022-10-30 01:08:24,567:INFO: 		 ADE on hotel                     dataset:	 2.119729995727539
2022-10-30 01:08:25,623:INFO: 		 ADE on univ                      dataset:	 2.014658212661743
2022-10-30 01:08:26,325:INFO: 		 ADE on zara1                     dataset:	 3.057145833969116
2022-10-30 01:08:27,546:INFO: 		 ADE on zara2                     dataset:	 2.6989095211029053
2022-10-30 01:08:27,546:INFO: Average training:	ADE  2.2226	FDE  4.0963
2022-10-30 01:08:27,557:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_480.pth.tar
2022-10-30 01:08:27,557:INFO: 
===> EPOCH: 481 (P4)
2022-10-30 01:08:27,558:INFO: - Computing loss (training)
2022-10-30 01:08:28,838:INFO: Dataset: hotel               Batch: 1/4	Loss 83.7417 (83.7417)
2022-10-30 01:08:29,905:INFO: Dataset: hotel               Batch: 2/4	Loss 81.8741 (82.7796)
2022-10-30 01:08:30,954:INFO: Dataset: hotel               Batch: 3/4	Loss 84.5340 (83.3635)
2022-10-30 01:08:31,732:INFO: Dataset: hotel               Batch: 4/4	Loss 51.7541 (78.5679)
2022-10-30 01:08:33,183:INFO: Dataset: univ                Batch:  1/15	Loss 84.5250 (84.5250)
2022-10-30 01:08:34,318:INFO: Dataset: univ                Batch:  2/15	Loss 85.4849 (85.0250)
2022-10-30 01:08:35,448:INFO: Dataset: univ                Batch:  3/15	Loss 87.4627 (85.8391)
2022-10-30 01:08:36,561:INFO: Dataset: univ                Batch:  4/15	Loss 86.8543 (86.0758)
2022-10-30 01:08:37,688:INFO: Dataset: univ                Batch:  5/15	Loss 85.9087 (86.0419)
2022-10-30 01:08:38,815:INFO: Dataset: univ                Batch:  6/15	Loss 86.5744 (86.1310)
2022-10-30 01:08:39,947:INFO: Dataset: univ                Batch:  7/15	Loss 87.0451 (86.2591)
2022-10-30 01:08:41,062:INFO: Dataset: univ                Batch:  8/15	Loss 88.4621 (86.4909)
2022-10-30 01:08:42,177:INFO: Dataset: univ                Batch:  9/15	Loss 88.5711 (86.6995)
2022-10-30 01:08:43,299:INFO: Dataset: univ                Batch: 10/15	Loss 84.9259 (86.5242)
2022-10-30 01:08:44,412:INFO: Dataset: univ                Batch: 11/15	Loss 86.2348 (86.5019)
2022-10-30 01:08:45,544:INFO: Dataset: univ                Batch: 12/15	Loss 84.0366 (86.2966)
2022-10-30 01:08:46,664:INFO: Dataset: univ                Batch: 13/15	Loss 96.7230 (87.0357)
2022-10-30 01:08:47,784:INFO: Dataset: univ                Batch: 14/15	Loss 91.9497 (87.3773)
2022-10-30 01:08:48,264:INFO: Dataset: univ                Batch: 15/15	Loss 16.5869 (86.5185)
2022-10-30 01:08:49,646:INFO: Dataset: zara1               Batch: 1/8	Loss 83.0921 (83.0921)
2022-10-30 01:08:50,713:INFO: Dataset: zara1               Batch: 2/8	Loss 93.9766 (88.5120)
2022-10-30 01:08:51,771:INFO: Dataset: zara1               Batch: 3/8	Loss 97.3802 (91.4681)
2022-10-30 01:08:52,836:INFO: Dataset: zara1               Batch: 4/8	Loss 91.0174 (91.3582)
2022-10-30 01:08:53,900:INFO: Dataset: zara1               Batch: 5/8	Loss 92.0755 (91.4915)
2022-10-30 01:08:54,964:INFO: Dataset: zara1               Batch: 6/8	Loss 94.1728 (91.9450)
2022-10-30 01:08:56,018:INFO: Dataset: zara1               Batch: 7/8	Loss 91.8659 (91.9339)
2022-10-30 01:08:56,985:INFO: Dataset: zara1               Batch: 8/8	Loss 78.1718 (90.2245)
2022-10-30 01:08:58,390:INFO: Dataset: zara2               Batch:  1/18	Loss 91.4457 (91.4457)
2022-10-30 01:08:59,476:INFO: Dataset: zara2               Batch:  2/18	Loss 86.9608 (89.0642)
2022-10-30 01:09:00,556:INFO: Dataset: zara2               Batch:  3/18	Loss 110.1279 (95.6548)
2022-10-30 01:09:01,636:INFO: Dataset: zara2               Batch:  4/18	Loss 94.5956 (95.3640)
2022-10-30 01:09:02,720:INFO: Dataset: zara2               Batch:  5/18	Loss 92.0861 (94.7029)
2022-10-30 01:09:03,805:INFO: Dataset: zara2               Batch:  6/18	Loss 93.5232 (94.5144)
2022-10-30 01:09:04,866:INFO: Dataset: zara2               Batch:  7/18	Loss 90.1108 (93.8682)
2022-10-30 01:09:05,927:INFO: Dataset: zara2               Batch:  8/18	Loss 100.0343 (94.7014)
2022-10-30 01:09:06,993:INFO: Dataset: zara2               Batch:  9/18	Loss 93.3727 (94.5545)
2022-10-30 01:09:08,058:INFO: Dataset: zara2               Batch: 10/18	Loss 93.8256 (94.4761)
2022-10-30 01:09:09,138:INFO: Dataset: zara2               Batch: 11/18	Loss 102.1838 (95.2450)
2022-10-30 01:09:10,231:INFO: Dataset: zara2               Batch: 12/18	Loss 86.2273 (94.5338)
2022-10-30 01:09:11,312:INFO: Dataset: zara2               Batch: 13/18	Loss 89.1836 (94.1191)
2022-10-30 01:09:12,386:INFO: Dataset: zara2               Batch: 14/18	Loss 89.6598 (93.7870)
2022-10-30 01:09:13,473:INFO: Dataset: zara2               Batch: 15/18	Loss 89.4142 (93.4776)
2022-10-30 01:09:14,534:INFO: Dataset: zara2               Batch: 16/18	Loss 89.0559 (93.2149)
2022-10-30 01:09:15,595:INFO: Dataset: zara2               Batch: 17/18	Loss 88.9217 (92.9741)
2022-10-30 01:09:16,547:INFO: Dataset: zara2               Batch: 18/18	Loss 77.6961 (92.2897)
2022-10-30 01:09:16,613:INFO: - Computing ADE (validation o)
2022-10-30 01:09:16,963:INFO: 		 ADE on eth                       dataset:	 4.32004976272583
2022-10-30 01:09:16,963:INFO: Average validation o:	ADE  4.3200	FDE  7.5917
2022-10-30 01:09:16,964:INFO: - Computing ADE (validation)
2022-10-30 01:09:17,317:INFO: 		 ADE on hotel                     dataset:	 1.9224720001220703
2022-10-30 01:09:17,776:INFO: 		 ADE on univ                      dataset:	 2.0866000652313232
2022-10-30 01:09:18,126:INFO: 		 ADE on zara1                     dataset:	 2.2407712936401367
2022-10-30 01:09:18,673:INFO: 		 ADE on zara2                     dataset:	 2.2857017517089844
2022-10-30 01:09:18,673:INFO: Average validation:	ADE  2.1596	FDE  3.9691
2022-10-30 01:09:18,674:INFO: - Computing ADE (training)
2022-10-30 01:09:19,171:INFO: 		 ADE on hotel                     dataset:	 2.1158270835876465
2022-10-30 01:09:20,255:INFO: 		 ADE on univ                      dataset:	 2.00701904296875
2022-10-30 01:09:20,963:INFO: 		 ADE on zara1                     dataset:	 3.0617401599884033
2022-10-30 01:09:22,171:INFO: 		 ADE on zara2                     dataset:	 2.6777937412261963
2022-10-30 01:09:22,172:INFO: Average training:	ADE  2.2131	FDE  4.0789
2022-10-30 01:09:22,183:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_481.pth.tar
2022-10-30 01:09:22,183:INFO: 
===> EPOCH: 482 (P4)
2022-10-30 01:09:22,183:INFO: - Computing loss (training)
2022-10-30 01:09:23,487:INFO: Dataset: hotel               Batch: 1/4	Loss 84.1018 (84.1018)
2022-10-30 01:09:24,559:INFO: Dataset: hotel               Batch: 2/4	Loss 83.1946 (83.6420)
2022-10-30 01:09:25,615:INFO: Dataset: hotel               Batch: 3/4	Loss 83.2861 (83.5252)
2022-10-30 01:09:26,393:INFO: Dataset: hotel               Batch: 4/4	Loss 51.9611 (78.8614)
2022-10-30 01:09:27,859:INFO: Dataset: univ                Batch:  1/15	Loss 92.9905 (92.9905)
2022-10-30 01:09:28,990:INFO: Dataset: univ                Batch:  2/15	Loss 92.5778 (92.7720)
2022-10-30 01:09:30,114:INFO: Dataset: univ                Batch:  3/15	Loss 321.4243 (168.5410)
2022-10-30 01:09:31,255:INFO: Dataset: univ                Batch:  4/15	Loss 86.8858 (146.0827)
2022-10-30 01:09:32,390:INFO: Dataset: univ                Batch:  5/15	Loss 129.9861 (142.8902)
2022-10-30 01:09:33,525:INFO: Dataset: univ                Batch:  6/15	Loss 102.8802 (136.3141)
2022-10-30 01:09:34,661:INFO: Dataset: univ                Batch:  7/15	Loss 93.9935 (130.0499)
2022-10-30 01:09:35,784:INFO: Dataset: univ                Batch:  8/15	Loss 87.0989 (124.9454)
2022-10-30 01:09:36,914:INFO: Dataset: univ                Batch:  9/15	Loss 87.0209 (120.6438)
2022-10-30 01:09:38,049:INFO: Dataset: univ                Batch: 10/15	Loss 86.3380 (116.9233)
2022-10-30 01:09:39,184:INFO: Dataset: univ                Batch: 11/15	Loss 85.6716 (114.0532)
2022-10-30 01:09:40,320:INFO: Dataset: univ                Batch: 12/15	Loss 138.2561 (116.2244)
2022-10-30 01:09:41,443:INFO: Dataset: univ                Batch: 13/15	Loss 91.3440 (114.3585)
2022-10-30 01:09:42,656:INFO: Dataset: univ                Batch: 14/15	Loss 84.3608 (112.0706)
2022-10-30 01:09:43,136:INFO: Dataset: univ                Batch: 15/15	Loss 17.0438 (111.0393)
2022-10-30 01:09:44,472:INFO: Dataset: zara1               Batch: 1/8	Loss 91.3214 (91.3214)
2022-10-30 01:09:45,519:INFO: Dataset: zara1               Batch: 2/8	Loss 138.6582 (114.7883)
2022-10-30 01:09:46,577:INFO: Dataset: zara1               Batch: 3/8	Loss 90.1526 (107.0560)
2022-10-30 01:09:47,616:INFO: Dataset: zara1               Batch: 4/8	Loss 93.4085 (103.6703)
2022-10-30 01:09:48,655:INFO: Dataset: zara1               Batch: 5/8	Loss 92.1239 (101.1452)
2022-10-30 01:09:49,697:INFO: Dataset: zara1               Batch: 6/8	Loss 100.7116 (101.0676)
2022-10-30 01:09:50,754:INFO: Dataset: zara1               Batch: 7/8	Loss 90.8518 (99.4406)
2022-10-30 01:09:51,710:INFO: Dataset: zara1               Batch: 8/8	Loss 78.0550 (97.0657)
2022-10-30 01:09:53,075:INFO: Dataset: zara2               Batch:  1/18	Loss 92.3336 (92.3336)
2022-10-30 01:09:54,126:INFO: Dataset: zara2               Batch:  2/18	Loss 100.8495 (96.6475)
2022-10-30 01:09:55,188:INFO: Dataset: zara2               Batch:  3/18	Loss 90.1563 (94.4816)
2022-10-30 01:09:56,245:INFO: Dataset: zara2               Batch:  4/18	Loss 113.8530 (99.1348)
2022-10-30 01:09:57,299:INFO: Dataset: zara2               Batch:  5/18	Loss 91.4095 (97.5724)
2022-10-30 01:09:58,350:INFO: Dataset: zara2               Batch:  6/18	Loss 87.4323 (95.7928)
2022-10-30 01:09:59,392:INFO: Dataset: zara2               Batch:  7/18	Loss 87.5164 (94.5776)
2022-10-30 01:10:00,438:INFO: Dataset: zara2               Batch:  8/18	Loss 93.3296 (94.4362)
2022-10-30 01:10:01,486:INFO: Dataset: zara2               Batch:  9/18	Loss 94.1143 (94.4024)
2022-10-30 01:10:02,535:INFO: Dataset: zara2               Batch: 10/18	Loss 96.9276 (94.6686)
2022-10-30 01:10:03,590:INFO: Dataset: zara2               Batch: 11/18	Loss 90.9700 (94.3008)
2022-10-30 01:10:04,637:INFO: Dataset: zara2               Batch: 12/18	Loss 89.5862 (93.9321)
2022-10-30 01:10:05,692:INFO: Dataset: zara2               Batch: 13/18	Loss 89.4451 (93.5840)
2022-10-30 01:10:06,749:INFO: Dataset: zara2               Batch: 14/18	Loss 86.8812 (93.1094)
2022-10-30 01:10:07,800:INFO: Dataset: zara2               Batch: 15/18	Loss 97.7024 (93.4206)
2022-10-30 01:10:08,866:INFO: Dataset: zara2               Batch: 16/18	Loss 89.8962 (93.2103)
2022-10-30 01:10:09,928:INFO: Dataset: zara2               Batch: 17/18	Loss 103.1644 (93.7710)
2022-10-30 01:10:10,880:INFO: Dataset: zara2               Batch: 18/18	Loss 76.5909 (92.9786)
2022-10-30 01:10:10,947:INFO: - Computing ADE (validation o)
2022-10-30 01:10:11,310:INFO: 		 ADE on eth                       dataset:	 4.35802698135376
2022-10-30 01:10:11,311:INFO: Average validation o:	ADE  4.3580	FDE  7.6368
2022-10-30 01:10:11,312:INFO: - Computing ADE (validation)
2022-10-30 01:10:11,671:INFO: 		 ADE on hotel                     dataset:	 1.9121994972229004
2022-10-30 01:10:12,101:INFO: 		 ADE on univ                      dataset:	 2.0750370025634766
2022-10-30 01:10:12,456:INFO: 		 ADE on zara1                     dataset:	 2.2640810012817383
2022-10-30 01:10:13,011:INFO: 		 ADE on zara2                     dataset:	 2.2831525802612305
2022-10-30 01:10:13,011:INFO: Average validation:	ADE  2.1534	FDE  3.9575
2022-10-30 01:10:13,012:INFO: - Computing ADE (training)
2022-10-30 01:10:13,491:INFO: 		 ADE on hotel                     dataset:	 2.071989059448242
2022-10-30 01:10:14,553:INFO: 		 ADE on univ                      dataset:	 2.003225088119507
2022-10-30 01:10:15,262:INFO: 		 ADE on zara1                     dataset:	 3.070591926574707
2022-10-30 01:10:16,501:INFO: 		 ADE on zara2                     dataset:	 2.680920124053955
2022-10-30 01:10:16,501:INFO: Average training:	ADE  2.2105	FDE  4.0701
2022-10-30 01:10:16,513:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_482.pth.tar
2022-10-30 01:10:16,513:INFO: 
===> EPOCH: 483 (P4)
2022-10-30 01:10:16,513:INFO: - Computing loss (training)
2022-10-30 01:10:17,800:INFO: Dataset: hotel               Batch: 1/4	Loss 84.7529 (84.7529)
2022-10-30 01:10:18,865:INFO: Dataset: hotel               Batch: 2/4	Loss 84.1479 (84.4642)
2022-10-30 01:10:19,921:INFO: Dataset: hotel               Batch: 3/4	Loss 82.7686 (83.9109)
2022-10-30 01:10:20,686:INFO: Dataset: hotel               Batch: 4/4	Loss 50.4027 (77.7662)
2022-10-30 01:10:22,127:INFO: Dataset: univ                Batch:  1/15	Loss 86.7576 (86.7576)
2022-10-30 01:10:23,256:INFO: Dataset: univ                Batch:  2/15	Loss 90.0903 (88.4959)
2022-10-30 01:10:24,389:INFO: Dataset: univ                Batch:  3/15	Loss 88.8490 (88.6153)
2022-10-30 01:10:25,528:INFO: Dataset: univ                Batch:  4/15	Loss 90.1165 (89.0082)
2022-10-30 01:10:26,652:INFO: Dataset: univ                Batch:  5/15	Loss 93.8397 (89.9438)
2022-10-30 01:10:27,790:INFO: Dataset: univ                Batch:  6/15	Loss 86.8829 (89.4270)
2022-10-30 01:10:28,916:INFO: Dataset: univ                Batch:  7/15	Loss 87.5224 (89.1899)
2022-10-30 01:10:30,047:INFO: Dataset: univ                Batch:  8/15	Loss 87.0242 (88.9061)
2022-10-30 01:10:31,169:INFO: Dataset: univ                Batch:  9/15	Loss 84.1639 (88.3903)
2022-10-30 01:10:32,289:INFO: Dataset: univ                Batch: 10/15	Loss 90.5872 (88.5914)
2022-10-30 01:10:33,426:INFO: Dataset: univ                Batch: 11/15	Loss 85.9112 (88.3567)
2022-10-30 01:10:34,567:INFO: Dataset: univ                Batch: 12/15	Loss 84.9866 (88.0487)
2022-10-30 01:10:35,692:INFO: Dataset: univ                Batch: 13/15	Loss 91.1842 (88.2792)
2022-10-30 01:10:36,829:INFO: Dataset: univ                Batch: 14/15	Loss 93.1698 (88.6417)
2022-10-30 01:10:37,312:INFO: Dataset: univ                Batch: 15/15	Loss 15.8985 (87.6695)
2022-10-30 01:10:38,692:INFO: Dataset: zara1               Batch: 1/8	Loss 91.5076 (91.5076)
2022-10-30 01:10:39,769:INFO: Dataset: zara1               Batch: 2/8	Loss 90.5704 (91.0570)
2022-10-30 01:10:40,842:INFO: Dataset: zara1               Batch: 3/8	Loss 104.3786 (94.8283)
2022-10-30 01:10:41,911:INFO: Dataset: zara1               Batch: 4/8	Loss 90.2050 (93.8045)
2022-10-30 01:10:42,982:INFO: Dataset: zara1               Batch: 5/8	Loss 91.2413 (93.2952)
2022-10-30 01:10:44,042:INFO: Dataset: zara1               Batch: 6/8	Loss 94.3968 (93.4722)
2022-10-30 01:10:45,102:INFO: Dataset: zara1               Batch: 7/8	Loss 96.8331 (93.9183)
2022-10-30 01:10:46,072:INFO: Dataset: zara1               Batch: 8/8	Loss 77.2809 (91.9918)
2022-10-30 01:10:47,448:INFO: Dataset: zara2               Batch:  1/18	Loss 97.7586 (97.7586)
2022-10-30 01:10:48,510:INFO: Dataset: zara2               Batch:  2/18	Loss 89.7070 (93.6360)
2022-10-30 01:10:49,580:INFO: Dataset: zara2               Batch:  3/18	Loss 89.5257 (92.3199)
2022-10-30 01:10:50,657:INFO: Dataset: zara2               Batch:  4/18	Loss 88.0992 (91.2190)
2022-10-30 01:10:51,728:INFO: Dataset: zara2               Batch:  5/18	Loss 88.0926 (90.6353)
2022-10-30 01:10:52,809:INFO: Dataset: zara2               Batch:  6/18	Loss 89.5089 (90.4485)
2022-10-30 01:10:53,887:INFO: Dataset: zara2               Batch:  7/18	Loss 92.2451 (90.6989)
2022-10-30 01:10:54,968:INFO: Dataset: zara2               Batch:  8/18	Loss 110.9240 (93.0762)
2022-10-30 01:10:56,061:INFO: Dataset: zara2               Batch:  9/18	Loss 86.8393 (92.3677)
2022-10-30 01:10:57,153:INFO: Dataset: zara2               Batch: 10/18	Loss 89.0113 (92.0306)
2022-10-30 01:10:58,253:INFO: Dataset: zara2               Batch: 11/18	Loss 89.3984 (91.7973)
2022-10-30 01:10:59,344:INFO: Dataset: zara2               Batch: 12/18	Loss 150.6594 (96.4162)
2022-10-30 01:11:00,413:INFO: Dataset: zara2               Batch: 13/18	Loss 88.7445 (95.8856)
2022-10-30 01:11:01,490:INFO: Dataset: zara2               Batch: 14/18	Loss 99.3125 (96.1291)
2022-10-30 01:11:02,565:INFO: Dataset: zara2               Batch: 15/18	Loss 91.2181 (95.7700)
2022-10-30 01:11:03,639:INFO: Dataset: zara2               Batch: 16/18	Loss 88.4672 (95.3366)
2022-10-30 01:11:04,721:INFO: Dataset: zara2               Batch: 17/18	Loss 95.8005 (95.3651)
2022-10-30 01:11:05,696:INFO: Dataset: zara2               Batch: 18/18	Loss 107.1002 (95.9199)
2022-10-30 01:11:05,761:INFO: - Computing ADE (validation o)
2022-10-30 01:11:06,119:INFO: 		 ADE on eth                       dataset:	 4.3311052322387695
2022-10-30 01:11:06,119:INFO: Average validation o:	ADE  4.3311	FDE  7.5984
2022-10-30 01:11:06,120:INFO: - Computing ADE (validation)
2022-10-30 01:11:06,467:INFO: 		 ADE on hotel                     dataset:	 1.9192017316818237
2022-10-30 01:11:06,938:INFO: 		 ADE on univ                      dataset:	 2.0760905742645264
2022-10-30 01:11:07,300:INFO: 		 ADE on zara1                     dataset:	 2.2521615028381348
2022-10-30 01:11:07,836:INFO: 		 ADE on zara2                     dataset:	 2.2647597789764404
2022-10-30 01:11:07,836:INFO: Average validation:	ADE  2.1469	FDE  3.9402
2022-10-30 01:11:07,836:INFO: - Computing ADE (training)
2022-10-30 01:11:08,330:INFO: 		 ADE on hotel                     dataset:	 2.0782485008239746
2022-10-30 01:11:09,423:INFO: 		 ADE on univ                      dataset:	 1.9974844455718994
2022-10-30 01:11:10,128:INFO: 		 ADE on zara1                     dataset:	 3.046494483947754
2022-10-30 01:11:11,325:INFO: 		 ADE on zara2                     dataset:	 2.677288055419922
2022-10-30 01:11:11,325:INFO: Average training:	ADE  2.2044	FDE  4.0555
2022-10-30 01:11:11,336:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_483.pth.tar
2022-10-30 01:11:11,336:INFO: 
===> EPOCH: 484 (P4)
2022-10-30 01:11:11,337:INFO: - Computing loss (training)
2022-10-30 01:11:12,613:INFO: Dataset: hotel               Batch: 1/4	Loss 84.1215 (84.1215)
2022-10-30 01:11:13,668:INFO: Dataset: hotel               Batch: 2/4	Loss 82.5992 (83.3478)
2022-10-30 01:11:14,708:INFO: Dataset: hotel               Batch: 3/4	Loss 84.2892 (83.6601)
2022-10-30 01:11:15,479:INFO: Dataset: hotel               Batch: 4/4	Loss 50.5580 (78.4634)
2022-10-30 01:11:16,964:INFO: Dataset: univ                Batch:  1/15	Loss 93.2706 (93.2706)
2022-10-30 01:11:18,107:INFO: Dataset: univ                Batch:  2/15	Loss 89.7782 (91.6056)
2022-10-30 01:11:19,248:INFO: Dataset: univ                Batch:  3/15	Loss 93.4223 (92.1896)
2022-10-30 01:11:20,388:INFO: Dataset: univ                Batch:  4/15	Loss 87.1956 (90.9326)
2022-10-30 01:11:21,522:INFO: Dataset: univ                Batch:  5/15	Loss 85.2663 (89.7974)
2022-10-30 01:11:22,650:INFO: Dataset: univ                Batch:  6/15	Loss 88.1023 (89.5414)
2022-10-30 01:11:23,774:INFO: Dataset: univ                Batch:  7/15	Loss 87.6385 (89.3013)
2022-10-30 01:11:24,911:INFO: Dataset: univ                Batch:  8/15	Loss 87.9813 (89.1355)
2022-10-30 01:11:26,051:INFO: Dataset: univ                Batch:  9/15	Loss 85.4259 (88.6936)
2022-10-30 01:11:27,184:INFO: Dataset: univ                Batch: 10/15	Loss 88.3767 (88.6608)
2022-10-30 01:11:28,322:INFO: Dataset: univ                Batch: 11/15	Loss 87.6900 (88.5706)
2022-10-30 01:11:29,457:INFO: Dataset: univ                Batch: 12/15	Loss 89.8120 (88.6726)
2022-10-30 01:11:30,605:INFO: Dataset: univ                Batch: 13/15	Loss 87.8804 (88.6030)
2022-10-30 01:11:31,823:INFO: Dataset: univ                Batch: 14/15	Loss 88.3070 (88.5817)
2022-10-30 01:11:32,315:INFO: Dataset: univ                Batch: 15/15	Loss 15.8263 (87.7128)
2022-10-30 01:11:33,678:INFO: Dataset: zara1               Batch: 1/8	Loss 91.2907 (91.2907)
2022-10-30 01:11:34,742:INFO: Dataset: zara1               Batch: 2/8	Loss 93.3588 (92.2469)
2022-10-30 01:11:35,812:INFO: Dataset: zara1               Batch: 3/8	Loss 93.6968 (92.7295)
2022-10-30 01:11:36,867:INFO: Dataset: zara1               Batch: 4/8	Loss 84.2072 (90.5264)
2022-10-30 01:11:37,924:INFO: Dataset: zara1               Batch: 5/8	Loss 102.3738 (92.8959)
2022-10-30 01:11:38,990:INFO: Dataset: zara1               Batch: 6/8	Loss 88.4153 (92.0918)
2022-10-30 01:11:40,056:INFO: Dataset: zara1               Batch: 7/8	Loss 91.2634 (91.9699)
2022-10-30 01:11:41,071:INFO: Dataset: zara1               Batch: 8/8	Loss 78.8124 (90.4395)
2022-10-30 01:11:42,450:INFO: Dataset: zara2               Batch:  1/18	Loss 90.8123 (90.8123)
2022-10-30 01:11:43,531:INFO: Dataset: zara2               Batch:  2/18	Loss 88.4213 (89.7302)
2022-10-30 01:11:44,602:INFO: Dataset: zara2               Batch:  3/18	Loss 165.0126 (113.0807)
2022-10-30 01:11:45,680:INFO: Dataset: zara2               Batch:  4/18	Loss 325.9692 (162.8532)
2022-10-30 01:11:46,765:INFO: Dataset: zara2               Batch:  5/18	Loss 90.2462 (147.5885)
2022-10-30 01:11:47,846:INFO: Dataset: zara2               Batch:  6/18	Loss 91.1367 (137.7350)
2022-10-30 01:11:48,903:INFO: Dataset: zara2               Batch:  7/18	Loss 85.1297 (130.2070)
2022-10-30 01:11:49,954:INFO: Dataset: zara2               Batch:  8/18	Loss 87.2917 (124.3869)
2022-10-30 01:11:51,002:INFO: Dataset: zara2               Batch:  9/18	Loss 92.8817 (120.7158)
2022-10-30 01:11:52,058:INFO: Dataset: zara2               Batch: 10/18	Loss 87.2616 (117.1891)
2022-10-30 01:11:53,125:INFO: Dataset: zara2               Batch: 11/18	Loss 97.0899 (115.2966)
2022-10-30 01:11:54,193:INFO: Dataset: zara2               Batch: 12/18	Loss 89.8166 (113.1039)
2022-10-30 01:11:55,244:INFO: Dataset: zara2               Batch: 13/18	Loss 86.7475 (111.2576)
2022-10-30 01:11:56,302:INFO: Dataset: zara2               Batch: 14/18	Loss 91.8815 (109.7843)
2022-10-30 01:11:57,356:INFO: Dataset: zara2               Batch: 15/18	Loss 93.4169 (108.7531)
2022-10-30 01:11:58,404:INFO: Dataset: zara2               Batch: 16/18	Loss 101.7077 (108.3465)
2022-10-30 01:11:59,454:INFO: Dataset: zara2               Batch: 17/18	Loss 127.3474 (109.5635)
2022-10-30 01:12:00,409:INFO: Dataset: zara2               Batch: 18/18	Loss 74.9445 (108.0068)
2022-10-30 01:12:00,474:INFO: - Computing ADE (validation o)
2022-10-30 01:12:00,839:INFO: 		 ADE on eth                       dataset:	 4.296195030212402
2022-10-30 01:12:00,840:INFO: Average validation o:	ADE  4.2962	FDE  7.5342
2022-10-30 01:12:00,840:INFO: - Computing ADE (validation)
2022-10-30 01:12:01,185:INFO: 		 ADE on hotel                     dataset:	 1.9224271774291992
2022-10-30 01:12:01,632:INFO: 		 ADE on univ                      dataset:	 2.0666213035583496
2022-10-30 01:12:01,991:INFO: 		 ADE on zara1                     dataset:	 2.2294492721557617
2022-10-30 01:12:02,542:INFO: 		 ADE on zara2                     dataset:	 2.256152629852295
2022-10-30 01:12:02,543:INFO: Average validation:	ADE  2.1377	FDE  3.9267
2022-10-30 01:12:02,543:INFO: - Computing ADE (training)
2022-10-30 01:12:03,021:INFO: 		 ADE on hotel                     dataset:	 2.0917840003967285
2022-10-30 01:12:04,103:INFO: 		 ADE on univ                      dataset:	 1.9892452955245972
2022-10-30 01:12:04,798:INFO: 		 ADE on zara1                     dataset:	 3.0450491905212402
2022-10-30 01:12:05,964:INFO: 		 ADE on zara2                     dataset:	 2.6715285778045654
2022-10-30 01:12:05,964:INFO: Average training:	ADE  2.1976	FDE  4.0399
2022-10-30 01:12:05,975:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_484.pth.tar
2022-10-30 01:12:05,975:INFO: 
===> EPOCH: 485 (P4)
2022-10-30 01:12:05,975:INFO: - Computing loss (training)
2022-10-30 01:12:07,308:INFO: Dataset: hotel               Batch: 1/4	Loss 82.7655 (82.7655)
2022-10-30 01:12:08,388:INFO: Dataset: hotel               Batch: 2/4	Loss 83.6230 (83.1827)
2022-10-30 01:12:09,462:INFO: Dataset: hotel               Batch: 3/4	Loss 83.9139 (83.4377)
2022-10-30 01:12:10,254:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7225 (77.9564)
2022-10-30 01:12:11,707:INFO: Dataset: univ                Batch:  1/15	Loss 88.4105 (88.4105)
2022-10-30 01:12:12,850:INFO: Dataset: univ                Batch:  2/15	Loss 87.6651 (88.0066)
2022-10-30 01:12:13,989:INFO: Dataset: univ                Batch:  3/15	Loss 84.4152 (86.7310)
2022-10-30 01:12:15,149:INFO: Dataset: univ                Batch:  4/15	Loss 93.8183 (88.8266)
2022-10-30 01:12:16,299:INFO: Dataset: univ                Batch:  5/15	Loss 89.4450 (88.9510)
2022-10-30 01:12:17,432:INFO: Dataset: univ                Batch:  6/15	Loss 85.7680 (88.4482)
2022-10-30 01:12:18,567:INFO: Dataset: univ                Batch:  7/15	Loss 85.2503 (87.9897)
2022-10-30 01:12:19,705:INFO: Dataset: univ                Batch:  8/15	Loss 93.3296 (88.5725)
2022-10-30 01:12:20,835:INFO: Dataset: univ                Batch:  9/15	Loss 91.7580 (88.9173)
2022-10-30 01:12:21,967:INFO: Dataset: univ                Batch: 10/15	Loss 91.5784 (89.1874)
2022-10-30 01:12:23,098:INFO: Dataset: univ                Batch: 11/15	Loss 97.2768 (89.8526)
2022-10-30 01:12:24,243:INFO: Dataset: univ                Batch: 12/15	Loss 90.8633 (89.9413)
2022-10-30 01:12:25,393:INFO: Dataset: univ                Batch: 13/15	Loss 85.8799 (89.6255)
2022-10-30 01:12:26,531:INFO: Dataset: univ                Batch: 14/15	Loss 91.9774 (89.8045)
2022-10-30 01:12:27,022:INFO: Dataset: univ                Batch: 15/15	Loss 16.4986 (89.0472)
2022-10-30 01:12:28,364:INFO: Dataset: zara1               Batch: 1/8	Loss 93.9202 (93.9202)
2022-10-30 01:12:29,413:INFO: Dataset: zara1               Batch: 2/8	Loss 90.0420 (92.0000)
2022-10-30 01:12:30,463:INFO: Dataset: zara1               Batch: 3/8	Loss 92.3543 (92.1342)
2022-10-30 01:12:31,519:INFO: Dataset: zara1               Batch: 4/8	Loss 88.1682 (90.9469)
2022-10-30 01:12:32,575:INFO: Dataset: zara1               Batch: 5/8	Loss 90.5822 (90.8631)
2022-10-30 01:12:33,621:INFO: Dataset: zara1               Batch: 6/8	Loss 89.6182 (90.6690)
2022-10-30 01:12:34,681:INFO: Dataset: zara1               Batch: 7/8	Loss 92.2473 (90.9114)
2022-10-30 01:12:35,632:INFO: Dataset: zara1               Batch: 8/8	Loss 78.9165 (89.7246)
2022-10-30 01:12:37,019:INFO: Dataset: zara2               Batch:  1/18	Loss 88.4570 (88.4570)
2022-10-30 01:12:38,097:INFO: Dataset: zara2               Batch:  2/18	Loss 90.3684 (89.4255)
2022-10-30 01:12:39,181:INFO: Dataset: zara2               Batch:  3/18	Loss 89.8039 (89.5542)
2022-10-30 01:12:40,260:INFO: Dataset: zara2               Batch:  4/18	Loss 87.7777 (89.1310)
2022-10-30 01:12:41,339:INFO: Dataset: zara2               Batch:  5/18	Loss 90.3491 (89.3851)
2022-10-30 01:12:42,422:INFO: Dataset: zara2               Batch:  6/18	Loss 85.0422 (88.6215)
2022-10-30 01:12:43,495:INFO: Dataset: zara2               Batch:  7/18	Loss 85.5409 (88.1651)
2022-10-30 01:12:44,580:INFO: Dataset: zara2               Batch:  8/18	Loss 93.3893 (88.8210)
2022-10-30 01:12:45,646:INFO: Dataset: zara2               Batch:  9/18	Loss 139.6970 (94.5975)
2022-10-30 01:12:46,707:INFO: Dataset: zara2               Batch: 10/18	Loss 85.2824 (93.6622)
2022-10-30 01:12:47,768:INFO: Dataset: zara2               Batch: 11/18	Loss 403.0518 (121.3736)
2022-10-30 01:12:48,826:INFO: Dataset: zara2               Batch: 12/18	Loss 88.1553 (118.8582)
2022-10-30 01:12:49,886:INFO: Dataset: zara2               Batch: 13/18	Loss 82.5864 (116.3507)
2022-10-30 01:12:50,948:INFO: Dataset: zara2               Batch: 14/18	Loss 104.3815 (115.4640)
2022-10-30 01:12:52,008:INFO: Dataset: zara2               Batch: 15/18	Loss 89.0455 (113.7675)
2022-10-30 01:12:53,089:INFO: Dataset: zara2               Batch: 16/18	Loss 115.7298 (113.8907)
2022-10-30 01:12:54,169:INFO: Dataset: zara2               Batch: 17/18	Loss 88.8237 (112.4430)
2022-10-30 01:12:55,123:INFO: Dataset: zara2               Batch: 18/18	Loss 76.5249 (110.7626)
2022-10-30 01:12:55,190:INFO: - Computing ADE (validation o)
2022-10-30 01:12:55,558:INFO: 		 ADE on eth                       dataset:	 4.3354291915893555
2022-10-30 01:12:55,559:INFO: Average validation o:	ADE  4.3354	FDE  7.5870
2022-10-30 01:12:55,560:INFO: - Computing ADE (validation)
2022-10-30 01:12:55,890:INFO: 		 ADE on hotel                     dataset:	 1.878599762916565
2022-10-30 01:12:56,345:INFO: 		 ADE on univ                      dataset:	 2.0644681453704834
2022-10-30 01:12:56,696:INFO: 		 ADE on zara1                     dataset:	 2.2472081184387207
2022-10-30 01:12:57,243:INFO: 		 ADE on zara2                     dataset:	 2.246379852294922
2022-10-30 01:12:57,243:INFO: Average validation:	ADE  2.1316	FDE  3.9106
2022-10-30 01:12:57,244:INFO: - Computing ADE (training)
2022-10-30 01:12:57,721:INFO: 		 ADE on hotel                     dataset:	 2.0567331314086914
2022-10-30 01:12:58,775:INFO: 		 ADE on univ                      dataset:	 1.9881386756896973
2022-10-30 01:12:59,473:INFO: 		 ADE on zara1                     dataset:	 3.03356671333313
2022-10-30 01:13:00,667:INFO: 		 ADE on zara2                     dataset:	 2.676403760910034
2022-10-30 01:13:00,667:INFO: Average training:	ADE  2.1962	FDE  4.0336
2022-10-30 01:13:00,678:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_485.pth.tar
2022-10-30 01:13:00,678:INFO: 
===> EPOCH: 486 (P4)
2022-10-30 01:13:00,679:INFO: - Computing loss (training)
2022-10-30 01:13:01,957:INFO: Dataset: hotel               Batch: 1/4	Loss 84.4058 (84.4058)
2022-10-30 01:13:03,011:INFO: Dataset: hotel               Batch: 2/4	Loss 82.4798 (83.4542)
2022-10-30 01:13:04,068:INFO: Dataset: hotel               Batch: 3/4	Loss 83.5957 (83.5020)
2022-10-30 01:13:04,840:INFO: Dataset: hotel               Batch: 4/4	Loss 50.4760 (78.4479)
2022-10-30 01:13:06,307:INFO: Dataset: univ                Batch:  1/15	Loss 84.0942 (84.0942)
2022-10-30 01:13:07,459:INFO: Dataset: univ                Batch:  2/15	Loss 104.9002 (94.4832)
2022-10-30 01:13:08,604:INFO: Dataset: univ                Batch:  3/15	Loss 88.3085 (92.4361)
2022-10-30 01:13:09,754:INFO: Dataset: univ                Batch:  4/15	Loss 91.8022 (92.2779)
2022-10-30 01:13:10,908:INFO: Dataset: univ                Batch:  5/15	Loss 85.9860 (90.9751)
2022-10-30 01:13:12,069:INFO: Dataset: univ                Batch:  6/15	Loss 85.8006 (90.0808)
2022-10-30 01:13:13,227:INFO: Dataset: univ                Batch:  7/15	Loss 105.8453 (92.4203)
2022-10-30 01:13:14,389:INFO: Dataset: univ                Batch:  8/15	Loss 92.7153 (92.4613)
2022-10-30 01:13:15,532:INFO: Dataset: univ                Batch:  9/15	Loss 84.4006 (91.5988)
2022-10-30 01:13:16,672:INFO: Dataset: univ                Batch: 10/15	Loss 90.8483 (91.5278)
2022-10-30 01:13:17,809:INFO: Dataset: univ                Batch: 11/15	Loss 87.3068 (91.1674)
2022-10-30 01:13:18,939:INFO: Dataset: univ                Batch: 12/15	Loss 86.2528 (90.8042)
2022-10-30 01:13:20,165:INFO: Dataset: univ                Batch: 13/15	Loss 88.8493 (90.6601)
2022-10-30 01:13:21,304:INFO: Dataset: univ                Batch: 14/15	Loss 86.6040 (90.3882)
2022-10-30 01:13:21,791:INFO: Dataset: univ                Batch: 15/15	Loss 16.4251 (89.4208)
2022-10-30 01:13:23,177:INFO: Dataset: zara1               Batch: 1/8	Loss 88.3606 (88.3606)
2022-10-30 01:13:24,249:INFO: Dataset: zara1               Batch: 2/8	Loss 99.0785 (93.8132)
2022-10-30 01:13:25,320:INFO: Dataset: zara1               Batch: 3/8	Loss 91.6378 (93.0838)
2022-10-30 01:13:26,396:INFO: Dataset: zara1               Batch: 4/8	Loss 89.8247 (92.1208)
2022-10-30 01:13:27,462:INFO: Dataset: zara1               Batch: 5/8	Loss 91.4303 (91.9838)
2022-10-30 01:13:28,542:INFO: Dataset: zara1               Batch: 6/8	Loss 93.2577 (92.1873)
2022-10-30 01:13:29,612:INFO: Dataset: zara1               Batch: 7/8	Loss 92.3917 (92.2151)
2022-10-30 01:13:30,585:INFO: Dataset: zara1               Batch: 8/8	Loss 78.6227 (90.6412)
2022-10-30 01:13:32,003:INFO: Dataset: zara2               Batch:  1/18	Loss 96.1436 (96.1436)
2022-10-30 01:13:33,099:INFO: Dataset: zara2               Batch:  2/18	Loss 87.4743 (91.6175)
2022-10-30 01:13:34,175:INFO: Dataset: zara2               Batch:  3/18	Loss 93.9031 (92.3999)
2022-10-30 01:13:35,257:INFO: Dataset: zara2               Batch:  4/18	Loss 92.2404 (92.3574)
2022-10-30 01:13:36,339:INFO: Dataset: zara2               Batch:  5/18	Loss 87.9906 (91.5502)
2022-10-30 01:13:37,418:INFO: Dataset: zara2               Batch:  6/18	Loss 87.9850 (90.9428)
2022-10-30 01:13:38,490:INFO: Dataset: zara2               Batch:  7/18	Loss 89.0781 (90.6466)
2022-10-30 01:13:39,567:INFO: Dataset: zara2               Batch:  8/18	Loss 88.0468 (90.2977)
2022-10-30 01:13:40,657:INFO: Dataset: zara2               Batch:  9/18	Loss 87.7180 (90.0145)
2022-10-30 01:13:41,736:INFO: Dataset: zara2               Batch: 10/18	Loss 85.9857 (89.5707)
2022-10-30 01:13:42,821:INFO: Dataset: zara2               Batch: 11/18	Loss 117.9263 (92.2469)
2022-10-30 01:13:43,894:INFO: Dataset: zara2               Batch: 12/18	Loss 89.5844 (92.0427)
2022-10-30 01:13:44,978:INFO: Dataset: zara2               Batch: 13/18	Loss 88.0446 (91.7273)
2022-10-30 01:13:46,058:INFO: Dataset: zara2               Batch: 14/18	Loss 100.0461 (92.3437)
2022-10-30 01:13:47,144:INFO: Dataset: zara2               Batch: 15/18	Loss 103.9236 (93.1448)
2022-10-30 01:13:48,225:INFO: Dataset: zara2               Batch: 16/18	Loss 89.1140 (92.8972)
2022-10-30 01:13:49,315:INFO: Dataset: zara2               Batch: 17/18	Loss 88.1405 (92.6111)
2022-10-30 01:13:50,287:INFO: Dataset: zara2               Batch: 18/18	Loss 79.3161 (91.9583)
2022-10-30 01:13:50,354:INFO: - Computing ADE (validation o)
2022-10-30 01:13:50,713:INFO: 		 ADE on eth                       dataset:	 4.303864002227783
2022-10-30 01:13:50,713:INFO: Average validation o:	ADE  4.3039	FDE  7.5415
2022-10-30 01:13:50,714:INFO: - Computing ADE (validation)
2022-10-30 01:13:51,062:INFO: 		 ADE on hotel                     dataset:	 1.8688318729400635
2022-10-30 01:13:51,517:INFO: 		 ADE on univ                      dataset:	 2.070978879928589
2022-10-30 01:13:51,878:INFO: 		 ADE on zara1                     dataset:	 2.267082691192627
2022-10-30 01:13:52,447:INFO: 		 ADE on zara2                     dataset:	 2.252556324005127
2022-10-30 01:13:52,447:INFO: Average validation:	ADE  2.1379	FDE  3.9190
2022-10-30 01:13:52,448:INFO: - Computing ADE (training)
2022-10-30 01:13:52,926:INFO: 		 ADE on hotel                     dataset:	 2.07066011428833
2022-10-30 01:13:53,992:INFO: 		 ADE on univ                      dataset:	 1.9824657440185547
2022-10-30 01:13:54,681:INFO: 		 ADE on zara1                     dataset:	 3.038445472717285
2022-10-30 01:13:55,915:INFO: 		 ADE on zara2                     dataset:	 2.660844564437866
2022-10-30 01:13:55,915:INFO: Average training:	ADE  2.1897	FDE  4.0227
2022-10-30 01:13:55,926:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_486.pth.tar
2022-10-30 01:13:55,927:INFO: 
===> EPOCH: 487 (P4)
2022-10-30 01:13:55,927:INFO: - Computing loss (training)
2022-10-30 01:13:57,221:INFO: Dataset: hotel               Batch: 1/4	Loss 83.6916 (83.6916)
2022-10-30 01:13:58,284:INFO: Dataset: hotel               Batch: 2/4	Loss 82.9475 (83.3169)
2022-10-30 01:13:59,349:INFO: Dataset: hotel               Batch: 3/4	Loss 83.4261 (83.3533)
2022-10-30 01:14:00,132:INFO: Dataset: hotel               Batch: 4/4	Loss 50.6619 (77.9191)
2022-10-30 01:14:01,548:INFO: Dataset: univ                Batch:  1/15	Loss 87.4696 (87.4696)
2022-10-30 01:14:02,659:INFO: Dataset: univ                Batch:  2/15	Loss 169.1801 (129.5159)
2022-10-30 01:14:03,778:INFO: Dataset: univ                Batch:  3/15	Loss 84.6922 (113.0696)
2022-10-30 01:14:04,891:INFO: Dataset: univ                Batch:  4/15	Loss 88.2198 (106.7736)
2022-10-30 01:14:05,998:INFO: Dataset: univ                Batch:  5/15	Loss 89.6240 (103.5260)
2022-10-30 01:14:07,108:INFO: Dataset: univ                Batch:  6/15	Loss 86.4970 (100.8782)
2022-10-30 01:14:08,236:INFO: Dataset: univ                Batch:  7/15	Loss 87.4977 (98.7975)
2022-10-30 01:14:09,346:INFO: Dataset: univ                Batch:  8/15	Loss 88.1665 (97.5461)
2022-10-30 01:14:10,469:INFO: Dataset: univ                Batch:  9/15	Loss 86.5940 (96.2927)
2022-10-30 01:14:11,584:INFO: Dataset: univ                Batch: 10/15	Loss 88.3961 (95.5158)
2022-10-30 01:14:12,701:INFO: Dataset: univ                Batch: 11/15	Loss 85.7063 (94.5684)
2022-10-30 01:14:13,819:INFO: Dataset: univ                Batch: 12/15	Loss 84.2323 (93.6717)
2022-10-30 01:14:14,925:INFO: Dataset: univ                Batch: 13/15	Loss 88.1676 (93.2860)
2022-10-30 01:14:16,042:INFO: Dataset: univ                Batch: 14/15	Loss 89.5864 (93.0177)
2022-10-30 01:14:16,536:INFO: Dataset: univ                Batch: 15/15	Loss 15.8898 (91.8518)
2022-10-30 01:14:17,949:INFO: Dataset: zara1               Batch: 1/8	Loss 94.1891 (94.1891)
2022-10-30 01:14:19,055:INFO: Dataset: zara1               Batch: 2/8	Loss 89.3581 (91.7519)
2022-10-30 01:14:20,171:INFO: Dataset: zara1               Batch: 3/8	Loss 92.8558 (92.1584)
2022-10-30 01:14:21,255:INFO: Dataset: zara1               Batch: 4/8	Loss 94.1509 (92.6496)
2022-10-30 01:14:22,316:INFO: Dataset: zara1               Batch: 5/8	Loss 89.9016 (92.0856)
2022-10-30 01:14:23,383:INFO: Dataset: zara1               Batch: 6/8	Loss 90.6825 (91.8483)
2022-10-30 01:14:24,458:INFO: Dataset: zara1               Batch: 7/8	Loss 92.0015 (91.8721)
2022-10-30 01:14:25,431:INFO: Dataset: zara1               Batch: 8/8	Loss 78.8113 (90.3598)
2022-10-30 01:14:26,835:INFO: Dataset: zara2               Batch:  1/18	Loss 89.8870 (89.8870)
2022-10-30 01:14:27,908:INFO: Dataset: zara2               Batch:  2/18	Loss 98.3834 (94.1411)
2022-10-30 01:14:28,989:INFO: Dataset: zara2               Batch:  3/18	Loss 93.7263 (94.0124)
2022-10-30 01:14:30,059:INFO: Dataset: zara2               Batch:  4/18	Loss 95.2831 (94.3305)
2022-10-30 01:14:31,131:INFO: Dataset: zara2               Batch:  5/18	Loss 88.2902 (93.2455)
2022-10-30 01:14:32,216:INFO: Dataset: zara2               Batch:  6/18	Loss 90.1632 (92.7092)
2022-10-30 01:14:33,295:INFO: Dataset: zara2               Batch:  7/18	Loss 89.4481 (92.2641)
2022-10-30 01:14:34,378:INFO: Dataset: zara2               Batch:  8/18	Loss 90.0923 (91.9998)
2022-10-30 01:14:35,445:INFO: Dataset: zara2               Batch:  9/18	Loss 91.0217 (91.9052)
2022-10-30 01:14:36,513:INFO: Dataset: zara2               Batch: 10/18	Loss 85.9822 (91.2791)
2022-10-30 01:14:37,583:INFO: Dataset: zara2               Batch: 11/18	Loss 87.5862 (90.9368)
2022-10-30 01:14:38,652:INFO: Dataset: zara2               Batch: 12/18	Loss 96.8743 (91.4041)
2022-10-30 01:14:39,715:INFO: Dataset: zara2               Batch: 13/18	Loss 91.5474 (91.4149)
2022-10-30 01:14:40,773:INFO: Dataset: zara2               Batch: 14/18	Loss 87.4714 (91.1448)
2022-10-30 01:14:41,834:INFO: Dataset: zara2               Batch: 15/18	Loss 87.1808 (90.8938)
2022-10-30 01:14:42,894:INFO: Dataset: zara2               Batch: 16/18	Loss 91.2240 (90.9163)
2022-10-30 01:14:43,951:INFO: Dataset: zara2               Batch: 17/18	Loss 88.5277 (90.7516)
2022-10-30 01:14:44,922:INFO: Dataset: zara2               Batch: 18/18	Loss 76.1931 (90.0801)
2022-10-30 01:14:44,985:INFO: - Computing ADE (validation o)
2022-10-30 01:14:45,337:INFO: 		 ADE on eth                       dataset:	 4.289875507354736
2022-10-30 01:14:45,337:INFO: Average validation o:	ADE  4.2899	FDE  7.5189
2022-10-30 01:14:45,337:INFO: - Computing ADE (validation)
2022-10-30 01:14:45,698:INFO: 		 ADE on hotel                     dataset:	 1.8986560106277466
2022-10-30 01:14:46,140:INFO: 		 ADE on univ                      dataset:	 2.0577762126922607
2022-10-30 01:14:46,499:INFO: 		 ADE on zara1                     dataset:	 2.256537675857544
2022-10-30 01:14:47,050:INFO: 		 ADE on zara2                     dataset:	 2.2482545375823975
2022-10-30 01:14:47,050:INFO: Average validation:	ADE  2.1305	FDE  3.9038
2022-10-30 01:14:47,051:INFO: - Computing ADE (training)
2022-10-30 01:14:47,533:INFO: 		 ADE on hotel                     dataset:	 2.0690054893493652
2022-10-30 01:14:48,616:INFO: 		 ADE on univ                      dataset:	 1.9780346155166626
2022-10-30 01:14:49,319:INFO: 		 ADE on zara1                     dataset:	 3.055767059326172
2022-10-30 01:14:50,532:INFO: 		 ADE on zara2                     dataset:	 2.661031484603882
2022-10-30 01:14:50,532:INFO: Average training:	ADE  2.1876	FDE  4.0143
2022-10-30 01:14:50,544:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_487.pth.tar
2022-10-30 01:14:50,544:INFO: 
===> EPOCH: 488 (P4)
2022-10-30 01:14:50,544:INFO: - Computing loss (training)
2022-10-30 01:14:51,841:INFO: Dataset: hotel               Batch: 1/4	Loss 80.9035 (80.9035)
2022-10-30 01:14:52,919:INFO: Dataset: hotel               Batch: 2/4	Loss 85.3475 (83.1101)
2022-10-30 01:14:53,993:INFO: Dataset: hotel               Batch: 3/4	Loss 83.0446 (83.0892)
2022-10-30 01:14:54,778:INFO: Dataset: hotel               Batch: 4/4	Loss 51.2948 (77.9719)
2022-10-30 01:14:56,244:INFO: Dataset: univ                Batch:  1/15	Loss 89.6365 (89.6365)
2022-10-30 01:14:57,362:INFO: Dataset: univ                Batch:  2/15	Loss 88.5718 (89.1024)
2022-10-30 01:14:58,483:INFO: Dataset: univ                Batch:  3/15	Loss 87.2140 (88.3908)
2022-10-30 01:14:59,604:INFO: Dataset: univ                Batch:  4/15	Loss 86.3722 (87.8699)
2022-10-30 01:15:00,728:INFO: Dataset: univ                Batch:  5/15	Loss 87.5819 (87.8093)
2022-10-30 01:15:01,846:INFO: Dataset: univ                Batch:  6/15	Loss 86.0827 (87.5268)
2022-10-30 01:15:02,959:INFO: Dataset: univ                Batch:  7/15	Loss 86.4202 (87.3737)
2022-10-30 01:15:04,076:INFO: Dataset: univ                Batch:  8/15	Loss 109.9017 (90.2805)
2022-10-30 01:15:05,196:INFO: Dataset: univ                Batch:  9/15	Loss -279.3579 (48.6912)
2022-10-30 01:15:06,319:INFO: Dataset: univ                Batch: 10/15	Loss 87.1699 (52.9168)
2022-10-30 01:15:07,458:INFO: Dataset: univ                Batch: 11/15	Loss 98.5837 (58.0092)
2022-10-30 01:15:08,576:INFO: Dataset: univ                Batch: 12/15	Loss 85.4808 (60.3314)
2022-10-30 01:15:09,697:INFO: Dataset: univ                Batch: 13/15	Loss 86.5061 (62.3143)
2022-10-30 01:15:10,820:INFO: Dataset: univ                Batch: 14/15	Loss 92.6664 (64.4004)
2022-10-30 01:15:11,298:INFO: Dataset: univ                Batch: 15/15	Loss 15.6945 (63.8188)
2022-10-30 01:15:12,680:INFO: Dataset: zara1               Batch: 1/8	Loss 89.5339 (89.5339)
2022-10-30 01:15:13,739:INFO: Dataset: zara1               Batch: 2/8	Loss 89.0202 (89.2867)
2022-10-30 01:15:14,912:INFO: Dataset: zara1               Batch: 3/8	Loss 91.5696 (90.0607)
2022-10-30 01:15:16,003:INFO: Dataset: zara1               Batch: 4/8	Loss 91.3627 (90.3950)
2022-10-30 01:15:17,095:INFO: Dataset: zara1               Batch: 5/8	Loss 92.8899 (90.8081)
2022-10-30 01:15:18,158:INFO: Dataset: zara1               Batch: 6/8	Loss 90.2282 (90.7149)
2022-10-30 01:15:19,200:INFO: Dataset: zara1               Batch: 7/8	Loss 90.0297 (90.6228)
2022-10-30 01:15:20,153:INFO: Dataset: zara1               Batch: 8/8	Loss 80.1623 (89.4997)
2022-10-30 01:15:21,556:INFO: Dataset: zara2               Batch:  1/18	Loss 90.2697 (90.2697)
2022-10-30 01:15:22,636:INFO: Dataset: zara2               Batch:  2/18	Loss 87.0799 (88.7710)
2022-10-30 01:15:23,728:INFO: Dataset: zara2               Batch:  3/18	Loss 86.5336 (87.9973)
2022-10-30 01:15:24,812:INFO: Dataset: zara2               Batch:  4/18	Loss 91.5803 (88.8826)
2022-10-30 01:15:25,896:INFO: Dataset: zara2               Batch:  5/18	Loss 87.5564 (88.5788)
2022-10-30 01:15:26,981:INFO: Dataset: zara2               Batch:  6/18	Loss 86.9171 (88.2982)
2022-10-30 01:15:28,063:INFO: Dataset: zara2               Batch:  7/18	Loss 93.1914 (89.0300)
2022-10-30 01:15:29,152:INFO: Dataset: zara2               Batch:  8/18	Loss 95.9244 (89.9114)
2022-10-30 01:15:30,243:INFO: Dataset: zara2               Batch:  9/18	Loss 88.3644 (89.7253)
2022-10-30 01:15:31,334:INFO: Dataset: zara2               Batch: 10/18	Loss 97.1322 (90.4993)
2022-10-30 01:15:32,423:INFO: Dataset: zara2               Batch: 11/18	Loss 91.3495 (90.5763)
2022-10-30 01:15:33,512:INFO: Dataset: zara2               Batch: 12/18	Loss 87.8011 (90.3422)
2022-10-30 01:15:34,597:INFO: Dataset: zara2               Batch: 13/18	Loss 84.9227 (89.9489)
2022-10-30 01:15:35,688:INFO: Dataset: zara2               Batch: 14/18	Loss 87.0042 (89.7427)
2022-10-30 01:15:36,774:INFO: Dataset: zara2               Batch: 15/18	Loss 99.5972 (90.3694)
2022-10-30 01:15:37,853:INFO: Dataset: zara2               Batch: 16/18	Loss 91.3199 (90.4276)
2022-10-30 01:15:38,942:INFO: Dataset: zara2               Batch: 17/18	Loss 98.6948 (90.9083)
2022-10-30 01:15:39,933:INFO: Dataset: zara2               Batch: 18/18	Loss 76.9447 (90.2042)
2022-10-30 01:15:40,001:INFO: - Computing ADE (validation o)
2022-10-30 01:15:40,353:INFO: 		 ADE on eth                       dataset:	 4.31781005859375
2022-10-30 01:15:40,353:INFO: Average validation o:	ADE  4.3178	FDE  7.5457
2022-10-30 01:15:40,354:INFO: - Computing ADE (validation)
2022-10-30 01:15:40,711:INFO: 		 ADE on hotel                     dataset:	 1.8966134786605835
2022-10-30 01:15:41,167:INFO: 		 ADE on univ                      dataset:	 2.0638926029205322
2022-10-30 01:15:41,523:INFO: 		 ADE on zara1                     dataset:	 2.249133825302124
2022-10-30 01:15:42,088:INFO: 		 ADE on zara2                     dataset:	 2.260592222213745
2022-10-30 01:15:42,088:INFO: Average validation:	ADE  2.1376	FDE  3.9213
2022-10-30 01:15:42,089:INFO: - Computing ADE (training)
2022-10-30 01:15:42,572:INFO: 		 ADE on hotel                     dataset:	 2.0789668560028076
2022-10-30 01:15:43,628:INFO: 		 ADE on univ                      dataset:	 1.9821196794509888
2022-10-30 01:15:44,347:INFO: 		 ADE on zara1                     dataset:	 3.0430662631988525
2022-10-30 01:15:45,549:INFO: 		 ADE on zara2                     dataset:	 2.669365644454956
2022-10-30 01:15:45,550:INFO: Average training:	ADE  2.1917	FDE  4.0274
2022-10-30 01:15:45,561:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_488.pth.tar
2022-10-30 01:15:45,561:INFO: 
===> EPOCH: 489 (P4)
2022-10-30 01:15:45,561:INFO: - Computing loss (training)
2022-10-30 01:15:46,859:INFO: Dataset: hotel               Batch: 1/4	Loss 84.0937 (84.0937)
2022-10-30 01:15:47,921:INFO: Dataset: hotel               Batch: 2/4	Loss 82.3594 (83.2016)
2022-10-30 01:15:48,986:INFO: Dataset: hotel               Batch: 3/4	Loss 83.6548 (83.3574)
2022-10-30 01:15:49,768:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7483 (78.1520)
2022-10-30 01:15:51,236:INFO: Dataset: univ                Batch:  1/15	Loss 84.7536 (84.7536)
2022-10-30 01:15:52,373:INFO: Dataset: univ                Batch:  2/15	Loss 85.9868 (85.3658)
2022-10-30 01:15:53,500:INFO: Dataset: univ                Batch:  3/15	Loss 86.8364 (85.8485)
2022-10-30 01:15:54,622:INFO: Dataset: univ                Batch:  4/15	Loss 86.7982 (86.0862)
2022-10-30 01:15:55,745:INFO: Dataset: univ                Batch:  5/15	Loss 92.3164 (87.2883)
2022-10-30 01:15:56,865:INFO: Dataset: univ                Batch:  6/15	Loss 87.0420 (87.2489)
2022-10-30 01:15:57,984:INFO: Dataset: univ                Batch:  7/15	Loss 85.1141 (86.9276)
2022-10-30 01:15:59,098:INFO: Dataset: univ                Batch:  8/15	Loss 127.2208 (91.8098)
2022-10-30 01:16:00,212:INFO: Dataset: univ                Batch:  9/15	Loss 88.5608 (91.4787)
2022-10-30 01:16:01,327:INFO: Dataset: univ                Batch: 10/15	Loss 88.1460 (91.1523)
2022-10-30 01:16:02,457:INFO: Dataset: univ                Batch: 11/15	Loss 88.5392 (90.8966)
2022-10-30 01:16:03,575:INFO: Dataset: univ                Batch: 12/15	Loss 86.5170 (90.5316)
2022-10-30 01:16:04,689:INFO: Dataset: univ                Batch: 13/15	Loss 87.3088 (90.3037)
2022-10-30 01:16:05,807:INFO: Dataset: univ                Batch: 14/15	Loss 149.2329 (94.2663)
2022-10-30 01:16:06,288:INFO: Dataset: univ                Batch: 15/15	Loss 17.8495 (93.2994)
2022-10-30 01:16:07,664:INFO: Dataset: zara1               Batch: 1/8	Loss 90.6023 (90.6023)
2022-10-30 01:16:08,728:INFO: Dataset: zara1               Batch: 2/8	Loss 92.5678 (91.5850)
2022-10-30 01:16:09,792:INFO: Dataset: zara1               Batch: 3/8	Loss 92.5021 (91.8705)
2022-10-30 01:16:10,870:INFO: Dataset: zara1               Batch: 4/8	Loss 91.6726 (91.8248)
2022-10-30 01:16:11,943:INFO: Dataset: zara1               Batch: 5/8	Loss 88.1307 (91.0392)
2022-10-30 01:16:13,004:INFO: Dataset: zara1               Batch: 6/8	Loss 90.9500 (91.0240)
2022-10-30 01:16:14,079:INFO: Dataset: zara1               Batch: 7/8	Loss 91.2788 (91.0602)
2022-10-30 01:16:15,040:INFO: Dataset: zara1               Batch: 8/8	Loss 78.9420 (89.6825)
2022-10-30 01:16:16,405:INFO: Dataset: zara2               Batch:  1/18	Loss 27.6004 (27.6004)
2022-10-30 01:16:17,471:INFO: Dataset: zara2               Batch:  2/18	Loss 86.2941 (55.7012)
2022-10-30 01:16:18,538:INFO: Dataset: zara2               Batch:  3/18	Loss 97.1186 (70.1776)
2022-10-30 01:16:19,598:INFO: Dataset: zara2               Batch:  4/18	Loss 101.8093 (77.8112)
2022-10-30 01:16:20,654:INFO: Dataset: zara2               Batch:  5/18	Loss 100.1770 (82.2221)
2022-10-30 01:16:21,716:INFO: Dataset: zara2               Batch:  6/18	Loss 217.2620 (105.0109)
2022-10-30 01:16:22,771:INFO: Dataset: zara2               Batch:  7/18	Loss 88.2505 (102.6802)
2022-10-30 01:16:23,830:INFO: Dataset: zara2               Batch:  8/18	Loss 105.0381 (102.9703)
2022-10-30 01:16:24,889:INFO: Dataset: zara2               Batch:  9/18	Loss 89.1500 (101.4283)
2022-10-30 01:16:25,944:INFO: Dataset: zara2               Batch: 10/18	Loss 93.4859 (100.7258)
2022-10-30 01:16:27,000:INFO: Dataset: zara2               Batch: 11/18	Loss 88.0943 (99.5017)
2022-10-30 01:16:28,054:INFO: Dataset: zara2               Batch: 12/18	Loss 102.8296 (99.7516)
2022-10-30 01:16:29,115:INFO: Dataset: zara2               Batch: 13/18	Loss 91.4543 (99.1002)
2022-10-30 01:16:30,177:INFO: Dataset: zara2               Batch: 14/18	Loss 85.7171 (98.2021)
2022-10-30 01:16:31,235:INFO: Dataset: zara2               Batch: 15/18	Loss 91.4942 (97.7517)
2022-10-30 01:16:32,289:INFO: Dataset: zara2               Batch: 16/18	Loss 103.8720 (98.1516)
2022-10-30 01:16:33,343:INFO: Dataset: zara2               Batch: 17/18	Loss 93.5130 (97.8724)
2022-10-30 01:16:34,295:INFO: Dataset: zara2               Batch: 18/18	Loss 77.3779 (96.9102)
2022-10-30 01:16:34,362:INFO: - Computing ADE (validation o)
2022-10-30 01:16:34,729:INFO: 		 ADE on eth                       dataset:	 4.328378200531006
2022-10-30 01:16:34,730:INFO: Average validation o:	ADE  4.3284	FDE  7.5757
2022-10-30 01:16:34,730:INFO: - Computing ADE (validation)
2022-10-30 01:16:35,081:INFO: 		 ADE on hotel                     dataset:	 1.9081332683563232
2022-10-30 01:16:35,535:INFO: 		 ADE on univ                      dataset:	 2.0689589977264404
2022-10-30 01:16:35,892:INFO: 		 ADE on zara1                     dataset:	 2.2886462211608887
2022-10-30 01:16:36,427:INFO: 		 ADE on zara2                     dataset:	 2.248537302017212
2022-10-30 01:16:36,427:INFO: Average validation:	ADE  2.1388	FDE  3.9171
2022-10-30 01:16:36,428:INFO: - Computing ADE (training)
2022-10-30 01:16:36,921:INFO: 		 ADE on hotel                     dataset:	 2.067416191101074
2022-10-30 01:16:38,021:INFO: 		 ADE on univ                      dataset:	 1.9792582988739014
2022-10-30 01:16:38,721:INFO: 		 ADE on zara1                     dataset:	 3.0671277046203613
2022-10-30 01:16:39,941:INFO: 		 ADE on zara2                     dataset:	 2.668778419494629
2022-10-30 01:16:39,941:INFO: Average training:	ADE  2.1908	FDE  4.0216
2022-10-30 01:16:39,952:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_489.pth.tar
2022-10-30 01:16:39,952:INFO: 
===> EPOCH: 490 (P4)
2022-10-30 01:16:39,952:INFO: - Computing loss (training)
2022-10-30 01:16:41,212:INFO: Dataset: hotel               Batch: 1/4	Loss 83.1024 (83.1024)
2022-10-30 01:16:42,270:INFO: Dataset: hotel               Batch: 2/4	Loss 83.1843 (83.1414)
2022-10-30 01:16:43,315:INFO: Dataset: hotel               Batch: 3/4	Loss 83.0351 (83.1051)
2022-10-30 01:16:44,079:INFO: Dataset: hotel               Batch: 4/4	Loss 51.3257 (77.9482)
2022-10-30 01:16:45,548:INFO: Dataset: univ                Batch:  1/15	Loss 86.5591 (86.5591)
2022-10-30 01:16:46,669:INFO: Dataset: univ                Batch:  2/15	Loss 105.9730 (95.3049)
2022-10-30 01:16:47,796:INFO: Dataset: univ                Batch:  3/15	Loss 87.6677 (92.7106)
2022-10-30 01:16:48,915:INFO: Dataset: univ                Batch:  4/15	Loss 85.0735 (90.6875)
2022-10-30 01:16:50,032:INFO: Dataset: univ                Batch:  5/15	Loss 88.4667 (90.2843)
2022-10-30 01:16:51,154:INFO: Dataset: univ                Batch:  6/15	Loss 94.3001 (90.9556)
2022-10-30 01:16:52,277:INFO: Dataset: univ                Batch:  7/15	Loss 90.5648 (90.8951)
2022-10-30 01:16:53,393:INFO: Dataset: univ                Batch:  8/15	Loss 86.6671 (90.3631)
2022-10-30 01:16:54,514:INFO: Dataset: univ                Batch:  9/15	Loss 94.5217 (90.8339)
2022-10-30 01:16:55,636:INFO: Dataset: univ                Batch: 10/15	Loss 90.9267 (90.8427)
2022-10-30 01:16:56,761:INFO: Dataset: univ                Batch: 11/15	Loss 87.0914 (90.5089)
2022-10-30 01:16:57,883:INFO: Dataset: univ                Batch: 12/15	Loss 86.5206 (90.1699)
2022-10-30 01:16:58,997:INFO: Dataset: univ                Batch: 13/15	Loss 88.2782 (90.0384)
2022-10-30 01:17:00,122:INFO: Dataset: univ                Batch: 14/15	Loss 86.6569 (89.7981)
2022-10-30 01:17:00,604:INFO: Dataset: univ                Batch: 15/15	Loss 15.3461 (88.6585)
2022-10-30 01:17:01,968:INFO: Dataset: zara1               Batch: 1/8	Loss 90.0088 (90.0088)
2022-10-30 01:17:03,037:INFO: Dataset: zara1               Batch: 2/8	Loss 90.9312 (90.5114)
2022-10-30 01:17:04,172:INFO: Dataset: zara1               Batch: 3/8	Loss 91.3768 (90.7804)
2022-10-30 01:17:05,238:INFO: Dataset: zara1               Batch: 4/8	Loss 95.7304 (92.1096)
2022-10-30 01:17:06,294:INFO: Dataset: zara1               Batch: 5/8	Loss 88.1773 (91.2277)
2022-10-30 01:17:07,341:INFO: Dataset: zara1               Batch: 6/8	Loss 102.0901 (92.9739)
2022-10-30 01:17:08,391:INFO: Dataset: zara1               Batch: 7/8	Loss 92.8298 (92.9550)
2022-10-30 01:17:09,346:INFO: Dataset: zara1               Batch: 8/8	Loss 85.0759 (92.2003)
2022-10-30 01:17:10,719:INFO: Dataset: zara2               Batch:  1/18	Loss 104.1914 (104.1914)
2022-10-30 01:17:11,832:INFO: Dataset: zara2               Batch:  2/18	Loss 89.0066 (95.8889)
2022-10-30 01:17:12,938:INFO: Dataset: zara2               Batch:  3/18	Loss 103.8462 (98.4977)
2022-10-30 01:17:14,045:INFO: Dataset: zara2               Batch:  4/18	Loss 88.8772 (96.1171)
2022-10-30 01:17:15,135:INFO: Dataset: zara2               Batch:  5/18	Loss 87.3332 (94.4662)
2022-10-30 01:17:16,230:INFO: Dataset: zara2               Batch:  6/18	Loss 101.5320 (95.5243)
2022-10-30 01:17:17,289:INFO: Dataset: zara2               Batch:  7/18	Loss 87.2217 (94.2286)
2022-10-30 01:17:18,345:INFO: Dataset: zara2               Batch:  8/18	Loss 91.6893 (93.8786)
2022-10-30 01:17:19,408:INFO: Dataset: zara2               Batch:  9/18	Loss 87.3475 (93.1621)
2022-10-30 01:17:20,483:INFO: Dataset: zara2               Batch: 10/18	Loss 89.0172 (92.7208)
2022-10-30 01:17:21,563:INFO: Dataset: zara2               Batch: 11/18	Loss 95.0183 (92.9226)
2022-10-30 01:17:22,652:INFO: Dataset: zara2               Batch: 12/18	Loss 87.6727 (92.5415)
2022-10-30 01:17:23,750:INFO: Dataset: zara2               Batch: 13/18	Loss 91.1740 (92.4365)
2022-10-30 01:17:24,840:INFO: Dataset: zara2               Batch: 14/18	Loss 88.1683 (92.1380)
2022-10-30 01:17:25,907:INFO: Dataset: zara2               Batch: 15/18	Loss 78.5160 (91.2728)
2022-10-30 01:17:27,013:INFO: Dataset: zara2               Batch: 16/18	Loss 90.9190 (91.2506)
2022-10-30 01:17:28,124:INFO: Dataset: zara2               Batch: 17/18	Loss 92.3829 (91.3207)
2022-10-30 01:17:29,133:INFO: Dataset: zara2               Batch: 18/18	Loss 74.0058 (90.4620)
2022-10-30 01:17:29,203:INFO: - Computing ADE (validation o)
2022-10-30 01:17:29,562:INFO: 		 ADE on eth                       dataset:	 4.3085832595825195
2022-10-30 01:17:29,563:INFO: Average validation o:	ADE  4.3086	FDE  7.5427
2022-10-30 01:17:29,563:INFO: - Computing ADE (validation)
2022-10-30 01:17:29,919:INFO: 		 ADE on hotel                     dataset:	 1.8837807178497314
2022-10-30 01:17:30,357:INFO: 		 ADE on univ                      dataset:	 2.052326202392578
2022-10-30 01:17:30,735:INFO: 		 ADE on zara1                     dataset:	 2.275179862976074
2022-10-30 01:17:31,288:INFO: 		 ADE on zara2                     dataset:	 2.229767084121704
2022-10-30 01:17:31,289:INFO: Average validation:	ADE  2.1211	FDE  3.8853
2022-10-30 01:17:31,289:INFO: - Computing ADE (training)
2022-10-30 01:17:31,766:INFO: 		 ADE on hotel                     dataset:	 2.0499167442321777
2022-10-30 01:17:32,842:INFO: 		 ADE on univ                      dataset:	 1.9786527156829834
2022-10-30 01:17:33,528:INFO: 		 ADE on zara1                     dataset:	 3.038802146911621
2022-10-30 01:17:34,744:INFO: 		 ADE on zara2                     dataset:	 2.65635085105896
2022-10-30 01:17:34,744:INFO: Average training:	ADE  2.1856	FDE  4.0079
2022-10-30 01:17:34,755:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_490.pth.tar
2022-10-30 01:17:34,755:INFO: 
===> EPOCH: 491 (P4)
2022-10-30 01:17:34,756:INFO: - Computing loss (training)
2022-10-30 01:17:36,062:INFO: Dataset: hotel               Batch: 1/4	Loss 83.9377 (83.9377)
2022-10-30 01:17:37,137:INFO: Dataset: hotel               Batch: 2/4	Loss 81.5022 (82.7670)
2022-10-30 01:17:38,220:INFO: Dataset: hotel               Batch: 3/4	Loss 84.2396 (83.2579)
2022-10-30 01:17:39,012:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9932 (77.4264)
2022-10-30 01:17:40,529:INFO: Dataset: univ                Batch:  1/15	Loss 86.9473 (86.9473)
2022-10-30 01:17:41,696:INFO: Dataset: univ                Batch:  2/15	Loss 98.9056 (92.9644)
2022-10-30 01:17:42,864:INFO: Dataset: univ                Batch:  3/15	Loss 85.6048 (90.4717)
2022-10-30 01:17:44,037:INFO: Dataset: univ                Batch:  4/15	Loss 88.0643 (89.8790)
2022-10-30 01:17:45,191:INFO: Dataset: univ                Batch:  5/15	Loss 87.6325 (89.4403)
2022-10-30 01:17:46,342:INFO: Dataset: univ                Batch:  6/15	Loss 84.9557 (88.7600)
2022-10-30 01:17:47,484:INFO: Dataset: univ                Batch:  7/15	Loss 86.2243 (88.3943)
2022-10-30 01:17:48,599:INFO: Dataset: univ                Batch:  8/15	Loss 86.2303 (88.1385)
2022-10-30 01:17:49,715:INFO: Dataset: univ                Batch:  9/15	Loss 85.8353 (87.8619)
2022-10-30 01:17:50,811:INFO: Dataset: univ                Batch: 10/15	Loss 84.5167 (87.4998)
2022-10-30 01:17:51,900:INFO: Dataset: univ                Batch: 11/15	Loss 86.3925 (87.3971)
2022-10-30 01:17:53,002:INFO: Dataset: univ                Batch: 12/15	Loss 97.5539 (88.2496)
2022-10-30 01:17:54,093:INFO: Dataset: univ                Batch: 13/15	Loss 92.5095 (88.5806)
2022-10-30 01:17:55,191:INFO: Dataset: univ                Batch: 14/15	Loss 86.1252 (88.4140)
2022-10-30 01:17:55,656:INFO: Dataset: univ                Batch: 15/15	Loss 15.6975 (87.4319)
2022-10-30 01:17:56,985:INFO: Dataset: zara1               Batch: 1/8	Loss 90.1863 (90.1863)
2022-10-30 01:17:58,013:INFO: Dataset: zara1               Batch: 2/8	Loss 90.2224 (90.2033)
2022-10-30 01:17:59,047:INFO: Dataset: zara1               Batch: 3/8	Loss 90.6202 (90.3471)
2022-10-30 01:18:00,099:INFO: Dataset: zara1               Batch: 4/8	Loss 91.6076 (90.6743)
2022-10-30 01:18:01,171:INFO: Dataset: zara1               Batch: 5/8	Loss 95.2867 (91.5668)
2022-10-30 01:18:02,216:INFO: Dataset: zara1               Batch: 6/8	Loss 91.2620 (91.5181)
2022-10-30 01:18:03,273:INFO: Dataset: zara1               Batch: 7/8	Loss 91.7251 (91.5511)
2022-10-30 01:18:04,203:INFO: Dataset: zara1               Batch: 8/8	Loss 79.2776 (90.2462)
2022-10-30 01:18:05,542:INFO: Dataset: zara2               Batch:  1/18	Loss 88.5476 (88.5476)
2022-10-30 01:18:06,602:INFO: Dataset: zara2               Batch:  2/18	Loss 92.0729 (90.4211)
2022-10-30 01:18:07,680:INFO: Dataset: zara2               Batch:  3/18	Loss 88.6781 (89.7639)
2022-10-30 01:18:08,753:INFO: Dataset: zara2               Batch:  4/18	Loss 91.8154 (90.2638)
2022-10-30 01:18:09,792:INFO: Dataset: zara2               Batch:  5/18	Loss 123.3335 (96.4557)
2022-10-30 01:18:10,833:INFO: Dataset: zara2               Batch:  6/18	Loss 94.2608 (96.0809)
2022-10-30 01:18:11,846:INFO: Dataset: zara2               Batch:  7/18	Loss 313.7007 (126.3007)
2022-10-30 01:18:12,853:INFO: Dataset: zara2               Batch:  8/18	Loss 87.2267 (121.1521)
2022-10-30 01:18:13,865:INFO: Dataset: zara2               Batch:  9/18	Loss 88.7524 (117.0810)
2022-10-30 01:18:14,914:INFO: Dataset: zara2               Batch: 10/18	Loss 89.1650 (114.3296)
2022-10-30 01:18:15,952:INFO: Dataset: zara2               Batch: 11/18	Loss 89.1531 (112.2005)
2022-10-30 01:18:16,995:INFO: Dataset: zara2               Batch: 12/18	Loss 93.5122 (110.6666)
2022-10-30 01:18:18,026:INFO: Dataset: zara2               Batch: 13/18	Loss 100.1825 (109.8493)
2022-10-30 01:18:19,042:INFO: Dataset: zara2               Batch: 14/18	Loss 116.6991 (110.3366)
2022-10-30 01:18:20,074:INFO: Dataset: zara2               Batch: 15/18	Loss 86.1707 (108.6516)
2022-10-30 01:18:21,081:INFO: Dataset: zara2               Batch: 16/18	Loss 106.3713 (108.5014)
2022-10-30 01:18:22,096:INFO: Dataset: zara2               Batch: 17/18	Loss 88.6484 (107.3099)
2022-10-30 01:18:23,024:INFO: Dataset: zara2               Batch: 18/18	Loss 78.2566 (106.0275)
2022-10-30 01:18:23,090:INFO: - Computing ADE (validation o)
2022-10-30 01:18:23,447:INFO: 		 ADE on eth                       dataset:	 4.262213706970215
2022-10-30 01:18:23,447:INFO: Average validation o:	ADE  4.2622	FDE  7.4583
2022-10-30 01:18:23,448:INFO: - Computing ADE (validation)
2022-10-30 01:18:23,776:INFO: 		 ADE on hotel                     dataset:	 1.8458715677261353
2022-10-30 01:18:24,223:INFO: 		 ADE on univ                      dataset:	 2.0388288497924805
2022-10-30 01:18:24,585:INFO: 		 ADE on zara1                     dataset:	 2.318509340286255
2022-10-30 01:18:25,135:INFO: 		 ADE on zara2                     dataset:	 2.220010757446289
2022-10-30 01:18:25,135:INFO: Average validation:	ADE  2.1110	FDE  3.8630
2022-10-30 01:18:25,136:INFO: - Computing ADE (training)
2022-10-30 01:18:25,624:INFO: 		 ADE on hotel                     dataset:	 2.0402185916900635
2022-10-30 01:18:26,707:INFO: 		 ADE on univ                      dataset:	 1.9701446294784546
2022-10-30 01:18:27,371:INFO: 		 ADE on zara1                     dataset:	 3.0653626918792725
2022-10-30 01:18:28,598:INFO: 		 ADE on zara2                     dataset:	 2.6461591720581055
2022-10-30 01:18:28,598:INFO: Average training:	ADE  2.1789	FDE  3.9891
2022-10-30 01:18:28,610:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_491.pth.tar
2022-10-30 01:18:28,610:INFO: 
===> EPOCH: 492 (P4)
2022-10-30 01:18:28,610:INFO: - Computing loss (training)
2022-10-30 01:18:29,883:INFO: Dataset: hotel               Batch: 1/4	Loss 82.6376 (82.6376)
2022-10-30 01:18:30,920:INFO: Dataset: hotel               Batch: 2/4	Loss 83.6686 (83.1843)
2022-10-30 01:18:31,985:INFO: Dataset: hotel               Batch: 3/4	Loss 83.7448 (83.3622)
2022-10-30 01:18:32,755:INFO: Dataset: hotel               Batch: 4/4	Loss 49.8887 (77.7097)
2022-10-30 01:18:34,220:INFO: Dataset: univ                Batch:  1/15	Loss 85.0729 (85.0729)
2022-10-30 01:18:35,354:INFO: Dataset: univ                Batch:  2/15	Loss 90.5183 (87.7107)
2022-10-30 01:18:36,463:INFO: Dataset: univ                Batch:  3/15	Loss 87.7707 (87.7301)
2022-10-30 01:18:37,598:INFO: Dataset: univ                Batch:  4/15	Loss 95.9563 (89.6714)
2022-10-30 01:18:38,685:INFO: Dataset: univ                Batch:  5/15	Loss 86.4700 (88.9640)
2022-10-30 01:18:39,786:INFO: Dataset: univ                Batch:  6/15	Loss 87.9415 (88.7828)
2022-10-30 01:18:40,877:INFO: Dataset: univ                Batch:  7/15	Loss 85.4820 (88.3233)

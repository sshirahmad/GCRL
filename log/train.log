2022-11-02 16:47:36,380:INFO: Initializing Training Set
2022-11-02 16:47:40,936:INFO: Initializing Validation Set
2022-11-02 16:47:41,449:INFO: Initializing Validation O Set
2022-11-02 16:47:44,401:INFO: 
===> EPOCH: 551 (P4)
2022-11-02 16:47:44,402:INFO: - Computing loss (training)
2022-11-02 16:47:53,162:INFO: Dataset: hotel               Batch: 1/8	Loss 46.4507 (46.4507)
2022-11-02 16:47:53,849:INFO: Dataset: hotel               Batch: 2/8	Loss 44.0851 (45.2409)
2022-11-02 16:47:54,367:INFO: Dataset: hotel               Batch: 3/8	Loss 45.4829 (45.3213)
2022-11-02 16:47:54,801:INFO: Dataset: hotel               Batch: 4/8	Loss 44.4493 (45.1098)
2022-11-02 16:47:55,185:INFO: Dataset: hotel               Batch: 5/8	Loss 46.2412 (45.3081)
2022-11-02 16:47:55,572:INFO: Dataset: hotel               Batch: 6/8	Loss 44.0012 (45.0817)
2022-11-02 16:47:55,971:INFO: Dataset: hotel               Batch: 7/8	Loss 47.4335 (45.3989)
2022-11-02 16:47:56,217:INFO: Dataset: hotel               Batch: 8/8	Loss -8.5929 (43.6894)
2022-11-02 16:48:26,242:INFO: Dataset: univ                Batch:  1/29	Loss 41.9196 (41.9196)
2022-11-02 16:48:26,708:INFO: Dataset: univ                Batch:  2/29	Loss 42.9499 (42.4574)
2022-11-02 16:48:27,096:INFO: Dataset: univ                Batch:  3/29	Loss 43.9763 (42.9338)
2022-11-02 16:48:27,501:INFO: Dataset: univ                Batch:  4/29	Loss 42.6386 (42.8682)
2022-11-02 16:48:27,884:INFO: Dataset: univ                Batch:  5/29	Loss 42.8207 (42.8606)
2022-11-02 16:48:28,291:INFO: Dataset: univ                Batch:  6/29	Loss 42.5664 (42.8074)
2022-11-02 16:48:28,756:INFO: Dataset: univ                Batch:  7/29	Loss 42.3623 (42.7380)
2022-11-02 16:48:29,139:INFO: Dataset: univ                Batch:  8/29	Loss 43.4455 (42.8191)
2022-11-02 16:48:29,528:INFO: Dataset: univ                Batch:  9/29	Loss 43.5282 (42.9030)
2022-11-02 16:48:29,904:INFO: Dataset: univ                Batch: 10/29	Loss 42.2082 (42.8284)
2022-11-02 16:48:30,332:INFO: Dataset: univ                Batch: 11/29	Loss 42.6731 (42.8134)
2022-11-02 16:48:30,775:INFO: Dataset: univ                Batch: 12/29	Loss 42.2291 (42.7645)
2022-11-02 16:48:31,184:INFO: Dataset: univ                Batch: 13/29	Loss 42.8736 (42.7725)
2022-11-02 16:48:31,601:INFO: Dataset: univ                Batch: 14/29	Loss 42.5338 (42.7545)
2022-11-02 16:48:32,033:INFO: Dataset: univ                Batch: 15/29	Loss 43.6717 (42.8157)
2022-11-02 16:48:32,459:INFO: Dataset: univ                Batch: 16/29	Loss 42.8961 (42.8207)
2022-11-02 16:48:32,861:INFO: Dataset: univ                Batch: 17/29	Loss 43.0188 (42.8305)
2022-11-02 16:48:33,289:INFO: Dataset: univ                Batch: 18/29	Loss 42.8897 (42.8337)
2022-11-02 16:48:33,716:INFO: Dataset: univ                Batch: 19/29	Loss 43.3836 (42.8595)
2022-11-02 16:48:34,115:INFO: Dataset: univ                Batch: 20/29	Loss 41.8690 (42.8078)
2022-11-02 16:48:34,532:INFO: Dataset: univ                Batch: 21/29	Loss 41.5543 (42.7357)
2022-11-02 16:48:34,976:INFO: Dataset: univ                Batch: 22/29	Loss 42.2589 (42.7160)
2022-11-02 16:48:35,367:INFO: Dataset: univ                Batch: 23/29	Loss 42.7230 (42.7162)
2022-11-02 16:48:35,761:INFO: Dataset: univ                Batch: 24/29	Loss 42.2154 (42.6974)
2022-11-02 16:48:36,161:INFO: Dataset: univ                Batch: 25/29	Loss 42.6273 (42.6947)
2022-11-02 16:48:36,533:INFO: Dataset: univ                Batch: 26/29	Loss 41.4256 (42.6384)
2022-11-02 16:48:36,921:INFO: Dataset: univ                Batch: 27/29	Loss 42.2535 (42.6245)
2022-11-02 16:48:37,311:INFO: Dataset: univ                Batch: 28/29	Loss 42.2735 (42.6106)
2022-11-02 16:48:37,581:INFO: Dataset: univ                Batch: 29/29	Loss 3.1133 (42.1651)
2022-11-02 16:48:47,371:INFO: Dataset: zara1               Batch:  1/16	Loss 48.5207 (48.5207)
2022-11-02 16:48:47,984:INFO: Dataset: zara1               Batch:  2/16	Loss 48.6120 (48.5606)
2022-11-02 16:48:48,560:INFO: Dataset: zara1               Batch:  3/16	Loss 48.4180 (48.5114)
2022-11-02 16:48:49,042:INFO: Dataset: zara1               Batch:  4/16	Loss 48.1834 (48.4254)
2022-11-02 16:48:49,490:INFO: Dataset: zara1               Batch:  5/16	Loss 49.3555 (48.5828)
2022-11-02 16:48:49,910:INFO: Dataset: zara1               Batch:  6/16	Loss 49.7282 (48.7730)
2022-11-02 16:48:50,356:INFO: Dataset: zara1               Batch:  7/16	Loss 48.4313 (48.7253)
2022-11-02 16:48:50,783:INFO: Dataset: zara1               Batch:  8/16	Loss 49.6559 (48.8202)
2022-11-02 16:48:51,180:INFO: Dataset: zara1               Batch:  9/16	Loss 48.5685 (48.7967)
2022-11-02 16:48:51,603:INFO: Dataset: zara1               Batch: 10/16	Loss 48.6001 (48.7760)
2022-11-02 16:48:51,990:INFO: Dataset: zara1               Batch: 11/16	Loss 49.4072 (48.8324)
2022-11-02 16:48:52,420:INFO: Dataset: zara1               Batch: 12/16	Loss 49.0737 (48.8533)
2022-11-02 16:48:52,809:INFO: Dataset: zara1               Batch: 13/16	Loss 48.1308 (48.7966)
2022-11-02 16:48:53,189:INFO: Dataset: zara1               Batch: 14/16	Loss 48.6521 (48.7865)
2022-11-02 16:48:53,587:INFO: Dataset: zara1               Batch: 15/16	Loss 48.1805 (48.7515)
2022-11-02 16:48:53,908:INFO: Dataset: zara1               Batch: 16/16	Loss 28.9541 (47.8971)
2022-11-02 16:49:23,975:INFO: Dataset: zara2               Batch:  1/36	Loss 41.8587 (41.8587)
2022-11-02 16:49:24,376:INFO: Dataset: zara2               Batch:  2/36	Loss 39.9093 (40.8267)
2022-11-02 16:49:24,799:INFO: Dataset: zara2               Batch:  3/36	Loss 40.3884 (40.6859)
2022-11-02 16:49:25,196:INFO: Dataset: zara2               Batch:  4/36	Loss 41.9179 (40.9535)
2022-11-02 16:49:25,591:INFO: Dataset: zara2               Batch:  5/36	Loss 40.1603 (40.7957)
2022-11-02 16:49:25,988:INFO: Dataset: zara2               Batch:  6/36	Loss 41.0927 (40.8464)
2022-11-02 16:49:26,367:INFO: Dataset: zara2               Batch:  7/36	Loss 40.5123 (40.7993)
2022-11-02 16:49:26,767:INFO: Dataset: zara2               Batch:  8/36	Loss 40.4828 (40.7572)
2022-11-02 16:49:27,154:INFO: Dataset: zara2               Batch:  9/36	Loss 40.0547 (40.6690)
2022-11-02 16:49:27,584:INFO: Dataset: zara2               Batch: 10/36	Loss 40.9324 (40.6953)
2022-11-02 16:49:27,965:INFO: Dataset: zara2               Batch: 11/36	Loss 40.9352 (40.7156)
2022-11-02 16:49:28,334:INFO: Dataset: zara2               Batch: 12/36	Loss 41.0297 (40.7413)
2022-11-02 16:49:28,737:INFO: Dataset: zara2               Batch: 13/36	Loss 40.5979 (40.7304)
2022-11-02 16:49:29,131:INFO: Dataset: zara2               Batch: 14/36	Loss 41.2607 (40.7681)
2022-11-02 16:49:29,533:INFO: Dataset: zara2               Batch: 15/36	Loss 40.4823 (40.7506)
2022-11-02 16:49:29,932:INFO: Dataset: zara2               Batch: 16/36	Loss 41.1905 (40.7789)
2022-11-02 16:49:30,366:INFO: Dataset: zara2               Batch: 17/36	Loss 41.0900 (40.7977)
2022-11-02 16:49:30,787:INFO: Dataset: zara2               Batch: 18/36	Loss 40.2819 (40.7700)
2022-11-02 16:49:31,218:INFO: Dataset: zara2               Batch: 19/36	Loss 40.5019 (40.7550)
2022-11-02 16:49:31,627:INFO: Dataset: zara2               Batch: 20/36	Loss 41.9495 (40.8091)
2022-11-02 16:49:32,034:INFO: Dataset: zara2               Batch: 21/36	Loss 41.0758 (40.8220)
2022-11-02 16:49:32,427:INFO: Dataset: zara2               Batch: 22/36	Loss 41.2315 (40.8408)
2022-11-02 16:49:32,821:INFO: Dataset: zara2               Batch: 23/36	Loss 40.6389 (40.8328)
2022-11-02 16:49:33,231:INFO: Dataset: zara2               Batch: 24/36	Loss 40.3638 (40.8168)
2022-11-02 16:49:33,667:INFO: Dataset: zara2               Batch: 25/36	Loss 41.7153 (40.8525)
2022-11-02 16:49:34,037:INFO: Dataset: zara2               Batch: 26/36	Loss 41.2458 (40.8655)
2022-11-02 16:49:34,433:INFO: Dataset: zara2               Batch: 27/36	Loss 41.0585 (40.8732)
2022-11-02 16:49:34,841:INFO: Dataset: zara2               Batch: 28/36	Loss 40.5931 (40.8623)
2022-11-02 16:49:35,230:INFO: Dataset: zara2               Batch: 29/36	Loss 41.1091 (40.8702)
2022-11-02 16:49:35,675:INFO: Dataset: zara2               Batch: 30/36	Loss 40.9591 (40.8729)
2022-11-02 16:49:36,064:INFO: Dataset: zara2               Batch: 31/36	Loss 40.9442 (40.8753)
2022-11-02 16:49:36,438:INFO: Dataset: zara2               Batch: 32/36	Loss 40.2403 (40.8541)
2022-11-02 16:49:36,805:INFO: Dataset: zara2               Batch: 33/36	Loss 39.7525 (40.8175)
2022-11-02 16:49:37,172:INFO: Dataset: zara2               Batch: 34/36	Loss 40.5337 (40.8082)
2022-11-02 16:49:37,523:INFO: Dataset: zara2               Batch: 35/36	Loss 39.3157 (40.7663)
2022-11-02 16:49:37,852:INFO: Dataset: zara2               Batch: 36/36	Loss 23.8106 (40.3879)
2022-11-02 16:49:38,964:INFO: - Computing ADE (validation o)
2022-11-02 16:49:49,954:INFO: 		 ADE on eth                       dataset:	 2.776306390762329
2022-11-02 16:49:49,956:INFO: Average validation o:	ADE  2.7763	FDE  4.7500
2022-11-02 16:49:49,957:INFO: - Computing ADE (validation)
2022-11-02 16:50:00,967:INFO: 		 ADE on hotel                     dataset:	 0.9146718978881836
2022-11-02 16:50:12,040:INFO: 		 ADE on univ                      dataset:	 1.4602220058441162
2022-11-02 16:50:23,120:INFO: 		 ADE on zara1                     dataset:	 2.4592251777648926
2022-11-02 16:50:34,779:INFO: 		 ADE on zara2                     dataset:	 1.3701035976409912
2022-11-02 16:50:34,779:INFO: Average validation:	ADE  1.4554	FDE  2.6665
2022-11-02 16:50:34,782:INFO: - Computing ADE (training)
2022-11-02 16:50:46,194:INFO: 		 ADE on hotel                     dataset:	 1.2739754915237427
2022-11-02 16:51:18,286:INFO: 		 ADE on univ                      dataset:	 1.3518553972244263
2022-11-02 16:51:30,710:INFO: 		 ADE on zara1                     dataset:	 2.4144413471221924
2022-11-02 16:52:03,973:INFO: 		 ADE on zara2                     dataset:	 1.6402678489685059
2022-11-02 16:52:03,974:INFO: Average training:	ADE  1.4761	FDE  2.7103
2022-11-02 16:52:03,994:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_551.pth.tar
2022-11-02 16:52:03,994:INFO: 
===> EPOCH: 552 (P4)
2022-11-02 16:52:03,994:INFO: - Computing loss (training)
2022-11-02 16:52:12,670:INFO: Dataset: hotel               Batch: 1/8	Loss 46.0588 (46.0588)
2022-11-02 16:52:13,241:INFO: Dataset: hotel               Batch: 2/8	Loss 44.6402 (45.3428)
2022-11-02 16:52:13,712:INFO: Dataset: hotel               Batch: 3/8	Loss 45.9323 (45.5304)
2022-11-02 16:52:14,219:INFO: Dataset: hotel               Batch: 4/8	Loss 46.2356 (45.7096)
2022-11-02 16:52:14,651:INFO: Dataset: hotel               Batch: 5/8	Loss 46.0418 (45.7780)
2022-11-02 16:52:15,043:INFO: Dataset: hotel               Batch: 6/8	Loss 44.4214 (45.5610)
2022-11-02 16:52:15,469:INFO: Dataset: hotel               Batch: 7/8	Loss 44.9924 (45.4719)
2022-11-02 16:52:15,726:INFO: Dataset: hotel               Batch: 8/8	Loss -9.4243 (43.7338)
2022-11-02 16:52:46,895:INFO: Dataset: univ                Batch:  1/29	Loss 42.5466 (42.5466)
2022-11-02 16:52:47,210:INFO: Dataset: univ                Batch:  2/29	Loss 42.9369 (42.7448)
2022-11-02 16:52:47,529:INFO: Dataset: univ                Batch:  3/29	Loss 41.3222 (42.2310)
2022-11-02 16:52:47,820:INFO: Dataset: univ                Batch:  4/29	Loss 43.5829 (42.5248)
2022-11-02 16:52:48,109:INFO: Dataset: univ                Batch:  5/29	Loss 43.0526 (42.6317)
2022-11-02 16:52:48,421:INFO: Dataset: univ                Batch:  6/29	Loss 41.5057 (42.4183)
2022-11-02 16:52:48,720:INFO: Dataset: univ                Batch:  7/29	Loss 41.9222 (42.3501)
2022-11-02 16:52:49,051:INFO: Dataset: univ                Batch:  8/29	Loss 41.8307 (42.2783)
2022-11-02 16:52:49,367:INFO: Dataset: univ                Batch:  9/29	Loss 42.2443 (42.2747)
2022-11-02 16:52:49,690:INFO: Dataset: univ                Batch: 10/29	Loss 42.9615 (42.3405)
2022-11-02 16:52:50,001:INFO: Dataset: univ                Batch: 11/29	Loss 42.1578 (42.3233)
2022-11-02 16:52:50,305:INFO: Dataset: univ                Batch: 12/29	Loss 41.6862 (42.2754)
2022-11-02 16:52:50,603:INFO: Dataset: univ                Batch: 13/29	Loss 42.5585 (42.2976)
2022-11-02 16:52:50,895:INFO: Dataset: univ                Batch: 14/29	Loss 42.5765 (42.3174)
2022-11-02 16:52:51,192:INFO: Dataset: univ                Batch: 15/29	Loss 42.3093 (42.3168)
2022-11-02 16:52:51,502:INFO: Dataset: univ                Batch: 16/29	Loss 42.7345 (42.3444)
2022-11-02 16:52:51,852:INFO: Dataset: univ                Batch: 17/29	Loss 43.3725 (42.3996)
2022-11-02 16:52:52,163:INFO: Dataset: univ                Batch: 18/29	Loss 41.8918 (42.3730)
2022-11-02 16:52:52,462:INFO: Dataset: univ                Batch: 19/29	Loss 41.5938 (42.3263)
2022-11-02 16:52:52,775:INFO: Dataset: univ                Batch: 20/29	Loss 43.5052 (42.3747)
2022-11-02 16:52:53,209:INFO: Dataset: univ                Batch: 21/29	Loss 42.1128 (42.3630)
2022-11-02 16:52:53,547:INFO: Dataset: univ                Batch: 22/29	Loss 42.9022 (42.3874)
2022-11-02 16:52:53,887:INFO: Dataset: univ                Batch: 23/29	Loss 41.9111 (42.3659)
2022-11-02 16:52:54,185:INFO: Dataset: univ                Batch: 24/29	Loss 41.5614 (42.3303)
2022-11-02 16:52:54,486:INFO: Dataset: univ                Batch: 25/29	Loss 41.5667 (42.2966)
2022-11-02 16:52:54,805:INFO: Dataset: univ                Batch: 26/29	Loss 42.1754 (42.2917)
2022-11-02 16:52:55,102:INFO: Dataset: univ                Batch: 27/29	Loss 42.5239 (42.3004)
2022-11-02 16:52:55,413:INFO: Dataset: univ                Batch: 28/29	Loss 41.1755 (42.2501)
2022-11-02 16:52:55,605:INFO: Dataset: univ                Batch: 29/29	Loss 1.3735 (41.5953)
2022-11-02 16:53:05,895:INFO: Dataset: zara1               Batch:  1/16	Loss 47.9907 (47.9907)
2022-11-02 16:53:06,637:INFO: Dataset: zara1               Batch:  2/16	Loss 48.8699 (48.4448)
2022-11-02 16:53:07,175:INFO: Dataset: zara1               Batch:  3/16	Loss 47.7675 (48.2166)
2022-11-02 16:53:07,619:INFO: Dataset: zara1               Batch:  4/16	Loss 48.4941 (48.2822)
2022-11-02 16:53:08,073:INFO: Dataset: zara1               Batch:  5/16	Loss 48.9754 (48.4176)
2022-11-02 16:53:08,499:INFO: Dataset: zara1               Batch:  6/16	Loss 48.1974 (48.3824)
2022-11-02 16:53:08,945:INFO: Dataset: zara1               Batch:  7/16	Loss 47.9652 (48.3253)
2022-11-02 16:53:09,453:INFO: Dataset: zara1               Batch:  8/16	Loss 48.5334 (48.3563)
2022-11-02 16:53:09,853:INFO: Dataset: zara1               Batch:  9/16	Loss 48.9732 (48.4272)
2022-11-02 16:53:10,278:INFO: Dataset: zara1               Batch: 10/16	Loss 48.4656 (48.4316)
2022-11-02 16:53:10,695:INFO: Dataset: zara1               Batch: 11/16	Loss 48.2400 (48.4135)
2022-11-02 16:53:11,087:INFO: Dataset: zara1               Batch: 12/16	Loss 47.0539 (48.3089)
2022-11-02 16:53:11,500:INFO: Dataset: zara1               Batch: 13/16	Loss 48.2414 (48.3037)
2022-11-02 16:53:11,917:INFO: Dataset: zara1               Batch: 14/16	Loss 48.8036 (48.3374)
2022-11-02 16:53:12,331:INFO: Dataset: zara1               Batch: 15/16	Loss 47.5915 (48.2892)
2022-11-02 16:53:12,696:INFO: Dataset: zara1               Batch: 16/16	Loss 27.5803 (47.4717)
2022-11-02 16:53:42,572:INFO: Dataset: zara2               Batch:  1/36	Loss 40.9729 (40.9729)
2022-11-02 16:53:42,986:INFO: Dataset: zara2               Batch:  2/36	Loss 39.8351 (40.4058)
2022-11-02 16:53:43,389:INFO: Dataset: zara2               Batch:  3/36	Loss 40.2311 (40.3441)
2022-11-02 16:53:43,824:INFO: Dataset: zara2               Batch:  4/36	Loss 40.5139 (40.3845)
2022-11-02 16:53:44,241:INFO: Dataset: zara2               Batch:  5/36	Loss 38.9113 (40.0665)
2022-11-02 16:53:44,674:INFO: Dataset: zara2               Batch:  6/36	Loss 40.1565 (40.0810)
2022-11-02 16:53:45,071:INFO: Dataset: zara2               Batch:  7/36	Loss 39.5302 (39.9953)
2022-11-02 16:53:45,488:INFO: Dataset: zara2               Batch:  8/36	Loss 41.2121 (40.1583)
2022-11-02 16:53:45,870:INFO: Dataset: zara2               Batch:  9/36	Loss 40.5965 (40.2108)
2022-11-02 16:53:46,270:INFO: Dataset: zara2               Batch: 10/36	Loss 39.8637 (40.1802)
2022-11-02 16:53:46,664:INFO: Dataset: zara2               Batch: 11/36	Loss 39.9371 (40.1572)
2022-11-02 16:53:47,079:INFO: Dataset: zara2               Batch: 12/36	Loss 39.7036 (40.1135)
2022-11-02 16:53:47,490:INFO: Dataset: zara2               Batch: 13/36	Loss 40.0334 (40.1080)
2022-11-02 16:53:47,883:INFO: Dataset: zara2               Batch: 14/36	Loss 39.9228 (40.0927)
2022-11-02 16:53:48,297:INFO: Dataset: zara2               Batch: 15/36	Loss 40.1832 (40.0986)
2022-11-02 16:53:48,692:INFO: Dataset: zara2               Batch: 16/36	Loss 40.2601 (40.1092)
2022-11-02 16:53:49,095:INFO: Dataset: zara2               Batch: 17/36	Loss 40.4874 (40.1273)
2022-11-02 16:53:49,556:INFO: Dataset: zara2               Batch: 18/36	Loss 39.9738 (40.1187)
2022-11-02 16:53:49,951:INFO: Dataset: zara2               Batch: 19/36	Loss 39.9427 (40.1081)
2022-11-02 16:53:50,370:INFO: Dataset: zara2               Batch: 20/36	Loss 39.4924 (40.0771)
2022-11-02 16:53:50,796:INFO: Dataset: zara2               Batch: 21/36	Loss 40.1522 (40.0804)
2022-11-02 16:53:51,227:INFO: Dataset: zara2               Batch: 22/36	Loss 40.3703 (40.0933)
2022-11-02 16:53:51,698:INFO: Dataset: zara2               Batch: 23/36	Loss 40.0680 (40.0923)
2022-11-02 16:53:52,080:INFO: Dataset: zara2               Batch: 24/36	Loss 41.4255 (40.1466)
2022-11-02 16:53:52,504:INFO: Dataset: zara2               Batch: 25/36	Loss 40.0290 (40.1417)
2022-11-02 16:53:52,917:INFO: Dataset: zara2               Batch: 26/36	Loss 39.4534 (40.1169)
2022-11-02 16:53:53,314:INFO: Dataset: zara2               Batch: 27/36	Loss 39.2346 (40.0801)
2022-11-02 16:53:53,737:INFO: Dataset: zara2               Batch: 28/36	Loss 39.2350 (40.0472)
2022-11-02 16:53:54,146:INFO: Dataset: zara2               Batch: 29/36	Loss 40.0776 (40.0481)
2022-11-02 16:53:54,560:INFO: Dataset: zara2               Batch: 30/36	Loss 40.5723 (40.0657)
2022-11-02 16:53:54,959:INFO: Dataset: zara2               Batch: 31/36	Loss 39.1345 (40.0364)
2022-11-02 16:53:55,371:INFO: Dataset: zara2               Batch: 32/36	Loss 40.8678 (40.0619)
2022-11-02 16:53:55,768:INFO: Dataset: zara2               Batch: 33/36	Loss 40.2887 (40.0685)
2022-11-02 16:53:56,180:INFO: Dataset: zara2               Batch: 34/36	Loss 39.2237 (40.0414)
2022-11-02 16:53:56,568:INFO: Dataset: zara2               Batch: 35/36	Loss 39.6493 (40.0302)
2022-11-02 16:53:56,895:INFO: Dataset: zara2               Batch: 36/36	Loss 22.7734 (39.6822)
2022-11-02 16:53:58,076:INFO: - Computing ADE (validation o)
2022-11-02 16:54:09,238:INFO: 		 ADE on eth                       dataset:	 2.3752200603485107
2022-11-02 16:54:09,238:INFO: Average validation o:	ADE  2.3752	FDE  4.1192
2022-11-02 16:54:09,239:INFO: - Computing ADE (validation)
2022-11-02 16:54:20,297:INFO: 		 ADE on hotel                     dataset:	 0.9606555104255676
2022-11-02 16:54:31,389:INFO: 		 ADE on univ                      dataset:	 1.3240562677383423
2022-11-02 16:54:42,395:INFO: 		 ADE on zara1                     dataset:	 2.0733914375305176
2022-11-02 16:54:53,988:INFO: 		 ADE on zara2                     dataset:	 1.2072985172271729
2022-11-02 16:54:53,988:INFO: Average validation:	ADE  1.3049	FDE  2.4291
2022-11-02 16:54:53,989:INFO: - Computing ADE (training)
2022-11-02 16:55:05,280:INFO: 		 ADE on hotel                     dataset:	 1.3076965808868408
2022-11-02 16:55:37,627:INFO: 		 ADE on univ                      dataset:	 1.2269253730773926
2022-11-02 16:55:49,796:INFO: 		 ADE on zara1                     dataset:	 2.0763721466064453
2022-11-02 16:56:22,667:INFO: 		 ADE on zara2                     dataset:	 1.4361956119537354
2022-11-02 16:56:22,667:INFO: Average training:	ADE  1.3256	FDE  2.4726
2022-11-02 16:56:22,688:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_552.pth.tar
2022-11-02 16:56:22,688:INFO: 
===> EPOCH: 553 (P4)
2022-11-02 16:56:22,689:INFO: - Computing loss (training)
2022-11-02 16:56:31,319:INFO: Dataset: hotel               Batch: 1/8	Loss 45.1518 (45.1518)
2022-11-02 16:56:31,969:INFO: Dataset: hotel               Batch: 2/8	Loss 44.2200 (44.6881)
2022-11-02 16:56:32,507:INFO: Dataset: hotel               Batch: 3/8	Loss 47.7106 (45.6860)
2022-11-02 16:56:32,988:INFO: Dataset: hotel               Batch: 4/8	Loss 46.2604 (45.8254)
2022-11-02 16:56:33,400:INFO: Dataset: hotel               Batch: 5/8	Loss 44.4776 (45.5621)
2022-11-02 16:56:33,809:INFO: Dataset: hotel               Batch: 6/8	Loss 44.6795 (45.4179)
2022-11-02 16:56:34,220:INFO: Dataset: hotel               Batch: 7/8	Loss 44.0008 (45.1972)
2022-11-02 16:56:34,457:INFO: Dataset: hotel               Batch: 8/8	Loss -8.5743 (43.3528)
2022-11-02 16:57:04,020:INFO: Dataset: univ                Batch:  1/29	Loss 40.5842 (40.5842)
2022-11-02 16:57:04,423:INFO: Dataset: univ                Batch:  2/29	Loss 40.8772 (40.7403)
2022-11-02 16:57:04,820:INFO: Dataset: univ                Batch:  3/29	Loss 43.3231 (41.5155)
2022-11-02 16:57:05,220:INFO: Dataset: univ                Batch:  4/29	Loss 41.6823 (41.5525)
2022-11-02 16:57:05,590:INFO: Dataset: univ                Batch:  5/29	Loss 42.6919 (41.7488)
2022-11-02 16:57:05,979:INFO: Dataset: univ                Batch:  6/29	Loss 42.2433 (41.8227)
2022-11-02 16:57:06,377:INFO: Dataset: univ                Batch:  7/29	Loss 40.3770 (41.5692)
2022-11-02 16:57:06,803:INFO: Dataset: univ                Batch:  8/29	Loss 41.2298 (41.5237)
2022-11-02 16:57:07,196:INFO: Dataset: univ                Batch:  9/29	Loss 41.6126 (41.5340)
2022-11-02 16:57:07,580:INFO: Dataset: univ                Batch: 10/29	Loss 40.2249 (41.3903)
2022-11-02 16:57:07,942:INFO: Dataset: univ                Batch: 11/29	Loss 41.3963 (41.3908)
2022-11-02 16:57:08,312:INFO: Dataset: univ                Batch: 12/29	Loss 40.8496 (41.3463)
2022-11-02 16:57:08,692:INFO: Dataset: univ                Batch: 13/29	Loss 41.5240 (41.3583)
2022-11-02 16:57:09,126:INFO: Dataset: univ                Batch: 14/29	Loss 40.3588 (41.2829)
2022-11-02 16:57:09,505:INFO: Dataset: univ                Batch: 15/29	Loss 40.4729 (41.2218)
2022-11-02 16:57:09,880:INFO: Dataset: univ                Batch: 16/29	Loss 41.5348 (41.2387)
2022-11-02 16:57:10,287:INFO: Dataset: univ                Batch: 17/29	Loss 40.4448 (41.1899)
2022-11-02 16:57:10,713:INFO: Dataset: univ                Batch: 18/29	Loss 39.9846 (41.1140)
2022-11-02 16:57:11,129:INFO: Dataset: univ                Batch: 19/29	Loss 40.9238 (41.1049)
2022-11-02 16:57:11,546:INFO: Dataset: univ                Batch: 20/29	Loss 40.2745 (41.0600)
2022-11-02 16:57:12,000:INFO: Dataset: univ                Batch: 21/29	Loss 40.1068 (41.0070)
2022-11-02 16:57:12,413:INFO: Dataset: univ                Batch: 22/29	Loss 41.3398 (41.0223)
2022-11-02 16:57:12,817:INFO: Dataset: univ                Batch: 23/29	Loss 41.0691 (41.0241)
2022-11-02 16:57:13,212:INFO: Dataset: univ                Batch: 24/29	Loss 40.8503 (41.0174)
2022-11-02 16:57:13,602:INFO: Dataset: univ                Batch: 25/29	Loss 40.4619 (40.9946)
2022-11-02 16:57:13,987:INFO: Dataset: univ                Batch: 26/29	Loss 40.1192 (40.9596)
2022-11-02 16:57:14,444:INFO: Dataset: univ                Batch: 27/29	Loss 40.0573 (40.9196)
2022-11-02 16:57:14,864:INFO: Dataset: univ                Batch: 28/29	Loss 40.5968 (40.9072)
2022-11-02 16:57:15,130:INFO: Dataset: univ                Batch: 29/29	Loss 1.3436 (40.4498)
2022-11-02 16:57:25,112:INFO: Dataset: zara1               Batch:  1/16	Loss 43.9979 (43.9979)
2022-11-02 16:57:25,661:INFO: Dataset: zara1               Batch:  2/16	Loss 43.7754 (43.8770)
2022-11-02 16:57:26,145:INFO: Dataset: zara1               Batch:  3/16	Loss 44.4294 (44.0586)
2022-11-02 16:57:26,556:INFO: Dataset: zara1               Batch:  4/16	Loss 43.7252 (43.9772)
2022-11-02 16:57:26,982:INFO: Dataset: zara1               Batch:  5/16	Loss 43.8327 (43.9508)
2022-11-02 16:57:27,408:INFO: Dataset: zara1               Batch:  6/16	Loss 44.1591 (43.9915)
2022-11-02 16:57:27,802:INFO: Dataset: zara1               Batch:  7/16	Loss 43.8957 (43.9767)
2022-11-02 16:57:28,237:INFO: Dataset: zara1               Batch:  8/16	Loss 44.2043 (44.0053)
2022-11-02 16:57:28,666:INFO: Dataset: zara1               Batch:  9/16	Loss 43.8856 (43.9904)
2022-11-02 16:57:29,065:INFO: Dataset: zara1               Batch: 10/16	Loss 43.6670 (43.9628)
2022-11-02 16:57:29,476:INFO: Dataset: zara1               Batch: 11/16	Loss 43.7146 (43.9394)
2022-11-02 16:57:29,891:INFO: Dataset: zara1               Batch: 12/16	Loss 43.6016 (43.9105)
2022-11-02 16:57:30,269:INFO: Dataset: zara1               Batch: 13/16	Loss 44.0422 (43.9204)
2022-11-02 16:57:30,675:INFO: Dataset: zara1               Batch: 14/16	Loss 44.0789 (43.9312)
2022-11-02 16:57:31,179:INFO: Dataset: zara1               Batch: 15/16	Loss 43.6971 (43.9152)
2022-11-02 16:57:31,623:INFO: Dataset: zara1               Batch: 16/16	Loss 24.9434 (43.2462)
2022-11-02 16:58:01,402:INFO: Dataset: zara2               Batch:  1/36	Loss 37.6444 (37.6444)
2022-11-02 16:58:01,810:INFO: Dataset: zara2               Batch:  2/36	Loss 37.5594 (37.6028)
2022-11-02 16:58:02,207:INFO: Dataset: zara2               Batch:  3/36	Loss 37.8372 (37.6879)
2022-11-02 16:58:02,596:INFO: Dataset: zara2               Batch:  4/36	Loss 37.1012 (37.5408)
2022-11-02 16:58:03,015:INFO: Dataset: zara2               Batch:  5/36	Loss 37.0487 (37.4590)
2022-11-02 16:58:03,431:INFO: Dataset: zara2               Batch:  6/36	Loss 36.9716 (37.3700)
2022-11-02 16:58:03,842:INFO: Dataset: zara2               Batch:  7/36	Loss 37.5578 (37.3950)
2022-11-02 16:58:04,289:INFO: Dataset: zara2               Batch:  8/36	Loss 36.7706 (37.3140)
2022-11-02 16:58:04,675:INFO: Dataset: zara2               Batch:  9/36	Loss 37.3233 (37.3151)
2022-11-02 16:58:05,092:INFO: Dataset: zara2               Batch: 10/36	Loss 37.6871 (37.3541)
2022-11-02 16:58:05,503:INFO: Dataset: zara2               Batch: 11/36	Loss 37.3074 (37.3500)
2022-11-02 16:58:05,919:INFO: Dataset: zara2               Batch: 12/36	Loss 36.6122 (37.2866)
2022-11-02 16:58:06,338:INFO: Dataset: zara2               Batch: 13/36	Loss 38.0519 (37.3386)
2022-11-02 16:58:06,769:INFO: Dataset: zara2               Batch: 14/36	Loss 37.6045 (37.3579)
2022-11-02 16:58:07,186:INFO: Dataset: zara2               Batch: 15/36	Loss 37.0787 (37.3384)
2022-11-02 16:58:07,591:INFO: Dataset: zara2               Batch: 16/36	Loss 37.7743 (37.3656)
2022-11-02 16:58:07,999:INFO: Dataset: zara2               Batch: 17/36	Loss 37.1152 (37.3498)
2022-11-02 16:58:08,378:INFO: Dataset: zara2               Batch: 18/36	Loss 38.0171 (37.3859)
2022-11-02 16:58:08,780:INFO: Dataset: zara2               Batch: 19/36	Loss 36.9162 (37.3621)
2022-11-02 16:58:09,176:INFO: Dataset: zara2               Batch: 20/36	Loss 37.3154 (37.3597)
2022-11-02 16:58:09,559:INFO: Dataset: zara2               Batch: 21/36	Loss 37.9631 (37.3895)
2022-11-02 16:58:09,911:INFO: Dataset: zara2               Batch: 22/36	Loss 37.4132 (37.3905)
2022-11-02 16:58:10,328:INFO: Dataset: zara2               Batch: 23/36	Loss 37.5347 (37.3964)
2022-11-02 16:58:10,753:INFO: Dataset: zara2               Batch: 24/36	Loss 36.1034 (37.3460)
2022-11-02 16:58:11,200:INFO: Dataset: zara2               Batch: 25/36	Loss 36.7889 (37.3241)
2022-11-02 16:58:11,623:INFO: Dataset: zara2               Batch: 26/36	Loss 36.6739 (37.2984)
2022-11-02 16:58:12,020:INFO: Dataset: zara2               Batch: 27/36	Loss 36.8209 (37.2829)
2022-11-02 16:58:12,424:INFO: Dataset: zara2               Batch: 28/36	Loss 36.5360 (37.2563)
2022-11-02 16:58:12,845:INFO: Dataset: zara2               Batch: 29/36	Loss 36.7639 (37.2418)
2022-11-02 16:58:13,254:INFO: Dataset: zara2               Batch: 30/36	Loss 37.0064 (37.2327)
2022-11-02 16:58:13,670:INFO: Dataset: zara2               Batch: 31/36	Loss 37.4968 (37.2405)
2022-11-02 16:58:14,033:INFO: Dataset: zara2               Batch: 32/36	Loss 37.1695 (37.2383)
2022-11-02 16:58:14,413:INFO: Dataset: zara2               Batch: 33/36	Loss 37.5179 (37.2465)
2022-11-02 16:58:14,817:INFO: Dataset: zara2               Batch: 34/36	Loss 37.2353 (37.2462)
2022-11-02 16:58:15,219:INFO: Dataset: zara2               Batch: 35/36	Loss 35.9589 (37.2108)
2022-11-02 16:58:15,586:INFO: Dataset: zara2               Batch: 36/36	Loss 21.0618 (36.8851)
2022-11-02 16:58:16,726:INFO: - Computing ADE (validation o)
2022-11-02 16:58:28,208:INFO: 		 ADE on eth                       dataset:	 1.3034958839416504
2022-11-02 16:58:28,208:INFO: Average validation o:	ADE  1.3035	FDE  2.0524
2022-11-02 16:58:28,209:INFO: - Computing ADE (validation)
2022-11-02 16:58:39,307:INFO: 		 ADE on hotel                     dataset:	 1.0518629550933838
2022-11-02 16:58:50,461:INFO: 		 ADE on univ                      dataset:	 0.9786761999130249
2022-11-02 16:59:01,491:INFO: 		 ADE on zara1                     dataset:	 1.0206094980239868
2022-11-02 16:59:13,005:INFO: 		 ADE on zara2                     dataset:	 0.7139550447463989
2022-11-02 16:59:13,005:INFO: Average validation:	ADE  0.8880	FDE  1.5956
2022-11-02 16:59:13,007:INFO: - Computing ADE (training)
2022-11-02 16:59:24,411:INFO: 		 ADE on hotel                     dataset:	 1.3765743970870972
2022-11-02 16:59:56,477:INFO: 		 ADE on univ                      dataset:	 0.9259827136993408
2022-11-02 17:00:08,904:INFO: 		 ADE on zara1                     dataset:	 1.0042755603790283
2022-11-02 17:00:42,087:INFO: 		 ADE on zara2                     dataset:	 0.7804851531982422
2022-11-02 17:00:42,087:INFO: Average training:	ADE  0.9129	FDE  1.6460
2022-11-02 17:00:42,116:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_553.pth.tar
2022-11-02 17:00:42,116:INFO: 
===> EPOCH: 554 (P4)
2022-11-02 17:00:42,116:INFO: - Computing loss (training)
2022-11-02 17:00:50,943:INFO: Dataset: hotel               Batch: 1/8	Loss 44.8529 (44.8529)
2022-11-02 17:00:51,552:INFO: Dataset: hotel               Batch: 2/8	Loss 45.3928 (45.1241)
2022-11-02 17:00:52,059:INFO: Dataset: hotel               Batch: 3/8	Loss 44.7419 (45.0027)
2022-11-02 17:00:52,543:INFO: Dataset: hotel               Batch: 4/8	Loss 46.1896 (45.2735)
2022-11-02 17:00:52,981:INFO: Dataset: hotel               Batch: 5/8	Loss 43.3711 (44.9050)
2022-11-02 17:00:53,379:INFO: Dataset: hotel               Batch: 6/8	Loss 44.9465 (44.9127)
2022-11-02 17:00:53,815:INFO: Dataset: hotel               Batch: 7/8	Loss 46.1392 (45.0941)
2022-11-02 17:00:54,059:INFO: Dataset: hotel               Batch: 8/8	Loss -8.5260 (43.6086)
2022-11-02 17:01:24,745:INFO: Dataset: univ                Batch:  1/29	Loss 40.3178 (40.3178)
2022-11-02 17:01:25,149:INFO: Dataset: univ                Batch:  2/29	Loss 39.6548 (39.9448)
2022-11-02 17:01:25,546:INFO: Dataset: univ                Batch:  3/29	Loss 40.5705 (40.1492)
2022-11-02 17:01:25,968:INFO: Dataset: univ                Batch:  4/29	Loss 39.7824 (40.0547)
2022-11-02 17:01:26,388:INFO: Dataset: univ                Batch:  5/29	Loss 39.2236 (39.8632)
2022-11-02 17:01:26,789:INFO: Dataset: univ                Batch:  6/29	Loss 39.3761 (39.7779)
2022-11-02 17:01:27,199:INFO: Dataset: univ                Batch:  7/29	Loss 40.0478 (39.8124)
2022-11-02 17:01:27,605:INFO: Dataset: univ                Batch:  8/29	Loss 40.1862 (39.8555)
2022-11-02 17:01:28,021:INFO: Dataset: univ                Batch:  9/29	Loss 40.1070 (39.8820)
2022-11-02 17:01:28,457:INFO: Dataset: univ                Batch: 10/29	Loss 39.9117 (39.8853)
2022-11-02 17:01:28,867:INFO: Dataset: univ                Batch: 11/29	Loss 39.2839 (39.8299)
2022-11-02 17:01:29,298:INFO: Dataset: univ                Batch: 12/29	Loss 40.0384 (39.8453)
2022-11-02 17:01:29,704:INFO: Dataset: univ                Batch: 13/29	Loss 40.0361 (39.8594)
2022-11-02 17:01:30,066:INFO: Dataset: univ                Batch: 14/29	Loss 40.1065 (39.8774)
2022-11-02 17:01:30,464:INFO: Dataset: univ                Batch: 15/29	Loss 40.0767 (39.8889)
2022-11-02 17:01:30,835:INFO: Dataset: univ                Batch: 16/29	Loss 39.0862 (39.8273)
2022-11-02 17:01:31,221:INFO: Dataset: univ                Batch: 17/29	Loss 39.8988 (39.8315)
2022-11-02 17:01:31,631:INFO: Dataset: univ                Batch: 18/29	Loss 40.0954 (39.8442)
2022-11-02 17:01:32,053:INFO: Dataset: univ                Batch: 19/29	Loss 40.1176 (39.8580)
2022-11-02 17:01:32,431:INFO: Dataset: univ                Batch: 20/29	Loss 39.3377 (39.8334)
2022-11-02 17:01:32,805:INFO: Dataset: univ                Batch: 21/29	Loss 39.9167 (39.8374)
2022-11-02 17:01:33,176:INFO: Dataset: univ                Batch: 22/29	Loss 40.0730 (39.8463)
2022-11-02 17:01:33,579:INFO: Dataset: univ                Batch: 23/29	Loss 39.5194 (39.8298)
2022-11-02 17:01:33,946:INFO: Dataset: univ                Batch: 24/29	Loss 39.7634 (39.8272)
2022-11-02 17:01:34,314:INFO: Dataset: univ                Batch: 25/29	Loss 39.2516 (39.8045)
2022-11-02 17:01:34,717:INFO: Dataset: univ                Batch: 26/29	Loss 39.9075 (39.8080)
2022-11-02 17:01:35,109:INFO: Dataset: univ                Batch: 27/29	Loss 38.6960 (39.7622)
2022-11-02 17:01:35,514:INFO: Dataset: univ                Batch: 28/29	Loss 39.4760 (39.7517)
2022-11-02 17:01:35,792:INFO: Dataset: univ                Batch: 29/29	Loss 0.9852 (39.2300)
2022-11-02 17:01:45,845:INFO: Dataset: zara1               Batch:  1/16	Loss 43.4420 (43.4420)
2022-11-02 17:01:46,456:INFO: Dataset: zara1               Batch:  2/16	Loss 43.7087 (43.5804)
2022-11-02 17:01:46,948:INFO: Dataset: zara1               Batch:  3/16	Loss 43.3968 (43.5210)
2022-11-02 17:01:47,395:INFO: Dataset: zara1               Batch:  4/16	Loss 42.8648 (43.3589)
2022-11-02 17:01:47,811:INFO: Dataset: zara1               Batch:  5/16	Loss 43.3407 (43.3556)
2022-11-02 17:01:48,260:INFO: Dataset: zara1               Batch:  6/16	Loss 43.5922 (43.3958)
2022-11-02 17:01:48,668:INFO: Dataset: zara1               Batch:  7/16	Loss 43.6828 (43.4336)
2022-11-02 17:01:49,069:INFO: Dataset: zara1               Batch:  8/16	Loss 43.4754 (43.4380)
2022-11-02 17:01:49,471:INFO: Dataset: zara1               Batch:  9/16	Loss 43.5881 (43.4534)
2022-11-02 17:01:49,843:INFO: Dataset: zara1               Batch: 10/16	Loss 43.8516 (43.4927)
2022-11-02 17:01:50,274:INFO: Dataset: zara1               Batch: 11/16	Loss 43.6685 (43.5086)
2022-11-02 17:01:50,676:INFO: Dataset: zara1               Batch: 12/16	Loss 43.9860 (43.5461)
2022-11-02 17:01:51,064:INFO: Dataset: zara1               Batch: 13/16	Loss 43.6754 (43.5567)
2022-11-02 17:01:51,439:INFO: Dataset: zara1               Batch: 14/16	Loss 43.2750 (43.5371)
2022-11-02 17:01:51,842:INFO: Dataset: zara1               Batch: 15/16	Loss 43.3805 (43.5264)
2022-11-02 17:01:52,203:INFO: Dataset: zara1               Batch: 16/16	Loss 24.4409 (42.7529)
2022-11-02 17:02:21,898:INFO: Dataset: zara2               Batch:  1/36	Loss 36.6572 (36.6572)
2022-11-02 17:02:22,326:INFO: Dataset: zara2               Batch:  2/36	Loss 36.6350 (36.6467)
2022-11-02 17:02:22,723:INFO: Dataset: zara2               Batch:  3/36	Loss 36.4834 (36.5890)
2022-11-02 17:02:23,145:INFO: Dataset: zara2               Batch:  4/36	Loss 36.4012 (36.5423)
2022-11-02 17:02:23,519:INFO: Dataset: zara2               Batch:  5/36	Loss 36.5025 (36.5340)
2022-11-02 17:02:23,924:INFO: Dataset: zara2               Batch:  6/36	Loss 37.2389 (36.6484)
2022-11-02 17:02:24,304:INFO: Dataset: zara2               Batch:  7/36	Loss 36.1444 (36.5747)
2022-11-02 17:02:24,711:INFO: Dataset: zara2               Batch:  8/36	Loss 37.4419 (36.6791)
2022-11-02 17:02:25,102:INFO: Dataset: zara2               Batch:  9/36	Loss 36.7517 (36.6873)
2022-11-02 17:02:25,505:INFO: Dataset: zara2               Batch: 10/36	Loss 37.1818 (36.7324)
2022-11-02 17:02:25,920:INFO: Dataset: zara2               Batch: 11/36	Loss 36.2705 (36.6888)
2022-11-02 17:02:26,370:INFO: Dataset: zara2               Batch: 12/36	Loss 36.5410 (36.6756)
2022-11-02 17:02:26,785:INFO: Dataset: zara2               Batch: 13/36	Loss 35.9001 (36.6171)
2022-11-02 17:02:27,205:INFO: Dataset: zara2               Batch: 14/36	Loss 36.7094 (36.6241)
2022-11-02 17:02:27,592:INFO: Dataset: zara2               Batch: 15/36	Loss 36.3583 (36.6059)
2022-11-02 17:02:27,988:INFO: Dataset: zara2               Batch: 16/36	Loss 37.2590 (36.6461)
2022-11-02 17:02:28,381:INFO: Dataset: zara2               Batch: 17/36	Loss 36.6873 (36.6484)
2022-11-02 17:02:28,782:INFO: Dataset: zara2               Batch: 18/36	Loss 36.2544 (36.6237)
2022-11-02 17:02:29,180:INFO: Dataset: zara2               Batch: 19/36	Loss 35.9917 (36.5942)
2022-11-02 17:02:29,641:INFO: Dataset: zara2               Batch: 20/36	Loss 36.1802 (36.5718)
2022-11-02 17:02:30,047:INFO: Dataset: zara2               Batch: 21/36	Loss 36.5933 (36.5727)
2022-11-02 17:02:30,459:INFO: Dataset: zara2               Batch: 22/36	Loss 37.3358 (36.6062)
2022-11-02 17:02:30,835:INFO: Dataset: zara2               Batch: 23/36	Loss 36.9362 (36.6237)
2022-11-02 17:02:31,227:INFO: Dataset: zara2               Batch: 24/36	Loss 36.7133 (36.6272)
2022-11-02 17:02:31,618:INFO: Dataset: zara2               Batch: 25/36	Loss 36.1382 (36.6085)
2022-11-02 17:02:32,010:INFO: Dataset: zara2               Batch: 26/36	Loss 36.7574 (36.6143)
2022-11-02 17:02:32,418:INFO: Dataset: zara2               Batch: 27/36	Loss 36.7359 (36.6182)
2022-11-02 17:02:32,824:INFO: Dataset: zara2               Batch: 28/36	Loss 36.9381 (36.6299)
2022-11-02 17:02:33,232:INFO: Dataset: zara2               Batch: 29/36	Loss 36.3373 (36.6195)
2022-11-02 17:02:33,660:INFO: Dataset: zara2               Batch: 30/36	Loss 36.3385 (36.6103)
2022-11-02 17:02:34,065:INFO: Dataset: zara2               Batch: 31/36	Loss 36.0689 (36.5915)
2022-11-02 17:02:34,466:INFO: Dataset: zara2               Batch: 32/36	Loss 36.2880 (36.5828)
2022-11-02 17:02:34,888:INFO: Dataset: zara2               Batch: 33/36	Loss 36.5525 (36.5819)
2022-11-02 17:02:35,300:INFO: Dataset: zara2               Batch: 34/36	Loss 35.9211 (36.5610)
2022-11-02 17:02:35,726:INFO: Dataset: zara2               Batch: 35/36	Loss 36.6046 (36.5620)
2022-11-02 17:02:36,081:INFO: Dataset: zara2               Batch: 36/36	Loss 20.7897 (36.2752)
2022-11-02 17:02:37,253:INFO: - Computing ADE (validation o)
2022-11-02 17:02:48,149:INFO: 		 ADE on eth                       dataset:	 1.3657453060150146
2022-11-02 17:02:48,149:INFO: Average validation o:	ADE  1.3657	FDE  2.1379
2022-11-02 17:02:48,150:INFO: - Computing ADE (validation)
2022-11-02 17:02:59,169:INFO: 		 ADE on hotel                     dataset:	 1.0064643621444702
2022-11-02 17:03:10,335:INFO: 		 ADE on univ                      dataset:	 0.9575085043907166
2022-11-02 17:03:21,390:INFO: 		 ADE on zara1                     dataset:	 0.9088112115859985
2022-11-02 17:03:32,828:INFO: 		 ADE on zara2                     dataset:	 0.6796805262565613
2022-11-02 17:03:32,828:INFO: Average validation:	ADE  0.8555	FDE  1.5111
2022-11-02 17:03:32,829:INFO: - Computing ADE (training)
2022-11-02 17:03:44,099:INFO: 		 ADE on hotel                     dataset:	 1.3133399486541748
2022-11-02 17:04:16,541:INFO: 		 ADE on univ                      dataset:	 0.8864311575889587
2022-11-02 17:04:28,590:INFO: 		 ADE on zara1                     dataset:	 1.0361287593841553
2022-11-02 17:05:01,366:INFO: 		 ADE on zara2                     dataset:	 0.774351179599762
2022-11-02 17:05:01,366:INFO: Average training:	ADE  0.8841	FDE  1.5606
2022-11-02 17:05:01,384:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_554.pth.tar
2022-11-02 17:05:01,385:INFO: 
===> EPOCH: 555 (P4)
2022-11-02 17:05:01,385:INFO: - Computing loss (training)
2022-11-02 17:05:10,066:INFO: Dataset: hotel               Batch: 1/8	Loss 44.0214 (44.0214)
2022-11-02 17:05:10,705:INFO: Dataset: hotel               Batch: 2/8	Loss 45.1673 (44.6079)
2022-11-02 17:05:11,205:INFO: Dataset: hotel               Batch: 3/8	Loss 46.2558 (45.1793)
2022-11-02 17:05:11,668:INFO: Dataset: hotel               Batch: 4/8	Loss 45.4302 (45.2431)
2022-11-02 17:05:12,139:INFO: Dataset: hotel               Batch: 5/8	Loss 45.0113 (45.2014)
2022-11-02 17:05:12,526:INFO: Dataset: hotel               Batch: 6/8	Loss 45.6010 (45.2671)
2022-11-02 17:05:12,920:INFO: Dataset: hotel               Batch: 7/8	Loss 44.0313 (45.0983)
2022-11-02 17:05:13,163:INFO: Dataset: hotel               Batch: 8/8	Loss -9.3795 (43.2297)
2022-11-02 17:05:42,700:INFO: Dataset: univ                Batch:  1/29	Loss 40.9037 (40.9037)
2022-11-02 17:05:43,073:INFO: Dataset: univ                Batch:  2/29	Loss 39.6257 (40.2521)
2022-11-02 17:05:43,452:INFO: Dataset: univ                Batch:  3/29	Loss 40.1066 (40.2069)
2022-11-02 17:05:43,884:INFO: Dataset: univ                Batch:  4/29	Loss 39.6353 (40.0694)
2022-11-02 17:05:44,286:INFO: Dataset: univ                Batch:  5/29	Loss 39.2899 (39.9211)
2022-11-02 17:05:44,682:INFO: Dataset: univ                Batch:  6/29	Loss 38.9571 (39.7502)
2022-11-02 17:05:45,069:INFO: Dataset: univ                Batch:  7/29	Loss 39.6824 (39.7409)
2022-11-02 17:05:45,473:INFO: Dataset: univ                Batch:  8/29	Loss 39.0250 (39.6453)
2022-11-02 17:05:45,862:INFO: Dataset: univ                Batch:  9/29	Loss 39.1732 (39.5958)
2022-11-02 17:05:46,265:INFO: Dataset: univ                Batch: 10/29	Loss 39.0684 (39.5389)
2022-11-02 17:05:46,659:INFO: Dataset: univ                Batch: 11/29	Loss 39.5400 (39.5390)
2022-11-02 17:05:47,043:INFO: Dataset: univ                Batch: 12/29	Loss 38.5210 (39.4539)
2022-11-02 17:05:47,431:INFO: Dataset: univ                Batch: 13/29	Loss 39.3338 (39.4450)
2022-11-02 17:05:47,844:INFO: Dataset: univ                Batch: 14/29	Loss 39.5244 (39.4506)
2022-11-02 17:05:48,219:INFO: Dataset: univ                Batch: 15/29	Loss 38.4980 (39.3769)
2022-11-02 17:05:48,609:INFO: Dataset: univ                Batch: 16/29	Loss 38.8354 (39.3425)
2022-11-02 17:05:48,994:INFO: Dataset: univ                Batch: 17/29	Loss 38.8104 (39.3127)
2022-11-02 17:05:49,376:INFO: Dataset: univ                Batch: 18/29	Loss 38.8753 (39.2936)
2022-11-02 17:05:49,775:INFO: Dataset: univ                Batch: 19/29	Loss 39.1473 (39.2860)
2022-11-02 17:05:50,179:INFO: Dataset: univ                Batch: 20/29	Loss 39.1257 (39.2784)
2022-11-02 17:05:50,567:INFO: Dataset: univ                Batch: 21/29	Loss 38.9339 (39.2612)
2022-11-02 17:05:51,001:INFO: Dataset: univ                Batch: 22/29	Loss 39.0174 (39.2494)
2022-11-02 17:05:51,431:INFO: Dataset: univ                Batch: 23/29	Loss 38.6058 (39.2178)
2022-11-02 17:05:51,822:INFO: Dataset: univ                Batch: 24/29	Loss 38.9571 (39.2065)
2022-11-02 17:05:52,241:INFO: Dataset: univ                Batch: 25/29	Loss 38.8307 (39.1899)
2022-11-02 17:05:52,639:INFO: Dataset: univ                Batch: 26/29	Loss 38.9595 (39.1810)
2022-11-02 17:05:53,061:INFO: Dataset: univ                Batch: 27/29	Loss 38.5813 (39.1592)
2022-11-02 17:05:53,478:INFO: Dataset: univ                Batch: 28/29	Loss 39.3263 (39.1656)
2022-11-02 17:05:53,762:INFO: Dataset: univ                Batch: 29/29	Loss 1.2630 (38.7327)
2022-11-02 17:06:03,609:INFO: Dataset: zara1               Batch:  1/16	Loss 42.9817 (42.9817)
2022-11-02 17:06:04,212:INFO: Dataset: zara1               Batch:  2/16	Loss 42.9288 (42.9545)
2022-11-02 17:06:04,692:INFO: Dataset: zara1               Batch:  3/16	Loss 43.0377 (42.9820)
2022-11-02 17:06:05,119:INFO: Dataset: zara1               Batch:  4/16	Loss 43.3360 (43.0614)
2022-11-02 17:06:05,526:INFO: Dataset: zara1               Batch:  5/16	Loss 43.5349 (43.1599)
2022-11-02 17:06:05,928:INFO: Dataset: zara1               Batch:  6/16	Loss 42.8825 (43.1181)
2022-11-02 17:06:06,343:INFO: Dataset: zara1               Batch:  7/16	Loss 43.1952 (43.1289)
2022-11-02 17:06:06,735:INFO: Dataset: zara1               Batch:  8/16	Loss 42.7451 (43.0755)
2022-11-02 17:06:07,133:INFO: Dataset: zara1               Batch:  9/16	Loss 43.7909 (43.1582)
2022-11-02 17:06:07,549:INFO: Dataset: zara1               Batch: 10/16	Loss 42.9076 (43.1324)
2022-11-02 17:06:07,964:INFO: Dataset: zara1               Batch: 11/16	Loss 42.7036 (43.0951)
2022-11-02 17:06:08,373:INFO: Dataset: zara1               Batch: 12/16	Loss 42.8536 (43.0725)
2022-11-02 17:06:08,776:INFO: Dataset: zara1               Batch: 13/16	Loss 42.3254 (43.0086)
2022-11-02 17:06:09,176:INFO: Dataset: zara1               Batch: 14/16	Loss 42.8320 (42.9951)
2022-11-02 17:06:09,614:INFO: Dataset: zara1               Batch: 15/16	Loss 42.6907 (42.9752)
2022-11-02 17:06:09,991:INFO: Dataset: zara1               Batch: 16/16	Loss 23.7661 (42.1259)
2022-11-02 17:06:42,366:INFO: Dataset: zara2               Batch:  1/36	Loss 36.0560 (36.0560)
2022-11-02 17:06:42,761:INFO: Dataset: zara2               Batch:  2/36	Loss 36.0151 (36.0360)
2022-11-02 17:06:43,085:INFO: Dataset: zara2               Batch:  3/36	Loss 36.8126 (36.2908)
2022-11-02 17:06:43,414:INFO: Dataset: zara2               Batch:  4/36	Loss 36.6808 (36.3944)
2022-11-02 17:06:43,779:INFO: Dataset: zara2               Batch:  5/36	Loss 36.3050 (36.3753)
2022-11-02 17:06:44,159:INFO: Dataset: zara2               Batch:  6/36	Loss 35.9968 (36.3225)
2022-11-02 17:06:44,535:INFO: Dataset: zara2               Batch:  7/36	Loss 36.6531 (36.3645)
2022-11-02 17:06:44,941:INFO: Dataset: zara2               Batch:  8/36	Loss 36.1934 (36.3432)
2022-11-02 17:06:45,360:INFO: Dataset: zara2               Batch:  9/36	Loss 37.1086 (36.4307)
2022-11-02 17:06:45,784:INFO: Dataset: zara2               Batch: 10/36	Loss 37.3583 (36.5092)
2022-11-02 17:06:46,192:INFO: Dataset: zara2               Batch: 11/36	Loss 36.6299 (36.5200)
2022-11-02 17:06:46,568:INFO: Dataset: zara2               Batch: 12/36	Loss 36.4244 (36.5127)
2022-11-02 17:06:46,976:INFO: Dataset: zara2               Batch: 13/36	Loss 35.6817 (36.4409)
2022-11-02 17:06:47,366:INFO: Dataset: zara2               Batch: 14/36	Loss 36.0169 (36.4137)
2022-11-02 17:06:47,769:INFO: Dataset: zara2               Batch: 15/36	Loss 36.4541 (36.4162)
2022-11-02 17:06:48,149:INFO: Dataset: zara2               Batch: 16/36	Loss 36.1563 (36.3982)
2022-11-02 17:06:48,466:INFO: Dataset: zara2               Batch: 17/36	Loss 35.8695 (36.3622)
2022-11-02 17:06:48,776:INFO: Dataset: zara2               Batch: 18/36	Loss 35.9103 (36.3364)
2022-11-02 17:06:49,097:INFO: Dataset: zara2               Batch: 19/36	Loss 36.3427 (36.3368)
2022-11-02 17:06:49,646:INFO: Dataset: zara2               Batch: 20/36	Loss 36.2055 (36.3304)
2022-11-02 17:06:50,132:INFO: Dataset: zara2               Batch: 21/36	Loss 35.3022 (36.2839)
2022-11-02 17:06:50,686:INFO: Dataset: zara2               Batch: 22/36	Loss 35.5141 (36.2464)
2022-11-02 17:06:51,093:INFO: Dataset: zara2               Batch: 23/36	Loss 35.5461 (36.2139)
2022-11-02 17:06:51,444:INFO: Dataset: zara2               Batch: 24/36	Loss 35.9753 (36.2038)
2022-11-02 17:06:51,785:INFO: Dataset: zara2               Batch: 25/36	Loss 35.6503 (36.1814)
2022-11-02 17:06:52,167:INFO: Dataset: zara2               Batch: 26/36	Loss 35.3585 (36.1502)
2022-11-02 17:06:52,589:INFO: Dataset: zara2               Batch: 27/36	Loss 35.5202 (36.1268)
2022-11-02 17:06:52,931:INFO: Dataset: zara2               Batch: 28/36	Loss 36.1159 (36.1265)
2022-11-02 17:06:53,277:INFO: Dataset: zara2               Batch: 29/36	Loss 35.3546 (36.0969)
2022-11-02 17:06:53,613:INFO: Dataset: zara2               Batch: 30/36	Loss 36.0775 (36.0962)
2022-11-02 17:06:53,970:INFO: Dataset: zara2               Batch: 31/36	Loss 35.5419 (36.0769)
2022-11-02 17:06:54,275:INFO: Dataset: zara2               Batch: 32/36	Loss 35.4925 (36.0577)
2022-11-02 17:06:54,622:INFO: Dataset: zara2               Batch: 33/36	Loss 36.1289 (36.0599)
2022-11-02 17:06:55,042:INFO: Dataset: zara2               Batch: 34/36	Loss 35.6959 (36.0463)
2022-11-02 17:06:55,382:INFO: Dataset: zara2               Batch: 35/36	Loss 35.5232 (36.0314)
2022-11-02 17:06:55,682:INFO: Dataset: zara2               Batch: 36/36	Loss 20.1762 (35.7116)
2022-11-02 17:06:56,782:INFO: - Computing ADE (validation o)
2022-11-02 17:07:08,909:INFO: 		 ADE on eth                       dataset:	 1.2411315441131592
2022-11-02 17:07:08,909:INFO: Average validation o:	ADE  1.2411	FDE  1.9922
2022-11-02 17:07:08,911:INFO: - Computing ADE (validation)
2022-11-02 17:07:19,845:INFO: 		 ADE on hotel                     dataset:	 0.977724015712738
2022-11-02 17:07:32,491:INFO: 		 ADE on univ                      dataset:	 0.9006642699241638
2022-11-02 17:07:44,312:INFO: 		 ADE on zara1                     dataset:	 0.8246177434921265
2022-11-02 17:07:56,408:INFO: 		 ADE on zara2                     dataset:	 0.5958322882652283
2022-11-02 17:07:56,408:INFO: Average validation:	ADE  0.7887	FDE  1.3994
2022-11-02 17:07:56,409:INFO: - Computing ADE (training)
2022-11-02 17:08:08,349:INFO: 		 ADE on hotel                     dataset:	 1.2824805974960327
2022-11-02 17:08:40,914:INFO: 		 ADE on univ                      dataset:	 0.8363659977912903
2022-11-02 17:08:53,854:INFO: 		 ADE on zara1                     dataset:	 0.8837331533432007
2022-11-02 17:09:27,502:INFO: 		 ADE on zara2                     dataset:	 0.661436915397644
2022-11-02 17:09:27,504:INFO: Average training:	ADE  0.8152	FDE  1.4492
2022-11-02 17:09:27,527:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_555.pth.tar
2022-11-02 17:09:27,527:INFO: 
===> EPOCH: 556 (P4)
2022-11-02 17:09:27,528:INFO: - Computing loss (training)
2022-11-02 17:09:36,322:INFO: Dataset: hotel               Batch: 1/8	Loss 44.2257 (44.2257)
2022-11-02 17:09:36,929:INFO: Dataset: hotel               Batch: 2/8	Loss 45.5739 (44.9191)
2022-11-02 17:09:37,467:INFO: Dataset: hotel               Batch: 3/8	Loss 44.5902 (44.8101)
2022-11-02 17:09:37,929:INFO: Dataset: hotel               Batch: 4/8	Loss 44.5501 (44.7436)
2022-11-02 17:09:38,279:INFO: Dataset: hotel               Batch: 5/8	Loss 44.1536 (44.6251)
2022-11-02 17:09:38,629:INFO: Dataset: hotel               Batch: 6/8	Loss 44.1328 (44.5441)
2022-11-02 17:09:38,968:INFO: Dataset: hotel               Batch: 7/8	Loss 44.5417 (44.5438)
2022-11-02 17:09:39,174:INFO: Dataset: hotel               Batch: 8/8	Loss -9.2455 (42.8407)
2022-11-02 17:10:08,911:INFO: Dataset: univ                Batch:  1/29	Loss 39.3123 (39.3123)
2022-11-02 17:10:09,256:INFO: Dataset: univ                Batch:  2/29	Loss 40.5781 (39.8817)
2022-11-02 17:10:09,605:INFO: Dataset: univ                Batch:  3/29	Loss 39.2371 (39.6666)
2022-11-02 17:10:09,985:INFO: Dataset: univ                Batch:  4/29	Loss 40.0570 (39.7533)
2022-11-02 17:10:10,334:INFO: Dataset: univ                Batch:  5/29	Loss 39.3822 (39.6781)
2022-11-02 17:10:10,658:INFO: Dataset: univ                Batch:  6/29	Loss 39.2769 (39.6066)
2022-11-02 17:10:10,994:INFO: Dataset: univ                Batch:  7/29	Loss 39.5849 (39.6032)
2022-11-02 17:10:11,306:INFO: Dataset: univ                Batch:  8/29	Loss 39.2728 (39.5567)
2022-11-02 17:10:11,642:INFO: Dataset: univ                Batch:  9/29	Loss 39.4309 (39.5439)
2022-11-02 17:10:12,001:INFO: Dataset: univ                Batch: 10/29	Loss 39.1552 (39.5037)
2022-11-02 17:10:12,343:INFO: Dataset: univ                Batch: 11/29	Loss 38.6143 (39.4174)
2022-11-02 17:10:12,674:INFO: Dataset: univ                Batch: 12/29	Loss 38.5201 (39.3369)
2022-11-02 17:10:13,022:INFO: Dataset: univ                Batch: 13/29	Loss 38.8958 (39.3046)
2022-11-02 17:10:13,357:INFO: Dataset: univ                Batch: 14/29	Loss 38.4723 (39.2470)
2022-11-02 17:10:13,681:INFO: Dataset: univ                Batch: 15/29	Loss 38.1850 (39.1785)
2022-11-02 17:10:14,019:INFO: Dataset: univ                Batch: 16/29	Loss 39.4427 (39.1927)
2022-11-02 17:10:14,354:INFO: Dataset: univ                Batch: 17/29	Loss 38.2908 (39.1415)
2022-11-02 17:10:14,702:INFO: Dataset: univ                Batch: 18/29	Loss 37.8900 (39.0616)
2022-11-02 17:10:15,033:INFO: Dataset: univ                Batch: 19/29	Loss 38.7831 (39.0499)
2022-11-02 17:10:15,367:INFO: Dataset: univ                Batch: 20/29	Loss 38.6018 (39.0277)
2022-11-02 17:10:15,711:INFO: Dataset: univ                Batch: 21/29	Loss 38.2871 (38.9894)
2022-11-02 17:10:16,079:INFO: Dataset: univ                Batch: 22/29	Loss 38.8473 (38.9837)
2022-11-02 17:10:16,465:INFO: Dataset: univ                Batch: 23/29	Loss 38.0112 (38.9423)
2022-11-02 17:10:16,842:INFO: Dataset: univ                Batch: 24/29	Loss 38.2441 (38.9118)
2022-11-02 17:10:17,202:INFO: Dataset: univ                Batch: 25/29	Loss 38.1401 (38.8790)
2022-11-02 17:10:17,548:INFO: Dataset: univ                Batch: 26/29	Loss 37.8026 (38.8383)
2022-11-02 17:10:17,895:INFO: Dataset: univ                Batch: 27/29	Loss 38.5809 (38.8301)
2022-11-02 17:10:18,254:INFO: Dataset: univ                Batch: 28/29	Loss 37.9189 (38.7967)
2022-11-02 17:10:18,504:INFO: Dataset: univ                Batch: 29/29	Loss 0.4908 (38.2213)
2022-11-02 17:10:28,370:INFO: Dataset: zara1               Batch:  1/16	Loss 42.4716 (42.4716)
2022-11-02 17:10:29,002:INFO: Dataset: zara1               Batch:  2/16	Loss 42.4734 (42.4726)
2022-11-02 17:10:29,552:INFO: Dataset: zara1               Batch:  3/16	Loss 42.4628 (42.4691)
2022-11-02 17:10:30,005:INFO: Dataset: zara1               Batch:  4/16	Loss 42.6808 (42.5153)
2022-11-02 17:10:30,391:INFO: Dataset: zara1               Batch:  5/16	Loss 42.6702 (42.5488)
2022-11-02 17:10:30,781:INFO: Dataset: zara1               Batch:  6/16	Loss 42.4179 (42.5286)
2022-11-02 17:10:31,181:INFO: Dataset: zara1               Batch:  7/16	Loss 42.5140 (42.5263)
2022-11-02 17:10:31,542:INFO: Dataset: zara1               Batch:  8/16	Loss 42.2528 (42.4960)
2022-11-02 17:10:31,884:INFO: Dataset: zara1               Batch:  9/16	Loss 42.5073 (42.4972)
2022-11-02 17:10:32,262:INFO: Dataset: zara1               Batch: 10/16	Loss 42.5061 (42.4981)
2022-11-02 17:10:32,679:INFO: Dataset: zara1               Batch: 11/16	Loss 41.9857 (42.4496)
2022-11-02 17:10:32,995:INFO: Dataset: zara1               Batch: 12/16	Loss 42.1376 (42.4245)
2022-11-02 17:10:33,318:INFO: Dataset: zara1               Batch: 13/16	Loss 42.3775 (42.4216)
2022-11-02 17:10:33,629:INFO: Dataset: zara1               Batch: 14/16	Loss 41.9699 (42.3870)
2022-11-02 17:10:33,962:INFO: Dataset: zara1               Batch: 15/16	Loss 42.0033 (42.3623)
2022-11-02 17:10:34,331:INFO: Dataset: zara1               Batch: 16/16	Loss 23.4258 (41.5351)
2022-11-02 17:11:04,192:INFO: Dataset: zara2               Batch:  1/36	Loss 35.6865 (35.6865)
2022-11-02 17:11:04,510:INFO: Dataset: zara2               Batch:  2/36	Loss 36.1167 (35.9049)
2022-11-02 17:11:04,830:INFO: Dataset: zara2               Batch:  3/36	Loss 36.7874 (36.1774)
2022-11-02 17:11:05,138:INFO: Dataset: zara2               Batch:  4/36	Loss 35.8139 (36.0749)
2022-11-02 17:11:05,448:INFO: Dataset: zara2               Batch:  5/36	Loss 36.1505 (36.0910)
2022-11-02 17:11:05,766:INFO: Dataset: zara2               Batch:  6/36	Loss 36.2712 (36.1210)
2022-11-02 17:11:06,120:INFO: Dataset: zara2               Batch:  7/36	Loss 35.8053 (36.0764)
2022-11-02 17:11:06,460:INFO: Dataset: zara2               Batch:  8/36	Loss 36.1363 (36.0847)
2022-11-02 17:11:06,815:INFO: Dataset: zara2               Batch:  9/36	Loss 35.4857 (36.0239)
2022-11-02 17:11:07,185:INFO: Dataset: zara2               Batch: 10/36	Loss 35.6178 (35.9819)
2022-11-02 17:11:07,570:INFO: Dataset: zara2               Batch: 11/36	Loss 35.8257 (35.9688)
2022-11-02 17:11:07,923:INFO: Dataset: zara2               Batch: 12/36	Loss 35.8623 (35.9609)
2022-11-02 17:11:08,277:INFO: Dataset: zara2               Batch: 13/36	Loss 35.8552 (35.9532)
2022-11-02 17:11:08,679:INFO: Dataset: zara2               Batch: 14/36	Loss 35.5535 (35.9230)
2022-11-02 17:11:09,065:INFO: Dataset: zara2               Batch: 15/36	Loss 35.4515 (35.8864)
2022-11-02 17:11:09,463:INFO: Dataset: zara2               Batch: 16/36	Loss 35.6152 (35.8714)
2022-11-02 17:11:09,854:INFO: Dataset: zara2               Batch: 17/36	Loss 35.6385 (35.8573)
2022-11-02 17:11:10,249:INFO: Dataset: zara2               Batch: 18/36	Loss 35.7646 (35.8518)
2022-11-02 17:11:10,613:INFO: Dataset: zara2               Batch: 19/36	Loss 35.4729 (35.8307)
2022-11-02 17:11:10,997:INFO: Dataset: zara2               Batch: 20/36	Loss 35.3058 (35.8035)
2022-11-02 17:11:11,412:INFO: Dataset: zara2               Batch: 21/36	Loss 35.4880 (35.7880)
2022-11-02 17:11:12,188:INFO: Dataset: zara2               Batch: 22/36	Loss 35.1347 (35.7565)
2022-11-02 17:11:12,760:INFO: Dataset: zara2               Batch: 23/36	Loss 35.2276 (35.7346)
2022-11-02 17:11:13,102:INFO: Dataset: zara2               Batch: 24/36	Loss 35.3468 (35.7189)
2022-11-02 17:11:13,451:INFO: Dataset: zara2               Batch: 25/36	Loss 35.1866 (35.6973)
2022-11-02 17:11:13,792:INFO: Dataset: zara2               Batch: 26/36	Loss 35.3197 (35.6819)
2022-11-02 17:11:14,146:INFO: Dataset: zara2               Batch: 27/36	Loss 34.9586 (35.6554)
2022-11-02 17:11:14,472:INFO: Dataset: zara2               Batch: 28/36	Loss 35.3482 (35.6444)
2022-11-02 17:11:14,797:INFO: Dataset: zara2               Batch: 29/36	Loss 35.0334 (35.6205)
2022-11-02 17:11:15,121:INFO: Dataset: zara2               Batch: 30/36	Loss 34.9822 (35.5994)
2022-11-02 17:11:15,483:INFO: Dataset: zara2               Batch: 31/36	Loss 34.9225 (35.5768)
2022-11-02 17:11:15,795:INFO: Dataset: zara2               Batch: 32/36	Loss 34.7669 (35.5543)
2022-11-02 17:11:16,136:INFO: Dataset: zara2               Batch: 33/36	Loss 35.2096 (35.5446)
2022-11-02 17:11:16,495:INFO: Dataset: zara2               Batch: 34/36	Loss 34.4485 (35.5082)
2022-11-02 17:11:16,877:INFO: Dataset: zara2               Batch: 35/36	Loss 35.1709 (35.4994)
2022-11-02 17:11:17,197:INFO: Dataset: zara2               Batch: 36/36	Loss 19.7156 (35.2098)
2022-11-02 17:11:18,470:INFO: - Computing ADE (validation o)
2022-11-02 17:11:30,876:INFO: 		 ADE on eth                       dataset:	 1.2127255201339722
2022-11-02 17:11:30,876:INFO: Average validation o:	ADE  1.2127	FDE  2.0343
2022-11-02 17:11:30,877:INFO: - Computing ADE (validation)
2022-11-02 17:11:41,713:INFO: 		 ADE on hotel                     dataset:	 0.9685025811195374
2022-11-02 17:11:52,763:INFO: 		 ADE on univ                      dataset:	 0.8877238631248474
2022-11-02 17:12:03,925:INFO: 		 ADE on zara1                     dataset:	 0.851294994354248
2022-11-02 17:12:14,835:INFO: 		 ADE on zara2                     dataset:	 0.5999358892440796
2022-11-02 17:12:14,835:INFO: Average validation:	ADE  0.7845	FDE  1.4498
2022-11-02 17:12:14,836:INFO: - Computing ADE (training)
2022-11-02 17:12:26,045:INFO: 		 ADE on hotel                     dataset:	 1.2634034156799316
2022-11-02 17:12:57,856:INFO: 		 ADE on univ                      dataset:	 0.8316760063171387
2022-11-02 17:13:10,305:INFO: 		 ADE on zara1                     dataset:	 0.8697845935821533
2022-11-02 17:13:41,488:INFO: 		 ADE on zara2                     dataset:	 0.6515472531318665
2022-11-02 17:13:41,489:INFO: Average training:	ADE  0.8085	FDE  1.4933
2022-11-02 17:13:41,504:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_556.pth.tar
2022-11-02 17:13:41,504:INFO: 
===> EPOCH: 557 (P4)
2022-11-02 17:13:41,505:INFO: - Computing loss (training)
2022-11-02 17:13:49,931:INFO: Dataset: hotel               Batch: 1/8	Loss 45.6545 (45.6545)
2022-11-02 17:13:50,922:INFO: Dataset: hotel               Batch: 2/8	Loss 44.0435 (44.8874)
2022-11-02 17:13:51,550:INFO: Dataset: hotel               Batch: 3/8	Loss 44.4424 (44.7285)
2022-11-02 17:13:52,045:INFO: Dataset: hotel               Batch: 4/8	Loss 43.5666 (44.3957)
2022-11-02 17:13:52,383:INFO: Dataset: hotel               Batch: 5/8	Loss 45.4247 (44.5951)
2022-11-02 17:13:52,736:INFO: Dataset: hotel               Batch: 6/8	Loss 44.0660 (44.4972)
2022-11-02 17:13:53,113:INFO: Dataset: hotel               Batch: 7/8	Loss 44.2765 (44.4653)
2022-11-02 17:13:53,349:INFO: Dataset: hotel               Batch: 8/8	Loss -9.8054 (42.6753)
2022-11-02 17:14:25,127:INFO: Dataset: univ                Batch:  1/29	Loss 39.3451 (39.3451)
2022-11-02 17:14:25,573:INFO: Dataset: univ                Batch:  2/29	Loss 38.7179 (39.0159)
2022-11-02 17:14:25,999:INFO: Dataset: univ                Batch:  3/29	Loss 39.1036 (39.0440)
2022-11-02 17:14:26,373:INFO: Dataset: univ                Batch:  4/29	Loss 38.7563 (38.9686)
2022-11-02 17:14:26,820:INFO: Dataset: univ                Batch:  5/29	Loss 38.7709 (38.9293)
2022-11-02 17:14:27,295:INFO: Dataset: univ                Batch:  6/29	Loss 38.7854 (38.9065)
2022-11-02 17:14:27,715:INFO: Dataset: univ                Batch:  7/29	Loss 38.9422 (38.9113)
2022-11-02 17:14:28,181:INFO: Dataset: univ                Batch:  8/29	Loss 38.7085 (38.8877)
2022-11-02 17:14:28,582:INFO: Dataset: univ                Batch:  9/29	Loss 38.4526 (38.8423)
2022-11-02 17:14:28,977:INFO: Dataset: univ                Batch: 10/29	Loss 38.9740 (38.8532)
2022-11-02 17:14:29,405:INFO: Dataset: univ                Batch: 11/29	Loss 38.3207 (38.8047)
2022-11-02 17:14:29,913:INFO: Dataset: univ                Batch: 12/29	Loss 37.8331 (38.6998)
2022-11-02 17:14:30,349:INFO: Dataset: univ                Batch: 13/29	Loss 39.0928 (38.7237)
2022-11-02 17:14:30,755:INFO: Dataset: univ                Batch: 14/29	Loss 38.2446 (38.6880)
2022-11-02 17:14:31,613:INFO: Dataset: univ                Batch: 15/29	Loss 37.6720 (38.6107)
2022-11-02 17:14:32,200:INFO: Dataset: univ                Batch: 16/29	Loss 38.9712 (38.6304)
2022-11-02 17:14:32,590:INFO: Dataset: univ                Batch: 17/29	Loss 38.2598 (38.6063)
2022-11-02 17:14:32,998:INFO: Dataset: univ                Batch: 18/29	Loss 38.0166 (38.5727)
2022-11-02 17:14:33,400:INFO: Dataset: univ                Batch: 19/29	Loss 37.9639 (38.5413)
2022-11-02 17:14:33,808:INFO: Dataset: univ                Batch: 20/29	Loss 38.4992 (38.5395)
2022-11-02 17:14:34,245:INFO: Dataset: univ                Batch: 21/29	Loss 38.2140 (38.5262)
2022-11-02 17:14:34,674:INFO: Dataset: univ                Batch: 22/29	Loss 37.7957 (38.4943)
2022-11-02 17:14:35,128:INFO: Dataset: univ                Batch: 23/29	Loss 37.8577 (38.4676)
2022-11-02 17:14:35,522:INFO: Dataset: univ                Batch: 24/29	Loss 38.5266 (38.4697)
2022-11-02 17:14:35,896:INFO: Dataset: univ                Batch: 25/29	Loss 38.3043 (38.4642)
2022-11-02 17:14:36,233:INFO: Dataset: univ                Batch: 26/29	Loss 37.4416 (38.4171)
2022-11-02 17:14:36,603:INFO: Dataset: univ                Batch: 27/29	Loss 37.6747 (38.3841)
2022-11-02 17:14:36,947:INFO: Dataset: univ                Batch: 28/29	Loss 37.6206 (38.3536)
2022-11-02 17:14:37,238:INFO: Dataset: univ                Batch: 29/29	Loss 0.4028 (37.8824)
2022-11-02 17:14:46,991:INFO: Dataset: zara1               Batch:  1/16	Loss 41.9409 (41.9409)
2022-11-02 17:14:47,571:INFO: Dataset: zara1               Batch:  2/16	Loss 41.7270 (41.8229)
2022-11-02 17:14:48,109:INFO: Dataset: zara1               Batch:  3/16	Loss 41.8583 (41.8342)
2022-11-02 17:14:48,541:INFO: Dataset: zara1               Batch:  4/16	Loss 41.7029 (41.7975)
2022-11-02 17:14:48,911:INFO: Dataset: zara1               Batch:  5/16	Loss 41.8962 (41.8164)
2022-11-02 17:14:49,320:INFO: Dataset: zara1               Batch:  6/16	Loss 41.7218 (41.8002)
2022-11-02 17:14:50,602:INFO: Dataset: zara1               Batch:  7/16	Loss 41.7776 (41.7966)
2022-11-02 17:14:51,502:INFO: Dataset: zara1               Batch:  8/16	Loss 41.7275 (41.7882)
2022-11-02 17:14:51,901:INFO: Dataset: zara1               Batch:  9/16	Loss 41.3612 (41.7415)
2022-11-02 17:14:52,236:INFO: Dataset: zara1               Batch: 10/16	Loss 41.5980 (41.7269)
2022-11-02 17:14:52,566:INFO: Dataset: zara1               Batch: 11/16	Loss 41.2038 (41.6843)
2022-11-02 17:14:52,901:INFO: Dataset: zara1               Batch: 12/16	Loss 41.7513 (41.6894)
2022-11-02 17:14:53,220:INFO: Dataset: zara1               Batch: 13/16	Loss 41.0887 (41.6435)
2022-11-02 17:14:53,533:INFO: Dataset: zara1               Batch: 14/16	Loss 41.4490 (41.6301)
2022-11-02 17:14:54,020:INFO: Dataset: zara1               Batch: 15/16	Loss 41.1450 (41.5969)
2022-11-02 17:14:54,355:INFO: Dataset: zara1               Batch: 16/16	Loss 23.0018 (40.8629)
2022-11-02 17:15:24,656:INFO: Dataset: zara2               Batch:  1/36	Loss 35.4929 (35.4929)
2022-11-02 17:15:25,053:INFO: Dataset: zara2               Batch:  2/36	Loss 34.9797 (35.2453)
2022-11-02 17:15:25,452:INFO: Dataset: zara2               Batch:  3/36	Loss 35.6036 (35.3694)
2022-11-02 17:15:25,846:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1615 (35.3196)
2022-11-02 17:15:26,247:INFO: Dataset: zara2               Batch:  5/36	Loss 35.5974 (35.3727)
2022-11-02 17:15:26,658:INFO: Dataset: zara2               Batch:  6/36	Loss 34.6443 (35.2294)
2022-11-02 17:15:27,071:INFO: Dataset: zara2               Batch:  7/36	Loss 35.2006 (35.2252)
2022-11-02 17:15:27,466:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1399 (35.2138)
2022-11-02 17:15:27,840:INFO: Dataset: zara2               Batch:  9/36	Loss 34.8327 (35.1713)
2022-11-02 17:15:28,242:INFO: Dataset: zara2               Batch: 10/36	Loss 35.2905 (35.1822)
2022-11-02 17:15:28,640:INFO: Dataset: zara2               Batch: 11/36	Loss 34.8872 (35.1555)
2022-11-02 17:15:29,052:INFO: Dataset: zara2               Batch: 12/36	Loss 35.0059 (35.1427)
2022-11-02 17:15:29,475:INFO: Dataset: zara2               Batch: 13/36	Loss 34.9388 (35.1271)
2022-11-02 17:15:29,895:INFO: Dataset: zara2               Batch: 14/36	Loss 35.2089 (35.1333)
2022-11-02 17:15:30,294:INFO: Dataset: zara2               Batch: 15/36	Loss 34.9838 (35.1222)
2022-11-02 17:15:30,717:INFO: Dataset: zara2               Batch: 16/36	Loss 34.4627 (35.0800)
2022-11-02 17:15:31,128:INFO: Dataset: zara2               Batch: 17/36	Loss 34.9161 (35.0685)
2022-11-02 17:15:31,542:INFO: Dataset: zara2               Batch: 18/36	Loss 34.4731 (35.0319)
2022-11-02 17:15:31,984:INFO: Dataset: zara2               Batch: 19/36	Loss 34.7156 (35.0167)
2022-11-02 17:15:32,372:INFO: Dataset: zara2               Batch: 20/36	Loss 34.2032 (34.9781)
2022-11-02 17:15:32,779:INFO: Dataset: zara2               Batch: 21/36	Loss 34.4162 (34.9509)
2022-11-02 17:15:33,186:INFO: Dataset: zara2               Batch: 22/36	Loss 34.4330 (34.9249)
2022-11-02 17:15:33,578:INFO: Dataset: zara2               Batch: 23/36	Loss 34.3162 (34.9003)
2022-11-02 17:15:33,985:INFO: Dataset: zara2               Batch: 24/36	Loss 34.2862 (34.8768)
2022-11-02 17:15:34,432:INFO: Dataset: zara2               Batch: 25/36	Loss 34.5126 (34.8603)
2022-11-02 17:15:34,839:INFO: Dataset: zara2               Batch: 26/36	Loss 34.2071 (34.8343)
2022-11-02 17:15:35,261:INFO: Dataset: zara2               Batch: 27/36	Loss 34.2911 (34.8124)
2022-11-02 17:15:35,687:INFO: Dataset: zara2               Batch: 28/36	Loss 34.6752 (34.8071)
2022-11-02 17:15:36,083:INFO: Dataset: zara2               Batch: 29/36	Loss 34.8425 (34.8082)
2022-11-02 17:15:36,514:INFO: Dataset: zara2               Batch: 30/36	Loss 34.2949 (34.7906)
2022-11-02 17:15:36,939:INFO: Dataset: zara2               Batch: 31/36	Loss 34.4924 (34.7800)
2022-11-02 17:15:37,352:INFO: Dataset: zara2               Batch: 32/36	Loss 34.4292 (34.7702)
2022-11-02 17:15:37,755:INFO: Dataset: zara2               Batch: 33/36	Loss 34.0375 (34.7490)
2022-11-02 17:15:38,147:INFO: Dataset: zara2               Batch: 34/36	Loss 34.6100 (34.7448)
2022-11-02 17:15:38,578:INFO: Dataset: zara2               Batch: 35/36	Loss 34.0778 (34.7237)
2022-11-02 17:15:38,917:INFO: Dataset: zara2               Batch: 36/36	Loss 18.8911 (34.4829)
2022-11-02 17:15:40,009:INFO: - Computing ADE (validation o)
2022-11-02 17:15:51,092:INFO: 		 ADE on eth                       dataset:	 1.1508713960647583
2022-11-02 17:15:51,092:INFO: Average validation o:	ADE  1.1509	FDE  2.0227
2022-11-02 17:15:51,094:INFO: - Computing ADE (validation)
2022-11-02 17:16:02,154:INFO: 		 ADE on hotel                     dataset:	 0.907736599445343
2022-11-02 17:16:13,336:INFO: 		 ADE on univ                      dataset:	 0.8273899555206299
2022-11-02 17:16:24,487:INFO: 		 ADE on zara1                     dataset:	 0.6982758045196533
2022-11-02 17:16:36,492:INFO: 		 ADE on zara2                     dataset:	 0.5402110815048218
2022-11-02 17:16:36,494:INFO: Average validation:	ADE  0.7189	FDE  1.3806
2022-11-02 17:16:36,495:INFO: - Computing ADE (training)
2022-11-02 17:16:47,106:INFO: 		 ADE on hotel                     dataset:	 1.1700457334518433
2022-11-02 17:17:16,559:INFO: 		 ADE on univ                      dataset:	 0.769086480140686
2022-11-02 17:17:27,916:INFO: 		 ADE on zara1                     dataset:	 0.746425986289978
2022-11-02 17:17:58,748:INFO: 		 ADE on zara2                     dataset:	 0.5749417543411255
2022-11-02 17:17:58,749:INFO: Average training:	ADE  0.7384	FDE  1.4144
2022-11-02 17:17:58,773:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 400)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_557.pth.tar
2022-11-02 17:17:58,773:INFO: 
===> EPOCH: 558 (P4)
2022-11-02 17:17:58,774:INFO: - Computing loss (training)
2022-11-02 17:18:08,814:INFO: Dataset: hotel               Batch: 1/8	Loss 44.2510 (44.2510)
2022-11-02 17:18:09,437:INFO: Dataset: hotel               Batch: 2/8	Loss 44.4475 (44.3465)
2022-11-02 17:18:09,901:INFO: Dataset: hotel               Batch: 3/8	Loss 45.3492 (44.6679)
2022-11-02 17:18:10,233:INFO: Dataset: hotel               Batch: 4/8	Loss 44.8107 (44.7025)
2022-11-02 17:18:10,705:INFO: Dataset: hotel               Batch: 5/8	Loss 43.1910 (44.3727)
2022-11-02 17:18:11,218:INFO: Dataset: hotel               Batch: 6/8	Loss 44.0802 (44.3229)
2022-11-02 17:18:11,651:INFO: Dataset: hotel               Batch: 7/8	Loss 43.6549 (44.2352)
2022-11-02 17:18:11,876:INFO: Dataset: hotel               Batch: 8/8	Loss -9.5993 (42.3176)
2022-11-02 17:18:41,549:INFO: Dataset: univ                Batch:  1/29	Loss 38.5511 (38.5511)
2022-11-02 17:18:41,954:INFO: Dataset: univ                Batch:  2/29	Loss 38.4771 (38.5172)

2022-11-09 17:43:28,220:INFO: Initializing Training Set
2022-11-09 17:43:29,771:INFO: Initializing Validation Set
2022-11-09 17:43:29,952:INFO: Initializing Validation O Set
2022-11-09 17:43:31,361:INFO: 
===> EPOCH: 1 (P1)
2022-11-09 17:43:31,361:INFO: - Computing loss (training)
2022-11-09 17:43:32,020:INFO: Dataset: hotel               Batch: 1/4	Loss 17.1459 (17.1459)
2022-11-09 17:43:32,045:INFO: Dataset: hotel               Batch: 2/4	Loss 15.9239 (16.5619)
2022-11-09 17:43:32,071:INFO: Dataset: hotel               Batch: 3/4	Loss 15.2851 (16.1449)
2022-11-09 17:43:32,087:INFO: Dataset: hotel               Batch: 4/4	Loss 9.1661 (15.0585)
2022-11-09 17:43:32,331:INFO: Dataset: univ                Batch:  1/15	Loss 15.6488 (15.6488)
2022-11-09 17:43:32,364:INFO: Dataset: univ                Batch:  2/15	Loss 14.1666 (14.8284)
2022-11-09 17:43:32,389:INFO: Dataset: univ                Batch:  3/15	Loss 14.7030 (14.7868)
2022-11-09 17:43:32,415:INFO: Dataset: univ                Batch:  4/15	Loss 13.7346 (14.5232)
2022-11-09 17:43:32,442:INFO: Dataset: univ                Batch:  5/15	Loss 16.3866 (14.9019)
2022-11-09 17:43:32,470:INFO: Dataset: univ                Batch:  6/15	Loss 14.0757 (14.7590)
2022-11-09 17:43:32,495:INFO: Dataset: univ                Batch:  7/15	Loss 13.0016 (14.5080)
2022-11-09 17:43:32,520:INFO: Dataset: univ                Batch:  8/15	Loss 14.2818 (14.4812)
2022-11-09 17:43:32,546:INFO: Dataset: univ                Batch:  9/15	Loss 12.5835 (14.2758)
2022-11-09 17:43:32,573:INFO: Dataset: univ                Batch: 10/15	Loss 14.6395 (14.3075)
2022-11-09 17:43:32,601:INFO: Dataset: univ                Batch: 11/15	Loss 13.0878 (14.1996)
2022-11-09 17:43:32,627:INFO: Dataset: univ                Batch: 12/15	Loss 10.6986 (13.8886)
2022-11-09 17:43:32,652:INFO: Dataset: univ                Batch: 13/15	Loss 14.0103 (13.8975)
2022-11-09 17:43:32,677:INFO: Dataset: univ                Batch: 14/15	Loss 11.6172 (13.7287)
2022-11-09 17:43:32,687:INFO: Dataset: univ                Batch: 15/15	Loss 2.3548 (13.5783)
2022-11-09 17:43:32,941:INFO: Dataset: zara1               Batch: 1/8	Loss 19.2352 (19.2352)
2022-11-09 17:43:32,965:INFO: Dataset: zara1               Batch: 2/8	Loss 18.8836 (19.0336)
2022-11-09 17:43:32,988:INFO: Dataset: zara1               Batch: 3/8	Loss 20.6083 (19.5422)
2022-11-09 17:43:33,013:INFO: Dataset: zara1               Batch: 4/8	Loss 17.5908 (19.0502)
2022-11-09 17:43:33,037:INFO: Dataset: zara1               Batch: 5/8	Loss 18.5921 (18.9570)
2022-11-09 17:43:33,063:INFO: Dataset: zara1               Batch: 6/8	Loss 17.8420 (18.7692)
2022-11-09 17:43:33,087:INFO: Dataset: zara1               Batch: 7/8	Loss 17.9860 (18.6674)
2022-11-09 17:43:33,108:INFO: Dataset: zara1               Batch: 8/8	Loss 13.7847 (17.9941)
2022-11-09 17:43:33,378:INFO: Dataset: zara2               Batch:  1/18	Loss 13.0964 (13.0964)
2022-11-09 17:43:33,404:INFO: Dataset: zara2               Batch:  2/18	Loss 14.7783 (13.9275)
2022-11-09 17:43:33,429:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1924 (14.0135)
2022-11-09 17:43:33,455:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9031 (13.9855)
2022-11-09 17:43:33,480:INFO: Dataset: zara2               Batch:  5/18	Loss 11.4517 (13.4698)
2022-11-09 17:43:33,508:INFO: Dataset: zara2               Batch:  6/18	Loss 14.1178 (13.5727)
2022-11-09 17:43:33,531:INFO: Dataset: zara2               Batch:  7/18	Loss 13.9607 (13.6292)
2022-11-09 17:43:33,555:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7391 (13.6415)
2022-11-09 17:43:33,578:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4938 (13.6245)
2022-11-09 17:43:33,603:INFO: Dataset: zara2               Batch: 10/18	Loss 14.0644 (13.6657)
2022-11-09 17:43:33,628:INFO: Dataset: zara2               Batch: 11/18	Loss 11.1142 (13.4363)
2022-11-09 17:43:33,654:INFO: Dataset: zara2               Batch: 12/18	Loss 13.5035 (13.4414)
2022-11-09 17:43:33,677:INFO: Dataset: zara2               Batch: 13/18	Loss 12.0269 (13.3268)
2022-11-09 17:43:33,701:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5819 (13.2725)
2022-11-09 17:43:33,726:INFO: Dataset: zara2               Batch: 15/18	Loss 12.6301 (13.2296)
2022-11-09 17:43:33,752:INFO: Dataset: zara2               Batch: 16/18	Loss 11.3795 (13.0933)
2022-11-09 17:43:33,778:INFO: Dataset: zara2               Batch: 17/18	Loss 10.5627 (12.9250)
2022-11-09 17:43:33,802:INFO: Dataset: zara2               Batch: 18/18	Loss 8.9191 (12.7337)
2022-11-09 17:43:33,852:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_1.pth.tar
2022-11-09 17:43:33,852:INFO: 
===> EPOCH: 2 (P1)
2022-11-09 17:43:33,853:INFO: - Computing loss (training)
2022-11-09 17:43:34,057:INFO: Dataset: hotel               Batch: 1/4	Loss 10.9491 (10.9491)
2022-11-09 17:43:34,081:INFO: Dataset: hotel               Batch: 2/4	Loss 13.4093 (12.1485)
2022-11-09 17:43:34,106:INFO: Dataset: hotel               Batch: 3/4	Loss 11.1479 (11.7914)
2022-11-09 17:43:34,124:INFO: Dataset: hotel               Batch: 4/4	Loss 7.2344 (10.9737)
2022-11-09 17:43:34,425:INFO: Dataset: univ                Batch:  1/15	Loss 9.9313 (9.9313)
2022-11-09 17:43:34,453:INFO: Dataset: univ                Batch:  2/15	Loss 10.9358 (10.4147)
2022-11-09 17:43:34,479:INFO: Dataset: univ                Batch:  3/15	Loss 10.6457 (10.4909)
2022-11-09 17:43:34,504:INFO: Dataset: univ                Batch:  4/15	Loss 11.4082 (10.7243)
2022-11-09 17:43:34,529:INFO: Dataset: univ                Batch:  5/15	Loss 10.3548 (10.6444)
2022-11-09 17:43:34,557:INFO: Dataset: univ                Batch:  6/15	Loss 10.3483 (10.5915)
2022-11-09 17:43:34,583:INFO: Dataset: univ                Batch:  7/15	Loss 9.5355 (10.4293)
2022-11-09 17:43:34,609:INFO: Dataset: univ                Batch:  8/15	Loss 10.3861 (10.4244)
2022-11-09 17:43:34,635:INFO: Dataset: univ                Batch:  9/15	Loss 10.8231 (10.4665)
2022-11-09 17:43:34,662:INFO: Dataset: univ                Batch: 10/15	Loss 11.5363 (10.5621)
2022-11-09 17:43:34,689:INFO: Dataset: univ                Batch: 11/15	Loss 9.6480 (10.4743)
2022-11-09 17:43:34,717:INFO: Dataset: univ                Batch: 12/15	Loss 9.6668 (10.4063)
2022-11-09 17:43:34,744:INFO: Dataset: univ                Batch: 13/15	Loss 8.3236 (10.2243)
2022-11-09 17:43:34,772:INFO: Dataset: univ                Batch: 14/15	Loss 9.3854 (10.1584)
2022-11-09 17:43:34,782:INFO: Dataset: univ                Batch: 15/15	Loss 1.3907 (10.0532)
2022-11-09 17:43:35,034:INFO: Dataset: zara1               Batch: 1/8	Loss 16.8929 (16.8929)
2022-11-09 17:43:35,061:INFO: Dataset: zara1               Batch: 2/8	Loss 13.2516 (15.0021)
2022-11-09 17:43:35,086:INFO: Dataset: zara1               Batch: 3/8	Loss 17.4535 (15.7842)
2022-11-09 17:43:35,111:INFO: Dataset: zara1               Batch: 4/8	Loss 16.4454 (15.9504)
2022-11-09 17:43:35,135:INFO: Dataset: zara1               Batch: 5/8	Loss 15.3186 (15.8189)
2022-11-09 17:43:35,160:INFO: Dataset: zara1               Batch: 6/8	Loss 16.3971 (15.9096)
2022-11-09 17:43:35,183:INFO: Dataset: zara1               Batch: 7/8	Loss 17.2545 (16.0967)
2022-11-09 17:43:35,204:INFO: Dataset: zara1               Batch: 8/8	Loss 12.7216 (15.7094)
2022-11-09 17:43:35,459:INFO: Dataset: zara2               Batch:  1/18	Loss 10.3560 (10.3560)
2022-11-09 17:43:35,485:INFO: Dataset: zara2               Batch:  2/18	Loss 10.4820 (10.4186)
2022-11-09 17:43:35,514:INFO: Dataset: zara2               Batch:  3/18	Loss 11.5609 (10.8165)
2022-11-09 17:43:35,537:INFO: Dataset: zara2               Batch:  4/18	Loss 11.4824 (10.9816)
2022-11-09 17:43:35,563:INFO: Dataset: zara2               Batch:  5/18	Loss 9.8315 (10.7855)
2022-11-09 17:43:35,591:INFO: Dataset: zara2               Batch:  6/18	Loss 10.7763 (10.7841)
2022-11-09 17:43:35,616:INFO: Dataset: zara2               Batch:  7/18	Loss 9.9379 (10.6713)
2022-11-09 17:43:35,643:INFO: Dataset: zara2               Batch:  8/18	Loss 12.0363 (10.8546)
2022-11-09 17:43:35,669:INFO: Dataset: zara2               Batch:  9/18	Loss 10.7034 (10.8394)
2022-11-09 17:43:35,695:INFO: Dataset: zara2               Batch: 10/18	Loss 10.8646 (10.8421)
2022-11-09 17:43:35,722:INFO: Dataset: zara2               Batch: 11/18	Loss 11.2439 (10.8804)
2022-11-09 17:43:35,746:INFO: Dataset: zara2               Batch: 12/18	Loss 10.2898 (10.8324)
2022-11-09 17:43:35,770:INFO: Dataset: zara2               Batch: 13/18	Loss 11.4747 (10.8845)
2022-11-09 17:43:35,794:INFO: Dataset: zara2               Batch: 14/18	Loss 10.9972 (10.8922)
2022-11-09 17:43:35,817:INFO: Dataset: zara2               Batch: 15/18	Loss 9.4193 (10.7849)
2022-11-09 17:43:35,843:INFO: Dataset: zara2               Batch: 16/18	Loss 10.7435 (10.7823)
2022-11-09 17:43:35,869:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6991 (10.8346)
2022-11-09 17:43:35,894:INFO: Dataset: zara2               Batch: 18/18	Loss 10.0017 (10.7940)
2022-11-09 17:43:35,958:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_2.pth.tar
2022-11-09 17:43:35,959:INFO: 
===> EPOCH: 3 (P1)
2022-11-09 17:43:35,959:INFO: - Computing loss (training)
2022-11-09 17:43:36,291:INFO: Dataset: hotel               Batch: 1/4	Loss 11.3054 (11.3054)
2022-11-09 17:43:36,335:INFO: Dataset: hotel               Batch: 2/4	Loss 12.5438 (11.9473)
2022-11-09 17:43:36,377:INFO: Dataset: hotel               Batch: 3/4	Loss 11.9157 (11.9364)
2022-11-09 17:43:36,409:INFO: Dataset: hotel               Batch: 4/4	Loss 5.7910 (10.8581)
2022-11-09 17:43:36,755:INFO: Dataset: univ                Batch:  1/15	Loss 8.8128 (8.8128)
2022-11-09 17:43:36,791:INFO: Dataset: univ                Batch:  2/15	Loss 8.7135 (8.7646)
2022-11-09 17:43:36,823:INFO: Dataset: univ                Batch:  3/15	Loss 8.7561 (8.7617)
2022-11-09 17:43:36,850:INFO: Dataset: univ                Batch:  4/15	Loss 8.5299 (8.7022)
2022-11-09 17:43:36,875:INFO: Dataset: univ                Batch:  5/15	Loss 9.1462 (8.7860)
2022-11-09 17:43:36,906:INFO: Dataset: univ                Batch:  6/15	Loss 8.3240 (8.7043)
2022-11-09 17:43:36,932:INFO: Dataset: univ                Batch:  7/15	Loss 9.5864 (8.8274)
2022-11-09 17:43:36,957:INFO: Dataset: univ                Batch:  8/15	Loss 7.9464 (8.7101)
2022-11-09 17:43:36,983:INFO: Dataset: univ                Batch:  9/15	Loss 9.3430 (8.7784)
2022-11-09 17:43:37,009:INFO: Dataset: univ                Batch: 10/15	Loss 8.5873 (8.7601)
2022-11-09 17:43:37,035:INFO: Dataset: univ                Batch: 11/15	Loss 9.3547 (8.8083)
2022-11-09 17:43:37,064:INFO: Dataset: univ                Batch: 12/15	Loss 10.2853 (8.9156)
2022-11-09 17:43:37,090:INFO: Dataset: univ                Batch: 13/15	Loss 8.5841 (8.8905)
2022-11-09 17:43:37,115:INFO: Dataset: univ                Batch: 14/15	Loss 8.9126 (8.8922)
2022-11-09 17:43:37,125:INFO: Dataset: univ                Batch: 15/15	Loss 2.3151 (8.8190)
2022-11-09 17:43:37,379:INFO: Dataset: zara1               Batch: 1/8	Loss 14.4552 (14.4552)
2022-11-09 17:43:37,404:INFO: Dataset: zara1               Batch: 2/8	Loss 13.0581 (13.7163)
2022-11-09 17:43:37,428:INFO: Dataset: zara1               Batch: 3/8	Loss 12.6650 (13.4069)
2022-11-09 17:43:37,453:INFO: Dataset: zara1               Batch: 4/8	Loss 13.3214 (13.3849)
2022-11-09 17:43:37,477:INFO: Dataset: zara1               Batch: 5/8	Loss 13.4539 (13.3979)
2022-11-09 17:43:37,503:INFO: Dataset: zara1               Batch: 6/8	Loss 14.9024 (13.6342)
2022-11-09 17:43:37,527:INFO: Dataset: zara1               Batch: 7/8	Loss 13.6942 (13.6414)
2022-11-09 17:43:37,549:INFO: Dataset: zara1               Batch: 8/8	Loss 10.5441 (13.3154)
2022-11-09 17:43:37,808:INFO: Dataset: zara2               Batch:  1/18	Loss 9.3408 (9.3408)
2022-11-09 17:43:37,832:INFO: Dataset: zara2               Batch:  2/18	Loss 9.8956 (9.6214)
2022-11-09 17:43:37,857:INFO: Dataset: zara2               Batch:  3/18	Loss 9.9650 (9.7310)
2022-11-09 17:43:37,884:INFO: Dataset: zara2               Batch:  4/18	Loss 9.5402 (9.6802)
2022-11-09 17:43:37,909:INFO: Dataset: zara2               Batch:  5/18	Loss 9.5958 (9.6645)
2022-11-09 17:43:37,938:INFO: Dataset: zara2               Batch:  6/18	Loss 9.1290 (9.5746)
2022-11-09 17:43:37,962:INFO: Dataset: zara2               Batch:  7/18	Loss 8.9513 (9.4848)
2022-11-09 17:43:37,985:INFO: Dataset: zara2               Batch:  8/18	Loss 9.6195 (9.5015)
2022-11-09 17:43:38,009:INFO: Dataset: zara2               Batch:  9/18	Loss 10.0739 (9.5689)
2022-11-09 17:43:38,034:INFO: Dataset: zara2               Batch: 10/18	Loss 10.4698 (9.6568)
2022-11-09 17:43:38,059:INFO: Dataset: zara2               Batch: 11/18	Loss 10.3237 (9.7195)
2022-11-09 17:43:38,085:INFO: Dataset: zara2               Batch: 12/18	Loss 9.1968 (9.6792)
2022-11-09 17:43:38,110:INFO: Dataset: zara2               Batch: 13/18	Loss 10.3983 (9.7339)
2022-11-09 17:43:38,134:INFO: Dataset: zara2               Batch: 14/18	Loss 9.6751 (9.7300)
2022-11-09 17:43:38,158:INFO: Dataset: zara2               Batch: 15/18	Loss 8.8661 (9.6764)
2022-11-09 17:43:38,184:INFO: Dataset: zara2               Batch: 16/18	Loss 9.2598 (9.6479)
2022-11-09 17:43:38,209:INFO: Dataset: zara2               Batch: 17/18	Loss 9.8082 (9.6574)
2022-11-09 17:43:38,234:INFO: Dataset: zara2               Batch: 18/18	Loss 7.5003 (9.5561)
2022-11-09 17:43:38,284:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_3.pth.tar
2022-11-09 17:43:38,284:INFO: 
===> EPOCH: 4 (P1)
2022-11-09 17:43:38,284:INFO: - Computing loss (training)
2022-11-09 17:43:38,493:INFO: Dataset: hotel               Batch: 1/4	Loss 8.3715 (8.3715)
2022-11-09 17:43:38,520:INFO: Dataset: hotel               Batch: 2/4	Loss 11.0718 (9.6806)
2022-11-09 17:43:38,544:INFO: Dataset: hotel               Batch: 3/4	Loss 9.9106 (9.7566)
2022-11-09 17:43:38,560:INFO: Dataset: hotel               Batch: 4/4	Loss 7.3546 (9.3764)
2022-11-09 17:43:38,873:INFO: Dataset: univ                Batch:  1/15	Loss 8.2841 (8.2841)
2022-11-09 17:43:38,901:INFO: Dataset: univ                Batch:  2/15	Loss 8.0848 (8.1813)
2022-11-09 17:43:38,927:INFO: Dataset: univ                Batch:  3/15	Loss 7.6079 (7.9827)
2022-11-09 17:43:38,952:INFO: Dataset: univ                Batch:  4/15	Loss 8.0804 (8.0048)
2022-11-09 17:43:38,977:INFO: Dataset: univ                Batch:  5/15	Loss 8.1274 (8.0277)
2022-11-09 17:43:39,006:INFO: Dataset: univ                Batch:  6/15	Loss 7.0976 (7.8692)
2022-11-09 17:43:39,031:INFO: Dataset: univ                Batch:  7/15	Loss 7.6567 (7.8382)
2022-11-09 17:43:39,056:INFO: Dataset: univ                Batch:  8/15	Loss 8.3595 (7.9011)
2022-11-09 17:43:39,081:INFO: Dataset: univ                Batch:  9/15	Loss 7.6958 (7.8794)
2022-11-09 17:43:39,107:INFO: Dataset: univ                Batch: 10/15	Loss 7.6774 (7.8593)
2022-11-09 17:43:39,134:INFO: Dataset: univ                Batch: 11/15	Loss 8.2408 (7.8936)
2022-11-09 17:43:39,162:INFO: Dataset: univ                Batch: 12/15	Loss 7.9703 (7.8998)
2022-11-09 17:43:39,187:INFO: Dataset: univ                Batch: 13/15	Loss 8.1343 (7.9167)
2022-11-09 17:43:39,213:INFO: Dataset: univ                Batch: 14/15	Loss 9.5695 (8.0272)
2022-11-09 17:43:39,223:INFO: Dataset: univ                Batch: 15/15	Loss 1.4721 (7.9352)
2022-11-09 17:43:39,483:INFO: Dataset: zara1               Batch: 1/8	Loss 13.3368 (13.3368)
2022-11-09 17:43:39,507:INFO: Dataset: zara1               Batch: 2/8	Loss 13.1238 (13.2374)
2022-11-09 17:43:39,531:INFO: Dataset: zara1               Batch: 3/8	Loss 11.5382 (12.6801)
2022-11-09 17:43:39,555:INFO: Dataset: zara1               Batch: 4/8	Loss 13.3153 (12.8600)
2022-11-09 17:43:39,580:INFO: Dataset: zara1               Batch: 5/8	Loss 11.8356 (12.6498)
2022-11-09 17:43:39,605:INFO: Dataset: zara1               Batch: 6/8	Loss 12.8570 (12.6854)
2022-11-09 17:43:39,629:INFO: Dataset: zara1               Batch: 7/8	Loss 11.7685 (12.5445)
2022-11-09 17:43:39,649:INFO: Dataset: zara1               Batch: 8/8	Loss 9.1292 (12.2174)
2022-11-09 17:43:39,899:INFO: Dataset: zara2               Batch:  1/18	Loss 8.3573 (8.3573)
2022-11-09 17:43:39,927:INFO: Dataset: zara2               Batch:  2/18	Loss 9.2089 (8.7683)
2022-11-09 17:43:39,951:INFO: Dataset: zara2               Batch:  3/18	Loss 8.8404 (8.7913)
2022-11-09 17:43:39,974:INFO: Dataset: zara2               Batch:  4/18	Loss 8.6858 (8.7634)
2022-11-09 17:43:40,001:INFO: Dataset: zara2               Batch:  5/18	Loss 9.0240 (8.8176)
2022-11-09 17:43:40,026:INFO: Dataset: zara2               Batch:  6/18	Loss 9.1721 (8.8722)
2022-11-09 17:43:40,050:INFO: Dataset: zara2               Batch:  7/18	Loss 8.2333 (8.7876)
2022-11-09 17:43:40,073:INFO: Dataset: zara2               Batch:  8/18	Loss 8.9210 (8.8023)
2022-11-09 17:43:40,097:INFO: Dataset: zara2               Batch:  9/18	Loss 9.1315 (8.8395)
2022-11-09 17:43:40,122:INFO: Dataset: zara2               Batch: 10/18	Loss 8.8116 (8.8372)
2022-11-09 17:43:40,150:INFO: Dataset: zara2               Batch: 11/18	Loss 8.7552 (8.8299)
2022-11-09 17:43:40,175:INFO: Dataset: zara2               Batch: 12/18	Loss 8.0595 (8.7621)
2022-11-09 17:43:40,200:INFO: Dataset: zara2               Batch: 13/18	Loss 9.5352 (8.8238)
2022-11-09 17:43:40,225:INFO: Dataset: zara2               Batch: 14/18	Loss 8.8235 (8.8238)
2022-11-09 17:43:40,249:INFO: Dataset: zara2               Batch: 15/18	Loss 8.3841 (8.7975)
2022-11-09 17:43:40,274:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2042 (8.7620)
2022-11-09 17:43:40,299:INFO: Dataset: zara2               Batch: 17/18	Loss 9.4482 (8.7978)
2022-11-09 17:43:40,321:INFO: Dataset: zara2               Batch: 18/18	Loss 7.3857 (8.7334)
2022-11-09 17:43:40,369:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_4.pth.tar
2022-11-09 17:43:40,370:INFO: 
===> EPOCH: 5 (P1)
2022-11-09 17:43:40,370:INFO: - Computing loss (training)
2022-11-09 17:43:40,572:INFO: Dataset: hotel               Batch: 1/4	Loss 8.5559 (8.5559)
2022-11-09 17:43:40,598:INFO: Dataset: hotel               Batch: 2/4	Loss 9.7706 (9.1794)
2022-11-09 17:43:40,622:INFO: Dataset: hotel               Batch: 3/4	Loss 8.0626 (8.7924)
2022-11-09 17:43:40,639:INFO: Dataset: hotel               Batch: 4/4	Loss 5.5443 (8.2525)
2022-11-09 17:43:40,974:INFO: Dataset: univ                Batch:  1/15	Loss 8.2247 (8.2247)
2022-11-09 17:43:41,005:INFO: Dataset: univ                Batch:  2/15	Loss 7.2974 (7.7451)
2022-11-09 17:43:41,030:INFO: Dataset: univ                Batch:  3/15	Loss 6.6405 (7.3543)
2022-11-09 17:43:41,056:INFO: Dataset: univ                Batch:  4/15	Loss 6.8109 (7.2168)
2022-11-09 17:43:41,081:INFO: Dataset: univ                Batch:  5/15	Loss 7.5559 (7.2840)
2022-11-09 17:43:41,108:INFO: Dataset: univ                Batch:  6/15	Loss 8.1857 (7.4211)
2022-11-09 17:43:41,133:INFO: Dataset: univ                Batch:  7/15	Loss 7.2830 (7.4006)
2022-11-09 17:43:41,158:INFO: Dataset: univ                Batch:  8/15	Loss 7.0506 (7.3601)
2022-11-09 17:43:41,184:INFO: Dataset: univ                Batch:  9/15	Loss 7.2805 (7.3507)
2022-11-09 17:43:41,211:INFO: Dataset: univ                Batch: 10/15	Loss 7.7751 (7.3900)
2022-11-09 17:43:41,238:INFO: Dataset: univ                Batch: 11/15	Loss 7.5688 (7.4054)
2022-11-09 17:43:41,266:INFO: Dataset: univ                Batch: 12/15	Loss 7.1492 (7.3827)
2022-11-09 17:43:41,293:INFO: Dataset: univ                Batch: 13/15	Loss 7.9343 (7.4249)
2022-11-09 17:43:41,319:INFO: Dataset: univ                Batch: 14/15	Loss 7.1224 (7.4029)
2022-11-09 17:43:41,328:INFO: Dataset: univ                Batch: 15/15	Loss 1.2928 (7.3183)
2022-11-09 17:43:41,590:INFO: Dataset: zara1               Batch: 1/8	Loss 11.8332 (11.8332)
2022-11-09 17:43:41,615:INFO: Dataset: zara1               Batch: 2/8	Loss 11.6116 (11.7335)
2022-11-09 17:43:41,639:INFO: Dataset: zara1               Batch: 3/8	Loss 11.9307 (11.7943)
2022-11-09 17:43:41,662:INFO: Dataset: zara1               Batch: 4/8	Loss 11.1658 (11.6240)
2022-11-09 17:43:41,688:INFO: Dataset: zara1               Batch: 5/8	Loss 10.7011 (11.4199)
2022-11-09 17:43:41,713:INFO: Dataset: zara1               Batch: 6/8	Loss 11.8759 (11.4978)
2022-11-09 17:43:41,737:INFO: Dataset: zara1               Batch: 7/8	Loss 11.3711 (11.4789)
2022-11-09 17:43:41,758:INFO: Dataset: zara1               Batch: 8/8	Loss 9.6225 (11.2766)
2022-11-09 17:43:42,011:INFO: Dataset: zara2               Batch:  1/18	Loss 8.8173 (8.8173)
2022-11-09 17:43:42,036:INFO: Dataset: zara2               Batch:  2/18	Loss 7.6387 (8.2009)
2022-11-09 17:43:42,059:INFO: Dataset: zara2               Batch:  3/18	Loss 7.6156 (8.0032)
2022-11-09 17:43:42,084:INFO: Dataset: zara2               Batch:  4/18	Loss 8.7579 (8.1898)
2022-11-09 17:43:42,110:INFO: Dataset: zara2               Batch:  5/18	Loss 8.6963 (8.2847)
2022-11-09 17:43:42,136:INFO: Dataset: zara2               Batch:  6/18	Loss 8.1497 (8.2629)
2022-11-09 17:43:42,159:INFO: Dataset: zara2               Batch:  7/18	Loss 7.7248 (8.1888)
2022-11-09 17:43:42,183:INFO: Dataset: zara2               Batch:  8/18	Loss 8.4493 (8.2220)
2022-11-09 17:43:42,206:INFO: Dataset: zara2               Batch:  9/18	Loss 8.4768 (8.2493)
2022-11-09 17:43:42,230:INFO: Dataset: zara2               Batch: 10/18	Loss 8.4515 (8.2698)
2022-11-09 17:43:42,257:INFO: Dataset: zara2               Batch: 11/18	Loss 9.5215 (8.3739)
2022-11-09 17:43:42,282:INFO: Dataset: zara2               Batch: 12/18	Loss 8.6663 (8.3985)
2022-11-09 17:43:42,306:INFO: Dataset: zara2               Batch: 13/18	Loss 8.5222 (8.4084)
2022-11-09 17:43:42,330:INFO: Dataset: zara2               Batch: 14/18	Loss 7.8905 (8.3745)
2022-11-09 17:43:42,355:INFO: Dataset: zara2               Batch: 15/18	Loss 8.8734 (8.4097)
2022-11-09 17:43:42,379:INFO: Dataset: zara2               Batch: 16/18	Loss 8.8378 (8.4374)
2022-11-09 17:43:42,404:INFO: Dataset: zara2               Batch: 17/18	Loss 7.5380 (8.3859)
2022-11-09 17:43:42,426:INFO: Dataset: zara2               Batch: 18/18	Loss 6.9938 (8.3181)
2022-11-09 17:43:42,474:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_5.pth.tar
2022-11-09 17:43:42,474:INFO: 
===> EPOCH: 6 (P1)
2022-11-09 17:43:42,475:INFO: - Computing loss (training)
2022-11-09 17:43:42,676:INFO: Dataset: hotel               Batch: 1/4	Loss 7.2441 (7.2441)
2022-11-09 17:43:42,702:INFO: Dataset: hotel               Batch: 2/4	Loss 8.8664 (8.0493)
2022-11-09 17:43:42,727:INFO: Dataset: hotel               Batch: 3/4	Loss 7.4794 (7.8584)
2022-11-09 17:43:42,745:INFO: Dataset: hotel               Batch: 4/4	Loss 4.8490 (7.2787)
2022-11-09 17:43:42,996:INFO: Dataset: univ                Batch:  1/15	Loss 7.7561 (7.7561)
2022-11-09 17:43:43,023:INFO: Dataset: univ                Batch:  2/15	Loss 6.9114 (7.3361)
2022-11-09 17:43:43,050:INFO: Dataset: univ                Batch:  3/15	Loss 7.0934 (7.2558)
2022-11-09 17:43:43,075:INFO: Dataset: univ                Batch:  4/15	Loss 7.6422 (7.3509)
2022-11-09 17:43:43,104:INFO: Dataset: univ                Batch:  5/15	Loss 6.9336 (7.2704)
2022-11-09 17:43:43,132:INFO: Dataset: univ                Batch:  6/15	Loss 6.7770 (7.1837)
2022-11-09 17:43:43,158:INFO: Dataset: univ                Batch:  7/15	Loss 6.6120 (7.1041)
2022-11-09 17:43:43,183:INFO: Dataset: univ                Batch:  8/15	Loss 6.4317 (7.0201)
2022-11-09 17:43:43,209:INFO: Dataset: univ                Batch:  9/15	Loss 6.6022 (6.9712)
2022-11-09 17:43:43,236:INFO: Dataset: univ                Batch: 10/15	Loss 6.7430 (6.9472)
2022-11-09 17:43:43,264:INFO: Dataset: univ                Batch: 11/15	Loss 7.4795 (6.9945)
2022-11-09 17:43:43,291:INFO: Dataset: univ                Batch: 12/15	Loss 6.6128 (6.9615)
2022-11-09 17:43:43,316:INFO: Dataset: univ                Batch: 13/15	Loss 6.6153 (6.9354)
2022-11-09 17:43:43,341:INFO: Dataset: univ                Batch: 14/15	Loss 7.5024 (6.9754)
2022-11-09 17:43:43,351:INFO: Dataset: univ                Batch: 15/15	Loss 1.3461 (6.9069)
2022-11-09 17:43:43,605:INFO: Dataset: zara1               Batch: 1/8	Loss 11.1395 (11.1395)
2022-11-09 17:43:43,630:INFO: Dataset: zara1               Batch: 2/8	Loss 11.0122 (11.0738)
2022-11-09 17:43:43,657:INFO: Dataset: zara1               Batch: 3/8	Loss 12.7995 (11.6251)
2022-11-09 17:43:43,681:INFO: Dataset: zara1               Batch: 4/8	Loss 11.4814 (11.5885)
2022-11-09 17:43:43,704:INFO: Dataset: zara1               Batch: 5/8	Loss 10.4424 (11.3681)
2022-11-09 17:43:43,729:INFO: Dataset: zara1               Batch: 6/8	Loss 10.5597 (11.2492)
2022-11-09 17:43:43,753:INFO: Dataset: zara1               Batch: 7/8	Loss 10.4183 (11.1201)
2022-11-09 17:43:43,774:INFO: Dataset: zara1               Batch: 8/8	Loss 9.2470 (10.8904)
2022-11-09 17:43:44,098:INFO: Dataset: zara2               Batch:  1/18	Loss 7.7933 (7.7933)
2022-11-09 17:43:44,121:INFO: Dataset: zara2               Batch:  2/18	Loss 8.1184 (7.9597)
2022-11-09 17:43:44,145:INFO: Dataset: zara2               Batch:  3/18	Loss 7.8083 (7.9059)
2022-11-09 17:43:44,172:INFO: Dataset: zara2               Batch:  4/18	Loss 8.6773 (8.0929)
2022-11-09 17:43:44,196:INFO: Dataset: zara2               Batch:  5/18	Loss 8.5828 (8.1931)
2022-11-09 17:43:44,222:INFO: Dataset: zara2               Batch:  6/18	Loss 8.9700 (8.3088)
2022-11-09 17:43:44,245:INFO: Dataset: zara2               Batch:  7/18	Loss 7.5976 (8.2047)
2022-11-09 17:43:44,269:INFO: Dataset: zara2               Batch:  8/18	Loss 9.6630 (8.3801)
2022-11-09 17:43:44,293:INFO: Dataset: zara2               Batch:  9/18	Loss 7.8430 (8.3139)
2022-11-09 17:43:44,317:INFO: Dataset: zara2               Batch: 10/18	Loss 7.7215 (8.2531)
2022-11-09 17:43:44,342:INFO: Dataset: zara2               Batch: 11/18	Loss 8.2453 (8.2525)
2022-11-09 17:43:44,368:INFO: Dataset: zara2               Batch: 12/18	Loss 8.3035 (8.2566)
2022-11-09 17:43:44,394:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9560 (8.1513)
2022-11-09 17:43:44,419:INFO: Dataset: zara2               Batch: 14/18	Loss 7.7359 (8.1210)
2022-11-09 17:43:44,443:INFO: Dataset: zara2               Batch: 15/18	Loss 7.6340 (8.0919)
2022-11-09 17:43:44,468:INFO: Dataset: zara2               Batch: 16/18	Loss 8.4469 (8.1142)
2022-11-09 17:43:44,492:INFO: Dataset: zara2               Batch: 17/18	Loss 8.3027 (8.1238)
2022-11-09 17:43:44,514:INFO: Dataset: zara2               Batch: 18/18	Loss 6.5236 (8.0426)
2022-11-09 17:43:44,566:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_6.pth.tar
2022-11-09 17:43:44,566:INFO: 
===> EPOCH: 7 (P1)
2022-11-09 17:43:44,566:INFO: - Computing loss (training)
2022-11-09 17:43:44,759:INFO: Dataset: hotel               Batch: 1/4	Loss 6.7222 (6.7222)
2022-11-09 17:43:44,785:INFO: Dataset: hotel               Batch: 2/4	Loss 7.0338 (6.8862)
2022-11-09 17:43:44,810:INFO: Dataset: hotel               Batch: 3/4	Loss 7.0251 (6.9326)
2022-11-09 17:43:44,828:INFO: Dataset: hotel               Batch: 4/4	Loss 4.0439 (6.4372)
2022-11-09 17:43:45,068:INFO: Dataset: univ                Batch:  1/15	Loss 6.8821 (6.8821)
2022-11-09 17:43:45,095:INFO: Dataset: univ                Batch:  2/15	Loss 6.8650 (6.8738)
2022-11-09 17:43:45,121:INFO: Dataset: univ                Batch:  3/15	Loss 7.4856 (7.0794)
2022-11-09 17:43:45,151:INFO: Dataset: univ                Batch:  4/15	Loss 6.1172 (6.8272)
2022-11-09 17:43:45,176:INFO: Dataset: univ                Batch:  5/15	Loss 5.2816 (6.4882)
2022-11-09 17:43:45,203:INFO: Dataset: univ                Batch:  6/15	Loss 6.2882 (6.4536)
2022-11-09 17:43:45,228:INFO: Dataset: univ                Batch:  7/15	Loss 6.8573 (6.5057)
2022-11-09 17:43:45,254:INFO: Dataset: univ                Batch:  8/15	Loss 6.9170 (6.5585)
2022-11-09 17:43:45,279:INFO: Dataset: univ                Batch:  9/15	Loss 7.4800 (6.6553)
2022-11-09 17:43:45,305:INFO: Dataset: univ                Batch: 10/15	Loss 7.6470 (6.7531)
2022-11-09 17:43:45,331:INFO: Dataset: univ                Batch: 11/15	Loss 5.4949 (6.6369)
2022-11-09 17:43:45,358:INFO: Dataset: univ                Batch: 12/15	Loss 7.0866 (6.6791)
2022-11-09 17:43:45,384:INFO: Dataset: univ                Batch: 13/15	Loss 6.1583 (6.6354)
2022-11-09 17:43:45,409:INFO: Dataset: univ                Batch: 14/15	Loss 7.4564 (6.6889)
2022-11-09 17:43:45,419:INFO: Dataset: univ                Batch: 15/15	Loss 1.0270 (6.6156)
2022-11-09 17:43:45,664:INFO: Dataset: zara1               Batch: 1/8	Loss 9.5188 (9.5188)
2022-11-09 17:43:45,691:INFO: Dataset: zara1               Batch: 2/8	Loss 11.0931 (10.3539)
2022-11-09 17:43:45,716:INFO: Dataset: zara1               Batch: 3/8	Loss 11.7078 (10.8026)
2022-11-09 17:43:45,739:INFO: Dataset: zara1               Batch: 4/8	Loss 10.7756 (10.7955)
2022-11-09 17:43:45,763:INFO: Dataset: zara1               Batch: 5/8	Loss 11.9245 (11.0109)
2022-11-09 17:43:45,788:INFO: Dataset: zara1               Batch: 6/8	Loss 10.1709 (10.8598)
2022-11-09 17:43:45,812:INFO: Dataset: zara1               Batch: 7/8	Loss 10.7951 (10.8503)
2022-11-09 17:43:45,833:INFO: Dataset: zara1               Batch: 8/8	Loss 9.6432 (10.6896)
2022-11-09 17:43:46,081:INFO: Dataset: zara2               Batch:  1/18	Loss 7.7119 (7.7119)
2022-11-09 17:43:46,107:INFO: Dataset: zara2               Batch:  2/18	Loss 8.7485 (8.2132)
2022-11-09 17:43:46,134:INFO: Dataset: zara2               Batch:  3/18	Loss 9.2780 (8.5678)
2022-11-09 17:43:46,157:INFO: Dataset: zara2               Batch:  4/18	Loss 8.2487 (8.4860)
2022-11-09 17:43:46,184:INFO: Dataset: zara2               Batch:  5/18	Loss 7.7167 (8.3319)
2022-11-09 17:43:46,210:INFO: Dataset: zara2               Batch:  6/18	Loss 8.1368 (8.3034)
2022-11-09 17:43:46,234:INFO: Dataset: zara2               Batch:  7/18	Loss 8.3135 (8.3049)
2022-11-09 17:43:46,258:INFO: Dataset: zara2               Batch:  8/18	Loss 7.8014 (8.2392)
2022-11-09 17:43:46,282:INFO: Dataset: zara2               Batch:  9/18	Loss 7.7820 (8.1926)
2022-11-09 17:43:46,307:INFO: Dataset: zara2               Batch: 10/18	Loss 7.7982 (8.1568)
2022-11-09 17:43:46,332:INFO: Dataset: zara2               Batch: 11/18	Loss 6.6962 (8.0148)
2022-11-09 17:43:46,358:INFO: Dataset: zara2               Batch: 12/18	Loss 7.9714 (8.0110)
2022-11-09 17:43:46,382:INFO: Dataset: zara2               Batch: 13/18	Loss 7.3851 (7.9618)
2022-11-09 17:43:46,406:INFO: Dataset: zara2               Batch: 14/18	Loss 8.1398 (7.9730)
2022-11-09 17:43:46,432:INFO: Dataset: zara2               Batch: 15/18	Loss 8.4815 (8.0052)
2022-11-09 17:43:46,457:INFO: Dataset: zara2               Batch: 16/18	Loss 9.6779 (8.1026)
2022-11-09 17:43:46,482:INFO: Dataset: zara2               Batch: 17/18	Loss 7.6448 (8.0768)
2022-11-09 17:43:46,505:INFO: Dataset: zara2               Batch: 18/18	Loss 6.3242 (7.9908)
2022-11-09 17:43:46,556:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_7.pth.tar
2022-11-09 17:43:46,556:INFO: 
===> EPOCH: 8 (P1)
2022-11-09 17:43:46,557:INFO: - Computing loss (training)
2022-11-09 17:43:46,767:INFO: Dataset: hotel               Batch: 1/4	Loss 6.0831 (6.0831)
2022-11-09 17:43:46,794:INFO: Dataset: hotel               Batch: 2/4	Loss 6.3352 (6.2121)
2022-11-09 17:43:46,819:INFO: Dataset: hotel               Batch: 3/4	Loss 6.5975 (6.3345)
2022-11-09 17:43:46,837:INFO: Dataset: hotel               Batch: 4/4	Loss 4.1082 (5.9673)
2022-11-09 17:43:47,095:INFO: Dataset: univ                Batch:  1/15	Loss 6.5292 (6.5292)
2022-11-09 17:43:47,121:INFO: Dataset: univ                Batch:  2/15	Loss 6.8568 (6.6842)
2022-11-09 17:43:47,147:INFO: Dataset: univ                Batch:  3/15	Loss 5.6941 (6.3347)
2022-11-09 17:43:47,173:INFO: Dataset: univ                Batch:  4/15	Loss 5.9029 (6.2154)
2022-11-09 17:43:47,203:INFO: Dataset: univ                Batch:  5/15	Loss 7.1004 (6.3780)
2022-11-09 17:43:47,230:INFO: Dataset: univ                Batch:  6/15	Loss 6.3866 (6.3795)
2022-11-09 17:43:47,263:INFO: Dataset: univ                Batch:  7/15	Loss 7.4554 (6.5382)
2022-11-09 17:43:47,293:INFO: Dataset: univ                Batch:  8/15	Loss 7.2337 (6.6196)
2022-11-09 17:43:47,322:INFO: Dataset: univ                Batch:  9/15	Loss 5.9638 (6.5444)
2022-11-09 17:43:47,353:INFO: Dataset: univ                Batch: 10/15	Loss 6.2597 (6.5173)
2022-11-09 17:43:47,385:INFO: Dataset: univ                Batch: 11/15	Loss 6.0642 (6.4760)
2022-11-09 17:43:47,416:INFO: Dataset: univ                Batch: 12/15	Loss 5.7809 (6.4131)
2022-11-09 17:43:47,458:INFO: Dataset: univ                Batch: 13/15	Loss 6.6873 (6.4345)
2022-11-09 17:43:47,489:INFO: Dataset: univ                Batch: 14/15	Loss 6.0573 (6.4063)
2022-11-09 17:43:47,502:INFO: Dataset: univ                Batch: 15/15	Loss 1.4186 (6.3328)
2022-11-09 17:43:47,795:INFO: Dataset: zara1               Batch: 1/8	Loss 11.0598 (11.0598)
2022-11-09 17:43:47,826:INFO: Dataset: zara1               Batch: 2/8	Loss 10.4735 (10.7430)
2022-11-09 17:43:47,851:INFO: Dataset: zara1               Batch: 3/8	Loss 11.5519 (10.9970)
2022-11-09 17:43:47,878:INFO: Dataset: zara1               Batch: 4/8	Loss 11.7737 (11.1908)
2022-11-09 17:43:47,906:INFO: Dataset: zara1               Batch: 5/8	Loss 11.3807 (11.2307)
2022-11-09 17:43:47,932:INFO: Dataset: zara1               Batch: 6/8	Loss 11.2040 (11.2266)
2022-11-09 17:43:47,957:INFO: Dataset: zara1               Batch: 7/8	Loss 9.9382 (11.0683)
2022-11-09 17:43:47,979:INFO: Dataset: zara1               Batch: 8/8	Loss 8.7626 (10.8572)
2022-11-09 17:43:48,251:INFO: Dataset: zara2               Batch:  1/18	Loss 6.9803 (6.9803)
2022-11-09 17:43:48,278:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2814 (7.1335)
2022-11-09 17:43:48,305:INFO: Dataset: zara2               Batch:  3/18	Loss 7.4980 (7.2512)
2022-11-09 17:43:48,329:INFO: Dataset: zara2               Batch:  4/18	Loss 8.4127 (7.5254)
2022-11-09 17:43:48,357:INFO: Dataset: zara2               Batch:  5/18	Loss 7.4944 (7.5190)
2022-11-09 17:43:48,383:INFO: Dataset: zara2               Batch:  6/18	Loss 8.7493 (7.7270)
2022-11-09 17:43:48,407:INFO: Dataset: zara2               Batch:  7/18	Loss 8.1403 (7.7863)
2022-11-09 17:43:48,431:INFO: Dataset: zara2               Batch:  8/18	Loss 8.5386 (7.8853)
2022-11-09 17:43:48,454:INFO: Dataset: zara2               Batch:  9/18	Loss 7.6584 (7.8600)
2022-11-09 17:43:48,479:INFO: Dataset: zara2               Batch: 10/18	Loss 7.9158 (7.8660)
2022-11-09 17:43:48,503:INFO: Dataset: zara2               Batch: 11/18	Loss 8.0301 (7.8806)
2022-11-09 17:43:48,529:INFO: Dataset: zara2               Batch: 12/18	Loss 7.6919 (7.8652)
2022-11-09 17:43:48,554:INFO: Dataset: zara2               Batch: 13/18	Loss 7.5157 (7.8384)
2022-11-09 17:43:48,579:INFO: Dataset: zara2               Batch: 14/18	Loss 7.8921 (7.8422)
2022-11-09 17:43:48,605:INFO: Dataset: zara2               Batch: 15/18	Loss 7.0528 (7.7894)
2022-11-09 17:43:48,631:INFO: Dataset: zara2               Batch: 16/18	Loss 7.7178 (7.7850)
2022-11-09 17:43:48,658:INFO: Dataset: zara2               Batch: 17/18	Loss 7.5052 (7.7692)
2022-11-09 17:43:48,682:INFO: Dataset: zara2               Batch: 18/18	Loss 7.2889 (7.7460)
2022-11-09 17:43:48,733:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_8.pth.tar
2022-11-09 17:43:48,733:INFO: 
===> EPOCH: 9 (P1)
2022-11-09 17:43:48,733:INFO: - Computing loss (training)
2022-11-09 17:43:48,943:INFO: Dataset: hotel               Batch: 1/4	Loss 5.2392 (5.2392)
2022-11-09 17:43:48,968:INFO: Dataset: hotel               Batch: 2/4	Loss 7.7865 (6.5128)
2022-11-09 17:43:48,993:INFO: Dataset: hotel               Batch: 3/4	Loss 5.2941 (6.1194)
2022-11-09 17:43:49,010:INFO: Dataset: hotel               Batch: 4/4	Loss 3.5098 (5.6959)
2022-11-09 17:43:49,261:INFO: Dataset: univ                Batch:  1/15	Loss 5.8385 (5.8385)
2022-11-09 17:43:49,289:INFO: Dataset: univ                Batch:  2/15	Loss 7.4817 (6.6199)
2022-11-09 17:43:49,315:INFO: Dataset: univ                Batch:  3/15	Loss 6.0083 (6.4194)
2022-11-09 17:43:49,341:INFO: Dataset: univ                Batch:  4/15	Loss 6.1739 (6.3562)
2022-11-09 17:43:49,370:INFO: Dataset: univ                Batch:  5/15	Loss 7.2764 (6.5420)
2022-11-09 17:43:49,396:INFO: Dataset: univ                Batch:  6/15	Loss 6.6406 (6.5582)
2022-11-09 17:43:49,421:INFO: Dataset: univ                Batch:  7/15	Loss 5.5935 (6.4079)
2022-11-09 17:43:49,446:INFO: Dataset: univ                Batch:  8/15	Loss 5.9052 (6.3406)
2022-11-09 17:43:49,472:INFO: Dataset: univ                Batch:  9/15	Loss 5.5665 (6.2482)
2022-11-09 17:43:49,498:INFO: Dataset: univ                Batch: 10/15	Loss 6.1121 (6.2352)
2022-11-09 17:43:49,529:INFO: Dataset: univ                Batch: 11/15	Loss 6.0810 (6.2216)
2022-11-09 17:43:49,555:INFO: Dataset: univ                Batch: 12/15	Loss 6.0016 (6.2026)
2022-11-09 17:43:49,580:INFO: Dataset: univ                Batch: 13/15	Loss 6.2401 (6.2054)
2022-11-09 17:43:49,605:INFO: Dataset: univ                Batch: 14/15	Loss 6.9054 (6.2520)
2022-11-09 17:43:49,613:INFO: Dataset: univ                Batch: 15/15	Loss 1.0961 (6.1819)
2022-11-09 17:43:49,859:INFO: Dataset: zara1               Batch: 1/8	Loss 10.7301 (10.7301)
2022-11-09 17:43:49,884:INFO: Dataset: zara1               Batch: 2/8	Loss 9.6151 (10.1650)
2022-11-09 17:43:49,910:INFO: Dataset: zara1               Batch: 3/8	Loss 11.6711 (10.6168)
2022-11-09 17:43:49,933:INFO: Dataset: zara1               Batch: 4/8	Loss 10.9766 (10.7064)
2022-11-09 17:43:49,957:INFO: Dataset: zara1               Batch: 5/8	Loss 10.7172 (10.7085)
2022-11-09 17:43:49,982:INFO: Dataset: zara1               Batch: 6/8	Loss 11.8016 (10.8994)
2022-11-09 17:43:50,007:INFO: Dataset: zara1               Batch: 7/8	Loss 10.0720 (10.7868)
2022-11-09 17:43:50,029:INFO: Dataset: zara1               Batch: 8/8	Loss 8.7063 (10.5645)
2022-11-09 17:43:50,292:INFO: Dataset: zara2               Batch:  1/18	Loss 7.9483 (7.9483)
2022-11-09 17:43:50,321:INFO: Dataset: zara2               Batch:  2/18	Loss 7.4288 (7.6867)
2022-11-09 17:43:50,348:INFO: Dataset: zara2               Batch:  3/18	Loss 8.1857 (7.8560)
2022-11-09 17:43:50,374:INFO: Dataset: zara2               Batch:  4/18	Loss 7.2862 (7.7147)
2022-11-09 17:43:50,402:INFO: Dataset: zara2               Batch:  5/18	Loss 8.2431 (7.8112)
2022-11-09 17:43:50,428:INFO: Dataset: zara2               Batch:  6/18	Loss 7.0724 (7.6858)
2022-11-09 17:43:50,452:INFO: Dataset: zara2               Batch:  7/18	Loss 8.0609 (7.7449)
2022-11-09 17:43:50,476:INFO: Dataset: zara2               Batch:  8/18	Loss 7.2630 (7.6826)
2022-11-09 17:43:50,500:INFO: Dataset: zara2               Batch:  9/18	Loss 7.3681 (7.6511)
2022-11-09 17:43:50,525:INFO: Dataset: zara2               Batch: 10/18	Loss 7.7882 (7.6643)
2022-11-09 17:43:50,553:INFO: Dataset: zara2               Batch: 11/18	Loss 8.3301 (7.7197)
2022-11-09 17:43:50,577:INFO: Dataset: zara2               Batch: 12/18	Loss 7.4109 (7.6949)
2022-11-09 17:43:50,601:INFO: Dataset: zara2               Batch: 13/18	Loss 7.5927 (7.6878)
2022-11-09 17:43:50,625:INFO: Dataset: zara2               Batch: 14/18	Loss 7.6060 (7.6820)
2022-11-09 17:43:50,650:INFO: Dataset: zara2               Batch: 15/18	Loss 7.4246 (7.6636)
2022-11-09 17:43:50,675:INFO: Dataset: zara2               Batch: 16/18	Loss 6.6571 (7.5959)
2022-11-09 17:43:50,700:INFO: Dataset: zara2               Batch: 17/18	Loss 8.1337 (7.6275)
2022-11-09 17:43:50,723:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0758 (7.5503)
2022-11-09 17:43:50,775:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_9.pth.tar
2022-11-09 17:43:50,775:INFO: 
===> EPOCH: 10 (P1)
2022-11-09 17:43:50,775:INFO: - Computing loss (training)
2022-11-09 17:43:50,994:INFO: Dataset: hotel               Batch: 1/4	Loss 4.5527 (4.5527)
2022-11-09 17:43:51,020:INFO: Dataset: hotel               Batch: 2/4	Loss 6.9489 (5.7426)
2022-11-09 17:43:51,046:INFO: Dataset: hotel               Batch: 3/4	Loss 5.8989 (5.7917)
2022-11-09 17:43:51,064:INFO: Dataset: hotel               Batch: 4/4	Loss 3.8351 (5.4793)
2022-11-09 17:43:51,320:INFO: Dataset: univ                Batch:  1/15	Loss 6.0431 (6.0431)
2022-11-09 17:43:51,347:INFO: Dataset: univ                Batch:  2/15	Loss 6.2993 (6.1655)
2022-11-09 17:43:51,375:INFO: Dataset: univ                Batch:  3/15	Loss 5.5883 (5.9699)
2022-11-09 17:43:51,404:INFO: Dataset: univ                Batch:  4/15	Loss 5.2973 (5.7867)
2022-11-09 17:43:51,430:INFO: Dataset: univ                Batch:  5/15	Loss 7.1684 (6.0331)
2022-11-09 17:43:51,460:INFO: Dataset: univ                Batch:  6/15	Loss 6.2154 (6.0629)
2022-11-09 17:43:51,486:INFO: Dataset: univ                Batch:  7/15	Loss 5.9707 (6.0490)
2022-11-09 17:43:51,512:INFO: Dataset: univ                Batch:  8/15	Loss 7.4439 (6.2047)
2022-11-09 17:43:51,538:INFO: Dataset: univ                Batch:  9/15	Loss 6.3599 (6.2215)
2022-11-09 17:43:51,566:INFO: Dataset: univ                Batch: 10/15	Loss 5.8410 (6.1817)
2022-11-09 17:43:51,593:INFO: Dataset: univ                Batch: 11/15	Loss 7.1143 (6.2659)
2022-11-09 17:43:51,620:INFO: Dataset: univ                Batch: 12/15	Loss 5.7909 (6.2252)
2022-11-09 17:43:51,648:INFO: Dataset: univ                Batch: 13/15	Loss 5.4238 (6.1577)
2022-11-09 17:43:51,674:INFO: Dataset: univ                Batch: 14/15	Loss 5.4811 (6.1046)
2022-11-09 17:43:51,683:INFO: Dataset: univ                Batch: 15/15	Loss 1.4393 (6.0573)
2022-11-09 17:43:51,939:INFO: Dataset: zara1               Batch: 1/8	Loss 12.1380 (12.1380)
2022-11-09 17:43:51,963:INFO: Dataset: zara1               Batch: 2/8	Loss 14.1192 (13.1161)
2022-11-09 17:43:51,990:INFO: Dataset: zara1               Batch: 3/8	Loss 12.4303 (12.8940)
2022-11-09 17:43:52,015:INFO: Dataset: zara1               Batch: 4/8	Loss 9.5539 (12.0135)
2022-11-09 17:43:52,041:INFO: Dataset: zara1               Batch: 5/8	Loss 9.9316 (11.5711)
2022-11-09 17:43:52,067:INFO: Dataset: zara1               Batch: 6/8	Loss 10.2105 (11.3584)
2022-11-09 17:43:52,091:INFO: Dataset: zara1               Batch: 7/8	Loss 11.1957 (11.3341)
2022-11-09 17:43:52,113:INFO: Dataset: zara1               Batch: 8/8	Loss 8.6785 (11.0420)
2022-11-09 17:43:52,387:INFO: Dataset: zara2               Batch:  1/18	Loss 7.4261 (7.4261)
2022-11-09 17:43:52,412:INFO: Dataset: zara2               Batch:  2/18	Loss 7.3728 (7.4000)
2022-11-09 17:43:52,438:INFO: Dataset: zara2               Batch:  3/18	Loss 7.3308 (7.3776)
2022-11-09 17:43:52,467:INFO: Dataset: zara2               Batch:  4/18	Loss 7.8469 (7.4947)
2022-11-09 17:43:52,491:INFO: Dataset: zara2               Batch:  5/18	Loss 8.2542 (7.6442)
2022-11-09 17:43:52,518:INFO: Dataset: zara2               Batch:  6/18	Loss 7.3212 (7.5933)
2022-11-09 17:43:52,542:INFO: Dataset: zara2               Batch:  7/18	Loss 7.7528 (7.6186)
2022-11-09 17:43:52,566:INFO: Dataset: zara2               Batch:  8/18	Loss 7.5511 (7.6100)
2022-11-09 17:43:52,590:INFO: Dataset: zara2               Batch:  9/18	Loss 7.5224 (7.6002)
2022-11-09 17:43:52,615:INFO: Dataset: zara2               Batch: 10/18	Loss 7.9036 (7.6297)
2022-11-09 17:43:52,643:INFO: Dataset: zara2               Batch: 11/18	Loss 7.4665 (7.6142)
2022-11-09 17:43:52,670:INFO: Dataset: zara2               Batch: 12/18	Loss 7.9013 (7.6364)
2022-11-09 17:43:52,695:INFO: Dataset: zara2               Batch: 13/18	Loss 7.6803 (7.6400)
2022-11-09 17:43:52,720:INFO: Dataset: zara2               Batch: 14/18	Loss 7.3265 (7.6168)
2022-11-09 17:43:52,744:INFO: Dataset: zara2               Batch: 15/18	Loss 8.1554 (7.6561)
2022-11-09 17:43:52,769:INFO: Dataset: zara2               Batch: 16/18	Loss 6.7181 (7.5955)
2022-11-09 17:43:52,795:INFO: Dataset: zara2               Batch: 17/18	Loss 7.5443 (7.5924)
2022-11-09 17:43:52,820:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0566 (7.5216)
2022-11-09 17:43:52,871:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_10.pth.tar
2022-11-09 17:43:52,872:INFO: 
===> EPOCH: 11 (P1)
2022-11-09 17:43:52,872:INFO: - Computing loss (training)
2022-11-09 17:43:53,088:INFO: Dataset: hotel               Batch: 1/4	Loss 5.0918 (5.0918)
2022-11-09 17:43:53,113:INFO: Dataset: hotel               Batch: 2/4	Loss 5.8479 (5.4552)
2022-11-09 17:43:53,137:INFO: Dataset: hotel               Batch: 3/4	Loss 6.1692 (5.6901)
2022-11-09 17:43:53,156:INFO: Dataset: hotel               Batch: 4/4	Loss 3.1503 (5.2076)
2022-11-09 17:43:53,407:INFO: Dataset: univ                Batch:  1/15	Loss 5.7841 (5.7841)
2022-11-09 17:43:53,436:INFO: Dataset: univ                Batch:  2/15	Loss 6.2341 (6.0154)
2022-11-09 17:43:53,466:INFO: Dataset: univ                Batch:  3/15	Loss 5.8590 (5.9618)
2022-11-09 17:43:53,494:INFO: Dataset: univ                Batch:  4/15	Loss 5.6420 (5.8865)
2022-11-09 17:43:53,522:INFO: Dataset: univ                Batch:  5/15	Loss 7.2755 (6.1691)
2022-11-09 17:43:53,550:INFO: Dataset: univ                Batch:  6/15	Loss 5.3437 (6.0277)
2022-11-09 17:43:53,576:INFO: Dataset: univ                Batch:  7/15	Loss 6.1358 (6.0438)
2022-11-09 17:43:53,603:INFO: Dataset: univ                Batch:  8/15	Loss 6.1591 (6.0589)
2022-11-09 17:43:53,630:INFO: Dataset: univ                Batch:  9/15	Loss 5.9042 (6.0421)
2022-11-09 17:43:53,659:INFO: Dataset: univ                Batch: 10/15	Loss 5.8240 (6.0205)
2022-11-09 17:43:53,690:INFO: Dataset: univ                Batch: 11/15	Loss 6.0622 (6.0247)
2022-11-09 17:43:53,717:INFO: Dataset: univ                Batch: 12/15	Loss 5.8311 (6.0087)
2022-11-09 17:43:53,743:INFO: Dataset: univ                Batch: 13/15	Loss 5.8973 (6.0003)
2022-11-09 17:43:53,769:INFO: Dataset: univ                Batch: 14/15	Loss 6.5784 (6.0407)
2022-11-09 17:43:53,779:INFO: Dataset: univ                Batch: 15/15	Loss 1.0923 (5.9682)
2022-11-09 17:43:54,033:INFO: Dataset: zara1               Batch: 1/8	Loss 11.0129 (11.0129)
2022-11-09 17:43:54,058:INFO: Dataset: zara1               Batch: 2/8	Loss 13.2534 (12.2072)
2022-11-09 17:43:54,088:INFO: Dataset: zara1               Batch: 3/8	Loss 10.4516 (11.6403)
2022-11-09 17:43:54,115:INFO: Dataset: zara1               Batch: 4/8	Loss 11.0674 (11.5054)
2022-11-09 17:43:54,142:INFO: Dataset: zara1               Batch: 5/8	Loss 10.1849 (11.2326)
2022-11-09 17:43:54,169:INFO: Dataset: zara1               Batch: 6/8	Loss 9.4765 (10.9531)
2022-11-09 17:43:54,193:INFO: Dataset: zara1               Batch: 7/8	Loss 10.1690 (10.8339)
2022-11-09 17:43:54,215:INFO: Dataset: zara1               Batch: 8/8	Loss 9.5977 (10.7064)
2022-11-09 17:43:54,483:INFO: Dataset: zara2               Batch:  1/18	Loss 8.1703 (8.1703)
2022-11-09 17:43:54,509:INFO: Dataset: zara2               Batch:  2/18	Loss 7.7182 (7.9514)
2022-11-09 17:43:54,540:INFO: Dataset: zara2               Batch:  3/18	Loss 8.4594 (8.1303)
2022-11-09 17:43:54,565:INFO: Dataset: zara2               Batch:  4/18	Loss 7.3642 (7.9316)
2022-11-09 17:43:54,589:INFO: Dataset: zara2               Batch:  5/18	Loss 7.6243 (7.8662)
2022-11-09 17:43:54,616:INFO: Dataset: zara2               Batch:  6/18	Loss 8.5204 (7.9671)
2022-11-09 17:43:54,639:INFO: Dataset: zara2               Batch:  7/18	Loss 8.6974 (8.0650)
2022-11-09 17:43:54,663:INFO: Dataset: zara2               Batch:  8/18	Loss 7.1322 (7.9478)
2022-11-09 17:43:54,687:INFO: Dataset: zara2               Batch:  9/18	Loss 7.1639 (7.8540)
2022-11-09 17:43:54,712:INFO: Dataset: zara2               Batch: 10/18	Loss 6.2206 (7.6931)
2022-11-09 17:43:54,737:INFO: Dataset: zara2               Batch: 11/18	Loss 7.4807 (7.6749)
2022-11-09 17:43:54,763:INFO: Dataset: zara2               Batch: 12/18	Loss 7.0582 (7.6211)
2022-11-09 17:43:54,787:INFO: Dataset: zara2               Batch: 13/18	Loss 7.3033 (7.5979)
2022-11-09 17:43:54,811:INFO: Dataset: zara2               Batch: 14/18	Loss 7.1574 (7.5682)
2022-11-09 17:43:54,835:INFO: Dataset: zara2               Batch: 15/18	Loss 6.1100 (7.4704)
2022-11-09 17:43:54,862:INFO: Dataset: zara2               Batch: 16/18	Loss 7.8362 (7.4936)
2022-11-09 17:43:54,890:INFO: Dataset: zara2               Batch: 17/18	Loss 7.1449 (7.4739)
2022-11-09 17:43:54,917:INFO: Dataset: zara2               Batch: 18/18	Loss 7.3677 (7.4690)
2022-11-09 17:43:54,973:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_11.pth.tar
2022-11-09 17:43:54,973:INFO: 
===> EPOCH: 12 (P1)
2022-11-09 17:43:54,973:INFO: - Computing loss (training)
2022-11-09 17:43:55,194:INFO: Dataset: hotel               Batch: 1/4	Loss 4.6949 (4.6949)
2022-11-09 17:43:55,220:INFO: Dataset: hotel               Batch: 2/4	Loss 6.5521 (5.6235)
2022-11-09 17:43:55,245:INFO: Dataset: hotel               Batch: 3/4	Loss 4.9066 (5.3738)
2022-11-09 17:43:55,263:INFO: Dataset: hotel               Batch: 4/4	Loss 3.6588 (5.0684)
2022-11-09 17:43:55,534:INFO: Dataset: univ                Batch:  1/15	Loss 6.1304 (6.1304)
2022-11-09 17:43:55,588:INFO: Dataset: univ                Batch:  2/15	Loss 5.9382 (6.0320)
2022-11-09 17:43:55,614:INFO: Dataset: univ                Batch:  3/15	Loss 5.7706 (5.9405)
2022-11-09 17:43:55,640:INFO: Dataset: univ                Batch:  4/15	Loss 5.7280 (5.8862)
2022-11-09 17:43:55,666:INFO: Dataset: univ                Batch:  5/15	Loss 5.3153 (5.7691)
2022-11-09 17:43:55,693:INFO: Dataset: univ                Batch:  6/15	Loss 6.4793 (5.8873)
2022-11-09 17:43:55,718:INFO: Dataset: univ                Batch:  7/15	Loss 5.3088 (5.8014)
2022-11-09 17:43:55,744:INFO: Dataset: univ                Batch:  8/15	Loss 6.3238 (5.8693)
2022-11-09 17:43:55,770:INFO: Dataset: univ                Batch:  9/15	Loss 6.4006 (5.9218)
2022-11-09 17:43:55,796:INFO: Dataset: univ                Batch: 10/15	Loss 5.5441 (5.8830)
2022-11-09 17:43:55,824:INFO: Dataset: univ                Batch: 11/15	Loss 6.4299 (5.9313)
2022-11-09 17:43:55,850:INFO: Dataset: univ                Batch: 12/15	Loss 5.8374 (5.9229)
2022-11-09 17:43:55,875:INFO: Dataset: univ                Batch: 13/15	Loss 5.9898 (5.9280)
2022-11-09 17:43:55,901:INFO: Dataset: univ                Batch: 14/15	Loss 5.8991 (5.9259)
2022-11-09 17:43:55,911:INFO: Dataset: univ                Batch: 15/15	Loss 1.2929 (5.8713)
2022-11-09 17:43:56,173:INFO: Dataset: zara1               Batch: 1/8	Loss 11.4764 (11.4764)
2022-11-09 17:43:56,201:INFO: Dataset: zara1               Batch: 2/8	Loss 12.8672 (12.1581)
2022-11-09 17:43:56,228:INFO: Dataset: zara1               Batch: 3/8	Loss 10.2783 (11.5516)
2022-11-09 17:43:56,255:INFO: Dataset: zara1               Batch: 4/8	Loss 12.0584 (11.6701)
2022-11-09 17:43:56,279:INFO: Dataset: zara1               Batch: 5/8	Loss 10.9064 (11.5321)
2022-11-09 17:43:56,303:INFO: Dataset: zara1               Batch: 6/8	Loss 9.0152 (11.1379)
2022-11-09 17:43:56,326:INFO: Dataset: zara1               Batch: 7/8	Loss 10.5086 (11.0381)
2022-11-09 17:43:56,347:INFO: Dataset: zara1               Batch: 8/8	Loss 9.0220 (10.8015)
2022-11-09 17:43:56,609:INFO: Dataset: zara2               Batch:  1/18	Loss 7.1379 (7.1379)
2022-11-09 17:43:56,654:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2249 (7.1853)
2022-11-09 17:43:56,681:INFO: Dataset: zara2               Batch:  3/18	Loss 7.6929 (7.3538)
2022-11-09 17:43:56,704:INFO: Dataset: zara2               Batch:  4/18	Loss 7.1468 (7.2965)
2022-11-09 17:43:56,728:INFO: Dataset: zara2               Batch:  5/18	Loss 7.4307 (7.3244)
2022-11-09 17:43:56,755:INFO: Dataset: zara2               Batch:  6/18	Loss 7.1386 (7.2961)
2022-11-09 17:43:56,779:INFO: Dataset: zara2               Batch:  7/18	Loss 7.6915 (7.3559)
2022-11-09 17:43:56,802:INFO: Dataset: zara2               Batch:  8/18	Loss 7.7061 (7.3985)
2022-11-09 17:43:56,827:INFO: Dataset: zara2               Batch:  9/18	Loss 7.5371 (7.4123)
2022-11-09 17:43:56,851:INFO: Dataset: zara2               Batch: 10/18	Loss 7.7434 (7.4458)
2022-11-09 17:43:56,876:INFO: Dataset: zara2               Batch: 11/18	Loss 7.7232 (7.4693)
2022-11-09 17:43:56,902:INFO: Dataset: zara2               Batch: 12/18	Loss 7.4517 (7.4678)
2022-11-09 17:43:56,927:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9013 (7.4279)
2022-11-09 17:43:56,951:INFO: Dataset: zara2               Batch: 14/18	Loss 7.4308 (7.4281)
2022-11-09 17:43:56,979:INFO: Dataset: zara2               Batch: 15/18	Loss 6.8070 (7.3823)
2022-11-09 17:43:57,008:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2135 (7.4381)
2022-11-09 17:43:57,038:INFO: Dataset: zara2               Batch: 17/18	Loss 7.1129 (7.4153)
2022-11-09 17:43:57,065:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0443 (7.3476)
2022-11-09 17:43:57,124:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_12.pth.tar
2022-11-09 17:43:57,124:INFO: 
===> EPOCH: 13 (P1)
2022-11-09 17:43:57,125:INFO: - Computing loss (training)
2022-11-09 17:43:57,355:INFO: Dataset: hotel               Batch: 1/4	Loss 4.7287 (4.7287)
2022-11-09 17:43:57,381:INFO: Dataset: hotel               Batch: 2/4	Loss 4.6339 (4.6820)
2022-11-09 17:43:57,404:INFO: Dataset: hotel               Batch: 3/4	Loss 6.1581 (5.1364)
2022-11-09 17:43:57,422:INFO: Dataset: hotel               Batch: 4/4	Loss 3.6520 (4.9053)
2022-11-09 17:43:57,671:INFO: Dataset: univ                Batch:  1/15	Loss 5.7731 (5.7731)
2022-11-09 17:43:57,702:INFO: Dataset: univ                Batch:  2/15	Loss 5.7412 (5.7569)
2022-11-09 17:43:57,730:INFO: Dataset: univ                Batch:  3/15	Loss 5.5380 (5.6852)
2022-11-09 17:43:57,757:INFO: Dataset: univ                Batch:  4/15	Loss 6.5118 (5.8794)
2022-11-09 17:43:57,783:INFO: Dataset: univ                Batch:  5/15	Loss 5.8902 (5.8816)
2022-11-09 17:43:57,811:INFO: Dataset: univ                Batch:  6/15	Loss 5.4509 (5.8058)
2022-11-09 17:43:57,837:INFO: Dataset: univ                Batch:  7/15	Loss 4.9098 (5.6707)
2022-11-09 17:43:57,862:INFO: Dataset: univ                Batch:  8/15	Loss 6.2287 (5.7363)
2022-11-09 17:43:57,888:INFO: Dataset: univ                Batch:  9/15	Loss 6.5447 (5.8222)
2022-11-09 17:43:57,915:INFO: Dataset: univ                Batch: 10/15	Loss 6.3377 (5.8723)
2022-11-09 17:43:57,943:INFO: Dataset: univ                Batch: 11/15	Loss 5.4977 (5.8405)
2022-11-09 17:43:57,970:INFO: Dataset: univ                Batch: 12/15	Loss 7.6599 (5.9792)
2022-11-09 17:43:57,996:INFO: Dataset: univ                Batch: 13/15	Loss 5.7241 (5.9604)
2022-11-09 17:43:58,022:INFO: Dataset: univ                Batch: 14/15	Loss 5.2437 (5.9078)
2022-11-09 17:43:58,031:INFO: Dataset: univ                Batch: 15/15	Loss 0.9214 (5.8393)
2022-11-09 17:43:58,279:INFO: Dataset: zara1               Batch: 1/8	Loss 10.3655 (10.3655)
2022-11-09 17:43:58,304:INFO: Dataset: zara1               Batch: 2/8	Loss 10.2846 (10.3240)
2022-11-09 17:43:58,331:INFO: Dataset: zara1               Batch: 3/8	Loss 12.3809 (11.0573)
2022-11-09 17:43:58,355:INFO: Dataset: zara1               Batch: 4/8	Loss 9.7275 (10.7607)
2022-11-09 17:43:58,379:INFO: Dataset: zara1               Batch: 5/8	Loss 10.2410 (10.6554)
2022-11-09 17:43:58,404:INFO: Dataset: zara1               Batch: 6/8	Loss 10.5544 (10.6376)
2022-11-09 17:43:58,427:INFO: Dataset: zara1               Batch: 7/8	Loss 10.2751 (10.5852)
2022-11-09 17:43:58,448:INFO: Dataset: zara1               Batch: 8/8	Loss 8.8352 (10.4148)
2022-11-09 17:43:58,705:INFO: Dataset: zara2               Batch:  1/18	Loss 7.8733 (7.8733)
2022-11-09 17:43:58,734:INFO: Dataset: zara2               Batch:  2/18	Loss 6.6903 (7.2878)
2022-11-09 17:43:58,758:INFO: Dataset: zara2               Batch:  3/18	Loss 7.2038 (7.2623)
2022-11-09 17:43:58,781:INFO: Dataset: zara2               Batch:  4/18	Loss 7.7318 (7.3828)
2022-11-09 17:43:58,806:INFO: Dataset: zara2               Batch:  5/18	Loss 7.7930 (7.4722)
2022-11-09 17:43:58,833:INFO: Dataset: zara2               Batch:  6/18	Loss 7.3387 (7.4501)
2022-11-09 17:43:58,858:INFO: Dataset: zara2               Batch:  7/18	Loss 6.9236 (7.3756)
2022-11-09 17:43:58,885:INFO: Dataset: zara2               Batch:  8/18	Loss 8.1678 (7.4638)
2022-11-09 17:43:58,914:INFO: Dataset: zara2               Batch:  9/18	Loss 8.1318 (7.5379)
2022-11-09 17:43:58,942:INFO: Dataset: zara2               Batch: 10/18	Loss 7.7905 (7.5632)
2022-11-09 17:43:58,973:INFO: Dataset: zara2               Batch: 11/18	Loss 7.0906 (7.5176)
2022-11-09 17:43:59,000:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8641 (7.4612)
2022-11-09 17:43:59,028:INFO: Dataset: zara2               Batch: 13/18	Loss 7.0973 (7.4307)
2022-11-09 17:43:59,057:INFO: Dataset: zara2               Batch: 14/18	Loss 7.8850 (7.4619)
2022-11-09 17:43:59,087:INFO: Dataset: zara2               Batch: 15/18	Loss 7.6261 (7.4726)
2022-11-09 17:43:59,118:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4718 (7.4194)
2022-11-09 17:43:59,148:INFO: Dataset: zara2               Batch: 17/18	Loss 7.5189 (7.4252)
2022-11-09 17:43:59,177:INFO: Dataset: zara2               Batch: 18/18	Loss 5.3046 (7.3151)
2022-11-09 17:43:59,232:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_13.pth.tar
2022-11-09 17:43:59,232:INFO: 
===> EPOCH: 14 (P1)
2022-11-09 17:43:59,232:INFO: - Computing loss (training)
2022-11-09 17:43:59,450:INFO: Dataset: hotel               Batch: 1/4	Loss 4.6596 (4.6596)
2022-11-09 17:43:59,476:INFO: Dataset: hotel               Batch: 2/4	Loss 5.6722 (5.1945)
2022-11-09 17:43:59,501:INFO: Dataset: hotel               Batch: 3/4	Loss 5.6215 (5.3291)
2022-11-09 17:43:59,517:INFO: Dataset: hotel               Batch: 4/4	Loss 2.9429 (4.8915)
2022-11-09 17:43:59,765:INFO: Dataset: univ                Batch:  1/15	Loss 5.2892 (5.2892)
2022-11-09 17:43:59,793:INFO: Dataset: univ                Batch:  2/15	Loss 5.1640 (5.2267)
2022-11-09 17:43:59,824:INFO: Dataset: univ                Batch:  3/15	Loss 5.4770 (5.3108)
2022-11-09 17:43:59,849:INFO: Dataset: univ                Batch:  4/15	Loss 6.5953 (5.6285)
2022-11-09 17:43:59,875:INFO: Dataset: univ                Batch:  5/15	Loss 5.9323 (5.6835)
2022-11-09 17:43:59,903:INFO: Dataset: univ                Batch:  6/15	Loss 5.4134 (5.6388)
2022-11-09 17:43:59,928:INFO: Dataset: univ                Batch:  7/15	Loss 5.6092 (5.6350)
2022-11-09 17:43:59,953:INFO: Dataset: univ                Batch:  8/15	Loss 5.8028 (5.6555)
2022-11-09 17:43:59,979:INFO: Dataset: univ                Batch:  9/15	Loss 5.9137 (5.6845)
2022-11-09 17:44:00,005:INFO: Dataset: univ                Batch: 10/15	Loss 5.4315 (5.6561)
2022-11-09 17:44:00,032:INFO: Dataset: univ                Batch: 11/15	Loss 6.5146 (5.7340)
2022-11-09 17:44:00,059:INFO: Dataset: univ                Batch: 12/15	Loss 6.6684 (5.8032)
2022-11-09 17:44:00,085:INFO: Dataset: univ                Batch: 13/15	Loss 5.3555 (5.7688)
2022-11-09 17:44:00,110:INFO: Dataset: univ                Batch: 14/15	Loss 5.5849 (5.7561)
2022-11-09 17:44:00,119:INFO: Dataset: univ                Batch: 15/15	Loss 1.1626 (5.7058)
2022-11-09 17:44:00,377:INFO: Dataset: zara1               Batch: 1/8	Loss 10.8037 (10.8037)
2022-11-09 17:44:00,402:INFO: Dataset: zara1               Batch: 2/8	Loss 12.5042 (11.6504)
2022-11-09 17:44:00,426:INFO: Dataset: zara1               Batch: 3/8	Loss 9.8952 (11.0381)
2022-11-09 17:44:00,464:INFO: Dataset: zara1               Batch: 4/8	Loss 10.4454 (10.8935)
2022-11-09 17:44:00,503:INFO: Dataset: zara1               Batch: 5/8	Loss 10.5760 (10.8287)
2022-11-09 17:44:00,533:INFO: Dataset: zara1               Batch: 6/8	Loss 11.1342 (10.8771)
2022-11-09 17:44:00,560:INFO: Dataset: zara1               Batch: 7/8	Loss 12.0943 (11.0400)
2022-11-09 17:44:00,581:INFO: Dataset: zara1               Batch: 8/8	Loss 8.4601 (10.7236)
2022-11-09 17:44:00,924:INFO: Dataset: zara2               Batch:  1/18	Loss 7.4532 (7.4532)
2022-11-09 17:44:00,951:INFO: Dataset: zara2               Batch:  2/18	Loss 6.5853 (7.0241)
2022-11-09 17:44:00,977:INFO: Dataset: zara2               Batch:  3/18	Loss 7.5887 (7.2176)
2022-11-09 17:44:01,001:INFO: Dataset: zara2               Batch:  4/18	Loss 6.9903 (7.1579)
2022-11-09 17:44:01,024:INFO: Dataset: zara2               Batch:  5/18	Loss 8.0474 (7.3386)
2022-11-09 17:44:01,052:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8976 (7.2565)
2022-11-09 17:44:01,076:INFO: Dataset: zara2               Batch:  7/18	Loss 7.4835 (7.2903)
2022-11-09 17:44:01,099:INFO: Dataset: zara2               Batch:  8/18	Loss 7.0169 (7.2542)
2022-11-09 17:44:01,123:INFO: Dataset: zara2               Batch:  9/18	Loss 7.8985 (7.3240)
2022-11-09 17:44:01,147:INFO: Dataset: zara2               Batch: 10/18	Loss 6.8507 (7.2780)
2022-11-09 17:44:01,171:INFO: Dataset: zara2               Batch: 11/18	Loss 7.2881 (7.2790)
2022-11-09 17:44:01,197:INFO: Dataset: zara2               Batch: 12/18	Loss 7.4920 (7.2966)
2022-11-09 17:44:01,222:INFO: Dataset: zara2               Batch: 13/18	Loss 7.5360 (7.3149)
2022-11-09 17:44:01,247:INFO: Dataset: zara2               Batch: 14/18	Loss 7.1006 (7.3008)
2022-11-09 17:44:01,271:INFO: Dataset: zara2               Batch: 15/18	Loss 7.0604 (7.2845)
2022-11-09 17:44:01,296:INFO: Dataset: zara2               Batch: 16/18	Loss 7.1135 (7.2728)
2022-11-09 17:44:01,321:INFO: Dataset: zara2               Batch: 17/18	Loss 7.2409 (7.2710)
2022-11-09 17:44:01,343:INFO: Dataset: zara2               Batch: 18/18	Loss 6.7299 (7.2440)
2022-11-09 17:44:01,392:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_14.pth.tar
2022-11-09 17:44:01,392:INFO: 
===> EPOCH: 15 (P1)
2022-11-09 17:44:01,393:INFO: - Computing loss (training)
2022-11-09 17:44:01,591:INFO: Dataset: hotel               Batch: 1/4	Loss 6.8195 (6.8195)
2022-11-09 17:44:01,617:INFO: Dataset: hotel               Batch: 2/4	Loss 4.9167 (5.8425)
2022-11-09 17:44:01,643:INFO: Dataset: hotel               Batch: 3/4	Loss 4.5668 (5.3963)
2022-11-09 17:44:01,660:INFO: Dataset: hotel               Batch: 4/4	Loss 2.8130 (4.9567)
2022-11-09 17:44:01,914:INFO: Dataset: univ                Batch:  1/15	Loss 6.0888 (6.0888)
2022-11-09 17:44:01,942:INFO: Dataset: univ                Batch:  2/15	Loss 5.9825 (6.0370)
2022-11-09 17:44:01,967:INFO: Dataset: univ                Batch:  3/15	Loss 5.7873 (5.9541)
2022-11-09 17:44:01,996:INFO: Dataset: univ                Batch:  4/15	Loss 5.4642 (5.8264)
2022-11-09 17:44:02,021:INFO: Dataset: univ                Batch:  5/15	Loss 5.5239 (5.7617)
2022-11-09 17:44:02,048:INFO: Dataset: univ                Batch:  6/15	Loss 6.4834 (5.8765)
2022-11-09 17:44:02,073:INFO: Dataset: univ                Batch:  7/15	Loss 6.0418 (5.8992)
2022-11-09 17:44:02,098:INFO: Dataset: univ                Batch:  8/15	Loss 7.4877 (6.0820)
2022-11-09 17:44:02,123:INFO: Dataset: univ                Batch:  9/15	Loss 5.6877 (6.0350)
2022-11-09 17:44:02,150:INFO: Dataset: univ                Batch: 10/15	Loss 5.8220 (6.0110)
2022-11-09 17:44:02,179:INFO: Dataset: univ                Batch: 11/15	Loss 5.7233 (5.9853)
2022-11-09 17:44:02,205:INFO: Dataset: univ                Batch: 12/15	Loss 5.6698 (5.9586)
2022-11-09 17:44:02,230:INFO: Dataset: univ                Batch: 13/15	Loss 5.0468 (5.8792)
2022-11-09 17:44:02,255:INFO: Dataset: univ                Batch: 14/15	Loss 5.3409 (5.8399)
2022-11-09 17:44:02,264:INFO: Dataset: univ                Batch: 15/15	Loss 0.9661 (5.7762)
2022-11-09 17:44:02,519:INFO: Dataset: zara1               Batch: 1/8	Loss 11.9265 (11.9265)
2022-11-09 17:44:02,544:INFO: Dataset: zara1               Batch: 2/8	Loss 9.0598 (10.4644)
2022-11-09 17:44:02,568:INFO: Dataset: zara1               Batch: 3/8	Loss 11.4313 (10.7625)
2022-11-09 17:44:02,593:INFO: Dataset: zara1               Batch: 4/8	Loss 10.6368 (10.7332)
2022-11-09 17:44:02,616:INFO: Dataset: zara1               Batch: 5/8	Loss 9.3050 (10.4377)
2022-11-09 17:44:02,642:INFO: Dataset: zara1               Batch: 6/8	Loss 9.5320 (10.2893)
2022-11-09 17:44:02,665:INFO: Dataset: zara1               Batch: 7/8	Loss 10.4510 (10.3128)
2022-11-09 17:44:02,686:INFO: Dataset: zara1               Batch: 8/8	Loss 8.0257 (10.0203)
2022-11-09 17:44:02,937:INFO: Dataset: zara2               Batch:  1/18	Loss 6.9873 (6.9873)
2022-11-09 17:44:02,964:INFO: Dataset: zara2               Batch:  2/18	Loss 7.4397 (7.2051)
2022-11-09 17:44:02,990:INFO: Dataset: zara2               Batch:  3/18	Loss 6.3594 (6.9244)
2022-11-09 17:44:03,015:INFO: Dataset: zara2               Batch:  4/18	Loss 7.1244 (6.9764)
2022-11-09 17:44:03,040:INFO: Dataset: zara2               Batch:  5/18	Loss 6.4697 (6.8704)
2022-11-09 17:44:03,067:INFO: Dataset: zara2               Batch:  6/18	Loss 6.7654 (6.8507)
2022-11-09 17:44:03,092:INFO: Dataset: zara2               Batch:  7/18	Loss 7.9607 (7.0059)
2022-11-09 17:44:03,115:INFO: Dataset: zara2               Batch:  8/18	Loss 7.6392 (7.0839)
2022-11-09 17:44:03,139:INFO: Dataset: zara2               Batch:  9/18	Loss 7.1868 (7.0957)
2022-11-09 17:44:03,163:INFO: Dataset: zara2               Batch: 10/18	Loss 6.7400 (7.0620)
2022-11-09 17:44:03,188:INFO: Dataset: zara2               Batch: 11/18	Loss 7.3505 (7.0875)
2022-11-09 17:44:03,214:INFO: Dataset: zara2               Batch: 12/18	Loss 7.9800 (7.1647)
2022-11-09 17:44:03,238:INFO: Dataset: zara2               Batch: 13/18	Loss 7.7245 (7.2058)
2022-11-09 17:44:03,262:INFO: Dataset: zara2               Batch: 14/18	Loss 6.9133 (7.1833)
2022-11-09 17:44:03,286:INFO: Dataset: zara2               Batch: 15/18	Loss 7.2490 (7.1878)
2022-11-09 17:44:03,311:INFO: Dataset: zara2               Batch: 16/18	Loss 7.2827 (7.1936)
2022-11-09 17:44:03,335:INFO: Dataset: zara2               Batch: 17/18	Loss 6.4579 (7.1498)
2022-11-09 17:44:03,357:INFO: Dataset: zara2               Batch: 18/18	Loss 5.4944 (7.0669)
2022-11-09 17:44:03,405:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_15.pth.tar
2022-11-09 17:44:03,405:INFO: 
===> EPOCH: 16 (P1)
2022-11-09 17:44:03,405:INFO: - Computing loss (training)
2022-11-09 17:44:03,616:INFO: Dataset: hotel               Batch: 1/4	Loss 4.3863 (4.3863)
2022-11-09 17:44:03,642:INFO: Dataset: hotel               Batch: 2/4	Loss 5.2953 (4.8441)
2022-11-09 17:44:03,665:INFO: Dataset: hotel               Batch: 3/4	Loss 4.4127 (4.6956)
2022-11-09 17:44:03,682:INFO: Dataset: hotel               Batch: 4/4	Loss 4.6914 (4.6949)
2022-11-09 17:44:03,947:INFO: Dataset: univ                Batch:  1/15	Loss 5.4837 (5.4837)
2022-11-09 17:44:03,975:INFO: Dataset: univ                Batch:  2/15	Loss 5.8009 (5.6378)
2022-11-09 17:44:04,000:INFO: Dataset: univ                Batch:  3/15	Loss 6.6039 (5.9653)
2022-11-09 17:44:04,025:INFO: Dataset: univ                Batch:  4/15	Loss 5.1933 (5.7740)
2022-11-09 17:44:04,053:INFO: Dataset: univ                Batch:  5/15	Loss 6.4753 (5.9055)
2022-11-09 17:44:04,081:INFO: Dataset: univ                Batch:  6/15	Loss 5.4259 (5.8298)
2022-11-09 17:44:04,106:INFO: Dataset: univ                Batch:  7/15	Loss 5.2376 (5.7449)
2022-11-09 17:44:04,131:INFO: Dataset: univ                Batch:  8/15	Loss 6.4119 (5.8253)
2022-11-09 17:44:04,156:INFO: Dataset: univ                Batch:  9/15	Loss 5.4478 (5.7853)
2022-11-09 17:44:04,182:INFO: Dataset: univ                Batch: 10/15	Loss 5.3916 (5.7443)
2022-11-09 17:44:04,209:INFO: Dataset: univ                Batch: 11/15	Loss 5.5561 (5.7252)
2022-11-09 17:44:04,236:INFO: Dataset: univ                Batch: 12/15	Loss 5.0597 (5.6682)
2022-11-09 17:44:04,261:INFO: Dataset: univ                Batch: 13/15	Loss 6.9269 (5.7475)
2022-11-09 17:44:04,287:INFO: Dataset: univ                Batch: 14/15	Loss 5.4448 (5.7242)
2022-11-09 17:44:04,296:INFO: Dataset: univ                Batch: 15/15	Loss 0.7318 (5.6447)
2022-11-09 17:44:04,560:INFO: Dataset: zara1               Batch: 1/8	Loss 11.0761 (11.0761)
2022-11-09 17:44:04,584:INFO: Dataset: zara1               Batch: 2/8	Loss 13.4495 (12.2142)
2022-11-09 17:44:04,608:INFO: Dataset: zara1               Batch: 3/8	Loss 13.0693 (12.4741)
2022-11-09 17:44:04,631:INFO: Dataset: zara1               Batch: 4/8	Loss 11.7721 (12.3214)
2022-11-09 17:44:04,657:INFO: Dataset: zara1               Batch: 5/8	Loss 9.5190 (11.7097)
2022-11-09 17:44:04,681:INFO: Dataset: zara1               Batch: 6/8	Loss 9.1635 (11.2854)
2022-11-09 17:44:04,705:INFO: Dataset: zara1               Batch: 7/8	Loss 8.6731 (10.9108)
2022-11-09 17:44:04,726:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6452 (10.5465)
2022-11-09 17:44:04,978:INFO: Dataset: zara2               Batch:  1/18	Loss 6.8490 (6.8490)
2022-11-09 17:44:05,003:INFO: Dataset: zara2               Batch:  2/18	Loss 6.9061 (6.8767)
2022-11-09 17:44:05,029:INFO: Dataset: zara2               Batch:  3/18	Loss 7.8583 (7.1926)
2022-11-09 17:44:05,055:INFO: Dataset: zara2               Batch:  4/18	Loss 7.7391 (7.3319)
2022-11-09 17:44:05,079:INFO: Dataset: zara2               Batch:  5/18	Loss 8.0033 (7.4753)
2022-11-09 17:44:05,105:INFO: Dataset: zara2               Batch:  6/18	Loss 7.9604 (7.5611)
2022-11-09 17:44:05,129:INFO: Dataset: zara2               Batch:  7/18	Loss 6.6002 (7.4201)
2022-11-09 17:44:05,152:INFO: Dataset: zara2               Batch:  8/18	Loss 7.0864 (7.3798)
2022-11-09 17:44:05,176:INFO: Dataset: zara2               Batch:  9/18	Loss 6.8209 (7.3164)
2022-11-09 17:44:05,200:INFO: Dataset: zara2               Batch: 10/18	Loss 7.1818 (7.3022)
2022-11-09 17:44:05,226:INFO: Dataset: zara2               Batch: 11/18	Loss 6.6345 (7.2398)
2022-11-09 17:44:05,252:INFO: Dataset: zara2               Batch: 12/18	Loss 7.2385 (7.2397)
2022-11-09 17:44:05,277:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9092 (7.2172)
2022-11-09 17:44:05,301:INFO: Dataset: zara2               Batch: 14/18	Loss 7.3006 (7.2229)
2022-11-09 17:44:05,326:INFO: Dataset: zara2               Batch: 15/18	Loss 7.1494 (7.2180)
2022-11-09 17:44:05,350:INFO: Dataset: zara2               Batch: 16/18	Loss 6.9760 (7.2040)
2022-11-09 17:44:05,375:INFO: Dataset: zara2               Batch: 17/18	Loss 6.9653 (7.1904)
2022-11-09 17:44:05,397:INFO: Dataset: zara2               Batch: 18/18	Loss 5.9934 (7.1304)
2022-11-09 17:44:05,446:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_16.pth.tar
2022-11-09 17:44:05,446:INFO: 
===> EPOCH: 17 (P1)
2022-11-09 17:44:05,447:INFO: - Computing loss (training)
2022-11-09 17:44:05,659:INFO: Dataset: hotel               Batch: 1/4	Loss 4.8131 (4.8131)
2022-11-09 17:44:05,684:INFO: Dataset: hotel               Batch: 2/4	Loss 4.6938 (4.7533)
2022-11-09 17:44:05,710:INFO: Dataset: hotel               Batch: 3/4	Loss 6.4403 (5.3165)
2022-11-09 17:44:05,727:INFO: Dataset: hotel               Batch: 4/4	Loss 2.6247 (4.8691)
2022-11-09 17:44:05,968:INFO: Dataset: univ                Batch:  1/15	Loss 5.3812 (5.3812)
2022-11-09 17:44:05,996:INFO: Dataset: univ                Batch:  2/15	Loss 5.3432 (5.3623)
2022-11-09 17:44:06,021:INFO: Dataset: univ                Batch:  3/15	Loss 5.3994 (5.3749)
2022-11-09 17:44:06,050:INFO: Dataset: univ                Batch:  4/15	Loss 5.1316 (5.3064)
2022-11-09 17:44:06,076:INFO: Dataset: univ                Batch:  5/15	Loss 5.4524 (5.3367)
2022-11-09 17:44:06,102:INFO: Dataset: univ                Batch:  6/15	Loss 6.0550 (5.4434)
2022-11-09 17:44:06,127:INFO: Dataset: univ                Batch:  7/15	Loss 5.8826 (5.5070)
2022-11-09 17:44:06,152:INFO: Dataset: univ                Batch:  8/15	Loss 5.7599 (5.5366)
2022-11-09 17:44:06,177:INFO: Dataset: univ                Batch:  9/15	Loss 6.1197 (5.5973)
2022-11-09 17:44:06,203:INFO: Dataset: univ                Batch: 10/15	Loss 5.6396 (5.6019)
2022-11-09 17:44:06,232:INFO: Dataset: univ                Batch: 11/15	Loss 5.0724 (5.5524)
2022-11-09 17:44:06,258:INFO: Dataset: univ                Batch: 12/15	Loss 5.1672 (5.5205)
2022-11-09 17:44:06,284:INFO: Dataset: univ                Batch: 13/15	Loss 5.0530 (5.4800)
2022-11-09 17:44:06,309:INFO: Dataset: univ                Batch: 14/15	Loss 6.4794 (5.5482)
2022-11-09 17:44:06,319:INFO: Dataset: univ                Batch: 15/15	Loss 1.0597 (5.4975)
2022-11-09 17:44:06,564:INFO: Dataset: zara1               Batch: 1/8	Loss 11.6949 (11.6949)
2022-11-09 17:44:06,590:INFO: Dataset: zara1               Batch: 2/8	Loss 12.3244 (12.0025)
2022-11-09 17:44:06,614:INFO: Dataset: zara1               Batch: 3/8	Loss 10.5341 (11.4715)
2022-11-09 17:44:06,640:INFO: Dataset: zara1               Batch: 4/8	Loss 9.5898 (11.0355)
2022-11-09 17:44:06,664:INFO: Dataset: zara1               Batch: 5/8	Loss 9.7906 (10.7997)
2022-11-09 17:44:06,688:INFO: Dataset: zara1               Batch: 6/8	Loss 9.5446 (10.5884)
2022-11-09 17:44:06,712:INFO: Dataset: zara1               Batch: 7/8	Loss 9.4052 (10.4115)
2022-11-09 17:44:06,733:INFO: Dataset: zara1               Batch: 8/8	Loss 10.3991 (10.4105)
2022-11-09 17:44:07,088:INFO: Dataset: zara2               Batch:  1/18	Loss 7.3675 (7.3675)
2022-11-09 17:44:07,119:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1544 (7.2565)
2022-11-09 17:44:07,143:INFO: Dataset: zara2               Batch:  3/18	Loss 7.7299 (7.4233)
2022-11-09 17:44:07,166:INFO: Dataset: zara2               Batch:  4/18	Loss 7.9018 (7.5506)
2022-11-09 17:44:07,190:INFO: Dataset: zara2               Batch:  5/18	Loss 7.4119 (7.5239)
2022-11-09 17:44:07,216:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8376 (7.3979)
2022-11-09 17:44:07,240:INFO: Dataset: zara2               Batch:  7/18	Loss 7.3977 (7.3978)
2022-11-09 17:44:07,263:INFO: Dataset: zara2               Batch:  8/18	Loss 7.5350 (7.4140)
2022-11-09 17:44:07,286:INFO: Dataset: zara2               Batch:  9/18	Loss 7.0589 (7.3713)
2022-11-09 17:44:07,312:INFO: Dataset: zara2               Batch: 10/18	Loss 7.0383 (7.3362)
2022-11-09 17:44:07,338:INFO: Dataset: zara2               Batch: 11/18	Loss 7.7391 (7.3762)
2022-11-09 17:44:07,365:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8176 (7.3309)
2022-11-09 17:44:07,391:INFO: Dataset: zara2               Batch: 13/18	Loss 7.7506 (7.3613)
2022-11-09 17:44:07,416:INFO: Dataset: zara2               Batch: 14/18	Loss 8.0847 (7.4030)
2022-11-09 17:44:07,441:INFO: Dataset: zara2               Batch: 15/18	Loss 8.2062 (7.4567)
2022-11-09 17:44:07,467:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4302 (7.3917)
2022-11-09 17:44:07,491:INFO: Dataset: zara2               Batch: 17/18	Loss 7.8676 (7.4198)
2022-11-09 17:44:07,513:INFO: Dataset: zara2               Batch: 18/18	Loss 5.6156 (7.3387)
2022-11-09 17:44:07,562:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_17.pth.tar
2022-11-09 17:44:07,562:INFO: 
===> EPOCH: 18 (P1)
2022-11-09 17:44:07,563:INFO: - Computing loss (training)
2022-11-09 17:44:07,775:INFO: Dataset: hotel               Batch: 1/4	Loss 4.8273 (4.8273)
2022-11-09 17:44:07,800:INFO: Dataset: hotel               Batch: 2/4	Loss 6.5843 (5.6616)
2022-11-09 17:44:07,825:INFO: Dataset: hotel               Batch: 3/4	Loss 4.5567 (5.2800)
2022-11-09 17:44:07,841:INFO: Dataset: hotel               Batch: 4/4	Loss 3.0857 (4.9297)
2022-11-09 17:44:08,103:INFO: Dataset: univ                Batch:  1/15	Loss 6.0097 (6.0097)
2022-11-09 17:44:08,132:INFO: Dataset: univ                Batch:  2/15	Loss 5.6628 (5.8366)
2022-11-09 17:44:08,158:INFO: Dataset: univ                Batch:  3/15	Loss 6.0927 (5.9177)
2022-11-09 17:44:08,184:INFO: Dataset: univ                Batch:  4/15	Loss 5.4286 (5.7908)
2022-11-09 17:44:08,213:INFO: Dataset: univ                Batch:  5/15	Loss 5.9318 (5.8184)
2022-11-09 17:44:08,242:INFO: Dataset: univ                Batch:  6/15	Loss 5.6868 (5.7948)
2022-11-09 17:44:08,267:INFO: Dataset: univ                Batch:  7/15	Loss 5.3281 (5.7292)
2022-11-09 17:44:08,292:INFO: Dataset: univ                Batch:  8/15	Loss 5.8983 (5.7512)
2022-11-09 17:44:08,318:INFO: Dataset: univ                Batch:  9/15	Loss 5.3400 (5.7014)
2022-11-09 17:44:08,345:INFO: Dataset: univ                Batch: 10/15	Loss 6.1623 (5.7468)
2022-11-09 17:44:08,371:INFO: Dataset: univ                Batch: 11/15	Loss 5.6520 (5.7380)
2022-11-09 17:44:08,398:INFO: Dataset: univ                Batch: 12/15	Loss 5.4157 (5.7113)
2022-11-09 17:44:08,423:INFO: Dataset: univ                Batch: 13/15	Loss 5.5999 (5.7025)
2022-11-09 17:44:08,448:INFO: Dataset: univ                Batch: 14/15	Loss 5.2577 (5.6685)
2022-11-09 17:44:08,458:INFO: Dataset: univ                Batch: 15/15	Loss 1.1959 (5.6199)
2022-11-09 17:44:08,708:INFO: Dataset: zara1               Batch: 1/8	Loss 12.5730 (12.5730)
2022-11-09 17:44:08,733:INFO: Dataset: zara1               Batch: 2/8	Loss 13.9661 (13.2571)
2022-11-09 17:44:08,758:INFO: Dataset: zara1               Batch: 3/8	Loss 11.2618 (12.6290)
2022-11-09 17:44:08,783:INFO: Dataset: zara1               Batch: 4/8	Loss 9.9947 (11.9880)
2022-11-09 17:44:08,807:INFO: Dataset: zara1               Batch: 5/8	Loss 11.0355 (11.8211)
2022-11-09 17:44:08,831:INFO: Dataset: zara1               Batch: 6/8	Loss 9.8470 (11.4785)
2022-11-09 17:44:08,854:INFO: Dataset: zara1               Batch: 7/8	Loss 9.5964 (11.1916)
2022-11-09 17:44:08,875:INFO: Dataset: zara1               Batch: 8/8	Loss 9.3917 (10.9889)
2022-11-09 17:44:09,133:INFO: Dataset: zara2               Batch:  1/18	Loss 7.5111 (7.5111)
2022-11-09 17:44:09,158:INFO: Dataset: zara2               Batch:  2/18	Loss 7.0415 (7.2748)
2022-11-09 17:44:09,182:INFO: Dataset: zara2               Batch:  3/18	Loss 7.2967 (7.2825)
2022-11-09 17:44:09,206:INFO: Dataset: zara2               Batch:  4/18	Loss 6.8956 (7.1887)
2022-11-09 17:44:09,232:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1556 (7.1821)
2022-11-09 17:44:09,259:INFO: Dataset: zara2               Batch:  6/18	Loss 6.9344 (7.1369)
2022-11-09 17:44:09,282:INFO: Dataset: zara2               Batch:  7/18	Loss 7.0190 (7.1202)
2022-11-09 17:44:09,306:INFO: Dataset: zara2               Batch:  8/18	Loss 7.2273 (7.1336)
2022-11-09 17:44:09,329:INFO: Dataset: zara2               Batch:  9/18	Loss 7.2302 (7.1460)
2022-11-09 17:44:09,354:INFO: Dataset: zara2               Batch: 10/18	Loss 6.5678 (7.0895)
2022-11-09 17:44:09,379:INFO: Dataset: zara2               Batch: 11/18	Loss 6.6822 (7.0493)
2022-11-09 17:44:09,405:INFO: Dataset: zara2               Batch: 12/18	Loss 7.0243 (7.0471)
2022-11-09 17:44:09,429:INFO: Dataset: zara2               Batch: 13/18	Loss 6.5200 (7.0010)
2022-11-09 17:44:09,453:INFO: Dataset: zara2               Batch: 14/18	Loss 6.9677 (6.9987)
2022-11-09 17:44:09,478:INFO: Dataset: zara2               Batch: 15/18	Loss 7.5429 (7.0351)
2022-11-09 17:44:09,503:INFO: Dataset: zara2               Batch: 16/18	Loss 7.0637 (7.0369)
2022-11-09 17:44:09,527:INFO: Dataset: zara2               Batch: 17/18	Loss 8.1155 (7.0997)
2022-11-09 17:44:09,549:INFO: Dataset: zara2               Batch: 18/18	Loss 5.8320 (7.0446)
2022-11-09 17:44:09,598:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_18.pth.tar
2022-11-09 17:44:09,598:INFO: 
===> EPOCH: 19 (P1)
2022-11-09 17:44:09,599:INFO: - Computing loss (training)
2022-11-09 17:44:09,822:INFO: Dataset: hotel               Batch: 1/4	Loss 5.0943 (5.0943)
2022-11-09 17:44:09,848:INFO: Dataset: hotel               Batch: 2/4	Loss 6.1700 (5.6437)
2022-11-09 17:44:09,871:INFO: Dataset: hotel               Batch: 3/4	Loss 4.3500 (5.1970)
2022-11-09 17:44:09,888:INFO: Dataset: hotel               Batch: 4/4	Loss 2.8011 (4.8240)
2022-11-09 17:44:10,159:INFO: Dataset: univ                Batch:  1/15	Loss 6.9796 (6.9796)
2022-11-09 17:44:10,186:INFO: Dataset: univ                Batch:  2/15	Loss 5.4349 (6.1509)
2022-11-09 17:44:10,213:INFO: Dataset: univ                Batch:  3/15	Loss 6.2661 (6.1906)
2022-11-09 17:44:10,241:INFO: Dataset: univ                Batch:  4/15	Loss 5.7920 (6.0899)
2022-11-09 17:44:10,267:INFO: Dataset: univ                Batch:  5/15	Loss 5.6490 (5.9995)
2022-11-09 17:44:10,295:INFO: Dataset: univ                Batch:  6/15	Loss 5.4652 (5.9073)
2022-11-09 17:44:10,321:INFO: Dataset: univ                Batch:  7/15	Loss 5.4691 (5.8410)
2022-11-09 17:44:10,346:INFO: Dataset: univ                Batch:  8/15	Loss 5.1772 (5.7556)
2022-11-09 17:44:10,371:INFO: Dataset: univ                Batch:  9/15	Loss 6.1577 (5.7920)
2022-11-09 17:44:10,398:INFO: Dataset: univ                Batch: 10/15	Loss 5.4111 (5.7519)
2022-11-09 17:44:10,428:INFO: Dataset: univ                Batch: 11/15	Loss 5.3656 (5.7175)
2022-11-09 17:44:10,454:INFO: Dataset: univ                Batch: 12/15	Loss 5.0141 (5.6542)
2022-11-09 17:44:10,479:INFO: Dataset: univ                Batch: 13/15	Loss 6.0394 (5.6834)
2022-11-09 17:44:10,505:INFO: Dataset: univ                Batch: 14/15	Loss 4.8715 (5.6209)
2022-11-09 17:44:10,514:INFO: Dataset: univ                Batch: 15/15	Loss 0.8925 (5.5548)
2022-11-09 17:44:10,761:INFO: Dataset: zara1               Batch: 1/8	Loss 10.9333 (10.9333)
2022-11-09 17:44:10,786:INFO: Dataset: zara1               Batch: 2/8	Loss 10.5902 (10.7527)
2022-11-09 17:44:10,810:INFO: Dataset: zara1               Batch: 3/8	Loss 10.0229 (10.5188)
2022-11-09 17:44:10,833:INFO: Dataset: zara1               Batch: 4/8	Loss 11.4349 (10.7581)
2022-11-09 17:44:10,857:INFO: Dataset: zara1               Batch: 5/8	Loss 10.4670 (10.7005)
2022-11-09 17:44:10,883:INFO: Dataset: zara1               Batch: 6/8	Loss 9.9667 (10.5796)
2022-11-09 17:44:10,906:INFO: Dataset: zara1               Batch: 7/8	Loss 10.0626 (10.5155)
2022-11-09 17:44:10,927:INFO: Dataset: zara1               Batch: 8/8	Loss 8.8553 (10.3206)
2022-11-09 17:44:11,166:INFO: Dataset: zara2               Batch:  1/18	Loss 6.8727 (6.8727)
2022-11-09 17:44:11,192:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1781 (7.0312)
2022-11-09 17:44:11,217:INFO: Dataset: zara2               Batch:  3/18	Loss 7.6331 (7.2246)
2022-11-09 17:44:11,245:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0509 (7.1809)
2022-11-09 17:44:11,271:INFO: Dataset: zara2               Batch:  5/18	Loss 7.5065 (7.2510)
2022-11-09 17:44:11,296:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8222 (7.1762)
2022-11-09 17:44:11,320:INFO: Dataset: zara2               Batch:  7/18	Loss 7.9725 (7.2865)
2022-11-09 17:44:11,343:INFO: Dataset: zara2               Batch:  8/18	Loss 7.6275 (7.3303)
2022-11-09 17:44:11,367:INFO: Dataset: zara2               Batch:  9/18	Loss 7.2402 (7.3203)
2022-11-09 17:44:11,391:INFO: Dataset: zara2               Batch: 10/18	Loss 6.6016 (7.2406)
2022-11-09 17:44:11,415:INFO: Dataset: zara2               Batch: 11/18	Loss 7.8097 (7.2874)
2022-11-09 17:44:11,441:INFO: Dataset: zara2               Batch: 12/18	Loss 7.3200 (7.2902)
2022-11-09 17:44:11,465:INFO: Dataset: zara2               Batch: 13/18	Loss 6.5798 (7.2328)
2022-11-09 17:44:11,488:INFO: Dataset: zara2               Batch: 14/18	Loss 6.9234 (7.2110)
2022-11-09 17:44:11,513:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7471 (7.1774)
2022-11-09 17:44:11,538:INFO: Dataset: zara2               Batch: 16/18	Loss 6.9484 (7.1632)
2022-11-09 17:44:11,562:INFO: Dataset: zara2               Batch: 17/18	Loss 6.4111 (7.1193)
2022-11-09 17:44:11,585:INFO: Dataset: zara2               Batch: 18/18	Loss 5.8262 (7.0564)
2022-11-09 17:44:11,634:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_19.pth.tar
2022-11-09 17:44:11,634:INFO: 
===> EPOCH: 20 (P1)
2022-11-09 17:44:11,634:INFO: - Computing loss (training)
2022-11-09 17:44:11,844:INFO: Dataset: hotel               Batch: 1/4	Loss 6.3205 (6.3205)
2022-11-09 17:44:11,870:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4296 (5.3819)
2022-11-09 17:44:11,895:INFO: Dataset: hotel               Batch: 3/4	Loss 4.9956 (5.2517)
2022-11-09 17:44:11,911:INFO: Dataset: hotel               Batch: 4/4	Loss 3.2866 (4.9095)
2022-11-09 17:44:12,163:INFO: Dataset: univ                Batch:  1/15	Loss 6.7587 (6.7587)
2022-11-09 17:44:12,192:INFO: Dataset: univ                Batch:  2/15	Loss 5.1892 (5.9920)
2022-11-09 17:44:12,222:INFO: Dataset: univ                Batch:  3/15	Loss 5.0488 (5.6527)
2022-11-09 17:44:12,247:INFO: Dataset: univ                Batch:  4/15	Loss 5.7876 (5.6835)
2022-11-09 17:44:12,272:INFO: Dataset: univ                Batch:  5/15	Loss 5.3584 (5.6163)
2022-11-09 17:44:12,302:INFO: Dataset: univ                Batch:  6/15	Loss 5.2425 (5.5578)
2022-11-09 17:44:12,327:INFO: Dataset: univ                Batch:  7/15	Loss 5.2091 (5.5053)
2022-11-09 17:44:12,352:INFO: Dataset: univ                Batch:  8/15	Loss 6.1599 (5.5823)
2022-11-09 17:44:12,378:INFO: Dataset: univ                Batch:  9/15	Loss 5.2246 (5.5429)
2022-11-09 17:44:12,405:INFO: Dataset: univ                Batch: 10/15	Loss 5.9005 (5.5793)
2022-11-09 17:44:12,432:INFO: Dataset: univ                Batch: 11/15	Loss 5.8570 (5.6036)
2022-11-09 17:44:12,460:INFO: Dataset: univ                Batch: 12/15	Loss 6.8622 (5.7082)
2022-11-09 17:44:12,485:INFO: Dataset: univ                Batch: 13/15	Loss 5.7202 (5.7092)
2022-11-09 17:44:12,510:INFO: Dataset: univ                Batch: 14/15	Loss 5.7916 (5.7149)
2022-11-09 17:44:12,520:INFO: Dataset: univ                Batch: 15/15	Loss 1.1275 (5.6653)
2022-11-09 17:44:12,777:INFO: Dataset: zara1               Batch: 1/8	Loss 10.0456 (10.0456)
2022-11-09 17:44:12,803:INFO: Dataset: zara1               Batch: 2/8	Loss 9.5523 (9.7873)
2022-11-09 17:44:12,826:INFO: Dataset: zara1               Batch: 3/8	Loss 12.7748 (10.7570)
2022-11-09 17:44:12,849:INFO: Dataset: zara1               Batch: 4/8	Loss 12.4997 (11.1927)
2022-11-09 17:44:12,872:INFO: Dataset: zara1               Batch: 5/8	Loss 10.0648 (10.9655)
2022-11-09 17:44:12,898:INFO: Dataset: zara1               Batch: 6/8	Loss 10.0736 (10.7935)
2022-11-09 17:44:12,921:INFO: Dataset: zara1               Batch: 7/8	Loss 10.3998 (10.7314)
2022-11-09 17:44:12,942:INFO: Dataset: zara1               Batch: 8/8	Loss 9.4087 (10.5832)
2022-11-09 17:44:13,197:INFO: Dataset: zara2               Batch:  1/18	Loss 7.3202 (7.3202)
2022-11-09 17:44:13,223:INFO: Dataset: zara2               Batch:  2/18	Loss 6.9890 (7.1520)
2022-11-09 17:44:13,247:INFO: Dataset: zara2               Batch:  3/18	Loss 7.3562 (7.2292)
2022-11-09 17:44:13,270:INFO: Dataset: zara2               Batch:  4/18	Loss 8.0221 (7.4296)
2022-11-09 17:44:13,298:INFO: Dataset: zara2               Batch:  5/18	Loss 7.0120 (7.3438)
2022-11-09 17:44:13,324:INFO: Dataset: zara2               Batch:  6/18	Loss 6.6601 (7.2278)
2022-11-09 17:44:13,348:INFO: Dataset: zara2               Batch:  7/18	Loss 6.0648 (7.0674)
2022-11-09 17:44:13,371:INFO: Dataset: zara2               Batch:  8/18	Loss 7.0709 (7.0678)
2022-11-09 17:44:13,394:INFO: Dataset: zara2               Batch:  9/18	Loss 7.1440 (7.0769)
2022-11-09 17:44:13,419:INFO: Dataset: zara2               Batch: 10/18	Loss 7.0412 (7.0736)
2022-11-09 17:44:13,444:INFO: Dataset: zara2               Batch: 11/18	Loss 7.3271 (7.0950)
2022-11-09 17:44:13,471:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8620 (7.0735)
2022-11-09 17:44:13,495:INFO: Dataset: zara2               Batch: 13/18	Loss 7.6473 (7.1135)
2022-11-09 17:44:13,519:INFO: Dataset: zara2               Batch: 14/18	Loss 7.3566 (7.1294)
2022-11-09 17:44:13,544:INFO: Dataset: zara2               Batch: 15/18	Loss 6.8419 (7.1111)
2022-11-09 17:44:13,568:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4186 (7.0688)
2022-11-09 17:44:13,593:INFO: Dataset: zara2               Batch: 17/18	Loss 6.9510 (7.0620)
2022-11-09 17:44:13,615:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0032 (7.0100)
2022-11-09 17:44:13,667:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_20.pth.tar
2022-11-09 17:44:13,667:INFO: 
===> EPOCH: 21 (P1)
2022-11-09 17:44:13,667:INFO: - Computing loss (training)
2022-11-09 17:44:13,869:INFO: Dataset: hotel               Batch: 1/4	Loss 4.3463 (4.3463)
2022-11-09 17:44:13,895:INFO: Dataset: hotel               Batch: 2/4	Loss 6.2402 (5.2659)
2022-11-09 17:44:13,921:INFO: Dataset: hotel               Batch: 3/4	Loss 4.2588 (4.9186)
2022-11-09 17:44:13,938:INFO: Dataset: hotel               Batch: 4/4	Loss 2.9190 (4.5941)
2022-11-09 17:44:14,182:INFO: Dataset: univ                Batch:  1/15	Loss 6.0684 (6.0684)
2022-11-09 17:44:14,210:INFO: Dataset: univ                Batch:  2/15	Loss 5.9730 (6.0207)
2022-11-09 17:44:14,238:INFO: Dataset: univ                Batch:  3/15	Loss 6.0035 (6.0149)
2022-11-09 17:44:14,267:INFO: Dataset: univ                Batch:  4/15	Loss 5.2697 (5.8219)
2022-11-09 17:44:14,292:INFO: Dataset: univ                Batch:  5/15	Loss 5.8238 (5.8223)
2022-11-09 17:44:14,321:INFO: Dataset: univ                Batch:  6/15	Loss 5.6269 (5.7919)
2022-11-09 17:44:14,346:INFO: Dataset: univ                Batch:  7/15	Loss 5.5187 (5.7574)
2022-11-09 17:44:14,371:INFO: Dataset: univ                Batch:  8/15	Loss 5.4996 (5.7258)
2022-11-09 17:44:14,398:INFO: Dataset: univ                Batch:  9/15	Loss 5.9678 (5.7541)
2022-11-09 17:44:14,425:INFO: Dataset: univ                Batch: 10/15	Loss 5.3217 (5.7089)
2022-11-09 17:44:14,452:INFO: Dataset: univ                Batch: 11/15	Loss 5.5276 (5.6922)
2022-11-09 17:44:14,480:INFO: Dataset: univ                Batch: 12/15	Loss 5.0461 (5.6394)
2022-11-09 17:44:14,505:INFO: Dataset: univ                Batch: 13/15	Loss 5.0708 (5.5928)
2022-11-09 17:44:14,531:INFO: Dataset: univ                Batch: 14/15	Loss 5.3360 (5.5754)
2022-11-09 17:44:14,539:INFO: Dataset: univ                Batch: 15/15	Loss 1.0963 (5.5145)
2022-11-09 17:44:14,787:INFO: Dataset: zara1               Batch: 1/8	Loss 14.6587 (14.6587)
2022-11-09 17:44:14,812:INFO: Dataset: zara1               Batch: 2/8	Loss 11.6727 (13.2471)
2022-11-09 17:44:14,838:INFO: Dataset: zara1               Batch: 3/8	Loss 10.4657 (12.3132)
2022-11-09 17:44:14,862:INFO: Dataset: zara1               Batch: 4/8	Loss 8.4349 (11.2470)
2022-11-09 17:44:14,887:INFO: Dataset: zara1               Batch: 5/8	Loss 10.3797 (11.0673)
2022-11-09 17:44:14,911:INFO: Dataset: zara1               Batch: 6/8	Loss 10.8945 (11.0401)
2022-11-09 17:44:14,934:INFO: Dataset: zara1               Batch: 7/8	Loss 11.5822 (11.1203)
2022-11-09 17:44:14,955:INFO: Dataset: zara1               Batch: 8/8	Loss 7.2858 (10.6663)
2022-11-09 17:44:15,212:INFO: Dataset: zara2               Batch:  1/18	Loss 8.6188 (8.6188)
2022-11-09 17:44:15,238:INFO: Dataset: zara2               Batch:  2/18	Loss 9.2092 (8.9262)
2022-11-09 17:44:15,264:INFO: Dataset: zara2               Batch:  3/18	Loss 7.6257 (8.4782)
2022-11-09 17:44:15,290:INFO: Dataset: zara2               Batch:  4/18	Loss 7.2514 (8.1439)
2022-11-09 17:44:15,313:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1689 (7.9457)
2022-11-09 17:44:15,340:INFO: Dataset: zara2               Batch:  6/18	Loss 7.5924 (7.8817)
2022-11-09 17:44:15,363:INFO: Dataset: zara2               Batch:  7/18	Loss 8.0350 (7.9020)
2022-11-09 17:44:15,387:INFO: Dataset: zara2               Batch:  8/18	Loss 7.3117 (7.8271)
2022-11-09 17:44:15,410:INFO: Dataset: zara2               Batch:  9/18	Loss 6.3536 (7.6552)
2022-11-09 17:44:15,436:INFO: Dataset: zara2               Batch: 10/18	Loss 6.8816 (7.5657)
2022-11-09 17:44:15,464:INFO: Dataset: zara2               Batch: 11/18	Loss 7.2364 (7.5342)
2022-11-09 17:44:15,491:INFO: Dataset: zara2               Batch: 12/18	Loss 6.4596 (7.4533)
2022-11-09 17:44:15,515:INFO: Dataset: zara2               Batch: 13/18	Loss 7.0786 (7.4245)
2022-11-09 17:44:15,539:INFO: Dataset: zara2               Batch: 14/18	Loss 6.2865 (7.3528)
2022-11-09 17:44:15,563:INFO: Dataset: zara2               Batch: 15/18	Loss 6.5535 (7.3035)
2022-11-09 17:44:15,588:INFO: Dataset: zara2               Batch: 16/18	Loss 7.3694 (7.3073)
2022-11-09 17:44:15,613:INFO: Dataset: zara2               Batch: 17/18	Loss 7.2916 (7.3063)
2022-11-09 17:44:15,635:INFO: Dataset: zara2               Batch: 18/18	Loss 5.3146 (7.1956)
2022-11-09 17:44:15,685:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_21.pth.tar
2022-11-09 17:44:15,685:INFO: 
===> EPOCH: 22 (P1)
2022-11-09 17:44:15,685:INFO: - Computing loss (training)
2022-11-09 17:44:15,891:INFO: Dataset: hotel               Batch: 1/4	Loss 5.9781 (5.9781)
2022-11-09 17:44:15,917:INFO: Dataset: hotel               Batch: 2/4	Loss 4.0034 (4.9594)
2022-11-09 17:44:15,942:INFO: Dataset: hotel               Batch: 3/4	Loss 4.7651 (4.8898)
2022-11-09 17:44:15,959:INFO: Dataset: hotel               Batch: 4/4	Loss 3.0446 (4.5953)
2022-11-09 17:44:16,254:INFO: Dataset: univ                Batch:  1/15	Loss 6.8643 (6.8643)
2022-11-09 17:44:16,282:INFO: Dataset: univ                Batch:  2/15	Loss 6.7230 (6.7960)
2022-11-09 17:44:16,307:INFO: Dataset: univ                Batch:  3/15	Loss 5.4642 (6.3614)
2022-11-09 17:44:16,332:INFO: Dataset: univ                Batch:  4/15	Loss 5.3220 (6.0874)
2022-11-09 17:44:16,357:INFO: Dataset: univ                Batch:  5/15	Loss 6.0848 (6.0869)
2022-11-09 17:44:16,387:INFO: Dataset: univ                Batch:  6/15	Loss 4.8424 (5.8521)
2022-11-09 17:44:16,412:INFO: Dataset: univ                Batch:  7/15	Loss 5.2896 (5.7694)
2022-11-09 17:44:16,437:INFO: Dataset: univ                Batch:  8/15	Loss 5.9229 (5.7884)
2022-11-09 17:44:16,463:INFO: Dataset: univ                Batch:  9/15	Loss 5.9596 (5.8074)
2022-11-09 17:44:16,488:INFO: Dataset: univ                Batch: 10/15	Loss 4.6583 (5.6907)
2022-11-09 17:44:16,515:INFO: Dataset: univ                Batch: 11/15	Loss 5.5440 (5.6768)
2022-11-09 17:44:16,542:INFO: Dataset: univ                Batch: 12/15	Loss 5.4055 (5.6529)
2022-11-09 17:44:16,567:INFO: Dataset: univ                Batch: 13/15	Loss 5.2287 (5.6162)
2022-11-09 17:44:16,593:INFO: Dataset: univ                Batch: 14/15	Loss 4.9210 (5.5665)
2022-11-09 17:44:16,603:INFO: Dataset: univ                Batch: 15/15	Loss 1.1485 (5.5098)
2022-11-09 17:44:16,853:INFO: Dataset: zara1               Batch: 1/8	Loss 8.5013 (8.5013)
2022-11-09 17:44:16,878:INFO: Dataset: zara1               Batch: 2/8	Loss 11.2215 (9.8317)
2022-11-09 17:44:16,902:INFO: Dataset: zara1               Batch: 3/8	Loss 10.1168 (9.9256)
2022-11-09 17:44:16,928:INFO: Dataset: zara1               Batch: 4/8	Loss 9.0473 (9.7036)
2022-11-09 17:44:16,953:INFO: Dataset: zara1               Batch: 5/8	Loss 13.4358 (10.5351)
2022-11-09 17:44:16,977:INFO: Dataset: zara1               Batch: 6/8	Loss 10.9891 (10.6139)
2022-11-09 17:44:17,001:INFO: Dataset: zara1               Batch: 7/8	Loss 10.7698 (10.6386)
2022-11-09 17:44:17,022:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6889 (10.3142)
2022-11-09 17:44:17,278:INFO: Dataset: zara2               Batch:  1/18	Loss 6.4664 (6.4664)
2022-11-09 17:44:17,303:INFO: Dataset: zara2               Batch:  2/18	Loss 6.1946 (6.3307)
2022-11-09 17:44:17,331:INFO: Dataset: zara2               Batch:  3/18	Loss 7.2332 (6.5985)
2022-11-09 17:44:17,355:INFO: Dataset: zara2               Batch:  4/18	Loss 7.1987 (6.7379)
2022-11-09 17:44:17,378:INFO: Dataset: zara2               Batch:  5/18	Loss 6.6936 (6.7294)
2022-11-09 17:44:17,406:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8700 (6.7534)
2022-11-09 17:44:17,429:INFO: Dataset: zara2               Batch:  7/18	Loss 7.3537 (6.8424)
2022-11-09 17:44:17,453:INFO: Dataset: zara2               Batch:  8/18	Loss 6.7312 (6.8285)
2022-11-09 17:44:17,477:INFO: Dataset: zara2               Batch:  9/18	Loss 6.8961 (6.8351)
2022-11-09 17:44:17,501:INFO: Dataset: zara2               Batch: 10/18	Loss 7.4449 (6.8957)
2022-11-09 17:44:17,525:INFO: Dataset: zara2               Batch: 11/18	Loss 6.9001 (6.8961)
2022-11-09 17:44:17,551:INFO: Dataset: zara2               Batch: 12/18	Loss 7.1353 (6.9166)
2022-11-09 17:44:17,576:INFO: Dataset: zara2               Batch: 13/18	Loss 6.3662 (6.8703)
2022-11-09 17:44:17,600:INFO: Dataset: zara2               Batch: 14/18	Loss 7.1355 (6.8891)
2022-11-09 17:44:17,625:INFO: Dataset: zara2               Batch: 15/18	Loss 7.1145 (6.9038)
2022-11-09 17:44:17,650:INFO: Dataset: zara2               Batch: 16/18	Loss 7.2513 (6.9240)
2022-11-09 17:44:17,674:INFO: Dataset: zara2               Batch: 17/18	Loss 8.0065 (6.9934)
2022-11-09 17:44:17,696:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0888 (6.9517)
2022-11-09 17:44:17,746:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_22.pth.tar
2022-11-09 17:44:17,746:INFO: 
===> EPOCH: 23 (P1)
2022-11-09 17:44:17,747:INFO: - Computing loss (training)
2022-11-09 17:44:17,945:INFO: Dataset: hotel               Batch: 1/4	Loss 4.7435 (4.7435)
2022-11-09 17:44:17,971:INFO: Dataset: hotel               Batch: 2/4	Loss 4.5950 (4.6696)
2022-11-09 17:44:17,996:INFO: Dataset: hotel               Batch: 3/4	Loss 4.7552 (4.6981)
2022-11-09 17:44:18,012:INFO: Dataset: hotel               Batch: 4/4	Loss 2.2344 (4.2983)
2022-11-09 17:44:18,321:INFO: Dataset: univ                Batch:  1/15	Loss 5.5264 (5.5264)
2022-11-09 17:44:18,348:INFO: Dataset: univ                Batch:  2/15	Loss 5.8609 (5.6923)
2022-11-09 17:44:18,373:INFO: Dataset: univ                Batch:  3/15	Loss 6.6490 (5.9691)
2022-11-09 17:44:18,399:INFO: Dataset: univ                Batch:  4/15	Loss 6.0666 (5.9917)
2022-11-09 17:44:18,424:INFO: Dataset: univ                Batch:  5/15	Loss 5.6597 (5.9231)
2022-11-09 17:44:18,454:INFO: Dataset: univ                Batch:  6/15	Loss 4.8323 (5.7234)
2022-11-09 17:44:18,479:INFO: Dataset: univ                Batch:  7/15	Loss 4.9994 (5.6001)
2022-11-09 17:44:18,504:INFO: Dataset: univ                Batch:  8/15	Loss 5.7215 (5.6148)
2022-11-09 17:44:18,530:INFO: Dataset: univ                Batch:  9/15	Loss 5.0080 (5.5411)
2022-11-09 17:44:18,556:INFO: Dataset: univ                Batch: 10/15	Loss 5.3318 (5.5199)
2022-11-09 17:44:18,582:INFO: Dataset: univ                Batch: 11/15	Loss 5.4271 (5.5112)
2022-11-09 17:44:18,610:INFO: Dataset: univ                Batch: 12/15	Loss 5.2132 (5.4854)
2022-11-09 17:44:18,636:INFO: Dataset: univ                Batch: 13/15	Loss 6.1430 (5.5360)
2022-11-09 17:44:18,663:INFO: Dataset: univ                Batch: 14/15	Loss 5.2993 (5.5185)
2022-11-09 17:44:18,672:INFO: Dataset: univ                Batch: 15/15	Loss 0.8296 (5.4534)
2022-11-09 17:44:18,931:INFO: Dataset: zara1               Batch: 1/8	Loss 8.5159 (8.5159)
2022-11-09 17:44:18,957:INFO: Dataset: zara1               Batch: 2/8	Loss 10.4909 (9.4796)
2022-11-09 17:44:18,981:INFO: Dataset: zara1               Batch: 3/8	Loss 11.1884 (10.1053)
2022-11-09 17:44:19,007:INFO: Dataset: zara1               Batch: 4/8	Loss 10.8930 (10.2869)
2022-11-09 17:44:19,030:INFO: Dataset: zara1               Batch: 5/8	Loss 9.6990 (10.1600)
2022-11-09 17:44:19,055:INFO: Dataset: zara1               Batch: 6/8	Loss 9.5649 (10.0526)
2022-11-09 17:44:19,079:INFO: Dataset: zara1               Batch: 7/8	Loss 10.3639 (10.0974)
2022-11-09 17:44:19,100:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6833 (9.8471)
2022-11-09 17:44:19,352:INFO: Dataset: zara2               Batch:  1/18	Loss 6.1665 (6.1665)
2022-11-09 17:44:19,378:INFO: Dataset: zara2               Batch:  2/18	Loss 7.0062 (6.5771)
2022-11-09 17:44:19,403:INFO: Dataset: zara2               Batch:  3/18	Loss 7.3914 (6.8536)
2022-11-09 17:44:19,429:INFO: Dataset: zara2               Batch:  4/18	Loss 7.3196 (6.9815)
2022-11-09 17:44:19,455:INFO: Dataset: zara2               Batch:  5/18	Loss 6.8118 (6.9460)
2022-11-09 17:44:19,481:INFO: Dataset: zara2               Batch:  6/18	Loss 7.3013 (7.0012)
2022-11-09 17:44:19,505:INFO: Dataset: zara2               Batch:  7/18	Loss 7.1726 (7.0243)
2022-11-09 17:44:19,528:INFO: Dataset: zara2               Batch:  8/18	Loss 7.1596 (7.0444)
2022-11-09 17:44:19,552:INFO: Dataset: zara2               Batch:  9/18	Loss 7.5017 (7.0988)
2022-11-09 17:44:19,576:INFO: Dataset: zara2               Batch: 10/18	Loss 7.9126 (7.1868)
2022-11-09 17:44:19,603:INFO: Dataset: zara2               Batch: 11/18	Loss 6.1866 (7.0993)
2022-11-09 17:44:19,628:INFO: Dataset: zara2               Batch: 12/18	Loss 7.3557 (7.1216)
2022-11-09 17:44:19,653:INFO: Dataset: zara2               Batch: 13/18	Loss 6.7545 (7.0936)
2022-11-09 17:44:19,676:INFO: Dataset: zara2               Batch: 14/18	Loss 6.4244 (7.0478)
2022-11-09 17:44:19,701:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7146 (7.0257)
2022-11-09 17:44:19,726:INFO: Dataset: zara2               Batch: 16/18	Loss 6.5129 (6.9938)
2022-11-09 17:44:19,751:INFO: Dataset: zara2               Batch: 17/18	Loss 7.3063 (7.0115)
2022-11-09 17:44:19,773:INFO: Dataset: zara2               Batch: 18/18	Loss 6.6696 (6.9952)
2022-11-09 17:44:19,822:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_23.pth.tar
2022-11-09 17:44:19,822:INFO: 
===> EPOCH: 24 (P1)
2022-11-09 17:44:19,823:INFO: - Computing loss (training)
2022-11-09 17:44:20,034:INFO: Dataset: hotel               Batch: 1/4	Loss 5.7252 (5.7252)
2022-11-09 17:44:20,060:INFO: Dataset: hotel               Batch: 2/4	Loss 5.4866 (5.6132)
2022-11-09 17:44:20,085:INFO: Dataset: hotel               Batch: 3/4	Loss 3.6114 (4.9045)
2022-11-09 17:44:20,102:INFO: Dataset: hotel               Batch: 4/4	Loss 2.7896 (4.5390)
2022-11-09 17:44:20,351:INFO: Dataset: univ                Batch:  1/15	Loss 5.4070 (5.4070)
2022-11-09 17:44:20,378:INFO: Dataset: univ                Batch:  2/15	Loss 5.1901 (5.3053)
2022-11-09 17:44:20,404:INFO: Dataset: univ                Batch:  3/15	Loss 6.9187 (5.7853)
2022-11-09 17:44:20,430:INFO: Dataset: univ                Batch:  4/15	Loss 5.2887 (5.6524)
2022-11-09 17:44:20,459:INFO: Dataset: univ                Batch:  5/15	Loss 4.5001 (5.4130)
2022-11-09 17:44:20,486:INFO: Dataset: univ                Batch:  6/15	Loss 5.3161 (5.3978)
2022-11-09 17:44:20,511:INFO: Dataset: univ                Batch:  7/15	Loss 5.1244 (5.3595)
2022-11-09 17:44:20,536:INFO: Dataset: univ                Batch:  8/15	Loss 4.7891 (5.2839)
2022-11-09 17:44:20,562:INFO: Dataset: univ                Batch:  9/15	Loss 4.9835 (5.2505)
2022-11-09 17:44:20,588:INFO: Dataset: univ                Batch: 10/15	Loss 5.4099 (5.2657)
2022-11-09 17:44:20,617:INFO: Dataset: univ                Batch: 11/15	Loss 5.8442 (5.3229)
2022-11-09 17:44:20,643:INFO: Dataset: univ                Batch: 12/15	Loss 5.0842 (5.3035)
2022-11-09 17:44:20,668:INFO: Dataset: univ                Batch: 13/15	Loss 5.3579 (5.3075)
2022-11-09 17:44:20,694:INFO: Dataset: univ                Batch: 14/15	Loss 7.4451 (5.4506)
2022-11-09 17:44:20,704:INFO: Dataset: univ                Batch: 15/15	Loss 1.0366 (5.3943)
2022-11-09 17:44:20,953:INFO: Dataset: zara1               Batch: 1/8	Loss 13.8889 (13.8889)
2022-11-09 17:44:20,978:INFO: Dataset: zara1               Batch: 2/8	Loss 10.4785 (12.1369)
2022-11-09 17:44:21,004:INFO: Dataset: zara1               Batch: 3/8	Loss 10.4782 (11.5895)
2022-11-09 17:44:21,027:INFO: Dataset: zara1               Batch: 4/8	Loss 10.4479 (11.2796)
2022-11-09 17:44:21,051:INFO: Dataset: zara1               Batch: 5/8	Loss 9.2773 (10.9050)
2022-11-09 17:44:21,076:INFO: Dataset: zara1               Batch: 6/8	Loss 10.3972 (10.8170)
2022-11-09 17:44:21,099:INFO: Dataset: zara1               Batch: 7/8	Loss 9.5395 (10.6250)
2022-11-09 17:44:21,120:INFO: Dataset: zara1               Batch: 8/8	Loss 8.1968 (10.3656)
2022-11-09 17:44:21,384:INFO: Dataset: zara2               Batch:  1/18	Loss 6.5443 (6.5443)
2022-11-09 17:44:21,410:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2085 (6.8717)
2022-11-09 17:44:21,433:INFO: Dataset: zara2               Batch:  3/18	Loss 7.1615 (6.9692)
2022-11-09 17:44:21,457:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0608 (6.9911)
2022-11-09 17:44:21,481:INFO: Dataset: zara2               Batch:  5/18	Loss 6.6089 (6.9178)
2022-11-09 17:44:21,508:INFO: Dataset: zara2               Batch:  6/18	Loss 8.1280 (7.0925)
2022-11-09 17:44:21,532:INFO: Dataset: zara2               Batch:  7/18	Loss 8.7060 (7.3217)
2022-11-09 17:44:21,556:INFO: Dataset: zara2               Batch:  8/18	Loss 6.9065 (7.2709)
2022-11-09 17:44:21,579:INFO: Dataset: zara2               Batch:  9/18	Loss 8.4825 (7.4105)
2022-11-09 17:44:21,604:INFO: Dataset: zara2               Batch: 10/18	Loss 7.5549 (7.4246)
2022-11-09 17:44:21,629:INFO: Dataset: zara2               Batch: 11/18	Loss 6.7825 (7.3614)
2022-11-09 17:44:21,655:INFO: Dataset: zara2               Batch: 12/18	Loss 7.1516 (7.3443)
2022-11-09 17:44:21,680:INFO: Dataset: zara2               Batch: 13/18	Loss 6.5545 (7.2795)
2022-11-09 17:44:21,704:INFO: Dataset: zara2               Batch: 14/18	Loss 6.3465 (7.2151)
2022-11-09 17:44:21,729:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7813 (7.1879)
2022-11-09 17:44:21,754:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4106 (7.1364)
2022-11-09 17:44:21,779:INFO: Dataset: zara2               Batch: 17/18	Loss 7.3692 (7.1502)
2022-11-09 17:44:21,801:INFO: Dataset: zara2               Batch: 18/18	Loss 5.4470 (7.0725)
2022-11-09 17:44:21,851:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_24.pth.tar
2022-11-09 17:44:21,851:INFO: 
===> EPOCH: 25 (P1)
2022-11-09 17:44:21,851:INFO: - Computing loss (training)
2022-11-09 17:44:22,058:INFO: Dataset: hotel               Batch: 1/4	Loss 3.7550 (3.7550)
2022-11-09 17:44:22,085:INFO: Dataset: hotel               Batch: 2/4	Loss 4.1961 (3.9889)
2022-11-09 17:44:22,109:INFO: Dataset: hotel               Batch: 3/4	Loss 6.8689 (4.8543)
2022-11-09 17:44:22,127:INFO: Dataset: hotel               Batch: 4/4	Loss 2.6741 (4.5120)
2022-11-09 17:44:22,381:INFO: Dataset: univ                Batch:  1/15	Loss 5.0783 (5.0783)
2022-11-09 17:44:22,409:INFO: Dataset: univ                Batch:  2/15	Loss 4.4281 (4.7196)
2022-11-09 17:44:22,438:INFO: Dataset: univ                Batch:  3/15	Loss 4.5792 (4.6724)
2022-11-09 17:44:22,465:INFO: Dataset: univ                Batch:  4/15	Loss 5.3361 (4.8295)
2022-11-09 17:44:22,490:INFO: Dataset: univ                Batch:  5/15	Loss 4.8658 (4.8359)
2022-11-09 17:44:22,518:INFO: Dataset: univ                Batch:  6/15	Loss 6.5088 (5.0884)
2022-11-09 17:44:22,543:INFO: Dataset: univ                Batch:  7/15	Loss 5.2665 (5.1133)
2022-11-09 17:44:22,568:INFO: Dataset: univ                Batch:  8/15	Loss 5.1869 (5.1230)
2022-11-09 17:44:22,594:INFO: Dataset: univ                Batch:  9/15	Loss 4.9334 (5.1019)
2022-11-09 17:44:22,620:INFO: Dataset: univ                Batch: 10/15	Loss 5.4534 (5.1385)
2022-11-09 17:44:22,648:INFO: Dataset: univ                Batch: 11/15	Loss 6.6218 (5.2640)
2022-11-09 17:44:22,674:INFO: Dataset: univ                Batch: 12/15	Loss 6.8907 (5.3805)
2022-11-09 17:44:22,699:INFO: Dataset: univ                Batch: 13/15	Loss 5.5139 (5.3910)
2022-11-09 17:44:22,725:INFO: Dataset: univ                Batch: 14/15	Loss 4.9959 (5.3618)
2022-11-09 17:44:22,735:INFO: Dataset: univ                Batch: 15/15	Loss 1.1176 (5.3183)
2022-11-09 17:44:22,985:INFO: Dataset: zara1               Batch: 1/8	Loss 12.4231 (12.4231)
2022-11-09 17:44:23,009:INFO: Dataset: zara1               Batch: 2/8	Loss 9.8913 (11.1279)
2022-11-09 17:44:23,033:INFO: Dataset: zara1               Batch: 3/8	Loss 11.0252 (11.0971)
2022-11-09 17:44:23,056:INFO: Dataset: zara1               Batch: 4/8	Loss 11.4207 (11.1834)
2022-11-09 17:44:23,080:INFO: Dataset: zara1               Batch: 5/8	Loss 8.6371 (10.6610)
2022-11-09 17:44:23,105:INFO: Dataset: zara1               Batch: 6/8	Loss 8.5953 (10.2654)
2022-11-09 17:44:23,129:INFO: Dataset: zara1               Batch: 7/8	Loss 8.1360 (9.9624)
2022-11-09 17:44:23,150:INFO: Dataset: zara1               Batch: 8/8	Loss 9.3527 (9.8918)
2022-11-09 17:44:23,398:INFO: Dataset: zara2               Batch:  1/18	Loss 9.5650 (9.5650)
2022-11-09 17:44:23,442:INFO: Dataset: zara2               Batch:  2/18	Loss 11.2266 (10.4388)
2022-11-09 17:44:23,466:INFO: Dataset: zara2               Batch:  3/18	Loss 8.1035 (9.6515)
2022-11-09 17:44:23,491:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0980 (8.9500)
2022-11-09 17:44:23,515:INFO: Dataset: zara2               Batch:  5/18	Loss 6.9347 (8.5540)
2022-11-09 17:44:23,544:INFO: Dataset: zara2               Batch:  6/18	Loss 8.0003 (8.4548)
2022-11-09 17:44:23,567:INFO: Dataset: zara2               Batch:  7/18	Loss 7.2336 (8.2916)
2022-11-09 17:44:23,591:INFO: Dataset: zara2               Batch:  8/18	Loss 5.7041 (7.9347)
2022-11-09 17:44:23,616:INFO: Dataset: zara2               Batch:  9/18	Loss 6.9567 (7.8258)
2022-11-09 17:44:23,641:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1422 (7.6558)
2022-11-09 17:44:23,667:INFO: Dataset: zara2               Batch: 11/18	Loss 5.7003 (7.4613)
2022-11-09 17:44:23,692:INFO: Dataset: zara2               Batch: 12/18	Loss 7.2171 (7.4425)
2022-11-09 17:44:23,716:INFO: Dataset: zara2               Batch: 13/18	Loss 5.9273 (7.3299)
2022-11-09 17:44:23,740:INFO: Dataset: zara2               Batch: 14/18	Loss 6.8406 (7.2963)
2022-11-09 17:44:23,764:INFO: Dataset: zara2               Batch: 15/18	Loss 7.3318 (7.2986)
2022-11-09 17:44:23,789:INFO: Dataset: zara2               Batch: 16/18	Loss 7.5868 (7.3191)
2022-11-09 17:44:23,814:INFO: Dataset: zara2               Batch: 17/18	Loss 7.5352 (7.3316)
2022-11-09 17:44:23,837:INFO: Dataset: zara2               Batch: 18/18	Loss 5.2123 (7.2227)
2022-11-09 17:44:23,888:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_25.pth.tar
2022-11-09 17:44:23,888:INFO: 
===> EPOCH: 26 (P1)
2022-11-09 17:44:23,888:INFO: - Computing loss (training)
2022-11-09 17:44:24,105:INFO: Dataset: hotel               Batch: 1/4	Loss 4.3904 (4.3904)
2022-11-09 17:44:24,129:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4506 (4.4197)
2022-11-09 17:44:24,154:INFO: Dataset: hotel               Batch: 3/4	Loss 4.4334 (4.4244)
2022-11-09 17:44:24,171:INFO: Dataset: hotel               Batch: 4/4	Loss 2.9892 (4.1763)
2022-11-09 17:44:24,419:INFO: Dataset: univ                Batch:  1/15	Loss 4.4691 (4.4691)
2022-11-09 17:44:24,448:INFO: Dataset: univ                Batch:  2/15	Loss 4.5797 (4.5214)
2022-11-09 17:44:24,477:INFO: Dataset: univ                Batch:  3/15	Loss 5.5619 (4.8371)
2022-11-09 17:44:24,504:INFO: Dataset: univ                Batch:  4/15	Loss 6.6388 (5.2427)
2022-11-09 17:44:24,529:INFO: Dataset: univ                Batch:  5/15	Loss 5.5949 (5.3095)
2022-11-09 17:44:24,557:INFO: Dataset: univ                Batch:  6/15	Loss 5.8391 (5.4010)
2022-11-09 17:44:24,582:INFO: Dataset: univ                Batch:  7/15	Loss 5.0027 (5.3407)
2022-11-09 17:44:24,607:INFO: Dataset: univ                Batch:  8/15	Loss 6.0164 (5.4193)
2022-11-09 17:44:24,633:INFO: Dataset: univ                Batch:  9/15	Loss 5.9774 (5.4764)
2022-11-09 17:44:24,659:INFO: Dataset: univ                Batch: 10/15	Loss 5.1402 (5.4417)
2022-11-09 17:44:24,685:INFO: Dataset: univ                Batch: 11/15	Loss 5.5008 (5.4474)
2022-11-09 17:44:24,713:INFO: Dataset: univ                Batch: 12/15	Loss 5.1059 (5.4182)
2022-11-09 17:44:24,739:INFO: Dataset: univ                Batch: 13/15	Loss 4.8689 (5.3711)
2022-11-09 17:44:24,764:INFO: Dataset: univ                Batch: 14/15	Loss 4.6853 (5.3225)
2022-11-09 17:44:24,774:INFO: Dataset: univ                Batch: 15/15	Loss 1.1468 (5.2784)
2022-11-09 17:44:25,020:INFO: Dataset: zara1               Batch: 1/8	Loss 14.7858 (14.7858)
2022-11-09 17:44:25,046:INFO: Dataset: zara1               Batch: 2/8	Loss 10.4511 (12.5522)
2022-11-09 17:44:25,069:INFO: Dataset: zara1               Batch: 3/8	Loss 12.1045 (12.3958)
2022-11-09 17:44:25,093:INFO: Dataset: zara1               Batch: 4/8	Loss 11.3825 (12.1326)
2022-11-09 17:44:25,116:INFO: Dataset: zara1               Batch: 5/8	Loss 8.0317 (11.2925)
2022-11-09 17:44:25,141:INFO: Dataset: zara1               Batch: 6/8	Loss 9.3871 (10.9763)
2022-11-09 17:44:25,165:INFO: Dataset: zara1               Batch: 7/8	Loss 9.5720 (10.7814)
2022-11-09 17:44:25,186:INFO: Dataset: zara1               Batch: 8/8	Loss 8.4215 (10.4896)
2022-11-09 17:44:25,508:INFO: Dataset: zara2               Batch:  1/18	Loss 7.2460 (7.2460)
2022-11-09 17:44:25,538:INFO: Dataset: zara2               Batch:  2/18	Loss 6.1345 (6.6353)
2022-11-09 17:44:25,562:INFO: Dataset: zara2               Batch:  3/18	Loss 7.1204 (6.7970)
2022-11-09 17:44:25,586:INFO: Dataset: zara2               Batch:  4/18	Loss 6.5208 (6.7250)
2022-11-09 17:44:25,610:INFO: Dataset: zara2               Batch:  5/18	Loss 7.7137 (6.9131)
2022-11-09 17:44:25,635:INFO: Dataset: zara2               Batch:  6/18	Loss 7.0589 (6.9365)
2022-11-09 17:44:25,659:INFO: Dataset: zara2               Batch:  7/18	Loss 8.5928 (7.1969)
2022-11-09 17:44:25,682:INFO: Dataset: zara2               Batch:  8/18	Loss 7.2265 (7.2006)
2022-11-09 17:44:25,706:INFO: Dataset: zara2               Batch:  9/18	Loss 7.3691 (7.2168)
2022-11-09 17:44:25,730:INFO: Dataset: zara2               Batch: 10/18	Loss 7.4248 (7.2390)
2022-11-09 17:44:25,757:INFO: Dataset: zara2               Batch: 11/18	Loss 6.4089 (7.1671)
2022-11-09 17:44:25,782:INFO: Dataset: zara2               Batch: 12/18	Loss 6.4237 (7.1044)
2022-11-09 17:44:25,808:INFO: Dataset: zara2               Batch: 13/18	Loss 6.3113 (7.0481)
2022-11-09 17:44:25,833:INFO: Dataset: zara2               Batch: 14/18	Loss 6.6296 (7.0196)
2022-11-09 17:44:25,857:INFO: Dataset: zara2               Batch: 15/18	Loss 6.6635 (6.9978)
2022-11-09 17:44:25,882:INFO: Dataset: zara2               Batch: 16/18	Loss 6.6109 (6.9724)
2022-11-09 17:44:25,906:INFO: Dataset: zara2               Batch: 17/18	Loss 6.3896 (6.9384)
2022-11-09 17:44:25,928:INFO: Dataset: zara2               Batch: 18/18	Loss 5.4156 (6.8674)
2022-11-09 17:44:25,978:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_26.pth.tar
2022-11-09 17:44:25,978:INFO: 
===> EPOCH: 27 (P1)
2022-11-09 17:44:25,979:INFO: - Computing loss (training)
2022-11-09 17:44:26,183:INFO: Dataset: hotel               Batch: 1/4	Loss 3.5579 (3.5579)
2022-11-09 17:44:26,209:INFO: Dataset: hotel               Batch: 2/4	Loss 4.2054 (3.8809)
2022-11-09 17:44:26,235:INFO: Dataset: hotel               Batch: 3/4	Loss 4.5853 (4.1095)
2022-11-09 17:44:26,251:INFO: Dataset: hotel               Batch: 4/4	Loss 4.4080 (4.1556)
2022-11-09 17:44:26,511:INFO: Dataset: univ                Batch:  1/15	Loss 5.1184 (5.1184)
2022-11-09 17:44:26,545:INFO: Dataset: univ                Batch:  2/15	Loss 5.4055 (5.2622)
2022-11-09 17:44:26,570:INFO: Dataset: univ                Batch:  3/15	Loss 4.7725 (5.0998)
2022-11-09 17:44:26,595:INFO: Dataset: univ                Batch:  4/15	Loss 5.3628 (5.1635)
2022-11-09 17:44:26,620:INFO: Dataset: univ                Batch:  5/15	Loss 5.6734 (5.2633)
2022-11-09 17:44:26,650:INFO: Dataset: univ                Batch:  6/15	Loss 4.9279 (5.2074)
2022-11-09 17:44:26,675:INFO: Dataset: univ                Batch:  7/15	Loss 4.9628 (5.1720)
2022-11-09 17:44:26,700:INFO: Dataset: univ                Batch:  8/15	Loss 5.0755 (5.1600)
2022-11-09 17:44:26,726:INFO: Dataset: univ                Batch:  9/15	Loss 5.0452 (5.1470)
2022-11-09 17:44:26,752:INFO: Dataset: univ                Batch: 10/15	Loss 4.9259 (5.1230)
2022-11-09 17:44:26,779:INFO: Dataset: univ                Batch: 11/15	Loss 5.7870 (5.1807)
2022-11-09 17:44:26,805:INFO: Dataset: univ                Batch: 12/15	Loss 4.8949 (5.1568)
2022-11-09 17:44:26,830:INFO: Dataset: univ                Batch: 13/15	Loss 5.7803 (5.1997)
2022-11-09 17:44:26,856:INFO: Dataset: univ                Batch: 14/15	Loss 4.8927 (5.1780)
2022-11-09 17:44:26,865:INFO: Dataset: univ                Batch: 15/15	Loss 1.2492 (5.1308)
2022-11-09 17:44:27,108:INFO: Dataset: zara1               Batch: 1/8	Loss 9.5516 (9.5516)
2022-11-09 17:44:27,133:INFO: Dataset: zara1               Batch: 2/8	Loss 11.7992 (10.6705)
2022-11-09 17:44:27,158:INFO: Dataset: zara1               Batch: 3/8	Loss 10.1208 (10.4784)
2022-11-09 17:44:27,182:INFO: Dataset: zara1               Batch: 4/8	Loss 9.0235 (10.1190)
2022-11-09 17:44:27,207:INFO: Dataset: zara1               Batch: 5/8	Loss 8.9078 (9.8577)
2022-11-09 17:44:27,232:INFO: Dataset: zara1               Batch: 6/8	Loss 10.5816 (9.9748)
2022-11-09 17:44:27,255:INFO: Dataset: zara1               Batch: 7/8	Loss 8.9006 (9.7996)
2022-11-09 17:44:27,276:INFO: Dataset: zara1               Batch: 8/8	Loss 8.0555 (9.6086)
2022-11-09 17:44:27,537:INFO: Dataset: zara2               Batch:  1/18	Loss 6.5555 (6.5555)
2022-11-09 17:44:27,565:INFO: Dataset: zara2               Batch:  2/18	Loss 6.8307 (6.6972)
2022-11-09 17:44:27,592:INFO: Dataset: zara2               Batch:  3/18	Loss 6.6487 (6.6836)
2022-11-09 17:44:27,616:INFO: Dataset: zara2               Batch:  4/18	Loss 8.2643 (7.0744)
2022-11-09 17:44:27,640:INFO: Dataset: zara2               Batch:  5/18	Loss 6.9611 (7.0517)
2022-11-09 17:44:27,668:INFO: Dataset: zara2               Batch:  6/18	Loss 6.7045 (6.9903)
2022-11-09 17:44:27,692:INFO: Dataset: zara2               Batch:  7/18	Loss 8.9377 (7.2544)
2022-11-09 17:44:27,716:INFO: Dataset: zara2               Batch:  8/18	Loss 6.4515 (7.1429)
2022-11-09 17:44:27,739:INFO: Dataset: zara2               Batch:  9/18	Loss 6.8136 (7.1076)
2022-11-09 17:44:27,764:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1304 (7.0079)
2022-11-09 17:44:27,788:INFO: Dataset: zara2               Batch: 11/18	Loss 5.9096 (6.9230)
2022-11-09 17:44:27,815:INFO: Dataset: zara2               Batch: 12/18	Loss 7.0673 (6.9347)
2022-11-09 17:44:27,839:INFO: Dataset: zara2               Batch: 13/18	Loss 6.4571 (6.8938)
2022-11-09 17:44:27,862:INFO: Dataset: zara2               Batch: 14/18	Loss 6.1963 (6.8456)
2022-11-09 17:44:27,887:INFO: Dataset: zara2               Batch: 15/18	Loss 6.2805 (6.8085)
2022-11-09 17:44:27,912:INFO: Dataset: zara2               Batch: 16/18	Loss 7.0300 (6.8211)
2022-11-09 17:44:27,936:INFO: Dataset: zara2               Batch: 17/18	Loss 6.3447 (6.7943)
2022-11-09 17:44:27,959:INFO: Dataset: zara2               Batch: 18/18	Loss 7.1921 (6.8142)
2022-11-09 17:44:28,009:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_27.pth.tar
2022-11-09 17:44:28,009:INFO: 
===> EPOCH: 28 (P1)
2022-11-09 17:44:28,009:INFO: - Computing loss (training)
2022-11-09 17:44:28,222:INFO: Dataset: hotel               Batch: 1/4	Loss 4.9914 (4.9914)
2022-11-09 17:44:28,248:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4817 (4.7308)
2022-11-09 17:44:28,272:INFO: Dataset: hotel               Batch: 3/4	Loss 4.1876 (4.5401)
2022-11-09 17:44:28,290:INFO: Dataset: hotel               Batch: 4/4	Loss 2.7160 (4.2032)
2022-11-09 17:44:28,533:INFO: Dataset: univ                Batch:  1/15	Loss 5.0662 (5.0662)
2022-11-09 17:44:28,558:INFO: Dataset: univ                Batch:  2/15	Loss 5.3078 (5.1873)
2022-11-09 17:44:28,585:INFO: Dataset: univ                Batch:  3/15	Loss 5.2182 (5.1981)
2022-11-09 17:44:28,610:INFO: Dataset: univ                Batch:  4/15	Loss 5.5303 (5.2796)
2022-11-09 17:44:28,636:INFO: Dataset: univ                Batch:  5/15	Loss 5.3011 (5.2841)
2022-11-09 17:44:28,667:INFO: Dataset: univ                Batch:  6/15	Loss 5.4267 (5.3072)
2022-11-09 17:44:28,692:INFO: Dataset: univ                Batch:  7/15	Loss 5.1222 (5.2806)
2022-11-09 17:44:28,717:INFO: Dataset: univ                Batch:  8/15	Loss 7.2346 (5.5024)
2022-11-09 17:44:28,743:INFO: Dataset: univ                Batch:  9/15	Loss 5.4870 (5.5007)
2022-11-09 17:44:28,769:INFO: Dataset: univ                Batch: 10/15	Loss 4.6535 (5.4038)
2022-11-09 17:44:28,795:INFO: Dataset: univ                Batch: 11/15	Loss 5.2086 (5.3862)
2022-11-09 17:44:28,822:INFO: Dataset: univ                Batch: 12/15	Loss 5.0408 (5.3583)
2022-11-09 17:44:28,848:INFO: Dataset: univ                Batch: 13/15	Loss 4.6259 (5.3000)
2022-11-09 17:44:28,874:INFO: Dataset: univ                Batch: 14/15	Loss 5.0747 (5.2830)
2022-11-09 17:44:28,884:INFO: Dataset: univ                Batch: 15/15	Loss 0.8012 (5.2231)
2022-11-09 17:44:29,142:INFO: Dataset: zara1               Batch: 1/8	Loss 10.4844 (10.4844)
2022-11-09 17:44:29,170:INFO: Dataset: zara1               Batch: 2/8	Loss 8.7940 (9.6478)
2022-11-09 17:44:29,194:INFO: Dataset: zara1               Batch: 3/8	Loss 9.9713 (9.7537)
2022-11-09 17:44:29,218:INFO: Dataset: zara1               Batch: 4/8	Loss 14.2653 (10.8943)
2022-11-09 17:44:29,243:INFO: Dataset: zara1               Batch: 5/8	Loss 9.1516 (10.5392)
2022-11-09 17:44:29,268:INFO: Dataset: zara1               Batch: 6/8	Loss 10.1810 (10.4883)
2022-11-09 17:44:29,291:INFO: Dataset: zara1               Batch: 7/8	Loss 9.4709 (10.3395)
2022-11-09 17:44:29,312:INFO: Dataset: zara1               Batch: 8/8	Loss 9.3370 (10.2245)
2022-11-09 17:44:29,572:INFO: Dataset: zara2               Batch:  1/18	Loss 5.8326 (5.8326)
2022-11-09 17:44:29,598:INFO: Dataset: zara2               Batch:  2/18	Loss 6.8406 (6.3000)
2022-11-09 17:44:29,622:INFO: Dataset: zara2               Batch:  3/18	Loss 7.2969 (6.6643)
2022-11-09 17:44:29,645:INFO: Dataset: zara2               Batch:  4/18	Loss 8.8138 (7.2865)
2022-11-09 17:44:29,671:INFO: Dataset: zara2               Batch:  5/18	Loss 7.0526 (7.2382)
2022-11-09 17:44:29,696:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8891 (7.1790)
2022-11-09 17:44:29,720:INFO: Dataset: zara2               Batch:  7/18	Loss 6.2281 (7.0510)
2022-11-09 17:44:29,743:INFO: Dataset: zara2               Batch:  8/18	Loss 6.8722 (7.0267)
2022-11-09 17:44:29,767:INFO: Dataset: zara2               Batch:  9/18	Loss 6.7677 (6.9962)
2022-11-09 17:44:29,791:INFO: Dataset: zara2               Batch: 10/18	Loss 6.5204 (6.9505)
2022-11-09 17:44:29,819:INFO: Dataset: zara2               Batch: 11/18	Loss 6.5740 (6.9168)
2022-11-09 17:44:29,844:INFO: Dataset: zara2               Batch: 12/18	Loss 6.3319 (6.8703)
2022-11-09 17:44:29,868:INFO: Dataset: zara2               Batch: 13/18	Loss 6.3304 (6.8275)
2022-11-09 17:44:29,892:INFO: Dataset: zara2               Batch: 14/18	Loss 6.9259 (6.8344)
2022-11-09 17:44:29,916:INFO: Dataset: zara2               Batch: 15/18	Loss 7.0180 (6.8462)
2022-11-09 17:44:29,941:INFO: Dataset: zara2               Batch: 16/18	Loss 7.0825 (6.8604)
2022-11-09 17:44:29,966:INFO: Dataset: zara2               Batch: 17/18	Loss 7.5606 (6.8996)
2022-11-09 17:44:29,988:INFO: Dataset: zara2               Batch: 18/18	Loss 5.4791 (6.8280)
2022-11-09 17:44:30,039:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_28.pth.tar
2022-11-09 17:44:30,039:INFO: 
===> EPOCH: 29 (P1)
2022-11-09 17:44:30,039:INFO: - Computing loss (training)
2022-11-09 17:44:30,249:INFO: Dataset: hotel               Batch: 1/4	Loss 4.4500 (4.4500)
2022-11-09 17:44:30,276:INFO: Dataset: hotel               Batch: 2/4	Loss 4.2315 (4.3368)
2022-11-09 17:44:30,300:INFO: Dataset: hotel               Batch: 3/4	Loss 3.9847 (4.2208)
2022-11-09 17:44:30,317:INFO: Dataset: hotel               Batch: 4/4	Loss 2.3595 (3.9089)
2022-11-09 17:44:30,574:INFO: Dataset: univ                Batch:  1/15	Loss 4.6643 (4.6643)
2022-11-09 17:44:30,600:INFO: Dataset: univ                Batch:  2/15	Loss 5.4685 (5.0488)
2022-11-09 17:44:30,626:INFO: Dataset: univ                Batch:  3/15	Loss 4.8550 (4.9818)
2022-11-09 17:44:30,654:INFO: Dataset: univ                Batch:  4/15	Loss 5.8876 (5.1885)
2022-11-09 17:44:30,679:INFO: Dataset: univ                Batch:  5/15	Loss 5.5218 (5.2504)
2022-11-09 17:44:30,707:INFO: Dataset: univ                Batch:  6/15	Loss 4.8519 (5.1795)
2022-11-09 17:44:30,732:INFO: Dataset: univ                Batch:  7/15	Loss 4.9497 (5.1452)
2022-11-09 17:44:30,757:INFO: Dataset: univ                Batch:  8/15	Loss 5.8700 (5.2298)
2022-11-09 17:44:30,783:INFO: Dataset: univ                Batch:  9/15	Loss 5.1229 (5.2178)
2022-11-09 17:44:30,809:INFO: Dataset: univ                Batch: 10/15	Loss 5.5803 (5.2569)
2022-11-09 17:44:30,837:INFO: Dataset: univ                Batch: 11/15	Loss 4.9028 (5.2218)
2022-11-09 17:44:30,864:INFO: Dataset: univ                Batch: 12/15	Loss 4.5239 (5.1600)
2022-11-09 17:44:30,890:INFO: Dataset: univ                Batch: 13/15	Loss 4.9865 (5.1472)
2022-11-09 17:44:30,915:INFO: Dataset: univ                Batch: 14/15	Loss 4.6787 (5.1107)
2022-11-09 17:44:30,923:INFO: Dataset: univ                Batch: 15/15	Loss 0.8751 (5.0551)
2022-11-09 17:44:31,169:INFO: Dataset: zara1               Batch: 1/8	Loss 13.1371 (13.1371)
2022-11-09 17:44:31,196:INFO: Dataset: zara1               Batch: 2/8	Loss 11.3259 (12.2166)
2022-11-09 17:44:31,220:INFO: Dataset: zara1               Batch: 3/8	Loss 9.7584 (11.3471)
2022-11-09 17:44:31,246:INFO: Dataset: zara1               Batch: 4/8	Loss 12.3216 (11.5753)
2022-11-09 17:44:31,269:INFO: Dataset: zara1               Batch: 5/8	Loss 9.1806 (11.0972)
2022-11-09 17:44:31,294:INFO: Dataset: zara1               Batch: 6/8	Loss 10.4556 (11.0019)
2022-11-09 17:44:31,317:INFO: Dataset: zara1               Batch: 7/8	Loss 8.1509 (10.5818)
2022-11-09 17:44:31,338:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6320 (10.2558)
2022-11-09 17:44:31,595:INFO: Dataset: zara2               Batch:  1/18	Loss 6.4917 (6.4917)
2022-11-09 17:44:31,652:INFO: Dataset: zara2               Batch:  2/18	Loss 9.2282 (7.8523)
2022-11-09 17:44:31,677:INFO: Dataset: zara2               Batch:  3/18	Loss 7.6933 (7.8005)
2022-11-09 17:44:31,701:INFO: Dataset: zara2               Batch:  4/18	Loss 6.6848 (7.5151)
2022-11-09 17:44:31,725:INFO: Dataset: zara2               Batch:  5/18	Loss 10.4144 (8.0243)
2022-11-09 17:44:31,751:INFO: Dataset: zara2               Batch:  6/18	Loss 8.3778 (8.0810)
2022-11-09 17:44:31,774:INFO: Dataset: zara2               Batch:  7/18	Loss 6.3766 (7.8253)
2022-11-09 17:44:31,797:INFO: Dataset: zara2               Batch:  8/18	Loss 6.2846 (7.6364)
2022-11-09 17:44:31,822:INFO: Dataset: zara2               Batch:  9/18	Loss 6.5698 (7.5311)
2022-11-09 17:44:31,846:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1993 (7.4071)
2022-11-09 17:44:31,873:INFO: Dataset: zara2               Batch: 11/18	Loss 6.7689 (7.3510)
2022-11-09 17:44:31,897:INFO: Dataset: zara2               Batch: 12/18	Loss 5.8718 (7.2331)
2022-11-09 17:44:31,920:INFO: Dataset: zara2               Batch: 13/18	Loss 5.5027 (7.0912)
2022-11-09 17:44:31,945:INFO: Dataset: zara2               Batch: 14/18	Loss 7.1793 (7.0977)
2022-11-09 17:44:31,970:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7106 (7.0734)
2022-11-09 17:44:31,994:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4236 (7.0294)
2022-11-09 17:44:32,020:INFO: Dataset: zara2               Batch: 17/18	Loss 5.7682 (6.9560)
2022-11-09 17:44:32,044:INFO: Dataset: zara2               Batch: 18/18	Loss 5.8206 (6.9008)
2022-11-09 17:44:32,092:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_29.pth.tar
2022-11-09 17:44:32,092:INFO: 
===> EPOCH: 30 (P1)
2022-11-09 17:44:32,093:INFO: - Computing loss (training)
2022-11-09 17:44:32,304:INFO: Dataset: hotel               Batch: 1/4	Loss 4.1713 (4.1713)
2022-11-09 17:44:32,328:INFO: Dataset: hotel               Batch: 2/4	Loss 6.6915 (5.3937)
2022-11-09 17:44:32,351:INFO: Dataset: hotel               Batch: 3/4	Loss 4.1064 (4.9800)
2022-11-09 17:44:32,367:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0993 (4.5354)
2022-11-09 17:44:32,618:INFO: Dataset: univ                Batch:  1/15	Loss 4.7724 (4.7724)
2022-11-09 17:44:32,649:INFO: Dataset: univ                Batch:  2/15	Loss 5.1723 (4.9751)
2022-11-09 17:44:32,674:INFO: Dataset: univ                Batch:  3/15	Loss 5.1366 (5.0284)
2022-11-09 17:44:32,699:INFO: Dataset: univ                Batch:  4/15	Loss 5.5124 (5.1418)
2022-11-09 17:44:32,725:INFO: Dataset: univ                Batch:  5/15	Loss 5.5968 (5.2328)
2022-11-09 17:44:32,753:INFO: Dataset: univ                Batch:  6/15	Loss 5.2392 (5.2339)
2022-11-09 17:44:32,778:INFO: Dataset: univ                Batch:  7/15	Loss 5.2076 (5.2305)
2022-11-09 17:44:32,803:INFO: Dataset: univ                Batch:  8/15	Loss 4.8279 (5.1766)
2022-11-09 17:44:32,828:INFO: Dataset: univ                Batch:  9/15	Loss 5.3816 (5.1982)
2022-11-09 17:44:32,855:INFO: Dataset: univ                Batch: 10/15	Loss 4.6846 (5.1420)
2022-11-09 17:44:32,882:INFO: Dataset: univ                Batch: 11/15	Loss 4.5271 (5.0789)
2022-11-09 17:44:32,909:INFO: Dataset: univ                Batch: 12/15	Loss 4.8265 (5.0590)
2022-11-09 17:44:32,935:INFO: Dataset: univ                Batch: 13/15	Loss 5.1947 (5.0694)
2022-11-09 17:44:32,959:INFO: Dataset: univ                Batch: 14/15	Loss 4.8291 (5.0538)
2022-11-09 17:44:32,969:INFO: Dataset: univ                Batch: 15/15	Loss 0.9187 (5.0017)
2022-11-09 17:44:33,215:INFO: Dataset: zara1               Batch: 1/8	Loss 14.4046 (14.4046)
2022-11-09 17:44:33,240:INFO: Dataset: zara1               Batch: 2/8	Loss 13.9302 (14.1633)
2022-11-09 17:44:33,264:INFO: Dataset: zara1               Batch: 3/8	Loss 13.2173 (13.8923)
2022-11-09 17:44:33,289:INFO: Dataset: zara1               Batch: 4/8	Loss 9.2550 (12.7884)
2022-11-09 17:44:33,313:INFO: Dataset: zara1               Batch: 5/8	Loss 9.2779 (12.1472)
2022-11-09 17:44:33,339:INFO: Dataset: zara1               Batch: 6/8	Loss 10.2792 (11.8474)
2022-11-09 17:44:33,362:INFO: Dataset: zara1               Batch: 7/8	Loss 7.6158 (11.2201)
2022-11-09 17:44:33,383:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6387 (10.7413)
2022-11-09 17:44:33,653:INFO: Dataset: zara2               Batch:  1/18	Loss 7.1246 (7.1246)
2022-11-09 17:44:33,680:INFO: Dataset: zara2               Batch:  2/18	Loss 6.3705 (6.7418)
2022-11-09 17:44:33,704:INFO: Dataset: zara2               Batch:  3/18	Loss 6.1510 (6.5469)
2022-11-09 17:44:33,727:INFO: Dataset: zara2               Batch:  4/18	Loss 6.5210 (6.5401)
2022-11-09 17:44:33,754:INFO: Dataset: zara2               Batch:  5/18	Loss 5.9026 (6.4284)
2022-11-09 17:44:33,781:INFO: Dataset: zara2               Batch:  6/18	Loss 7.5431 (6.6249)
2022-11-09 17:44:33,804:INFO: Dataset: zara2               Batch:  7/18	Loss 6.3170 (6.5769)
2022-11-09 17:44:33,828:INFO: Dataset: zara2               Batch:  8/18	Loss 7.3376 (6.6714)
2022-11-09 17:44:33,851:INFO: Dataset: zara2               Batch:  9/18	Loss 7.1092 (6.7168)
2022-11-09 17:44:33,875:INFO: Dataset: zara2               Batch: 10/18	Loss 7.9689 (6.8515)
2022-11-09 17:44:33,899:INFO: Dataset: zara2               Batch: 11/18	Loss 7.3961 (6.8998)
2022-11-09 17:44:33,926:INFO: Dataset: zara2               Batch: 12/18	Loss 6.6001 (6.8737)
2022-11-09 17:44:33,950:INFO: Dataset: zara2               Batch: 13/18	Loss 6.6894 (6.8593)
2022-11-09 17:44:33,974:INFO: Dataset: zara2               Batch: 14/18	Loss 6.2357 (6.8091)
2022-11-09 17:44:33,998:INFO: Dataset: zara2               Batch: 15/18	Loss 7.1797 (6.8339)
2022-11-09 17:44:34,023:INFO: Dataset: zara2               Batch: 16/18	Loss 6.3543 (6.8008)
2022-11-09 17:44:34,047:INFO: Dataset: zara2               Batch: 17/18	Loss 6.3128 (6.7731)
2022-11-09 17:44:34,069:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6663 (6.6728)
2022-11-09 17:44:34,117:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_30.pth.tar
2022-11-09 17:44:34,117:INFO: 
===> EPOCH: 31 (P1)
2022-11-09 17:44:34,117:INFO: - Computing loss (training)
2022-11-09 17:44:34,328:INFO: Dataset: hotel               Batch: 1/4	Loss 3.5988 (3.5988)
2022-11-09 17:44:34,354:INFO: Dataset: hotel               Batch: 2/4	Loss 3.6134 (3.6061)
2022-11-09 17:44:34,379:INFO: Dataset: hotel               Batch: 3/4	Loss 4.3301 (3.8370)
2022-11-09 17:44:34,395:INFO: Dataset: hotel               Batch: 4/4	Loss 2.4561 (3.5984)
2022-11-09 17:44:34,661:INFO: Dataset: univ                Batch:  1/15	Loss 4.9360 (4.9360)
2022-11-09 17:44:34,690:INFO: Dataset: univ                Batch:  2/15	Loss 5.0469 (4.9908)
2022-11-09 17:44:34,718:INFO: Dataset: univ                Batch:  3/15	Loss 4.4075 (4.7870)
2022-11-09 17:44:34,743:INFO: Dataset: univ                Batch:  4/15	Loss 5.7130 (5.0153)
2022-11-09 17:44:34,771:INFO: Dataset: univ                Batch:  5/15	Loss 4.4903 (4.9094)
2022-11-09 17:44:34,799:INFO: Dataset: univ                Batch:  6/15	Loss 4.6441 (4.8684)
2022-11-09 17:44:34,824:INFO: Dataset: univ                Batch:  7/15	Loss 5.0593 (4.8955)
2022-11-09 17:44:34,849:INFO: Dataset: univ                Batch:  8/15	Loss 4.2207 (4.8116)
2022-11-09 17:44:34,875:INFO: Dataset: univ                Batch:  9/15	Loss 5.0886 (4.8420)
2022-11-09 17:44:34,902:INFO: Dataset: univ                Batch: 10/15	Loss 4.7651 (4.8338)
2022-11-09 17:44:34,929:INFO: Dataset: univ                Batch: 11/15	Loss 4.7514 (4.8269)
2022-11-09 17:44:34,957:INFO: Dataset: univ                Batch: 12/15	Loss 5.4903 (4.8800)
2022-11-09 17:44:34,982:INFO: Dataset: univ                Batch: 13/15	Loss 7.0704 (5.0380)
2022-11-09 17:44:35,007:INFO: Dataset: univ                Batch: 14/15	Loss 5.1996 (5.0490)
2022-11-09 17:44:35,017:INFO: Dataset: univ                Batch: 15/15	Loss 1.2540 (5.0086)
2022-11-09 17:44:35,254:INFO: Dataset: zara1               Batch: 1/8	Loss 10.7441 (10.7441)
2022-11-09 17:44:35,280:INFO: Dataset: zara1               Batch: 2/8	Loss 8.8499 (9.7464)
2022-11-09 17:44:35,305:INFO: Dataset: zara1               Batch: 3/8	Loss 14.2587 (11.1329)
2022-11-09 17:44:35,331:INFO: Dataset: zara1               Batch: 4/8	Loss 11.6142 (11.2449)
2022-11-09 17:44:35,354:INFO: Dataset: zara1               Batch: 5/8	Loss 11.7157 (11.3344)
2022-11-09 17:44:35,378:INFO: Dataset: zara1               Batch: 6/8	Loss 9.1165 (10.9307)
2022-11-09 17:44:35,402:INFO: Dataset: zara1               Batch: 7/8	Loss 7.7121 (10.4610)
2022-11-09 17:44:35,423:INFO: Dataset: zara1               Batch: 8/8	Loss 8.2152 (10.1998)
2022-11-09 17:44:35,676:INFO: Dataset: zara2               Batch:  1/18	Loss 6.6927 (6.6927)
2022-11-09 17:44:35,702:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1091 (6.8983)
2022-11-09 17:44:35,727:INFO: Dataset: zara2               Batch:  3/18	Loss 7.5886 (7.1289)
2022-11-09 17:44:35,754:INFO: Dataset: zara2               Batch:  4/18	Loss 7.5463 (7.2376)
2022-11-09 17:44:35,780:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1741 (7.2234)
2022-11-09 17:44:35,806:INFO: Dataset: zara2               Batch:  6/18	Loss 8.2176 (7.3721)
2022-11-09 17:44:35,829:INFO: Dataset: zara2               Batch:  7/18	Loss 8.5619 (7.5278)
2022-11-09 17:44:35,853:INFO: Dataset: zara2               Batch:  8/18	Loss 6.7380 (7.4270)
2022-11-09 17:44:35,876:INFO: Dataset: zara2               Batch:  9/18	Loss 6.7206 (7.3468)
2022-11-09 17:44:35,901:INFO: Dataset: zara2               Batch: 10/18	Loss 6.3156 (7.2422)
2022-11-09 17:44:35,928:INFO: Dataset: zara2               Batch: 11/18	Loss 6.0390 (7.1258)
2022-11-09 17:44:35,953:INFO: Dataset: zara2               Batch: 12/18	Loss 6.7604 (7.0919)
2022-11-09 17:44:35,977:INFO: Dataset: zara2               Batch: 13/18	Loss 6.3491 (7.0344)
2022-11-09 17:44:36,000:INFO: Dataset: zara2               Batch: 14/18	Loss 6.2039 (6.9790)
2022-11-09 17:44:36,025:INFO: Dataset: zara2               Batch: 15/18	Loss 6.2645 (6.9303)
2022-11-09 17:44:36,049:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4001 (6.8993)
2022-11-09 17:44:36,074:INFO: Dataset: zara2               Batch: 17/18	Loss 5.9929 (6.8438)
2022-11-09 17:44:36,097:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1090 (6.7589)
2022-11-09 17:44:36,145:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_31.pth.tar
2022-11-09 17:44:36,146:INFO: 
===> EPOCH: 32 (P1)
2022-11-09 17:44:36,146:INFO: - Computing loss (training)
2022-11-09 17:44:36,353:INFO: Dataset: hotel               Batch: 1/4	Loss 3.2698 (3.2698)
2022-11-09 17:44:36,381:INFO: Dataset: hotel               Batch: 2/4	Loss 4.8170 (4.0229)
2022-11-09 17:44:36,404:INFO: Dataset: hotel               Batch: 3/4	Loss 4.6188 (4.2256)
2022-11-09 17:44:36,422:INFO: Dataset: hotel               Batch: 4/4	Loss 2.7854 (3.9805)
2022-11-09 17:44:36,678:INFO: Dataset: univ                Batch:  1/15	Loss 5.1056 (5.1056)
2022-11-09 17:44:36,706:INFO: Dataset: univ                Batch:  2/15	Loss 5.6469 (5.3775)
2022-11-09 17:44:36,732:INFO: Dataset: univ                Batch:  3/15	Loss 4.8641 (5.2023)
2022-11-09 17:44:36,761:INFO: Dataset: univ                Batch:  4/15	Loss 4.9182 (5.1335)
2022-11-09 17:44:36,787:INFO: Dataset: univ                Batch:  5/15	Loss 5.2691 (5.1593)
2022-11-09 17:44:36,814:INFO: Dataset: univ                Batch:  6/15	Loss 4.9728 (5.1282)
2022-11-09 17:44:36,839:INFO: Dataset: univ                Batch:  7/15	Loss 4.8450 (5.0853)
2022-11-09 17:44:36,864:INFO: Dataset: univ                Batch:  8/15	Loss 5.3045 (5.1120)
2022-11-09 17:44:36,889:INFO: Dataset: univ                Batch:  9/15	Loss 5.6079 (5.1653)
2022-11-09 17:44:36,915:INFO: Dataset: univ                Batch: 10/15	Loss 4.7735 (5.1257)
2022-11-09 17:44:36,941:INFO: Dataset: univ                Batch: 11/15	Loss 5.2209 (5.1344)
2022-11-09 17:44:36,969:INFO: Dataset: univ                Batch: 12/15	Loss 4.9207 (5.1157)
2022-11-09 17:44:36,994:INFO: Dataset: univ                Batch: 13/15	Loss 5.2746 (5.1289)
2022-11-09 17:44:37,020:INFO: Dataset: univ                Batch: 14/15	Loss 4.8368 (5.1065)
2022-11-09 17:44:37,030:INFO: Dataset: univ                Batch: 15/15	Loss 0.8954 (5.0519)
2022-11-09 17:44:37,284:INFO: Dataset: zara1               Batch: 1/8	Loss 9.0949 (9.0949)
2022-11-09 17:44:37,310:INFO: Dataset: zara1               Batch: 2/8	Loss 11.4183 (10.2244)
2022-11-09 17:44:37,334:INFO: Dataset: zara1               Batch: 3/8	Loss 13.4212 (11.3154)
2022-11-09 17:44:37,360:INFO: Dataset: zara1               Batch: 4/8	Loss 10.1370 (11.0092)
2022-11-09 17:44:37,383:INFO: Dataset: zara1               Batch: 5/8	Loss 12.4990 (11.3047)
2022-11-09 17:44:37,408:INFO: Dataset: zara1               Batch: 6/8	Loss 8.7595 (10.8998)
2022-11-09 17:44:37,431:INFO: Dataset: zara1               Batch: 7/8	Loss 8.4794 (10.5488)
2022-11-09 17:44:37,452:INFO: Dataset: zara1               Batch: 8/8	Loss 7.0982 (10.1293)
2022-11-09 17:44:37,713:INFO: Dataset: zara2               Batch:  1/18	Loss 6.4467 (6.4467)
2022-11-09 17:44:37,743:INFO: Dataset: zara2               Batch:  2/18	Loss 6.4507 (6.4487)
2022-11-09 17:44:37,767:INFO: Dataset: zara2               Batch:  3/18	Loss 6.6159 (6.5036)
2022-11-09 17:44:37,790:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0309 (6.6288)
2022-11-09 17:44:37,816:INFO: Dataset: zara2               Batch:  5/18	Loss 6.6990 (6.6430)
2022-11-09 17:44:37,841:INFO: Dataset: zara2               Batch:  6/18	Loss 6.3009 (6.5824)
2022-11-09 17:44:37,864:INFO: Dataset: zara2               Batch:  7/18	Loss 7.6316 (6.7219)
2022-11-09 17:44:37,888:INFO: Dataset: zara2               Batch:  8/18	Loss 7.1728 (6.7740)
2022-11-09 17:44:37,911:INFO: Dataset: zara2               Batch:  9/18	Loss 6.7003 (6.7657)
2022-11-09 17:44:37,936:INFO: Dataset: zara2               Batch: 10/18	Loss 6.5480 (6.7443)
2022-11-09 17:44:37,962:INFO: Dataset: zara2               Batch: 11/18	Loss 7.3671 (6.8046)
2022-11-09 17:44:37,988:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8094 (6.8049)
2022-11-09 17:44:38,013:INFO: Dataset: zara2               Batch: 13/18	Loss 6.8932 (6.8116)
2022-11-09 17:44:38,037:INFO: Dataset: zara2               Batch: 14/18	Loss 5.7483 (6.7382)
2022-11-09 17:44:38,062:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7507 (6.7390)
2022-11-09 17:44:38,087:INFO: Dataset: zara2               Batch: 16/18	Loss 6.1413 (6.7021)
2022-11-09 17:44:38,112:INFO: Dataset: zara2               Batch: 17/18	Loss 6.0873 (6.6649)
2022-11-09 17:44:38,135:INFO: Dataset: zara2               Batch: 18/18	Loss 5.4151 (6.6075)
2022-11-09 17:44:38,196:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_32.pth.tar
2022-11-09 17:44:38,196:INFO: 
===> EPOCH: 33 (P1)
2022-11-09 17:44:38,196:INFO: - Computing loss (training)
2022-11-09 17:44:38,408:INFO: Dataset: hotel               Batch: 1/4	Loss 4.4641 (4.4641)
2022-11-09 17:44:38,432:INFO: Dataset: hotel               Batch: 2/4	Loss 4.1383 (4.2881)
2022-11-09 17:44:38,455:INFO: Dataset: hotel               Batch: 3/4	Loss 3.9530 (4.1848)
2022-11-09 17:44:38,472:INFO: Dataset: hotel               Batch: 4/4	Loss 2.3803 (3.8777)
2022-11-09 17:44:38,732:INFO: Dataset: univ                Batch:  1/15	Loss 5.2779 (5.2779)
2022-11-09 17:44:38,759:INFO: Dataset: univ                Batch:  2/15	Loss 4.6026 (4.9260)
2022-11-09 17:44:38,784:INFO: Dataset: univ                Batch:  3/15	Loss 4.6753 (4.8427)
2022-11-09 17:44:38,811:INFO: Dataset: univ                Batch:  4/15	Loss 4.7007 (4.8081)
2022-11-09 17:44:38,839:INFO: Dataset: univ                Batch:  5/15	Loss 4.5600 (4.7598)
2022-11-09 17:44:38,867:INFO: Dataset: univ                Batch:  6/15	Loss 4.8766 (4.7786)
2022-11-09 17:44:38,892:INFO: Dataset: univ                Batch:  7/15	Loss 5.2257 (4.8393)
2022-11-09 17:44:38,917:INFO: Dataset: univ                Batch:  8/15	Loss 5.8243 (4.9484)
2022-11-09 17:44:38,942:INFO: Dataset: univ                Batch:  9/15	Loss 5.3703 (4.9892)
2022-11-09 17:44:38,968:INFO: Dataset: univ                Batch: 10/15	Loss 4.7079 (4.9622)
2022-11-09 17:44:38,998:INFO: Dataset: univ                Batch: 11/15	Loss 4.2239 (4.8911)
2022-11-09 17:44:39,024:INFO: Dataset: univ                Batch: 12/15	Loss 4.7101 (4.8749)
2022-11-09 17:44:39,049:INFO: Dataset: univ                Batch: 13/15	Loss 4.3872 (4.8361)
2022-11-09 17:44:39,075:INFO: Dataset: univ                Batch: 14/15	Loss 4.4280 (4.8045)
2022-11-09 17:44:39,084:INFO: Dataset: univ                Batch: 15/15	Loss 1.0613 (4.7653)
2022-11-09 17:44:39,342:INFO: Dataset: zara1               Batch: 1/8	Loss 12.8138 (12.8138)
2022-11-09 17:44:39,367:INFO: Dataset: zara1               Batch: 2/8	Loss 8.9263 (10.8207)
2022-11-09 17:44:39,390:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9933 (9.8768)
2022-11-09 17:44:39,414:INFO: Dataset: zara1               Batch: 4/8	Loss 10.9063 (10.1789)
2022-11-09 17:44:39,437:INFO: Dataset: zara1               Batch: 5/8	Loss 8.8712 (9.9101)
2022-11-09 17:44:39,463:INFO: Dataset: zara1               Batch: 6/8	Loss 11.1936 (10.1432)
2022-11-09 17:44:39,487:INFO: Dataset: zara1               Batch: 7/8	Loss 9.2765 (10.0122)
2022-11-09 17:44:39,508:INFO: Dataset: zara1               Batch: 8/8	Loss 7.7365 (9.7331)
2022-11-09 17:44:39,763:INFO: Dataset: zara2               Batch:  1/18	Loss 6.8337 (6.8337)
2022-11-09 17:44:39,815:INFO: Dataset: zara2               Batch:  2/18	Loss 7.7730 (7.3367)
2022-11-09 17:44:39,838:INFO: Dataset: zara2               Batch:  3/18	Loss 9.2388 (7.9598)
2022-11-09 17:44:39,862:INFO: Dataset: zara2               Batch:  4/18	Loss 7.4123 (7.8247)
2022-11-09 17:44:39,886:INFO: Dataset: zara2               Batch:  5/18	Loss 7.6815 (7.7957)
2022-11-09 17:44:39,912:INFO: Dataset: zara2               Batch:  6/18	Loss 7.0690 (7.6665)
2022-11-09 17:44:39,935:INFO: Dataset: zara2               Batch:  7/18	Loss 6.3901 (7.4789)
2022-11-09 17:44:39,959:INFO: Dataset: zara2               Batch:  8/18	Loss 5.8362 (7.2727)
2022-11-09 17:44:39,983:INFO: Dataset: zara2               Batch:  9/18	Loss 6.3634 (7.1678)
2022-11-09 17:44:40,008:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1597 (7.0698)
2022-11-09 17:44:40,034:INFO: Dataset: zara2               Batch: 11/18	Loss 6.0094 (6.9766)
2022-11-09 17:44:40,059:INFO: Dataset: zara2               Batch: 12/18	Loss 5.3264 (6.8403)
2022-11-09 17:44:40,082:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9603 (6.8493)
2022-11-09 17:44:40,107:INFO: Dataset: zara2               Batch: 14/18	Loss 5.7774 (6.7601)
2022-11-09 17:44:40,131:INFO: Dataset: zara2               Batch: 15/18	Loss 8.1819 (6.8503)
2022-11-09 17:44:40,156:INFO: Dataset: zara2               Batch: 16/18	Loss 7.1817 (6.8717)
2022-11-09 17:44:40,180:INFO: Dataset: zara2               Batch: 17/18	Loss 6.5446 (6.8506)
2022-11-09 17:44:40,203:INFO: Dataset: zara2               Batch: 18/18	Loss 5.8011 (6.7982)
2022-11-09 17:44:40,257:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_33.pth.tar
2022-11-09 17:44:40,257:INFO: 
===> EPOCH: 34 (P1)
2022-11-09 17:44:40,257:INFO: - Computing loss (training)
2022-11-09 17:44:40,463:INFO: Dataset: hotel               Batch: 1/4	Loss 5.4093 (5.4093)
2022-11-09 17:44:40,487:INFO: Dataset: hotel               Batch: 2/4	Loss 5.3487 (5.3790)
2022-11-09 17:44:40,512:INFO: Dataset: hotel               Batch: 3/4	Loss 3.4536 (4.6938)
2022-11-09 17:44:40,529:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0103 (4.2088)
2022-11-09 17:44:40,783:INFO: Dataset: univ                Batch:  1/15	Loss 4.5246 (4.5246)
2022-11-09 17:44:40,809:INFO: Dataset: univ                Batch:  2/15	Loss 5.1958 (4.8316)
2022-11-09 17:44:40,836:INFO: Dataset: univ                Batch:  3/15	Loss 4.6469 (4.7633)
2022-11-09 17:44:40,864:INFO: Dataset: univ                Batch:  4/15	Loss 4.6548 (4.7344)
2022-11-09 17:44:40,889:INFO: Dataset: univ                Batch:  5/15	Loss 4.8039 (4.7493)
2022-11-09 17:44:40,919:INFO: Dataset: univ                Batch:  6/15	Loss 4.6178 (4.7282)
2022-11-09 17:44:40,944:INFO: Dataset: univ                Batch:  7/15	Loss 5.2395 (4.7994)
2022-11-09 17:44:40,969:INFO: Dataset: univ                Batch:  8/15	Loss 4.4980 (4.7615)
2022-11-09 17:44:40,995:INFO: Dataset: univ                Batch:  9/15	Loss 4.7168 (4.7563)
2022-11-09 17:44:41,021:INFO: Dataset: univ                Batch: 10/15	Loss 4.6060 (4.7423)
2022-11-09 17:44:41,047:INFO: Dataset: univ                Batch: 11/15	Loss 5.7644 (4.8303)
2022-11-09 17:44:41,074:INFO: Dataset: univ                Batch: 12/15	Loss 5.0506 (4.8495)
2022-11-09 17:44:41,099:INFO: Dataset: univ                Batch: 13/15	Loss 4.8027 (4.8462)
2022-11-09 17:44:41,125:INFO: Dataset: univ                Batch: 14/15	Loss 4.8870 (4.8493)
2022-11-09 17:44:41,135:INFO: Dataset: univ                Batch: 15/15	Loss 0.9484 (4.8102)
2022-11-09 17:44:41,385:INFO: Dataset: zara1               Batch: 1/8	Loss 10.8106 (10.8106)
2022-11-09 17:44:41,410:INFO: Dataset: zara1               Batch: 2/8	Loss 10.4429 (10.6189)
2022-11-09 17:44:41,434:INFO: Dataset: zara1               Batch: 3/8	Loss 11.0227 (10.7508)
2022-11-09 17:44:41,458:INFO: Dataset: zara1               Batch: 4/8	Loss 10.6740 (10.7315)
2022-11-09 17:44:41,482:INFO: Dataset: zara1               Batch: 5/8	Loss 9.6049 (10.5168)
2022-11-09 17:44:41,508:INFO: Dataset: zara1               Batch: 6/8	Loss 8.2807 (10.1434)
2022-11-09 17:44:41,532:INFO: Dataset: zara1               Batch: 7/8	Loss 9.5353 (10.0573)
2022-11-09 17:44:41,553:INFO: Dataset: zara1               Batch: 8/8	Loss 6.8396 (9.6983)
2022-11-09 17:44:41,800:INFO: Dataset: zara2               Batch:  1/18	Loss 7.3410 (7.3410)
2022-11-09 17:44:41,828:INFO: Dataset: zara2               Batch:  2/18	Loss 5.9595 (6.6410)
2022-11-09 17:44:41,854:INFO: Dataset: zara2               Batch:  3/18	Loss 6.2247 (6.5086)
2022-11-09 17:44:41,878:INFO: Dataset: zara2               Batch:  4/18	Loss 8.5129 (7.0187)
2022-11-09 17:44:41,903:INFO: Dataset: zara2               Batch:  5/18	Loss 7.0825 (7.0310)
2022-11-09 17:44:41,930:INFO: Dataset: zara2               Batch:  6/18	Loss 7.7524 (7.1560)
2022-11-09 17:44:41,953:INFO: Dataset: zara2               Batch:  7/18	Loss 6.2966 (7.0288)
2022-11-09 17:44:41,977:INFO: Dataset: zara2               Batch:  8/18	Loss 6.6879 (6.9869)
2022-11-09 17:44:42,001:INFO: Dataset: zara2               Batch:  9/18	Loss 6.3677 (6.9146)
2022-11-09 17:44:42,025:INFO: Dataset: zara2               Batch: 10/18	Loss 6.9640 (6.9197)
2022-11-09 17:44:42,050:INFO: Dataset: zara2               Batch: 11/18	Loss 5.0759 (6.7544)
2022-11-09 17:44:42,076:INFO: Dataset: zara2               Batch: 12/18	Loss 5.9943 (6.6909)
2022-11-09 17:44:42,100:INFO: Dataset: zara2               Batch: 13/18	Loss 6.0336 (6.6418)
2022-11-09 17:44:42,124:INFO: Dataset: zara2               Batch: 14/18	Loss 5.8348 (6.5815)
2022-11-09 17:44:42,148:INFO: Dataset: zara2               Batch: 15/18	Loss 5.7151 (6.5183)
2022-11-09 17:44:42,172:INFO: Dataset: zara2               Batch: 16/18	Loss 7.2151 (6.5628)
2022-11-09 17:44:42,197:INFO: Dataset: zara2               Batch: 17/18	Loss 5.9265 (6.5287)
2022-11-09 17:44:42,220:INFO: Dataset: zara2               Batch: 18/18	Loss 6.5021 (6.5273)
2022-11-09 17:44:42,272:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_34.pth.tar
2022-11-09 17:44:42,272:INFO: 
===> EPOCH: 35 (P1)
2022-11-09 17:44:42,273:INFO: - Computing loss (training)
2022-11-09 17:44:42,488:INFO: Dataset: hotel               Batch: 1/4	Loss 3.2827 (3.2827)
2022-11-09 17:44:42,513:INFO: Dataset: hotel               Batch: 2/4	Loss 3.7668 (3.5259)
2022-11-09 17:44:42,538:INFO: Dataset: hotel               Batch: 3/4	Loss 3.3529 (3.4693)
2022-11-09 17:44:42,555:INFO: Dataset: hotel               Batch: 4/4	Loss 2.8751 (3.3690)
2022-11-09 17:44:42,812:INFO: Dataset: univ                Batch:  1/15	Loss 4.7489 (4.7489)
2022-11-09 17:44:42,843:INFO: Dataset: univ                Batch:  2/15	Loss 4.7768 (4.7624)
2022-11-09 17:44:42,869:INFO: Dataset: univ                Batch:  3/15	Loss 4.9762 (4.8329)
2022-11-09 17:44:42,894:INFO: Dataset: univ                Batch:  4/15	Loss 4.3207 (4.6917)
2022-11-09 17:44:42,920:INFO: Dataset: univ                Batch:  5/15	Loss 4.4347 (4.6399)
2022-11-09 17:44:42,950:INFO: Dataset: univ                Batch:  6/15	Loss 5.3069 (4.7573)
2022-11-09 17:44:42,975:INFO: Dataset: univ                Batch:  7/15	Loss 4.8525 (4.7704)
2022-11-09 17:44:43,000:INFO: Dataset: univ                Batch:  8/15	Loss 4.3501 (4.7173)
2022-11-09 17:44:43,025:INFO: Dataset: univ                Batch:  9/15	Loss 4.9824 (4.7468)
2022-11-09 17:44:43,051:INFO: Dataset: univ                Batch: 10/15	Loss 4.2821 (4.7007)
2022-11-09 17:44:43,078:INFO: Dataset: univ                Batch: 11/15	Loss 4.5726 (4.6892)
2022-11-09 17:44:43,105:INFO: Dataset: univ                Batch: 12/15	Loss 4.9895 (4.7131)
2022-11-09 17:44:43,131:INFO: Dataset: univ                Batch: 13/15	Loss 5.4969 (4.7708)
2022-11-09 17:44:43,156:INFO: Dataset: univ                Batch: 14/15	Loss 4.5819 (4.7569)
2022-11-09 17:44:43,164:INFO: Dataset: univ                Batch: 15/15	Loss 0.9235 (4.7004)
2022-11-09 17:44:43,421:INFO: Dataset: zara1               Batch: 1/8	Loss 10.9958 (10.9958)
2022-11-09 17:44:43,447:INFO: Dataset: zara1               Batch: 2/8	Loss 15.2877 (13.0544)
2022-11-09 17:44:43,470:INFO: Dataset: zara1               Batch: 3/8	Loss 11.9178 (12.6410)
2022-11-09 17:44:43,494:INFO: Dataset: zara1               Batch: 4/8	Loss 9.1015 (11.6712)
2022-11-09 17:44:43,517:INFO: Dataset: zara1               Batch: 5/8	Loss 9.8583 (11.3340)
2022-11-09 17:44:43,542:INFO: Dataset: zara1               Batch: 6/8	Loss 10.6709 (11.2277)
2022-11-09 17:44:43,565:INFO: Dataset: zara1               Batch: 7/8	Loss 7.8321 (10.7898)
2022-11-09 17:44:43,586:INFO: Dataset: zara1               Batch: 8/8	Loss 7.0843 (10.3958)
2022-11-09 17:44:43,867:INFO: Dataset: zara2               Batch:  1/18	Loss 5.5862 (5.5862)
2022-11-09 17:44:43,895:INFO: Dataset: zara2               Batch:  2/18	Loss 6.6975 (6.1484)
2022-11-09 17:44:43,923:INFO: Dataset: zara2               Batch:  3/18	Loss 7.4325 (6.5705)
2022-11-09 17:44:43,946:INFO: Dataset: zara2               Batch:  4/18	Loss 8.1339 (6.9889)
2022-11-09 17:44:43,970:INFO: Dataset: zara2               Batch:  5/18	Loss 7.6751 (7.1185)
2022-11-09 17:44:43,996:INFO: Dataset: zara2               Batch:  6/18	Loss 7.0104 (7.1000)
2022-11-09 17:44:44,019:INFO: Dataset: zara2               Batch:  7/18	Loss 7.1675 (7.1089)
2022-11-09 17:44:44,043:INFO: Dataset: zara2               Batch:  8/18	Loss 7.2453 (7.1268)
2022-11-09 17:44:44,066:INFO: Dataset: zara2               Batch:  9/18	Loss 6.4095 (7.0560)
2022-11-09 17:44:44,091:INFO: Dataset: zara2               Batch: 10/18	Loss 6.3486 (6.9853)
2022-11-09 17:44:44,116:INFO: Dataset: zara2               Batch: 11/18	Loss 6.3163 (6.9226)
2022-11-09 17:44:44,142:INFO: Dataset: zara2               Batch: 12/18	Loss 6.3684 (6.8751)
2022-11-09 17:44:44,167:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9272 (6.8794)
2022-11-09 17:44:44,191:INFO: Dataset: zara2               Batch: 14/18	Loss 5.6264 (6.7886)
2022-11-09 17:44:44,216:INFO: Dataset: zara2               Batch: 15/18	Loss 6.3452 (6.7599)
2022-11-09 17:44:44,241:INFO: Dataset: zara2               Batch: 16/18	Loss 6.0391 (6.7121)
2022-11-09 17:44:44,266:INFO: Dataset: zara2               Batch: 17/18	Loss 6.4151 (6.6956)
2022-11-09 17:44:44,289:INFO: Dataset: zara2               Batch: 18/18	Loss 5.2789 (6.6239)
2022-11-09 17:44:44,340:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_35.pth.tar
2022-11-09 17:44:44,340:INFO: 
===> EPOCH: 36 (P1)
2022-11-09 17:44:44,341:INFO: - Computing loss (training)
2022-11-09 17:44:44,548:INFO: Dataset: hotel               Batch: 1/4	Loss 4.2436 (4.2436)
2022-11-09 17:44:44,574:INFO: Dataset: hotel               Batch: 2/4	Loss 3.7706 (3.9938)
2022-11-09 17:44:44,598:INFO: Dataset: hotel               Batch: 3/4	Loss 3.2415 (3.7478)
2022-11-09 17:44:44,615:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7726 (3.4221)
2022-11-09 17:44:44,871:INFO: Dataset: univ                Batch:  1/15	Loss 4.3408 (4.3408)
2022-11-09 17:44:44,942:INFO: Dataset: univ                Batch:  2/15	Loss 5.5689 (4.9428)
2022-11-09 17:44:44,971:INFO: Dataset: univ                Batch:  3/15	Loss 4.7581 (4.8785)
2022-11-09 17:44:44,996:INFO: Dataset: univ                Batch:  4/15	Loss 4.5713 (4.7997)
2022-11-09 17:44:45,022:INFO: Dataset: univ                Batch:  5/15	Loss 4.3845 (4.7134)
2022-11-09 17:44:45,050:INFO: Dataset: univ                Batch:  6/15	Loss 4.1252 (4.6021)
2022-11-09 17:44:45,075:INFO: Dataset: univ                Batch:  7/15	Loss 4.3728 (4.5675)
2022-11-09 17:44:45,101:INFO: Dataset: univ                Batch:  8/15	Loss 4.8931 (4.6053)
2022-11-09 17:44:45,128:INFO: Dataset: univ                Batch:  9/15	Loss 5.0198 (4.6503)
2022-11-09 17:44:45,153:INFO: Dataset: univ                Batch: 10/15	Loss 4.6629 (4.6515)
2022-11-09 17:44:45,178:INFO: Dataset: univ                Batch: 11/15	Loss 4.3331 (4.6243)
2022-11-09 17:44:45,205:INFO: Dataset: univ                Batch: 12/15	Loss 5.0939 (4.6615)
2022-11-09 17:44:45,230:INFO: Dataset: univ                Batch: 13/15	Loss 4.3304 (4.6340)
2022-11-09 17:44:45,257:INFO: Dataset: univ                Batch: 14/15	Loss 5.1648 (4.6702)
2022-11-09 17:44:45,266:INFO: Dataset: univ                Batch: 15/15	Loss 0.7004 (4.6157)
2022-11-09 17:44:45,520:INFO: Dataset: zara1               Batch: 1/8	Loss 9.1810 (9.1810)
2022-11-09 17:44:45,549:INFO: Dataset: zara1               Batch: 2/8	Loss 11.5332 (10.3617)
2022-11-09 17:44:45,572:INFO: Dataset: zara1               Batch: 3/8	Loss 13.3569 (11.3653)
2022-11-09 17:44:45,596:INFO: Dataset: zara1               Batch: 4/8	Loss 11.4065 (11.3750)
2022-11-09 17:44:45,619:INFO: Dataset: zara1               Batch: 5/8	Loss 8.5597 (10.8683)
2022-11-09 17:44:45,645:INFO: Dataset: zara1               Batch: 6/8	Loss 9.2220 (10.6230)
2022-11-09 17:44:45,669:INFO: Dataset: zara1               Batch: 7/8	Loss 8.6251 (10.3210)
2022-11-09 17:44:45,689:INFO: Dataset: zara1               Batch: 8/8	Loss 7.3655 (10.0099)
2022-11-09 17:44:45,949:INFO: Dataset: zara2               Batch:  1/18	Loss 6.9879 (6.9879)
2022-11-09 17:44:45,976:INFO: Dataset: zara2               Batch:  2/18	Loss 7.5937 (7.3076)
2022-11-09 17:44:46,000:INFO: Dataset: zara2               Batch:  3/18	Loss 7.2724 (7.2959)
2022-11-09 17:44:46,024:INFO: Dataset: zara2               Batch:  4/18	Loss 6.3676 (7.0576)
2022-11-09 17:44:46,051:INFO: Dataset: zara2               Batch:  5/18	Loss 6.4212 (6.9402)
2022-11-09 17:44:46,078:INFO: Dataset: zara2               Batch:  6/18	Loss 6.3429 (6.8336)
2022-11-09 17:44:46,102:INFO: Dataset: zara2               Batch:  7/18	Loss 6.4471 (6.7766)
2022-11-09 17:44:46,125:INFO: Dataset: zara2               Batch:  8/18	Loss 6.3030 (6.7217)
2022-11-09 17:44:46,149:INFO: Dataset: zara2               Batch:  9/18	Loss 5.4014 (6.5630)
2022-11-09 17:44:46,174:INFO: Dataset: zara2               Batch: 10/18	Loss 6.7362 (6.5817)
2022-11-09 17:44:46,200:INFO: Dataset: zara2               Batch: 11/18	Loss 6.4231 (6.5667)
2022-11-09 17:44:46,227:INFO: Dataset: zara2               Batch: 12/18	Loss 7.3327 (6.6301)
2022-11-09 17:44:46,252:INFO: Dataset: zara2               Batch: 13/18	Loss 5.8020 (6.5688)
2022-11-09 17:44:46,277:INFO: Dataset: zara2               Batch: 14/18	Loss 7.7063 (6.6393)
2022-11-09 17:44:46,302:INFO: Dataset: zara2               Batch: 15/18	Loss 5.5358 (6.5707)
2022-11-09 17:44:46,326:INFO: Dataset: zara2               Batch: 16/18	Loss 6.7151 (6.5796)
2022-11-09 17:44:46,350:INFO: Dataset: zara2               Batch: 17/18	Loss 5.7344 (6.5357)
2022-11-09 17:44:46,372:INFO: Dataset: zara2               Batch: 18/18	Loss 5.0829 (6.4682)
2022-11-09 17:44:46,421:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_36.pth.tar
2022-11-09 17:44:46,422:INFO: 
===> EPOCH: 37 (P1)
2022-11-09 17:44:46,422:INFO: - Computing loss (training)
2022-11-09 17:44:46,628:INFO: Dataset: hotel               Batch: 1/4	Loss 4.0526 (4.0526)
2022-11-09 17:44:46,654:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5222 (3.8016)
2022-11-09 17:44:46,680:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7161 (3.7742)
2022-11-09 17:44:46,696:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1892 (3.5087)
2022-11-09 17:44:46,942:INFO: Dataset: univ                Batch:  1/15	Loss 4.1797 (4.1797)
2022-11-09 17:44:46,969:INFO: Dataset: univ                Batch:  2/15	Loss 4.3800 (4.2823)
2022-11-09 17:44:46,995:INFO: Dataset: univ                Batch:  3/15	Loss 4.3491 (4.3056)
2022-11-09 17:44:47,024:INFO: Dataset: univ                Batch:  4/15	Loss 4.1948 (4.2782)
2022-11-09 17:44:47,049:INFO: Dataset: univ                Batch:  5/15	Loss 5.5283 (4.5066)
2022-11-09 17:44:47,077:INFO: Dataset: univ                Batch:  6/15	Loss 5.2904 (4.6361)
2022-11-09 17:44:47,102:INFO: Dataset: univ                Batch:  7/15	Loss 4.2014 (4.5774)
2022-11-09 17:44:47,127:INFO: Dataset: univ                Batch:  8/15	Loss 4.5198 (4.5698)
2022-11-09 17:44:47,152:INFO: Dataset: univ                Batch:  9/15	Loss 4.6344 (4.5765)
2022-11-09 17:44:47,179:INFO: Dataset: univ                Batch: 10/15	Loss 4.4464 (4.5630)
2022-11-09 17:44:47,205:INFO: Dataset: univ                Batch: 11/15	Loss 4.9999 (4.6001)
2022-11-09 17:44:47,232:INFO: Dataset: univ                Batch: 12/15	Loss 7.0016 (4.7883)
2022-11-09 17:44:47,257:INFO: Dataset: univ                Batch: 13/15	Loss 4.5350 (4.7699)
2022-11-09 17:44:47,283:INFO: Dataset: univ                Batch: 14/15	Loss 5.6030 (4.8290)
2022-11-09 17:44:47,293:INFO: Dataset: univ                Batch: 15/15	Loss 0.6115 (4.7498)
2022-11-09 17:44:47,549:INFO: Dataset: zara1               Batch: 1/8	Loss 11.7118 (11.7118)
2022-11-09 17:44:47,575:INFO: Dataset: zara1               Batch: 2/8	Loss 11.4787 (11.5975)
2022-11-09 17:44:47,599:INFO: Dataset: zara1               Batch: 3/8	Loss 9.9154 (11.0268)
2022-11-09 17:44:47,622:INFO: Dataset: zara1               Batch: 4/8	Loss 16.2809 (12.1768)
2022-11-09 17:44:47,645:INFO: Dataset: zara1               Batch: 5/8	Loss 11.5235 (12.0395)
2022-11-09 17:44:47,670:INFO: Dataset: zara1               Batch: 6/8	Loss 8.4802 (11.4823)
2022-11-09 17:44:47,694:INFO: Dataset: zara1               Batch: 7/8	Loss 8.7197 (11.0368)
2022-11-09 17:44:47,714:INFO: Dataset: zara1               Batch: 8/8	Loss 6.6392 (10.4998)
2022-11-09 17:44:47,998:INFO: Dataset: zara2               Batch:  1/18	Loss 7.6990 (7.6990)
2022-11-09 17:44:48,029:INFO: Dataset: zara2               Batch:  2/18	Loss 6.5748 (7.1160)
2022-11-09 17:44:48,052:INFO: Dataset: zara2               Batch:  3/18	Loss 7.7531 (7.3187)
2022-11-09 17:44:48,076:INFO: Dataset: zara2               Batch:  4/18	Loss 7.1246 (7.2690)
2022-11-09 17:44:48,100:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1846 (7.2509)
2022-11-09 17:44:48,126:INFO: Dataset: zara2               Batch:  6/18	Loss 7.8416 (7.3493)
2022-11-09 17:44:48,150:INFO: Dataset: zara2               Batch:  7/18	Loss 5.8579 (7.1314)
2022-11-09 17:44:48,173:INFO: Dataset: zara2               Batch:  8/18	Loss 6.3733 (7.0454)
2022-11-09 17:44:48,197:INFO: Dataset: zara2               Batch:  9/18	Loss 5.9746 (6.9302)
2022-11-09 17:44:48,222:INFO: Dataset: zara2               Batch: 10/18	Loss 6.5133 (6.8869)
2022-11-09 17:44:48,247:INFO: Dataset: zara2               Batch: 11/18	Loss 6.0331 (6.8105)
2022-11-09 17:44:48,273:INFO: Dataset: zara2               Batch: 12/18	Loss 5.8126 (6.7182)
2022-11-09 17:44:48,297:INFO: Dataset: zara2               Batch: 13/18	Loss 6.6254 (6.7116)
2022-11-09 17:44:48,322:INFO: Dataset: zara2               Batch: 14/18	Loss 6.2855 (6.6804)
2022-11-09 17:44:48,347:INFO: Dataset: zara2               Batch: 15/18	Loss 5.7363 (6.6149)
2022-11-09 17:44:48,371:INFO: Dataset: zara2               Batch: 16/18	Loss 5.5477 (6.5478)
2022-11-09 17:44:48,396:INFO: Dataset: zara2               Batch: 17/18	Loss 5.8801 (6.5081)
2022-11-09 17:44:48,419:INFO: Dataset: zara2               Batch: 18/18	Loss 5.8187 (6.4734)
2022-11-09 17:44:48,469:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_37.pth.tar
2022-11-09 17:44:48,469:INFO: 
===> EPOCH: 38 (P1)
2022-11-09 17:44:48,470:INFO: - Computing loss (training)
2022-11-09 17:44:48,677:INFO: Dataset: hotel               Batch: 1/4	Loss 5.8093 (5.8093)
2022-11-09 17:44:48,703:INFO: Dataset: hotel               Batch: 2/4	Loss 3.3825 (4.5634)
2022-11-09 17:44:48,727:INFO: Dataset: hotel               Batch: 3/4	Loss 2.6958 (3.9299)
2022-11-09 17:44:48,744:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6841 (3.5269)
2022-11-09 17:44:49,054:INFO: Dataset: univ                Batch:  1/15	Loss 4.6565 (4.6565)
2022-11-09 17:44:49,083:INFO: Dataset: univ                Batch:  2/15	Loss 4.2223 (4.4245)
2022-11-09 17:44:49,108:INFO: Dataset: univ                Batch:  3/15	Loss 4.8396 (4.5443)
2022-11-09 17:44:49,134:INFO: Dataset: univ                Batch:  4/15	Loss 4.4109 (4.5084)
2022-11-09 17:44:49,159:INFO: Dataset: univ                Batch:  5/15	Loss 4.7093 (4.5492)
2022-11-09 17:44:49,187:INFO: Dataset: univ                Batch:  6/15	Loss 4.1792 (4.4905)
2022-11-09 17:44:49,212:INFO: Dataset: univ                Batch:  7/15	Loss 4.5280 (4.4960)
2022-11-09 17:44:49,237:INFO: Dataset: univ                Batch:  8/15	Loss 4.4751 (4.4934)
2022-11-09 17:44:49,263:INFO: Dataset: univ                Batch:  9/15	Loss 4.2751 (4.4692)
2022-11-09 17:44:49,289:INFO: Dataset: univ                Batch: 10/15	Loss 5.8209 (4.5920)
2022-11-09 17:44:49,315:INFO: Dataset: univ                Batch: 11/15	Loss 5.5858 (4.6760)
2022-11-09 17:44:49,343:INFO: Dataset: univ                Batch: 12/15	Loss 4.2097 (4.6357)
2022-11-09 17:44:49,369:INFO: Dataset: univ                Batch: 13/15	Loss 4.4792 (4.6236)
2022-11-09 17:44:49,395:INFO: Dataset: univ                Batch: 14/15	Loss 4.6008 (4.6220)
2022-11-09 17:44:49,405:INFO: Dataset: univ                Batch: 15/15	Loss 0.6738 (4.5692)
2022-11-09 17:44:49,663:INFO: Dataset: zara1               Batch: 1/8	Loss 13.3036 (13.3036)
2022-11-09 17:44:49,687:INFO: Dataset: zara1               Batch: 2/8	Loss 9.9951 (11.5820)
2022-11-09 17:44:49,710:INFO: Dataset: zara1               Batch: 3/8	Loss 8.9343 (10.5940)
2022-11-09 17:44:49,733:INFO: Dataset: zara1               Batch: 4/8	Loss 12.9310 (11.1361)
2022-11-09 17:44:49,757:INFO: Dataset: zara1               Batch: 5/8	Loss 9.1544 (10.7313)
2022-11-09 17:44:49,782:INFO: Dataset: zara1               Batch: 6/8	Loss 8.1308 (10.3050)
2022-11-09 17:44:49,806:INFO: Dataset: zara1               Batch: 7/8	Loss 10.1563 (10.2832)
2022-11-09 17:44:49,826:INFO: Dataset: zara1               Batch: 8/8	Loss 6.9539 (9.9468)
2022-11-09 17:44:50,089:INFO: Dataset: zara2               Batch:  1/18	Loss 6.8871 (6.8871)
2022-11-09 17:44:50,116:INFO: Dataset: zara2               Batch:  2/18	Loss 6.8440 (6.8647)
2022-11-09 17:44:50,143:INFO: Dataset: zara2               Batch:  3/18	Loss 10.7006 (8.0285)
2022-11-09 17:44:50,166:INFO: Dataset: zara2               Batch:  4/18	Loss 7.1845 (7.8128)
2022-11-09 17:44:50,192:INFO: Dataset: zara2               Batch:  5/18	Loss 6.6262 (7.5913)
2022-11-09 17:44:50,218:INFO: Dataset: zara2               Batch:  6/18	Loss 7.2209 (7.5280)
2022-11-09 17:44:50,241:INFO: Dataset: zara2               Batch:  7/18	Loss 6.0828 (7.3129)
2022-11-09 17:44:50,265:INFO: Dataset: zara2               Batch:  8/18	Loss 6.0349 (7.1567)
2022-11-09 17:44:50,288:INFO: Dataset: zara2               Batch:  9/18	Loss 6.2767 (7.0546)
2022-11-09 17:44:50,313:INFO: Dataset: zara2               Batch: 10/18	Loss 5.7265 (6.9251)
2022-11-09 17:44:50,340:INFO: Dataset: zara2               Batch: 11/18	Loss 5.5803 (6.7988)
2022-11-09 17:44:50,365:INFO: Dataset: zara2               Batch: 12/18	Loss 6.1096 (6.7426)
2022-11-09 17:44:50,389:INFO: Dataset: zara2               Batch: 13/18	Loss 6.0421 (6.6959)
2022-11-09 17:44:50,412:INFO: Dataset: zara2               Batch: 14/18	Loss 5.8739 (6.6417)
2022-11-09 17:44:50,437:INFO: Dataset: zara2               Batch: 15/18	Loss 5.0105 (6.5379)
2022-11-09 17:44:50,462:INFO: Dataset: zara2               Batch: 16/18	Loss 5.6885 (6.4810)
2022-11-09 17:44:50,486:INFO: Dataset: zara2               Batch: 17/18	Loss 6.2313 (6.4671)
2022-11-09 17:44:50,508:INFO: Dataset: zara2               Batch: 18/18	Loss 4.1083 (6.3400)
2022-11-09 17:44:50,560:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_38.pth.tar
2022-11-09 17:44:50,560:INFO: 
===> EPOCH: 39 (P1)
2022-11-09 17:44:50,561:INFO: - Computing loss (training)
2022-11-09 17:44:50,774:INFO: Dataset: hotel               Batch: 1/4	Loss 3.9448 (3.9448)
2022-11-09 17:44:50,798:INFO: Dataset: hotel               Batch: 2/4	Loss 3.6697 (3.8089)
2022-11-09 17:44:50,822:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7864 (3.8012)
2022-11-09 17:44:50,838:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0819 (3.5268)
2022-11-09 17:44:51,092:INFO: Dataset: univ                Batch:  1/15	Loss 4.1405 (4.1405)
2022-11-09 17:44:51,119:INFO: Dataset: univ                Batch:  2/15	Loss 4.9859 (4.5542)
2022-11-09 17:44:51,144:INFO: Dataset: univ                Batch:  3/15	Loss 4.4053 (4.5029)
2022-11-09 17:44:51,172:INFO: Dataset: univ                Batch:  4/15	Loss 5.3753 (4.7074)
2022-11-09 17:44:51,199:INFO: Dataset: univ                Batch:  5/15	Loss 6.3525 (5.0080)
2022-11-09 17:44:51,227:INFO: Dataset: univ                Batch:  6/15	Loss 5.8943 (5.1481)
2022-11-09 17:44:51,252:INFO: Dataset: univ                Batch:  7/15	Loss 4.1613 (4.9987)
2022-11-09 17:44:51,277:INFO: Dataset: univ                Batch:  8/15	Loss 6.1484 (5.1378)
2022-11-09 17:44:51,303:INFO: Dataset: univ                Batch:  9/15	Loss 4.8670 (5.1084)
2022-11-09 17:44:51,331:INFO: Dataset: univ                Batch: 10/15	Loss 3.9695 (4.9799)
2022-11-09 17:44:51,360:INFO: Dataset: univ                Batch: 11/15	Loss 4.1939 (4.9049)
2022-11-09 17:44:51,386:INFO: Dataset: univ                Batch: 12/15	Loss 4.9170 (4.9060)
2022-11-09 17:44:51,411:INFO: Dataset: univ                Batch: 13/15	Loss 4.0708 (4.8317)
2022-11-09 17:44:51,436:INFO: Dataset: univ                Batch: 14/15	Loss 5.3966 (4.8680)
2022-11-09 17:44:51,446:INFO: Dataset: univ                Batch: 15/15	Loss 0.8273 (4.8154)
2022-11-09 17:44:51,704:INFO: Dataset: zara1               Batch: 1/8	Loss 8.8646 (8.8646)
2022-11-09 17:44:51,729:INFO: Dataset: zara1               Batch: 2/8	Loss 11.2656 (10.0280)
2022-11-09 17:44:51,754:INFO: Dataset: zara1               Batch: 3/8	Loss 13.9080 (11.1846)
2022-11-09 17:44:51,779:INFO: Dataset: zara1               Batch: 4/8	Loss 9.7324 (10.8009)
2022-11-09 17:44:51,802:INFO: Dataset: zara1               Batch: 5/8	Loss 14.0891 (11.4078)
2022-11-09 17:44:51,826:INFO: Dataset: zara1               Batch: 6/8	Loss 8.4313 (10.9510)
2022-11-09 17:44:51,849:INFO: Dataset: zara1               Batch: 7/8	Loss 9.0782 (10.6912)
2022-11-09 17:44:51,870:INFO: Dataset: zara1               Batch: 8/8	Loss 8.2239 (10.4146)
2022-11-09 17:44:52,127:INFO: Dataset: zara2               Batch:  1/18	Loss 6.2173 (6.2173)
2022-11-09 17:44:52,151:INFO: Dataset: zara2               Batch:  2/18	Loss 6.2652 (6.2409)
2022-11-09 17:44:52,175:INFO: Dataset: zara2               Batch:  3/18	Loss 6.1000 (6.1930)
2022-11-09 17:44:52,201:INFO: Dataset: zara2               Batch:  4/18	Loss 6.3447 (6.2311)
2022-11-09 17:44:52,227:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1109 (6.4142)
2022-11-09 17:44:52,254:INFO: Dataset: zara2               Batch:  6/18	Loss 7.5627 (6.6162)
2022-11-09 17:44:52,278:INFO: Dataset: zara2               Batch:  7/18	Loss 5.9129 (6.5202)
2022-11-09 17:44:52,301:INFO: Dataset: zara2               Batch:  8/18	Loss 6.4100 (6.5060)
2022-11-09 17:44:52,325:INFO: Dataset: zara2               Batch:  9/18	Loss 6.5237 (6.5079)
2022-11-09 17:44:52,349:INFO: Dataset: zara2               Batch: 10/18	Loss 6.6031 (6.5166)
2022-11-09 17:44:52,373:INFO: Dataset: zara2               Batch: 11/18	Loss 6.3986 (6.5048)
2022-11-09 17:44:52,400:INFO: Dataset: zara2               Batch: 12/18	Loss 6.0604 (6.4707)
2022-11-09 17:44:52,425:INFO: Dataset: zara2               Batch: 13/18	Loss 6.0536 (6.4418)
2022-11-09 17:44:52,448:INFO: Dataset: zara2               Batch: 14/18	Loss 6.5139 (6.4466)
2022-11-09 17:44:52,473:INFO: Dataset: zara2               Batch: 15/18	Loss 7.2719 (6.5006)
2022-11-09 17:44:52,498:INFO: Dataset: zara2               Batch: 16/18	Loss 5.8539 (6.4574)
2022-11-09 17:44:52,522:INFO: Dataset: zara2               Batch: 17/18	Loss 5.4849 (6.3981)
2022-11-09 17:44:52,544:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1604 (6.3345)
2022-11-09 17:44:52,593:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_39.pth.tar
2022-11-09 17:44:52,593:INFO: 
===> EPOCH: 40 (P1)
2022-11-09 17:44:52,594:INFO: - Computing loss (training)
2022-11-09 17:44:52,805:INFO: Dataset: hotel               Batch: 1/4	Loss 3.3259 (3.3259)
2022-11-09 17:44:52,830:INFO: Dataset: hotel               Batch: 2/4	Loss 2.9840 (3.1494)
2022-11-09 17:44:52,854:INFO: Dataset: hotel               Batch: 3/4	Loss 7.4538 (4.5434)
2022-11-09 17:44:52,871:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8138 (4.0933)
2022-11-09 17:44:53,114:INFO: Dataset: univ                Batch:  1/15	Loss 4.6667 (4.6667)
2022-11-09 17:44:53,153:INFO: Dataset: univ                Batch:  2/15	Loss 4.3522 (4.5018)
2022-11-09 17:44:53,179:INFO: Dataset: univ                Batch:  3/15	Loss 4.3876 (4.4629)
2022-11-09 17:44:53,204:INFO: Dataset: univ                Batch:  4/15	Loss 4.5875 (4.4944)
2022-11-09 17:44:53,229:INFO: Dataset: univ                Batch:  5/15	Loss 4.8894 (4.5671)
2022-11-09 17:44:53,257:INFO: Dataset: univ                Batch:  6/15	Loss 4.1272 (4.4900)
2022-11-09 17:44:53,283:INFO: Dataset: univ                Batch:  7/15	Loss 5.0029 (4.5597)
2022-11-09 17:44:53,308:INFO: Dataset: univ                Batch:  8/15	Loss 5.1875 (4.6359)
2022-11-09 17:44:53,334:INFO: Dataset: univ                Batch:  9/15	Loss 4.3612 (4.6023)
2022-11-09 17:44:53,361:INFO: Dataset: univ                Batch: 10/15	Loss 3.9185 (4.5327)
2022-11-09 17:44:53,388:INFO: Dataset: univ                Batch: 11/15	Loss 4.4712 (4.5273)
2022-11-09 17:44:53,414:INFO: Dataset: univ                Batch: 12/15	Loss 4.2672 (4.5062)
2022-11-09 17:44:53,440:INFO: Dataset: univ                Batch: 13/15	Loss 4.1540 (4.4763)
2022-11-09 17:44:53,465:INFO: Dataset: univ                Batch: 14/15	Loss 4.3865 (4.4696)
2022-11-09 17:44:53,475:INFO: Dataset: univ                Batch: 15/15	Loss 0.6848 (4.4108)
2022-11-09 17:44:53,738:INFO: Dataset: zara1               Batch: 1/8	Loss 9.2163 (9.2163)
2022-11-09 17:44:53,766:INFO: Dataset: zara1               Batch: 2/8	Loss 8.8821 (9.0755)
2022-11-09 17:44:53,789:INFO: Dataset: zara1               Batch: 3/8	Loss 10.5775 (9.5267)
2022-11-09 17:44:53,813:INFO: Dataset: zara1               Batch: 4/8	Loss 12.3264 (10.2348)
2022-11-09 17:44:53,836:INFO: Dataset: zara1               Batch: 5/8	Loss 8.3084 (9.8069)
2022-11-09 17:44:53,861:INFO: Dataset: zara1               Batch: 6/8	Loss 8.9876 (9.6774)
2022-11-09 17:44:53,884:INFO: Dataset: zara1               Batch: 7/8	Loss 9.1899 (9.6081)
2022-11-09 17:44:53,905:INFO: Dataset: zara1               Batch: 8/8	Loss 7.4694 (9.3706)
2022-11-09 17:44:54,158:INFO: Dataset: zara2               Batch:  1/18	Loss 6.0996 (6.0996)
2022-11-09 17:44:54,182:INFO: Dataset: zara2               Batch:  2/18	Loss 6.4485 (6.2755)
2022-11-09 17:44:54,206:INFO: Dataset: zara2               Batch:  3/18	Loss 6.5308 (6.3707)
2022-11-09 17:44:54,232:INFO: Dataset: zara2               Batch:  4/18	Loss 8.0362 (6.8094)
2022-11-09 17:44:54,257:INFO: Dataset: zara2               Batch:  5/18	Loss 5.9428 (6.6361)
2022-11-09 17:44:54,284:INFO: Dataset: zara2               Batch:  6/18	Loss 6.6573 (6.6399)
2022-11-09 17:44:54,307:INFO: Dataset: zara2               Batch:  7/18	Loss 6.0648 (6.5533)
2022-11-09 17:44:54,331:INFO: Dataset: zara2               Batch:  8/18	Loss 5.8625 (6.4816)
2022-11-09 17:44:54,354:INFO: Dataset: zara2               Batch:  9/18	Loss 6.1677 (6.4450)
2022-11-09 17:44:54,378:INFO: Dataset: zara2               Batch: 10/18	Loss 5.8955 (6.3916)
2022-11-09 17:44:54,403:INFO: Dataset: zara2               Batch: 11/18	Loss 6.1789 (6.3716)
2022-11-09 17:44:54,429:INFO: Dataset: zara2               Batch: 12/18	Loss 6.3184 (6.3668)
2022-11-09 17:44:54,453:INFO: Dataset: zara2               Batch: 13/18	Loss 5.3200 (6.2843)
2022-11-09 17:44:54,479:INFO: Dataset: zara2               Batch: 14/18	Loss 5.5222 (6.2359)
2022-11-09 17:44:54,503:INFO: Dataset: zara2               Batch: 15/18	Loss 5.4986 (6.1870)
2022-11-09 17:44:54,528:INFO: Dataset: zara2               Batch: 16/18	Loss 6.2391 (6.1900)
2022-11-09 17:44:54,553:INFO: Dataset: zara2               Batch: 17/18	Loss 5.5931 (6.1515)
2022-11-09 17:44:54,575:INFO: Dataset: zara2               Batch: 18/18	Loss 4.9887 (6.0921)
2022-11-09 17:44:54,625:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_40.pth.tar
2022-11-09 17:44:54,625:INFO: 
===> EPOCH: 41 (P1)
2022-11-09 17:44:54,625:INFO: - Computing loss (training)
2022-11-09 17:44:54,838:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0740 (3.0740)
2022-11-09 17:44:54,865:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4314 (3.7353)
2022-11-09 17:44:54,890:INFO: Dataset: hotel               Batch: 3/4	Loss 3.0221 (3.5086)
2022-11-09 17:44:54,906:INFO: Dataset: hotel               Batch: 4/4	Loss 2.4205 (3.3234)
2022-11-09 17:44:55,158:INFO: Dataset: univ                Batch:  1/15	Loss 6.5036 (6.5036)
2022-11-09 17:44:55,188:INFO: Dataset: univ                Batch:  2/15	Loss 4.0989 (5.2035)
2022-11-09 17:44:55,215:INFO: Dataset: univ                Batch:  3/15	Loss 4.3791 (4.9293)
2022-11-09 17:44:55,245:INFO: Dataset: univ                Batch:  4/15	Loss 4.4466 (4.7988)
2022-11-09 17:44:55,270:INFO: Dataset: univ                Batch:  5/15	Loss 4.1324 (4.6654)
2022-11-09 17:44:55,298:INFO: Dataset: univ                Batch:  6/15	Loss 4.4518 (4.6316)
2022-11-09 17:44:55,323:INFO: Dataset: univ                Batch:  7/15	Loss 3.9949 (4.5357)
2022-11-09 17:44:55,348:INFO: Dataset: univ                Batch:  8/15	Loss 4.7345 (4.5604)
2022-11-09 17:44:55,374:INFO: Dataset: univ                Batch:  9/15	Loss 3.9842 (4.4904)
2022-11-09 17:44:55,400:INFO: Dataset: univ                Batch: 10/15	Loss 4.2465 (4.4642)
2022-11-09 17:44:55,428:INFO: Dataset: univ                Batch: 11/15	Loss 3.8609 (4.4067)
2022-11-09 17:44:55,455:INFO: Dataset: univ                Batch: 12/15	Loss 4.7050 (4.4298)
2022-11-09 17:44:55,480:INFO: Dataset: univ                Batch: 13/15	Loss 4.1674 (4.4108)
2022-11-09 17:44:55,505:INFO: Dataset: univ                Batch: 14/15	Loss 4.6098 (4.4254)
2022-11-09 17:44:55,515:INFO: Dataset: univ                Batch: 15/15	Loss 0.9588 (4.3764)
2022-11-09 17:44:55,777:INFO: Dataset: zara1               Batch: 1/8	Loss 10.2815 (10.2815)
2022-11-09 17:44:55,803:INFO: Dataset: zara1               Batch: 2/8	Loss 12.6188 (11.4252)
2022-11-09 17:44:55,827:INFO: Dataset: zara1               Batch: 3/8	Loss 11.1958 (11.3360)
2022-11-09 17:44:55,850:INFO: Dataset: zara1               Batch: 4/8	Loss 9.7127 (10.8975)
2022-11-09 17:44:55,874:INFO: Dataset: zara1               Batch: 5/8	Loss 10.7142 (10.8572)
2022-11-09 17:44:55,900:INFO: Dataset: zara1               Batch: 6/8	Loss 7.1876 (10.2732)
2022-11-09 17:44:55,923:INFO: Dataset: zara1               Batch: 7/8	Loss 8.8266 (10.0540)
2022-11-09 17:44:55,944:INFO: Dataset: zara1               Batch: 8/8	Loss 6.5292 (9.6756)
2022-11-09 17:44:56,203:INFO: Dataset: zara2               Batch:  1/18	Loss 6.1414 (6.1414)
2022-11-09 17:44:56,228:INFO: Dataset: zara2               Batch:  2/18	Loss 6.5675 (6.3428)
2022-11-09 17:44:56,257:INFO: Dataset: zara2               Batch:  3/18	Loss 8.3396 (7.0117)
2022-11-09 17:44:56,281:INFO: Dataset: zara2               Batch:  4/18	Loss 7.5141 (7.1344)
2022-11-09 17:44:56,305:INFO: Dataset: zara2               Batch:  5/18	Loss 8.2916 (7.3671)
2022-11-09 17:44:56,331:INFO: Dataset: zara2               Batch:  6/18	Loss 6.2404 (7.1747)
2022-11-09 17:44:56,355:INFO: Dataset: zara2               Batch:  7/18	Loss 6.5688 (7.0868)
2022-11-09 17:44:56,379:INFO: Dataset: zara2               Batch:  8/18	Loss 6.2843 (6.9890)
2022-11-09 17:44:56,402:INFO: Dataset: zara2               Batch:  9/18	Loss 5.9017 (6.8627)
2022-11-09 17:44:56,428:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1784 (6.7973)
2022-11-09 17:44:56,455:INFO: Dataset: zara2               Batch: 11/18	Loss 5.9268 (6.7241)
2022-11-09 17:44:56,482:INFO: Dataset: zara2               Batch: 12/18	Loss 5.3752 (6.6133)
2022-11-09 17:44:56,506:INFO: Dataset: zara2               Batch: 13/18	Loss 5.3416 (6.5200)
2022-11-09 17:44:56,530:INFO: Dataset: zara2               Batch: 14/18	Loss 5.6116 (6.4531)
2022-11-09 17:44:56,555:INFO: Dataset: zara2               Batch: 15/18	Loss 7.6069 (6.5338)
2022-11-09 17:44:56,579:INFO: Dataset: zara2               Batch: 16/18	Loss 6.3144 (6.5205)
2022-11-09 17:44:56,603:INFO: Dataset: zara2               Batch: 17/18	Loss 5.1317 (6.4367)
2022-11-09 17:44:56,625:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6020 (6.3309)
2022-11-09 17:44:56,676:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_41.pth.tar
2022-11-09 17:44:56,676:INFO: 
===> EPOCH: 42 (P1)
2022-11-09 17:44:56,676:INFO: - Computing loss (training)
2022-11-09 17:44:56,888:INFO: Dataset: hotel               Batch: 1/4	Loss 2.5444 (2.5444)
2022-11-09 17:44:56,915:INFO: Dataset: hotel               Batch: 2/4	Loss 4.3073 (3.4579)
2022-11-09 17:44:56,940:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7956 (3.2284)
2022-11-09 17:44:56,956:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1624 (3.0512)
2022-11-09 17:44:57,215:INFO: Dataset: univ                Batch:  1/15	Loss 4.4352 (4.4352)
2022-11-09 17:44:57,242:INFO: Dataset: univ                Batch:  2/15	Loss 4.2840 (4.3557)
2022-11-09 17:44:57,269:INFO: Dataset: univ                Batch:  3/15	Loss 5.1736 (4.6165)
2022-11-09 17:44:57,295:INFO: Dataset: univ                Batch:  4/15	Loss 4.1272 (4.4872)
2022-11-09 17:44:57,323:INFO: Dataset: univ                Batch:  5/15	Loss 4.0001 (4.3799)
2022-11-09 17:44:57,350:INFO: Dataset: univ                Batch:  6/15	Loss 3.9739 (4.3098)
2022-11-09 17:44:57,375:INFO: Dataset: univ                Batch:  7/15	Loss 4.4657 (4.3317)
2022-11-09 17:44:57,400:INFO: Dataset: univ                Batch:  8/15	Loss 4.5318 (4.3583)
2022-11-09 17:44:57,426:INFO: Dataset: univ                Batch:  9/15	Loss 3.8860 (4.3050)
2022-11-09 17:44:57,452:INFO: Dataset: univ                Batch: 10/15	Loss 5.5446 (4.4261)
2022-11-09 17:44:57,481:INFO: Dataset: univ                Batch: 11/15	Loss 3.8751 (4.3751)
2022-11-09 17:44:57,509:INFO: Dataset: univ                Batch: 12/15	Loss 4.7400 (4.4050)
2022-11-09 17:44:57,535:INFO: Dataset: univ                Batch: 13/15	Loss 4.3355 (4.3995)
2022-11-09 17:44:57,560:INFO: Dataset: univ                Batch: 14/15	Loss 4.2921 (4.3917)
2022-11-09 17:44:57,569:INFO: Dataset: univ                Batch: 15/15	Loss 0.7173 (4.3440)
2022-11-09 17:44:57,832:INFO: Dataset: zara1               Batch: 1/8	Loss 14.5232 (14.5232)
2022-11-09 17:44:57,859:INFO: Dataset: zara1               Batch: 2/8	Loss 7.5505 (11.2071)
2022-11-09 17:44:57,883:INFO: Dataset: zara1               Batch: 3/8	Loss 10.9594 (11.1282)
2022-11-09 17:44:57,906:INFO: Dataset: zara1               Batch: 4/8	Loss 8.3932 (10.4016)
2022-11-09 17:44:57,930:INFO: Dataset: zara1               Batch: 5/8	Loss 10.7579 (10.4819)
2022-11-09 17:44:57,955:INFO: Dataset: zara1               Batch: 6/8	Loss 6.6685 (9.8464)
2022-11-09 17:44:57,979:INFO: Dataset: zara1               Batch: 7/8	Loss 9.2429 (9.7620)
2022-11-09 17:44:58,000:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6066 (9.5294)
2022-11-09 17:44:58,251:INFO: Dataset: zara2               Batch:  1/18	Loss 6.6677 (6.6677)
2022-11-09 17:44:58,303:INFO: Dataset: zara2               Batch:  2/18	Loss 6.1466 (6.4052)
2022-11-09 17:44:58,327:INFO: Dataset: zara2               Batch:  3/18	Loss 6.8219 (6.5516)
2022-11-09 17:44:58,350:INFO: Dataset: zara2               Batch:  4/18	Loss 7.7596 (6.8436)
2022-11-09 17:44:58,374:INFO: Dataset: zara2               Batch:  5/18	Loss 6.5041 (6.7716)
2022-11-09 17:44:58,400:INFO: Dataset: zara2               Batch:  6/18	Loss 7.0043 (6.8088)
2022-11-09 17:44:58,424:INFO: Dataset: zara2               Batch:  7/18	Loss 6.9205 (6.8263)
2022-11-09 17:44:58,447:INFO: Dataset: zara2               Batch:  8/18	Loss 6.0062 (6.7201)
2022-11-09 17:44:58,472:INFO: Dataset: zara2               Batch:  9/18	Loss 5.4195 (6.5681)
2022-11-09 17:44:58,498:INFO: Dataset: zara2               Batch: 10/18	Loss 5.4830 (6.4704)
2022-11-09 17:44:58,523:INFO: Dataset: zara2               Batch: 11/18	Loss 5.5282 (6.3896)
2022-11-09 17:44:58,548:INFO: Dataset: zara2               Batch: 12/18	Loss 5.2005 (6.2842)
2022-11-09 17:44:58,572:INFO: Dataset: zara2               Batch: 13/18	Loss 5.4116 (6.2175)
2022-11-09 17:44:58,596:INFO: Dataset: zara2               Batch: 14/18	Loss 4.9436 (6.1263)
2022-11-09 17:44:58,621:INFO: Dataset: zara2               Batch: 15/18	Loss 6.0490 (6.1216)
2022-11-09 17:44:58,645:INFO: Dataset: zara2               Batch: 16/18	Loss 5.4484 (6.0769)
2022-11-09 17:44:58,670:INFO: Dataset: zara2               Batch: 17/18	Loss 6.3736 (6.0936)
2022-11-09 17:44:58,693:INFO: Dataset: zara2               Batch: 18/18	Loss 6.1140 (6.0945)
2022-11-09 17:44:58,743:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_42.pth.tar
2022-11-09 17:44:58,743:INFO: 
===> EPOCH: 43 (P1)
2022-11-09 17:44:58,744:INFO: - Computing loss (training)
2022-11-09 17:44:58,947:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0830 (3.0830)
2022-11-09 17:44:58,973:INFO: Dataset: hotel               Batch: 2/4	Loss 2.9810 (3.0329)
2022-11-09 17:44:58,998:INFO: Dataset: hotel               Batch: 3/4	Loss 3.0053 (3.0237)
2022-11-09 17:44:59,014:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9578 (2.8507)
2022-11-09 17:44:59,275:INFO: Dataset: univ                Batch:  1/15	Loss 3.9896 (3.9896)
2022-11-09 17:44:59,362:INFO: Dataset: univ                Batch:  2/15	Loss 4.0185 (4.0046)
2022-11-09 17:44:59,390:INFO: Dataset: univ                Batch:  3/15	Loss 4.1118 (4.0414)
2022-11-09 17:44:59,414:INFO: Dataset: univ                Batch:  4/15	Loss 4.0663 (4.0476)
2022-11-09 17:44:59,439:INFO: Dataset: univ                Batch:  5/15	Loss 3.6617 (3.9662)
2022-11-09 17:44:59,467:INFO: Dataset: univ                Batch:  6/15	Loss 4.3339 (4.0286)
2022-11-09 17:44:59,493:INFO: Dataset: univ                Batch:  7/15	Loss 4.2102 (4.0537)
2022-11-09 17:44:59,519:INFO: Dataset: univ                Batch:  8/15	Loss 4.2473 (4.0770)
2022-11-09 17:44:59,545:INFO: Dataset: univ                Batch:  9/15	Loss 3.8756 (4.0543)
2022-11-09 17:44:59,570:INFO: Dataset: univ                Batch: 10/15	Loss 3.5170 (4.0011)
2022-11-09 17:44:59,597:INFO: Dataset: univ                Batch: 11/15	Loss 4.4924 (4.0425)
2022-11-09 17:44:59,622:INFO: Dataset: univ                Batch: 12/15	Loss 4.7705 (4.0980)
2022-11-09 17:44:59,647:INFO: Dataset: univ                Batch: 13/15	Loss 5.6463 (4.1933)
2022-11-09 17:44:59,673:INFO: Dataset: univ                Batch: 14/15	Loss 3.9228 (4.1727)
2022-11-09 17:44:59,683:INFO: Dataset: univ                Batch: 15/15	Loss 0.7694 (4.1271)
2022-11-09 17:44:59,943:INFO: Dataset: zara1               Batch: 1/8	Loss 9.4526 (9.4526)
2022-11-09 17:44:59,971:INFO: Dataset: zara1               Batch: 2/8	Loss 9.0731 (9.2781)
2022-11-09 17:44:59,995:INFO: Dataset: zara1               Batch: 3/8	Loss 13.5869 (10.7534)
2022-11-09 17:45:00,019:INFO: Dataset: zara1               Batch: 4/8	Loss 11.4119 (10.8981)
2022-11-09 17:45:00,042:INFO: Dataset: zara1               Batch: 5/8	Loss 7.8234 (10.3255)
2022-11-09 17:45:00,068:INFO: Dataset: zara1               Batch: 6/8	Loss 8.2969 (9.9895)
2022-11-09 17:45:00,091:INFO: Dataset: zara1               Batch: 7/8	Loss 7.8149 (9.6816)
2022-11-09 17:45:00,112:INFO: Dataset: zara1               Batch: 8/8	Loss 6.7019 (9.3711)
2022-11-09 17:45:00,365:INFO: Dataset: zara2               Batch:  1/18	Loss 7.0407 (7.0407)
2022-11-09 17:45:00,402:INFO: Dataset: zara2               Batch:  2/18	Loss 9.4261 (8.2386)
2022-11-09 17:45:00,426:INFO: Dataset: zara2               Batch:  3/18	Loss 7.4162 (7.9621)
2022-11-09 17:45:00,452:INFO: Dataset: zara2               Batch:  4/18	Loss 8.8855 (8.1782)
2022-11-09 17:45:00,475:INFO: Dataset: zara2               Batch:  5/18	Loss 7.0767 (7.9532)
2022-11-09 17:45:00,502:INFO: Dataset: zara2               Batch:  6/18	Loss 6.7813 (7.7600)
2022-11-09 17:45:00,526:INFO: Dataset: zara2               Batch:  7/18	Loss 5.1480 (7.3990)
2022-11-09 17:45:00,551:INFO: Dataset: zara2               Batch:  8/18	Loss 5.4370 (7.1709)
2022-11-09 17:45:00,576:INFO: Dataset: zara2               Batch:  9/18	Loss 5.5395 (6.9909)
2022-11-09 17:45:00,600:INFO: Dataset: zara2               Batch: 10/18	Loss 5.4167 (6.8282)
2022-11-09 17:45:00,625:INFO: Dataset: zara2               Batch: 11/18	Loss 6.8382 (6.8289)
2022-11-09 17:45:00,650:INFO: Dataset: zara2               Batch: 12/18	Loss 5.9994 (6.7536)
2022-11-09 17:45:00,673:INFO: Dataset: zara2               Batch: 13/18	Loss 5.3243 (6.6407)
2022-11-09 17:45:00,697:INFO: Dataset: zara2               Batch: 14/18	Loss 4.9444 (6.5134)
2022-11-09 17:45:00,722:INFO: Dataset: zara2               Batch: 15/18	Loss 5.3176 (6.4375)
2022-11-09 17:45:00,746:INFO: Dataset: zara2               Batch: 16/18	Loss 4.9508 (6.3394)
2022-11-09 17:45:00,770:INFO: Dataset: zara2               Batch: 17/18	Loss 5.9714 (6.3165)
2022-11-09 17:45:00,793:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6667 (6.2355)
2022-11-09 17:45:00,843:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_43.pth.tar
2022-11-09 17:45:00,843:INFO: 
===> EPOCH: 44 (P1)
2022-11-09 17:45:00,844:INFO: - Computing loss (training)
2022-11-09 17:45:01,050:INFO: Dataset: hotel               Batch: 1/4	Loss 3.7301 (3.7301)
2022-11-09 17:45:01,077:INFO: Dataset: hotel               Batch: 2/4	Loss 3.3052 (3.5108)
2022-11-09 17:45:01,101:INFO: Dataset: hotel               Batch: 3/4	Loss 2.8762 (3.3100)
2022-11-09 17:45:01,117:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8152 (3.0556)
2022-11-09 17:45:01,375:INFO: Dataset: univ                Batch:  1/15	Loss 4.0332 (4.0332)
2022-11-09 17:45:01,403:INFO: Dataset: univ                Batch:  2/15	Loss 4.0950 (4.0631)
2022-11-09 17:45:01,429:INFO: Dataset: univ                Batch:  3/15	Loss 4.3912 (4.1708)
2022-11-09 17:45:01,454:INFO: Dataset: univ                Batch:  4/15	Loss 4.3541 (4.2149)
2022-11-09 17:45:01,482:INFO: Dataset: univ                Batch:  5/15	Loss 4.1666 (4.2056)
2022-11-09 17:45:01,509:INFO: Dataset: univ                Batch:  6/15	Loss 4.5779 (4.2692)
2022-11-09 17:45:01,534:INFO: Dataset: univ                Batch:  7/15	Loss 4.2952 (4.2728)
2022-11-09 17:45:01,559:INFO: Dataset: univ                Batch:  8/15	Loss 4.2949 (4.2758)
2022-11-09 17:45:01,584:INFO: Dataset: univ                Batch:  9/15	Loss 3.7836 (4.2194)
2022-11-09 17:45:01,610:INFO: Dataset: univ                Batch: 10/15	Loss 4.8102 (4.2730)
2022-11-09 17:45:01,639:INFO: Dataset: univ                Batch: 11/15	Loss 4.9475 (4.3323)
2022-11-09 17:45:01,665:INFO: Dataset: univ                Batch: 12/15	Loss 4.3724 (4.3355)
2022-11-09 17:45:01,690:INFO: Dataset: univ                Batch: 13/15	Loss 4.9309 (4.3799)
2022-11-09 17:45:01,715:INFO: Dataset: univ                Batch: 14/15	Loss 3.9032 (4.3469)
2022-11-09 17:45:01,725:INFO: Dataset: univ                Batch: 15/15	Loss 0.8178 (4.2925)
2022-11-09 17:45:01,988:INFO: Dataset: zara1               Batch: 1/8	Loss 13.0123 (13.0123)
2022-11-09 17:45:02,013:INFO: Dataset: zara1               Batch: 2/8	Loss 15.2379 (14.1633)
2022-11-09 17:45:02,037:INFO: Dataset: zara1               Batch: 3/8	Loss 14.7370 (14.3490)
2022-11-09 17:45:02,062:INFO: Dataset: zara1               Batch: 4/8	Loss 9.2205 (13.0447)
2022-11-09 17:45:02,087:INFO: Dataset: zara1               Batch: 5/8	Loss 7.2273 (11.7405)
2022-11-09 17:45:02,111:INFO: Dataset: zara1               Batch: 6/8	Loss 8.5718 (11.2091)
2022-11-09 17:45:02,134:INFO: Dataset: zara1               Batch: 7/8	Loss 7.7701 (10.7099)
2022-11-09 17:45:02,155:INFO: Dataset: zara1               Batch: 8/8	Loss 5.9011 (10.1379)
2022-11-09 17:45:02,406:INFO: Dataset: zara2               Batch:  1/18	Loss 5.7833 (5.7833)
2022-11-09 17:45:02,435:INFO: Dataset: zara2               Batch:  2/18	Loss 7.3268 (6.5698)
2022-11-09 17:45:02,459:INFO: Dataset: zara2               Batch:  3/18	Loss 6.1322 (6.4224)
2022-11-09 17:45:02,483:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0541 (6.5802)
2022-11-09 17:45:02,509:INFO: Dataset: zara2               Batch:  5/18	Loss 7.0429 (6.6790)
2022-11-09 17:45:02,535:INFO: Dataset: zara2               Batch:  6/18	Loss 6.4802 (6.6481)
2022-11-09 17:45:02,558:INFO: Dataset: zara2               Batch:  7/18	Loss 6.5561 (6.6348)
2022-11-09 17:45:02,581:INFO: Dataset: zara2               Batch:  8/18	Loss 5.9759 (6.5614)
2022-11-09 17:45:02,605:INFO: Dataset: zara2               Batch:  9/18	Loss 7.5564 (6.6620)
2022-11-09 17:45:02,629:INFO: Dataset: zara2               Batch: 10/18	Loss 5.8480 (6.5750)
2022-11-09 17:45:02,656:INFO: Dataset: zara2               Batch: 11/18	Loss 6.1895 (6.5394)
2022-11-09 17:45:02,681:INFO: Dataset: zara2               Batch: 12/18	Loss 6.0166 (6.4964)
2022-11-09 17:45:02,706:INFO: Dataset: zara2               Batch: 13/18	Loss 5.0326 (6.3749)
2022-11-09 17:45:02,730:INFO: Dataset: zara2               Batch: 14/18	Loss 4.9724 (6.2781)
2022-11-09 17:45:02,754:INFO: Dataset: zara2               Batch: 15/18	Loss 5.8352 (6.2505)
2022-11-09 17:45:02,779:INFO: Dataset: zara2               Batch: 16/18	Loss 5.6543 (6.2111)
2022-11-09 17:45:02,803:INFO: Dataset: zara2               Batch: 17/18	Loss 4.7680 (6.1272)
2022-11-09 17:45:02,825:INFO: Dataset: zara2               Batch: 18/18	Loss 4.7769 (6.0631)
2022-11-09 17:45:02,874:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_44.pth.tar
2022-11-09 17:45:02,874:INFO: 
===> EPOCH: 45 (P1)
2022-11-09 17:45:02,875:INFO: - Computing loss (training)
2022-11-09 17:45:03,082:INFO: Dataset: hotel               Batch: 1/4	Loss 2.5633 (2.5633)
2022-11-09 17:45:03,109:INFO: Dataset: hotel               Batch: 2/4	Loss 2.6550 (2.6083)
2022-11-09 17:45:03,134:INFO: Dataset: hotel               Batch: 3/4	Loss 2.8615 (2.6930)
2022-11-09 17:45:03,150:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7969 (2.5357)
2022-11-09 17:45:03,406:INFO: Dataset: univ                Batch:  1/15	Loss 4.4839 (4.4839)
2022-11-09 17:45:03,454:INFO: Dataset: univ                Batch:  2/15	Loss 4.2729 (4.3705)
2022-11-09 17:45:03,480:INFO: Dataset: univ                Batch:  3/15	Loss 5.0206 (4.5898)
2022-11-09 17:45:03,505:INFO: Dataset: univ                Batch:  4/15	Loss 4.0752 (4.4502)
2022-11-09 17:45:03,531:INFO: Dataset: univ                Batch:  5/15	Loss 3.8753 (4.3447)
2022-11-09 17:45:03,559:INFO: Dataset: univ                Batch:  6/15	Loss 3.6687 (4.2319)
2022-11-09 17:45:03,584:INFO: Dataset: univ                Batch:  7/15	Loss 4.0059 (4.2037)
2022-11-09 17:45:03,610:INFO: Dataset: univ                Batch:  8/15	Loss 3.7422 (4.1435)
2022-11-09 17:45:03,637:INFO: Dataset: univ                Batch:  9/15	Loss 3.8186 (4.1068)
2022-11-09 17:45:03,663:INFO: Dataset: univ                Batch: 10/15	Loss 4.1028 (4.1064)
2022-11-09 17:45:03,691:INFO: Dataset: univ                Batch: 11/15	Loss 3.6554 (4.0622)
2022-11-09 17:45:03,717:INFO: Dataset: univ                Batch: 12/15	Loss 4.1306 (4.0677)
2022-11-09 17:45:03,743:INFO: Dataset: univ                Batch: 13/15	Loss 3.9902 (4.0609)
2022-11-09 17:45:03,770:INFO: Dataset: univ                Batch: 14/15	Loss 3.6247 (4.0250)
2022-11-09 17:45:03,780:INFO: Dataset: univ                Batch: 15/15	Loss 0.9209 (3.9948)
2022-11-09 17:45:04,043:INFO: Dataset: zara1               Batch: 1/8	Loss 11.7784 (11.7784)
2022-11-09 17:45:04,068:INFO: Dataset: zara1               Batch: 2/8	Loss 13.1339 (12.4680)
2022-11-09 17:45:04,093:INFO: Dataset: zara1               Batch: 3/8	Loss 10.9147 (12.0178)
2022-11-09 17:45:04,117:INFO: Dataset: zara1               Batch: 4/8	Loss 7.1749 (10.7325)
2022-11-09 17:45:04,143:INFO: Dataset: zara1               Batch: 5/8	Loss 8.7324 (10.3967)
2022-11-09 17:45:04,167:INFO: Dataset: zara1               Batch: 6/8	Loss 6.6825 (9.7041)
2022-11-09 17:45:04,190:INFO: Dataset: zara1               Batch: 7/8	Loss 7.2821 (9.3827)
2022-11-09 17:45:04,210:INFO: Dataset: zara1               Batch: 8/8	Loss 6.1409 (9.0210)
2022-11-09 17:45:04,517:INFO: Dataset: zara2               Batch:  1/18	Loss 11.1245 (11.1245)
2022-11-09 17:45:04,546:INFO: Dataset: zara2               Batch:  2/18	Loss 10.2184 (10.6979)
2022-11-09 17:45:04,570:INFO: Dataset: zara2               Batch:  3/18	Loss 6.9613 (9.4363)
2022-11-09 17:45:04,593:INFO: Dataset: zara2               Batch:  4/18	Loss 6.9116 (8.8075)
2022-11-09 17:45:04,617:INFO: Dataset: zara2               Batch:  5/18	Loss 5.2059 (8.0655)
2022-11-09 17:45:04,645:INFO: Dataset: zara2               Batch:  6/18	Loss 5.7205 (7.7027)
2022-11-09 17:45:04,668:INFO: Dataset: zara2               Batch:  7/18	Loss 5.5761 (7.4075)
2022-11-09 17:45:04,692:INFO: Dataset: zara2               Batch:  8/18	Loss 5.2362 (7.1322)
2022-11-09 17:45:04,715:INFO: Dataset: zara2               Batch:  9/18	Loss 6.2067 (7.0335)
2022-11-09 17:45:04,739:INFO: Dataset: zara2               Batch: 10/18	Loss 5.4229 (6.8613)
2022-11-09 17:45:04,764:INFO: Dataset: zara2               Batch: 11/18	Loss 5.9376 (6.7755)
2022-11-09 17:45:04,789:INFO: Dataset: zara2               Batch: 12/18	Loss 6.0748 (6.7161)
2022-11-09 17:45:04,814:INFO: Dataset: zara2               Batch: 13/18	Loss 5.5317 (6.6294)
2022-11-09 17:45:04,838:INFO: Dataset: zara2               Batch: 14/18	Loss 5.1201 (6.5144)
2022-11-09 17:45:04,862:INFO: Dataset: zara2               Batch: 15/18	Loss 4.9002 (6.4163)
2022-11-09 17:45:04,887:INFO: Dataset: zara2               Batch: 16/18	Loss 4.5588 (6.2994)
2022-11-09 17:45:04,911:INFO: Dataset: zara2               Batch: 17/18	Loss 6.9499 (6.3409)
2022-11-09 17:45:04,933:INFO: Dataset: zara2               Batch: 18/18	Loss 4.7815 (6.2517)
2022-11-09 17:45:04,983:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_45.pth.tar
2022-11-09 17:45:04,983:INFO: 
===> EPOCH: 46 (P1)
2022-11-09 17:45:04,983:INFO: - Computing loss (training)
2022-11-09 17:45:05,196:INFO: Dataset: hotel               Batch: 1/4	Loss 2.9496 (2.9496)
2022-11-09 17:45:05,222:INFO: Dataset: hotel               Batch: 2/4	Loss 2.7369 (2.8442)
2022-11-09 17:45:05,246:INFO: Dataset: hotel               Batch: 3/4	Loss 2.8733 (2.8538)
2022-11-09 17:45:05,262:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4983 (2.6266)
2022-11-09 17:45:05,524:INFO: Dataset: univ                Batch:  1/15	Loss 4.2013 (4.2013)
2022-11-09 17:45:05,553:INFO: Dataset: univ                Batch:  2/15	Loss 4.4956 (4.3370)
2022-11-09 17:45:05,581:INFO: Dataset: univ                Batch:  3/15	Loss 3.7664 (4.1476)
2022-11-09 17:45:05,608:INFO: Dataset: univ                Batch:  4/15	Loss 4.4601 (4.2223)
2022-11-09 17:45:05,636:INFO: Dataset: univ                Batch:  5/15	Loss 3.8802 (4.1567)
2022-11-09 17:45:05,663:INFO: Dataset: univ                Batch:  6/15	Loss 4.4482 (4.2073)
2022-11-09 17:45:05,688:INFO: Dataset: univ                Batch:  7/15	Loss 4.3427 (4.2267)
2022-11-09 17:45:05,714:INFO: Dataset: univ                Batch:  8/15	Loss 4.0665 (4.2046)
2022-11-09 17:45:05,740:INFO: Dataset: univ                Batch:  9/15	Loss 3.9444 (4.1771)
2022-11-09 17:45:05,768:INFO: Dataset: univ                Batch: 10/15	Loss 4.3340 (4.1934)
2022-11-09 17:45:05,797:INFO: Dataset: univ                Batch: 11/15	Loss 4.3518 (4.2078)
2022-11-09 17:45:05,823:INFO: Dataset: univ                Batch: 12/15	Loss 3.8594 (4.1791)
2022-11-09 17:45:05,848:INFO: Dataset: univ                Batch: 13/15	Loss 4.2622 (4.1849)
2022-11-09 17:45:05,873:INFO: Dataset: univ                Batch: 14/15	Loss 4.8336 (4.2258)
2022-11-09 17:45:05,883:INFO: Dataset: univ                Batch: 15/15	Loss 0.6948 (4.1855)
2022-11-09 17:45:06,143:INFO: Dataset: zara1               Batch: 1/8	Loss 8.4021 (8.4021)
2022-11-09 17:45:06,170:INFO: Dataset: zara1               Batch: 2/8	Loss 14.2910 (11.3225)
2022-11-09 17:45:06,194:INFO: Dataset: zara1               Batch: 3/8	Loss 10.4362 (11.0223)
2022-11-09 17:45:06,219:INFO: Dataset: zara1               Batch: 4/8	Loss 8.9895 (10.5095)
2022-11-09 17:45:06,243:INFO: Dataset: zara1               Batch: 5/8	Loss 9.7464 (10.3538)
2022-11-09 17:45:06,267:INFO: Dataset: zara1               Batch: 6/8	Loss 8.6576 (10.1030)
2022-11-09 17:45:06,291:INFO: Dataset: zara1               Batch: 7/8	Loss 7.4757 (9.7592)
2022-11-09 17:45:06,312:INFO: Dataset: zara1               Batch: 8/8	Loss 5.9812 (9.3237)
2022-11-09 17:45:06,557:INFO: Dataset: zara2               Batch:  1/18	Loss 6.3031 (6.3031)
2022-11-09 17:45:06,623:INFO: Dataset: zara2               Batch:  2/18	Loss 6.4427 (6.3712)
2022-11-09 17:45:06,647:INFO: Dataset: zara2               Batch:  3/18	Loss 7.5684 (6.7190)
2022-11-09 17:45:06,672:INFO: Dataset: zara2               Batch:  4/18	Loss 6.2950 (6.6035)
2022-11-09 17:45:06,695:INFO: Dataset: zara2               Batch:  5/18	Loss 7.3018 (6.7243)
2022-11-09 17:45:06,722:INFO: Dataset: zara2               Batch:  6/18	Loss 6.4283 (6.6761)
2022-11-09 17:45:06,745:INFO: Dataset: zara2               Batch:  7/18	Loss 6.3528 (6.6301)
2022-11-09 17:45:06,769:INFO: Dataset: zara2               Batch:  8/18	Loss 5.6675 (6.5004)
2022-11-09 17:45:06,793:INFO: Dataset: zara2               Batch:  9/18	Loss 6.7198 (6.5259)
2022-11-09 17:45:06,818:INFO: Dataset: zara2               Batch: 10/18	Loss 5.2836 (6.3989)
2022-11-09 17:45:06,844:INFO: Dataset: zara2               Batch: 11/18	Loss 5.0731 (6.2849)
2022-11-09 17:45:06,868:INFO: Dataset: zara2               Batch: 12/18	Loss 5.4808 (6.2103)
2022-11-09 17:45:06,891:INFO: Dataset: zara2               Batch: 13/18	Loss 5.5678 (6.1633)
2022-11-09 17:45:06,915:INFO: Dataset: zara2               Batch: 14/18	Loss 5.8470 (6.1422)
2022-11-09 17:45:06,940:INFO: Dataset: zara2               Batch: 15/18	Loss 4.9213 (6.0686)
2022-11-09 17:45:06,964:INFO: Dataset: zara2               Batch: 16/18	Loss 4.9043 (5.9942)
2022-11-09 17:45:06,989:INFO: Dataset: zara2               Batch: 17/18	Loss 4.8087 (5.9239)
2022-11-09 17:45:07,012:INFO: Dataset: zara2               Batch: 18/18	Loss 3.6540 (5.8117)
2022-11-09 17:45:07,064:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_46.pth.tar
2022-11-09 17:45:07,064:INFO: 
===> EPOCH: 47 (P1)
2022-11-09 17:45:07,065:INFO: - Computing loss (training)
2022-11-09 17:45:07,262:INFO: Dataset: hotel               Batch: 1/4	Loss 3.3282 (3.3282)
2022-11-09 17:45:07,286:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2752 (2.8041)
2022-11-09 17:45:07,310:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7349 (2.7816)
2022-11-09 17:45:07,327:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7398 (2.6236)
2022-11-09 17:45:07,581:INFO: Dataset: univ                Batch:  1/15	Loss 4.1520 (4.1520)
2022-11-09 17:45:07,609:INFO: Dataset: univ                Batch:  2/15	Loss 3.8449 (3.9864)
2022-11-09 17:45:07,637:INFO: Dataset: univ                Batch:  3/15	Loss 4.1284 (4.0316)
2022-11-09 17:45:07,665:INFO: Dataset: univ                Batch:  4/15	Loss 3.8616 (3.9893)
2022-11-09 17:45:07,692:INFO: Dataset: univ                Batch:  5/15	Loss 4.2748 (4.0477)
2022-11-09 17:45:07,722:INFO: Dataset: univ                Batch:  6/15	Loss 4.3900 (4.0976)
2022-11-09 17:45:07,747:INFO: Dataset: univ                Batch:  7/15	Loss 4.7186 (4.1815)
2022-11-09 17:45:07,774:INFO: Dataset: univ                Batch:  8/15	Loss 3.7732 (4.1290)
2022-11-09 17:45:07,800:INFO: Dataset: univ                Batch:  9/15	Loss 3.7421 (4.0814)
2022-11-09 17:45:07,827:INFO: Dataset: univ                Batch: 10/15	Loss 3.4664 (4.0124)
2022-11-09 17:45:07,854:INFO: Dataset: univ                Batch: 11/15	Loss 3.5804 (3.9703)
2022-11-09 17:45:07,882:INFO: Dataset: univ                Batch: 12/15	Loss 4.3279 (3.9980)
2022-11-09 17:45:07,908:INFO: Dataset: univ                Batch: 13/15	Loss 3.8820 (3.9896)
2022-11-09 17:45:07,934:INFO: Dataset: univ                Batch: 14/15	Loss 3.6960 (3.9693)
2022-11-09 17:45:07,944:INFO: Dataset: univ                Batch: 15/15	Loss 0.6038 (3.9260)
2022-11-09 17:45:08,204:INFO: Dataset: zara1               Batch: 1/8	Loss 7.9264 (7.9264)
2022-11-09 17:45:08,228:INFO: Dataset: zara1               Batch: 2/8	Loss 10.1107 (9.0491)
2022-11-09 17:45:08,253:INFO: Dataset: zara1               Batch: 3/8	Loss 10.4042 (9.4962)
2022-11-09 17:45:08,277:INFO: Dataset: zara1               Batch: 4/8	Loss 8.9919 (9.3603)
2022-11-09 17:45:08,300:INFO: Dataset: zara1               Batch: 5/8	Loss 11.0623 (9.7315)
2022-11-09 17:45:08,326:INFO: Dataset: zara1               Batch: 6/8	Loss 8.3837 (9.5468)
2022-11-09 17:45:08,349:INFO: Dataset: zara1               Batch: 7/8	Loss 6.8780 (9.1227)
2022-11-09 17:45:08,370:INFO: Dataset: zara1               Batch: 8/8	Loss 5.6972 (8.7152)
2022-11-09 17:45:08,634:INFO: Dataset: zara2               Batch:  1/18	Loss 7.1195 (7.1195)
2022-11-09 17:45:08,659:INFO: Dataset: zara2               Batch:  2/18	Loss 8.7797 (7.9569)
2022-11-09 17:45:08,682:INFO: Dataset: zara2               Batch:  3/18	Loss 7.7313 (7.8819)
2022-11-09 17:45:08,708:INFO: Dataset: zara2               Batch:  4/18	Loss 6.6354 (7.5721)
2022-11-09 17:45:08,733:INFO: Dataset: zara2               Batch:  5/18	Loss 6.2732 (7.3008)
2022-11-09 17:45:08,760:INFO: Dataset: zara2               Batch:  6/18	Loss 6.6122 (7.1925)
2022-11-09 17:45:08,783:INFO: Dataset: zara2               Batch:  7/18	Loss 6.2928 (7.0732)
2022-11-09 17:45:08,807:INFO: Dataset: zara2               Batch:  8/18	Loss 5.3812 (6.8436)
2022-11-09 17:45:08,830:INFO: Dataset: zara2               Batch:  9/18	Loss 4.7716 (6.5979)
2022-11-09 17:45:08,854:INFO: Dataset: zara2               Batch: 10/18	Loss 4.9496 (6.4374)
2022-11-09 17:45:08,880:INFO: Dataset: zara2               Batch: 11/18	Loss 5.2532 (6.3347)
2022-11-09 17:45:08,907:INFO: Dataset: zara2               Batch: 12/18	Loss 4.4484 (6.1620)
2022-11-09 17:45:08,931:INFO: Dataset: zara2               Batch: 13/18	Loss 5.2543 (6.0921)
2022-11-09 17:45:08,955:INFO: Dataset: zara2               Batch: 14/18	Loss 5.1632 (6.0248)
2022-11-09 17:45:08,979:INFO: Dataset: zara2               Batch: 15/18	Loss 6.4223 (6.0509)
2022-11-09 17:45:09,004:INFO: Dataset: zara2               Batch: 16/18	Loss 5.5326 (6.0187)
2022-11-09 17:45:09,029:INFO: Dataset: zara2               Batch: 17/18	Loss 4.3904 (5.9262)
2022-11-09 17:45:09,051:INFO: Dataset: zara2               Batch: 18/18	Loss 4.2083 (5.8529)
2022-11-09 17:45:09,100:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_47.pth.tar
2022-11-09 17:45:09,100:INFO: 
===> EPOCH: 48 (P1)
2022-11-09 17:45:09,100:INFO: - Computing loss (training)
2022-11-09 17:45:09,314:INFO: Dataset: hotel               Batch: 1/4	Loss 6.5957 (6.5957)
2022-11-09 17:45:09,340:INFO: Dataset: hotel               Batch: 2/4	Loss 3.1519 (4.8866)
2022-11-09 17:45:09,366:INFO: Dataset: hotel               Batch: 3/4	Loss 3.4144 (4.3775)
2022-11-09 17:45:09,382:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7288 (3.8813)
2022-11-09 17:45:09,634:INFO: Dataset: univ                Batch:  1/15	Loss 4.1285 (4.1285)
2022-11-09 17:45:09,661:INFO: Dataset: univ                Batch:  2/15	Loss 3.6130 (3.8657)
2022-11-09 17:45:09,688:INFO: Dataset: univ                Batch:  3/15	Loss 3.9694 (3.9008)
2022-11-09 17:45:09,716:INFO: Dataset: univ                Batch:  4/15	Loss 3.8502 (3.8882)
2022-11-09 17:45:09,741:INFO: Dataset: univ                Batch:  5/15	Loss 4.4692 (4.0000)
2022-11-09 17:45:09,772:INFO: Dataset: univ                Batch:  6/15	Loss 3.8427 (3.9742)
2022-11-09 17:45:09,797:INFO: Dataset: univ                Batch:  7/15	Loss 4.8793 (4.0907)
2022-11-09 17:45:09,822:INFO: Dataset: univ                Batch:  8/15	Loss 4.4388 (4.1303)
2022-11-09 17:45:09,849:INFO: Dataset: univ                Batch:  9/15	Loss 3.3751 (4.0384)
2022-11-09 17:45:09,876:INFO: Dataset: univ                Batch: 10/15	Loss 4.7583 (4.1096)
2022-11-09 17:45:09,903:INFO: Dataset: univ                Batch: 11/15	Loss 3.8872 (4.0890)
2022-11-09 17:45:09,930:INFO: Dataset: univ                Batch: 12/15	Loss 3.9048 (4.0727)
2022-11-09 17:45:09,955:INFO: Dataset: univ                Batch: 13/15	Loss 4.2715 (4.0858)
2022-11-09 17:45:09,980:INFO: Dataset: univ                Batch: 14/15	Loss 3.7139 (4.0595)
2022-11-09 17:45:09,989:INFO: Dataset: univ                Batch: 15/15	Loss 0.9666 (4.0291)
2022-11-09 17:45:10,240:INFO: Dataset: zara1               Batch: 1/8	Loss 9.9082 (9.9082)
2022-11-09 17:45:10,265:INFO: Dataset: zara1               Batch: 2/8	Loss 11.0874 (10.4775)
2022-11-09 17:45:10,290:INFO: Dataset: zara1               Batch: 3/8	Loss 9.5753 (10.1821)
2022-11-09 17:45:10,314:INFO: Dataset: zara1               Batch: 4/8	Loss 9.6661 (10.0601)
2022-11-09 17:45:10,340:INFO: Dataset: zara1               Batch: 5/8	Loss 8.1062 (9.6628)
2022-11-09 17:45:10,364:INFO: Dataset: zara1               Batch: 6/8	Loss 7.1269 (9.2199)
2022-11-09 17:45:10,387:INFO: Dataset: zara1               Batch: 7/8	Loss 6.8666 (8.9081)
2022-11-09 17:45:10,408:INFO: Dataset: zara1               Batch: 8/8	Loss 6.5862 (8.6429)
2022-11-09 17:45:10,666:INFO: Dataset: zara2               Batch:  1/18	Loss 6.5919 (6.5919)
2022-11-09 17:45:10,714:INFO: Dataset: zara2               Batch:  2/18	Loss 6.7806 (6.6815)
2022-11-09 17:45:10,738:INFO: Dataset: zara2               Batch:  3/18	Loss 6.4914 (6.6141)
2022-11-09 17:45:10,764:INFO: Dataset: zara2               Batch:  4/18	Loss 6.2502 (6.5279)
2022-11-09 17:45:10,787:INFO: Dataset: zara2               Batch:  5/18	Loss 8.6435 (6.9546)
2022-11-09 17:45:10,813:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8327 (6.9327)
2022-11-09 17:45:10,836:INFO: Dataset: zara2               Batch:  7/18	Loss 6.5340 (6.8768)
2022-11-09 17:45:10,859:INFO: Dataset: zara2               Batch:  8/18	Loss 6.2602 (6.7992)
2022-11-09 17:45:10,883:INFO: Dataset: zara2               Batch:  9/18	Loss 5.7807 (6.6914)
2022-11-09 17:45:10,908:INFO: Dataset: zara2               Batch: 10/18	Loss 5.9353 (6.6105)
2022-11-09 17:45:10,934:INFO: Dataset: zara2               Batch: 11/18	Loss 4.9408 (6.4544)
2022-11-09 17:45:10,959:INFO: Dataset: zara2               Batch: 12/18	Loss 5.0867 (6.3452)
2022-11-09 17:45:10,982:INFO: Dataset: zara2               Batch: 13/18	Loss 5.5848 (6.2883)
2022-11-09 17:45:11,007:INFO: Dataset: zara2               Batch: 14/18	Loss 5.1560 (6.2046)
2022-11-09 17:45:11,031:INFO: Dataset: zara2               Batch: 15/18	Loss 4.9156 (6.1218)
2022-11-09 17:45:11,055:INFO: Dataset: zara2               Batch: 16/18	Loss 5.3805 (6.0735)
2022-11-09 17:45:11,080:INFO: Dataset: zara2               Batch: 17/18	Loss 5.1694 (6.0175)
2022-11-09 17:45:11,102:INFO: Dataset: zara2               Batch: 18/18	Loss 3.8939 (5.9196)
2022-11-09 17:45:11,152:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_48.pth.tar
2022-11-09 17:45:11,152:INFO: 
===> EPOCH: 49 (P1)
2022-11-09 17:45:11,153:INFO: - Computing loss (training)
2022-11-09 17:45:11,361:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4992 (2.4992)
2022-11-09 17:45:11,385:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4003 (3.4930)
2022-11-09 17:45:11,410:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1318 (3.0386)
2022-11-09 17:45:11,426:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0365 (2.8641)
2022-11-09 17:45:11,687:INFO: Dataset: univ                Batch:  1/15	Loss 5.2290 (5.2290)
2022-11-09 17:45:11,713:INFO: Dataset: univ                Batch:  2/15	Loss 3.6742 (4.4553)
2022-11-09 17:45:11,741:INFO: Dataset: univ                Batch:  3/15	Loss 3.3924 (4.0470)
2022-11-09 17:45:11,766:INFO: Dataset: univ                Batch:  4/15	Loss 3.9855 (4.0320)
2022-11-09 17:45:11,795:INFO: Dataset: univ                Batch:  5/15	Loss 3.4001 (3.9034)
2022-11-09 17:45:11,821:INFO: Dataset: univ                Batch:  6/15	Loss 3.4578 (3.8268)
2022-11-09 17:45:11,845:INFO: Dataset: univ                Batch:  7/15	Loss 3.9723 (3.8463)
2022-11-09 17:45:11,870:INFO: Dataset: univ                Batch:  8/15	Loss 3.5553 (3.8082)
2022-11-09 17:45:11,895:INFO: Dataset: univ                Batch:  9/15	Loss 3.3451 (3.7579)
2022-11-09 17:45:11,921:INFO: Dataset: univ                Batch: 10/15	Loss 3.6178 (3.7442)
2022-11-09 17:45:11,949:INFO: Dataset: univ                Batch: 11/15	Loss 3.7392 (3.7438)
2022-11-09 17:45:11,975:INFO: Dataset: univ                Batch: 12/15	Loss 3.6842 (3.7391)
2022-11-09 17:45:12,001:INFO: Dataset: univ                Batch: 13/15	Loss 3.5075 (3.7197)
2022-11-09 17:45:12,027:INFO: Dataset: univ                Batch: 14/15	Loss 4.0152 (3.7398)
2022-11-09 17:45:12,036:INFO: Dataset: univ                Batch: 15/15	Loss 0.5802 (3.6888)
2022-11-09 17:45:12,301:INFO: Dataset: zara1               Batch: 1/8	Loss 10.2727 (10.2727)
2022-11-09 17:45:12,327:INFO: Dataset: zara1               Batch: 2/8	Loss 8.7630 (9.5764)
2022-11-09 17:45:12,351:INFO: Dataset: zara1               Batch: 3/8	Loss 6.7419 (8.5917)
2022-11-09 17:45:12,376:INFO: Dataset: zara1               Batch: 4/8	Loss 7.7092 (8.3443)
2022-11-09 17:45:12,399:INFO: Dataset: zara1               Batch: 5/8	Loss 7.4334 (8.1675)
2022-11-09 17:45:12,425:INFO: Dataset: zara1               Batch: 6/8	Loss 8.3988 (8.2044)
2022-11-09 17:45:12,448:INFO: Dataset: zara1               Batch: 7/8	Loss 6.8327 (8.0093)
2022-11-09 17:45:12,469:INFO: Dataset: zara1               Batch: 8/8	Loss 5.3930 (7.7352)
2022-11-09 17:45:12,720:INFO: Dataset: zara2               Batch:  1/18	Loss 6.1974 (6.1974)
2022-11-09 17:45:12,744:INFO: Dataset: zara2               Batch:  2/18	Loss 8.8866 (7.5704)
2022-11-09 17:45:12,768:INFO: Dataset: zara2               Batch:  3/18	Loss 7.0463 (7.3933)
2022-11-09 17:45:12,791:INFO: Dataset: zara2               Batch:  4/18	Loss 7.5007 (7.4176)
2022-11-09 17:45:12,815:INFO: Dataset: zara2               Batch:  5/18	Loss 7.3354 (7.4004)
2022-11-09 17:45:12,843:INFO: Dataset: zara2               Batch:  6/18	Loss 5.4618 (7.0935)
2022-11-09 17:45:12,866:INFO: Dataset: zara2               Batch:  7/18	Loss 5.9003 (6.9156)
2022-11-09 17:45:12,889:INFO: Dataset: zara2               Batch:  8/18	Loss 4.9214 (6.6757)
2022-11-09 17:45:12,912:INFO: Dataset: zara2               Batch:  9/18	Loss 5.1687 (6.5234)
2022-11-09 17:45:12,936:INFO: Dataset: zara2               Batch: 10/18	Loss 4.7180 (6.3382)
2022-11-09 17:45:12,960:INFO: Dataset: zara2               Batch: 11/18	Loss 4.2530 (6.1328)
2022-11-09 17:45:12,986:INFO: Dataset: zara2               Batch: 12/18	Loss 5.1457 (6.0514)
2022-11-09 17:45:13,011:INFO: Dataset: zara2               Batch: 13/18	Loss 5.0488 (5.9763)
2022-11-09 17:45:13,034:INFO: Dataset: zara2               Batch: 14/18	Loss 4.9568 (5.9090)
2022-11-09 17:45:13,058:INFO: Dataset: zara2               Batch: 15/18	Loss 5.2247 (5.8692)
2022-11-09 17:45:13,082:INFO: Dataset: zara2               Batch: 16/18	Loss 4.8159 (5.8010)
2022-11-09 17:45:13,106:INFO: Dataset: zara2               Batch: 17/18	Loss 6.6760 (5.8501)
2022-11-09 17:45:13,128:INFO: Dataset: zara2               Batch: 18/18	Loss 4.3331 (5.7854)
2022-11-09 17:45:13,185:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_49.pth.tar
2022-11-09 17:45:13,185:INFO: 
===> EPOCH: 50 (P1)
2022-11-09 17:45:13,185:INFO: - Computing loss (training)
2022-11-09 17:45:13,410:INFO: Dataset: hotel               Batch: 1/4	Loss 4.6518 (4.6518)
2022-11-09 17:45:13,438:INFO: Dataset: hotel               Batch: 2/4	Loss 6.9201 (5.7688)
2022-11-09 17:45:13,461:INFO: Dataset: hotel               Batch: 3/4	Loss 3.0875 (4.8143)
2022-11-09 17:45:13,478:INFO: Dataset: hotel               Batch: 4/4	Loss 2.2090 (4.3331)
2022-11-09 17:45:13,751:INFO: Dataset: univ                Batch:  1/15	Loss 3.7138 (3.7138)
2022-11-09 17:45:13,778:INFO: Dataset: univ                Batch:  2/15	Loss 3.5654 (3.6395)
2022-11-09 17:45:13,804:INFO: Dataset: univ                Batch:  3/15	Loss 3.5391 (3.6060)
2022-11-09 17:45:13,830:INFO: Dataset: univ                Batch:  4/15	Loss 3.9818 (3.6974)
2022-11-09 17:45:13,860:INFO: Dataset: univ                Batch:  5/15	Loss 4.1314 (3.7847)
2022-11-09 17:45:13,888:INFO: Dataset: univ                Batch:  6/15	Loss 5.5548 (4.0592)
2022-11-09 17:45:13,913:INFO: Dataset: univ                Batch:  7/15	Loss 3.4513 (3.9737)
2022-11-09 17:45:13,939:INFO: Dataset: univ                Batch:  8/15	Loss 3.7262 (3.9427)
2022-11-09 17:45:13,965:INFO: Dataset: univ                Batch:  9/15	Loss 4.9026 (4.0375)
2022-11-09 17:45:13,992:INFO: Dataset: univ                Batch: 10/15	Loss 3.5818 (3.9898)
2022-11-09 17:45:14,019:INFO: Dataset: univ                Batch: 11/15	Loss 3.5421 (3.9503)
2022-11-09 17:45:14,046:INFO: Dataset: univ                Batch: 12/15	Loss 3.5422 (3.9141)
2022-11-09 17:45:14,072:INFO: Dataset: univ                Batch: 13/15	Loss 3.8609 (3.9101)
2022-11-09 17:45:14,099:INFO: Dataset: univ                Batch: 14/15	Loss 4.3951 (3.9410)
2022-11-09 17:45:14,109:INFO: Dataset: univ                Batch: 15/15	Loss 0.6993 (3.8962)
2022-11-09 17:45:14,353:INFO: Dataset: zara1               Batch: 1/8	Loss 11.5909 (11.5909)
2022-11-09 17:45:14,378:INFO: Dataset: zara1               Batch: 2/8	Loss 11.3496 (11.4805)
2022-11-09 17:45:14,403:INFO: Dataset: zara1               Batch: 3/8	Loss 11.3554 (11.4375)
2022-11-09 17:45:14,428:INFO: Dataset: zara1               Batch: 4/8	Loss 8.7104 (10.7766)
2022-11-09 17:45:14,453:INFO: Dataset: zara1               Batch: 5/8	Loss 8.5891 (10.3162)
2022-11-09 17:45:14,477:INFO: Dataset: zara1               Batch: 6/8	Loss 8.0981 (9.9496)
2022-11-09 17:45:14,500:INFO: Dataset: zara1               Batch: 7/8	Loss 5.9155 (9.3919)
2022-11-09 17:45:14,521:INFO: Dataset: zara1               Batch: 8/8	Loss 6.0116 (8.9845)
2022-11-09 17:45:14,773:INFO: Dataset: zara2               Batch:  1/18	Loss 6.1455 (6.1455)
2022-11-09 17:45:14,798:INFO: Dataset: zara2               Batch:  2/18	Loss 6.4762 (6.3145)
2022-11-09 17:45:14,823:INFO: Dataset: zara2               Batch:  3/18	Loss 8.4102 (6.9772)
2022-11-09 17:45:14,847:INFO: Dataset: zara2               Batch:  4/18	Loss 5.9349 (6.7238)
2022-11-09 17:45:14,872:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1397 (6.8088)
2022-11-09 17:45:14,899:INFO: Dataset: zara2               Batch:  6/18	Loss 6.3034 (6.7184)
2022-11-09 17:45:14,923:INFO: Dataset: zara2               Batch:  7/18	Loss 5.7356 (6.5637)
2022-11-09 17:45:14,947:INFO: Dataset: zara2               Batch:  8/18	Loss 6.3877 (6.5432)
2022-11-09 17:45:14,971:INFO: Dataset: zara2               Batch:  9/18	Loss 5.0708 (6.3853)
2022-11-09 17:45:14,995:INFO: Dataset: zara2               Batch: 10/18	Loss 5.5273 (6.2942)
2022-11-09 17:45:15,023:INFO: Dataset: zara2               Batch: 11/18	Loss 5.6460 (6.2335)
2022-11-09 17:45:15,048:INFO: Dataset: zara2               Batch: 12/18	Loss 5.2817 (6.1557)
2022-11-09 17:45:15,072:INFO: Dataset: zara2               Batch: 13/18	Loss 5.2552 (6.0826)
2022-11-09 17:45:15,096:INFO: Dataset: zara2               Batch: 14/18	Loss 4.7839 (5.9865)
2022-11-09 17:45:15,121:INFO: Dataset: zara2               Batch: 15/18	Loss 5.3141 (5.9437)
2022-11-09 17:45:15,146:INFO: Dataset: zara2               Batch: 16/18	Loss 4.6547 (5.8620)
2022-11-09 17:45:15,171:INFO: Dataset: zara2               Batch: 17/18	Loss 4.4517 (5.7784)
2022-11-09 17:45:15,193:INFO: Dataset: zara2               Batch: 18/18	Loss 4.8545 (5.7382)
2022-11-09 17:45:15,242:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_50.pth.tar
2022-11-09 17:45:15,242:INFO: 
===> EPOCH: 51 (P1)
2022-11-09 17:45:15,242:INFO: - Computing loss (training)
2022-11-09 17:45:15,445:INFO: Dataset: hotel               Batch: 1/4	Loss 3.8945 (3.8945)
2022-11-09 17:45:15,475:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5647 (3.7308)
2022-11-09 17:45:15,500:INFO: Dataset: hotel               Batch: 3/4	Loss 3.9143 (3.7931)
2022-11-09 17:45:15,516:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2921 (3.3147)
2022-11-09 17:45:15,769:INFO: Dataset: univ                Batch:  1/15	Loss 3.7941 (3.7941)
2022-11-09 17:45:15,798:INFO: Dataset: univ                Batch:  2/15	Loss 3.7371 (3.7663)
2022-11-09 17:45:15,828:INFO: Dataset: univ                Batch:  3/15	Loss 3.6300 (3.7201)
2022-11-09 17:45:15,855:INFO: Dataset: univ                Batch:  4/15	Loss 3.4780 (3.6577)
2022-11-09 17:45:15,881:INFO: Dataset: univ                Batch:  5/15	Loss 3.3993 (3.6048)
2022-11-09 17:45:15,910:INFO: Dataset: univ                Batch:  6/15	Loss 4.9069 (3.7937)
2022-11-09 17:45:15,935:INFO: Dataset: univ                Batch:  7/15	Loss 3.6097 (3.7681)
2022-11-09 17:45:15,961:INFO: Dataset: univ                Batch:  8/15	Loss 3.6584 (3.7533)
2022-11-09 17:45:15,988:INFO: Dataset: univ                Batch:  9/15	Loss 3.4668 (3.7229)
2022-11-09 17:45:16,016:INFO: Dataset: univ                Batch: 10/15	Loss 3.3896 (3.6890)
2022-11-09 17:45:16,046:INFO: Dataset: univ                Batch: 11/15	Loss 3.8825 (3.7068)
2022-11-09 17:45:16,071:INFO: Dataset: univ                Batch: 12/15	Loss 3.4940 (3.6903)
2022-11-09 17:45:16,097:INFO: Dataset: univ                Batch: 13/15	Loss 4.0410 (3.7157)
2022-11-09 17:45:16,122:INFO: Dataset: univ                Batch: 14/15	Loss 3.6081 (3.7083)
2022-11-09 17:45:16,131:INFO: Dataset: univ                Batch: 15/15	Loss 0.4941 (3.6671)
2022-11-09 17:45:16,369:INFO: Dataset: zara1               Batch: 1/8	Loss 9.6910 (9.6910)
2022-11-09 17:45:16,394:INFO: Dataset: zara1               Batch: 2/8	Loss 10.3709 (10.0649)
2022-11-09 17:45:16,422:INFO: Dataset: zara1               Batch: 3/8	Loss 11.2262 (10.4231)
2022-11-09 17:45:16,446:INFO: Dataset: zara1               Batch: 4/8	Loss 7.2070 (9.6676)
2022-11-09 17:45:16,471:INFO: Dataset: zara1               Batch: 5/8	Loss 7.0358 (9.1214)
2022-11-09 17:45:16,496:INFO: Dataset: zara1               Batch: 6/8	Loss 6.6224 (8.6462)
2022-11-09 17:45:16,520:INFO: Dataset: zara1               Batch: 7/8	Loss 5.9058 (8.2409)
2022-11-09 17:45:16,542:INFO: Dataset: zara1               Batch: 8/8	Loss 5.5064 (7.9487)
2022-11-09 17:45:16,787:INFO: Dataset: zara2               Batch:  1/18	Loss 8.1086 (8.1086)
2022-11-09 17:45:16,813:INFO: Dataset: zara2               Batch:  2/18	Loss 7.3928 (7.7415)
2022-11-09 17:45:16,841:INFO: Dataset: zara2               Batch:  3/18	Loss 7.7209 (7.7346)
2022-11-09 17:45:16,865:INFO: Dataset: zara2               Batch:  4/18	Loss 8.0127 (7.8093)
2022-11-09 17:45:16,891:INFO: Dataset: zara2               Batch:  5/18	Loss 5.2888 (7.3264)
2022-11-09 17:45:16,916:INFO: Dataset: zara2               Batch:  6/18	Loss 4.8423 (6.9034)
2022-11-09 17:45:16,939:INFO: Dataset: zara2               Batch:  7/18	Loss 4.3488 (6.5400)
2022-11-09 17:45:16,963:INFO: Dataset: zara2               Batch:  8/18	Loss 5.0871 (6.3610)
2022-11-09 17:45:16,986:INFO: Dataset: zara2               Batch:  9/18	Loss 4.1150 (6.1192)
2022-11-09 17:45:17,011:INFO: Dataset: zara2               Batch: 10/18	Loss 4.7104 (5.9834)
2022-11-09 17:45:17,037:INFO: Dataset: zara2               Batch: 11/18	Loss 3.7078 (5.7451)
2022-11-09 17:45:17,064:INFO: Dataset: zara2               Batch: 12/18	Loss 4.6769 (5.6592)
2022-11-09 17:45:17,088:INFO: Dataset: zara2               Batch: 13/18	Loss 4.8517 (5.5961)
2022-11-09 17:45:17,112:INFO: Dataset: zara2               Batch: 14/18	Loss 5.4840 (5.5886)
2022-11-09 17:45:17,136:INFO: Dataset: zara2               Batch: 15/18	Loss 5.2433 (5.5670)
2022-11-09 17:45:17,161:INFO: Dataset: zara2               Batch: 16/18	Loss 6.0699 (5.5981)
2022-11-09 17:45:17,186:INFO: Dataset: zara2               Batch: 17/18	Loss 5.2874 (5.5800)
2022-11-09 17:45:17,207:INFO: Dataset: zara2               Batch: 18/18	Loss 4.3078 (5.5101)
2022-11-09 17:45:17,257:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_51.pth.tar
2022-11-09 17:45:17,257:INFO: 
===> EPOCH: 52 (P1)
2022-11-09 17:45:17,257:INFO: - Computing loss (training)
2022-11-09 17:45:17,466:INFO: Dataset: hotel               Batch: 1/4	Loss 3.5115 (3.5115)
2022-11-09 17:45:17,493:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5667 (3.5388)
2022-11-09 17:45:17,518:INFO: Dataset: hotel               Batch: 3/4	Loss 3.6510 (3.5751)
2022-11-09 17:45:17,536:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6202 (3.2141)
2022-11-09 17:45:17,790:INFO: Dataset: univ                Batch:  1/15	Loss 3.4039 (3.4039)
2022-11-09 17:45:17,825:INFO: Dataset: univ                Batch:  2/15	Loss 3.5687 (3.4853)
2022-11-09 17:45:17,852:INFO: Dataset: univ                Batch:  3/15	Loss 3.4527 (3.4746)
2022-11-09 17:45:17,877:INFO: Dataset: univ                Batch:  4/15	Loss 3.9908 (3.6013)
2022-11-09 17:45:17,902:INFO: Dataset: univ                Batch:  5/15	Loss 3.6360 (3.6077)
2022-11-09 17:45:17,931:INFO: Dataset: univ                Batch:  6/15	Loss 3.1416 (3.5174)
2022-11-09 17:45:17,956:INFO: Dataset: univ                Batch:  7/15	Loss 4.4429 (3.6489)
2022-11-09 17:45:17,982:INFO: Dataset: univ                Batch:  8/15	Loss 3.1887 (3.5819)
2022-11-09 17:45:18,008:INFO: Dataset: univ                Batch:  9/15	Loss 3.0909 (3.5248)
2022-11-09 17:45:18,035:INFO: Dataset: univ                Batch: 10/15	Loss 4.0197 (3.5725)
2022-11-09 17:45:18,062:INFO: Dataset: univ                Batch: 11/15	Loss 3.6576 (3.5804)
2022-11-09 17:45:18,089:INFO: Dataset: univ                Batch: 12/15	Loss 3.8276 (3.5992)
2022-11-09 17:45:18,114:INFO: Dataset: univ                Batch: 13/15	Loss 3.5979 (3.5991)
2022-11-09 17:45:18,140:INFO: Dataset: univ                Batch: 14/15	Loss 4.0578 (3.6303)
2022-11-09 17:45:18,150:INFO: Dataset: univ                Batch: 15/15	Loss 0.6686 (3.5905)
2022-11-09 17:45:18,408:INFO: Dataset: zara1               Batch: 1/8	Loss 8.7148 (8.7148)
2022-11-09 17:45:18,432:INFO: Dataset: zara1               Batch: 2/8	Loss 12.6026 (10.5458)
2022-11-09 17:45:18,456:INFO: Dataset: zara1               Batch: 3/8	Loss 8.8842 (9.9283)
2022-11-09 17:45:18,479:INFO: Dataset: zara1               Batch: 4/8	Loss 7.5991 (9.3779)
2022-11-09 17:45:18,504:INFO: Dataset: zara1               Batch: 5/8	Loss 6.4769 (8.8471)
2022-11-09 17:45:18,529:INFO: Dataset: zara1               Batch: 6/8	Loss 6.9115 (8.5396)
2022-11-09 17:45:18,553:INFO: Dataset: zara1               Batch: 7/8	Loss 7.1580 (8.3259)
2022-11-09 17:45:18,574:INFO: Dataset: zara1               Batch: 8/8	Loss 4.6640 (7.8788)
2022-11-09 17:45:18,839:INFO: Dataset: zara2               Batch:  1/18	Loss 5.8213 (5.8213)
2022-11-09 17:45:18,865:INFO: Dataset: zara2               Batch:  2/18	Loss 6.0680 (5.9443)
2022-11-09 17:45:18,892:INFO: Dataset: zara2               Batch:  3/18	Loss 7.0256 (6.3041)
2022-11-09 17:45:18,915:INFO: Dataset: zara2               Batch:  4/18	Loss 5.8270 (6.2012)
2022-11-09 17:45:18,941:INFO: Dataset: zara2               Batch:  5/18	Loss 6.5074 (6.2606)
2022-11-09 17:45:18,969:INFO: Dataset: zara2               Batch:  6/18	Loss 7.4384 (6.4643)
2022-11-09 17:45:18,992:INFO: Dataset: zara2               Batch:  7/18	Loss 5.8823 (6.3685)
2022-11-09 17:45:19,016:INFO: Dataset: zara2               Batch:  8/18	Loss 5.1903 (6.2257)
2022-11-09 17:45:19,040:INFO: Dataset: zara2               Batch:  9/18	Loss 5.0556 (6.0932)
2022-11-09 17:45:19,066:INFO: Dataset: zara2               Batch: 10/18	Loss 5.0496 (5.9879)
2022-11-09 17:45:19,092:INFO: Dataset: zara2               Batch: 11/18	Loss 5.0466 (5.9116)
2022-11-09 17:45:19,119:INFO: Dataset: zara2               Batch: 12/18	Loss 4.4719 (5.7939)
2022-11-09 17:45:19,144:INFO: Dataset: zara2               Batch: 13/18	Loss 5.7664 (5.7916)
2022-11-09 17:45:19,168:INFO: Dataset: zara2               Batch: 14/18	Loss 4.5203 (5.7004)
2022-11-09 17:45:19,193:INFO: Dataset: zara2               Batch: 15/18	Loss 5.7261 (5.7021)
2022-11-09 17:45:19,217:INFO: Dataset: zara2               Batch: 16/18	Loss 4.5124 (5.6318)
2022-11-09 17:45:19,242:INFO: Dataset: zara2               Batch: 17/18	Loss 5.0272 (5.6006)
2022-11-09 17:45:19,266:INFO: Dataset: zara2               Batch: 18/18	Loss 3.9567 (5.5288)
2022-11-09 17:45:19,317:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_52.pth.tar
2022-11-09 17:45:19,317:INFO: 
===> EPOCH: 53 (P1)
2022-11-09 17:45:19,317:INFO: - Computing loss (training)
2022-11-09 17:45:19,520:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4169 (2.4169)
2022-11-09 17:45:19,545:INFO: Dataset: hotel               Batch: 2/4	Loss 3.0421 (2.7196)
2022-11-09 17:45:19,570:INFO: Dataset: hotel               Batch: 3/4	Loss 2.5663 (2.6673)
2022-11-09 17:45:19,586:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5260 (2.4655)
2022-11-09 17:45:19,843:INFO: Dataset: univ                Batch:  1/15	Loss 3.5333 (3.5333)
2022-11-09 17:45:19,871:INFO: Dataset: univ                Batch:  2/15	Loss 3.4797 (3.5068)
2022-11-09 17:45:19,901:INFO: Dataset: univ                Batch:  3/15	Loss 5.5182 (4.1624)
2022-11-09 17:45:19,927:INFO: Dataset: univ                Batch:  4/15	Loss 3.4252 (3.9715)
2022-11-09 17:45:19,953:INFO: Dataset: univ                Batch:  5/15	Loss 3.2446 (3.8151)
2022-11-09 17:45:19,982:INFO: Dataset: univ                Batch:  6/15	Loss 3.2736 (3.7217)
2022-11-09 17:45:20,009:INFO: Dataset: univ                Batch:  7/15	Loss 3.1368 (3.6330)
2022-11-09 17:45:20,035:INFO: Dataset: univ                Batch:  8/15	Loss 3.6053 (3.6294)
2022-11-09 17:45:20,061:INFO: Dataset: univ                Batch:  9/15	Loss 3.5422 (3.6198)
2022-11-09 17:45:20,088:INFO: Dataset: univ                Batch: 10/15	Loss 3.4707 (3.6046)
2022-11-09 17:45:20,116:INFO: Dataset: univ                Batch: 11/15	Loss 3.6823 (3.6113)
2022-11-09 17:45:20,144:INFO: Dataset: univ                Batch: 12/15	Loss 3.6334 (3.6135)
2022-11-09 17:45:20,170:INFO: Dataset: univ                Batch: 13/15	Loss 3.2497 (3.5843)
2022-11-09 17:45:20,195:INFO: Dataset: univ                Batch: 14/15	Loss 3.1773 (3.5548)
2022-11-09 17:45:20,205:INFO: Dataset: univ                Batch: 15/15	Loss 0.6017 (3.5112)
2022-11-09 17:45:20,473:INFO: Dataset: zara1               Batch: 1/8	Loss 9.3139 (9.3139)
2022-11-09 17:45:20,499:INFO: Dataset: zara1               Batch: 2/8	Loss 7.3261 (8.1820)
2022-11-09 17:45:20,524:INFO: Dataset: zara1               Batch: 3/8	Loss 9.1766 (8.5117)
2022-11-09 17:45:20,550:INFO: Dataset: zara1               Batch: 4/8	Loss 7.1563 (8.1949)
2022-11-09 17:45:20,577:INFO: Dataset: zara1               Batch: 5/8	Loss 6.1184 (7.7689)
2022-11-09 17:45:20,602:INFO: Dataset: zara1               Batch: 6/8	Loss 7.4731 (7.7197)
2022-11-09 17:45:20,626:INFO: Dataset: zara1               Batch: 7/8	Loss 6.1113 (7.5121)
2022-11-09 17:45:20,648:INFO: Dataset: zara1               Batch: 8/8	Loss 4.9811 (7.2524)
2022-11-09 17:45:20,902:INFO: Dataset: zara2               Batch:  1/18	Loss 6.6347 (6.6347)
2022-11-09 17:45:20,930:INFO: Dataset: zara2               Batch:  2/18	Loss 6.5391 (6.5859)
2022-11-09 17:45:20,956:INFO: Dataset: zara2               Batch:  3/18	Loss 8.2044 (7.1194)
2022-11-09 17:45:20,979:INFO: Dataset: zara2               Batch:  4/18	Loss 5.9924 (6.8800)
2022-11-09 17:45:21,004:INFO: Dataset: zara2               Batch:  5/18	Loss 6.0287 (6.7124)
2022-11-09 17:45:21,032:INFO: Dataset: zara2               Batch:  6/18	Loss 6.7953 (6.7265)
2022-11-09 17:45:21,055:INFO: Dataset: zara2               Batch:  7/18	Loss 4.9276 (6.4942)
2022-11-09 17:45:21,079:INFO: Dataset: zara2               Batch:  8/18	Loss 4.5242 (6.2477)
2022-11-09 17:45:21,102:INFO: Dataset: zara2               Batch:  9/18	Loss 4.2589 (6.0381)
2022-11-09 17:45:21,127:INFO: Dataset: zara2               Batch: 10/18	Loss 3.8842 (5.8287)
2022-11-09 17:45:21,153:INFO: Dataset: zara2               Batch: 11/18	Loss 4.5043 (5.7107)
2022-11-09 17:45:21,180:INFO: Dataset: zara2               Batch: 12/18	Loss 4.1586 (5.5774)
2022-11-09 17:45:21,204:INFO: Dataset: zara2               Batch: 13/18	Loss 4.6981 (5.5103)
2022-11-09 17:45:21,228:INFO: Dataset: zara2               Batch: 14/18	Loss 4.8856 (5.4653)
2022-11-09 17:45:21,252:INFO: Dataset: zara2               Batch: 15/18	Loss 4.2496 (5.3775)
2022-11-09 17:45:21,276:INFO: Dataset: zara2               Batch: 16/18	Loss 4.6093 (5.3305)
2022-11-09 17:45:21,302:INFO: Dataset: zara2               Batch: 17/18	Loss 4.9263 (5.3069)
2022-11-09 17:45:21,325:INFO: Dataset: zara2               Batch: 18/18	Loss 3.2620 (5.2048)
2022-11-09 17:45:21,376:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_53.pth.tar
2022-11-09 17:45:21,376:INFO: 
===> EPOCH: 54 (P1)
2022-11-09 17:45:21,377:INFO: - Computing loss (training)
2022-11-09 17:45:21,590:INFO: Dataset: hotel               Batch: 1/4	Loss 2.7878 (2.7878)
2022-11-09 17:45:21,616:INFO: Dataset: hotel               Batch: 2/4	Loss 3.4321 (3.1242)
2022-11-09 17:45:21,641:INFO: Dataset: hotel               Batch: 3/4	Loss 2.6036 (2.9407)
2022-11-09 17:45:21,659:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6105 (2.7161)
2022-11-09 17:45:21,915:INFO: Dataset: univ                Batch:  1/15	Loss 3.1628 (3.1628)
2022-11-09 17:45:21,945:INFO: Dataset: univ                Batch:  2/15	Loss 3.7539 (3.4652)
2022-11-09 17:45:21,971:INFO: Dataset: univ                Batch:  3/15	Loss 3.2027 (3.3744)
2022-11-09 17:45:21,998:INFO: Dataset: univ                Batch:  4/15	Loss 4.6516 (3.6921)
2022-11-09 17:45:22,024:INFO: Dataset: univ                Batch:  5/15	Loss 3.3612 (3.6232)
2022-11-09 17:45:22,051:INFO: Dataset: univ                Batch:  6/15	Loss 3.4308 (3.5914)
2022-11-09 17:45:22,077:INFO: Dataset: univ                Batch:  7/15	Loss 3.3249 (3.5499)
2022-11-09 17:45:22,102:INFO: Dataset: univ                Batch:  8/15	Loss 3.3713 (3.5260)
2022-11-09 17:45:22,128:INFO: Dataset: univ                Batch:  9/15	Loss 3.7136 (3.5453)
2022-11-09 17:45:22,156:INFO: Dataset: univ                Batch: 10/15	Loss 3.5463 (3.5454)
2022-11-09 17:45:22,185:INFO: Dataset: univ                Batch: 11/15	Loss 3.3858 (3.5295)
2022-11-09 17:45:22,212:INFO: Dataset: univ                Batch: 12/15	Loss 3.4978 (3.5268)
2022-11-09 17:45:22,238:INFO: Dataset: univ                Batch: 13/15	Loss 4.2877 (3.5794)
2022-11-09 17:45:22,264:INFO: Dataset: univ                Batch: 14/15	Loss 3.2120 (3.5535)
2022-11-09 17:45:22,274:INFO: Dataset: univ                Batch: 15/15	Loss 0.5960 (3.5123)
2022-11-09 17:45:22,534:INFO: Dataset: zara1               Batch: 1/8	Loss 13.8367 (13.8367)
2022-11-09 17:45:22,562:INFO: Dataset: zara1               Batch: 2/8	Loss 9.4804 (11.7829)
2022-11-09 17:45:22,585:INFO: Dataset: zara1               Batch: 3/8	Loss 7.1914 (10.2459)
2022-11-09 17:45:22,609:INFO: Dataset: zara1               Batch: 4/8	Loss 9.3929 (10.0327)
2022-11-09 17:45:22,632:INFO: Dataset: zara1               Batch: 5/8	Loss 6.1474 (9.1640)
2022-11-09 17:45:22,658:INFO: Dataset: zara1               Batch: 6/8	Loss 6.8218 (8.8558)
2022-11-09 17:45:22,681:INFO: Dataset: zara1               Batch: 7/8	Loss 5.6153 (8.3402)
2022-11-09 17:45:22,702:INFO: Dataset: zara1               Batch: 8/8	Loss 4.9459 (7.9329)
2022-11-09 17:45:23,019:INFO: Dataset: zara2               Batch:  1/18	Loss 9.5019 (9.5019)
2022-11-09 17:45:23,047:INFO: Dataset: zara2               Batch:  2/18	Loss 5.9250 (7.8011)
2022-11-09 17:45:23,071:INFO: Dataset: zara2               Batch:  3/18	Loss 7.5470 (7.7208)
2022-11-09 17:45:23,094:INFO: Dataset: zara2               Batch:  4/18	Loss 5.7109 (7.2103)
2022-11-09 17:45:23,118:INFO: Dataset: zara2               Batch:  5/18	Loss 5.4235 (6.8450)
2022-11-09 17:45:23,143:INFO: Dataset: zara2               Batch:  6/18	Loss 4.8112 (6.5014)
2022-11-09 17:45:23,167:INFO: Dataset: zara2               Batch:  7/18	Loss 4.4170 (6.1918)
2022-11-09 17:45:23,190:INFO: Dataset: zara2               Batch:  8/18	Loss 4.2797 (5.9402)
2022-11-09 17:45:23,214:INFO: Dataset: zara2               Batch:  9/18	Loss 4.5012 (5.7843)
2022-11-09 17:45:23,238:INFO: Dataset: zara2               Batch: 10/18	Loss 4.3518 (5.6350)
2022-11-09 17:45:23,264:INFO: Dataset: zara2               Batch: 11/18	Loss 4.2264 (5.5000)
2022-11-09 17:45:23,290:INFO: Dataset: zara2               Batch: 12/18	Loss 3.9977 (5.3744)
2022-11-09 17:45:23,315:INFO: Dataset: zara2               Batch: 13/18	Loss 3.8775 (5.2667)
2022-11-09 17:45:23,340:INFO: Dataset: zara2               Batch: 14/18	Loss 5.1617 (5.2586)
2022-11-09 17:45:23,364:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6325 (5.2191)
2022-11-09 17:45:23,389:INFO: Dataset: zara2               Batch: 16/18	Loss 4.0438 (5.1398)
2022-11-09 17:45:23,414:INFO: Dataset: zara2               Batch: 17/18	Loss 4.0775 (5.0832)
2022-11-09 17:45:23,436:INFO: Dataset: zara2               Batch: 18/18	Loss 3.7646 (5.0184)
2022-11-09 17:45:23,487:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_54.pth.tar
2022-11-09 17:45:23,487:INFO: 
===> EPOCH: 55 (P1)
2022-11-09 17:45:23,487:INFO: - Computing loss (training)
2022-11-09 17:45:23,690:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4108 (2.4108)
2022-11-09 17:45:23,717:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4436 (2.4281)
2022-11-09 17:45:23,744:INFO: Dataset: hotel               Batch: 3/4	Loss 2.6469 (2.4998)
2022-11-09 17:45:23,760:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8688 (2.3999)
2022-11-09 17:45:24,010:INFO: Dataset: univ                Batch:  1/15	Loss 3.3820 (3.3820)
2022-11-09 17:45:24,041:INFO: Dataset: univ                Batch:  2/15	Loss 3.3551 (3.3682)
2022-11-09 17:45:24,069:INFO: Dataset: univ                Batch:  3/15	Loss 3.2716 (3.3357)
2022-11-09 17:45:24,094:INFO: Dataset: univ                Batch:  4/15	Loss 3.8717 (3.4650)
2022-11-09 17:45:24,120:INFO: Dataset: univ                Batch:  5/15	Loss 3.1185 (3.3933)
2022-11-09 17:45:24,155:INFO: Dataset: univ                Batch:  6/15	Loss 3.2255 (3.3661)
2022-11-09 17:45:24,181:INFO: Dataset: univ                Batch:  7/15	Loss 3.4604 (3.3792)
2022-11-09 17:45:24,206:INFO: Dataset: univ                Batch:  8/15	Loss 3.0146 (3.3289)
2022-11-09 17:45:24,232:INFO: Dataset: univ                Batch:  9/15	Loss 3.8975 (3.3877)
2022-11-09 17:45:24,259:INFO: Dataset: univ                Batch: 10/15	Loss 3.0770 (3.3574)
2022-11-09 17:45:24,285:INFO: Dataset: univ                Batch: 11/15	Loss 3.4104 (3.3623)
2022-11-09 17:45:24,312:INFO: Dataset: univ                Batch: 12/15	Loss 3.0443 (3.3351)
2022-11-09 17:45:24,338:INFO: Dataset: univ                Batch: 13/15	Loss 3.2967 (3.3323)
2022-11-09 17:45:24,363:INFO: Dataset: univ                Batch: 14/15	Loss 3.0679 (3.3141)
2022-11-09 17:45:24,373:INFO: Dataset: univ                Batch: 15/15	Loss 0.4918 (3.2765)
2022-11-09 17:45:24,629:INFO: Dataset: zara1               Batch: 1/8	Loss 12.8877 (12.8877)
2022-11-09 17:45:24,653:INFO: Dataset: zara1               Batch: 2/8	Loss 9.2802 (11.1582)
2022-11-09 17:45:24,677:INFO: Dataset: zara1               Batch: 3/8	Loss 9.5357 (10.6495)
2022-11-09 17:45:24,702:INFO: Dataset: zara1               Batch: 4/8	Loss 5.1910 (9.3066)
2022-11-09 17:45:24,725:INFO: Dataset: zara1               Batch: 5/8	Loss 5.2107 (8.4400)
2022-11-09 17:45:24,751:INFO: Dataset: zara1               Batch: 6/8	Loss 5.9414 (8.0209)
2022-11-09 17:45:24,775:INFO: Dataset: zara1               Batch: 7/8	Loss 5.3192 (7.6096)
2022-11-09 17:45:24,796:INFO: Dataset: zara1               Batch: 8/8	Loss 5.9017 (7.4190)
2022-11-09 17:45:25,062:INFO: Dataset: zara2               Batch:  1/18	Loss 7.1300 (7.1300)
2022-11-09 17:45:25,086:INFO: Dataset: zara2               Batch:  2/18	Loss 9.5120 (8.2924)
2022-11-09 17:45:25,111:INFO: Dataset: zara2               Batch:  3/18	Loss 6.2125 (7.6019)
2022-11-09 17:45:25,136:INFO: Dataset: zara2               Batch:  4/18	Loss 5.3112 (7.0400)
2022-11-09 17:45:25,162:INFO: Dataset: zara2               Batch:  5/18	Loss 4.3853 (6.5056)
2022-11-09 17:45:25,189:INFO: Dataset: zara2               Batch:  6/18	Loss 4.2061 (6.0861)
2022-11-09 17:45:25,213:INFO: Dataset: zara2               Batch:  7/18	Loss 3.5785 (5.6861)
2022-11-09 17:45:25,236:INFO: Dataset: zara2               Batch:  8/18	Loss 4.0293 (5.4826)
2022-11-09 17:45:25,260:INFO: Dataset: zara2               Batch:  9/18	Loss 3.8081 (5.2917)
2022-11-09 17:45:25,284:INFO: Dataset: zara2               Batch: 10/18	Loss 3.7018 (5.1492)
2022-11-09 17:45:25,310:INFO: Dataset: zara2               Batch: 11/18	Loss 3.9379 (5.0403)
2022-11-09 17:45:25,335:INFO: Dataset: zara2               Batch: 12/18	Loss 5.5199 (5.0770)
2022-11-09 17:45:25,359:INFO: Dataset: zara2               Batch: 13/18	Loss 4.9129 (5.0651)
2022-11-09 17:45:25,383:INFO: Dataset: zara2               Batch: 14/18	Loss 3.8031 (4.9727)
2022-11-09 17:45:25,408:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6414 (4.9505)
2022-11-09 17:45:25,433:INFO: Dataset: zara2               Batch: 16/18	Loss 4.0260 (4.8957)
2022-11-09 17:45:25,457:INFO: Dataset: zara2               Batch: 17/18	Loss 3.9415 (4.8322)
2022-11-09 17:45:25,480:INFO: Dataset: zara2               Batch: 18/18	Loss 3.6084 (4.7717)
2022-11-09 17:45:25,528:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_55.pth.tar
2022-11-09 17:45:25,528:INFO: 
===> EPOCH: 56 (P1)
2022-11-09 17:45:25,529:INFO: - Computing loss (training)
2022-11-09 17:45:25,735:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6919 (2.6919)
2022-11-09 17:45:25,761:INFO: Dataset: hotel               Batch: 2/4	Loss 2.8252 (2.7584)
2022-11-09 17:45:25,787:INFO: Dataset: hotel               Batch: 3/4	Loss 4.2063 (3.2630)
2022-11-09 17:45:25,804:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6038 (2.9981)
2022-11-09 17:45:26,053:INFO: Dataset: univ                Batch:  1/15	Loss 3.3380 (3.3380)
2022-11-09 17:45:26,081:INFO: Dataset: univ                Batch:  2/15	Loss 3.1396 (3.2398)
2022-11-09 17:45:26,110:INFO: Dataset: univ                Batch:  3/15	Loss 2.9950 (3.1537)
2022-11-09 17:45:26,136:INFO: Dataset: univ                Batch:  4/15	Loss 3.2911 (3.1871)
2022-11-09 17:45:26,162:INFO: Dataset: univ                Batch:  5/15	Loss 3.4527 (3.2408)
2022-11-09 17:45:26,192:INFO: Dataset: univ                Batch:  6/15	Loss 3.3760 (3.2641)
2022-11-09 17:45:26,217:INFO: Dataset: univ                Batch:  7/15	Loss 3.2292 (3.2593)
2022-11-09 17:45:26,243:INFO: Dataset: univ                Batch:  8/15	Loss 3.2621 (3.2597)
2022-11-09 17:45:26,269:INFO: Dataset: univ                Batch:  9/15	Loss 2.8681 (3.2173)
2022-11-09 17:45:26,295:INFO: Dataset: univ                Batch: 10/15	Loss 3.5858 (3.2507)
2022-11-09 17:45:26,322:INFO: Dataset: univ                Batch: 11/15	Loss 3.0580 (3.2307)
2022-11-09 17:45:26,349:INFO: Dataset: univ                Batch: 12/15	Loss 3.3361 (3.2388)
2022-11-09 17:45:26,375:INFO: Dataset: univ                Batch: 13/15	Loss 3.0943 (3.2265)
2022-11-09 17:45:26,401:INFO: Dataset: univ                Batch: 14/15	Loss 2.8791 (3.1990)
2022-11-09 17:45:26,411:INFO: Dataset: univ                Batch: 15/15	Loss 0.6742 (3.1672)
2022-11-09 17:45:26,660:INFO: Dataset: zara1               Batch: 1/8	Loss 7.1304 (7.1304)
2022-11-09 17:45:26,685:INFO: Dataset: zara1               Batch: 2/8	Loss 6.2198 (6.6231)
2022-11-09 17:45:26,710:INFO: Dataset: zara1               Batch: 3/8	Loss 8.5111 (7.2362)
2022-11-09 17:45:26,734:INFO: Dataset: zara1               Batch: 4/8	Loss 7.9500 (7.4079)
2022-11-09 17:45:26,757:INFO: Dataset: zara1               Batch: 5/8	Loss 7.6252 (7.4525)
2022-11-09 17:45:26,782:INFO: Dataset: zara1               Batch: 6/8	Loss 5.1245 (7.0829)
2022-11-09 17:45:26,806:INFO: Dataset: zara1               Batch: 7/8	Loss 5.5970 (6.8885)
2022-11-09 17:45:26,827:INFO: Dataset: zara1               Batch: 8/8	Loss 4.8213 (6.6426)
2022-11-09 17:45:27,082:INFO: Dataset: zara2               Batch:  1/18	Loss 6.4755 (6.4755)
2022-11-09 17:45:27,114:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2142 (6.8427)
2022-11-09 17:45:27,138:INFO: Dataset: zara2               Batch:  3/18	Loss 6.7101 (6.8015)
2022-11-09 17:45:27,163:INFO: Dataset: zara2               Batch:  4/18	Loss 5.1685 (6.3885)
2022-11-09 17:45:27,186:INFO: Dataset: zara2               Batch:  5/18	Loss 6.8673 (6.4819)
2022-11-09 17:45:27,214:INFO: Dataset: zara2               Batch:  6/18	Loss 4.9060 (6.2127)
2022-11-09 17:45:27,238:INFO: Dataset: zara2               Batch:  7/18	Loss 4.1256 (5.9097)
2022-11-09 17:45:27,262:INFO: Dataset: zara2               Batch:  8/18	Loss 4.3677 (5.7142)
2022-11-09 17:45:27,286:INFO: Dataset: zara2               Batch:  9/18	Loss 4.2718 (5.5451)
2022-11-09 17:45:27,310:INFO: Dataset: zara2               Batch: 10/18	Loss 3.8324 (5.3779)
2022-11-09 17:45:27,335:INFO: Dataset: zara2               Batch: 11/18	Loss 3.5687 (5.2025)
2022-11-09 17:45:27,361:INFO: Dataset: zara2               Batch: 12/18	Loss 3.4395 (5.0336)
2022-11-09 17:45:27,385:INFO: Dataset: zara2               Batch: 13/18	Loss 3.9400 (4.9498)
2022-11-09 17:45:27,409:INFO: Dataset: zara2               Batch: 14/18	Loss 4.0988 (4.8948)
2022-11-09 17:45:27,434:INFO: Dataset: zara2               Batch: 15/18	Loss 4.4296 (4.8643)
2022-11-09 17:45:27,459:INFO: Dataset: zara2               Batch: 16/18	Loss 3.6847 (4.7794)
2022-11-09 17:45:27,484:INFO: Dataset: zara2               Batch: 17/18	Loss 4.2114 (4.7480)
2022-11-09 17:45:27,507:INFO: Dataset: zara2               Batch: 18/18	Loss 2.9930 (4.6493)
2022-11-09 17:45:27,560:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_56.pth.tar
2022-11-09 17:45:27,561:INFO: 
===> EPOCH: 57 (P1)
2022-11-09 17:45:27,561:INFO: - Computing loss (training)
2022-11-09 17:45:27,764:INFO: Dataset: hotel               Batch: 1/4	Loss 3.9553 (3.9553)
2022-11-09 17:45:27,791:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4527 (3.1684)
2022-11-09 17:45:27,814:INFO: Dataset: hotel               Batch: 3/4	Loss 2.3770 (2.9096)
2022-11-09 17:45:27,831:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4857 (2.6636)
2022-11-09 17:45:28,078:INFO: Dataset: univ                Batch:  1/15	Loss 2.8333 (2.8333)
2022-11-09 17:45:28,107:INFO: Dataset: univ                Batch:  2/15	Loss 2.7431 (2.7875)
2022-11-09 17:45:28,136:INFO: Dataset: univ                Batch:  3/15	Loss 3.0767 (2.8787)
2022-11-09 17:45:28,162:INFO: Dataset: univ                Batch:  4/15	Loss 3.2234 (2.9499)
2022-11-09 17:45:28,187:INFO: Dataset: univ                Batch:  5/15	Loss 3.2338 (3.0029)
2022-11-09 17:45:28,217:INFO: Dataset: univ                Batch:  6/15	Loss 3.0660 (3.0134)
2022-11-09 17:45:28,242:INFO: Dataset: univ                Batch:  7/15	Loss 3.2013 (3.0393)
2022-11-09 17:45:28,268:INFO: Dataset: univ                Batch:  8/15	Loss 2.7866 (3.0030)
2022-11-09 17:45:28,296:INFO: Dataset: univ                Batch:  9/15	Loss 3.0044 (3.0032)
2022-11-09 17:45:28,325:INFO: Dataset: univ                Batch: 10/15	Loss 2.7376 (2.9778)
2022-11-09 17:45:28,352:INFO: Dataset: univ                Batch: 11/15	Loss 2.8495 (2.9656)
2022-11-09 17:45:28,380:INFO: Dataset: univ                Batch: 12/15	Loss 3.0351 (2.9716)
2022-11-09 17:45:28,405:INFO: Dataset: univ                Batch: 13/15	Loss 2.7797 (2.9579)
2022-11-09 17:45:28,432:INFO: Dataset: univ                Batch: 14/15	Loss 2.9412 (2.9568)
2022-11-09 17:45:28,441:INFO: Dataset: univ                Batch: 15/15	Loss 0.5077 (2.9266)
2022-11-09 17:45:28,688:INFO: Dataset: zara1               Batch: 1/8	Loss 6.5871 (6.5871)
2022-11-09 17:45:28,712:INFO: Dataset: zara1               Batch: 2/8	Loss 8.4890 (7.5971)
2022-11-09 17:45:28,736:INFO: Dataset: zara1               Batch: 3/8	Loss 6.1304 (7.1131)
2022-11-09 17:45:28,759:INFO: Dataset: zara1               Batch: 4/8	Loss 8.6361 (7.5429)
2022-11-09 17:45:28,783:INFO: Dataset: zara1               Batch: 5/8	Loss 5.0087 (7.0558)
2022-11-09 17:45:28,808:INFO: Dataset: zara1               Batch: 6/8	Loss 5.3069 (6.7822)
2022-11-09 17:45:28,831:INFO: Dataset: zara1               Batch: 7/8	Loss 4.7817 (6.4775)
2022-11-09 17:45:28,852:INFO: Dataset: zara1               Batch: 8/8	Loss 4.6996 (6.2726)
2022-11-09 17:45:29,134:INFO: Dataset: zara2               Batch:  1/18	Loss 7.8823 (7.8823)
2022-11-09 17:45:29,160:INFO: Dataset: zara2               Batch:  2/18	Loss 6.4794 (7.1965)
2022-11-09 17:45:29,185:INFO: Dataset: zara2               Batch:  3/18	Loss 5.5466 (6.7036)
2022-11-09 17:45:29,210:INFO: Dataset: zara2               Batch:  4/18	Loss 4.7692 (6.2066)
2022-11-09 17:45:29,234:INFO: Dataset: zara2               Batch:  5/18	Loss 4.7660 (5.9249)
2022-11-09 17:45:29,261:INFO: Dataset: zara2               Batch:  6/18	Loss 5.1506 (5.8005)
2022-11-09 17:45:29,285:INFO: Dataset: zara2               Batch:  7/18	Loss 3.9661 (5.5573)
2022-11-09 17:45:29,308:INFO: Dataset: zara2               Batch:  8/18	Loss 3.1203 (5.2786)
2022-11-09 17:45:29,332:INFO: Dataset: zara2               Batch:  9/18	Loss 4.0149 (5.1345)
2022-11-09 17:45:29,356:INFO: Dataset: zara2               Batch: 10/18	Loss 3.4013 (4.9584)
2022-11-09 17:45:29,381:INFO: Dataset: zara2               Batch: 11/18	Loss 3.6748 (4.8366)
2022-11-09 17:45:29,408:INFO: Dataset: zara2               Batch: 12/18	Loss 3.2922 (4.7013)
2022-11-09 17:45:29,434:INFO: Dataset: zara2               Batch: 13/18	Loss 4.6581 (4.6983)
2022-11-09 17:45:29,459:INFO: Dataset: zara2               Batch: 14/18	Loss 3.3603 (4.5909)
2022-11-09 17:45:29,484:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6922 (4.5980)
2022-11-09 17:45:29,509:INFO: Dataset: zara2               Batch: 16/18	Loss 4.2947 (4.5779)
2022-11-09 17:45:29,533:INFO: Dataset: zara2               Batch: 17/18	Loss 3.3497 (4.5021)
2022-11-09 17:45:29,556:INFO: Dataset: zara2               Batch: 18/18	Loss 3.2577 (4.4467)
2022-11-09 17:45:29,604:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_57.pth.tar
2022-11-09 17:45:29,604:INFO: 
===> EPOCH: 58 (P1)
2022-11-09 17:45:29,605:INFO: - Computing loss (training)
2022-11-09 17:45:29,803:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0767 (3.0767)
2022-11-09 17:45:29,828:INFO: Dataset: hotel               Batch: 2/4	Loss 2.8706 (2.9746)
2022-11-09 17:45:29,854:INFO: Dataset: hotel               Batch: 3/4	Loss 3.6000 (3.1798)
2022-11-09 17:45:29,871:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7328 (2.9488)
2022-11-09 17:45:30,120:INFO: Dataset: univ                Batch:  1/15	Loss 2.9144 (2.9144)
2022-11-09 17:45:30,207:INFO: Dataset: univ                Batch:  2/15	Loss 2.8342 (2.8773)
2022-11-09 17:45:30,233:INFO: Dataset: univ                Batch:  3/15	Loss 4.1084 (3.2421)
2022-11-09 17:45:30,258:INFO: Dataset: univ                Batch:  4/15	Loss 3.6054 (3.3315)
2022-11-09 17:45:30,284:INFO: Dataset: univ                Batch:  5/15	Loss 2.7463 (3.2046)
2022-11-09 17:45:30,312:INFO: Dataset: univ                Batch:  6/15	Loss 2.7602 (3.1226)
2022-11-09 17:45:30,338:INFO: Dataset: univ                Batch:  7/15	Loss 3.1550 (3.1271)
2022-11-09 17:45:30,364:INFO: Dataset: univ                Batch:  8/15	Loss 2.6738 (3.0719)
2022-11-09 17:45:30,391:INFO: Dataset: univ                Batch:  9/15	Loss 2.8865 (3.0513)
2022-11-09 17:45:30,416:INFO: Dataset: univ                Batch: 10/15	Loss 3.0705 (3.0534)
2022-11-09 17:45:30,443:INFO: Dataset: univ                Batch: 11/15	Loss 2.7823 (3.0278)
2022-11-09 17:45:30,469:INFO: Dataset: univ                Batch: 12/15	Loss 2.8494 (3.0127)
2022-11-09 17:45:30,494:INFO: Dataset: univ                Batch: 13/15	Loss 2.8145 (2.9978)
2022-11-09 17:45:30,520:INFO: Dataset: univ                Batch: 14/15	Loss 2.7986 (2.9835)
2022-11-09 17:45:30,530:INFO: Dataset: univ                Batch: 15/15	Loss 0.5499 (2.9540)
2022-11-09 17:45:30,781:INFO: Dataset: zara1               Batch: 1/8	Loss 9.4275 (9.4275)
2022-11-09 17:45:30,806:INFO: Dataset: zara1               Batch: 2/8	Loss 8.2418 (8.8161)
2022-11-09 17:45:30,831:INFO: Dataset: zara1               Batch: 3/8	Loss 5.3670 (7.7204)
2022-11-09 17:45:30,856:INFO: Dataset: zara1               Batch: 4/8	Loss 8.1370 (7.8269)
2022-11-09 17:45:30,881:INFO: Dataset: zara1               Batch: 5/8	Loss 5.5535 (7.3596)
2022-11-09 17:45:30,905:INFO: Dataset: zara1               Batch: 6/8	Loss 5.5408 (7.0590)
2022-11-09 17:45:30,929:INFO: Dataset: zara1               Batch: 7/8	Loss 5.3311 (6.7998)
2022-11-09 17:45:30,950:INFO: Dataset: zara1               Batch: 8/8	Loss 3.8824 (6.4512)
2022-11-09 17:45:31,202:INFO: Dataset: zara2               Batch:  1/18	Loss 9.6478 (9.6478)
2022-11-09 17:45:31,228:INFO: Dataset: zara2               Batch:  2/18	Loss 6.1508 (7.9346)
2022-11-09 17:45:31,257:INFO: Dataset: zara2               Batch:  3/18	Loss 7.4965 (7.7853)
2022-11-09 17:45:31,281:INFO: Dataset: zara2               Batch:  4/18	Loss 5.7518 (7.3012)
2022-11-09 17:45:31,305:INFO: Dataset: zara2               Batch:  5/18	Loss 3.9067 (6.6550)
2022-11-09 17:45:31,333:INFO: Dataset: zara2               Batch:  6/18	Loss 4.0809 (6.2487)
2022-11-09 17:45:31,356:INFO: Dataset: zara2               Batch:  7/18	Loss 3.9910 (5.9384)
2022-11-09 17:45:31,380:INFO: Dataset: zara2               Batch:  8/18	Loss 3.3160 (5.5783)
2022-11-09 17:45:31,404:INFO: Dataset: zara2               Batch:  9/18	Loss 3.4445 (5.3298)
2022-11-09 17:45:31,428:INFO: Dataset: zara2               Batch: 10/18	Loss 4.0043 (5.2069)
2022-11-09 17:45:31,452:INFO: Dataset: zara2               Batch: 11/18	Loss 3.1554 (5.0154)
2022-11-09 17:45:31,478:INFO: Dataset: zara2               Batch: 12/18	Loss 3.5327 (4.8841)
2022-11-09 17:45:31,503:INFO: Dataset: zara2               Batch: 13/18	Loss 3.9576 (4.8158)
2022-11-09 17:45:31,526:INFO: Dataset: zara2               Batch: 14/18	Loss 3.0261 (4.6795)
2022-11-09 17:45:31,550:INFO: Dataset: zara2               Batch: 15/18	Loss 3.5256 (4.6128)
2022-11-09 17:45:31,573:INFO: Dataset: zara2               Batch: 16/18	Loss 5.2413 (4.6478)
2022-11-09 17:45:31,598:INFO: Dataset: zara2               Batch: 17/18	Loss 3.3814 (4.5651)
2022-11-09 17:45:31,620:INFO: Dataset: zara2               Batch: 18/18	Loss 3.4654 (4.5128)
2022-11-09 17:45:31,669:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_58.pth.tar
2022-11-09 17:45:31,670:INFO: 
===> EPOCH: 59 (P1)
2022-11-09 17:45:31,670:INFO: - Computing loss (training)
2022-11-09 17:45:31,872:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4766 (2.4766)
2022-11-09 17:45:31,899:INFO: Dataset: hotel               Batch: 2/4	Loss 2.5157 (2.4954)
2022-11-09 17:45:31,924:INFO: Dataset: hotel               Batch: 3/4	Loss 2.6447 (2.5446)
2022-11-09 17:45:31,942:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0397 (2.4520)
2022-11-09 17:45:32,224:INFO: Dataset: univ                Batch:  1/15	Loss 3.2787 (3.2787)
2022-11-09 17:45:32,254:INFO: Dataset: univ                Batch:  2/15	Loss 3.2952 (3.2879)
2022-11-09 17:45:32,281:INFO: Dataset: univ                Batch:  3/15	Loss 3.0572 (3.2154)
2022-11-09 17:45:32,306:INFO: Dataset: univ                Batch:  4/15	Loss 3.2722 (3.2287)
2022-11-09 17:45:32,331:INFO: Dataset: univ                Batch:  5/15	Loss 2.9403 (3.1707)
2022-11-09 17:45:32,359:INFO: Dataset: univ                Batch:  6/15	Loss 3.2047 (3.1763)
2022-11-09 17:45:32,385:INFO: Dataset: univ                Batch:  7/15	Loss 2.7337 (3.1122)
2022-11-09 17:45:32,411:INFO: Dataset: univ                Batch:  8/15	Loss 3.1643 (3.1186)
2022-11-09 17:45:32,439:INFO: Dataset: univ                Batch:  9/15	Loss 2.8071 (3.0827)
2022-11-09 17:45:32,468:INFO: Dataset: univ                Batch: 10/15	Loss 2.9783 (3.0723)
2022-11-09 17:45:32,496:INFO: Dataset: univ                Batch: 11/15	Loss 2.4607 (3.0135)
2022-11-09 17:45:32,523:INFO: Dataset: univ                Batch: 12/15	Loss 2.8121 (2.9962)
2022-11-09 17:45:32,548:INFO: Dataset: univ                Batch: 13/15	Loss 4.6079 (3.1015)
2022-11-09 17:45:32,574:INFO: Dataset: univ                Batch: 14/15	Loss 2.8126 (3.0816)
2022-11-09 17:45:32,584:INFO: Dataset: univ                Batch: 15/15	Loss 0.6101 (3.0491)
2022-11-09 17:45:32,831:INFO: Dataset: zara1               Batch: 1/8	Loss 14.8753 (14.8753)
2022-11-09 17:45:32,855:INFO: Dataset: zara1               Batch: 2/8	Loss 11.2232 (13.0075)
2022-11-09 17:45:32,879:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9977 (11.2322)
2022-11-09 17:45:32,905:INFO: Dataset: zara1               Batch: 4/8	Loss 7.7515 (10.3922)
2022-11-09 17:45:32,930:INFO: Dataset: zara1               Batch: 5/8	Loss 6.3998 (9.7110)
2022-11-09 17:45:32,954:INFO: Dataset: zara1               Batch: 6/8	Loss 4.5322 (8.7903)
2022-11-09 17:45:32,978:INFO: Dataset: zara1               Batch: 7/8	Loss 4.3752 (8.1372)
2022-11-09 17:45:32,999:INFO: Dataset: zara1               Batch: 8/8	Loss 4.5098 (7.7363)
2022-11-09 17:45:33,267:INFO: Dataset: zara2               Batch:  1/18	Loss 13.0301 (13.0301)
2022-11-09 17:45:33,297:INFO: Dataset: zara2               Batch:  2/18	Loss 11.7339 (12.3995)
2022-11-09 17:45:33,325:INFO: Dataset: zara2               Batch:  3/18	Loss 4.8683 (9.8003)
2022-11-09 17:45:33,348:INFO: Dataset: zara2               Batch:  4/18	Loss 5.6820 (8.8076)
2022-11-09 17:45:33,372:INFO: Dataset: zara2               Batch:  5/18	Loss 4.6376 (7.9977)
2022-11-09 17:45:33,400:INFO: Dataset: zara2               Batch:  6/18	Loss 4.1311 (7.3493)
2022-11-09 17:45:33,423:INFO: Dataset: zara2               Batch:  7/18	Loss 3.4391 (6.7707)
2022-11-09 17:45:33,447:INFO: Dataset: zara2               Batch:  8/18	Loss 3.9980 (6.4479)
2022-11-09 17:45:33,470:INFO: Dataset: zara2               Batch:  9/18	Loss 4.7039 (6.2571)
2022-11-09 17:45:33,495:INFO: Dataset: zara2               Batch: 10/18	Loss 3.5000 (5.9860)
2022-11-09 17:45:33,519:INFO: Dataset: zara2               Batch: 11/18	Loss 3.8204 (5.7840)
2022-11-09 17:45:33,545:INFO: Dataset: zara2               Batch: 12/18	Loss 3.0716 (5.5451)
2022-11-09 17:45:33,569:INFO: Dataset: zara2               Batch: 13/18	Loss 4.3980 (5.4562)
2022-11-09 17:45:33,594:INFO: Dataset: zara2               Batch: 14/18	Loss 3.2057 (5.2629)
2022-11-09 17:45:33,618:INFO: Dataset: zara2               Batch: 15/18	Loss 3.2601 (5.1259)
2022-11-09 17:45:33,643:INFO: Dataset: zara2               Batch: 16/18	Loss 3.3757 (5.0215)
2022-11-09 17:45:33,667:INFO: Dataset: zara2               Batch: 17/18	Loss 3.4157 (4.9281)
2022-11-09 17:45:33,689:INFO: Dataset: zara2               Batch: 18/18	Loss 2.8448 (4.8234)
2022-11-09 17:45:33,743:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_59.pth.tar
2022-11-09 17:45:33,743:INFO: 
===> EPOCH: 60 (P1)
2022-11-09 17:45:33,744:INFO: - Computing loss (training)
2022-11-09 17:45:33,946:INFO: Dataset: hotel               Batch: 1/4	Loss 3.4703 (3.4703)
2022-11-09 17:45:33,972:INFO: Dataset: hotel               Batch: 2/4	Loss 3.0939 (3.2772)
2022-11-09 17:45:33,998:INFO: Dataset: hotel               Batch: 3/4	Loss 4.3222 (3.6217)
2022-11-09 17:45:34,014:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0476 (3.3642)
2022-11-09 17:45:34,265:INFO: Dataset: univ                Batch:  1/15	Loss 4.0951 (4.0951)
2022-11-09 17:45:34,313:INFO: Dataset: univ                Batch:  2/15	Loss 3.1374 (3.6169)
2022-11-09 17:45:34,339:INFO: Dataset: univ                Batch:  3/15	Loss 3.8464 (3.6874)
2022-11-09 17:45:34,364:INFO: Dataset: univ                Batch:  4/15	Loss 3.0985 (3.5409)
2022-11-09 17:45:34,390:INFO: Dataset: univ                Batch:  5/15	Loss 2.8030 (3.3754)
2022-11-09 17:45:34,419:INFO: Dataset: univ                Batch:  6/15	Loss 3.1950 (3.3485)
2022-11-09 17:45:34,445:INFO: Dataset: univ                Batch:  7/15	Loss 2.7356 (3.2549)
2022-11-09 17:45:34,471:INFO: Dataset: univ                Batch:  8/15	Loss 4.6184 (3.4184)
2022-11-09 17:45:34,500:INFO: Dataset: univ                Batch:  9/15	Loss 3.7847 (3.4547)
2022-11-09 17:45:34,525:INFO: Dataset: univ                Batch: 10/15	Loss 2.9541 (3.4082)
2022-11-09 17:45:34,551:INFO: Dataset: univ                Batch: 11/15	Loss 2.8324 (3.3540)
2022-11-09 17:45:34,578:INFO: Dataset: univ                Batch: 12/15	Loss 2.6148 (3.2877)
2022-11-09 17:45:34,604:INFO: Dataset: univ                Batch: 13/15	Loss 2.6663 (3.2366)
2022-11-09 17:45:34,629:INFO: Dataset: univ                Batch: 14/15	Loss 3.0006 (3.2203)
2022-11-09 17:45:34,639:INFO: Dataset: univ                Batch: 15/15	Loss 0.5989 (3.1869)
2022-11-09 17:45:34,903:INFO: Dataset: zara1               Batch: 1/8	Loss 10.8983 (10.8983)
2022-11-09 17:45:34,931:INFO: Dataset: zara1               Batch: 2/8	Loss 12.4712 (11.6157)
2022-11-09 17:45:34,955:INFO: Dataset: zara1               Batch: 3/8	Loss 8.2321 (10.4237)
2022-11-09 17:45:34,978:INFO: Dataset: zara1               Batch: 4/8	Loss 4.9023 (9.1418)
2022-11-09 17:45:35,002:INFO: Dataset: zara1               Batch: 5/8	Loss 4.9093 (8.2717)
2022-11-09 17:45:35,027:INFO: Dataset: zara1               Batch: 6/8	Loss 3.9994 (7.4917)
2022-11-09 17:45:35,051:INFO: Dataset: zara1               Batch: 7/8	Loss 4.9553 (7.1216)
2022-11-09 17:45:35,072:INFO: Dataset: zara1               Batch: 8/8	Loss 3.9989 (6.7814)
2022-11-09 17:45:35,334:INFO: Dataset: zara2               Batch:  1/18	Loss 6.8234 (6.8234)
2022-11-09 17:45:35,363:INFO: Dataset: zara2               Batch:  2/18	Loss 8.3648 (7.5838)
2022-11-09 17:45:35,387:INFO: Dataset: zara2               Batch:  3/18	Loss 6.0041 (7.0547)
2022-11-09 17:45:35,414:INFO: Dataset: zara2               Batch:  4/18	Loss 6.1131 (6.8189)
2022-11-09 17:45:35,438:INFO: Dataset: zara2               Batch:  5/18	Loss 4.0315 (6.2608)
2022-11-09 17:45:35,464:INFO: Dataset: zara2               Batch:  6/18	Loss 4.3165 (5.9452)
2022-11-09 17:45:35,487:INFO: Dataset: zara2               Batch:  7/18	Loss 3.2042 (5.5695)
2022-11-09 17:45:35,511:INFO: Dataset: zara2               Batch:  8/18	Loss 3.3217 (5.2758)
2022-11-09 17:45:35,534:INFO: Dataset: zara2               Batch:  9/18	Loss 3.1896 (5.0372)
2022-11-09 17:45:35,559:INFO: Dataset: zara2               Batch: 10/18	Loss 3.4850 (4.8746)
2022-11-09 17:45:35,584:INFO: Dataset: zara2               Batch: 11/18	Loss 3.2903 (4.7301)
2022-11-09 17:45:35,610:INFO: Dataset: zara2               Batch: 12/18	Loss 3.3538 (4.6035)
2022-11-09 17:45:35,634:INFO: Dataset: zara2               Batch: 13/18	Loss 5.1131 (4.6370)
2022-11-09 17:45:35,659:INFO: Dataset: zara2               Batch: 14/18	Loss 3.5759 (4.5532)
2022-11-09 17:45:35,683:INFO: Dataset: zara2               Batch: 15/18	Loss 4.0505 (4.5195)
2022-11-09 17:45:35,708:INFO: Dataset: zara2               Batch: 16/18	Loss 2.7894 (4.4269)
2022-11-09 17:45:35,732:INFO: Dataset: zara2               Batch: 17/18	Loss 3.8974 (4.3948)
2022-11-09 17:45:35,755:INFO: Dataset: zara2               Batch: 18/18	Loss 3.2382 (4.3447)
2022-11-09 17:45:35,805:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_60.pth.tar
2022-11-09 17:45:35,805:INFO: 
===> EPOCH: 61 (P1)
2022-11-09 17:45:35,806:INFO: - Computing loss (training)
2022-11-09 17:45:36,013:INFO: Dataset: hotel               Batch: 1/4	Loss 2.3201 (2.3201)
2022-11-09 17:45:36,038:INFO: Dataset: hotel               Batch: 2/4	Loss 2.9990 (2.6518)
2022-11-09 17:45:36,063:INFO: Dataset: hotel               Batch: 3/4	Loss 2.3761 (2.5654)
2022-11-09 17:45:36,080:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4830 (2.3855)
2022-11-09 17:45:36,351:INFO: Dataset: univ                Batch:  1/15	Loss 2.7497 (2.7497)
2022-11-09 17:45:36,380:INFO: Dataset: univ                Batch:  2/15	Loss 3.0550 (2.9128)
2022-11-09 17:45:36,408:INFO: Dataset: univ                Batch:  3/15	Loss 2.6895 (2.8332)
2022-11-09 17:45:36,434:INFO: Dataset: univ                Batch:  4/15	Loss 2.7995 (2.8247)
2022-11-09 17:45:36,461:INFO: Dataset: univ                Batch:  5/15	Loss 2.8500 (2.8298)
2022-11-09 17:45:36,492:INFO: Dataset: univ                Batch:  6/15	Loss 2.6572 (2.8017)
2022-11-09 17:45:36,518:INFO: Dataset: univ                Batch:  7/15	Loss 3.0329 (2.8343)
2022-11-09 17:45:36,544:INFO: Dataset: univ                Batch:  8/15	Loss 2.4820 (2.7922)
2022-11-09 17:45:36,570:INFO: Dataset: univ                Batch:  9/15	Loss 2.9225 (2.8070)
2022-11-09 17:45:36,598:INFO: Dataset: univ                Batch: 10/15	Loss 2.4943 (2.7732)
2022-11-09 17:45:36,627:INFO: Dataset: univ                Batch: 11/15	Loss 2.7337 (2.7698)
2022-11-09 17:45:36,654:INFO: Dataset: univ                Batch: 12/15	Loss 2.5171 (2.7486)
2022-11-09 17:45:36,679:INFO: Dataset: univ                Batch: 13/15	Loss 3.2258 (2.7850)
2022-11-09 17:45:36,705:INFO: Dataset: univ                Batch: 14/15	Loss 2.4571 (2.7589)
2022-11-09 17:45:36,715:INFO: Dataset: univ                Batch: 15/15	Loss 0.8378 (2.7416)
2022-11-09 17:45:36,975:INFO: Dataset: zara1               Batch: 1/8	Loss 10.0765 (10.0765)
2022-11-09 17:45:37,003:INFO: Dataset: zara1               Batch: 2/8	Loss 9.7607 (9.9173)
2022-11-09 17:45:37,027:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9287 (9.3067)
2022-11-09 17:45:37,050:INFO: Dataset: zara1               Batch: 4/8	Loss 8.3171 (9.0606)
2022-11-09 17:45:37,073:INFO: Dataset: zara1               Batch: 5/8	Loss 7.2980 (8.7060)
2022-11-09 17:45:37,098:INFO: Dataset: zara1               Batch: 6/8	Loss 5.2596 (8.1781)
2022-11-09 17:45:37,121:INFO: Dataset: zara1               Batch: 7/8	Loss 4.1696 (7.5430)
2022-11-09 17:45:37,143:INFO: Dataset: zara1               Batch: 8/8	Loss 3.4049 (7.0203)
2022-11-09 17:45:37,401:INFO: Dataset: zara2               Batch:  1/18	Loss 5.8770 (5.8770)
2022-11-09 17:45:37,436:INFO: Dataset: zara2               Batch:  2/18	Loss 6.3879 (6.1350)
2022-11-09 17:45:37,462:INFO: Dataset: zara2               Batch:  3/18	Loss 8.2636 (6.8174)
2022-11-09 17:45:37,486:INFO: Dataset: zara2               Batch:  4/18	Loss 5.1185 (6.3815)
2022-11-09 17:45:37,510:INFO: Dataset: zara2               Batch:  5/18	Loss 5.7952 (6.2735)
2022-11-09 17:45:37,539:INFO: Dataset: zara2               Batch:  6/18	Loss 4.8850 (6.0475)
2022-11-09 17:45:37,562:INFO: Dataset: zara2               Batch:  7/18	Loss 4.2310 (5.8046)
2022-11-09 17:45:37,586:INFO: Dataset: zara2               Batch:  8/18	Loss 3.9949 (5.5685)
2022-11-09 17:45:37,609:INFO: Dataset: zara2               Batch:  9/18	Loss 3.0984 (5.3143)
2022-11-09 17:45:37,634:INFO: Dataset: zara2               Batch: 10/18	Loss 3.9842 (5.1763)
2022-11-09 17:45:37,659:INFO: Dataset: zara2               Batch: 11/18	Loss 4.5539 (5.1191)
2022-11-09 17:45:37,684:INFO: Dataset: zara2               Batch: 12/18	Loss 2.8162 (4.9272)
2022-11-09 17:45:37,708:INFO: Dataset: zara2               Batch: 13/18	Loss 3.2930 (4.7974)
2022-11-09 17:45:37,732:INFO: Dataset: zara2               Batch: 14/18	Loss 4.6060 (4.7828)
2022-11-09 17:45:37,757:INFO: Dataset: zara2               Batch: 15/18	Loss 2.9691 (4.6534)
2022-11-09 17:45:37,781:INFO: Dataset: zara2               Batch: 16/18	Loss 3.9288 (4.6104)
2022-11-09 17:45:37,806:INFO: Dataset: zara2               Batch: 17/18	Loss 3.0535 (4.5004)
2022-11-09 17:45:37,829:INFO: Dataset: zara2               Batch: 18/18	Loss 2.6497 (4.4153)
2022-11-09 17:45:37,880:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_61.pth.tar
2022-11-09 17:45:37,881:INFO: 
===> EPOCH: 62 (P1)
2022-11-09 17:45:37,881:INFO: - Computing loss (training)
2022-11-09 17:45:38,092:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4388 (2.4388)
2022-11-09 17:45:38,117:INFO: Dataset: hotel               Batch: 2/4	Loss 2.5868 (2.5158)
2022-11-09 17:45:38,141:INFO: Dataset: hotel               Batch: 3/4	Loss 3.3169 (2.7807)
2022-11-09 17:45:38,158:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3414 (2.5358)
2022-11-09 17:45:38,417:INFO: Dataset: univ                Batch:  1/15	Loss 2.6857 (2.6857)
2022-11-09 17:45:38,483:INFO: Dataset: univ                Batch:  2/15	Loss 2.7787 (2.7311)
2022-11-09 17:45:38,509:INFO: Dataset: univ                Batch:  3/15	Loss 2.6292 (2.6979)
2022-11-09 17:45:38,534:INFO: Dataset: univ                Batch:  4/15	Loss 2.4427 (2.6284)
2022-11-09 17:45:38,559:INFO: Dataset: univ                Batch:  5/15	Loss 2.6005 (2.6227)
2022-11-09 17:45:38,587:INFO: Dataset: univ                Batch:  6/15	Loss 2.6790 (2.6322)
2022-11-09 17:45:38,612:INFO: Dataset: univ                Batch:  7/15	Loss 2.7532 (2.6491)
2022-11-09 17:45:38,638:INFO: Dataset: univ                Batch:  8/15	Loss 2.4566 (2.6225)
2022-11-09 17:45:38,664:INFO: Dataset: univ                Batch:  9/15	Loss 2.7367 (2.6349)
2022-11-09 17:45:38,691:INFO: Dataset: univ                Batch: 10/15	Loss 2.7057 (2.6419)
2022-11-09 17:45:38,718:INFO: Dataset: univ                Batch: 11/15	Loss 2.4374 (2.6220)
2022-11-09 17:45:38,744:INFO: Dataset: univ                Batch: 12/15	Loss 2.6816 (2.6270)
2022-11-09 17:45:38,769:INFO: Dataset: univ                Batch: 13/15	Loss 2.6702 (2.6304)
2022-11-09 17:45:38,795:INFO: Dataset: univ                Batch: 14/15	Loss 2.7717 (2.6403)
2022-11-09 17:45:38,805:INFO: Dataset: univ                Batch: 15/15	Loss 0.5112 (2.6147)
2022-11-09 17:45:39,062:INFO: Dataset: zara1               Batch: 1/8	Loss 11.1179 (11.1179)
2022-11-09 17:45:39,086:INFO: Dataset: zara1               Batch: 2/8	Loss 5.6490 (8.2065)
2022-11-09 17:45:39,110:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9293 (8.1187)
2022-11-09 17:45:39,135:INFO: Dataset: zara1               Batch: 4/8	Loss 5.9862 (7.5782)
2022-11-09 17:45:39,158:INFO: Dataset: zara1               Batch: 5/8	Loss 4.0863 (6.8745)
2022-11-09 17:45:39,183:INFO: Dataset: zara1               Batch: 6/8	Loss 3.8530 (6.3518)
2022-11-09 17:45:39,206:INFO: Dataset: zara1               Batch: 7/8	Loss 3.9761 (5.9739)
2022-11-09 17:45:39,227:INFO: Dataset: zara1               Batch: 8/8	Loss 3.7542 (5.7297)
2022-11-09 17:45:39,487:INFO: Dataset: zara2               Batch:  1/18	Loss 4.1328 (4.1328)
2022-11-09 17:45:39,511:INFO: Dataset: zara2               Batch:  2/18	Loss 7.9762 (6.0322)
2022-11-09 17:45:39,536:INFO: Dataset: zara2               Batch:  3/18	Loss 6.3952 (6.1508)
2022-11-09 17:45:39,563:INFO: Dataset: zara2               Batch:  4/18	Loss 5.8013 (6.0669)
2022-11-09 17:45:39,589:INFO: Dataset: zara2               Batch:  5/18	Loss 4.2016 (5.6835)
2022-11-09 17:45:39,615:INFO: Dataset: zara2               Batch:  6/18	Loss 3.4875 (5.3280)
2022-11-09 17:45:39,639:INFO: Dataset: zara2               Batch:  7/18	Loss 3.4303 (5.0712)
2022-11-09 17:45:39,662:INFO: Dataset: zara2               Batch:  8/18	Loss 2.9727 (4.7870)
2022-11-09 17:45:39,686:INFO: Dataset: zara2               Batch:  9/18	Loss 3.4052 (4.6191)
2022-11-09 17:45:39,710:INFO: Dataset: zara2               Batch: 10/18	Loss 3.0276 (4.4577)
2022-11-09 17:45:39,735:INFO: Dataset: zara2               Batch: 11/18	Loss 4.7402 (4.4835)
2022-11-09 17:45:39,760:INFO: Dataset: zara2               Batch: 12/18	Loss 4.9120 (4.5217)
2022-11-09 17:45:39,785:INFO: Dataset: zara2               Batch: 13/18	Loss 2.7424 (4.3979)
2022-11-09 17:45:39,809:INFO: Dataset: zara2               Batch: 14/18	Loss 3.4606 (4.3342)
2022-11-09 17:45:39,834:INFO: Dataset: zara2               Batch: 15/18	Loss 3.8250 (4.3035)
2022-11-09 17:45:39,858:INFO: Dataset: zara2               Batch: 16/18	Loss 3.4675 (4.2564)
2022-11-09 17:45:39,883:INFO: Dataset: zara2               Batch: 17/18	Loss 2.9709 (4.1759)
2022-11-09 17:45:39,905:INFO: Dataset: zara2               Batch: 18/18	Loss 2.4494 (4.0915)
2022-11-09 17:45:39,954:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_62.pth.tar
2022-11-09 17:45:39,954:INFO: 
===> EPOCH: 63 (P1)
2022-11-09 17:45:39,955:INFO: - Computing loss (training)
2022-11-09 17:45:40,157:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6796 (2.6796)
2022-11-09 17:45:40,183:INFO: Dataset: hotel               Batch: 2/4	Loss 2.9228 (2.7954)
2022-11-09 17:45:40,209:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7224 (2.7712)
2022-11-09 17:45:40,226:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5067 (2.5494)
2022-11-09 17:45:40,478:INFO: Dataset: univ                Batch:  1/15	Loss 2.9335 (2.9335)
2022-11-09 17:45:40,528:INFO: Dataset: univ                Batch:  2/15	Loss 3.4259 (3.2031)
2022-11-09 17:45:40,554:INFO: Dataset: univ                Batch:  3/15	Loss 2.6265 (3.0137)
2022-11-09 17:45:40,582:INFO: Dataset: univ                Batch:  4/15	Loss 2.4326 (2.8683)
2022-11-09 17:45:40,608:INFO: Dataset: univ                Batch:  5/15	Loss 2.5885 (2.8110)
2022-11-09 17:45:40,639:INFO: Dataset: univ                Batch:  6/15	Loss 2.8092 (2.8107)
2022-11-09 17:45:40,665:INFO: Dataset: univ                Batch:  7/15	Loss 2.4638 (2.7662)
2022-11-09 17:45:40,691:INFO: Dataset: univ                Batch:  8/15	Loss 2.4682 (2.7283)
2022-11-09 17:45:40,719:INFO: Dataset: univ                Batch:  9/15	Loss 2.4981 (2.7020)
2022-11-09 17:45:40,746:INFO: Dataset: univ                Batch: 10/15	Loss 2.2043 (2.6465)
2022-11-09 17:45:40,773:INFO: Dataset: univ                Batch: 11/15	Loss 2.2831 (2.6122)
2022-11-09 17:45:40,801:INFO: Dataset: univ                Batch: 12/15	Loss 2.9741 (2.6404)
2022-11-09 17:45:40,827:INFO: Dataset: univ                Batch: 13/15	Loss 2.4799 (2.6278)
2022-11-09 17:45:40,853:INFO: Dataset: univ                Batch: 14/15	Loss 2.2353 (2.6001)
2022-11-09 17:45:40,863:INFO: Dataset: univ                Batch: 15/15	Loss 0.5350 (2.5765)
2022-11-09 17:45:41,117:INFO: Dataset: zara1               Batch: 1/8	Loss 10.4273 (10.4273)
2022-11-09 17:45:41,142:INFO: Dataset: zara1               Batch: 2/8	Loss 7.6538 (9.0376)
2022-11-09 17:45:41,170:INFO: Dataset: zara1               Batch: 3/8	Loss 8.9002 (8.9903)
2022-11-09 17:45:41,194:INFO: Dataset: zara1               Batch: 4/8	Loss 5.7968 (8.1315)
2022-11-09 17:45:41,217:INFO: Dataset: zara1               Batch: 5/8	Loss 6.0738 (7.7397)
2022-11-09 17:45:41,242:INFO: Dataset: zara1               Batch: 6/8	Loss 4.8623 (7.2661)
2022-11-09 17:45:41,266:INFO: Dataset: zara1               Batch: 7/8	Loss 3.9598 (6.8107)
2022-11-09 17:45:41,287:INFO: Dataset: zara1               Batch: 8/8	Loss 3.8333 (6.4612)
2022-11-09 17:45:41,545:INFO: Dataset: zara2               Batch:  1/18	Loss 8.6596 (8.6596)
2022-11-09 17:45:41,570:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1590 (7.8810)
2022-11-09 17:45:41,598:INFO: Dataset: zara2               Batch:  3/18	Loss 5.3763 (6.9632)
2022-11-09 17:45:41,622:INFO: Dataset: zara2               Batch:  4/18	Loss 4.9000 (6.4447)
2022-11-09 17:45:41,646:INFO: Dataset: zara2               Batch:  5/18	Loss 3.3289 (5.8324)
2022-11-09 17:45:41,673:INFO: Dataset: zara2               Batch:  6/18	Loss 5.0903 (5.7071)
2022-11-09 17:45:41,697:INFO: Dataset: zara2               Batch:  7/18	Loss 3.1391 (5.3058)
2022-11-09 17:45:41,720:INFO: Dataset: zara2               Batch:  8/18	Loss 3.0515 (5.0357)
2022-11-09 17:45:41,744:INFO: Dataset: zara2               Batch:  9/18	Loss 2.7899 (4.7725)
2022-11-09 17:45:41,769:INFO: Dataset: zara2               Batch: 10/18	Loss 3.1222 (4.6203)
2022-11-09 17:45:41,796:INFO: Dataset: zara2               Batch: 11/18	Loss 2.8692 (4.4575)
2022-11-09 17:45:41,823:INFO: Dataset: zara2               Batch: 12/18	Loss 3.2118 (4.3659)
2022-11-09 17:45:41,847:INFO: Dataset: zara2               Batch: 13/18	Loss 2.9490 (4.2598)
2022-11-09 17:45:41,871:INFO: Dataset: zara2               Batch: 14/18	Loss 2.6655 (4.1418)
2022-11-09 17:45:41,896:INFO: Dataset: zara2               Batch: 15/18	Loss 2.6411 (4.0375)
2022-11-09 17:45:41,921:INFO: Dataset: zara2               Batch: 16/18	Loss 3.1576 (3.9813)
2022-11-09 17:45:41,946:INFO: Dataset: zara2               Batch: 17/18	Loss 3.2989 (3.9392)
2022-11-09 17:45:41,968:INFO: Dataset: zara2               Batch: 18/18	Loss 3.8965 (3.9371)
2022-11-09 17:45:42,018:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_63.pth.tar
2022-11-09 17:45:42,018:INFO: 
===> EPOCH: 64 (P1)
2022-11-09 17:45:42,018:INFO: - Computing loss (training)
2022-11-09 17:45:42,233:INFO: Dataset: hotel               Batch: 1/4	Loss 2.3512 (2.3512)
2022-11-09 17:45:42,258:INFO: Dataset: hotel               Batch: 2/4	Loss 2.3396 (2.3454)
2022-11-09 17:45:42,282:INFO: Dataset: hotel               Batch: 3/4	Loss 3.0768 (2.5714)
2022-11-09 17:45:42,299:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8870 (2.4568)
2022-11-09 17:45:42,560:INFO: Dataset: univ                Batch:  1/15	Loss 2.4815 (2.4815)
2022-11-09 17:45:42,587:INFO: Dataset: univ                Batch:  2/15	Loss 2.4845 (2.4830)
2022-11-09 17:45:42,617:INFO: Dataset: univ                Batch:  3/15	Loss 2.4810 (2.4824)
2022-11-09 17:45:42,643:INFO: Dataset: univ                Batch:  4/15	Loss 2.5375 (2.4959)
2022-11-09 17:45:42,668:INFO: Dataset: univ                Batch:  5/15	Loss 2.8675 (2.5655)
2022-11-09 17:45:42,697:INFO: Dataset: univ                Batch:  6/15	Loss 2.9232 (2.6199)
2022-11-09 17:45:42,722:INFO: Dataset: univ                Batch:  7/15	Loss 2.3855 (2.5827)
2022-11-09 17:45:42,748:INFO: Dataset: univ                Batch:  8/15	Loss 2.7067 (2.5973)
2022-11-09 17:45:42,773:INFO: Dataset: univ                Batch:  9/15	Loss 2.8447 (2.6219)
2022-11-09 17:45:42,809:INFO: Dataset: univ                Batch: 10/15	Loss 2.3400 (2.5912)
2022-11-09 17:45:42,838:INFO: Dataset: univ                Batch: 11/15	Loss 3.0151 (2.6352)
2022-11-09 17:45:42,866:INFO: Dataset: univ                Batch: 12/15	Loss 3.1736 (2.6758)
2022-11-09 17:45:42,891:INFO: Dataset: univ                Batch: 13/15	Loss 2.3707 (2.6529)
2022-11-09 17:45:42,917:INFO: Dataset: univ                Batch: 14/15	Loss 2.2835 (2.6240)
2022-11-09 17:45:42,928:INFO: Dataset: univ                Batch: 15/15	Loss 0.3630 (2.5846)
2022-11-09 17:45:43,197:INFO: Dataset: zara1               Batch: 1/8	Loss 6.5467 (6.5467)
2022-11-09 17:45:43,221:INFO: Dataset: zara1               Batch: 2/8	Loss 5.4455 (5.9972)
2022-11-09 17:45:43,246:INFO: Dataset: zara1               Batch: 3/8	Loss 6.4592 (6.1539)
2022-11-09 17:45:43,269:INFO: Dataset: zara1               Batch: 4/8	Loss 7.0170 (6.3744)
2022-11-09 17:45:43,293:INFO: Dataset: zara1               Batch: 5/8	Loss 4.6906 (6.0339)
2022-11-09 17:45:43,318:INFO: Dataset: zara1               Batch: 6/8	Loss 3.5073 (5.6473)
2022-11-09 17:45:43,341:INFO: Dataset: zara1               Batch: 7/8	Loss 3.6885 (5.4086)
2022-11-09 17:45:43,362:INFO: Dataset: zara1               Batch: 8/8	Loss 2.9581 (5.1274)
2022-11-09 17:45:43,625:INFO: Dataset: zara2               Batch:  1/18	Loss 10.6733 (10.6733)
2022-11-09 17:45:43,651:INFO: Dataset: zara2               Batch:  2/18	Loss 8.3363 (9.5656)
2022-11-09 17:45:43,679:INFO: Dataset: zara2               Batch:  3/18	Loss 4.1518 (7.8358)
2022-11-09 17:45:43,704:INFO: Dataset: zara2               Batch:  4/18	Loss 4.1536 (6.8511)
2022-11-09 17:45:43,729:INFO: Dataset: zara2               Batch:  5/18	Loss 5.3298 (6.5494)
2022-11-09 17:45:43,755:INFO: Dataset: zara2               Batch:  6/18	Loss 3.2848 (5.9903)
2022-11-09 17:45:43,779:INFO: Dataset: zara2               Batch:  7/18	Loss 2.5270 (5.5153)
2022-11-09 17:45:43,803:INFO: Dataset: zara2               Batch:  8/18	Loss 2.7494 (5.1512)
2022-11-09 17:45:43,826:INFO: Dataset: zara2               Batch:  9/18	Loss 3.5510 (4.9697)
2022-11-09 17:45:43,851:INFO: Dataset: zara2               Batch: 10/18	Loss 2.3648 (4.7448)
2022-11-09 17:45:43,875:INFO: Dataset: zara2               Batch: 11/18	Loss 3.9698 (4.6698)
2022-11-09 17:45:43,903:INFO: Dataset: zara2               Batch: 12/18	Loss 2.8385 (4.5073)
2022-11-09 17:45:43,927:INFO: Dataset: zara2               Batch: 13/18	Loss 3.8521 (4.4528)
2022-11-09 17:45:43,951:INFO: Dataset: zara2               Batch: 14/18	Loss 3.4721 (4.3849)
2022-11-09 17:45:43,974:INFO: Dataset: zara2               Batch: 15/18	Loss 4.1425 (4.3700)
2022-11-09 17:45:43,999:INFO: Dataset: zara2               Batch: 16/18	Loss 4.0095 (4.3492)
2022-11-09 17:45:44,024:INFO: Dataset: zara2               Batch: 17/18	Loss 3.0264 (4.2621)
2022-11-09 17:45:44,049:INFO: Dataset: zara2               Batch: 18/18	Loss 2.4993 (4.1805)
2022-11-09 17:45:44,100:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_64.pth.tar
2022-11-09 17:45:44,100:INFO: 
===> EPOCH: 65 (P1)
2022-11-09 17:45:44,100:INFO: - Computing loss (training)
2022-11-09 17:45:44,336:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4363 (2.4363)
2022-11-09 17:45:44,364:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1568 (2.2926)
2022-11-09 17:45:44,391:INFO: Dataset: hotel               Batch: 3/4	Loss 2.5231 (2.3684)
2022-11-09 17:45:44,409:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4463 (2.2079)
2022-11-09 17:45:44,668:INFO: Dataset: univ                Batch:  1/15	Loss 3.3835 (3.3835)
2022-11-09 17:45:44,696:INFO: Dataset: univ                Batch:  2/15	Loss 5.6574 (4.5265)
2022-11-09 17:45:44,722:INFO: Dataset: univ                Batch:  3/15	Loss 2.7671 (3.9282)
2022-11-09 17:45:44,751:INFO: Dataset: univ                Batch:  4/15	Loss 2.4362 (3.5727)
2022-11-09 17:45:44,778:INFO: Dataset: univ                Batch:  5/15	Loss 2.7290 (3.4045)
2022-11-09 17:45:44,807:INFO: Dataset: univ                Batch:  6/15	Loss 2.2932 (3.2180)
2022-11-09 17:45:44,832:INFO: Dataset: univ                Batch:  7/15	Loss 2.4292 (3.1135)
2022-11-09 17:45:44,859:INFO: Dataset: univ                Batch:  8/15	Loss 2.1564 (2.9851)
2022-11-09 17:45:44,885:INFO: Dataset: univ                Batch:  9/15	Loss 2.1060 (2.8852)
2022-11-09 17:45:44,912:INFO: Dataset: univ                Batch: 10/15	Loss 2.5887 (2.8564)
2022-11-09 17:45:44,939:INFO: Dataset: univ                Batch: 11/15	Loss 2.3155 (2.8082)
2022-11-09 17:45:44,967:INFO: Dataset: univ                Batch: 12/15	Loss 2.6329 (2.7941)
2022-11-09 17:45:44,993:INFO: Dataset: univ                Batch: 13/15	Loss 2.2030 (2.7484)
2022-11-09 17:45:45,020:INFO: Dataset: univ                Batch: 14/15	Loss 2.3817 (2.7218)
2022-11-09 17:45:45,029:INFO: Dataset: univ                Batch: 15/15	Loss 0.3909 (2.6935)
2022-11-09 17:45:45,267:INFO: Dataset: zara1               Batch: 1/8	Loss 11.3171 (11.3171)
2022-11-09 17:45:45,293:INFO: Dataset: zara1               Batch: 2/8	Loss 9.5896 (10.4763)
2022-11-09 17:45:45,318:INFO: Dataset: zara1               Batch: 3/8	Loss 6.4072 (9.0567)
2022-11-09 17:45:45,346:INFO: Dataset: zara1               Batch: 4/8	Loss 9.0429 (9.0533)
2022-11-09 17:45:45,370:INFO: Dataset: zara1               Batch: 5/8	Loss 5.9377 (8.4794)
2022-11-09 17:45:45,395:INFO: Dataset: zara1               Batch: 6/8	Loss 4.7217 (7.8862)
2022-11-09 17:45:45,420:INFO: Dataset: zara1               Batch: 7/8	Loss 3.7198 (7.2656)
2022-11-09 17:45:45,442:INFO: Dataset: zara1               Batch: 8/8	Loss 4.7616 (7.0179)
2022-11-09 17:45:45,694:INFO: Dataset: zara2               Batch:  1/18	Loss 9.0520 (9.0520)
2022-11-09 17:45:45,719:INFO: Dataset: zara2               Batch:  2/18	Loss 7.0376 (8.0153)
2022-11-09 17:45:45,747:INFO: Dataset: zara2               Batch:  3/18	Loss 5.6868 (7.2437)
2022-11-09 17:45:45,771:INFO: Dataset: zara2               Batch:  4/18	Loss 6.0874 (6.9414)
2022-11-09 17:45:45,796:INFO: Dataset: zara2               Batch:  5/18	Loss 4.1495 (6.4175)
2022-11-09 17:45:45,823:INFO: Dataset: zara2               Batch:  6/18	Loss 4.5736 (6.1124)
2022-11-09 17:45:45,847:INFO: Dataset: zara2               Batch:  7/18	Loss 3.8646 (5.7820)
2022-11-09 17:45:45,870:INFO: Dataset: zara2               Batch:  8/18	Loss 3.0013 (5.4417)
2022-11-09 17:45:45,894:INFO: Dataset: zara2               Batch:  9/18	Loss 2.7232 (5.1581)
2022-11-09 17:45:45,919:INFO: Dataset: zara2               Batch: 10/18	Loss 2.7109 (4.8860)
2022-11-09 17:45:45,945:INFO: Dataset: zara2               Batch: 11/18	Loss 2.5898 (4.6771)
2022-11-09 17:45:45,970:INFO: Dataset: zara2               Batch: 12/18	Loss 3.2385 (4.5518)
2022-11-09 17:45:45,994:INFO: Dataset: zara2               Batch: 13/18	Loss 2.5201 (4.4002)
2022-11-09 17:45:46,018:INFO: Dataset: zara2               Batch: 14/18	Loss 3.5976 (4.3445)
2022-11-09 17:45:46,043:INFO: Dataset: zara2               Batch: 15/18	Loss 3.8744 (4.3153)
2022-11-09 17:45:46,068:INFO: Dataset: zara2               Batch: 16/18	Loss 2.3551 (4.1907)
2022-11-09 17:45:46,092:INFO: Dataset: zara2               Batch: 17/18	Loss 4.4217 (4.2041)
2022-11-09 17:45:46,115:INFO: Dataset: zara2               Batch: 18/18	Loss 3.6602 (4.1798)
2022-11-09 17:45:46,166:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_65.pth.tar
2022-11-09 17:45:46,166:INFO: 
===> EPOCH: 66 (P1)
2022-11-09 17:45:46,167:INFO: - Computing loss (training)
2022-11-09 17:45:46,371:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8196 (1.8196)
2022-11-09 17:45:46,397:INFO: Dataset: hotel               Batch: 2/4	Loss 2.6014 (2.2133)
2022-11-09 17:45:46,421:INFO: Dataset: hotel               Batch: 3/4	Loss 2.2645 (2.2306)
2022-11-09 17:45:46,439:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7956 (2.1565)
2022-11-09 17:45:46,697:INFO: Dataset: univ                Batch:  1/15	Loss 2.8739 (2.8739)
2022-11-09 17:45:46,725:INFO: Dataset: univ                Batch:  2/15	Loss 2.6055 (2.7384)
2022-11-09 17:45:46,752:INFO: Dataset: univ                Batch:  3/15	Loss 3.1339 (2.8747)
2022-11-09 17:45:46,779:INFO: Dataset: univ                Batch:  4/15	Loss 3.3642 (2.9929)
2022-11-09 17:45:46,804:INFO: Dataset: univ                Batch:  5/15	Loss 2.5151 (2.8887)
2022-11-09 17:45:46,833:INFO: Dataset: univ                Batch:  6/15	Loss 3.0562 (2.9167)
2022-11-09 17:45:46,858:INFO: Dataset: univ                Batch:  7/15	Loss 2.2886 (2.8306)
2022-11-09 17:45:46,883:INFO: Dataset: univ                Batch:  8/15	Loss 2.2517 (2.7610)
2022-11-09 17:45:46,908:INFO: Dataset: univ                Batch:  9/15	Loss 2.6910 (2.7537)
2022-11-09 17:45:46,935:INFO: Dataset: univ                Batch: 10/15	Loss 2.0490 (2.6781)
2022-11-09 17:45:46,965:INFO: Dataset: univ                Batch: 11/15	Loss 2.2140 (2.6320)
2022-11-09 17:45:46,991:INFO: Dataset: univ                Batch: 12/15	Loss 3.0128 (2.6587)
2022-11-09 17:45:47,017:INFO: Dataset: univ                Batch: 13/15	Loss 2.2003 (2.6223)
2022-11-09 17:45:47,042:INFO: Dataset: univ                Batch: 14/15	Loss 2.1910 (2.5912)
2022-11-09 17:45:47,052:INFO: Dataset: univ                Batch: 15/15	Loss 0.3761 (2.5543)
2022-11-09 17:45:47,313:INFO: Dataset: zara1               Batch: 1/8	Loss 8.5537 (8.5537)
2022-11-09 17:45:47,337:INFO: Dataset: zara1               Batch: 2/8	Loss 5.2295 (6.8986)
2022-11-09 17:45:47,362:INFO: Dataset: zara1               Batch: 3/8	Loss 6.9540 (6.9173)
2022-11-09 17:45:47,389:INFO: Dataset: zara1               Batch: 4/8	Loss 4.6934 (6.3384)
2022-11-09 17:45:47,413:INFO: Dataset: zara1               Batch: 5/8	Loss 5.2368 (6.1351)
2022-11-09 17:45:47,439:INFO: Dataset: zara1               Batch: 6/8	Loss 3.5372 (5.6551)
2022-11-09 17:45:47,462:INFO: Dataset: zara1               Batch: 7/8	Loss 3.5344 (5.3550)
2022-11-09 17:45:47,483:INFO: Dataset: zara1               Batch: 8/8	Loss 2.8952 (5.0909)
2022-11-09 17:45:47,838:INFO: Dataset: zara2               Batch:  1/18	Loss 5.0254 (5.0254)
2022-11-09 17:45:47,866:INFO: Dataset: zara2               Batch:  2/18	Loss 3.9303 (4.5025)
2022-11-09 17:45:47,890:INFO: Dataset: zara2               Batch:  3/18	Loss 9.4658 (6.1924)
2022-11-09 17:45:47,914:INFO: Dataset: zara2               Batch:  4/18	Loss 6.6603 (6.3097)
2022-11-09 17:45:47,938:INFO: Dataset: zara2               Batch:  5/18	Loss 3.4129 (5.7364)
2022-11-09 17:45:47,966:INFO: Dataset: zara2               Batch:  6/18	Loss 3.5731 (5.3597)
2022-11-09 17:45:47,989:INFO: Dataset: zara2               Batch:  7/18	Loss 2.7388 (4.9856)
2022-11-09 17:45:48,014:INFO: Dataset: zara2               Batch:  8/18	Loss 2.5420 (4.6707)
2022-11-09 17:45:48,038:INFO: Dataset: zara2               Batch:  9/18	Loss 2.5414 (4.4327)
2022-11-09 17:45:48,062:INFO: Dataset: zara2               Batch: 10/18	Loss 2.4467 (4.2254)
2022-11-09 17:45:48,087:INFO: Dataset: zara2               Batch: 11/18	Loss 3.1364 (4.1228)
2022-11-09 17:45:48,113:INFO: Dataset: zara2               Batch: 12/18	Loss 2.4223 (3.9771)
2022-11-09 17:45:48,138:INFO: Dataset: zara2               Batch: 13/18	Loss 2.4395 (3.8529)
2022-11-09 17:45:48,162:INFO: Dataset: zara2               Batch: 14/18	Loss 2.5188 (3.7513)
2022-11-09 17:45:48,187:INFO: Dataset: zara2               Batch: 15/18	Loss 2.8609 (3.6901)
2022-11-09 17:45:48,211:INFO: Dataset: zara2               Batch: 16/18	Loss 3.1182 (3.6522)
2022-11-09 17:45:48,235:INFO: Dataset: zara2               Batch: 17/18	Loss 2.9773 (3.6110)
2022-11-09 17:45:48,258:INFO: Dataset: zara2               Batch: 18/18	Loss 3.2545 (3.5948)
2022-11-09 17:45:48,309:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_66.pth.tar
2022-11-09 17:45:48,309:INFO: 
===> EPOCH: 67 (P1)
2022-11-09 17:45:48,309:INFO: - Computing loss (training)
2022-11-09 17:45:48,520:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1177 (2.1177)
2022-11-09 17:45:48,546:INFO: Dataset: hotel               Batch: 2/4	Loss 2.7702 (2.4439)
2022-11-09 17:45:48,571:INFO: Dataset: hotel               Batch: 3/4	Loss 4.6166 (3.1865)
2022-11-09 17:45:48,587:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2380 (2.8626)
2022-11-09 17:45:48,848:INFO: Dataset: univ                Batch:  1/15	Loss 2.2142 (2.2142)
2022-11-09 17:45:48,875:INFO: Dataset: univ                Batch:  2/15	Loss 2.1776 (2.1959)
2022-11-09 17:45:48,906:INFO: Dataset: univ                Batch:  3/15	Loss 2.3497 (2.2471)
2022-11-09 17:45:48,932:INFO: Dataset: univ                Batch:  4/15	Loss 2.1018 (2.2123)
2022-11-09 17:45:48,957:INFO: Dataset: univ                Batch:  5/15	Loss 2.6876 (2.3161)
2022-11-09 17:45:48,985:INFO: Dataset: univ                Batch:  6/15	Loss 2.9141 (2.4156)
2022-11-09 17:45:49,010:INFO: Dataset: univ                Batch:  7/15	Loss 2.3856 (2.4115)
2022-11-09 17:45:49,035:INFO: Dataset: univ                Batch:  8/15	Loss 2.2385 (2.3893)
2022-11-09 17:45:49,061:INFO: Dataset: univ                Batch:  9/15	Loss 2.2819 (2.3781)
2022-11-09 17:45:49,091:INFO: Dataset: univ                Batch: 10/15	Loss 2.3938 (2.3796)
2022-11-09 17:45:49,120:INFO: Dataset: univ                Batch: 11/15	Loss 2.0834 (2.3545)
2022-11-09 17:45:49,146:INFO: Dataset: univ                Batch: 12/15	Loss 2.0466 (2.3280)
2022-11-09 17:45:49,171:INFO: Dataset: univ                Batch: 13/15	Loss 2.3755 (2.3314)
2022-11-09 17:45:49,197:INFO: Dataset: univ                Batch: 14/15	Loss 2.0124 (2.3060)
2022-11-09 17:45:49,207:INFO: Dataset: univ                Batch: 15/15	Loss 0.3263 (2.2865)
2022-11-09 17:45:49,453:INFO: Dataset: zara1               Batch: 1/8	Loss 7.1721 (7.1721)
2022-11-09 17:45:49,477:INFO: Dataset: zara1               Batch: 2/8	Loss 8.3884 (7.7192)
2022-11-09 17:45:49,501:INFO: Dataset: zara1               Batch: 3/8	Loss 5.0166 (6.8158)
2022-11-09 17:45:49,525:INFO: Dataset: zara1               Batch: 4/8	Loss 4.0925 (6.1573)
2022-11-09 17:45:49,550:INFO: Dataset: zara1               Batch: 5/8	Loss 3.8631 (5.6919)
2022-11-09 17:45:49,575:INFO: Dataset: zara1               Batch: 6/8	Loss 3.5864 (5.2973)
2022-11-09 17:45:49,599:INFO: Dataset: zara1               Batch: 7/8	Loss 3.2844 (5.0216)
2022-11-09 17:45:49,620:INFO: Dataset: zara1               Batch: 8/8	Loss 2.6928 (4.7691)
2022-11-09 17:45:49,875:INFO: Dataset: zara2               Batch:  1/18	Loss 6.7106 (6.7106)
2022-11-09 17:45:49,900:INFO: Dataset: zara2               Batch:  2/18	Loss 5.5885 (6.1609)
2022-11-09 17:45:49,924:INFO: Dataset: zara2               Batch:  3/18	Loss 5.0675 (5.8014)
2022-11-09 17:45:49,952:INFO: Dataset: zara2               Batch:  4/18	Loss 5.8585 (5.8143)
2022-11-09 17:45:49,977:INFO: Dataset: zara2               Batch:  5/18	Loss 3.3189 (5.3212)
2022-11-09 17:45:50,003:INFO: Dataset: zara2               Batch:  6/18	Loss 3.5498 (5.0462)
2022-11-09 17:45:50,027:INFO: Dataset: zara2               Batch:  7/18	Loss 2.7158 (4.6829)
2022-11-09 17:45:50,050:INFO: Dataset: zara2               Batch:  8/18	Loss 2.4286 (4.3978)
2022-11-09 17:45:50,074:INFO: Dataset: zara2               Batch:  9/18	Loss 2.5873 (4.1941)
2022-11-09 17:45:50,099:INFO: Dataset: zara2               Batch: 10/18	Loss 2.4158 (4.0089)
2022-11-09 17:45:50,123:INFO: Dataset: zara2               Batch: 11/18	Loss 2.8610 (3.9032)
2022-11-09 17:45:50,149:INFO: Dataset: zara2               Batch: 12/18	Loss 2.1383 (3.7620)
2022-11-09 17:45:50,174:INFO: Dataset: zara2               Batch: 13/18	Loss 3.2871 (3.7240)
2022-11-09 17:45:50,198:INFO: Dataset: zara2               Batch: 14/18	Loss 2.5313 (3.6430)
2022-11-09 17:45:50,223:INFO: Dataset: zara2               Batch: 15/18	Loss 2.4707 (3.5629)
2022-11-09 17:45:50,247:INFO: Dataset: zara2               Batch: 16/18	Loss 2.7139 (3.5054)
2022-11-09 17:45:50,272:INFO: Dataset: zara2               Batch: 17/18	Loss 2.4759 (3.4423)
2022-11-09 17:45:50,294:INFO: Dataset: zara2               Batch: 18/18	Loss 2.2386 (3.3842)
2022-11-09 17:45:50,345:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_67.pth.tar
2022-11-09 17:45:50,345:INFO: 
===> EPOCH: 68 (P1)
2022-11-09 17:45:50,345:INFO: - Computing loss (training)
2022-11-09 17:45:50,552:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1715 (2.1715)
2022-11-09 17:45:50,580:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4039 (3.2618)
2022-11-09 17:45:50,605:INFO: Dataset: hotel               Batch: 3/4	Loss 2.2867 (2.9418)
2022-11-09 17:45:50,622:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4229 (2.7054)
2022-11-09 17:45:50,938:INFO: Dataset: univ                Batch:  1/15	Loss 1.9948 (1.9948)
2022-11-09 17:45:50,966:INFO: Dataset: univ                Batch:  2/15	Loss 1.8831 (1.9368)
2022-11-09 17:45:50,991:INFO: Dataset: univ                Batch:  3/15	Loss 2.4956 (2.1024)
2022-11-09 17:45:51,016:INFO: Dataset: univ                Batch:  4/15	Loss 2.0371 (2.0861)
2022-11-09 17:45:51,041:INFO: Dataset: univ                Batch:  5/15	Loss 2.3877 (2.1458)
2022-11-09 17:45:51,069:INFO: Dataset: univ                Batch:  6/15	Loss 1.8406 (2.0911)
2022-11-09 17:45:51,095:INFO: Dataset: univ                Batch:  7/15	Loss 1.8569 (2.0553)
2022-11-09 17:45:51,120:INFO: Dataset: univ                Batch:  8/15	Loss 1.9726 (2.0442)
2022-11-09 17:45:51,145:INFO: Dataset: univ                Batch:  9/15	Loss 2.0717 (2.0469)
2022-11-09 17:45:51,172:INFO: Dataset: univ                Batch: 10/15	Loss 1.9027 (2.0308)
2022-11-09 17:45:51,198:INFO: Dataset: univ                Batch: 11/15	Loss 1.9924 (2.0276)
2022-11-09 17:45:51,225:INFO: Dataset: univ                Batch: 12/15	Loss 1.9222 (2.0188)
2022-11-09 17:45:51,252:INFO: Dataset: univ                Batch: 13/15	Loss 1.7692 (1.9988)
2022-11-09 17:45:51,278:INFO: Dataset: univ                Batch: 14/15	Loss 2.1175 (2.0066)
2022-11-09 17:45:51,288:INFO: Dataset: univ                Batch: 15/15	Loss 0.3501 (1.9868)
2022-11-09 17:45:51,537:INFO: Dataset: zara1               Batch: 1/8	Loss 6.2429 (6.2429)
2022-11-09 17:45:51,562:INFO: Dataset: zara1               Batch: 2/8	Loss 4.0367 (5.1050)
2022-11-09 17:45:51,587:INFO: Dataset: zara1               Batch: 3/8	Loss 4.4830 (4.8902)
2022-11-09 17:45:51,612:INFO: Dataset: zara1               Batch: 4/8	Loss 4.2996 (4.7529)
2022-11-09 17:45:51,636:INFO: Dataset: zara1               Batch: 5/8	Loss 3.1658 (4.4383)
2022-11-09 17:45:51,661:INFO: Dataset: zara1               Batch: 6/8	Loss 2.9003 (4.2010)
2022-11-09 17:45:51,684:INFO: Dataset: zara1               Batch: 7/8	Loss 3.0622 (4.0511)
2022-11-09 17:45:51,706:INFO: Dataset: zara1               Batch: 8/8	Loss 2.7037 (3.9213)
2022-11-09 17:45:51,956:INFO: Dataset: zara2               Batch:  1/18	Loss 5.2470 (5.2470)
2022-11-09 17:45:52,024:INFO: Dataset: zara2               Batch:  2/18	Loss 4.1340 (4.6679)
2022-11-09 17:45:52,049:INFO: Dataset: zara2               Batch:  3/18	Loss 3.4186 (4.2485)
2022-11-09 17:45:52,073:INFO: Dataset: zara2               Batch:  4/18	Loss 3.3806 (4.0432)
2022-11-09 17:45:52,096:INFO: Dataset: zara2               Batch:  5/18	Loss 2.5963 (3.7823)
2022-11-09 17:45:52,124:INFO: Dataset: zara2               Batch:  6/18	Loss 2.4048 (3.5899)
2022-11-09 17:45:52,147:INFO: Dataset: zara2               Batch:  7/18	Loss 2.2904 (3.4037)
2022-11-09 17:45:52,171:INFO: Dataset: zara2               Batch:  8/18	Loss 2.1895 (3.2409)
2022-11-09 17:45:52,196:INFO: Dataset: zara2               Batch:  9/18	Loss 2.2471 (3.1338)
2022-11-09 17:45:52,220:INFO: Dataset: zara2               Batch: 10/18	Loss 2.0614 (3.0359)
2022-11-09 17:45:52,244:INFO: Dataset: zara2               Batch: 11/18	Loss 2.1135 (2.9583)
2022-11-09 17:45:52,269:INFO: Dataset: zara2               Batch: 12/18	Loss 2.8358 (2.9476)
2022-11-09 17:45:52,292:INFO: Dataset: zara2               Batch: 13/18	Loss 2.5230 (2.9192)
2022-11-09 17:45:52,317:INFO: Dataset: zara2               Batch: 14/18	Loss 2.4031 (2.8842)
2022-11-09 17:45:52,342:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0012 (2.8245)
2022-11-09 17:45:52,366:INFO: Dataset: zara2               Batch: 16/18	Loss 3.8254 (2.8818)
2022-11-09 17:45:52,391:INFO: Dataset: zara2               Batch: 17/18	Loss 2.3903 (2.8509)
2022-11-09 17:45:52,415:INFO: Dataset: zara2               Batch: 18/18	Loss 2.6420 (2.8403)
2022-11-09 17:45:52,465:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_68.pth.tar
2022-11-09 17:45:52,465:INFO: 
===> EPOCH: 69 (P1)
2022-11-09 17:45:52,466:INFO: - Computing loss (training)
2022-11-09 17:45:52,666:INFO: Dataset: hotel               Batch: 1/4	Loss 2.3604 (2.3604)
2022-11-09 17:45:52,693:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9346 (2.1410)
2022-11-09 17:45:52,717:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1611 (2.1477)
2022-11-09 17:45:52,734:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3651 (2.0207)
2022-11-09 17:45:53,000:INFO: Dataset: univ                Batch:  1/15	Loss 2.0885 (2.0885)
2022-11-09 17:45:53,027:INFO: Dataset: univ                Batch:  2/15	Loss 2.0784 (2.0837)
2022-11-09 17:45:53,054:INFO: Dataset: univ                Batch:  3/15	Loss 2.0343 (2.0662)
2022-11-09 17:45:53,080:INFO: Dataset: univ                Batch:  4/15	Loss 1.8015 (2.0019)
2022-11-09 17:45:53,109:INFO: Dataset: univ                Batch:  5/15	Loss 2.0761 (2.0173)
2022-11-09 17:45:53,136:INFO: Dataset: univ                Batch:  6/15	Loss 2.5111 (2.1052)
2022-11-09 17:45:53,161:INFO: Dataset: univ                Batch:  7/15	Loss 1.9995 (2.0903)
2022-11-09 17:45:53,186:INFO: Dataset: univ                Batch:  8/15	Loss 2.1945 (2.1033)
2022-11-09 17:45:53,212:INFO: Dataset: univ                Batch:  9/15	Loss 1.9153 (2.0821)
2022-11-09 17:45:53,238:INFO: Dataset: univ                Batch: 10/15	Loss 2.1480 (2.0893)
2022-11-09 17:45:53,266:INFO: Dataset: univ                Batch: 11/15	Loss 1.9515 (2.0769)
2022-11-09 17:45:53,293:INFO: Dataset: univ                Batch: 12/15	Loss 1.8484 (2.0586)
2022-11-09 17:45:53,318:INFO: Dataset: univ                Batch: 13/15	Loss 1.8481 (2.0430)
2022-11-09 17:45:53,345:INFO: Dataset: univ                Batch: 14/15	Loss 1.7740 (2.0227)
2022-11-09 17:45:53,354:INFO: Dataset: univ                Batch: 15/15	Loss 0.3099 (1.9977)
2022-11-09 17:45:53,610:INFO: Dataset: zara1               Batch: 1/8	Loss 10.4483 (10.4483)
2022-11-09 17:45:53,637:INFO: Dataset: zara1               Batch: 2/8	Loss 6.2704 (8.4386)
2022-11-09 17:45:53,662:INFO: Dataset: zara1               Batch: 3/8	Loss 6.2745 (7.7436)
2022-11-09 17:45:53,686:INFO: Dataset: zara1               Batch: 4/8	Loss 4.6585 (6.9537)
2022-11-09 17:45:53,710:INFO: Dataset: zara1               Batch: 5/8	Loss 2.8954 (6.1737)
2022-11-09 17:45:53,737:INFO: Dataset: zara1               Batch: 6/8	Loss 2.5821 (5.5804)
2022-11-09 17:45:53,761:INFO: Dataset: zara1               Batch: 7/8	Loss 3.4039 (5.2675)
2022-11-09 17:45:53,783:INFO: Dataset: zara1               Batch: 8/8	Loss 2.5062 (5.0030)
2022-11-09 17:45:54,024:INFO: Dataset: zara2               Batch:  1/18	Loss 7.1538 (7.1538)
2022-11-09 17:45:54,050:INFO: Dataset: zara2               Batch:  2/18	Loss 6.8097 (6.9784)
2022-11-09 17:45:54,075:INFO: Dataset: zara2               Batch:  3/18	Loss 3.6476 (5.9802)
2022-11-09 17:45:54,099:INFO: Dataset: zara2               Batch:  4/18	Loss 2.9240 (5.1499)
2022-11-09 17:45:54,127:INFO: Dataset: zara2               Batch:  5/18	Loss 2.7068 (4.6339)
2022-11-09 17:45:54,153:INFO: Dataset: zara2               Batch:  6/18	Loss 2.2972 (4.2401)
2022-11-09 17:45:54,178:INFO: Dataset: zara2               Batch:  7/18	Loss 2.8253 (4.0083)
2022-11-09 17:45:54,202:INFO: Dataset: zara2               Batch:  8/18	Loss 2.1000 (3.7608)
2022-11-09 17:45:54,226:INFO: Dataset: zara2               Batch:  9/18	Loss 2.9620 (3.6724)
2022-11-09 17:45:54,252:INFO: Dataset: zara2               Batch: 10/18	Loss 2.0168 (3.5114)
2022-11-09 17:45:54,280:INFO: Dataset: zara2               Batch: 11/18	Loss 2.7726 (3.4390)
2022-11-09 17:45:54,306:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0247 (3.3294)
2022-11-09 17:45:54,331:INFO: Dataset: zara2               Batch: 13/18	Loss 2.4604 (3.2553)
2022-11-09 17:45:54,355:INFO: Dataset: zara2               Batch: 14/18	Loss 2.9131 (3.2326)
2022-11-09 17:45:54,380:INFO: Dataset: zara2               Batch: 15/18	Loss 2.3787 (3.1746)
2022-11-09 17:45:54,405:INFO: Dataset: zara2               Batch: 16/18	Loss 2.8665 (3.1565)
2022-11-09 17:45:54,430:INFO: Dataset: zara2               Batch: 17/18	Loss 2.9941 (3.1467)
2022-11-09 17:45:54,452:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7100 (3.0766)
2022-11-09 17:45:54,500:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_69.pth.tar
2022-11-09 17:45:54,500:INFO: 
===> EPOCH: 70 (P1)
2022-11-09 17:45:54,501:INFO: - Computing loss (training)
2022-11-09 17:45:54,701:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6381 (1.6381)
2022-11-09 17:45:54,726:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0308 (1.8359)
2022-11-09 17:45:54,751:INFO: Dataset: hotel               Batch: 3/4	Loss 4.3179 (2.7195)
2022-11-09 17:45:54,769:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2267 (2.4714)
2022-11-09 17:45:55,015:INFO: Dataset: univ                Batch:  1/15	Loss 1.9555 (1.9555)
2022-11-09 17:45:55,042:INFO: Dataset: univ                Batch:  2/15	Loss 1.7403 (1.8456)
2022-11-09 17:45:55,069:INFO: Dataset: univ                Batch:  3/15	Loss 2.2348 (1.9801)
2022-11-09 17:45:55,095:INFO: Dataset: univ                Batch:  4/15	Loss 2.0865 (2.0053)
2022-11-09 17:45:55,124:INFO: Dataset: univ                Batch:  5/15	Loss 2.0157 (2.0074)
2022-11-09 17:45:55,152:INFO: Dataset: univ                Batch:  6/15	Loss 1.8502 (1.9813)
2022-11-09 17:45:55,177:INFO: Dataset: univ                Batch:  7/15	Loss 1.8598 (1.9653)
2022-11-09 17:45:55,202:INFO: Dataset: univ                Batch:  8/15	Loss 1.9363 (1.9622)
2022-11-09 17:45:55,228:INFO: Dataset: univ                Batch:  9/15	Loss 1.8688 (1.9524)
2022-11-09 17:45:55,255:INFO: Dataset: univ                Batch: 10/15	Loss 1.8919 (1.9463)
2022-11-09 17:45:55,282:INFO: Dataset: univ                Batch: 11/15	Loss 1.8214 (1.9355)
2022-11-09 17:45:55,310:INFO: Dataset: univ                Batch: 12/15	Loss 1.8079 (1.9249)
2022-11-09 17:45:55,335:INFO: Dataset: univ                Batch: 13/15	Loss 1.7092 (1.9077)
2022-11-09 17:45:55,361:INFO: Dataset: univ                Batch: 14/15	Loss 1.6891 (1.8916)
2022-11-09 17:45:55,370:INFO: Dataset: univ                Batch: 15/15	Loss 0.3779 (1.8749)
2022-11-09 17:45:55,629:INFO: Dataset: zara1               Batch: 1/8	Loss 6.3227 (6.3227)
2022-11-09 17:45:55,653:INFO: Dataset: zara1               Batch: 2/8	Loss 5.7461 (6.0582)
2022-11-09 17:45:55,677:INFO: Dataset: zara1               Batch: 3/8	Loss 6.4271 (6.1845)
2022-11-09 17:45:55,700:INFO: Dataset: zara1               Batch: 4/8	Loss 5.5059 (6.0356)
2022-11-09 17:45:55,724:INFO: Dataset: zara1               Batch: 5/8	Loss 3.3700 (5.4612)
2022-11-09 17:45:55,749:INFO: Dataset: zara1               Batch: 6/8	Loss 2.4446 (4.9487)
2022-11-09 17:45:55,773:INFO: Dataset: zara1               Batch: 7/8	Loss 4.6192 (4.9017)
2022-11-09 17:45:55,794:INFO: Dataset: zara1               Batch: 8/8	Loss 3.6082 (4.7581)
2022-11-09 17:45:56,043:INFO: Dataset: zara2               Batch:  1/18	Loss 5.2974 (5.2974)
2022-11-09 17:45:56,070:INFO: Dataset: zara2               Batch:  2/18	Loss 4.6191 (4.9672)
2022-11-09 17:45:56,094:INFO: Dataset: zara2               Batch:  3/18	Loss 5.3039 (5.0855)
2022-11-09 17:45:56,123:INFO: Dataset: zara2               Batch:  4/18	Loss 3.4589 (4.6649)
2022-11-09 17:45:56,148:INFO: Dataset: zara2               Batch:  5/18	Loss 3.3711 (4.4014)
2022-11-09 17:45:56,175:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9812 (4.0298)
2022-11-09 17:45:56,200:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9007 (3.7405)
2022-11-09 17:45:56,225:INFO: Dataset: zara2               Batch:  8/18	Loss 2.3278 (3.5626)
2022-11-09 17:45:56,249:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7830 (3.3848)
2022-11-09 17:45:56,274:INFO: Dataset: zara2               Batch: 10/18	Loss 2.3080 (3.2817)
2022-11-09 17:45:56,302:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0520 (3.1747)
2022-11-09 17:45:56,328:INFO: Dataset: zara2               Batch: 12/18	Loss 3.0700 (3.1658)
2022-11-09 17:45:56,353:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9758 (3.0724)
2022-11-09 17:45:56,377:INFO: Dataset: zara2               Batch: 14/18	Loss 2.4221 (3.0278)
2022-11-09 17:45:56,403:INFO: Dataset: zara2               Batch: 15/18	Loss 4.0161 (3.0915)
2022-11-09 17:45:56,429:INFO: Dataset: zara2               Batch: 16/18	Loss 2.8588 (3.0783)
2022-11-09 17:45:56,455:INFO: Dataset: zara2               Batch: 17/18	Loss 2.3716 (3.0382)
2022-11-09 17:45:56,478:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0172 (2.9911)
2022-11-09 17:45:56,529:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_70.pth.tar
2022-11-09 17:45:56,529:INFO: 
===> EPOCH: 71 (P1)
2022-11-09 17:45:56,530:INFO: - Computing loss (training)
2022-11-09 17:45:56,733:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9955 (1.9955)
2022-11-09 17:45:56,759:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9696 (1.9822)
2022-11-09 17:45:56,783:INFO: Dataset: hotel               Batch: 3/4	Loss 2.4836 (2.1520)
2022-11-09 17:45:56,799:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3170 (2.0033)
2022-11-09 17:45:57,041:INFO: Dataset: univ                Batch:  1/15	Loss 3.0524 (3.0524)
2022-11-09 17:45:57,070:INFO: Dataset: univ                Batch:  2/15	Loss 3.0963 (3.0752)
2022-11-09 17:45:57,097:INFO: Dataset: univ                Batch:  3/15	Loss 2.1980 (2.7802)
2022-11-09 17:45:57,122:INFO: Dataset: univ                Batch:  4/15	Loss 2.3795 (2.6900)
2022-11-09 17:45:57,148:INFO: Dataset: univ                Batch:  5/15	Loss 2.2320 (2.5966)
2022-11-09 17:45:57,178:INFO: Dataset: univ                Batch:  6/15	Loss 1.8212 (2.4660)
2022-11-09 17:45:57,204:INFO: Dataset: univ                Batch:  7/15	Loss 1.8185 (2.3746)
2022-11-09 17:45:57,229:INFO: Dataset: univ                Batch:  8/15	Loss 1.7734 (2.2993)
2022-11-09 17:45:57,255:INFO: Dataset: univ                Batch:  9/15	Loss 1.8231 (2.2525)
2022-11-09 17:45:57,281:INFO: Dataset: univ                Batch: 10/15	Loss 1.6720 (2.1977)
2022-11-09 17:45:57,310:INFO: Dataset: univ                Batch: 11/15	Loss 1.6987 (2.1508)
2022-11-09 17:45:57,335:INFO: Dataset: univ                Batch: 12/15	Loss 2.2083 (2.1551)
2022-11-09 17:45:57,361:INFO: Dataset: univ                Batch: 13/15	Loss 1.6581 (2.1145)
2022-11-09 17:45:57,387:INFO: Dataset: univ                Batch: 14/15	Loss 1.5096 (2.0696)
2022-11-09 17:45:57,396:INFO: Dataset: univ                Batch: 15/15	Loss 0.2709 (2.0424)
2022-11-09 17:45:57,649:INFO: Dataset: zara1               Batch: 1/8	Loss 4.3493 (4.3493)
2022-11-09 17:45:57,675:INFO: Dataset: zara1               Batch: 2/8	Loss 5.3387 (4.8527)
2022-11-09 17:45:57,699:INFO: Dataset: zara1               Batch: 3/8	Loss 7.7814 (5.7265)
2022-11-09 17:45:57,723:INFO: Dataset: zara1               Batch: 4/8	Loss 6.2699 (5.8641)
2022-11-09 17:45:57,747:INFO: Dataset: zara1               Batch: 5/8	Loss 4.3183 (5.5547)
2022-11-09 17:45:57,772:INFO: Dataset: zara1               Batch: 6/8	Loss 2.3938 (5.0510)
2022-11-09 17:45:57,795:INFO: Dataset: zara1               Batch: 7/8	Loss 2.2560 (4.6568)
2022-11-09 17:45:57,816:INFO: Dataset: zara1               Batch: 8/8	Loss 2.7976 (4.4631)
2022-11-09 17:45:58,069:INFO: Dataset: zara2               Batch:  1/18	Loss 5.7760 (5.7760)
2022-11-09 17:45:58,096:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2627 (6.5257)
2022-11-09 17:45:58,122:INFO: Dataset: zara2               Batch:  3/18	Loss 5.5279 (6.2022)
2022-11-09 17:45:58,150:INFO: Dataset: zara2               Batch:  4/18	Loss 4.3385 (5.7753)
2022-11-09 17:45:58,174:INFO: Dataset: zara2               Batch:  5/18	Loss 4.7597 (5.5607)
2022-11-09 17:45:58,200:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9688 (4.9500)
2022-11-09 17:45:58,224:INFO: Dataset: zara2               Batch:  7/18	Loss 2.0254 (4.5430)
2022-11-09 17:45:58,247:INFO: Dataset: zara2               Batch:  8/18	Loss 2.1089 (4.2314)
2022-11-09 17:45:58,271:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8510 (3.9558)
2022-11-09 17:45:58,296:INFO: Dataset: zara2               Batch: 10/18	Loss 2.5544 (3.8148)
2022-11-09 17:45:58,324:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0425 (3.6550)
2022-11-09 17:45:58,349:INFO: Dataset: zara2               Batch: 12/18	Loss 2.1498 (3.5392)
2022-11-09 17:45:58,373:INFO: Dataset: zara2               Batch: 13/18	Loss 2.7425 (3.4792)
2022-11-09 17:45:58,397:INFO: Dataset: zara2               Batch: 14/18	Loss 2.8085 (3.4291)
2022-11-09 17:45:58,421:INFO: Dataset: zara2               Batch: 15/18	Loss 2.6094 (3.3750)
2022-11-09 17:45:58,446:INFO: Dataset: zara2               Batch: 16/18	Loss 2.3700 (3.3132)
2022-11-09 17:45:58,471:INFO: Dataset: zara2               Batch: 17/18	Loss 2.6708 (3.2760)
2022-11-09 17:45:58,494:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7379 (3.2030)
2022-11-09 17:45:58,544:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_71.pth.tar
2022-11-09 17:45:58,544:INFO: 
===> EPOCH: 72 (P1)
2022-11-09 17:45:58,544:INFO: - Computing loss (training)
2022-11-09 17:45:58,753:INFO: Dataset: hotel               Batch: 1/4	Loss 2.3246 (2.3246)
2022-11-09 17:45:58,779:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0154 (2.1653)
2022-11-09 17:45:58,804:INFO: Dataset: hotel               Batch: 3/4	Loss 2.9314 (2.4121)
2022-11-09 17:45:58,820:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1265 (2.1848)
2022-11-09 17:45:59,075:INFO: Dataset: univ                Batch:  1/15	Loss 1.7323 (1.7323)
2022-11-09 17:45:59,102:INFO: Dataset: univ                Batch:  2/15	Loss 2.1158 (1.9242)
2022-11-09 17:45:59,135:INFO: Dataset: univ                Batch:  3/15	Loss 2.0308 (1.9585)
2022-11-09 17:45:59,163:INFO: Dataset: univ                Batch:  4/15	Loss 1.9434 (1.9549)
2022-11-09 17:45:59,190:INFO: Dataset: univ                Batch:  5/15	Loss 1.7821 (1.9221)
2022-11-09 17:45:59,219:INFO: Dataset: univ                Batch:  6/15	Loss 2.2736 (1.9801)
2022-11-09 17:45:59,245:INFO: Dataset: univ                Batch:  7/15	Loss 1.7194 (1.9414)
2022-11-09 17:45:59,271:INFO: Dataset: univ                Batch:  8/15	Loss 1.7610 (1.9171)
2022-11-09 17:45:59,297:INFO: Dataset: univ                Batch:  9/15	Loss 1.6806 (1.8924)
2022-11-09 17:45:59,324:INFO: Dataset: univ                Batch: 10/15	Loss 1.6479 (1.8678)
2022-11-09 17:45:59,353:INFO: Dataset: univ                Batch: 11/15	Loss 1.5347 (1.8350)
2022-11-09 17:45:59,379:INFO: Dataset: univ                Batch: 12/15	Loss 1.5718 (1.8099)
2022-11-09 17:45:59,405:INFO: Dataset: univ                Batch: 13/15	Loss 1.6388 (1.7952)
2022-11-09 17:45:59,431:INFO: Dataset: univ                Batch: 14/15	Loss 1.7561 (1.7922)
2022-11-09 17:45:59,441:INFO: Dataset: univ                Batch: 15/15	Loss 0.2874 (1.7687)
2022-11-09 17:45:59,688:INFO: Dataset: zara1               Batch: 1/8	Loss 5.3605 (5.3605)
2022-11-09 17:45:59,716:INFO: Dataset: zara1               Batch: 2/8	Loss 8.3836 (6.8914)
2022-11-09 17:45:59,740:INFO: Dataset: zara1               Batch: 3/8	Loss 7.8060 (7.1937)
2022-11-09 17:45:59,763:INFO: Dataset: zara1               Batch: 4/8	Loss 2.4384 (6.0711)
2022-11-09 17:45:59,788:INFO: Dataset: zara1               Batch: 5/8	Loss 2.2646 (5.2088)
2022-11-09 17:45:59,813:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1430 (4.6564)
2022-11-09 17:45:59,836:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1301 (4.2987)
2022-11-09 17:45:59,857:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7932 (4.0099)
2022-11-09 17:46:00,113:INFO: Dataset: zara2               Batch:  1/18	Loss 4.2025 (4.2025)
2022-11-09 17:46:00,138:INFO: Dataset: zara2               Batch:  2/18	Loss 5.6019 (4.9400)
2022-11-09 17:46:00,166:INFO: Dataset: zara2               Batch:  3/18	Loss 4.6568 (4.8521)
2022-11-09 17:46:00,190:INFO: Dataset: zara2               Batch:  4/18	Loss 5.1848 (4.9290)
2022-11-09 17:46:00,215:INFO: Dataset: zara2               Batch:  5/18	Loss 4.8424 (4.9106)
2022-11-09 17:46:00,242:INFO: Dataset: zara2               Batch:  6/18	Loss 4.0931 (4.7775)
2022-11-09 17:46:00,266:INFO: Dataset: zara2               Batch:  7/18	Loss 2.9298 (4.5155)
2022-11-09 17:46:00,290:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9712 (4.1928)
2022-11-09 17:46:00,313:INFO: Dataset: zara2               Batch:  9/18	Loss 2.4274 (3.9854)
2022-11-09 17:46:00,338:INFO: Dataset: zara2               Batch: 10/18	Loss 2.0555 (3.7825)
2022-11-09 17:46:00,362:INFO: Dataset: zara2               Batch: 11/18	Loss 2.7271 (3.6813)
2022-11-09 17:46:00,389:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8918 (3.5359)
2022-11-09 17:46:00,413:INFO: Dataset: zara2               Batch: 13/18	Loss 2.1384 (3.4234)
2022-11-09 17:46:00,438:INFO: Dataset: zara2               Batch: 14/18	Loss 3.6395 (3.4386)
2022-11-09 17:46:00,463:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7812 (3.3416)
2022-11-09 17:46:00,487:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6795 (3.2402)
2022-11-09 17:46:00,512:INFO: Dataset: zara2               Batch: 17/18	Loss 2.9151 (3.2214)
2022-11-09 17:46:00,533:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6810 (3.1470)
2022-11-09 17:46:00,586:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_72.pth.tar
2022-11-09 17:46:00,586:INFO: 
===> EPOCH: 73 (P1)
2022-11-09 17:46:00,587:INFO: - Computing loss (training)
2022-11-09 17:46:00,798:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0915 (3.0915)
2022-11-09 17:46:00,823:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8648 (2.4796)
2022-11-09 17:46:00,847:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1235 (2.3667)
2022-11-09 17:46:00,864:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9668 (2.1322)
2022-11-09 17:46:01,112:INFO: Dataset: univ                Batch:  1/15	Loss 1.6566 (1.6566)
2022-11-09 17:46:01,140:INFO: Dataset: univ                Batch:  2/15	Loss 1.7107 (1.6833)
2022-11-09 17:46:01,166:INFO: Dataset: univ                Batch:  3/15	Loss 1.6853 (1.6840)
2022-11-09 17:46:01,191:INFO: Dataset: univ                Batch:  4/15	Loss 1.7301 (1.6952)
2022-11-09 17:46:01,219:INFO: Dataset: univ                Batch:  5/15	Loss 1.9385 (1.7501)
2022-11-09 17:46:01,249:INFO: Dataset: univ                Batch:  6/15	Loss 1.5184 (1.7130)
2022-11-09 17:46:01,274:INFO: Dataset: univ                Batch:  7/15	Loss 1.5709 (1.6927)
2022-11-09 17:46:01,300:INFO: Dataset: univ                Batch:  8/15	Loss 1.7844 (1.7043)
2022-11-09 17:46:01,325:INFO: Dataset: univ                Batch:  9/15	Loss 1.6493 (1.6984)
2022-11-09 17:46:01,351:INFO: Dataset: univ                Batch: 10/15	Loss 1.5918 (1.6881)
2022-11-09 17:46:01,378:INFO: Dataset: univ                Batch: 11/15	Loss 1.7991 (1.6981)
2022-11-09 17:46:01,405:INFO: Dataset: univ                Batch: 12/15	Loss 1.5516 (1.6850)
2022-11-09 17:46:01,431:INFO: Dataset: univ                Batch: 13/15	Loss 1.5223 (1.6729)
2022-11-09 17:46:01,456:INFO: Dataset: univ                Batch: 14/15	Loss 1.6608 (1.6721)
2022-11-09 17:46:01,466:INFO: Dataset: univ                Batch: 15/15	Loss 0.3316 (1.6537)
2022-11-09 17:46:01,716:INFO: Dataset: zara1               Batch: 1/8	Loss 9.4075 (9.4075)
2022-11-09 17:46:01,740:INFO: Dataset: zara1               Batch: 2/8	Loss 3.9480 (6.9395)
2022-11-09 17:46:01,766:INFO: Dataset: zara1               Batch: 3/8	Loss 5.1856 (6.4339)
2022-11-09 17:46:01,790:INFO: Dataset: zara1               Batch: 4/8	Loss 2.2854 (5.4813)
2022-11-09 17:46:01,814:INFO: Dataset: zara1               Batch: 5/8	Loss 2.7701 (4.9447)
2022-11-09 17:46:01,838:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1719 (4.4141)
2022-11-09 17:46:01,862:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0714 (4.0824)
2022-11-09 17:46:01,883:INFO: Dataset: zara1               Batch: 8/8	Loss 2.3001 (3.8704)
2022-11-09 17:46:02,131:INFO: Dataset: zara2               Batch:  1/18	Loss 4.9065 (4.9065)
2022-11-09 17:46:02,158:INFO: Dataset: zara2               Batch:  2/18	Loss 5.1529 (5.0310)
2022-11-09 17:46:02,183:INFO: Dataset: zara2               Batch:  3/18	Loss 2.7716 (4.2653)
2022-11-09 17:46:02,208:INFO: Dataset: zara2               Batch:  4/18	Loss 5.3874 (4.5266)
2022-11-09 17:46:02,235:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9946 (4.0077)
2022-11-09 17:46:02,261:INFO: Dataset: zara2               Batch:  6/18	Loss 2.1755 (3.7022)
2022-11-09 17:46:02,284:INFO: Dataset: zara2               Batch:  7/18	Loss 1.6390 (3.3968)
2022-11-09 17:46:02,308:INFO: Dataset: zara2               Batch:  8/18	Loss 2.1564 (3.2458)
2022-11-09 17:46:02,332:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7472 (3.0738)
2022-11-09 17:46:02,357:INFO: Dataset: zara2               Batch: 10/18	Loss 2.0002 (2.9659)
2022-11-09 17:46:02,382:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5741 (2.8269)
2022-11-09 17:46:02,409:INFO: Dataset: zara2               Batch: 12/18	Loss 2.3707 (2.7929)
2022-11-09 17:46:02,433:INFO: Dataset: zara2               Batch: 13/18	Loss 2.3067 (2.7511)
2022-11-09 17:46:02,457:INFO: Dataset: zara2               Batch: 14/18	Loss 2.5392 (2.7361)
2022-11-09 17:46:02,482:INFO: Dataset: zara2               Batch: 15/18	Loss 2.4790 (2.7186)
2022-11-09 17:46:02,506:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8251 (2.6692)
2022-11-09 17:46:02,531:INFO: Dataset: zara2               Batch: 17/18	Loss 2.1158 (2.6357)
2022-11-09 17:46:02,553:INFO: Dataset: zara2               Batch: 18/18	Loss 1.5509 (2.5814)
2022-11-09 17:46:02,602:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_73.pth.tar
2022-11-09 17:46:02,602:INFO: 
===> EPOCH: 74 (P1)
2022-11-09 17:46:02,603:INFO: - Computing loss (training)
2022-11-09 17:46:02,815:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0073 (3.0073)
2022-11-09 17:46:02,843:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1249 (2.5518)
2022-11-09 17:46:02,867:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6086 (2.2483)
2022-11-09 17:46:02,883:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2667 (2.0916)
2022-11-09 17:46:03,139:INFO: Dataset: univ                Batch:  1/15	Loss 1.6436 (1.6436)
2022-11-09 17:46:03,166:INFO: Dataset: univ                Batch:  2/15	Loss 1.7478 (1.6992)
2022-11-09 17:46:03,194:INFO: Dataset: univ                Batch:  3/15	Loss 3.2148 (2.2493)
2022-11-09 17:46:03,222:INFO: Dataset: univ                Batch:  4/15	Loss 1.5946 (2.0862)
2022-11-09 17:46:03,249:INFO: Dataset: univ                Batch:  5/15	Loss 2.1349 (2.0956)
2022-11-09 17:46:03,277:INFO: Dataset: univ                Batch:  6/15	Loss 2.4711 (2.1611)
2022-11-09 17:46:03,302:INFO: Dataset: univ                Batch:  7/15	Loss 1.9027 (2.1260)
2022-11-09 17:46:03,327:INFO: Dataset: univ                Batch:  8/15	Loss 1.6294 (2.0677)
2022-11-09 17:46:03,353:INFO: Dataset: univ                Batch:  9/15	Loss 1.5302 (2.0035)
2022-11-09 17:46:03,379:INFO: Dataset: univ                Batch: 10/15	Loss 1.4885 (1.9482)
2022-11-09 17:46:03,407:INFO: Dataset: univ                Batch: 11/15	Loss 1.6304 (1.9159)
2022-11-09 17:46:03,435:INFO: Dataset: univ                Batch: 12/15	Loss 1.5317 (1.8822)
2022-11-09 17:46:03,460:INFO: Dataset: univ                Batch: 13/15	Loss 1.6858 (1.8679)
2022-11-09 17:46:03,485:INFO: Dataset: univ                Batch: 14/15	Loss 1.7922 (1.8623)
2022-11-09 17:46:03,494:INFO: Dataset: univ                Batch: 15/15	Loss 0.2545 (1.8381)
2022-11-09 17:46:03,735:INFO: Dataset: zara1               Batch: 1/8	Loss 7.5254 (7.5254)
2022-11-09 17:46:03,762:INFO: Dataset: zara1               Batch: 2/8	Loss 6.4864 (6.9962)
2022-11-09 17:46:03,785:INFO: Dataset: zara1               Batch: 3/8	Loss 4.6731 (6.2557)
2022-11-09 17:46:03,810:INFO: Dataset: zara1               Batch: 4/8	Loss 4.6042 (5.8101)
2022-11-09 17:46:03,835:INFO: Dataset: zara1               Batch: 5/8	Loss 2.6096 (5.1548)
2022-11-09 17:46:03,859:INFO: Dataset: zara1               Batch: 6/8	Loss 2.3042 (4.7112)
2022-11-09 17:46:03,882:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0999 (4.3610)
2022-11-09 17:46:03,903:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9349 (4.0673)
2022-11-09 17:46:04,158:INFO: Dataset: zara2               Batch:  1/18	Loss 4.7174 (4.7174)
2022-11-09 17:46:04,194:INFO: Dataset: zara2               Batch:  2/18	Loss 5.1668 (4.9477)
2022-11-09 17:46:04,220:INFO: Dataset: zara2               Batch:  3/18	Loss 5.3681 (5.0887)
2022-11-09 17:46:04,243:INFO: Dataset: zara2               Batch:  4/18	Loss 3.1892 (4.5921)
2022-11-09 17:46:04,267:INFO: Dataset: zara2               Batch:  5/18	Loss 4.4066 (4.5558)
2022-11-09 17:46:04,293:INFO: Dataset: zara2               Batch:  6/18	Loss 3.6352 (4.3924)
2022-11-09 17:46:04,316:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7082 (3.9926)
2022-11-09 17:46:04,340:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5901 (3.6474)
2022-11-09 17:46:04,363:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7940 (3.4507)
2022-11-09 17:46:04,388:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5366 (3.2493)
2022-11-09 17:46:04,414:INFO: Dataset: zara2               Batch: 11/18	Loss 2.2881 (3.1573)
2022-11-09 17:46:04,440:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0525 (3.0618)
2022-11-09 17:46:04,464:INFO: Dataset: zara2               Batch: 13/18	Loss 2.2341 (2.9995)
2022-11-09 17:46:04,488:INFO: Dataset: zara2               Batch: 14/18	Loss 2.1892 (2.9443)
2022-11-09 17:46:04,513:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9813 (2.8801)
2022-11-09 17:46:04,537:INFO: Dataset: zara2               Batch: 16/18	Loss 2.4292 (2.8542)
2022-11-09 17:46:04,562:INFO: Dataset: zara2               Batch: 17/18	Loss 2.3303 (2.8238)
2022-11-09 17:46:04,584:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0158 (2.7861)
2022-11-09 17:46:04,642:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_74.pth.tar
2022-11-09 17:46:04,642:INFO: 
===> EPOCH: 75 (P1)
2022-11-09 17:46:04,643:INFO: - Computing loss (training)
2022-11-09 17:46:04,848:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6526 (1.6526)
2022-11-09 17:46:04,872:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0190 (1.8324)
2022-11-09 17:46:04,895:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0643 (1.9050)
2022-11-09 17:46:04,911:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2384 (1.7942)
2022-11-09 17:46:05,171:INFO: Dataset: univ                Batch:  1/15	Loss 1.5238 (1.5238)
2022-11-09 17:46:05,254:INFO: Dataset: univ                Batch:  2/15	Loss 1.5682 (1.5448)
2022-11-09 17:46:05,280:INFO: Dataset: univ                Batch:  3/15	Loss 1.5670 (1.5521)
2022-11-09 17:46:05,307:INFO: Dataset: univ                Batch:  4/15	Loss 2.1157 (1.6880)
2022-11-09 17:46:05,332:INFO: Dataset: univ                Batch:  5/15	Loss 1.4390 (1.6348)
2022-11-09 17:46:05,362:INFO: Dataset: univ                Batch:  6/15	Loss 1.4972 (1.6110)
2022-11-09 17:46:05,389:INFO: Dataset: univ                Batch:  7/15	Loss 1.9333 (1.6622)
2022-11-09 17:46:05,416:INFO: Dataset: univ                Batch:  8/15	Loss 1.6713 (1.6633)
2022-11-09 17:46:05,442:INFO: Dataset: univ                Batch:  9/15	Loss 1.5326 (1.6492)
2022-11-09 17:46:05,469:INFO: Dataset: univ                Batch: 10/15	Loss 1.4611 (1.6301)
2022-11-09 17:46:05,496:INFO: Dataset: univ                Batch: 11/15	Loss 1.7933 (1.6429)
2022-11-09 17:46:05,522:INFO: Dataset: univ                Batch: 12/15	Loss 1.8695 (1.6629)
2022-11-09 17:46:05,547:INFO: Dataset: univ                Batch: 13/15	Loss 1.5433 (1.6536)
2022-11-09 17:46:05,574:INFO: Dataset: univ                Batch: 14/15	Loss 1.6048 (1.6497)
2022-11-09 17:46:05,584:INFO: Dataset: univ                Batch: 15/15	Loss 0.3544 (1.6382)
2022-11-09 17:46:05,833:INFO: Dataset: zara1               Batch: 1/8	Loss 9.2170 (9.2170)
2022-11-09 17:46:05,858:INFO: Dataset: zara1               Batch: 2/8	Loss 6.0567 (7.6401)
2022-11-09 17:46:05,883:INFO: Dataset: zara1               Batch: 3/8	Loss 5.3886 (6.8562)
2022-11-09 17:46:05,906:INFO: Dataset: zara1               Batch: 4/8	Loss 3.3120 (6.0167)
2022-11-09 17:46:05,932:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8381 (5.2193)
2022-11-09 17:46:05,956:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8563 (4.6883)
2022-11-09 17:46:05,979:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1676 (4.2968)
2022-11-09 17:46:06,000:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0817 (4.0485)
2022-11-09 17:46:06,258:INFO: Dataset: zara2               Batch:  1/18	Loss 4.8860 (4.8860)
2022-11-09 17:46:06,284:INFO: Dataset: zara2               Batch:  2/18	Loss 4.6419 (4.7588)
2022-11-09 17:46:06,312:INFO: Dataset: zara2               Batch:  3/18	Loss 6.9872 (5.5236)
2022-11-09 17:46:06,336:INFO: Dataset: zara2               Batch:  4/18	Loss 2.4851 (4.7218)
2022-11-09 17:46:06,362:INFO: Dataset: zara2               Batch:  5/18	Loss 2.1653 (4.2073)
2022-11-09 17:46:06,389:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6678 (3.8212)
2022-11-09 17:46:06,412:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8969 (3.5555)
2022-11-09 17:46:06,436:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5708 (3.3186)
2022-11-09 17:46:06,460:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5088 (3.1414)
2022-11-09 17:46:06,485:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5805 (2.9875)
2022-11-09 17:46:06,510:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9096 (2.8900)
2022-11-09 17:46:06,536:INFO: Dataset: zara2               Batch: 12/18	Loss 2.5634 (2.8596)
2022-11-09 17:46:06,561:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4790 (2.7507)
2022-11-09 17:46:06,585:INFO: Dataset: zara2               Batch: 14/18	Loss 3.3211 (2.7892)
2022-11-09 17:46:06,609:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4210 (2.6930)
2022-11-09 17:46:06,634:INFO: Dataset: zara2               Batch: 16/18	Loss 3.1116 (2.7179)
2022-11-09 17:46:06,659:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0006 (2.6794)
2022-11-09 17:46:06,682:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8134 (2.6436)
2022-11-09 17:46:06,732:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_75.pth.tar
2022-11-09 17:46:06,732:INFO: 
===> EPOCH: 76 (P1)
2022-11-09 17:46:06,732:INFO: - Computing loss (training)
2022-11-09 17:46:06,941:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8725 (1.8725)
2022-11-09 17:46:06,965:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7985 (1.8337)
2022-11-09 17:46:06,989:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6896 (1.7885)
2022-11-09 17:46:07,006:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6500 (1.7642)
2022-11-09 17:46:07,265:INFO: Dataset: univ                Batch:  1/15	Loss 1.6774 (1.6774)
2022-11-09 17:46:07,293:INFO: Dataset: univ                Batch:  2/15	Loss 1.8138 (1.7447)
2022-11-09 17:46:07,320:INFO: Dataset: univ                Batch:  3/15	Loss 1.4251 (1.6348)
2022-11-09 17:46:07,345:INFO: Dataset: univ                Batch:  4/15	Loss 1.5140 (1.6068)
2022-11-09 17:46:07,373:INFO: Dataset: univ                Batch:  5/15	Loss 1.5595 (1.5973)
2022-11-09 17:46:07,402:INFO: Dataset: univ                Batch:  6/15	Loss 1.5854 (1.5952)
2022-11-09 17:46:07,427:INFO: Dataset: univ                Batch:  7/15	Loss 1.6296 (1.6001)
2022-11-09 17:46:07,452:INFO: Dataset: univ                Batch:  8/15	Loss 1.4945 (1.5885)
2022-11-09 17:46:07,477:INFO: Dataset: univ                Batch:  9/15	Loss 1.2751 (1.5552)
2022-11-09 17:46:07,503:INFO: Dataset: univ                Batch: 10/15	Loss 1.4736 (1.5473)
2022-11-09 17:46:07,531:INFO: Dataset: univ                Batch: 11/15	Loss 1.4051 (1.5332)
2022-11-09 17:46:07,559:INFO: Dataset: univ                Batch: 12/15	Loss 1.8818 (1.5606)
2022-11-09 17:46:07,584:INFO: Dataset: univ                Batch: 13/15	Loss 1.3072 (1.5422)
2022-11-09 17:46:07,609:INFO: Dataset: univ                Batch: 14/15	Loss 1.6337 (1.5485)
2022-11-09 17:46:07,619:INFO: Dataset: univ                Batch: 15/15	Loss 0.3176 (1.5359)
2022-11-09 17:46:07,868:INFO: Dataset: zara1               Batch: 1/8	Loss 5.0664 (5.0664)
2022-11-09 17:46:07,895:INFO: Dataset: zara1               Batch: 2/8	Loss 2.9153 (3.9798)
2022-11-09 17:46:07,920:INFO: Dataset: zara1               Batch: 3/8	Loss 3.7288 (3.9019)
2022-11-09 17:46:07,944:INFO: Dataset: zara1               Batch: 4/8	Loss 2.8069 (3.6293)
2022-11-09 17:46:07,967:INFO: Dataset: zara1               Batch: 5/8	Loss 2.2500 (3.3460)
2022-11-09 17:46:07,992:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8825 (3.0808)
2022-11-09 17:46:08,015:INFO: Dataset: zara1               Batch: 7/8	Loss 2.6955 (3.0281)
2022-11-09 17:46:08,037:INFO: Dataset: zara1               Batch: 8/8	Loss 2.5964 (2.9783)
2022-11-09 17:46:08,293:INFO: Dataset: zara2               Batch:  1/18	Loss 3.4240 (3.4240)
2022-11-09 17:46:08,317:INFO: Dataset: zara2               Batch:  2/18	Loss 2.8656 (3.1516)
2022-11-09 17:46:08,342:INFO: Dataset: zara2               Batch:  3/18	Loss 3.7958 (3.3845)
2022-11-09 17:46:08,366:INFO: Dataset: zara2               Batch:  4/18	Loss 2.6932 (3.2145)
2022-11-09 17:46:08,390:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0280 (2.9558)
2022-11-09 17:46:08,418:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5221 (2.7045)
2022-11-09 17:46:08,442:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7144 (2.5518)
2022-11-09 17:46:08,465:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6240 (2.4353)
2022-11-09 17:46:08,489:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8973 (2.3774)
2022-11-09 17:46:08,512:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2906 (2.2627)
2022-11-09 17:46:08,540:INFO: Dataset: zara2               Batch: 11/18	Loss 3.2283 (2.3572)
2022-11-09 17:46:08,565:INFO: Dataset: zara2               Batch: 12/18	Loss 3.2883 (2.4293)
2022-11-09 17:46:08,589:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6826 (2.3717)
2022-11-09 17:46:08,613:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8557 (2.3296)
2022-11-09 17:46:08,637:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5440 (2.2736)
2022-11-09 17:46:08,661:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5449 (2.2299)
2022-11-09 17:46:08,686:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5144 (2.1882)
2022-11-09 17:46:08,708:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2617 (2.1449)
2022-11-09 17:46:08,759:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_76.pth.tar
2022-11-09 17:46:08,759:INFO: 
===> EPOCH: 77 (P1)
2022-11-09 17:46:08,760:INFO: - Computing loss (training)
2022-11-09 17:46:08,965:INFO: Dataset: hotel               Batch: 1/4	Loss 4.4375 (4.4375)
2022-11-09 17:46:08,994:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7386 (3.1173)
2022-11-09 17:46:09,018:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6582 (2.6240)
2022-11-09 17:46:09,035:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1737 (2.3733)
2022-11-09 17:46:09,346:INFO: Dataset: univ                Batch:  1/15	Loss 1.7058 (1.7058)
2022-11-09 17:46:09,374:INFO: Dataset: univ                Batch:  2/15	Loss 1.6389 (1.6706)
2022-11-09 17:46:09,403:INFO: Dataset: univ                Batch:  3/15	Loss 1.7836 (1.7092)
2022-11-09 17:46:09,428:INFO: Dataset: univ                Batch:  4/15	Loss 1.9377 (1.7689)
2022-11-09 17:46:09,453:INFO: Dataset: univ                Batch:  5/15	Loss 1.6060 (1.7357)
2022-11-09 17:46:09,481:INFO: Dataset: univ                Batch:  6/15	Loss 1.9822 (1.7788)
2022-11-09 17:46:09,506:INFO: Dataset: univ                Batch:  7/15	Loss 1.5290 (1.7449)
2022-11-09 17:46:09,531:INFO: Dataset: univ                Batch:  8/15	Loss 1.3131 (1.6934)
2022-11-09 17:46:09,556:INFO: Dataset: univ                Batch:  9/15	Loss 1.3670 (1.6575)
2022-11-09 17:46:09,583:INFO: Dataset: univ                Batch: 10/15	Loss 1.3012 (1.6205)
2022-11-09 17:46:09,610:INFO: Dataset: univ                Batch: 11/15	Loss 1.3519 (1.5960)
2022-11-09 17:46:09,638:INFO: Dataset: univ                Batch: 12/15	Loss 1.4091 (1.5791)
2022-11-09 17:46:09,665:INFO: Dataset: univ                Batch: 13/15	Loss 1.4634 (1.5702)
2022-11-09 17:46:09,691:INFO: Dataset: univ                Batch: 14/15	Loss 1.3899 (1.5565)
2022-11-09 17:46:09,701:INFO: Dataset: univ                Batch: 15/15	Loss 0.2695 (1.5362)
2022-11-09 17:46:09,968:INFO: Dataset: zara1               Batch: 1/8	Loss 5.9891 (5.9891)
2022-11-09 17:46:09,992:INFO: Dataset: zara1               Batch: 2/8	Loss 4.9832 (5.4564)
2022-11-09 17:46:10,016:INFO: Dataset: zara1               Batch: 3/8	Loss 4.6362 (5.1925)
2022-11-09 17:46:10,040:INFO: Dataset: zara1               Batch: 4/8	Loss 3.1172 (4.6379)
2022-11-09 17:46:10,063:INFO: Dataset: zara1               Batch: 5/8	Loss 2.5862 (4.1950)
2022-11-09 17:46:10,089:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1326 (3.8720)
2022-11-09 17:46:10,112:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7461 (3.6056)
2022-11-09 17:46:10,133:INFO: Dataset: zara1               Batch: 8/8	Loss 2.6085 (3.5049)
2022-11-09 17:46:10,381:INFO: Dataset: zara2               Batch:  1/18	Loss 5.8238 (5.8238)
2022-11-09 17:46:10,406:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1158 (6.4926)
2022-11-09 17:46:10,432:INFO: Dataset: zara2               Batch:  3/18	Loss 5.6468 (6.2237)
2022-11-09 17:46:10,459:INFO: Dataset: zara2               Batch:  4/18	Loss 2.9701 (5.3588)
2022-11-09 17:46:10,484:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7629 (4.6507)
2022-11-09 17:46:10,511:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5174 (4.1508)
2022-11-09 17:46:10,534:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4039 (3.7607)
2022-11-09 17:46:10,558:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6679 (3.5018)
2022-11-09 17:46:10,581:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7364 (3.2961)
2022-11-09 17:46:10,606:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3943 (3.1154)
2022-11-09 17:46:10,630:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4978 (2.9662)
2022-11-09 17:46:10,657:INFO: Dataset: zara2               Batch: 12/18	Loss 1.5744 (2.8427)
2022-11-09 17:46:10,681:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6875 (2.7495)
2022-11-09 17:46:10,705:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6632 (2.6682)
2022-11-09 17:46:10,731:INFO: Dataset: zara2               Batch: 15/18	Loss 2.6787 (2.6690)
2022-11-09 17:46:10,755:INFO: Dataset: zara2               Batch: 16/18	Loss 2.4178 (2.6535)
2022-11-09 17:46:10,779:INFO: Dataset: zara2               Batch: 17/18	Loss 2.2177 (2.6284)
2022-11-09 17:46:10,801:INFO: Dataset: zara2               Batch: 18/18	Loss 2.2274 (2.6103)
2022-11-09 17:46:10,852:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_77.pth.tar
2022-11-09 17:46:10,852:INFO: 
===> EPOCH: 78 (P1)
2022-11-09 17:46:10,853:INFO: - Computing loss (training)
2022-11-09 17:46:11,063:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8443 (1.8443)
2022-11-09 17:46:11,089:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4595 (1.6572)
2022-11-09 17:46:11,114:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1331 (1.8078)
2022-11-09 17:46:11,130:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4390 (1.7465)
2022-11-09 17:46:11,385:INFO: Dataset: univ                Batch:  1/15	Loss 1.7041 (1.7041)
2022-11-09 17:46:11,414:INFO: Dataset: univ                Batch:  2/15	Loss 2.8194 (2.2855)
2022-11-09 17:46:11,440:INFO: Dataset: univ                Batch:  3/15	Loss 1.8723 (2.1628)
2022-11-09 17:46:11,471:INFO: Dataset: univ                Batch:  4/15	Loss 1.9600 (2.1088)
2022-11-09 17:46:11,497:INFO: Dataset: univ                Batch:  5/15	Loss 1.7687 (2.0371)
2022-11-09 17:46:11,526:INFO: Dataset: univ                Batch:  6/15	Loss 1.4027 (1.9295)
2022-11-09 17:46:11,552:INFO: Dataset: univ                Batch:  7/15	Loss 1.5089 (1.8675)
2022-11-09 17:46:11,578:INFO: Dataset: univ                Batch:  8/15	Loss 1.6534 (1.8392)
2022-11-09 17:46:11,605:INFO: Dataset: univ                Batch:  9/15	Loss 1.4119 (1.7942)
2022-11-09 17:46:11,633:INFO: Dataset: univ                Batch: 10/15	Loss 1.6981 (1.7849)
2022-11-09 17:46:11,661:INFO: Dataset: univ                Batch: 11/15	Loss 1.2188 (1.7333)
2022-11-09 17:46:11,689:INFO: Dataset: univ                Batch: 12/15	Loss 1.3282 (1.7006)
2022-11-09 17:46:11,715:INFO: Dataset: univ                Batch: 13/15	Loss 1.2864 (1.6704)
2022-11-09 17:46:11,741:INFO: Dataset: univ                Batch: 14/15	Loss 1.2833 (1.6410)
2022-11-09 17:46:11,751:INFO: Dataset: univ                Batch: 15/15	Loss 0.2655 (1.6292)
2022-11-09 17:46:12,000:INFO: Dataset: zara1               Batch: 1/8	Loss 9.2738 (9.2738)
2022-11-09 17:46:12,024:INFO: Dataset: zara1               Batch: 2/8	Loss 6.4195 (7.9113)
2022-11-09 17:46:12,048:INFO: Dataset: zara1               Batch: 3/8	Loss 3.4298 (6.4422)
2022-11-09 17:46:12,071:INFO: Dataset: zara1               Batch: 4/8	Loss 4.4624 (5.9381)
2022-11-09 17:46:12,096:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9642 (5.1658)
2022-11-09 17:46:12,121:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5647 (4.5507)
2022-11-09 17:46:12,144:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6670 (4.1329)
2022-11-09 17:46:12,165:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8521 (3.8904)
2022-11-09 17:46:12,412:INFO: Dataset: zara2               Batch:  1/18	Loss 7.0871 (7.0871)
2022-11-09 17:46:12,438:INFO: Dataset: zara2               Batch:  2/18	Loss 8.8684 (7.9084)
2022-11-09 17:46:12,462:INFO: Dataset: zara2               Batch:  3/18	Loss 4.8077 (6.9235)
2022-11-09 17:46:12,487:INFO: Dataset: zara2               Batch:  4/18	Loss 3.3925 (6.0164)
2022-11-09 17:46:12,515:INFO: Dataset: zara2               Batch:  5/18	Loss 2.2350 (5.2276)
2022-11-09 17:46:12,540:INFO: Dataset: zara2               Batch:  6/18	Loss 2.4920 (4.7554)
2022-11-09 17:46:12,564:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2915 (4.2641)
2022-11-09 17:46:12,587:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5000 (3.9243)
2022-11-09 17:46:12,611:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2913 (3.6437)
2022-11-09 17:46:12,635:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3693 (3.4012)
2022-11-09 17:46:12,662:INFO: Dataset: zara2               Batch: 11/18	Loss 3.4360 (3.4043)
2022-11-09 17:46:12,686:INFO: Dataset: zara2               Batch: 12/18	Loss 2.5349 (3.3339)
2022-11-09 17:46:12,710:INFO: Dataset: zara2               Batch: 13/18	Loss 2.2233 (3.2490)
2022-11-09 17:46:12,734:INFO: Dataset: zara2               Batch: 14/18	Loss 2.5288 (3.1958)
2022-11-09 17:46:12,759:INFO: Dataset: zara2               Batch: 15/18	Loss 2.9966 (3.1835)
2022-11-09 17:46:12,784:INFO: Dataset: zara2               Batch: 16/18	Loss 2.7033 (3.1517)
2022-11-09 17:46:12,808:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9541 (3.0831)
2022-11-09 17:46:12,830:INFO: Dataset: zara2               Batch: 18/18	Loss 1.5693 (3.0105)
2022-11-09 17:46:12,878:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_78.pth.tar
2022-11-09 17:46:12,878:INFO: 
===> EPOCH: 79 (P1)
2022-11-09 17:46:12,879:INFO: - Computing loss (training)
2022-11-09 17:46:13,086:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3578 (1.3578)
2022-11-09 17:46:13,111:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1489 (1.7389)
2022-11-09 17:46:13,136:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0630 (1.8485)
2022-11-09 17:46:13,153:INFO: Dataset: hotel               Batch: 4/4	Loss 2.7129 (2.0047)
2022-11-09 17:46:13,409:INFO: Dataset: univ                Batch:  1/15	Loss 2.3368 (2.3368)
2022-11-09 17:46:13,437:INFO: Dataset: univ                Batch:  2/15	Loss 1.5147 (1.9649)
2022-11-09 17:46:13,464:INFO: Dataset: univ                Batch:  3/15	Loss 1.3585 (1.7583)
2022-11-09 17:46:13,489:INFO: Dataset: univ                Batch:  4/15	Loss 1.6011 (1.7207)
2022-11-09 17:46:13,515:INFO: Dataset: univ                Batch:  5/15	Loss 1.9761 (1.7765)
2022-11-09 17:46:13,545:INFO: Dataset: univ                Batch:  6/15	Loss 1.5637 (1.7394)
2022-11-09 17:46:13,570:INFO: Dataset: univ                Batch:  7/15	Loss 1.5410 (1.7119)
2022-11-09 17:46:13,595:INFO: Dataset: univ                Batch:  8/15	Loss 1.4722 (1.6815)
2022-11-09 17:46:13,620:INFO: Dataset: univ                Batch:  9/15	Loss 1.5219 (1.6644)
2022-11-09 17:46:13,647:INFO: Dataset: univ                Batch: 10/15	Loss 1.8555 (1.6837)
2022-11-09 17:46:13,675:INFO: Dataset: univ                Batch: 11/15	Loss 1.2192 (1.6454)
2022-11-09 17:46:13,702:INFO: Dataset: univ                Batch: 12/15	Loss 1.4794 (1.6326)
2022-11-09 17:46:13,728:INFO: Dataset: univ                Batch: 13/15	Loss 1.3371 (1.6111)
2022-11-09 17:46:13,755:INFO: Dataset: univ                Batch: 14/15	Loss 1.3977 (1.5953)
2022-11-09 17:46:13,764:INFO: Dataset: univ                Batch: 15/15	Loss 0.2386 (1.5760)
2022-11-09 17:46:14,015:INFO: Dataset: zara1               Batch: 1/8	Loss 4.7104 (4.7104)
2022-11-09 17:46:14,039:INFO: Dataset: zara1               Batch: 2/8	Loss 3.3473 (3.9501)
2022-11-09 17:46:14,063:INFO: Dataset: zara1               Batch: 3/8	Loss 3.2516 (3.7152)
2022-11-09 17:46:14,087:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6087 (3.1834)
2022-11-09 17:46:14,111:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9607 (2.9208)
2022-11-09 17:46:14,137:INFO: Dataset: zara1               Batch: 6/8	Loss 3.7930 (3.0730)
2022-11-09 17:46:14,161:INFO: Dataset: zara1               Batch: 7/8	Loss 2.3351 (2.9630)
2022-11-09 17:46:14,183:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3731 (2.7488)
2022-11-09 17:46:14,437:INFO: Dataset: zara2               Batch:  1/18	Loss 3.4300 (3.4300)
2022-11-09 17:46:14,492:INFO: Dataset: zara2               Batch:  2/18	Loss 4.5134 (3.9855)
2022-11-09 17:46:14,516:INFO: Dataset: zara2               Batch:  3/18	Loss 3.4821 (3.8152)
2022-11-09 17:46:14,540:INFO: Dataset: zara2               Batch:  4/18	Loss 2.9627 (3.5955)
2022-11-09 17:46:14,564:INFO: Dataset: zara2               Batch:  5/18	Loss 3.4386 (3.5639)
2022-11-09 17:46:14,591:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5587 (3.2624)
2022-11-09 17:46:14,614:INFO: Dataset: zara2               Batch:  7/18	Loss 2.2729 (3.1126)
2022-11-09 17:46:14,638:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5125 (2.9247)
2022-11-09 17:46:14,663:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3712 (2.7457)
2022-11-09 17:46:14,688:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5538 (2.6239)
2022-11-09 17:46:14,715:INFO: Dataset: zara2               Batch: 11/18	Loss 1.6276 (2.5364)
2022-11-09 17:46:14,740:INFO: Dataset: zara2               Batch: 12/18	Loss 1.2949 (2.4277)
2022-11-09 17:46:14,763:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7552 (2.3713)
2022-11-09 17:46:14,787:INFO: Dataset: zara2               Batch: 14/18	Loss 2.5930 (2.3858)
2022-11-09 17:46:14,812:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6073 (2.3334)
2022-11-09 17:46:14,836:INFO: Dataset: zara2               Batch: 16/18	Loss 2.4498 (2.3403)
2022-11-09 17:46:14,861:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7113 (2.2994)
2022-11-09 17:46:14,883:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2402 (2.2483)
2022-11-09 17:46:14,934:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_79.pth.tar
2022-11-09 17:46:14,934:INFO: 
===> EPOCH: 80 (P1)
2022-11-09 17:46:14,934:INFO: - Computing loss (training)
2022-11-09 17:46:15,146:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4580 (1.4580)
2022-11-09 17:46:15,172:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1289 (1.7748)
2022-11-09 17:46:15,197:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7229 (2.0759)
2022-11-09 17:46:15,213:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8127 (1.8676)
2022-11-09 17:46:15,537:INFO: Dataset: univ                Batch:  1/15	Loss 2.1002 (2.1002)
2022-11-09 17:46:15,568:INFO: Dataset: univ                Batch:  2/15	Loss 1.3678 (1.7464)
2022-11-09 17:46:15,596:INFO: Dataset: univ                Batch:  3/15	Loss 1.7668 (1.7536)
2022-11-09 17:46:15,622:INFO: Dataset: univ                Batch:  4/15	Loss 1.3487 (1.6502)
2022-11-09 17:46:15,648:INFO: Dataset: univ                Batch:  5/15	Loss 1.4929 (1.6203)
2022-11-09 17:46:15,677:INFO: Dataset: univ                Batch:  6/15	Loss 2.4608 (1.7624)
2022-11-09 17:46:15,703:INFO: Dataset: univ                Batch:  7/15	Loss 1.7890 (1.7660)
2022-11-09 17:46:15,729:INFO: Dataset: univ                Batch:  8/15	Loss 1.3897 (1.7184)
2022-11-09 17:46:15,756:INFO: Dataset: univ                Batch:  9/15	Loss 1.2324 (1.6656)
2022-11-09 17:46:15,783:INFO: Dataset: univ                Batch: 10/15	Loss 1.2316 (1.6249)
2022-11-09 17:46:15,810:INFO: Dataset: univ                Batch: 11/15	Loss 1.2384 (1.5910)
2022-11-09 17:46:15,838:INFO: Dataset: univ                Batch: 12/15	Loss 1.8879 (1.6113)
2022-11-09 17:46:15,863:INFO: Dataset: univ                Batch: 13/15	Loss 1.2292 (1.5820)
2022-11-09 17:46:15,890:INFO: Dataset: univ                Batch: 14/15	Loss 1.3542 (1.5656)
2022-11-09 17:46:15,900:INFO: Dataset: univ                Batch: 15/15	Loss 0.2201 (1.5477)
2022-11-09 17:46:16,157:INFO: Dataset: zara1               Batch: 1/8	Loss 4.1200 (4.1200)
2022-11-09 17:46:16,182:INFO: Dataset: zara1               Batch: 2/8	Loss 4.8680 (4.5005)
2022-11-09 17:46:16,208:INFO: Dataset: zara1               Batch: 3/8	Loss 6.3703 (5.1858)
2022-11-09 17:46:16,232:INFO: Dataset: zara1               Batch: 4/8	Loss 2.7041 (4.5654)
2022-11-09 17:46:16,255:INFO: Dataset: zara1               Batch: 5/8	Loss 6.2927 (4.8671)
2022-11-09 17:46:16,281:INFO: Dataset: zara1               Batch: 6/8	Loss 2.2052 (4.3932)
2022-11-09 17:46:16,304:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6503 (3.9566)
2022-11-09 17:46:16,325:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6873 (3.7058)
2022-11-09 17:46:16,574:INFO: Dataset: zara2               Batch:  1/18	Loss 4.0344 (4.0344)
2022-11-09 17:46:16,599:INFO: Dataset: zara2               Batch:  2/18	Loss 4.1112 (4.0746)
2022-11-09 17:46:16,627:INFO: Dataset: zara2               Batch:  3/18	Loss 5.4258 (4.5150)
2022-11-09 17:46:16,651:INFO: Dataset: zara2               Batch:  4/18	Loss 4.6871 (4.5587)
2022-11-09 17:46:16,675:INFO: Dataset: zara2               Batch:  5/18	Loss 3.8345 (4.4181)
2022-11-09 17:46:16,701:INFO: Dataset: zara2               Batch:  6/18	Loss 3.6311 (4.2885)
2022-11-09 17:46:16,724:INFO: Dataset: zara2               Batch:  7/18	Loss 4.4924 (4.3163)
2022-11-09 17:46:16,748:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6469 (3.9925)
2022-11-09 17:46:16,772:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9862 (3.7507)
2022-11-09 17:46:16,796:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2765 (3.4884)
2022-11-09 17:46:16,824:INFO: Dataset: zara2               Batch: 11/18	Loss 2.6158 (3.4122)
2022-11-09 17:46:16,849:INFO: Dataset: zara2               Batch: 12/18	Loss 3.0318 (3.3820)
2022-11-09 17:46:16,874:INFO: Dataset: zara2               Batch: 13/18	Loss 2.3197 (3.3023)
2022-11-09 17:46:16,898:INFO: Dataset: zara2               Batch: 14/18	Loss 3.2444 (3.2982)
2022-11-09 17:46:16,922:INFO: Dataset: zara2               Batch: 15/18	Loss 2.1381 (3.2233)
2022-11-09 17:46:16,947:INFO: Dataset: zara2               Batch: 16/18	Loss 2.2620 (3.1716)
2022-11-09 17:46:16,971:INFO: Dataset: zara2               Batch: 17/18	Loss 4.1343 (3.2266)
2022-11-09 17:46:16,993:INFO: Dataset: zara2               Batch: 18/18	Loss 2.5153 (3.1928)
2022-11-09 17:46:17,043:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_80.pth.tar
2022-11-09 17:46:17,043:INFO: 
===> EPOCH: 81 (P1)
2022-11-09 17:46:17,043:INFO: - Computing loss (training)
2022-11-09 17:46:17,269:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4623 (1.4623)
2022-11-09 17:46:17,294:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6090 (1.5300)
2022-11-09 17:46:17,318:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5156 (1.5252)
2022-11-09 17:46:17,334:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9758 (1.4295)
2022-11-09 17:46:17,589:INFO: Dataset: univ                Batch:  1/15	Loss 1.5218 (1.5218)
2022-11-09 17:46:17,618:INFO: Dataset: univ                Batch:  2/15	Loss 1.8479 (1.6793)
2022-11-09 17:46:17,647:INFO: Dataset: univ                Batch:  3/15	Loss 1.7289 (1.6966)
2022-11-09 17:46:17,673:INFO: Dataset: univ                Batch:  4/15	Loss 1.3701 (1.6134)
2022-11-09 17:46:17,698:INFO: Dataset: univ                Batch:  5/15	Loss 1.1611 (1.5325)
2022-11-09 17:46:17,727:INFO: Dataset: univ                Batch:  6/15	Loss 1.3814 (1.5082)
2022-11-09 17:46:17,753:INFO: Dataset: univ                Batch:  7/15	Loss 1.2479 (1.4719)
2022-11-09 17:46:17,778:INFO: Dataset: univ                Batch:  8/15	Loss 1.1621 (1.4344)
2022-11-09 17:46:17,804:INFO: Dataset: univ                Batch:  9/15	Loss 1.3875 (1.4294)
2022-11-09 17:46:17,831:INFO: Dataset: univ                Batch: 10/15	Loss 1.2436 (1.4106)
2022-11-09 17:46:17,858:INFO: Dataset: univ                Batch: 11/15	Loss 1.6792 (1.4375)
2022-11-09 17:46:17,885:INFO: Dataset: univ                Batch: 12/15	Loss 1.4031 (1.4344)
2022-11-09 17:46:17,911:INFO: Dataset: univ                Batch: 13/15	Loss 1.2178 (1.4186)
2022-11-09 17:46:17,936:INFO: Dataset: univ                Batch: 14/15	Loss 1.4445 (1.4202)
2022-11-09 17:46:17,945:INFO: Dataset: univ                Batch: 15/15	Loss 0.2453 (1.4076)
2022-11-09 17:46:18,203:INFO: Dataset: zara1               Batch: 1/8	Loss 3.2713 (3.2713)
2022-11-09 17:46:18,228:INFO: Dataset: zara1               Batch: 2/8	Loss 4.6128 (3.9148)
2022-11-09 17:46:18,253:INFO: Dataset: zara1               Batch: 3/8	Loss 3.6692 (3.8363)
2022-11-09 17:46:18,281:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7071 (3.3057)
2022-11-09 17:46:18,305:INFO: Dataset: zara1               Batch: 5/8	Loss 3.1520 (3.2745)
2022-11-09 17:46:18,330:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5725 (2.9817)
2022-11-09 17:46:18,353:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3908 (2.7741)
2022-11-09 17:46:18,375:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5187 (2.6274)
2022-11-09 17:46:18,641:INFO: Dataset: zara2               Batch:  1/18	Loss 3.1821 (3.1821)
2022-11-09 17:46:18,666:INFO: Dataset: zara2               Batch:  2/18	Loss 3.0283 (3.1141)
2022-11-09 17:46:18,692:INFO: Dataset: zara2               Batch:  3/18	Loss 2.9904 (3.0705)
2022-11-09 17:46:18,718:INFO: Dataset: zara2               Batch:  4/18	Loss 4.2834 (3.3585)
2022-11-09 17:46:18,745:INFO: Dataset: zara2               Batch:  5/18	Loss 2.8951 (3.2630)
2022-11-09 17:46:18,773:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4691 (2.9716)
2022-11-09 17:46:18,797:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8635 (2.8093)
2022-11-09 17:46:18,821:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7003 (2.6761)
2022-11-09 17:46:18,845:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4240 (2.5284)
2022-11-09 17:46:18,871:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6031 (2.4278)
2022-11-09 17:46:18,897:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4635 (2.3410)
2022-11-09 17:46:18,924:INFO: Dataset: zara2               Batch: 12/18	Loss 1.5277 (2.2694)
2022-11-09 17:46:18,948:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7969 (2.2364)
2022-11-09 17:46:18,972:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3906 (2.1712)
2022-11-09 17:46:18,998:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9043 (2.1537)
2022-11-09 17:46:19,023:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2717 (2.0964)
2022-11-09 17:46:19,048:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4853 (2.0573)
2022-11-09 17:46:19,072:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9573 (2.0020)
2022-11-09 17:46:19,128:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_81.pth.tar
2022-11-09 17:46:19,128:INFO: 
===> EPOCH: 82 (P1)
2022-11-09 17:46:19,128:INFO: - Computing loss (training)
2022-11-09 17:46:19,345:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2504 (2.2504)
2022-11-09 17:46:19,370:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3608 (1.8035)
2022-11-09 17:46:19,394:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3114 (1.6343)
2022-11-09 17:46:19,410:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2257 (1.5674)
2022-11-09 17:46:19,666:INFO: Dataset: univ                Batch:  1/15	Loss 1.1828 (1.1828)
2022-11-09 17:46:19,693:INFO: Dataset: univ                Batch:  2/15	Loss 1.1963 (1.1896)
2022-11-09 17:46:19,720:INFO: Dataset: univ                Batch:  3/15	Loss 1.1679 (1.1824)
2022-11-09 17:46:19,747:INFO: Dataset: univ                Batch:  4/15	Loss 1.4348 (1.2456)
2022-11-09 17:46:19,778:INFO: Dataset: univ                Batch:  5/15	Loss 1.2509 (1.2466)
2022-11-09 17:46:19,806:INFO: Dataset: univ                Batch:  6/15	Loss 1.1723 (1.2344)
2022-11-09 17:46:19,831:INFO: Dataset: univ                Batch:  7/15	Loss 1.1891 (1.2285)
2022-11-09 17:46:19,857:INFO: Dataset: univ                Batch:  8/15	Loss 1.1182 (1.2141)
2022-11-09 17:46:19,883:INFO: Dataset: univ                Batch:  9/15	Loss 1.2765 (1.2208)
2022-11-09 17:46:19,909:INFO: Dataset: univ                Batch: 10/15	Loss 1.1811 (1.2172)
2022-11-09 17:46:19,935:INFO: Dataset: univ                Batch: 11/15	Loss 1.2364 (1.2189)
2022-11-09 17:46:19,963:INFO: Dataset: univ                Batch: 12/15	Loss 1.2626 (1.2224)
2022-11-09 17:46:19,988:INFO: Dataset: univ                Batch: 13/15	Loss 1.2615 (1.2253)
2022-11-09 17:46:20,014:INFO: Dataset: univ                Batch: 14/15	Loss 1.1467 (1.2197)
2022-11-09 17:46:20,022:INFO: Dataset: univ                Batch: 15/15	Loss 0.2072 (1.2041)
2022-11-09 17:46:20,274:INFO: Dataset: zara1               Batch: 1/8	Loss 5.0263 (5.0263)
2022-11-09 17:46:20,298:INFO: Dataset: zara1               Batch: 2/8	Loss 5.2239 (5.1225)
2022-11-09 17:46:20,323:INFO: Dataset: zara1               Batch: 3/8	Loss 5.8007 (5.3298)
2022-11-09 17:46:20,347:INFO: Dataset: zara1               Batch: 4/8	Loss 4.7914 (5.1892)
2022-11-09 17:46:20,370:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8081 (4.5046)
2022-11-09 17:46:20,397:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6015 (3.9524)
2022-11-09 17:46:20,420:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4702 (3.6251)
2022-11-09 17:46:20,442:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5020 (3.4173)
2022-11-09 17:46:20,722:INFO: Dataset: zara2               Batch:  1/18	Loss 3.4459 (3.4459)
2022-11-09 17:46:20,750:INFO: Dataset: zara2               Batch:  2/18	Loss 3.5308 (3.4894)
2022-11-09 17:46:20,776:INFO: Dataset: zara2               Batch:  3/18	Loss 3.9954 (3.6596)
2022-11-09 17:46:20,802:INFO: Dataset: zara2               Batch:  4/18	Loss 4.0682 (3.7631)
2022-11-09 17:46:20,827:INFO: Dataset: zara2               Batch:  5/18	Loss 3.1823 (3.6548)
2022-11-09 17:46:20,856:INFO: Dataset: zara2               Batch:  6/18	Loss 3.3648 (3.6085)
2022-11-09 17:46:20,881:INFO: Dataset: zara2               Batch:  7/18	Loss 2.2279 (3.4256)
2022-11-09 17:46:20,906:INFO: Dataset: zara2               Batch:  8/18	Loss 2.1426 (3.2721)
2022-11-09 17:46:20,930:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2507 (3.0264)
2022-11-09 17:46:20,956:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8634 (2.9066)
2022-11-09 17:46:20,982:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2752 (2.7640)
2022-11-09 17:46:21,008:INFO: Dataset: zara2               Batch: 12/18	Loss 2.4082 (2.7337)
2022-11-09 17:46:21,033:INFO: Dataset: zara2               Batch: 13/18	Loss 2.5333 (2.7179)
2022-11-09 17:46:21,058:INFO: Dataset: zara2               Batch: 14/18	Loss 2.8874 (2.7316)
2022-11-09 17:46:21,084:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2624 (2.6322)
2022-11-09 17:46:21,110:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0929 (2.5962)
2022-11-09 17:46:21,135:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0512 (2.5624)
2022-11-09 17:46:21,159:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1934 (2.5006)
2022-11-09 17:46:21,209:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_82.pth.tar
2022-11-09 17:46:21,209:INFO: 
===> EPOCH: 83 (P1)
2022-11-09 17:46:21,209:INFO: - Computing loss (training)
2022-11-09 17:46:21,421:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3419 (1.3419)
2022-11-09 17:46:21,447:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4993 (1.4200)
2022-11-09 17:46:21,471:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5630 (1.4681)
2022-11-09 17:46:21,487:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9800 (1.3901)
2022-11-09 17:46:21,750:INFO: Dataset: univ                Batch:  1/15	Loss 1.2038 (1.2038)
2022-11-09 17:46:21,778:INFO: Dataset: univ                Batch:  2/15	Loss 1.1836 (1.1940)
2022-11-09 17:46:21,806:INFO: Dataset: univ                Batch:  3/15	Loss 1.1436 (1.1773)
2022-11-09 17:46:21,832:INFO: Dataset: univ                Batch:  4/15	Loss 1.2878 (1.2039)
2022-11-09 17:46:21,862:INFO: Dataset: univ                Batch:  5/15	Loss 1.2981 (1.2212)
2022-11-09 17:46:21,889:INFO: Dataset: univ                Batch:  6/15	Loss 1.7856 (1.3079)
2022-11-09 17:46:21,915:INFO: Dataset: univ                Batch:  7/15	Loss 1.0657 (1.2718)
2022-11-09 17:46:21,941:INFO: Dataset: univ                Batch:  8/15	Loss 1.1142 (1.2549)
2022-11-09 17:46:21,969:INFO: Dataset: univ                Batch:  9/15	Loss 1.1551 (1.2431)
2022-11-09 17:46:21,997:INFO: Dataset: univ                Batch: 10/15	Loss 1.1320 (1.2315)
2022-11-09 17:46:22,027:INFO: Dataset: univ                Batch: 11/15	Loss 1.1314 (1.2221)
2022-11-09 17:46:22,055:INFO: Dataset: univ                Batch: 12/15	Loss 1.1179 (1.2134)
2022-11-09 17:46:22,082:INFO: Dataset: univ                Batch: 13/15	Loss 1.3235 (1.2227)
2022-11-09 17:46:22,108:INFO: Dataset: univ                Batch: 14/15	Loss 1.4170 (1.2364)
2022-11-09 17:46:22,118:INFO: Dataset: univ                Batch: 15/15	Loss 0.2135 (1.2239)
2022-11-09 17:46:22,364:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9509 (1.9509)
2022-11-09 17:46:22,390:INFO: Dataset: zara1               Batch: 2/8	Loss 3.3406 (2.6338)
2022-11-09 17:46:22,417:INFO: Dataset: zara1               Batch: 3/8	Loss 3.4541 (2.9065)
2022-11-09 17:46:22,443:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6062 (2.5459)
2022-11-09 17:46:22,469:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4263 (2.3521)
2022-11-09 17:46:22,495:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4113 (2.1745)
2022-11-09 17:46:22,520:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2881 (2.0506)
2022-11-09 17:46:22,542:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0912 (1.9400)
2022-11-09 17:46:22,870:INFO: Dataset: zara2               Batch:  1/18	Loss 2.8609 (2.8609)
2022-11-09 17:46:22,896:INFO: Dataset: zara2               Batch:  2/18	Loss 2.1820 (2.5267)
2022-11-09 17:46:22,919:INFO: Dataset: zara2               Batch:  3/18	Loss 3.3855 (2.8138)
2022-11-09 17:46:22,943:INFO: Dataset: zara2               Batch:  4/18	Loss 2.6903 (2.7842)
2022-11-09 17:46:22,967:INFO: Dataset: zara2               Batch:  5/18	Loss 3.0103 (2.8283)
2022-11-09 17:46:22,994:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8010 (2.6634)
2022-11-09 17:46:23,018:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3019 (2.4721)
2022-11-09 17:46:23,041:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2799 (2.3497)
2022-11-09 17:46:23,064:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2854 (2.2272)
2022-11-09 17:46:23,088:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3225 (2.1439)
2022-11-09 17:46:23,116:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4877 (2.0888)
2022-11-09 17:46:23,142:INFO: Dataset: zara2               Batch: 12/18	Loss 2.4532 (2.1186)
2022-11-09 17:46:23,168:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1901 (2.0426)
2022-11-09 17:46:23,193:INFO: Dataset: zara2               Batch: 14/18	Loss 2.8625 (2.1014)
2022-11-09 17:46:23,218:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3101 (2.0465)
2022-11-09 17:46:23,242:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6122 (2.0165)
2022-11-09 17:46:23,267:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5213 (1.9834)
2022-11-09 17:46:23,288:INFO: Dataset: zara2               Batch: 18/18	Loss 2.4049 (2.0023)
2022-11-09 17:46:23,338:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_83.pth.tar
2022-11-09 17:46:23,338:INFO: 
===> EPOCH: 84 (P1)
2022-11-09 17:46:23,338:INFO: - Computing loss (training)
2022-11-09 17:46:23,547:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4729 (2.4729)
2022-11-09 17:46:23,573:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2774 (1.8823)
2022-11-09 17:46:23,598:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2776 (1.6722)
2022-11-09 17:46:23,615:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8421 (1.5353)
2022-11-09 17:46:23,863:INFO: Dataset: univ                Batch:  1/15	Loss 1.0863 (1.0863)
2022-11-09 17:46:23,892:INFO: Dataset: univ                Batch:  2/15	Loss 1.5689 (1.3313)
2022-11-09 17:46:23,918:INFO: Dataset: univ                Batch:  3/15	Loss 1.7411 (1.4689)
2022-11-09 17:46:23,948:INFO: Dataset: univ                Batch:  4/15	Loss 1.2110 (1.4035)
2022-11-09 17:46:23,974:INFO: Dataset: univ                Batch:  5/15	Loss 1.0807 (1.3459)
2022-11-09 17:46:24,002:INFO: Dataset: univ                Batch:  6/15	Loss 1.3418 (1.3452)
2022-11-09 17:46:24,027:INFO: Dataset: univ                Batch:  7/15	Loss 1.3507 (1.3459)
2022-11-09 17:46:24,052:INFO: Dataset: univ                Batch:  8/15	Loss 1.1178 (1.3191)
2022-11-09 17:46:24,079:INFO: Dataset: univ                Batch:  9/15	Loss 1.1372 (1.3000)
2022-11-09 17:46:24,105:INFO: Dataset: univ                Batch: 10/15	Loss 1.0821 (1.2796)
2022-11-09 17:46:24,132:INFO: Dataset: univ                Batch: 11/15	Loss 1.2092 (1.2732)
2022-11-09 17:46:24,159:INFO: Dataset: univ                Batch: 12/15	Loss 1.0731 (1.2575)
2022-11-09 17:46:24,185:INFO: Dataset: univ                Batch: 13/15	Loss 1.4014 (1.2700)
2022-11-09 17:46:24,212:INFO: Dataset: univ                Batch: 14/15	Loss 1.0351 (1.2547)
2022-11-09 17:46:24,222:INFO: Dataset: univ                Batch: 15/15	Loss 0.1684 (1.2401)
2022-11-09 17:46:24,481:INFO: Dataset: zara1               Batch: 1/8	Loss 2.8677 (2.8677)
2022-11-09 17:46:24,507:INFO: Dataset: zara1               Batch: 2/8	Loss 3.5493 (3.2057)
2022-11-09 17:46:24,530:INFO: Dataset: zara1               Batch: 3/8	Loss 2.1274 (2.8231)
2022-11-09 17:46:24,553:INFO: Dataset: zara1               Batch: 4/8	Loss 3.1211 (2.8918)
2022-11-09 17:46:24,577:INFO: Dataset: zara1               Batch: 5/8	Loss 2.6163 (2.8376)
2022-11-09 17:46:24,603:INFO: Dataset: zara1               Batch: 6/8	Loss 2.3146 (2.7552)
2022-11-09 17:46:24,626:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2994 (2.5392)
2022-11-09 17:46:24,647:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6697 (2.4376)
2022-11-09 17:46:24,955:INFO: Dataset: zara2               Batch:  1/18	Loss 2.5995 (2.5995)
2022-11-09 17:46:24,984:INFO: Dataset: zara2               Batch:  2/18	Loss 3.9498 (3.2887)
2022-11-09 17:46:25,007:INFO: Dataset: zara2               Batch:  3/18	Loss 2.6624 (3.0914)
2022-11-09 17:46:25,031:INFO: Dataset: zara2               Batch:  4/18	Loss 3.1666 (3.1099)
2022-11-09 17:46:25,055:INFO: Dataset: zara2               Batch:  5/18	Loss 2.4096 (2.9673)
2022-11-09 17:46:25,083:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7926 (2.7612)
2022-11-09 17:46:25,106:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3450 (2.5493)
2022-11-09 17:46:25,130:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0664 (2.3649)
2022-11-09 17:46:25,154:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1114 (2.2124)
2022-11-09 17:46:25,180:INFO: Dataset: zara2               Batch: 10/18	Loss 1.4275 (2.1364)
2022-11-09 17:46:25,207:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8419 (2.1104)
2022-11-09 17:46:25,234:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6877 (2.0726)
2022-11-09 17:46:25,260:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1691 (2.0062)
2022-11-09 17:46:25,285:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3680 (1.9606)
2022-11-09 17:46:25,310:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2516 (1.9130)
2022-11-09 17:46:25,335:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5407 (1.8881)
2022-11-09 17:46:25,360:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4726 (1.8630)
2022-11-09 17:46:25,382:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9476 (1.8199)
2022-11-09 17:46:25,433:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_84.pth.tar
2022-11-09 17:46:25,433:INFO: 
===> EPOCH: 85 (P1)
2022-11-09 17:46:25,434:INFO: - Computing loss (training)
2022-11-09 17:46:25,637:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4957 (1.4957)
2022-11-09 17:46:25,663:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1680 (1.3326)
2022-11-09 17:46:25,689:INFO: Dataset: hotel               Batch: 3/4	Loss 3.5940 (2.0793)
2022-11-09 17:46:25,705:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7884 (1.8767)
2022-11-09 17:46:25,956:INFO: Dataset: univ                Batch:  1/15	Loss 1.3031 (1.3031)
2022-11-09 17:46:25,989:INFO: Dataset: univ                Batch:  2/15	Loss 1.0421 (1.1766)
2022-11-09 17:46:26,015:INFO: Dataset: univ                Batch:  3/15	Loss 1.1700 (1.1744)
2022-11-09 17:46:26,041:INFO: Dataset: univ                Batch:  4/15	Loss 1.6909 (1.3108)
2022-11-09 17:46:26,066:INFO: Dataset: univ                Batch:  5/15	Loss 1.4878 (1.3473)
2022-11-09 17:46:26,095:INFO: Dataset: univ                Batch:  6/15	Loss 1.3192 (1.3428)
2022-11-09 17:46:26,120:INFO: Dataset: univ                Batch:  7/15	Loss 1.1634 (1.3189)
2022-11-09 17:46:26,146:INFO: Dataset: univ                Batch:  8/15	Loss 1.1431 (1.2982)
2022-11-09 17:46:26,172:INFO: Dataset: univ                Batch:  9/15	Loss 1.0552 (1.2728)
2022-11-09 17:46:26,199:INFO: Dataset: univ                Batch: 10/15	Loss 1.3976 (1.2865)
2022-11-09 17:46:26,225:INFO: Dataset: univ                Batch: 11/15	Loss 1.0077 (1.2611)
2022-11-09 17:46:26,253:INFO: Dataset: univ                Batch: 12/15	Loss 1.0173 (1.2400)
2022-11-09 17:46:26,278:INFO: Dataset: univ                Batch: 13/15	Loss 1.0017 (1.2232)
2022-11-09 17:46:26,304:INFO: Dataset: univ                Batch: 14/15	Loss 1.1036 (1.2154)
2022-11-09 17:46:26,314:INFO: Dataset: univ                Batch: 15/15	Loss 0.1802 (1.2000)
2022-11-09 17:46:26,563:INFO: Dataset: zara1               Batch: 1/8	Loss 5.0375 (5.0375)
2022-11-09 17:46:26,588:INFO: Dataset: zara1               Batch: 2/8	Loss 4.0963 (4.5822)
2022-11-09 17:46:26,613:INFO: Dataset: zara1               Batch: 3/8	Loss 2.7042 (4.0036)
2022-11-09 17:46:26,640:INFO: Dataset: zara1               Batch: 4/8	Loss 2.9624 (3.7584)
2022-11-09 17:46:26,664:INFO: Dataset: zara1               Batch: 5/8	Loss 2.4201 (3.4944)
2022-11-09 17:46:26,689:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2624 (3.1421)
2022-11-09 17:46:26,712:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5534 (2.9214)
2022-11-09 17:46:26,734:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3643 (2.7510)
2022-11-09 17:46:26,979:INFO: Dataset: zara2               Batch:  1/18	Loss 4.1842 (4.1842)
2022-11-09 17:46:27,005:INFO: Dataset: zara2               Batch:  2/18	Loss 4.5018 (4.3430)
2022-11-09 17:46:27,030:INFO: Dataset: zara2               Batch:  3/18	Loss 2.9490 (3.8589)
2022-11-09 17:46:27,056:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7907 (3.3089)
2022-11-09 17:46:27,084:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6491 (2.9865)
2022-11-09 17:46:27,110:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5036 (2.7417)
2022-11-09 17:46:27,134:INFO: Dataset: zara2               Batch:  7/18	Loss 1.5359 (2.5699)
2022-11-09 17:46:27,158:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0019 (2.3760)
2022-11-09 17:46:27,182:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0855 (2.2238)
2022-11-09 17:46:27,207:INFO: Dataset: zara2               Batch: 10/18	Loss 1.4989 (2.1544)
2022-11-09 17:46:27,235:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1037 (2.0544)
2022-11-09 17:46:27,260:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0461 (1.9702)
2022-11-09 17:46:27,284:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0572 (1.9032)
2022-11-09 17:46:27,308:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3247 (1.8651)
2022-11-09 17:46:27,334:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9734 (1.8727)
2022-11-09 17:46:27,358:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5884 (1.8540)
2022-11-09 17:46:27,383:INFO: Dataset: zara2               Batch: 17/18	Loss 1.1192 (1.8100)
2022-11-09 17:46:27,406:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8776 (1.7677)
2022-11-09 17:46:27,456:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_85.pth.tar
2022-11-09 17:46:27,456:INFO: 
===> EPOCH: 86 (P1)
2022-11-09 17:46:27,457:INFO: - Computing loss (training)
2022-11-09 17:46:27,664:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8271 (1.8271)
2022-11-09 17:46:27,690:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9359 (1.8806)
2022-11-09 17:46:27,715:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2573 (1.6573)
2022-11-09 17:46:27,732:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7527 (1.5058)
2022-11-09 17:46:27,981:INFO: Dataset: univ                Batch:  1/15	Loss 1.0087 (1.0087)
2022-11-09 17:46:28,011:INFO: Dataset: univ                Batch:  2/15	Loss 1.4145 (1.2150)
2022-11-09 17:46:28,037:INFO: Dataset: univ                Batch:  3/15	Loss 0.9734 (1.1325)
2022-11-09 17:46:28,065:INFO: Dataset: univ                Batch:  4/15	Loss 1.1046 (1.1252)
2022-11-09 17:46:28,091:INFO: Dataset: univ                Batch:  5/15	Loss 0.9927 (1.1013)
2022-11-09 17:46:28,121:INFO: Dataset: univ                Batch:  6/15	Loss 1.0941 (1.1000)
2022-11-09 17:46:28,146:INFO: Dataset: univ                Batch:  7/15	Loss 1.0412 (1.0924)
2022-11-09 17:46:28,172:INFO: Dataset: univ                Batch:  8/15	Loss 0.9909 (1.0791)
2022-11-09 17:46:28,198:INFO: Dataset: univ                Batch:  9/15	Loss 1.0042 (1.0706)
2022-11-09 17:46:28,225:INFO: Dataset: univ                Batch: 10/15	Loss 1.1618 (1.0804)
2022-11-09 17:46:28,251:INFO: Dataset: univ                Batch: 11/15	Loss 1.0583 (1.0784)
2022-11-09 17:46:28,279:INFO: Dataset: univ                Batch: 12/15	Loss 1.0554 (1.0766)
2022-11-09 17:46:28,305:INFO: Dataset: univ                Batch: 13/15	Loss 1.6948 (1.1176)
2022-11-09 17:46:28,331:INFO: Dataset: univ                Batch: 14/15	Loss 1.0629 (1.1138)
2022-11-09 17:46:28,340:INFO: Dataset: univ                Batch: 15/15	Loss 0.1772 (1.0982)
2022-11-09 17:46:28,600:INFO: Dataset: zara1               Batch: 1/8	Loss 5.1728 (5.1728)
2022-11-09 17:46:28,626:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0594 (3.3883)
2022-11-09 17:46:28,651:INFO: Dataset: zara1               Batch: 3/8	Loss 2.7873 (3.2109)
2022-11-09 17:46:28,674:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4082 (2.7176)
2022-11-09 17:46:28,699:INFO: Dataset: zara1               Batch: 5/8	Loss 2.3606 (2.6451)
2022-11-09 17:46:28,725:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3389 (2.4366)
2022-11-09 17:46:28,749:INFO: Dataset: zara1               Batch: 7/8	Loss 2.4523 (2.4388)
2022-11-09 17:46:28,769:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4576 (2.3211)
2022-11-09 17:46:29,017:INFO: Dataset: zara2               Batch:  1/18	Loss 3.6388 (3.6388)
2022-11-09 17:46:29,046:INFO: Dataset: zara2               Batch:  2/18	Loss 3.6413 (3.6401)
2022-11-09 17:46:29,071:INFO: Dataset: zara2               Batch:  3/18	Loss 2.9188 (3.4015)
2022-11-09 17:46:29,095:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0234 (3.0611)
2022-11-09 17:46:29,119:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6520 (2.7859)
2022-11-09 17:46:29,146:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1694 (2.5102)
2022-11-09 17:46:29,171:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9757 (2.2877)
2022-11-09 17:46:29,195:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1528 (2.1507)
2022-11-09 17:46:29,218:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1880 (2.0476)
2022-11-09 17:46:29,243:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1255 (1.9610)
2022-11-09 17:46:29,269:INFO: Dataset: zara2               Batch: 11/18	Loss 1.6549 (1.9347)
2022-11-09 17:46:29,295:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1262 (1.8672)
2022-11-09 17:46:29,319:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1497 (1.8158)
2022-11-09 17:46:29,344:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5015 (1.7954)
2022-11-09 17:46:29,368:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4780 (1.7732)
2022-11-09 17:46:29,393:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1089 (1.7345)
2022-11-09 17:46:29,418:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2674 (1.7055)
2022-11-09 17:46:29,441:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9146 (1.7143)
2022-11-09 17:46:29,493:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_86.pth.tar
2022-11-09 17:46:29,493:INFO: 
===> EPOCH: 87 (P1)
2022-11-09 17:46:29,493:INFO: - Computing loss (training)
2022-11-09 17:46:29,701:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2533 (1.2533)
2022-11-09 17:46:29,727:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2899 (1.2710)
2022-11-09 17:46:29,752:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4498 (1.3299)
2022-11-09 17:46:29,769:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9071 (1.2624)
2022-11-09 17:46:30,027:INFO: Dataset: univ                Batch:  1/15	Loss 1.0525 (1.0525)
2022-11-09 17:46:30,057:INFO: Dataset: univ                Batch:  2/15	Loss 0.9035 (0.9765)
2022-11-09 17:46:30,085:INFO: Dataset: univ                Batch:  3/15	Loss 1.0444 (0.9989)
2022-11-09 17:46:30,113:INFO: Dataset: univ                Batch:  4/15	Loss 1.0400 (1.0095)
2022-11-09 17:46:30,139:INFO: Dataset: univ                Batch:  5/15	Loss 1.2030 (1.0486)
2022-11-09 17:46:30,169:INFO: Dataset: univ                Batch:  6/15	Loss 0.9232 (1.0270)
2022-11-09 17:46:30,195:INFO: Dataset: univ                Batch:  7/15	Loss 0.9844 (1.0214)
2022-11-09 17:46:30,221:INFO: Dataset: univ                Batch:  8/15	Loss 1.1369 (1.0351)
2022-11-09 17:46:30,248:INFO: Dataset: univ                Batch:  9/15	Loss 0.9772 (1.0284)
2022-11-09 17:46:30,276:INFO: Dataset: univ                Batch: 10/15	Loss 1.4091 (1.0693)
2022-11-09 17:46:30,305:INFO: Dataset: univ                Batch: 11/15	Loss 0.9312 (1.0581)
2022-11-09 17:46:30,332:INFO: Dataset: univ                Batch: 12/15	Loss 1.0298 (1.0559)
2022-11-09 17:46:30,358:INFO: Dataset: univ                Batch: 13/15	Loss 0.9815 (1.0498)
2022-11-09 17:46:30,383:INFO: Dataset: univ                Batch: 14/15	Loss 0.9426 (1.0419)
2022-11-09 17:46:30,393:INFO: Dataset: univ                Batch: 15/15	Loss 0.1584 (1.0310)
2022-11-09 17:46:30,653:INFO: Dataset: zara1               Batch: 1/8	Loss 4.6532 (4.6532)
2022-11-09 17:46:30,679:INFO: Dataset: zara1               Batch: 2/8	Loss 3.6150 (4.1330)
2022-11-09 17:46:30,703:INFO: Dataset: zara1               Batch: 3/8	Loss 2.9415 (3.7131)
2022-11-09 17:46:30,729:INFO: Dataset: zara1               Batch: 4/8	Loss 2.1299 (3.3091)
2022-11-09 17:46:30,753:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1711 (2.9207)
2022-11-09 17:46:30,778:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2923 (2.6080)
2022-11-09 17:46:30,801:INFO: Dataset: zara1               Batch: 7/8	Loss 3.7314 (2.7524)
2022-11-09 17:46:30,822:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8884 (2.6491)
2022-11-09 17:46:31,070:INFO: Dataset: zara2               Batch:  1/18	Loss 3.6946 (3.6946)
2022-11-09 17:46:31,095:INFO: Dataset: zara2               Batch:  2/18	Loss 2.6010 (3.1771)
2022-11-09 17:46:31,119:INFO: Dataset: zara2               Batch:  3/18	Loss 2.3955 (2.9207)
2022-11-09 17:46:31,146:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3036 (2.5131)
2022-11-09 17:46:31,171:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5407 (2.3169)
2022-11-09 17:46:31,197:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8525 (2.0919)
2022-11-09 17:46:31,221:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2402 (1.9674)
2022-11-09 17:46:31,245:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0584 (1.8517)
2022-11-09 17:46:31,268:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1017 (1.7686)
2022-11-09 17:46:31,293:INFO: Dataset: zara2               Batch: 10/18	Loss 2.9580 (1.8924)
2022-11-09 17:46:31,317:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0170 (1.9040)
2022-11-09 17:46:31,344:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4425 (1.8645)
2022-11-09 17:46:31,369:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0217 (1.7965)
2022-11-09 17:46:31,393:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6454 (1.7853)
2022-11-09 17:46:31,418:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9772 (1.7992)
2022-11-09 17:46:31,442:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4787 (1.7781)
2022-11-09 17:46:31,466:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0132 (1.7339)
2022-11-09 17:46:31,488:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7916 (1.6930)
2022-11-09 17:46:31,538:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_87.pth.tar
2022-11-09 17:46:31,538:INFO: 
===> EPOCH: 88 (P1)
2022-11-09 17:46:31,539:INFO: - Computing loss (training)
2022-11-09 17:46:31,756:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1490 (1.1490)
2022-11-09 17:46:31,783:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1736 (1.6698)
2022-11-09 17:46:31,807:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6555 (1.6650)
2022-11-09 17:46:31,824:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8973 (1.5384)
2022-11-09 17:46:32,062:INFO: Dataset: univ                Batch:  1/15	Loss 1.6312 (1.6312)
2022-11-09 17:46:32,090:INFO: Dataset: univ                Batch:  2/15	Loss 1.6484 (1.6395)
2022-11-09 17:46:32,117:INFO: Dataset: univ                Batch:  3/15	Loss 1.6996 (1.6603)
2022-11-09 17:46:32,145:INFO: Dataset: univ                Batch:  4/15	Loss 1.4857 (1.6199)
2022-11-09 17:46:32,172:INFO: Dataset: univ                Batch:  5/15	Loss 2.3276 (1.7647)
2022-11-09 17:46:32,200:INFO: Dataset: univ                Batch:  6/15	Loss 1.7999 (1.7707)
2022-11-09 17:46:32,225:INFO: Dataset: univ                Batch:  7/15	Loss 1.5547 (1.7425)
2022-11-09 17:46:32,251:INFO: Dataset: univ                Batch:  8/15	Loss 1.0307 (1.6528)
2022-11-09 17:46:32,277:INFO: Dataset: univ                Batch:  9/15	Loss 1.0027 (1.5790)
2022-11-09 17:46:32,303:INFO: Dataset: univ                Batch: 10/15	Loss 0.9697 (1.5204)
2022-11-09 17:46:32,331:INFO: Dataset: univ                Batch: 11/15	Loss 1.4350 (1.5131)
2022-11-09 17:46:32,358:INFO: Dataset: univ                Batch: 12/15	Loss 1.1730 (1.4830)
2022-11-09 17:46:32,383:INFO: Dataset: univ                Batch: 13/15	Loss 0.9714 (1.4431)
2022-11-09 17:46:32,409:INFO: Dataset: univ                Batch: 14/15	Loss 1.0195 (1.4147)
2022-11-09 17:46:32,419:INFO: Dataset: univ                Batch: 15/15	Loss 0.1889 (1.3999)
2022-11-09 17:46:32,672:INFO: Dataset: zara1               Batch: 1/8	Loss 5.0793 (5.0793)
2022-11-09 17:46:32,696:INFO: Dataset: zara1               Batch: 2/8	Loss 5.7408 (5.4195)
2022-11-09 17:46:32,720:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6864 (4.1164)
2022-11-09 17:46:32,744:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6614 (3.4696)
2022-11-09 17:46:32,767:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1081 (2.9772)
2022-11-09 17:46:32,793:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4466 (2.7132)
2022-11-09 17:46:32,816:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3186 (2.5150)
2022-11-09 17:46:32,837:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5076 (2.4031)
2022-11-09 17:46:33,099:INFO: Dataset: zara2               Batch:  1/18	Loss 2.6071 (2.6071)
2022-11-09 17:46:33,124:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0582 (2.3391)
2022-11-09 17:46:33,151:INFO: Dataset: zara2               Batch:  3/18	Loss 2.1214 (2.2692)
2022-11-09 17:46:33,175:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7061 (2.1271)
2022-11-09 17:46:33,199:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0275 (1.9056)
2022-11-09 17:46:33,228:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4577 (1.8274)
2022-11-09 17:46:33,252:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1633 (1.7282)
2022-11-09 17:46:33,275:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0746 (1.6464)
2022-11-09 17:46:33,299:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1065 (1.5880)
2022-11-09 17:46:33,323:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9951 (1.5271)
2022-11-09 17:46:33,348:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9196 (1.4800)
2022-11-09 17:46:33,374:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4183 (1.4744)
2022-11-09 17:46:33,398:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0035 (1.4358)
2022-11-09 17:46:33,422:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0857 (1.4120)
2022-11-09 17:46:33,447:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0737 (1.3882)
2022-11-09 17:46:33,471:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4942 (1.3947)
2022-11-09 17:46:33,496:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0278 (1.3725)
2022-11-09 17:46:33,518:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8522 (1.3482)
2022-11-09 17:46:33,566:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_88.pth.tar
2022-11-09 17:46:33,567:INFO: 
===> EPOCH: 89 (P1)
2022-11-09 17:46:33,567:INFO: - Computing loss (training)
2022-11-09 17:46:33,782:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3339 (1.3339)
2022-11-09 17:46:33,808:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1979 (1.2617)
2022-11-09 17:46:33,832:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5798 (1.3691)
2022-11-09 17:46:33,848:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7067 (1.2555)
2022-11-09 17:46:34,100:INFO: Dataset: univ                Batch:  1/15	Loss 0.9841 (0.9841)
2022-11-09 17:46:34,128:INFO: Dataset: univ                Batch:  2/15	Loss 1.7643 (1.4141)
2022-11-09 17:46:34,154:INFO: Dataset: univ                Batch:  3/15	Loss 1.1382 (1.3229)
2022-11-09 17:46:34,184:INFO: Dataset: univ                Batch:  4/15	Loss 1.6606 (1.4118)
2022-11-09 17:46:34,210:INFO: Dataset: univ                Batch:  5/15	Loss 1.4457 (1.4187)
2022-11-09 17:46:34,238:INFO: Dataset: univ                Batch:  6/15	Loss 0.9937 (1.3516)
2022-11-09 17:46:34,264:INFO: Dataset: univ                Batch:  7/15	Loss 1.0616 (1.3167)
2022-11-09 17:46:34,289:INFO: Dataset: univ                Batch:  8/15	Loss 1.2694 (1.3105)
2022-11-09 17:46:34,316:INFO: Dataset: univ                Batch:  9/15	Loss 1.0596 (1.2848)
2022-11-09 17:46:34,343:INFO: Dataset: univ                Batch: 10/15	Loss 1.5351 (1.3080)
2022-11-09 17:46:34,371:INFO: Dataset: univ                Batch: 11/15	Loss 0.9053 (1.2733)
2022-11-09 17:46:34,398:INFO: Dataset: univ                Batch: 12/15	Loss 0.9460 (1.2452)
2022-11-09 17:46:34,424:INFO: Dataset: univ                Batch: 13/15	Loss 0.8820 (1.2140)
2022-11-09 17:46:34,449:INFO: Dataset: univ                Batch: 14/15	Loss 0.9530 (1.1965)
2022-11-09 17:46:34,459:INFO: Dataset: univ                Batch: 15/15	Loss 0.2188 (1.1850)
2022-11-09 17:46:34,714:INFO: Dataset: zara1               Batch: 1/8	Loss 3.7874 (3.7874)
2022-11-09 17:46:34,739:INFO: Dataset: zara1               Batch: 2/8	Loss 2.3595 (3.0919)
2022-11-09 17:46:34,764:INFO: Dataset: zara1               Batch: 3/8	Loss 2.2167 (2.8014)
2022-11-09 17:46:34,791:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2334 (2.4443)
2022-11-09 17:46:34,815:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1129 (2.1692)
2022-11-09 17:46:34,841:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2390 (2.0172)
2022-11-09 17:46:34,865:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1858 (1.9106)
2022-11-09 17:46:34,887:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3922 (1.8520)
2022-11-09 17:46:35,135:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3236 (1.3236)
2022-11-09 17:46:35,162:INFO: Dataset: zara2               Batch:  2/18	Loss 3.2350 (2.3523)
2022-11-09 17:46:35,186:INFO: Dataset: zara2               Batch:  3/18	Loss 3.0123 (2.5618)
2022-11-09 17:46:35,213:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2145 (2.2081)
2022-11-09 17:46:35,236:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2203 (2.0013)
2022-11-09 17:46:35,265:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0387 (1.8251)
2022-11-09 17:46:35,289:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0409 (1.7080)
2022-11-09 17:46:35,313:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9144 (1.6171)
2022-11-09 17:46:35,336:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0654 (1.5551)
2022-11-09 17:46:35,360:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9402 (1.4895)
2022-11-09 17:46:35,385:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0624 (1.4508)
2022-11-09 17:46:35,411:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0983 (1.4227)
2022-11-09 17:46:35,435:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1314 (1.3986)
2022-11-09 17:46:35,458:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9735 (1.3656)
2022-11-09 17:46:35,482:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2855 (1.3602)
2022-11-09 17:46:35,507:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3080 (1.3569)
2022-11-09 17:46:35,531:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6088 (1.3700)
2022-11-09 17:46:35,553:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7583 (1.3393)
2022-11-09 17:46:35,604:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_89.pth.tar
2022-11-09 17:46:35,604:INFO: 
===> EPOCH: 90 (P1)
2022-11-09 17:46:35,604:INFO: - Computing loss (training)
2022-11-09 17:46:35,804:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1239 (1.1239)
2022-11-09 17:46:35,830:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4883 (1.3065)
2022-11-09 17:46:35,854:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3353 (1.3158)
2022-11-09 17:46:35,871:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8237 (1.2288)
2022-11-09 17:46:36,121:INFO: Dataset: univ                Batch:  1/15	Loss 0.8854 (0.8854)
2022-11-09 17:46:36,149:INFO: Dataset: univ                Batch:  2/15	Loss 0.9376 (0.9122)
2022-11-09 17:46:36,179:INFO: Dataset: univ                Batch:  3/15	Loss 1.0600 (0.9606)
2022-11-09 17:46:36,205:INFO: Dataset: univ                Batch:  4/15	Loss 1.1733 (1.0230)
2022-11-09 17:46:36,232:INFO: Dataset: univ                Batch:  5/15	Loss 0.8433 (0.9885)
2022-11-09 17:46:36,261:INFO: Dataset: univ                Batch:  6/15	Loss 1.5181 (1.0730)
2022-11-09 17:46:36,286:INFO: Dataset: univ                Batch:  7/15	Loss 0.8772 (1.0479)
2022-11-09 17:46:36,312:INFO: Dataset: univ                Batch:  8/15	Loss 0.8854 (1.0276)
2022-11-09 17:46:36,338:INFO: Dataset: univ                Batch:  9/15	Loss 0.9875 (1.0229)
2022-11-09 17:46:36,364:INFO: Dataset: univ                Batch: 10/15	Loss 1.0374 (1.0243)
2022-11-09 17:46:36,391:INFO: Dataset: univ                Batch: 11/15	Loss 1.2718 (1.0477)
2022-11-09 17:46:36,418:INFO: Dataset: univ                Batch: 12/15	Loss 0.9942 (1.0433)
2022-11-09 17:46:36,443:INFO: Dataset: univ                Batch: 13/15	Loss 1.3332 (1.0645)
2022-11-09 17:46:36,468:INFO: Dataset: univ                Batch: 14/15	Loss 0.9440 (1.0552)
2022-11-09 17:46:36,478:INFO: Dataset: univ                Batch: 15/15	Loss 0.2132 (1.0446)
2022-11-09 17:46:36,745:INFO: Dataset: zara1               Batch: 1/8	Loss 6.0207 (6.0207)
2022-11-09 17:46:36,770:INFO: Dataset: zara1               Batch: 2/8	Loss 2.9583 (4.4450)
2022-11-09 17:46:36,794:INFO: Dataset: zara1               Batch: 3/8	Loss 3.4272 (4.0894)
2022-11-09 17:46:36,819:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6144 (3.3949)
2022-11-09 17:46:36,843:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1801 (2.9281)
2022-11-09 17:46:36,868:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1379 (2.6277)
2022-11-09 17:46:36,892:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6539 (2.4902)
2022-11-09 17:46:36,912:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9909 (2.4358)
2022-11-09 17:46:37,177:INFO: Dataset: zara2               Batch:  1/18	Loss 2.9497 (2.9497)
2022-11-09 17:46:37,232:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9676 (2.4502)
2022-11-09 17:46:37,257:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6341 (2.1722)
2022-11-09 17:46:37,282:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4445 (1.9911)
2022-11-09 17:46:37,305:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4208 (1.8746)
2022-11-09 17:46:37,335:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1447 (1.7413)
2022-11-09 17:46:37,358:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1080 (1.6467)
2022-11-09 17:46:37,383:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0193 (1.5683)
2022-11-09 17:46:37,408:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0562 (1.5069)
2022-11-09 17:46:37,434:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9448 (1.4481)
2022-11-09 17:46:37,459:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2518 (1.4314)
2022-11-09 17:46:37,484:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0333 (1.3973)
2022-11-09 17:46:37,507:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9820 (1.3645)
2022-11-09 17:46:37,531:INFO: Dataset: zara2               Batch: 14/18	Loss 2.3744 (1.4349)
2022-11-09 17:46:37,556:INFO: Dataset: zara2               Batch: 15/18	Loss 2.5242 (1.5189)
2022-11-09 17:46:37,580:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6947 (1.5299)
2022-11-09 17:46:37,606:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3996 (1.5220)
2022-11-09 17:46:37,630:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9404 (1.4953)
2022-11-09 17:46:37,682:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_90.pth.tar
2022-11-09 17:46:37,682:INFO: 
===> EPOCH: 91 (P1)
2022-11-09 17:46:37,682:INFO: - Computing loss (training)
2022-11-09 17:46:37,890:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9606 (0.9606)
2022-11-09 17:46:37,914:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0713 (1.0128)
2022-11-09 17:46:37,938:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2302 (1.0907)
2022-11-09 17:46:37,955:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9248 (1.0622)
2022-11-09 17:46:38,210:INFO: Dataset: univ                Batch:  1/15	Loss 1.1519 (1.1519)
2022-11-09 17:46:38,239:INFO: Dataset: univ                Batch:  2/15	Loss 0.9860 (1.0640)
2022-11-09 17:46:38,271:INFO: Dataset: univ                Batch:  3/15	Loss 1.0432 (1.0567)
2022-11-09 17:46:38,301:INFO: Dataset: univ                Batch:  4/15	Loss 1.1071 (1.0699)
2022-11-09 17:46:38,327:INFO: Dataset: univ                Batch:  5/15	Loss 1.1257 (1.0802)
2022-11-09 17:46:38,355:INFO: Dataset: univ                Batch:  6/15	Loss 1.2791 (1.1132)
2022-11-09 17:46:38,381:INFO: Dataset: univ                Batch:  7/15	Loss 1.4554 (1.1671)
2022-11-09 17:46:38,407:INFO: Dataset: univ                Batch:  8/15	Loss 1.2279 (1.1749)
2022-11-09 17:46:38,435:INFO: Dataset: univ                Batch:  9/15	Loss 0.9022 (1.1450)
2022-11-09 17:46:38,464:INFO: Dataset: univ                Batch: 10/15	Loss 1.0535 (1.1362)
2022-11-09 17:46:38,491:INFO: Dataset: univ                Batch: 11/15	Loss 1.2549 (1.1483)
2022-11-09 17:46:38,519:INFO: Dataset: univ                Batch: 12/15	Loss 0.8528 (1.1232)
2022-11-09 17:46:38,545:INFO: Dataset: univ                Batch: 13/15	Loss 0.9158 (1.1078)
2022-11-09 17:46:38,571:INFO: Dataset: univ                Batch: 14/15	Loss 1.0560 (1.1043)
2022-11-09 17:46:38,581:INFO: Dataset: univ                Batch: 15/15	Loss 0.1564 (1.0919)
2022-11-09 17:46:38,844:INFO: Dataset: zara1               Batch: 1/8	Loss 4.6614 (4.6614)
2022-11-09 17:46:38,871:INFO: Dataset: zara1               Batch: 2/8	Loss 3.3689 (4.0240)
2022-11-09 17:46:38,895:INFO: Dataset: zara1               Batch: 3/8	Loss 2.4109 (3.5055)
2022-11-09 17:46:38,921:INFO: Dataset: zara1               Batch: 4/8	Loss 2.2435 (3.2151)
2022-11-09 17:46:38,945:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1260 (2.8299)
2022-11-09 17:46:38,969:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1869 (2.5635)
2022-11-09 17:46:38,993:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5652 (2.4071)
2022-11-09 17:46:39,014:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8454 (2.3491)
2022-11-09 17:46:39,263:INFO: Dataset: zara2               Batch:  1/18	Loss 2.8445 (2.8445)
2022-11-09 17:46:39,291:INFO: Dataset: zara2               Batch:  2/18	Loss 3.0224 (2.9371)
2022-11-09 17:46:39,318:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0892 (2.6596)
2022-11-09 17:46:39,344:INFO: Dataset: zara2               Batch:  4/18	Loss 2.2154 (2.5446)
2022-11-09 17:46:39,370:INFO: Dataset: zara2               Batch:  5/18	Loss 2.5641 (2.5481)
2022-11-09 17:46:39,398:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0944 (2.2979)
2022-11-09 17:46:39,422:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2705 (2.1572)
2022-11-09 17:46:39,447:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9897 (2.0204)
2022-11-09 17:46:39,472:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0522 (1.9047)
2022-11-09 17:46:39,497:INFO: Dataset: zara2               Batch: 10/18	Loss 2.4634 (1.9537)
2022-11-09 17:46:39,523:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8989 (1.8482)
2022-11-09 17:46:39,549:INFO: Dataset: zara2               Batch: 12/18	Loss 1.5414 (1.8229)
2022-11-09 17:46:39,574:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0670 (1.7592)
2022-11-09 17:46:39,599:INFO: Dataset: zara2               Batch: 14/18	Loss 1.2828 (1.7212)
2022-11-09 17:46:39,625:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9619 (1.7381)
2022-11-09 17:46:39,651:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0033 (1.6898)
2022-11-09 17:46:39,676:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2783 (1.6644)
2022-11-09 17:46:39,700:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7975 (1.6247)
2022-11-09 17:46:39,751:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_91.pth.tar
2022-11-09 17:46:39,751:INFO: 
===> EPOCH: 92 (P1)
2022-11-09 17:46:39,751:INFO: - Computing loss (training)
2022-11-09 17:46:39,950:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1614 (1.1614)
2022-11-09 17:46:39,979:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2006 (1.1811)
2022-11-09 17:46:40,003:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0381 (1.1343)
2022-11-09 17:46:40,020:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7543 (1.0711)
2022-11-09 17:46:40,344:INFO: Dataset: univ                Batch:  1/15	Loss 0.9795 (0.9795)
2022-11-09 17:46:40,371:INFO: Dataset: univ                Batch:  2/15	Loss 1.4188 (1.2005)
2022-11-09 17:46:40,397:INFO: Dataset: univ                Batch:  3/15	Loss 0.9587 (1.1207)
2022-11-09 17:46:40,422:INFO: Dataset: univ                Batch:  4/15	Loss 1.0283 (1.0988)
2022-11-09 17:46:40,448:INFO: Dataset: univ                Batch:  5/15	Loss 1.0702 (1.0927)
2022-11-09 17:46:40,479:INFO: Dataset: univ                Batch:  6/15	Loss 0.9332 (1.0638)
2022-11-09 17:46:40,505:INFO: Dataset: univ                Batch:  7/15	Loss 0.9124 (1.0419)
2022-11-09 17:46:40,531:INFO: Dataset: univ                Batch:  8/15	Loss 0.9073 (1.0253)
2022-11-09 17:46:40,558:INFO: Dataset: univ                Batch:  9/15	Loss 1.4460 (1.0647)
2022-11-09 17:46:40,586:INFO: Dataset: univ                Batch: 10/15	Loss 0.9437 (1.0528)
2022-11-09 17:46:40,614:INFO: Dataset: univ                Batch: 11/15	Loss 0.8763 (1.0369)
2022-11-09 17:46:40,642:INFO: Dataset: univ                Batch: 12/15	Loss 1.3702 (1.0629)
2022-11-09 17:46:40,668:INFO: Dataset: univ                Batch: 13/15	Loss 0.9620 (1.0555)
2022-11-09 17:46:40,695:INFO: Dataset: univ                Batch: 14/15	Loss 0.8680 (1.0434)
2022-11-09 17:46:40,705:INFO: Dataset: univ                Batch: 15/15	Loss 0.1588 (1.0289)
2022-11-09 17:46:40,967:INFO: Dataset: zara1               Batch: 1/8	Loss 2.8933 (2.8933)
2022-11-09 17:46:40,993:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8109 (2.3853)
2022-11-09 17:46:41,019:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2907 (1.9898)
2022-11-09 17:46:41,045:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2244 (1.7883)
2022-11-09 17:46:41,068:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9908 (1.6212)
2022-11-09 17:46:41,093:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0789 (1.5355)
2022-11-09 17:46:41,117:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0735 (1.4671)
2022-11-09 17:46:41,138:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8110 (1.4012)
2022-11-09 17:46:41,401:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5166 (1.5166)
2022-11-09 17:46:41,426:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9801 (1.7387)
2022-11-09 17:46:41,450:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6164 (1.6942)
2022-11-09 17:46:41,476:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8723 (1.7354)
2022-11-09 17:46:41,501:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0981 (1.7985)
2022-11-09 17:46:41,528:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4204 (1.7381)
2022-11-09 17:46:41,551:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1207 (1.6522)
2022-11-09 17:46:41,575:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9717 (1.5680)
2022-11-09 17:46:41,599:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8904 (1.4949)
2022-11-09 17:46:41,623:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9850 (1.4482)
2022-11-09 17:46:41,648:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5325 (1.4557)
2022-11-09 17:46:41,673:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6232 (1.4692)
2022-11-09 17:46:41,698:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0827 (1.4336)
2022-11-09 17:46:41,721:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9303 (1.3997)
2022-11-09 17:46:41,746:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3563 (1.3967)
2022-11-09 17:46:41,770:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0738 (1.3777)
2022-11-09 17:46:41,795:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2677 (1.3708)
2022-11-09 17:46:41,817:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8209 (1.3469)
2022-11-09 17:46:41,867:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_92.pth.tar
2022-11-09 17:46:41,868:INFO: 
===> EPOCH: 93 (P1)
2022-11-09 17:46:41,868:INFO: - Computing loss (training)
2022-11-09 17:46:42,078:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9745 (0.9745)
2022-11-09 17:46:42,104:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9366 (0.9552)
2022-11-09 17:46:42,127:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0896 (0.9995)
2022-11-09 17:46:42,144:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6787 (0.9432)
2022-11-09 17:46:42,411:INFO: Dataset: univ                Batch:  1/15	Loss 0.9791 (0.9791)
2022-11-09 17:46:42,439:INFO: Dataset: univ                Batch:  2/15	Loss 0.9084 (0.9471)
2022-11-09 17:46:42,467:INFO: Dataset: univ                Batch:  3/15	Loss 0.8510 (0.9154)
2022-11-09 17:46:42,492:INFO: Dataset: univ                Batch:  4/15	Loss 0.9581 (0.9260)
2022-11-09 17:46:42,520:INFO: Dataset: univ                Batch:  5/15	Loss 1.0614 (0.9573)
2022-11-09 17:46:42,550:INFO: Dataset: univ                Batch:  6/15	Loss 1.0000 (0.9648)
2022-11-09 17:46:42,576:INFO: Dataset: univ                Batch:  7/15	Loss 1.3349 (1.0215)
2022-11-09 17:46:42,602:INFO: Dataset: univ                Batch:  8/15	Loss 1.1754 (1.0406)
2022-11-09 17:46:42,628:INFO: Dataset: univ                Batch:  9/15	Loss 0.8887 (1.0236)
2022-11-09 17:46:42,656:INFO: Dataset: univ                Batch: 10/15	Loss 0.8296 (1.0055)
2022-11-09 17:46:42,685:INFO: Dataset: univ                Batch: 11/15	Loss 1.1109 (1.0151)
2022-11-09 17:46:42,711:INFO: Dataset: univ                Batch: 12/15	Loss 0.8585 (1.0028)
2022-11-09 17:46:42,737:INFO: Dataset: univ                Batch: 13/15	Loss 0.8981 (0.9949)
2022-11-09 17:46:42,762:INFO: Dataset: univ                Batch: 14/15	Loss 0.9473 (0.9911)
2022-11-09 17:46:42,772:INFO: Dataset: univ                Batch: 15/15	Loss 0.1526 (0.9800)
2022-11-09 17:46:43,021:INFO: Dataset: zara1               Batch: 1/8	Loss 3.4107 (3.4107)
2022-11-09 17:46:43,048:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8868 (2.6641)
2022-11-09 17:46:43,074:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4780 (2.2730)
2022-11-09 17:46:43,098:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0523 (2.2230)
2022-11-09 17:46:43,123:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4400 (2.0603)
2022-11-09 17:46:43,148:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9393 (1.8784)
2022-11-09 17:46:43,172:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9478 (1.7367)
2022-11-09 17:46:43,193:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2582 (1.6866)
2022-11-09 17:46:43,446:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5603 (1.5603)
2022-11-09 17:46:43,471:INFO: Dataset: zara2               Batch:  2/18	Loss 2.7948 (2.2101)
2022-11-09 17:46:43,497:INFO: Dataset: zara2               Batch:  3/18	Loss 3.0498 (2.4982)
2022-11-09 17:46:43,523:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9502 (2.3622)
2022-11-09 17:46:43,549:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7476 (2.2553)
2022-11-09 17:46:43,576:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1739 (2.0701)
2022-11-09 17:46:43,600:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1355 (1.9458)
2022-11-09 17:46:43,624:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9190 (1.8233)
2022-11-09 17:46:43,647:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8021 (1.7144)
2022-11-09 17:46:43,673:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8571 (1.6318)
2022-11-09 17:46:43,702:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8634 (1.5566)
2022-11-09 17:46:43,728:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3228 (1.5369)
2022-11-09 17:46:43,752:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8203 (1.5585)
2022-11-09 17:46:43,776:INFO: Dataset: zara2               Batch: 14/18	Loss 2.3538 (1.6122)
2022-11-09 17:46:43,801:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4516 (1.6017)
2022-11-09 17:46:43,826:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2857 (1.5826)
2022-11-09 17:46:43,851:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9518 (1.5444)
2022-11-09 17:46:43,876:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0868 (1.5200)
2022-11-09 17:46:43,925:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_93.pth.tar
2022-11-09 17:46:43,925:INFO: 
===> EPOCH: 94 (P1)
2022-11-09 17:46:43,925:INFO: - Computing loss (training)
2022-11-09 17:46:44,128:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9258 (0.9258)
2022-11-09 17:46:44,155:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1815 (1.5104)
2022-11-09 17:46:44,180:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1169 (1.3733)
2022-11-09 17:46:44,196:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6119 (1.2377)
2022-11-09 17:46:44,464:INFO: Dataset: univ                Batch:  1/15	Loss 1.0679 (1.0679)
2022-11-09 17:46:44,493:INFO: Dataset: univ                Batch:  2/15	Loss 1.7733 (1.4321)
2022-11-09 17:46:44,520:INFO: Dataset: univ                Batch:  3/15	Loss 0.8373 (1.2590)
2022-11-09 17:46:44,548:INFO: Dataset: univ                Batch:  4/15	Loss 0.9563 (1.1834)
2022-11-09 17:46:44,578:INFO: Dataset: univ                Batch:  5/15	Loss 0.8326 (1.1168)
2022-11-09 17:46:44,606:INFO: Dataset: univ                Batch:  6/15	Loss 0.8957 (1.0822)
2022-11-09 17:46:44,632:INFO: Dataset: univ                Batch:  7/15	Loss 0.9996 (1.0693)
2022-11-09 17:46:44,659:INFO: Dataset: univ                Batch:  8/15	Loss 0.9682 (1.0562)
2022-11-09 17:46:44,686:INFO: Dataset: univ                Batch:  9/15	Loss 1.0346 (1.0538)
2022-11-09 17:46:44,713:INFO: Dataset: univ                Batch: 10/15	Loss 0.9729 (1.0456)
2022-11-09 17:46:44,743:INFO: Dataset: univ                Batch: 11/15	Loss 0.8248 (1.0270)
2022-11-09 17:46:44,770:INFO: Dataset: univ                Batch: 12/15	Loss 0.8625 (1.0141)
2022-11-09 17:46:44,796:INFO: Dataset: univ                Batch: 13/15	Loss 1.0661 (1.0184)
2022-11-09 17:46:44,823:INFO: Dataset: univ                Batch: 14/15	Loss 0.7945 (1.0025)
2022-11-09 17:46:44,833:INFO: Dataset: univ                Batch: 15/15	Loss 0.1394 (0.9909)
2022-11-09 17:46:45,085:INFO: Dataset: zara1               Batch: 1/8	Loss 2.8871 (2.8871)
2022-11-09 17:46:45,111:INFO: Dataset: zara1               Batch: 2/8	Loss 2.3237 (2.6111)
2022-11-09 17:46:45,135:INFO: Dataset: zara1               Batch: 3/8	Loss 2.4513 (2.5635)
2022-11-09 17:46:45,158:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2677 (2.2319)
2022-11-09 17:46:45,182:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1673 (1.9940)
2022-11-09 17:46:45,207:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0124 (1.8174)
2022-11-09 17:46:45,231:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4133 (1.7606)
2022-11-09 17:46:45,252:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2984 (1.7153)
2022-11-09 17:46:45,523:INFO: Dataset: zara2               Batch:  1/18	Loss 2.2727 (2.2727)
2022-11-09 17:46:45,548:INFO: Dataset: zara2               Batch:  2/18	Loss 2.4935 (2.3784)
2022-11-09 17:46:45,575:INFO: Dataset: zara2               Batch:  3/18	Loss 2.7763 (2.5137)
2022-11-09 17:46:45,599:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2431 (2.1785)
2022-11-09 17:46:45,625:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8981 (1.9136)
2022-11-09 17:46:45,652:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8984 (1.7669)
2022-11-09 17:46:45,676:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8306 (1.6200)
2022-11-09 17:46:45,699:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2803 (1.5779)
2022-11-09 17:46:45,723:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9968 (1.5158)
2022-11-09 17:46:45,747:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0281 (1.4689)
2022-11-09 17:46:45,773:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2008 (1.4414)
2022-11-09 17:46:45,799:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9228 (1.3974)
2022-11-09 17:46:45,824:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8199 (1.3570)
2022-11-09 17:46:45,848:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8851 (1.3219)
2022-11-09 17:46:45,872:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6629 (1.3476)
2022-11-09 17:46:45,897:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9077 (1.3191)
2022-11-09 17:46:45,922:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4601 (1.3271)
2022-11-09 17:46:45,944:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8033 (1.3000)
2022-11-09 17:46:45,994:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_94.pth.tar
2022-11-09 17:46:45,994:INFO: 
===> EPOCH: 95 (P1)
2022-11-09 17:46:45,994:INFO: - Computing loss (training)
2022-11-09 17:46:46,207:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9884 (0.9884)
2022-11-09 17:46:46,235:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1543 (1.0746)
2022-11-09 17:46:46,259:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8525 (1.0028)
2022-11-09 17:46:46,276:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6993 (0.9520)
2022-11-09 17:46:46,523:INFO: Dataset: univ                Batch:  1/15	Loss 2.4072 (2.4072)
2022-11-09 17:46:46,552:INFO: Dataset: univ                Batch:  2/15	Loss 2.4235 (2.4156)
2022-11-09 17:46:46,583:INFO: Dataset: univ                Batch:  3/15	Loss 1.0694 (2.0243)
2022-11-09 17:46:46,610:INFO: Dataset: univ                Batch:  4/15	Loss 0.9507 (1.7864)
2022-11-09 17:46:46,637:INFO: Dataset: univ                Batch:  5/15	Loss 0.8517 (1.6213)
2022-11-09 17:46:46,667:INFO: Dataset: univ                Batch:  6/15	Loss 1.1661 (1.5405)
2022-11-09 17:46:46,694:INFO: Dataset: univ                Batch:  7/15	Loss 1.0781 (1.4686)
2022-11-09 17:46:46,720:INFO: Dataset: univ                Batch:  8/15	Loss 0.8678 (1.4031)
2022-11-09 17:46:46,746:INFO: Dataset: univ                Batch:  9/15	Loss 1.0194 (1.3592)
2022-11-09 17:46:46,773:INFO: Dataset: univ                Batch: 10/15	Loss 0.7549 (1.2993)
2022-11-09 17:46:46,800:INFO: Dataset: univ                Batch: 11/15	Loss 0.8000 (1.2537)
2022-11-09 17:46:46,828:INFO: Dataset: univ                Batch: 12/15	Loss 1.0538 (1.2368)
2022-11-09 17:46:46,854:INFO: Dataset: univ                Batch: 13/15	Loss 0.9506 (1.2139)
2022-11-09 17:46:46,880:INFO: Dataset: univ                Batch: 14/15	Loss 0.9015 (1.1928)
2022-11-09 17:46:46,890:INFO: Dataset: univ                Batch: 15/15	Loss 0.1466 (1.1807)
2022-11-09 17:46:47,149:INFO: Dataset: zara1               Batch: 1/8	Loss 3.1671 (3.1671)
2022-11-09 17:46:47,175:INFO: Dataset: zara1               Batch: 2/8	Loss 2.1043 (2.6123)
2022-11-09 17:46:47,198:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7642 (2.3124)
2022-11-09 17:46:47,224:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7864 (2.1805)
2022-11-09 17:46:47,249:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1765 (1.9979)
2022-11-09 17:46:47,273:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1056 (1.8399)
2022-11-09 17:46:47,297:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1009 (1.7384)
2022-11-09 17:46:47,318:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9930 (1.6584)
2022-11-09 17:46:47,580:INFO: Dataset: zara2               Batch:  1/18	Loss 2.7082 (2.7082)
2022-11-09 17:46:47,605:INFO: Dataset: zara2               Batch:  2/18	Loss 2.8338 (2.7712)
2022-11-09 17:46:47,633:INFO: Dataset: zara2               Batch:  3/18	Loss 2.4361 (2.6635)
2022-11-09 17:46:47,657:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4471 (2.3441)
2022-11-09 17:46:47,681:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8939 (2.0492)
2022-11-09 17:46:47,709:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1096 (1.8919)
2022-11-09 17:46:47,733:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2127 (1.7871)
2022-11-09 17:46:47,757:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8731 (1.6825)
2022-11-09 17:46:47,781:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1588 (1.6235)
2022-11-09 17:46:47,808:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8389 (1.5510)
2022-11-09 17:46:47,835:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4116 (1.5384)
2022-11-09 17:46:47,862:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0863 (1.5019)
2022-11-09 17:46:47,886:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5362 (1.5044)
2022-11-09 17:46:47,910:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8997 (1.4623)
2022-11-09 17:46:47,935:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8538 (1.4227)
2022-11-09 17:46:47,960:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9842 (1.3978)
2022-11-09 17:46:47,984:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8178 (1.3606)
2022-11-09 17:46:48,010:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7612 (1.3323)
2022-11-09 17:46:48,062:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_95.pth.tar
2022-11-09 17:46:48,062:INFO: 
===> EPOCH: 96 (P1)
2022-11-09 17:46:48,063:INFO: - Computing loss (training)
2022-11-09 17:46:48,269:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4260 (1.4260)
2022-11-09 17:46:48,294:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9274 (1.1779)
2022-11-09 17:46:48,317:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1036 (1.1534)
2022-11-09 17:46:48,334:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8268 (1.1030)
2022-11-09 17:46:48,594:INFO: Dataset: univ                Batch:  1/15	Loss 1.4773 (1.4773)
2022-11-09 17:46:48,626:INFO: Dataset: univ                Batch:  2/15	Loss 0.9076 (1.2071)
2022-11-09 17:46:48,652:INFO: Dataset: univ                Batch:  3/15	Loss 1.3124 (1.2420)
2022-11-09 17:46:48,678:INFO: Dataset: univ                Batch:  4/15	Loss 1.1296 (1.2136)
2022-11-09 17:46:48,706:INFO: Dataset: univ                Batch:  5/15	Loss 1.1426 (1.1980)
2022-11-09 17:46:48,734:INFO: Dataset: univ                Batch:  6/15	Loss 0.8151 (1.1348)
2022-11-09 17:46:48,759:INFO: Dataset: univ                Batch:  7/15	Loss 0.8027 (1.0854)
2022-11-09 17:46:48,785:INFO: Dataset: univ                Batch:  8/15	Loss 0.8567 (1.0577)
2022-11-09 17:46:48,811:INFO: Dataset: univ                Batch:  9/15	Loss 0.7948 (1.0272)
2022-11-09 17:46:48,838:INFO: Dataset: univ                Batch: 10/15	Loss 0.8116 (1.0063)
2022-11-09 17:46:48,865:INFO: Dataset: univ                Batch: 11/15	Loss 0.8367 (0.9904)
2022-11-09 17:46:48,893:INFO: Dataset: univ                Batch: 12/15	Loss 1.1304 (1.0018)
2022-11-09 17:46:48,920:INFO: Dataset: univ                Batch: 13/15	Loss 0.9167 (0.9951)
2022-11-09 17:46:48,947:INFO: Dataset: univ                Batch: 14/15	Loss 1.0208 (0.9968)
2022-11-09 17:46:48,957:INFO: Dataset: univ                Batch: 15/15	Loss 0.1407 (0.9829)
2022-11-09 17:46:49,208:INFO: Dataset: zara1               Batch: 1/8	Loss 3.4320 (3.4320)
2022-11-09 17:46:49,236:INFO: Dataset: zara1               Batch: 2/8	Loss 2.8522 (3.1839)
2022-11-09 17:46:49,259:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7118 (2.7073)
2022-11-09 17:46:49,283:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1464 (2.2866)
2022-11-09 17:46:49,306:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9390 (2.0351)
2022-11-09 17:46:49,332:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0658 (1.8782)
2022-11-09 17:46:49,356:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9319 (1.7439)
2022-11-09 17:46:49,377:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4652 (1.7155)
2022-11-09 17:46:49,633:INFO: Dataset: zara2               Batch:  1/18	Loss 3.5324 (3.5324)
2022-11-09 17:46:49,662:INFO: Dataset: zara2               Batch:  2/18	Loss 3.1541 (3.3458)
2022-11-09 17:46:49,689:INFO: Dataset: zara2               Batch:  3/18	Loss 2.9425 (3.2136)
2022-11-09 17:46:49,714:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1110 (2.7192)
2022-11-09 17:46:49,742:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8172 (2.3787)
2022-11-09 17:46:49,771:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1223 (2.1852)
2022-11-09 17:46:49,796:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1008 (2.0267)
2022-11-09 17:46:49,821:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0011 (1.8865)
2022-11-09 17:46:49,846:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9357 (1.7838)
2022-11-09 17:46:49,872:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2185 (1.7265)
2022-11-09 17:46:49,898:INFO: Dataset: zara2               Batch: 11/18	Loss 2.4350 (1.7847)
2022-11-09 17:46:49,925:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1397 (1.7305)
2022-11-09 17:46:49,951:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1390 (1.6844)
2022-11-09 17:46:49,976:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3414 (1.6611)
2022-11-09 17:46:50,002:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9745 (1.6157)
2022-11-09 17:46:50,028:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9479 (1.6356)
2022-11-09 17:46:50,055:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0373 (1.5993)
2022-11-09 17:46:50,079:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7896 (1.5624)
2022-11-09 17:46:50,130:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_96.pth.tar
2022-11-09 17:46:50,130:INFO: 
===> EPOCH: 97 (P1)
2022-11-09 17:46:50,130:INFO: - Computing loss (training)
2022-11-09 17:46:50,340:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9817 (0.9817)
2022-11-09 17:46:50,365:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4019 (1.1908)
2022-11-09 17:46:50,391:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1244 (1.1698)
2022-11-09 17:46:50,410:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6391 (1.0753)
2022-11-09 17:46:50,666:INFO: Dataset: univ                Batch:  1/15	Loss 0.9025 (0.9025)
2022-11-09 17:46:50,695:INFO: Dataset: univ                Batch:  2/15	Loss 0.8105 (0.8579)
2022-11-09 17:46:50,722:INFO: Dataset: univ                Batch:  3/15	Loss 1.2478 (0.9887)
2022-11-09 17:46:50,749:INFO: Dataset: univ                Batch:  4/15	Loss 0.7524 (0.9317)
2022-11-09 17:46:50,775:INFO: Dataset: univ                Batch:  5/15	Loss 0.8707 (0.9197)
2022-11-09 17:46:50,806:INFO: Dataset: univ                Batch:  6/15	Loss 1.1002 (0.9499)
2022-11-09 17:46:50,832:INFO: Dataset: univ                Batch:  7/15	Loss 0.7693 (0.9234)
2022-11-09 17:46:50,857:INFO: Dataset: univ                Batch:  8/15	Loss 0.7859 (0.9062)
2022-11-09 17:46:50,884:INFO: Dataset: univ                Batch:  9/15	Loss 1.0983 (0.9306)
2022-11-09 17:46:50,911:INFO: Dataset: univ                Batch: 10/15	Loss 0.7947 (0.9165)
2022-11-09 17:46:50,941:INFO: Dataset: univ                Batch: 11/15	Loss 0.8196 (0.9085)
2022-11-09 17:46:50,968:INFO: Dataset: univ                Batch: 12/15	Loss 0.7533 (0.8944)
2022-11-09 17:46:50,994:INFO: Dataset: univ                Batch: 13/15	Loss 0.8086 (0.8877)
2022-11-09 17:46:51,020:INFO: Dataset: univ                Batch: 14/15	Loss 0.7925 (0.8806)
2022-11-09 17:46:51,029:INFO: Dataset: univ                Batch: 15/15	Loss 0.1388 (0.8715)
2022-11-09 17:46:51,278:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2895 (1.2895)
2022-11-09 17:46:51,304:INFO: Dataset: zara1               Batch: 2/8	Loss 2.6027 (1.8824)
2022-11-09 17:46:51,328:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4281 (1.7235)
2022-11-09 17:46:51,352:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0848 (1.5509)
2022-11-09 17:46:51,376:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9268 (1.4300)
2022-11-09 17:46:51,402:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9437 (1.3513)
2022-11-09 17:46:51,425:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1761 (1.3279)
2022-11-09 17:46:51,447:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0755 (1.3021)
2022-11-09 17:46:51,701:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5805 (1.5805)
2022-11-09 17:46:51,727:INFO: Dataset: zara2               Batch:  2/18	Loss 3.0954 (2.3717)
2022-11-09 17:46:51,752:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9686 (2.2347)
2022-11-09 17:46:51,779:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2714 (2.0011)
2022-11-09 17:46:51,804:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8729 (1.7591)
2022-11-09 17:46:51,832:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1942 (1.6586)
2022-11-09 17:46:51,855:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7229 (1.5259)
2022-11-09 17:46:51,879:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8632 (1.4453)
2022-11-09 17:46:51,902:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9185 (1.3916)
2022-11-09 17:46:51,927:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7757 (1.3288)
2022-11-09 17:46:51,952:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8377 (1.2838)
2022-11-09 17:46:51,978:INFO: Dataset: zara2               Batch: 12/18	Loss 1.2592 (1.2819)
2022-11-09 17:46:52,002:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3446 (1.2870)
2022-11-09 17:46:52,026:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9620 (1.2647)
2022-11-09 17:46:52,050:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0633 (1.2514)
2022-11-09 17:46:52,075:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8877 (1.2301)
2022-11-09 17:46:52,100:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8996 (1.2109)
2022-11-09 17:46:52,122:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2472 (1.2124)
2022-11-09 17:46:52,173:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_97.pth.tar
2022-11-09 17:46:52,173:INFO: 
===> EPOCH: 98 (P1)
2022-11-09 17:46:52,173:INFO: - Computing loss (training)
2022-11-09 17:46:52,382:INFO: Dataset: hotel               Batch: 1/4	Loss 2.5808 (2.5808)
2022-11-09 17:46:52,409:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2501 (2.4163)
2022-11-09 17:46:52,435:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0108 (1.9137)
2022-11-09 17:46:52,451:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9945 (1.7609)
2022-11-09 17:46:52,711:INFO: Dataset: univ                Batch:  1/15	Loss 0.8016 (0.8016)
2022-11-09 17:46:52,738:INFO: Dataset: univ                Batch:  2/15	Loss 0.9326 (0.8646)
2022-11-09 17:46:52,764:INFO: Dataset: univ                Batch:  3/15	Loss 0.9002 (0.8753)
2022-11-09 17:46:52,791:INFO: Dataset: univ                Batch:  4/15	Loss 1.2052 (0.9672)
2022-11-09 17:46:52,819:INFO: Dataset: univ                Batch:  5/15	Loss 0.9221 (0.9575)
2022-11-09 17:46:52,848:INFO: Dataset: univ                Batch:  6/15	Loss 1.2989 (1.0188)
2022-11-09 17:46:52,874:INFO: Dataset: univ                Batch:  7/15	Loss 1.7915 (1.1332)
2022-11-09 17:46:52,900:INFO: Dataset: univ                Batch:  8/15	Loss 1.8809 (1.2301)
2022-11-09 17:46:52,928:INFO: Dataset: univ                Batch:  9/15	Loss 1.6367 (1.2816)
2022-11-09 17:46:52,956:INFO: Dataset: univ                Batch: 10/15	Loss 1.2216 (1.2749)
2022-11-09 17:46:52,986:INFO: Dataset: univ                Batch: 11/15	Loss 1.2999 (1.2773)
2022-11-09 17:46:53,013:INFO: Dataset: univ                Batch: 12/15	Loss 0.7961 (1.2377)
2022-11-09 17:46:53,039:INFO: Dataset: univ                Batch: 13/15	Loss 0.7363 (1.1949)
2022-11-09 17:46:53,066:INFO: Dataset: univ                Batch: 14/15	Loss 0.7114 (1.1616)
2022-11-09 17:46:53,076:INFO: Dataset: univ                Batch: 15/15	Loss 0.1796 (1.1511)
2022-11-09 17:46:53,335:INFO: Dataset: zara1               Batch: 1/8	Loss 3.8264 (3.8264)
2022-11-09 17:46:53,360:INFO: Dataset: zara1               Batch: 2/8	Loss 3.4161 (3.6462)
2022-11-09 17:46:53,387:INFO: Dataset: zara1               Batch: 3/8	Loss 2.4687 (3.2476)
2022-11-09 17:46:53,411:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6054 (2.8628)
2022-11-09 17:46:53,435:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2948 (2.5149)
2022-11-09 17:46:53,459:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0659 (2.2692)
2022-11-09 17:46:53,483:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1872 (2.0987)
2022-11-09 17:46:53,503:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0128 (1.9850)
2022-11-09 17:46:53,810:INFO: Dataset: zara2               Batch:  1/18	Loss 2.4789 (2.4789)
2022-11-09 17:46:53,837:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7712 (2.1524)
2022-11-09 17:46:53,861:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4751 (1.9372)
2022-11-09 17:46:53,885:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6909 (1.8745)
2022-11-09 17:46:53,909:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1171 (1.7262)
2022-11-09 17:46:53,935:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9731 (1.6013)
2022-11-09 17:46:53,959:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8556 (1.4966)
2022-11-09 17:46:53,982:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2437 (1.4649)
2022-11-09 17:46:54,006:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7175 (1.3822)
2022-11-09 17:46:54,030:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8388 (1.3280)
2022-11-09 17:46:54,056:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7625 (1.2746)
2022-11-09 17:46:54,083:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0588 (1.2566)
2022-11-09 17:46:54,108:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9319 (1.2293)
2022-11-09 17:46:54,133:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1475 (1.2237)
2022-11-09 17:46:54,158:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2258 (1.2238)
2022-11-09 17:46:54,183:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6935 (1.2540)
2022-11-09 17:46:54,207:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8564 (1.2315)
2022-11-09 17:46:54,229:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8497 (1.2115)
2022-11-09 17:46:54,282:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_98.pth.tar
2022-11-09 17:46:54,282:INFO: 
===> EPOCH: 99 (P1)
2022-11-09 17:46:54,282:INFO: - Computing loss (training)
2022-11-09 17:46:54,490:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0460 (1.0460)
2022-11-09 17:46:54,515:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1542 (1.1020)
2022-11-09 17:46:54,540:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9437 (1.0490)
2022-11-09 17:46:54,558:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7269 (0.9959)
2022-11-09 17:46:54,858:INFO: Dataset: univ                Batch:  1/15	Loss 0.8668 (0.8668)
2022-11-09 17:46:54,885:INFO: Dataset: univ                Batch:  2/15	Loss 0.7826 (0.8289)
2022-11-09 17:46:54,911:INFO: Dataset: univ                Batch:  3/15	Loss 0.9701 (0.8722)
2022-11-09 17:46:54,937:INFO: Dataset: univ                Batch:  4/15	Loss 0.8241 (0.8608)
2022-11-09 17:46:54,963:INFO: Dataset: univ                Batch:  5/15	Loss 1.1289 (0.9142)
2022-11-09 17:46:54,994:INFO: Dataset: univ                Batch:  6/15	Loss 1.4195 (1.0062)
2022-11-09 17:46:55,020:INFO: Dataset: univ                Batch:  7/15	Loss 0.7072 (0.9643)
2022-11-09 17:46:55,045:INFO: Dataset: univ                Batch:  8/15	Loss 0.7635 (0.9408)
2022-11-09 17:46:55,071:INFO: Dataset: univ                Batch:  9/15	Loss 0.7327 (0.9206)
2022-11-09 17:46:55,098:INFO: Dataset: univ                Batch: 10/15	Loss 0.7771 (0.9072)
2022-11-09 17:46:55,124:INFO: Dataset: univ                Batch: 11/15	Loss 0.9302 (0.9093)
2022-11-09 17:46:55,152:INFO: Dataset: univ                Batch: 12/15	Loss 0.8854 (0.9073)
2022-11-09 17:46:55,178:INFO: Dataset: univ                Batch: 13/15	Loss 0.8067 (0.8992)
2022-11-09 17:46:55,205:INFO: Dataset: univ                Batch: 14/15	Loss 0.7818 (0.8903)
2022-11-09 17:46:55,215:INFO: Dataset: univ                Batch: 15/15	Loss 0.1442 (0.8793)
2022-11-09 17:46:55,473:INFO: Dataset: zara1               Batch: 1/8	Loss 3.0608 (3.0608)
2022-11-09 17:46:55,499:INFO: Dataset: zara1               Batch: 2/8	Loss 2.2923 (2.6650)
2022-11-09 17:46:55,523:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2107 (2.1611)
2022-11-09 17:46:55,547:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8924 (1.8317)
2022-11-09 17:46:55,571:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8426 (1.6451)
2022-11-09 17:46:55,598:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1736 (1.5587)
2022-11-09 17:46:55,621:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0018 (1.4784)
2022-11-09 17:46:55,642:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1131 (1.4382)
2022-11-09 17:46:55,904:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1524 (1.1524)
2022-11-09 17:46:55,929:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6208 (1.3852)
2022-11-09 17:46:55,957:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1367 (1.3032)
2022-11-09 17:46:55,981:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9534 (1.2227)
2022-11-09 17:46:56,005:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0380 (1.1842)
2022-11-09 17:46:56,032:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8684 (1.1351)
2022-11-09 17:46:56,056:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1827 (1.1420)
2022-11-09 17:46:56,079:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7604 (1.0933)
2022-11-09 17:46:56,103:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8634 (1.0689)
2022-11-09 17:46:56,127:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9294 (1.0557)
2022-11-09 17:46:56,152:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7933 (1.0328)
2022-11-09 17:46:56,178:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9946 (1.0293)
2022-11-09 17:46:56,202:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9296 (1.0212)
2022-11-09 17:46:56,226:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1237 (1.0285)
2022-11-09 17:46:56,251:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8157 (1.0142)
2022-11-09 17:46:56,276:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8028 (1.0006)
2022-11-09 17:46:56,300:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7556 (0.9849)
2022-11-09 17:46:56,323:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6296 (0.9666)
2022-11-09 17:46:56,377:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_99.pth.tar
2022-11-09 17:46:56,377:INFO: 
===> EPOCH: 100 (P1)
2022-11-09 17:46:56,378:INFO: - Computing loss (training)
2022-11-09 17:46:56,586:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2039 (1.2039)
2022-11-09 17:46:56,614:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1362 (1.1706)
2022-11-09 17:46:56,638:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9857 (1.1095)
2022-11-09 17:46:56,655:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7466 (1.0468)
2022-11-09 17:46:56,905:INFO: Dataset: univ                Batch:  1/15	Loss 0.7967 (0.7967)
2022-11-09 17:46:56,934:INFO: Dataset: univ                Batch:  2/15	Loss 0.8962 (0.8466)
2022-11-09 17:46:56,961:INFO: Dataset: univ                Batch:  3/15	Loss 1.3246 (1.0243)
2022-11-09 17:46:56,987:INFO: Dataset: univ                Batch:  4/15	Loss 0.8226 (0.9742)
2022-11-09 17:46:57,017:INFO: Dataset: univ                Batch:  5/15	Loss 0.7142 (0.9238)
2022-11-09 17:46:57,046:INFO: Dataset: univ                Batch:  6/15	Loss 0.6861 (0.8867)
2022-11-09 17:46:57,072:INFO: Dataset: univ                Batch:  7/15	Loss 0.8067 (0.8756)
2022-11-09 17:46:57,098:INFO: Dataset: univ                Batch:  8/15	Loss 0.9784 (0.8889)
2022-11-09 17:46:57,124:INFO: Dataset: univ                Batch:  9/15	Loss 0.7186 (0.8696)
2022-11-09 17:46:57,151:INFO: Dataset: univ                Batch: 10/15	Loss 1.0073 (0.8845)
2022-11-09 17:46:57,178:INFO: Dataset: univ                Batch: 11/15	Loss 0.8855 (0.8846)
2022-11-09 17:46:57,206:INFO: Dataset: univ                Batch: 12/15	Loss 0.7817 (0.8767)
2022-11-09 17:46:57,231:INFO: Dataset: univ                Batch: 13/15	Loss 0.7598 (0.8682)
2022-11-09 17:46:57,257:INFO: Dataset: univ                Batch: 14/15	Loss 0.7413 (0.8594)
2022-11-09 17:46:57,267:INFO: Dataset: univ                Batch: 15/15	Loss 0.1499 (0.8499)
2022-11-09 17:46:57,523:INFO: Dataset: zara1               Batch: 1/8	Loss 3.2379 (3.2379)
2022-11-09 17:46:57,547:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0070 (2.1657)
2022-11-09 17:46:57,572:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0820 (1.8092)
2022-11-09 17:46:57,596:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1067 (1.6337)
2022-11-09 17:46:57,619:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0518 (1.5282)
2022-11-09 17:46:57,645:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9168 (1.4344)
2022-11-09 17:46:57,669:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8081 (1.3549)
2022-11-09 17:46:57,690:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7759 (1.2921)
2022-11-09 17:46:57,940:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6426 (1.6426)
2022-11-09 17:46:57,966:INFO: Dataset: zara2               Batch:  2/18	Loss 2.6045 (2.1278)
2022-11-09 17:46:57,991:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5446 (1.9396)
2022-11-09 17:46:58,018:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6832 (1.8711)
2022-11-09 17:46:58,044:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1619 (1.7372)
2022-11-09 17:46:58,070:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7449 (1.5672)
2022-11-09 17:46:58,094:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7609 (1.4479)
2022-11-09 17:46:58,118:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7194 (1.3547)
2022-11-09 17:46:58,142:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7470 (1.2873)
2022-11-09 17:46:58,167:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7870 (1.2335)
2022-11-09 17:46:58,193:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8955 (1.2036)
2022-11-09 17:46:58,219:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7420 (1.1648)
2022-11-09 17:46:58,243:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8534 (1.1412)
2022-11-09 17:46:58,267:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9384 (1.1280)
2022-11-09 17:46:58,292:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9133 (1.1150)
2022-11-09 17:46:58,316:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5782 (1.1442)
2022-11-09 17:46:58,341:INFO: Dataset: zara2               Batch: 17/18	Loss 2.6420 (1.2374)
2022-11-09 17:46:58,364:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1016 (1.2315)
2022-11-09 17:46:58,416:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_100.pth.tar
2022-11-09 17:46:58,416:INFO: 
===> EPOCH: 101 (P1)
2022-11-09 17:46:58,416:INFO: - Computing loss (training)
2022-11-09 17:46:58,610:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0415 (1.0415)
2022-11-09 17:46:58,635:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0694 (1.0553)
2022-11-09 17:46:58,661:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0051 (1.0379)
2022-11-09 17:46:58,678:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6741 (0.9808)
2022-11-09 17:46:58,931:INFO: Dataset: univ                Batch:  1/15	Loss 0.8245 (0.8245)
2022-11-09 17:46:58,961:INFO: Dataset: univ                Batch:  2/15	Loss 1.2070 (1.0316)
2022-11-09 17:46:58,993:INFO: Dataset: univ                Batch:  3/15	Loss 1.2132 (1.0947)
2022-11-09 17:46:59,019:INFO: Dataset: univ                Batch:  4/15	Loss 0.8918 (1.0446)
2022-11-09 17:46:59,045:INFO: Dataset: univ                Batch:  5/15	Loss 0.7371 (0.9862)
2022-11-09 17:46:59,075:INFO: Dataset: univ                Batch:  6/15	Loss 0.7560 (0.9488)
2022-11-09 17:46:59,100:INFO: Dataset: univ                Batch:  7/15	Loss 0.8052 (0.9309)
2022-11-09 17:46:59,126:INFO: Dataset: univ                Batch:  8/15	Loss 0.9596 (0.9347)
2022-11-09 17:46:59,152:INFO: Dataset: univ                Batch:  9/15	Loss 0.9161 (0.9327)
2022-11-09 17:46:59,181:INFO: Dataset: univ                Batch: 10/15	Loss 0.8028 (0.9193)
2022-11-09 17:46:59,209:INFO: Dataset: univ                Batch: 11/15	Loss 0.7165 (0.8998)
2022-11-09 17:46:59,236:INFO: Dataset: univ                Batch: 12/15	Loss 0.7153 (0.8840)
2022-11-09 17:46:59,261:INFO: Dataset: univ                Batch: 13/15	Loss 0.6821 (0.8684)
2022-11-09 17:46:59,287:INFO: Dataset: univ                Batch: 14/15	Loss 0.7479 (0.8597)
2022-11-09 17:46:59,296:INFO: Dataset: univ                Batch: 15/15	Loss 0.1334 (0.8491)
2022-11-09 17:46:59,562:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1903 (1.1903)
2022-11-09 17:46:59,589:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3619 (1.2825)
2022-11-09 17:46:59,615:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0477 (1.5224)
2022-11-09 17:46:59,639:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7116 (1.5697)
2022-11-09 17:46:59,664:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8316 (1.4125)
2022-11-09 17:46:59,689:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8742 (1.3244)
2022-11-09 17:46:59,713:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2920 (1.3197)
2022-11-09 17:46:59,734:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9580 (1.2843)
2022-11-09 17:47:00,001:INFO: Dataset: zara2               Batch:  1/18	Loss 2.5359 (2.5359)
2022-11-09 17:47:00,025:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7385 (2.1788)
2022-11-09 17:47:00,049:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2888 (1.8977)
2022-11-09 17:47:00,076:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9674 (1.6673)
2022-11-09 17:47:00,100:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8578 (1.5010)
2022-11-09 17:47:00,127:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7145 (1.3809)
2022-11-09 17:47:00,150:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7473 (1.2933)
2022-11-09 17:47:00,174:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7160 (1.2229)
2022-11-09 17:47:00,199:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9335 (1.1896)
2022-11-09 17:47:00,227:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0779 (1.1793)
2022-11-09 17:47:00,254:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7274 (1.1349)
2022-11-09 17:47:00,281:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7574 (1.1020)
2022-11-09 17:47:00,306:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1230 (1.1035)
2022-11-09 17:47:00,330:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7577 (1.0795)
2022-11-09 17:47:00,355:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7721 (1.1225)
2022-11-09 17:47:00,379:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8891 (1.1082)
2022-11-09 17:47:00,404:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7812 (1.0896)
2022-11-09 17:47:00,425:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6385 (1.0702)
2022-11-09 17:47:00,475:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_101.pth.tar
2022-11-09 17:47:00,475:INFO: 
===> EPOCH: 102 (P1)
2022-11-09 17:47:00,475:INFO: - Computing loss (training)
2022-11-09 17:47:00,693:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2081 (2.2081)
2022-11-09 17:47:00,720:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0181 (2.1066)
2022-11-09 17:47:00,746:INFO: Dataset: hotel               Batch: 3/4	Loss 3.1579 (2.4695)
2022-11-09 17:47:00,762:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5277 (2.1160)
2022-11-09 17:47:01,021:INFO: Dataset: univ                Batch:  1/15	Loss 0.9794 (0.9794)
2022-11-09 17:47:01,049:INFO: Dataset: univ                Batch:  2/15	Loss 1.1602 (1.0598)
2022-11-09 17:47:01,078:INFO: Dataset: univ                Batch:  3/15	Loss 1.0604 (1.0600)
2022-11-09 17:47:01,106:INFO: Dataset: univ                Batch:  4/15	Loss 1.8908 (1.2810)
2022-11-09 17:47:01,132:INFO: Dataset: univ                Batch:  5/15	Loss 1.4017 (1.3052)
2022-11-09 17:47:01,162:INFO: Dataset: univ                Batch:  6/15	Loss 1.0545 (1.2623)
2022-11-09 17:47:01,188:INFO: Dataset: univ                Batch:  7/15	Loss 2.0839 (1.3801)
2022-11-09 17:47:01,216:INFO: Dataset: univ                Batch:  8/15	Loss 0.8083 (1.3046)
2022-11-09 17:47:01,246:INFO: Dataset: univ                Batch:  9/15	Loss 0.7397 (1.2443)
2022-11-09 17:47:01,273:INFO: Dataset: univ                Batch: 10/15	Loss 0.8348 (1.2050)
2022-11-09 17:47:01,301:INFO: Dataset: univ                Batch: 11/15	Loss 0.8337 (1.1702)
2022-11-09 17:47:01,329:INFO: Dataset: univ                Batch: 12/15	Loss 0.8716 (1.1450)
2022-11-09 17:47:01,355:INFO: Dataset: univ                Batch: 13/15	Loss 0.9297 (1.1283)
2022-11-09 17:47:01,381:INFO: Dataset: univ                Batch: 14/15	Loss 0.7137 (1.0989)
2022-11-09 17:47:01,391:INFO: Dataset: univ                Batch: 15/15	Loss 0.1282 (1.0878)
2022-11-09 17:47:01,643:INFO: Dataset: zara1               Batch: 1/8	Loss 2.6678 (2.6678)
2022-11-09 17:47:01,668:INFO: Dataset: zara1               Batch: 2/8	Loss 2.6856 (2.6768)
2022-11-09 17:47:01,692:INFO: Dataset: zara1               Batch: 3/8	Loss 2.7897 (2.7145)
2022-11-09 17:47:01,716:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7957 (2.4858)
2022-11-09 17:47:01,739:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8597 (2.1846)
2022-11-09 17:47:01,765:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7853 (1.9286)
2022-11-09 17:47:01,789:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8548 (1.7657)
2022-11-09 17:47:01,810:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7333 (1.7626)
2022-11-09 17:47:02,071:INFO: Dataset: zara2               Batch:  1/18	Loss 2.8364 (2.8364)
2022-11-09 17:47:02,095:INFO: Dataset: zara2               Batch:  2/18	Loss 2.4989 (2.6620)
2022-11-09 17:47:02,121:INFO: Dataset: zara2               Batch:  3/18	Loss 2.8259 (2.7172)
2022-11-09 17:47:02,148:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4892 (2.4323)
2022-11-09 17:47:02,173:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9070 (2.1238)
2022-11-09 17:47:02,201:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9981 (1.9354)
2022-11-09 17:47:02,225:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8341 (1.7855)
2022-11-09 17:47:02,249:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8156 (1.6670)
2022-11-09 17:47:02,272:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8325 (1.5716)
2022-11-09 17:47:02,297:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2499 (1.5398)
2022-11-09 17:47:02,325:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4817 (1.5345)
2022-11-09 17:47:02,350:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1883 (1.5081)
2022-11-09 17:47:02,374:INFO: Dataset: zara2               Batch: 13/18	Loss 2.3958 (1.5775)
2022-11-09 17:47:02,398:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1346 (1.5414)
2022-11-09 17:47:02,423:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8364 (1.4912)
2022-11-09 17:47:02,447:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9730 (1.4592)
2022-11-09 17:47:02,472:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7223 (1.4151)
2022-11-09 17:47:02,494:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7251 (1.3825)
2022-11-09 17:47:02,545:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_102.pth.tar
2022-11-09 17:47:02,545:INFO: 
===> EPOCH: 103 (P1)
2022-11-09 17:47:02,545:INFO: - Computing loss (training)
2022-11-09 17:47:02,757:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9626 (1.9626)
2022-11-09 17:47:02,783:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2003 (1.5851)
2022-11-09 17:47:02,807:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0549 (1.4023)
2022-11-09 17:47:02,824:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6902 (1.2867)
2022-11-09 17:47:03,082:INFO: Dataset: univ                Batch:  1/15	Loss 0.8806 (0.8806)
2022-11-09 17:47:03,112:INFO: Dataset: univ                Batch:  2/15	Loss 1.0109 (0.9485)
2022-11-09 17:47:03,143:INFO: Dataset: univ                Batch:  3/15	Loss 0.7937 (0.8927)
2022-11-09 17:47:03,172:INFO: Dataset: univ                Batch:  4/15	Loss 0.7585 (0.8609)
2022-11-09 17:47:03,200:INFO: Dataset: univ                Batch:  5/15	Loss 0.8343 (0.8553)
2022-11-09 17:47:03,229:INFO: Dataset: univ                Batch:  6/15	Loss 0.7399 (0.8369)
2022-11-09 17:47:03,256:INFO: Dataset: univ                Batch:  7/15	Loss 0.7236 (0.8201)
2022-11-09 17:47:03,283:INFO: Dataset: univ                Batch:  8/15	Loss 0.7005 (0.8060)
2022-11-09 17:47:03,311:INFO: Dataset: univ                Batch:  9/15	Loss 0.7890 (0.8040)
2022-11-09 17:47:03,339:INFO: Dataset: univ                Batch: 10/15	Loss 1.1037 (0.8326)
2022-11-09 17:47:03,367:INFO: Dataset: univ                Batch: 11/15	Loss 0.7253 (0.8227)
2022-11-09 17:47:03,395:INFO: Dataset: univ                Batch: 12/15	Loss 0.8458 (0.8246)
2022-11-09 17:47:03,422:INFO: Dataset: univ                Batch: 13/15	Loss 0.7088 (0.8152)
2022-11-09 17:47:03,449:INFO: Dataset: univ                Batch: 14/15	Loss 0.6618 (0.8040)
2022-11-09 17:47:03,459:INFO: Dataset: univ                Batch: 15/15	Loss 0.1380 (0.7938)
2022-11-09 17:47:03,733:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3771 (2.3771)
2022-11-09 17:47:03,757:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6995 (2.0301)
2022-11-09 17:47:03,781:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1192 (1.6846)
2022-11-09 17:47:03,804:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1785 (1.5586)
2022-11-09 17:47:03,828:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8480 (1.4073)
2022-11-09 17:47:03,854:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9010 (1.3252)
2022-11-09 17:47:03,877:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9963 (1.2810)
2022-11-09 17:47:03,898:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9033 (1.3432)
2022-11-09 17:47:04,216:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5922 (1.5922)
2022-11-09 17:47:04,243:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0696 (1.3536)
2022-11-09 17:47:04,267:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0573 (1.2457)
2022-11-09 17:47:04,292:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1109 (1.2119)
2022-11-09 17:47:04,317:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7603 (1.1197)
2022-11-09 17:47:04,343:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6894 (1.0400)
2022-11-09 17:47:04,367:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6678 (0.9885)
2022-11-09 17:47:04,391:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7814 (0.9622)
2022-11-09 17:47:04,415:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3149 (0.9966)
2022-11-09 17:47:04,441:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8146 (0.9747)
2022-11-09 17:47:04,466:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7866 (0.9571)
2022-11-09 17:47:04,497:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6858 (0.9335)
2022-11-09 17:47:04,524:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0044 (0.9385)
2022-11-09 17:47:04,550:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0356 (0.9460)
2022-11-09 17:47:04,574:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6571 (0.9267)
2022-11-09 17:47:04,598:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6749 (0.9121)
2022-11-09 17:47:04,623:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7116 (0.9004)
2022-11-09 17:47:04,645:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6316 (0.8862)
2022-11-09 17:47:04,699:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_103.pth.tar
2022-11-09 17:47:04,699:INFO: 
===> EPOCH: 104 (P1)
2022-11-09 17:47:04,700:INFO: - Computing loss (training)
2022-11-09 17:47:04,908:INFO: Dataset: hotel               Batch: 1/4	Loss 3.1451 (3.1451)
2022-11-09 17:47:04,934:INFO: Dataset: hotel               Batch: 2/4	Loss 3.6243 (3.3772)
2022-11-09 17:47:04,959:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4652 (2.7553)
2022-11-09 17:47:04,976:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7983 (2.5786)
2022-11-09 17:47:05,230:INFO: Dataset: univ                Batch:  1/15	Loss 0.8337 (0.8337)
2022-11-09 17:47:05,264:INFO: Dataset: univ                Batch:  2/15	Loss 0.7943 (0.8126)
2022-11-09 17:47:05,292:INFO: Dataset: univ                Batch:  3/15	Loss 0.9557 (0.8597)
2022-11-09 17:47:05,319:INFO: Dataset: univ                Batch:  4/15	Loss 1.0953 (0.9153)
2022-11-09 17:47:05,345:INFO: Dataset: univ                Batch:  5/15	Loss 0.7379 (0.8823)
2022-11-09 17:47:05,375:INFO: Dataset: univ                Batch:  6/15	Loss 1.4764 (0.9799)
2022-11-09 17:47:05,400:INFO: Dataset: univ                Batch:  7/15	Loss 1.1924 (1.0110)
2022-11-09 17:47:05,426:INFO: Dataset: univ                Batch:  8/15	Loss 1.3106 (1.0506)
2022-11-09 17:47:05,453:INFO: Dataset: univ                Batch:  9/15	Loss 0.8075 (1.0239)
2022-11-09 17:47:05,480:INFO: Dataset: univ                Batch: 10/15	Loss 0.8119 (1.0014)
2022-11-09 17:47:05,507:INFO: Dataset: univ                Batch: 11/15	Loss 0.7402 (0.9792)
2022-11-09 17:47:05,534:INFO: Dataset: univ                Batch: 12/15	Loss 1.0383 (0.9838)
2022-11-09 17:47:05,560:INFO: Dataset: univ                Batch: 13/15	Loss 0.7406 (0.9661)
2022-11-09 17:47:05,586:INFO: Dataset: univ                Batch: 14/15	Loss 0.9110 (0.9621)
2022-11-09 17:47:05,596:INFO: Dataset: univ                Batch: 15/15	Loss 0.1382 (0.9482)
2022-11-09 17:47:05,861:INFO: Dataset: zara1               Batch: 1/8	Loss 2.8672 (2.8672)
2022-11-09 17:47:05,889:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4303 (2.1258)
2022-11-09 17:47:05,913:INFO: Dataset: zara1               Batch: 3/8	Loss 2.7794 (2.3338)
2022-11-09 17:47:05,936:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9458 (1.9804)
2022-11-09 17:47:05,960:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7959 (1.7312)
2022-11-09 17:47:05,986:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9947 (1.5901)
2022-11-09 17:47:06,009:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0021 (1.5094)
2022-11-09 17:47:06,030:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9681 (1.4456)
2022-11-09 17:47:06,282:INFO: Dataset: zara2               Batch:  1/18	Loss 2.5158 (2.5158)
2022-11-09 17:47:06,310:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2941 (1.8815)
2022-11-09 17:47:06,334:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0441 (1.5864)
2022-11-09 17:47:06,359:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8751 (1.4247)
2022-11-09 17:47:06,386:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9810 (1.3400)
2022-11-09 17:47:06,411:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8893 (1.2643)
2022-11-09 17:47:06,435:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6994 (1.1839)
2022-11-09 17:47:06,459:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8556 (1.1416)
2022-11-09 17:47:06,482:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6726 (1.0876)
2022-11-09 17:47:06,507:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7933 (1.0572)
2022-11-09 17:47:06,532:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9632 (1.0491)
2022-11-09 17:47:06,558:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7887 (1.0311)
2022-11-09 17:47:06,583:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8995 (1.0199)
2022-11-09 17:47:06,608:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9015 (1.0113)
2022-11-09 17:47:06,633:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6736 (0.9883)
2022-11-09 17:47:06,658:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9505 (0.9859)
2022-11-09 17:47:06,682:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7407 (0.9715)
2022-11-09 17:47:06,706:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6211 (0.9532)
2022-11-09 17:47:06,755:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_104.pth.tar
2022-11-09 17:47:06,755:INFO: 
===> EPOCH: 105 (P1)
2022-11-09 17:47:06,755:INFO: - Computing loss (training)
2022-11-09 17:47:06,962:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2887 (1.2887)
2022-11-09 17:47:06,988:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9145 (1.1003)
2022-11-09 17:47:07,012:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8935 (1.0345)
2022-11-09 17:47:07,030:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5143 (0.9501)
2022-11-09 17:47:07,298:INFO: Dataset: univ                Batch:  1/15	Loss 0.9950 (0.9950)
2022-11-09 17:47:07,327:INFO: Dataset: univ                Batch:  2/15	Loss 0.7987 (0.8877)
2022-11-09 17:47:07,356:INFO: Dataset: univ                Batch:  3/15	Loss 0.7590 (0.8486)
2022-11-09 17:47:07,383:INFO: Dataset: univ                Batch:  4/15	Loss 0.7247 (0.8202)
2022-11-09 17:47:07,410:INFO: Dataset: univ                Batch:  5/15	Loss 0.7218 (0.7992)
2022-11-09 17:47:07,438:INFO: Dataset: univ                Batch:  6/15	Loss 0.7266 (0.7876)
2022-11-09 17:47:07,464:INFO: Dataset: univ                Batch:  7/15	Loss 0.6649 (0.7701)
2022-11-09 17:47:07,490:INFO: Dataset: univ                Batch:  8/15	Loss 0.8004 (0.7742)
2022-11-09 17:47:07,517:INFO: Dataset: univ                Batch:  9/15	Loss 1.1044 (0.8135)
2022-11-09 17:47:07,545:INFO: Dataset: univ                Batch: 10/15	Loss 0.6886 (0.8011)
2022-11-09 17:47:07,575:INFO: Dataset: univ                Batch: 11/15	Loss 0.8456 (0.8052)
2022-11-09 17:47:07,602:INFO: Dataset: univ                Batch: 12/15	Loss 0.6952 (0.7968)
2022-11-09 17:47:07,628:INFO: Dataset: univ                Batch: 13/15	Loss 0.6568 (0.7871)
2022-11-09 17:47:07,654:INFO: Dataset: univ                Batch: 14/15	Loss 0.6976 (0.7804)
2022-11-09 17:47:07,664:INFO: Dataset: univ                Batch: 15/15	Loss 0.1288 (0.7719)
2022-11-09 17:47:07,909:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1698 (1.1698)
2022-11-09 17:47:07,935:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1609 (1.1650)
2022-11-09 17:47:07,962:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9373 (1.3839)
2022-11-09 17:47:07,986:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1304 (1.3240)
2022-11-09 17:47:08,011:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7919 (1.2123)
2022-11-09 17:47:08,037:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8384 (1.1533)
2022-11-09 17:47:08,062:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0727 (1.1417)
2022-11-09 17:47:08,084:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7596 (1.1021)
2022-11-09 17:47:08,355:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3261 (1.3261)
2022-11-09 17:47:08,380:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1890 (1.2550)
2022-11-09 17:47:08,407:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1793 (1.2300)
2022-11-09 17:47:08,434:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2297 (1.2300)
2022-11-09 17:47:08,458:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1189 (1.2077)
2022-11-09 17:47:08,486:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9892 (1.1764)
2022-11-09 17:47:08,511:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6870 (1.1071)
2022-11-09 17:47:08,535:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8538 (1.0735)
2022-11-09 17:47:08,559:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0024 (1.0660)
2022-11-09 17:47:08,586:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9941 (1.1589)
2022-11-09 17:47:08,614:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7370 (1.1176)
2022-11-09 17:47:08,640:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8546 (1.0982)
2022-11-09 17:47:08,665:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7196 (1.0682)
2022-11-09 17:47:08,689:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8310 (1.0511)
2022-11-09 17:47:08,715:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9512 (1.0449)
2022-11-09 17:47:08,740:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8566 (1.0339)
2022-11-09 17:47:08,766:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7922 (1.0182)
2022-11-09 17:47:08,791:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6479 (0.9996)
2022-11-09 17:47:08,842:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_105.pth.tar
2022-11-09 17:47:08,842:INFO: 
===> EPOCH: 106 (P1)
2022-11-09 17:47:08,842:INFO: - Computing loss (training)
2022-11-09 17:47:09,048:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6051 (1.6051)
2022-11-09 17:47:09,073:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9341 (1.2744)
2022-11-09 17:47:09,098:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1945 (1.2476)
2022-11-09 17:47:09,116:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5866 (1.1377)
2022-11-09 17:47:09,441:INFO: Dataset: univ                Batch:  1/15	Loss 1.1343 (1.1343)
2022-11-09 17:47:09,470:INFO: Dataset: univ                Batch:  2/15	Loss 1.7063 (1.4173)
2022-11-09 17:47:09,499:INFO: Dataset: univ                Batch:  3/15	Loss 0.8612 (1.2336)
2022-11-09 17:47:09,525:INFO: Dataset: univ                Batch:  4/15	Loss 0.8331 (1.1462)
2022-11-09 17:47:09,551:INFO: Dataset: univ                Batch:  5/15	Loss 1.4662 (1.2128)
2022-11-09 17:47:09,581:INFO: Dataset: univ                Batch:  6/15	Loss 0.9192 (1.1627)
2022-11-09 17:47:09,608:INFO: Dataset: univ                Batch:  7/15	Loss 0.8869 (1.1234)
2022-11-09 17:47:09,634:INFO: Dataset: univ                Batch:  8/15	Loss 0.7737 (1.0845)
2022-11-09 17:47:09,661:INFO: Dataset: univ                Batch:  9/15	Loss 0.6957 (1.0419)
2022-11-09 17:47:09,689:INFO: Dataset: univ                Batch: 10/15	Loss 0.6943 (1.0078)
2022-11-09 17:47:09,716:INFO: Dataset: univ                Batch: 11/15	Loss 0.7567 (0.9847)
2022-11-09 17:47:09,745:INFO: Dataset: univ                Batch: 12/15	Loss 0.6763 (0.9598)
2022-11-09 17:47:09,772:INFO: Dataset: univ                Batch: 13/15	Loss 0.8601 (0.9523)
2022-11-09 17:47:09,800:INFO: Dataset: univ                Batch: 14/15	Loss 0.6850 (0.9338)
2022-11-09 17:47:09,810:INFO: Dataset: univ                Batch: 15/15	Loss 0.1424 (0.9235)
2022-11-09 17:47:10,063:INFO: Dataset: zara1               Batch: 1/8	Loss 2.4296 (2.4296)
2022-11-09 17:47:10,087:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7300 (2.0697)
2022-11-09 17:47:10,111:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1671 (1.7755)
2022-11-09 17:47:10,137:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8773 (1.5540)
2022-11-09 17:47:10,160:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7576 (1.4047)
2022-11-09 17:47:10,185:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8681 (1.3073)
2022-11-09 17:47:10,208:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5664 (1.3467)
2022-11-09 17:47:10,229:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4015 (1.3528)
2022-11-09 17:47:10,491:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5391 (1.5391)
2022-11-09 17:47:10,519:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2444 (1.3958)
2022-11-09 17:47:10,544:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6442 (1.4688)
2022-11-09 17:47:10,567:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4204 (1.4578)
2022-11-09 17:47:10,595:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8732 (1.3544)
2022-11-09 17:47:10,621:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6536 (1.2435)
2022-11-09 17:47:10,645:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6925 (1.1615)
2022-11-09 17:47:10,669:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0129 (1.1410)
2022-11-09 17:47:10,693:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5298 (1.1844)
2022-11-09 17:47:10,719:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8916 (1.1557)
2022-11-09 17:47:10,745:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0226 (1.1438)
2022-11-09 17:47:10,772:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7617 (1.1122)
2022-11-09 17:47:10,796:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7312 (1.0804)
2022-11-09 17:47:10,820:INFO: Dataset: zara2               Batch: 14/18	Loss 1.4200 (1.1040)
2022-11-09 17:47:10,845:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7189 (1.0799)
2022-11-09 17:47:10,870:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7719 (1.0628)
2022-11-09 17:47:10,894:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6917 (1.0401)
2022-11-09 17:47:10,918:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6336 (1.0199)
2022-11-09 17:47:10,970:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_106.pth.tar
2022-11-09 17:47:10,970:INFO: 
===> EPOCH: 107 (P1)
2022-11-09 17:47:10,971:INFO: - Computing loss (training)
2022-11-09 17:47:11,186:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2788 (1.2788)
2022-11-09 17:47:11,211:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9008 (1.0813)
2022-11-09 17:47:11,235:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7610 (0.9671)
2022-11-09 17:47:11,252:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6544 (0.9122)
2022-11-09 17:47:11,505:INFO: Dataset: univ                Batch:  1/15	Loss 0.8284 (0.8284)
2022-11-09 17:47:11,538:INFO: Dataset: univ                Batch:  2/15	Loss 1.1345 (0.9883)
2022-11-09 17:47:11,566:INFO: Dataset: univ                Batch:  3/15	Loss 1.0667 (1.0150)
2022-11-09 17:47:11,593:INFO: Dataset: univ                Batch:  4/15	Loss 1.3170 (1.0912)
2022-11-09 17:47:11,619:INFO: Dataset: univ                Batch:  5/15	Loss 0.7324 (1.0264)
2022-11-09 17:47:11,648:INFO: Dataset: univ                Batch:  6/15	Loss 0.8454 (0.9972)
2022-11-09 17:47:11,674:INFO: Dataset: univ                Batch:  7/15	Loss 0.6532 (0.9449)
2022-11-09 17:47:11,699:INFO: Dataset: univ                Batch:  8/15	Loss 0.9866 (0.9500)
2022-11-09 17:47:11,726:INFO: Dataset: univ                Batch:  9/15	Loss 0.7333 (0.9242)
2022-11-09 17:47:11,753:INFO: Dataset: univ                Batch: 10/15	Loss 0.7077 (0.9041)
2022-11-09 17:47:11,780:INFO: Dataset: univ                Batch: 11/15	Loss 0.7776 (0.8924)
2022-11-09 17:47:11,807:INFO: Dataset: univ                Batch: 12/15	Loss 0.9792 (0.8988)
2022-11-09 17:47:11,833:INFO: Dataset: univ                Batch: 13/15	Loss 0.6370 (0.8774)
2022-11-09 17:47:11,859:INFO: Dataset: univ                Batch: 14/15	Loss 0.6448 (0.8611)
2022-11-09 17:47:11,868:INFO: Dataset: univ                Batch: 15/15	Loss 0.1265 (0.8512)
2022-11-09 17:47:12,128:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3414 (1.3414)
2022-11-09 17:47:12,152:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3667 (1.3548)
2022-11-09 17:47:12,180:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9936 (1.2241)
2022-11-09 17:47:12,203:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4417 (1.2793)
2022-11-09 17:47:12,227:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7688 (1.1828)
2022-11-09 17:47:12,251:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8632 (1.1320)
2022-11-09 17:47:12,275:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7406 (1.0719)
2022-11-09 17:47:12,296:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6109 (1.0185)
2022-11-09 17:47:12,623:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9745 (1.9745)
2022-11-09 17:47:12,651:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2619 (1.6194)
2022-11-09 17:47:12,675:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0164 (1.7690)
2022-11-09 17:47:12,701:INFO: Dataset: zara2               Batch:  4/18	Loss 2.2408 (1.8844)
2022-11-09 17:47:12,725:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9311 (1.6905)
2022-11-09 17:47:12,752:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0761 (1.5913)
2022-11-09 17:47:12,776:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6384 (1.4550)
2022-11-09 17:47:12,800:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6595 (1.3493)
2022-11-09 17:47:12,823:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7694 (1.2837)
2022-11-09 17:47:12,848:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7987 (1.2352)
2022-11-09 17:47:12,872:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6958 (1.1879)
2022-11-09 17:47:12,898:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6927 (1.1407)
2022-11-09 17:47:12,923:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3581 (1.1573)
2022-11-09 17:47:12,948:INFO: Dataset: zara2               Batch: 14/18	Loss 1.2606 (1.1650)
2022-11-09 17:47:12,973:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6500 (1.1966)
2022-11-09 17:47:12,998:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8387 (1.2417)
2022-11-09 17:47:13,022:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6365 (1.2018)
2022-11-09 17:47:13,045:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5889 (1.1693)
2022-11-09 17:47:13,096:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_107.pth.tar
2022-11-09 17:47:13,096:INFO: 
===> EPOCH: 108 (P1)
2022-11-09 17:47:13,097:INFO: - Computing loss (training)
2022-11-09 17:47:13,310:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7845 (0.7845)
2022-11-09 17:47:13,335:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8975 (1.3384)
2022-11-09 17:47:13,362:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7588 (1.1434)
2022-11-09 17:47:13,378:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5270 (1.0466)
2022-11-09 17:47:13,631:INFO: Dataset: univ                Batch:  1/15	Loss 0.7339 (0.7339)
2022-11-09 17:47:13,659:INFO: Dataset: univ                Batch:  2/15	Loss 0.8592 (0.8005)
2022-11-09 17:47:13,687:INFO: Dataset: univ                Batch:  3/15	Loss 1.2021 (0.9548)
2022-11-09 17:47:13,716:INFO: Dataset: univ                Batch:  4/15	Loss 1.0338 (0.9753)
2022-11-09 17:47:13,744:INFO: Dataset: univ                Batch:  5/15	Loss 0.7727 (0.9331)
2022-11-09 17:47:13,774:INFO: Dataset: univ                Batch:  6/15	Loss 1.1645 (0.9733)
2022-11-09 17:47:13,799:INFO: Dataset: univ                Batch:  7/15	Loss 0.7293 (0.9396)
2022-11-09 17:47:13,825:INFO: Dataset: univ                Batch:  8/15	Loss 0.9365 (0.9392)
2022-11-09 17:47:13,852:INFO: Dataset: univ                Batch:  9/15	Loss 0.7711 (0.9198)
2022-11-09 17:47:13,879:INFO: Dataset: univ                Batch: 10/15	Loss 0.7879 (0.9067)
2022-11-09 17:47:13,907:INFO: Dataset: univ                Batch: 11/15	Loss 0.7463 (0.8910)
2022-11-09 17:47:13,934:INFO: Dataset: univ                Batch: 12/15	Loss 0.6664 (0.8711)
2022-11-09 17:47:13,960:INFO: Dataset: univ                Batch: 13/15	Loss 0.7060 (0.8588)
2022-11-09 17:47:13,986:INFO: Dataset: univ                Batch: 14/15	Loss 0.6682 (0.8451)
2022-11-09 17:47:13,997:INFO: Dataset: univ                Batch: 15/15	Loss 0.1284 (0.8347)
2022-11-09 17:47:14,247:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9098 (1.9098)
2022-11-09 17:47:14,271:INFO: Dataset: zara1               Batch: 2/8	Loss 3.5182 (2.7490)
2022-11-09 17:47:14,296:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2982 (2.2264)
2022-11-09 17:47:14,320:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3673 (2.0087)
2022-11-09 17:47:14,346:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0110 (1.8037)
2022-11-09 17:47:14,371:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9770 (1.6709)
2022-11-09 17:47:14,395:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3706 (1.6248)
2022-11-09 17:47:14,416:INFO: Dataset: zara1               Batch: 8/8	Loss 2.2562 (1.6893)
2022-11-09 17:47:14,676:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9687 (0.9687)
2022-11-09 17:47:14,703:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3308 (1.1476)
2022-11-09 17:47:14,731:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2220 (1.1736)
2022-11-09 17:47:14,756:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6997 (1.0628)
2022-11-09 17:47:14,784:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9255 (1.0339)
2022-11-09 17:47:14,811:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7560 (0.9861)
2022-11-09 17:47:14,835:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6970 (0.9451)
2022-11-09 17:47:14,860:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7002 (0.9134)
2022-11-09 17:47:14,884:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8053 (0.9020)
2022-11-09 17:47:14,910:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7876 (0.8911)
2022-11-09 17:47:14,935:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6675 (0.8714)
2022-11-09 17:47:14,961:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7211 (0.8603)
2022-11-09 17:47:14,986:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9284 (0.8657)
2022-11-09 17:47:15,010:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0159 (0.8770)
2022-11-09 17:47:15,034:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7028 (0.8657)
2022-11-09 17:47:15,060:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8428 (0.8644)
2022-11-09 17:47:15,085:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6865 (0.8537)
2022-11-09 17:47:15,109:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6163 (0.8435)
2022-11-09 17:47:15,163:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_108.pth.tar
2022-11-09 17:47:15,163:INFO: 
===> EPOCH: 109 (P1)
2022-11-09 17:47:15,163:INFO: - Computing loss (training)
2022-11-09 17:47:15,380:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2079 (2.2079)
2022-11-09 17:47:15,407:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7315 (1.4954)
2022-11-09 17:47:15,432:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7582 (1.2354)
2022-11-09 17:47:15,449:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4858 (1.1000)
2022-11-09 17:47:15,713:INFO: Dataset: univ                Batch:  1/15	Loss 0.6464 (0.6464)
2022-11-09 17:47:15,742:INFO: Dataset: univ                Batch:  2/15	Loss 0.7721 (0.7096)
2022-11-09 17:47:15,770:INFO: Dataset: univ                Batch:  3/15	Loss 0.9421 (0.7893)
2022-11-09 17:47:15,800:INFO: Dataset: univ                Batch:  4/15	Loss 0.6647 (0.7608)
2022-11-09 17:47:15,826:INFO: Dataset: univ                Batch:  5/15	Loss 0.7638 (0.7614)
2022-11-09 17:47:15,855:INFO: Dataset: univ                Batch:  6/15	Loss 0.8161 (0.7709)
2022-11-09 17:47:15,880:INFO: Dataset: univ                Batch:  7/15	Loss 0.8432 (0.7826)
2022-11-09 17:47:15,906:INFO: Dataset: univ                Batch:  8/15	Loss 0.6585 (0.7676)
2022-11-09 17:47:15,933:INFO: Dataset: univ                Batch:  9/15	Loss 0.6933 (0.7588)
2022-11-09 17:47:15,961:INFO: Dataset: univ                Batch: 10/15	Loss 0.7318 (0.7559)
2022-11-09 17:47:15,988:INFO: Dataset: univ                Batch: 11/15	Loss 0.6431 (0.7455)
2022-11-09 17:47:16,016:INFO: Dataset: univ                Batch: 12/15	Loss 0.7362 (0.7447)
2022-11-09 17:47:16,042:INFO: Dataset: univ                Batch: 13/15	Loss 0.7572 (0.7457)
2022-11-09 17:47:16,068:INFO: Dataset: univ                Batch: 14/15	Loss 0.7410 (0.7453)
2022-11-09 17:47:16,078:INFO: Dataset: univ                Batch: 15/15	Loss 0.1129 (0.7369)
2022-11-09 17:47:16,342:INFO: Dataset: zara1               Batch: 1/8	Loss 2.4398 (2.4398)
2022-11-09 17:47:16,366:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6115 (2.0466)
2022-11-09 17:47:16,391:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5568 (1.8766)
2022-11-09 17:47:16,415:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2989 (1.7385)
2022-11-09 17:47:16,441:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0526 (1.6166)
2022-11-09 17:47:16,466:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7471 (1.4712)
2022-11-09 17:47:16,490:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7980 (1.3559)
2022-11-09 17:47:16,511:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1316 (1.3309)
2022-11-09 17:47:16,769:INFO: Dataset: zara2               Batch:  1/18	Loss 2.4913 (2.4913)
2022-11-09 17:47:16,797:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6640 (2.0842)
2022-11-09 17:47:16,822:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0766 (2.0814)
2022-11-09 17:47:16,846:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6708 (1.7365)
2022-11-09 17:47:16,871:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1837 (1.6150)
2022-11-09 17:47:16,898:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7004 (1.4712)
2022-11-09 17:47:16,922:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6458 (1.3499)
2022-11-09 17:47:16,945:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7466 (1.2687)
2022-11-09 17:47:16,969:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6948 (1.2089)
2022-11-09 17:47:16,994:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0394 (1.1913)
2022-11-09 17:47:17,021:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2512 (1.1969)
2022-11-09 17:47:17,046:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9710 (1.1783)
2022-11-09 17:47:17,071:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7436 (1.1436)
2022-11-09 17:47:17,095:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6299 (1.1041)
2022-11-09 17:47:17,120:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8736 (1.0895)
2022-11-09 17:47:17,144:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9756 (1.0832)
2022-11-09 17:47:17,169:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7726 (1.0647)
2022-11-09 17:47:17,191:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5758 (1.0405)
2022-11-09 17:47:17,240:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_109.pth.tar
2022-11-09 17:47:17,240:INFO: 
===> EPOCH: 110 (P1)
2022-11-09 17:47:17,241:INFO: - Computing loss (training)
2022-11-09 17:47:17,449:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7887 (0.7887)
2022-11-09 17:47:17,477:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8602 (0.8239)
2022-11-09 17:47:17,501:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1186 (0.9212)
2022-11-09 17:47:17,519:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5558 (0.8595)
2022-11-09 17:47:17,858:INFO: Dataset: univ                Batch:  1/15	Loss 0.7323 (0.7323)
2022-11-09 17:47:17,887:INFO: Dataset: univ                Batch:  2/15	Loss 0.9812 (0.8515)
2022-11-09 17:47:17,917:INFO: Dataset: univ                Batch:  3/15	Loss 1.0747 (0.9265)
2022-11-09 17:47:17,943:INFO: Dataset: univ                Batch:  4/15	Loss 0.7289 (0.8759)
2022-11-09 17:47:17,973:INFO: Dataset: univ                Batch:  5/15	Loss 1.0732 (0.9185)
2022-11-09 17:47:18,001:INFO: Dataset: univ                Batch:  6/15	Loss 0.7108 (0.8889)
2022-11-09 17:47:18,027:INFO: Dataset: univ                Batch:  7/15	Loss 0.7122 (0.8641)
2022-11-09 17:47:18,053:INFO: Dataset: univ                Batch:  8/15	Loss 0.7280 (0.8473)
2022-11-09 17:47:18,080:INFO: Dataset: univ                Batch:  9/15	Loss 0.6598 (0.8269)
2022-11-09 17:47:18,108:INFO: Dataset: univ                Batch: 10/15	Loss 0.5716 (0.8004)
2022-11-09 17:47:18,136:INFO: Dataset: univ                Batch: 11/15	Loss 0.7409 (0.7953)
2022-11-09 17:47:18,164:INFO: Dataset: univ                Batch: 12/15	Loss 0.6717 (0.7848)
2022-11-09 17:47:18,190:INFO: Dataset: univ                Batch: 13/15	Loss 0.8000 (0.7859)
2022-11-09 17:47:18,217:INFO: Dataset: univ                Batch: 14/15	Loss 0.6365 (0.7751)
2022-11-09 17:47:18,227:INFO: Dataset: univ                Batch: 15/15	Loss 0.1208 (0.7676)
2022-11-09 17:47:18,476:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7873 (1.7873)
2022-11-09 17:47:18,500:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5612 (1.6672)
2022-11-09 17:47:18,524:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9105 (1.4336)
2022-11-09 17:47:18,547:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9885 (1.3160)
2022-11-09 17:47:18,571:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7714 (1.2083)
2022-11-09 17:47:18,596:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2532 (1.2162)
2022-11-09 17:47:18,620:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9204 (1.1744)
2022-11-09 17:47:18,641:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6169 (1.1227)
2022-11-09 17:47:18,900:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2963 (1.2963)
2022-11-09 17:47:18,927:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1092 (1.2008)
2022-11-09 17:47:18,951:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4063 (1.2679)
2022-11-09 17:47:18,975:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2413 (1.2611)
2022-11-09 17:47:19,003:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8324 (1.1816)
2022-11-09 17:47:19,029:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6198 (1.0849)
2022-11-09 17:47:19,053:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6344 (1.0206)
2022-11-09 17:47:19,076:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7187 (0.9841)
2022-11-09 17:47:19,100:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6571 (0.9478)
2022-11-09 17:47:19,125:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7245 (0.9254)
2022-11-09 17:47:19,149:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7445 (0.9105)
2022-11-09 17:47:19,175:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7119 (0.8938)
2022-11-09 17:47:19,199:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2935 (0.9219)
2022-11-09 17:47:19,223:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7295 (0.9085)
2022-11-09 17:47:19,248:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6618 (0.8938)
2022-11-09 17:47:19,272:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8272 (0.8897)
2022-11-09 17:47:19,297:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9491 (0.8932)
2022-11-09 17:47:19,320:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5255 (0.8750)
2022-11-09 17:47:19,370:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_110.pth.tar
2022-11-09 17:47:19,370:INFO: 
===> EPOCH: 111 (P1)
2022-11-09 17:47:19,370:INFO: - Computing loss (training)
2022-11-09 17:47:19,593:INFO: Dataset: hotel               Batch: 1/4	Loss 2.3407 (2.3407)
2022-11-09 17:47:19,619:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9791 (1.6824)
2022-11-09 17:47:19,643:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5833 (1.6491)
2022-11-09 17:47:19,659:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4743 (1.4631)
2022-11-09 17:47:19,916:INFO: Dataset: univ                Batch:  1/15	Loss 1.2266 (1.2266)
2022-11-09 17:47:19,945:INFO: Dataset: univ                Batch:  2/15	Loss 0.7894 (1.0066)
2022-11-09 17:47:19,971:INFO: Dataset: univ                Batch:  3/15	Loss 1.2621 (1.0899)
2022-11-09 17:47:20,001:INFO: Dataset: univ                Batch:  4/15	Loss 0.6917 (0.9968)
2022-11-09 17:47:20,028:INFO: Dataset: univ                Batch:  5/15	Loss 0.8688 (0.9708)
2022-11-09 17:47:20,057:INFO: Dataset: univ                Batch:  6/15	Loss 0.7714 (0.9368)
2022-11-09 17:47:20,083:INFO: Dataset: univ                Batch:  7/15	Loss 0.6746 (0.9016)
2022-11-09 17:47:20,109:INFO: Dataset: univ                Batch:  8/15	Loss 0.8503 (0.8956)
2022-11-09 17:47:20,135:INFO: Dataset: univ                Batch:  9/15	Loss 0.7908 (0.8837)
2022-11-09 17:47:20,163:INFO: Dataset: univ                Batch: 10/15	Loss 1.1432 (0.9121)
2022-11-09 17:47:20,190:INFO: Dataset: univ                Batch: 11/15	Loss 1.0125 (0.9221)
2022-11-09 17:47:20,218:INFO: Dataset: univ                Batch: 12/15	Loss 1.0432 (0.9311)
2022-11-09 17:47:20,244:INFO: Dataset: univ                Batch: 13/15	Loss 0.6535 (0.9114)
2022-11-09 17:47:20,269:INFO: Dataset: univ                Batch: 14/15	Loss 0.7375 (0.8992)
2022-11-09 17:47:20,280:INFO: Dataset: univ                Batch: 15/15	Loss 0.1154 (0.8886)
2022-11-09 17:47:20,546:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8400 (1.8400)
2022-11-09 17:47:20,573:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7221 (1.7784)
2022-11-09 17:47:20,597:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2735 (1.5987)
2022-11-09 17:47:20,621:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7361 (1.6338)
2022-11-09 17:47:20,647:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8000 (1.4729)
2022-11-09 17:47:20,672:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7565 (1.3570)
2022-11-09 17:47:20,695:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8337 (1.2896)
2022-11-09 17:47:20,716:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7008 (1.2109)
2022-11-09 17:47:20,970:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2494 (1.2494)
2022-11-09 17:47:21,078:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1056 (1.1748)
2022-11-09 17:47:21,107:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7795 (1.3609)
2022-11-09 17:47:21,130:INFO: Dataset: zara2               Batch:  4/18	Loss 2.2373 (1.5761)
2022-11-09 17:47:21,155:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1866 (1.4982)
2022-11-09 17:47:21,181:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7969 (1.3733)
2022-11-09 17:47:21,206:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6411 (1.2694)
2022-11-09 17:47:21,231:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7094 (1.1984)
2022-11-09 17:47:21,255:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7450 (1.1541)
2022-11-09 17:47:21,279:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6835 (1.1023)
2022-11-09 17:47:21,305:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9035 (1.0842)
2022-11-09 17:47:21,330:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6240 (1.0455)
2022-11-09 17:47:21,354:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0763 (1.0478)
2022-11-09 17:47:21,379:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7149 (1.0248)
2022-11-09 17:47:21,405:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6847 (1.0030)
2022-11-09 17:47:21,432:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6670 (0.9821)
2022-11-09 17:47:21,458:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9667 (0.9811)
2022-11-09 17:47:21,482:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8944 (0.9772)
2022-11-09 17:47:21,529:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_111.pth.tar
2022-11-09 17:47:21,529:INFO: 
===> EPOCH: 112 (P1)
2022-11-09 17:47:21,530:INFO: - Computing loss (training)
2022-11-09 17:47:21,725:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9670 (0.9670)
2022-11-09 17:47:21,751:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9532 (0.9600)
2022-11-09 17:47:21,775:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1592 (1.0273)
2022-11-09 17:47:21,793:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4792 (0.9391)
2022-11-09 17:47:22,044:INFO: Dataset: univ                Batch:  1/15	Loss 0.7835 (0.7835)
2022-11-09 17:47:22,074:INFO: Dataset: univ                Batch:  2/15	Loss 0.7647 (0.7743)
2022-11-09 17:47:22,102:INFO: Dataset: univ                Batch:  3/15	Loss 0.8203 (0.7896)
2022-11-09 17:47:22,128:INFO: Dataset: univ                Batch:  4/15	Loss 0.7626 (0.7827)
2022-11-09 17:47:22,154:INFO: Dataset: univ                Batch:  5/15	Loss 0.7029 (0.7667)
2022-11-09 17:47:22,184:INFO: Dataset: univ                Batch:  6/15	Loss 0.8446 (0.7799)
2022-11-09 17:47:22,210:INFO: Dataset: univ                Batch:  7/15	Loss 0.6901 (0.7658)
2022-11-09 17:47:22,235:INFO: Dataset: univ                Batch:  8/15	Loss 0.8380 (0.7747)
2022-11-09 17:47:22,263:INFO: Dataset: univ                Batch:  9/15	Loss 0.6399 (0.7586)
2022-11-09 17:47:22,291:INFO: Dataset: univ                Batch: 10/15	Loss 0.7533 (0.7581)
2022-11-09 17:47:22,319:INFO: Dataset: univ                Batch: 11/15	Loss 0.9255 (0.7721)
2022-11-09 17:47:22,346:INFO: Dataset: univ                Batch: 12/15	Loss 1.5235 (0.8300)
2022-11-09 17:47:22,372:INFO: Dataset: univ                Batch: 13/15	Loss 0.6665 (0.8173)
2022-11-09 17:47:22,398:INFO: Dataset: univ                Batch: 14/15	Loss 1.2152 (0.8456)
2022-11-09 17:47:22,408:INFO: Dataset: univ                Batch: 15/15	Loss 0.1264 (0.8362)
2022-11-09 17:47:22,670:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9982 (1.9982)
2022-11-09 17:47:22,695:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0285 (2.0137)
2022-11-09 17:47:22,718:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9098 (1.6304)
2022-11-09 17:47:22,742:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8165 (1.4425)
2022-11-09 17:47:22,765:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6629 (1.2920)
2022-11-09 17:47:22,791:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2354 (1.2828)
2022-11-09 17:47:22,814:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4570 (1.3096)
2022-11-09 17:47:22,835:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0547 (1.2815)
2022-11-09 17:47:23,087:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7680 (1.7680)
2022-11-09 17:47:23,152:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2169 (1.4966)
2022-11-09 17:47:23,176:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8713 (1.2911)
2022-11-09 17:47:23,199:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0180 (1.2184)
2022-11-09 17:47:23,223:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8296 (1.1449)
2022-11-09 17:47:23,251:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6375 (1.0626)
2022-11-09 17:47:23,275:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7099 (1.0075)
2022-11-09 17:47:23,299:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7072 (0.9689)
2022-11-09 17:47:23,324:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0415 (1.0845)
2022-11-09 17:47:23,348:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9255 (1.0684)
2022-11-09 17:47:23,373:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9991 (1.0619)
2022-11-09 17:47:23,399:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7134 (1.0307)
2022-11-09 17:47:23,422:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9985 (1.0281)
2022-11-09 17:47:23,446:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6823 (1.0024)
2022-11-09 17:47:23,471:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6148 (0.9786)
2022-11-09 17:47:23,495:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3746 (1.0030)
2022-11-09 17:47:23,521:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7844 (0.9903)
2022-11-09 17:47:23,544:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5582 (0.9712)
2022-11-09 17:47:23,593:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_112.pth.tar
2022-11-09 17:47:23,593:INFO: 
===> EPOCH: 113 (P1)
2022-11-09 17:47:23,593:INFO: - Computing loss (training)
2022-11-09 17:47:23,807:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4506 (1.4506)
2022-11-09 17:47:23,832:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1084 (1.7826)
2022-11-09 17:47:23,855:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8086 (1.4539)
2022-11-09 17:47:23,872:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5788 (1.3073)
2022-11-09 17:47:24,120:INFO: Dataset: univ                Batch:  1/15	Loss 0.9910 (0.9910)
2022-11-09 17:47:24,177:INFO: Dataset: univ                Batch:  2/15	Loss 1.0719 (1.0280)
2022-11-09 17:47:24,203:INFO: Dataset: univ                Batch:  3/15	Loss 2.0230 (1.3627)
2022-11-09 17:47:24,229:INFO: Dataset: univ                Batch:  4/15	Loss 1.6606 (1.4417)
2022-11-09 17:47:24,256:INFO: Dataset: univ                Batch:  5/15	Loss 1.5386 (1.4623)
2022-11-09 17:47:24,284:INFO: Dataset: univ                Batch:  6/15	Loss 1.5021 (1.4685)
2022-11-09 17:47:24,310:INFO: Dataset: univ                Batch:  7/15	Loss 0.7076 (1.3632)
2022-11-09 17:47:24,337:INFO: Dataset: univ                Batch:  8/15	Loss 0.7043 (1.2817)
2022-11-09 17:47:24,364:INFO: Dataset: univ                Batch:  9/15	Loss 0.6696 (1.2199)
2022-11-09 17:47:24,393:INFO: Dataset: univ                Batch: 10/15	Loss 0.6543 (1.1618)
2022-11-09 17:47:24,422:INFO: Dataset: univ                Batch: 11/15	Loss 0.7235 (1.1236)
2022-11-09 17:47:24,448:INFO: Dataset: univ                Batch: 12/15	Loss 0.6608 (1.0853)
2022-11-09 17:47:24,474:INFO: Dataset: univ                Batch: 13/15	Loss 1.0761 (1.0846)
2022-11-09 17:47:24,501:INFO: Dataset: univ                Batch: 14/15	Loss 0.8973 (1.0724)
2022-11-09 17:47:24,511:INFO: Dataset: univ                Batch: 15/15	Loss 0.1336 (1.0612)
2022-11-09 17:47:24,767:INFO: Dataset: zara1               Batch: 1/8	Loss 2.6072 (2.6072)
2022-11-09 17:47:24,795:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3912 (2.0215)
2022-11-09 17:47:24,818:INFO: Dataset: zara1               Batch: 3/8	Loss 2.1981 (2.0787)
2022-11-09 17:47:24,842:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2763 (1.8848)
2022-11-09 17:47:24,865:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7248 (1.6264)
2022-11-09 17:47:24,891:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1383 (1.5512)
2022-11-09 17:47:24,914:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7781 (1.5826)
2022-11-09 17:47:24,935:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0070 (1.6301)
2022-11-09 17:47:25,188:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5534 (1.5534)
2022-11-09 17:47:25,216:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4381 (1.4967)
2022-11-09 17:47:25,240:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6152 (1.5349)
2022-11-09 17:47:25,267:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8267 (1.3708)
2022-11-09 17:47:25,292:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6308 (1.2213)
2022-11-09 17:47:25,319:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6098 (1.1221)
2022-11-09 17:47:25,343:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7792 (1.0771)
2022-11-09 17:47:25,366:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8457 (1.0485)
2022-11-09 17:47:25,390:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9713 (1.0390)
2022-11-09 17:47:25,415:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7195 (1.0045)
2022-11-09 17:47:25,439:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7491 (0.9835)
2022-11-09 17:47:25,465:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9687 (0.9823)
2022-11-09 17:47:25,490:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6238 (0.9530)
2022-11-09 17:47:25,514:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7654 (0.9391)
2022-11-09 17:47:25,539:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8418 (0.9324)
2022-11-09 17:47:25,563:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7893 (0.9231)
2022-11-09 17:47:25,588:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9921 (0.9272)
2022-11-09 17:47:25,610:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6924 (0.9163)
2022-11-09 17:47:25,661:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_113.pth.tar
2022-11-09 17:47:25,661:INFO: 
===> EPOCH: 114 (P1)
2022-11-09 17:47:25,661:INFO: - Computing loss (training)
2022-11-09 17:47:25,867:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2151 (1.2151)
2022-11-09 17:47:25,893:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8262 (1.0243)
2022-11-09 17:47:25,920:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7503 (0.9341)
2022-11-09 17:47:25,936:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5491 (0.8717)
2022-11-09 17:47:26,192:INFO: Dataset: univ                Batch:  1/15	Loss 0.7985 (0.7985)
2022-11-09 17:47:26,219:INFO: Dataset: univ                Batch:  2/15	Loss 0.7260 (0.7603)
2022-11-09 17:47:26,247:INFO: Dataset: univ                Batch:  3/15	Loss 1.4338 (0.9790)
2022-11-09 17:47:26,275:INFO: Dataset: univ                Batch:  4/15	Loss 1.1733 (1.0273)
2022-11-09 17:47:26,303:INFO: Dataset: univ                Batch:  5/15	Loss 0.9521 (1.0133)
2022-11-09 17:47:26,331:INFO: Dataset: univ                Batch:  6/15	Loss 0.7492 (0.9678)
2022-11-09 17:47:26,357:INFO: Dataset: univ                Batch:  7/15	Loss 0.6591 (0.9251)
2022-11-09 17:47:26,383:INFO: Dataset: univ                Batch:  8/15	Loss 0.6392 (0.8936)
2022-11-09 17:47:26,409:INFO: Dataset: univ                Batch:  9/15	Loss 0.6565 (0.8696)
2022-11-09 17:47:26,436:INFO: Dataset: univ                Batch: 10/15	Loss 0.8734 (0.8700)
2022-11-09 17:47:26,462:INFO: Dataset: univ                Batch: 11/15	Loss 0.7399 (0.8581)
2022-11-09 17:47:26,490:INFO: Dataset: univ                Batch: 12/15	Loss 0.6760 (0.8428)
2022-11-09 17:47:26,516:INFO: Dataset: univ                Batch: 13/15	Loss 0.6828 (0.8299)
2022-11-09 17:47:26,541:INFO: Dataset: univ                Batch: 14/15	Loss 1.4366 (0.8703)
2022-11-09 17:47:26,552:INFO: Dataset: univ                Batch: 15/15	Loss 0.1553 (0.8620)
2022-11-09 17:47:26,807:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3035 (2.3035)
2022-11-09 17:47:26,830:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2701 (1.8204)
2022-11-09 17:47:26,854:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1099 (1.5779)
2022-11-09 17:47:26,878:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6805 (1.3515)
2022-11-09 17:47:26,901:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6648 (1.2230)
2022-11-09 17:47:26,927:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0413 (1.1948)
2022-11-09 17:47:26,950:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1725 (1.1917)
2022-11-09 17:47:26,971:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9341 (1.1632)
2022-11-09 17:47:27,228:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3358 (1.3358)
2022-11-09 17:47:27,256:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7154 (1.0352)
2022-11-09 17:47:27,286:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0621 (1.0439)
2022-11-09 17:47:27,312:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0688 (1.0506)
2022-11-09 17:47:27,339:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7635 (0.9918)
2022-11-09 17:47:27,366:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1834 (1.0189)
2022-11-09 17:47:27,391:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6657 (0.9645)
2022-11-09 17:47:27,416:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8195 (0.9478)
2022-11-09 17:47:27,441:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6118 (0.9153)
2022-11-09 17:47:27,467:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1480 (0.9372)
2022-11-09 17:47:27,495:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9802 (0.9410)
2022-11-09 17:47:27,521:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6625 (0.9192)
2022-11-09 17:47:27,546:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6027 (0.8936)
2022-11-09 17:47:27,571:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1750 (0.9139)
2022-11-09 17:47:27,597:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6796 (0.8997)
2022-11-09 17:47:27,623:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8033 (0.8943)
2022-11-09 17:47:27,649:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7008 (0.8824)
2022-11-09 17:47:27,673:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5145 (0.8654)
2022-11-09 17:47:27,722:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_114.pth.tar
2022-11-09 17:47:27,722:INFO: 
===> EPOCH: 115 (P1)
2022-11-09 17:47:27,722:INFO: - Computing loss (training)
2022-11-09 17:47:27,943:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1446 (1.1446)
2022-11-09 17:47:27,969:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7585 (0.9597)
2022-11-09 17:47:27,993:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8861 (0.9344)
2022-11-09 17:47:28,010:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6670 (0.8846)
2022-11-09 17:47:28,265:INFO: Dataset: univ                Batch:  1/15	Loss 1.6852 (1.6852)
2022-11-09 17:47:28,294:INFO: Dataset: univ                Batch:  2/15	Loss 0.8697 (1.2913)
2022-11-09 17:47:28,326:INFO: Dataset: univ                Batch:  3/15	Loss 0.8656 (1.1400)
2022-11-09 17:47:28,352:INFO: Dataset: univ                Batch:  4/15	Loss 0.9184 (1.0861)
2022-11-09 17:47:28,378:INFO: Dataset: univ                Batch:  5/15	Loss 0.7854 (1.0262)
2022-11-09 17:47:28,406:INFO: Dataset: univ                Batch:  6/15	Loss 0.8460 (0.9966)
2022-11-09 17:47:28,432:INFO: Dataset: univ                Batch:  7/15	Loss 0.8225 (0.9713)
2022-11-09 17:47:28,457:INFO: Dataset: univ                Batch:  8/15	Loss 0.6632 (0.9352)
2022-11-09 17:47:28,485:INFO: Dataset: univ                Batch:  9/15	Loss 0.9357 (0.9352)
2022-11-09 17:47:28,513:INFO: Dataset: univ                Batch: 10/15	Loss 0.6517 (0.9069)
2022-11-09 17:47:28,541:INFO: Dataset: univ                Batch: 11/15	Loss 0.8665 (0.9034)
2022-11-09 17:47:28,569:INFO: Dataset: univ                Batch: 12/15	Loss 0.6918 (0.8850)
2022-11-09 17:47:28,595:INFO: Dataset: univ                Batch: 13/15	Loss 0.6283 (0.8638)
2022-11-09 17:47:28,621:INFO: Dataset: univ                Batch: 14/15	Loss 0.6664 (0.8495)
2022-11-09 17:47:28,631:INFO: Dataset: univ                Batch: 15/15	Loss 0.1397 (0.8430)
2022-11-09 17:47:28,891:INFO: Dataset: zara1               Batch: 1/8	Loss 3.3279 (3.3279)
2022-11-09 17:47:28,915:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4001 (2.2598)
2022-11-09 17:47:28,939:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2875 (1.9737)
2022-11-09 17:47:28,963:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0897 (1.7635)
2022-11-09 17:47:28,986:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8466 (1.5775)
2022-11-09 17:47:29,012:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8759 (1.4521)
2022-11-09 17:47:29,036:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9013 (1.3770)
2022-11-09 17:47:29,057:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2190 (1.3607)
2022-11-09 17:47:29,323:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6502 (1.6502)
2022-11-09 17:47:29,348:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6189 (1.6355)
2022-11-09 17:47:29,373:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2802 (1.5145)
2022-11-09 17:47:29,399:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1163 (1.4117)
2022-11-09 17:47:29,426:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6179 (1.2572)
2022-11-09 17:47:29,452:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6603 (1.1674)
2022-11-09 17:47:29,476:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5961 (1.0869)
2022-11-09 17:47:29,500:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6315 (1.0257)
2022-11-09 17:47:29,523:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0415 (1.0275)
2022-11-09 17:47:29,548:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7185 (0.9961)
2022-11-09 17:47:29,573:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6836 (0.9680)
2022-11-09 17:47:29,599:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1889 (0.9861)
2022-11-09 17:47:29,623:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7502 (0.9689)
2022-11-09 17:47:29,647:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6586 (0.9463)
2022-11-09 17:47:29,672:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7529 (0.9335)
2022-11-09 17:47:29,697:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6884 (0.9161)
2022-11-09 17:47:29,721:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6895 (0.9045)
2022-11-09 17:47:29,743:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5618 (0.8880)
2022-11-09 17:47:29,792:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_115.pth.tar
2022-11-09 17:47:29,792:INFO: 
===> EPOCH: 116 (P1)
2022-11-09 17:47:29,793:INFO: - Computing loss (training)
2022-11-09 17:47:29,995:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8075 (0.8075)
2022-11-09 17:47:30,022:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7279 (0.7673)
2022-11-09 17:47:30,048:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6506 (0.7276)
2022-11-09 17:47:30,065:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5241 (0.6911)
2022-11-09 17:47:30,310:INFO: Dataset: univ                Batch:  1/15	Loss 0.8334 (0.8334)
2022-11-09 17:47:30,338:INFO: Dataset: univ                Batch:  2/15	Loss 0.8156 (0.8242)
2022-11-09 17:47:30,366:INFO: Dataset: univ                Batch:  3/15	Loss 1.0256 (0.8928)
2022-11-09 17:47:30,398:INFO: Dataset: univ                Batch:  4/15	Loss 0.9572 (0.9097)
2022-11-09 17:47:30,424:INFO: Dataset: univ                Batch:  5/15	Loss 1.1297 (0.9548)
2022-11-09 17:47:30,453:INFO: Dataset: univ                Batch:  6/15	Loss 0.6398 (0.9041)
2022-11-09 17:47:30,480:INFO: Dataset: univ                Batch:  7/15	Loss 0.6992 (0.8732)
2022-11-09 17:47:30,506:INFO: Dataset: univ                Batch:  8/15	Loss 0.6243 (0.8413)
2022-11-09 17:47:30,534:INFO: Dataset: univ                Batch:  9/15	Loss 0.6863 (0.8255)
2022-11-09 17:47:30,562:INFO: Dataset: univ                Batch: 10/15	Loss 0.7235 (0.8155)
2022-11-09 17:47:30,591:INFO: Dataset: univ                Batch: 11/15	Loss 0.6069 (0.7967)
2022-11-09 17:47:30,619:INFO: Dataset: univ                Batch: 12/15	Loss 0.6288 (0.7823)
2022-11-09 17:47:30,645:INFO: Dataset: univ                Batch: 13/15	Loss 0.5777 (0.7669)
2022-11-09 17:47:30,673:INFO: Dataset: univ                Batch: 14/15	Loss 0.6215 (0.7552)
2022-11-09 17:47:30,683:INFO: Dataset: univ                Batch: 15/15	Loss 0.1075 (0.7478)
2022-11-09 17:47:30,930:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3325 (2.3325)
2022-11-09 17:47:30,954:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7669 (1.5650)
2022-11-09 17:47:30,978:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4924 (1.5417)
2022-11-09 17:47:31,001:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2369 (1.4620)
2022-11-09 17:47:31,028:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7341 (1.3019)
2022-11-09 17:47:31,052:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7224 (1.1993)
2022-11-09 17:47:31,076:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8245 (1.1443)
2022-11-09 17:47:31,097:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9046 (1.1161)
2022-11-09 17:47:31,346:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2636 (1.2636)
2022-11-09 17:47:31,372:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3390 (1.3020)
2022-11-09 17:47:31,397:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5444 (1.3830)
2022-11-09 17:47:31,422:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8895 (1.2569)
2022-11-09 17:47:31,449:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7522 (1.1615)
2022-11-09 17:47:31,475:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7099 (1.0937)
2022-11-09 17:47:31,499:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6086 (1.0296)
2022-11-09 17:47:31,522:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6622 (0.9851)
2022-11-09 17:47:31,546:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6199 (0.9493)
2022-11-09 17:47:31,571:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6002 (0.9158)
2022-11-09 17:47:31,595:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7272 (0.8981)
2022-11-09 17:47:31,621:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9124 (0.8991)
2022-11-09 17:47:31,645:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5086 (0.9473)
2022-11-09 17:47:31,668:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6999 (0.9303)
2022-11-09 17:47:31,693:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6587 (0.9124)
2022-11-09 17:47:31,718:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6275 (0.8938)
2022-11-09 17:47:31,742:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6625 (0.8801)
2022-11-09 17:47:31,765:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6289 (0.8680)
2022-11-09 17:47:31,814:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_116.pth.tar
2022-11-09 17:47:31,814:INFO: 
===> EPOCH: 117 (P1)
2022-11-09 17:47:31,814:INFO: - Computing loss (training)
2022-11-09 17:47:32,014:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5207 (1.5207)
2022-11-09 17:47:32,039:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8157 (1.1580)
2022-11-09 17:47:32,064:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6810 (0.9934)
2022-11-09 17:47:32,080:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4719 (0.9047)
2022-11-09 17:47:32,332:INFO: Dataset: univ                Batch:  1/15	Loss 1.0688 (1.0688)
2022-11-09 17:47:32,362:INFO: Dataset: univ                Batch:  2/15	Loss 1.0214 (1.0453)
2022-11-09 17:47:32,389:INFO: Dataset: univ                Batch:  3/15	Loss 0.7874 (0.9526)
2022-11-09 17:47:32,418:INFO: Dataset: univ                Batch:  4/15	Loss 0.6264 (0.8724)
2022-11-09 17:47:32,447:INFO: Dataset: univ                Batch:  5/15	Loss 0.9015 (0.8788)
2022-11-09 17:47:32,476:INFO: Dataset: univ                Batch:  6/15	Loss 0.6256 (0.8385)
2022-11-09 17:47:32,502:INFO: Dataset: univ                Batch:  7/15	Loss 0.7001 (0.8190)
2022-11-09 17:47:32,528:INFO: Dataset: univ                Batch:  8/15	Loss 0.6818 (0.8023)
2022-11-09 17:47:32,555:INFO: Dataset: univ                Batch:  9/15	Loss 0.8094 (0.8030)
2022-11-09 17:47:32,583:INFO: Dataset: univ                Batch: 10/15	Loss 0.6859 (0.7904)
2022-11-09 17:47:32,612:INFO: Dataset: univ                Batch: 11/15	Loss 0.5917 (0.7709)
2022-11-09 17:47:32,639:INFO: Dataset: univ                Batch: 12/15	Loss 0.7517 (0.7693)
2022-11-09 17:47:32,665:INFO: Dataset: univ                Batch: 13/15	Loss 0.8655 (0.7769)
2022-11-09 17:47:32,691:INFO: Dataset: univ                Batch: 14/15	Loss 0.6327 (0.7665)
2022-11-09 17:47:32,701:INFO: Dataset: univ                Batch: 15/15	Loss 0.1008 (0.7580)
2022-11-09 17:47:32,952:INFO: Dataset: zara1               Batch: 1/8	Loss 2.9785 (2.9785)
2022-11-09 17:47:32,976:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6235 (2.2822)
2022-11-09 17:47:33,001:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4848 (2.0399)
2022-11-09 17:47:33,025:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7806 (1.7034)
2022-11-09 17:47:33,051:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7343 (1.5159)
2022-11-09 17:47:33,075:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9252 (1.4089)
2022-11-09 17:47:33,099:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1194 (1.3724)
2022-11-09 17:47:33,120:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0723 (1.3427)
2022-11-09 17:47:33,365:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9643 (0.9643)
2022-11-09 17:47:33,390:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2855 (1.1210)
2022-11-09 17:47:33,415:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5159 (1.2522)
2022-11-09 17:47:33,442:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9927 (1.1842)
2022-11-09 17:47:33,468:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7291 (1.0889)
2022-11-09 17:47:33,493:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6567 (1.0127)
2022-11-09 17:47:33,517:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6657 (0.9664)
2022-11-09 17:47:33,540:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6084 (0.9230)
2022-11-09 17:47:33,564:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6112 (0.8905)
2022-11-09 17:47:33,588:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0487 (0.9072)
2022-11-09 17:47:33,615:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6689 (0.8847)
2022-11-09 17:47:33,639:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8149 (0.8794)
2022-11-09 17:47:33,664:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6968 (0.8667)
2022-11-09 17:47:33,688:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7777 (0.8602)
2022-11-09 17:47:33,713:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6649 (0.8477)
2022-11-09 17:47:33,737:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6215 (0.8325)
2022-11-09 17:47:33,762:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5540 (0.8165)
2022-11-09 17:47:33,784:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7612 (0.8138)
2022-11-09 17:47:33,833:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_117.pth.tar
2022-11-09 17:47:33,834:INFO: 
===> EPOCH: 118 (P1)
2022-11-09 17:47:33,834:INFO: - Computing loss (training)
2022-11-09 17:47:34,056:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9283 (0.9283)
2022-11-09 17:47:34,084:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7549 (0.8372)
2022-11-09 17:47:34,108:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6944 (0.7909)
2022-11-09 17:47:34,125:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4251 (0.7321)
2022-11-09 17:47:34,380:INFO: Dataset: univ                Batch:  1/15	Loss 0.8338 (0.8338)
2022-11-09 17:47:34,421:INFO: Dataset: univ                Batch:  2/15	Loss 0.8625 (0.8477)
2022-11-09 17:47:34,448:INFO: Dataset: univ                Batch:  3/15	Loss 0.6151 (0.7692)
2022-11-09 17:47:34,473:INFO: Dataset: univ                Batch:  4/15	Loss 0.6938 (0.7523)
2022-11-09 17:47:34,499:INFO: Dataset: univ                Batch:  5/15	Loss 0.8686 (0.7717)
2022-11-09 17:47:34,528:INFO: Dataset: univ                Batch:  6/15	Loss 0.7441 (0.7672)
2022-11-09 17:47:34,554:INFO: Dataset: univ                Batch:  7/15	Loss 0.8107 (0.7735)
2022-11-09 17:47:34,580:INFO: Dataset: univ                Batch:  8/15	Loss 0.8145 (0.7789)
2022-11-09 17:47:34,609:INFO: Dataset: univ                Batch:  9/15	Loss 0.5646 (0.7537)
2022-11-09 17:47:34,637:INFO: Dataset: univ                Batch: 10/15	Loss 0.7452 (0.7529)
2022-11-09 17:47:34,666:INFO: Dataset: univ                Batch: 11/15	Loss 0.9196 (0.7693)
2022-11-09 17:47:34,693:INFO: Dataset: univ                Batch: 12/15	Loss 0.6162 (0.7555)
2022-11-09 17:47:34,719:INFO: Dataset: univ                Batch: 13/15	Loss 0.6238 (0.7446)
2022-11-09 17:47:34,745:INFO: Dataset: univ                Batch: 14/15	Loss 0.5578 (0.7315)
2022-11-09 17:47:34,756:INFO: Dataset: univ                Batch: 15/15	Loss 0.1058 (0.7224)
2022-11-09 17:47:34,994:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3363 (1.3363)
2022-11-09 17:47:35,019:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3736 (1.3549)
2022-11-09 17:47:35,043:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7652 (1.4916)
2022-11-09 17:47:35,069:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9044 (1.3291)
2022-11-09 17:47:35,094:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7458 (1.2142)
2022-11-09 17:47:35,118:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7941 (1.1409)
2022-11-09 17:47:35,142:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7245 (1.0793)
2022-11-09 17:47:35,163:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8119 (1.0486)
2022-11-09 17:47:35,419:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0003 (1.0003)
2022-11-09 17:47:35,451:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4630 (1.2445)
2022-11-09 17:47:35,475:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2511 (1.2467)
2022-11-09 17:47:35,500:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9683 (1.1749)
2022-11-09 17:47:35,524:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5972 (1.0528)
2022-11-09 17:47:35,553:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5400 (0.9620)
2022-11-09 17:47:35,577:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5883 (0.9039)
2022-11-09 17:47:35,601:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6789 (0.8790)
2022-11-09 17:47:35,626:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6273 (0.8541)
2022-11-09 17:47:35,653:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5969 (0.8281)
2022-11-09 17:47:35,681:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5499 (0.8049)
2022-11-09 17:47:35,707:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6202 (0.7874)
2022-11-09 17:47:35,731:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0035 (0.8030)
2022-11-09 17:47:35,756:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7059 (0.7956)
2022-11-09 17:47:35,782:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1515 (0.8212)
2022-11-09 17:47:35,807:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5994 (0.8079)
2022-11-09 17:47:35,832:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6145 (0.7972)
2022-11-09 17:47:35,857:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6171 (0.7889)
2022-11-09 17:47:35,906:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_118.pth.tar
2022-11-09 17:47:35,906:INFO: 
===> EPOCH: 119 (P1)
2022-11-09 17:47:35,907:INFO: - Computing loss (training)
2022-11-09 17:47:36,118:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1235 (1.1235)
2022-11-09 17:47:36,144:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5781 (1.3546)
2022-11-09 17:47:36,170:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0730 (1.2615)
2022-11-09 17:47:36,187:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4165 (1.1143)
2022-11-09 17:47:36,444:INFO: Dataset: univ                Batch:  1/15	Loss 1.0491 (1.0491)
2022-11-09 17:47:36,473:INFO: Dataset: univ                Batch:  2/15	Loss 1.1285 (1.0888)
2022-11-09 17:47:36,500:INFO: Dataset: univ                Batch:  3/15	Loss 1.1380 (1.1042)
2022-11-09 17:47:36,530:INFO: Dataset: univ                Batch:  4/15	Loss 1.0142 (1.0820)
2022-11-09 17:47:36,557:INFO: Dataset: univ                Batch:  5/15	Loss 0.6139 (0.9941)
2022-11-09 17:47:36,585:INFO: Dataset: univ                Batch:  6/15	Loss 1.1706 (1.0245)
2022-11-09 17:47:36,611:INFO: Dataset: univ                Batch:  7/15	Loss 0.7621 (0.9849)
2022-11-09 17:47:36,637:INFO: Dataset: univ                Batch:  8/15	Loss 0.8466 (0.9697)
2022-11-09 17:47:36,664:INFO: Dataset: univ                Batch:  9/15	Loss 1.0046 (0.9737)
2022-11-09 17:47:36,691:INFO: Dataset: univ                Batch: 10/15	Loss 0.6347 (0.9412)
2022-11-09 17:47:36,720:INFO: Dataset: univ                Batch: 11/15	Loss 0.8190 (0.9299)
2022-11-09 17:47:36,746:INFO: Dataset: univ                Batch: 12/15	Loss 0.7446 (0.9167)
2022-11-09 17:47:36,772:INFO: Dataset: univ                Batch: 13/15	Loss 0.6281 (0.8928)
2022-11-09 17:47:36,797:INFO: Dataset: univ                Batch: 14/15	Loss 0.6109 (0.8742)
2022-11-09 17:47:36,806:INFO: Dataset: univ                Batch: 15/15	Loss 0.1453 (0.8658)
2022-11-09 17:47:37,065:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0110 (2.0110)
2022-11-09 17:47:37,090:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3101 (1.6510)
2022-11-09 17:47:37,114:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9590 (1.4333)
2022-11-09 17:47:37,141:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6214 (1.2149)
2022-11-09 17:47:37,166:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6729 (1.1022)
2022-11-09 17:47:37,191:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6315 (1.0269)
2022-11-09 17:47:37,215:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7990 (0.9930)
2022-11-09 17:47:37,237:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6886 (0.9568)
2022-11-09 17:47:37,493:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9220 (0.9220)
2022-11-09 17:47:37,528:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9492 (0.9359)
2022-11-09 17:47:37,553:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0073 (0.9589)
2022-11-09 17:47:37,577:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9086 (0.9459)
2022-11-09 17:47:37,601:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8425 (0.9239)
2022-11-09 17:47:37,628:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0375 (0.9435)
2022-11-09 17:47:37,652:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6388 (0.8999)
2022-11-09 17:47:37,676:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6144 (0.8625)
2022-11-09 17:47:37,700:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6741 (0.8427)
2022-11-09 17:47:37,726:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6111 (0.8194)
2022-11-09 17:47:37,752:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0931 (0.8467)
2022-11-09 17:47:37,779:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1218 (0.8705)
2022-11-09 17:47:37,803:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6248 (0.8530)
2022-11-09 17:47:37,827:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7030 (0.8420)
2022-11-09 17:47:37,852:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6399 (0.8265)
2022-11-09 17:47:37,877:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7309 (0.8206)
2022-11-09 17:47:37,901:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5876 (0.8081)
2022-11-09 17:47:37,925:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5318 (0.7942)
2022-11-09 17:47:37,974:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_119.pth.tar
2022-11-09 17:47:37,974:INFO: 
===> EPOCH: 120 (P1)
2022-11-09 17:47:37,975:INFO: - Computing loss (training)
2022-11-09 17:47:38,188:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0325 (1.0325)
2022-11-09 17:47:38,213:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7832 (0.9114)
2022-11-09 17:47:38,238:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1224 (0.9808)
2022-11-09 17:47:38,254:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5097 (0.9025)
2022-11-09 17:47:38,507:INFO: Dataset: univ                Batch:  1/15	Loss 0.7934 (0.7934)
2022-11-09 17:47:38,585:INFO: Dataset: univ                Batch:  2/15	Loss 0.7179 (0.7538)
2022-11-09 17:47:38,612:INFO: Dataset: univ                Batch:  3/15	Loss 0.7924 (0.7667)
2022-11-09 17:47:38,639:INFO: Dataset: univ                Batch:  4/15	Loss 0.6757 (0.7426)
2022-11-09 17:47:38,665:INFO: Dataset: univ                Batch:  5/15	Loss 0.7683 (0.7477)
2022-11-09 17:47:38,695:INFO: Dataset: univ                Batch:  6/15	Loss 0.5925 (0.7213)
2022-11-09 17:47:38,722:INFO: Dataset: univ                Batch:  7/15	Loss 0.7946 (0.7323)
2022-11-09 17:47:38,750:INFO: Dataset: univ                Batch:  8/15	Loss 0.5883 (0.7148)
2022-11-09 17:47:38,777:INFO: Dataset: univ                Batch:  9/15	Loss 0.6459 (0.7067)
2022-11-09 17:47:38,803:INFO: Dataset: univ                Batch: 10/15	Loss 0.5804 (0.6938)
2022-11-09 17:47:38,831:INFO: Dataset: univ                Batch: 11/15	Loss 0.5836 (0.6840)
2022-11-09 17:47:38,857:INFO: Dataset: univ                Batch: 12/15	Loss 0.7460 (0.6895)
2022-11-09 17:47:38,883:INFO: Dataset: univ                Batch: 13/15	Loss 0.5795 (0.6810)
2022-11-09 17:47:38,909:INFO: Dataset: univ                Batch: 14/15	Loss 0.5648 (0.6733)
2022-11-09 17:47:38,919:INFO: Dataset: univ                Batch: 15/15	Loss 0.1151 (0.6652)
2022-11-09 17:47:39,179:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8937 (0.8937)
2022-11-09 17:47:39,205:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9405 (0.9183)
2022-11-09 17:47:39,229:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0521 (0.9648)
2022-11-09 17:47:39,253:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7096 (0.8901)
2022-11-09 17:47:39,279:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8846 (0.8890)
2022-11-09 17:47:39,304:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6451 (0.8500)
2022-11-09 17:47:39,327:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6989 (0.8302)
2022-11-09 17:47:39,348:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5223 (0.7998)
2022-11-09 17:47:39,604:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6808 (0.6808)
2022-11-09 17:47:39,629:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7374 (0.7080)
2022-11-09 17:47:39,653:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0323 (0.8132)
2022-11-09 17:47:39,678:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9551 (0.8490)
2022-11-09 17:47:39,704:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9520 (0.8706)
2022-11-09 17:47:39,731:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2018 (0.9302)
2022-11-09 17:47:39,755:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6299 (0.8895)
2022-11-09 17:47:39,779:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5580 (0.8499)
2022-11-09 17:47:39,803:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6680 (0.8318)
2022-11-09 17:47:39,828:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6841 (0.8179)
2022-11-09 17:47:39,853:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5840 (0.7959)
2022-11-09 17:47:39,880:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8953 (0.8036)
2022-11-09 17:47:39,905:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6744 (0.7930)
2022-11-09 17:47:39,929:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5924 (0.7786)
2022-11-09 17:47:39,954:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7080 (0.7745)
2022-11-09 17:47:39,979:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8065 (0.7766)
2022-11-09 17:47:40,003:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6011 (0.7661)
2022-11-09 17:47:40,025:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7555 (0.7656)
2022-11-09 17:47:40,077:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_120.pth.tar
2022-11-09 17:47:40,077:INFO: 
===> EPOCH: 121 (P1)
2022-11-09 17:47:40,078:INFO: - Computing loss (training)
2022-11-09 17:47:40,295:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1561 (2.1561)
2022-11-09 17:47:40,321:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1546 (1.6438)
2022-11-09 17:47:40,347:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7628 (1.6832)
2022-11-09 17:47:40,364:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9467 (1.5744)
2022-11-09 17:47:40,633:INFO: Dataset: univ                Batch:  1/15	Loss 0.6744 (0.6744)
2022-11-09 17:47:40,664:INFO: Dataset: univ                Batch:  2/15	Loss 0.8458 (0.7586)
2022-11-09 17:47:40,691:INFO: Dataset: univ                Batch:  3/15	Loss 0.8448 (0.7852)
2022-11-09 17:47:40,718:INFO: Dataset: univ                Batch:  4/15	Loss 0.8542 (0.8001)
2022-11-09 17:47:40,750:INFO: Dataset: univ                Batch:  5/15	Loss 1.5925 (0.9698)
2022-11-09 17:47:40,777:INFO: Dataset: univ                Batch:  6/15	Loss 0.9910 (0.9734)
2022-11-09 17:47:40,803:INFO: Dataset: univ                Batch:  7/15	Loss 0.7081 (0.9378)
2022-11-09 17:47:40,829:INFO: Dataset: univ                Batch:  8/15	Loss 0.7694 (0.9183)
2022-11-09 17:47:40,856:INFO: Dataset: univ                Batch:  9/15	Loss 0.6341 (0.8891)
2022-11-09 17:47:40,883:INFO: Dataset: univ                Batch: 10/15	Loss 0.6101 (0.8618)
2022-11-09 17:47:40,913:INFO: Dataset: univ                Batch: 11/15	Loss 0.7202 (0.8497)
2022-11-09 17:47:40,939:INFO: Dataset: univ                Batch: 12/15	Loss 0.6230 (0.8313)
2022-11-09 17:47:40,965:INFO: Dataset: univ                Batch: 13/15	Loss 0.6226 (0.8161)
2022-11-09 17:47:40,993:INFO: Dataset: univ                Batch: 14/15	Loss 0.8778 (0.8211)
2022-11-09 17:47:41,004:INFO: Dataset: univ                Batch: 15/15	Loss 0.1527 (0.8102)
2022-11-09 17:47:41,250:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0719 (2.0719)
2022-11-09 17:47:41,275:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6484 (1.4063)
2022-11-09 17:47:41,302:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3495 (1.3877)
2022-11-09 17:47:41,326:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1108 (1.3228)
2022-11-09 17:47:41,350:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8688 (1.2384)
2022-11-09 17:47:41,374:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6334 (1.1373)
2022-11-09 17:47:41,398:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6848 (1.0689)
2022-11-09 17:47:41,419:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1911 (1.0805)
2022-11-09 17:47:41,673:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5572 (1.5572)
2022-11-09 17:47:41,703:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0948 (1.8338)
2022-11-09 17:47:41,728:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1412 (1.6007)
2022-11-09 17:47:41,752:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8351 (1.4104)
2022-11-09 17:47:41,776:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5692 (1.2410)
2022-11-09 17:47:41,803:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6599 (1.1412)
2022-11-09 17:47:41,827:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7083 (1.0800)
2022-11-09 17:47:41,851:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5843 (1.0185)
2022-11-09 17:47:41,875:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7805 (0.9918)
2022-11-09 17:47:41,899:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8468 (0.9789)
2022-11-09 17:47:41,924:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5731 (0.9431)
2022-11-09 17:47:41,951:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7802 (0.9300)
2022-11-09 17:47:41,975:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6781 (0.9102)
2022-11-09 17:47:41,999:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6620 (0.8917)
2022-11-09 17:47:42,024:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1248 (0.9059)
2022-11-09 17:47:42,049:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6742 (0.8900)
2022-11-09 17:47:42,074:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6751 (0.8782)
2022-11-09 17:47:42,097:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5471 (0.8621)
2022-11-09 17:47:42,146:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_121.pth.tar
2022-11-09 17:47:42,146:INFO: 
===> EPOCH: 122 (P1)
2022-11-09 17:47:42,147:INFO: - Computing loss (training)
2022-11-09 17:47:42,353:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1660 (1.1660)
2022-11-09 17:47:42,381:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2829 (1.2261)
2022-11-09 17:47:42,405:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7804 (1.0785)
2022-11-09 17:47:42,421:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3688 (0.9596)
2022-11-09 17:47:42,672:INFO: Dataset: univ                Batch:  1/15	Loss 0.6407 (0.6407)
2022-11-09 17:47:42,703:INFO: Dataset: univ                Batch:  2/15	Loss 0.8676 (0.7608)
2022-11-09 17:47:42,730:INFO: Dataset: univ                Batch:  3/15	Loss 0.7968 (0.7739)
2022-11-09 17:47:42,758:INFO: Dataset: univ                Batch:  4/15	Loss 0.7508 (0.7686)
2022-11-09 17:47:42,787:INFO: Dataset: univ                Batch:  5/15	Loss 0.7252 (0.7601)
2022-11-09 17:47:42,818:INFO: Dataset: univ                Batch:  6/15	Loss 0.6204 (0.7381)
2022-11-09 17:47:42,844:INFO: Dataset: univ                Batch:  7/15	Loss 0.6013 (0.7170)
2022-11-09 17:47:42,870:INFO: Dataset: univ                Batch:  8/15	Loss 0.6025 (0.7028)
2022-11-09 17:47:42,898:INFO: Dataset: univ                Batch:  9/15	Loss 0.5711 (0.6888)
2022-11-09 17:47:42,926:INFO: Dataset: univ                Batch: 10/15	Loss 0.8404 (0.7047)
2022-11-09 17:47:42,956:INFO: Dataset: univ                Batch: 11/15	Loss 0.5954 (0.6936)
2022-11-09 17:47:42,983:INFO: Dataset: univ                Batch: 12/15	Loss 0.5662 (0.6824)
2022-11-09 17:47:43,009:INFO: Dataset: univ                Batch: 13/15	Loss 0.7543 (0.6875)
2022-11-09 17:47:43,035:INFO: Dataset: univ                Batch: 14/15	Loss 0.5535 (0.6782)
2022-11-09 17:47:43,045:INFO: Dataset: univ                Batch: 15/15	Loss 0.1146 (0.6705)
2022-11-09 17:47:43,293:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7936 (1.7936)
2022-11-09 17:47:43,318:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3294 (1.5631)
2022-11-09 17:47:43,345:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7514 (1.2850)
2022-11-09 17:47:43,369:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7728 (1.1526)
2022-11-09 17:47:43,392:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6001 (1.0346)
2022-11-09 17:47:43,417:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7179 (0.9821)
2022-11-09 17:47:43,441:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2773 (1.0339)
2022-11-09 17:47:43,462:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7869 (1.0074)
2022-11-09 17:47:43,718:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2814 (1.2814)
2022-11-09 17:47:43,745:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6310 (1.4629)
2022-11-09 17:47:43,773:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8520 (1.2549)
2022-11-09 17:47:43,798:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6776 (1.1268)
2022-11-09 17:47:43,824:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6569 (1.0320)
2022-11-09 17:47:43,849:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6299 (0.9678)
2022-11-09 17:47:43,873:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6485 (0.9218)
2022-11-09 17:47:43,897:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9785 (0.9287)
2022-11-09 17:47:43,920:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7932 (0.9129)
2022-11-09 17:47:43,946:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0604 (0.9266)
2022-11-09 17:47:43,973:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7045 (0.9073)
2022-11-09 17:47:44,000:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0483 (0.9176)
2022-11-09 17:47:44,024:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5577 (0.9700)
2022-11-09 17:47:44,047:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6822 (0.9494)
2022-11-09 17:47:44,072:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5498 (0.9235)
2022-11-09 17:47:44,096:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6457 (0.9063)
2022-11-09 17:47:44,121:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7951 (0.8992)
2022-11-09 17:47:44,145:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5113 (0.8801)
2022-11-09 17:47:44,197:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_122.pth.tar
2022-11-09 17:47:44,197:INFO: 
===> EPOCH: 123 (P1)
2022-11-09 17:47:44,197:INFO: - Computing loss (training)
2022-11-09 17:47:44,409:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9284 (0.9284)
2022-11-09 17:47:44,434:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5315 (2.2054)
2022-11-09 17:47:44,458:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9972 (2.1371)
2022-11-09 17:47:44,476:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3299 (2.0018)
2022-11-09 17:47:44,738:INFO: Dataset: univ                Batch:  1/15	Loss 0.5645 (0.5645)
2022-11-09 17:47:44,768:INFO: Dataset: univ                Batch:  2/15	Loss 1.3929 (0.9939)
2022-11-09 17:47:44,795:INFO: Dataset: univ                Batch:  3/15	Loss 1.1114 (1.0338)
2022-11-09 17:47:44,824:INFO: Dataset: univ                Batch:  4/15	Loss 0.7936 (0.9734)
2022-11-09 17:47:44,853:INFO: Dataset: univ                Batch:  5/15	Loss 1.3996 (1.0623)
2022-11-09 17:47:44,884:INFO: Dataset: univ                Batch:  6/15	Loss 1.7759 (1.1829)
2022-11-09 17:47:44,910:INFO: Dataset: univ                Batch:  7/15	Loss 0.6035 (1.1018)
2022-11-09 17:47:44,937:INFO: Dataset: univ                Batch:  8/15	Loss 0.8695 (1.0715)
2022-11-09 17:47:44,965:INFO: Dataset: univ                Batch:  9/15	Loss 1.3866 (1.1067)
2022-11-09 17:47:44,992:INFO: Dataset: univ                Batch: 10/15	Loss 0.5735 (1.0583)
2022-11-09 17:47:45,020:INFO: Dataset: univ                Batch: 11/15	Loss 0.6315 (1.0137)
2022-11-09 17:47:45,048:INFO: Dataset: univ                Batch: 12/15	Loss 0.5461 (0.9752)
2022-11-09 17:47:45,074:INFO: Dataset: univ                Batch: 13/15	Loss 0.7283 (0.9555)
2022-11-09 17:47:45,101:INFO: Dataset: univ                Batch: 14/15	Loss 0.6977 (0.9391)
2022-11-09 17:47:45,110:INFO: Dataset: univ                Batch: 15/15	Loss 0.1076 (0.9276)
2022-11-09 17:47:45,366:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8280 (1.8280)
2022-11-09 17:47:45,391:INFO: Dataset: zara1               Batch: 2/8	Loss 3.1339 (2.4632)
2022-11-09 17:47:45,418:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8383 (2.2677)
2022-11-09 17:47:45,442:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7289 (1.8765)
2022-11-09 17:47:45,466:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6448 (1.6292)
2022-11-09 17:47:45,492:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6369 (1.4732)
2022-11-09 17:47:45,516:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8024 (1.3855)
2022-11-09 17:47:45,537:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7081 (1.3188)
2022-11-09 17:47:45,793:INFO: Dataset: zara2               Batch:  1/18	Loss 2.0844 (2.0844)
2022-11-09 17:47:45,820:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9604 (1.5444)
2022-11-09 17:47:45,845:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6331 (1.2470)
2022-11-09 17:47:45,870:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8602 (1.1403)
2022-11-09 17:47:45,899:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8982 (1.0973)
2022-11-09 17:47:45,925:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6890 (1.0354)
2022-11-09 17:47:45,950:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5808 (0.9744)
2022-11-09 17:47:45,974:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6390 (0.9335)
2022-11-09 17:47:45,999:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6468 (0.8995)
2022-11-09 17:47:46,024:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6328 (0.8735)
2022-11-09 17:47:46,052:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7435 (0.8630)
2022-11-09 17:47:46,078:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8140 (0.8590)
2022-11-09 17:47:46,102:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8401 (0.8576)
2022-11-09 17:47:46,127:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6189 (0.8404)
2022-11-09 17:47:46,152:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7271 (0.8324)
2022-11-09 17:47:46,178:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7875 (0.8299)
2022-11-09 17:47:46,203:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7653 (0.8264)
2022-11-09 17:47:46,227:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5256 (0.8122)
2022-11-09 17:47:46,277:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_123.pth.tar
2022-11-09 17:47:46,277:INFO: 
===> EPOCH: 124 (P1)
2022-11-09 17:47:46,278:INFO: - Computing loss (training)
2022-11-09 17:47:46,496:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6028 (1.6028)
2022-11-09 17:47:46,522:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9648 (1.2884)
2022-11-09 17:47:46,547:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7192 (1.0915)
2022-11-09 17:47:46,564:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4438 (0.9873)
2022-11-09 17:47:46,817:INFO: Dataset: univ                Batch:  1/15	Loss 0.6558 (0.6558)
2022-11-09 17:47:46,844:INFO: Dataset: univ                Batch:  2/15	Loss 0.5734 (0.6144)
2022-11-09 17:47:46,874:INFO: Dataset: univ                Batch:  3/15	Loss 0.5671 (0.5987)
2022-11-09 17:47:46,902:INFO: Dataset: univ                Batch:  4/15	Loss 0.6618 (0.6141)
2022-11-09 17:47:46,929:INFO: Dataset: univ                Batch:  5/15	Loss 0.5899 (0.6090)
2022-11-09 17:47:46,960:INFO: Dataset: univ                Batch:  6/15	Loss 0.6265 (0.6119)
2022-11-09 17:47:46,986:INFO: Dataset: univ                Batch:  7/15	Loss 0.5660 (0.6055)
2022-11-09 17:47:47,013:INFO: Dataset: univ                Batch:  8/15	Loss 0.6229 (0.6078)
2022-11-09 17:47:47,041:INFO: Dataset: univ                Batch:  9/15	Loss 0.6082 (0.6078)
2022-11-09 17:47:47,069:INFO: Dataset: univ                Batch: 10/15	Loss 0.5673 (0.6038)
2022-11-09 17:47:47,097:INFO: Dataset: univ                Batch: 11/15	Loss 0.5809 (0.6017)
2022-11-09 17:47:47,125:INFO: Dataset: univ                Batch: 12/15	Loss 0.7526 (0.6126)
2022-11-09 17:47:47,152:INFO: Dataset: univ                Batch: 13/15	Loss 0.6308 (0.6139)
2022-11-09 17:47:47,178:INFO: Dataset: univ                Batch: 14/15	Loss 0.6267 (0.6147)
2022-11-09 17:47:47,189:INFO: Dataset: univ                Batch: 15/15	Loss 0.1213 (0.6082)
2022-11-09 17:47:47,448:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1081 (1.1081)
2022-11-09 17:47:47,474:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9157 (1.0061)
2022-11-09 17:47:47,499:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2049 (1.0693)
2022-11-09 17:47:47,523:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6666 (0.9686)
2022-11-09 17:47:47,549:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8487 (0.9433)
2022-11-09 17:47:47,574:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4151 (1.0214)
2022-11-09 17:47:47,597:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8842 (1.1556)
2022-11-09 17:47:47,618:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5845 (1.0946)
2022-11-09 17:47:47,875:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8065 (0.8065)
2022-11-09 17:47:47,901:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4941 (1.1555)
2022-11-09 17:47:47,928:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6000 (0.9823)
2022-11-09 17:47:47,953:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0601 (1.0009)
2022-11-09 17:47:47,979:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5871 (0.9230)
2022-11-09 17:47:48,006:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6334 (0.8773)
2022-11-09 17:47:48,029:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6462 (0.8452)
2022-11-09 17:47:48,053:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9002 (0.8514)
2022-11-09 17:47:48,077:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5797 (0.8189)
2022-11-09 17:47:48,101:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1024 (0.8463)
2022-11-09 17:47:48,126:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6528 (0.8285)
2022-11-09 17:47:48,152:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6582 (0.8148)
2022-11-09 17:47:48,176:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6513 (0.8019)
2022-11-09 17:47:48,200:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6143 (0.7876)
2022-11-09 17:47:48,225:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6285 (0.7769)
2022-11-09 17:47:48,250:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5982 (0.7663)
2022-11-09 17:47:48,275:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5840 (0.7555)
2022-11-09 17:47:48,298:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6606 (0.7508)
2022-11-09 17:47:48,349:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_124.pth.tar
2022-11-09 17:47:48,349:INFO: 
===> EPOCH: 125 (P1)
2022-11-09 17:47:48,349:INFO: - Computing loss (training)
2022-11-09 17:47:48,549:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9838 (1.9838)
2022-11-09 17:47:48,575:INFO: Dataset: hotel               Batch: 2/4	Loss 4.2791 (3.1583)
2022-11-09 17:47:48,601:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3963 (2.6010)
2022-11-09 17:47:48,618:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4459 (2.2257)
2022-11-09 17:47:48,875:INFO: Dataset: univ                Batch:  1/15	Loss 0.6260 (0.6260)
2022-11-09 17:47:48,902:INFO: Dataset: univ                Batch:  2/15	Loss 0.6875 (0.6576)
2022-11-09 17:47:48,932:INFO: Dataset: univ                Batch:  3/15	Loss 1.0560 (0.7864)
2022-11-09 17:47:48,958:INFO: Dataset: univ                Batch:  4/15	Loss 0.7466 (0.7762)
2022-11-09 17:47:48,984:INFO: Dataset: univ                Batch:  5/15	Loss 0.9513 (0.8096)
2022-11-09 17:47:49,013:INFO: Dataset: univ                Batch:  6/15	Loss 1.0508 (0.8522)
2022-11-09 17:47:49,039:INFO: Dataset: univ                Batch:  7/15	Loss 0.8181 (0.8472)
2022-11-09 17:47:49,065:INFO: Dataset: univ                Batch:  8/15	Loss 0.6636 (0.8271)
2022-11-09 17:47:49,092:INFO: Dataset: univ                Batch:  9/15	Loss 0.7851 (0.8224)
2022-11-09 17:47:49,119:INFO: Dataset: univ                Batch: 10/15	Loss 0.7523 (0.8148)
2022-11-09 17:47:49,149:INFO: Dataset: univ                Batch: 11/15	Loss 0.8158 (0.8149)
2022-11-09 17:47:49,175:INFO: Dataset: univ                Batch: 12/15	Loss 0.5932 (0.7956)
2022-11-09 17:47:49,202:INFO: Dataset: univ                Batch: 13/15	Loss 0.5461 (0.7754)
2022-11-09 17:47:49,228:INFO: Dataset: univ                Batch: 14/15	Loss 0.5869 (0.7610)
2022-11-09 17:47:49,238:INFO: Dataset: univ                Batch: 15/15	Loss 0.1153 (0.7522)
2022-11-09 17:47:49,494:INFO: Dataset: zara1               Batch: 1/8	Loss 2.8025 (2.8025)
2022-11-09 17:47:49,518:INFO: Dataset: zara1               Batch: 2/8	Loss 3.4086 (3.1380)
2022-11-09 17:47:49,542:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2772 (2.5213)
2022-11-09 17:47:49,566:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2916 (2.1617)
2022-11-09 17:47:49,590:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6589 (1.8713)
2022-11-09 17:47:49,616:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6732 (1.6704)
2022-11-09 17:47:49,640:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6846 (1.5283)
2022-11-09 17:47:49,661:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0807 (1.4845)
2022-11-09 17:47:49,932:INFO: Dataset: zara2               Batch:  1/18	Loss 2.0061 (2.0061)
2022-11-09 17:47:49,958:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9435 (1.9776)
2022-11-09 17:47:49,984:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8214 (1.9246)
2022-11-09 17:47:50,008:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7222 (1.8717)
2022-11-09 17:47:50,032:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8409 (1.6664)
2022-11-09 17:47:50,062:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0368 (1.5662)
2022-11-09 17:47:50,086:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5580 (1.4336)
2022-11-09 17:47:50,109:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9485 (1.3762)
2022-11-09 17:47:50,133:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6596 (1.2984)
2022-11-09 17:47:50,157:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0482 (1.2750)
2022-11-09 17:47:50,182:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7939 (1.2322)
2022-11-09 17:47:50,207:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6996 (1.1907)
2022-11-09 17:47:50,232:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7633 (1.1593)
2022-11-09 17:47:50,255:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7087 (1.1243)
2022-11-09 17:47:50,280:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8343 (1.1057)
2022-11-09 17:47:50,305:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6954 (1.0765)
2022-11-09 17:47:50,330:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7231 (1.0513)
2022-11-09 17:47:50,352:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4997 (1.0268)
2022-11-09 17:47:50,403:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_125.pth.tar
2022-11-09 17:47:50,403:INFO: 
===> EPOCH: 126 (P1)
2022-11-09 17:47:50,403:INFO: - Computing loss (training)
2022-11-09 17:47:50,620:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3742 (1.3742)
2022-11-09 17:47:50,645:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2065 (1.2892)
2022-11-09 17:47:50,669:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7796 (1.1150)
2022-11-09 17:47:50,687:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5840 (1.0288)
2022-11-09 17:47:50,948:INFO: Dataset: univ                Batch:  1/15	Loss 0.6816 (0.6816)
2022-11-09 17:47:50,978:INFO: Dataset: univ                Batch:  2/15	Loss 0.8342 (0.7642)
2022-11-09 17:47:51,006:INFO: Dataset: univ                Batch:  3/15	Loss 0.6025 (0.7064)
2022-11-09 17:47:51,033:INFO: Dataset: univ                Batch:  4/15	Loss 1.2606 (0.8468)
2022-11-09 17:47:51,063:INFO: Dataset: univ                Batch:  5/15	Loss 0.5751 (0.7953)
2022-11-09 17:47:51,092:INFO: Dataset: univ                Batch:  6/15	Loss 0.6231 (0.7651)
2022-11-09 17:47:51,118:INFO: Dataset: univ                Batch:  7/15	Loss 0.6612 (0.7502)
2022-11-09 17:47:51,143:INFO: Dataset: univ                Batch:  8/15	Loss 0.6063 (0.7336)
2022-11-09 17:47:51,169:INFO: Dataset: univ                Batch:  9/15	Loss 0.5723 (0.7155)
2022-11-09 17:47:51,196:INFO: Dataset: univ                Batch: 10/15	Loss 0.8842 (0.7317)
2022-11-09 17:47:51,223:INFO: Dataset: univ                Batch: 11/15	Loss 1.7726 (0.8117)
2022-11-09 17:47:51,250:INFO: Dataset: univ                Batch: 12/15	Loss 0.6394 (0.7968)
2022-11-09 17:47:51,276:INFO: Dataset: univ                Batch: 13/15	Loss 0.6918 (0.7888)
2022-11-09 17:47:51,301:INFO: Dataset: univ                Batch: 14/15	Loss 0.7016 (0.7825)
2022-11-09 17:47:51,311:INFO: Dataset: univ                Batch: 15/15	Loss 0.1048 (0.7717)
2022-11-09 17:47:51,552:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8071 (0.8071)
2022-11-09 17:47:51,579:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6271 (0.7248)
2022-11-09 17:47:51,602:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6424 (0.7000)
2022-11-09 17:47:51,630:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8444 (0.7387)
2022-11-09 17:47:51,654:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9018 (0.7683)
2022-11-09 17:47:51,679:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7070 (0.7574)
2022-11-09 17:47:51,702:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7852 (0.7612)
2022-11-09 17:47:51,723:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4920 (0.7265)
2022-11-09 17:47:51,999:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6366 (0.6366)
2022-11-09 17:47:52,026:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7429 (0.6898)
2022-11-09 17:47:52,051:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8947 (0.7594)
2022-11-09 17:47:52,075:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6816 (0.7386)
2022-11-09 17:47:52,103:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6406 (0.7187)
2022-11-09 17:47:52,130:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6323 (0.7019)
2022-11-09 17:47:52,155:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6396 (0.6921)
2022-11-09 17:47:52,179:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6038 (0.6811)
2022-11-09 17:47:52,203:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6058 (0.6739)
2022-11-09 17:47:52,229:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5836 (0.6658)
2022-11-09 17:47:52,256:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9226 (0.6871)
2022-11-09 17:47:52,281:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6106 (0.6810)
2022-11-09 17:47:52,305:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5709 (0.6723)
2022-11-09 17:47:52,329:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6321 (0.6696)
2022-11-09 17:47:52,354:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5332 (0.6606)
2022-11-09 17:47:52,378:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5847 (0.6564)
2022-11-09 17:47:52,403:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5808 (0.6520)
2022-11-09 17:47:52,425:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5918 (0.6488)
2022-11-09 17:47:52,474:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_126.pth.tar
2022-11-09 17:47:52,474:INFO: 
===> EPOCH: 127 (P1)
2022-11-09 17:47:52,475:INFO: - Computing loss (training)
2022-11-09 17:47:52,677:INFO: Dataset: hotel               Batch: 1/4	Loss 5.3212 (5.3212)
2022-11-09 17:47:52,702:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5047 (4.4239)
2022-11-09 17:47:52,727:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3293 (3.3615)
2022-11-09 17:47:52,744:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1798 (3.0075)
2022-11-09 17:47:53,056:INFO: Dataset: univ                Batch:  1/15	Loss 0.8811 (0.8811)
2022-11-09 17:47:53,087:INFO: Dataset: univ                Batch:  2/15	Loss 0.8354 (0.8581)
2022-11-09 17:47:53,113:INFO: Dataset: univ                Batch:  3/15	Loss 0.6585 (0.7901)
2022-11-09 17:47:53,139:INFO: Dataset: univ                Batch:  4/15	Loss 1.0764 (0.8577)
2022-11-09 17:47:53,165:INFO: Dataset: univ                Batch:  5/15	Loss 0.9649 (0.8794)
2022-11-09 17:47:53,197:INFO: Dataset: univ                Batch:  6/15	Loss 1.5178 (1.0012)
2022-11-09 17:47:53,223:INFO: Dataset: univ                Batch:  7/15	Loss 0.5797 (0.9418)
2022-11-09 17:47:53,249:INFO: Dataset: univ                Batch:  8/15	Loss 0.6008 (0.9022)
2022-11-09 17:47:53,277:INFO: Dataset: univ                Batch:  9/15	Loss 0.7866 (0.8883)
2022-11-09 17:47:53,304:INFO: Dataset: univ                Batch: 10/15	Loss 0.6203 (0.8624)
2022-11-09 17:47:53,331:INFO: Dataset: univ                Batch: 11/15	Loss 0.6725 (0.8458)
2022-11-09 17:47:53,359:INFO: Dataset: univ                Batch: 12/15	Loss 1.2197 (0.8780)
2022-11-09 17:47:53,387:INFO: Dataset: univ                Batch: 13/15	Loss 0.5768 (0.8562)
2022-11-09 17:47:53,414:INFO: Dataset: univ                Batch: 14/15	Loss 0.6065 (0.8384)
2022-11-09 17:47:53,425:INFO: Dataset: univ                Batch: 15/15	Loss 0.1069 (0.8267)
2022-11-09 17:47:53,673:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3941 (2.3941)
2022-11-09 17:47:53,698:INFO: Dataset: zara1               Batch: 2/8	Loss 3.4373 (2.8643)
2022-11-09 17:47:53,722:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0148 (2.2829)
2022-11-09 17:47:53,746:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9064 (1.9558)
2022-11-09 17:47:53,770:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7385 (1.7270)
2022-11-09 17:47:53,797:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6772 (1.5506)
2022-11-09 17:47:53,820:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6396 (1.4320)
2022-11-09 17:47:53,842:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0683 (1.3905)
2022-11-09 17:47:54,092:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1469 (1.1469)
2022-11-09 17:47:54,122:INFO: Dataset: zara2               Batch:  2/18	Loss 2.1667 (1.6274)
2022-11-09 17:47:54,146:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3726 (1.5442)
2022-11-09 17:47:54,171:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9007 (1.3770)
2022-11-09 17:47:54,196:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6604 (1.2244)
2022-11-09 17:47:54,224:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0070 (1.1894)
2022-11-09 17:47:54,248:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6204 (1.1038)
2022-11-09 17:47:54,272:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5616 (1.0325)
2022-11-09 17:47:54,296:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0440 (1.0338)
2022-11-09 17:47:54,321:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8391 (1.0119)
2022-11-09 17:47:54,346:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0014 (1.0110)
2022-11-09 17:47:54,372:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8561 (0.9972)
2022-11-09 17:47:54,397:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8520 (0.9867)
2022-11-09 17:47:54,421:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7170 (0.9674)
2022-11-09 17:47:54,445:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6060 (0.9418)
2022-11-09 17:47:54,470:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5875 (0.9184)
2022-11-09 17:47:54,495:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7007 (0.9064)
2022-11-09 17:47:54,519:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5281 (0.8863)
2022-11-09 17:47:54,568:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_127.pth.tar
2022-11-09 17:47:54,569:INFO: 
===> EPOCH: 128 (P1)
2022-11-09 17:47:54,569:INFO: - Computing loss (training)
2022-11-09 17:47:54,782:INFO: Dataset: hotel               Batch: 1/4	Loss 3.1457 (3.1457)
2022-11-09 17:47:54,808:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9292 (2.5433)
2022-11-09 17:47:54,832:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2770 (2.1078)
2022-11-09 17:47:54,848:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5132 (1.8343)
2022-11-09 17:47:55,109:INFO: Dataset: univ                Batch:  1/15	Loss 0.5949 (0.5949)
2022-11-09 17:47:55,139:INFO: Dataset: univ                Batch:  2/15	Loss 0.5550 (0.5768)
2022-11-09 17:47:55,172:INFO: Dataset: univ                Batch:  3/15	Loss 0.5705 (0.5749)
2022-11-09 17:47:55,200:INFO: Dataset: univ                Batch:  4/15	Loss 0.7953 (0.6331)
2022-11-09 17:47:55,228:INFO: Dataset: univ                Batch:  5/15	Loss 0.5788 (0.6225)
2022-11-09 17:47:55,259:INFO: Dataset: univ                Batch:  6/15	Loss 0.5702 (0.6133)
2022-11-09 17:47:55,286:INFO: Dataset: univ                Batch:  7/15	Loss 0.5431 (0.6032)
2022-11-09 17:47:55,314:INFO: Dataset: univ                Batch:  8/15	Loss 0.9109 (0.6392)
2022-11-09 17:47:55,343:INFO: Dataset: univ                Batch:  9/15	Loss 0.5637 (0.6304)
2022-11-09 17:47:55,372:INFO: Dataset: univ                Batch: 10/15	Loss 0.6689 (0.6349)
2022-11-09 17:47:55,400:INFO: Dataset: univ                Batch: 11/15	Loss 0.6474 (0.6361)
2022-11-09 17:47:55,429:INFO: Dataset: univ                Batch: 12/15	Loss 0.6646 (0.6385)
2022-11-09 17:47:55,457:INFO: Dataset: univ                Batch: 13/15	Loss 0.5534 (0.6314)
2022-11-09 17:47:55,483:INFO: Dataset: univ                Batch: 14/15	Loss 0.6153 (0.6302)
2022-11-09 17:47:55,494:INFO: Dataset: univ                Batch: 15/15	Loss 0.1299 (0.6217)
2022-11-09 17:47:55,749:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7490 (0.7490)
2022-11-09 17:47:55,776:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8154 (0.7832)
2022-11-09 17:47:55,802:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7920 (0.7860)
2022-11-09 17:47:55,826:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7219 (0.7706)
2022-11-09 17:47:55,851:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9667 (0.8085)
2022-11-09 17:47:55,877:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6924 (0.7886)
2022-11-09 17:47:55,900:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6253 (0.7674)
2022-11-09 17:47:55,921:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7017 (0.7605)
2022-11-09 17:47:56,177:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7207 (0.7207)
2022-11-09 17:47:56,202:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2315 (0.9740)
2022-11-09 17:47:56,231:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7529 (0.8925)
2022-11-09 17:47:56,256:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0601 (0.9397)
2022-11-09 17:47:56,280:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5469 (0.8576)
2022-11-09 17:47:56,307:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6698 (0.8246)
2022-11-09 17:47:56,331:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6246 (0.7944)
2022-11-09 17:47:56,355:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5953 (0.7711)
2022-11-09 17:47:56,379:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6267 (0.7549)
2022-11-09 17:47:56,403:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5128 (0.7324)
2022-11-09 17:47:56,428:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1929 (0.7711)
2022-11-09 17:47:56,455:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3092 (0.8161)
2022-11-09 17:47:56,479:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6057 (0.8000)
2022-11-09 17:47:56,503:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7562 (0.7968)
2022-11-09 17:47:56,528:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5514 (0.7801)
2022-11-09 17:47:56,552:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9955 (0.7940)
2022-11-09 17:47:56,577:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5624 (0.7810)
2022-11-09 17:47:56,600:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6664 (0.7750)
2022-11-09 17:47:56,653:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_128.pth.tar
2022-11-09 17:47:56,653:INFO: 
===> EPOCH: 129 (P1)
2022-11-09 17:47:56,653:INFO: - Computing loss (training)
2022-11-09 17:47:56,857:INFO: Dataset: hotel               Batch: 1/4	Loss 8.4530 (8.4530)
2022-11-09 17:47:56,884:INFO: Dataset: hotel               Batch: 2/4	Loss 3.8020 (6.0532)
2022-11-09 17:47:56,908:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7595 (5.0567)
2022-11-09 17:47:56,926:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6106 (4.2942)
2022-11-09 17:47:57,175:INFO: Dataset: univ                Batch:  1/15	Loss 2.0464 (2.0464)
2022-11-09 17:47:57,203:INFO: Dataset: univ                Batch:  2/15	Loss 2.9906 (2.5061)
2022-11-09 17:47:57,234:INFO: Dataset: univ                Batch:  3/15	Loss 3.0977 (2.6979)
2022-11-09 17:47:57,261:INFO: Dataset: univ                Batch:  4/15	Loss 1.9265 (2.5204)
2022-11-09 17:47:57,289:INFO: Dataset: univ                Batch:  5/15	Loss 1.4488 (2.3062)
2022-11-09 17:47:57,318:INFO: Dataset: univ                Batch:  6/15	Loss 1.5130 (2.1794)
2022-11-09 17:47:57,344:INFO: Dataset: univ                Batch:  7/15	Loss 0.6516 (1.9632)
2022-11-09 17:47:57,370:INFO: Dataset: univ                Batch:  8/15	Loss 0.6416 (1.7991)
2022-11-09 17:47:57,397:INFO: Dataset: univ                Batch:  9/15	Loss 0.9787 (1.7076)
2022-11-09 17:47:57,425:INFO: Dataset: univ                Batch: 10/15	Loss 0.5832 (1.5948)
2022-11-09 17:47:57,456:INFO: Dataset: univ                Batch: 11/15	Loss 0.9188 (1.5323)
2022-11-09 17:47:57,483:INFO: Dataset: univ                Batch: 12/15	Loss 0.9735 (1.4849)
2022-11-09 17:47:57,509:INFO: Dataset: univ                Batch: 13/15	Loss 0.6937 (1.4209)
2022-11-09 17:47:57,536:INFO: Dataset: univ                Batch: 14/15	Loss 0.8312 (1.3763)
2022-11-09 17:47:57,547:INFO: Dataset: univ                Batch: 15/15	Loss 0.1655 (1.3609)
2022-11-09 17:47:57,787:INFO: Dataset: zara1               Batch: 1/8	Loss 2.9022 (2.9022)
2022-11-09 17:47:57,812:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0830 (2.4901)
2022-11-09 17:47:57,839:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8292 (2.2800)
2022-11-09 17:47:57,864:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9661 (1.9567)
2022-11-09 17:47:57,888:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1646 (1.8036)
2022-11-09 17:47:57,913:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1802 (1.6828)
2022-11-09 17:47:57,937:INFO: Dataset: zara1               Batch: 7/8	Loss 3.3052 (1.9222)
2022-11-09 17:47:57,958:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4413 (1.8736)
2022-11-09 17:47:58,210:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4227 (1.4227)
2022-11-09 17:47:58,236:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6948 (1.0493)
2022-11-09 17:47:58,265:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6725 (0.9200)
2022-11-09 17:47:58,290:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5444 (0.8262)
2022-11-09 17:47:58,314:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6276 (0.7880)
2022-11-09 17:47:58,341:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7207 (0.7762)
2022-11-09 17:47:58,365:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6218 (0.7533)
2022-11-09 17:47:58,389:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8174 (0.7612)
2022-11-09 17:47:58,413:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0167 (0.7903)
2022-11-09 17:47:58,438:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7539 (0.7862)
2022-11-09 17:47:58,464:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9250 (0.7992)
2022-11-09 17:47:58,490:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6529 (0.8639)
2022-11-09 17:47:58,515:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6230 (0.8450)
2022-11-09 17:47:58,539:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6734 (0.8309)
2022-11-09 17:47:58,564:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1692 (0.8534)
2022-11-09 17:47:58,589:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8146 (0.8510)
2022-11-09 17:47:58,614:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6427 (0.8399)
2022-11-09 17:47:58,637:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5163 (0.8262)
2022-11-09 17:47:58,687:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_129.pth.tar
2022-11-09 17:47:58,687:INFO: 
===> EPOCH: 130 (P1)
2022-11-09 17:47:58,688:INFO: - Computing loss (training)
2022-11-09 17:47:58,887:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9319 (0.9319)
2022-11-09 17:47:58,912:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0880 (1.0087)
2022-11-09 17:47:58,938:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6597 (0.8929)
2022-11-09 17:47:58,955:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4730 (0.8203)
2022-11-09 17:47:59,220:INFO: Dataset: univ                Batch:  1/15	Loss 0.6315 (0.6315)
2022-11-09 17:47:59,254:INFO: Dataset: univ                Batch:  2/15	Loss 0.5873 (0.6080)
2022-11-09 17:47:59,281:INFO: Dataset: univ                Batch:  3/15	Loss 0.6633 (0.6266)
2022-11-09 17:47:59,307:INFO: Dataset: univ                Batch:  4/15	Loss 0.8538 (0.6786)
2022-11-09 17:47:59,335:INFO: Dataset: univ                Batch:  5/15	Loss 0.5441 (0.6497)
2022-11-09 17:47:59,364:INFO: Dataset: univ                Batch:  6/15	Loss 0.5561 (0.6347)
2022-11-09 17:47:59,390:INFO: Dataset: univ                Batch:  7/15	Loss 0.5809 (0.6274)
2022-11-09 17:47:59,416:INFO: Dataset: univ                Batch:  8/15	Loss 0.6812 (0.6341)
2022-11-09 17:47:59,444:INFO: Dataset: univ                Batch:  9/15	Loss 0.5868 (0.6285)
2022-11-09 17:47:59,472:INFO: Dataset: univ                Batch: 10/15	Loss 0.5415 (0.6194)
2022-11-09 17:47:59,501:INFO: Dataset: univ                Batch: 11/15	Loss 0.5458 (0.6133)
2022-11-09 17:47:59,529:INFO: Dataset: univ                Batch: 12/15	Loss 0.6413 (0.6156)
2022-11-09 17:47:59,556:INFO: Dataset: univ                Batch: 13/15	Loss 0.5542 (0.6110)
2022-11-09 17:47:59,582:INFO: Dataset: univ                Batch: 14/15	Loss 0.7155 (0.6194)
2022-11-09 17:47:59,593:INFO: Dataset: univ                Batch: 15/15	Loss 0.1089 (0.6124)
2022-11-09 17:47:59,836:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6157 (0.6157)
2022-11-09 17:47:59,864:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6659 (0.6423)
2022-11-09 17:47:59,888:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8153 (0.7023)
2022-11-09 17:47:59,912:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9436 (0.7585)
2022-11-09 17:47:59,936:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5677 (0.7225)
2022-11-09 17:47:59,961:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6661 (0.7113)
2022-11-09 17:47:59,985:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5620 (0.6896)
2022-11-09 17:48:00,006:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5228 (0.6727)
2022-11-09 17:48:00,271:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1529 (1.1529)
2022-11-09 17:48:00,300:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8038 (0.9895)
2022-11-09 17:48:00,327:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8554 (0.9495)
2022-11-09 17:48:00,352:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6385 (0.8708)
2022-11-09 17:48:00,376:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5779 (0.8081)
2022-11-09 17:48:00,404:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7447 (0.7980)
2022-11-09 17:48:00,428:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5537 (0.7616)
2022-11-09 17:48:00,452:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5355 (0.7361)
2022-11-09 17:48:00,476:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7659 (0.7393)
2022-11-09 17:48:00,501:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5468 (0.7192)
2022-11-09 17:48:00,526:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5062 (0.7000)
2022-11-09 17:48:00,552:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5541 (0.6871)
2022-11-09 17:48:00,577:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5566 (0.6779)
2022-11-09 17:48:00,601:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5790 (0.6705)
2022-11-09 17:48:00,626:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5049 (0.6600)
2022-11-09 17:48:00,651:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6592 (0.6599)
2022-11-09 17:48:00,676:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5315 (0.6516)
2022-11-09 17:48:00,699:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7926 (0.6577)
2022-11-09 17:48:00,749:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_130.pth.tar
2022-11-09 17:48:00,749:INFO: 
===> EPOCH: 131 (P1)
2022-11-09 17:48:00,749:INFO: - Computing loss (training)
2022-11-09 17:48:00,962:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1640 (2.1640)
2022-11-09 17:48:00,987:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0627 (2.1156)
2022-11-09 17:48:01,013:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7569 (1.6563)
2022-11-09 17:48:01,029:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4678 (1.4650)
2022-11-09 17:48:01,286:INFO: Dataset: univ                Batch:  1/15	Loss 0.5455 (0.5455)
2022-11-09 17:48:01,349:INFO: Dataset: univ                Batch:  2/15	Loss 0.5713 (0.5584)
2022-11-09 17:48:01,375:INFO: Dataset: univ                Batch:  3/15	Loss 1.1444 (0.7710)
2022-11-09 17:48:01,402:INFO: Dataset: univ                Batch:  4/15	Loss 0.8497 (0.7916)
2022-11-09 17:48:01,428:INFO: Dataset: univ                Batch:  5/15	Loss 0.6319 (0.7643)
2022-11-09 17:48:01,457:INFO: Dataset: univ                Batch:  6/15	Loss 0.6352 (0.7442)
2022-11-09 17:48:01,483:INFO: Dataset: univ                Batch:  7/15	Loss 0.8074 (0.7527)
2022-11-09 17:48:01,511:INFO: Dataset: univ                Batch:  8/15	Loss 1.0990 (0.7938)
2022-11-09 17:48:01,539:INFO: Dataset: univ                Batch:  9/15	Loss 0.5819 (0.7721)
2022-11-09 17:48:01,567:INFO: Dataset: univ                Batch: 10/15	Loss 0.6410 (0.7581)
2022-11-09 17:48:01,596:INFO: Dataset: univ                Batch: 11/15	Loss 0.6350 (0.7478)
2022-11-09 17:48:01,622:INFO: Dataset: univ                Batch: 12/15	Loss 0.5308 (0.7294)
2022-11-09 17:48:01,649:INFO: Dataset: univ                Batch: 13/15	Loss 0.6474 (0.7225)
2022-11-09 17:48:01,677:INFO: Dataset: univ                Batch: 14/15	Loss 0.6583 (0.7176)
2022-11-09 17:48:01,688:INFO: Dataset: univ                Batch: 15/15	Loss 0.1315 (0.7105)
2022-11-09 17:48:01,939:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4314 (1.4314)
2022-11-09 17:48:01,965:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4952 (1.4618)
2022-11-09 17:48:01,992:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8954 (1.6081)
2022-11-09 17:48:02,018:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8701 (1.4197)
2022-11-09 17:48:02,042:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7050 (1.2884)
2022-11-09 17:48:02,067:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5814 (1.1705)
2022-11-09 17:48:02,091:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5803 (1.0943)
2022-11-09 17:48:02,112:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0367 (1.0874)
2022-11-09 17:48:02,476:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1823 (1.1823)
2022-11-09 17:48:02,503:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6749 (1.4233)
2022-11-09 17:48:02,527:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5854 (1.4778)
2022-11-09 17:48:02,553:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0362 (1.3734)
2022-11-09 17:48:02,577:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6853 (1.2373)
2022-11-09 17:48:02,603:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5944 (1.1343)
2022-11-09 17:48:02,626:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6069 (1.0599)
2022-11-09 17:48:02,650:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6622 (1.0076)
2022-11-09 17:48:02,674:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9539 (1.0019)
2022-11-09 17:48:02,699:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6784 (0.9718)
2022-11-09 17:48:02,725:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7114 (0.9490)
2022-11-09 17:48:02,751:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9796 (0.9516)
2022-11-09 17:48:02,777:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6888 (0.9302)
2022-11-09 17:48:02,802:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6965 (0.9099)
2022-11-09 17:48:02,827:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7487 (0.9008)
2022-11-09 17:48:02,852:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8409 (0.8972)
2022-11-09 17:48:02,877:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6014 (0.8794)
2022-11-09 17:48:02,899:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5554 (0.8651)
2022-11-09 17:48:02,951:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_131.pth.tar
2022-11-09 17:48:02,951:INFO: 
===> EPOCH: 132 (P1)
2022-11-09 17:48:02,952:INFO: - Computing loss (training)
2022-11-09 17:48:03,161:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6709 (0.6709)
2022-11-09 17:48:03,187:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9675 (0.8143)
2022-11-09 17:48:03,211:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6387 (0.7571)
2022-11-09 17:48:03,229:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4253 (0.7059)
2022-11-09 17:48:03,505:INFO: Dataset: univ                Batch:  1/15	Loss 1.0627 (1.0627)
2022-11-09 17:48:03,533:INFO: Dataset: univ                Batch:  2/15	Loss 0.7882 (0.9304)
2022-11-09 17:48:03,561:INFO: Dataset: univ                Batch:  3/15	Loss 1.3919 (1.0984)
2022-11-09 17:48:03,588:INFO: Dataset: univ                Batch:  4/15	Loss 0.6220 (0.9854)
2022-11-09 17:48:03,618:INFO: Dataset: univ                Batch:  5/15	Loss 0.9814 (0.9845)
2022-11-09 17:48:03,647:INFO: Dataset: univ                Batch:  6/15	Loss 0.5755 (0.9245)
2022-11-09 17:48:03,674:INFO: Dataset: univ                Batch:  7/15	Loss 0.5288 (0.8717)
2022-11-09 17:48:03,701:INFO: Dataset: univ                Batch:  8/15	Loss 0.5654 (0.8363)
2022-11-09 17:48:03,729:INFO: Dataset: univ                Batch:  9/15	Loss 0.5228 (0.8003)
2022-11-09 17:48:03,758:INFO: Dataset: univ                Batch: 10/15	Loss 0.5866 (0.7793)
2022-11-09 17:48:03,790:INFO: Dataset: univ                Batch: 11/15	Loss 0.7587 (0.7776)
2022-11-09 17:48:03,817:INFO: Dataset: univ                Batch: 12/15	Loss 0.5540 (0.7605)
2022-11-09 17:48:03,843:INFO: Dataset: univ                Batch: 13/15	Loss 0.5637 (0.7467)
2022-11-09 17:48:03,870:INFO: Dataset: univ                Batch: 14/15	Loss 0.8037 (0.7505)
2022-11-09 17:48:03,881:INFO: Dataset: univ                Batch: 15/15	Loss 0.1102 (0.7400)
2022-11-09 17:48:04,126:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3575 (1.3575)
2022-11-09 17:48:04,151:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7806 (1.0607)
2022-11-09 17:48:04,177:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7766 (0.9516)
2022-11-09 17:48:04,202:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5963 (0.8633)
2022-11-09 17:48:04,227:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6180 (0.8137)
2022-11-09 17:48:04,252:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7844 (0.8087)
2022-11-09 17:48:04,276:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7565 (0.8015)
2022-11-09 17:48:04,297:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7155 (0.7924)
2022-11-09 17:48:04,541:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8604 (0.8604)
2022-11-09 17:48:04,567:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9304 (0.8967)
2022-11-09 17:48:04,592:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6245 (0.8009)
2022-11-09 17:48:04,619:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6184 (0.7559)
2022-11-09 17:48:04,645:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5671 (0.7198)
2022-11-09 17:48:04,673:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6032 (0.6997)
2022-11-09 17:48:04,697:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5725 (0.6792)
2022-11-09 17:48:04,720:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7501 (0.6880)
2022-11-09 17:48:04,745:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6200 (0.6804)
2022-11-09 17:48:04,769:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5822 (0.6697)
2022-11-09 17:48:04,794:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6259 (0.6656)
2022-11-09 17:48:04,821:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6773 (0.6667)
2022-11-09 17:48:04,846:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5275 (0.6547)
2022-11-09 17:48:04,870:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0780 (0.6854)
2022-11-09 17:48:04,895:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8706 (0.6960)
2022-11-09 17:48:04,919:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6301 (0.6919)
2022-11-09 17:48:04,944:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6973 (0.6923)
2022-11-09 17:48:04,966:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5400 (0.6852)
2022-11-09 17:48:05,018:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_132.pth.tar
2022-11-09 17:48:05,018:INFO: 
===> EPOCH: 133 (P1)
2022-11-09 17:48:05,019:INFO: - Computing loss (training)
2022-11-09 17:48:05,223:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4268 (1.4268)
2022-11-09 17:48:05,249:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3689 (1.3981)
2022-11-09 17:48:05,276:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3308 (1.3757)
2022-11-09 17:48:05,294:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3695 (1.1991)
2022-11-09 17:48:05,556:INFO: Dataset: univ                Batch:  1/15	Loss 0.6336 (0.6336)
2022-11-09 17:48:05,584:INFO: Dataset: univ                Batch:  2/15	Loss 0.8707 (0.7592)
2022-11-09 17:48:05,610:INFO: Dataset: univ                Batch:  3/15	Loss 0.9899 (0.8366)
2022-11-09 17:48:05,637:INFO: Dataset: univ                Batch:  4/15	Loss 0.8509 (0.8402)
2022-11-09 17:48:05,666:INFO: Dataset: univ                Batch:  5/15	Loss 0.5921 (0.7919)
2022-11-09 17:48:05,695:INFO: Dataset: univ                Batch:  6/15	Loss 0.7521 (0.7857)
2022-11-09 17:48:05,721:INFO: Dataset: univ                Batch:  7/15	Loss 0.7504 (0.7802)
2022-11-09 17:48:05,748:INFO: Dataset: univ                Batch:  8/15	Loss 0.9197 (0.7986)
2022-11-09 17:48:05,774:INFO: Dataset: univ                Batch:  9/15	Loss 0.6044 (0.7761)
2022-11-09 17:48:05,801:INFO: Dataset: univ                Batch: 10/15	Loss 0.5251 (0.7503)
2022-11-09 17:48:05,831:INFO: Dataset: univ                Batch: 11/15	Loss 0.5721 (0.7334)
2022-11-09 17:48:05,858:INFO: Dataset: univ                Batch: 12/15	Loss 0.5792 (0.7206)
2022-11-09 17:48:05,884:INFO: Dataset: univ                Batch: 13/15	Loss 0.6356 (0.7146)
2022-11-09 17:48:05,910:INFO: Dataset: univ                Batch: 14/15	Loss 0.5074 (0.6994)
2022-11-09 17:48:05,921:INFO: Dataset: univ                Batch: 15/15	Loss 0.1194 (0.6916)
2022-11-09 17:48:06,178:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0985 (2.0985)
2022-11-09 17:48:06,202:INFO: Dataset: zara1               Batch: 2/8	Loss 2.1926 (2.1444)
2022-11-09 17:48:06,226:INFO: Dataset: zara1               Batch: 3/8	Loss 2.2866 (2.1932)
2022-11-09 17:48:06,250:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7625 (1.8100)
2022-11-09 17:48:06,274:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5657 (1.5542)
2022-11-09 17:48:06,300:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5784 (1.3967)
2022-11-09 17:48:06,324:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1321 (1.3621)
2022-11-09 17:48:06,345:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2518 (1.3498)
2022-11-09 17:48:06,594:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8187 (1.8187)
2022-11-09 17:48:06,656:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5940 (1.7021)
2022-11-09 17:48:06,681:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6343 (1.3441)
2022-11-09 17:48:06,705:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8849 (1.2286)
2022-11-09 17:48:06,730:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5581 (1.0969)
2022-11-09 17:48:06,759:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7661 (1.0438)
2022-11-09 17:48:06,783:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7259 (0.9990)
2022-11-09 17:48:06,807:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6256 (0.9510)
2022-11-09 17:48:06,832:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5271 (0.9052)
2022-11-09 17:48:06,857:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5251 (0.8672)
2022-11-09 17:48:06,884:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1228 (0.8900)
2022-11-09 17:48:06,908:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7805 (0.8809)
2022-11-09 17:48:06,933:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9143 (0.8834)
2022-11-09 17:48:06,958:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9775 (0.8897)
2022-11-09 17:48:06,983:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0873 (0.9020)
2022-11-09 17:48:07,008:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6768 (0.8877)
2022-11-09 17:48:07,034:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5986 (0.8710)
2022-11-09 17:48:07,058:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7899 (0.8673)
2022-11-09 17:48:07,108:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_133.pth.tar
2022-11-09 17:48:07,108:INFO: 
===> EPOCH: 134 (P1)
2022-11-09 17:48:07,108:INFO: - Computing loss (training)
2022-11-09 17:48:07,320:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7395 (0.7395)
2022-11-09 17:48:07,344:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6557 (0.6963)
2022-11-09 17:48:07,368:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6314 (0.6741)
2022-11-09 17:48:07,385:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8029 (0.6964)
2022-11-09 17:48:07,643:INFO: Dataset: univ                Batch:  1/15	Loss 0.9525 (0.9525)
2022-11-09 17:48:07,671:INFO: Dataset: univ                Batch:  2/15	Loss 1.4251 (1.2001)
2022-11-09 17:48:07,701:INFO: Dataset: univ                Batch:  3/15	Loss 0.8187 (1.0802)
2022-11-09 17:48:07,730:INFO: Dataset: univ                Batch:  4/15	Loss 0.7612 (0.9968)
2022-11-09 17:48:07,756:INFO: Dataset: univ                Batch:  5/15	Loss 1.1233 (1.0233)
2022-11-09 17:48:07,786:INFO: Dataset: univ                Batch:  6/15	Loss 0.6312 (0.9606)
2022-11-09 17:48:07,812:INFO: Dataset: univ                Batch:  7/15	Loss 0.5394 (0.9007)
2022-11-09 17:48:07,838:INFO: Dataset: univ                Batch:  8/15	Loss 0.8456 (0.8932)
2022-11-09 17:48:07,865:INFO: Dataset: univ                Batch:  9/15	Loss 0.6364 (0.8638)
2022-11-09 17:48:07,892:INFO: Dataset: univ                Batch: 10/15	Loss 0.5651 (0.8317)
2022-11-09 17:48:07,922:INFO: Dataset: univ                Batch: 11/15	Loss 0.9108 (0.8387)
2022-11-09 17:48:07,949:INFO: Dataset: univ                Batch: 12/15	Loss 1.5928 (0.9004)
2022-11-09 17:48:07,975:INFO: Dataset: univ                Batch: 13/15	Loss 0.8958 (0.9000)
2022-11-09 17:48:08,002:INFO: Dataset: univ                Batch: 14/15	Loss 0.5896 (0.8792)
2022-11-09 17:48:08,012:INFO: Dataset: univ                Batch: 15/15	Loss 0.1491 (0.8691)
2022-11-09 17:48:08,272:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8772 (0.8772)
2022-11-09 17:48:08,296:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4498 (1.1491)
2022-11-09 17:48:08,320:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9568 (1.0853)
2022-11-09 17:48:08,344:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5601 (0.9560)
2022-11-09 17:48:08,368:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5506 (0.8714)
2022-11-09 17:48:08,394:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5664 (0.8245)
2022-11-09 17:48:08,417:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2516 (0.8797)
2022-11-09 17:48:08,438:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8716 (0.8788)
2022-11-09 17:48:08,694:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9003 (0.9003)
2022-11-09 17:48:08,721:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4661 (1.1828)
2022-11-09 17:48:08,745:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7369 (1.0380)
2022-11-09 17:48:08,769:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5129 (0.9035)
2022-11-09 17:48:08,797:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4982 (0.8275)
2022-11-09 17:48:08,824:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5534 (0.7748)
2022-11-09 17:48:08,848:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5577 (0.7415)
2022-11-09 17:48:08,871:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5203 (0.7158)
2022-11-09 17:48:08,895:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5227 (0.8071)
2022-11-09 17:48:08,921:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5570 (0.7813)
2022-11-09 17:48:08,946:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9777 (0.7988)
2022-11-09 17:48:08,973:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9850 (0.8134)
2022-11-09 17:48:08,998:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6086 (0.7958)
2022-11-09 17:48:09,022:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5123 (0.7749)
2022-11-09 17:48:09,046:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7059 (0.7698)
2022-11-09 17:48:09,071:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5856 (0.7588)
2022-11-09 17:48:09,096:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6554 (0.7532)
2022-11-09 17:48:09,120:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4762 (0.7397)
2022-11-09 17:48:09,170:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_134.pth.tar
2022-11-09 17:48:09,170:INFO: 
===> EPOCH: 135 (P1)
2022-11-09 17:48:09,171:INFO: - Computing loss (training)
2022-11-09 17:48:09,378:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6878 (1.6878)
2022-11-09 17:48:09,404:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9982 (1.3438)
2022-11-09 17:48:09,429:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3172 (1.3348)
2022-11-09 17:48:09,446:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8593 (1.2520)
2022-11-09 17:48:09,718:INFO: Dataset: univ                Batch:  1/15	Loss 0.6194 (0.6194)
2022-11-09 17:48:09,746:INFO: Dataset: univ                Batch:  2/15	Loss 0.5804 (0.6006)
2022-11-09 17:48:09,775:INFO: Dataset: univ                Batch:  3/15	Loss 0.7328 (0.6461)
2022-11-09 17:48:09,803:INFO: Dataset: univ                Batch:  4/15	Loss 1.0559 (0.7559)
2022-11-09 17:48:09,832:INFO: Dataset: univ                Batch:  5/15	Loss 0.8331 (0.7720)
2022-11-09 17:48:09,863:INFO: Dataset: univ                Batch:  6/15	Loss 0.6671 (0.7532)
2022-11-09 17:48:09,891:INFO: Dataset: univ                Batch:  7/15	Loss 0.6876 (0.7434)
2022-11-09 17:48:09,918:INFO: Dataset: univ                Batch:  8/15	Loss 0.5220 (0.7158)
2022-11-09 17:48:09,946:INFO: Dataset: univ                Batch:  9/15	Loss 0.8719 (0.7346)
2022-11-09 17:48:09,974:INFO: Dataset: univ                Batch: 10/15	Loss 0.6618 (0.7266)
2022-11-09 17:48:10,002:INFO: Dataset: univ                Batch: 11/15	Loss 0.6568 (0.7198)
2022-11-09 17:48:10,030:INFO: Dataset: univ                Batch: 12/15	Loss 0.5145 (0.7026)
2022-11-09 17:48:10,057:INFO: Dataset: univ                Batch: 13/15	Loss 0.5076 (0.6877)
2022-11-09 17:48:10,083:INFO: Dataset: univ                Batch: 14/15	Loss 0.8644 (0.6982)
2022-11-09 17:48:10,094:INFO: Dataset: univ                Batch: 15/15	Loss 0.2101 (0.6927)
2022-11-09 17:48:10,350:INFO: Dataset: zara1               Batch: 1/8	Loss 4.2712 (4.2712)
2022-11-09 17:48:10,375:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8942 (2.2711)
2022-11-09 17:48:10,403:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0663 (1.8835)
2022-11-09 17:48:10,426:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5625 (1.5208)
2022-11-09 17:48:10,450:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6110 (1.3538)
2022-11-09 17:48:10,475:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7474 (1.2441)
2022-11-09 17:48:10,498:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5947 (1.1418)
2022-11-09 17:48:10,520:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0056 (1.1261)
2022-11-09 17:48:10,767:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0825 (1.0825)
2022-11-09 17:48:10,797:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6414 (1.3615)
2022-11-09 17:48:10,822:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0354 (1.2495)
2022-11-09 17:48:10,851:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9029 (1.1615)
2022-11-09 17:48:10,876:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6936 (1.0713)
2022-11-09 17:48:10,906:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5642 (0.9917)
2022-11-09 17:48:10,931:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4976 (0.9164)
2022-11-09 17:48:10,956:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5166 (0.8671)
2022-11-09 17:48:10,982:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5495 (0.8324)
2022-11-09 17:48:11,008:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8888 (0.8376)
2022-11-09 17:48:11,035:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9362 (0.8469)
2022-11-09 17:48:11,062:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5723 (0.8251)
2022-11-09 17:48:11,087:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5043 (0.7983)
2022-11-09 17:48:11,113:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8707 (0.8037)
2022-11-09 17:48:11,138:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6699 (0.7944)
2022-11-09 17:48:11,165:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6165 (0.7833)
2022-11-09 17:48:11,192:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5525 (0.7687)
2022-11-09 17:48:11,216:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6517 (0.7637)
2022-11-09 17:48:11,266:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_135.pth.tar
2022-11-09 17:48:11,266:INFO: 
===> EPOCH: 136 (P1)
2022-11-09 17:48:11,266:INFO: - Computing loss (training)
2022-11-09 17:48:11,475:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4772 (1.4772)
2022-11-09 17:48:11,499:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9820 (1.2302)
2022-11-09 17:48:11,524:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7568 (1.0716)
2022-11-09 17:48:11,542:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3806 (0.9522)
2022-11-09 17:48:11,802:INFO: Dataset: univ                Batch:  1/15	Loss 1.4130 (1.4130)
2022-11-09 17:48:11,832:INFO: Dataset: univ                Batch:  2/15	Loss 0.6857 (1.0300)
2022-11-09 17:48:11,860:INFO: Dataset: univ                Batch:  3/15	Loss 0.8498 (0.9721)
2022-11-09 17:48:11,890:INFO: Dataset: univ                Batch:  4/15	Loss 0.8299 (0.9336)
2022-11-09 17:48:11,917:INFO: Dataset: univ                Batch:  5/15	Loss 0.6242 (0.8759)
2022-11-09 17:48:11,947:INFO: Dataset: univ                Batch:  6/15	Loss 0.6868 (0.8425)
2022-11-09 17:48:11,973:INFO: Dataset: univ                Batch:  7/15	Loss 0.6145 (0.8139)
2022-11-09 17:48:12,000:INFO: Dataset: univ                Batch:  8/15	Loss 0.5486 (0.7819)
2022-11-09 17:48:12,027:INFO: Dataset: univ                Batch:  9/15	Loss 0.7591 (0.7795)
2022-11-09 17:48:12,055:INFO: Dataset: univ                Batch: 10/15	Loss 0.4822 (0.7496)
2022-11-09 17:48:12,084:INFO: Dataset: univ                Batch: 11/15	Loss 0.5664 (0.7306)
2022-11-09 17:48:12,112:INFO: Dataset: univ                Batch: 12/15	Loss 0.4662 (0.7079)
2022-11-09 17:48:12,139:INFO: Dataset: univ                Batch: 13/15	Loss 0.5243 (0.6934)
2022-11-09 17:48:12,166:INFO: Dataset: univ                Batch: 14/15	Loss 0.7176 (0.6951)
2022-11-09 17:48:12,176:INFO: Dataset: univ                Batch: 15/15	Loss 0.1040 (0.6866)
2022-11-09 17:48:12,440:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6224 (0.6224)
2022-11-09 17:48:12,468:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5483 (0.5879)
2022-11-09 17:48:12,491:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4942 (0.5546)
2022-11-09 17:48:12,515:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5132 (0.5422)
2022-11-09 17:48:12,539:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5585 (0.5457)
2022-11-09 17:48:12,565:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5850 (0.5518)
2022-11-09 17:48:12,589:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5834 (0.5562)
2022-11-09 17:48:12,610:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4821 (0.5483)
2022-11-09 17:48:12,878:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7163 (0.7163)
2022-11-09 17:48:12,904:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5456 (0.6327)
2022-11-09 17:48:12,932:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6078 (0.6244)
2022-11-09 17:48:12,961:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8852 (0.6855)
2022-11-09 17:48:12,986:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6613 (0.6813)
2022-11-09 17:48:13,015:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5231 (0.6553)
2022-11-09 17:48:13,040:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5243 (0.6373)
2022-11-09 17:48:13,065:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4769 (0.6157)
2022-11-09 17:48:13,091:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5690 (0.6108)
2022-11-09 17:48:13,117:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5303 (0.6021)
2022-11-09 17:48:13,144:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5584 (0.5984)
2022-11-09 17:48:13,171:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5025 (0.5905)
2022-11-09 17:48:13,196:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5745 (0.5893)
2022-11-09 17:48:13,221:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5776 (0.5885)
2022-11-09 17:48:13,248:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5009 (0.5830)
2022-11-09 17:48:13,274:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4764 (0.5757)
2022-11-09 17:48:13,300:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6664 (0.5804)
2022-11-09 17:48:13,325:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4785 (0.5757)
2022-11-09 17:48:13,375:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_136.pth.tar
2022-11-09 17:48:13,375:INFO: 
===> EPOCH: 137 (P1)
2022-11-09 17:48:13,376:INFO: - Computing loss (training)
2022-11-09 17:48:13,578:INFO: Dataset: hotel               Batch: 1/4	Loss 8.8941 (8.8941)
2022-11-09 17:48:13,603:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2011 (5.3912)
2022-11-09 17:48:13,627:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8812 (3.9500)
2022-11-09 17:48:13,644:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3898 (3.3441)
2022-11-09 17:48:13,913:INFO: Dataset: univ                Batch:  1/15	Loss 0.6038 (0.6038)
2022-11-09 17:48:13,943:INFO: Dataset: univ                Batch:  2/15	Loss 0.8452 (0.7275)
2022-11-09 17:48:13,970:INFO: Dataset: univ                Batch:  3/15	Loss 0.5005 (0.6534)
2022-11-09 17:48:14,000:INFO: Dataset: univ                Batch:  4/15	Loss 0.8776 (0.7106)
2022-11-09 17:48:14,026:INFO: Dataset: univ                Batch:  5/15	Loss 0.7028 (0.7091)
2022-11-09 17:48:14,055:INFO: Dataset: univ                Batch:  6/15	Loss 0.8092 (0.7231)
2022-11-09 17:48:14,081:INFO: Dataset: univ                Batch:  7/15	Loss 0.6095 (0.7083)
2022-11-09 17:48:14,107:INFO: Dataset: univ                Batch:  8/15	Loss 0.6223 (0.6977)
2022-11-09 17:48:14,133:INFO: Dataset: univ                Batch:  9/15	Loss 0.8673 (0.7166)
2022-11-09 17:48:14,161:INFO: Dataset: univ                Batch: 10/15	Loss 0.7695 (0.7219)
2022-11-09 17:48:14,189:INFO: Dataset: univ                Batch: 11/15	Loss 0.9895 (0.7441)
2022-11-09 17:48:14,217:INFO: Dataset: univ                Batch: 12/15	Loss 0.5284 (0.7244)
2022-11-09 17:48:14,243:INFO: Dataset: univ                Batch: 13/15	Loss 0.5488 (0.7117)
2022-11-09 17:48:14,270:INFO: Dataset: univ                Batch: 14/15	Loss 0.6611 (0.7081)
2022-11-09 17:48:14,280:INFO: Dataset: univ                Batch: 15/15	Loss 0.0914 (0.7008)
2022-11-09 17:48:14,527:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1361 (1.1361)
2022-11-09 17:48:14,555:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2090 (1.1722)
2022-11-09 17:48:14,581:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3299 (1.2219)
2022-11-09 17:48:14,605:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9201 (1.1403)
2022-11-09 17:48:14,628:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7559 (1.0641)
2022-11-09 17:48:14,653:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6377 (0.9887)
2022-11-09 17:48:14,677:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2691 (1.0279)
2022-11-09 17:48:14,698:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9782 (1.1269)
2022-11-09 17:48:14,956:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2373 (1.2373)
2022-11-09 17:48:14,984:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1090 (1.1726)
2022-11-09 17:48:15,009:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8394 (1.0637)
2022-11-09 17:48:15,034:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5422 (0.9290)
2022-11-09 17:48:15,059:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4948 (0.8241)
2022-11-09 17:48:15,088:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6870 (0.8008)
2022-11-09 17:48:15,112:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5811 (0.7698)
2022-11-09 17:48:15,135:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5971 (0.7490)
2022-11-09 17:48:15,159:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4988 (0.7224)
2022-11-09 17:48:15,184:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6403 (0.7137)
2022-11-09 17:48:15,210:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8871 (0.7308)
2022-11-09 17:48:15,236:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8596 (0.7409)
2022-11-09 17:48:15,260:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6514 (0.7341)
2022-11-09 17:48:15,284:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6123 (0.7263)
2022-11-09 17:48:15,309:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6051 (0.7185)
2022-11-09 17:48:15,334:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5922 (0.7110)
2022-11-09 17:48:15,358:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5112 (0.7001)
2022-11-09 17:48:15,381:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4894 (0.6904)
2022-11-09 17:48:15,434:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_137.pth.tar
2022-11-09 17:48:15,434:INFO: 
===> EPOCH: 138 (P1)
2022-11-09 17:48:15,434:INFO: - Computing loss (training)
2022-11-09 17:48:15,637:INFO: Dataset: hotel               Batch: 1/4	Loss 3.7724 (3.7724)
2022-11-09 17:48:15,662:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7784 (2.7730)
2022-11-09 17:48:15,687:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0905 (2.1918)
2022-11-09 17:48:15,704:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5677 (1.9261)
2022-11-09 17:48:15,969:INFO: Dataset: univ                Batch:  1/15	Loss 0.5944 (0.5944)
2022-11-09 17:48:15,998:INFO: Dataset: univ                Batch:  2/15	Loss 0.6038 (0.5994)
2022-11-09 17:48:16,028:INFO: Dataset: univ                Batch:  3/15	Loss 0.5141 (0.5700)
2022-11-09 17:48:16,055:INFO: Dataset: univ                Batch:  4/15	Loss 0.5694 (0.5699)
2022-11-09 17:48:16,084:INFO: Dataset: univ                Batch:  5/15	Loss 0.5437 (0.5644)
2022-11-09 17:48:16,113:INFO: Dataset: univ                Batch:  6/15	Loss 0.5864 (0.5679)
2022-11-09 17:48:16,139:INFO: Dataset: univ                Batch:  7/15	Loss 0.5281 (0.5618)
2022-11-09 17:48:16,165:INFO: Dataset: univ                Batch:  8/15	Loss 0.5472 (0.5601)
2022-11-09 17:48:16,192:INFO: Dataset: univ                Batch:  9/15	Loss 0.7608 (0.5803)
2022-11-09 17:48:16,219:INFO: Dataset: univ                Batch: 10/15	Loss 0.5482 (0.5771)
2022-11-09 17:48:16,247:INFO: Dataset: univ                Batch: 11/15	Loss 0.5411 (0.5739)
2022-11-09 17:48:16,274:INFO: Dataset: univ                Batch: 12/15	Loss 0.6227 (0.5773)
2022-11-09 17:48:16,301:INFO: Dataset: univ                Batch: 13/15	Loss 0.5173 (0.5722)
2022-11-09 17:48:16,327:INFO: Dataset: univ                Batch: 14/15	Loss 0.6113 (0.5752)
2022-11-09 17:48:16,337:INFO: Dataset: univ                Batch: 15/15	Loss 0.1101 (0.5682)
2022-11-09 17:48:16,584:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6298 (0.6298)
2022-11-09 17:48:16,609:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5858 (0.6075)
2022-11-09 17:48:16,634:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6460 (0.6213)
2022-11-09 17:48:16,661:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7822 (0.6621)
2022-11-09 17:48:16,686:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5869 (0.6489)
2022-11-09 17:48:16,711:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5315 (0.6326)
2022-11-09 17:48:16,735:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6666 (0.6371)
2022-11-09 17:48:16,757:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4904 (0.6209)
2022-11-09 17:48:17,010:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9450 (0.9450)
2022-11-09 17:48:17,040:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0015 (0.9729)
2022-11-09 17:48:17,064:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5853 (0.8349)
2022-11-09 17:48:17,088:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6324 (0.7834)
2022-11-09 17:48:17,114:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7211 (0.7715)
2022-11-09 17:48:17,141:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5760 (0.7368)
2022-11-09 17:48:17,165:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5575 (0.7132)
2022-11-09 17:48:17,189:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5110 (0.6902)
2022-11-09 17:48:17,212:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5817 (0.6782)
2022-11-09 17:48:17,237:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7456 (0.6844)
2022-11-09 17:48:17,261:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6935 (0.6852)
2022-11-09 17:48:17,287:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4969 (0.6694)
2022-11-09 17:48:17,313:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4656 (0.7295)
2022-11-09 17:48:17,339:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6247 (0.7217)
2022-11-09 17:48:17,364:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5351 (0.7097)
2022-11-09 17:48:17,389:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5163 (0.6968)
2022-11-09 17:48:17,414:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5273 (0.6876)
2022-11-09 17:48:17,437:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5152 (0.6797)
2022-11-09 17:48:17,486:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_138.pth.tar
2022-11-09 17:48:17,486:INFO: 
===> EPOCH: 139 (P1)
2022-11-09 17:48:17,487:INFO: - Computing loss (training)
2022-11-09 17:48:17,692:INFO: Dataset: hotel               Batch: 1/4	Loss 5.5739 (5.5739)
2022-11-09 17:48:17,719:INFO: Dataset: hotel               Batch: 2/4	Loss 6.9048 (6.2503)
2022-11-09 17:48:17,746:INFO: Dataset: hotel               Batch: 3/4	Loss 6.2631 (6.2543)
2022-11-09 17:48:17,763:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9506 (5.3237)
2022-11-09 17:48:18,015:INFO: Dataset: univ                Batch:  1/15	Loss 0.7164 (0.7164)
2022-11-09 17:48:18,044:INFO: Dataset: univ                Batch:  2/15	Loss 0.6771 (0.6966)
2022-11-09 17:48:18,075:INFO: Dataset: univ                Batch:  3/15	Loss 0.9850 (0.8001)
2022-11-09 17:48:18,101:INFO: Dataset: univ                Batch:  4/15	Loss 1.5464 (0.9896)
2022-11-09 17:48:18,129:INFO: Dataset: univ                Batch:  5/15	Loss 1.9494 (1.1874)
2022-11-09 17:48:18,158:INFO: Dataset: univ                Batch:  6/15	Loss 1.1261 (1.1779)
2022-11-09 17:48:18,184:INFO: Dataset: univ                Batch:  7/15	Loss 0.8464 (1.1322)
2022-11-09 17:48:18,210:INFO: Dataset: univ                Batch:  8/15	Loss 0.8289 (1.0929)
2022-11-09 17:48:18,237:INFO: Dataset: univ                Batch:  9/15	Loss 0.9340 (1.0738)
2022-11-09 17:48:18,264:INFO: Dataset: univ                Batch: 10/15	Loss 0.6451 (1.0292)
2022-11-09 17:48:18,291:INFO: Dataset: univ                Batch: 11/15	Loss 0.6511 (0.9947)
2022-11-09 17:48:18,319:INFO: Dataset: univ                Batch: 12/15	Loss 0.6041 (0.9609)
2022-11-09 17:48:18,345:INFO: Dataset: univ                Batch: 13/15	Loss 0.5482 (0.9310)
2022-11-09 17:48:18,371:INFO: Dataset: univ                Batch: 14/15	Loss 0.8981 (0.9287)
2022-11-09 17:48:18,380:INFO: Dataset: univ                Batch: 15/15	Loss 0.1037 (0.9190)
2022-11-09 17:48:18,631:INFO: Dataset: zara1               Batch: 1/8	Loss 2.6363 (2.6363)
2022-11-09 17:48:18,657:INFO: Dataset: zara1               Batch: 2/8	Loss 2.4072 (2.5261)
2022-11-09 17:48:18,681:INFO: Dataset: zara1               Batch: 3/8	Loss 2.1318 (2.3914)
2022-11-09 17:48:18,705:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3925 (2.1057)
2022-11-09 17:48:18,729:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5569 (1.7782)
2022-11-09 17:48:18,755:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6275 (1.5748)
2022-11-09 17:48:18,779:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0727 (1.5041)
2022-11-09 17:48:18,800:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3758 (1.4909)
2022-11-09 17:48:19,091:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5172 (1.5172)
2022-11-09 17:48:19,114:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2167 (1.3733)
2022-11-09 17:48:19,138:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3693 (1.3721)
2022-11-09 17:48:19,162:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9797 (1.2753)
2022-11-09 17:48:19,186:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7063 (1.1536)
2022-11-09 17:48:19,212:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5932 (1.0599)
2022-11-09 17:48:19,236:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5119 (0.9771)
2022-11-09 17:48:19,260:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5550 (0.9257)
2022-11-09 17:48:19,283:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6318 (0.8916)
2022-11-09 17:48:19,308:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1404 (0.9147)
2022-11-09 17:48:19,334:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8020 (0.9044)
2022-11-09 17:48:19,360:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8072 (0.8955)
2022-11-09 17:48:19,385:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6481 (0.8755)
2022-11-09 17:48:19,410:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6171 (0.8574)
2022-11-09 17:48:19,436:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5962 (0.8412)
2022-11-09 17:48:19,461:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5450 (0.8203)
2022-11-09 17:48:19,486:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5084 (0.7988)
2022-11-09 17:48:19,509:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8920 (0.8028)
2022-11-09 17:48:19,560:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_139.pth.tar
2022-11-09 17:48:19,561:INFO: 
===> EPOCH: 140 (P1)
2022-11-09 17:48:19,561:INFO: - Computing loss (training)
2022-11-09 17:48:19,776:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5454 (1.5454)
2022-11-09 17:48:19,801:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7699 (1.1520)
2022-11-09 17:48:19,828:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5440 (0.9397)
2022-11-09 17:48:19,844:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4201 (0.8519)
2022-11-09 17:48:20,097:INFO: Dataset: univ                Batch:  1/15	Loss 0.5901 (0.5901)
2022-11-09 17:48:20,144:INFO: Dataset: univ                Batch:  2/15	Loss 0.8222 (0.7120)
2022-11-09 17:48:20,171:INFO: Dataset: univ                Batch:  3/15	Loss 0.9788 (0.8026)
2022-11-09 17:48:20,198:INFO: Dataset: univ                Batch:  4/15	Loss 0.5405 (0.7375)
2022-11-09 17:48:20,224:INFO: Dataset: univ                Batch:  5/15	Loss 0.6097 (0.7104)
2022-11-09 17:48:20,253:INFO: Dataset: univ                Batch:  6/15	Loss 0.5566 (0.6842)
2022-11-09 17:48:20,279:INFO: Dataset: univ                Batch:  7/15	Loss 0.5477 (0.6674)
2022-11-09 17:48:20,306:INFO: Dataset: univ                Batch:  8/15	Loss 0.7046 (0.6720)
2022-11-09 17:48:20,333:INFO: Dataset: univ                Batch:  9/15	Loss 0.4988 (0.6516)
2022-11-09 17:48:20,361:INFO: Dataset: univ                Batch: 10/15	Loss 0.5622 (0.6425)
2022-11-09 17:48:20,390:INFO: Dataset: univ                Batch: 11/15	Loss 0.4904 (0.6274)
2022-11-09 17:48:20,416:INFO: Dataset: univ                Batch: 12/15	Loss 0.8146 (0.6427)
2022-11-09 17:48:20,442:INFO: Dataset: univ                Batch: 13/15	Loss 0.4693 (0.6285)
2022-11-09 17:48:20,470:INFO: Dataset: univ                Batch: 14/15	Loss 0.4675 (0.6159)
2022-11-09 17:48:20,480:INFO: Dataset: univ                Batch: 15/15	Loss 0.1015 (0.6096)
2022-11-09 17:48:20,729:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1488 (1.1488)
2022-11-09 17:48:20,754:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9498 (1.0557)
2022-11-09 17:48:20,781:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5454 (0.8842)
2022-11-09 17:48:20,805:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5743 (0.8021)
2022-11-09 17:48:20,829:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6150 (0.7603)
2022-11-09 17:48:20,854:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5892 (0.7349)
2022-11-09 17:48:20,878:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8431 (0.7509)
2022-11-09 17:48:20,899:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6430 (0.7389)
2022-11-09 17:48:21,166:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8431 (0.8431)
2022-11-09 17:48:21,193:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6238 (0.7346)
2022-11-09 17:48:21,219:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5884 (0.6901)
2022-11-09 17:48:21,243:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5262 (0.6462)
2022-11-09 17:48:21,272:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5119 (0.6196)
2022-11-09 17:48:21,299:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5141 (0.6021)
2022-11-09 17:48:21,322:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3996 (0.5711)
2022-11-09 17:48:21,346:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4968 (0.5614)
2022-11-09 17:48:21,371:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5112 (0.5557)
2022-11-09 17:48:21,396:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4747 (0.5479)
2022-11-09 17:48:21,421:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7898 (0.5677)
2022-11-09 17:48:21,447:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7180 (0.5798)
2022-11-09 17:48:21,471:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5799 (0.5798)
2022-11-09 17:48:21,496:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5715 (0.5792)
2022-11-09 17:48:21,521:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6628 (0.5852)
2022-11-09 17:48:21,546:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5200 (0.5817)
2022-11-09 17:48:21,571:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5086 (0.5776)
2022-11-09 17:48:21,595:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4300 (0.5705)
2022-11-09 17:48:21,645:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_140.pth.tar
2022-11-09 17:48:21,645:INFO: 
===> EPOCH: 141 (P1)
2022-11-09 17:48:21,646:INFO: - Computing loss (training)
2022-11-09 17:48:21,853:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4847 (1.4847)
2022-11-09 17:48:21,877:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8252 (1.6561)
2022-11-09 17:48:21,903:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9455 (1.7515)
2022-11-09 17:48:21,920:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3729 (1.5205)
2022-11-09 17:48:22,178:INFO: Dataset: univ                Batch:  1/15	Loss 0.6244 (0.6244)
2022-11-09 17:48:22,264:INFO: Dataset: univ                Batch:  2/15	Loss 0.6508 (0.6374)
2022-11-09 17:48:22,292:INFO: Dataset: univ                Batch:  3/15	Loss 0.7969 (0.6872)
2022-11-09 17:48:22,319:INFO: Dataset: univ                Batch:  4/15	Loss 1.2575 (0.8417)
2022-11-09 17:48:22,345:INFO: Dataset: univ                Batch:  5/15	Loss 1.0746 (0.8908)
2022-11-09 17:48:22,374:INFO: Dataset: univ                Batch:  6/15	Loss 0.9835 (0.9056)
2022-11-09 17:48:22,402:INFO: Dataset: univ                Batch:  7/15	Loss 0.7235 (0.8795)
2022-11-09 17:48:22,430:INFO: Dataset: univ                Batch:  8/15	Loss 0.5743 (0.8451)
2022-11-09 17:48:22,457:INFO: Dataset: univ                Batch:  9/15	Loss 0.5516 (0.8147)
2022-11-09 17:48:22,484:INFO: Dataset: univ                Batch: 10/15	Loss 0.6225 (0.7977)
2022-11-09 17:48:22,513:INFO: Dataset: univ                Batch: 11/15	Loss 0.5381 (0.7741)
2022-11-09 17:48:22,539:INFO: Dataset: univ                Batch: 12/15	Loss 0.6040 (0.7591)
2022-11-09 17:48:22,566:INFO: Dataset: univ                Batch: 13/15	Loss 0.6430 (0.7504)
2022-11-09 17:48:22,593:INFO: Dataset: univ                Batch: 14/15	Loss 0.5544 (0.7369)
2022-11-09 17:48:22,603:INFO: Dataset: univ                Batch: 15/15	Loss 0.0938 (0.7285)
2022-11-09 17:48:22,869:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4747 (1.4747)
2022-11-09 17:48:22,894:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0061 (1.2608)
2022-11-09 17:48:22,919:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1651 (1.2305)
2022-11-09 17:48:22,945:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1168 (1.2040)
2022-11-09 17:48:22,970:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5140 (1.0649)
2022-11-09 17:48:22,995:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5239 (0.9769)
2022-11-09 17:48:23,019:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4909 (0.9072)
2022-11-09 17:48:23,040:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7836 (0.8933)
2022-11-09 17:48:23,297:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2587 (1.2587)
2022-11-09 17:48:23,323:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5017 (1.3839)
2022-11-09 17:48:23,349:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5599 (1.4397)
2022-11-09 17:48:23,378:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6947 (1.2532)
2022-11-09 17:48:23,402:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6855 (1.1339)
2022-11-09 17:48:23,429:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5071 (1.0253)
2022-11-09 17:48:23,453:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4768 (0.9512)
2022-11-09 17:48:23,477:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5029 (0.8958)
2022-11-09 17:48:23,501:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5019 (0.8475)
2022-11-09 17:48:23,526:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4751 (0.8111)
2022-11-09 17:48:23,551:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5306 (0.7856)
2022-11-09 17:48:23,577:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9629 (0.8007)
2022-11-09 17:48:23,602:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2867 (0.8385)
2022-11-09 17:48:23,626:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5032 (0.8832)
2022-11-09 17:48:23,650:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1474 (0.9000)
2022-11-09 17:48:23,676:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7733 (0.8927)
2022-11-09 17:48:23,701:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4755 (0.8692)
2022-11-09 17:48:23,724:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4417 (0.8492)
2022-11-09 17:48:23,775:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_141.pth.tar
2022-11-09 17:48:23,775:INFO: 
===> EPOCH: 142 (P1)
2022-11-09 17:48:23,776:INFO: - Computing loss (training)
2022-11-09 17:48:23,976:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4520 (2.4520)
2022-11-09 17:48:24,002:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7800 (2.1248)
2022-11-09 17:48:24,027:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7595 (1.6590)
2022-11-09 17:48:24,045:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4876 (1.4704)
2022-11-09 17:48:24,294:INFO: Dataset: univ                Batch:  1/15	Loss 0.5484 (0.5484)
2022-11-09 17:48:24,324:INFO: Dataset: univ                Batch:  2/15	Loss 0.8314 (0.6863)
2022-11-09 17:48:24,354:INFO: Dataset: univ                Batch:  3/15	Loss 0.7668 (0.7122)
2022-11-09 17:48:24,383:INFO: Dataset: univ                Batch:  4/15	Loss 0.9074 (0.7630)
2022-11-09 17:48:24,410:INFO: Dataset: univ                Batch:  5/15	Loss 0.7561 (0.7616)
2022-11-09 17:48:24,438:INFO: Dataset: univ                Batch:  6/15	Loss 0.5521 (0.7314)
2022-11-09 17:48:24,464:INFO: Dataset: univ                Batch:  7/15	Loss 0.4752 (0.6921)
2022-11-09 17:48:24,490:INFO: Dataset: univ                Batch:  8/15	Loss 0.5090 (0.6694)
2022-11-09 17:48:24,517:INFO: Dataset: univ                Batch:  9/15	Loss 0.5427 (0.6546)
2022-11-09 17:48:24,545:INFO: Dataset: univ                Batch: 10/15	Loss 0.5453 (0.6439)
2022-11-09 17:48:24,572:INFO: Dataset: univ                Batch: 11/15	Loss 0.5438 (0.6348)
2022-11-09 17:48:24,600:INFO: Dataset: univ                Batch: 12/15	Loss 0.9165 (0.6555)
2022-11-09 17:48:24,626:INFO: Dataset: univ                Batch: 13/15	Loss 0.5438 (0.6479)
2022-11-09 17:48:24,652:INFO: Dataset: univ                Batch: 14/15	Loss 0.4988 (0.6364)
2022-11-09 17:48:24,663:INFO: Dataset: univ                Batch: 15/15	Loss 0.0856 (0.6305)
2022-11-09 17:48:24,911:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5616 (0.5616)
2022-11-09 17:48:24,936:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5411 (0.5506)
2022-11-09 17:48:24,960:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6777 (0.5950)
2022-11-09 17:48:24,984:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5113 (0.5765)
2022-11-09 17:48:25,010:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6165 (0.5843)
2022-11-09 17:48:25,035:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5864 (0.5846)
2022-11-09 17:48:25,058:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5528 (0.5802)
2022-11-09 17:48:25,080:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4407 (0.5650)
2022-11-09 17:48:25,332:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5658 (0.5658)
2022-11-09 17:48:25,358:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5337 (0.5488)
2022-11-09 17:48:25,385:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1542 (0.7553)
2022-11-09 17:48:25,412:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7780 (0.7616)
2022-11-09 17:48:25,439:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7820 (0.7655)
2022-11-09 17:48:25,466:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4756 (0.7184)
2022-11-09 17:48:25,490:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4954 (0.6880)
2022-11-09 17:48:25,515:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4588 (0.6605)
2022-11-09 17:48:25,540:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4651 (0.6402)
2022-11-09 17:48:25,565:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5138 (0.6284)
2022-11-09 17:48:25,593:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4853 (0.6151)
2022-11-09 17:48:25,618:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5697 (0.6116)
2022-11-09 17:48:25,643:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7129 (0.6196)
2022-11-09 17:48:25,667:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6789 (0.6236)
2022-11-09 17:48:25,693:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6078 (0.6224)
2022-11-09 17:48:25,719:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5826 (0.6199)
2022-11-09 17:48:25,744:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4518 (0.6096)
2022-11-09 17:48:25,768:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4556 (0.6022)
2022-11-09 17:48:25,820:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_142.pth.tar
2022-11-09 17:48:25,820:INFO: 
===> EPOCH: 143 (P1)
2022-11-09 17:48:25,820:INFO: - Computing loss (training)
2022-11-09 17:48:26,025:INFO: Dataset: hotel               Batch: 1/4	Loss 7.3944 (7.3944)
2022-11-09 17:48:26,051:INFO: Dataset: hotel               Batch: 2/4	Loss 7.0977 (7.2419)
2022-11-09 17:48:26,076:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4858 (5.1613)
2022-11-09 17:48:26,092:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6816 (4.3930)
2022-11-09 17:48:26,344:INFO: Dataset: univ                Batch:  1/15	Loss 0.6231 (0.6231)
2022-11-09 17:48:26,373:INFO: Dataset: univ                Batch:  2/15	Loss 1.0264 (0.8269)
2022-11-09 17:48:26,400:INFO: Dataset: univ                Batch:  3/15	Loss 1.7398 (1.1484)
2022-11-09 17:48:26,426:INFO: Dataset: univ                Batch:  4/15	Loss 0.7988 (1.0617)
2022-11-09 17:48:26,453:INFO: Dataset: univ                Batch:  5/15	Loss 0.9157 (1.0330)
2022-11-09 17:48:26,485:INFO: Dataset: univ                Batch:  6/15	Loss 1.3133 (1.0807)
2022-11-09 17:48:26,511:INFO: Dataset: univ                Batch:  7/15	Loss 0.6253 (1.0159)
2022-11-09 17:48:26,537:INFO: Dataset: univ                Batch:  8/15	Loss 0.5939 (0.9653)
2022-11-09 17:48:26,564:INFO: Dataset: univ                Batch:  9/15	Loss 0.5497 (0.9181)
2022-11-09 17:48:26,592:INFO: Dataset: univ                Batch: 10/15	Loss 0.7548 (0.9011)
2022-11-09 17:48:26,619:INFO: Dataset: univ                Batch: 11/15	Loss 0.5862 (0.8735)
2022-11-09 17:48:26,646:INFO: Dataset: univ                Batch: 12/15	Loss 1.0354 (0.8864)
2022-11-09 17:48:26,673:INFO: Dataset: univ                Batch: 13/15	Loss 0.7396 (0.8761)
2022-11-09 17:48:26,699:INFO: Dataset: univ                Batch: 14/15	Loss 1.0964 (0.8897)
2022-11-09 17:48:26,708:INFO: Dataset: univ                Batch: 15/15	Loss 0.1099 (0.8771)
2022-11-09 17:48:26,952:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8131 (1.8131)
2022-11-09 17:48:26,978:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9311 (1.3840)
2022-11-09 17:48:27,004:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7366 (1.1778)
2022-11-09 17:48:27,030:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5367 (1.0153)
2022-11-09 17:48:27,053:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6226 (0.9406)
2022-11-09 17:48:27,078:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0869 (0.9623)
2022-11-09 17:48:27,102:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1825 (1.1330)
2022-11-09 17:48:27,123:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0833 (1.1284)
2022-11-09 17:48:27,392:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7492 (0.7492)
2022-11-09 17:48:27,418:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7280 (0.7389)
2022-11-09 17:48:27,443:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6977 (0.7245)
2022-11-09 17:48:27,471:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7638 (0.7336)
2022-11-09 17:48:27,496:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4842 (0.6843)
2022-11-09 17:48:27,525:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5376 (0.6594)
2022-11-09 17:48:27,549:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5540 (0.6436)
2022-11-09 17:48:27,574:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6352 (0.6426)
2022-11-09 17:48:27,598:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7043 (0.6494)
2022-11-09 17:48:27,624:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5486 (0.7396)
2022-11-09 17:48:27,649:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8140 (0.7457)
2022-11-09 17:48:27,677:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6120 (0.7356)
2022-11-09 17:48:27,702:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5598 (0.7208)
2022-11-09 17:48:27,726:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6403 (0.7151)
2022-11-09 17:48:27,751:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5014 (0.7009)
2022-11-09 17:48:27,776:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6398 (0.6971)
2022-11-09 17:48:27,802:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4858 (0.6835)
2022-11-09 17:48:27,825:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8278 (0.6898)
2022-11-09 17:48:27,876:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_143.pth.tar
2022-11-09 17:48:27,876:INFO: 
===> EPOCH: 144 (P1)
2022-11-09 17:48:27,876:INFO: - Computing loss (training)
2022-11-09 17:48:28,081:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0862 (1.0862)
2022-11-09 17:48:28,107:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1792 (1.1326)
2022-11-09 17:48:28,131:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9537 (1.0761)
2022-11-09 17:48:28,147:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5066 (0.9799)
2022-11-09 17:48:28,409:INFO: Dataset: univ                Batch:  1/15	Loss 0.5317 (0.5317)
2022-11-09 17:48:28,440:INFO: Dataset: univ                Batch:  2/15	Loss 0.5739 (0.5536)
2022-11-09 17:48:28,466:INFO: Dataset: univ                Batch:  3/15	Loss 0.7625 (0.6208)
2022-11-09 17:48:28,494:INFO: Dataset: univ                Batch:  4/15	Loss 0.5979 (0.6152)
2022-11-09 17:48:28,520:INFO: Dataset: univ                Batch:  5/15	Loss 0.4704 (0.5848)
2022-11-09 17:48:28,550:INFO: Dataset: univ                Batch:  6/15	Loss 0.5216 (0.5755)
2022-11-09 17:48:28,576:INFO: Dataset: univ                Batch:  7/15	Loss 0.5063 (0.5649)
2022-11-09 17:48:28,602:INFO: Dataset: univ                Batch:  8/15	Loss 0.6012 (0.5694)
2022-11-09 17:48:28,629:INFO: Dataset: univ                Batch:  9/15	Loss 1.3487 (0.6486)
2022-11-09 17:48:28,657:INFO: Dataset: univ                Batch: 10/15	Loss 0.5469 (0.6384)
2022-11-09 17:48:28,687:INFO: Dataset: univ                Batch: 11/15	Loss 0.4960 (0.6251)
2022-11-09 17:48:28,715:INFO: Dataset: univ                Batch: 12/15	Loss 0.4559 (0.6095)
2022-11-09 17:48:28,742:INFO: Dataset: univ                Batch: 13/15	Loss 0.5548 (0.6050)
2022-11-09 17:48:28,770:INFO: Dataset: univ                Batch: 14/15	Loss 0.5278 (0.5994)
2022-11-09 17:48:28,780:INFO: Dataset: univ                Batch: 15/15	Loss 0.1136 (0.5943)
2022-11-09 17:48:29,024:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6220 (0.6220)
2022-11-09 17:48:29,051:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5814 (0.6017)
2022-11-09 17:48:29,077:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5149 (0.5726)
2022-11-09 17:48:29,104:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5008 (0.5548)
2022-11-09 17:48:29,130:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5109 (0.5464)
2022-11-09 17:48:29,156:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6905 (0.5728)
2022-11-09 17:48:29,183:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5157 (0.5642)
2022-11-09 17:48:29,205:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4985 (0.5577)
2022-11-09 17:48:29,457:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7610 (0.7610)
2022-11-09 17:48:29,483:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5957 (0.6786)
2022-11-09 17:48:29,510:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5959 (0.6499)
2022-11-09 17:48:29,537:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5224 (0.6180)
2022-11-09 17:48:29,561:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4885 (0.5909)
2022-11-09 17:48:29,588:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5074 (0.5778)
2022-11-09 17:48:29,612:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4602 (0.5611)
2022-11-09 17:48:29,636:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7246 (0.5810)
2022-11-09 17:48:29,660:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7506 (0.6002)
2022-11-09 17:48:29,685:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5597 (0.5962)
2022-11-09 17:48:29,712:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5803 (0.5947)
2022-11-09 17:48:29,738:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5804 (0.5936)
2022-11-09 17:48:29,762:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4553 (0.5839)
2022-11-09 17:48:29,786:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4985 (0.5779)
2022-11-09 17:48:29,811:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5921 (0.5789)
2022-11-09 17:48:29,836:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5798 (0.5789)
2022-11-09 17:48:29,861:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5203 (0.5753)
2022-11-09 17:48:29,885:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4904 (0.5712)
2022-11-09 17:48:29,937:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_144.pth.tar
2022-11-09 17:48:29,937:INFO: 
===> EPOCH: 145 (P1)
2022-11-09 17:48:29,938:INFO: - Computing loss (training)
2022-11-09 17:48:30,157:INFO: Dataset: hotel               Batch: 1/4	Loss 2.7769 (2.7769)
2022-11-09 17:48:30,182:INFO: Dataset: hotel               Batch: 2/4	Loss 2.6725 (2.7238)
2022-11-09 17:48:30,206:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5987 (2.0212)
2022-11-09 17:48:30,222:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4182 (1.7293)
2022-11-09 17:48:30,476:INFO: Dataset: univ                Batch:  1/15	Loss 0.8243 (0.8243)
2022-11-09 17:48:30,504:INFO: Dataset: univ                Batch:  2/15	Loss 0.9194 (0.8734)
2022-11-09 17:48:30,536:INFO: Dataset: univ                Batch:  3/15	Loss 0.7137 (0.8233)
2022-11-09 17:48:30,562:INFO: Dataset: univ                Batch:  4/15	Loss 0.7209 (0.7981)
2022-11-09 17:48:30,589:INFO: Dataset: univ                Batch:  5/15	Loss 1.0276 (0.8414)
2022-11-09 17:48:30,617:INFO: Dataset: univ                Batch:  6/15	Loss 0.4747 (0.7863)
2022-11-09 17:48:30,643:INFO: Dataset: univ                Batch:  7/15	Loss 0.7907 (0.7870)
2022-11-09 17:48:30,669:INFO: Dataset: univ                Batch:  8/15	Loss 0.4987 (0.7523)
2022-11-09 17:48:30,698:INFO: Dataset: univ                Batch:  9/15	Loss 0.4822 (0.7213)
2022-11-09 17:48:30,726:INFO: Dataset: univ                Batch: 10/15	Loss 0.8062 (0.7306)
2022-11-09 17:48:30,755:INFO: Dataset: univ                Batch: 11/15	Loss 0.8094 (0.7377)
2022-11-09 17:48:30,782:INFO: Dataset: univ                Batch: 12/15	Loss 0.4855 (0.7180)
2022-11-09 17:48:30,808:INFO: Dataset: univ                Batch: 13/15	Loss 0.5164 (0.7014)
2022-11-09 17:48:30,834:INFO: Dataset: univ                Batch: 14/15	Loss 0.4822 (0.6859)
2022-11-09 17:48:30,844:INFO: Dataset: univ                Batch: 15/15	Loss 0.2053 (0.6798)
2022-11-09 17:48:31,090:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9477 (1.9477)
2022-11-09 17:48:31,115:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5529 (1.7385)
2022-11-09 17:48:31,139:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0559 (1.5088)
2022-11-09 17:48:31,163:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9591 (1.3812)
2022-11-09 17:48:31,188:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5938 (1.2092)
2022-11-09 17:48:31,214:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5102 (1.0866)
2022-11-09 17:48:31,239:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7686 (1.0446)
2022-11-09 17:48:31,262:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5424 (0.9922)
2022-11-09 17:48:31,522:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7148 (0.7148)
2022-11-09 17:48:31,549:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6646 (1.2100)
2022-11-09 17:48:31,579:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6837 (1.3742)
2022-11-09 17:48:31,604:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8121 (1.2316)
2022-11-09 17:48:31,628:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6494 (1.1176)
2022-11-09 17:48:31,656:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4502 (1.0165)
2022-11-09 17:48:31,681:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4533 (0.9357)
2022-11-09 17:48:31,706:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5149 (0.8864)
2022-11-09 17:48:31,730:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5428 (0.8452)
2022-11-09 17:48:31,756:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6042 (0.8207)
2022-11-09 17:48:31,784:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4966 (0.7915)
2022-11-09 17:48:31,809:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1542 (0.8208)
2022-11-09 17:48:31,834:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5494 (0.7994)
2022-11-09 17:48:31,859:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5035 (0.7783)
2022-11-09 17:48:31,884:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1896 (0.8047)
2022-11-09 17:48:31,909:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5072 (0.7866)
2022-11-09 17:48:31,935:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5398 (0.7714)
2022-11-09 17:48:31,959:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4276 (0.7550)
2022-11-09 17:48:32,011:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_145.pth.tar
2022-11-09 17:48:32,011:INFO: 
===> EPOCH: 146 (P1)
2022-11-09 17:48:32,011:INFO: - Computing loss (training)
2022-11-09 17:48:32,222:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6579 (0.6579)
2022-11-09 17:48:32,248:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0204 (0.8358)
2022-11-09 17:48:32,275:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0611 (0.9104)
2022-11-09 17:48:32,291:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4662 (0.8360)
2022-11-09 17:48:32,550:INFO: Dataset: univ                Batch:  1/15	Loss 0.6654 (0.6654)
2022-11-09 17:48:32,581:INFO: Dataset: univ                Batch:  2/15	Loss 0.4787 (0.5758)
2022-11-09 17:48:32,611:INFO: Dataset: univ                Batch:  3/15	Loss 0.8330 (0.6655)
2022-11-09 17:48:32,637:INFO: Dataset: univ                Batch:  4/15	Loss 0.5126 (0.6304)
2022-11-09 17:48:32,665:INFO: Dataset: univ                Batch:  5/15	Loss 0.5697 (0.6191)
2022-11-09 17:48:32,694:INFO: Dataset: univ                Batch:  6/15	Loss 0.5542 (0.6080)
2022-11-09 17:48:32,720:INFO: Dataset: univ                Batch:  7/15	Loss 0.8080 (0.6354)
2022-11-09 17:48:32,745:INFO: Dataset: univ                Batch:  8/15	Loss 0.5345 (0.6224)
2022-11-09 17:48:32,773:INFO: Dataset: univ                Batch:  9/15	Loss 0.5565 (0.6145)
2022-11-09 17:48:32,801:INFO: Dataset: univ                Batch: 10/15	Loss 0.4649 (0.5991)
2022-11-09 17:48:32,832:INFO: Dataset: univ                Batch: 11/15	Loss 0.6190 (0.6009)
2022-11-09 17:48:32,858:INFO: Dataset: univ                Batch: 12/15	Loss 0.5011 (0.5928)
2022-11-09 17:48:32,884:INFO: Dataset: univ                Batch: 13/15	Loss 0.4867 (0.5841)
2022-11-09 17:48:32,910:INFO: Dataset: univ                Batch: 14/15	Loss 0.4606 (0.5754)
2022-11-09 17:48:32,919:INFO: Dataset: univ                Batch: 15/15	Loss 0.0803 (0.5695)
2022-11-09 17:48:33,164:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5517 (0.5517)
2022-11-09 17:48:33,190:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7797 (0.6607)
2022-11-09 17:48:33,219:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8754 (0.7377)
2022-11-09 17:48:33,243:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5321 (0.6866)
2022-11-09 17:48:33,267:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5718 (0.6649)
2022-11-09 17:48:33,292:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6246 (0.6582)
2022-11-09 17:48:33,316:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5870 (0.6467)
2022-11-09 17:48:33,338:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4252 (0.6206)
2022-11-09 17:48:33,595:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7064 (0.7064)
2022-11-09 17:48:33,620:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5596 (0.6318)
2022-11-09 17:48:33,648:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6531 (0.6391)
2022-11-09 17:48:33,674:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5575 (0.6210)
2022-11-09 17:48:33,700:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5244 (0.6000)
2022-11-09 17:48:33,729:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5584 (0.5941)
2022-11-09 17:48:33,753:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6278 (0.5995)
2022-11-09 17:48:33,778:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5705 (0.5962)
2022-11-09 17:48:33,802:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9451 (0.6330)
2022-11-09 17:48:33,827:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6043 (0.6302)
2022-11-09 17:48:33,854:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5313 (0.6211)
2022-11-09 17:48:33,879:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6517 (0.6234)
2022-11-09 17:48:33,904:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5669 (0.6188)
2022-11-09 17:48:33,928:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4793 (0.6090)
2022-11-09 17:48:33,954:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5141 (0.6019)
2022-11-09 17:48:33,979:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4824 (0.5942)
2022-11-09 17:48:34,004:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4576 (0.5861)
2022-11-09 17:48:34,027:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4453 (0.5789)
2022-11-09 17:48:34,077:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_146.pth.tar
2022-11-09 17:48:34,077:INFO: 
===> EPOCH: 147 (P1)
2022-11-09 17:48:34,078:INFO: - Computing loss (training)
2022-11-09 17:48:34,290:INFO: Dataset: hotel               Batch: 1/4	Loss 3.1682 (3.1682)
2022-11-09 17:48:34,317:INFO: Dataset: hotel               Batch: 2/4	Loss 3.4045 (3.2835)
2022-11-09 17:48:34,342:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9555 (2.4758)
2022-11-09 17:48:34,359:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4073 (2.1456)
2022-11-09 17:48:34,626:INFO: Dataset: univ                Batch:  1/15	Loss 0.5326 (0.5326)
2022-11-09 17:48:34,655:INFO: Dataset: univ                Batch:  2/15	Loss 0.6291 (0.5828)
2022-11-09 17:48:34,684:INFO: Dataset: univ                Batch:  3/15	Loss 0.8818 (0.6763)
2022-11-09 17:48:34,712:INFO: Dataset: univ                Batch:  4/15	Loss 0.7007 (0.6824)
2022-11-09 17:48:34,741:INFO: Dataset: univ                Batch:  5/15	Loss 0.9397 (0.7343)
2022-11-09 17:48:34,770:INFO: Dataset: univ                Batch:  6/15	Loss 1.0897 (0.7923)
2022-11-09 17:48:34,795:INFO: Dataset: univ                Batch:  7/15	Loss 0.6030 (0.7653)
2022-11-09 17:48:34,821:INFO: Dataset: univ                Batch:  8/15	Loss 0.4704 (0.7310)
2022-11-09 17:48:34,849:INFO: Dataset: univ                Batch:  9/15	Loss 0.7587 (0.7344)
2022-11-09 17:48:34,876:INFO: Dataset: univ                Batch: 10/15	Loss 0.5020 (0.7124)
2022-11-09 17:48:34,903:INFO: Dataset: univ                Batch: 11/15	Loss 0.5139 (0.6943)
2022-11-09 17:48:34,930:INFO: Dataset: univ                Batch: 12/15	Loss 0.9196 (0.7123)
2022-11-09 17:48:34,956:INFO: Dataset: univ                Batch: 13/15	Loss 0.4417 (0.6898)
2022-11-09 17:48:34,982:INFO: Dataset: univ                Batch: 14/15	Loss 0.4576 (0.6732)
2022-11-09 17:48:34,992:INFO: Dataset: univ                Batch: 15/15	Loss 0.0932 (0.6645)
2022-11-09 17:48:35,251:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6831 (1.6831)
2022-11-09 17:48:35,276:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3712 (1.5229)
2022-11-09 17:48:35,300:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0390 (1.3650)
2022-11-09 17:48:35,326:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8169 (1.2346)
2022-11-09 17:48:35,350:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6001 (1.1211)
2022-11-09 17:48:35,375:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6680 (1.0453)
2022-11-09 17:48:35,399:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6946 (0.9991)
2022-11-09 17:48:35,421:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0492 (1.0046)
2022-11-09 17:48:35,664:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9429 (0.9429)
2022-11-09 17:48:35,690:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0251 (0.9862)
2022-11-09 17:48:35,715:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7478 (0.9129)
2022-11-09 17:48:35,743:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5534 (0.8241)
2022-11-09 17:48:35,767:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5571 (0.7776)
2022-11-09 17:48:35,795:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5095 (0.7325)
2022-11-09 17:48:35,818:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4985 (0.6965)
2022-11-09 17:48:35,842:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8596 (0.7165)
2022-11-09 17:48:35,866:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7323 (0.7183)
2022-11-09 17:48:35,892:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8069 (0.7268)
2022-11-09 17:48:35,917:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4860 (0.7051)
2022-11-09 17:48:35,944:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5301 (0.6912)
2022-11-09 17:48:35,968:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6071 (0.6850)
2022-11-09 17:48:35,992:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4408 (0.6673)
2022-11-09 17:48:36,018:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7541 (0.6730)
2022-11-09 17:48:36,043:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4853 (0.6618)
2022-11-09 17:48:36,068:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4933 (0.6515)
2022-11-09 17:48:36,092:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4120 (0.6392)
2022-11-09 17:48:36,143:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_147.pth.tar
2022-11-09 17:48:36,143:INFO: 
===> EPOCH: 148 (P1)
2022-11-09 17:48:36,143:INFO: - Computing loss (training)
2022-11-09 17:48:36,356:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9207 (0.9207)
2022-11-09 17:48:36,382:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2442 (1.0821)
2022-11-09 17:48:36,407:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7981 (0.9903)
2022-11-09 17:48:36,425:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3711 (0.8816)
2022-11-09 17:48:36,682:INFO: Dataset: univ                Batch:  1/15	Loss 0.6401 (0.6401)
2022-11-09 17:48:36,713:INFO: Dataset: univ                Batch:  2/15	Loss 0.4603 (0.5523)
2022-11-09 17:48:36,742:INFO: Dataset: univ                Batch:  3/15	Loss 0.9529 (0.6867)
2022-11-09 17:48:36,768:INFO: Dataset: univ                Batch:  4/15	Loss 0.5135 (0.6462)
2022-11-09 17:48:36,795:INFO: Dataset: univ                Batch:  5/15	Loss 0.4792 (0.6129)
2022-11-09 17:48:36,824:INFO: Dataset: univ                Batch:  6/15	Loss 0.5942 (0.6096)
2022-11-09 17:48:36,850:INFO: Dataset: univ                Batch:  7/15	Loss 0.5384 (0.5988)
2022-11-09 17:48:36,876:INFO: Dataset: univ                Batch:  8/15	Loss 0.5879 (0.5976)
2022-11-09 17:48:36,903:INFO: Dataset: univ                Batch:  9/15	Loss 0.4711 (0.5829)
2022-11-09 17:48:36,929:INFO: Dataset: univ                Batch: 10/15	Loss 0.4574 (0.5701)
2022-11-09 17:48:36,959:INFO: Dataset: univ                Batch: 11/15	Loss 0.6026 (0.5730)
2022-11-09 17:48:36,985:INFO: Dataset: univ                Batch: 12/15	Loss 0.5490 (0.5708)
2022-11-09 17:48:37,012:INFO: Dataset: univ                Batch: 13/15	Loss 0.4361 (0.5598)
2022-11-09 17:48:37,038:INFO: Dataset: univ                Batch: 14/15	Loss 0.4616 (0.5530)
2022-11-09 17:48:37,047:INFO: Dataset: univ                Batch: 15/15	Loss 0.1205 (0.5474)
2022-11-09 17:48:37,306:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5000 (0.5000)
2022-11-09 17:48:37,331:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9647 (0.7229)
2022-11-09 17:48:37,355:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5286 (0.6588)
2022-11-09 17:48:37,379:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5743 (0.6350)
2022-11-09 17:48:37,403:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5379 (0.6164)
2022-11-09 17:48:37,429:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5286 (0.6018)
2022-11-09 17:48:37,452:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9090 (0.6456)
2022-11-09 17:48:37,473:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5624 (0.6358)
2022-11-09 17:48:37,735:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9006 (0.9006)
2022-11-09 17:48:37,760:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5788 (0.7383)
2022-11-09 17:48:37,788:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7509 (0.7424)
2022-11-09 17:48:37,812:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6865 (0.7287)
2022-11-09 17:48:37,838:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4783 (0.6762)
2022-11-09 17:48:37,865:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4735 (0.6435)
2022-11-09 17:48:37,889:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6200 (0.6403)
2022-11-09 17:48:37,913:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5294 (0.6274)
2022-11-09 17:48:37,937:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6239 (0.6270)
2022-11-09 17:48:37,962:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7253 (0.6364)
2022-11-09 17:48:37,986:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0415 (0.6727)
2022-11-09 17:48:38,013:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5901 (0.6655)
2022-11-09 17:48:38,037:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5109 (0.6530)
2022-11-09 17:48:38,061:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4472 (0.6378)
2022-11-09 17:48:38,086:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4879 (0.6278)
2022-11-09 17:48:38,111:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4899 (0.6192)
2022-11-09 17:48:38,136:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5311 (0.6145)
2022-11-09 17:48:38,159:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3949 (0.6029)
2022-11-09 17:48:38,209:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_148.pth.tar
2022-11-09 17:48:38,209:INFO: 
===> EPOCH: 149 (P1)
2022-11-09 17:48:38,210:INFO: - Computing loss (training)
2022-11-09 17:48:38,419:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4671 (1.4671)
2022-11-09 17:48:38,446:INFO: Dataset: hotel               Batch: 2/4	Loss 4.2950 (2.8845)
2022-11-09 17:48:38,470:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2192 (2.3321)
2022-11-09 17:48:38,487:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7156 (2.0335)
2022-11-09 17:48:38,831:INFO: Dataset: univ                Batch:  1/15	Loss 0.6553 (0.6553)
2022-11-09 17:48:38,862:INFO: Dataset: univ                Batch:  2/15	Loss 0.5367 (0.5985)
2022-11-09 17:48:38,889:INFO: Dataset: univ                Batch:  3/15	Loss 0.6705 (0.6215)
2022-11-09 17:48:38,915:INFO: Dataset: univ                Batch:  4/15	Loss 0.6180 (0.6206)
2022-11-09 17:48:38,941:INFO: Dataset: univ                Batch:  5/15	Loss 0.5536 (0.6071)
2022-11-09 17:48:38,969:INFO: Dataset: univ                Batch:  6/15	Loss 0.7950 (0.6412)
2022-11-09 17:48:38,995:INFO: Dataset: univ                Batch:  7/15	Loss 1.2523 (0.7325)
2022-11-09 17:48:39,020:INFO: Dataset: univ                Batch:  8/15	Loss 0.5241 (0.7087)
2022-11-09 17:48:39,047:INFO: Dataset: univ                Batch:  9/15	Loss 0.5064 (0.6879)
2022-11-09 17:48:39,073:INFO: Dataset: univ                Batch: 10/15	Loss 0.5102 (0.6697)
2022-11-09 17:48:39,103:INFO: Dataset: univ                Batch: 11/15	Loss 0.6008 (0.6635)
2022-11-09 17:48:39,129:INFO: Dataset: univ                Batch: 12/15	Loss 0.5032 (0.6499)
2022-11-09 17:48:39,156:INFO: Dataset: univ                Batch: 13/15	Loss 0.4731 (0.6369)
2022-11-09 17:48:39,183:INFO: Dataset: univ                Batch: 14/15	Loss 0.4831 (0.6251)
2022-11-09 17:48:39,193:INFO: Dataset: univ                Batch: 15/15	Loss 0.1714 (0.6202)
2022-11-09 17:48:39,453:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8148 (1.8148)
2022-11-09 17:48:39,481:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1627 (1.4854)
2022-11-09 17:48:39,506:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5681 (1.5132)
2022-11-09 17:48:39,531:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1891 (1.4356)
2022-11-09 17:48:39,558:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4934 (1.2530)
2022-11-09 17:48:39,584:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5656 (1.1261)
2022-11-09 17:48:39,609:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9538 (1.1028)
2022-11-09 17:48:39,631:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6768 (1.1663)
2022-11-09 17:48:39,878:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8002 (0.8002)
2022-11-09 17:48:39,917:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7838 (0.7920)
2022-11-09 17:48:39,942:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7275 (0.7697)
2022-11-09 17:48:39,969:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4780 (0.6951)
2022-11-09 17:48:39,993:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4251 (0.6398)
2022-11-09 17:48:40,020:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6383 (0.6396)
2022-11-09 17:48:40,045:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4765 (0.6157)
2022-11-09 17:48:40,070:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5834 (0.6114)
2022-11-09 17:48:40,095:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9679 (0.6502)
2022-11-09 17:48:40,120:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5670 (0.6425)
2022-11-09 17:48:40,147:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6045 (0.6390)
2022-11-09 17:48:40,173:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6158 (0.6370)
2022-11-09 17:48:40,198:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5445 (0.6299)
2022-11-09 17:48:40,224:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5371 (0.6227)
2022-11-09 17:48:40,249:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4620 (0.6125)
2022-11-09 17:48:40,274:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5550 (0.6087)
2022-11-09 17:48:40,300:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5869 (0.6075)
2022-11-09 17:48:40,324:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5273 (0.6033)
2022-11-09 17:48:40,376:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_149.pth.tar
2022-11-09 17:48:40,376:INFO: 
===> EPOCH: 150 (P1)
2022-11-09 17:48:40,376:INFO: - Computing loss (training)
2022-11-09 17:48:40,597:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7443 (1.7443)
2022-11-09 17:48:40,622:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8544 (1.7995)
2022-11-09 17:48:40,648:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7318 (1.4577)
2022-11-09 17:48:40,664:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3784 (1.2769)
2022-11-09 17:48:40,972:INFO: Dataset: univ                Batch:  1/15	Loss 0.4710 (0.4710)
2022-11-09 17:48:40,999:INFO: Dataset: univ                Batch:  2/15	Loss 0.6757 (0.5599)
2022-11-09 17:48:41,028:INFO: Dataset: univ                Batch:  3/15	Loss 0.4947 (0.5377)
2022-11-09 17:48:41,055:INFO: Dataset: univ                Batch:  4/15	Loss 0.4897 (0.5258)
2022-11-09 17:48:41,082:INFO: Dataset: univ                Batch:  5/15	Loss 0.8952 (0.6065)
2022-11-09 17:48:41,112:INFO: Dataset: univ                Batch:  6/15	Loss 0.6551 (0.6155)
2022-11-09 17:48:41,139:INFO: Dataset: univ                Batch:  7/15	Loss 0.4845 (0.5973)
2022-11-09 17:48:41,166:INFO: Dataset: univ                Batch:  8/15	Loss 0.4847 (0.5846)
2022-11-09 17:48:41,193:INFO: Dataset: univ                Batch:  9/15	Loss 0.4689 (0.5721)
2022-11-09 17:48:41,221:INFO: Dataset: univ                Batch: 10/15	Loss 0.4503 (0.5605)
2022-11-09 17:48:41,249:INFO: Dataset: univ                Batch: 11/15	Loss 1.1104 (0.6100)
2022-11-09 17:48:41,278:INFO: Dataset: univ                Batch: 12/15	Loss 0.4723 (0.5972)
2022-11-09 17:48:41,305:INFO: Dataset: univ                Batch: 13/15	Loss 0.8030 (0.6135)
2022-11-09 17:48:41,334:INFO: Dataset: univ                Batch: 14/15	Loss 0.4776 (0.6020)
2022-11-09 17:48:41,344:INFO: Dataset: univ                Batch: 15/15	Loss 0.0858 (0.5943)
2022-11-09 17:48:41,595:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6929 (0.6929)
2022-11-09 17:48:41,621:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6586 (0.6759)
2022-11-09 17:48:41,645:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5745 (0.6441)
2022-11-09 17:48:41,669:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4736 (0.5968)
2022-11-09 17:48:41,694:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5637 (0.5903)
2022-11-09 17:48:41,720:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5386 (0.5808)
2022-11-09 17:48:41,744:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7833 (0.6104)
2022-11-09 17:48:41,765:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6269 (0.6120)
2022-11-09 17:48:42,019:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0642 (1.0642)
2022-11-09 17:48:42,045:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5551 (0.8276)
2022-11-09 17:48:42,071:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5825 (0.7498)
2022-11-09 17:48:42,095:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4858 (0.6842)
2022-11-09 17:48:42,123:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4893 (0.6464)
2022-11-09 17:48:42,149:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4372 (0.6107)
2022-11-09 17:48:42,173:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7338 (0.6274)
2022-11-09 17:48:42,197:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1977 (0.7053)
2022-11-09 17:48:42,221:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8843 (0.7248)
2022-11-09 17:48:42,246:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6864 (0.7207)
2022-11-09 17:48:42,273:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5196 (0.7030)
2022-11-09 17:48:42,298:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4787 (0.6855)
2022-11-09 17:48:42,322:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4051 (0.6630)
2022-11-09 17:48:42,346:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4819 (0.6500)
2022-11-09 17:48:42,371:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5072 (0.6409)
2022-11-09 17:48:42,396:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5999 (0.6386)
2022-11-09 17:48:42,421:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4707 (0.6289)
2022-11-09 17:48:42,443:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4312 (0.6179)
2022-11-09 17:48:42,492:INFO:  --> Model Saved in ./models/E4//P1/CRMF_epoch_150.pth.tar
2022-11-09 17:48:42,492:INFO: 
===> EPOCH: 151 (P2)
2022-11-09 17:48:42,492:INFO: - Computing loss (training)
2022-11-09 17:48:42,844:INFO: Dataset: hotel               Batch: 1/4	Loss 21.2717 (21.2717)
2022-11-09 17:48:43,004:INFO: Dataset: hotel               Batch: 2/4	Loss 15.3720 (18.1467)
2022-11-09 17:48:43,163:INFO: Dataset: hotel               Batch: 3/4	Loss 15.1338 (17.1126)
2022-11-09 17:48:43,264:INFO: Dataset: hotel               Batch: 4/4	Loss 6.9391 (15.5423)
2022-11-09 17:48:43,666:INFO: Dataset: univ                Batch:  1/15	Loss 12.1069 (12.1069)
2022-11-09 17:48:43,844:INFO: Dataset: univ                Batch:  2/15	Loss 14.7326 (13.2504)
2022-11-09 17:48:44,010:INFO: Dataset: univ                Batch:  3/15	Loss 11.7095 (12.7385)
2022-11-09 17:48:44,175:INFO: Dataset: univ                Batch:  4/15	Loss 11.2319 (12.3499)
2022-11-09 17:48:44,342:INFO: Dataset: univ                Batch:  5/15	Loss 11.2620 (12.1381)
2022-11-09 17:48:44,508:INFO: Dataset: univ                Batch:  6/15	Loss 10.8591 (11.9342)
2022-11-09 17:48:44,674:INFO: Dataset: univ                Batch:  7/15	Loss 12.8794 (12.0650)
2022-11-09 17:48:44,839:INFO: Dataset: univ                Batch:  8/15	Loss 10.2620 (11.8243)
2022-11-09 17:48:45,006:INFO: Dataset: univ                Batch:  9/15	Loss 7.5340 (11.2838)
2022-11-09 17:48:45,170:INFO: Dataset: univ                Batch: 10/15	Loss 9.6078 (11.1160)
2022-11-09 17:48:45,337:INFO: Dataset: univ                Batch: 11/15	Loss 9.8758 (11.0074)
2022-11-09 17:48:45,502:INFO: Dataset: univ                Batch: 12/15	Loss 7.7464 (10.7451)
2022-11-09 17:48:45,669:INFO: Dataset: univ                Batch: 13/15	Loss 8.3270 (10.5614)
2022-11-09 17:48:45,835:INFO: Dataset: univ                Batch: 14/15	Loss 8.8444 (10.4450)
2022-11-09 17:48:45,878:INFO: Dataset: univ                Batch: 15/15	Loss 1.6433 (10.3428)
2022-11-09 17:48:46,270:INFO: Dataset: zara1               Batch: 1/8	Loss 18.9125 (18.9125)
2022-11-09 17:48:46,432:INFO: Dataset: zara1               Batch: 2/8	Loss 18.9930 (18.9550)
2022-11-09 17:48:46,593:INFO: Dataset: zara1               Batch: 3/8	Loss 11.8823 (16.6351)
2022-11-09 17:48:46,753:INFO: Dataset: zara1               Batch: 4/8	Loss 16.3706 (16.5710)
2022-11-09 17:48:46,914:INFO: Dataset: zara1               Batch: 5/8	Loss 11.4009 (15.6240)
2022-11-09 17:48:47,072:INFO: Dataset: zara1               Batch: 6/8	Loss 14.2739 (15.4102)
2022-11-09 17:48:47,233:INFO: Dataset: zara1               Batch: 7/8	Loss 10.8732 (14.7391)
2022-11-09 17:48:47,369:INFO: Dataset: zara1               Batch: 8/8	Loss 9.9692 (14.2119)
2022-11-09 17:48:47,757:INFO: Dataset: zara2               Batch:  1/18	Loss 8.1085 (8.1085)
2022-11-09 17:48:47,920:INFO: Dataset: zara2               Batch:  2/18	Loss 8.7563 (8.4479)
2022-11-09 17:48:48,083:INFO: Dataset: zara2               Batch:  3/18	Loss 9.0506 (8.6537)
2022-11-09 17:48:48,247:INFO: Dataset: zara2               Batch:  4/18	Loss 8.2896 (8.5573)
2022-11-09 17:48:48,412:INFO: Dataset: zara2               Batch:  5/18	Loss 11.5109 (9.1133)
2022-11-09 17:48:48,573:INFO: Dataset: zara2               Batch:  6/18	Loss 8.3215 (8.9910)
2022-11-09 17:48:48,736:INFO: Dataset: zara2               Batch:  7/18	Loss 7.6969 (8.8160)
2022-11-09 17:48:48,895:INFO: Dataset: zara2               Batch:  8/18	Loss 8.5624 (8.7869)
2022-11-09 17:48:49,056:INFO: Dataset: zara2               Batch:  9/18	Loss 8.0573 (8.7066)
2022-11-09 17:48:49,216:INFO: Dataset: zara2               Batch: 10/18	Loss 8.2166 (8.6598)
2022-11-09 17:48:49,377:INFO: Dataset: zara2               Batch: 11/18	Loss 6.6635 (8.4777)
2022-11-09 17:48:49,537:INFO: Dataset: zara2               Batch: 12/18	Loss 8.9032 (8.5116)
2022-11-09 17:48:49,699:INFO: Dataset: zara2               Batch: 13/18	Loss 7.5859 (8.4419)
2022-11-09 17:48:49,859:INFO: Dataset: zara2               Batch: 14/18	Loss 8.8744 (8.4724)
2022-11-09 17:48:50,024:INFO: Dataset: zara2               Batch: 15/18	Loss 7.2320 (8.3927)
2022-11-09 17:48:50,186:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2728 (8.3853)
2022-11-09 17:48:50,350:INFO: Dataset: zara2               Batch: 17/18	Loss 8.4079 (8.3865)
2022-11-09 17:48:50,490:INFO: Dataset: zara2               Batch: 18/18	Loss 7.5961 (8.3521)
2022-11-09 17:48:50,551:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_151.pth.tar
2022-11-09 17:48:50,552:INFO: 
===> EPOCH: 152 (P2)
2022-11-09 17:48:50,552:INFO: - Computing loss (training)
2022-11-09 17:48:50,893:INFO: Dataset: hotel               Batch: 1/4	Loss 5.7730 (5.7730)
2022-11-09 17:48:51,056:INFO: Dataset: hotel               Batch: 2/4	Loss 4.9523 (5.3636)
2022-11-09 17:48:51,218:INFO: Dataset: hotel               Batch: 3/4	Loss 5.5513 (5.4239)
2022-11-09 17:48:51,317:INFO: Dataset: hotel               Batch: 4/4	Loss 3.3465 (5.1115)
2022-11-09 17:48:51,725:INFO: Dataset: univ                Batch:  1/15	Loss 7.1035 (7.1035)
2022-11-09 17:48:51,891:INFO: Dataset: univ                Batch:  2/15	Loss 5.8685 (6.4619)
2022-11-09 17:48:52,057:INFO: Dataset: univ                Batch:  3/15	Loss 7.0475 (6.6518)
2022-11-09 17:48:52,218:INFO: Dataset: univ                Batch:  4/15	Loss 6.4089 (6.5884)
2022-11-09 17:48:52,384:INFO: Dataset: univ                Batch:  5/15	Loss 7.8976 (6.8210)
2022-11-09 17:48:52,547:INFO: Dataset: univ                Batch:  6/15	Loss 6.3097 (6.7373)
2022-11-09 17:48:52,711:INFO: Dataset: univ                Batch:  7/15	Loss 7.8658 (6.8865)
2022-11-09 17:48:52,873:INFO: Dataset: univ                Batch:  8/15	Loss 5.0618 (6.6219)
2022-11-09 17:48:53,036:INFO: Dataset: univ                Batch:  9/15	Loss 5.5279 (6.4869)
2022-11-09 17:48:53,197:INFO: Dataset: univ                Batch: 10/15	Loss 6.6102 (6.4998)
2022-11-09 17:48:53,361:INFO: Dataset: univ                Batch: 11/15	Loss 5.8290 (6.4382)
2022-11-09 17:48:53,523:INFO: Dataset: univ                Batch: 12/15	Loss 6.0775 (6.4104)
2022-11-09 17:48:53,687:INFO: Dataset: univ                Batch: 13/15	Loss 6.0047 (6.3777)
2022-11-09 17:48:53,851:INFO: Dataset: univ                Batch: 14/15	Loss 5.1065 (6.2850)
2022-11-09 17:48:53,892:INFO: Dataset: univ                Batch: 15/15	Loss 1.3167 (6.2186)
2022-11-09 17:48:54,289:INFO: Dataset: zara1               Batch: 1/8	Loss 12.2804 (12.2804)
2022-11-09 17:48:54,455:INFO: Dataset: zara1               Batch: 2/8	Loss 12.9306 (12.6330)
2022-11-09 17:48:54,617:INFO: Dataset: zara1               Batch: 3/8	Loss 12.1659 (12.4780)
2022-11-09 17:48:54,779:INFO: Dataset: zara1               Batch: 4/8	Loss 11.8657 (12.3142)
2022-11-09 17:48:54,945:INFO: Dataset: zara1               Batch: 5/8	Loss 9.5340 (11.7020)
2022-11-09 17:48:55,108:INFO: Dataset: zara1               Batch: 6/8	Loss 9.7909 (11.3434)
2022-11-09 17:48:55,271:INFO: Dataset: zara1               Batch: 7/8	Loss 9.4046 (11.0886)
2022-11-09 17:48:55,412:INFO: Dataset: zara1               Batch: 8/8	Loss 9.5567 (10.9370)
2022-11-09 17:48:55,804:INFO: Dataset: zara2               Batch:  1/18	Loss 8.1846 (8.1846)
2022-11-09 17:48:55,966:INFO: Dataset: zara2               Batch:  2/18	Loss 8.9030 (8.5673)
2022-11-09 17:48:56,134:INFO: Dataset: zara2               Batch:  3/18	Loss 8.5328 (8.5553)
2022-11-09 17:48:56,297:INFO: Dataset: zara2               Batch:  4/18	Loss 9.9174 (8.8926)
2022-11-09 17:48:56,463:INFO: Dataset: zara2               Batch:  5/18	Loss 8.8035 (8.8731)
2022-11-09 17:48:56,625:INFO: Dataset: zara2               Batch:  6/18	Loss 8.3945 (8.8041)
2022-11-09 17:48:56,790:INFO: Dataset: zara2               Batch:  7/18	Loss 7.5519 (8.6168)
2022-11-09 17:48:56,950:INFO: Dataset: zara2               Batch:  8/18	Loss 7.6758 (8.5076)
2022-11-09 17:48:57,114:INFO: Dataset: zara2               Batch:  9/18	Loss 8.0305 (8.4579)
2022-11-09 17:48:57,275:INFO: Dataset: zara2               Batch: 10/18	Loss 8.0994 (8.4254)
2022-11-09 17:48:57,442:INFO: Dataset: zara2               Batch: 11/18	Loss 6.9370 (8.2811)
2022-11-09 17:48:57,602:INFO: Dataset: zara2               Batch: 12/18	Loss 7.4555 (8.2140)
2022-11-09 17:48:57,767:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9736 (8.1270)
2022-11-09 17:48:57,929:INFO: Dataset: zara2               Batch: 14/18	Loss 8.4020 (8.1464)
2022-11-09 17:48:58,096:INFO: Dataset: zara2               Batch: 15/18	Loss 7.3056 (8.0828)
2022-11-09 17:48:58,259:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2614 (8.0945)
2022-11-09 17:48:58,425:INFO: Dataset: zara2               Batch: 17/18	Loss 6.9222 (8.0258)
2022-11-09 17:48:58,566:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1737 (7.8759)
2022-11-09 17:48:58,619:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_152.pth.tar
2022-11-09 17:48:58,619:INFO: 
===> EPOCH: 153 (P2)
2022-11-09 17:48:58,619:INFO: - Computing loss (training)
2022-11-09 17:48:58,971:INFO: Dataset: hotel               Batch: 1/4	Loss 7.5873 (7.5873)
2022-11-09 17:48:59,134:INFO: Dataset: hotel               Batch: 2/4	Loss 5.1735 (6.3083)
2022-11-09 17:48:59,294:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7817 (5.4279)
2022-11-09 17:48:59,394:INFO: Dataset: hotel               Batch: 4/4	Loss 2.3348 (4.8525)
2022-11-09 17:48:59,796:INFO: Dataset: univ                Batch:  1/15	Loss 6.2897 (6.2897)
2022-11-09 17:48:59,965:INFO: Dataset: univ                Batch:  2/15	Loss 6.1110 (6.1992)
2022-11-09 17:49:00,133:INFO: Dataset: univ                Batch:  3/15	Loss 6.3145 (6.2360)
2022-11-09 17:49:00,309:INFO: Dataset: univ                Batch:  4/15	Loss 5.8972 (6.1523)
2022-11-09 17:49:00,475:INFO: Dataset: univ                Batch:  5/15	Loss 6.0622 (6.1352)
2022-11-09 17:49:00,642:INFO: Dataset: univ                Batch:  6/15	Loss 5.0201 (5.9466)
2022-11-09 17:49:00,808:INFO: Dataset: univ                Batch:  7/15	Loss 6.1090 (5.9699)
2022-11-09 17:49:00,973:INFO: Dataset: univ                Batch:  8/15	Loss 5.2738 (5.8728)
2022-11-09 17:49:01,139:INFO: Dataset: univ                Batch:  9/15	Loss 5.9817 (5.8845)
2022-11-09 17:49:01,311:INFO: Dataset: univ                Batch: 10/15	Loss 5.2071 (5.8108)
2022-11-09 17:49:01,479:INFO: Dataset: univ                Batch: 11/15	Loss 5.4591 (5.7803)
2022-11-09 17:49:01,643:INFO: Dataset: univ                Batch: 12/15	Loss 5.0522 (5.7206)
2022-11-09 17:49:01,809:INFO: Dataset: univ                Batch: 13/15	Loss 4.9381 (5.6536)
2022-11-09 17:49:01,973:INFO: Dataset: univ                Batch: 14/15	Loss 5.1314 (5.6128)
2022-11-09 17:49:02,018:INFO: Dataset: univ                Batch: 15/15	Loss 0.7182 (5.5279)
2022-11-09 17:49:02,422:INFO: Dataset: zara1               Batch: 1/8	Loss 13.2470 (13.2470)
2022-11-09 17:49:02,585:INFO: Dataset: zara1               Batch: 2/8	Loss 12.3409 (12.8068)
2022-11-09 17:49:02,748:INFO: Dataset: zara1               Batch: 3/8	Loss 11.2103 (12.2616)
2022-11-09 17:49:02,906:INFO: Dataset: zara1               Batch: 4/8	Loss 9.8058 (11.6786)
2022-11-09 17:49:03,071:INFO: Dataset: zara1               Batch: 5/8	Loss 8.7162 (10.9801)
2022-11-09 17:49:03,230:INFO: Dataset: zara1               Batch: 6/8	Loss 9.5377 (10.7233)
2022-11-09 17:49:03,393:INFO: Dataset: zara1               Batch: 7/8	Loss 10.7527 (10.7276)
2022-11-09 17:49:03,530:INFO: Dataset: zara1               Batch: 8/8	Loss 9.0338 (10.5511)
2022-11-09 17:49:03,924:INFO: Dataset: zara2               Batch:  1/18	Loss 10.0834 (10.0834)
2022-11-09 17:49:04,086:INFO: Dataset: zara2               Batch:  2/18	Loss 6.5535 (8.3159)
2022-11-09 17:49:04,248:INFO: Dataset: zara2               Batch:  3/18	Loss 6.8937 (7.8750)
2022-11-09 17:49:04,406:INFO: Dataset: zara2               Batch:  4/18	Loss 6.6561 (7.5419)
2022-11-09 17:49:04,566:INFO: Dataset: zara2               Batch:  5/18	Loss 7.9464 (7.6140)
2022-11-09 17:49:04,727:INFO: Dataset: zara2               Batch:  6/18	Loss 6.4991 (7.4435)
2022-11-09 17:49:04,887:INFO: Dataset: zara2               Batch:  7/18	Loss 6.7850 (7.3491)
2022-11-09 17:49:05,043:INFO: Dataset: zara2               Batch:  8/18	Loss 6.9059 (7.2923)
2022-11-09 17:49:05,202:INFO: Dataset: zara2               Batch:  9/18	Loss 6.5966 (7.2107)
2022-11-09 17:49:05,360:INFO: Dataset: zara2               Batch: 10/18	Loss 6.8256 (7.1689)
2022-11-09 17:49:05,520:INFO: Dataset: zara2               Batch: 11/18	Loss 6.6642 (7.1210)
2022-11-09 17:49:05,678:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8852 (7.1025)
2022-11-09 17:49:05,838:INFO: Dataset: zara2               Batch: 13/18	Loss 5.9757 (7.0140)
2022-11-09 17:49:05,996:INFO: Dataset: zara2               Batch: 14/18	Loss 8.6748 (7.1236)
2022-11-09 17:49:06,157:INFO: Dataset: zara2               Batch: 15/18	Loss 7.2558 (7.1318)
2022-11-09 17:49:06,316:INFO: Dataset: zara2               Batch: 16/18	Loss 5.8536 (7.0571)
2022-11-09 17:49:06,478:INFO: Dataset: zara2               Batch: 17/18	Loss 6.0149 (6.9931)
2022-11-09 17:49:06,616:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1992 (6.9029)
2022-11-09 17:49:06,671:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_153.pth.tar
2022-11-09 17:49:06,671:INFO: 
===> EPOCH: 154 (P2)
2022-11-09 17:49:06,672:INFO: - Computing loss (training)
2022-11-09 17:49:07,012:INFO: Dataset: hotel               Batch: 1/4	Loss 3.8677 (3.8677)
2022-11-09 17:49:07,176:INFO: Dataset: hotel               Batch: 2/4	Loss 5.5185 (4.6671)
2022-11-09 17:49:07,335:INFO: Dataset: hotel               Batch: 3/4	Loss 4.4724 (4.6007)
2022-11-09 17:49:07,435:INFO: Dataset: hotel               Batch: 4/4	Loss 2.3853 (4.2178)
2022-11-09 17:49:07,828:INFO: Dataset: univ                Batch:  1/15	Loss 16.4771 (16.4771)
2022-11-09 17:49:07,997:INFO: Dataset: univ                Batch:  2/15	Loss 6.0838 (11.4338)
2022-11-09 17:49:08,165:INFO: Dataset: univ                Batch:  3/15	Loss 6.4864 (9.7835)
2022-11-09 17:49:08,328:INFO: Dataset: univ                Batch:  4/15	Loss 4.1202 (8.3053)
2022-11-09 17:49:08,495:INFO: Dataset: univ                Batch:  5/15	Loss 3.7826 (7.3706)
2022-11-09 17:49:08,738:INFO: Dataset: univ                Batch:  6/15	Loss 5.0191 (6.9773)
2022-11-09 17:49:08,900:INFO: Dataset: univ                Batch:  7/15	Loss 5.5152 (6.7614)
2022-11-09 17:49:09,066:INFO: Dataset: univ                Batch:  8/15	Loss 5.6502 (6.6260)
2022-11-09 17:49:09,228:INFO: Dataset: univ                Batch:  9/15	Loss 5.1439 (6.4527)
2022-11-09 17:49:09,396:INFO: Dataset: univ                Batch: 10/15	Loss 5.2263 (6.3255)
2022-11-09 17:49:09,557:INFO: Dataset: univ                Batch: 11/15	Loss 5.2857 (6.2296)
2022-11-09 17:49:09,726:INFO: Dataset: univ                Batch: 12/15	Loss 5.9852 (6.2112)
2022-11-09 17:49:09,890:INFO: Dataset: univ                Batch: 13/15	Loss 5.6957 (6.1700)
2022-11-09 17:49:10,059:INFO: Dataset: univ                Batch: 14/15	Loss 5.0696 (6.0917)
2022-11-09 17:49:10,101:INFO: Dataset: univ                Batch: 15/15	Loss 0.8216 (6.0243)
2022-11-09 17:49:10,502:INFO: Dataset: zara1               Batch: 1/8	Loss 10.9326 (10.9326)
2022-11-09 17:49:10,666:INFO: Dataset: zara1               Batch: 2/8	Loss 12.8166 (11.8366)
2022-11-09 17:49:10,828:INFO: Dataset: zara1               Batch: 3/8	Loss 8.5199 (10.6628)
2022-11-09 17:49:10,987:INFO: Dataset: zara1               Batch: 4/8	Loss 8.1040 (10.0172)
2022-11-09 17:49:11,150:INFO: Dataset: zara1               Batch: 5/8	Loss 8.7493 (9.7642)
2022-11-09 17:49:11,311:INFO: Dataset: zara1               Batch: 6/8	Loss 8.7719 (9.6054)
2022-11-09 17:49:11,473:INFO: Dataset: zara1               Batch: 7/8	Loss 13.7476 (10.1979)
2022-11-09 17:49:11,611:INFO: Dataset: zara1               Batch: 8/8	Loss 8.1653 (9.9754)
2022-11-09 17:49:11,998:INFO: Dataset: zara2               Batch:  1/18	Loss 11.9607 (11.9607)
2022-11-09 17:49:12,158:INFO: Dataset: zara2               Batch:  2/18	Loss 9.8755 (10.9074)
2022-11-09 17:49:12,321:INFO: Dataset: zara2               Batch:  3/18	Loss 5.8517 (9.2672)
2022-11-09 17:49:12,480:INFO: Dataset: zara2               Batch:  4/18	Loss 6.8578 (8.6934)
2022-11-09 17:49:12,643:INFO: Dataset: zara2               Batch:  5/18	Loss 6.0498 (8.1682)
2022-11-09 17:49:12,804:INFO: Dataset: zara2               Batch:  6/18	Loss 5.9203 (7.7919)
2022-11-09 17:49:12,964:INFO: Dataset: zara2               Batch:  7/18	Loss 8.5405 (7.9098)
2022-11-09 17:49:13,121:INFO: Dataset: zara2               Batch:  8/18	Loss 7.3004 (7.8442)
2022-11-09 17:49:13,282:INFO: Dataset: zara2               Batch:  9/18	Loss 7.4455 (7.7970)
2022-11-09 17:49:13,440:INFO: Dataset: zara2               Batch: 10/18	Loss 8.6343 (7.8827)
2022-11-09 17:49:13,600:INFO: Dataset: zara2               Batch: 11/18	Loss 7.8199 (7.8762)
2022-11-09 17:49:13,760:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8929 (7.7978)
2022-11-09 17:49:13,920:INFO: Dataset: zara2               Batch: 13/18	Loss 5.2377 (7.5959)
2022-11-09 17:49:14,081:INFO: Dataset: zara2               Batch: 14/18	Loss 6.1020 (7.4990)
2022-11-09 17:49:14,246:INFO: Dataset: zara2               Batch: 15/18	Loss 6.2866 (7.4101)
2022-11-09 17:49:14,407:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4936 (7.3526)
2022-11-09 17:49:14,568:INFO: Dataset: zara2               Batch: 17/18	Loss 8.3946 (7.4157)
2022-11-09 17:49:14,707:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1360 (7.3087)
2022-11-09 17:49:14,760:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_154.pth.tar
2022-11-09 17:49:14,760:INFO: 
===> EPOCH: 155 (P2)
2022-11-09 17:49:14,761:INFO: - Computing loss (training)
2022-11-09 17:49:15,112:INFO: Dataset: hotel               Batch: 1/4	Loss 4.0751 (4.0751)
2022-11-09 17:49:15,274:INFO: Dataset: hotel               Batch: 2/4	Loss 4.0495 (4.0626)
2022-11-09 17:49:15,432:INFO: Dataset: hotel               Batch: 3/4	Loss 3.8757 (3.9999)
2022-11-09 17:49:15,530:INFO: Dataset: hotel               Batch: 4/4	Loss 3.5456 (3.9208)
2022-11-09 17:49:15,942:INFO: Dataset: univ                Batch:  1/15	Loss 7.7909 (7.7909)
2022-11-09 17:49:16,107:INFO: Dataset: univ                Batch:  2/15	Loss 5.8513 (6.7942)
2022-11-09 17:49:16,276:INFO: Dataset: univ                Batch:  3/15	Loss 4.8368 (6.1088)
2022-11-09 17:49:16,439:INFO: Dataset: univ                Batch:  4/15	Loss 4.7747 (5.7655)
2022-11-09 17:49:16,604:INFO: Dataset: univ                Batch:  5/15	Loss 8.4995 (6.3032)
2022-11-09 17:49:16,774:INFO: Dataset: univ                Batch:  6/15	Loss 4.1326 (5.9002)
2022-11-09 17:49:16,940:INFO: Dataset: univ                Batch:  7/15	Loss 5.2662 (5.8119)
2022-11-09 17:49:17,102:INFO: Dataset: univ                Batch:  8/15	Loss 5.9280 (5.8257)
2022-11-09 17:49:17,269:INFO: Dataset: univ                Batch:  9/15	Loss 4.1968 (5.6348)
2022-11-09 17:49:17,432:INFO: Dataset: univ                Batch: 10/15	Loss 4.7441 (5.5508)
2022-11-09 17:49:17,599:INFO: Dataset: univ                Batch: 11/15	Loss 5.2933 (5.5271)
2022-11-09 17:49:17,765:INFO: Dataset: univ                Batch: 12/15	Loss 5.3446 (5.5117)
2022-11-09 17:49:17,932:INFO: Dataset: univ                Batch: 13/15	Loss 5.3424 (5.4989)
2022-11-09 17:49:18,097:INFO: Dataset: univ                Batch: 14/15	Loss 4.8768 (5.4563)
2022-11-09 17:49:18,140:INFO: Dataset: univ                Batch: 15/15	Loss 1.0750 (5.3944)
2022-11-09 17:49:18,526:INFO: Dataset: zara1               Batch: 1/8	Loss 14.6855 (14.6855)
2022-11-09 17:49:18,686:INFO: Dataset: zara1               Batch: 2/8	Loss 12.3302 (13.5130)
2022-11-09 17:49:18,849:INFO: Dataset: zara1               Batch: 3/8	Loss 7.4378 (11.5483)
2022-11-09 17:49:19,005:INFO: Dataset: zara1               Batch: 4/8	Loss 8.4431 (10.6503)
2022-11-09 17:49:19,164:INFO: Dataset: zara1               Batch: 5/8	Loss 8.8461 (10.2666)
2022-11-09 17:49:19,323:INFO: Dataset: zara1               Batch: 6/8	Loss 8.4228 (9.9720)
2022-11-09 17:49:19,481:INFO: Dataset: zara1               Batch: 7/8	Loss 14.7871 (10.6328)
2022-11-09 17:49:19,617:INFO: Dataset: zara1               Batch: 8/8	Loss 7.8253 (10.2693)
2022-11-09 17:49:20,007:INFO: Dataset: zara2               Batch:  1/18	Loss 8.5589 (8.5589)
2022-11-09 17:49:20,168:INFO: Dataset: zara2               Batch:  2/18	Loss 7.8567 (8.2185)
2022-11-09 17:49:20,336:INFO: Dataset: zara2               Batch:  3/18	Loss 7.0377 (7.8345)
2022-11-09 17:49:20,498:INFO: Dataset: zara2               Batch:  4/18	Loss 6.2049 (7.4132)
2022-11-09 17:49:20,661:INFO: Dataset: zara2               Batch:  5/18	Loss 5.6206 (7.0717)
2022-11-09 17:49:20,833:INFO: Dataset: zara2               Batch:  6/18	Loss 6.8920 (7.0423)
2022-11-09 17:49:20,996:INFO: Dataset: zara2               Batch:  7/18	Loss 7.2995 (7.0786)
2022-11-09 17:49:21,156:INFO: Dataset: zara2               Batch:  8/18	Loss 7.7681 (7.1667)
2022-11-09 17:49:21,319:INFO: Dataset: zara2               Batch:  9/18	Loss 6.2575 (7.0703)
2022-11-09 17:49:21,479:INFO: Dataset: zara2               Batch: 10/18	Loss 9.0359 (7.2608)
2022-11-09 17:49:21,645:INFO: Dataset: zara2               Batch: 11/18	Loss 7.4002 (7.2734)
2022-11-09 17:49:21,807:INFO: Dataset: zara2               Batch: 12/18	Loss 7.0058 (7.2514)
2022-11-09 17:49:21,971:INFO: Dataset: zara2               Batch: 13/18	Loss 6.4257 (7.1913)
2022-11-09 17:49:22,133:INFO: Dataset: zara2               Batch: 14/18	Loss 5.3900 (7.0539)
2022-11-09 17:49:22,299:INFO: Dataset: zara2               Batch: 15/18	Loss 6.0495 (6.9848)
2022-11-09 17:49:22,462:INFO: Dataset: zara2               Batch: 16/18	Loss 5.7050 (6.9019)
2022-11-09 17:49:22,627:INFO: Dataset: zara2               Batch: 17/18	Loss 5.9204 (6.8463)
2022-11-09 17:49:22,771:INFO: Dataset: zara2               Batch: 18/18	Loss 5.2447 (6.7701)
2022-11-09 17:49:22,823:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_155.pth.tar
2022-11-09 17:49:22,823:INFO: 
===> EPOCH: 156 (P2)
2022-11-09 17:49:22,823:INFO: - Computing loss (training)
2022-11-09 17:49:23,166:INFO: Dataset: hotel               Batch: 1/4	Loss 5.4855 (5.4855)
2022-11-09 17:49:23,330:INFO: Dataset: hotel               Batch: 2/4	Loss 6.5434 (5.9994)
2022-11-09 17:49:23,488:INFO: Dataset: hotel               Batch: 3/4	Loss 5.5717 (5.8591)
2022-11-09 17:49:23,587:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8624 (5.1737)
2022-11-09 17:49:23,981:INFO: Dataset: univ                Batch:  1/15	Loss 7.9906 (7.9906)
2022-11-09 17:49:24,146:INFO: Dataset: univ                Batch:  2/15	Loss 6.2888 (7.1537)
2022-11-09 17:49:24,311:INFO: Dataset: univ                Batch:  3/15	Loss 6.3540 (6.8928)
2022-11-09 17:49:24,477:INFO: Dataset: univ                Batch:  4/15	Loss 5.0138 (6.3968)
2022-11-09 17:49:24,639:INFO: Dataset: univ                Batch:  5/15	Loss 4.9558 (6.1026)
2022-11-09 17:49:24,803:INFO: Dataset: univ                Batch:  6/15	Loss 4.1535 (5.7861)
2022-11-09 17:49:24,966:INFO: Dataset: univ                Batch:  7/15	Loss 4.3157 (5.5681)
2022-11-09 17:49:25,127:INFO: Dataset: univ                Batch:  8/15	Loss 4.3864 (5.4191)
2022-11-09 17:49:25,291:INFO: Dataset: univ                Batch:  9/15	Loss 5.2778 (5.4035)
2022-11-09 17:49:25,451:INFO: Dataset: univ                Batch: 10/15	Loss 6.0298 (5.4595)
2022-11-09 17:49:25,616:INFO: Dataset: univ                Batch: 11/15	Loss 5.0388 (5.4233)
2022-11-09 17:49:25,779:INFO: Dataset: univ                Batch: 12/15	Loss 5.1967 (5.4056)
2022-11-09 17:49:25,942:INFO: Dataset: univ                Batch: 13/15	Loss 5.6192 (5.4222)
2022-11-09 17:49:26,105:INFO: Dataset: univ                Batch: 14/15	Loss 4.6249 (5.3614)
2022-11-09 17:49:26,146:INFO: Dataset: univ                Batch: 15/15	Loss 0.7408 (5.3106)
2022-11-09 17:49:26,542:INFO: Dataset: zara1               Batch: 1/8	Loss 8.8100 (8.8100)
2022-11-09 17:49:26,703:INFO: Dataset: zara1               Batch: 2/8	Loss 11.1265 (10.0207)
2022-11-09 17:49:26,861:INFO: Dataset: zara1               Batch: 3/8	Loss 11.1008 (10.3697)
2022-11-09 17:49:27,019:INFO: Dataset: zara1               Batch: 4/8	Loss 8.2177 (9.8562)
2022-11-09 17:49:27,177:INFO: Dataset: zara1               Batch: 5/8	Loss 6.7783 (9.1711)
2022-11-09 17:49:27,336:INFO: Dataset: zara1               Batch: 6/8	Loss 8.0496 (8.9706)
2022-11-09 17:49:27,494:INFO: Dataset: zara1               Batch: 7/8	Loss 11.1890 (9.2560)
2022-11-09 17:49:27,629:INFO: Dataset: zara1               Batch: 8/8	Loss 7.7575 (9.0943)
2022-11-09 17:49:28,029:INFO: Dataset: zara2               Batch:  1/18	Loss 12.8287 (12.8287)
2022-11-09 17:49:28,189:INFO: Dataset: zara2               Batch:  2/18	Loss 6.2647 (9.5745)
2022-11-09 17:49:28,350:INFO: Dataset: zara2               Batch:  3/18	Loss 5.8902 (8.3418)
2022-11-09 17:49:28,510:INFO: Dataset: zara2               Batch:  4/18	Loss 5.9792 (7.7557)
2022-11-09 17:49:28,672:INFO: Dataset: zara2               Batch:  5/18	Loss 5.9525 (7.3852)
2022-11-09 17:49:28,832:INFO: Dataset: zara2               Batch:  6/18	Loss 6.1152 (7.1801)
2022-11-09 17:49:28,992:INFO: Dataset: zara2               Batch:  7/18	Loss 5.4430 (6.9464)
2022-11-09 17:49:29,147:INFO: Dataset: zara2               Batch:  8/18	Loss 6.4598 (6.8909)
2022-11-09 17:49:29,309:INFO: Dataset: zara2               Batch:  9/18	Loss 5.7449 (6.7715)
2022-11-09 17:49:29,465:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1386 (6.7042)
2022-11-09 17:49:29,627:INFO: Dataset: zara2               Batch: 11/18	Loss 6.6451 (6.6994)
2022-11-09 17:49:29,785:INFO: Dataset: zara2               Batch: 12/18	Loss 5.1058 (6.5590)
2022-11-09 17:49:29,946:INFO: Dataset: zara2               Batch: 13/18	Loss 5.3870 (6.4739)
2022-11-09 17:49:30,103:INFO: Dataset: zara2               Batch: 14/18	Loss 5.5671 (6.4110)
2022-11-09 17:49:30,265:INFO: Dataset: zara2               Batch: 15/18	Loss 6.9876 (6.4484)
2022-11-09 17:49:30,424:INFO: Dataset: zara2               Batch: 16/18	Loss 5.9162 (6.4152)
2022-11-09 17:49:30,585:INFO: Dataset: zara2               Batch: 17/18	Loss 5.6232 (6.3734)
2022-11-09 17:49:30,722:INFO: Dataset: zara2               Batch: 18/18	Loss 5.0996 (6.3161)
2022-11-09 17:49:30,776:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_156.pth.tar
2022-11-09 17:49:30,776:INFO: 
===> EPOCH: 157 (P2)
2022-11-09 17:49:30,776:INFO: - Computing loss (training)
2022-11-09 17:49:31,127:INFO: Dataset: hotel               Batch: 1/4	Loss 3.6191 (3.6191)
2022-11-09 17:49:31,288:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5610 (3.5895)
2022-11-09 17:49:31,445:INFO: Dataset: hotel               Batch: 3/4	Loss 4.1367 (3.7696)
2022-11-09 17:49:31,544:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9419 (3.4803)
2022-11-09 17:49:31,933:INFO: Dataset: univ                Batch:  1/15	Loss 5.6210 (5.6210)
2022-11-09 17:49:32,098:INFO: Dataset: univ                Batch:  2/15	Loss 6.2323 (5.9182)
2022-11-09 17:49:32,264:INFO: Dataset: univ                Batch:  3/15	Loss 5.3560 (5.7256)
2022-11-09 17:49:32,432:INFO: Dataset: univ                Batch:  4/15	Loss 5.1459 (5.5821)
2022-11-09 17:49:32,595:INFO: Dataset: univ                Batch:  5/15	Loss 5.0777 (5.4827)
2022-11-09 17:49:32,760:INFO: Dataset: univ                Batch:  6/15	Loss 4.6467 (5.3414)
2022-11-09 17:49:32,923:INFO: Dataset: univ                Batch:  7/15	Loss 4.7158 (5.2464)
2022-11-09 17:49:33,083:INFO: Dataset: univ                Batch:  8/15	Loss 4.3680 (5.1353)
2022-11-09 17:49:33,246:INFO: Dataset: univ                Batch:  9/15	Loss 5.7168 (5.1957)
2022-11-09 17:49:33,407:INFO: Dataset: univ                Batch: 10/15	Loss 5.3585 (5.2125)
2022-11-09 17:49:33,570:INFO: Dataset: univ                Batch: 11/15	Loss 5.0923 (5.2018)
2022-11-09 17:49:33,731:INFO: Dataset: univ                Batch: 12/15	Loss 4.1312 (5.1105)
2022-11-09 17:49:33,895:INFO: Dataset: univ                Batch: 13/15	Loss 3.9881 (5.0161)
2022-11-09 17:49:34,058:INFO: Dataset: univ                Batch: 14/15	Loss 5.4192 (5.0433)
2022-11-09 17:49:34,100:INFO: Dataset: univ                Batch: 15/15	Loss 0.6803 (4.9869)
2022-11-09 17:49:34,496:INFO: Dataset: zara1               Batch: 1/8	Loss 13.3817 (13.3817)
2022-11-09 17:49:34,658:INFO: Dataset: zara1               Batch: 2/8	Loss 14.9334 (14.2156)
2022-11-09 17:49:34,817:INFO: Dataset: zara1               Batch: 3/8	Loss 16.7049 (15.0410)
2022-11-09 17:49:34,974:INFO: Dataset: zara1               Batch: 4/8	Loss 9.4253 (13.7803)
2022-11-09 17:49:35,133:INFO: Dataset: zara1               Batch: 5/8	Loss 7.1104 (12.4682)
2022-11-09 17:49:35,291:INFO: Dataset: zara1               Batch: 6/8	Loss 7.0486 (11.5434)
2022-11-09 17:49:35,449:INFO: Dataset: zara1               Batch: 7/8	Loss 7.4666 (10.9922)
2022-11-09 17:49:35,584:INFO: Dataset: zara1               Batch: 8/8	Loss 6.7124 (10.5439)
2022-11-09 17:49:35,979:INFO: Dataset: zara2               Batch:  1/18	Loss 7.9171 (7.9171)
2022-11-09 17:49:36,137:INFO: Dataset: zara2               Batch:  2/18	Loss 6.9354 (7.4643)
2022-11-09 17:49:36,306:INFO: Dataset: zara2               Batch:  3/18	Loss 8.0962 (7.6827)
2022-11-09 17:49:36,463:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0377 (7.5445)
2022-11-09 17:49:36,624:INFO: Dataset: zara2               Batch:  5/18	Loss 7.8731 (7.6131)
2022-11-09 17:49:36,785:INFO: Dataset: zara2               Batch:  6/18	Loss 8.2529 (7.7221)
2022-11-09 17:49:36,947:INFO: Dataset: zara2               Batch:  7/18	Loss 6.1047 (7.4844)
2022-11-09 17:49:37,104:INFO: Dataset: zara2               Batch:  8/18	Loss 6.3772 (7.3502)
2022-11-09 17:49:37,266:INFO: Dataset: zara2               Batch:  9/18	Loss 5.2187 (7.1070)
2022-11-09 17:49:37,423:INFO: Dataset: zara2               Batch: 10/18	Loss 5.3030 (6.9262)
2022-11-09 17:49:37,584:INFO: Dataset: zara2               Batch: 11/18	Loss 5.8480 (6.8349)
2022-11-09 17:49:37,744:INFO: Dataset: zara2               Batch: 12/18	Loss 5.1143 (6.6868)
2022-11-09 17:49:37,906:INFO: Dataset: zara2               Batch: 13/18	Loss 5.5589 (6.5940)
2022-11-09 17:49:38,064:INFO: Dataset: zara2               Batch: 14/18	Loss 5.0814 (6.4829)
2022-11-09 17:49:38,226:INFO: Dataset: zara2               Batch: 15/18	Loss 5.7919 (6.4354)
2022-11-09 17:49:38,386:INFO: Dataset: zara2               Batch: 16/18	Loss 5.6239 (6.3793)
2022-11-09 17:49:38,549:INFO: Dataset: zara2               Batch: 17/18	Loss 5.9557 (6.3532)
2022-11-09 17:49:38,688:INFO: Dataset: zara2               Batch: 18/18	Loss 4.3591 (6.2576)
2022-11-09 17:49:38,741:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_157.pth.tar
2022-11-09 17:49:38,741:INFO: 
===> EPOCH: 158 (P2)
2022-11-09 17:49:38,741:INFO: - Computing loss (training)
2022-11-09 17:49:39,099:INFO: Dataset: hotel               Batch: 1/4	Loss 11.7974 (11.7974)
2022-11-09 17:49:39,261:INFO: Dataset: hotel               Batch: 2/4	Loss 6.4612 (8.9641)
2022-11-09 17:49:39,418:INFO: Dataset: hotel               Batch: 3/4	Loss 3.6191 (7.1938)
2022-11-09 17:49:39,518:INFO: Dataset: hotel               Batch: 4/4	Loss 2.2185 (6.3405)
2022-11-09 17:49:39,913:INFO: Dataset: univ                Batch:  1/15	Loss 4.7490 (4.7490)
2022-11-09 17:49:40,079:INFO: Dataset: univ                Batch:  2/15	Loss 5.5649 (5.1630)
2022-11-09 17:49:40,245:INFO: Dataset: univ                Batch:  3/15	Loss 4.4422 (4.9184)
2022-11-09 17:49:40,407:INFO: Dataset: univ                Batch:  4/15	Loss 4.7206 (4.8679)
2022-11-09 17:49:40,574:INFO: Dataset: univ                Batch:  5/15	Loss 4.9999 (4.8933)
2022-11-09 17:49:40,738:INFO: Dataset: univ                Batch:  6/15	Loss 4.7125 (4.8653)
2022-11-09 17:49:40,902:INFO: Dataset: univ                Batch:  7/15	Loss 4.1878 (4.7710)
2022-11-09 17:49:41,064:INFO: Dataset: univ                Batch:  8/15	Loss 3.3953 (4.5914)
2022-11-09 17:49:41,227:INFO: Dataset: univ                Batch:  9/15	Loss 4.4615 (4.5778)
2022-11-09 17:49:41,390:INFO: Dataset: univ                Batch: 10/15	Loss 4.4428 (4.5644)
2022-11-09 17:49:41,556:INFO: Dataset: univ                Batch: 11/15	Loss 5.4356 (4.6403)
2022-11-09 17:49:41,719:INFO: Dataset: univ                Batch: 12/15	Loss 3.8073 (4.5652)
2022-11-09 17:49:41,883:INFO: Dataset: univ                Batch: 13/15	Loss 4.7378 (4.5778)
2022-11-09 17:49:42,046:INFO: Dataset: univ                Batch: 14/15	Loss 3.9841 (4.5316)
2022-11-09 17:49:42,087:INFO: Dataset: univ                Batch: 15/15	Loss 0.6382 (4.4904)
2022-11-09 17:49:42,479:INFO: Dataset: zara1               Batch: 1/8	Loss 12.6524 (12.6524)
2022-11-09 17:49:42,642:INFO: Dataset: zara1               Batch: 2/8	Loss 10.7829 (11.6823)
2022-11-09 17:49:42,802:INFO: Dataset: zara1               Batch: 3/8	Loss 12.6991 (12.0250)
2022-11-09 17:49:42,959:INFO: Dataset: zara1               Batch: 4/8	Loss 9.6093 (11.4584)
2022-11-09 17:49:43,118:INFO: Dataset: zara1               Batch: 5/8	Loss 6.4146 (10.3278)
2022-11-09 17:49:43,277:INFO: Dataset: zara1               Batch: 6/8	Loss 7.0643 (9.7835)
2022-11-09 17:49:43,436:INFO: Dataset: zara1               Batch: 7/8	Loss 7.3882 (9.4315)
2022-11-09 17:49:43,572:INFO: Dataset: zara1               Batch: 8/8	Loss 8.5562 (9.3398)
2022-11-09 17:49:43,974:INFO: Dataset: zara2               Batch:  1/18	Loss 12.4487 (12.4487)
2022-11-09 17:49:44,135:INFO: Dataset: zara2               Batch:  2/18	Loss 14.4185 (13.4336)
2022-11-09 17:49:44,299:INFO: Dataset: zara2               Batch:  3/18	Loss 7.9171 (11.7076)
2022-11-09 17:49:44,461:INFO: Dataset: zara2               Batch:  4/18	Loss 5.7455 (10.1296)
2022-11-09 17:49:44,625:INFO: Dataset: zara2               Batch:  5/18	Loss 4.6095 (9.0110)
2022-11-09 17:49:44,788:INFO: Dataset: zara2               Batch:  6/18	Loss 5.4650 (8.4328)
2022-11-09 17:49:44,950:INFO: Dataset: zara2               Batch:  7/18	Loss 4.5703 (7.8713)
2022-11-09 17:49:45,109:INFO: Dataset: zara2               Batch:  8/18	Loss 5.2077 (7.5490)
2022-11-09 17:49:45,272:INFO: Dataset: zara2               Batch:  9/18	Loss 5.0793 (7.2677)
2022-11-09 17:49:45,510:INFO: Dataset: zara2               Batch: 10/18	Loss 5.0947 (7.0391)
2022-11-09 17:49:45,672:INFO: Dataset: zara2               Batch: 11/18	Loss 5.1271 (6.8553)
2022-11-09 17:49:45,835:INFO: Dataset: zara2               Batch: 12/18	Loss 5.3097 (6.7288)
2022-11-09 17:49:45,994:INFO: Dataset: zara2               Batch: 13/18	Loss 4.8987 (6.5891)
2022-11-09 17:49:46,158:INFO: Dataset: zara2               Batch: 14/18	Loss 5.6968 (6.5264)
2022-11-09 17:49:46,320:INFO: Dataset: zara2               Batch: 15/18	Loss 5.1024 (6.4214)
2022-11-09 17:49:46,484:INFO: Dataset: zara2               Batch: 16/18	Loss 5.2911 (6.3466)
2022-11-09 17:49:46,644:INFO: Dataset: zara2               Batch: 17/18	Loss 4.8471 (6.2597)
2022-11-09 17:49:46,788:INFO: Dataset: zara2               Batch: 18/18	Loss 4.1297 (6.1586)
2022-11-09 17:49:46,844:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_158.pth.tar
2022-11-09 17:49:46,844:INFO: 
===> EPOCH: 159 (P2)
2022-11-09 17:49:46,845:INFO: - Computing loss (training)
2022-11-09 17:49:47,196:INFO: Dataset: hotel               Batch: 1/4	Loss 4.1055 (4.1055)
2022-11-09 17:49:47,361:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5208 (3.8078)
2022-11-09 17:49:47,522:INFO: Dataset: hotel               Batch: 3/4	Loss 3.9827 (3.8653)
2022-11-09 17:49:47,623:INFO: Dataset: hotel               Batch: 4/4	Loss 2.7973 (3.7047)
2022-11-09 17:49:48,018:INFO: Dataset: univ                Batch:  1/15	Loss 4.3950 (4.3950)
2022-11-09 17:49:48,183:INFO: Dataset: univ                Batch:  2/15	Loss 4.3135 (4.3562)
2022-11-09 17:49:48,351:INFO: Dataset: univ                Batch:  3/15	Loss 4.5187 (4.4066)
2022-11-09 17:49:48,514:INFO: Dataset: univ                Batch:  4/15	Loss 3.9988 (4.2971)
2022-11-09 17:49:48,680:INFO: Dataset: univ                Batch:  5/15	Loss 3.9269 (4.2236)
2022-11-09 17:49:48,845:INFO: Dataset: univ                Batch:  6/15	Loss 4.1713 (4.2150)
2022-11-09 17:49:49,009:INFO: Dataset: univ                Batch:  7/15	Loss 5.6103 (4.4030)
2022-11-09 17:49:49,170:INFO: Dataset: univ                Batch:  8/15	Loss 5.1946 (4.4971)
2022-11-09 17:49:49,335:INFO: Dataset: univ                Batch:  9/15	Loss 4.8405 (4.5329)
2022-11-09 17:49:49,498:INFO: Dataset: univ                Batch: 10/15	Loss 3.5517 (4.4228)
2022-11-09 17:49:49,664:INFO: Dataset: univ                Batch: 11/15	Loss 5.1749 (4.4917)
2022-11-09 17:49:49,827:INFO: Dataset: univ                Batch: 12/15	Loss 6.0673 (4.6252)
2022-11-09 17:49:49,992:INFO: Dataset: univ                Batch: 13/15	Loss 3.7313 (4.5603)
2022-11-09 17:49:50,155:INFO: Dataset: univ                Batch: 14/15	Loss 4.0553 (4.5248)
2022-11-09 17:49:50,197:INFO: Dataset: univ                Batch: 15/15	Loss 0.6203 (4.4661)
2022-11-09 17:49:50,593:INFO: Dataset: zara1               Batch: 1/8	Loss 12.1689 (12.1689)
2022-11-09 17:49:50,756:INFO: Dataset: zara1               Batch: 2/8	Loss 6.7042 (9.3869)
2022-11-09 17:49:50,917:INFO: Dataset: zara1               Batch: 3/8	Loss 10.5254 (9.7669)
2022-11-09 17:49:51,075:INFO: Dataset: zara1               Batch: 4/8	Loss 5.9743 (8.7235)
2022-11-09 17:49:51,238:INFO: Dataset: zara1               Batch: 5/8	Loss 6.5271 (8.3559)
2022-11-09 17:49:51,398:INFO: Dataset: zara1               Batch: 6/8	Loss 6.5560 (8.0471)
2022-11-09 17:49:51,558:INFO: Dataset: zara1               Batch: 7/8	Loss 7.4391 (7.9656)
2022-11-09 17:49:51,695:INFO: Dataset: zara1               Batch: 8/8	Loss 6.9544 (7.8676)
2022-11-09 17:49:52,080:INFO: Dataset: zara2               Batch:  1/18	Loss 10.7735 (10.7735)
2022-11-09 17:49:52,241:INFO: Dataset: zara2               Batch:  2/18	Loss 11.0905 (10.9437)
2022-11-09 17:49:52,402:INFO: Dataset: zara2               Batch:  3/18	Loss 9.6432 (10.5407)
2022-11-09 17:49:52,562:INFO: Dataset: zara2               Batch:  4/18	Loss 4.8656 (9.1002)
2022-11-09 17:49:52,722:INFO: Dataset: zara2               Batch:  5/18	Loss 5.5061 (8.4166)
2022-11-09 17:49:52,882:INFO: Dataset: zara2               Batch:  6/18	Loss 5.1462 (7.8197)
2022-11-09 17:49:53,042:INFO: Dataset: zara2               Batch:  7/18	Loss 5.4321 (7.4915)
2022-11-09 17:49:53,198:INFO: Dataset: zara2               Batch:  8/18	Loss 8.5795 (7.6402)
2022-11-09 17:49:53,357:INFO: Dataset: zara2               Batch:  9/18	Loss 6.7686 (7.5490)
2022-11-09 17:49:53,513:INFO: Dataset: zara2               Batch: 10/18	Loss 3.8184 (7.1868)
2022-11-09 17:49:53,672:INFO: Dataset: zara2               Batch: 11/18	Loss 6.4197 (7.1202)
2022-11-09 17:49:53,831:INFO: Dataset: zara2               Batch: 12/18	Loss 6.9525 (7.1067)
2022-11-09 17:49:53,991:INFO: Dataset: zara2               Batch: 13/18	Loss 4.3226 (6.8870)
2022-11-09 17:49:54,148:INFO: Dataset: zara2               Batch: 14/18	Loss 5.8325 (6.8156)
2022-11-09 17:49:54,311:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6460 (6.6772)
2022-11-09 17:49:54,468:INFO: Dataset: zara2               Batch: 16/18	Loss 6.9484 (6.6941)
2022-11-09 17:49:54,630:INFO: Dataset: zara2               Batch: 17/18	Loss 4.9611 (6.5989)
2022-11-09 17:49:54,767:INFO: Dataset: zara2               Batch: 18/18	Loss 4.4982 (6.4950)
2022-11-09 17:49:54,821:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_159.pth.tar
2022-11-09 17:49:54,821:INFO: 
===> EPOCH: 160 (P2)
2022-11-09 17:49:54,821:INFO: - Computing loss (training)
2022-11-09 17:49:55,170:INFO: Dataset: hotel               Batch: 1/4	Loss 6.9149 (6.9149)
2022-11-09 17:49:55,335:INFO: Dataset: hotel               Batch: 2/4	Loss 2.9635 (4.8039)
2022-11-09 17:49:55,492:INFO: Dataset: hotel               Batch: 3/4	Loss 6.6889 (5.3968)
2022-11-09 17:49:55,588:INFO: Dataset: hotel               Batch: 4/4	Loss 2.6630 (4.9676)
2022-11-09 17:49:55,986:INFO: Dataset: univ                Batch:  1/15	Loss 3.9883 (3.9883)
2022-11-09 17:49:56,153:INFO: Dataset: univ                Batch:  2/15	Loss 3.8916 (3.9425)
2022-11-09 17:49:56,322:INFO: Dataset: univ                Batch:  3/15	Loss 3.7511 (3.8718)
2022-11-09 17:49:56,483:INFO: Dataset: univ                Batch:  4/15	Loss 4.0933 (3.9277)
2022-11-09 17:49:56,646:INFO: Dataset: univ                Batch:  5/15	Loss 3.8012 (3.9031)
2022-11-09 17:49:56,810:INFO: Dataset: univ                Batch:  6/15	Loss 4.0570 (3.9263)
2022-11-09 17:49:56,975:INFO: Dataset: univ                Batch:  7/15	Loss 3.5814 (3.8740)
2022-11-09 17:49:57,135:INFO: Dataset: univ                Batch:  8/15	Loss 3.9764 (3.8871)
2022-11-09 17:49:57,299:INFO: Dataset: univ                Batch:  9/15	Loss 5.4491 (4.0499)
2022-11-09 17:49:57,460:INFO: Dataset: univ                Batch: 10/15	Loss 3.8872 (4.0340)
2022-11-09 17:49:57,625:INFO: Dataset: univ                Batch: 11/15	Loss 3.8380 (4.0160)
2022-11-09 17:49:57,788:INFO: Dataset: univ                Batch: 12/15	Loss 3.5810 (3.9765)
2022-11-09 17:49:57,953:INFO: Dataset: univ                Batch: 13/15	Loss 4.0860 (3.9847)
2022-11-09 17:49:58,115:INFO: Dataset: univ                Batch: 14/15	Loss 4.1350 (3.9949)
2022-11-09 17:49:58,157:INFO: Dataset: univ                Batch: 15/15	Loss 0.5860 (3.9367)
2022-11-09 17:49:58,550:INFO: Dataset: zara1               Batch: 1/8	Loss 8.7733 (8.7733)
2022-11-09 17:49:58,715:INFO: Dataset: zara1               Batch: 2/8	Loss 8.3913 (8.5812)
2022-11-09 17:49:58,876:INFO: Dataset: zara1               Batch: 3/8	Loss 6.3219 (7.8171)
2022-11-09 17:49:59,033:INFO: Dataset: zara1               Batch: 4/8	Loss 9.2684 (8.1597)
2022-11-09 17:49:59,197:INFO: Dataset: zara1               Batch: 5/8	Loss 4.9927 (7.5361)
2022-11-09 17:49:59,358:INFO: Dataset: zara1               Batch: 6/8	Loss 6.0109 (7.2930)
2022-11-09 17:49:59,519:INFO: Dataset: zara1               Batch: 7/8	Loss 5.6091 (7.0573)
2022-11-09 17:49:59,656:INFO: Dataset: zara1               Batch: 8/8	Loss 5.9410 (6.9398)
2022-11-09 17:50:00,067:INFO: Dataset: zara2               Batch:  1/18	Loss 11.3931 (11.3931)
2022-11-09 17:50:00,225:INFO: Dataset: zara2               Batch:  2/18	Loss 8.6513 (9.8956)
2022-11-09 17:50:00,385:INFO: Dataset: zara2               Batch:  3/18	Loss 7.4883 (9.1566)
2022-11-09 17:50:00,544:INFO: Dataset: zara2               Batch:  4/18	Loss 4.8071 (8.0878)
2022-11-09 17:50:00,706:INFO: Dataset: zara2               Batch:  5/18	Loss 5.3649 (7.5695)
2022-11-09 17:50:00,866:INFO: Dataset: zara2               Batch:  6/18	Loss 5.2417 (7.2154)
2022-11-09 17:50:01,026:INFO: Dataset: zara2               Batch:  7/18	Loss 4.4090 (6.8315)
2022-11-09 17:50:01,182:INFO: Dataset: zara2               Batch:  8/18	Loss 4.0848 (6.4877)
2022-11-09 17:50:01,343:INFO: Dataset: zara2               Batch:  9/18	Loss 4.1772 (6.2266)
2022-11-09 17:50:01,500:INFO: Dataset: zara2               Batch: 10/18	Loss 4.4319 (6.0312)
2022-11-09 17:50:01,661:INFO: Dataset: zara2               Batch: 11/18	Loss 4.3579 (5.8883)
2022-11-09 17:50:01,819:INFO: Dataset: zara2               Batch: 12/18	Loss 5.0391 (5.8144)
2022-11-09 17:50:01,979:INFO: Dataset: zara2               Batch: 13/18	Loss 5.6829 (5.8052)
2022-11-09 17:50:02,136:INFO: Dataset: zara2               Batch: 14/18	Loss 4.8381 (5.7381)
2022-11-09 17:50:02,298:INFO: Dataset: zara2               Batch: 15/18	Loss 4.7827 (5.6746)
2022-11-09 17:50:02,460:INFO: Dataset: zara2               Batch: 16/18	Loss 4.2089 (5.5854)
2022-11-09 17:50:02,622:INFO: Dataset: zara2               Batch: 17/18	Loss 4.5391 (5.5208)
2022-11-09 17:50:02,759:INFO: Dataset: zara2               Batch: 18/18	Loss 4.0874 (5.4566)
2022-11-09 17:50:02,813:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_160.pth.tar
2022-11-09 17:50:02,814:INFO: 
===> EPOCH: 161 (P2)
2022-11-09 17:50:02,814:INFO: - Computing loss (training)
2022-11-09 17:50:03,156:INFO: Dataset: hotel               Batch: 1/4	Loss 3.6183 (3.6183)
2022-11-09 17:50:03,318:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4150 (4.0204)
2022-11-09 17:50:03,480:INFO: Dataset: hotel               Batch: 3/4	Loss 3.3749 (3.7952)
2022-11-09 17:50:03,577:INFO: Dataset: hotel               Batch: 4/4	Loss 2.3556 (3.5806)
2022-11-09 17:50:03,980:INFO: Dataset: univ                Batch:  1/15	Loss 4.6732 (4.6732)
2022-11-09 17:50:04,143:INFO: Dataset: univ                Batch:  2/15	Loss 4.0204 (4.3501)
2022-11-09 17:50:04,309:INFO: Dataset: univ                Batch:  3/15	Loss 4.5584 (4.4286)
2022-11-09 17:50:04,469:INFO: Dataset: univ                Batch:  4/15	Loss 4.8440 (4.5312)
2022-11-09 17:50:04,632:INFO: Dataset: univ                Batch:  5/15	Loss 4.1672 (4.4599)
2022-11-09 17:50:04,798:INFO: Dataset: univ                Batch:  6/15	Loss 3.9182 (4.3628)
2022-11-09 17:50:04,961:INFO: Dataset: univ                Batch:  7/15	Loss 3.6519 (4.2486)
2022-11-09 17:50:05,122:INFO: Dataset: univ                Batch:  8/15	Loss 3.5849 (4.1635)
2022-11-09 17:50:05,283:INFO: Dataset: univ                Batch:  9/15	Loss 3.4789 (4.0923)
2022-11-09 17:50:05,445:INFO: Dataset: univ                Batch: 10/15	Loss 3.6217 (4.0475)
2022-11-09 17:50:05,607:INFO: Dataset: univ                Batch: 11/15	Loss 4.0428 (4.0471)
2022-11-09 17:50:05,769:INFO: Dataset: univ                Batch: 12/15	Loss 3.2808 (3.9826)
2022-11-09 17:50:05,932:INFO: Dataset: univ                Batch: 13/15	Loss 4.8625 (4.0470)
2022-11-09 17:50:06,094:INFO: Dataset: univ                Batch: 14/15	Loss 3.0307 (3.9736)
2022-11-09 17:50:06,136:INFO: Dataset: univ                Batch: 15/15	Loss 0.4846 (3.9275)
2022-11-09 17:50:06,535:INFO: Dataset: zara1               Batch: 1/8	Loss 8.7327 (8.7327)
2022-11-09 17:50:06,698:INFO: Dataset: zara1               Batch: 2/8	Loss 14.1370 (11.2173)
2022-11-09 17:50:06,860:INFO: Dataset: zara1               Batch: 3/8	Loss 10.8258 (11.0779)
2022-11-09 17:50:07,021:INFO: Dataset: zara1               Batch: 4/8	Loss 6.9083 (10.0722)
2022-11-09 17:50:07,181:INFO: Dataset: zara1               Batch: 5/8	Loss 5.6793 (9.1863)
2022-11-09 17:50:07,342:INFO: Dataset: zara1               Batch: 6/8	Loss 4.3190 (8.3309)
2022-11-09 17:50:07,503:INFO: Dataset: zara1               Batch: 7/8	Loss 5.6099 (7.9721)
2022-11-09 17:50:07,639:INFO: Dataset: zara1               Batch: 8/8	Loss 4.9270 (7.6371)
2022-11-09 17:50:08,029:INFO: Dataset: zara2               Batch:  1/18	Loss 10.6744 (10.6744)
2022-11-09 17:50:08,189:INFO: Dataset: zara2               Batch:  2/18	Loss 10.7636 (10.7189)
2022-11-09 17:50:08,351:INFO: Dataset: zara2               Batch:  3/18	Loss 9.3532 (10.2784)
2022-11-09 17:50:08,515:INFO: Dataset: zara2               Batch:  4/18	Loss 4.7246 (8.8207)
2022-11-09 17:50:08,675:INFO: Dataset: zara2               Batch:  5/18	Loss 4.2072 (7.8660)
2022-11-09 17:50:08,837:INFO: Dataset: zara2               Batch:  6/18	Loss 3.7012 (7.1484)
2022-11-09 17:50:08,998:INFO: Dataset: zara2               Batch:  7/18	Loss 3.8425 (6.6833)
2022-11-09 17:50:09,155:INFO: Dataset: zara2               Batch:  8/18	Loss 3.4806 (6.2734)
2022-11-09 17:50:09,316:INFO: Dataset: zara2               Batch:  9/18	Loss 3.5155 (5.9646)
2022-11-09 17:50:09,474:INFO: Dataset: zara2               Batch: 10/18	Loss 5.2361 (5.8937)
2022-11-09 17:50:09,634:INFO: Dataset: zara2               Batch: 11/18	Loss 3.4592 (5.6708)
2022-11-09 17:50:09,794:INFO: Dataset: zara2               Batch: 12/18	Loss 5.2467 (5.6299)
2022-11-09 17:50:09,955:INFO: Dataset: zara2               Batch: 13/18	Loss 3.5426 (5.4494)
2022-11-09 17:50:10,117:INFO: Dataset: zara2               Batch: 14/18	Loss 4.5356 (5.3858)
2022-11-09 17:50:10,279:INFO: Dataset: zara2               Batch: 15/18	Loss 3.8457 (5.2844)
2022-11-09 17:50:10,439:INFO: Dataset: zara2               Batch: 16/18	Loss 3.6631 (5.1851)
2022-11-09 17:50:10,601:INFO: Dataset: zara2               Batch: 17/18	Loss 4.6636 (5.1511)
2022-11-09 17:50:10,740:INFO: Dataset: zara2               Batch: 18/18	Loss 3.0041 (5.0553)
2022-11-09 17:50:10,794:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_161.pth.tar
2022-11-09 17:50:10,794:INFO: 
===> EPOCH: 162 (P2)
2022-11-09 17:50:10,795:INFO: - Computing loss (training)
2022-11-09 17:50:11,128:INFO: Dataset: hotel               Batch: 1/4	Loss 5.4539 (5.4539)
2022-11-09 17:50:11,292:INFO: Dataset: hotel               Batch: 2/4	Loss 3.0947 (4.2961)
2022-11-09 17:50:11,453:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7513 (4.1284)
2022-11-09 17:50:11,555:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0800 (3.7744)
2022-11-09 17:50:11,952:INFO: Dataset: univ                Batch:  1/15	Loss 4.2953 (4.2953)
2022-11-09 17:50:12,117:INFO: Dataset: univ                Batch:  2/15	Loss 4.1270 (4.2114)
2022-11-09 17:50:12,284:INFO: Dataset: univ                Batch:  3/15	Loss 3.9418 (4.1203)
2022-11-09 17:50:12,446:INFO: Dataset: univ                Batch:  4/15	Loss 4.6413 (4.2515)
2022-11-09 17:50:12,611:INFO: Dataset: univ                Batch:  5/15	Loss 4.9397 (4.3893)
2022-11-09 17:50:12,775:INFO: Dataset: univ                Batch:  6/15	Loss 3.3915 (4.2157)
2022-11-09 17:50:12,939:INFO: Dataset: univ                Batch:  7/15	Loss 3.2312 (4.0851)
2022-11-09 17:50:13,100:INFO: Dataset: univ                Batch:  8/15	Loss 3.1944 (3.9806)
2022-11-09 17:50:13,264:INFO: Dataset: univ                Batch:  9/15	Loss 3.3785 (3.9142)
2022-11-09 17:50:13,426:INFO: Dataset: univ                Batch: 10/15	Loss 3.2599 (3.8423)
2022-11-09 17:50:13,592:INFO: Dataset: univ                Batch: 11/15	Loss 2.9532 (3.7500)
2022-11-09 17:50:13,754:INFO: Dataset: univ                Batch: 12/15	Loss 2.8585 (3.6725)
2022-11-09 17:50:13,918:INFO: Dataset: univ                Batch: 13/15	Loss 3.9215 (3.6924)
2022-11-09 17:50:14,082:INFO: Dataset: univ                Batch: 14/15	Loss 3.2947 (3.6646)
2022-11-09 17:50:14,124:INFO: Dataset: univ                Batch: 15/15	Loss 0.4732 (3.6289)
2022-11-09 17:50:14,512:INFO: Dataset: zara1               Batch: 1/8	Loss 8.9200 (8.9200)
2022-11-09 17:50:14,670:INFO: Dataset: zara1               Batch: 2/8	Loss 13.3632 (11.3443)
2022-11-09 17:50:14,832:INFO: Dataset: zara1               Batch: 3/8	Loss 10.2835 (10.9979)
2022-11-09 17:50:14,987:INFO: Dataset: zara1               Batch: 4/8	Loss 9.1092 (10.6083)
2022-11-09 17:50:15,148:INFO: Dataset: zara1               Batch: 5/8	Loss 4.5594 (9.5071)
2022-11-09 17:50:15,305:INFO: Dataset: zara1               Batch: 6/8	Loss 4.3049 (8.5768)
2022-11-09 17:50:15,465:INFO: Dataset: zara1               Batch: 7/8	Loss 7.1470 (8.3842)
2022-11-09 17:50:15,599:INFO: Dataset: zara1               Batch: 8/8	Loss 5.3132 (8.0496)
2022-11-09 17:50:15,985:INFO: Dataset: zara2               Batch:  1/18	Loss 7.6267 (7.6267)
2022-11-09 17:50:16,150:INFO: Dataset: zara2               Batch:  2/18	Loss 8.9348 (8.2836)
2022-11-09 17:50:16,314:INFO: Dataset: zara2               Batch:  3/18	Loss 5.9693 (7.4772)
2022-11-09 17:50:16,479:INFO: Dataset: zara2               Batch:  4/18	Loss 4.3557 (6.6607)
2022-11-09 17:50:16,644:INFO: Dataset: zara2               Batch:  5/18	Loss 3.9236 (6.1433)
2022-11-09 17:50:16,804:INFO: Dataset: zara2               Batch:  6/18	Loss 3.6738 (5.7477)
2022-11-09 17:50:16,965:INFO: Dataset: zara2               Batch:  7/18	Loss 3.3736 (5.4114)
2022-11-09 17:50:17,123:INFO: Dataset: zara2               Batch:  8/18	Loss 3.6199 (5.1788)
2022-11-09 17:50:17,284:INFO: Dataset: zara2               Batch:  9/18	Loss 3.7230 (5.0170)
2022-11-09 17:50:17,443:INFO: Dataset: zara2               Batch: 10/18	Loss 3.6252 (4.8909)
2022-11-09 17:50:17,608:INFO: Dataset: zara2               Batch: 11/18	Loss 2.9656 (4.7095)
2022-11-09 17:50:17,768:INFO: Dataset: zara2               Batch: 12/18	Loss 3.0892 (4.5746)
2022-11-09 17:50:17,931:INFO: Dataset: zara2               Batch: 13/18	Loss 3.5015 (4.4897)
2022-11-09 17:50:18,092:INFO: Dataset: zara2               Batch: 14/18	Loss 3.0194 (4.3888)
2022-11-09 17:50:18,257:INFO: Dataset: zara2               Batch: 15/18	Loss 3.2333 (4.3139)
2022-11-09 17:50:18,419:INFO: Dataset: zara2               Batch: 16/18	Loss 3.2715 (4.2495)
2022-11-09 17:50:18,583:INFO: Dataset: zara2               Batch: 17/18	Loss 3.2999 (4.1967)
2022-11-09 17:50:18,724:INFO: Dataset: zara2               Batch: 18/18	Loss 2.8097 (4.1354)
2022-11-09 17:50:18,781:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_162.pth.tar
2022-11-09 17:50:18,781:INFO: 
===> EPOCH: 163 (P2)
2022-11-09 17:50:18,781:INFO: - Computing loss (training)
2022-11-09 17:50:19,119:INFO: Dataset: hotel               Batch: 1/4	Loss 4.9018 (4.9018)
2022-11-09 17:50:19,293:INFO: Dataset: hotel               Batch: 2/4	Loss 6.6314 (5.7729)
2022-11-09 17:50:19,453:INFO: Dataset: hotel               Batch: 3/4	Loss 3.0782 (4.8790)
2022-11-09 17:50:19,553:INFO: Dataset: hotel               Batch: 4/4	Loss 5.3438 (4.9630)
2022-11-09 17:50:20,005:INFO: Dataset: univ                Batch:  1/15	Loss 2.7932 (2.7932)
2022-11-09 17:50:20,169:INFO: Dataset: univ                Batch:  2/15	Loss 3.1656 (2.9819)
2022-11-09 17:50:20,333:INFO: Dataset: univ                Batch:  3/15	Loss 3.4169 (3.1167)
2022-11-09 17:50:20,497:INFO: Dataset: univ                Batch:  4/15	Loss 3.2022 (3.1366)
2022-11-09 17:50:20,661:INFO: Dataset: univ                Batch:  5/15	Loss 3.0310 (3.1159)
2022-11-09 17:50:20,827:INFO: Dataset: univ                Batch:  6/15	Loss 2.9369 (3.0860)
2022-11-09 17:50:20,989:INFO: Dataset: univ                Batch:  7/15	Loss 2.8420 (3.0523)
2022-11-09 17:50:21,227:INFO: Dataset: univ                Batch:  8/15	Loss 2.9257 (3.0356)
2022-11-09 17:50:21,390:INFO: Dataset: univ                Batch:  9/15	Loss 3.0349 (3.0355)
2022-11-09 17:50:21,554:INFO: Dataset: univ                Batch: 10/15	Loss 2.7751 (3.0075)
2022-11-09 17:50:21,716:INFO: Dataset: univ                Batch: 11/15	Loss 3.0609 (3.0130)
2022-11-09 17:50:21,881:INFO: Dataset: univ                Batch: 12/15	Loss 3.7044 (3.0703)
2022-11-09 17:50:22,044:INFO: Dataset: univ                Batch: 13/15	Loss 3.1020 (3.0727)
2022-11-09 17:50:22,209:INFO: Dataset: univ                Batch: 14/15	Loss 2.7870 (3.0527)
2022-11-09 17:50:22,251:INFO: Dataset: univ                Batch: 15/15	Loss 0.6345 (3.0229)
2022-11-09 17:50:22,641:INFO: Dataset: zara1               Batch: 1/8	Loss 12.8733 (12.8733)
2022-11-09 17:50:22,803:INFO: Dataset: zara1               Batch: 2/8	Loss 5.4207 (9.0282)
2022-11-09 17:50:22,963:INFO: Dataset: zara1               Batch: 3/8	Loss 4.5642 (7.5128)
2022-11-09 17:50:23,119:INFO: Dataset: zara1               Batch: 4/8	Loss 4.0252 (6.6418)
2022-11-09 17:50:23,278:INFO: Dataset: zara1               Batch: 5/8	Loss 3.7194 (6.1321)
2022-11-09 17:50:23,439:INFO: Dataset: zara1               Batch: 6/8	Loss 3.4456 (5.7158)
2022-11-09 17:50:23,598:INFO: Dataset: zara1               Batch: 7/8	Loss 4.7478 (5.5814)
2022-11-09 17:50:23,734:INFO: Dataset: zara1               Batch: 8/8	Loss 3.3424 (5.3293)
2022-11-09 17:50:24,133:INFO: Dataset: zara2               Batch:  1/18	Loss 5.1239 (5.1239)
2022-11-09 17:50:24,306:INFO: Dataset: zara2               Batch:  2/18	Loss 6.2903 (5.6449)
2022-11-09 17:50:24,467:INFO: Dataset: zara2               Batch:  3/18	Loss 6.4984 (5.9231)
2022-11-09 17:50:24,625:INFO: Dataset: zara2               Batch:  4/18	Loss 4.5351 (5.5633)
2022-11-09 17:50:24,785:INFO: Dataset: zara2               Batch:  5/18	Loss 3.4366 (5.1665)
2022-11-09 17:50:24,948:INFO: Dataset: zara2               Batch:  6/18	Loss 5.2618 (5.1816)
2022-11-09 17:50:25,109:INFO: Dataset: zara2               Batch:  7/18	Loss 2.8766 (4.8637)
2022-11-09 17:50:25,265:INFO: Dataset: zara2               Batch:  8/18	Loss 2.9760 (4.6374)
2022-11-09 17:50:25,426:INFO: Dataset: zara2               Batch:  9/18	Loss 2.8029 (4.4192)
2022-11-09 17:50:25,583:INFO: Dataset: zara2               Batch: 10/18	Loss 3.0135 (4.2672)
2022-11-09 17:50:25,743:INFO: Dataset: zara2               Batch: 11/18	Loss 2.6354 (4.1084)
2022-11-09 17:50:25,903:INFO: Dataset: zara2               Batch: 12/18	Loss 3.8825 (4.0916)
2022-11-09 17:50:26,063:INFO: Dataset: zara2               Batch: 13/18	Loss 4.5711 (4.1251)
2022-11-09 17:50:26,221:INFO: Dataset: zara2               Batch: 14/18	Loss 5.3037 (4.2121)
2022-11-09 17:50:26,384:INFO: Dataset: zara2               Batch: 15/18	Loss 2.8542 (4.1245)
2022-11-09 17:50:26,543:INFO: Dataset: zara2               Batch: 16/18	Loss 3.0271 (4.0649)
2022-11-09 17:50:26,704:INFO: Dataset: zara2               Batch: 17/18	Loss 4.6961 (4.1019)
2022-11-09 17:50:26,843:INFO: Dataset: zara2               Batch: 18/18	Loss 2.1907 (4.0096)
2022-11-09 17:50:26,896:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_163.pth.tar
2022-11-09 17:50:26,897:INFO: 
===> EPOCH: 164 (P2)
2022-11-09 17:50:26,897:INFO: - Computing loss (training)
2022-11-09 17:50:27,259:INFO: Dataset: hotel               Batch: 1/4	Loss 3.8648 (3.8648)
2022-11-09 17:50:27,426:INFO: Dataset: hotel               Batch: 2/4	Loss 4.8688 (4.3607)
2022-11-09 17:50:27,585:INFO: Dataset: hotel               Batch: 3/4	Loss 3.2793 (3.9938)
2022-11-09 17:50:27,686:INFO: Dataset: hotel               Batch: 4/4	Loss 2.5035 (3.7205)
2022-11-09 17:50:28,084:INFO: Dataset: univ                Batch:  1/15	Loss 2.7379 (2.7379)
2022-11-09 17:50:28,254:INFO: Dataset: univ                Batch:  2/15	Loss 2.9203 (2.8257)
2022-11-09 17:50:28,419:INFO: Dataset: univ                Batch:  3/15	Loss 2.8105 (2.8207)
2022-11-09 17:50:28,580:INFO: Dataset: univ                Batch:  4/15	Loss 2.9395 (2.8493)
2022-11-09 17:50:28,745:INFO: Dataset: univ                Batch:  5/15	Loss 2.8148 (2.8424)
2022-11-09 17:50:28,912:INFO: Dataset: univ                Batch:  6/15	Loss 3.1412 (2.8917)
2022-11-09 17:50:29,077:INFO: Dataset: univ                Batch:  7/15	Loss 3.0593 (2.9163)
2022-11-09 17:50:29,242:INFO: Dataset: univ                Batch:  8/15	Loss 3.6026 (3.0057)
2022-11-09 17:50:29,407:INFO: Dataset: univ                Batch:  9/15	Loss 2.9531 (3.0000)
2022-11-09 17:50:29,569:INFO: Dataset: univ                Batch: 10/15	Loss 2.7338 (2.9735)
2022-11-09 17:50:29,737:INFO: Dataset: univ                Batch: 11/15	Loss 2.5792 (2.9332)
2022-11-09 17:50:29,901:INFO: Dataset: univ                Batch: 12/15	Loss 2.9999 (2.9394)
2022-11-09 17:50:30,068:INFO: Dataset: univ                Batch: 13/15	Loss 2.4686 (2.9055)
2022-11-09 17:50:30,231:INFO: Dataset: univ                Batch: 14/15	Loss 5.6255 (3.0810)
2022-11-09 17:50:30,275:INFO: Dataset: univ                Batch: 15/15	Loss 0.8564 (3.0526)
2022-11-09 17:50:30,661:INFO: Dataset: zara1               Batch: 1/8	Loss 9.0892 (9.0892)
2022-11-09 17:50:30,827:INFO: Dataset: zara1               Batch: 2/8	Loss 10.8233 (9.9642)
2022-11-09 17:50:30,986:INFO: Dataset: zara1               Batch: 3/8	Loss 6.6203 (8.6610)
2022-11-09 17:50:31,144:INFO: Dataset: zara1               Batch: 4/8	Loss 3.1551 (7.2773)
2022-11-09 17:50:31,306:INFO: Dataset: zara1               Batch: 5/8	Loss 3.0426 (6.3500)
2022-11-09 17:50:31,466:INFO: Dataset: zara1               Batch: 6/8	Loss 3.3629 (5.8315)
2022-11-09 17:50:31,625:INFO: Dataset: zara1               Batch: 7/8	Loss 4.1666 (5.6133)
2022-11-09 17:50:31,761:INFO: Dataset: zara1               Batch: 8/8	Loss 4.3229 (5.4890)
2022-11-09 17:50:32,166:INFO: Dataset: zara2               Batch:  1/18	Loss 6.3989 (6.3989)
2022-11-09 17:50:32,326:INFO: Dataset: zara2               Batch:  2/18	Loss 4.6271 (5.5079)
2022-11-09 17:50:32,489:INFO: Dataset: zara2               Batch:  3/18	Loss 3.0899 (4.8005)
2022-11-09 17:50:32,650:INFO: Dataset: zara2               Batch:  4/18	Loss 3.9086 (4.5709)
2022-11-09 17:50:32,811:INFO: Dataset: zara2               Batch:  5/18	Loss 3.0742 (4.2852)
2022-11-09 17:50:32,974:INFO: Dataset: zara2               Batch:  6/18	Loss 2.5757 (3.9842)
2022-11-09 17:50:33,136:INFO: Dataset: zara2               Batch:  7/18	Loss 2.4834 (3.7638)
2022-11-09 17:50:33,293:INFO: Dataset: zara2               Batch:  8/18	Loss 2.6591 (3.6281)
2022-11-09 17:50:33,455:INFO: Dataset: zara2               Batch:  9/18	Loss 2.2200 (3.4715)
2022-11-09 17:50:33,613:INFO: Dataset: zara2               Batch: 10/18	Loss 2.8326 (3.4044)
2022-11-09 17:50:33,776:INFO: Dataset: zara2               Batch: 11/18	Loss 3.9491 (3.4554)
2022-11-09 17:50:33,935:INFO: Dataset: zara2               Batch: 12/18	Loss 3.3938 (3.4506)
2022-11-09 17:50:34,096:INFO: Dataset: zara2               Batch: 13/18	Loss 3.2351 (3.4341)
2022-11-09 17:50:34,255:INFO: Dataset: zara2               Batch: 14/18	Loss 3.6006 (3.4463)
2022-11-09 17:50:34,418:INFO: Dataset: zara2               Batch: 15/18	Loss 2.5627 (3.3892)
2022-11-09 17:50:34,577:INFO: Dataset: zara2               Batch: 16/18	Loss 3.4215 (3.3911)
2022-11-09 17:50:34,740:INFO: Dataset: zara2               Batch: 17/18	Loss 3.1342 (3.3750)
2022-11-09 17:50:34,881:INFO: Dataset: zara2               Batch: 18/18	Loss 2.2964 (3.3199)
2022-11-09 17:50:34,936:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_164.pth.tar
2022-11-09 17:50:34,936:INFO: 
===> EPOCH: 165 (P2)
2022-11-09 17:50:34,937:INFO: - Computing loss (training)
2022-11-09 17:50:35,270:INFO: Dataset: hotel               Batch: 1/4	Loss 4.0970 (4.0970)
2022-11-09 17:50:35,434:INFO: Dataset: hotel               Batch: 2/4	Loss 4.5427 (4.3194)
2022-11-09 17:50:35,593:INFO: Dataset: hotel               Batch: 3/4	Loss 3.4723 (4.0472)
2022-11-09 17:50:35,693:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6488 (3.6675)
2022-11-09 17:50:36,103:INFO: Dataset: univ                Batch:  1/15	Loss 2.4754 (2.4754)
2022-11-09 17:50:36,268:INFO: Dataset: univ                Batch:  2/15	Loss 4.0062 (3.2881)
2022-11-09 17:50:36,435:INFO: Dataset: univ                Batch:  3/15	Loss 2.7658 (3.1168)
2022-11-09 17:50:36,595:INFO: Dataset: univ                Batch:  4/15	Loss 2.7790 (3.0402)
2022-11-09 17:50:36,760:INFO: Dataset: univ                Batch:  5/15	Loss 2.4807 (2.9442)
2022-11-09 17:50:36,929:INFO: Dataset: univ                Batch:  6/15	Loss 2.5563 (2.8792)
2022-11-09 17:50:37,094:INFO: Dataset: univ                Batch:  7/15	Loss 2.3638 (2.8046)
2022-11-09 17:50:37,255:INFO: Dataset: univ                Batch:  8/15	Loss 2.2268 (2.7313)
2022-11-09 17:50:37,420:INFO: Dataset: univ                Batch:  9/15	Loss 2.2984 (2.6829)
2022-11-09 17:50:37,582:INFO: Dataset: univ                Batch: 10/15	Loss 2.3768 (2.6487)
2022-11-09 17:50:37,747:INFO: Dataset: univ                Batch: 11/15	Loss 2.5299 (2.6375)
2022-11-09 17:50:37,912:INFO: Dataset: univ                Batch: 12/15	Loss 2.3790 (2.6136)
2022-11-09 17:50:38,078:INFO: Dataset: univ                Batch: 13/15	Loss 2.1632 (2.5763)
2022-11-09 17:50:38,241:INFO: Dataset: univ                Batch: 14/15	Loss 2.5907 (2.5774)
2022-11-09 17:50:38,283:INFO: Dataset: univ                Batch: 15/15	Loss 0.6930 (2.5509)
2022-11-09 17:50:38,662:INFO: Dataset: zara1               Batch: 1/8	Loss 4.6716 (4.6716)
2022-11-09 17:50:38,825:INFO: Dataset: zara1               Batch: 2/8	Loss 4.6178 (4.6469)
2022-11-09 17:50:38,984:INFO: Dataset: zara1               Batch: 3/8	Loss 3.6763 (4.3191)
2022-11-09 17:50:39,141:INFO: Dataset: zara1               Batch: 4/8	Loss 3.5410 (4.1369)
2022-11-09 17:50:39,301:INFO: Dataset: zara1               Batch: 5/8	Loss 3.0184 (3.8872)
2022-11-09 17:50:39,461:INFO: Dataset: zara1               Batch: 6/8	Loss 3.4528 (3.8081)
2022-11-09 17:50:39,620:INFO: Dataset: zara1               Batch: 7/8	Loss 3.7234 (3.7953)
2022-11-09 17:50:39,756:INFO: Dataset: zara1               Batch: 8/8	Loss 2.6355 (3.6555)
2022-11-09 17:50:40,151:INFO: Dataset: zara2               Batch:  1/18	Loss 2.7621 (2.7621)
2022-11-09 17:50:40,314:INFO: Dataset: zara2               Batch:  2/18	Loss 3.4055 (3.0807)
2022-11-09 17:50:40,477:INFO: Dataset: zara2               Batch:  3/18	Loss 4.7591 (3.5901)
2022-11-09 17:50:40,639:INFO: Dataset: zara2               Batch:  4/18	Loss 2.6512 (3.3438)
2022-11-09 17:50:40,802:INFO: Dataset: zara2               Batch:  5/18	Loss 4.2543 (3.5156)
2022-11-09 17:50:40,964:INFO: Dataset: zara2               Batch:  6/18	Loss 3.0340 (3.4416)
2022-11-09 17:50:41,125:INFO: Dataset: zara2               Batch:  7/18	Loss 2.1224 (3.2493)
2022-11-09 17:50:41,283:INFO: Dataset: zara2               Batch:  8/18	Loss 2.9144 (3.2082)
2022-11-09 17:50:41,444:INFO: Dataset: zara2               Batch:  9/18	Loss 2.4401 (3.1238)
2022-11-09 17:50:41,602:INFO: Dataset: zara2               Batch: 10/18	Loss 2.3159 (3.0441)
2022-11-09 17:50:41,763:INFO: Dataset: zara2               Batch: 11/18	Loss 3.0139 (3.0414)
2022-11-09 17:50:41,924:INFO: Dataset: zara2               Batch: 12/18	Loss 3.4410 (3.0761)
2022-11-09 17:50:42,086:INFO: Dataset: zara2               Batch: 13/18	Loss 2.6913 (3.0476)
2022-11-09 17:50:42,245:INFO: Dataset: zara2               Batch: 14/18	Loss 2.3563 (2.9976)
2022-11-09 17:50:42,409:INFO: Dataset: zara2               Batch: 15/18	Loss 3.2170 (3.0140)
2022-11-09 17:50:42,569:INFO: Dataset: zara2               Batch: 16/18	Loss 2.6889 (2.9942)
2022-11-09 17:50:42,732:INFO: Dataset: zara2               Batch: 17/18	Loss 2.4763 (2.9636)
2022-11-09 17:50:42,877:INFO: Dataset: zara2               Batch: 18/18	Loss 2.8557 (2.9579)
2022-11-09 17:50:42,930:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_165.pth.tar
2022-11-09 17:50:42,930:INFO: 
===> EPOCH: 166 (P2)
2022-11-09 17:50:42,930:INFO: - Computing loss (training)
2022-11-09 17:50:43,275:INFO: Dataset: hotel               Batch: 1/4	Loss 3.1076 (3.1076)
2022-11-09 17:50:43,438:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5542 (3.3214)
2022-11-09 17:50:43,594:INFO: Dataset: hotel               Batch: 3/4	Loss 5.8214 (4.1443)
2022-11-09 17:50:43,693:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7951 (3.7631)
2022-11-09 17:50:44,100:INFO: Dataset: univ                Batch:  1/15	Loss 2.6194 (2.6194)
2022-11-09 17:50:44,268:INFO: Dataset: univ                Batch:  2/15	Loss 2.3731 (2.5066)
2022-11-09 17:50:44,432:INFO: Dataset: univ                Batch:  3/15	Loss 2.5666 (2.5261)
2022-11-09 17:50:44,594:INFO: Dataset: univ                Batch:  4/15	Loss 2.7235 (2.5771)
2022-11-09 17:50:44,760:INFO: Dataset: univ                Batch:  5/15	Loss 2.6326 (2.5887)
2022-11-09 17:50:44,923:INFO: Dataset: univ                Batch:  6/15	Loss 2.3616 (2.5488)
2022-11-09 17:50:45,085:INFO: Dataset: univ                Batch:  7/15	Loss 2.4311 (2.5325)
2022-11-09 17:50:45,245:INFO: Dataset: univ                Batch:  8/15	Loss 2.1188 (2.4862)
2022-11-09 17:50:45,409:INFO: Dataset: univ                Batch:  9/15	Loss 2.2673 (2.4633)
2022-11-09 17:50:45,569:INFO: Dataset: univ                Batch: 10/15	Loss 2.1076 (2.4256)
2022-11-09 17:50:45,733:INFO: Dataset: univ                Batch: 11/15	Loss 2.0440 (2.3923)
2022-11-09 17:50:45,897:INFO: Dataset: univ                Batch: 12/15	Loss 3.8711 (2.5092)
2022-11-09 17:50:46,060:INFO: Dataset: univ                Batch: 13/15	Loss 2.0097 (2.4705)
2022-11-09 17:50:46,223:INFO: Dataset: univ                Batch: 14/15	Loss 2.8789 (2.4985)
2022-11-09 17:50:46,264:INFO: Dataset: univ                Batch: 15/15	Loss 0.4004 (2.4722)
2022-11-09 17:50:46,651:INFO: Dataset: zara1               Batch: 1/8	Loss 2.9223 (2.9223)
2022-11-09 17:50:46,813:INFO: Dataset: zara1               Batch: 2/8	Loss 4.3370 (3.5899)
2022-11-09 17:50:46,975:INFO: Dataset: zara1               Batch: 3/8	Loss 4.1575 (3.7815)
2022-11-09 17:50:47,136:INFO: Dataset: zara1               Batch: 4/8	Loss 2.5326 (3.4484)
2022-11-09 17:50:47,298:INFO: Dataset: zara1               Batch: 5/8	Loss 2.5835 (3.2774)
2022-11-09 17:50:47,458:INFO: Dataset: zara1               Batch: 6/8	Loss 3.6647 (3.3409)
2022-11-09 17:50:47,620:INFO: Dataset: zara1               Batch: 7/8	Loss 3.9317 (3.4209)
2022-11-09 17:50:47,758:INFO: Dataset: zara1               Batch: 8/8	Loss 2.1855 (3.2954)
2022-11-09 17:50:48,161:INFO: Dataset: zara2               Batch:  1/18	Loss 2.9307 (2.9307)
2022-11-09 17:50:48,321:INFO: Dataset: zara2               Batch:  2/18	Loss 3.4619 (3.1904)
2022-11-09 17:50:48,483:INFO: Dataset: zara2               Batch:  3/18	Loss 2.2515 (2.9020)
2022-11-09 17:50:48,646:INFO: Dataset: zara2               Batch:  4/18	Loss 2.2148 (2.7292)
2022-11-09 17:50:48,809:INFO: Dataset: zara2               Batch:  5/18	Loss 2.2331 (2.6316)
2022-11-09 17:50:48,971:INFO: Dataset: zara2               Batch:  6/18	Loss 2.0083 (2.5375)
2022-11-09 17:50:49,131:INFO: Dataset: zara2               Batch:  7/18	Loss 2.0472 (2.4705)
2022-11-09 17:50:49,290:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8501 (2.3907)
2022-11-09 17:50:49,452:INFO: Dataset: zara2               Batch:  9/18	Loss 2.5979 (2.4124)
2022-11-09 17:50:49,610:INFO: Dataset: zara2               Batch: 10/18	Loss 2.0374 (2.3734)
2022-11-09 17:50:49,771:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8462 (2.3163)
2022-11-09 17:50:49,932:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9481 (2.2853)
2022-11-09 17:50:50,093:INFO: Dataset: zara2               Batch: 13/18	Loss 2.7208 (2.3179)
2022-11-09 17:50:50,252:INFO: Dataset: zara2               Batch: 14/18	Loss 3.3820 (2.3891)
2022-11-09 17:50:50,416:INFO: Dataset: zara2               Batch: 15/18	Loss 2.6663 (2.4073)
2022-11-09 17:50:50,576:INFO: Dataset: zara2               Batch: 16/18	Loss 2.4273 (2.4085)
2022-11-09 17:50:50,739:INFO: Dataset: zara2               Batch: 17/18	Loss 2.1585 (2.3953)
2022-11-09 17:50:50,880:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6241 (2.3629)
2022-11-09 17:50:50,933:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_166.pth.tar
2022-11-09 17:50:50,933:INFO: 
===> EPOCH: 167 (P2)
2022-11-09 17:50:50,933:INFO: - Computing loss (training)
2022-11-09 17:50:51,279:INFO: Dataset: hotel               Batch: 1/4	Loss 2.5577 (2.5577)
2022-11-09 17:50:51,442:INFO: Dataset: hotel               Batch: 2/4	Loss 8.0465 (5.3355)
2022-11-09 17:50:51,598:INFO: Dataset: hotel               Batch: 3/4	Loss 6.0377 (5.5818)
2022-11-09 17:50:51,698:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7748 (4.9540)
2022-11-09 17:50:52,095:INFO: Dataset: univ                Batch:  1/15	Loss 2.2700 (2.2700)
2022-11-09 17:50:52,260:INFO: Dataset: univ                Batch:  2/15	Loss 2.0667 (2.1754)
2022-11-09 17:50:52,431:INFO: Dataset: univ                Batch:  3/15	Loss 2.3779 (2.2439)
2022-11-09 17:50:52,593:INFO: Dataset: univ                Batch:  4/15	Loss 1.9718 (2.1750)
2022-11-09 17:50:52,756:INFO: Dataset: univ                Batch:  5/15	Loss 1.8990 (2.1215)
2022-11-09 17:50:52,920:INFO: Dataset: univ                Batch:  6/15	Loss 2.1529 (2.1269)
2022-11-09 17:50:53,085:INFO: Dataset: univ                Batch:  7/15	Loss 2.4886 (2.1804)
2022-11-09 17:50:53,244:INFO: Dataset: univ                Batch:  8/15	Loss 2.2689 (2.1910)
2022-11-09 17:50:53,410:INFO: Dataset: univ                Batch:  9/15	Loss 2.0283 (2.1734)
2022-11-09 17:50:53,570:INFO: Dataset: univ                Batch: 10/15	Loss 1.8671 (2.1417)
2022-11-09 17:50:53,736:INFO: Dataset: univ                Batch: 11/15	Loss 1.7052 (2.1045)
2022-11-09 17:50:53,899:INFO: Dataset: univ                Batch: 12/15	Loss 1.7793 (2.0766)
2022-11-09 17:50:54,063:INFO: Dataset: univ                Batch: 13/15	Loss 1.8052 (2.0566)
2022-11-09 17:50:54,225:INFO: Dataset: univ                Batch: 14/15	Loss 1.9117 (2.0468)
2022-11-09 17:50:54,267:INFO: Dataset: univ                Batch: 15/15	Loss 0.5123 (2.0319)
2022-11-09 17:50:54,665:INFO: Dataset: zara1               Batch: 1/8	Loss 3.3598 (3.3598)
2022-11-09 17:50:54,826:INFO: Dataset: zara1               Batch: 2/8	Loss 5.4712 (4.4196)
2022-11-09 17:50:54,986:INFO: Dataset: zara1               Batch: 3/8	Loss 3.0563 (4.0067)
2022-11-09 17:50:55,143:INFO: Dataset: zara1               Batch: 4/8	Loss 2.5165 (3.6747)
2022-11-09 17:50:55,302:INFO: Dataset: zara1               Batch: 5/8	Loss 2.1907 (3.3671)
2022-11-09 17:50:55,462:INFO: Dataset: zara1               Batch: 6/8	Loss 2.4142 (3.2024)
2022-11-09 17:50:55,622:INFO: Dataset: zara1               Batch: 7/8	Loss 2.2664 (3.0763)
2022-11-09 17:50:55,835:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8601 (2.9387)
2022-11-09 17:50:56,238:INFO: Dataset: zara2               Batch:  1/18	Loss 5.7292 (5.7292)
2022-11-09 17:50:56,397:INFO: Dataset: zara2               Batch:  2/18	Loss 4.5963 (5.1059)
2022-11-09 17:50:56,563:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9422 (4.1224)
2022-11-09 17:50:56,720:INFO: Dataset: zara2               Batch:  4/18	Loss 2.5108 (3.7695)
2022-11-09 17:50:56,882:INFO: Dataset: zara2               Batch:  5/18	Loss 2.7995 (3.5791)
2022-11-09 17:50:57,043:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8788 (3.3089)
2022-11-09 17:50:57,203:INFO: Dataset: zara2               Batch:  7/18	Loss 2.5349 (3.1992)
2022-11-09 17:50:57,361:INFO: Dataset: zara2               Batch:  8/18	Loss 2.2404 (3.0826)
2022-11-09 17:50:57,521:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9277 (2.9551)
2022-11-09 17:50:57,679:INFO: Dataset: zara2               Batch: 10/18	Loss 3.7550 (3.0252)
2022-11-09 17:50:57,839:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9161 (2.9224)
2022-11-09 17:50:57,999:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6739 (2.8307)
2022-11-09 17:50:58,160:INFO: Dataset: zara2               Batch: 13/18	Loss 3.0607 (2.8465)
2022-11-09 17:50:58,318:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7849 (2.7614)
2022-11-09 17:50:58,479:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7146 (2.6946)
2022-11-09 17:50:58,637:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7374 (2.6394)
2022-11-09 17:50:58,798:INFO: Dataset: zara2               Batch: 17/18	Loss 2.4153 (2.6265)
2022-11-09 17:50:58,937:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9716 (2.5942)
2022-11-09 17:50:58,992:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_167.pth.tar
2022-11-09 17:50:58,992:INFO: 
===> EPOCH: 168 (P2)
2022-11-09 17:50:58,992:INFO: - Computing loss (training)
2022-11-09 17:50:59,334:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6054 (2.6054)
2022-11-09 17:50:59,500:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2241 (2.4213)
2022-11-09 17:50:59,662:INFO: Dataset: hotel               Batch: 3/4	Loss 4.7566 (3.2122)
2022-11-09 17:50:59,765:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4276 (2.8944)
2022-11-09 17:51:00,172:INFO: Dataset: univ                Batch:  1/15	Loss 2.4312 (2.4312)
2022-11-09 17:51:00,339:INFO: Dataset: univ                Batch:  2/15	Loss 1.6631 (2.0188)
2022-11-09 17:51:00,508:INFO: Dataset: univ                Batch:  3/15	Loss 4.5439 (2.7223)
2022-11-09 17:51:00,670:INFO: Dataset: univ                Batch:  4/15	Loss 2.7537 (2.7303)
2022-11-09 17:51:00,840:INFO: Dataset: univ                Batch:  5/15	Loss 1.9279 (2.5640)
2022-11-09 17:51:01,006:INFO: Dataset: univ                Batch:  6/15	Loss 2.0852 (2.4873)
2022-11-09 17:51:01,170:INFO: Dataset: univ                Batch:  7/15	Loss 2.1501 (2.4469)
2022-11-09 17:51:01,335:INFO: Dataset: univ                Batch:  8/15	Loss 1.6840 (2.3393)
2022-11-09 17:51:01,502:INFO: Dataset: univ                Batch:  9/15	Loss 1.9557 (2.2949)
2022-11-09 17:51:01,665:INFO: Dataset: univ                Batch: 10/15	Loss 2.1713 (2.2833)
2022-11-09 17:51:01,834:INFO: Dataset: univ                Batch: 11/15	Loss 1.5690 (2.2132)
2022-11-09 17:51:01,999:INFO: Dataset: univ                Batch: 12/15	Loss 2.2091 (2.2129)
2022-11-09 17:51:02,166:INFO: Dataset: univ                Batch: 13/15	Loss 1.6168 (2.1650)
2022-11-09 17:51:02,332:INFO: Dataset: univ                Batch: 14/15	Loss 1.5267 (2.1179)
2022-11-09 17:51:02,379:INFO: Dataset: univ                Batch: 15/15	Loss 0.3023 (2.0897)
2022-11-09 17:51:02,790:INFO: Dataset: zara1               Batch: 1/8	Loss 4.0767 (4.0767)
2022-11-09 17:51:02,951:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0115 (2.9921)
2022-11-09 17:51:03,111:INFO: Dataset: zara1               Batch: 3/8	Loss 2.6220 (2.8460)
2022-11-09 17:51:03,268:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0941 (2.6647)
2022-11-09 17:51:03,438:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9989 (2.5301)
2022-11-09 17:51:03,599:INFO: Dataset: zara1               Batch: 6/8	Loss 2.9815 (2.6007)
2022-11-09 17:51:03,758:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8734 (2.4987)
2022-11-09 17:51:03,896:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7870 (2.4309)
2022-11-09 17:51:04,284:INFO: Dataset: zara2               Batch:  1/18	Loss 2.6631 (2.6631)
2022-11-09 17:51:04,448:INFO: Dataset: zara2               Batch:  2/18	Loss 3.3401 (2.9943)
2022-11-09 17:51:04,614:INFO: Dataset: zara2               Batch:  3/18	Loss 3.6572 (3.2259)
2022-11-09 17:51:04,776:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8760 (2.8856)
2022-11-09 17:51:04,938:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8215 (2.6874)
2022-11-09 17:51:05,099:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8681 (2.5602)
2022-11-09 17:51:05,262:INFO: Dataset: zara2               Batch:  7/18	Loss 1.6402 (2.4220)
2022-11-09 17:51:05,422:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6806 (2.3370)
2022-11-09 17:51:05,584:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8522 (2.2882)
2022-11-09 17:51:05,742:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5541 (2.2075)
2022-11-09 17:51:05,905:INFO: Dataset: zara2               Batch: 11/18	Loss 1.6306 (2.1483)
2022-11-09 17:51:06,066:INFO: Dataset: zara2               Batch: 12/18	Loss 2.2677 (2.1576)
2022-11-09 17:51:06,228:INFO: Dataset: zara2               Batch: 13/18	Loss 3.5456 (2.2634)
2022-11-09 17:51:06,389:INFO: Dataset: zara2               Batch: 14/18	Loss 2.2656 (2.2636)
2022-11-09 17:51:06,553:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5429 (2.2200)
2022-11-09 17:51:06,713:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8672 (2.1990)
2022-11-09 17:51:06,877:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0433 (2.1900)
2022-11-09 17:51:07,017:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3985 (2.1525)
2022-11-09 17:51:07,071:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_168.pth.tar
2022-11-09 17:51:07,072:INFO: 
===> EPOCH: 169 (P2)
2022-11-09 17:51:07,072:INFO: - Computing loss (training)
2022-11-09 17:51:07,413:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2089 (2.2089)
2022-11-09 17:51:07,574:INFO: Dataset: hotel               Batch: 2/4	Loss 2.3531 (2.2825)
2022-11-09 17:51:07,732:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0623 (2.2074)
2022-11-09 17:51:07,832:INFO: Dataset: hotel               Batch: 4/4	Loss 2.5400 (2.2622)
2022-11-09 17:51:08,229:INFO: Dataset: univ                Batch:  1/15	Loss 1.7459 (1.7459)
2022-11-09 17:51:08,402:INFO: Dataset: univ                Batch:  2/15	Loss 1.7517 (1.7490)
2022-11-09 17:51:08,571:INFO: Dataset: univ                Batch:  3/15	Loss 1.6349 (1.7113)
2022-11-09 17:51:08,742:INFO: Dataset: univ                Batch:  4/15	Loss 1.6559 (1.6964)
2022-11-09 17:51:08,911:INFO: Dataset: univ                Batch:  5/15	Loss 1.6823 (1.6936)
2022-11-09 17:51:09,080:INFO: Dataset: univ                Batch:  6/15	Loss 2.2194 (1.7851)
2022-11-09 17:51:09,248:INFO: Dataset: univ                Batch:  7/15	Loss 1.6115 (1.7612)
2022-11-09 17:51:09,414:INFO: Dataset: univ                Batch:  8/15	Loss 1.6242 (1.7457)
2022-11-09 17:51:09,582:INFO: Dataset: univ                Batch:  9/15	Loss 1.4789 (1.7171)
2022-11-09 17:51:09,747:INFO: Dataset: univ                Batch: 10/15	Loss 1.7095 (1.7163)
2022-11-09 17:51:09,915:INFO: Dataset: univ                Batch: 11/15	Loss 1.7289 (1.7173)
2022-11-09 17:51:10,079:INFO: Dataset: univ                Batch: 12/15	Loss 1.4211 (1.6906)
2022-11-09 17:51:10,245:INFO: Dataset: univ                Batch: 13/15	Loss 1.4516 (1.6725)
2022-11-09 17:51:10,410:INFO: Dataset: univ                Batch: 14/15	Loss 1.6933 (1.6740)
2022-11-09 17:51:10,452:INFO: Dataset: univ                Batch: 15/15	Loss 0.2670 (1.6497)
2022-11-09 17:51:10,838:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8497 (1.8497)
2022-11-09 17:51:10,999:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9034 (1.8784)
2022-11-09 17:51:11,159:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9283 (1.8971)
2022-11-09 17:51:11,321:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7571 (1.8549)
2022-11-09 17:51:11,481:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0153 (1.8883)
2022-11-09 17:51:11,640:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1406 (1.9252)
2022-11-09 17:51:11,800:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7115 (1.8941)
2022-11-09 17:51:11,938:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9141 (1.8966)
2022-11-09 17:51:12,326:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8365 (1.8365)
2022-11-09 17:51:12,489:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0011 (1.9160)
2022-11-09 17:51:12,654:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9014 (1.9107)
2022-11-09 17:51:12,813:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6168 (1.8400)
2022-11-09 17:51:12,974:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4619 (1.7661)
2022-11-09 17:51:13,136:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4815 (1.7191)
2022-11-09 17:51:13,297:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3533 (1.6689)
2022-11-09 17:51:13,457:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5092 (1.6490)
2022-11-09 17:51:13,617:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7485 (1.6605)
2022-11-09 17:51:13,775:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5964 (1.6539)
2022-11-09 17:51:13,937:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8265 (1.6690)
2022-11-09 17:51:14,097:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6286 (1.6657)
2022-11-09 17:51:14,261:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9745 (1.6894)
2022-11-09 17:51:14,422:INFO: Dataset: zara2               Batch: 14/18	Loss 1.4036 (1.6689)
2022-11-09 17:51:14,585:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6320 (1.6666)
2022-11-09 17:51:14,744:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3349 (1.6460)
2022-11-09 17:51:14,908:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4253 (1.6329)
2022-11-09 17:51:15,048:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6879 (1.6356)
2022-11-09 17:51:15,105:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_169.pth.tar
2022-11-09 17:51:15,105:INFO: 
===> EPOCH: 170 (P2)
2022-11-09 17:51:15,105:INFO: - Computing loss (training)
2022-11-09 17:51:15,458:INFO: Dataset: hotel               Batch: 1/4	Loss 3.2499 (3.2499)
2022-11-09 17:51:15,620:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4418 (2.8420)
2022-11-09 17:51:15,778:INFO: Dataset: hotel               Batch: 3/4	Loss 2.4722 (2.7156)
2022-11-09 17:51:15,877:INFO: Dataset: hotel               Batch: 4/4	Loss 2.4126 (2.6664)
2022-11-09 17:51:16,311:INFO: Dataset: univ                Batch:  1/15	Loss 1.2804 (1.2804)
2022-11-09 17:51:16,475:INFO: Dataset: univ                Batch:  2/15	Loss 1.4555 (1.3652)
2022-11-09 17:51:16,637:INFO: Dataset: univ                Batch:  3/15	Loss 1.3774 (1.3689)
2022-11-09 17:51:16,798:INFO: Dataset: univ                Batch:  4/15	Loss 1.6955 (1.4529)
2022-11-09 17:51:16,960:INFO: Dataset: univ                Batch:  5/15	Loss 1.3503 (1.4327)
2022-11-09 17:51:17,126:INFO: Dataset: univ                Batch:  6/15	Loss 1.3213 (1.4134)
2022-11-09 17:51:17,290:INFO: Dataset: univ                Batch:  7/15	Loss 1.3851 (1.4090)
2022-11-09 17:51:17,453:INFO: Dataset: univ                Batch:  8/15	Loss 1.2191 (1.3838)
2022-11-09 17:51:17,616:INFO: Dataset: univ                Batch:  9/15	Loss 1.4586 (1.3922)
2022-11-09 17:51:17,778:INFO: Dataset: univ                Batch: 10/15	Loss 1.2315 (1.3759)
2022-11-09 17:51:17,943:INFO: Dataset: univ                Batch: 11/15	Loss 1.5041 (1.3877)
2022-11-09 17:51:18,105:INFO: Dataset: univ                Batch: 12/15	Loss 1.8296 (1.4210)
2022-11-09 17:51:18,269:INFO: Dataset: univ                Batch: 13/15	Loss 2.1521 (1.4698)
2022-11-09 17:51:18,433:INFO: Dataset: univ                Batch: 14/15	Loss 1.3238 (1.4589)
2022-11-09 17:51:18,475:INFO: Dataset: univ                Batch: 15/15	Loss 0.2182 (1.4440)
2022-11-09 17:51:18,865:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5779 (1.5779)
2022-11-09 17:51:19,031:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9415 (1.7561)
2022-11-09 17:51:19,198:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6658 (1.7276)
2022-11-09 17:51:19,360:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9240 (1.7750)
2022-11-09 17:51:19,527:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8541 (1.7911)
2022-11-09 17:51:19,686:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1410 (1.8453)
2022-11-09 17:51:19,845:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6349 (1.8161)
2022-11-09 17:51:19,982:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3377 (1.7604)
2022-11-09 17:51:20,470:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5762 (1.5762)
2022-11-09 17:51:20,629:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4228 (1.4990)
2022-11-09 17:51:20,788:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2804 (1.4354)
2022-11-09 17:51:20,946:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4192 (1.4312)
2022-11-09 17:51:21,106:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4572 (1.4369)
2022-11-09 17:51:21,264:INFO: Dataset: zara2               Batch:  6/18	Loss 1.3925 (1.4288)
2022-11-09 17:51:21,424:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8181 (1.4837)
2022-11-09 17:51:21,581:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3535 (1.4665)
2022-11-09 17:51:21,740:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4898 (1.4691)
2022-11-09 17:51:21,898:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3285 (1.4526)
2022-11-09 17:51:22,060:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8212 (1.4887)
2022-11-09 17:51:22,217:INFO: Dataset: zara2               Batch: 12/18	Loss 1.2488 (1.4696)
2022-11-09 17:51:22,378:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7611 (1.4926)
2022-11-09 17:51:22,536:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3418 (1.4828)
2022-11-09 17:51:22,697:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4411 (1.4801)
2022-11-09 17:51:22,855:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3413 (1.4716)
2022-11-09 17:51:23,017:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2817 (1.4611)
2022-11-09 17:51:23,155:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2639 (1.4512)
2022-11-09 17:51:23,208:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_170.pth.tar
2022-11-09 17:51:23,208:INFO: 
===> EPOCH: 171 (P2)
2022-11-09 17:51:23,209:INFO: - Computing loss (training)
2022-11-09 17:51:23,550:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2172 (2.2172)
2022-11-09 17:51:23,713:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2753 (2.2479)
2022-11-09 17:51:23,870:INFO: Dataset: hotel               Batch: 3/4	Loss 2.2236 (2.2397)
2022-11-09 17:51:23,969:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3808 (2.0856)
2022-11-09 17:51:24,380:INFO: Dataset: univ                Batch:  1/15	Loss 1.4386 (1.4386)
2022-11-09 17:51:24,550:INFO: Dataset: univ                Batch:  2/15	Loss 1.6403 (1.5423)
2022-11-09 17:51:24,714:INFO: Dataset: univ                Batch:  3/15	Loss 1.2838 (1.4639)
2022-11-09 17:51:24,876:INFO: Dataset: univ                Batch:  4/15	Loss 1.3432 (1.4343)
2022-11-09 17:51:25,045:INFO: Dataset: univ                Batch:  5/15	Loss 1.3983 (1.4269)
2022-11-09 17:51:25,208:INFO: Dataset: univ                Batch:  6/15	Loss 1.2992 (1.4066)
2022-11-09 17:51:25,374:INFO: Dataset: univ                Batch:  7/15	Loss 1.5599 (1.4287)
2022-11-09 17:51:25,537:INFO: Dataset: univ                Batch:  8/15	Loss 1.1064 (1.3887)
2022-11-09 17:51:25,702:INFO: Dataset: univ                Batch:  9/15	Loss 1.7987 (1.4358)
2022-11-09 17:51:25,863:INFO: Dataset: univ                Batch: 10/15	Loss 3.1431 (1.5876)
2022-11-09 17:51:26,032:INFO: Dataset: univ                Batch: 11/15	Loss 1.2042 (1.5509)
2022-11-09 17:51:26,197:INFO: Dataset: univ                Batch: 12/15	Loss 1.1599 (1.5160)
2022-11-09 17:51:26,362:INFO: Dataset: univ                Batch: 13/15	Loss 1.1661 (1.4877)
2022-11-09 17:51:26,527:INFO: Dataset: univ                Batch: 14/15	Loss 1.2580 (1.4716)
2022-11-09 17:51:26,569:INFO: Dataset: univ                Batch: 15/15	Loss 0.2134 (1.4556)
2022-11-09 17:51:26,956:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3251 (2.3251)
2022-11-09 17:51:27,121:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0322 (2.1728)
2022-11-09 17:51:27,280:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9215 (2.0946)
2022-11-09 17:51:27,437:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6063 (1.9737)
2022-11-09 17:51:27,597:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4237 (1.8591)
2022-11-09 17:51:27,757:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5321 (1.8070)
2022-11-09 17:51:27,917:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1880 (1.8626)
2022-11-09 17:51:28,052:INFO: Dataset: zara1               Batch: 8/8	Loss 2.8174 (1.9606)
2022-11-09 17:51:28,460:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5017 (1.5017)
2022-11-09 17:51:28,620:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2706 (1.3928)
2022-11-09 17:51:28,781:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4435 (1.4113)
2022-11-09 17:51:28,943:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3920 (1.4065)
2022-11-09 17:51:29,105:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8764 (1.5002)
2022-11-09 17:51:29,272:INFO: Dataset: zara2               Batch:  6/18	Loss 1.3987 (1.4851)
2022-11-09 17:51:29,434:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3605 (1.4675)
2022-11-09 17:51:29,593:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2266 (1.4384)
2022-11-09 17:51:29,754:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3678 (1.4303)
2022-11-09 17:51:29,913:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2666 (1.4148)
2022-11-09 17:51:30,077:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9022 (1.4574)
2022-11-09 17:51:30,235:INFO: Dataset: zara2               Batch: 12/18	Loss 1.2272 (1.4382)
2022-11-09 17:51:30,397:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3100 (1.4288)
2022-11-09 17:51:30,635:INFO: Dataset: zara2               Batch: 14/18	Loss 1.2522 (1.4158)
2022-11-09 17:51:30,796:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8296 (1.4408)
2022-11-09 17:51:30,959:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1870 (1.4258)
2022-11-09 17:51:31,119:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3259 (1.4199)
2022-11-09 17:51:31,261:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0328 (1.4023)
2022-11-09 17:51:31,317:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_171.pth.tar
2022-11-09 17:51:31,317:INFO: 
===> EPOCH: 172 (P2)
2022-11-09 17:51:31,317:INFO: - Computing loss (training)
2022-11-09 17:51:31,661:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6817 (2.6817)
2022-11-09 17:51:31,824:INFO: Dataset: hotel               Batch: 2/4	Loss 3.1427 (2.9203)
2022-11-09 17:51:31,983:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7389 (3.1902)
2022-11-09 17:51:32,082:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9932 (2.8395)
2022-11-09 17:51:32,555:INFO: Dataset: univ                Batch:  1/15	Loss 1.4651 (1.4651)
2022-11-09 17:51:32,718:INFO: Dataset: univ                Batch:  2/15	Loss 1.0994 (1.2894)
2022-11-09 17:51:32,881:INFO: Dataset: univ                Batch:  3/15	Loss 1.4147 (1.3309)
2022-11-09 17:51:33,044:INFO: Dataset: univ                Batch:  4/15	Loss 1.1329 (1.2805)
2022-11-09 17:51:33,206:INFO: Dataset: univ                Batch:  5/15	Loss 1.2053 (1.2656)
2022-11-09 17:51:33,369:INFO: Dataset: univ                Batch:  6/15	Loss 1.5202 (1.3092)
2022-11-09 17:51:33,532:INFO: Dataset: univ                Batch:  7/15	Loss 1.0469 (1.2722)
2022-11-09 17:51:33,692:INFO: Dataset: univ                Batch:  8/15	Loss 1.8155 (1.3333)
2022-11-09 17:51:33,855:INFO: Dataset: univ                Batch:  9/15	Loss 1.3456 (1.3345)
2022-11-09 17:51:34,017:INFO: Dataset: univ                Batch: 10/15	Loss 1.0098 (1.3012)
2022-11-09 17:51:34,183:INFO: Dataset: univ                Batch: 11/15	Loss 1.1377 (1.2870)
2022-11-09 17:51:34,344:INFO: Dataset: univ                Batch: 12/15	Loss 1.8278 (1.3321)
2022-11-09 17:51:34,508:INFO: Dataset: univ                Batch: 13/15	Loss 2.0434 (1.3881)
2022-11-09 17:51:34,672:INFO: Dataset: univ                Batch: 14/15	Loss 2.0126 (1.4374)
2022-11-09 17:51:34,714:INFO: Dataset: univ                Batch: 15/15	Loss 0.2203 (1.4207)
2022-11-09 17:51:35,114:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5689 (1.5689)
2022-11-09 17:51:35,279:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3112 (1.4470)
2022-11-09 17:51:35,437:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3321 (1.4115)
2022-11-09 17:51:35,593:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6442 (1.4610)
2022-11-09 17:51:35,751:INFO: Dataset: zara1               Batch: 5/8	Loss 1.3562 (1.4400)
2022-11-09 17:51:35,910:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5145 (1.4527)
2022-11-09 17:51:36,068:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3469 (1.4368)
2022-11-09 17:51:36,203:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7200 (1.4663)
2022-11-09 17:51:36,603:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2661 (1.2661)
2022-11-09 17:51:36,762:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4277 (1.3404)
2022-11-09 17:51:36,928:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1501 (1.2782)
2022-11-09 17:51:37,086:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2483 (1.2711)
2022-11-09 17:51:37,247:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2231 (1.2616)
2022-11-09 17:51:37,413:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2539 (1.2603)
2022-11-09 17:51:37,575:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2280 (1.2553)
2022-11-09 17:51:37,733:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3510 (1.2674)
2022-11-09 17:51:37,895:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1617 (1.2550)
2022-11-09 17:51:38,054:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2041 (1.2503)
2022-11-09 17:51:38,215:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1929 (1.2452)
2022-11-09 17:51:38,374:INFO: Dataset: zara2               Batch: 12/18	Loss 2.1125 (1.3169)
2022-11-09 17:51:38,536:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4069 (1.3233)
2022-11-09 17:51:38,695:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6258 (1.3439)
2022-11-09 17:51:38,858:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2302 (1.3361)
2022-11-09 17:51:39,019:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1917 (1.3275)
2022-11-09 17:51:39,181:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2084 (1.3200)
2022-11-09 17:51:39,321:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1188 (1.3112)
2022-11-09 17:51:39,375:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_172.pth.tar
2022-11-09 17:51:39,375:INFO: 
===> EPOCH: 173 (P2)
2022-11-09 17:51:39,376:INFO: - Computing loss (training)
2022-11-09 17:51:39,723:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6733 (1.6733)
2022-11-09 17:51:39,886:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4476 (1.5657)
2022-11-09 17:51:40,045:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0677 (1.7414)
2022-11-09 17:51:40,144:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9287 (1.5966)
2022-11-09 17:51:40,534:INFO: Dataset: univ                Batch:  1/15	Loss 1.0641 (1.0641)
2022-11-09 17:51:40,699:INFO: Dataset: univ                Batch:  2/15	Loss 2.1479 (1.5584)
2022-11-09 17:51:40,863:INFO: Dataset: univ                Batch:  3/15	Loss 1.0835 (1.3967)
2022-11-09 17:51:41,027:INFO: Dataset: univ                Batch:  4/15	Loss 1.0947 (1.3183)
2022-11-09 17:51:41,191:INFO: Dataset: univ                Batch:  5/15	Loss 1.1936 (1.2944)
2022-11-09 17:51:41,358:INFO: Dataset: univ                Batch:  6/15	Loss 1.4338 (1.3191)
2022-11-09 17:51:41,522:INFO: Dataset: univ                Batch:  7/15	Loss 1.2338 (1.3068)
2022-11-09 17:51:41,684:INFO: Dataset: univ                Batch:  8/15	Loss 1.1572 (1.2879)
2022-11-09 17:51:41,847:INFO: Dataset: univ                Batch:  9/15	Loss 1.0725 (1.2650)
2022-11-09 17:51:42,010:INFO: Dataset: univ                Batch: 10/15	Loss 1.1580 (1.2538)
2022-11-09 17:51:42,174:INFO: Dataset: univ                Batch: 11/15	Loss 1.0358 (1.2324)
2022-11-09 17:51:42,337:INFO: Dataset: univ                Batch: 12/15	Loss 0.9699 (1.2093)
2022-11-09 17:51:42,502:INFO: Dataset: univ                Batch: 13/15	Loss 1.0307 (1.1955)
2022-11-09 17:51:42,665:INFO: Dataset: univ                Batch: 14/15	Loss 1.0151 (1.1822)
2022-11-09 17:51:42,707:INFO: Dataset: univ                Batch: 15/15	Loss 0.1902 (1.1692)
2022-11-09 17:51:43,099:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3201 (1.3201)
2022-11-09 17:51:43,261:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7800 (1.5556)
2022-11-09 17:51:43,424:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3820 (1.5016)
2022-11-09 17:51:43,582:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2745 (1.4445)
2022-11-09 17:51:43,742:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2466 (1.4052)
2022-11-09 17:51:43,900:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8099 (1.4729)
2022-11-09 17:51:44,060:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1904 (1.4311)
2022-11-09 17:51:44,198:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2071 (1.4073)
2022-11-09 17:51:44,597:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2180 (1.2180)
2022-11-09 17:51:44,757:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1913 (1.2041)
2022-11-09 17:51:44,922:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1160 (1.1764)
2022-11-09 17:51:45,079:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1177 (1.1613)
2022-11-09 17:51:45,239:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2757 (1.1851)
2022-11-09 17:51:45,400:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0474 (1.1619)
2022-11-09 17:51:45,560:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9413 (1.1303)
2022-11-09 17:51:45,717:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0958 (1.1263)
2022-11-09 17:51:45,876:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3809 (1.1510)
2022-11-09 17:51:46,034:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1465 (1.1506)
2022-11-09 17:51:46,196:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2222 (1.1576)
2022-11-09 17:51:46,353:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1859 (1.1601)
2022-11-09 17:51:46,513:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1157 (1.1569)
2022-11-09 17:51:46,672:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5260 (1.1842)
2022-11-09 17:51:46,832:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5420 (1.2079)
2022-11-09 17:51:46,992:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2480 (1.2102)
2022-11-09 17:51:47,154:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0156 (1.1969)
2022-11-09 17:51:47,292:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0471 (1.1902)
2022-11-09 17:51:47,346:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_173.pth.tar
2022-11-09 17:51:47,346:INFO: 
===> EPOCH: 174 (P2)
2022-11-09 17:51:47,346:INFO: - Computing loss (training)
2022-11-09 17:51:47,680:INFO: Dataset: hotel               Batch: 1/4	Loss 4.5157 (4.5157)
2022-11-09 17:51:47,854:INFO: Dataset: hotel               Batch: 2/4	Loss 3.1201 (3.8096)
2022-11-09 17:51:48,013:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7501 (3.4593)
2022-11-09 17:51:48,111:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8228 (3.1808)
2022-11-09 17:51:48,520:INFO: Dataset: univ                Batch:  1/15	Loss 1.1703 (1.1703)
2022-11-09 17:51:48,686:INFO: Dataset: univ                Batch:  2/15	Loss 2.0232 (1.6126)
2022-11-09 17:51:48,849:INFO: Dataset: univ                Batch:  3/15	Loss 1.2029 (1.4768)
2022-11-09 17:51:49,013:INFO: Dataset: univ                Batch:  4/15	Loss 1.1483 (1.3972)
2022-11-09 17:51:49,176:INFO: Dataset: univ                Batch:  5/15	Loss 1.8421 (1.4836)
2022-11-09 17:51:49,339:INFO: Dataset: univ                Batch:  6/15	Loss 1.9688 (1.5666)
2022-11-09 17:51:49,502:INFO: Dataset: univ                Batch:  7/15	Loss 1.0500 (1.4985)
2022-11-09 17:51:49,663:INFO: Dataset: univ                Batch:  8/15	Loss 1.2591 (1.4711)
2022-11-09 17:51:49,827:INFO: Dataset: univ                Batch:  9/15	Loss 1.2868 (1.4472)
2022-11-09 17:51:49,989:INFO: Dataset: univ                Batch: 10/15	Loss 1.2776 (1.4303)
2022-11-09 17:51:50,152:INFO: Dataset: univ                Batch: 11/15	Loss 0.9318 (1.3810)
2022-11-09 17:51:50,314:INFO: Dataset: univ                Batch: 12/15	Loss 1.1514 (1.3619)
2022-11-09 17:51:50,479:INFO: Dataset: univ                Batch: 13/15	Loss 0.9644 (1.3287)
2022-11-09 17:51:50,641:INFO: Dataset: univ                Batch: 14/15	Loss 1.0850 (1.3105)
2022-11-09 17:51:50,683:INFO: Dataset: univ                Batch: 15/15	Loss 0.1773 (1.2928)
2022-11-09 17:51:51,078:INFO: Dataset: zara1               Batch: 1/8	Loss 3.3988 (3.3988)
2022-11-09 17:51:51,244:INFO: Dataset: zara1               Batch: 2/8	Loss 2.1423 (2.7507)
2022-11-09 17:51:51,404:INFO: Dataset: zara1               Batch: 3/8	Loss 2.9748 (2.8287)
2022-11-09 17:51:51,563:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2302 (2.4754)
2022-11-09 17:51:51,722:INFO: Dataset: zara1               Batch: 5/8	Loss 1.3511 (2.2600)
2022-11-09 17:51:51,881:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4549 (2.1329)
2022-11-09 17:51:52,041:INFO: Dataset: zara1               Batch: 7/8	Loss 2.3813 (2.1660)
2022-11-09 17:51:52,178:INFO: Dataset: zara1               Batch: 8/8	Loss 2.1490 (2.1642)
2022-11-09 17:51:52,571:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8035 (1.8035)
2022-11-09 17:51:52,733:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0594 (1.9294)
2022-11-09 17:51:52,895:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1032 (1.6732)
2022-11-09 17:51:53,053:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0336 (1.5065)
2022-11-09 17:51:53,217:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1760 (1.4458)
2022-11-09 17:51:53,380:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0002 (1.3678)
2022-11-09 17:51:53,540:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1488 (1.3388)
2022-11-09 17:51:53,698:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2113 (1.3223)
2022-11-09 17:51:53,859:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4714 (1.3408)
2022-11-09 17:51:54,017:INFO: Dataset: zara2               Batch: 10/18	Loss 2.7556 (1.4846)
2022-11-09 17:51:54,178:INFO: Dataset: zara2               Batch: 11/18	Loss 2.6754 (1.5963)
2022-11-09 17:51:54,338:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7842 (1.6129)
2022-11-09 17:51:54,499:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1187 (1.5730)
2022-11-09 17:51:54,658:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1542 (1.5398)
2022-11-09 17:51:54,820:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2211 (1.5212)
2022-11-09 17:51:54,981:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6715 (1.5309)
2022-11-09 17:51:55,145:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4020 (1.5237)
2022-11-09 17:51:55,284:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0907 (1.5024)
2022-11-09 17:51:55,335:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_174.pth.tar
2022-11-09 17:51:55,336:INFO: 
===> EPOCH: 175 (P2)
2022-11-09 17:51:55,336:INFO: - Computing loss (training)
2022-11-09 17:51:55,674:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5225 (1.5225)
2022-11-09 17:51:55,835:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3983 (1.4592)
2022-11-09 17:51:55,992:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3050 (1.4078)
2022-11-09 17:51:56,091:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9920 (1.3393)
2022-11-09 17:51:56,488:INFO: Dataset: univ                Batch:  1/15	Loss 1.0503 (1.0503)
2022-11-09 17:51:56,651:INFO: Dataset: univ                Batch:  2/15	Loss 1.0773 (1.0642)
2022-11-09 17:51:56,819:INFO: Dataset: univ                Batch:  3/15	Loss 1.2597 (1.1311)
2022-11-09 17:51:56,980:INFO: Dataset: univ                Batch:  4/15	Loss 1.1583 (1.1379)
2022-11-09 17:51:57,146:INFO: Dataset: univ                Batch:  5/15	Loss 1.0759 (1.1252)
2022-11-09 17:51:57,308:INFO: Dataset: univ                Batch:  6/15	Loss 1.0015 (1.1030)
2022-11-09 17:51:57,472:INFO: Dataset: univ                Batch:  7/15	Loss 1.1122 (1.1042)
2022-11-09 17:51:57,633:INFO: Dataset: univ                Batch:  8/15	Loss 0.8968 (1.0761)
2022-11-09 17:51:57,797:INFO: Dataset: univ                Batch:  9/15	Loss 1.2131 (1.0912)
2022-11-09 17:51:57,959:INFO: Dataset: univ                Batch: 10/15	Loss 1.1340 (1.0957)
2022-11-09 17:51:58,126:INFO: Dataset: univ                Batch: 11/15	Loss 0.9846 (1.0851)
2022-11-09 17:51:58,288:INFO: Dataset: univ                Batch: 12/15	Loss 1.0232 (1.0803)
2022-11-09 17:51:58,453:INFO: Dataset: univ                Batch: 13/15	Loss 1.6429 (1.1269)
2022-11-09 17:51:58,616:INFO: Dataset: univ                Batch: 14/15	Loss 0.8864 (1.1096)
2022-11-09 17:51:58,659:INFO: Dataset: univ                Batch: 15/15	Loss 0.1940 (1.0980)
2022-11-09 17:51:59,044:INFO: Dataset: zara1               Batch: 1/8	Loss 2.1072 (2.1072)
2022-11-09 17:51:59,210:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7379 (1.9244)
2022-11-09 17:51:59,372:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1053 (1.6401)
2022-11-09 17:51:59,530:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3807 (1.5777)
2022-11-09 17:51:59,691:INFO: Dataset: zara1               Batch: 5/8	Loss 2.1766 (1.6862)
2022-11-09 17:51:59,849:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5175 (1.6586)
2022-11-09 17:52:00,008:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5605 (1.6441)
2022-11-09 17:52:00,144:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0427 (1.5776)
2022-11-09 17:52:00,550:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4037 (1.4037)
2022-11-09 17:52:00,710:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3085 (1.3568)
2022-11-09 17:52:00,876:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9136 (1.1921)
2022-11-09 17:52:01,036:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9826 (1.1437)
2022-11-09 17:52:01,198:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1690 (1.1489)
2022-11-09 17:52:01,360:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1207 (1.1446)
2022-11-09 17:52:01,521:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1230 (1.1419)
2022-11-09 17:52:01,679:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1016 (1.1371)
2022-11-09 17:52:01,840:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7709 (1.2003)
2022-11-09 17:52:02,001:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0936 (1.1891)
2022-11-09 17:52:02,162:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9623 (1.1694)
2022-11-09 17:52:02,322:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9936 (1.1550)
2022-11-09 17:52:02,488:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0009 (1.1427)
2022-11-09 17:52:02,649:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9835 (1.1310)
2022-11-09 17:52:02,812:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9592 (1.1203)
2022-11-09 17:52:02,974:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0565 (1.1160)
2022-11-09 17:52:03,137:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0525 (1.1124)
2022-11-09 17:52:03,278:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1080 (1.1122)
2022-11-09 17:52:03,332:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_175.pth.tar
2022-11-09 17:52:03,332:INFO: 
===> EPOCH: 176 (P2)
2022-11-09 17:52:03,332:INFO: - Computing loss (training)
2022-11-09 17:52:03,686:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0968 (2.0968)
2022-11-09 17:52:03,850:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0809 (2.0887)
2022-11-09 17:52:04,010:INFO: Dataset: hotel               Batch: 3/4	Loss 4.5606 (2.8797)
2022-11-09 17:52:04,112:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6395 (2.4866)
2022-11-09 17:52:04,510:INFO: Dataset: univ                Batch:  1/15	Loss 1.1756 (1.1756)
2022-11-09 17:52:04,674:INFO: Dataset: univ                Batch:  2/15	Loss 1.5119 (1.3531)
2022-11-09 17:52:04,842:INFO: Dataset: univ                Batch:  3/15	Loss 1.0631 (1.2537)
2022-11-09 17:52:05,005:INFO: Dataset: univ                Batch:  4/15	Loss 0.8726 (1.1650)
2022-11-09 17:52:05,169:INFO: Dataset: univ                Batch:  5/15	Loss 0.9884 (1.1351)
2022-11-09 17:52:05,335:INFO: Dataset: univ                Batch:  6/15	Loss 1.3699 (1.1732)
2022-11-09 17:52:05,499:INFO: Dataset: univ                Batch:  7/15	Loss 0.9532 (1.1436)
2022-11-09 17:52:05,660:INFO: Dataset: univ                Batch:  8/15	Loss 1.0327 (1.1301)
2022-11-09 17:52:05,825:INFO: Dataset: univ                Batch:  9/15	Loss 0.9445 (1.1112)
2022-11-09 17:52:05,987:INFO: Dataset: univ                Batch: 10/15	Loss 0.9109 (1.0905)
2022-11-09 17:52:06,153:INFO: Dataset: univ                Batch: 11/15	Loss 0.8700 (1.0686)
2022-11-09 17:52:06,316:INFO: Dataset: univ                Batch: 12/15	Loss 0.9301 (1.0575)
2022-11-09 17:52:06,557:INFO: Dataset: univ                Batch: 13/15	Loss 1.3477 (1.0819)
2022-11-09 17:52:06,721:INFO: Dataset: univ                Batch: 14/15	Loss 1.1104 (1.0841)
2022-11-09 17:52:06,762:INFO: Dataset: univ                Batch: 15/15	Loss 0.1986 (1.0720)
2022-11-09 17:52:07,169:INFO: Dataset: zara1               Batch: 1/8	Loss 3.2363 (3.2363)
2022-11-09 17:52:07,330:INFO: Dataset: zara1               Batch: 2/8	Loss 3.5050 (3.3813)
2022-11-09 17:52:07,493:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7341 (2.8478)
2022-11-09 17:52:07,652:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0536 (2.3627)
2022-11-09 17:52:07,813:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4483 (2.1891)
2022-11-09 17:52:07,973:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2070 (2.0279)
2022-11-09 17:52:08,132:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0573 (2.0321)
2022-11-09 17:52:08,269:INFO: Dataset: zara1               Batch: 8/8	Loss 2.4267 (2.0798)
2022-11-09 17:52:08,664:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7119 (1.7119)
2022-11-09 17:52:08,825:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5670 (1.6378)
2022-11-09 17:52:08,987:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5393 (1.6011)
2022-11-09 17:52:09,146:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0464 (1.4600)
2022-11-09 17:52:09,307:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1256 (1.3949)
2022-11-09 17:52:09,470:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9115 (1.3127)
2022-11-09 17:52:09,631:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2396 (1.3026)
2022-11-09 17:52:09,787:INFO: Dataset: zara2               Batch:  8/18	Loss 2.6923 (1.4604)
2022-11-09 17:52:09,949:INFO: Dataset: zara2               Batch:  9/18	Loss 2.3605 (1.5546)
2022-11-09 17:52:10,109:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2820 (1.5265)
2022-11-09 17:52:10,269:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7784 (1.5481)
2022-11-09 17:52:10,429:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3416 (1.5310)
2022-11-09 17:52:10,590:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0755 (1.4973)
2022-11-09 17:52:10,748:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8969 (1.4518)
2022-11-09 17:52:10,911:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0724 (1.4243)
2022-11-09 17:52:11,071:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9829 (1.3964)
2022-11-09 17:52:11,233:INFO: Dataset: zara2               Batch: 17/18	Loss 1.1039 (1.3783)
2022-11-09 17:52:11,371:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0217 (1.3644)
2022-11-09 17:52:11,425:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_176.pth.tar
2022-11-09 17:52:11,425:INFO: 
===> EPOCH: 177 (P2)
2022-11-09 17:52:11,426:INFO: - Computing loss (training)
2022-11-09 17:52:11,758:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6679 (1.6679)
2022-11-09 17:52:11,923:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0902 (1.8888)
2022-11-09 17:52:12,082:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9003 (1.8927)
2022-11-09 17:52:12,181:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6742 (1.6773)
2022-11-09 17:52:12,572:INFO: Dataset: univ                Batch:  1/15	Loss 1.4891 (1.4891)
2022-11-09 17:52:12,738:INFO: Dataset: univ                Batch:  2/15	Loss 1.1290 (1.3088)
2022-11-09 17:52:12,903:INFO: Dataset: univ                Batch:  3/15	Loss 0.9788 (1.1946)
2022-11-09 17:52:13,070:INFO: Dataset: univ                Batch:  4/15	Loss 0.8782 (1.1158)
2022-11-09 17:52:13,237:INFO: Dataset: univ                Batch:  5/15	Loss 0.9717 (1.0879)
2022-11-09 17:52:13,400:INFO: Dataset: univ                Batch:  6/15	Loss 0.8087 (1.0422)
2022-11-09 17:52:13,565:INFO: Dataset: univ                Batch:  7/15	Loss 0.8722 (1.0179)
2022-11-09 17:52:13,727:INFO: Dataset: univ                Batch:  8/15	Loss 0.9792 (1.0133)
2022-11-09 17:52:13,891:INFO: Dataset: univ                Batch:  9/15	Loss 1.1529 (1.0271)
2022-11-09 17:52:14,055:INFO: Dataset: univ                Batch: 10/15	Loss 0.8713 (1.0115)
2022-11-09 17:52:14,223:INFO: Dataset: univ                Batch: 11/15	Loss 2.0290 (1.0956)
2022-11-09 17:52:14,388:INFO: Dataset: univ                Batch: 12/15	Loss 1.6437 (1.1396)
2022-11-09 17:52:14,553:INFO: Dataset: univ                Batch: 13/15	Loss 0.9161 (1.1228)
2022-11-09 17:52:14,717:INFO: Dataset: univ                Batch: 14/15	Loss 1.1363 (1.1238)
2022-11-09 17:52:14,760:INFO: Dataset: univ                Batch: 15/15	Loss 0.1772 (1.1076)
2022-11-09 17:52:15,152:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0594 (1.0594)
2022-11-09 17:52:15,313:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0654 (1.0623)
2022-11-09 17:52:15,474:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0283 (1.0512)
2022-11-09 17:52:15,631:INFO: Dataset: zara1               Batch: 4/8	Loss 2.2396 (1.3197)
2022-11-09 17:52:15,793:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2303 (1.3010)
2022-11-09 17:52:15,953:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0550 (1.2549)
2022-11-09 17:52:16,113:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1742 (1.2448)
2022-11-09 17:52:16,250:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9842 (1.2153)
2022-11-09 17:52:16,649:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9243 (0.9243)
2022-11-09 17:52:16,813:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9837 (0.9531)
2022-11-09 17:52:16,979:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9553 (0.9539)
2022-11-09 17:52:17,142:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9753 (0.9593)
2022-11-09 17:52:17,309:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8695 (0.9397)
2022-11-09 17:52:17,475:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9138 (0.9354)
2022-11-09 17:52:17,639:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9398 (0.9360)
2022-11-09 17:52:17,802:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0327 (0.9479)
2022-11-09 17:52:17,967:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9405 (0.9471)
2022-11-09 17:52:18,130:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9703 (0.9490)
2022-11-09 17:52:18,296:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8621 (0.9405)
2022-11-09 17:52:18,459:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7729 (1.0004)
2022-11-09 17:52:18,623:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0127 (1.0014)
2022-11-09 17:52:18,787:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1127 (1.0103)
2022-11-09 17:52:18,953:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9085 (1.0032)
2022-11-09 17:52:19,118:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9631 (1.0007)
2022-11-09 17:52:19,285:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8478 (0.9913)
2022-11-09 17:52:19,428:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7761 (0.9801)
2022-11-09 17:52:19,484:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_177.pth.tar
2022-11-09 17:52:19,485:INFO: 
===> EPOCH: 178 (P2)
2022-11-09 17:52:19,485:INFO: - Computing loss (training)
2022-11-09 17:52:19,831:INFO: Dataset: hotel               Batch: 1/4	Loss 5.0763 (5.0763)
2022-11-09 17:52:19,994:INFO: Dataset: hotel               Batch: 2/4	Loss 3.6310 (4.3487)
2022-11-09 17:52:20,153:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2777 (3.3784)
2022-11-09 17:52:20,251:INFO: Dataset: hotel               Batch: 4/4	Loss 2.2615 (3.1942)
2022-11-09 17:52:20,657:INFO: Dataset: univ                Batch:  1/15	Loss 1.4418 (1.4418)
2022-11-09 17:52:20,821:INFO: Dataset: univ                Batch:  2/15	Loss 0.8808 (1.2105)
2022-11-09 17:52:20,987:INFO: Dataset: univ                Batch:  3/15	Loss 1.5575 (1.3281)
2022-11-09 17:52:21,149:INFO: Dataset: univ                Batch:  4/15	Loss 0.8114 (1.1914)
2022-11-09 17:52:21,315:INFO: Dataset: univ                Batch:  5/15	Loss 1.1101 (1.1752)
2022-11-09 17:52:21,481:INFO: Dataset: univ                Batch:  6/15	Loss 0.8233 (1.1150)
2022-11-09 17:52:21,645:INFO: Dataset: univ                Batch:  7/15	Loss 0.8356 (1.0784)
2022-11-09 17:52:21,807:INFO: Dataset: univ                Batch:  8/15	Loss 0.9452 (1.0604)
2022-11-09 17:52:21,973:INFO: Dataset: univ                Batch:  9/15	Loss 0.9546 (1.0496)
2022-11-09 17:52:22,135:INFO: Dataset: univ                Batch: 10/15	Loss 0.9731 (1.0418)
2022-11-09 17:52:22,300:INFO: Dataset: univ                Batch: 11/15	Loss 1.2178 (1.0570)
2022-11-09 17:52:22,463:INFO: Dataset: univ                Batch: 12/15	Loss 0.9387 (1.0480)
2022-11-09 17:52:22,628:INFO: Dataset: univ                Batch: 13/15	Loss 0.9000 (1.0368)
2022-11-09 17:52:22,792:INFO: Dataset: univ                Batch: 14/15	Loss 1.0205 (1.0357)
2022-11-09 17:52:22,833:INFO: Dataset: univ                Batch: 15/15	Loss 0.1591 (1.0263)
2022-11-09 17:52:23,223:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5572 (1.5572)
2022-11-09 17:52:23,383:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5129 (1.5342)
2022-11-09 17:52:23,542:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1300 (1.4081)
2022-11-09 17:52:23,697:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2057 (1.3557)
2022-11-09 17:52:23,856:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0777 (1.2958)
2022-11-09 17:52:24,015:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0449 (1.2566)
2022-11-09 17:52:24,173:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8259 (1.3452)
2022-11-09 17:52:24,307:INFO: Dataset: zara1               Batch: 8/8	Loss 2.1381 (1.4308)
2022-11-09 17:52:24,715:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5453 (1.5453)
2022-11-09 17:52:24,876:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0342 (1.2860)
2022-11-09 17:52:25,038:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0288 (1.1903)
2022-11-09 17:52:25,195:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2850 (1.2152)
2022-11-09 17:52:25,360:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9854 (1.1680)
2022-11-09 17:52:25,521:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9500 (1.1299)
2022-11-09 17:52:25,681:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0835 (1.1227)
2022-11-09 17:52:25,838:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7838 (1.0822)
2022-11-09 17:52:26,001:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9066 (1.0620)
2022-11-09 17:52:26,160:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0934 (1.0650)
2022-11-09 17:52:26,322:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8348 (1.0416)
2022-11-09 17:52:26,483:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8868 (1.0301)
2022-11-09 17:52:26,644:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9333 (1.0229)
2022-11-09 17:52:26,802:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1547 (1.0321)
2022-11-09 17:52:26,965:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0285 (1.0319)
2022-11-09 17:52:27,125:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9934 (1.0295)
2022-11-09 17:52:27,286:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5221 (1.0606)
2022-11-09 17:52:27,425:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7797 (1.0483)
2022-11-09 17:52:27,481:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_178.pth.tar
2022-11-09 17:52:27,481:INFO: 
===> EPOCH: 179 (P2)
2022-11-09 17:52:27,482:INFO: - Computing loss (training)
2022-11-09 17:52:27,826:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1719 (1.1719)
2022-11-09 17:52:27,991:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9703 (1.0747)
2022-11-09 17:52:28,150:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0592 (1.0694)
2022-11-09 17:52:28,250:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7482 (1.0177)
2022-11-09 17:52:28,644:INFO: Dataset: univ                Batch:  1/15	Loss 0.8920 (0.8920)
2022-11-09 17:52:28,808:INFO: Dataset: univ                Batch:  2/15	Loss 0.9220 (0.9070)
2022-11-09 17:52:28,974:INFO: Dataset: univ                Batch:  3/15	Loss 0.7691 (0.8608)
2022-11-09 17:52:29,142:INFO: Dataset: univ                Batch:  4/15	Loss 1.3483 (0.9959)
2022-11-09 17:52:29,306:INFO: Dataset: univ                Batch:  5/15	Loss 1.4964 (1.0851)
2022-11-09 17:52:29,470:INFO: Dataset: univ                Batch:  6/15	Loss 0.7694 (1.0318)
2022-11-09 17:52:29,635:INFO: Dataset: univ                Batch:  7/15	Loss 0.8146 (0.9990)
2022-11-09 17:52:29,796:INFO: Dataset: univ                Batch:  8/15	Loss 0.8162 (0.9764)
2022-11-09 17:52:29,961:INFO: Dataset: univ                Batch:  9/15	Loss 0.8444 (0.9603)
2022-11-09 17:52:30,123:INFO: Dataset: univ                Batch: 10/15	Loss 0.9739 (0.9616)
2022-11-09 17:52:30,290:INFO: Dataset: univ                Batch: 11/15	Loss 0.7506 (0.9419)
2022-11-09 17:52:30,452:INFO: Dataset: univ                Batch: 12/15	Loss 0.7841 (0.9290)
2022-11-09 17:52:30,616:INFO: Dataset: univ                Batch: 13/15	Loss 0.8875 (0.9260)
2022-11-09 17:52:30,780:INFO: Dataset: univ                Batch: 14/15	Loss 0.9107 (0.9249)
2022-11-09 17:52:30,822:INFO: Dataset: univ                Batch: 15/15	Loss 0.1371 (0.9157)
2022-11-09 17:52:31,208:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3352 (1.3352)
2022-11-09 17:52:31,368:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9708 (1.1485)
2022-11-09 17:52:31,527:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0649 (1.1203)
2022-11-09 17:52:31,682:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8848 (1.0610)
2022-11-09 17:52:31,840:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0369 (1.0564)
2022-11-09 17:52:32,000:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0106 (1.0492)
2022-11-09 17:52:32,159:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7135 (1.1500)
2022-11-09 17:52:32,294:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3336 (1.1700)
2022-11-09 17:52:32,709:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1697 (1.1697)
2022-11-09 17:52:32,868:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2080 (1.1887)
2022-11-09 17:52:33,028:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8705 (1.0886)
2022-11-09 17:52:33,190:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8614 (1.0287)
2022-11-09 17:52:33,350:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5013 (1.1245)
2022-11-09 17:52:33,512:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7845 (1.0656)
2022-11-09 17:52:33,671:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2202 (1.0851)
2022-11-09 17:52:33,827:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7557 (1.1714)
2022-11-09 17:52:33,987:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7943 (1.1291)
2022-11-09 17:52:34,144:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9774 (1.1147)
2022-11-09 17:52:34,304:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8646 (1.0935)
2022-11-09 17:52:34,464:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7530 (1.0621)
2022-11-09 17:52:34,623:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7973 (1.0423)
2022-11-09 17:52:34,781:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9339 (1.0346)
2022-11-09 17:52:34,942:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8201 (1.0206)
2022-11-09 17:52:35,101:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9654 (1.0170)
2022-11-09 17:52:35,262:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5620 (1.0486)
2022-11-09 17:52:35,401:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2278 (1.0573)
2022-11-09 17:52:35,457:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_179.pth.tar
2022-11-09 17:52:35,457:INFO: 
===> EPOCH: 180 (P2)
2022-11-09 17:52:35,457:INFO: - Computing loss (training)
2022-11-09 17:52:35,798:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1717 (1.1717)
2022-11-09 17:52:35,961:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9090 (1.0384)
2022-11-09 17:52:36,118:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0534 (1.0437)
2022-11-09 17:52:36,218:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5614 (0.9591)
2022-11-09 17:52:36,612:INFO: Dataset: univ                Batch:  1/15	Loss 0.8453 (0.8453)
2022-11-09 17:52:36,778:INFO: Dataset: univ                Batch:  2/15	Loss 1.1943 (1.0113)
2022-11-09 17:52:36,944:INFO: Dataset: univ                Batch:  3/15	Loss 0.9886 (1.0038)
2022-11-09 17:52:37,110:INFO: Dataset: univ                Batch:  4/15	Loss 0.7857 (0.9465)
2022-11-09 17:52:37,275:INFO: Dataset: univ                Batch:  5/15	Loss 0.7637 (0.9072)
2022-11-09 17:52:37,438:INFO: Dataset: univ                Batch:  6/15	Loss 0.8019 (0.8899)
2022-11-09 17:52:37,604:INFO: Dataset: univ                Batch:  7/15	Loss 0.7825 (0.8736)
2022-11-09 17:52:37,766:INFO: Dataset: univ                Batch:  8/15	Loss 0.8122 (0.8656)
2022-11-09 17:52:37,931:INFO: Dataset: univ                Batch:  9/15	Loss 0.8274 (0.8613)
2022-11-09 17:52:38,094:INFO: Dataset: univ                Batch: 10/15	Loss 1.0501 (0.8803)
2022-11-09 17:52:38,260:INFO: Dataset: univ                Batch: 11/15	Loss 0.7709 (0.8712)
2022-11-09 17:52:38,422:INFO: Dataset: univ                Batch: 12/15	Loss 0.7680 (0.8625)
2022-11-09 17:52:38,588:INFO: Dataset: univ                Batch: 13/15	Loss 0.8519 (0.8617)
2022-11-09 17:52:38,751:INFO: Dataset: univ                Batch: 14/15	Loss 0.7644 (0.8549)
2022-11-09 17:52:38,793:INFO: Dataset: univ                Batch: 15/15	Loss 0.1258 (0.8424)
2022-11-09 17:52:39,180:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8631 (1.8631)
2022-11-09 17:52:39,340:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1956 (1.5294)
2022-11-09 17:52:39,501:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0417 (1.3691)
2022-11-09 17:52:39,656:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0449 (1.2902)
2022-11-09 17:52:39,818:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9450 (1.2196)
2022-11-09 17:52:39,976:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1718 (1.2116)
2022-11-09 17:52:40,135:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1245 (1.1984)
2022-11-09 17:52:40,271:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9223 (1.1664)
2022-11-09 17:52:40,685:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2722 (1.2722)
2022-11-09 17:52:40,845:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0245 (1.1491)
2022-11-09 17:52:41,007:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8084 (1.0310)
2022-11-09 17:52:41,167:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8083 (0.9717)
2022-11-09 17:52:41,405:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8225 (0.9386)
2022-11-09 17:52:41,566:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8097 (0.9185)
2022-11-09 17:52:41,726:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7899 (0.8984)
2022-11-09 17:52:41,882:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7542 (0.8797)
2022-11-09 17:52:42,043:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8014 (0.8706)
2022-11-09 17:52:42,200:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7841 (0.8626)
2022-11-09 17:52:42,360:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7190 (0.8479)
2022-11-09 17:52:42,521:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3776 (0.8863)
2022-11-09 17:52:42,682:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9090 (0.8882)
2022-11-09 17:52:42,841:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7461 (0.8781)
2022-11-09 17:52:43,003:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7800 (0.8707)
2022-11-09 17:52:43,162:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7412 (0.8629)
2022-11-09 17:52:43,324:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7721 (0.8571)
2022-11-09 17:52:43,463:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1119 (0.8678)
2022-11-09 17:52:43,516:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_180.pth.tar
2022-11-09 17:52:43,516:INFO: 
===> EPOCH: 181 (P2)
2022-11-09 17:52:43,517:INFO: - Computing loss (training)
2022-11-09 17:52:43,861:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4096 (1.4096)
2022-11-09 17:52:44,025:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0397 (1.2269)
2022-11-09 17:52:44,185:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1493 (1.2018)
2022-11-09 17:52:44,284:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5351 (1.0769)
2022-11-09 17:52:44,680:INFO: Dataset: univ                Batch:  1/15	Loss 1.2293 (1.2293)
2022-11-09 17:52:44,849:INFO: Dataset: univ                Batch:  2/15	Loss 0.7072 (0.9925)
2022-11-09 17:52:45,013:INFO: Dataset: univ                Batch:  3/15	Loss 1.0967 (1.0240)
2022-11-09 17:52:45,175:INFO: Dataset: univ                Batch:  4/15	Loss 1.2732 (1.0889)
2022-11-09 17:52:45,340:INFO: Dataset: univ                Batch:  5/15	Loss 0.7817 (1.0273)
2022-11-09 17:52:45,504:INFO: Dataset: univ                Batch:  6/15	Loss 0.9294 (1.0111)
2022-11-09 17:52:45,668:INFO: Dataset: univ                Batch:  7/15	Loss 0.7516 (0.9775)
2022-11-09 17:52:45,829:INFO: Dataset: univ                Batch:  8/15	Loss 0.9031 (0.9681)
2022-11-09 17:52:45,994:INFO: Dataset: univ                Batch:  9/15	Loss 0.7566 (0.9440)
2022-11-09 17:52:46,155:INFO: Dataset: univ                Batch: 10/15	Loss 0.7045 (0.9225)
2022-11-09 17:52:46,319:INFO: Dataset: univ                Batch: 11/15	Loss 1.0578 (0.9336)
2022-11-09 17:52:46,484:INFO: Dataset: univ                Batch: 12/15	Loss 0.7029 (0.9122)
2022-11-09 17:52:46,649:INFO: Dataset: univ                Batch: 13/15	Loss 0.7471 (0.8984)
2022-11-09 17:52:46,812:INFO: Dataset: univ                Batch: 14/15	Loss 0.7948 (0.8908)
2022-11-09 17:52:46,853:INFO: Dataset: univ                Batch: 15/15	Loss 0.1439 (0.8810)
2022-11-09 17:52:47,242:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9691 (1.9691)
2022-11-09 17:52:47,409:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0864 (1.5296)
2022-11-09 17:52:47,568:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1156 (1.3920)
2022-11-09 17:52:47,724:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1114 (1.3240)
2022-11-09 17:52:47,885:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8589 (1.2195)
2022-11-09 17:52:48,045:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9459 (1.1776)
2022-11-09 17:52:48,205:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2825 (1.1925)
2022-11-09 17:52:48,341:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1083 (1.1829)
2022-11-09 17:52:48,769:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2002 (1.2002)
2022-11-09 17:52:48,928:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9668 (1.0827)
2022-11-09 17:52:49,090:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0245 (1.0606)
2022-11-09 17:52:49,249:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8200 (0.9938)
2022-11-09 17:52:49,412:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8383 (0.9624)
2022-11-09 17:52:49,576:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8314 (0.9398)
2022-11-09 17:52:49,737:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7924 (0.9200)
2022-11-09 17:52:49,896:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2567 (0.9656)
2022-11-09 17:52:50,057:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9056 (0.9589)
2022-11-09 17:52:50,216:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0384 (0.9682)
2022-11-09 17:52:50,378:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7660 (1.0352)
2022-11-09 17:52:50,540:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8408 (1.0179)
2022-11-09 17:52:50,701:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7294 (0.9969)
2022-11-09 17:52:50,862:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9667 (0.9945)
2022-11-09 17:52:51,025:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8639 (0.9855)
2022-11-09 17:52:51,186:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7953 (0.9729)
2022-11-09 17:52:51,348:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7509 (0.9590)
2022-11-09 17:52:51,489:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6799 (0.9440)
2022-11-09 17:52:51,545:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_181.pth.tar
2022-11-09 17:52:51,545:INFO: 
===> EPOCH: 182 (P2)
2022-11-09 17:52:51,546:INFO: - Computing loss (training)
2022-11-09 17:52:51,889:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9290 (0.9290)
2022-11-09 17:52:52,052:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8088 (0.8683)
2022-11-09 17:52:52,212:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9378 (0.8910)
2022-11-09 17:52:52,311:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5201 (0.9856)
2022-11-09 17:52:52,707:INFO: Dataset: univ                Batch:  1/15	Loss 0.9692 (0.9692)
2022-11-09 17:52:52,875:INFO: Dataset: univ                Batch:  2/15	Loss 0.6789 (0.8065)
2022-11-09 17:52:53,045:INFO: Dataset: univ                Batch:  3/15	Loss 1.0473 (0.8892)
2022-11-09 17:52:53,207:INFO: Dataset: univ                Batch:  4/15	Loss 0.9051 (0.8930)
2022-11-09 17:52:53,371:INFO: Dataset: univ                Batch:  5/15	Loss 0.7524 (0.8629)
2022-11-09 17:52:53,536:INFO: Dataset: univ                Batch:  6/15	Loss 0.6991 (0.8331)
2022-11-09 17:52:53,700:INFO: Dataset: univ                Batch:  7/15	Loss 0.8465 (0.8349)
2022-11-09 17:52:53,861:INFO: Dataset: univ                Batch:  8/15	Loss 1.0480 (0.8576)
2022-11-09 17:52:54,026:INFO: Dataset: univ                Batch:  9/15	Loss 0.8233 (0.8537)
2022-11-09 17:52:54,189:INFO: Dataset: univ                Batch: 10/15	Loss 0.9536 (0.8641)
2022-11-09 17:52:54,356:INFO: Dataset: univ                Batch: 11/15	Loss 1.0398 (0.8799)
2022-11-09 17:52:54,520:INFO: Dataset: univ                Batch: 12/15	Loss 0.8910 (0.8808)
2022-11-09 17:52:54,685:INFO: Dataset: univ                Batch: 13/15	Loss 0.7583 (0.8707)
2022-11-09 17:52:54,850:INFO: Dataset: univ                Batch: 14/15	Loss 0.7442 (0.8610)
2022-11-09 17:52:54,893:INFO: Dataset: univ                Batch: 15/15	Loss 0.1459 (0.8517)
2022-11-09 17:52:55,279:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7883 (1.7883)
2022-11-09 17:52:55,439:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0451 (1.3725)
2022-11-09 17:52:55,599:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1256 (1.2993)
2022-11-09 17:52:55,755:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8693 (1.1919)
2022-11-09 17:52:55,913:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9323 (1.1434)
2022-11-09 17:52:56,073:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2813 (1.1661)
2022-11-09 17:52:56,231:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8701 (1.1257)
2022-11-09 17:52:56,367:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7619 (1.0845)
2022-11-09 17:52:56,766:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2288 (1.2288)
2022-11-09 17:52:56,932:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8118 (1.0318)
2022-11-09 17:52:57,094:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7843 (0.9479)
2022-11-09 17:52:57,253:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7277 (0.8944)
2022-11-09 17:52:57,415:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8770 (0.8911)
2022-11-09 17:52:57,580:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8270 (0.8796)
2022-11-09 17:52:57,741:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7097 (0.8570)
2022-11-09 17:52:57,901:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8112 (0.8505)
2022-11-09 17:52:58,064:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1755 (0.8844)
2022-11-09 17:52:58,224:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7698 (0.8723)
2022-11-09 17:52:58,386:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8319 (0.8688)
2022-11-09 17:52:58,549:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8332 (0.8656)
2022-11-09 17:52:58,711:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8073 (0.8608)
2022-11-09 17:52:58,872:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7128 (0.8500)
2022-11-09 17:52:59,036:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1371 (0.8668)
2022-11-09 17:52:59,199:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7310 (0.8587)
2022-11-09 17:52:59,363:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7652 (0.8529)
2022-11-09 17:52:59,505:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7325 (0.8478)
2022-11-09 17:52:59,557:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_182.pth.tar
2022-11-09 17:52:59,557:INFO: 
===> EPOCH: 183 (P2)
2022-11-09 17:52:59,558:INFO: - Computing loss (training)
2022-11-09 17:52:59,899:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9787 (0.9787)
2022-11-09 17:53:00,064:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4608 (1.2158)
2022-11-09 17:53:00,221:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0573 (1.1637)
2022-11-09 17:53:00,319:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4721 (1.0497)
2022-11-09 17:53:00,721:INFO: Dataset: univ                Batch:  1/15	Loss 0.8374 (0.8374)
2022-11-09 17:53:00,887:INFO: Dataset: univ                Batch:  2/15	Loss 0.8752 (0.8546)
2022-11-09 17:53:01,054:INFO: Dataset: univ                Batch:  3/15	Loss 0.8631 (0.8575)
2022-11-09 17:53:01,214:INFO: Dataset: univ                Batch:  4/15	Loss 0.8160 (0.8475)
2022-11-09 17:53:01,378:INFO: Dataset: univ                Batch:  5/15	Loss 0.6950 (0.8161)
2022-11-09 17:53:01,542:INFO: Dataset: univ                Batch:  6/15	Loss 0.7448 (0.8038)
2022-11-09 17:53:01,705:INFO: Dataset: univ                Batch:  7/15	Loss 0.6962 (0.7890)
2022-11-09 17:53:01,864:INFO: Dataset: univ                Batch:  8/15	Loss 0.7169 (0.7796)
2022-11-09 17:53:02,028:INFO: Dataset: univ                Batch:  9/15	Loss 0.6765 (0.7677)
2022-11-09 17:53:02,189:INFO: Dataset: univ                Batch: 10/15	Loss 0.6787 (0.7593)
2022-11-09 17:53:02,355:INFO: Dataset: univ                Batch: 11/15	Loss 0.6679 (0.7513)
2022-11-09 17:53:02,519:INFO: Dataset: univ                Batch: 12/15	Loss 0.7009 (0.7476)
2022-11-09 17:53:02,684:INFO: Dataset: univ                Batch: 13/15	Loss 0.8908 (0.7586)
2022-11-09 17:53:02,847:INFO: Dataset: univ                Batch: 14/15	Loss 1.7624 (0.8276)
2022-11-09 17:53:02,889:INFO: Dataset: univ                Batch: 15/15	Loss 0.1468 (0.8183)
2022-11-09 17:53:03,293:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8089 (1.8089)
2022-11-09 17:53:03,463:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2427 (1.5292)
2022-11-09 17:53:03,624:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0500 (1.3716)
2022-11-09 17:53:03,781:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9470 (1.2621)
2022-11-09 17:53:03,940:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6799 (1.3380)
2022-11-09 17:53:04,102:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1605 (1.3118)
2022-11-09 17:53:04,261:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2491 (1.3026)
2022-11-09 17:53:04,399:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8991 (1.3679)
2022-11-09 17:53:04,876:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8072 (0.8072)
2022-11-09 17:53:05,034:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7664 (0.7863)
2022-11-09 17:53:05,196:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6601 (0.7427)
2022-11-09 17:53:05,353:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7142 (0.7355)
2022-11-09 17:53:05,514:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8701 (0.7612)
2022-11-09 17:53:05,674:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9745 (0.7991)
2022-11-09 17:53:05,834:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0558 (0.8354)
2022-11-09 17:53:05,991:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5034 (0.9285)
2022-11-09 17:53:06,152:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9178 (0.9273)
2022-11-09 17:53:06,309:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7661 (0.9100)
2022-11-09 17:53:06,472:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0754 (0.9229)
2022-11-09 17:53:06,630:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7582 (0.9099)
2022-11-09 17:53:06,792:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7683 (0.8987)
2022-11-09 17:53:06,950:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9506 (0.9020)
2022-11-09 17:53:07,112:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6717 (0.8873)
2022-11-09 17:53:07,271:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9131 (0.8890)
2022-11-09 17:53:07,434:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6568 (0.8767)
2022-11-09 17:53:07,574:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5963 (0.8630)
2022-11-09 17:53:07,629:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_183.pth.tar
2022-11-09 17:53:07,629:INFO: 
===> EPOCH: 184 (P2)
2022-11-09 17:53:07,629:INFO: - Computing loss (training)
2022-11-09 17:53:07,969:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4405 (1.4405)
2022-11-09 17:53:08,132:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9518 (1.2014)
2022-11-09 17:53:08,291:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7881 (1.0599)
2022-11-09 17:53:08,390:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5496 (0.9724)
2022-11-09 17:53:08,790:INFO: Dataset: univ                Batch:  1/15	Loss 0.6251 (0.6251)
2022-11-09 17:53:08,954:INFO: Dataset: univ                Batch:  2/15	Loss 0.7276 (0.6777)
2022-11-09 17:53:09,119:INFO: Dataset: univ                Batch:  3/15	Loss 0.6811 (0.6788)
2022-11-09 17:53:09,280:INFO: Dataset: univ                Batch:  4/15	Loss 0.7230 (0.6891)
2022-11-09 17:53:09,449:INFO: Dataset: univ                Batch:  5/15	Loss 0.6716 (0.6854)
2022-11-09 17:53:09,612:INFO: Dataset: univ                Batch:  6/15	Loss 0.6539 (0.6799)
2022-11-09 17:53:09,776:INFO: Dataset: univ                Batch:  7/15	Loss 1.9907 (0.8436)
2022-11-09 17:53:09,937:INFO: Dataset: univ                Batch:  8/15	Loss 0.6111 (0.8126)
2022-11-09 17:53:10,102:INFO: Dataset: univ                Batch:  9/15	Loss 0.7012 (0.7990)
2022-11-09 17:53:10,264:INFO: Dataset: univ                Batch: 10/15	Loss 0.6984 (0.7886)
2022-11-09 17:53:10,429:INFO: Dataset: univ                Batch: 11/15	Loss 0.6602 (0.7772)
2022-11-09 17:53:10,592:INFO: Dataset: univ                Batch: 12/15	Loss 0.6923 (0.7707)
2022-11-09 17:53:10,755:INFO: Dataset: univ                Batch: 13/15	Loss 0.8275 (0.7751)
2022-11-09 17:53:10,917:INFO: Dataset: univ                Batch: 14/15	Loss 1.2963 (0.8173)
2022-11-09 17:53:10,959:INFO: Dataset: univ                Batch: 15/15	Loss 0.1150 (0.8090)
2022-11-09 17:53:11,352:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7960 (0.7960)
2022-11-09 17:53:11,515:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0159 (0.8936)
2022-11-09 17:53:11,674:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7071 (0.8286)
2022-11-09 17:53:11,833:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7303 (0.8046)
2022-11-09 17:53:11,994:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1337 (0.8745)
2022-11-09 17:53:12,152:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8110 (0.8627)
2022-11-09 17:53:12,312:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7969 (0.8530)
2022-11-09 17:53:12,448:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7797 (0.8446)
2022-11-09 17:53:12,863:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6833 (0.6833)
2022-11-09 17:53:13,025:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3135 (0.9894)
2022-11-09 17:53:13,188:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8842 (0.9530)
2022-11-09 17:53:13,347:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9247 (0.9460)
2022-11-09 17:53:13,514:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9561 (0.9481)
2022-11-09 17:53:13,675:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7406 (0.9129)
2022-11-09 17:53:13,836:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6352 (0.8716)
2022-11-09 17:53:13,995:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9998 (0.8876)
2022-11-09 17:53:14,156:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6981 (0.8682)
2022-11-09 17:53:14,317:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7063 (0.8521)
2022-11-09 17:53:14,478:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6975 (0.8373)
2022-11-09 17:53:14,639:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7082 (0.8261)
2022-11-09 17:53:14,800:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7328 (0.8191)
2022-11-09 17:53:14,960:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9718 (0.8299)
2022-11-09 17:53:15,123:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9886 (0.8397)
2022-11-09 17:53:15,283:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6388 (0.8280)
2022-11-09 17:53:15,446:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6502 (0.8181)
2022-11-09 17:53:15,587:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6778 (0.8117)
2022-11-09 17:53:15,641:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_184.pth.tar
2022-11-09 17:53:15,641:INFO: 
===> EPOCH: 185 (P2)
2022-11-09 17:53:15,641:INFO: - Computing loss (training)
2022-11-09 17:53:15,989:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0833 (1.0833)
2022-11-09 17:53:16,152:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8325 (0.9635)
2022-11-09 17:53:16,311:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4877 (1.1357)
2022-11-09 17:53:16,411:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6978 (1.0618)
2022-11-09 17:53:16,788:INFO: Dataset: univ                Batch:  1/15	Loss 1.0753 (1.0753)
2022-11-09 17:53:16,952:INFO: Dataset: univ                Batch:  2/15	Loss 0.6931 (0.8805)
2022-11-09 17:53:17,119:INFO: Dataset: univ                Batch:  3/15	Loss 1.6441 (1.1476)
2022-11-09 17:53:17,354:INFO: Dataset: univ                Batch:  4/15	Loss 0.9256 (1.0921)
2022-11-09 17:53:17,519:INFO: Dataset: univ                Batch:  5/15	Loss 1.3670 (1.1490)
2022-11-09 17:53:17,692:INFO: Dataset: univ                Batch:  6/15	Loss 1.0642 (1.1339)
2022-11-09 17:53:17,853:INFO: Dataset: univ                Batch:  7/15	Loss 0.6243 (1.0646)
2022-11-09 17:53:18,016:INFO: Dataset: univ                Batch:  8/15	Loss 0.6639 (1.0142)
2022-11-09 17:53:18,177:INFO: Dataset: univ                Batch:  9/15	Loss 0.6101 (0.9641)
2022-11-09 17:53:18,340:INFO: Dataset: univ                Batch: 10/15	Loss 0.6814 (0.9355)
2022-11-09 17:53:18,501:INFO: Dataset: univ                Batch: 11/15	Loss 0.7658 (0.9210)
2022-11-09 17:53:18,665:INFO: Dataset: univ                Batch: 12/15	Loss 0.6143 (0.8970)
2022-11-09 17:53:18,826:INFO: Dataset: univ                Batch: 13/15	Loss 1.3399 (0.9279)
2022-11-09 17:53:18,991:INFO: Dataset: univ                Batch: 14/15	Loss 1.1918 (0.9472)
2022-11-09 17:53:19,034:INFO: Dataset: univ                Batch: 15/15	Loss 0.2068 (0.9389)
2022-11-09 17:53:19,426:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2927 (1.2927)
2022-11-09 17:53:19,587:INFO: Dataset: zara1               Batch: 2/8	Loss 2.1243 (1.7139)
2022-11-09 17:53:19,748:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2485 (1.5592)
2022-11-09 17:53:19,904:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7066 (1.3316)
2022-11-09 17:53:20,068:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1146 (1.2851)
2022-11-09 17:53:20,226:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1262 (1.4339)
2022-11-09 17:53:20,386:INFO: Dataset: zara1               Batch: 7/8	Loss 2.2462 (1.5461)
2022-11-09 17:53:20,523:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0458 (1.4898)
2022-11-09 17:53:20,927:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8645 (0.8645)
2022-11-09 17:53:21,089:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8875 (0.8753)
2022-11-09 17:53:21,249:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6911 (0.8170)
2022-11-09 17:53:21,410:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0331 (0.8739)
2022-11-09 17:53:21,573:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9918 (0.8993)
2022-11-09 17:53:21,734:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8141 (0.8864)
2022-11-09 17:53:21,894:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6743 (0.8536)
2022-11-09 17:53:22,053:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7865 (0.8450)
2022-11-09 17:53:22,214:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1729 (0.8799)
2022-11-09 17:53:22,372:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2215 (0.9119)
2022-11-09 17:53:22,534:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5460 (0.9682)
2022-11-09 17:53:22,694:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7964 (0.9537)
2022-11-09 17:53:22,856:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6733 (0.9325)
2022-11-09 17:53:23,019:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7620 (0.9204)
2022-11-09 17:53:23,180:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6814 (0.9061)
2022-11-09 17:53:23,339:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8028 (0.8992)
2022-11-09 17:53:23,500:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6914 (0.8873)
2022-11-09 17:53:23,639:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7588 (0.8816)
2022-11-09 17:53:23,695:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_185.pth.tar
2022-11-09 17:53:23,695:INFO: 
===> EPOCH: 186 (P2)
2022-11-09 17:53:23,696:INFO: - Computing loss (training)
2022-11-09 17:53:24,034:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8399 (0.8399)
2022-11-09 17:53:24,198:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1685 (1.5089)
2022-11-09 17:53:24,357:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8057 (1.2816)
2022-11-09 17:53:24,458:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4385 (1.1337)
2022-11-09 17:53:24,861:INFO: Dataset: univ                Batch:  1/15	Loss 0.5989 (0.5989)
2022-11-09 17:53:25,025:INFO: Dataset: univ                Batch:  2/15	Loss 0.7255 (0.6563)
2022-11-09 17:53:25,188:INFO: Dataset: univ                Batch:  3/15	Loss 0.6438 (0.6521)
2022-11-09 17:53:25,351:INFO: Dataset: univ                Batch:  4/15	Loss 0.6810 (0.6600)
2022-11-09 17:53:25,515:INFO: Dataset: univ                Batch:  5/15	Loss 0.6373 (0.6549)
2022-11-09 17:53:25,679:INFO: Dataset: univ                Batch:  6/15	Loss 0.6065 (0.6472)
2022-11-09 17:53:25,843:INFO: Dataset: univ                Batch:  7/15	Loss 0.7958 (0.6704)
2022-11-09 17:53:26,005:INFO: Dataset: univ                Batch:  8/15	Loss 0.5697 (0.6583)
2022-11-09 17:53:26,169:INFO: Dataset: univ                Batch:  9/15	Loss 0.6064 (0.6521)
2022-11-09 17:53:26,329:INFO: Dataset: univ                Batch: 10/15	Loss 0.7715 (0.6629)
2022-11-09 17:53:26,496:INFO: Dataset: univ                Batch: 11/15	Loss 0.6035 (0.6570)
2022-11-09 17:53:26,657:INFO: Dataset: univ                Batch: 12/15	Loss 1.1962 (0.7041)
2022-11-09 17:53:26,820:INFO: Dataset: univ                Batch: 13/15	Loss 0.8507 (0.7158)
2022-11-09 17:53:26,983:INFO: Dataset: univ                Batch: 14/15	Loss 0.8481 (0.7244)
2022-11-09 17:53:27,025:INFO: Dataset: univ                Batch: 15/15	Loss 0.2058 (0.7199)
2022-11-09 17:53:27,421:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9292 (0.9292)
2022-11-09 17:53:27,581:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7887 (0.8548)
2022-11-09 17:53:27,744:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7541 (0.8230)
2022-11-09 17:53:27,899:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7112 (0.7938)
2022-11-09 17:53:28,059:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0848 (0.8605)
2022-11-09 17:53:28,218:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0917 (0.9038)
2022-11-09 17:53:28,377:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0789 (0.9305)
2022-11-09 17:53:28,513:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6964 (0.9071)
2022-11-09 17:53:28,906:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6753 (0.6753)
2022-11-09 17:53:29,065:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6570 (0.6662)
2022-11-09 17:53:29,230:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7322 (0.6876)
2022-11-09 17:53:29,393:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7828 (0.7107)
2022-11-09 17:53:29,555:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6682 (0.7020)
2022-11-09 17:53:29,716:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2570 (0.7939)
2022-11-09 17:53:29,876:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6809 (0.7781)
2022-11-09 17:53:30,034:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8156 (0.7826)
2022-11-09 17:53:30,194:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6682 (0.7695)
2022-11-09 17:53:30,352:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7762 (0.7701)
2022-11-09 17:53:30,513:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6328 (0.7576)
2022-11-09 17:53:30,673:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7405 (0.7563)
2022-11-09 17:53:30,833:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7944 (0.7591)
2022-11-09 17:53:30,991:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6383 (0.7503)
2022-11-09 17:53:31,153:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4614 (0.7957)
2022-11-09 17:53:31,312:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7386 (0.7922)
2022-11-09 17:53:31,475:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8202 (0.7938)
2022-11-09 17:53:31,614:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6237 (0.7864)
2022-11-09 17:53:31,667:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_186.pth.tar
2022-11-09 17:53:31,667:INFO: 
===> EPOCH: 187 (P2)
2022-11-09 17:53:31,668:INFO: - Computing loss (training)
2022-11-09 17:53:32,016:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7589 (1.7589)
2022-11-09 17:53:32,178:INFO: Dataset: hotel               Batch: 2/4	Loss 7.2274 (4.5250)
2022-11-09 17:53:32,336:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0291 (3.3761)
2022-11-09 17:53:32,436:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5878 (2.9384)
2022-11-09 17:53:32,882:INFO: Dataset: univ                Batch:  1/15	Loss 1.1984 (1.1984)
2022-11-09 17:53:33,047:INFO: Dataset: univ                Batch:  2/15	Loss 0.8325 (0.9946)
2022-11-09 17:53:33,212:INFO: Dataset: univ                Batch:  3/15	Loss 0.7678 (0.9182)
2022-11-09 17:53:33,377:INFO: Dataset: univ                Batch:  4/15	Loss 0.7931 (0.8864)
2022-11-09 17:53:33,542:INFO: Dataset: univ                Batch:  5/15	Loss 1.2301 (0.9587)
2022-11-09 17:53:33,708:INFO: Dataset: univ                Batch:  6/15	Loss 0.9445 (0.9564)
2022-11-09 17:53:33,874:INFO: Dataset: univ                Batch:  7/15	Loss 0.6666 (0.9136)
2022-11-09 17:53:34,037:INFO: Dataset: univ                Batch:  8/15	Loss 0.7286 (0.8907)
2022-11-09 17:53:34,202:INFO: Dataset: univ                Batch:  9/15	Loss 1.0835 (0.9118)
2022-11-09 17:53:34,365:INFO: Dataset: univ                Batch: 10/15	Loss 0.8193 (0.9020)
2022-11-09 17:53:34,531:INFO: Dataset: univ                Batch: 11/15	Loss 0.6617 (0.8812)
2022-11-09 17:53:34,695:INFO: Dataset: univ                Batch: 12/15	Loss 0.7231 (0.8666)
2022-11-09 17:53:34,860:INFO: Dataset: univ                Batch: 13/15	Loss 0.9307 (0.8716)
2022-11-09 17:53:35,025:INFO: Dataset: univ                Batch: 14/15	Loss 0.5586 (0.8494)
2022-11-09 17:53:35,068:INFO: Dataset: univ                Batch: 15/15	Loss 0.1164 (0.8376)
2022-11-09 17:53:35,462:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3499 (2.3499)
2022-11-09 17:53:35,625:INFO: Dataset: zara1               Batch: 2/8	Loss 3.3653 (2.8976)
2022-11-09 17:53:35,785:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0071 (2.6143)
2022-11-09 17:53:35,943:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3205 (2.2931)
2022-11-09 17:53:36,107:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0428 (2.0372)
2022-11-09 17:53:36,266:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7604 (1.8449)
2022-11-09 17:53:36,426:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1471 (1.8868)
2022-11-09 17:53:36,565:INFO: Dataset: zara1               Batch: 8/8	Loss 2.1385 (1.9129)
2022-11-09 17:53:36,977:INFO: Dataset: zara2               Batch:  1/18	Loss 2.8842 (2.8842)
2022-11-09 17:53:37,138:INFO: Dataset: zara2               Batch:  2/18	Loss 3.3867 (3.1347)
2022-11-09 17:53:37,301:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0113 (2.7444)
2022-11-09 17:53:37,458:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6332 (2.4808)
2022-11-09 17:53:37,629:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6570 (2.1295)
2022-11-09 17:53:37,788:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9891 (1.9438)
2022-11-09 17:53:37,948:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3441 (1.8536)
2022-11-09 17:53:38,106:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1659 (1.7625)
2022-11-09 17:53:38,266:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0200 (1.6849)
2022-11-09 17:53:38,423:INFO: Dataset: zara2               Batch: 10/18	Loss 2.8517 (1.8006)
2022-11-09 17:53:38,586:INFO: Dataset: zara2               Batch: 11/18	Loss 3.1186 (1.9032)
2022-11-09 17:53:38,743:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8908 (1.8149)
2022-11-09 17:53:38,904:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4173 (1.7842)
2022-11-09 17:53:39,063:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9114 (1.7234)
2022-11-09 17:53:39,225:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6047 (1.7144)
2022-11-09 17:53:39,383:INFO: Dataset: zara2               Batch: 16/18	Loss 2.8098 (1.7859)
2022-11-09 17:53:39,544:INFO: Dataset: zara2               Batch: 17/18	Loss 3.4306 (1.8869)
2022-11-09 17:53:39,683:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2946 (1.8606)
2022-11-09 17:53:39,740:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_187.pth.tar
2022-11-09 17:53:39,740:INFO: 
===> EPOCH: 188 (P2)
2022-11-09 17:53:39,740:INFO: - Computing loss (training)
2022-11-09 17:53:40,085:INFO: Dataset: hotel               Batch: 1/4	Loss 3.8522 (3.8522)
2022-11-09 17:53:40,246:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4527 (3.1839)
2022-11-09 17:53:40,405:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0388 (2.4161)
2022-11-09 17:53:40,503:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3969 (2.0565)
2022-11-09 17:53:40,906:INFO: Dataset: univ                Batch:  1/15	Loss 0.6620 (0.6620)
2022-11-09 17:53:41,069:INFO: Dataset: univ                Batch:  2/15	Loss 5.9195 (3.0433)
2022-11-09 17:53:41,234:INFO: Dataset: univ                Batch:  3/15	Loss 0.9773 (2.3238)
2022-11-09 17:53:41,397:INFO: Dataset: univ                Batch:  4/15	Loss 1.1172 (2.0154)
2022-11-09 17:53:41,565:INFO: Dataset: univ                Batch:  5/15	Loss 0.7019 (1.7232)
2022-11-09 17:53:41,728:INFO: Dataset: univ                Batch:  6/15	Loss 1.0638 (1.6189)
2022-11-09 17:53:41,890:INFO: Dataset: univ                Batch:  7/15	Loss 0.7539 (1.5075)
2022-11-09 17:53:42,052:INFO: Dataset: univ                Batch:  8/15	Loss 0.9981 (1.4377)
2022-11-09 17:53:42,216:INFO: Dataset: univ                Batch:  9/15	Loss 1.2687 (1.4195)
2022-11-09 17:53:42,378:INFO: Dataset: univ                Batch: 10/15	Loss 1.0099 (1.3724)
2022-11-09 17:53:42,541:INFO: Dataset: univ                Batch: 11/15	Loss 0.9137 (1.3301)
2022-11-09 17:53:42,704:INFO: Dataset: univ                Batch: 12/15	Loss 1.1329 (1.3144)
2022-11-09 17:53:42,867:INFO: Dataset: univ                Batch: 13/15	Loss 1.0952 (1.2986)
2022-11-09 17:53:43,030:INFO: Dataset: univ                Batch: 14/15	Loss 0.9162 (1.2702)
2022-11-09 17:53:43,073:INFO: Dataset: univ                Batch: 15/15	Loss 0.1391 (1.2545)
2022-11-09 17:53:43,467:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7502 (0.7502)
2022-11-09 17:53:43,627:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1152 (0.9277)
2022-11-09 17:53:43,786:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8096 (0.8876)
2022-11-09 17:53:43,944:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0748 (0.9319)
2022-11-09 17:53:44,103:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9424 (0.9341)
2022-11-09 17:53:44,264:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7584 (0.9046)
2022-11-09 17:53:44,424:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8758 (0.9002)
2022-11-09 17:53:44,561:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6279 (0.8697)
2022-11-09 17:53:44,950:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8919 (0.8919)
2022-11-09 17:53:45,112:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3576 (1.1130)
2022-11-09 17:53:45,277:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6066 (0.9559)
2022-11-09 17:53:45,436:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6523 (0.8857)
2022-11-09 17:53:45,600:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7834 (0.8653)
2022-11-09 17:53:45,759:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6236 (0.8236)
2022-11-09 17:53:45,918:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0354 (0.8549)
2022-11-09 17:53:46,075:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7975 (0.8480)
2022-11-09 17:53:46,235:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6214 (0.8236)
2022-11-09 17:53:46,391:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6470 (0.8046)
2022-11-09 17:53:46,553:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6033 (0.7863)
2022-11-09 17:53:46,711:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6612 (0.7758)
2022-11-09 17:53:46,870:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6124 (0.7638)
2022-11-09 17:53:47,028:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6077 (0.7532)
2022-11-09 17:53:47,189:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6818 (0.7484)
2022-11-09 17:53:47,347:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7842 (0.7507)
2022-11-09 17:53:47,508:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6646 (0.7458)
2022-11-09 17:53:47,647:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5002 (0.7336)
2022-11-09 17:53:47,699:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_188.pth.tar
2022-11-09 17:53:47,699:INFO: 
===> EPOCH: 189 (P2)
2022-11-09 17:53:47,700:INFO: - Computing loss (training)
2022-11-09 17:53:48,053:INFO: Dataset: hotel               Batch: 1/4	Loss 6.6435 (6.6435)
2022-11-09 17:53:48,217:INFO: Dataset: hotel               Batch: 2/4	Loss 3.1843 (4.8245)
2022-11-09 17:53:48,375:INFO: Dataset: hotel               Batch: 3/4	Loss 4.4225 (4.6974)
2022-11-09 17:53:48,475:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4043 (4.1109)
2022-11-09 17:53:48,884:INFO: Dataset: univ                Batch:  1/15	Loss 0.6485 (0.6485)
2022-11-09 17:53:49,054:INFO: Dataset: univ                Batch:  2/15	Loss 0.6692 (0.6592)
2022-11-09 17:53:49,227:INFO: Dataset: univ                Batch:  3/15	Loss 1.2083 (0.8420)
2022-11-09 17:53:49,392:INFO: Dataset: univ                Batch:  4/15	Loss 1.1106 (0.9122)
2022-11-09 17:53:49,561:INFO: Dataset: univ                Batch:  5/15	Loss 1.3758 (1.0148)
2022-11-09 17:53:49,730:INFO: Dataset: univ                Batch:  6/15	Loss 0.6413 (0.9560)
2022-11-09 17:53:49,897:INFO: Dataset: univ                Batch:  7/15	Loss 0.7125 (0.9195)
2022-11-09 17:53:50,063:INFO: Dataset: univ                Batch:  8/15	Loss 0.7447 (0.8988)
2022-11-09 17:53:50,231:INFO: Dataset: univ                Batch:  9/15	Loss 0.5619 (0.8604)
2022-11-09 17:53:50,397:INFO: Dataset: univ                Batch: 10/15	Loss 0.7211 (0.8477)
2022-11-09 17:53:50,566:INFO: Dataset: univ                Batch: 11/15	Loss 0.6182 (0.8265)
2022-11-09 17:53:50,733:INFO: Dataset: univ                Batch: 12/15	Loss 0.5366 (0.8020)
2022-11-09 17:53:50,901:INFO: Dataset: univ                Batch: 13/15	Loss 1.2533 (0.8322)
2022-11-09 17:53:51,070:INFO: Dataset: univ                Batch: 14/15	Loss 0.6488 (0.8201)
2022-11-09 17:53:51,114:INFO: Dataset: univ                Batch: 15/15	Loss 0.1571 (0.8108)
2022-11-09 17:53:51,497:INFO: Dataset: zara1               Batch: 1/8	Loss 2.4254 (2.4254)
2022-11-09 17:53:51,661:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5139 (1.9900)
2022-11-09 17:53:51,819:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4450 (1.8025)
2022-11-09 17:53:52,051:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7152 (1.5225)
2022-11-09 17:53:52,210:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6842 (1.3465)
2022-11-09 17:53:52,369:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7792 (1.2529)
2022-11-09 17:53:52,525:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1936 (1.3899)
2022-11-09 17:53:52,664:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3402 (1.3851)
2022-11-09 17:53:53,070:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5785 (1.5785)
2022-11-09 17:53:53,234:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1552 (1.3685)
2022-11-09 17:53:53,398:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9282 (1.2201)
2022-11-09 17:53:53,559:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5296 (1.0476)
2022-11-09 17:53:53,727:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6567 (0.9632)
2022-11-09 17:53:53,889:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6615 (0.9144)
2022-11-09 17:53:54,052:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3148 (0.9722)
2022-11-09 17:53:54,212:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6605 (1.0625)
2022-11-09 17:53:54,376:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7468 (1.1447)
2022-11-09 17:53:54,536:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9592 (1.1270)
2022-11-09 17:53:54,701:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9523 (1.1114)
2022-11-09 17:53:54,861:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5976 (1.0652)
2022-11-09 17:53:55,026:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6478 (1.0327)
2022-11-09 17:53:55,188:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7186 (1.0093)
2022-11-09 17:53:55,354:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9392 (1.0049)
2022-11-09 17:53:55,515:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7995 (0.9930)
2022-11-09 17:53:55,681:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6775 (0.9758)
2022-11-09 17:53:55,821:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9879 (0.9764)
2022-11-09 17:53:55,875:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_189.pth.tar
2022-11-09 17:53:55,875:INFO: 
===> EPOCH: 190 (P2)
2022-11-09 17:53:55,875:INFO: - Computing loss (training)
2022-11-09 17:53:56,225:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9883 (1.9883)
2022-11-09 17:53:56,386:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8297 (1.4343)
2022-11-09 17:53:56,547:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6558 (1.5041)
2022-11-09 17:53:56,646:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5428 (1.3481)
2022-11-09 17:53:57,048:INFO: Dataset: univ                Batch:  1/15	Loss 0.9162 (0.9162)
2022-11-09 17:53:57,214:INFO: Dataset: univ                Batch:  2/15	Loss 0.5702 (0.7418)
2022-11-09 17:53:57,378:INFO: Dataset: univ                Batch:  3/15	Loss 0.6923 (0.7265)
2022-11-09 17:53:57,545:INFO: Dataset: univ                Batch:  4/15	Loss 0.6075 (0.6960)
2022-11-09 17:53:57,709:INFO: Dataset: univ                Batch:  5/15	Loss 0.5821 (0.6736)
2022-11-09 17:53:57,874:INFO: Dataset: univ                Batch:  6/15	Loss 0.9782 (0.7224)
2022-11-09 17:53:58,038:INFO: Dataset: univ                Batch:  7/15	Loss 0.5506 (0.6974)
2022-11-09 17:53:58,200:INFO: Dataset: univ                Batch:  8/15	Loss 0.6962 (0.6972)
2022-11-09 17:53:58,364:INFO: Dataset: univ                Batch:  9/15	Loss 0.5648 (0.6838)
2022-11-09 17:53:58,526:INFO: Dataset: univ                Batch: 10/15	Loss 0.5958 (0.6743)
2022-11-09 17:53:58,691:INFO: Dataset: univ                Batch: 11/15	Loss 0.5787 (0.6648)
2022-11-09 17:53:58,854:INFO: Dataset: univ                Batch: 12/15	Loss 0.5583 (0.6563)
2022-11-09 17:53:59,019:INFO: Dataset: univ                Batch: 13/15	Loss 0.5932 (0.6510)
2022-11-09 17:53:59,185:INFO: Dataset: univ                Batch: 14/15	Loss 0.5060 (0.6404)
2022-11-09 17:53:59,228:INFO: Dataset: univ                Batch: 15/15	Loss 0.0974 (0.6334)
2022-11-09 17:53:59,633:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3318 (1.3318)
2022-11-09 17:53:59,794:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6580 (0.9767)
2022-11-09 17:53:59,953:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6011 (0.8481)
2022-11-09 17:54:00,112:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7622 (0.8270)
2022-11-09 17:54:00,275:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7088 (0.8045)
2022-11-09 17:54:00,433:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6518 (0.7760)
2022-11-09 17:54:00,593:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6493 (0.7578)
2022-11-09 17:54:00,730:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6110 (0.7389)
2022-11-09 17:54:01,122:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6005 (0.6005)
2022-11-09 17:54:01,286:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6031 (0.6018)
2022-11-09 17:54:01,446:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5576 (0.5892)
2022-11-09 17:54:01,607:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5589 (0.5822)
2022-11-09 17:54:01,768:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6420 (0.5942)
2022-11-09 17:54:01,929:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5875 (0.5932)
2022-11-09 17:54:02,090:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5681 (0.5890)
2022-11-09 17:54:02,248:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2292 (0.6717)
2022-11-09 17:54:02,411:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5883 (0.6625)
2022-11-09 17:54:02,572:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6013 (0.6564)
2022-11-09 17:54:02,733:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5815 (0.6500)
2022-11-09 17:54:02,892:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5419 (0.6411)
2022-11-09 17:54:03,053:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6815 (0.6439)
2022-11-09 17:54:03,214:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5222 (0.6348)
2022-11-09 17:54:03,376:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6231 (0.6341)
2022-11-09 17:54:03,543:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5816 (0.6311)
2022-11-09 17:54:03,706:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5708 (0.6276)
2022-11-09 17:54:03,845:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4734 (0.6190)
2022-11-09 17:54:03,904:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_190.pth.tar
2022-11-09 17:54:03,904:INFO: 
===> EPOCH: 191 (P2)
2022-11-09 17:54:03,904:INFO: - Computing loss (training)
2022-11-09 17:54:04,245:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2318 (2.2318)
2022-11-09 17:54:04,408:INFO: Dataset: hotel               Batch: 2/4	Loss 3.8116 (3.0105)
2022-11-09 17:54:04,567:INFO: Dataset: hotel               Batch: 3/4	Loss 3.6065 (3.2141)
2022-11-09 17:54:04,667:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8363 (2.8471)
2022-11-09 17:54:05,128:INFO: Dataset: univ                Batch:  1/15	Loss 1.0205 (1.0205)
2022-11-09 17:54:05,291:INFO: Dataset: univ                Batch:  2/15	Loss 0.6236 (0.8295)
2022-11-09 17:54:05,457:INFO: Dataset: univ                Batch:  3/15	Loss 0.6220 (0.7578)
2022-11-09 17:54:05,619:INFO: Dataset: univ                Batch:  4/15	Loss 1.0573 (0.8331)
2022-11-09 17:54:05,783:INFO: Dataset: univ                Batch:  5/15	Loss 1.1481 (0.8956)
2022-11-09 17:54:05,947:INFO: Dataset: univ                Batch:  6/15	Loss 1.0153 (0.9163)
2022-11-09 17:54:06,117:INFO: Dataset: univ                Batch:  7/15	Loss 0.7581 (0.8935)
2022-11-09 17:54:06,279:INFO: Dataset: univ                Batch:  8/15	Loss 0.8529 (0.8884)
2022-11-09 17:54:06,442:INFO: Dataset: univ                Batch:  9/15	Loss 0.7215 (0.8694)
2022-11-09 17:54:06,606:INFO: Dataset: univ                Batch: 10/15	Loss 0.7205 (0.8530)
2022-11-09 17:54:06,773:INFO: Dataset: univ                Batch: 11/15	Loss 1.0769 (0.8728)
2022-11-09 17:54:06,935:INFO: Dataset: univ                Batch: 12/15	Loss 0.5412 (0.8455)
2022-11-09 17:54:07,104:INFO: Dataset: univ                Batch: 13/15	Loss 0.7327 (0.8372)
2022-11-09 17:54:07,268:INFO: Dataset: univ                Batch: 14/15	Loss 0.5775 (0.8186)
2022-11-09 17:54:07,310:INFO: Dataset: univ                Batch: 15/15	Loss 0.0977 (0.8085)
2022-11-09 17:54:07,698:INFO: Dataset: zara1               Batch: 1/8	Loss 2.1375 (2.1375)
2022-11-09 17:54:07,859:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4940 (1.8412)
2022-11-09 17:54:08,022:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7974 (1.5082)
2022-11-09 17:54:08,180:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7910 (1.3416)
2022-11-09 17:54:08,342:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6605 (1.2095)
2022-11-09 17:54:08,500:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7607 (1.1368)
2022-11-09 17:54:08,660:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3303 (1.1674)
2022-11-09 17:54:08,796:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9551 (1.1403)
2022-11-09 17:54:09,201:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9964 (0.9964)
2022-11-09 17:54:09,366:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1771 (1.0881)
2022-11-09 17:54:09,526:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6834 (0.9684)
2022-11-09 17:54:09,685:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6562 (0.8944)
2022-11-09 17:54:09,848:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5925 (0.8358)
2022-11-09 17:54:10,010:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5861 (0.7981)
2022-11-09 17:54:10,170:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6419 (0.7754)
2022-11-09 17:54:10,328:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1306 (0.8191)
2022-11-09 17:54:10,489:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8960 (0.8277)
2022-11-09 17:54:10,648:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6299 (0.8097)
2022-11-09 17:54:10,809:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6050 (0.7907)
2022-11-09 17:54:10,967:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6183 (0.7760)
2022-11-09 17:54:11,128:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5397 (0.7570)
2022-11-09 17:54:11,287:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5799 (0.7436)
2022-11-09 17:54:11,449:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6640 (0.7380)
2022-11-09 17:54:11,609:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5980 (0.7297)
2022-11-09 17:54:11,771:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6871 (0.7274)
2022-11-09 17:54:11,910:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5040 (0.7151)
2022-11-09 17:54:11,962:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_191.pth.tar
2022-11-09 17:54:11,962:INFO: 
===> EPOCH: 192 (P2)
2022-11-09 17:54:11,963:INFO: - Computing loss (training)
2022-11-09 17:54:12,301:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9931 (1.9931)
2022-11-09 17:54:12,464:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6637 (1.3445)
2022-11-09 17:54:12,626:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2396 (1.3083)
2022-11-09 17:54:12,725:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5018 (1.1711)
2022-11-09 17:54:13,120:INFO: Dataset: univ                Batch:  1/15	Loss 0.5254 (0.5254)
2022-11-09 17:54:13,284:INFO: Dataset: univ                Batch:  2/15	Loss 0.6145 (0.5736)
2022-11-09 17:54:13,450:INFO: Dataset: univ                Batch:  3/15	Loss 0.6109 (0.5866)
2022-11-09 17:54:13,613:INFO: Dataset: univ                Batch:  4/15	Loss 0.7442 (0.6231)
2022-11-09 17:54:13,780:INFO: Dataset: univ                Batch:  5/15	Loss 0.8016 (0.6624)
2022-11-09 17:54:13,942:INFO: Dataset: univ                Batch:  6/15	Loss 0.6064 (0.6539)
2022-11-09 17:54:14,107:INFO: Dataset: univ                Batch:  7/15	Loss 0.5048 (0.6306)
2022-11-09 17:54:14,270:INFO: Dataset: univ                Batch:  8/15	Loss 0.5246 (0.6170)
2022-11-09 17:54:14,433:INFO: Dataset: univ                Batch:  9/15	Loss 0.5440 (0.6093)
2022-11-09 17:54:14,594:INFO: Dataset: univ                Batch: 10/15	Loss 0.5530 (0.6035)
2022-11-09 17:54:14,757:INFO: Dataset: univ                Batch: 11/15	Loss 0.7730 (0.6190)
2022-11-09 17:54:14,919:INFO: Dataset: univ                Batch: 12/15	Loss 1.0047 (0.6501)
2022-11-09 17:54:15,082:INFO: Dataset: univ                Batch: 13/15	Loss 0.5740 (0.6446)
2022-11-09 17:54:15,245:INFO: Dataset: univ                Batch: 14/15	Loss 0.5438 (0.6371)
2022-11-09 17:54:15,286:INFO: Dataset: univ                Batch: 15/15	Loss 0.0947 (0.6302)
2022-11-09 17:54:15,689:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7201 (0.7201)
2022-11-09 17:54:15,851:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7295 (0.7247)
2022-11-09 17:54:16,012:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7072 (0.7181)
2022-11-09 17:54:16,171:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6072 (0.6867)
2022-11-09 17:54:16,329:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6685 (0.6830)
2022-11-09 17:54:16,486:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6275 (0.6727)
2022-11-09 17:54:16,645:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3777 (0.7677)
2022-11-09 17:54:16,780:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8233 (0.7735)
2022-11-09 17:54:17,223:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6599 (0.6599)
2022-11-09 17:54:17,390:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5524 (0.6052)
2022-11-09 17:54:17,554:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5179 (0.5753)
2022-11-09 17:54:17,713:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5709 (0.5743)
2022-11-09 17:54:17,874:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6542 (0.5910)
2022-11-09 17:54:18,035:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6126 (0.5947)
2022-11-09 17:54:18,195:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7577 (0.6186)
2022-11-09 17:54:18,352:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1297 (0.6890)
2022-11-09 17:54:18,513:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7749 (0.6978)
2022-11-09 17:54:18,671:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7445 (0.7026)
2022-11-09 17:54:18,832:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7284 (0.7051)
2022-11-09 17:54:18,991:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7112 (0.7056)
2022-11-09 17:54:19,153:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6239 (0.6995)
2022-11-09 17:54:19,312:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5780 (0.6906)
2022-11-09 17:54:19,474:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6418 (0.6868)
2022-11-09 17:54:19,635:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7283 (0.6892)
2022-11-09 17:54:19,797:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7174 (0.6910)
2022-11-09 17:54:19,936:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6668 (0.6898)
2022-11-09 17:54:19,990:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_192.pth.tar
2022-11-09 17:54:19,990:INFO: 
===> EPOCH: 193 (P2)
2022-11-09 17:54:19,990:INFO: - Computing loss (training)
2022-11-09 17:54:20,335:INFO: Dataset: hotel               Batch: 1/4	Loss 2.9590 (2.9590)
2022-11-09 17:54:20,498:INFO: Dataset: hotel               Batch: 2/4	Loss 4.6041 (3.8152)
2022-11-09 17:54:20,657:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9165 (2.8260)
2022-11-09 17:54:20,755:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4135 (2.4186)
2022-11-09 17:54:21,191:INFO: Dataset: univ                Batch:  1/15	Loss 0.5445 (0.5445)
2022-11-09 17:54:21,354:INFO: Dataset: univ                Batch:  2/15	Loss 0.7396 (0.6422)
2022-11-09 17:54:21,517:INFO: Dataset: univ                Batch:  3/15	Loss 0.9080 (0.7426)
2022-11-09 17:54:21,678:INFO: Dataset: univ                Batch:  4/15	Loss 1.0488 (0.8242)
2022-11-09 17:54:21,840:INFO: Dataset: univ                Batch:  5/15	Loss 0.6007 (0.7830)
2022-11-09 17:54:22,004:INFO: Dataset: univ                Batch:  6/15	Loss 1.0120 (0.8238)
2022-11-09 17:54:22,168:INFO: Dataset: univ                Batch:  7/15	Loss 0.7649 (0.8147)
2022-11-09 17:54:22,328:INFO: Dataset: univ                Batch:  8/15	Loss 0.6115 (0.7892)
2022-11-09 17:54:22,490:INFO: Dataset: univ                Batch:  9/15	Loss 0.6052 (0.7698)
2022-11-09 17:54:22,653:INFO: Dataset: univ                Batch: 10/15	Loss 0.5506 (0.7493)
2022-11-09 17:54:22,819:INFO: Dataset: univ                Batch: 11/15	Loss 0.6834 (0.7435)
2022-11-09 17:54:22,984:INFO: Dataset: univ                Batch: 12/15	Loss 0.5622 (0.7275)
2022-11-09 17:54:23,151:INFO: Dataset: univ                Batch: 13/15	Loss 0.5406 (0.7133)
2022-11-09 17:54:23,316:INFO: Dataset: univ                Batch: 14/15	Loss 0.5592 (0.7014)
2022-11-09 17:54:23,359:INFO: Dataset: univ                Batch: 15/15	Loss 0.1060 (0.6934)
2022-11-09 17:54:23,752:INFO: Dataset: zara1               Batch: 1/8	Loss 3.3321 (3.3321)
2022-11-09 17:54:23,913:INFO: Dataset: zara1               Batch: 2/8	Loss 2.7147 (2.9954)
2022-11-09 17:54:24,074:INFO: Dataset: zara1               Batch: 3/8	Loss 2.3724 (2.7908)
2022-11-09 17:54:24,234:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6117 (2.2591)
2022-11-09 17:54:24,395:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7891 (1.9220)
2022-11-09 17:54:24,554:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0029 (1.7464)
2022-11-09 17:54:24,714:INFO: Dataset: zara1               Batch: 7/8	Loss 2.7845 (1.8946)
2022-11-09 17:54:24,851:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4588 (1.8510)
2022-11-09 17:54:25,244:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4393 (1.4393)
2022-11-09 17:54:25,408:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6576 (1.0211)
2022-11-09 17:54:25,572:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5415 (0.8548)
2022-11-09 17:54:25,732:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8772 (0.8604)
2022-11-09 17:54:25,894:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7105 (0.8289)
2022-11-09 17:54:26,060:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8725 (0.8360)
2022-11-09 17:54:26,223:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5694 (0.7965)
2022-11-09 17:54:26,382:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8037 (0.7974)
2022-11-09 17:54:26,544:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6124 (0.7781)
2022-11-09 17:54:26,704:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5656 (0.7559)
2022-11-09 17:54:26,865:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5427 (0.7377)
2022-11-09 17:54:27,025:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5654 (0.7231)
2022-11-09 17:54:27,187:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5796 (0.7114)
2022-11-09 17:54:27,347:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5648 (0.7009)
2022-11-09 17:54:27,511:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6375 (0.6965)
2022-11-09 17:54:27,749:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6145 (0.6918)
2022-11-09 17:54:27,911:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5962 (0.6863)
2022-11-09 17:54:28,054:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6104 (0.6823)
2022-11-09 17:54:28,112:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_193.pth.tar
2022-11-09 17:54:28,112:INFO: 
===> EPOCH: 194 (P2)
2022-11-09 17:54:28,112:INFO: - Computing loss (training)
2022-11-09 17:54:28,450:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5105 (1.5105)
2022-11-09 17:54:28,612:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6611 (1.5835)
2022-11-09 17:54:28,769:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0675 (1.4145)
2022-11-09 17:54:28,867:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4336 (1.2553)
2022-11-09 17:54:29,261:INFO: Dataset: univ                Batch:  1/15	Loss 0.8794 (0.8794)
2022-11-09 17:54:29,424:INFO: Dataset: univ                Batch:  2/15	Loss 0.5141 (0.7002)
2022-11-09 17:54:29,591:INFO: Dataset: univ                Batch:  3/15	Loss 0.5578 (0.6557)
2022-11-09 17:54:29,753:INFO: Dataset: univ                Batch:  4/15	Loss 0.5756 (0.6350)
2022-11-09 17:54:29,917:INFO: Dataset: univ                Batch:  5/15	Loss 0.7319 (0.6556)
2022-11-09 17:54:30,080:INFO: Dataset: univ                Batch:  6/15	Loss 0.5545 (0.6391)
2022-11-09 17:54:30,244:INFO: Dataset: univ                Batch:  7/15	Loss 0.5351 (0.6237)
2022-11-09 17:54:30,405:INFO: Dataset: univ                Batch:  8/15	Loss 1.3389 (0.7138)
2022-11-09 17:54:30,569:INFO: Dataset: univ                Batch:  9/15	Loss 0.5255 (0.6936)
2022-11-09 17:54:30,731:INFO: Dataset: univ                Batch: 10/15	Loss 0.5137 (0.6753)
2022-11-09 17:54:30,895:INFO: Dataset: univ                Batch: 11/15	Loss 0.5444 (0.6646)
2022-11-09 17:54:31,057:INFO: Dataset: univ                Batch: 12/15	Loss 1.1860 (0.7039)
2022-11-09 17:54:31,220:INFO: Dataset: univ                Batch: 13/15	Loss 0.5102 (0.6893)
2022-11-09 17:54:31,383:INFO: Dataset: univ                Batch: 14/15	Loss 0.6188 (0.6845)
2022-11-09 17:54:31,425:INFO: Dataset: univ                Batch: 15/15	Loss 0.0863 (0.6774)
2022-11-09 17:54:31,809:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6728 (0.6728)
2022-11-09 17:54:31,970:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1400 (0.9112)
2022-11-09 17:54:32,133:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6419 (0.8288)
2022-11-09 17:54:32,292:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6661 (0.7938)
2022-11-09 17:54:32,452:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6787 (0.7699)
2022-11-09 17:54:32,612:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6990 (0.7582)
2022-11-09 17:54:32,771:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6369 (0.7423)
2022-11-09 17:54:32,906:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5180 (0.7196)
2022-11-09 17:54:33,312:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5832 (0.5832)
2022-11-09 17:54:33,473:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5906 (0.5869)
2022-11-09 17:54:33,633:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5384 (0.5706)
2022-11-09 17:54:33,794:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8349 (0.6323)
2022-11-09 17:54:33,954:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6776 (0.6414)
2022-11-09 17:54:34,115:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5693 (0.6296)
2022-11-09 17:54:34,274:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5481 (0.6177)
2022-11-09 17:54:34,431:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5477 (0.6089)
2022-11-09 17:54:34,592:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5188 (0.5978)
2022-11-09 17:54:34,749:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5646 (0.5946)
2022-11-09 17:54:34,909:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5746 (0.5928)
2022-11-09 17:54:35,067:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4992 (0.5842)
2022-11-09 17:54:35,227:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5881 (0.5845)
2022-11-09 17:54:35,385:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4834 (0.5771)
2022-11-09 17:54:35,546:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4988 (0.5715)
2022-11-09 17:54:35,705:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7876 (0.5857)
2022-11-09 17:54:35,866:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4776 (0.5791)
2022-11-09 17:54:36,005:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4511 (0.5730)
2022-11-09 17:54:36,060:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_194.pth.tar
2022-11-09 17:54:36,060:INFO: 
===> EPOCH: 195 (P2)
2022-11-09 17:54:36,060:INFO: - Computing loss (training)
2022-11-09 17:54:36,406:INFO: Dataset: hotel               Batch: 1/4	Loss 2.5436 (2.5436)
2022-11-09 17:54:36,568:INFO: Dataset: hotel               Batch: 2/4	Loss 3.8270 (3.1549)
2022-11-09 17:54:36,726:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7556 (2.6899)
2022-11-09 17:54:36,824:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9196 (2.3956)
2022-11-09 17:54:37,220:INFO: Dataset: univ                Batch:  1/15	Loss 0.7594 (0.7594)
2022-11-09 17:54:37,385:INFO: Dataset: univ                Batch:  2/15	Loss 0.5076 (0.6435)
2022-11-09 17:54:37,547:INFO: Dataset: univ                Batch:  3/15	Loss 0.8806 (0.7270)
2022-11-09 17:54:37,706:INFO: Dataset: univ                Batch:  4/15	Loss 0.5492 (0.6865)
2022-11-09 17:54:37,868:INFO: Dataset: univ                Batch:  5/15	Loss 1.0021 (0.7475)
2022-11-09 17:54:38,036:INFO: Dataset: univ                Batch:  6/15	Loss 0.8316 (0.7618)
2022-11-09 17:54:38,198:INFO: Dataset: univ                Batch:  7/15	Loss 0.7617 (0.7618)
2022-11-09 17:54:38,357:INFO: Dataset: univ                Batch:  8/15	Loss 0.5885 (0.7394)
2022-11-09 17:54:38,518:INFO: Dataset: univ                Batch:  9/15	Loss 0.6190 (0.7256)
2022-11-09 17:54:38,680:INFO: Dataset: univ                Batch: 10/15	Loss 0.6146 (0.7142)
2022-11-09 17:54:38,842:INFO: Dataset: univ                Batch: 11/15	Loss 0.5561 (0.7007)
2022-11-09 17:54:39,002:INFO: Dataset: univ                Batch: 12/15	Loss 0.6578 (0.6970)
2022-11-09 17:54:39,165:INFO: Dataset: univ                Batch: 13/15	Loss 0.6152 (0.6907)
2022-11-09 17:54:39,326:INFO: Dataset: univ                Batch: 14/15	Loss 0.6174 (0.6857)
2022-11-09 17:54:39,367:INFO: Dataset: univ                Batch: 15/15	Loss 0.0910 (0.6770)
2022-11-09 17:54:39,753:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0627 (2.0627)
2022-11-09 17:54:39,915:INFO: Dataset: zara1               Batch: 2/8	Loss 2.1061 (2.0846)
2022-11-09 17:54:40,075:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8301 (1.9993)
2022-11-09 17:54:40,233:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8412 (1.7086)
2022-11-09 17:54:40,392:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9435 (1.5611)
2022-11-09 17:54:40,549:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5729 (1.5631)
2022-11-09 17:54:40,708:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2637 (1.5239)
2022-11-09 17:54:40,844:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4469 (1.5151)
2022-11-09 17:54:41,244:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8692 (0.8692)
2022-11-09 17:54:41,406:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7844 (0.8305)
2022-11-09 17:54:41,567:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5532 (0.7372)
2022-11-09 17:54:41,725:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5405 (0.6869)
2022-11-09 17:54:41,890:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2972 (0.8020)
2022-11-09 17:54:42,050:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7045 (0.7854)
2022-11-09 17:54:42,211:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5284 (0.7473)
2022-11-09 17:54:42,368:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6546 (0.7352)
2022-11-09 17:54:42,529:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6401 (0.7244)
2022-11-09 17:54:42,688:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5613 (0.7068)
2022-11-09 17:54:42,849:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5223 (0.6903)
2022-11-09 17:54:43,008:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5193 (0.6769)
2022-11-09 17:54:43,170:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7147 (0.6800)
2022-11-09 17:54:43,330:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5437 (0.6706)
2022-11-09 17:54:43,492:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5166 (0.6599)
2022-11-09 17:54:43,653:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5504 (0.6529)
2022-11-09 17:54:43,816:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5369 (0.6458)
2022-11-09 17:54:43,955:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4669 (0.6371)
2022-11-09 17:54:44,008:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_195.pth.tar
2022-11-09 17:54:44,009:INFO: 
===> EPOCH: 196 (P2)
2022-11-09 17:54:44,009:INFO: - Computing loss (training)
2022-11-09 17:54:44,351:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3445 (1.3445)
2022-11-09 17:54:44,512:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0543 (1.2021)
2022-11-09 17:54:44,670:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6587 (1.0216)
2022-11-09 17:54:44,769:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4765 (0.9331)
2022-11-09 17:54:45,176:INFO: Dataset: univ                Batch:  1/15	Loss 0.5244 (0.5244)
2022-11-09 17:54:45,339:INFO: Dataset: univ                Batch:  2/15	Loss 0.6517 (0.5843)
2022-11-09 17:54:45,504:INFO: Dataset: univ                Batch:  3/15	Loss 0.6062 (0.5916)
2022-11-09 17:54:45,665:INFO: Dataset: univ                Batch:  4/15	Loss 0.5384 (0.5782)
2022-11-09 17:54:45,832:INFO: Dataset: univ                Batch:  5/15	Loss 0.5327 (0.5697)
2022-11-09 17:54:45,993:INFO: Dataset: univ                Batch:  6/15	Loss 0.5242 (0.5620)
2022-11-09 17:54:46,157:INFO: Dataset: univ                Batch:  7/15	Loss 0.6527 (0.5748)
2022-11-09 17:54:46,317:INFO: Dataset: univ                Batch:  8/15	Loss 0.4720 (0.5620)
2022-11-09 17:54:46,481:INFO: Dataset: univ                Batch:  9/15	Loss 0.5361 (0.5590)
2022-11-09 17:54:46,643:INFO: Dataset: univ                Batch: 10/15	Loss 0.5308 (0.5560)
2022-11-09 17:54:46,808:INFO: Dataset: univ                Batch: 11/15	Loss 0.4987 (0.5504)
2022-11-09 17:54:46,968:INFO: Dataset: univ                Batch: 12/15	Loss 0.6645 (0.5597)
2022-11-09 17:54:47,133:INFO: Dataset: univ                Batch: 13/15	Loss 0.4681 (0.5524)
2022-11-09 17:54:47,295:INFO: Dataset: univ                Batch: 14/15	Loss 0.4985 (0.5484)
2022-11-09 17:54:47,337:INFO: Dataset: univ                Batch: 15/15	Loss 0.0887 (0.5429)
2022-11-09 17:54:47,729:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4862 (1.4862)
2022-11-09 17:54:47,894:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9622 (1.2263)
2022-11-09 17:54:48,056:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7505 (1.0623)
2022-11-09 17:54:48,215:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5467 (0.9354)
2022-11-09 17:54:48,379:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6487 (0.8865)
2022-11-09 17:54:48,539:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1984 (0.9351)
2022-11-09 17:54:48,701:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9697 (0.9403)
2022-11-09 17:54:48,838:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6884 (0.9138)
2022-11-09 17:54:49,291:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5123 (0.5123)
2022-11-09 17:54:49,450:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7048 (0.6053)
2022-11-09 17:54:49,613:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6075 (0.6060)
2022-11-09 17:54:49,771:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5766 (0.5985)
2022-11-09 17:54:49,931:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5460 (0.5885)
2022-11-09 17:54:50,091:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4668 (0.5666)
2022-11-09 17:54:50,251:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7860 (0.5965)
2022-11-09 17:54:50,408:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5909 (0.5959)
2022-11-09 17:54:50,567:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5919 (0.5954)
2022-11-09 17:54:50,725:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5110 (0.5873)
2022-11-09 17:54:50,885:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5405 (0.5830)
2022-11-09 17:54:51,043:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5468 (0.5800)
2022-11-09 17:54:51,205:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5427 (0.5768)
2022-11-09 17:54:51,363:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5094 (0.5723)
2022-11-09 17:54:51,526:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9388 (0.5958)
2022-11-09 17:54:51,686:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5196 (0.5914)
2022-11-09 17:54:51,847:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5098 (0.5865)
2022-11-09 17:54:51,986:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4122 (0.5778)
2022-11-09 17:54:52,039:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_196.pth.tar
2022-11-09 17:54:52,039:INFO: 
===> EPOCH: 197 (P2)
2022-11-09 17:54:52,039:INFO: - Computing loss (training)
2022-11-09 17:54:52,386:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9837 (0.9837)
2022-11-09 17:54:52,546:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8059 (0.8925)
2022-11-09 17:54:52,704:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6547 (1.1404)
2022-11-09 17:54:52,805:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3563 (0.9956)
2022-11-09 17:54:53,187:INFO: Dataset: univ                Batch:  1/15	Loss 0.7983 (0.7983)
2022-11-09 17:54:53,351:INFO: Dataset: univ                Batch:  2/15	Loss 0.8247 (0.8116)
2022-11-09 17:54:53,528:INFO: Dataset: univ                Batch:  3/15	Loss 0.7541 (0.7922)
2022-11-09 17:54:53,692:INFO: Dataset: univ                Batch:  4/15	Loss 0.7009 (0.7703)
2022-11-09 17:54:53,858:INFO: Dataset: univ                Batch:  5/15	Loss 0.5630 (0.7337)
2022-11-09 17:54:54,021:INFO: Dataset: univ                Batch:  6/15	Loss 0.5512 (0.7053)
2022-11-09 17:54:54,185:INFO: Dataset: univ                Batch:  7/15	Loss 0.7040 (0.7051)
2022-11-09 17:54:54,345:INFO: Dataset: univ                Batch:  8/15	Loss 0.5435 (0.6852)
2022-11-09 17:54:54,508:INFO: Dataset: univ                Batch:  9/15	Loss 0.5685 (0.6727)
2022-11-09 17:54:54,669:INFO: Dataset: univ                Batch: 10/15	Loss 0.4969 (0.6527)
2022-11-09 17:54:54,833:INFO: Dataset: univ                Batch: 11/15	Loss 0.4552 (0.6341)
2022-11-09 17:54:54,997:INFO: Dataset: univ                Batch: 12/15	Loss 0.4775 (0.6220)
2022-11-09 17:54:55,163:INFO: Dataset: univ                Batch: 13/15	Loss 0.5261 (0.6146)
2022-11-09 17:54:55,325:INFO: Dataset: univ                Batch: 14/15	Loss 0.5071 (0.6072)
2022-11-09 17:54:55,367:INFO: Dataset: univ                Batch: 15/15	Loss 0.0837 (0.5996)
2022-11-09 17:54:55,756:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3154 (1.3154)
2022-11-09 17:54:55,918:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6116 (1.4861)
2022-11-09 17:54:56,079:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7260 (1.5734)
2022-11-09 17:54:56,239:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6969 (1.3629)
2022-11-09 17:54:56,401:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7341 (1.2370)
2022-11-09 17:54:56,558:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6490 (1.1368)
2022-11-09 17:54:56,719:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7037 (1.0788)
2022-11-09 17:54:56,854:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5513 (1.0161)
2022-11-09 17:54:57,253:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8292 (0.8292)
2022-11-09 17:54:57,413:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5405 (0.6885)
2022-11-09 17:54:57,579:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5985 (0.6593)
2022-11-09 17:54:57,740:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4743 (0.6159)
2022-11-09 17:54:57,901:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7155 (0.6329)
2022-11-09 17:54:58,063:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4990 (0.6099)
2022-11-09 17:54:58,225:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5858 (0.6064)
2022-11-09 17:54:58,383:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5214 (0.5962)
2022-11-09 17:54:58,544:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6100 (0.5978)
2022-11-09 17:54:58,704:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5056 (0.5876)
2022-11-09 17:54:58,866:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5414 (0.5836)
2022-11-09 17:54:59,025:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4963 (0.5761)
2022-11-09 17:54:59,189:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7753 (0.5932)
2022-11-09 17:54:59,351:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5391 (0.5897)
2022-11-09 17:54:59,514:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4921 (0.5833)
2022-11-09 17:54:59,675:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4590 (0.5750)
2022-11-09 17:54:59,838:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8154 (0.5889)
2022-11-09 17:54:59,978:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4472 (0.5827)
2022-11-09 17:55:00,034:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_197.pth.tar
2022-11-09 17:55:00,034:INFO: 
===> EPOCH: 198 (P2)
2022-11-09 17:55:00,035:INFO: - Computing loss (training)
2022-11-09 17:55:00,393:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1283 (1.1283)
2022-11-09 17:55:00,554:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6013 (0.8766)
2022-11-09 17:55:00,714:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1703 (0.9814)
2022-11-09 17:55:00,811:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4228 (0.8834)
2022-11-09 17:55:01,301:INFO: Dataset: univ                Batch:  1/15	Loss 0.5124 (0.5124)
2022-11-09 17:55:01,466:INFO: Dataset: univ                Batch:  2/15	Loss 0.5164 (0.5145)
2022-11-09 17:55:01,633:INFO: Dataset: univ                Batch:  3/15	Loss 0.6794 (0.5709)
2022-11-09 17:55:01,797:INFO: Dataset: univ                Batch:  4/15	Loss 1.0813 (0.7027)
2022-11-09 17:55:01,963:INFO: Dataset: univ                Batch:  5/15	Loss 0.8569 (0.7361)
2022-11-09 17:55:02,131:INFO: Dataset: univ                Batch:  6/15	Loss 0.8035 (0.7479)
2022-11-09 17:55:02,297:INFO: Dataset: univ                Batch:  7/15	Loss 0.9265 (0.7753)
2022-11-09 17:55:02,467:INFO: Dataset: univ                Batch:  8/15	Loss 0.4748 (0.7353)
2022-11-09 17:55:02,635:INFO: Dataset: univ                Batch:  9/15	Loss 0.5070 (0.7117)
2022-11-09 17:55:02,799:INFO: Dataset: univ                Batch: 10/15	Loss 0.4918 (0.6909)
2022-11-09 17:55:02,968:INFO: Dataset: univ                Batch: 11/15	Loss 0.4940 (0.6728)
2022-11-09 17:55:03,133:INFO: Dataset: univ                Batch: 12/15	Loss 0.4750 (0.6564)
2022-11-09 17:55:03,301:INFO: Dataset: univ                Batch: 13/15	Loss 0.4619 (0.6408)
2022-11-09 17:55:03,472:INFO: Dataset: univ                Batch: 14/15	Loss 0.5224 (0.6319)
2022-11-09 17:55:03,516:INFO: Dataset: univ                Batch: 15/15	Loss 0.1039 (0.6257)
2022-11-09 17:55:03,982:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9186 (1.9186)
2022-11-09 17:55:04,145:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9998 (1.4894)
2022-11-09 17:55:04,309:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8451 (1.2820)
2022-11-09 17:55:04,469:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8052 (1.1691)
2022-11-09 17:55:04,635:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6211 (1.0653)
2022-11-09 17:55:04,796:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7111 (1.0091)
2022-11-09 17:55:04,958:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8900 (0.9924)
2022-11-09 17:55:05,096:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9219 (0.9850)
2022-11-09 17:55:05,493:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7497 (0.7497)
2022-11-09 17:55:05,652:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9121 (0.8385)
2022-11-09 17:55:05,818:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5262 (0.7371)
2022-11-09 17:55:05,976:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4829 (0.6758)
2022-11-09 17:55:06,141:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5663 (0.6555)
2022-11-09 17:55:06,302:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4935 (0.6273)
2022-11-09 17:55:06,464:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7630 (0.6466)
2022-11-09 17:55:06,622:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5164 (0.6311)
2022-11-09 17:55:06,783:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9126 (0.6634)
2022-11-09 17:55:06,941:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5339 (0.6503)
2022-11-09 17:55:07,105:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4931 (0.6338)
2022-11-09 17:55:07,264:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4961 (0.6223)
2022-11-09 17:55:07,426:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4728 (0.6114)
2022-11-09 17:55:07,586:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6697 (0.6158)
2022-11-09 17:55:07,749:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5216 (0.6098)
2022-11-09 17:55:07,908:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5344 (0.6053)
2022-11-09 17:55:08,071:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4883 (0.5985)
2022-11-09 17:55:08,211:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5651 (0.5969)
2022-11-09 17:55:08,264:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_198.pth.tar
2022-11-09 17:55:08,264:INFO: 
===> EPOCH: 199 (P2)
2022-11-09 17:55:08,265:INFO: - Computing loss (training)
2022-11-09 17:55:08,601:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5648 (0.5648)
2022-11-09 17:55:08,765:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5987 (0.5816)
2022-11-09 17:55:08,921:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4241 (0.8554)
2022-11-09 17:55:09,020:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1560 (0.9033)
2022-11-09 17:55:09,438:INFO: Dataset: univ                Batch:  1/15	Loss 0.5024 (0.5024)
2022-11-09 17:55:09,604:INFO: Dataset: univ                Batch:  2/15	Loss 0.7070 (0.6115)
2022-11-09 17:55:09,770:INFO: Dataset: univ                Batch:  3/15	Loss 0.5965 (0.6065)
2022-11-09 17:55:09,934:INFO: Dataset: univ                Batch:  4/15	Loss 0.5124 (0.5830)
2022-11-09 17:55:10,099:INFO: Dataset: univ                Batch:  5/15	Loss 0.5139 (0.5693)
2022-11-09 17:55:10,266:INFO: Dataset: univ                Batch:  6/15	Loss 0.5134 (0.5607)
2022-11-09 17:55:10,431:INFO: Dataset: univ                Batch:  7/15	Loss 0.4454 (0.5437)
2022-11-09 17:55:10,593:INFO: Dataset: univ                Batch:  8/15	Loss 0.6947 (0.5616)
2022-11-09 17:55:10,757:INFO: Dataset: univ                Batch:  9/15	Loss 0.4555 (0.5497)
2022-11-09 17:55:10,918:INFO: Dataset: univ                Batch: 10/15	Loss 0.6227 (0.5564)
2022-11-09 17:55:11,083:INFO: Dataset: univ                Batch: 11/15	Loss 0.7638 (0.5759)
2022-11-09 17:55:11,248:INFO: Dataset: univ                Batch: 12/15	Loss 0.4618 (0.5662)
2022-11-09 17:55:11,411:INFO: Dataset: univ                Batch: 13/15	Loss 0.4860 (0.5604)
2022-11-09 17:55:11,574:INFO: Dataset: univ                Batch: 14/15	Loss 0.5172 (0.5576)
2022-11-09 17:55:11,617:INFO: Dataset: univ                Batch: 15/15	Loss 0.0789 (0.5516)
2022-11-09 17:55:12,007:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8558 (0.8558)
2022-11-09 17:55:12,169:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6897 (0.7784)
2022-11-09 17:55:12,333:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5962 (0.7122)
2022-11-09 17:55:12,490:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5291 (0.6652)
2022-11-09 17:55:12,650:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5407 (0.6393)
2022-11-09 17:55:12,809:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8726 (0.6795)
2022-11-09 17:55:12,968:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5780 (0.6643)
2022-11-09 17:55:13,104:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6003 (0.6569)
2022-11-09 17:55:13,500:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7424 (0.7424)
2022-11-09 17:55:13,660:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5767 (0.6600)
2022-11-09 17:55:13,823:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5466 (0.6233)
2022-11-09 17:55:13,980:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9474 (0.6960)
2022-11-09 17:55:14,147:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4942 (0.6533)
2022-11-09 17:55:14,311:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5345 (0.6345)
2022-11-09 17:55:14,474:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7292 (0.6475)
2022-11-09 17:55:14,631:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8461 (0.6695)
2022-11-09 17:55:14,794:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5876 (0.6596)
2022-11-09 17:55:14,951:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7708 (0.6707)
2022-11-09 17:55:15,114:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4344 (0.6504)
2022-11-09 17:55:15,274:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4957 (0.6384)
2022-11-09 17:55:15,435:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4886 (0.6270)
2022-11-09 17:55:15,593:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6396 (0.6279)
2022-11-09 17:55:15,754:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4906 (0.6180)
2022-11-09 17:55:15,911:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5891 (0.6162)
2022-11-09 17:55:16,072:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6001 (0.6153)
2022-11-09 17:55:16,211:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3763 (0.6043)
2022-11-09 17:55:16,270:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_199.pth.tar
2022-11-09 17:55:16,270:INFO: 
===> EPOCH: 200 (P2)
2022-11-09 17:55:16,270:INFO: - Computing loss (training)
2022-11-09 17:55:16,615:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1887 (2.1887)
2022-11-09 17:55:16,775:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6845 (1.9273)
2022-11-09 17:55:16,931:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9651 (1.6126)
2022-11-09 17:55:17,028:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3538 (1.4199)
2022-11-09 17:55:17,487:INFO: Dataset: univ                Batch:  1/15	Loss 0.7952 (0.7952)
2022-11-09 17:55:17,648:INFO: Dataset: univ                Batch:  2/15	Loss 0.5874 (0.6902)
2022-11-09 17:55:17,811:INFO: Dataset: univ                Batch:  3/15	Loss 0.5175 (0.6378)
2022-11-09 17:55:17,971:INFO: Dataset: univ                Batch:  4/15	Loss 0.5295 (0.6122)
2022-11-09 17:55:18,134:INFO: Dataset: univ                Batch:  5/15	Loss 0.5286 (0.5959)
2022-11-09 17:55:18,296:INFO: Dataset: univ                Batch:  6/15	Loss 0.4534 (0.5719)
2022-11-09 17:55:18,457:INFO: Dataset: univ                Batch:  7/15	Loss 0.5054 (0.5627)
2022-11-09 17:55:18,617:INFO: Dataset: univ                Batch:  8/15	Loss 0.5071 (0.5557)
2022-11-09 17:55:18,779:INFO: Dataset: univ                Batch:  9/15	Loss 0.4568 (0.5434)
2022-11-09 17:55:18,938:INFO: Dataset: univ                Batch: 10/15	Loss 0.6017 (0.5491)
2022-11-09 17:55:19,103:INFO: Dataset: univ                Batch: 11/15	Loss 0.4821 (0.5426)
2022-11-09 17:55:19,263:INFO: Dataset: univ                Batch: 12/15	Loss 0.5736 (0.5451)
2022-11-09 17:55:19,426:INFO: Dataset: univ                Batch: 13/15	Loss 0.6791 (0.5552)
2022-11-09 17:55:19,588:INFO: Dataset: univ                Batch: 14/15	Loss 0.4936 (0.5505)
2022-11-09 17:55:19,629:INFO: Dataset: univ                Batch: 15/15	Loss 0.1156 (0.5461)
2022-11-09 17:55:20,007:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6501 (0.6501)
2022-11-09 17:55:20,165:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5409 (0.5924)
2022-11-09 17:55:20,326:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5746 (0.5864)
2022-11-09 17:55:20,481:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5634 (0.5810)
2022-11-09 17:55:20,640:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7715 (0.6189)
2022-11-09 17:55:20,797:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5634 (0.6101)
2022-11-09 17:55:20,955:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8189 (0.6430)
2022-11-09 17:55:21,089:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5390 (0.6329)
2022-11-09 17:55:21,477:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6129 (0.6129)
2022-11-09 17:55:21,636:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4476 (0.5281)
2022-11-09 17:55:21,795:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5548 (0.5364)
2022-11-09 17:55:21,952:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4717 (0.5197)
2022-11-09 17:55:22,113:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4540 (0.5072)
2022-11-09 17:55:22,274:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5323 (0.5118)
2022-11-09 17:55:22,432:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5775 (0.5220)
2022-11-09 17:55:22,588:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5687 (0.5276)
2022-11-09 17:55:22,747:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4107 (0.5121)
2022-11-09 17:55:22,903:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4460 (0.5050)
2022-11-09 17:55:23,063:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4680 (0.5014)
2022-11-09 17:55:23,221:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4489 (0.4967)
2022-11-09 17:55:23,379:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4607 (0.4941)
2022-11-09 17:55:23,536:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5616 (0.4983)
2022-11-09 17:55:23,698:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4534 (0.4953)
2022-11-09 17:55:23,855:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4875 (0.4948)
2022-11-09 17:55:24,015:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8347 (0.5160)
2022-11-09 17:55:24,153:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3982 (0.5101)
2022-11-09 17:55:24,206:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_200.pth.tar
2022-11-09 17:55:24,206:INFO: 
===> EPOCH: 201 (P2)
2022-11-09 17:55:24,207:INFO: - Computing loss (training)
2022-11-09 17:55:24,559:INFO: Dataset: hotel               Batch: 1/4	Loss 3.1680 (3.1680)
2022-11-09 17:55:24,735:INFO: Dataset: hotel               Batch: 2/4	Loss 6.2507 (4.7870)
2022-11-09 17:55:24,892:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7641 (4.4379)
2022-11-09 17:55:24,991:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7027 (3.8220)
2022-11-09 17:55:25,395:INFO: Dataset: univ                Batch:  1/15	Loss 0.9203 (0.9203)
2022-11-09 17:55:25,561:INFO: Dataset: univ                Batch:  2/15	Loss 1.0161 (0.9704)
2022-11-09 17:55:25,726:INFO: Dataset: univ                Batch:  3/15	Loss 0.5396 (0.8367)
2022-11-09 17:55:25,890:INFO: Dataset: univ                Batch:  4/15	Loss 0.8853 (0.8477)
2022-11-09 17:55:26,066:INFO: Dataset: univ                Batch:  5/15	Loss 0.7627 (0.8307)
2022-11-09 17:55:26,233:INFO: Dataset: univ                Batch:  6/15	Loss 0.6557 (0.8030)
2022-11-09 17:55:26,397:INFO: Dataset: univ                Batch:  7/15	Loss 1.0176 (0.8328)
2022-11-09 17:55:26,559:INFO: Dataset: univ                Batch:  8/15	Loss 0.5361 (0.7939)
2022-11-09 17:55:26,724:INFO: Dataset: univ                Batch:  9/15	Loss 0.5439 (0.7665)
2022-11-09 17:55:26,887:INFO: Dataset: univ                Batch: 10/15	Loss 0.9933 (0.7899)
2022-11-09 17:55:27,053:INFO: Dataset: univ                Batch: 11/15	Loss 0.7018 (0.7810)
2022-11-09 17:55:27,219:INFO: Dataset: univ                Batch: 12/15	Loss 0.6712 (0.7723)
2022-11-09 17:55:27,385:INFO: Dataset: univ                Batch: 13/15	Loss 0.5257 (0.7539)
2022-11-09 17:55:27,549:INFO: Dataset: univ                Batch: 14/15	Loss 0.5197 (0.7383)
2022-11-09 17:55:27,592:INFO: Dataset: univ                Batch: 15/15	Loss 0.0829 (0.7282)
2022-11-09 17:55:27,990:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1127 (1.1127)
2022-11-09 17:55:28,157:INFO: Dataset: zara1               Batch: 2/8	Loss 2.1255 (1.5887)
2022-11-09 17:55:28,319:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4892 (1.5581)
2022-11-09 17:55:28,481:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7079 (1.3430)
2022-11-09 17:55:28,643:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5068 (1.1790)
2022-11-09 17:55:28,805:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5462 (1.0828)
2022-11-09 17:55:28,966:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3107 (1.1138)
2022-11-09 17:55:29,105:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2227 (1.1254)
2022-11-09 17:55:29,516:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2634 (1.2634)
2022-11-09 17:55:29,678:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1806 (1.2228)
2022-11-09 17:55:29,838:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9918 (1.1502)
2022-11-09 17:55:29,995:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4829 (0.9745)
2022-11-09 17:55:30,161:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4458 (0.8703)
2022-11-09 17:55:30,319:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4689 (0.7997)
2022-11-09 17:55:30,479:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5862 (0.7674)
2022-11-09 17:55:30,635:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5230 (0.7392)
2022-11-09 17:55:30,795:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7797 (0.7436)
2022-11-09 17:55:30,952:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4751 (0.7164)
2022-11-09 17:55:31,113:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1477 (0.7521)
2022-11-09 17:55:31,271:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9985 (0.7725)
2022-11-09 17:55:31,431:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5751 (0.7566)
2022-11-09 17:55:31,589:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4860 (0.7368)
2022-11-09 17:55:31,750:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5165 (0.7226)
2022-11-09 17:55:31,908:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4702 (0.7067)
2022-11-09 17:55:32,069:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7320 (0.7083)
2022-11-09 17:55:32,210:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5812 (0.7029)
2022-11-09 17:55:32,266:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_201.pth.tar
2022-11-09 17:55:32,266:INFO: 
===> EPOCH: 202 (P2)
2022-11-09 17:55:32,267:INFO: - Computing loss (training)
2022-11-09 17:55:32,616:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7623 (1.7623)
2022-11-09 17:55:32,780:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8188 (1.2861)
2022-11-09 17:55:32,938:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5928 (1.0535)
2022-11-09 17:55:33,038:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4465 (0.9478)
2022-11-09 17:55:33,436:INFO: Dataset: univ                Batch:  1/15	Loss 0.9572 (0.9572)
2022-11-09 17:55:33,601:INFO: Dataset: univ                Batch:  2/15	Loss 1.1255 (1.0371)
2022-11-09 17:55:33,770:INFO: Dataset: univ                Batch:  3/15	Loss 0.4360 (0.8431)
2022-11-09 17:55:33,937:INFO: Dataset: univ                Batch:  4/15	Loss 0.4493 (0.7501)
2022-11-09 17:55:34,102:INFO: Dataset: univ                Batch:  5/15	Loss 0.5750 (0.7164)
2022-11-09 17:55:34,271:INFO: Dataset: univ                Batch:  6/15	Loss 0.5563 (0.6884)
2022-11-09 17:55:34,436:INFO: Dataset: univ                Batch:  7/15	Loss 0.4769 (0.6573)
2022-11-09 17:55:34,599:INFO: Dataset: univ                Batch:  8/15	Loss 0.5212 (0.6413)
2022-11-09 17:55:34,764:INFO: Dataset: univ                Batch:  9/15	Loss 0.5711 (0.6336)
2022-11-09 17:55:34,927:INFO: Dataset: univ                Batch: 10/15	Loss 0.4298 (0.6127)
2022-11-09 17:55:35,093:INFO: Dataset: univ                Batch: 11/15	Loss 0.6456 (0.6157)
2022-11-09 17:55:35,259:INFO: Dataset: univ                Batch: 12/15	Loss 0.5357 (0.6088)
2022-11-09 17:55:35,426:INFO: Dataset: univ                Batch: 13/15	Loss 0.4512 (0.5961)
2022-11-09 17:55:35,591:INFO: Dataset: univ                Batch: 14/15	Loss 0.4766 (0.5879)
2022-11-09 17:55:35,634:INFO: Dataset: univ                Batch: 15/15	Loss 0.0758 (0.5794)
2022-11-09 17:55:36,032:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5113 (0.5113)
2022-11-09 17:55:36,195:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5768 (0.5418)
2022-11-09 17:55:36,358:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6300 (0.5722)
2022-11-09 17:55:36,515:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4868 (0.5517)
2022-11-09 17:55:36,675:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4783 (0.5359)
2022-11-09 17:55:36,835:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5219 (0.5334)
2022-11-09 17:55:36,994:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5016 (0.5285)
2022-11-09 17:55:37,130:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4528 (0.5205)
2022-11-09 17:55:37,606:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4518 (0.4518)
2022-11-09 17:55:37,766:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4707 (0.4611)
2022-11-09 17:55:37,927:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5147 (0.4796)
2022-11-09 17:55:38,090:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3968 (0.4580)
2022-11-09 17:55:38,252:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4672 (0.4599)
2022-11-09 17:55:38,413:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4867 (0.4649)
2022-11-09 17:55:38,573:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4932 (0.4686)
2022-11-09 17:55:38,731:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4264 (0.4638)
2022-11-09 17:55:38,891:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5140 (0.4691)
2022-11-09 17:55:39,049:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4365 (0.4660)
2022-11-09 17:55:39,211:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4082 (0.4602)
2022-11-09 17:55:39,370:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5790 (0.4698)
2022-11-09 17:55:39,530:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5053 (0.4724)
2022-11-09 17:55:39,690:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4125 (0.4678)
2022-11-09 17:55:39,852:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6106 (0.4775)
2022-11-09 17:55:40,011:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4849 (0.4779)
2022-11-09 17:55:40,174:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4894 (0.4786)
2022-11-09 17:55:40,313:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4268 (0.4762)
2022-11-09 17:55:40,370:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_202.pth.tar
2022-11-09 17:55:40,370:INFO: 
===> EPOCH: 203 (P2)
2022-11-09 17:55:40,370:INFO: - Computing loss (training)
2022-11-09 17:55:40,713:INFO: Dataset: hotel               Batch: 1/4	Loss 2.7087 (2.7087)
2022-11-09 17:55:40,877:INFO: Dataset: hotel               Batch: 2/4	Loss 2.7642 (2.7358)
2022-11-09 17:55:41,033:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3720 (2.2885)
2022-11-09 17:55:41,131:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2946 (1.9308)
2022-11-09 17:55:41,532:INFO: Dataset: univ                Batch:  1/15	Loss 0.9630 (0.9630)
2022-11-09 17:55:41,696:INFO: Dataset: univ                Batch:  2/15	Loss 0.4966 (0.7339)
2022-11-09 17:55:41,862:INFO: Dataset: univ                Batch:  3/15	Loss 0.4727 (0.6386)
2022-11-09 17:55:42,024:INFO: Dataset: univ                Batch:  4/15	Loss 1.1718 (0.7750)
2022-11-09 17:55:42,192:INFO: Dataset: univ                Batch:  5/15	Loss 0.6089 (0.7432)
2022-11-09 17:55:42,355:INFO: Dataset: univ                Batch:  6/15	Loss 0.8353 (0.7588)
2022-11-09 17:55:42,519:INFO: Dataset: univ                Batch:  7/15	Loss 0.9908 (0.7914)
2022-11-09 17:55:42,681:INFO: Dataset: univ                Batch:  8/15	Loss 0.5721 (0.7616)
2022-11-09 17:55:42,847:INFO: Dataset: univ                Batch:  9/15	Loss 0.4684 (0.7290)
2022-11-09 17:55:43,010:INFO: Dataset: univ                Batch: 10/15	Loss 0.4279 (0.6983)
2022-11-09 17:55:43,175:INFO: Dataset: univ                Batch: 11/15	Loss 0.4478 (0.6755)
2022-11-09 17:55:43,339:INFO: Dataset: univ                Batch: 12/15	Loss 0.4740 (0.6590)
2022-11-09 17:55:43,503:INFO: Dataset: univ                Batch: 13/15	Loss 0.4762 (0.6443)
2022-11-09 17:55:43,667:INFO: Dataset: univ                Batch: 14/15	Loss 0.4873 (0.6337)
2022-11-09 17:55:43,709:INFO: Dataset: univ                Batch: 15/15	Loss 0.1439 (0.6289)
2022-11-09 17:55:44,106:INFO: Dataset: zara1               Batch: 1/8	Loss 2.5776 (2.5776)
2022-11-09 17:55:44,276:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1053 (1.8183)
2022-11-09 17:55:44,442:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2871 (1.6371)
2022-11-09 17:55:44,606:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6650 (1.3815)
2022-11-09 17:55:44,771:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5796 (1.2443)
2022-11-09 17:55:44,936:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5600 (1.1419)
2022-11-09 17:55:45,101:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8077 (1.0955)
2022-11-09 17:55:45,242:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7425 (1.0626)
2022-11-09 17:55:45,676:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7564 (0.7564)
2022-11-09 17:55:45,842:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1539 (0.9532)
2022-11-09 17:55:46,007:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7469 (0.8887)
2022-11-09 17:55:46,170:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5230 (0.8015)
2022-11-09 17:55:46,335:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9087 (0.8222)
2022-11-09 17:55:46,503:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5215 (0.7700)
2022-11-09 17:55:46,668:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7189 (0.7628)
2022-11-09 17:55:46,831:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5706 (0.7382)
2022-11-09 17:55:46,995:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8227 (0.7476)
2022-11-09 17:55:47,158:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5122 (0.7276)
2022-11-09 17:55:47,323:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4756 (0.7012)
2022-11-09 17:55:47,486:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6750 (0.6990)
2022-11-09 17:55:47,652:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5794 (0.6897)
2022-11-09 17:55:47,817:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5004 (0.6769)
2022-11-09 17:55:47,983:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6304 (0.6739)
2022-11-09 17:55:48,147:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4481 (0.6591)
2022-11-09 17:55:48,314:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4765 (0.6480)
2022-11-09 17:55:48,457:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5717 (0.6444)
2022-11-09 17:55:48,510:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_203.pth.tar
2022-11-09 17:55:48,511:INFO: 
===> EPOCH: 204 (P2)
2022-11-09 17:55:48,511:INFO: - Computing loss (training)
2022-11-09 17:55:48,857:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5561 (1.5561)
2022-11-09 17:55:49,017:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0922 (1.8295)
2022-11-09 17:55:49,175:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8936 (1.5015)
2022-11-09 17:55:49,275:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3641 (1.3019)
2022-11-09 17:55:49,678:INFO: Dataset: univ                Batch:  1/15	Loss 0.7846 (0.7846)
2022-11-09 17:55:49,844:INFO: Dataset: univ                Batch:  2/15	Loss 0.5352 (0.6565)
2022-11-09 17:55:50,008:INFO: Dataset: univ                Batch:  3/15	Loss 0.5502 (0.6207)
2022-11-09 17:55:50,167:INFO: Dataset: univ                Batch:  4/15	Loss 0.6975 (0.6390)
2022-11-09 17:55:50,334:INFO: Dataset: univ                Batch:  5/15	Loss 0.4665 (0.6027)
2022-11-09 17:55:50,497:INFO: Dataset: univ                Batch:  6/15	Loss 0.4769 (0.5817)
2022-11-09 17:55:50,661:INFO: Dataset: univ                Batch:  7/15	Loss 0.4867 (0.5680)
2022-11-09 17:55:50,822:INFO: Dataset: univ                Batch:  8/15	Loss 0.6199 (0.5741)
2022-11-09 17:55:50,986:INFO: Dataset: univ                Batch:  9/15	Loss 0.6384 (0.5814)
2022-11-09 17:55:51,146:INFO: Dataset: univ                Batch: 10/15	Loss 0.5911 (0.5823)
2022-11-09 17:55:51,311:INFO: Dataset: univ                Batch: 11/15	Loss 0.4264 (0.5671)
2022-11-09 17:55:51,471:INFO: Dataset: univ                Batch: 12/15	Loss 0.8588 (0.5920)
2022-11-09 17:55:51,634:INFO: Dataset: univ                Batch: 13/15	Loss 0.4649 (0.5821)
2022-11-09 17:55:51,797:INFO: Dataset: univ                Batch: 14/15	Loss 0.4489 (0.5724)
2022-11-09 17:55:51,839:INFO: Dataset: univ                Batch: 15/15	Loss 0.0794 (0.5656)
2022-11-09 17:55:52,228:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6316 (0.6316)
2022-11-09 17:55:52,392:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6089 (0.6199)
2022-11-09 17:55:52,553:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5105 (0.5838)
2022-11-09 17:55:52,713:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5541 (0.5764)
2022-11-09 17:55:52,872:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5144 (0.5645)
2022-11-09 17:55:53,031:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4888 (0.5528)
2022-11-09 17:55:53,190:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6301 (0.5624)
2022-11-09 17:55:53,327:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4563 (0.5508)
2022-11-09 17:55:53,718:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4863 (0.4863)
2022-11-09 17:55:53,879:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5135 (0.4996)
2022-11-09 17:55:54,044:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5950 (0.5348)
2022-11-09 17:55:54,203:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4411 (0.5098)
2022-11-09 17:55:54,365:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5584 (0.5197)
2022-11-09 17:55:54,527:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4425 (0.5069)
2022-11-09 17:55:54,688:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4485 (0.4990)
2022-11-09 17:55:54,846:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4536 (0.4935)
2022-11-09 17:55:55,008:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4548 (0.4897)
2022-11-09 17:55:55,166:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4360 (0.4850)
2022-11-09 17:55:55,328:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4771 (0.4843)
2022-11-09 17:55:55,488:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4241 (0.4786)
2022-11-09 17:55:55,649:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4062 (0.4733)
2022-11-09 17:55:55,810:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4373 (0.4706)
2022-11-09 17:55:55,972:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3939 (0.4656)
2022-11-09 17:55:56,132:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3806 (0.4599)
2022-11-09 17:55:56,295:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4165 (0.4570)
2022-11-09 17:55:56,434:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4024 (0.4544)
2022-11-09 17:55:56,488:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_204.pth.tar
2022-11-09 17:55:56,488:INFO: 
===> EPOCH: 205 (P2)
2022-11-09 17:55:56,488:INFO: - Computing loss (training)
2022-11-09 17:55:56,847:INFO: Dataset: hotel               Batch: 1/4	Loss 3.1759 (3.1759)
2022-11-09 17:55:57,012:INFO: Dataset: hotel               Batch: 2/4	Loss 4.1792 (3.6530)
2022-11-09 17:55:57,173:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8766 (3.0786)
2022-11-09 17:55:57,273:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6667 (2.6841)
2022-11-09 17:55:57,679:INFO: Dataset: univ                Batch:  1/15	Loss 0.5678 (0.5678)
2022-11-09 17:55:57,845:INFO: Dataset: univ                Batch:  2/15	Loss 0.4817 (0.5254)
2022-11-09 17:55:58,011:INFO: Dataset: univ                Batch:  3/15	Loss 0.6062 (0.5528)
2022-11-09 17:55:58,175:INFO: Dataset: univ                Batch:  4/15	Loss 0.8063 (0.6136)
2022-11-09 17:55:58,338:INFO: Dataset: univ                Batch:  5/15	Loss 0.5324 (0.5990)
2022-11-09 17:55:58,509:INFO: Dataset: univ                Batch:  6/15	Loss 0.7689 (0.6256)
2022-11-09 17:55:58,673:INFO: Dataset: univ                Batch:  7/15	Loss 0.6017 (0.6223)
2022-11-09 17:55:58,835:INFO: Dataset: univ                Batch:  8/15	Loss 0.6622 (0.6271)
2022-11-09 17:55:58,999:INFO: Dataset: univ                Batch:  9/15	Loss 0.5946 (0.6233)
2022-11-09 17:55:59,159:INFO: Dataset: univ                Batch: 10/15	Loss 0.4883 (0.6104)
2022-11-09 17:55:59,324:INFO: Dataset: univ                Batch: 11/15	Loss 0.4909 (0.6005)
2022-11-09 17:55:59,487:INFO: Dataset: univ                Batch: 12/15	Loss 0.4738 (0.5903)
2022-11-09 17:55:59,652:INFO: Dataset: univ                Batch: 13/15	Loss 0.5313 (0.5857)
2022-11-09 17:55:59,815:INFO: Dataset: univ                Batch: 14/15	Loss 0.7284 (0.5951)
2022-11-09 17:55:59,857:INFO: Dataset: univ                Batch: 15/15	Loss 0.0782 (0.5894)
2022-11-09 17:56:00,256:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5752 (1.5752)
2022-11-09 17:56:00,420:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4425 (1.5116)
2022-11-09 17:56:00,579:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8332 (1.2945)
2022-11-09 17:56:00,738:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7497 (1.1552)
2022-11-09 17:56:00,897:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5756 (1.0324)
2022-11-09 17:56:01,055:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0102 (1.0290)
2022-11-09 17:56:01,214:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8550 (1.0062)
2022-11-09 17:56:01,350:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7731 (0.9796)
2022-11-09 17:56:01,734:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9106 (0.9106)
2022-11-09 17:56:01,893:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8071 (0.8613)
2022-11-09 17:56:02,059:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4834 (0.7366)
2022-11-09 17:56:02,217:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4936 (0.6767)
2022-11-09 17:56:02,377:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4604 (0.6341)
2022-11-09 17:56:02,541:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4903 (0.6080)
2022-11-09 17:56:02,701:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4928 (0.5905)
2022-11-09 17:56:02,859:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5313 (0.5837)
2022-11-09 17:56:03,019:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4604 (0.5685)
2022-11-09 17:56:03,176:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5780 (0.5695)
2022-11-09 17:56:03,336:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7085 (0.5820)
2022-11-09 17:56:03,502:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5758 (0.5815)
2022-11-09 17:56:03,662:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5827 (0.5816)
2022-11-09 17:56:03,821:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4934 (0.5754)
2022-11-09 17:56:03,982:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4731 (0.5688)
2022-11-09 17:56:04,141:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4830 (0.5633)
2022-11-09 17:56:04,302:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4649 (0.5577)
2022-11-09 17:56:04,441:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4049 (0.5507)
2022-11-09 17:56:04,495:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_205.pth.tar
2022-11-09 17:56:04,495:INFO: 
===> EPOCH: 206 (P2)
2022-11-09 17:56:04,495:INFO: - Computing loss (training)
2022-11-09 17:56:04,835:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1721 (2.1721)
2022-11-09 17:56:04,997:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0566 (2.1170)
2022-11-09 17:56:05,157:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3012 (1.8702)
2022-11-09 17:56:05,256:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3394 (1.6077)
2022-11-09 17:56:05,646:INFO: Dataset: univ                Batch:  1/15	Loss 0.5136 (0.5136)
2022-11-09 17:56:05,816:INFO: Dataset: univ                Batch:  2/15	Loss 1.1613 (0.8712)
2022-11-09 17:56:05,978:INFO: Dataset: univ                Batch:  3/15	Loss 0.8070 (0.8505)
2022-11-09 17:56:06,138:INFO: Dataset: univ                Batch:  4/15	Loss 0.4244 (0.7440)
2022-11-09 17:56:06,304:INFO: Dataset: univ                Batch:  5/15	Loss 0.5021 (0.7003)
2022-11-09 17:56:06,465:INFO: Dataset: univ                Batch:  6/15	Loss 0.5576 (0.6781)
2022-11-09 17:56:06,628:INFO: Dataset: univ                Batch:  7/15	Loss 1.2479 (0.7590)
2022-11-09 17:56:06,789:INFO: Dataset: univ                Batch:  8/15	Loss 0.7218 (0.7546)
2022-11-09 17:56:06,952:INFO: Dataset: univ                Batch:  9/15	Loss 0.4690 (0.7196)
2022-11-09 17:56:07,112:INFO: Dataset: univ                Batch: 10/15	Loss 0.4680 (0.6942)
2022-11-09 17:56:07,278:INFO: Dataset: univ                Batch: 11/15	Loss 0.4313 (0.6699)
2022-11-09 17:56:07,441:INFO: Dataset: univ                Batch: 12/15	Loss 0.4466 (0.6520)
2022-11-09 17:56:07,605:INFO: Dataset: univ                Batch: 13/15	Loss 0.4526 (0.6357)
2022-11-09 17:56:07,770:INFO: Dataset: univ                Batch: 14/15	Loss 0.4241 (0.6197)
2022-11-09 17:56:07,813:INFO: Dataset: univ                Batch: 15/15	Loss 0.0750 (0.6132)
2022-11-09 17:56:08,206:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5195 (0.5195)
2022-11-09 17:56:08,365:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4838 (0.5006)
2022-11-09 17:56:08,528:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5065 (0.5025)
2022-11-09 17:56:08,686:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5579 (0.5185)
2022-11-09 17:56:08,845:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4958 (0.5136)
2022-11-09 17:56:09,002:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4832 (0.5082)
2022-11-09 17:56:09,161:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8841 (0.5649)
2022-11-09 17:56:09,298:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4892 (0.5566)
2022-11-09 17:56:09,690:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4977 (0.4977)
2022-11-09 17:56:09,850:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4789 (0.4879)
2022-11-09 17:56:10,012:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4481 (0.4754)
2022-11-09 17:56:10,176:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4300 (0.4636)
2022-11-09 17:56:10,337:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4930 (0.4694)
2022-11-09 17:56:10,498:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5265 (0.6462)
2022-11-09 17:56:10,658:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4849 (0.6228)
2022-11-09 17:56:10,817:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4219 (0.5982)
2022-11-09 17:56:10,977:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4120 (0.5782)
2022-11-09 17:56:11,135:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4459 (0.5643)
2022-11-09 17:56:11,295:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4849 (0.5577)
2022-11-09 17:56:11,454:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6262 (0.5639)
2022-11-09 17:56:11,614:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4317 (0.5542)
2022-11-09 17:56:11,775:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5201 (0.5520)
2022-11-09 17:56:11,937:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4893 (0.5481)
2022-11-09 17:56:12,096:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4682 (0.5430)
2022-11-09 17:56:12,259:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5843 (0.5453)
2022-11-09 17:56:12,398:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3513 (0.5350)
2022-11-09 17:56:12,461:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_206.pth.tar
2022-11-09 17:56:12,461:INFO: 
===> EPOCH: 207 (P2)
2022-11-09 17:56:12,461:INFO: - Computing loss (training)
2022-11-09 17:56:12,813:INFO: Dataset: hotel               Batch: 1/4	Loss 6.0998 (6.0998)
2022-11-09 17:56:12,977:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5912 (4.8486)
2022-11-09 17:56:13,210:INFO: Dataset: hotel               Batch: 3/4	Loss 2.9321 (4.1985)
2022-11-09 17:56:13,310:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6672 (3.5789)
2022-11-09 17:56:13,705:INFO: Dataset: univ                Batch:  1/15	Loss 0.5364 (0.5364)
2022-11-09 17:56:13,872:INFO: Dataset: univ                Batch:  2/15	Loss 0.6386 (0.5897)
2022-11-09 17:56:14,039:INFO: Dataset: univ                Batch:  3/15	Loss 0.6084 (0.5962)
2022-11-09 17:56:14,209:INFO: Dataset: univ                Batch:  4/15	Loss 0.9606 (0.6848)
2022-11-09 17:56:14,377:INFO: Dataset: univ                Batch:  5/15	Loss 0.8654 (0.7220)
2022-11-09 17:56:14,544:INFO: Dataset: univ                Batch:  6/15	Loss 0.8582 (0.7435)
2022-11-09 17:56:14,711:INFO: Dataset: univ                Batch:  7/15	Loss 0.5799 (0.7207)
2022-11-09 17:56:14,876:INFO: Dataset: univ                Batch:  8/15	Loss 1.0483 (0.7625)
2022-11-09 17:56:15,055:INFO: Dataset: univ                Batch:  9/15	Loss 0.5458 (0.7378)
2022-11-09 17:56:15,243:INFO: Dataset: univ                Batch: 10/15	Loss 0.5236 (0.7161)
2022-11-09 17:56:15,434:INFO: Dataset: univ                Batch: 11/15	Loss 0.7989 (0.7228)
2022-11-09 17:56:15,620:INFO: Dataset: univ                Batch: 12/15	Loss 1.0387 (0.7482)
2022-11-09 17:56:15,806:INFO: Dataset: univ                Batch: 13/15	Loss 0.4768 (0.7258)
2022-11-09 17:56:15,980:INFO: Dataset: univ                Batch: 14/15	Loss 0.4837 (0.7085)
2022-11-09 17:56:16,025:INFO: Dataset: univ                Batch: 15/15	Loss 0.2533 (0.7033)
2022-11-09 17:56:16,412:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2826 (1.2826)
2022-11-09 17:56:16,574:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2296 (1.2565)
2022-11-09 17:56:16,734:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7442 (1.4240)
2022-11-09 17:56:16,895:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5974 (1.1922)
2022-11-09 17:56:17,053:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5426 (1.0601)
2022-11-09 17:56:17,211:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5966 (0.9757)
2022-11-09 17:56:17,370:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5604 (1.0478)
2022-11-09 17:56:17,505:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6060 (1.1107)
2022-11-09 17:56:17,944:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0160 (1.0160)
2022-11-09 17:56:18,109:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5938 (0.8033)
2022-11-09 17:56:18,270:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5787 (0.7251)
2022-11-09 17:56:18,427:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6693 (0.7113)
2022-11-09 17:56:18,587:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4694 (0.6608)
2022-11-09 17:56:18,748:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4954 (0.6326)
2022-11-09 17:56:18,907:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5615 (0.6226)
2022-11-09 17:56:19,064:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1988 (0.6945)
2022-11-09 17:56:19,225:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6890 (0.6939)
2022-11-09 17:56:19,383:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6297 (0.6879)
2022-11-09 17:56:19,543:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5709 (0.6785)
2022-11-09 17:56:19,701:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5228 (0.6647)
2022-11-09 17:56:19,863:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4186 (0.6457)
2022-11-09 17:56:20,021:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4451 (0.6297)
2022-11-09 17:56:20,182:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5338 (0.6232)
2022-11-09 17:56:20,341:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5029 (0.6151)
2022-11-09 17:56:20,502:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6512 (0.6170)
2022-11-09 17:56:20,641:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4900 (0.6108)
2022-11-09 17:56:20,695:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_207.pth.tar
2022-11-09 17:56:20,695:INFO: 
===> EPOCH: 208 (P2)
2022-11-09 17:56:20,696:INFO: - Computing loss (training)
2022-11-09 17:56:21,038:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4687 (2.4687)
2022-11-09 17:56:21,202:INFO: Dataset: hotel               Batch: 2/4	Loss 2.3894 (2.4275)
2022-11-09 17:56:21,361:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7437 (1.8736)
2022-11-09 17:56:21,463:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3413 (1.5825)
2022-11-09 17:56:21,870:INFO: Dataset: univ                Batch:  1/15	Loss 0.5418 (0.5418)
2022-11-09 17:56:22,033:INFO: Dataset: univ                Batch:  2/15	Loss 0.8656 (0.7187)
2022-11-09 17:56:22,197:INFO: Dataset: univ                Batch:  3/15	Loss 0.3991 (0.6170)
2022-11-09 17:56:22,362:INFO: Dataset: univ                Batch:  4/15	Loss 0.4311 (0.5728)
2022-11-09 17:56:22,525:INFO: Dataset: univ                Batch:  5/15	Loss 0.4875 (0.5557)
2022-11-09 17:56:22,688:INFO: Dataset: univ                Batch:  6/15	Loss 1.2457 (0.6641)
2022-11-09 17:56:22,851:INFO: Dataset: univ                Batch:  7/15	Loss 0.4770 (0.6391)
2022-11-09 17:56:23,012:INFO: Dataset: univ                Batch:  8/15	Loss 0.4274 (0.6119)
2022-11-09 17:56:23,176:INFO: Dataset: univ                Batch:  9/15	Loss 0.4064 (0.5876)
2022-11-09 17:56:23,338:INFO: Dataset: univ                Batch: 10/15	Loss 0.4694 (0.5755)
2022-11-09 17:56:23,501:INFO: Dataset: univ                Batch: 11/15	Loss 0.4221 (0.5620)
2022-11-09 17:56:23,662:INFO: Dataset: univ                Batch: 12/15	Loss 0.4406 (0.5520)
2022-11-09 17:56:23,825:INFO: Dataset: univ                Batch: 13/15	Loss 0.4766 (0.5463)
2022-11-09 17:56:23,987:INFO: Dataset: univ                Batch: 14/15	Loss 0.4043 (0.5357)
2022-11-09 17:56:24,029:INFO: Dataset: univ                Batch: 15/15	Loss 0.0748 (0.5303)
2022-11-09 17:56:24,424:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5134 (0.5134)
2022-11-09 17:56:24,591:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4534 (0.4822)
2022-11-09 17:56:24,754:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9543 (0.6223)
2022-11-09 17:56:24,914:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6744 (0.6352)
2022-11-09 17:56:25,075:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5760 (0.6219)
2022-11-09 17:56:25,236:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4342 (0.5877)
2022-11-09 17:56:25,398:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0484 (0.6586)
2022-11-09 17:56:25,536:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4794 (0.6405)
2022-11-09 17:56:25,934:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4390 (0.4390)
2022-11-09 17:56:26,095:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4272 (0.4329)
2022-11-09 17:56:26,256:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4358 (0.4338)
2022-11-09 17:56:26,420:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4052 (0.4262)
2022-11-09 17:56:26,580:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4432 (0.4293)
2022-11-09 17:56:26,741:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4034 (0.4249)
2022-11-09 17:56:26,901:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5869 (0.4480)
2022-11-09 17:56:27,059:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0131 (0.5236)
2022-11-09 17:56:27,219:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4338 (0.5139)
2022-11-09 17:56:27,378:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4432 (0.5074)
2022-11-09 17:56:27,539:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5369 (0.5100)
2022-11-09 17:56:27,697:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6585 (0.5223)
2022-11-09 17:56:27,859:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4418 (0.5159)
2022-11-09 17:56:28,019:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5200 (0.5162)
2022-11-09 17:56:28,180:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5360 (0.5176)
2022-11-09 17:56:28,341:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8231 (0.5363)
2022-11-09 17:56:28,504:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3640 (0.5263)
2022-11-09 17:56:28,643:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3776 (0.5190)
2022-11-09 17:56:28,696:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_208.pth.tar
2022-11-09 17:56:28,696:INFO: 
===> EPOCH: 209 (P2)
2022-11-09 17:56:28,696:INFO: - Computing loss (training)
2022-11-09 17:56:29,038:INFO: Dataset: hotel               Batch: 1/4	Loss 2.8319 (2.8319)
2022-11-09 17:56:29,202:INFO: Dataset: hotel               Batch: 2/4	Loss 4.4943 (3.6491)
2022-11-09 17:56:29,361:INFO: Dataset: hotel               Batch: 3/4	Loss 3.1815 (3.4915)
2022-11-09 17:56:29,459:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3807 (2.9498)
2022-11-09 17:56:29,865:INFO: Dataset: univ                Batch:  1/15	Loss 0.4099 (0.4099)
2022-11-09 17:56:30,029:INFO: Dataset: univ                Batch:  2/15	Loss 0.4357 (0.4218)
2022-11-09 17:56:30,195:INFO: Dataset: univ                Batch:  3/15	Loss 0.4779 (0.4386)
2022-11-09 17:56:30,362:INFO: Dataset: univ                Batch:  4/15	Loss 0.4577 (0.4436)
2022-11-09 17:56:30,531:INFO: Dataset: univ                Batch:  5/15	Loss 0.5116 (0.4591)
2022-11-09 17:56:30,694:INFO: Dataset: univ                Batch:  6/15	Loss 0.6197 (0.4841)
2022-11-09 17:56:30,861:INFO: Dataset: univ                Batch:  7/15	Loss 0.4852 (0.4842)
2022-11-09 17:56:31,022:INFO: Dataset: univ                Batch:  8/15	Loss 0.5348 (0.4908)
2022-11-09 17:56:31,189:INFO: Dataset: univ                Batch:  9/15	Loss 1.0261 (0.5546)
2022-11-09 17:56:31,352:INFO: Dataset: univ                Batch: 10/15	Loss 0.6313 (0.5630)
2022-11-09 17:56:31,519:INFO: Dataset: univ                Batch: 11/15	Loss 0.9851 (0.6067)
2022-11-09 17:56:31,683:INFO: Dataset: univ                Batch: 12/15	Loss 0.5739 (0.6040)
2022-11-09 17:56:31,850:INFO: Dataset: univ                Batch: 13/15	Loss 0.4786 (0.5951)
2022-11-09 17:56:32,013:INFO: Dataset: univ                Batch: 14/15	Loss 0.5945 (0.5951)
2022-11-09 17:56:32,056:INFO: Dataset: univ                Batch: 15/15	Loss 0.0772 (0.5868)
2022-11-09 17:56:32,447:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8064 (1.8064)
2022-11-09 17:56:32,611:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3622 (1.5746)
2022-11-09 17:56:32,775:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8073 (1.2854)
2022-11-09 17:56:32,933:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7063 (1.1492)
2022-11-09 17:56:33,093:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5167 (1.0314)
2022-11-09 17:56:33,254:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5100 (0.9400)
2022-11-09 17:56:33,414:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8982 (0.9338)
2022-11-09 17:56:33,551:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4505 (0.9934)
2022-11-09 17:56:33,956:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8090 (0.8090)
2022-11-09 17:56:34,118:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6754 (0.7437)
2022-11-09 17:56:34,284:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6619 (0.7175)
2022-11-09 17:56:34,442:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4453 (0.6536)
2022-11-09 17:56:34,605:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4289 (0.6064)
2022-11-09 17:56:34,767:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4560 (0.5805)
2022-11-09 17:56:34,929:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7438 (0.6049)
2022-11-09 17:56:35,086:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9396 (0.6467)
2022-11-09 17:56:35,249:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9581 (0.6801)
2022-11-09 17:56:35,407:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4204 (0.6534)
2022-11-09 17:56:35,569:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5878 (0.6478)
2022-11-09 17:56:35,728:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4459 (0.6313)
2022-11-09 17:56:35,891:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7250 (0.6385)
2022-11-09 17:56:36,050:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5262 (0.6309)
2022-11-09 17:56:36,213:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5246 (0.6242)
2022-11-09 17:56:36,374:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6100 (0.6234)
2022-11-09 17:56:36,537:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5458 (0.6189)
2022-11-09 17:56:36,676:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5763 (0.6171)
2022-11-09 17:56:36,731:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_209.pth.tar
2022-11-09 17:56:36,731:INFO: 
===> EPOCH: 210 (P2)
2022-11-09 17:56:36,732:INFO: - Computing loss (training)
2022-11-09 17:56:37,076:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9714 (0.9714)
2022-11-09 17:56:37,240:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9104 (1.4221)
2022-11-09 17:56:37,398:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3544 (1.4000)
2022-11-09 17:56:37,497:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2868 (1.2091)
2022-11-09 17:56:37,901:INFO: Dataset: univ                Batch:  1/15	Loss 0.4658 (0.4658)
2022-11-09 17:56:38,066:INFO: Dataset: univ                Batch:  2/15	Loss 0.7003 (0.5774)
2022-11-09 17:56:38,232:INFO: Dataset: univ                Batch:  3/15	Loss 0.7367 (0.6266)
2022-11-09 17:56:38,397:INFO: Dataset: univ                Batch:  4/15	Loss 0.6763 (0.6391)
2022-11-09 17:56:38,562:INFO: Dataset: univ                Batch:  5/15	Loss 0.4742 (0.6015)
2022-11-09 17:56:38,728:INFO: Dataset: univ                Batch:  6/15	Loss 0.5700 (0.5960)
2022-11-09 17:56:38,892:INFO: Dataset: univ                Batch:  7/15	Loss 0.5022 (0.5821)
2022-11-09 17:56:39,054:INFO: Dataset: univ                Batch:  8/15	Loss 0.4268 (0.5617)
2022-11-09 17:56:39,219:INFO: Dataset: univ                Batch:  9/15	Loss 0.5728 (0.5629)
2022-11-09 17:56:39,382:INFO: Dataset: univ                Batch: 10/15	Loss 0.4326 (0.5489)
2022-11-09 17:56:39,549:INFO: Dataset: univ                Batch: 11/15	Loss 0.4362 (0.5384)
2022-11-09 17:56:39,711:INFO: Dataset: univ                Batch: 12/15	Loss 0.4441 (0.5309)
2022-11-09 17:56:39,876:INFO: Dataset: univ                Batch: 13/15	Loss 0.5047 (0.5291)
2022-11-09 17:56:40,040:INFO: Dataset: univ                Batch: 14/15	Loss 0.4252 (0.5225)
2022-11-09 17:56:40,082:INFO: Dataset: univ                Batch: 15/15	Loss 0.0702 (0.5162)
2022-11-09 17:56:40,466:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5600 (0.5600)
2022-11-09 17:56:40,631:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5192 (0.5390)
2022-11-09 17:56:40,792:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4545 (0.5101)
2022-11-09 17:56:40,953:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4695 (0.4992)
2022-11-09 17:56:41,112:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4585 (0.4911)
2022-11-09 17:56:41,272:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5638 (0.5020)
2022-11-09 17:56:41,432:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4404 (0.4925)
2022-11-09 17:56:41,568:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3723 (0.4803)
2022-11-09 17:56:41,954:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4806 (0.4806)
2022-11-09 17:56:42,112:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5167 (0.4979)
2022-11-09 17:56:42,277:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4832 (0.4931)
2022-11-09 17:56:42,436:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6331 (0.5294)
2022-11-09 17:56:42,596:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4277 (0.5081)
2022-11-09 17:56:42,764:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4370 (0.4953)
2022-11-09 17:56:42,924:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4070 (0.4824)
2022-11-09 17:56:43,081:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7987 (0.5188)
2022-11-09 17:56:43,241:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6948 (0.5377)
2022-11-09 17:56:43,398:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4774 (0.5316)
2022-11-09 17:56:43,560:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4303 (0.5210)
2022-11-09 17:56:43,718:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3927 (0.5104)
2022-11-09 17:56:43,879:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4280 (0.5038)
2022-11-09 17:56:44,036:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4225 (0.4983)
2022-11-09 17:56:44,199:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3920 (0.4913)
2022-11-09 17:56:44,359:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4228 (0.4871)
2022-11-09 17:56:44,521:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3627 (0.4794)
2022-11-09 17:56:44,659:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5466 (0.4827)
2022-11-09 17:56:44,713:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_210.pth.tar
2022-11-09 17:56:44,713:INFO: 
===> EPOCH: 211 (P2)
2022-11-09 17:56:44,714:INFO: - Computing loss (training)
2022-11-09 17:56:45,060:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6954 (2.6954)
2022-11-09 17:56:45,221:INFO: Dataset: hotel               Batch: 2/4	Loss 2.5063 (2.6057)
2022-11-09 17:56:45,383:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5551 (2.2459)
2022-11-09 17:56:45,484:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5503 (1.9350)
2022-11-09 17:56:45,882:INFO: Dataset: univ                Batch:  1/15	Loss 0.4157 (0.4157)
2022-11-09 17:56:46,050:INFO: Dataset: univ                Batch:  2/15	Loss 0.4772 (0.4474)
2022-11-09 17:56:46,216:INFO: Dataset: univ                Batch:  3/15	Loss 0.4417 (0.4455)
2022-11-09 17:56:46,385:INFO: Dataset: univ                Batch:  4/15	Loss 0.4342 (0.4425)
2022-11-09 17:56:46,552:INFO: Dataset: univ                Batch:  5/15	Loss 0.4549 (0.4454)
2022-11-09 17:56:46,718:INFO: Dataset: univ                Batch:  6/15	Loss 0.6971 (0.4911)
2022-11-09 17:56:46,883:INFO: Dataset: univ                Batch:  7/15	Loss 0.4623 (0.4871)
2022-11-09 17:56:47,049:INFO: Dataset: univ                Batch:  8/15	Loss 0.4983 (0.4885)
2022-11-09 17:56:47,215:INFO: Dataset: univ                Batch:  9/15	Loss 0.6711 (0.5097)
2022-11-09 17:56:47,379:INFO: Dataset: univ                Batch: 10/15	Loss 1.1494 (0.5762)
2022-11-09 17:56:47,548:INFO: Dataset: univ                Batch: 11/15	Loss 0.6213 (0.5802)
2022-11-09 17:56:47,711:INFO: Dataset: univ                Batch: 12/15	Loss 0.6213 (0.5836)
2022-11-09 17:56:47,879:INFO: Dataset: univ                Batch: 13/15	Loss 0.4578 (0.5738)
2022-11-09 17:56:48,045:INFO: Dataset: univ                Batch: 14/15	Loss 0.4180 (0.5621)
2022-11-09 17:56:48,088:INFO: Dataset: univ                Batch: 15/15	Loss 0.0709 (0.5554)
2022-11-09 17:56:48,476:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5359 (1.5359)
2022-11-09 17:56:48,638:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8433 (1.6842)
2022-11-09 17:56:48,797:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5183 (1.6254)
2022-11-09 17:56:48,953:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7111 (1.4030)
2022-11-09 17:56:49,115:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5135 (1.2021)
2022-11-09 17:56:49,349:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5377 (1.0876)
2022-11-09 17:56:49,506:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1175 (1.0919)
2022-11-09 17:56:49,644:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8517 (1.0660)
2022-11-09 17:56:50,034:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9442 (0.9442)
2022-11-09 17:56:50,199:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8835 (0.9160)
2022-11-09 17:56:50,359:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4650 (0.7578)
2022-11-09 17:56:50,515:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4245 (0.6758)
2022-11-09 17:56:50,675:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4263 (0.6272)
2022-11-09 17:56:50,836:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5044 (0.6053)
2022-11-09 17:56:50,995:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6663 (0.6145)
2022-11-09 17:56:51,152:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4923 (0.5994)
2022-11-09 17:56:51,312:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4256 (0.5810)
2022-11-09 17:56:51,469:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5721 (0.5802)
2022-11-09 17:56:51,628:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5056 (0.5741)
2022-11-09 17:56:51,788:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4602 (0.5649)
2022-11-09 17:56:51,948:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4619 (0.5574)
2022-11-09 17:56:52,106:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4265 (0.5485)
2022-11-09 17:56:52,268:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4559 (0.5424)
2022-11-09 17:56:52,427:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3994 (0.5330)
2022-11-09 17:56:52,589:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4232 (0.5266)
2022-11-09 17:56:52,728:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3648 (0.5188)
2022-11-09 17:56:52,782:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_211.pth.tar
2022-11-09 17:56:52,782:INFO: 
===> EPOCH: 212 (P2)
2022-11-09 17:56:52,782:INFO: - Computing loss (training)
2022-11-09 17:56:53,122:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9051 (1.9051)
2022-11-09 17:56:53,286:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8592 (1.3886)
2022-11-09 17:56:53,444:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8823 (1.2104)
2022-11-09 17:56:53,545:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3307 (1.0596)
2022-11-09 17:56:53,947:INFO: Dataset: univ                Batch:  1/15	Loss 0.4458 (0.4458)
2022-11-09 17:56:54,117:INFO: Dataset: univ                Batch:  2/15	Loss 0.4362 (0.4408)
2022-11-09 17:56:54,282:INFO: Dataset: univ                Batch:  3/15	Loss 0.3903 (0.4243)
2022-11-09 17:56:54,443:INFO: Dataset: univ                Batch:  4/15	Loss 0.6011 (0.4669)
2022-11-09 17:56:54,607:INFO: Dataset: univ                Batch:  5/15	Loss 0.4450 (0.4626)
2022-11-09 17:56:54,773:INFO: Dataset: univ                Batch:  6/15	Loss 0.4346 (0.4579)
2022-11-09 17:56:54,936:INFO: Dataset: univ                Batch:  7/15	Loss 0.4362 (0.4547)
2022-11-09 17:56:55,099:INFO: Dataset: univ                Batch:  8/15	Loss 0.4038 (0.4477)
2022-11-09 17:56:55,264:INFO: Dataset: univ                Batch:  9/15	Loss 0.4570 (0.4488)
2022-11-09 17:56:55,426:INFO: Dataset: univ                Batch: 10/15	Loss 0.4498 (0.4489)
2022-11-09 17:56:55,593:INFO: Dataset: univ                Batch: 11/15	Loss 0.4417 (0.4482)
2022-11-09 17:56:55,755:INFO: Dataset: univ                Batch: 12/15	Loss 0.4182 (0.4459)
2022-11-09 17:56:55,921:INFO: Dataset: univ                Batch: 13/15	Loss 0.3957 (0.4418)
2022-11-09 17:56:56,084:INFO: Dataset: univ                Batch: 14/15	Loss 0.4198 (0.4403)
2022-11-09 17:56:56,125:INFO: Dataset: univ                Batch: 15/15	Loss 0.0858 (0.4344)
2022-11-09 17:56:56,523:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5182 (0.5182)
2022-11-09 17:56:56,686:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3412 (0.9001)
2022-11-09 17:56:56,847:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5427 (0.7911)
2022-11-09 17:56:57,006:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5694 (0.7359)
2022-11-09 17:56:57,167:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4684 (0.6871)
2022-11-09 17:56:57,329:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5984 (0.6727)
2022-11-09 17:56:57,491:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4952 (0.6496)
2022-11-09 17:56:57,629:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4819 (0.6282)
2022-11-09 17:56:58,034:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3721 (0.3721)
2022-11-09 17:56:58,195:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6878 (0.5328)
2022-11-09 17:56:58,360:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4361 (0.5010)
2022-11-09 17:56:58,522:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4185 (0.4798)
2022-11-09 17:56:58,686:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4626 (0.4764)
2022-11-09 17:56:58,850:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6010 (0.4963)
2022-11-09 17:56:59,011:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4839 (0.4945)
2022-11-09 17:56:59,171:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5037 (0.4958)
2022-11-09 17:56:59,334:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5226 (0.4989)
2022-11-09 17:56:59,494:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3914 (0.4866)
2022-11-09 17:56:59,655:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4108 (0.4799)
2022-11-09 17:56:59,817:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3951 (0.4727)
2022-11-09 17:56:59,979:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3937 (0.4666)
2022-11-09 17:57:00,141:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4970 (0.4688)
2022-11-09 17:57:00,306:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4999 (0.4706)
2022-11-09 17:57:00,468:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5314 (0.4747)
2022-11-09 17:57:00,631:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4167 (0.4714)
2022-11-09 17:57:00,773:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3308 (0.4655)
2022-11-09 17:57:00,826:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_212.pth.tar
2022-11-09 17:57:00,827:INFO: 
===> EPOCH: 213 (P2)
2022-11-09 17:57:00,827:INFO: - Computing loss (training)
2022-11-09 17:57:01,170:INFO: Dataset: hotel               Batch: 1/4	Loss 3.3264 (3.3264)
2022-11-09 17:57:01,332:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4658 (2.9120)
2022-11-09 17:57:01,489:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2437 (2.3231)
2022-11-09 17:57:01,588:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7966 (2.0572)
2022-11-09 17:57:02,022:INFO: Dataset: univ                Batch:  1/15	Loss 0.4242 (0.4242)
2022-11-09 17:57:02,190:INFO: Dataset: univ                Batch:  2/15	Loss 0.4211 (0.4226)
2022-11-09 17:57:02,356:INFO: Dataset: univ                Batch:  3/15	Loss 0.4661 (0.4371)
2022-11-09 17:57:02,523:INFO: Dataset: univ                Batch:  4/15	Loss 0.4143 (0.4315)
2022-11-09 17:57:02,688:INFO: Dataset: univ                Batch:  5/15	Loss 0.4829 (0.4416)
2022-11-09 17:57:02,855:INFO: Dataset: univ                Batch:  6/15	Loss 0.5003 (0.4509)
2022-11-09 17:57:03,021:INFO: Dataset: univ                Batch:  7/15	Loss 0.5393 (0.4637)
2022-11-09 17:57:03,184:INFO: Dataset: univ                Batch:  8/15	Loss 0.4392 (0.4609)
2022-11-09 17:57:03,350:INFO: Dataset: univ                Batch:  9/15	Loss 0.7515 (0.4942)
2022-11-09 17:57:03,518:INFO: Dataset: univ                Batch: 10/15	Loss 0.6673 (0.5104)
2022-11-09 17:57:03,685:INFO: Dataset: univ                Batch: 11/15	Loss 0.4284 (0.5033)
2022-11-09 17:57:03,850:INFO: Dataset: univ                Batch: 12/15	Loss 0.5496 (0.5069)
2022-11-09 17:57:04,015:INFO: Dataset: univ                Batch: 13/15	Loss 0.4759 (0.5047)
2022-11-09 17:57:04,179:INFO: Dataset: univ                Batch: 14/15	Loss 0.4222 (0.4991)
2022-11-09 17:57:04,221:INFO: Dataset: univ                Batch: 15/15	Loss 0.0865 (0.4941)
2022-11-09 17:57:04,608:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4981 (1.4981)
2022-11-09 17:57:04,771:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8264 (1.1477)
2022-11-09 17:57:04,933:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5199 (0.9532)
2022-11-09 17:57:05,088:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4775 (0.8392)
2022-11-09 17:57:05,246:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5405 (0.7825)
2022-11-09 17:57:05,405:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5028 (0.7344)
2022-11-09 17:57:05,564:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6806 (0.7269)
2022-11-09 17:57:05,699:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5688 (0.7082)
2022-11-09 17:57:06,089:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6772 (0.6772)
2022-11-09 17:57:06,248:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4477 (0.5541)
2022-11-09 17:57:06,411:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4035 (0.5085)
2022-11-09 17:57:06,566:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8167 (0.5943)
2022-11-09 17:57:06,724:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3905 (0.5551)
2022-11-09 17:57:06,884:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4165 (0.5305)
2022-11-09 17:57:07,041:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4320 (0.5160)
2022-11-09 17:57:07,197:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4740 (0.5109)
2022-11-09 17:57:07,355:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5019 (0.5099)
2022-11-09 17:57:07,512:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4962 (0.5086)
2022-11-09 17:57:07,672:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6749 (0.5228)
2022-11-09 17:57:07,829:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4576 (0.5174)
2022-11-09 17:57:07,988:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4552 (0.5127)
2022-11-09 17:57:08,145:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4057 (0.5051)
2022-11-09 17:57:08,306:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3880 (0.4969)
2022-11-09 17:57:08,463:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4415 (0.4935)
2022-11-09 17:57:08,623:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4063 (0.4881)
2022-11-09 17:57:08,760:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4540 (0.4866)
2022-11-09 17:57:08,827:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_213.pth.tar
2022-11-09 17:57:08,827:INFO: 
===> EPOCH: 214 (P2)
2022-11-09 17:57:08,827:INFO: - Computing loss (training)
2022-11-09 17:57:09,179:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9647 (1.9647)
2022-11-09 17:57:09,341:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2432 (1.6270)
2022-11-09 17:57:09,498:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5882 (1.2773)
2022-11-09 17:57:09,595:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5143 (1.1334)
2022-11-09 17:57:10,003:INFO: Dataset: univ                Batch:  1/15	Loss 0.3996 (0.3996)
2022-11-09 17:57:10,167:INFO: Dataset: univ                Batch:  2/15	Loss 0.4343 (0.4169)
2022-11-09 17:57:10,333:INFO: Dataset: univ                Batch:  3/15	Loss 0.4850 (0.4405)
2022-11-09 17:57:10,494:INFO: Dataset: univ                Batch:  4/15	Loss 0.4656 (0.4466)
2022-11-09 17:57:10,659:INFO: Dataset: univ                Batch:  5/15	Loss 0.4662 (0.4506)
2022-11-09 17:57:10,825:INFO: Dataset: univ                Batch:  6/15	Loss 0.4229 (0.4459)
2022-11-09 17:57:10,988:INFO: Dataset: univ                Batch:  7/15	Loss 0.4351 (0.4443)
2022-11-09 17:57:11,148:INFO: Dataset: univ                Batch:  8/15	Loss 0.4739 (0.4481)
2022-11-09 17:57:11,312:INFO: Dataset: univ                Batch:  9/15	Loss 0.4112 (0.4442)
2022-11-09 17:57:11,473:INFO: Dataset: univ                Batch: 10/15	Loss 0.6360 (0.4634)
2022-11-09 17:57:11,638:INFO: Dataset: univ                Batch: 11/15	Loss 0.4783 (0.4646)
2022-11-09 17:57:11,801:INFO: Dataset: univ                Batch: 12/15	Loss 0.3878 (0.4577)
2022-11-09 17:57:11,964:INFO: Dataset: univ                Batch: 13/15	Loss 0.4162 (0.4543)
2022-11-09 17:57:12,126:INFO: Dataset: univ                Batch: 14/15	Loss 0.4499 (0.4540)
2022-11-09 17:57:12,168:INFO: Dataset: univ                Batch: 15/15	Loss 0.0773 (0.4498)
2022-11-09 17:57:12,570:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5887 (0.5887)
2022-11-09 17:57:12,743:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4830 (0.5398)
2022-11-09 17:57:12,904:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6274 (0.5686)
2022-11-09 17:57:13,060:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4565 (0.5420)
2022-11-09 17:57:13,222:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4451 (0.5229)
2022-11-09 17:57:13,380:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4096 (0.5049)
2022-11-09 17:57:13,539:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9430 (0.5709)
2022-11-09 17:57:13,675:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4008 (0.5536)
2022-11-09 17:57:14,064:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7527 (0.7527)
2022-11-09 17:57:14,229:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7595 (0.7561)
2022-11-09 17:57:14,390:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5074 (0.6742)
2022-11-09 17:57:14,546:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4178 (0.6128)
2022-11-09 17:57:14,707:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4190 (0.5738)
2022-11-09 17:57:14,868:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4058 (0.5481)
2022-11-09 17:57:15,028:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6716 (0.5655)
2022-11-09 17:57:15,184:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4967 (0.5569)
2022-11-09 17:57:15,345:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6192 (0.5638)
2022-11-09 17:57:15,502:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4186 (0.5497)
2022-11-09 17:57:15,664:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5198 (0.5469)
2022-11-09 17:57:15,822:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4088 (0.5343)
2022-11-09 17:57:15,982:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4233 (0.5263)
2022-11-09 17:57:16,140:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4845 (0.5236)
2022-11-09 17:57:16,302:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4351 (0.5172)
2022-11-09 17:57:16,461:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4075 (0.5103)
2022-11-09 17:57:16,622:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5980 (0.5151)
2022-11-09 17:57:16,761:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4461 (0.5115)
2022-11-09 17:57:16,820:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_214.pth.tar
2022-11-09 17:57:16,820:INFO: 
===> EPOCH: 215 (P2)
2022-11-09 17:57:16,821:INFO: - Computing loss (training)
2022-11-09 17:57:17,168:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8247 (1.8247)
2022-11-09 17:57:17,333:INFO: Dataset: hotel               Batch: 2/4	Loss 2.3764 (2.1052)
2022-11-09 17:57:17,490:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0206 (1.7385)
2022-11-09 17:57:17,590:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8848 (1.5943)
2022-11-09 17:57:17,996:INFO: Dataset: univ                Batch:  1/15	Loss 0.5776 (0.5776)
2022-11-09 17:57:18,165:INFO: Dataset: univ                Batch:  2/15	Loss 0.7274 (0.6616)
2022-11-09 17:57:18,331:INFO: Dataset: univ                Batch:  3/15	Loss 0.5456 (0.6237)
2022-11-09 17:57:18,493:INFO: Dataset: univ                Batch:  4/15	Loss 0.4988 (0.5892)
2022-11-09 17:57:18,658:INFO: Dataset: univ                Batch:  5/15	Loss 0.6707 (0.6055)
2022-11-09 17:57:18,823:INFO: Dataset: univ                Batch:  6/15	Loss 0.5160 (0.5898)
2022-11-09 17:57:18,986:INFO: Dataset: univ                Batch:  7/15	Loss 0.5219 (0.5800)
2022-11-09 17:57:19,148:INFO: Dataset: univ                Batch:  8/15	Loss 0.6020 (0.5832)
2022-11-09 17:57:19,313:INFO: Dataset: univ                Batch:  9/15	Loss 0.6161 (0.5874)
2022-11-09 17:57:19,474:INFO: Dataset: univ                Batch: 10/15	Loss 0.4716 (0.5765)
2022-11-09 17:57:19,637:INFO: Dataset: univ                Batch: 11/15	Loss 0.5641 (0.5753)
2022-11-09 17:57:19,801:INFO: Dataset: univ                Batch: 12/15	Loss 0.4555 (0.5656)
2022-11-09 17:57:19,965:INFO: Dataset: univ                Batch: 13/15	Loss 0.4571 (0.5573)
2022-11-09 17:57:20,128:INFO: Dataset: univ                Batch: 14/15	Loss 0.6378 (0.5633)
2022-11-09 17:57:20,170:INFO: Dataset: univ                Batch: 15/15	Loss 0.0651 (0.5572)
2022-11-09 17:57:20,562:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7885 (1.7885)
2022-11-09 17:57:20,724:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1367 (1.4339)
2022-11-09 17:57:20,883:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4532 (1.1144)
2022-11-09 17:57:21,038:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4775 (0.9421)
2022-11-09 17:57:21,197:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7169 (0.8988)
2022-11-09 17:57:21,355:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4050 (0.8094)
2022-11-09 17:57:21,512:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6789 (0.7921)
2022-11-09 17:57:21,648:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5612 (0.7670)
2022-11-09 17:57:22,042:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5501 (0.5501)
2022-11-09 17:57:22,201:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3998 (0.4788)
2022-11-09 17:57:22,364:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3684 (0.4421)
2022-11-09 17:57:22,526:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4144 (0.4355)
2022-11-09 17:57:22,689:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4846 (0.4457)
2022-11-09 17:57:22,852:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4361 (0.4442)
2022-11-09 17:57:23,013:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4192 (0.4408)
2022-11-09 17:57:23,170:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4757 (0.4452)
2022-11-09 17:57:23,332:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4114 (0.4417)
2022-11-09 17:57:23,490:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5311 (0.4505)
2022-11-09 17:57:23,651:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4537 (0.4508)
2022-11-09 17:57:23,889:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4141 (0.4481)
2022-11-09 17:57:24,048:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4280 (0.4464)
2022-11-09 17:57:24,210:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3993 (0.4429)
2022-11-09 17:57:24,371:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4097 (0.4408)
2022-11-09 17:57:24,533:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5391 (0.4473)
2022-11-09 17:57:24,693:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3888 (0.4438)
2022-11-09 17:57:24,836:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3521 (0.4398)
2022-11-09 17:57:24,893:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_215.pth.tar
2022-11-09 17:57:24,893:INFO: 
===> EPOCH: 216 (P2)
2022-11-09 17:57:24,893:INFO: - Computing loss (training)
2022-11-09 17:57:25,227:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5707 (0.5707)
2022-11-09 17:57:25,392:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9058 (0.7406)
2022-11-09 17:57:25,553:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9368 (0.8020)
2022-11-09 17:57:25,655:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4499 (0.7421)
2022-11-09 17:57:26,052:INFO: Dataset: univ                Batch:  1/15	Loss 0.5776 (0.5776)
2022-11-09 17:57:26,216:INFO: Dataset: univ                Batch:  2/15	Loss 0.4815 (0.5286)
2022-11-09 17:57:26,382:INFO: Dataset: univ                Batch:  3/15	Loss 0.5446 (0.5338)
2022-11-09 17:57:26,547:INFO: Dataset: univ                Batch:  4/15	Loss 0.4524 (0.5149)
2022-11-09 17:57:26,710:INFO: Dataset: univ                Batch:  5/15	Loss 0.4529 (0.5024)
2022-11-09 17:57:26,874:INFO: Dataset: univ                Batch:  6/15	Loss 0.3706 (0.4799)
2022-11-09 17:57:27,038:INFO: Dataset: univ                Batch:  7/15	Loss 0.4115 (0.4696)
2022-11-09 17:57:27,198:INFO: Dataset: univ                Batch:  8/15	Loss 1.5589 (0.5786)
2022-11-09 17:57:27,362:INFO: Dataset: univ                Batch:  9/15	Loss 0.4383 (0.5641)
2022-11-09 17:57:27,524:INFO: Dataset: univ                Batch: 10/15	Loss 0.3511 (0.5404)
2022-11-09 17:57:27,690:INFO: Dataset: univ                Batch: 11/15	Loss 0.3701 (0.5249)
2022-11-09 17:57:27,855:INFO: Dataset: univ                Batch: 12/15	Loss 0.4132 (0.5138)
2022-11-09 17:57:28,019:INFO: Dataset: univ                Batch: 13/15	Loss 0.3776 (0.5031)
2022-11-09 17:57:28,182:INFO: Dataset: univ                Batch: 14/15	Loss 0.7265 (0.5186)
2022-11-09 17:57:28,224:INFO: Dataset: univ                Batch: 15/15	Loss 0.0620 (0.5138)
2022-11-09 17:57:28,628:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5976 (0.5976)
2022-11-09 17:57:28,789:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5151 (0.5589)
2022-11-09 17:57:28,949:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5401 (0.5519)
2022-11-09 17:57:29,110:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5835 (0.5607)
2022-11-09 17:57:29,274:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0876 (0.6828)
2022-11-09 17:57:29,435:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4225 (0.6388)
2022-11-09 17:57:29,597:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4305 (0.6072)
2022-11-09 17:57:29,734:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3759 (0.5838)
2022-11-09 17:57:30,124:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5581 (0.5581)
2022-11-09 17:57:30,286:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3960 (0.4799)
2022-11-09 17:57:30,451:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6852 (0.5545)
2022-11-09 17:57:30,615:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7004 (0.5915)
2022-11-09 17:57:30,777:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4687 (0.5649)
2022-11-09 17:57:30,939:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5128 (0.5567)
2022-11-09 17:57:31,100:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5102 (0.5507)
2022-11-09 17:57:31,259:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5962 (0.5564)
2022-11-09 17:57:31,420:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3513 (0.5326)
2022-11-09 17:57:31,577:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4299 (0.5225)
2022-11-09 17:57:31,739:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4186 (0.5128)
2022-11-09 17:57:31,896:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4882 (0.5106)
2022-11-09 17:57:32,056:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4816 (0.5083)
2022-11-09 17:57:32,214:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9082 (0.5393)
2022-11-09 17:57:32,375:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3917 (0.5295)
2022-11-09 17:57:32,537:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4573 (0.5253)
2022-11-09 17:57:32,701:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5427 (0.5263)
2022-11-09 17:57:32,843:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3874 (0.5190)
2022-11-09 17:57:32,897:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_216.pth.tar
2022-11-09 17:57:32,897:INFO: 
===> EPOCH: 217 (P2)
2022-11-09 17:57:32,898:INFO: - Computing loss (training)
2022-11-09 17:57:33,231:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0749 (2.0749)
2022-11-09 17:57:33,393:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7148 (1.8931)
2022-11-09 17:57:33,553:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7949 (1.5188)
2022-11-09 17:57:33,653:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6422 (1.3615)
2022-11-09 17:57:34,145:INFO: Dataset: univ                Batch:  1/15	Loss 0.3949 (0.3949)
2022-11-09 17:57:34,309:INFO: Dataset: univ                Batch:  2/15	Loss 0.3915 (0.3932)
2022-11-09 17:57:34,474:INFO: Dataset: univ                Batch:  3/15	Loss 0.3652 (0.3831)
2022-11-09 17:57:34,635:INFO: Dataset: univ                Batch:  4/15	Loss 0.4365 (0.3957)
2022-11-09 17:57:34,799:INFO: Dataset: univ                Batch:  5/15	Loss 0.4856 (0.4134)
2022-11-09 17:57:34,963:INFO: Dataset: univ                Batch:  6/15	Loss 0.4096 (0.4128)
2022-11-09 17:57:35,127:INFO: Dataset: univ                Batch:  7/15	Loss 0.4153 (0.4131)
2022-11-09 17:57:35,289:INFO: Dataset: univ                Batch:  8/15	Loss 0.4586 (0.4187)
2022-11-09 17:57:35,453:INFO: Dataset: univ                Batch:  9/15	Loss 0.3876 (0.4150)
2022-11-09 17:57:35,615:INFO: Dataset: univ                Batch: 10/15	Loss 0.5669 (0.4302)
2022-11-09 17:57:35,782:INFO: Dataset: univ                Batch: 11/15	Loss 0.4536 (0.4322)
2022-11-09 17:57:35,944:INFO: Dataset: univ                Batch: 12/15	Loss 0.4067 (0.4303)
2022-11-09 17:57:36,112:INFO: Dataset: univ                Batch: 13/15	Loss 0.4075 (0.4284)
2022-11-09 17:57:36,275:INFO: Dataset: univ                Batch: 14/15	Loss 0.5204 (0.4344)
2022-11-09 17:57:36,318:INFO: Dataset: univ                Batch: 15/15	Loss 0.0720 (0.4291)
2022-11-09 17:57:36,706:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4637 (0.4637)
2022-11-09 17:57:36,872:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3152 (0.9162)
2022-11-09 17:57:37,033:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1634 (1.0103)
2022-11-09 17:57:37,189:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5769 (0.9069)
2022-11-09 17:57:37,351:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5174 (0.8293)
2022-11-09 17:57:37,510:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5737 (0.7895)
2022-11-09 17:57:37,671:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1633 (0.8412)
2022-11-09 17:57:37,809:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4328 (0.9107)
2022-11-09 17:57:38,210:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5360 (0.5360)
2022-11-09 17:57:38,373:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4578 (0.4947)
2022-11-09 17:57:38,538:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4527 (0.4804)
2022-11-09 17:57:38,697:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4675 (0.4771)
2022-11-09 17:57:38,859:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3991 (0.4617)
2022-11-09 17:57:39,022:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4588 (0.4613)
2022-11-09 17:57:39,183:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5210 (0.4698)
2022-11-09 17:57:39,342:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4432 (0.4667)
2022-11-09 17:57:39,502:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4724 (0.4673)
2022-11-09 17:57:39,660:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4814 (0.4687)
2022-11-09 17:57:39,821:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4902 (0.4705)
2022-11-09 17:57:39,981:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5534 (0.4774)
2022-11-09 17:57:40,142:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4823 (0.4778)
2022-11-09 17:57:40,302:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3929 (0.4720)
2022-11-09 17:57:40,465:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5054 (0.4742)
2022-11-09 17:57:40,625:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5446 (0.4786)
2022-11-09 17:57:40,788:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5021 (0.4801)
2022-11-09 17:57:40,928:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4789 (0.4801)
2022-11-09 17:57:40,982:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_217.pth.tar
2022-11-09 17:57:40,982:INFO: 
===> EPOCH: 218 (P2)
2022-11-09 17:57:40,982:INFO: - Computing loss (training)
2022-11-09 17:57:41,330:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9706 (1.9706)
2022-11-09 17:57:41,494:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2211 (1.6114)
2022-11-09 17:57:41,653:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0056 (1.4164)
2022-11-09 17:57:41,751:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6177 (1.2921)
2022-11-09 17:57:42,151:INFO: Dataset: univ                Batch:  1/15	Loss 0.5765 (0.5765)
2022-11-09 17:57:42,322:INFO: Dataset: univ                Batch:  2/15	Loss 0.4553 (0.5138)
2022-11-09 17:57:42,486:INFO: Dataset: univ                Batch:  3/15	Loss 0.3912 (0.4679)
2022-11-09 17:57:42,647:INFO: Dataset: univ                Batch:  4/15	Loss 0.4170 (0.4556)
2022-11-09 17:57:42,813:INFO: Dataset: univ                Batch:  5/15	Loss 0.4119 (0.4461)
2022-11-09 17:57:42,979:INFO: Dataset: univ                Batch:  6/15	Loss 0.4490 (0.4466)
2022-11-09 17:57:43,143:INFO: Dataset: univ                Batch:  7/15	Loss 0.3876 (0.4372)
2022-11-09 17:57:43,305:INFO: Dataset: univ                Batch:  8/15	Loss 0.4246 (0.4356)
2022-11-09 17:57:43,469:INFO: Dataset: univ                Batch:  9/15	Loss 0.3908 (0.4304)
2022-11-09 17:57:43,631:INFO: Dataset: univ                Batch: 10/15	Loss 0.4073 (0.4281)
2022-11-09 17:57:43,796:INFO: Dataset: univ                Batch: 11/15	Loss 0.3962 (0.4249)
2022-11-09 17:57:43,960:INFO: Dataset: univ                Batch: 12/15	Loss 0.4510 (0.4271)
2022-11-09 17:57:44,125:INFO: Dataset: univ                Batch: 13/15	Loss 0.3768 (0.4231)
2022-11-09 17:57:44,291:INFO: Dataset: univ                Batch: 14/15	Loss 0.4044 (0.4218)
2022-11-09 17:57:44,335:INFO: Dataset: univ                Batch: 15/15	Loss 0.0734 (0.4165)
2022-11-09 17:57:44,727:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5260 (0.5260)
2022-11-09 17:57:44,888:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5946 (0.5633)
2022-11-09 17:57:45,048:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6139 (0.5791)
2022-11-09 17:57:45,205:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4625 (0.5476)
2022-11-09 17:57:45,366:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5018 (0.5384)
2022-11-09 17:57:45,525:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4478 (0.5232)
2022-11-09 17:57:45,685:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7017 (0.5486)
2022-11-09 17:57:45,822:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5708 (0.5512)
2022-11-09 17:57:46,215:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4068 (0.4068)
2022-11-09 17:57:46,381:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4958 (0.4501)
2022-11-09 17:57:46,544:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4523 (0.4509)
2022-11-09 17:57:46,702:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3929 (0.4364)
2022-11-09 17:57:46,866:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4783 (0.4446)
2022-11-09 17:57:47,028:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3862 (0.4347)
2022-11-09 17:57:47,191:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7180 (0.4722)
2022-11-09 17:57:47,350:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5754 (0.4844)
2022-11-09 17:57:47,513:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4072 (0.4766)
2022-11-09 17:57:47,671:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3955 (0.4681)
2022-11-09 17:57:47,835:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3908 (0.4614)
2022-11-09 17:57:47,995:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3583 (0.4531)
2022-11-09 17:57:48,155:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4586 (0.4536)
2022-11-09 17:57:48,315:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5135 (0.4579)
2022-11-09 17:57:48,478:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3502 (0.4501)
2022-11-09 17:57:48,637:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3696 (0.4452)
2022-11-09 17:57:48,799:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3783 (0.4410)
2022-11-09 17:57:48,939:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3500 (0.4369)
2022-11-09 17:57:48,993:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_218.pth.tar
2022-11-09 17:57:48,993:INFO: 
===> EPOCH: 219 (P2)
2022-11-09 17:57:48,994:INFO: - Computing loss (training)
2022-11-09 17:57:49,335:INFO: Dataset: hotel               Batch: 1/4	Loss 4.7968 (4.7968)
2022-11-09 17:57:49,499:INFO: Dataset: hotel               Batch: 2/4	Loss 5.2556 (5.0319)
2022-11-09 17:57:49,659:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7075 (3.8633)
2022-11-09 17:57:49,758:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7115 (3.3019)
2022-11-09 17:57:50,151:INFO: Dataset: univ                Batch:  1/15	Loss 0.3910 (0.3910)
2022-11-09 17:57:50,321:INFO: Dataset: univ                Batch:  2/15	Loss 0.5615 (0.4763)
2022-11-09 17:57:50,486:INFO: Dataset: univ                Batch:  3/15	Loss 0.4391 (0.4646)
2022-11-09 17:57:50,647:INFO: Dataset: univ                Batch:  4/15	Loss 1.0532 (0.6156)
2022-11-09 17:57:50,814:INFO: Dataset: univ                Batch:  5/15	Loss 0.6409 (0.6205)
2022-11-09 17:57:50,978:INFO: Dataset: univ                Batch:  6/15	Loss 0.4567 (0.5933)
2022-11-09 17:57:51,143:INFO: Dataset: univ                Batch:  7/15	Loss 0.9081 (0.6392)
2022-11-09 17:57:51,305:INFO: Dataset: univ                Batch:  8/15	Loss 0.8982 (0.6693)
2022-11-09 17:57:51,470:INFO: Dataset: univ                Batch:  9/15	Loss 0.6894 (0.6717)
2022-11-09 17:57:51,633:INFO: Dataset: univ                Batch: 10/15	Loss 0.4085 (0.6459)
2022-11-09 17:57:51,798:INFO: Dataset: univ                Batch: 11/15	Loss 0.4591 (0.6291)
2022-11-09 17:57:51,963:INFO: Dataset: univ                Batch: 12/15	Loss 0.4292 (0.6119)
2022-11-09 17:57:52,127:INFO: Dataset: univ                Batch: 13/15	Loss 1.6701 (0.6872)
2022-11-09 17:57:52,291:INFO: Dataset: univ                Batch: 14/15	Loss 0.6607 (0.6853)
2022-11-09 17:57:52,334:INFO: Dataset: univ                Batch: 15/15	Loss 0.1029 (0.6780)
2022-11-09 17:57:52,733:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7457 (0.7457)
2022-11-09 17:57:52,898:INFO: Dataset: zara1               Batch: 2/8	Loss 2.2999 (1.4471)
2022-11-09 17:57:53,060:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6008 (1.1631)
2022-11-09 17:57:53,217:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4889 (1.0060)
2022-11-09 17:57:53,378:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5523 (0.9230)
2022-11-09 17:57:53,537:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9062 (0.9203)
2022-11-09 17:57:53,697:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6962 (0.8877)
2022-11-09 17:57:53,834:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4941 (0.8430)
2022-11-09 17:57:54,225:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6668 (0.6668)
2022-11-09 17:57:54,389:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7327 (0.6997)
2022-11-09 17:57:54,553:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6810 (0.6936)
2022-11-09 17:57:54,710:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4967 (0.6412)
2022-11-09 17:57:54,872:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3454 (0.5822)
2022-11-09 17:57:55,032:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4185 (0.5556)
2022-11-09 17:57:55,192:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6508 (0.5673)
2022-11-09 17:57:55,350:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5129 (0.5602)
2022-11-09 17:57:55,511:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4360 (0.5459)
2022-11-09 17:57:55,669:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5649 (0.5478)
2022-11-09 17:57:55,832:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4225 (0.5367)
2022-11-09 17:57:55,990:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4284 (0.5273)
2022-11-09 17:57:56,150:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5281 (0.5274)
2022-11-09 17:57:56,310:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3926 (0.5182)
2022-11-09 17:57:56,472:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5297 (0.5191)
2022-11-09 17:57:56,631:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4594 (0.5151)
2022-11-09 17:57:56,792:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3774 (0.5071)
2022-11-09 17:57:56,932:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3145 (0.4990)
2022-11-09 17:57:56,990:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_219.pth.tar
2022-11-09 17:57:56,990:INFO: 
===> EPOCH: 220 (P2)
2022-11-09 17:57:56,991:INFO: - Computing loss (training)
2022-11-09 17:57:57,335:INFO: Dataset: hotel               Batch: 1/4	Loss 2.8026 (2.8026)
2022-11-09 17:57:57,498:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1257 (1.9274)
2022-11-09 17:57:57,656:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2707 (1.6960)
2022-11-09 17:57:57,754:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4877 (1.4968)
2022-11-09 17:57:58,210:INFO: Dataset: univ                Batch:  1/15	Loss 0.7361 (0.7361)
2022-11-09 17:57:58,389:INFO: Dataset: univ                Batch:  2/15	Loss 0.3861 (0.5610)
2022-11-09 17:57:58,553:INFO: Dataset: univ                Batch:  3/15	Loss 0.5806 (0.5681)
2022-11-09 17:57:58,715:INFO: Dataset: univ                Batch:  4/15	Loss 0.3755 (0.5168)
2022-11-09 17:57:58,954:INFO: Dataset: univ                Batch:  5/15	Loss 1.0647 (0.6141)
2022-11-09 17:57:59,121:INFO: Dataset: univ                Batch:  6/15	Loss 0.4838 (0.5943)
2022-11-09 17:57:59,287:INFO: Dataset: univ                Batch:  7/15	Loss 0.4804 (0.5772)
2022-11-09 17:57:59,449:INFO: Dataset: univ                Batch:  8/15	Loss 0.4300 (0.5599)
2022-11-09 17:57:59,613:INFO: Dataset: univ                Batch:  9/15	Loss 0.6182 (0.5659)
2022-11-09 17:57:59,775:INFO: Dataset: univ                Batch: 10/15	Loss 0.3913 (0.5471)
2022-11-09 17:57:59,940:INFO: Dataset: univ                Batch: 11/15	Loss 0.7575 (0.5643)
2022-11-09 17:58:00,104:INFO: Dataset: univ                Batch: 12/15	Loss 0.4381 (0.5538)
2022-11-09 17:58:00,269:INFO: Dataset: univ                Batch: 13/15	Loss 0.4490 (0.5457)
2022-11-09 17:58:00,434:INFO: Dataset: univ                Batch: 14/15	Loss 0.4811 (0.5407)
2022-11-09 17:58:00,476:INFO: Dataset: univ                Batch: 15/15	Loss 0.1069 (0.5329)
2022-11-09 17:58:00,871:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5251 (0.5251)
2022-11-09 17:58:01,030:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5316 (0.5283)
2022-11-09 17:58:01,189:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5992 (0.5521)
2022-11-09 17:58:01,345:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5123 (0.5421)
2022-11-09 17:58:01,504:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5887 (0.5504)
2022-11-09 17:58:01,663:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4187 (0.5269)
2022-11-09 17:58:01,822:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4968 (0.5226)
2022-11-09 17:58:01,958:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5418 (0.5246)
2022-11-09 17:58:02,353:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4074 (0.4074)
2022-11-09 17:58:02,518:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4206 (0.4135)
2022-11-09 17:58:02,681:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5140 (0.4463)
2022-11-09 17:58:02,840:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7576 (0.5195)
2022-11-09 17:58:03,005:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5595 (0.5276)
2022-11-09 17:58:03,165:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4016 (0.5067)
2022-11-09 17:58:03,325:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4130 (0.4939)
2022-11-09 17:58:03,487:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4147 (0.4838)
2022-11-09 17:58:03,649:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3796 (0.4727)
2022-11-09 17:58:03,807:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4156 (0.4674)
2022-11-09 17:58:03,969:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6965 (0.4863)
2022-11-09 17:58:04,127:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4568 (0.4837)
2022-11-09 17:58:04,287:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3899 (0.4768)
2022-11-09 17:58:04,447:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3298 (0.4661)
2022-11-09 17:58:04,609:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3635 (0.4594)
2022-11-09 17:58:04,768:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3939 (0.4556)
2022-11-09 17:58:04,930:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3757 (0.4510)
2022-11-09 17:58:05,068:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4954 (0.4531)
2022-11-09 17:58:05,123:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_220.pth.tar
2022-11-09 17:58:05,124:INFO: 
===> EPOCH: 221 (P2)
2022-11-09 17:58:05,124:INFO: - Computing loss (training)
2022-11-09 17:58:05,470:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9212 (9.9212)
2022-11-09 17:58:05,633:INFO: Dataset: hotel               Batch: 2/4	Loss 2.7417 (6.2700)
2022-11-09 17:58:05,792:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9439 (4.7042)
2022-11-09 17:58:05,891:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4916 (4.0540)
2022-11-09 17:58:06,282:INFO: Dataset: univ                Batch:  1/15	Loss 0.4505 (0.4505)
2022-11-09 17:58:06,446:INFO: Dataset: univ                Batch:  2/15	Loss 0.4742 (0.4614)
2022-11-09 17:58:06,615:INFO: Dataset: univ                Batch:  3/15	Loss 0.4681 (0.4638)
2022-11-09 17:58:06,775:INFO: Dataset: univ                Batch:  4/15	Loss 0.9167 (0.5855)
2022-11-09 17:58:06,937:INFO: Dataset: univ                Batch:  5/15	Loss 0.6789 (0.6045)
2022-11-09 17:58:07,102:INFO: Dataset: univ                Batch:  6/15	Loss 0.7630 (0.6307)
2022-11-09 17:58:07,265:INFO: Dataset: univ                Batch:  7/15	Loss 0.7299 (0.6433)
2022-11-09 17:58:07,427:INFO: Dataset: univ                Batch:  8/15	Loss 0.7240 (0.6531)
2022-11-09 17:58:07,591:INFO: Dataset: univ                Batch:  9/15	Loss 0.8939 (0.6811)
2022-11-09 17:58:07,752:INFO: Dataset: univ                Batch: 10/15	Loss 0.4665 (0.6597)
2022-11-09 17:58:07,915:INFO: Dataset: univ                Batch: 11/15	Loss 0.5217 (0.6480)
2022-11-09 17:58:08,078:INFO: Dataset: univ                Batch: 12/15	Loss 0.4748 (0.6335)
2022-11-09 17:58:08,240:INFO: Dataset: univ                Batch: 13/15	Loss 0.6554 (0.6349)
2022-11-09 17:58:08,405:INFO: Dataset: univ                Batch: 14/15	Loss 0.7307 (0.6419)
2022-11-09 17:58:08,447:INFO: Dataset: univ                Batch: 15/15	Loss 0.0967 (0.6363)
2022-11-09 17:58:08,841:INFO: Dataset: zara1               Batch: 1/8	Loss 2.4243 (2.4243)
2022-11-09 17:58:09,004:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7399 (2.0821)
2022-11-09 17:58:09,165:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6900 (1.6235)
2022-11-09 17:58:09,323:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6606 (1.3596)
2022-11-09 17:58:09,486:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5951 (1.1902)
2022-11-09 17:58:09,644:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8053 (1.1309)
2022-11-09 17:58:09,804:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5249 (1.1944)
2022-11-09 17:58:09,942:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0397 (1.1791)
2022-11-09 17:58:10,401:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5329 (0.5329)
2022-11-09 17:58:10,559:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9337 (0.7349)
2022-11-09 17:58:10,719:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3868 (0.6132)
2022-11-09 17:58:10,879:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4958 (0.5811)
2022-11-09 17:58:11,039:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4560 (0.5551)
2022-11-09 17:58:11,198:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8056 (0.5987)
2022-11-09 17:58:11,358:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9284 (0.6462)
2022-11-09 17:58:11,515:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1153 (0.7092)
2022-11-09 17:58:11,675:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7102 (0.7093)
2022-11-09 17:58:11,833:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5807 (0.6957)
2022-11-09 17:58:11,994:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4526 (0.6745)
2022-11-09 17:58:12,151:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4114 (0.6545)
2022-11-09 17:58:12,312:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5703 (0.6486)
2022-11-09 17:58:12,472:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3742 (0.6267)
2022-11-09 17:58:12,634:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4458 (0.6136)
2022-11-09 17:58:12,793:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3767 (0.5982)
2022-11-09 17:58:12,954:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5787 (0.5972)
2022-11-09 17:58:13,092:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3321 (0.5849)
2022-11-09 17:58:13,148:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_221.pth.tar
2022-11-09 17:58:13,149:INFO: 
===> EPOCH: 222 (P2)
2022-11-09 17:58:13,149:INFO: - Computing loss (training)
2022-11-09 17:58:13,491:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0461 (2.0461)
2022-11-09 17:58:13,654:INFO: Dataset: hotel               Batch: 2/4	Loss 2.1681 (2.1071)
2022-11-09 17:58:13,815:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9434 (1.7179)
2022-11-09 17:58:13,916:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4001 (1.4971)
2022-11-09 17:58:14,312:INFO: Dataset: univ                Batch:  1/15	Loss 0.7717 (0.7717)
2022-11-09 17:58:14,483:INFO: Dataset: univ                Batch:  2/15	Loss 0.3588 (0.5610)
2022-11-09 17:58:14,649:INFO: Dataset: univ                Batch:  3/15	Loss 0.4063 (0.5083)
2022-11-09 17:58:14,811:INFO: Dataset: univ                Batch:  4/15	Loss 0.3712 (0.4722)
2022-11-09 17:58:14,974:INFO: Dataset: univ                Batch:  5/15	Loss 0.5146 (0.4805)
2022-11-09 17:58:15,139:INFO: Dataset: univ                Batch:  6/15	Loss 0.3816 (0.4628)
2022-11-09 17:58:15,302:INFO: Dataset: univ                Batch:  7/15	Loss 0.7246 (0.4969)
2022-11-09 17:58:15,466:INFO: Dataset: univ                Batch:  8/15	Loss 0.3979 (0.4831)
2022-11-09 17:58:15,630:INFO: Dataset: univ                Batch:  9/15	Loss 0.3912 (0.4730)
2022-11-09 17:58:15,791:INFO: Dataset: univ                Batch: 10/15	Loss 0.6056 (0.4860)
2022-11-09 17:58:15,956:INFO: Dataset: univ                Batch: 11/15	Loss 0.9174 (0.5236)
2022-11-09 17:58:16,120:INFO: Dataset: univ                Batch: 12/15	Loss 0.3805 (0.5104)
2022-11-09 17:58:16,284:INFO: Dataset: univ                Batch: 13/15	Loss 0.4333 (0.5047)
2022-11-09 17:58:16,450:INFO: Dataset: univ                Batch: 14/15	Loss 0.5040 (0.5047)
2022-11-09 17:58:16,492:INFO: Dataset: univ                Batch: 15/15	Loss 0.0725 (0.4982)
2022-11-09 17:58:16,891:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4558 (0.4558)
2022-11-09 17:58:17,053:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5993 (0.5311)
2022-11-09 17:58:17,215:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5305 (0.5309)
2022-11-09 17:58:17,372:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4664 (0.5163)
2022-11-09 17:58:17,532:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6839 (0.5486)
2022-11-09 17:58:17,689:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4563 (0.5330)
2022-11-09 17:58:17,849:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5929 (0.5414)
2022-11-09 17:58:17,985:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3977 (0.5233)
2022-11-09 17:58:18,373:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3887 (0.3887)
2022-11-09 17:58:18,532:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4230 (0.4053)
2022-11-09 17:58:18,696:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6026 (0.4658)
2022-11-09 17:58:18,857:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3681 (0.4393)
2022-11-09 17:58:19,020:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3982 (0.4315)
2022-11-09 17:58:19,181:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3644 (0.4205)
2022-11-09 17:58:19,342:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3737 (0.4139)
2022-11-09 17:58:19,500:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3798 (0.4095)
2022-11-09 17:58:19,662:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4992 (0.4185)
2022-11-09 17:58:19,819:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3706 (0.4140)
2022-11-09 17:58:19,981:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3698 (0.4097)
2022-11-09 17:58:20,140:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3678 (0.4063)
2022-11-09 17:58:20,301:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3748 (0.4038)
2022-11-09 17:58:20,463:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4292 (0.4055)
2022-11-09 17:58:20,625:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4438 (0.4080)
2022-11-09 17:58:20,784:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5424 (0.4163)
2022-11-09 17:58:20,947:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4455 (0.4180)
2022-11-09 17:58:21,085:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3179 (0.4130)
2022-11-09 17:58:21,139:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_222.pth.tar
2022-11-09 17:58:21,139:INFO: 
===> EPOCH: 223 (P2)
2022-11-09 17:58:21,139:INFO: - Computing loss (training)
2022-11-09 17:58:21,482:INFO: Dataset: hotel               Batch: 1/4	Loss 4.3109 (4.3109)
2022-11-09 17:58:21,646:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0847 (2.7891)
2022-11-09 17:58:21,806:INFO: Dataset: hotel               Batch: 3/4	Loss 3.1926 (2.9244)
2022-11-09 17:58:21,905:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6637 (2.7249)
2022-11-09 17:58:22,342:INFO: Dataset: univ                Batch:  1/15	Loss 0.3677 (0.3677)
2022-11-09 17:58:22,508:INFO: Dataset: univ                Batch:  2/15	Loss 0.4125 (0.3901)
2022-11-09 17:58:22,677:INFO: Dataset: univ                Batch:  3/15	Loss 0.3960 (0.3920)
2022-11-09 17:58:22,842:INFO: Dataset: univ                Batch:  4/15	Loss 0.4317 (0.4024)
2022-11-09 17:58:23,009:INFO: Dataset: univ                Batch:  5/15	Loss 0.4930 (0.4209)
2022-11-09 17:58:23,177:INFO: Dataset: univ                Batch:  6/15	Loss 0.4595 (0.4273)
2022-11-09 17:58:23,344:INFO: Dataset: univ                Batch:  7/15	Loss 0.5024 (0.4368)
2022-11-09 17:58:23,509:INFO: Dataset: univ                Batch:  8/15	Loss 0.5231 (0.4472)
2022-11-09 17:58:23,676:INFO: Dataset: univ                Batch:  9/15	Loss 0.5717 (0.4614)
2022-11-09 17:58:23,839:INFO: Dataset: univ                Batch: 10/15	Loss 0.4767 (0.4627)
2022-11-09 17:58:24,006:INFO: Dataset: univ                Batch: 11/15	Loss 0.7342 (0.4866)
2022-11-09 17:58:24,172:INFO: Dataset: univ                Batch: 12/15	Loss 0.4746 (0.4856)
2022-11-09 17:58:24,338:INFO: Dataset: univ                Batch: 13/15	Loss 0.4325 (0.4814)
2022-11-09 17:58:24,505:INFO: Dataset: univ                Batch: 14/15	Loss 0.4901 (0.4821)
2022-11-09 17:58:24,548:INFO: Dataset: univ                Batch: 15/15	Loss 0.0951 (0.4777)
2022-11-09 17:58:24,946:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0952 (2.0952)
2022-11-09 17:58:25,109:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9241 (1.4797)
2022-11-09 17:58:25,267:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6412 (1.2153)
2022-11-09 17:58:25,425:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5027 (1.0461)
2022-11-09 17:58:25,587:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4296 (0.9303)
2022-11-09 17:58:25,744:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2819 (0.9918)
2022-11-09 17:58:25,904:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0552 (1.0011)
2022-11-09 17:58:26,039:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5478 (1.0584)
2022-11-09 17:58:26,443:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4908 (0.4908)
2022-11-09 17:58:26,602:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3733 (0.4305)
2022-11-09 17:58:26,762:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3596 (0.4057)
2022-11-09 17:58:26,920:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4409 (0.4145)
2022-11-09 17:58:27,083:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3645 (0.4057)
2022-11-09 17:58:27,241:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5047 (0.4237)
2022-11-09 17:58:27,401:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4439 (0.4268)
2022-11-09 17:58:27,559:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3805 (0.4213)
2022-11-09 17:58:27,719:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6763 (0.4494)
2022-11-09 17:58:27,878:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5608 (0.4611)
2022-11-09 17:58:28,039:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4594 (0.4609)
2022-11-09 17:58:28,197:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4225 (0.4577)
2022-11-09 17:58:28,358:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4998 (0.4608)
2022-11-09 17:58:28,516:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5591 (0.4677)
2022-11-09 17:58:28,681:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4373 (0.4657)
2022-11-09 17:58:28,840:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5984 (0.4738)
2022-11-09 17:58:29,002:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4306 (0.4714)
2022-11-09 17:58:29,140:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4309 (0.4695)
2022-11-09 17:58:29,194:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_223.pth.tar
2022-11-09 17:58:29,194:INFO: 
===> EPOCH: 224 (P2)
2022-11-09 17:58:29,195:INFO: - Computing loss (training)
2022-11-09 17:58:29,538:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9682 (0.9682)
2022-11-09 17:58:29,700:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9440 (0.9558)
2022-11-09 17:58:29,858:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5798 (0.8235)
2022-11-09 17:58:29,958:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4470 (0.7589)
2022-11-09 17:58:30,391:INFO: Dataset: univ                Batch:  1/15	Loss 0.4610 (0.4610)
2022-11-09 17:58:30,556:INFO: Dataset: univ                Batch:  2/15	Loss 0.6645 (0.5646)
2022-11-09 17:58:30,719:INFO: Dataset: univ                Batch:  3/15	Loss 0.4806 (0.5351)
2022-11-09 17:58:30,881:INFO: Dataset: univ                Batch:  4/15	Loss 0.3957 (0.4982)
2022-11-09 17:58:31,045:INFO: Dataset: univ                Batch:  5/15	Loss 0.3531 (0.4676)
2022-11-09 17:58:31,210:INFO: Dataset: univ                Batch:  6/15	Loss 0.3810 (0.4529)
2022-11-09 17:58:31,375:INFO: Dataset: univ                Batch:  7/15	Loss 0.3946 (0.4447)
2022-11-09 17:58:31,537:INFO: Dataset: univ                Batch:  8/15	Loss 0.4062 (0.4399)
2022-11-09 17:58:31,701:INFO: Dataset: univ                Batch:  9/15	Loss 0.4356 (0.4394)
2022-11-09 17:58:31,864:INFO: Dataset: univ                Batch: 10/15	Loss 0.4575 (0.4412)
2022-11-09 17:58:32,029:INFO: Dataset: univ                Batch: 11/15	Loss 0.4823 (0.4448)
2022-11-09 17:58:32,192:INFO: Dataset: univ                Batch: 12/15	Loss 0.3467 (0.4363)
2022-11-09 17:58:32,358:INFO: Dataset: univ                Batch: 13/15	Loss 0.3514 (0.4301)
2022-11-09 17:58:32,524:INFO: Dataset: univ                Batch: 14/15	Loss 0.3513 (0.4249)
2022-11-09 17:58:32,567:INFO: Dataset: univ                Batch: 15/15	Loss 0.0671 (0.4189)
2022-11-09 17:58:32,963:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6013 (0.6013)
2022-11-09 17:58:33,129:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4440 (0.5159)
2022-11-09 17:58:33,289:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3860 (0.4701)
2022-11-09 17:58:33,448:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4310 (0.4612)
2022-11-09 17:58:33,608:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5313 (0.4752)
2022-11-09 17:58:33,768:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6956 (0.5150)
2022-11-09 17:58:33,929:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4518 (0.5071)
2022-11-09 17:58:34,066:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3280 (0.4878)
2022-11-09 17:58:34,467:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3323 (0.3323)
2022-11-09 17:58:34,628:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3315 (0.3319)
2022-11-09 17:58:34,789:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4442 (0.3680)
2022-11-09 17:58:34,950:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3965 (0.3745)
2022-11-09 17:58:35,111:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3690 (0.3733)
2022-11-09 17:58:35,272:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3957 (0.3772)
2022-11-09 17:58:35,433:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5627 (0.4036)
2022-11-09 17:58:35,591:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3314 (0.3955)
2022-11-09 17:58:35,827:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3745 (0.3934)
2022-11-09 17:58:35,986:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4024 (0.3942)
2022-11-09 17:58:36,148:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3447 (0.3903)
2022-11-09 17:58:36,305:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3568 (0.3874)
2022-11-09 17:58:36,465:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3748 (0.3864)
2022-11-09 17:58:36,624:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3814 (0.3861)
2022-11-09 17:58:36,786:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3408 (0.3831)
2022-11-09 17:58:36,946:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3560 (0.3814)
2022-11-09 17:58:37,108:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4089 (0.3830)
2022-11-09 17:58:37,247:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2968 (0.3790)
2022-11-09 17:58:37,300:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_224.pth.tar
2022-11-09 17:58:37,300:INFO: 
===> EPOCH: 225 (P2)
2022-11-09 17:58:37,301:INFO: - Computing loss (training)
2022-11-09 17:58:37,646:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0493 (3.0493)
2022-11-09 17:58:37,808:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0121 (2.0062)
2022-11-09 17:58:37,967:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1519 (1.7081)
2022-11-09 17:58:38,067:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3305 (1.4918)
2022-11-09 17:58:38,471:INFO: Dataset: univ                Batch:  1/15	Loss 0.4000 (0.4000)
2022-11-09 17:58:38,642:INFO: Dataset: univ                Batch:  2/15	Loss 0.5321 (0.4684)
2022-11-09 17:58:38,806:INFO: Dataset: univ                Batch:  3/15	Loss 0.4230 (0.4532)
2022-11-09 17:58:38,968:INFO: Dataset: univ                Batch:  4/15	Loss 0.4979 (0.4640)
2022-11-09 17:58:39,133:INFO: Dataset: univ                Batch:  5/15	Loss 0.4186 (0.4548)
2022-11-09 17:58:39,298:INFO: Dataset: univ                Batch:  6/15	Loss 0.7783 (0.5146)
2022-11-09 17:58:39,462:INFO: Dataset: univ                Batch:  7/15	Loss 0.4237 (0.5015)
2022-11-09 17:58:39,624:INFO: Dataset: univ                Batch:  8/15	Loss 0.3781 (0.4874)
2022-11-09 17:58:39,789:INFO: Dataset: univ                Batch:  9/15	Loss 0.4370 (0.4820)
2022-11-09 17:58:39,952:INFO: Dataset: univ                Batch: 10/15	Loss 0.5202 (0.4860)
2022-11-09 17:58:40,118:INFO: Dataset: univ                Batch: 11/15	Loss 0.6957 (0.5029)
2022-11-09 17:58:40,279:INFO: Dataset: univ                Batch: 12/15	Loss 0.4457 (0.4983)
2022-11-09 17:58:40,446:INFO: Dataset: univ                Batch: 13/15	Loss 0.4710 (0.4960)
2022-11-09 17:58:40,610:INFO: Dataset: univ                Batch: 14/15	Loss 0.7948 (0.5175)
2022-11-09 17:58:40,652:INFO: Dataset: univ                Batch: 15/15	Loss 0.0580 (0.5112)
2022-11-09 17:58:41,058:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9079 (0.9079)
2022-11-09 17:58:41,221:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8155 (0.8666)
2022-11-09 17:58:41,385:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4463 (0.7324)
2022-11-09 17:58:41,546:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9108 (0.7776)
2022-11-09 17:58:41,709:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4836 (0.7240)
2022-11-09 17:58:41,870:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4272 (0.6728)
2022-11-09 17:58:42,032:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6911 (0.6756)
2022-11-09 17:58:42,170:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1680 (0.7220)
2022-11-09 17:58:42,565:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8567 (0.8567)
2022-11-09 17:58:42,725:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4485 (0.6510)
2022-11-09 17:58:42,886:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3805 (0.5560)
2022-11-09 17:58:43,046:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3832 (0.5137)
2022-11-09 17:58:43,209:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3916 (0.4889)
2022-11-09 17:58:43,369:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9322 (0.5605)
2022-11-09 17:58:43,529:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3733 (0.5331)
2022-11-09 17:58:43,686:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3474 (0.5089)
2022-11-09 17:58:43,846:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5086 (0.5089)
2022-11-09 17:58:44,003:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3480 (0.4926)
2022-11-09 17:58:44,163:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4672 (0.4901)
2022-11-09 17:58:44,324:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3327 (0.4764)
2022-11-09 17:58:44,485:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4993 (0.4779)
2022-11-09 17:58:44,643:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3619 (0.4686)
2022-11-09 17:58:44,803:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3522 (0.4607)
2022-11-09 17:58:44,964:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4107 (0.4574)
2022-11-09 17:58:45,125:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4005 (0.4539)
2022-11-09 17:58:45,263:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3386 (0.4491)
2022-11-09 17:58:45,317:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_225.pth.tar
2022-11-09 17:58:45,317:INFO: 
===> EPOCH: 226 (P2)
2022-11-09 17:58:45,317:INFO: - Computing loss (training)
2022-11-09 17:58:45,670:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1616 (1.1616)
2022-11-09 17:58:45,832:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7303 (0.9496)
2022-11-09 17:58:45,990:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0487 (0.9834)
2022-11-09 17:58:46,090:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3243 (0.8660)
2022-11-09 17:58:46,481:INFO: Dataset: univ                Batch:  1/15	Loss 0.7145 (0.7145)
2022-11-09 17:58:46,645:INFO: Dataset: univ                Batch:  2/15	Loss 0.4310 (0.5758)
2022-11-09 17:58:46,809:INFO: Dataset: univ                Batch:  3/15	Loss 0.3916 (0.5161)
2022-11-09 17:58:46,975:INFO: Dataset: univ                Batch:  4/15	Loss 0.4037 (0.4898)
2022-11-09 17:58:47,137:INFO: Dataset: univ                Batch:  5/15	Loss 0.3546 (0.4624)
2022-11-09 17:58:47,300:INFO: Dataset: univ                Batch:  6/15	Loss 0.3532 (0.4452)
2022-11-09 17:58:47,464:INFO: Dataset: univ                Batch:  7/15	Loss 0.3467 (0.4305)
2022-11-09 17:58:47,624:INFO: Dataset: univ                Batch:  8/15	Loss 0.3533 (0.4211)
2022-11-09 17:58:47,788:INFO: Dataset: univ                Batch:  9/15	Loss 0.7472 (0.4583)
2022-11-09 17:58:47,949:INFO: Dataset: univ                Batch: 10/15	Loss 0.9838 (0.5041)
2022-11-09 17:58:48,112:INFO: Dataset: univ                Batch: 11/15	Loss 0.4598 (0.5005)
2022-11-09 17:58:48,275:INFO: Dataset: univ                Batch: 12/15	Loss 0.3640 (0.4888)
2022-11-09 17:58:48,439:INFO: Dataset: univ                Batch: 13/15	Loss 0.3420 (0.4761)
2022-11-09 17:58:48,603:INFO: Dataset: univ                Batch: 14/15	Loss 0.5499 (0.4818)
2022-11-09 17:58:48,644:INFO: Dataset: univ                Batch: 15/15	Loss 0.0923 (0.4765)
2022-11-09 17:58:49,037:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5186 (0.5186)
2022-11-09 17:58:49,199:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4277 (0.4739)
2022-11-09 17:58:49,359:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4698 (0.4726)
2022-11-09 17:58:49,520:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5642 (0.4955)
2022-11-09 17:58:49,681:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3454 (0.4653)
2022-11-09 17:58:49,839:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7114 (0.5075)
2022-11-09 17:58:50,000:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4754 (0.5028)
2022-11-09 17:58:50,136:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4196 (0.4937)
2022-11-09 17:58:50,527:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4387 (0.4387)
2022-11-09 17:58:50,694:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4287 (0.4335)
2022-11-09 17:58:50,855:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3733 (0.4123)
2022-11-09 17:58:51,016:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4953 (0.4336)
2022-11-09 17:58:51,178:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4795 (0.4431)
2022-11-09 17:58:51,340:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4048 (0.4363)
2022-11-09 17:58:51,501:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4295 (0.4354)
2022-11-09 17:58:51,660:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4799 (0.4409)
2022-11-09 17:58:51,821:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4305 (0.4397)
2022-11-09 17:58:51,981:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3942 (0.4355)
2022-11-09 17:58:52,145:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3294 (0.4261)
2022-11-09 17:58:52,303:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5529 (0.4365)
2022-11-09 17:58:52,466:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5533 (0.4458)
2022-11-09 17:58:52,627:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5054 (0.4497)
2022-11-09 17:58:52,790:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3343 (0.4418)
2022-11-09 17:58:52,953:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4458 (0.4421)
2022-11-09 17:58:53,117:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3568 (0.4368)
2022-11-09 17:58:53,256:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2634 (0.4287)
2022-11-09 17:58:53,310:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_226.pth.tar
2022-11-09 17:58:53,310:INFO: 
===> EPOCH: 227 (P2)
2022-11-09 17:58:53,310:INFO: - Computing loss (training)
2022-11-09 17:58:53,663:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7458 (1.7458)
2022-11-09 17:58:53,824:INFO: Dataset: hotel               Batch: 2/4	Loss 2.6023 (2.2000)
2022-11-09 17:58:53,983:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8265 (1.7507)
2022-11-09 17:58:54,083:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4229 (1.5423)
2022-11-09 17:58:54,508:INFO: Dataset: univ                Batch:  1/15	Loss 0.3521 (0.3521)
2022-11-09 17:58:54,674:INFO: Dataset: univ                Batch:  2/15	Loss 0.4189 (0.3820)
2022-11-09 17:58:54,838:INFO: Dataset: univ                Batch:  3/15	Loss 0.5213 (0.4289)
2022-11-09 17:58:54,999:INFO: Dataset: univ                Batch:  4/15	Loss 0.3956 (0.4208)
2022-11-09 17:58:55,163:INFO: Dataset: univ                Batch:  5/15	Loss 0.4981 (0.4371)
2022-11-09 17:58:55,327:INFO: Dataset: univ                Batch:  6/15	Loss 0.5605 (0.4569)
2022-11-09 17:58:55,491:INFO: Dataset: univ                Batch:  7/15	Loss 0.4203 (0.4518)
2022-11-09 17:58:55,653:INFO: Dataset: univ                Batch:  8/15	Loss 0.5708 (0.4672)
2022-11-09 17:58:55,816:INFO: Dataset: univ                Batch:  9/15	Loss 0.4339 (0.4636)
2022-11-09 17:58:55,978:INFO: Dataset: univ                Batch: 10/15	Loss 0.3768 (0.4550)
2022-11-09 17:58:56,141:INFO: Dataset: univ                Batch: 11/15	Loss 0.3780 (0.4474)
2022-11-09 17:58:56,304:INFO: Dataset: univ                Batch: 12/15	Loss 0.3788 (0.4420)
2022-11-09 17:58:56,468:INFO: Dataset: univ                Batch: 13/15	Loss 0.6083 (0.4547)
2022-11-09 17:58:56,630:INFO: Dataset: univ                Batch: 14/15	Loss 0.4473 (0.4541)
2022-11-09 17:58:56,672:INFO: Dataset: univ                Batch: 15/15	Loss 0.0748 (0.4492)
2022-11-09 17:58:57,069:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1064 (1.1064)
2022-11-09 17:58:57,230:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2186 (1.1610)
2022-11-09 17:58:57,391:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5325 (0.9678)
2022-11-09 17:58:57,548:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4263 (0.8322)
2022-11-09 17:58:57,708:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3705 (0.7209)
2022-11-09 17:58:57,868:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5400 (0.6902)
2022-11-09 17:58:58,029:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7560 (0.6986)
2022-11-09 17:58:58,166:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5810 (0.6871)
2022-11-09 17:58:58,557:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4075 (0.4075)
2022-11-09 17:58:58,716:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5734 (0.4890)
2022-11-09 17:58:58,877:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5288 (0.5025)
2022-11-09 17:58:59,040:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6325 (0.5349)
2022-11-09 17:58:59,206:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3780 (0.5033)
2022-11-09 17:58:59,368:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3716 (0.4831)
2022-11-09 17:58:59,529:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3844 (0.4701)
2022-11-09 17:58:59,687:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4012 (0.4614)
2022-11-09 17:58:59,848:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5985 (0.4766)
2022-11-09 17:59:00,007:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4437 (0.4738)
2022-11-09 17:59:00,169:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5577 (0.4817)
2022-11-09 17:59:00,328:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4890 (0.4823)
2022-11-09 17:59:00,489:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3717 (0.4736)
2022-11-09 17:59:00,649:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3584 (0.4647)
2022-11-09 17:59:00,811:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3742 (0.4592)
2022-11-09 17:59:00,971:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4648 (0.4595)
2022-11-09 17:59:01,133:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9105 (0.4866)
2022-11-09 17:59:01,273:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3992 (0.4820)
2022-11-09 17:59:01,328:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_227.pth.tar
2022-11-09 17:59:01,328:INFO: 
===> EPOCH: 228 (P2)
2022-11-09 17:59:01,328:INFO: - Computing loss (training)
2022-11-09 17:59:01,673:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4677 (1.4677)
2022-11-09 17:59:01,837:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1166 (1.2891)
2022-11-09 17:59:01,995:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1540 (1.2430)
2022-11-09 17:59:02,092:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3155 (1.0717)
2022-11-09 17:59:02,483:INFO: Dataset: univ                Batch:  1/15	Loss 0.3871 (0.3871)
2022-11-09 17:59:02,649:INFO: Dataset: univ                Batch:  2/15	Loss 0.3963 (0.3914)
2022-11-09 17:59:02,816:INFO: Dataset: univ                Batch:  3/15	Loss 0.3688 (0.3835)
2022-11-09 17:59:02,982:INFO: Dataset: univ                Batch:  4/15	Loss 0.3599 (0.3773)
2022-11-09 17:59:03,146:INFO: Dataset: univ                Batch:  5/15	Loss 0.3411 (0.3699)
2022-11-09 17:59:03,310:INFO: Dataset: univ                Batch:  6/15	Loss 0.5894 (0.4015)
2022-11-09 17:59:03,475:INFO: Dataset: univ                Batch:  7/15	Loss 0.6629 (0.4392)
2022-11-09 17:59:03,642:INFO: Dataset: univ                Batch:  8/15	Loss 0.3518 (0.4276)
2022-11-09 17:59:03,806:INFO: Dataset: univ                Batch:  9/15	Loss 0.4567 (0.4311)
2022-11-09 17:59:03,969:INFO: Dataset: univ                Batch: 10/15	Loss 0.3482 (0.4218)
2022-11-09 17:59:04,133:INFO: Dataset: univ                Batch: 11/15	Loss 0.4672 (0.4257)
2022-11-09 17:59:04,296:INFO: Dataset: univ                Batch: 12/15	Loss 0.3515 (0.4200)
2022-11-09 17:59:04,461:INFO: Dataset: univ                Batch: 13/15	Loss 0.4501 (0.4225)
2022-11-09 17:59:04,625:INFO: Dataset: univ                Batch: 14/15	Loss 0.4236 (0.4226)
2022-11-09 17:59:04,666:INFO: Dataset: univ                Batch: 15/15	Loss 0.0724 (0.4180)
2022-11-09 17:59:05,069:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5187 (0.5187)
2022-11-09 17:59:05,232:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4401 (0.4804)
2022-11-09 17:59:05,392:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4471 (0.4692)
2022-11-09 17:59:05,552:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6421 (0.5103)
2022-11-09 17:59:05,713:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5760 (0.5233)
2022-11-09 17:59:05,871:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3892 (0.5027)
2022-11-09 17:59:06,030:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4910 (0.5010)
2022-11-09 17:59:06,166:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4214 (0.4924)
2022-11-09 17:59:06,560:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3513 (0.3513)
2022-11-09 17:59:06,720:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3680 (0.3595)
2022-11-09 17:59:06,882:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3393 (0.3523)
2022-11-09 17:59:07,045:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3537 (0.3526)
2022-11-09 17:59:07,207:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4563 (0.3738)
2022-11-09 17:59:07,367:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8572 (0.4489)
2022-11-09 17:59:07,529:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3527 (0.4350)
2022-11-09 17:59:07,687:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3494 (0.4242)
2022-11-09 17:59:07,849:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3139 (0.4108)
2022-11-09 17:59:08,008:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3590 (0.4059)
2022-11-09 17:59:08,170:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3719 (0.4031)
2022-11-09 17:59:08,328:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3539 (0.3988)
2022-11-09 17:59:08,489:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4122 (0.3997)
2022-11-09 17:59:08,648:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3154 (0.3934)
2022-11-09 17:59:08,810:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4029 (0.3940)
2022-11-09 17:59:08,971:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3766 (0.3930)
2022-11-09 17:59:09,133:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3937 (0.3931)
2022-11-09 17:59:09,272:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3090 (0.3895)
2022-11-09 17:59:09,324:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_228.pth.tar
2022-11-09 17:59:09,324:INFO: 
===> EPOCH: 229 (P2)
2022-11-09 17:59:09,325:INFO: - Computing loss (training)
2022-11-09 17:59:09,668:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6403 (2.6403)
2022-11-09 17:59:09,831:INFO: Dataset: hotel               Batch: 2/4	Loss 3.9476 (3.2955)
2022-11-09 17:59:09,990:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0142 (2.8597)
2022-11-09 17:59:10,091:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3120 (2.4463)
2022-11-09 17:59:10,484:INFO: Dataset: univ                Batch:  1/15	Loss 0.4592 (0.4592)
2022-11-09 17:59:10,650:INFO: Dataset: univ                Batch:  2/15	Loss 0.5308 (0.4974)
2022-11-09 17:59:10,815:INFO: Dataset: univ                Batch:  3/15	Loss 1.3312 (0.7614)
2022-11-09 17:59:10,977:INFO: Dataset: univ                Batch:  4/15	Loss 0.4220 (0.6798)
2022-11-09 17:59:11,144:INFO: Dataset: univ                Batch:  5/15	Loss 0.5433 (0.6525)
2022-11-09 17:59:11,305:INFO: Dataset: univ                Batch:  6/15	Loss 0.5061 (0.6293)
2022-11-09 17:59:11,470:INFO: Dataset: univ                Batch:  7/15	Loss 0.4503 (0.6039)
2022-11-09 17:59:11,631:INFO: Dataset: univ                Batch:  8/15	Loss 0.6860 (0.6149)
2022-11-09 17:59:11,793:INFO: Dataset: univ                Batch:  9/15	Loss 0.4394 (0.5971)
2022-11-09 17:59:11,956:INFO: Dataset: univ                Batch: 10/15	Loss 0.5939 (0.5968)
2022-11-09 17:59:12,123:INFO: Dataset: univ                Batch: 11/15	Loss 0.4315 (0.5829)
2022-11-09 17:59:12,284:INFO: Dataset: univ                Batch: 12/15	Loss 0.4061 (0.5679)
2022-11-09 17:59:12,449:INFO: Dataset: univ                Batch: 13/15	Loss 0.4163 (0.5554)
2022-11-09 17:59:12,613:INFO: Dataset: univ                Batch: 14/15	Loss 0.3973 (0.5443)
2022-11-09 17:59:12,655:INFO: Dataset: univ                Batch: 15/15	Loss 0.0769 (0.5381)
2022-11-09 17:59:13,047:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9050 (1.9050)
2022-11-09 17:59:13,211:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1050 (1.4882)
2022-11-09 17:59:13,371:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1608 (1.3708)
2022-11-09 17:59:13,529:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4698 (1.1655)
2022-11-09 17:59:13,766:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4294 (1.0165)
2022-11-09 17:59:13,925:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8431 (0.9861)
2022-11-09 17:59:14,085:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4830 (0.9161)
2022-11-09 17:59:14,224:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0845 (0.9347)
2022-11-09 17:59:14,626:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2669 (1.2669)
2022-11-09 17:59:14,786:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7706 (1.0230)
2022-11-09 17:59:14,950:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6572 (0.9044)
2022-11-09 17:59:15,110:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5004 (0.8020)
2022-11-09 17:59:15,274:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3538 (0.7084)
2022-11-09 17:59:15,434:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6618 (0.7010)
2022-11-09 17:59:15,594:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4934 (0.6739)
2022-11-09 17:59:15,751:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6918 (0.6762)
2022-11-09 17:59:15,912:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6253 (0.6704)
2022-11-09 17:59:16,070:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3974 (0.6440)
2022-11-09 17:59:16,233:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4592 (0.6259)
2022-11-09 17:59:16,390:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4346 (0.6099)
2022-11-09 17:59:16,550:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4446 (0.5994)
2022-11-09 17:59:16,709:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3390 (0.5810)
2022-11-09 17:59:16,871:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3854 (0.5667)
2022-11-09 17:59:17,031:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5137 (0.5633)
2022-11-09 17:59:17,193:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3702 (0.5524)
2022-11-09 17:59:17,332:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3572 (0.5434)
2022-11-09 17:59:17,386:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_229.pth.tar
2022-11-09 17:59:17,386:INFO: 
===> EPOCH: 230 (P2)
2022-11-09 17:59:17,386:INFO: - Computing loss (training)
2022-11-09 17:59:17,731:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2448 (2.2448)
2022-11-09 17:59:17,893:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1334 (1.6851)
2022-11-09 17:59:18,053:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7548 (1.3835)
2022-11-09 17:59:18,152:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2790 (1.1868)
2022-11-09 17:59:18,546:INFO: Dataset: univ                Batch:  1/15	Loss 0.7318 (0.7318)
2022-11-09 17:59:18,713:INFO: Dataset: univ                Batch:  2/15	Loss 0.7174 (0.7249)
2022-11-09 17:59:18,876:INFO: Dataset: univ                Batch:  3/15	Loss 0.5455 (0.6701)
2022-11-09 17:59:19,040:INFO: Dataset: univ                Batch:  4/15	Loss 0.3678 (0.5970)
2022-11-09 17:59:19,204:INFO: Dataset: univ                Batch:  5/15	Loss 0.7569 (0.6265)
2022-11-09 17:59:19,368:INFO: Dataset: univ                Batch:  6/15	Loss 0.3844 (0.5860)
2022-11-09 17:59:19,530:INFO: Dataset: univ                Batch:  7/15	Loss 0.3726 (0.5562)
2022-11-09 17:59:19,690:INFO: Dataset: univ                Batch:  8/15	Loss 0.3394 (0.5291)
2022-11-09 17:59:19,853:INFO: Dataset: univ                Batch:  9/15	Loss 0.3671 (0.5104)
2022-11-09 17:59:20,014:INFO: Dataset: univ                Batch: 10/15	Loss 0.7129 (0.5293)
2022-11-09 17:59:20,178:INFO: Dataset: univ                Batch: 11/15	Loss 0.4097 (0.5186)
2022-11-09 17:59:20,340:INFO: Dataset: univ                Batch: 12/15	Loss 0.4499 (0.5127)
2022-11-09 17:59:20,503:INFO: Dataset: univ                Batch: 13/15	Loss 0.7967 (0.5322)
2022-11-09 17:59:20,665:INFO: Dataset: univ                Batch: 14/15	Loss 1.1629 (0.5731)
2022-11-09 17:59:20,707:INFO: Dataset: univ                Batch: 15/15	Loss 0.0643 (0.5664)
2022-11-09 17:59:21,093:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3858 (0.3858)
2022-11-09 17:59:21,256:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3594 (0.3738)
2022-11-09 17:59:21,415:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3765 (0.3746)
2022-11-09 17:59:21,573:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9128 (0.5177)
2022-11-09 17:59:21,735:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7834 (0.5682)
2022-11-09 17:59:21,892:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7128 (0.5948)
2022-11-09 17:59:22,052:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3503 (0.5603)
2022-11-09 17:59:22,188:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2833 (0.5306)
2022-11-09 17:59:22,579:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4624 (0.4624)
2022-11-09 17:59:22,738:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6856 (0.5772)
2022-11-09 17:59:22,903:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3442 (0.5012)
2022-11-09 17:59:23,062:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6737 (0.5455)
2022-11-09 17:59:23,221:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6706 (0.5714)
2022-11-09 17:59:23,383:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3551 (0.5367)
2022-11-09 17:59:23,543:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3921 (0.5156)
2022-11-09 17:59:23,700:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3307 (0.4911)
2022-11-09 17:59:23,860:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3475 (0.4751)
2022-11-09 17:59:24,018:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3627 (0.4640)
2022-11-09 17:59:24,178:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4228 (0.4598)
2022-11-09 17:59:24,337:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5899 (0.4711)
2022-11-09 17:59:24,498:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5185 (0.4744)
2022-11-09 17:59:24,656:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3233 (0.4633)
2022-11-09 17:59:24,818:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3648 (0.4565)
2022-11-09 17:59:24,979:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3849 (0.4520)
2022-11-09 17:59:25,141:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3680 (0.4469)
2022-11-09 17:59:25,279:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2935 (0.4394)
2022-11-09 17:59:25,336:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_230.pth.tar
2022-11-09 17:59:25,336:INFO: 
===> EPOCH: 231 (P2)
2022-11-09 17:59:25,336:INFO: - Computing loss (training)
2022-11-09 17:59:25,676:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5541 (1.5541)
2022-11-09 17:59:25,839:INFO: Dataset: hotel               Batch: 2/4	Loss 2.6973 (2.1353)
2022-11-09 17:59:25,998:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3151 (1.8580)
2022-11-09 17:59:26,098:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8982 (1.6921)
2022-11-09 17:59:26,506:INFO: Dataset: univ                Batch:  1/15	Loss 0.3482 (0.3482)
2022-11-09 17:59:26,674:INFO: Dataset: univ                Batch:  2/15	Loss 0.4954 (0.4210)
2022-11-09 17:59:26,839:INFO: Dataset: univ                Batch:  3/15	Loss 0.5184 (0.4554)
2022-11-09 17:59:27,002:INFO: Dataset: univ                Batch:  4/15	Loss 0.5087 (0.4675)
2022-11-09 17:59:27,167:INFO: Dataset: univ                Batch:  5/15	Loss 0.4058 (0.4563)
2022-11-09 17:59:27,334:INFO: Dataset: univ                Batch:  6/15	Loss 0.6186 (0.4821)
2022-11-09 17:59:27,499:INFO: Dataset: univ                Batch:  7/15	Loss 0.4160 (0.4735)
2022-11-09 17:59:27,661:INFO: Dataset: univ                Batch:  8/15	Loss 0.4070 (0.4651)
2022-11-09 17:59:27,826:INFO: Dataset: univ                Batch:  9/15	Loss 0.4000 (0.4579)
2022-11-09 17:59:27,989:INFO: Dataset: univ                Batch: 10/15	Loss 0.4790 (0.4599)
2022-11-09 17:59:28,153:INFO: Dataset: univ                Batch: 11/15	Loss 0.3921 (0.4541)
2022-11-09 17:59:28,316:INFO: Dataset: univ                Batch: 12/15	Loss 0.3836 (0.4484)
2022-11-09 17:59:28,482:INFO: Dataset: univ                Batch: 13/15	Loss 0.4425 (0.4479)
2022-11-09 17:59:28,646:INFO: Dataset: univ                Batch: 14/15	Loss 0.3747 (0.4424)
2022-11-09 17:59:28,687:INFO: Dataset: univ                Batch: 15/15	Loss 0.0630 (0.4386)
2022-11-09 17:59:29,079:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7961 (1.7961)
2022-11-09 17:59:29,248:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9654 (1.3554)
2022-11-09 17:59:29,415:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4956 (1.0823)
2022-11-09 17:59:29,574:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4483 (0.9215)
2022-11-09 17:59:29,737:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9554 (0.9283)
2022-11-09 17:59:29,897:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5620 (1.0395)
2022-11-09 17:59:30,060:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3981 (1.0895)
2022-11-09 17:59:30,197:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3503 (1.0121)
2022-11-09 17:59:30,591:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5553 (0.5553)
2022-11-09 17:59:30,754:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3414 (0.4467)
2022-11-09 17:59:30,914:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3754 (0.4235)
2022-11-09 17:59:31,074:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3929 (0.4156)
2022-11-09 17:59:31,235:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3975 (0.4119)
2022-11-09 17:59:31,396:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4748 (0.4221)
2022-11-09 17:59:31,557:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6596 (0.4555)
2022-11-09 17:59:31,714:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4258 (0.4517)
2022-11-09 17:59:31,875:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4929 (0.4562)
2022-11-09 17:59:32,033:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4554 (0.4561)
2022-11-09 17:59:32,193:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4017 (0.4509)
2022-11-09 17:59:32,353:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4363 (0.4498)
2022-11-09 17:59:32,516:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4666 (0.4509)
2022-11-09 17:59:32,677:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6992 (0.4695)
2022-11-09 17:59:32,839:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5397 (0.4743)
2022-11-09 17:59:33,000:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6839 (0.4878)
2022-11-09 17:59:33,162:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0768 (0.5240)
2022-11-09 17:59:33,301:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5141 (0.5236)
2022-11-09 17:59:33,353:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_231.pth.tar
2022-11-09 17:59:33,353:INFO: 
===> EPOCH: 232 (P2)
2022-11-09 17:59:33,354:INFO: - Computing loss (training)
2022-11-09 17:59:33,687:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9299 (0.9299)
2022-11-09 17:59:33,850:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2325 (1.0798)
2022-11-09 17:59:34,007:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4226 (0.8621)
2022-11-09 17:59:34,105:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2870 (0.7657)
2022-11-09 17:59:34,550:INFO: Dataset: univ                Batch:  1/15	Loss 0.6099 (0.6099)
2022-11-09 17:59:34,719:INFO: Dataset: univ                Batch:  2/15	Loss 0.4517 (0.5283)
2022-11-09 17:59:34,883:INFO: Dataset: univ                Batch:  3/15	Loss 1.7170 (0.8685)
2022-11-09 17:59:35,049:INFO: Dataset: univ                Batch:  4/15	Loss 0.6005 (0.8010)
2022-11-09 17:59:35,215:INFO: Dataset: univ                Batch:  5/15	Loss 0.6664 (0.7750)
2022-11-09 17:59:35,381:INFO: Dataset: univ                Batch:  6/15	Loss 0.8881 (0.7955)
2022-11-09 17:59:35,546:INFO: Dataset: univ                Batch:  7/15	Loss 0.3688 (0.7339)
2022-11-09 17:59:35,709:INFO: Dataset: univ                Batch:  8/15	Loss 0.3521 (0.6862)
2022-11-09 17:59:35,874:INFO: Dataset: univ                Batch:  9/15	Loss 0.3877 (0.6523)
2022-11-09 17:59:36,037:INFO: Dataset: univ                Batch: 10/15	Loss 0.4620 (0.6334)
2022-11-09 17:59:36,203:INFO: Dataset: univ                Batch: 11/15	Loss 0.4274 (0.6157)
2022-11-09 17:59:36,367:INFO: Dataset: univ                Batch: 12/15	Loss 0.4963 (0.6057)
2022-11-09 17:59:36,534:INFO: Dataset: univ                Batch: 13/15	Loss 0.4181 (0.5907)
2022-11-09 17:59:36,699:INFO: Dataset: univ                Batch: 14/15	Loss 0.7174 (0.6003)
2022-11-09 17:59:36,742:INFO: Dataset: univ                Batch: 15/15	Loss 0.0645 (0.5916)
2022-11-09 17:59:37,134:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3684 (0.3684)
2022-11-09 17:59:37,295:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5837 (0.4680)
2022-11-09 17:59:37,459:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4073 (0.4482)
2022-11-09 17:59:37,617:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3869 (0.4330)
2022-11-09 17:59:37,777:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4389 (0.4342)
2022-11-09 17:59:37,936:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4583 (0.4384)
2022-11-09 17:59:38,096:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3793 (0.4292)
2022-11-09 17:59:38,233:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3879 (0.4241)
2022-11-09 17:59:38,624:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3569 (0.3569)
2022-11-09 17:59:38,785:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3698 (0.3635)
2022-11-09 17:59:38,950:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4399 (0.3893)
2022-11-09 17:59:39,108:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3421 (0.3770)
2022-11-09 17:59:39,269:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3659 (0.3748)
2022-11-09 17:59:39,432:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3236 (0.3669)
2022-11-09 17:59:39,592:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3366 (0.3622)
2022-11-09 17:59:39,750:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3303 (0.3581)
2022-11-09 17:59:39,910:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5620 (0.3803)
2022-11-09 17:59:40,068:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3427 (0.3764)
2022-11-09 17:59:40,228:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3401 (0.3731)
2022-11-09 17:59:40,387:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4001 (0.3753)
2022-11-09 17:59:40,548:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3561 (0.3738)
2022-11-09 17:59:40,707:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3491 (0.3722)
2022-11-09 17:59:40,868:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2989 (0.3672)
2022-11-09 17:59:41,029:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3714 (0.3674)
2022-11-09 17:59:41,191:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4023 (0.3694)
2022-11-09 17:59:41,329:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3139 (0.3669)
2022-11-09 17:59:41,387:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_232.pth.tar
2022-11-09 17:59:41,387:INFO: 
===> EPOCH: 233 (P2)
2022-11-09 17:59:41,388:INFO: - Computing loss (training)
2022-11-09 17:59:41,724:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4814 (0.4814)
2022-11-09 17:59:41,886:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5266 (0.5035)
2022-11-09 17:59:42,046:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7425 (0.5803)
2022-11-09 17:59:42,144:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3453 (0.5412)
2022-11-09 17:59:42,534:INFO: Dataset: univ                Batch:  1/15	Loss 0.3887 (0.3887)
2022-11-09 17:59:42,697:INFO: Dataset: univ                Batch:  2/15	Loss 0.6461 (0.5238)
2022-11-09 17:59:42,862:INFO: Dataset: univ                Batch:  3/15	Loss 0.3580 (0.4698)
2022-11-09 17:59:43,023:INFO: Dataset: univ                Batch:  4/15	Loss 0.3535 (0.4425)
2022-11-09 17:59:43,191:INFO: Dataset: univ                Batch:  5/15	Loss 0.4256 (0.4391)
2022-11-09 17:59:43,354:INFO: Dataset: univ                Batch:  6/15	Loss 0.4006 (0.4324)
2022-11-09 17:59:43,518:INFO: Dataset: univ                Batch:  7/15	Loss 0.4905 (0.4411)
2022-11-09 17:59:43,678:INFO: Dataset: univ                Batch:  8/15	Loss 0.3435 (0.4292)
2022-11-09 17:59:43,842:INFO: Dataset: univ                Batch:  9/15	Loss 0.3927 (0.4250)
2022-11-09 17:59:44,003:INFO: Dataset: univ                Batch: 10/15	Loss 0.3440 (0.4170)
2022-11-09 17:59:44,167:INFO: Dataset: univ                Batch: 11/15	Loss 0.3942 (0.4149)
2022-11-09 17:59:44,331:INFO: Dataset: univ                Batch: 12/15	Loss 0.3262 (0.4068)
2022-11-09 17:59:44,497:INFO: Dataset: univ                Batch: 13/15	Loss 0.3369 (0.4015)
2022-11-09 17:59:44,660:INFO: Dataset: univ                Batch: 14/15	Loss 0.6057 (0.4147)
2022-11-09 17:59:44,702:INFO: Dataset: univ                Batch: 15/15	Loss 0.0740 (0.4103)
2022-11-09 17:59:45,108:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1640 (1.1640)
2022-11-09 17:59:45,271:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2790 (1.2152)
2022-11-09 17:59:45,431:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9711 (1.1339)
2022-11-09 17:59:45,587:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3953 (0.9523)
2022-11-09 17:59:45,749:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4199 (0.8476)
2022-11-09 17:59:45,906:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6483 (0.8130)
2022-11-09 17:59:46,066:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0055 (0.8376)
2022-11-09 17:59:46,202:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8159 (0.8349)
2022-11-09 17:59:46,661:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8187 (0.8187)
2022-11-09 17:59:46,818:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6077 (0.7240)
2022-11-09 17:59:46,979:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4322 (0.6169)
2022-11-09 17:59:47,137:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3622 (0.5602)
2022-11-09 17:59:47,297:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3486 (0.5184)
2022-11-09 17:59:47,459:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5300 (0.5203)
2022-11-09 17:59:47,619:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4785 (0.5150)
2022-11-09 17:59:47,775:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7865 (0.5463)
2022-11-09 17:59:47,935:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5498 (0.5467)
2022-11-09 17:59:48,093:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3821 (0.5292)
2022-11-09 17:59:48,253:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3489 (0.5129)
2022-11-09 17:59:48,411:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3677 (0.4995)
2022-11-09 17:59:48,572:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4420 (0.4952)
2022-11-09 17:59:48,730:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3797 (0.4872)
2022-11-09 17:59:48,891:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3461 (0.4782)
2022-11-09 17:59:49,051:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3443 (0.4700)
2022-11-09 17:59:49,212:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4641 (0.4696)
2022-11-09 17:59:49,350:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3291 (0.4632)
2022-11-09 17:59:49,405:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_233.pth.tar
2022-11-09 17:59:49,406:INFO: 
===> EPOCH: 234 (P2)
2022-11-09 17:59:49,406:INFO: - Computing loss (training)
2022-11-09 17:59:49,735:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4883 (0.4883)
2022-11-09 17:59:49,895:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7080 (0.5992)
2022-11-09 17:59:50,055:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4386 (0.5465)
2022-11-09 17:59:50,153:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3282 (0.5065)
2022-11-09 17:59:50,557:INFO: Dataset: univ                Batch:  1/15	Loss 0.4355 (0.4355)
2022-11-09 17:59:50,797:INFO: Dataset: univ                Batch:  2/15	Loss 0.4879 (0.4627)
2022-11-09 17:59:50,958:INFO: Dataset: univ                Batch:  3/15	Loss 0.4018 (0.4425)
2022-11-09 17:59:51,120:INFO: Dataset: univ                Batch:  4/15	Loss 0.5156 (0.4614)
2022-11-09 17:59:51,282:INFO: Dataset: univ                Batch:  5/15	Loss 0.4943 (0.4678)
2022-11-09 17:59:51,450:INFO: Dataset: univ                Batch:  6/15	Loss 0.3229 (0.4427)
2022-11-09 17:59:51,611:INFO: Dataset: univ                Batch:  7/15	Loss 0.3333 (0.4278)
2022-11-09 17:59:51,774:INFO: Dataset: univ                Batch:  8/15	Loss 0.4053 (0.4250)
2022-11-09 17:59:51,935:INFO: Dataset: univ                Batch:  9/15	Loss 0.3653 (0.4180)
2022-11-09 17:59:52,098:INFO: Dataset: univ                Batch: 10/15	Loss 0.3401 (0.4101)
2022-11-09 17:59:52,259:INFO: Dataset: univ                Batch: 11/15	Loss 0.3133 (0.4009)
2022-11-09 17:59:52,424:INFO: Dataset: univ                Batch: 12/15	Loss 0.3558 (0.3969)
2022-11-09 17:59:52,585:INFO: Dataset: univ                Batch: 13/15	Loss 0.3374 (0.3928)
2022-11-09 17:59:52,750:INFO: Dataset: univ                Batch: 14/15	Loss 0.2997 (0.3854)
2022-11-09 17:59:52,792:INFO: Dataset: univ                Batch: 15/15	Loss 0.0673 (0.3808)
2022-11-09 17:59:53,180:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5876 (0.5876)
2022-11-09 17:59:53,340:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0428 (0.8324)
2022-11-09 17:59:53,500:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3859 (0.6821)
2022-11-09 17:59:53,656:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3740 (0.6036)
2022-11-09 17:59:53,817:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4770 (0.5819)
2022-11-09 17:59:53,976:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4905 (0.5668)
2022-11-09 17:59:54,135:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3570 (0.5355)
2022-11-09 17:59:54,270:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4405 (0.5249)
2022-11-09 17:59:54,666:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3352 (0.3352)
2022-11-09 17:59:54,828:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3305 (0.3328)
2022-11-09 17:59:54,990:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2806 (0.3169)
2022-11-09 17:59:55,149:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3983 (0.3392)
2022-11-09 17:59:55,310:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3433 (0.3400)
2022-11-09 17:59:55,470:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3318 (0.3385)
2022-11-09 17:59:55,631:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3131 (0.3347)
2022-11-09 17:59:55,788:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3364 (0.3349)
2022-11-09 17:59:55,948:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3709 (0.3388)
2022-11-09 17:59:56,106:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3099 (0.3360)
2022-11-09 17:59:56,268:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3304 (0.3355)
2022-11-09 17:59:56,425:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4067 (0.3416)
2022-11-09 17:59:56,586:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3382 (0.3414)
2022-11-09 17:59:56,744:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3132 (0.3392)
2022-11-09 17:59:56,905:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3101 (0.3373)
2022-11-09 17:59:57,065:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3088 (0.3355)
2022-11-09 17:59:57,227:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3564 (0.3368)
2022-11-09 17:59:57,365:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2878 (0.3345)
2022-11-09 17:59:57,418:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_234.pth.tar
2022-11-09 17:59:57,419:INFO: 
===> EPOCH: 235 (P2)
2022-11-09 17:59:57,419:INFO: - Computing loss (training)
2022-11-09 17:59:57,759:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6339 (0.6339)
2022-11-09 17:59:57,922:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6245 (0.6292)
2022-11-09 17:59:58,081:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4076 (0.5590)
2022-11-09 17:59:58,179:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4743 (0.5438)
2022-11-09 17:59:58,570:INFO: Dataset: univ                Batch:  1/15	Loss 0.3509 (0.3509)
2022-11-09 17:59:58,736:INFO: Dataset: univ                Batch:  2/15	Loss 0.3543 (0.3527)
2022-11-09 17:59:58,902:INFO: Dataset: univ                Batch:  3/15	Loss 0.3776 (0.3608)
2022-11-09 17:59:59,067:INFO: Dataset: univ                Batch:  4/15	Loss 0.3581 (0.3602)
2022-11-09 17:59:59,232:INFO: Dataset: univ                Batch:  5/15	Loss 0.3366 (0.3556)
2022-11-09 17:59:59,396:INFO: Dataset: univ                Batch:  6/15	Loss 0.3388 (0.3525)
2022-11-09 17:59:59,560:INFO: Dataset: univ                Batch:  7/15	Loss 0.3207 (0.3482)
2022-11-09 17:59:59,720:INFO: Dataset: univ                Batch:  8/15	Loss 0.3343 (0.3465)
2022-11-09 17:59:59,883:INFO: Dataset: univ                Batch:  9/15	Loss 0.3895 (0.3510)
2022-11-09 18:00:00,046:INFO: Dataset: univ                Batch: 10/15	Loss 0.3840 (0.3542)
2022-11-09 18:00:00,212:INFO: Dataset: univ                Batch: 11/15	Loss 0.4357 (0.3620)
2022-11-09 18:00:00,373:INFO: Dataset: univ                Batch: 12/15	Loss 0.3296 (0.3593)
2022-11-09 18:00:00,537:INFO: Dataset: univ                Batch: 13/15	Loss 0.2860 (0.3540)
2022-11-09 18:00:00,701:INFO: Dataset: univ                Batch: 14/15	Loss 0.4132 (0.3587)
2022-11-09 18:00:00,743:INFO: Dataset: univ                Batch: 15/15	Loss 0.0665 (0.3548)
2022-11-09 18:00:01,151:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5459 (0.5459)
2022-11-09 18:00:01,315:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4380 (0.4916)
2022-11-09 18:00:01,475:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3504 (0.4426)
2022-11-09 18:00:01,637:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4156 (0.4354)
2022-11-09 18:00:01,797:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3902 (0.4275)
2022-11-09 18:00:01,956:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3394 (0.4124)
2022-11-09 18:00:02,116:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3579 (0.4048)
2022-11-09 18:00:02,253:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8508 (0.4510)
2022-11-09 18:00:02,658:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3747 (0.3747)
2022-11-09 18:00:02,819:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3533 (0.3636)
2022-11-09 18:00:02,988:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3146 (0.3478)
2022-11-09 18:00:03,147:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3172 (0.3406)
2022-11-09 18:00:03,311:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3239 (0.3371)
2022-11-09 18:00:03,480:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3146 (0.3335)
2022-11-09 18:00:03,646:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3317 (0.3332)
2022-11-09 18:00:03,805:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3094 (0.3300)
2022-11-09 18:00:03,969:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3918 (0.3366)
2022-11-09 18:00:04,128:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2940 (0.3320)
2022-11-09 18:00:04,291:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3328 (0.3321)
2022-11-09 18:00:04,451:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3528 (0.3337)
2022-11-09 18:00:04,615:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2962 (0.3308)
2022-11-09 18:00:04,774:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3299 (0.3307)
2022-11-09 18:00:04,938:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3268 (0.3305)
2022-11-09 18:00:05,099:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3915 (0.3339)
2022-11-09 18:00:05,263:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3096 (0.3325)
2022-11-09 18:00:05,403:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3694 (0.3342)
2022-11-09 18:00:05,457:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_235.pth.tar
2022-11-09 18:00:05,457:INFO: 
===> EPOCH: 236 (P2)
2022-11-09 18:00:05,457:INFO: - Computing loss (training)
2022-11-09 18:00:05,809:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4478 (0.4478)
2022-11-09 18:00:05,971:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9609 (0.7001)
2022-11-09 18:00:06,129:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3744 (0.5914)
2022-11-09 18:00:06,229:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2240 (0.5289)
2022-11-09 18:00:06,667:INFO: Dataset: univ                Batch:  1/15	Loss 0.5604 (0.5604)
2022-11-09 18:00:06,831:INFO: Dataset: univ                Batch:  2/15	Loss 0.4641 (0.5144)
2022-11-09 18:00:06,997:INFO: Dataset: univ                Batch:  3/15	Loss 0.3206 (0.4456)
2022-11-09 18:00:07,160:INFO: Dataset: univ                Batch:  4/15	Loss 0.4786 (0.4545)
2022-11-09 18:00:07,323:INFO: Dataset: univ                Batch:  5/15	Loss 0.3494 (0.4327)
2022-11-09 18:00:07,490:INFO: Dataset: univ                Batch:  6/15	Loss 0.3022 (0.4117)
2022-11-09 18:00:07,656:INFO: Dataset: univ                Batch:  7/15	Loss 0.3721 (0.4062)
2022-11-09 18:00:07,818:INFO: Dataset: univ                Batch:  8/15	Loss 0.2975 (0.3931)
2022-11-09 18:00:07,983:INFO: Dataset: univ                Batch:  9/15	Loss 0.3481 (0.3885)
2022-11-09 18:00:08,146:INFO: Dataset: univ                Batch: 10/15	Loss 0.3393 (0.3840)
2022-11-09 18:00:08,311:INFO: Dataset: univ                Batch: 11/15	Loss 0.3014 (0.3767)
2022-11-09 18:00:08,475:INFO: Dataset: univ                Batch: 12/15	Loss 0.3682 (0.3760)
2022-11-09 18:00:08,642:INFO: Dataset: univ                Batch: 13/15	Loss 0.9383 (0.4160)
2022-11-09 18:00:08,806:INFO: Dataset: univ                Batch: 14/15	Loss 0.4669 (0.4196)
2022-11-09 18:00:08,848:INFO: Dataset: univ                Batch: 15/15	Loss 0.0615 (0.4149)
2022-11-09 18:00:09,234:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5322 (0.5322)
2022-11-09 18:00:09,397:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3640 (0.4524)
2022-11-09 18:00:09,561:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3387 (0.4141)
2022-11-09 18:00:09,720:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3691 (0.4026)
2022-11-09 18:00:09,883:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3716 (0.3965)
2022-11-09 18:00:10,046:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4700 (0.4081)
2022-11-09 18:00:10,208:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5894 (0.4322)
2022-11-09 18:00:10,346:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4087 (0.4296)
2022-11-09 18:00:10,741:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3738 (0.3738)
2022-11-09 18:00:10,905:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3715 (0.3727)
2022-11-09 18:00:11,065:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3495 (0.3646)
2022-11-09 18:00:11,222:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4540 (0.3865)
2022-11-09 18:00:11,384:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4229 (0.3939)
2022-11-09 18:00:11,545:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3181 (0.3812)
2022-11-09 18:00:11,704:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4264 (0.3876)
2022-11-09 18:00:11,861:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4919 (0.4011)
2022-11-09 18:00:12,022:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3890 (0.3998)
2022-11-09 18:00:12,179:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5606 (0.4161)
2022-11-09 18:00:12,339:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3303 (0.4080)
2022-11-09 18:00:12,499:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3621 (0.4040)
2022-11-09 18:00:12,659:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3415 (0.3994)
2022-11-09 18:00:12,818:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4335 (0.4021)
2022-11-09 18:00:12,979:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3233 (0.3974)
2022-11-09 18:00:13,138:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6532 (0.4138)
2022-11-09 18:00:13,299:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5328 (0.4208)
2022-11-09 18:00:13,438:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3740 (0.4185)
2022-11-09 18:00:13,492:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_236.pth.tar
2022-11-09 18:00:13,492:INFO: 
===> EPOCH: 237 (P2)
2022-11-09 18:00:13,493:INFO: - Computing loss (training)
2022-11-09 18:00:13,837:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3794 (0.3794)
2022-11-09 18:00:14,000:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0626 (1.1870)
2022-11-09 18:00:14,159:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7730 (1.0466)
2022-11-09 18:00:14,261:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2905 (0.9259)
2022-11-09 18:00:14,658:INFO: Dataset: univ                Batch:  1/15	Loss 0.3425 (0.3425)
2022-11-09 18:00:14,821:INFO: Dataset: univ                Batch:  2/15	Loss 0.3415 (0.3420)
2022-11-09 18:00:14,989:INFO: Dataset: univ                Batch:  3/15	Loss 0.4748 (0.3884)
2022-11-09 18:00:15,149:INFO: Dataset: univ                Batch:  4/15	Loss 0.4073 (0.3932)
2022-11-09 18:00:15,312:INFO: Dataset: univ                Batch:  5/15	Loss 0.3495 (0.3840)
2022-11-09 18:00:15,475:INFO: Dataset: univ                Batch:  6/15	Loss 0.3098 (0.3706)
2022-11-09 18:00:15,638:INFO: Dataset: univ                Batch:  7/15	Loss 0.7034 (0.4223)
2022-11-09 18:00:15,798:INFO: Dataset: univ                Batch:  8/15	Loss 0.3636 (0.4142)
2022-11-09 18:00:15,960:INFO: Dataset: univ                Batch:  9/15	Loss 0.3945 (0.4120)
2022-11-09 18:00:16,120:INFO: Dataset: univ                Batch: 10/15	Loss 0.3404 (0.4053)
2022-11-09 18:00:16,283:INFO: Dataset: univ                Batch: 11/15	Loss 0.4317 (0.4076)
2022-11-09 18:00:16,444:INFO: Dataset: univ                Batch: 12/15	Loss 0.3890 (0.4059)
2022-11-09 18:00:16,607:INFO: Dataset: univ                Batch: 13/15	Loss 0.3508 (0.4016)
2022-11-09 18:00:16,769:INFO: Dataset: univ                Batch: 14/15	Loss 0.3336 (0.3969)
2022-11-09 18:00:16,811:INFO: Dataset: univ                Batch: 15/15	Loss 0.0605 (0.3916)
2022-11-09 18:00:17,206:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5759 (1.5759)
2022-11-09 18:00:17,368:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8186 (1.6999)
2022-11-09 18:00:17,527:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8772 (1.3901)
2022-11-09 18:00:17,683:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3975 (1.1534)
2022-11-09 18:00:17,844:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3887 (0.9909)
2022-11-09 18:00:18,002:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8548 (0.9668)
2022-11-09 18:00:18,160:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5828 (0.9127)
2022-11-09 18:00:18,294:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6319 (0.8846)
2022-11-09 18:00:18,694:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6454 (0.6454)
2022-11-09 18:00:18,854:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5154 (0.5821)
2022-11-09 18:00:19,018:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7477 (0.6404)
2022-11-09 18:00:19,177:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3965 (0.5818)
2022-11-09 18:00:19,349:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3151 (0.5284)
2022-11-09 18:00:19,512:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7432 (0.5666)
2022-11-09 18:00:19,674:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7054 (0.5848)
2022-11-09 18:00:19,834:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4946 (0.5726)
2022-11-09 18:00:19,996:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5308 (0.5678)
2022-11-09 18:00:20,155:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4029 (0.5508)
2022-11-09 18:00:20,319:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3540 (0.5316)
2022-11-09 18:00:20,479:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5164 (0.5305)
2022-11-09 18:00:20,641:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3100 (0.5145)
2022-11-09 18:00:20,802:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4118 (0.5076)
2022-11-09 18:00:20,965:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3412 (0.4967)
2022-11-09 18:00:21,126:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5096 (0.4975)
2022-11-09 18:00:21,289:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6138 (0.5042)
2022-11-09 18:00:21,429:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4171 (0.5005)
2022-11-09 18:00:21,487:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_237.pth.tar
2022-11-09 18:00:21,488:INFO: 
===> EPOCH: 238 (P2)
2022-11-09 18:00:21,488:INFO: - Computing loss (training)
2022-11-09 18:00:21,838:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0421 (2.0421)
2022-11-09 18:00:22,000:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0683 (1.5563)
2022-11-09 18:00:22,160:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4908 (1.2422)
2022-11-09 18:00:22,258:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3035 (1.0849)
2022-11-09 18:00:22,654:INFO: Dataset: univ                Batch:  1/15	Loss 0.4490 (0.4490)
2022-11-09 18:00:22,821:INFO: Dataset: univ                Batch:  2/15	Loss 0.4195 (0.4339)
2022-11-09 18:00:22,988:INFO: Dataset: univ                Batch:  3/15	Loss 0.4377 (0.4351)
2022-11-09 18:00:23,155:INFO: Dataset: univ                Batch:  4/15	Loss 0.4000 (0.4263)
2022-11-09 18:00:23,320:INFO: Dataset: univ                Batch:  5/15	Loss 0.3291 (0.4056)
2022-11-09 18:00:23,487:INFO: Dataset: univ                Batch:  6/15	Loss 0.3912 (0.4030)
2022-11-09 18:00:23,653:INFO: Dataset: univ                Batch:  7/15	Loss 0.4075 (0.4037)
2022-11-09 18:00:23,815:INFO: Dataset: univ                Batch:  8/15	Loss 0.3308 (0.3944)
2022-11-09 18:00:23,981:INFO: Dataset: univ                Batch:  9/15	Loss 0.3686 (0.3913)
2022-11-09 18:00:24,146:INFO: Dataset: univ                Batch: 10/15	Loss 0.3504 (0.3867)
2022-11-09 18:00:24,311:INFO: Dataset: univ                Batch: 11/15	Loss 0.3359 (0.3818)
2022-11-09 18:00:24,476:INFO: Dataset: univ                Batch: 12/15	Loss 0.5266 (0.3936)
2022-11-09 18:00:24,642:INFO: Dataset: univ                Batch: 13/15	Loss 0.8309 (0.4261)
2022-11-09 18:00:24,807:INFO: Dataset: univ                Batch: 14/15	Loss 0.4325 (0.4266)
2022-11-09 18:00:24,849:INFO: Dataset: univ                Batch: 15/15	Loss 0.0673 (0.4220)
2022-11-09 18:00:25,241:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9488 (0.9488)
2022-11-09 18:00:25,476:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6917 (0.8134)
2022-11-09 18:00:25,635:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3873 (0.6706)
2022-11-09 18:00:25,792:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3582 (0.5938)
2022-11-09 18:00:25,949:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6089 (0.5967)
2022-11-09 18:00:26,110:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7140 (0.6170)
2022-11-09 18:00:26,265:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1351 (0.6947)
2022-11-09 18:00:26,403:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4127 (0.6649)
2022-11-09 18:00:26,804:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3857 (0.3857)
2022-11-09 18:00:26,963:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3211 (0.3528)
2022-11-09 18:00:27,127:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3357 (0.3471)
2022-11-09 18:00:27,288:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4945 (0.3850)
2022-11-09 18:00:27,448:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7281 (0.4512)
2022-11-09 18:00:27,611:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5900 (0.4766)
2022-11-09 18:00:27,771:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2272 (0.5820)
2022-11-09 18:00:27,929:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6152 (0.5866)
2022-11-09 18:00:28,090:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3924 (0.5640)
2022-11-09 18:00:28,248:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3727 (0.5457)
2022-11-09 18:00:28,408:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3060 (0.5227)
2022-11-09 18:00:28,569:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3303 (0.5062)
2022-11-09 18:00:28,730:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4867 (0.5048)
2022-11-09 18:00:28,890:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4951 (0.5041)
2022-11-09 18:00:29,052:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6626 (0.5150)
2022-11-09 18:00:29,215:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4000 (0.5075)
2022-11-09 18:00:29,377:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4757 (0.5057)
2022-11-09 18:00:29,517:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2342 (0.4930)
2022-11-09 18:00:29,589:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_238.pth.tar
2022-11-09 18:00:29,589:INFO: 
===> EPOCH: 239 (P2)
2022-11-09 18:00:29,589:INFO: - Computing loss (training)
2022-11-09 18:00:29,938:INFO: Dataset: hotel               Batch: 1/4	Loss 3.3316 (3.3316)
2022-11-09 18:00:30,103:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3125 (2.2916)
2022-11-09 18:00:30,261:INFO: Dataset: hotel               Batch: 3/4	Loss 2.7268 (2.4300)
2022-11-09 18:00:30,360:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6021 (2.1262)
2022-11-09 18:00:30,765:INFO: Dataset: univ                Batch:  1/15	Loss 0.3533 (0.3533)
2022-11-09 18:00:30,929:INFO: Dataset: univ                Batch:  2/15	Loss 0.3088 (0.3322)
2022-11-09 18:00:31,094:INFO: Dataset: univ                Batch:  3/15	Loss 0.6615 (0.4305)
2022-11-09 18:00:31,255:INFO: Dataset: univ                Batch:  4/15	Loss 0.3413 (0.4089)
2022-11-09 18:00:31,423:INFO: Dataset: univ                Batch:  5/15	Loss 0.4291 (0.4125)
2022-11-09 18:00:31,588:INFO: Dataset: univ                Batch:  6/15	Loss 0.4077 (0.4117)
2022-11-09 18:00:31,753:INFO: Dataset: univ                Batch:  7/15	Loss 0.3834 (0.4075)
2022-11-09 18:00:31,914:INFO: Dataset: univ                Batch:  8/15	Loss 0.6317 (0.4353)
2022-11-09 18:00:32,079:INFO: Dataset: univ                Batch:  9/15	Loss 0.4255 (0.4342)
2022-11-09 18:00:32,242:INFO: Dataset: univ                Batch: 10/15	Loss 0.5647 (0.4466)
2022-11-09 18:00:32,408:INFO: Dataset: univ                Batch: 11/15	Loss 0.4414 (0.4462)
2022-11-09 18:00:32,574:INFO: Dataset: univ                Batch: 12/15	Loss 0.3688 (0.4392)
2022-11-09 18:00:32,739:INFO: Dataset: univ                Batch: 13/15	Loss 0.4502 (0.4401)
2022-11-09 18:00:32,903:INFO: Dataset: univ                Batch: 14/15	Loss 0.3641 (0.4348)
2022-11-09 18:00:32,945:INFO: Dataset: univ                Batch: 15/15	Loss 0.0693 (0.4302)
2022-11-09 18:00:33,348:INFO: Dataset: zara1               Batch: 1/8	Loss 2.2905 (2.2905)
2022-11-09 18:00:33,510:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0197 (1.6925)
2022-11-09 18:00:33,669:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1721 (1.5216)
2022-11-09 18:00:33,824:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5095 (1.2652)
2022-11-09 18:00:33,984:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0791 (1.2285)
2022-11-09 18:00:34,143:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1689 (1.2180)
2022-11-09 18:00:34,301:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7018 (1.2799)
2022-11-09 18:00:34,435:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6276 (1.2126)
2022-11-09 18:00:34,834:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3681 (0.3681)
2022-11-09 18:00:34,991:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3333 (0.3509)
2022-11-09 18:00:35,151:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4642 (0.3914)
2022-11-09 18:00:35,311:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3814 (0.3891)
2022-11-09 18:00:35,470:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3823 (0.3879)
2022-11-09 18:00:35,631:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3870 (0.3877)
2022-11-09 18:00:35,791:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9202 (0.4655)
2022-11-09 18:00:35,946:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3890 (0.4563)
2022-11-09 18:00:36,107:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3769 (0.4471)
2022-11-09 18:00:36,262:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3677 (0.4390)
2022-11-09 18:00:36,425:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3915 (0.4348)
2022-11-09 18:00:36,582:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6263 (0.4522)
2022-11-09 18:00:36,743:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4181 (0.4494)
2022-11-09 18:00:36,900:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5985 (0.4602)
2022-11-09 18:00:37,063:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4782 (0.4614)
2022-11-09 18:00:37,220:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3882 (0.4569)
2022-11-09 18:00:37,382:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3653 (0.4519)
2022-11-09 18:00:37,520:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3937 (0.4491)
2022-11-09 18:00:37,581:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_239.pth.tar
2022-11-09 18:00:37,581:INFO: 
===> EPOCH: 240 (P2)
2022-11-09 18:00:37,581:INFO: - Computing loss (training)
2022-11-09 18:00:37,919:INFO: Dataset: hotel               Batch: 1/4	Loss 2.6033 (2.6033)
2022-11-09 18:00:38,085:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2207 (1.9104)
2022-11-09 18:00:38,243:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7510 (1.5443)
2022-11-09 18:00:38,340:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2403 (1.3189)
2022-11-09 18:00:38,733:INFO: Dataset: univ                Batch:  1/15	Loss 0.3079 (0.3079)
2022-11-09 18:00:38,898:INFO: Dataset: univ                Batch:  2/15	Loss 0.3203 (0.3143)
2022-11-09 18:00:39,066:INFO: Dataset: univ                Batch:  3/15	Loss 0.3363 (0.3215)
2022-11-09 18:00:39,226:INFO: Dataset: univ                Batch:  4/15	Loss 0.2971 (0.3149)
2022-11-09 18:00:39,388:INFO: Dataset: univ                Batch:  5/15	Loss 0.6792 (0.3893)
2022-11-09 18:00:39,553:INFO: Dataset: univ                Batch:  6/15	Loss 0.3039 (0.3740)
2022-11-09 18:00:39,716:INFO: Dataset: univ                Batch:  7/15	Loss 0.4890 (0.3918)
2022-11-09 18:00:39,875:INFO: Dataset: univ                Batch:  8/15	Loss 0.3444 (0.3858)
2022-11-09 18:00:40,038:INFO: Dataset: univ                Batch:  9/15	Loss 1.2425 (0.4677)
2022-11-09 18:00:40,198:INFO: Dataset: univ                Batch: 10/15	Loss 0.3671 (0.4574)
2022-11-09 18:00:40,361:INFO: Dataset: univ                Batch: 11/15	Loss 0.4175 (0.4538)
2022-11-09 18:00:40,523:INFO: Dataset: univ                Batch: 12/15	Loss 0.5807 (0.4641)
2022-11-09 18:00:40,687:INFO: Dataset: univ                Batch: 13/15	Loss 0.3665 (0.4565)
2022-11-09 18:00:40,848:INFO: Dataset: univ                Batch: 14/15	Loss 0.4009 (0.4529)
2022-11-09 18:00:40,889:INFO: Dataset: univ                Batch: 15/15	Loss 0.0691 (0.4472)
2022-11-09 18:00:41,283:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3829 (0.3829)
2022-11-09 18:00:41,443:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6223 (0.5018)
2022-11-09 18:00:41,605:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6175 (0.5414)
2022-11-09 18:00:41,762:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4090 (0.5066)
2022-11-09 18:00:41,922:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3684 (0.4801)
2022-11-09 18:00:42,083:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4105 (0.4696)
2022-11-09 18:00:42,243:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4621 (0.4686)
2022-11-09 18:00:42,381:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2996 (0.4487)
2022-11-09 18:00:42,773:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4271 (0.4271)
2022-11-09 18:00:42,942:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4490 (0.4380)
2022-11-09 18:00:43,106:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3392 (0.4047)
2022-11-09 18:00:43,262:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3144 (0.3826)
2022-11-09 18:00:43,421:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3136 (0.3695)
2022-11-09 18:00:43,582:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3111 (0.3595)
2022-11-09 18:00:43,741:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3528 (0.3585)
2022-11-09 18:00:43,897:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3040 (0.3523)
2022-11-09 18:00:44,056:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3512 (0.3522)
2022-11-09 18:00:44,215:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3571 (0.3527)
2022-11-09 18:00:44,375:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3166 (0.3491)
2022-11-09 18:00:44,533:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3586 (0.3499)
2022-11-09 18:00:44,692:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2903 (0.3455)
2022-11-09 18:00:44,849:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3772 (0.3479)
2022-11-09 18:00:45,011:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3076 (0.3452)
2022-11-09 18:00:45,170:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4859 (0.3540)
2022-11-09 18:00:45,330:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2929 (0.3502)
2022-11-09 18:00:45,467:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3652 (0.3509)
2022-11-09 18:00:45,523:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_240.pth.tar
2022-11-09 18:00:45,523:INFO: 
===> EPOCH: 241 (P2)
2022-11-09 18:00:45,523:INFO: - Computing loss (training)
2022-11-09 18:00:45,864:INFO: Dataset: hotel               Batch: 1/4	Loss 3.6496 (3.6496)
2022-11-09 18:00:46,028:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4143 (3.0183)
2022-11-09 18:00:46,185:INFO: Dataset: hotel               Batch: 3/4	Loss 2.2276 (2.7376)
2022-11-09 18:00:46,283:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7718 (2.4083)
2022-11-09 18:00:46,741:INFO: Dataset: univ                Batch:  1/15	Loss 0.3825 (0.3825)
2022-11-09 18:00:46,905:INFO: Dataset: univ                Batch:  2/15	Loss 0.3894 (0.3858)
2022-11-09 18:00:47,070:INFO: Dataset: univ                Batch:  3/15	Loss 0.3739 (0.3818)
2022-11-09 18:00:47,232:INFO: Dataset: univ                Batch:  4/15	Loss 0.3177 (0.3665)
2022-11-09 18:00:47,395:INFO: Dataset: univ                Batch:  5/15	Loss 0.3619 (0.3657)
2022-11-09 18:00:47,564:INFO: Dataset: univ                Batch:  6/15	Loss 0.3477 (0.3628)
2022-11-09 18:00:47,728:INFO: Dataset: univ                Batch:  7/15	Loss 0.4267 (0.3708)
2022-11-09 18:00:47,890:INFO: Dataset: univ                Batch:  8/15	Loss 0.6920 (0.4120)
2022-11-09 18:00:48,055:INFO: Dataset: univ                Batch:  9/15	Loss 0.4034 (0.4111)
2022-11-09 18:00:48,218:INFO: Dataset: univ                Batch: 10/15	Loss 0.3589 (0.4060)
2022-11-09 18:00:48,385:INFO: Dataset: univ                Batch: 11/15	Loss 0.5610 (0.4211)
2022-11-09 18:00:48,548:INFO: Dataset: univ                Batch: 12/15	Loss 0.6522 (0.4409)
2022-11-09 18:00:48,714:INFO: Dataset: univ                Batch: 13/15	Loss 0.3560 (0.4346)
2022-11-09 18:00:48,878:INFO: Dataset: univ                Batch: 14/15	Loss 0.3263 (0.4267)
2022-11-09 18:00:48,919:INFO: Dataset: univ                Batch: 15/15	Loss 0.0849 (0.4234)
2022-11-09 18:00:49,316:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7980 (0.7980)
2022-11-09 18:00:49,480:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6513 (1.2317)
2022-11-09 18:00:49,639:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8788 (1.4474)
2022-11-09 18:00:49,795:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4040 (1.1707)
2022-11-09 18:00:49,955:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3383 (1.0225)
2022-11-09 18:00:50,115:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6955 (0.9666)
2022-11-09 18:00:50,275:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9155 (0.9588)
2022-11-09 18:00:50,410:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3873 (0.9017)
2022-11-09 18:00:50,806:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5634 (0.5634)
2022-11-09 18:00:50,974:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7951 (0.6801)
2022-11-09 18:00:51,139:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4029 (0.5906)
2022-11-09 18:00:51,301:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4314 (0.5528)
2022-11-09 18:00:51,470:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3546 (0.5137)
2022-11-09 18:00:51,637:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3364 (0.4813)
2022-11-09 18:00:51,801:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3917 (0.4680)
2022-11-09 18:00:51,963:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4910 (0.4709)
2022-11-09 18:00:52,128:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3889 (0.4616)
2022-11-09 18:00:52,291:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3297 (0.4462)
2022-11-09 18:00:52,456:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3430 (0.4371)
2022-11-09 18:00:52,622:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3313 (0.4284)
2022-11-09 18:00:52,787:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4339 (0.4288)
2022-11-09 18:00:52,951:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5162 (0.4354)
2022-11-09 18:00:53,118:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3709 (0.4312)
2022-11-09 18:00:53,282:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3070 (0.4234)
2022-11-09 18:00:53,448:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3440 (0.4191)
2022-11-09 18:00:53,593:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3029 (0.4131)
2022-11-09 18:00:53,648:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_241.pth.tar
2022-11-09 18:00:53,648:INFO: 
===> EPOCH: 242 (P2)
2022-11-09 18:00:53,648:INFO: - Computing loss (training)
2022-11-09 18:00:53,993:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2964 (2.2964)
2022-11-09 18:00:54,156:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9113 (2.0993)
2022-11-09 18:00:54,314:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0976 (1.7675)
2022-11-09 18:00:54,412:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4444 (1.5406)
2022-11-09 18:00:54,876:INFO: Dataset: univ                Batch:  1/15	Loss 0.3545 (0.3545)
2022-11-09 18:00:55,041:INFO: Dataset: univ                Batch:  2/15	Loss 0.3253 (0.3399)
2022-11-09 18:00:55,205:INFO: Dataset: univ                Batch:  3/15	Loss 0.3852 (0.3545)
2022-11-09 18:00:55,369:INFO: Dataset: univ                Batch:  4/15	Loss 0.5513 (0.4104)
2022-11-09 18:00:55,533:INFO: Dataset: univ                Batch:  5/15	Loss 0.3082 (0.3886)
2022-11-09 18:00:55,697:INFO: Dataset: univ                Batch:  6/15	Loss 0.4317 (0.3957)
2022-11-09 18:00:55,862:INFO: Dataset: univ                Batch:  7/15	Loss 0.3013 (0.3814)
2022-11-09 18:00:56,026:INFO: Dataset: univ                Batch:  8/15	Loss 0.3450 (0.3769)
2022-11-09 18:00:56,191:INFO: Dataset: univ                Batch:  9/15	Loss 0.6970 (0.4089)
2022-11-09 18:00:56,352:INFO: Dataset: univ                Batch: 10/15	Loss 0.8237 (0.4439)
2022-11-09 18:00:56,519:INFO: Dataset: univ                Batch: 11/15	Loss 0.3627 (0.4367)
2022-11-09 18:00:56,682:INFO: Dataset: univ                Batch: 12/15	Loss 0.5849 (0.4483)
2022-11-09 18:00:56,848:INFO: Dataset: univ                Batch: 13/15	Loss 0.3491 (0.4410)
2022-11-09 18:00:57,012:INFO: Dataset: univ                Batch: 14/15	Loss 0.3410 (0.4335)
2022-11-09 18:00:57,055:INFO: Dataset: univ                Batch: 15/15	Loss 0.1073 (0.4285)
2022-11-09 18:00:57,438:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5100 (0.5100)
2022-11-09 18:00:57,598:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3850 (0.4430)
2022-11-09 18:00:57,757:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5203 (0.4674)
2022-11-09 18:00:57,913:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5268 (0.4825)
2022-11-09 18:00:58,073:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3128 (0.4494)
2022-11-09 18:00:58,232:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6130 (0.4775)
2022-11-09 18:00:58,391:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5463 (0.4880)
2022-11-09 18:00:58,527:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2848 (0.4648)
2022-11-09 18:00:58,925:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3275 (0.3275)
2022-11-09 18:00:59,087:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3911 (0.3587)
2022-11-09 18:00:59,257:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3494 (0.3557)
2022-11-09 18:00:59,416:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4287 (0.3741)
2022-11-09 18:00:59,580:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4205 (0.3828)
2022-11-09 18:00:59,742:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3687 (0.3804)
2022-11-09 18:00:59,902:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5731 (0.4065)
2022-11-09 18:01:00,062:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4035 (0.4061)
2022-11-09 18:01:00,223:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3505 (0.3999)
2022-11-09 18:01:00,381:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3549 (0.3955)
2022-11-09 18:01:00,543:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4660 (0.4022)
2022-11-09 18:01:00,703:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3088 (0.3937)
2022-11-09 18:01:00,865:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5198 (0.4033)
2022-11-09 18:01:01,026:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3335 (0.3982)
2022-11-09 18:01:01,189:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2969 (0.3910)
2022-11-09 18:01:01,349:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3823 (0.3904)
2022-11-09 18:01:01,512:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3456 (0.3875)
2022-11-09 18:01:01,652:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2679 (0.3819)
2022-11-09 18:01:01,705:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_242.pth.tar
2022-11-09 18:01:01,705:INFO: 
===> EPOCH: 243 (P2)
2022-11-09 18:01:01,706:INFO: - Computing loss (training)
2022-11-09 18:01:02,049:INFO: Dataset: hotel               Batch: 1/4	Loss 6.6889 (6.6889)
2022-11-09 18:01:02,211:INFO: Dataset: hotel               Batch: 2/4	Loss 9.5520 (8.1416)
2022-11-09 18:01:02,445:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1032 (6.0574)
2022-11-09 18:01:02,549:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3316 (5.0150)
2022-11-09 18:01:02,942:INFO: Dataset: univ                Batch:  1/15	Loss 0.4499 (0.4499)
2022-11-09 18:01:03,109:INFO: Dataset: univ                Batch:  2/15	Loss 0.4314 (0.4410)
2022-11-09 18:01:03,277:INFO: Dataset: univ                Batch:  3/15	Loss 0.9201 (0.5948)
2022-11-09 18:01:03,439:INFO: Dataset: univ                Batch:  4/15	Loss 0.7780 (0.6415)
2022-11-09 18:01:03,615:INFO: Dataset: univ                Batch:  5/15	Loss 0.6153 (0.6364)
2022-11-09 18:01:03,779:INFO: Dataset: univ                Batch:  6/15	Loss 1.1546 (0.7237)
2022-11-09 18:01:03,944:INFO: Dataset: univ                Batch:  7/15	Loss 0.7805 (0.7314)
2022-11-09 18:01:04,107:INFO: Dataset: univ                Batch:  8/15	Loss 0.5259 (0.7066)
2022-11-09 18:01:04,272:INFO: Dataset: univ                Batch:  9/15	Loss 0.4613 (0.6786)
2022-11-09 18:01:04,434:INFO: Dataset: univ                Batch: 10/15	Loss 0.4177 (0.6499)
2022-11-09 18:01:04,604:INFO: Dataset: univ                Batch: 11/15	Loss 0.4561 (0.6316)
2022-11-09 18:01:04,766:INFO: Dataset: univ                Batch: 12/15	Loss 0.5366 (0.6234)
2022-11-09 18:01:04,933:INFO: Dataset: univ                Batch: 13/15	Loss 0.3496 (0.6019)
2022-11-09 18:01:05,097:INFO: Dataset: univ                Batch: 14/15	Loss 0.3485 (0.5836)
2022-11-09 18:01:05,140:INFO: Dataset: univ                Batch: 15/15	Loss 0.1362 (0.5785)
2022-11-09 18:01:05,539:INFO: Dataset: zara1               Batch: 1/8	Loss 3.0596 (3.0596)
2022-11-09 18:01:05,699:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0975 (2.5956)
2022-11-09 18:01:05,858:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0446 (2.1122)
2022-11-09 18:01:06,015:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8147 (1.8080)
2022-11-09 18:01:06,176:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5481 (1.5560)
2022-11-09 18:01:06,333:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3727 (1.3427)
2022-11-09 18:01:06,492:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8761 (1.2754)
2022-11-09 18:01:06,631:INFO: Dataset: zara1               Batch: 8/8	Loss 2.4134 (1.3844)
2022-11-09 18:01:07,049:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1625 (1.1625)
2022-11-09 18:01:07,215:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5487 (1.3510)
2022-11-09 18:01:07,379:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8721 (1.1805)
2022-11-09 18:01:07,544:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3983 (0.9808)
2022-11-09 18:01:07,714:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3486 (0.8418)
2022-11-09 18:01:07,887:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3766 (0.7632)
2022-11-09 18:01:08,057:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4848 (0.7207)
2022-11-09 18:01:08,220:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7178 (0.8515)
2022-11-09 18:01:08,384:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9925 (0.8659)
2022-11-09 18:01:08,547:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6995 (0.8507)
2022-11-09 18:01:08,713:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1343 (0.8781)
2022-11-09 18:01:08,878:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3653 (0.8370)
2022-11-09 18:01:09,057:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5387 (0.8127)
2022-11-09 18:01:09,231:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4632 (0.7876)
2022-11-09 18:01:09,407:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4813 (0.7691)
2022-11-09 18:01:09,578:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3867 (0.7447)
2022-11-09 18:01:09,751:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3943 (0.7243)
2022-11-09 18:01:09,897:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4622 (0.7119)
2022-11-09 18:01:09,955:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_243.pth.tar
2022-11-09 18:01:09,955:INFO: 
===> EPOCH: 244 (P2)
2022-11-09 18:01:09,956:INFO: - Computing loss (training)
2022-11-09 18:01:10,311:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6013 (1.6013)
2022-11-09 18:01:10,483:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2366 (1.9061)
2022-11-09 18:01:10,647:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6956 (1.8371)
2022-11-09 18:01:10,752:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4643 (1.5926)
2022-11-09 18:01:11,152:INFO: Dataset: univ                Batch:  1/15	Loss 0.4389 (0.4389)
2022-11-09 18:01:11,322:INFO: Dataset: univ                Batch:  2/15	Loss 0.3538 (0.3961)
2022-11-09 18:01:11,491:INFO: Dataset: univ                Batch:  3/15	Loss 0.5292 (0.4454)
2022-11-09 18:01:11,653:INFO: Dataset: univ                Batch:  4/15	Loss 0.3816 (0.4283)
2022-11-09 18:01:11,829:INFO: Dataset: univ                Batch:  5/15	Loss 0.3030 (0.4024)
2022-11-09 18:01:11,994:INFO: Dataset: univ                Batch:  6/15	Loss 0.8635 (0.4736)
2022-11-09 18:01:12,162:INFO: Dataset: univ                Batch:  7/15	Loss 0.3172 (0.4486)
2022-11-09 18:01:12,327:INFO: Dataset: univ                Batch:  8/15	Loss 0.3617 (0.4378)
2022-11-09 18:01:12,495:INFO: Dataset: univ                Batch:  9/15	Loss 0.7629 (0.4726)
2022-11-09 18:01:12,665:INFO: Dataset: univ                Batch: 10/15	Loss 1.1220 (0.5412)
2022-11-09 18:01:12,838:INFO: Dataset: univ                Batch: 11/15	Loss 0.3031 (0.5180)
2022-11-09 18:01:13,003:INFO: Dataset: univ                Batch: 12/15	Loss 0.3547 (0.5037)
2022-11-09 18:01:13,168:INFO: Dataset: univ                Batch: 13/15	Loss 0.5041 (0.5037)
2022-11-09 18:01:13,337:INFO: Dataset: univ                Batch: 14/15	Loss 1.0613 (0.5354)
2022-11-09 18:01:13,381:INFO: Dataset: univ                Batch: 15/15	Loss 0.0602 (0.5296)
2022-11-09 18:01:13,773:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3468 (0.3468)
2022-11-09 18:01:13,941:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6006 (0.4838)
2022-11-09 18:01:14,110:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5335 (0.5007)
2022-11-09 18:01:14,270:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4833 (0.4961)
2022-11-09 18:01:14,432:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3918 (0.4730)
2022-11-09 18:01:14,590:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4099 (0.4628)
2022-11-09 18:01:14,757:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3645 (0.4481)
2022-11-09 18:01:14,894:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3537 (0.4374)
2022-11-09 18:01:15,322:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3424 (0.3424)
2022-11-09 18:01:15,488:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3661 (0.3538)
2022-11-09 18:01:15,653:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3905 (0.3663)
2022-11-09 18:01:15,813:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3692 (0.3670)
2022-11-09 18:01:15,995:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3799 (0.3697)
2022-11-09 18:01:16,165:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3322 (0.3633)
2022-11-09 18:01:16,327:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3173 (0.3571)
2022-11-09 18:01:16,486:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4307 (0.3651)
2022-11-09 18:01:16,647:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3149 (0.3594)
2022-11-09 18:01:16,805:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3234 (0.3555)
2022-11-09 18:01:16,967:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3591 (0.3558)
2022-11-09 18:01:17,124:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4649 (0.3652)
2022-11-09 18:01:17,287:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3142 (0.3611)
2022-11-09 18:01:17,452:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3299 (0.3591)
2022-11-09 18:01:17,614:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3087 (0.3557)
2022-11-09 18:01:17,781:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2981 (0.3524)
2022-11-09 18:01:17,946:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2878 (0.3482)
2022-11-09 18:01:18,085:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2611 (0.3437)
2022-11-09 18:01:18,140:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_244.pth.tar
2022-11-09 18:01:18,140:INFO: 
===> EPOCH: 245 (P2)
2022-11-09 18:01:18,140:INFO: - Computing loss (training)
2022-11-09 18:01:18,505:INFO: Dataset: hotel               Batch: 1/4	Loss 2.8062 (2.8062)
2022-11-09 18:01:18,668:INFO: Dataset: hotel               Batch: 2/4	Loss 2.8754 (2.8400)
2022-11-09 18:01:18,829:INFO: Dataset: hotel               Batch: 3/4	Loss 4.5256 (3.4081)
2022-11-09 18:01:18,930:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4516 (2.9283)
2022-11-09 18:01:19,355:INFO: Dataset: univ                Batch:  1/15	Loss 0.3315 (0.3315)
2022-11-09 18:01:19,524:INFO: Dataset: univ                Batch:  2/15	Loss 0.4504 (0.3860)
2022-11-09 18:01:19,690:INFO: Dataset: univ                Batch:  3/15	Loss 0.5297 (0.4353)
2022-11-09 18:01:19,859:INFO: Dataset: univ                Batch:  4/15	Loss 0.3846 (0.4234)
2022-11-09 18:01:20,040:INFO: Dataset: univ                Batch:  5/15	Loss 1.4646 (0.6439)
2022-11-09 18:01:20,222:INFO: Dataset: univ                Batch:  6/15	Loss 0.4303 (0.6104)
2022-11-09 18:01:20,447:INFO: Dataset: univ                Batch:  7/15	Loss 0.7119 (0.6255)
2022-11-09 18:01:20,655:INFO: Dataset: univ                Batch:  8/15	Loss 0.6601 (0.6302)
2022-11-09 18:01:20,859:INFO: Dataset: univ                Batch:  9/15	Loss 0.4086 (0.6039)
2022-11-09 18:01:21,031:INFO: Dataset: univ                Batch: 10/15	Loss 0.3788 (0.5800)
2022-11-09 18:01:21,201:INFO: Dataset: univ                Batch: 11/15	Loss 0.4184 (0.5649)
2022-11-09 18:01:21,367:INFO: Dataset: univ                Batch: 12/15	Loss 0.3990 (0.5515)
2022-11-09 18:01:21,534:INFO: Dataset: univ                Batch: 13/15	Loss 0.3612 (0.5364)
2022-11-09 18:01:21,698:INFO: Dataset: univ                Batch: 14/15	Loss 0.3515 (0.5241)
2022-11-09 18:01:21,741:INFO: Dataset: univ                Batch: 15/15	Loss 0.0597 (0.5182)
2022-11-09 18:01:22,145:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0709 (1.0709)
2022-11-09 18:01:22,315:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9001 (1.5051)
2022-11-09 18:01:22,479:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7108 (1.2480)
2022-11-09 18:01:22,641:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7225 (1.1032)
2022-11-09 18:01:22,803:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3539 (0.9458)
2022-11-09 18:01:22,967:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1214 (0.9756)
2022-11-09 18:01:23,151:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2439 (1.0153)
2022-11-09 18:01:23,303:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7391 (0.9865)
2022-11-09 18:01:23,733:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4412 (0.4412)
2022-11-09 18:01:23,900:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4511 (0.4465)
2022-11-09 18:01:24,069:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3703 (0.4230)
2022-11-09 18:01:24,235:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3369 (0.4032)
2022-11-09 18:01:24,400:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3599 (0.3938)
2022-11-09 18:01:24,570:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4301 (0.4009)
2022-11-09 18:01:24,729:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1790 (0.5008)
2022-11-09 18:01:24,890:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4730 (0.4972)
2022-11-09 18:01:25,050:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3233 (0.4784)
2022-11-09 18:01:25,215:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3153 (0.4624)
2022-11-09 18:01:25,438:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2965 (0.4467)
2022-11-09 18:01:25,613:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3672 (0.4402)
2022-11-09 18:01:25,791:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2963 (0.4286)
2022-11-09 18:01:25,969:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3179 (0.4202)
2022-11-09 18:01:26,147:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3383 (0.4151)
2022-11-09 18:01:26,328:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3163 (0.4094)
2022-11-09 18:01:26,498:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3754 (0.4075)
2022-11-09 18:01:26,651:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2700 (0.4015)
2022-11-09 18:01:26,708:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_245.pth.tar
2022-11-09 18:01:26,708:INFO: 
===> EPOCH: 246 (P2)
2022-11-09 18:01:26,708:INFO: - Computing loss (training)
2022-11-09 18:01:27,052:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2318 (1.2318)
2022-11-09 18:01:27,223:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7961 (1.0196)
2022-11-09 18:01:27,384:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0326 (1.0239)
2022-11-09 18:01:27,486:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2385 (0.8902)
2022-11-09 18:01:27,922:INFO: Dataset: univ                Batch:  1/15	Loss 0.6210 (0.6210)
2022-11-09 18:01:28,118:INFO: Dataset: univ                Batch:  2/15	Loss 0.6070 (0.6136)
2022-11-09 18:01:28,340:INFO: Dataset: univ                Batch:  3/15	Loss 0.5764 (0.6010)
2022-11-09 18:01:28,562:INFO: Dataset: univ                Batch:  4/15	Loss 0.3565 (0.5388)
2022-11-09 18:01:28,755:INFO: Dataset: univ                Batch:  5/15	Loss 0.3213 (0.4951)
2022-11-09 18:01:28,948:INFO: Dataset: univ                Batch:  6/15	Loss 0.3203 (0.4624)
2022-11-09 18:01:29,111:INFO: Dataset: univ                Batch:  7/15	Loss 0.3106 (0.4411)
2022-11-09 18:01:29,276:INFO: Dataset: univ                Batch:  8/15	Loss 0.8421 (0.4888)
2022-11-09 18:01:29,446:INFO: Dataset: univ                Batch:  9/15	Loss 0.5346 (0.4940)
2022-11-09 18:01:29,622:INFO: Dataset: univ                Batch: 10/15	Loss 0.5969 (0.5043)
2022-11-09 18:01:29,789:INFO: Dataset: univ                Batch: 11/15	Loss 0.3423 (0.4897)
2022-11-09 18:01:29,960:INFO: Dataset: univ                Batch: 12/15	Loss 0.5044 (0.4910)
2022-11-09 18:01:30,122:INFO: Dataset: univ                Batch: 13/15	Loss 0.3314 (0.4785)
2022-11-09 18:01:30,286:INFO: Dataset: univ                Batch: 14/15	Loss 0.4022 (0.4732)
2022-11-09 18:01:30,327:INFO: Dataset: univ                Batch: 15/15	Loss 0.0652 (0.4681)
2022-11-09 18:01:30,740:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4123 (0.4123)
2022-11-09 18:01:30,913:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4128 (0.4125)
2022-11-09 18:01:31,071:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3973 (0.4076)
2022-11-09 18:01:31,229:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3437 (0.3916)
2022-11-09 18:01:31,388:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5922 (0.4322)
2022-11-09 18:01:31,545:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3031 (0.5849)
2022-11-09 18:01:31,703:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4080 (0.5583)
2022-11-09 18:01:31,838:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3841 (0.5392)
2022-11-09 18:01:32,245:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3187 (0.3187)
2022-11-09 18:01:32,406:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5727 (0.4381)
2022-11-09 18:01:32,572:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3663 (0.4146)
2022-11-09 18:01:32,738:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4885 (0.4337)
2022-11-09 18:01:32,902:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2838 (0.4069)
2022-11-09 18:01:33,069:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3475 (0.3967)
2022-11-09 18:01:33,234:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3125 (0.3845)
2022-11-09 18:01:33,393:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3157 (0.3762)
2022-11-09 18:01:33,552:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3204 (0.3706)
2022-11-09 18:01:33,723:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3340 (0.3666)
2022-11-09 18:01:33,901:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3203 (0.3625)
2022-11-09 18:01:34,072:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3701 (0.3631)
2022-11-09 18:01:34,234:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3678 (0.3635)
2022-11-09 18:01:34,402:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3081 (0.3597)
2022-11-09 18:01:34,582:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3506 (0.3591)
2022-11-09 18:01:34,750:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3072 (0.3559)
2022-11-09 18:01:34,913:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3046 (0.3528)
2022-11-09 18:01:35,057:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2727 (0.3489)
2022-11-09 18:01:35,111:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_246.pth.tar
2022-11-09 18:01:35,112:INFO: 
===> EPOCH: 247 (P2)
2022-11-09 18:01:35,112:INFO: - Computing loss (training)
2022-11-09 18:01:35,485:INFO: Dataset: hotel               Batch: 1/4	Loss 3.4528 (3.4528)
2022-11-09 18:01:35,649:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5573 (1.9908)
2022-11-09 18:01:35,805:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5011 (1.8197)
2022-11-09 18:01:35,912:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3285 (1.5561)
2022-11-09 18:01:36,371:INFO: Dataset: univ                Batch:  1/15	Loss 0.3387 (0.3387)
2022-11-09 18:01:36,533:INFO: Dataset: univ                Batch:  2/15	Loss 0.3532 (0.3458)
2022-11-09 18:01:36,695:INFO: Dataset: univ                Batch:  3/15	Loss 0.4206 (0.3709)
2022-11-09 18:01:36,870:INFO: Dataset: univ                Batch:  4/15	Loss 0.5890 (0.4328)
2022-11-09 18:01:37,052:INFO: Dataset: univ                Batch:  5/15	Loss 0.5606 (0.4586)
2022-11-09 18:01:37,239:INFO: Dataset: univ                Batch:  6/15	Loss 0.8890 (0.5340)
2022-11-09 18:01:37,420:INFO: Dataset: univ                Batch:  7/15	Loss 0.5998 (0.5440)
2022-11-09 18:01:37,606:INFO: Dataset: univ                Batch:  8/15	Loss 0.5618 (0.5463)
2022-11-09 18:01:37,791:INFO: Dataset: univ                Batch:  9/15	Loss 0.4101 (0.5315)
2022-11-09 18:01:37,965:INFO: Dataset: univ                Batch: 10/15	Loss 0.4079 (0.5193)
2022-11-09 18:01:38,151:INFO: Dataset: univ                Batch: 11/15	Loss 0.6987 (0.5357)
2022-11-09 18:01:38,323:INFO: Dataset: univ                Batch: 12/15	Loss 0.3560 (0.5197)
2022-11-09 18:01:38,496:INFO: Dataset: univ                Batch: 13/15	Loss 0.4328 (0.5128)
2022-11-09 18:01:38,673:INFO: Dataset: univ                Batch: 14/15	Loss 0.3018 (0.4977)
2022-11-09 18:01:38,717:INFO: Dataset: univ                Batch: 15/15	Loss 0.0505 (0.4924)
2022-11-09 18:01:39,119:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2190 (1.2190)
2022-11-09 18:01:39,301:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4420 (1.3288)
2022-11-09 18:01:39,476:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1043 (1.2606)
2022-11-09 18:01:39,650:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4489 (1.0516)
2022-11-09 18:01:39,816:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3394 (0.9248)
2022-11-09 18:01:39,995:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4872 (0.8551)
2022-11-09 18:01:40,174:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3842 (0.7875)
2022-11-09 18:01:40,338:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5746 (0.7643)
2022-11-09 18:01:40,797:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7458 (0.7458)
2022-11-09 18:01:40,971:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6429 (0.6935)
2022-11-09 18:01:41,148:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5737 (0.6555)
2022-11-09 18:01:41,420:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5185 (0.6212)
2022-11-09 18:01:41,588:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3959 (0.5747)
2022-11-09 18:01:41,757:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4887 (0.5593)
2022-11-09 18:01:41,921:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3282 (0.5268)
2022-11-09 18:01:42,090:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5270 (0.5268)
2022-11-09 18:01:42,258:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3164 (0.5061)
2022-11-09 18:01:42,425:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4807 (0.5037)
2022-11-09 18:01:42,586:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3354 (0.4885)
2022-11-09 18:01:42,749:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3485 (0.4775)
2022-11-09 18:01:42,913:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3218 (0.4675)
2022-11-09 18:01:43,082:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3205 (0.4570)
2022-11-09 18:01:43,244:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3458 (0.4496)
2022-11-09 18:01:43,410:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3016 (0.4394)
2022-11-09 18:01:43,576:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3876 (0.4363)
2022-11-09 18:01:43,719:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3867 (0.4340)
2022-11-09 18:01:43,776:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_247.pth.tar
2022-11-09 18:01:43,776:INFO: 
===> EPOCH: 248 (P2)
2022-11-09 18:01:43,777:INFO: - Computing loss (training)
2022-11-09 18:01:44,123:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5331 (0.5331)
2022-11-09 18:01:44,295:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9416 (0.7374)
2022-11-09 18:01:44,457:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4429 (0.6395)
2022-11-09 18:01:44,568:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2408 (0.5685)
2022-11-09 18:01:45,181:INFO: Dataset: univ                Batch:  1/15	Loss 0.2942 (0.2942)
2022-11-09 18:01:45,364:INFO: Dataset: univ                Batch:  2/15	Loss 0.3334 (0.3129)
2022-11-09 18:01:45,575:INFO: Dataset: univ                Batch:  3/15	Loss 0.3192 (0.3150)
2022-11-09 18:01:45,757:INFO: Dataset: univ                Batch:  4/15	Loss 0.3517 (0.3243)
2022-11-09 18:01:45,947:INFO: Dataset: univ                Batch:  5/15	Loss 0.3645 (0.3330)
2022-11-09 18:01:46,130:INFO: Dataset: univ                Batch:  6/15	Loss 0.3530 (0.3364)
2022-11-09 18:01:46,314:INFO: Dataset: univ                Batch:  7/15	Loss 0.3946 (0.3455)
2022-11-09 18:01:46,486:INFO: Dataset: univ                Batch:  8/15	Loss 0.3239 (0.3428)
2022-11-09 18:01:46,657:INFO: Dataset: univ                Batch:  9/15	Loss 0.4110 (0.3508)
2022-11-09 18:01:46,829:INFO: Dataset: univ                Batch: 10/15	Loss 0.4109 (0.3565)
2022-11-09 18:01:47,006:INFO: Dataset: univ                Batch: 11/15	Loss 0.2837 (0.3497)
2022-11-09 18:01:47,175:INFO: Dataset: univ                Batch: 12/15	Loss 0.5421 (0.3640)
2022-11-09 18:01:47,347:INFO: Dataset: univ                Batch: 13/15	Loss 0.3202 (0.3606)
2022-11-09 18:01:47,517:INFO: Dataset: univ                Batch: 14/15	Loss 0.2938 (0.3557)
2022-11-09 18:01:47,562:INFO: Dataset: univ                Batch: 15/15	Loss 0.0553 (0.3521)
2022-11-09 18:01:47,970:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6837 (0.6837)
2022-11-09 18:01:48,135:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6642 (0.6742)
2022-11-09 18:01:48,296:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4402 (0.5958)
2022-11-09 18:01:48,461:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5110 (0.5744)
2022-11-09 18:01:48,633:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3281 (0.5228)
2022-11-09 18:01:48,811:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5705 (0.5307)
2022-11-09 18:01:48,978:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9569 (0.5877)
2022-11-09 18:01:49,137:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6160 (0.5906)
2022-11-09 18:01:49,594:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7209 (0.7209)
2022-11-09 18:01:49,771:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3077 (0.5089)
2022-11-09 18:01:49,942:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3144 (0.4430)
2022-11-09 18:01:50,114:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3189 (0.4153)
2022-11-09 18:01:50,298:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2972 (0.3932)
2022-11-09 18:01:50,481:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3176 (0.3801)
2022-11-09 18:01:50,647:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4241 (0.3867)
2022-11-09 18:01:50,833:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3155 (0.3782)
2022-11-09 18:01:51,009:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3900 (0.3796)
2022-11-09 18:01:51,174:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3488 (0.3765)
2022-11-09 18:01:51,364:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4506 (0.3833)
2022-11-09 18:01:51,535:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3041 (0.3761)
2022-11-09 18:01:51,699:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3749 (0.3760)
2022-11-09 18:01:51,873:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5267 (0.3862)
2022-11-09 18:01:52,041:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3091 (0.3804)
2022-11-09 18:01:52,206:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2541 (0.3725)
2022-11-09 18:01:52,383:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3269 (0.3699)
2022-11-09 18:01:52,529:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2982 (0.3661)
2022-11-09 18:01:52,593:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_248.pth.tar
2022-11-09 18:01:52,593:INFO: 
===> EPOCH: 249 (P2)
2022-11-09 18:01:52,593:INFO: - Computing loss (training)
2022-11-09 18:01:52,947:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5861 (0.5861)
2022-11-09 18:01:53,108:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4866 (1.0514)
2022-11-09 18:01:53,266:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2203 (1.1071)
2022-11-09 18:01:53,365:INFO: Dataset: hotel               Batch: 4/4	Loss 0.1926 (0.9491)
2022-11-09 18:01:53,751:INFO: Dataset: univ                Batch:  1/15	Loss 0.7215 (0.7215)
2022-11-09 18:01:53,918:INFO: Dataset: univ                Batch:  2/15	Loss 0.6257 (0.6754)
2022-11-09 18:01:54,084:INFO: Dataset: univ                Batch:  3/15	Loss 0.6509 (0.6677)
2022-11-09 18:01:54,249:INFO: Dataset: univ                Batch:  4/15	Loss 0.4564 (0.6213)
2022-11-09 18:01:54,414:INFO: Dataset: univ                Batch:  5/15	Loss 0.4782 (0.5930)
2022-11-09 18:01:54,575:INFO: Dataset: univ                Batch:  6/15	Loss 0.3247 (0.5464)
2022-11-09 18:01:54,738:INFO: Dataset: univ                Batch:  7/15	Loss 0.2905 (0.5103)
2022-11-09 18:01:54,903:INFO: Dataset: univ                Batch:  8/15	Loss 0.2960 (0.4810)
2022-11-09 18:01:55,070:INFO: Dataset: univ                Batch:  9/15	Loss 0.3751 (0.4692)
2022-11-09 18:01:55,233:INFO: Dataset: univ                Batch: 10/15	Loss 0.3733 (0.4595)
2022-11-09 18:01:55,401:INFO: Dataset: univ                Batch: 11/15	Loss 0.2779 (0.4413)
2022-11-09 18:01:55,562:INFO: Dataset: univ                Batch: 12/15	Loss 0.4820 (0.4450)
2022-11-09 18:01:55,728:INFO: Dataset: univ                Batch: 13/15	Loss 0.3006 (0.4335)
2022-11-09 18:01:55,891:INFO: Dataset: univ                Batch: 14/15	Loss 0.3309 (0.4257)
2022-11-09 18:01:55,932:INFO: Dataset: univ                Batch: 15/15	Loss 0.0700 (0.4204)
2022-11-09 18:01:56,328:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5596 (1.5596)
2022-11-09 18:01:56,492:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1251 (1.3581)
2022-11-09 18:01:56,652:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6878 (1.1383)
2022-11-09 18:01:56,809:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4466 (0.9805)
2022-11-09 18:01:56,968:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4686 (0.8744)
2022-11-09 18:01:57,128:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7486 (0.8554)
2022-11-09 18:01:57,287:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8059 (0.8476)
2022-11-09 18:01:57,423:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6434 (0.8252)
2022-11-09 18:01:57,826:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4809 (0.4809)
2022-11-09 18:01:57,991:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5422 (0.5095)
2022-11-09 18:01:58,151:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2951 (0.4349)
2022-11-09 18:01:58,307:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4456 (0.4375)
2022-11-09 18:01:58,469:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3316 (0.4163)
2022-11-09 18:01:58,627:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3735 (0.4088)
2022-11-09 18:01:58,786:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3294 (0.3963)
2022-11-09 18:01:58,941:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3491 (0.3904)
2022-11-09 18:01:59,100:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4527 (0.3971)
2022-11-09 18:01:59,256:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2917 (0.3866)
2022-11-09 18:01:59,417:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2928 (0.3781)
2022-11-09 18:01:59,573:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4953 (0.3878)
2022-11-09 18:01:59,732:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3311 (0.3835)
2022-11-09 18:01:59,889:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3792 (0.3832)
2022-11-09 18:02:00,050:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3366 (0.3800)
2022-11-09 18:02:00,207:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4075 (0.3817)
2022-11-09 18:02:00,367:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3738 (0.3813)
2022-11-09 18:02:00,506:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3190 (0.3784)
2022-11-09 18:02:00,560:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_249.pth.tar
2022-11-09 18:02:00,560:INFO: 
===> EPOCH: 250 (P2)
2022-11-09 18:02:00,561:INFO: - Computing loss (training)
2022-11-09 18:02:00,921:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0812 (1.0812)
2022-11-09 18:02:01,085:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6638 (0.8626)
2022-11-09 18:02:01,248:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5126 (0.7467)
2022-11-09 18:02:01,349:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2439 (0.6625)
2022-11-09 18:02:01,744:INFO: Dataset: univ                Batch:  1/15	Loss 0.6900 (0.6900)
2022-11-09 18:02:01,912:INFO: Dataset: univ                Batch:  2/15	Loss 0.4212 (0.5589)
2022-11-09 18:02:02,075:INFO: Dataset: univ                Batch:  3/15	Loss 0.3076 (0.4843)
2022-11-09 18:02:02,236:INFO: Dataset: univ                Batch:  4/15	Loss 0.3152 (0.4460)
2022-11-09 18:02:02,399:INFO: Dataset: univ                Batch:  5/15	Loss 0.3805 (0.4334)
2022-11-09 18:02:02,563:INFO: Dataset: univ                Batch:  6/15	Loss 0.3101 (0.4135)
2022-11-09 18:02:02,727:INFO: Dataset: univ                Batch:  7/15	Loss 0.2970 (0.3957)
2022-11-09 18:02:02,888:INFO: Dataset: univ                Batch:  8/15	Loss 0.2992 (0.3831)
2022-11-09 18:02:03,051:INFO: Dataset: univ                Batch:  9/15	Loss 0.3136 (0.3756)
2022-11-09 18:02:03,212:INFO: Dataset: univ                Batch: 10/15	Loss 0.3908 (0.3770)
2022-11-09 18:02:03,379:INFO: Dataset: univ                Batch: 11/15	Loss 0.3197 (0.3716)
2022-11-09 18:02:03,542:INFO: Dataset: univ                Batch: 12/15	Loss 0.7591 (0.4026)
2022-11-09 18:02:03,705:INFO: Dataset: univ                Batch: 13/15	Loss 0.4942 (0.4095)
2022-11-09 18:02:03,868:INFO: Dataset: univ                Batch: 14/15	Loss 0.4548 (0.4127)
2022-11-09 18:02:03,909:INFO: Dataset: univ                Batch: 15/15	Loss 0.0554 (0.4085)
2022-11-09 18:02:04,310:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3683 (0.3683)
2022-11-09 18:02:04,474:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3961 (0.3827)
2022-11-09 18:02:04,635:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3913 (0.3861)
2022-11-09 18:02:04,793:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3288 (0.3703)
2022-11-09 18:02:04,955:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3668 (0.3696)
2022-11-09 18:02:05,116:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5574 (0.4006)
2022-11-09 18:02:05,277:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3833 (0.3984)
2022-11-09 18:02:05,415:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2480 (0.3830)
2022-11-09 18:02:05,816:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2783 (0.2783)
2022-11-09 18:02:05,978:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3185 (0.2994)
2022-11-09 18:02:06,142:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3127 (0.3034)
2022-11-09 18:02:06,302:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2698 (0.2943)
2022-11-09 18:02:06,462:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4300 (0.3205)
2022-11-09 18:02:06,623:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2953 (0.3161)
2022-11-09 18:02:06,783:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3212 (0.3168)
2022-11-09 18:02:06,940:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4138 (0.3299)
2022-11-09 18:02:07,101:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2842 (0.3244)
2022-11-09 18:02:07,261:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2868 (0.3212)
2022-11-09 18:02:07,422:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2838 (0.3176)
2022-11-09 18:02:07,582:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4150 (0.3264)
2022-11-09 18:02:07,743:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2850 (0.3234)
2022-11-09 18:02:07,903:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2735 (0.3200)
2022-11-09 18:02:08,066:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3032 (0.3189)
2022-11-09 18:02:08,225:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2963 (0.3175)
2022-11-09 18:02:08,388:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2853 (0.3154)
2022-11-09 18:02:08,528:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2655 (0.3129)
2022-11-09 18:02:08,584:INFO:  --> Model Saved in ./models/E4//P2/CRMF_epoch_250.pth.tar
2022-11-09 18:02:08,584:INFO: 
===> EPOCH: 251 (P3)
2022-11-09 18:02:08,584:INFO: - Computing loss (training)
2022-11-09 18:02:09,576:INFO: Dataset: hotel               Batch: 1/4	Loss 123.7895 (123.7895)
2022-11-09 18:02:10,291:INFO: Dataset: hotel               Batch: 2/4	Loss 122.4681 (123.1288)
2022-11-09 18:02:10,993:INFO: Dataset: hotel               Batch: 3/4	Loss 123.2765 (123.1762)
2022-11-09 18:02:11,657:INFO: Dataset: hotel               Batch: 4/4	Loss 74.3817 (115.7089)
2022-11-09 18:02:12,693:INFO: Dataset: univ                Batch:  1/15	Loss 123.5713 (123.5713)
2022-11-09 18:02:13,544:INFO: Dataset: univ                Batch:  2/15	Loss 123.3419 (123.4596)
2022-11-09 18:02:14,284:INFO: Dataset: univ                Batch:  3/15	Loss 122.9117 (123.2721)
2022-11-09 18:02:15,018:INFO: Dataset: univ                Batch:  4/15	Loss 121.4189 (122.8407)
2022-11-09 18:02:15,764:INFO: Dataset: univ                Batch:  5/15	Loss 119.1471 (122.0769)
2022-11-09 18:02:16,510:INFO: Dataset: univ                Batch:  6/15	Loss 118.7149 (121.4812)
2022-11-09 18:02:17,252:INFO: Dataset: univ                Batch:  7/15	Loss 119.9588 (121.2597)
2022-11-09 18:02:18,011:INFO: Dataset: univ                Batch:  8/15	Loss 115.5648 (120.4506)
2022-11-09 18:02:18,756:INFO: Dataset: univ                Batch:  9/15	Loss 116.9762 (120.0595)
2022-11-09 18:02:19,495:INFO: Dataset: univ                Batch: 10/15	Loss 116.5839 (119.7331)
2022-11-09 18:02:20,249:INFO: Dataset: univ                Batch: 11/15	Loss 114.1056 (119.2079)
2022-11-09 18:02:21,001:INFO: Dataset: univ                Batch: 12/15	Loss 112.5447 (118.5994)
2022-11-09 18:02:21,752:INFO: Dataset: univ                Batch: 13/15	Loss 112.6354 (118.1144)
2022-11-09 18:02:22,499:INFO: Dataset: univ                Batch: 14/15	Loss 111.3521 (117.6645)
2022-11-09 18:02:23,158:INFO: Dataset: univ                Batch: 15/15	Loss 20.3975 (116.3416)
2022-11-09 18:02:24,167:INFO: Dataset: zara1               Batch: 1/8	Loss 115.1993 (115.1993)
2022-11-09 18:02:24,898:INFO: Dataset: zara1               Batch: 2/8	Loss 114.9761 (115.0882)
2022-11-09 18:02:25,630:INFO: Dataset: zara1               Batch: 3/8	Loss 113.6380 (114.6214)
2022-11-09 18:02:26,362:INFO: Dataset: zara1               Batch: 4/8	Loss 112.2390 (114.0144)
2022-11-09 18:02:27,094:INFO: Dataset: zara1               Batch: 5/8	Loss 109.8060 (113.1316)
2022-11-09 18:02:27,826:INFO: Dataset: zara1               Batch: 6/8	Loss 108.5767 (112.3418)
2022-11-09 18:02:28,564:INFO: Dataset: zara1               Batch: 7/8	Loss 109.4524 (111.8802)
2022-11-09 18:02:29,290:INFO: Dataset: zara1               Batch: 8/8	Loss 91.6276 (109.8443)
2022-11-09 18:02:30,323:INFO: Dataset: zara2               Batch:  1/18	Loss 103.3740 (103.3740)
2022-11-09 18:02:31,217:INFO: Dataset: zara2               Batch:  2/18	Loss 102.8278 (103.1070)
2022-11-09 18:02:31,978:INFO: Dataset: zara2               Batch:  3/18	Loss 100.9289 (102.3274)
2022-11-09 18:02:32,784:INFO: Dataset: zara2               Batch:  4/18	Loss 100.0067 (101.7580)
2022-11-09 18:02:33,533:INFO: Dataset: zara2               Batch:  5/18	Loss 99.9230 (101.3730)
2022-11-09 18:02:34,290:INFO: Dataset: zara2               Batch:  6/18	Loss 99.4457 (101.0018)
2022-11-09 18:02:35,037:INFO: Dataset: zara2               Batch:  7/18	Loss 100.5164 (100.9294)
2022-11-09 18:02:35,804:INFO: Dataset: zara2               Batch:  8/18	Loss 99.0127 (100.7060)
2022-11-09 18:02:36,572:INFO: Dataset: zara2               Batch:  9/18	Loss 98.7337 (100.4684)
2022-11-09 18:02:37,337:INFO: Dataset: zara2               Batch: 10/18	Loss 97.7124 (100.1707)
2022-11-09 18:02:38,218:INFO: Dataset: zara2               Batch: 11/18	Loss 98.9602 (100.0634)
2022-11-09 18:02:39,033:INFO: Dataset: zara2               Batch: 12/18	Loss 96.1053 (99.7544)
2022-11-09 18:02:39,827:INFO: Dataset: zara2               Batch: 13/18	Loss 97.3868 (99.5800)
2022-11-09 18:02:40,884:INFO: Dataset: zara2               Batch: 14/18	Loss 97.2612 (99.4228)
2022-11-09 18:02:41,647:INFO: Dataset: zara2               Batch: 15/18	Loss 95.6221 (99.1555)
2022-11-09 18:02:42,399:INFO: Dataset: zara2               Batch: 16/18	Loss 95.1335 (98.9079)
2022-11-09 18:02:43,155:INFO: Dataset: zara2               Batch: 17/18	Loss 97.2823 (98.8181)
2022-11-09 18:02:43,893:INFO: Dataset: zara2               Batch: 18/18	Loss 81.9216 (97.9606)
2022-11-09 18:02:43,974:INFO: - Computing ADE (validation o)
2022-11-09 18:02:44,245:INFO: 		 ADE on eth                       dataset:	 3.46062970161438
2022-11-09 18:02:44,245:INFO: Average validation o:	ADE  3.4606	FDE  5.8514
2022-11-09 18:02:44,246:INFO: - Computing ADE (validation)
2022-11-09 18:02:44,510:INFO: 		 ADE on hotel                     dataset:	 0.9503917098045349
2022-11-09 18:02:44,823:INFO: 		 ADE on univ                      dataset:	 1.5251153707504272
2022-11-09 18:02:45,089:INFO: 		 ADE on zara1                     dataset:	 2.5703125
2022-11-09 18:02:45,466:INFO: 		 ADE on zara2                     dataset:	 1.4350123405456543
2022-11-09 18:02:45,466:INFO: Average validation:	ADE  1.5214	FDE  2.7608
2022-11-09 18:02:45,467:INFO: - Computing ADE (training)
2022-11-09 18:02:45,814:INFO: 		 ADE on hotel                     dataset:	 1.3164350986480713
2022-11-09 18:02:46,435:INFO: 		 ADE on univ                      dataset:	 1.3986986875534058
2022-11-09 18:02:46,885:INFO: 		 ADE on zara1                     dataset:	 2.4745264053344727
2022-11-09 18:02:47,592:INFO: 		 ADE on zara2                     dataset:	 1.6940151453018188
2022-11-09 18:02:47,592:INFO: Average training:	ADE  1.5251	FDE  2.7742
2022-11-09 18:02:47,602:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_251.pth.tar
2022-11-09 18:02:47,602:INFO: 
===> EPOCH: 252 (P3)
2022-11-09 18:02:47,602:INFO: - Computing loss (training)
2022-11-09 18:02:48,515:INFO: Dataset: hotel               Batch: 1/4	Loss 92.2193 (92.2193)
2022-11-09 18:02:49,242:INFO: Dataset: hotel               Batch: 2/4	Loss 92.1740 (92.1964)
2022-11-09 18:02:49,966:INFO: Dataset: hotel               Batch: 3/4	Loss 91.7234 (92.0398)
2022-11-09 18:02:50,650:INFO: Dataset: hotel               Batch: 4/4	Loss 54.6636 (85.1858)
2022-11-09 18:02:51,765:INFO: Dataset: univ                Batch:  1/15	Loss 90.9137 (90.9137)
2022-11-09 18:02:52,502:INFO: Dataset: univ                Batch:  2/15	Loss 93.1493 (91.9225)
2022-11-09 18:02:53,251:INFO: Dataset: univ                Batch:  3/15	Loss 91.1099 (91.6452)
2022-11-09 18:02:54,003:INFO: Dataset: univ                Batch:  4/15	Loss 90.3335 (91.3121)
2022-11-09 18:02:54,753:INFO: Dataset: univ                Batch:  5/15	Loss 90.0601 (91.0554)
2022-11-09 18:02:55,505:INFO: Dataset: univ                Batch:  6/15	Loss 89.8952 (90.8508)
2022-11-09 18:02:56,255:INFO: Dataset: univ                Batch:  7/15	Loss 88.1357 (90.4496)
2022-11-09 18:02:56,991:INFO: Dataset: univ                Batch:  8/15	Loss 90.2674 (90.4291)
2022-11-09 18:02:57,744:INFO: Dataset: univ                Batch:  9/15	Loss 88.0443 (90.1448)
2022-11-09 18:02:58,488:INFO: Dataset: univ                Batch: 10/15	Loss 90.5350 (90.1821)
2022-11-09 18:02:59,230:INFO: Dataset: univ                Batch: 11/15	Loss 89.4253 (90.1175)
2022-11-09 18:02:59,986:INFO: Dataset: univ                Batch: 12/15	Loss 87.1400 (89.8531)
2022-11-09 18:03:00,736:INFO: Dataset: univ                Batch: 13/15	Loss 88.3027 (89.7392)
2022-11-09 18:03:01,487:INFO: Dataset: univ                Batch: 14/15	Loss 89.6111 (89.7303)
2022-11-09 18:03:02,150:INFO: Dataset: univ                Batch: 15/15	Loss 17.1949 (88.8709)
2022-11-09 18:03:03,162:INFO: Dataset: zara1               Batch: 1/8	Loss 95.6334 (95.6334)
2022-11-09 18:03:03,904:INFO: Dataset: zara1               Batch: 2/8	Loss 94.5775 (95.0975)
2022-11-09 18:03:04,641:INFO: Dataset: zara1               Batch: 3/8	Loss 95.9844 (95.3763)
2022-11-09 18:03:05,376:INFO: Dataset: zara1               Batch: 4/8	Loss 94.0995 (95.0904)
2022-11-09 18:03:06,109:INFO: Dataset: zara1               Batch: 5/8	Loss 93.9112 (94.8630)
2022-11-09 18:03:06,844:INFO: Dataset: zara1               Batch: 6/8	Loss 94.8504 (94.8609)
2022-11-09 18:03:07,578:INFO: Dataset: zara1               Batch: 7/8	Loss 92.9126 (94.5848)
2022-11-09 18:03:08,307:INFO: Dataset: zara1               Batch: 8/8	Loss 81.6390 (93.4197)
2022-11-09 18:03:09,429:INFO: Dataset: zara2               Batch:  1/18	Loss 89.9141 (89.9141)
2022-11-09 18:03:10,168:INFO: Dataset: zara2               Batch:  2/18	Loss 87.6552 (88.7466)
2022-11-09 18:03:10,913:INFO: Dataset: zara2               Batch:  3/18	Loss 86.7875 (88.0369)
2022-11-09 18:03:11,655:INFO: Dataset: zara2               Batch:  4/18	Loss 88.7891 (88.2257)
2022-11-09 18:03:12,407:INFO: Dataset: zara2               Batch:  5/18	Loss 88.4379 (88.2643)
2022-11-09 18:03:13,153:INFO: Dataset: zara2               Batch:  6/18	Loss 89.6353 (88.4837)
2022-11-09 18:03:13,899:INFO: Dataset: zara2               Batch:  7/18	Loss 88.4501 (88.4791)
2022-11-09 18:03:14,641:INFO: Dataset: zara2               Batch:  8/18	Loss 89.3320 (88.5931)
2022-11-09 18:03:15,386:INFO: Dataset: zara2               Batch:  9/18	Loss 88.1896 (88.5535)
2022-11-09 18:03:16,134:INFO: Dataset: zara2               Batch: 10/18	Loss 87.8138 (88.4869)
2022-11-09 18:03:16,880:INFO: Dataset: zara2               Batch: 11/18	Loss 88.1251 (88.4501)
2022-11-09 18:03:17,627:INFO: Dataset: zara2               Batch: 12/18	Loss 87.8665 (88.4075)
2022-11-09 18:03:18,372:INFO: Dataset: zara2               Batch: 13/18	Loss 87.0257 (88.2973)
2022-11-09 18:03:19,119:INFO: Dataset: zara2               Batch: 14/18	Loss 88.3932 (88.3041)
2022-11-09 18:03:19,861:INFO: Dataset: zara2               Batch: 15/18	Loss 88.5781 (88.3233)
2022-11-09 18:03:20,609:INFO: Dataset: zara2               Batch: 16/18	Loss 87.4178 (88.2687)
2022-11-09 18:03:21,355:INFO: Dataset: zara2               Batch: 17/18	Loss 87.4063 (88.2231)
2022-11-09 18:03:22,089:INFO: Dataset: zara2               Batch: 18/18	Loss 74.8799 (87.5790)
2022-11-09 18:03:22,158:INFO: - Computing ADE (validation o)
2022-11-09 18:03:22,437:INFO: 		 ADE on eth                       dataset:	 3.4429585933685303
2022-11-09 18:03:22,438:INFO: Average validation o:	ADE  3.4430	FDE  5.8231
2022-11-09 18:03:22,438:INFO: - Computing ADE (validation)
2022-11-09 18:03:22,717:INFO: 		 ADE on hotel                     dataset:	 0.9811068773269653
2022-11-09 18:03:23,036:INFO: 		 ADE on univ                      dataset:	 1.5339605808258057
2022-11-09 18:03:23,314:INFO: 		 ADE on zara1                     dataset:	 2.593473196029663
2022-11-09 18:03:23,678:INFO: 		 ADE on zara2                     dataset:	 1.4571127891540527
2022-11-09 18:03:23,678:INFO: Average validation:	ADE  1.5371	FDE  2.7721
2022-11-09 18:03:23,679:INFO: - Computing ADE (training)
2022-11-09 18:03:24,005:INFO: 		 ADE on hotel                     dataset:	 1.331990122795105
2022-11-09 18:03:24,619:INFO: 		 ADE on univ                      dataset:	 1.4119762182235718
2022-11-09 18:03:25,060:INFO: 		 ADE on zara1                     dataset:	 2.480069160461426
2022-11-09 18:03:25,775:INFO: 		 ADE on zara2                     dataset:	 1.7079803943634033
2022-11-09 18:03:25,775:INFO: Average training:	ADE  1.5381	FDE  2.7828
2022-11-09 18:03:25,786:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_252.pth.tar
2022-11-09 18:03:25,786:INFO: 
===> EPOCH: 253 (P3)
2022-11-09 18:03:25,786:INFO: - Computing loss (training)
2022-11-09 18:03:26,721:INFO: Dataset: hotel               Batch: 1/4	Loss 85.3513 (85.3513)
2022-11-09 18:03:27,447:INFO: Dataset: hotel               Batch: 2/4	Loss 83.3963 (84.3877)
2022-11-09 18:03:28,162:INFO: Dataset: hotel               Batch: 3/4	Loss 85.3051 (84.6876)
2022-11-09 18:03:28,921:INFO: Dataset: hotel               Batch: 4/4	Loss 51.1086 (78.8844)
2022-11-09 18:03:29,938:INFO: Dataset: univ                Batch:  1/15	Loss 86.9022 (86.9022)
2022-11-09 18:03:30,686:INFO: Dataset: univ                Batch:  2/15	Loss 86.2046 (86.5643)
2022-11-09 18:03:31,425:INFO: Dataset: univ                Batch:  3/15	Loss 85.4821 (86.2015)
2022-11-09 18:03:32,182:INFO: Dataset: univ                Batch:  4/15	Loss 84.5490 (85.7332)
2022-11-09 18:03:32,927:INFO: Dataset: univ                Batch:  5/15	Loss 83.2838 (85.2147)
2022-11-09 18:03:33,679:INFO: Dataset: univ                Batch:  6/15	Loss 85.1795 (85.2084)
2022-11-09 18:03:34,431:INFO: Dataset: univ                Batch:  7/15	Loss 83.2854 (84.9074)
2022-11-09 18:03:35,183:INFO: Dataset: univ                Batch:  8/15	Loss 84.7443 (84.8858)
2022-11-09 18:03:35,930:INFO: Dataset: univ                Batch:  9/15	Loss 85.6588 (84.9712)
2022-11-09 18:03:36,672:INFO: Dataset: univ                Batch: 10/15	Loss 86.0231 (85.0665)
2022-11-09 18:03:37,417:INFO: Dataset: univ                Batch: 11/15	Loss 86.1137 (85.1536)
2022-11-09 18:03:38,168:INFO: Dataset: univ                Batch: 12/15	Loss 85.8932 (85.2154)
2022-11-09 18:03:38,927:INFO: Dataset: univ                Batch: 13/15	Loss 84.1094 (85.1234)
2022-11-09 18:03:39,677:INFO: Dataset: univ                Batch: 14/15	Loss 86.3653 (85.2100)
2022-11-09 18:03:40,337:INFO: Dataset: univ                Batch: 15/15	Loss 15.3234 (84.1138)
2022-11-09 18:03:41,359:INFO: Dataset: zara1               Batch: 1/8	Loss 90.3168 (90.3168)
2022-11-09 18:03:42,094:INFO: Dataset: zara1               Batch: 2/8	Loss 91.4746 (90.8957)
2022-11-09 18:03:42,828:INFO: Dataset: zara1               Batch: 3/8	Loss 93.7044 (91.8588)
2022-11-09 18:03:43,562:INFO: Dataset: zara1               Batch: 4/8	Loss 92.0942 (91.9117)
2022-11-09 18:03:44,301:INFO: Dataset: zara1               Batch: 5/8	Loss 92.1703 (91.9639)
2022-11-09 18:03:45,038:INFO: Dataset: zara1               Batch: 6/8	Loss 92.8567 (92.0927)
2022-11-09 18:03:45,774:INFO: Dataset: zara1               Batch: 7/8	Loss 90.6055 (91.8730)
2022-11-09 18:03:46,576:INFO: Dataset: zara1               Batch: 8/8	Loss 79.8528 (90.6014)
2022-11-09 18:03:47,575:INFO: Dataset: zara2               Batch:  1/18	Loss 86.4572 (86.4572)
2022-11-09 18:03:48,315:INFO: Dataset: zara2               Batch:  2/18	Loss 85.8527 (86.1588)
2022-11-09 18:03:49,054:INFO: Dataset: zara2               Batch:  3/18	Loss 86.3169 (86.2135)
2022-11-09 18:03:49,798:INFO: Dataset: zara2               Batch:  4/18	Loss 89.0007 (86.8499)
2022-11-09 18:03:50,541:INFO: Dataset: zara2               Batch:  5/18	Loss 87.2452 (86.9275)
2022-11-09 18:03:51,278:INFO: Dataset: zara2               Batch:  6/18	Loss 86.1767 (86.7990)
2022-11-09 18:03:52,013:INFO: Dataset: zara2               Batch:  7/18	Loss 86.4666 (86.7514)
2022-11-09 18:03:52,752:INFO: Dataset: zara2               Batch:  8/18	Loss 85.1002 (86.5505)
2022-11-09 18:03:53,489:INFO: Dataset: zara2               Batch:  9/18	Loss 85.9442 (86.4847)
2022-11-09 18:03:54,231:INFO: Dataset: zara2               Batch: 10/18	Loss 87.4741 (86.5693)
2022-11-09 18:03:54,969:INFO: Dataset: zara2               Batch: 11/18	Loss 86.8752 (86.5992)
2022-11-09 18:03:55,710:INFO: Dataset: zara2               Batch: 12/18	Loss 86.7380 (86.6093)
2022-11-09 18:03:56,450:INFO: Dataset: zara2               Batch: 13/18	Loss 87.4413 (86.6731)
2022-11-09 18:03:57,192:INFO: Dataset: zara2               Batch: 14/18	Loss 88.0804 (86.7680)
2022-11-09 18:03:57,933:INFO: Dataset: zara2               Batch: 15/18	Loss 85.9460 (86.7144)
2022-11-09 18:03:58,670:INFO: Dataset: zara2               Batch: 16/18	Loss 85.1226 (86.6027)
2022-11-09 18:03:59,412:INFO: Dataset: zara2               Batch: 17/18	Loss 87.2860 (86.6404)
2022-11-09 18:04:00,142:INFO: Dataset: zara2               Batch: 18/18	Loss 73.6576 (86.0566)
2022-11-09 18:04:00,211:INFO: - Computing ADE (validation o)
2022-11-09 18:04:00,485:INFO: 		 ADE on eth                       dataset:	 3.4241299629211426
2022-11-09 18:04:00,486:INFO: Average validation o:	ADE  3.4241	FDE  5.7997
2022-11-09 18:04:00,486:INFO: - Computing ADE (validation)
2022-11-09 18:04:00,763:INFO: 		 ADE on hotel                     dataset:	 0.991839587688446
2022-11-09 18:04:01,082:INFO: 		 ADE on univ                      dataset:	 1.5501033067703247
2022-11-09 18:04:01,353:INFO: 		 ADE on zara1                     dataset:	 2.6259031295776367
2022-11-09 18:04:01,734:INFO: 		 ADE on zara2                     dataset:	 1.471224308013916
2022-11-09 18:04:01,734:INFO: Average validation:	ADE  1.5531	FDE  2.8133
2022-11-09 18:04:01,735:INFO: - Computing ADE (training)
2022-11-09 18:04:02,062:INFO: 		 ADE on hotel                     dataset:	 1.3595134019851685
2022-11-09 18:04:02,683:INFO: 		 ADE on univ                      dataset:	 1.4240212440490723
2022-11-09 18:04:03,118:INFO: 		 ADE on zara1                     dataset:	 2.4845798015594482
2022-11-09 18:04:03,802:INFO: 		 ADE on zara2                     dataset:	 1.7161202430725098
2022-11-09 18:04:03,803:INFO: Average training:	ADE  1.5493	FDE  2.8106
2022-11-09 18:04:03,813:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_253.pth.tar
2022-11-09 18:04:03,813:INFO: 
===> EPOCH: 254 (P3)
2022-11-09 18:04:03,813:INFO: - Computing loss (training)
2022-11-09 18:04:04,743:INFO: Dataset: hotel               Batch: 1/4	Loss 82.5745 (82.5745)
2022-11-09 18:04:05,482:INFO: Dataset: hotel               Batch: 2/4	Loss 85.4225 (84.0276)
2022-11-09 18:04:06,354:INFO: Dataset: hotel               Batch: 3/4	Loss 83.4182 (83.8060)
2022-11-09 18:04:07,090:INFO: Dataset: hotel               Batch: 4/4	Loss 50.2299 (77.5160)
2022-11-09 18:04:08,275:INFO: Dataset: univ                Batch:  1/15	Loss 84.5025 (84.5025)
2022-11-09 18:04:09,083:INFO: Dataset: univ                Batch:  2/15	Loss 83.1164 (83.8006)
2022-11-09 18:04:09,872:INFO: Dataset: univ                Batch:  3/15	Loss 86.7718 (84.7220)
2022-11-09 18:04:10,637:INFO: Dataset: univ                Batch:  4/15	Loss 84.3818 (84.6344)
2022-11-09 18:04:11,403:INFO: Dataset: univ                Batch:  5/15	Loss 83.1596 (84.3102)
2022-11-09 18:04:12,164:INFO: Dataset: univ                Batch:  6/15	Loss 85.7181 (84.5270)
2022-11-09 18:04:12,930:INFO: Dataset: univ                Batch:  7/15	Loss 84.6627 (84.5466)
2022-11-09 18:04:13,735:INFO: Dataset: univ                Batch:  8/15	Loss 83.6341 (84.4280)
2022-11-09 18:04:14,516:INFO: Dataset: univ                Batch:  9/15	Loss 83.3486 (84.3073)
2022-11-09 18:04:15,285:INFO: Dataset: univ                Batch: 10/15	Loss 83.4969 (84.2178)
2022-11-09 18:04:16,108:INFO: Dataset: univ                Batch: 11/15	Loss 85.0379 (84.2914)
2022-11-09 18:04:16,895:INFO: Dataset: univ                Batch: 12/15	Loss 85.6490 (84.4004)
2022-11-09 18:04:17,711:INFO: Dataset: univ                Batch: 13/15	Loss 85.4239 (84.4769)
2022-11-09 18:04:18,521:INFO: Dataset: univ                Batch: 14/15	Loss 83.5346 (84.4105)
2022-11-09 18:04:19,248:INFO: Dataset: univ                Batch: 15/15	Loss 16.7383 (83.7467)
2022-11-09 18:04:20,445:INFO: Dataset: zara1               Batch: 1/8	Loss 91.7091 (91.7091)
2022-11-09 18:04:21,271:INFO: Dataset: zara1               Batch: 2/8	Loss 90.3142 (91.0247)
2022-11-09 18:04:22,083:INFO: Dataset: zara1               Batch: 3/8	Loss 91.2959 (91.1129)
2022-11-09 18:04:22,890:INFO: Dataset: zara1               Batch: 4/8	Loss 92.3320 (91.4228)
2022-11-09 18:04:23,744:INFO: Dataset: zara1               Batch: 5/8	Loss 91.9926 (91.5476)
2022-11-09 18:04:24,664:INFO: Dataset: zara1               Batch: 6/8	Loss 90.5091 (91.3791)
2022-11-09 18:04:25,438:INFO: Dataset: zara1               Batch: 7/8	Loss 91.2299 (91.3563)
2022-11-09 18:04:26,192:INFO: Dataset: zara1               Batch: 8/8	Loss 79.2935 (90.2453)
2022-11-09 18:04:27,217:INFO: Dataset: zara2               Batch:  1/18	Loss 87.1446 (87.1446)
2022-11-09 18:04:27,982:INFO: Dataset: zara2               Batch:  2/18	Loss 85.5812 (86.4111)
2022-11-09 18:04:28,737:INFO: Dataset: zara2               Batch:  3/18	Loss 87.4283 (86.7605)
2022-11-09 18:04:29,505:INFO: Dataset: zara2               Batch:  4/18	Loss 86.9826 (86.8145)
2022-11-09 18:04:30,269:INFO: Dataset: zara2               Batch:  5/18	Loss 85.3572 (86.5110)
2022-11-09 18:04:31,012:INFO: Dataset: zara2               Batch:  6/18	Loss 86.2355 (86.4615)
2022-11-09 18:04:31,749:INFO: Dataset: zara2               Batch:  7/18	Loss 86.5720 (86.4776)
2022-11-09 18:04:32,487:INFO: Dataset: zara2               Batch:  8/18	Loss 85.1232 (86.3038)
2022-11-09 18:04:33,223:INFO: Dataset: zara2               Batch:  9/18	Loss 85.1410 (86.1726)
2022-11-09 18:04:33,962:INFO: Dataset: zara2               Batch: 10/18	Loss 86.3598 (86.1902)
2022-11-09 18:04:34,699:INFO: Dataset: zara2               Batch: 11/18	Loss 86.9863 (86.2604)
2022-11-09 18:04:35,437:INFO: Dataset: zara2               Batch: 12/18	Loss 85.9216 (86.2319)
2022-11-09 18:04:36,177:INFO: Dataset: zara2               Batch: 13/18	Loss 85.3656 (86.1723)
2022-11-09 18:04:36,917:INFO: Dataset: zara2               Batch: 14/18	Loss 87.2850 (86.2507)
2022-11-09 18:04:37,655:INFO: Dataset: zara2               Batch: 15/18	Loss 85.6912 (86.2111)
2022-11-09 18:04:38,397:INFO: Dataset: zara2               Batch: 16/18	Loss 85.2890 (86.1538)
2022-11-09 18:04:39,139:INFO: Dataset: zara2               Batch: 17/18	Loss 85.5020 (86.1153)
2022-11-09 18:04:39,862:INFO: Dataset: zara2               Batch: 18/18	Loss 74.6667 (85.6630)
2022-11-09 18:04:39,931:INFO: - Computing ADE (validation o)
2022-11-09 18:04:40,212:INFO: 		 ADE on eth                       dataset:	 3.417720317840576
2022-11-09 18:04:40,213:INFO: Average validation o:	ADE  3.4177	FDE  5.7764
2022-11-09 18:04:40,213:INFO: - Computing ADE (validation)
2022-11-09 18:04:40,484:INFO: 		 ADE on hotel                     dataset:	 0.9828332662582397
2022-11-09 18:04:40,804:INFO: 		 ADE on univ                      dataset:	 1.5285661220550537
2022-11-09 18:04:41,073:INFO: 		 ADE on zara1                     dataset:	 2.6210451126098633
2022-11-09 18:04:41,436:INFO: 		 ADE on zara2                     dataset:	 1.4663389921188354
2022-11-09 18:04:41,436:INFO: Average validation:	ADE  1.5394	FDE  2.7925
2022-11-09 18:04:41,437:INFO: - Computing ADE (training)
2022-11-09 18:04:41,771:INFO: 		 ADE on hotel                     dataset:	 1.334025263786316
2022-11-09 18:04:42,391:INFO: 		 ADE on univ                      dataset:	 1.4140472412109375
2022-11-09 18:04:42,839:INFO: 		 ADE on zara1                     dataset:	 2.480510950088501
2022-11-09 18:04:43,531:INFO: 		 ADE on zara2                     dataset:	 1.710729718208313
2022-11-09 18:04:43,531:INFO: Average training:	ADE  1.5402	FDE  2.7969
2022-11-09 18:04:43,541:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_254.pth.tar
2022-11-09 18:04:43,541:INFO: 
===> EPOCH: 255 (P3)
2022-11-09 18:04:43,542:INFO: - Computing loss (training)
2022-11-09 18:04:44,455:INFO: Dataset: hotel               Batch: 1/4	Loss 83.1032 (83.1032)
2022-11-09 18:04:45,263:INFO: Dataset: hotel               Batch: 2/4	Loss 84.2809 (83.7120)
2022-11-09 18:04:45,981:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8405 (83.4123)
2022-11-09 18:04:46,659:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9830 (77.9789)
2022-11-09 18:04:47,674:INFO: Dataset: univ                Batch:  1/15	Loss 83.6202 (83.6202)
2022-11-09 18:04:48,433:INFO: Dataset: univ                Batch:  2/15	Loss 83.0390 (83.3156)
2022-11-09 18:04:49,177:INFO: Dataset: univ                Batch:  3/15	Loss 84.9267 (83.8499)
2022-11-09 18:04:49,924:INFO: Dataset: univ                Batch:  4/15	Loss 83.3586 (83.7293)
2022-11-09 18:04:50,675:INFO: Dataset: univ                Batch:  5/15	Loss 83.5614 (83.6949)
2022-11-09 18:04:51,423:INFO: Dataset: univ                Batch:  6/15	Loss 84.7868 (83.8782)
2022-11-09 18:04:52,170:INFO: Dataset: univ                Batch:  7/15	Loss 84.3247 (83.9425)
2022-11-09 18:04:52,915:INFO: Dataset: univ                Batch:  8/15	Loss 83.2881 (83.8627)
2022-11-09 18:04:53,669:INFO: Dataset: univ                Batch:  9/15	Loss 83.8831 (83.8651)
2022-11-09 18:04:54,416:INFO: Dataset: univ                Batch: 10/15	Loss 84.2675 (83.9040)
2022-11-09 18:04:55,163:INFO: Dataset: univ                Batch: 11/15	Loss 85.2922 (84.0253)
2022-11-09 18:04:55,913:INFO: Dataset: univ                Batch: 12/15	Loss 84.9833 (84.1025)
2022-11-09 18:04:56,658:INFO: Dataset: univ                Batch: 13/15	Loss 86.0288 (84.2414)
2022-11-09 18:04:57,408:INFO: Dataset: univ                Batch: 14/15	Loss 83.5807 (84.1959)
2022-11-09 18:04:58,070:INFO: Dataset: univ                Batch: 15/15	Loss 15.5992 (83.2857)
2022-11-09 18:04:59,073:INFO: Dataset: zara1               Batch: 1/8	Loss 93.0872 (93.0872)
2022-11-09 18:04:59,809:INFO: Dataset: zara1               Batch: 2/8	Loss 91.7244 (92.4239)
2022-11-09 18:05:00,552:INFO: Dataset: zara1               Batch: 3/8	Loss 90.7536 (91.8326)
2022-11-09 18:05:01,364:INFO: Dataset: zara1               Batch: 4/8	Loss 91.3169 (91.7116)
2022-11-09 18:05:02,102:INFO: Dataset: zara1               Batch: 5/8	Loss 91.2403 (91.6193)
2022-11-09 18:05:02,838:INFO: Dataset: zara1               Batch: 6/8	Loss 90.5454 (91.4397)
2022-11-09 18:05:03,578:INFO: Dataset: zara1               Batch: 7/8	Loss 89.9629 (91.2437)
2022-11-09 18:05:04,299:INFO: Dataset: zara1               Batch: 8/8	Loss 77.9706 (89.8675)
2022-11-09 18:05:05,320:INFO: Dataset: zara2               Batch:  1/18	Loss 85.2017 (85.2017)
2022-11-09 18:05:06,062:INFO: Dataset: zara2               Batch:  2/18	Loss 85.2145 (85.2085)
2022-11-09 18:05:06,804:INFO: Dataset: zara2               Batch:  3/18	Loss 85.5342 (85.3164)
2022-11-09 18:05:07,551:INFO: Dataset: zara2               Batch:  4/18	Loss 84.9492 (85.2287)
2022-11-09 18:05:08,298:INFO: Dataset: zara2               Batch:  5/18	Loss 86.5105 (85.4634)
2022-11-09 18:05:09,048:INFO: Dataset: zara2               Batch:  6/18	Loss 85.0086 (85.3840)
2022-11-09 18:05:09,788:INFO: Dataset: zara2               Batch:  7/18	Loss 86.9241 (85.6128)
2022-11-09 18:05:10,531:INFO: Dataset: zara2               Batch:  8/18	Loss 85.7187 (85.6272)
2022-11-09 18:05:11,276:INFO: Dataset: zara2               Batch:  9/18	Loss 84.4957 (85.5084)
2022-11-09 18:05:12,022:INFO: Dataset: zara2               Batch: 10/18	Loss 84.7775 (85.4374)
2022-11-09 18:05:12,787:INFO: Dataset: zara2               Batch: 11/18	Loss 86.9931 (85.5914)
2022-11-09 18:05:13,543:INFO: Dataset: zara2               Batch: 12/18	Loss 86.0948 (85.6333)
2022-11-09 18:05:14,304:INFO: Dataset: zara2               Batch: 13/18	Loss 86.0604 (85.6665)
2022-11-09 18:05:15,082:INFO: Dataset: zara2               Batch: 14/18	Loss 87.7287 (85.8070)
2022-11-09 18:05:15,890:INFO: Dataset: zara2               Batch: 15/18	Loss 85.9364 (85.8159)
2022-11-09 18:05:16,756:INFO: Dataset: zara2               Batch: 16/18	Loss 84.9717 (85.7638)
2022-11-09 18:05:17,576:INFO: Dataset: zara2               Batch: 17/18	Loss 87.8307 (85.8755)
2022-11-09 18:05:18,367:INFO: Dataset: zara2               Batch: 18/18	Loss 73.5258 (85.3447)
2022-11-09 18:05:18,443:INFO: - Computing ADE (validation o)
2022-11-09 18:05:18,777:INFO: 		 ADE on eth                       dataset:	 3.512082576751709
2022-11-09 18:05:18,778:INFO: Average validation o:	ADE  3.5121	FDE  5.9156
2022-11-09 18:05:18,778:INFO: - Computing ADE (validation)
2022-11-09 18:05:19,062:INFO: 		 ADE on hotel                     dataset:	 0.9964857697486877
2022-11-09 18:05:19,475:INFO: 		 ADE on univ                      dataset:	 1.5171771049499512
2022-11-09 18:05:19,755:INFO: 		 ADE on zara1                     dataset:	 2.5291409492492676
2022-11-09 18:05:20,116:INFO: 		 ADE on zara2                     dataset:	 1.4499444961547852
2022-11-09 18:05:20,116:INFO: Average validation:	ADE  1.5228	FDE  2.7469
2022-11-09 18:05:20,117:INFO: - Computing ADE (training)
2022-11-09 18:05:20,462:INFO: 		 ADE on hotel                     dataset:	 1.3431733846664429
2022-11-09 18:05:21,162:INFO: 		 ADE on univ                      dataset:	 1.4027478694915771
2022-11-09 18:05:21,644:INFO: 		 ADE on zara1                     dataset:	 2.479008674621582
2022-11-09 18:05:22,427:INFO: 		 ADE on zara2                     dataset:	 1.7207245826721191
2022-11-09 18:05:22,427:INFO: Average training:	ADE  1.5344	FDE  2.7723
2022-11-09 18:05:22,438:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_255.pth.tar
2022-11-09 18:05:22,438:INFO: 
===> EPOCH: 256 (P3)
2022-11-09 18:05:22,439:INFO: - Computing loss (training)
2022-11-09 18:05:23,421:INFO: Dataset: hotel               Batch: 1/4	Loss 84.7903 (84.7903)
2022-11-09 18:05:24,202:INFO: Dataset: hotel               Batch: 2/4	Loss 82.8380 (83.8049)
2022-11-09 18:05:24,968:INFO: Dataset: hotel               Batch: 3/4	Loss 83.6645 (83.7582)
2022-11-09 18:05:25,722:INFO: Dataset: hotel               Batch: 4/4	Loss 51.3361 (78.3688)
2022-11-09 18:05:26,804:INFO: Dataset: univ                Batch:  1/15	Loss 84.5170 (84.5170)
2022-11-09 18:05:27,570:INFO: Dataset: univ                Batch:  2/15	Loss 85.9984 (85.2534)
2022-11-09 18:05:28,348:INFO: Dataset: univ                Batch:  3/15	Loss 82.4890 (84.2345)
2022-11-09 18:05:29,105:INFO: Dataset: univ                Batch:  4/15	Loss 86.2891 (84.7231)
2022-11-09 18:05:29,872:INFO: Dataset: univ                Batch:  5/15	Loss 83.3400 (84.4370)
2022-11-09 18:05:30,625:INFO: Dataset: univ                Batch:  6/15	Loss 84.1138 (84.3859)
2022-11-09 18:05:31,417:INFO: Dataset: univ                Batch:  7/15	Loss 85.1346 (84.5025)
2022-11-09 18:05:32,181:INFO: Dataset: univ                Batch:  8/15	Loss 82.9396 (84.2853)
2022-11-09 18:05:32,945:INFO: Dataset: univ                Batch:  9/15	Loss 83.8357 (84.2315)
2022-11-09 18:05:33,755:INFO: Dataset: univ                Batch: 10/15	Loss 83.1116 (84.1094)
2022-11-09 18:05:34,502:INFO: Dataset: univ                Batch: 11/15	Loss 83.7093 (84.0737)
2022-11-09 18:05:35,261:INFO: Dataset: univ                Batch: 12/15	Loss 83.2938 (84.0070)
2022-11-09 18:05:36,009:INFO: Dataset: univ                Batch: 13/15	Loss 84.2519 (84.0241)
2022-11-09 18:05:36,791:INFO: Dataset: univ                Batch: 14/15	Loss 83.0544 (83.9543)
2022-11-09 18:05:37,450:INFO: Dataset: univ                Batch: 15/15	Loss 15.2369 (82.8699)
2022-11-09 18:05:38,468:INFO: Dataset: zara1               Batch: 1/8	Loss 92.6753 (92.6753)
2022-11-09 18:05:39,311:INFO: Dataset: zara1               Batch: 2/8	Loss 92.1240 (92.3984)
2022-11-09 18:05:40,494:INFO: Dataset: zara1               Batch: 3/8	Loss 91.4724 (92.0835)
2022-11-09 18:05:41,277:INFO: Dataset: zara1               Batch: 4/8	Loss 91.6341 (91.9589)
2022-11-09 18:05:42,060:INFO: Dataset: zara1               Batch: 5/8	Loss 90.3947 (91.6346)
2022-11-09 18:05:42,839:INFO: Dataset: zara1               Batch: 6/8	Loss 91.6297 (91.6338)
2022-11-09 18:05:43,618:INFO: Dataset: zara1               Batch: 7/8	Loss 90.3800 (91.4477)
2022-11-09 18:05:44,361:INFO: Dataset: zara1               Batch: 8/8	Loss 77.7310 (89.6934)
2022-11-09 18:05:45,415:INFO: Dataset: zara2               Batch:  1/18	Loss 87.1543 (87.1543)
2022-11-09 18:05:46,188:INFO: Dataset: zara2               Batch:  2/18	Loss 85.7201 (86.4946)
2022-11-09 18:05:46,957:INFO: Dataset: zara2               Batch:  3/18	Loss 85.8587 (86.2874)
2022-11-09 18:05:47,709:INFO: Dataset: zara2               Batch:  4/18	Loss 86.4682 (86.3302)
2022-11-09 18:05:48,460:INFO: Dataset: zara2               Batch:  5/18	Loss 84.0757 (85.8524)
2022-11-09 18:05:49,214:INFO: Dataset: zara2               Batch:  6/18	Loss 86.0128 (85.8826)
2022-11-09 18:05:49,961:INFO: Dataset: zara2               Batch:  7/18	Loss 87.1303 (86.0601)
2022-11-09 18:05:50,709:INFO: Dataset: zara2               Batch:  8/18	Loss 87.3016 (86.2008)
2022-11-09 18:05:51,457:INFO: Dataset: zara2               Batch:  9/18	Loss 87.5188 (86.3354)
2022-11-09 18:05:52,203:INFO: Dataset: zara2               Batch: 10/18	Loss 84.0010 (86.0979)
2022-11-09 18:05:52,948:INFO: Dataset: zara2               Batch: 11/18	Loss 85.6999 (86.0598)
2022-11-09 18:05:53,698:INFO: Dataset: zara2               Batch: 12/18	Loss 85.6834 (86.0291)
2022-11-09 18:05:54,446:INFO: Dataset: zara2               Batch: 13/18	Loss 85.3139 (85.9740)
2022-11-09 18:05:55,196:INFO: Dataset: zara2               Batch: 14/18	Loss 86.4385 (86.0050)
2022-11-09 18:05:55,947:INFO: Dataset: zara2               Batch: 15/18	Loss 85.8618 (85.9956)
2022-11-09 18:05:56,694:INFO: Dataset: zara2               Batch: 16/18	Loss 85.9977 (85.9958)
2022-11-09 18:05:57,441:INFO: Dataset: zara2               Batch: 17/18	Loss 84.5010 (85.9092)
2022-11-09 18:05:58,265:INFO: Dataset: zara2               Batch: 18/18	Loss 72.4734 (85.2806)
2022-11-09 18:05:58,339:INFO: - Computing ADE (validation o)
2022-11-09 18:05:58,614:INFO: 		 ADE on eth                       dataset:	 3.5142133235931396
2022-11-09 18:05:58,614:INFO: Average validation o:	ADE  3.5142	FDE  5.9428
2022-11-09 18:05:58,615:INFO: - Computing ADE (validation)
2022-11-09 18:05:58,884:INFO: 		 ADE on hotel                     dataset:	 0.9964602589607239
2022-11-09 18:05:59,199:INFO: 		 ADE on univ                      dataset:	 1.519531488418579
2022-11-09 18:05:59,463:INFO: 		 ADE on zara1                     dataset:	 2.5035946369171143
2022-11-09 18:05:59,835:INFO: 		 ADE on zara2                     dataset:	 1.4524011611938477
2022-11-09 18:05:59,836:INFO: Average validation:	ADE  1.5235	FDE  2.7446
2022-11-09 18:05:59,836:INFO: - Computing ADE (training)
2022-11-09 18:06:00,173:INFO: 		 ADE on hotel                     dataset:	 1.3302706480026245
2022-11-09 18:06:00,798:INFO: 		 ADE on univ                      dataset:	 1.4029674530029297
2022-11-09 18:06:01,236:INFO: 		 ADE on zara1                     dataset:	 2.4787988662719727
2022-11-09 18:06:01,930:INFO: 		 ADE on zara2                     dataset:	 1.7266783714294434
2022-11-09 18:06:01,930:INFO: Average training:	ADE  1.5354	FDE  2.7734
2022-11-09 18:06:01,940:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_256.pth.tar
2022-11-09 18:06:01,940:INFO: 
===> EPOCH: 257 (P3)
2022-11-09 18:06:01,941:INFO: - Computing loss (training)
2022-11-09 18:06:02,878:INFO: Dataset: hotel               Batch: 1/4	Loss 84.9674 (84.9674)
2022-11-09 18:06:03,616:INFO: Dataset: hotel               Batch: 2/4	Loss 84.4575 (84.7191)
2022-11-09 18:06:04,344:INFO: Dataset: hotel               Batch: 3/4	Loss 81.2396 (83.5873)
2022-11-09 18:06:05,034:INFO: Dataset: hotel               Batch: 4/4	Loss 50.0137 (77.5192)
2022-11-09 18:06:06,058:INFO: Dataset: univ                Batch:  1/15	Loss 84.5769 (84.5769)
2022-11-09 18:06:06,803:INFO: Dataset: univ                Batch:  2/15	Loss 84.4330 (84.5088)
2022-11-09 18:06:07,563:INFO: Dataset: univ                Batch:  3/15	Loss 83.9010 (84.2921)
2022-11-09 18:06:08,315:INFO: Dataset: univ                Batch:  4/15	Loss 84.0017 (84.2220)
2022-11-09 18:06:09,069:INFO: Dataset: univ                Batch:  5/15	Loss 82.9945 (83.9741)
2022-11-09 18:06:09,835:INFO: Dataset: univ                Batch:  6/15	Loss 85.3725 (84.2193)
2022-11-09 18:06:10,593:INFO: Dataset: univ                Batch:  7/15	Loss 82.7026 (84.0051)
2022-11-09 18:06:11,350:INFO: Dataset: univ                Batch:  8/15	Loss 83.0524 (83.8836)
2022-11-09 18:06:12,104:INFO: Dataset: univ                Batch:  9/15	Loss 82.3355 (83.7058)
2022-11-09 18:06:12,859:INFO: Dataset: univ                Batch: 10/15	Loss 83.5885 (83.6941)
2022-11-09 18:06:13,623:INFO: Dataset: univ                Batch: 11/15	Loss 82.1895 (83.5423)
2022-11-09 18:06:14,385:INFO: Dataset: univ                Batch: 12/15	Loss 82.8591 (83.4829)
2022-11-09 18:06:15,148:INFO: Dataset: univ                Batch: 13/15	Loss 82.7921 (83.4266)
2022-11-09 18:06:15,898:INFO: Dataset: univ                Batch: 14/15	Loss 84.7846 (83.5138)
2022-11-09 18:06:16,560:INFO: Dataset: univ                Batch: 15/15	Loss 16.3256 (82.7401)
2022-11-09 18:06:17,569:INFO: Dataset: zara1               Batch: 1/8	Loss 91.4620 (91.4620)
2022-11-09 18:06:18,388:INFO: Dataset: zara1               Batch: 2/8	Loss 90.4655 (90.9528)
2022-11-09 18:06:19,125:INFO: Dataset: zara1               Batch: 3/8	Loss 90.0864 (90.6675)
2022-11-09 18:06:19,864:INFO: Dataset: zara1               Batch: 4/8	Loss 88.9930 (90.2565)
2022-11-09 18:06:20,602:INFO: Dataset: zara1               Batch: 5/8	Loss 91.0442 (90.4076)
2022-11-09 18:06:21,342:INFO: Dataset: zara1               Batch: 6/8	Loss 89.9573 (90.3351)
2022-11-09 18:06:22,081:INFO: Dataset: zara1               Batch: 7/8	Loss 90.9274 (90.4160)
2022-11-09 18:06:22,808:INFO: Dataset: zara1               Batch: 8/8	Loss 77.5304 (88.9511)
2022-11-09 18:06:23,820:INFO: Dataset: zara2               Batch:  1/18	Loss 85.6756 (85.6756)
2022-11-09 18:06:24,579:INFO: Dataset: zara2               Batch:  2/18	Loss 85.4670 (85.5707)
2022-11-09 18:06:25,328:INFO: Dataset: zara2               Batch:  3/18	Loss 85.1602 (85.4426)
2022-11-09 18:06:26,072:INFO: Dataset: zara2               Batch:  4/18	Loss 85.0980 (85.3524)
2022-11-09 18:06:26,818:INFO: Dataset: zara2               Batch:  5/18	Loss 86.0652 (85.4888)
2022-11-09 18:06:27,566:INFO: Dataset: zara2               Batch:  6/18	Loss 85.2304 (85.4497)
2022-11-09 18:06:28,319:INFO: Dataset: zara2               Batch:  7/18	Loss 86.5941 (85.6190)
2022-11-09 18:06:29,066:INFO: Dataset: zara2               Batch:  8/18	Loss 87.9583 (85.9176)
2022-11-09 18:06:29,813:INFO: Dataset: zara2               Batch:  9/18	Loss 85.2805 (85.8518)
2022-11-09 18:06:30,559:INFO: Dataset: zara2               Batch: 10/18	Loss 85.6647 (85.8340)
2022-11-09 18:06:31,304:INFO: Dataset: zara2               Batch: 11/18	Loss 84.4480 (85.7036)
2022-11-09 18:06:32,047:INFO: Dataset: zara2               Batch: 12/18	Loss 85.1627 (85.6563)
2022-11-09 18:06:32,794:INFO: Dataset: zara2               Batch: 13/18	Loss 85.1030 (85.6137)
2022-11-09 18:06:33,541:INFO: Dataset: zara2               Batch: 14/18	Loss 84.8077 (85.5629)
2022-11-09 18:06:34,284:INFO: Dataset: zara2               Batch: 15/18	Loss 85.1423 (85.5348)
2022-11-09 18:06:35,028:INFO: Dataset: zara2               Batch: 16/18	Loss 83.9302 (85.4369)
2022-11-09 18:06:35,847:INFO: Dataset: zara2               Batch: 17/18	Loss 84.6377 (85.3890)
2022-11-09 18:06:36,572:INFO: Dataset: zara2               Batch: 18/18	Loss 72.7701 (84.7402)
2022-11-09 18:06:36,641:INFO: - Computing ADE (validation o)
2022-11-09 18:06:36,922:INFO: 		 ADE on eth                       dataset:	 3.5514485836029053
2022-11-09 18:06:36,922:INFO: Average validation o:	ADE  3.5514	FDE  6.0219
2022-11-09 18:06:36,923:INFO: - Computing ADE (validation)
2022-11-09 18:06:37,197:INFO: 		 ADE on hotel                     dataset:	 1.008751630783081
2022-11-09 18:06:37,509:INFO: 		 ADE on univ                      dataset:	 1.5107651948928833
2022-11-09 18:06:37,785:INFO: 		 ADE on zara1                     dataset:	 2.4634621143341064
2022-11-09 18:06:38,156:INFO: 		 ADE on zara2                     dataset:	 1.4496352672576904
2022-11-09 18:06:38,156:INFO: Average validation:	ADE  1.5162	FDE  2.7383
2022-11-09 18:06:38,157:INFO: - Computing ADE (training)
2022-11-09 18:06:38,481:INFO: 		 ADE on hotel                     dataset:	 1.347618579864502
2022-11-09 18:06:39,106:INFO: 		 ADE on univ                      dataset:	 1.3988639116287231
2022-11-09 18:06:39,555:INFO: 		 ADE on zara1                     dataset:	 2.4786064624786377
2022-11-09 18:06:40,248:INFO: 		 ADE on zara2                     dataset:	 1.738552212715149
2022-11-09 18:06:40,248:INFO: Average training:	ADE  1.5353	FDE  2.7798
2022-11-09 18:06:40,258:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_257.pth.tar
2022-11-09 18:06:40,258:INFO: 
===> EPOCH: 258 (P3)
2022-11-09 18:06:40,258:INFO: - Computing loss (training)
2022-11-09 18:06:41,194:INFO: Dataset: hotel               Batch: 1/4	Loss 84.0580 (84.0580)
2022-11-09 18:06:41,930:INFO: Dataset: hotel               Batch: 2/4	Loss 81.6313 (82.8097)
2022-11-09 18:06:42,653:INFO: Dataset: hotel               Batch: 3/4	Loss 83.3669 (83.0013)
2022-11-09 18:06:43,342:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7321 (77.7224)
2022-11-09 18:06:44,366:INFO: Dataset: univ                Batch:  1/15	Loss 83.2824 (83.2824)
2022-11-09 18:06:45,114:INFO: Dataset: univ                Batch:  2/15	Loss 84.1691 (83.7380)
2022-11-09 18:06:45,861:INFO: Dataset: univ                Batch:  3/15	Loss 83.4377 (83.6394)
2022-11-09 18:06:46,610:INFO: Dataset: univ                Batch:  4/15	Loss 84.1077 (83.7586)
2022-11-09 18:06:47,361:INFO: Dataset: univ                Batch:  5/15	Loss 83.8343 (83.7737)
2022-11-09 18:06:48,115:INFO: Dataset: univ                Batch:  6/15	Loss 82.8750 (83.6196)
2022-11-09 18:06:48,874:INFO: Dataset: univ                Batch:  7/15	Loss 82.6500 (83.4894)
2022-11-09 18:06:49,632:INFO: Dataset: univ                Batch:  8/15	Loss 81.9138 (83.2869)
2022-11-09 18:06:50,378:INFO: Dataset: univ                Batch:  9/15	Loss 83.4820 (83.3066)
2022-11-09 18:06:51,132:INFO: Dataset: univ                Batch: 10/15	Loss 82.9309 (83.2678)
2022-11-09 18:06:51,886:INFO: Dataset: univ                Batch: 11/15	Loss 82.9311 (83.2363)
2022-11-09 18:06:52,639:INFO: Dataset: univ                Batch: 12/15	Loss 83.7937 (83.2843)
2022-11-09 18:06:53,397:INFO: Dataset: univ                Batch: 13/15	Loss 82.1900 (83.1922)
2022-11-09 18:06:54,151:INFO: Dataset: univ                Batch: 14/15	Loss 82.8016 (83.1648)
2022-11-09 18:06:54,809:INFO: Dataset: univ                Batch: 15/15	Loss 16.0075 (82.3978)
2022-11-09 18:06:55,902:INFO: Dataset: zara1               Batch: 1/8	Loss 92.2645 (92.2645)
2022-11-09 18:06:56,647:INFO: Dataset: zara1               Batch: 2/8	Loss 89.9949 (91.2365)
2022-11-09 18:06:57,385:INFO: Dataset: zara1               Batch: 3/8	Loss 89.3371 (90.5738)
2022-11-09 18:06:58,122:INFO: Dataset: zara1               Batch: 4/8	Loss 88.5955 (89.9840)
2022-11-09 18:06:58,860:INFO: Dataset: zara1               Batch: 5/8	Loss 88.8234 (89.7443)
2022-11-09 18:06:59,592:INFO: Dataset: zara1               Batch: 6/8	Loss 89.6989 (89.7374)
2022-11-09 18:07:00,327:INFO: Dataset: zara1               Batch: 7/8	Loss 89.8587 (89.7554)
2022-11-09 18:07:01,045:INFO: Dataset: zara1               Batch: 8/8	Loss 76.5225 (88.3973)
2022-11-09 18:07:02,057:INFO: Dataset: zara2               Batch:  1/18	Loss 85.2483 (85.2483)
2022-11-09 18:07:02,799:INFO: Dataset: zara2               Batch:  2/18	Loss 86.4749 (85.8833)
2022-11-09 18:07:03,536:INFO: Dataset: zara2               Batch:  3/18	Loss 84.1483 (85.2849)
2022-11-09 18:07:04,277:INFO: Dataset: zara2               Batch:  4/18	Loss 85.5236 (85.3407)
2022-11-09 18:07:05,021:INFO: Dataset: zara2               Batch:  5/18	Loss 84.0822 (85.0961)
2022-11-09 18:07:05,766:INFO: Dataset: zara2               Batch:  6/18	Loss 83.7338 (84.8899)
2022-11-09 18:07:06,508:INFO: Dataset: zara2               Batch:  7/18	Loss 83.6979 (84.7171)
2022-11-09 18:07:07,247:INFO: Dataset: zara2               Batch:  8/18	Loss 84.1233 (84.6377)
2022-11-09 18:07:07,989:INFO: Dataset: zara2               Batch:  9/18	Loss 85.0798 (84.6854)
2022-11-09 18:07:08,727:INFO: Dataset: zara2               Batch: 10/18	Loss 84.7624 (84.6936)
2022-11-09 18:07:09,471:INFO: Dataset: zara2               Batch: 11/18	Loss 84.0212 (84.6349)
2022-11-09 18:07:10,216:INFO: Dataset: zara2               Batch: 12/18	Loss 83.2168 (84.5117)
2022-11-09 18:07:10,955:INFO: Dataset: zara2               Batch: 13/18	Loss 84.1355 (84.4798)
2022-11-09 18:07:11,774:INFO: Dataset: zara2               Batch: 14/18	Loss 85.3422 (84.5454)
2022-11-09 18:07:12,517:INFO: Dataset: zara2               Batch: 15/18	Loss 83.5840 (84.4854)
2022-11-09 18:07:13,262:INFO: Dataset: zara2               Batch: 16/18	Loss 83.6407 (84.4306)
2022-11-09 18:07:14,005:INFO: Dataset: zara2               Batch: 17/18	Loss 85.2413 (84.4772)
2022-11-09 18:07:14,734:INFO: Dataset: zara2               Batch: 18/18	Loss 72.5210 (83.8783)
2022-11-09 18:07:14,802:INFO: - Computing ADE (validation o)
2022-11-09 18:07:15,074:INFO: 		 ADE on eth                       dataset:	 3.512542724609375
2022-11-09 18:07:15,074:INFO: Average validation o:	ADE  3.5125	FDE  5.9604
2022-11-09 18:07:15,075:INFO: - Computing ADE (validation)
2022-11-09 18:07:15,352:INFO: 		 ADE on hotel                     dataset:	 0.9731618762016296
2022-11-09 18:07:15,670:INFO: 		 ADE on univ                      dataset:	 1.5088887214660645
2022-11-09 18:07:15,938:INFO: 		 ADE on zara1                     dataset:	 2.4838292598724365
2022-11-09 18:07:16,299:INFO: 		 ADE on zara2                     dataset:	 1.4375008344650269
2022-11-09 18:07:16,299:INFO: Average validation:	ADE  1.5100	FDE  2.7314
2022-11-09 18:07:16,300:INFO: - Computing ADE (training)
2022-11-09 18:07:16,626:INFO: 		 ADE on hotel                     dataset:	 1.3348569869995117
2022-11-09 18:07:17,269:INFO: 		 ADE on univ                      dataset:	 1.3939505815505981
2022-11-09 18:07:17,724:INFO: 		 ADE on zara1                     dataset:	 2.472818374633789
2022-11-09 18:07:18,430:INFO: 		 ADE on zara2                     dataset:	 1.7188355922698975
2022-11-09 18:07:18,430:INFO: Average training:	ADE  1.5271	FDE  2.7692
2022-11-09 18:07:18,441:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_258.pth.tar
2022-11-09 18:07:18,441:INFO: 
===> EPOCH: 259 (P3)
2022-11-09 18:07:18,441:INFO: - Computing loss (training)
2022-11-09 18:07:19,372:INFO: Dataset: hotel               Batch: 1/4	Loss 85.8856 (85.8856)
2022-11-09 18:07:20,098:INFO: Dataset: hotel               Batch: 2/4	Loss 82.2574 (84.0229)
2022-11-09 18:07:20,815:INFO: Dataset: hotel               Batch: 3/4	Loss 82.1755 (83.3904)
2022-11-09 18:07:21,496:INFO: Dataset: hotel               Batch: 4/4	Loss 50.1565 (77.5591)
2022-11-09 18:07:22,525:INFO: Dataset: univ                Batch:  1/15	Loss 82.2537 (82.2537)
2022-11-09 18:07:23,277:INFO: Dataset: univ                Batch:  2/15	Loss 81.7956 (82.0239)
2022-11-09 18:07:24,017:INFO: Dataset: univ                Batch:  3/15	Loss 82.9347 (82.2951)
2022-11-09 18:07:24,770:INFO: Dataset: univ                Batch:  4/15	Loss 82.7387 (82.3978)
2022-11-09 18:07:25,521:INFO: Dataset: univ                Batch:  5/15	Loss 81.4200 (82.1884)
2022-11-09 18:07:26,276:INFO: Dataset: univ                Batch:  6/15	Loss 82.8572 (82.2993)
2022-11-09 18:07:27,017:INFO: Dataset: univ                Batch:  7/15	Loss 83.7386 (82.4838)
2022-11-09 18:07:27,763:INFO: Dataset: univ                Batch:  8/15	Loss 84.0439 (82.6751)
2022-11-09 18:07:28,527:INFO: Dataset: univ                Batch:  9/15	Loss 80.7443 (82.4423)
2022-11-09 18:07:29,276:INFO: Dataset: univ                Batch: 10/15	Loss 83.7239 (82.5645)
2022-11-09 18:07:30,018:INFO: Dataset: univ                Batch: 11/15	Loss 83.8305 (82.6665)
2022-11-09 18:07:30,769:INFO: Dataset: univ                Batch: 12/15	Loss 83.0610 (82.6994)
2022-11-09 18:07:31,515:INFO: Dataset: univ                Batch: 13/15	Loss 83.7061 (82.7722)
2022-11-09 18:07:32,339:INFO: Dataset: univ                Batch: 14/15	Loss 84.3910 (82.8754)
2022-11-09 18:07:32,997:INFO: Dataset: univ                Batch: 15/15	Loss 15.9805 (82.1843)
2022-11-09 18:07:34,006:INFO: Dataset: zara1               Batch: 1/8	Loss 87.1903 (87.1903)
2022-11-09 18:07:34,748:INFO: Dataset: zara1               Batch: 2/8	Loss 89.0881 (88.1302)
2022-11-09 18:07:35,486:INFO: Dataset: zara1               Batch: 3/8	Loss 89.0132 (88.3919)
2022-11-09 18:07:36,239:INFO: Dataset: zara1               Batch: 4/8	Loss 88.9537 (88.5424)
2022-11-09 18:07:36,977:INFO: Dataset: zara1               Batch: 5/8	Loss 88.5390 (88.5418)
2022-11-09 18:07:37,715:INFO: Dataset: zara1               Batch: 6/8	Loss 87.6872 (88.4065)
2022-11-09 18:07:38,455:INFO: Dataset: zara1               Batch: 7/8	Loss 89.1485 (88.5072)
2022-11-09 18:07:39,177:INFO: Dataset: zara1               Batch: 8/8	Loss 75.6497 (87.2214)
2022-11-09 18:07:40,192:INFO: Dataset: zara2               Batch:  1/18	Loss 84.3535 (84.3535)
2022-11-09 18:07:40,943:INFO: Dataset: zara2               Batch:  2/18	Loss 83.4051 (83.8625)
2022-11-09 18:07:41,693:INFO: Dataset: zara2               Batch:  3/18	Loss 84.7471 (84.1382)
2022-11-09 18:07:42,437:INFO: Dataset: zara2               Batch:  4/18	Loss 83.5340 (83.9722)
2022-11-09 18:07:43,185:INFO: Dataset: zara2               Batch:  5/18	Loss 83.6063 (83.9011)
2022-11-09 18:07:43,940:INFO: Dataset: zara2               Batch:  6/18	Loss 83.2732 (83.7985)
2022-11-09 18:07:44,682:INFO: Dataset: zara2               Batch:  7/18	Loss 83.7621 (83.7930)
2022-11-09 18:07:45,430:INFO: Dataset: zara2               Batch:  8/18	Loss 83.2809 (83.7292)
2022-11-09 18:07:46,175:INFO: Dataset: zara2               Batch:  9/18	Loss 83.6760 (83.7234)
2022-11-09 18:07:46,925:INFO: Dataset: zara2               Batch: 10/18	Loss 84.3904 (83.7852)
2022-11-09 18:07:47,671:INFO: Dataset: zara2               Batch: 11/18	Loss 83.9676 (83.8024)
2022-11-09 18:07:48,421:INFO: Dataset: zara2               Batch: 12/18	Loss 83.1767 (83.7502)
2022-11-09 18:07:49,166:INFO: Dataset: zara2               Batch: 13/18	Loss 83.8615 (83.7595)
2022-11-09 18:07:49,994:INFO: Dataset: zara2               Batch: 14/18	Loss 83.9403 (83.7724)
2022-11-09 18:07:50,741:INFO: Dataset: zara2               Batch: 15/18	Loss 84.0181 (83.7891)
2022-11-09 18:07:51,488:INFO: Dataset: zara2               Batch: 16/18	Loss 83.1018 (83.7446)
2022-11-09 18:07:52,234:INFO: Dataset: zara2               Batch: 17/18	Loss 85.0803 (83.8248)
2022-11-09 18:07:52,965:INFO: Dataset: zara2               Batch: 18/18	Loss 73.9821 (83.4147)
2022-11-09 18:07:53,038:INFO: - Computing ADE (validation o)
2022-11-09 18:07:53,314:INFO: 		 ADE on eth                       dataset:	 3.522376537322998
2022-11-09 18:07:53,314:INFO: Average validation o:	ADE  3.5224	FDE  5.9875
2022-11-09 18:07:53,315:INFO: - Computing ADE (validation)
2022-11-09 18:07:53,582:INFO: 		 ADE on hotel                     dataset:	 0.9823798537254333
2022-11-09 18:07:53,903:INFO: 		 ADE on univ                      dataset:	 1.492596983909607
2022-11-09 18:07:54,177:INFO: 		 ADE on zara1                     dataset:	 2.448495626449585
2022-11-09 18:07:54,545:INFO: 		 ADE on zara2                     dataset:	 1.4305554628372192
2022-11-09 18:07:54,546:INFO: Average validation:	ADE  1.4975	FDE  2.7171
2022-11-09 18:07:54,546:INFO: - Computing ADE (training)
2022-11-09 18:07:54,875:INFO: 		 ADE on hotel                     dataset:	 1.3338524103164673
2022-11-09 18:07:55,503:INFO: 		 ADE on univ                      dataset:	 1.3842896223068237
2022-11-09 18:07:55,940:INFO: 		 ADE on zara1                     dataset:	 2.4685609340667725
2022-11-09 18:07:56,647:INFO: 		 ADE on zara2                     dataset:	 1.7241486310958862
2022-11-09 18:07:56,647:INFO: Average training:	ADE  1.5211	FDE  2.7653
2022-11-09 18:07:56,658:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_259.pth.tar
2022-11-09 18:07:56,658:INFO: 
===> EPOCH: 260 (P3)
2022-11-09 18:07:56,658:INFO: - Computing loss (training)
2022-11-09 18:07:57,607:INFO: Dataset: hotel               Batch: 1/4	Loss 83.6055 (83.6055)
2022-11-09 18:07:58,364:INFO: Dataset: hotel               Batch: 2/4	Loss 83.0852 (83.3303)
2022-11-09 18:07:59,122:INFO: Dataset: hotel               Batch: 3/4	Loss 82.2216 (82.9548)
2022-11-09 18:07:59,866:INFO: Dataset: hotel               Batch: 4/4	Loss 49.6775 (77.1598)
2022-11-09 18:08:01,011:INFO: Dataset: univ                Batch:  1/15	Loss 82.9863 (82.9863)
2022-11-09 18:08:01,784:INFO: Dataset: univ                Batch:  2/15	Loss 83.2019 (83.0909)
2022-11-09 18:08:02,593:INFO: Dataset: univ                Batch:  3/15	Loss 82.6688 (82.9464)
2022-11-09 18:08:03,400:INFO: Dataset: univ                Batch:  4/15	Loss 83.2104 (83.0160)
2022-11-09 18:08:04,186:INFO: Dataset: univ                Batch:  5/15	Loss 81.9541 (82.8046)
2022-11-09 18:08:04,950:INFO: Dataset: univ                Batch:  6/15	Loss 82.7541 (82.7966)
2022-11-09 18:08:05,719:INFO: Dataset: univ                Batch:  7/15	Loss 81.8556 (82.6560)
2022-11-09 18:08:06,487:INFO: Dataset: univ                Batch:  8/15	Loss 83.5984 (82.7650)
2022-11-09 18:08:07,235:INFO: Dataset: univ                Batch:  9/15	Loss 81.7157 (82.6531)
2022-11-09 18:08:07,984:INFO: Dataset: univ                Batch: 10/15	Loss 83.7137 (82.7449)
2022-11-09 18:08:08,767:INFO: Dataset: univ                Batch: 11/15	Loss 82.1149 (82.6864)
2022-11-09 18:08:09,534:INFO: Dataset: univ                Batch: 12/15	Loss 82.5153 (82.6712)
2022-11-09 18:08:10,323:INFO: Dataset: univ                Batch: 13/15	Loss 81.6829 (82.5945)
2022-11-09 18:08:11,123:INFO: Dataset: univ                Batch: 14/15	Loss 82.3601 (82.5769)
2022-11-09 18:08:11,880:INFO: Dataset: univ                Batch: 15/15	Loss 15.9079 (81.8187)
2022-11-09 18:08:12,983:INFO: Dataset: zara1               Batch: 1/8	Loss 88.1249 (88.1249)
2022-11-09 18:08:13,767:INFO: Dataset: zara1               Batch: 2/8	Loss 90.4922 (89.3519)
2022-11-09 18:08:14,602:INFO: Dataset: zara1               Batch: 3/8	Loss 88.7618 (89.1478)
2022-11-09 18:08:15,376:INFO: Dataset: zara1               Batch: 4/8	Loss 87.6997 (88.8143)
2022-11-09 18:08:16,150:INFO: Dataset: zara1               Batch: 5/8	Loss 88.8802 (88.8279)
2022-11-09 18:08:16,925:INFO: Dataset: zara1               Batch: 6/8	Loss 87.1781 (88.5904)
2022-11-09 18:08:17,692:INFO: Dataset: zara1               Batch: 7/8	Loss 88.0204 (88.5026)
2022-11-09 18:08:18,447:INFO: Dataset: zara1               Batch: 8/8	Loss 75.1674 (87.0989)
2022-11-09 18:08:19,478:INFO: Dataset: zara2               Batch:  1/18	Loss 84.4006 (84.4006)
2022-11-09 18:08:20,255:INFO: Dataset: zara2               Batch:  2/18	Loss 83.8195 (84.1058)
2022-11-09 18:08:21,133:INFO: Dataset: zara2               Batch:  3/18	Loss 83.7960 (84.0073)
2022-11-09 18:08:22,119:INFO: Dataset: zara2               Batch:  4/18	Loss 84.1886 (84.0510)
2022-11-09 18:08:22,875:INFO: Dataset: zara2               Batch:  5/18	Loss 83.4755 (83.9250)
2022-11-09 18:08:23,631:INFO: Dataset: zara2               Batch:  6/18	Loss 83.1287 (83.7869)
2022-11-09 18:08:24,386:INFO: Dataset: zara2               Batch:  7/18	Loss 84.0471 (83.8247)
2022-11-09 18:08:25,155:INFO: Dataset: zara2               Batch:  8/18	Loss 84.5031 (83.9073)
2022-11-09 18:08:25,924:INFO: Dataset: zara2               Batch:  9/18	Loss 84.0246 (83.9206)
2022-11-09 18:08:26,687:INFO: Dataset: zara2               Batch: 10/18	Loss 83.2502 (83.8551)
2022-11-09 18:08:27,436:INFO: Dataset: zara2               Batch: 11/18	Loss 83.6930 (83.8400)
2022-11-09 18:08:28,187:INFO: Dataset: zara2               Batch: 12/18	Loss 83.6668 (83.8256)
2022-11-09 18:08:29,011:INFO: Dataset: zara2               Batch: 13/18	Loss 83.0928 (83.7643)
2022-11-09 18:08:29,761:INFO: Dataset: zara2               Batch: 14/18	Loss 84.7111 (83.8287)
2022-11-09 18:08:30,512:INFO: Dataset: zara2               Batch: 15/18	Loss 85.2279 (83.9166)
2022-11-09 18:08:31,282:INFO: Dataset: zara2               Batch: 16/18	Loss 82.8043 (83.8410)
2022-11-09 18:08:32,048:INFO: Dataset: zara2               Batch: 17/18	Loss 83.7620 (83.8367)
2022-11-09 18:08:32,794:INFO: Dataset: zara2               Batch: 18/18	Loss 71.3907 (83.2092)
2022-11-09 18:08:32,863:INFO: - Computing ADE (validation o)
2022-11-09 18:08:33,133:INFO: 		 ADE on eth                       dataset:	 3.511716842651367
2022-11-09 18:08:33,134:INFO: Average validation o:	ADE  3.5117	FDE  5.9635
2022-11-09 18:08:33,134:INFO: - Computing ADE (validation)
2022-11-09 18:08:33,399:INFO: 		 ADE on hotel                     dataset:	 0.9645493626594543
2022-11-09 18:08:33,725:INFO: 		 ADE on univ                      dataset:	 1.4891048669815063
2022-11-09 18:08:33,993:INFO: 		 ADE on zara1                     dataset:	 2.46211314201355
2022-11-09 18:08:34,363:INFO: 		 ADE on zara2                     dataset:	 1.4153295755386353
2022-11-09 18:08:34,364:INFO: Average validation:	ADE  1.4899	FDE  2.7094
2022-11-09 18:08:34,364:INFO: - Computing ADE (training)
2022-11-09 18:08:34,700:INFO: 		 ADE on hotel                     dataset:	 1.3084042072296143
2022-11-09 18:08:35,326:INFO: 		 ADE on univ                      dataset:	 1.3773835897445679
2022-11-09 18:08:35,771:INFO: 		 ADE on zara1                     dataset:	 2.4595322608947754
2022-11-09 18:08:36,465:INFO: 		 ADE on zara2                     dataset:	 1.705198049545288
2022-11-09 18:08:36,465:INFO: Average training:	ADE  1.5111	FDE  2.7538
2022-11-09 18:08:36,475:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_260.pth.tar
2022-11-09 18:08:36,476:INFO: 
===> EPOCH: 261 (P3)
2022-11-09 18:08:36,476:INFO: - Computing loss (training)
2022-11-09 18:08:37,401:INFO: Dataset: hotel               Batch: 1/4	Loss 86.2231 (86.2231)
2022-11-09 18:08:38,140:INFO: Dataset: hotel               Batch: 2/4	Loss 81.5917 (83.9636)
2022-11-09 18:08:38,867:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8118 (83.5710)
2022-11-09 18:08:39,569:INFO: Dataset: hotel               Batch: 4/4	Loss 49.9081 (77.6645)
2022-11-09 18:08:40,586:INFO: Dataset: univ                Batch:  1/15	Loss 83.4312 (83.4312)
2022-11-09 18:08:41,349:INFO: Dataset: univ                Batch:  2/15	Loss 81.2491 (82.2698)
2022-11-09 18:08:42,095:INFO: Dataset: univ                Batch:  3/15	Loss 82.4905 (82.3371)
2022-11-09 18:08:42,855:INFO: Dataset: univ                Batch:  4/15	Loss 82.0511 (82.2614)
2022-11-09 18:08:43,615:INFO: Dataset: univ                Batch:  5/15	Loss 82.1412 (82.2374)
2022-11-09 18:08:44,370:INFO: Dataset: univ                Batch:  6/15	Loss 82.5666 (82.2935)
2022-11-09 18:08:45,127:INFO: Dataset: univ                Batch:  7/15	Loss 82.6000 (82.3393)
2022-11-09 18:08:45,879:INFO: Dataset: univ                Batch:  8/15	Loss 81.9646 (82.2932)
2022-11-09 18:08:46,626:INFO: Dataset: univ                Batch:  9/15	Loss 83.2526 (82.3912)
2022-11-09 18:08:47,383:INFO: Dataset: univ                Batch: 10/15	Loss 83.2341 (82.4737)
2022-11-09 18:08:48,130:INFO: Dataset: univ                Batch: 11/15	Loss 83.8335 (82.5899)
2022-11-09 18:08:48,971:INFO: Dataset: univ                Batch: 12/15	Loss 82.5603 (82.5873)
2022-11-09 18:08:49,730:INFO: Dataset: univ                Batch: 13/15	Loss 81.6275 (82.5137)
2022-11-09 18:08:50,493:INFO: Dataset: univ                Batch: 14/15	Loss 81.1460 (82.4079)
2022-11-09 18:08:51,158:INFO: Dataset: univ                Batch: 15/15	Loss 15.6475 (81.5252)
2022-11-09 18:08:52,174:INFO: Dataset: zara1               Batch: 1/8	Loss 88.2327 (88.2327)
2022-11-09 18:08:52,919:INFO: Dataset: zara1               Batch: 2/8	Loss 86.9076 (87.5319)
2022-11-09 18:08:53,662:INFO: Dataset: zara1               Batch: 3/8	Loss 87.3140 (87.4563)
2022-11-09 18:08:54,403:INFO: Dataset: zara1               Batch: 4/8	Loss 87.9720 (87.5871)
2022-11-09 18:08:55,142:INFO: Dataset: zara1               Batch: 5/8	Loss 88.3670 (87.7490)
2022-11-09 18:08:55,884:INFO: Dataset: zara1               Batch: 6/8	Loss 87.4916 (87.7074)
2022-11-09 18:08:56,626:INFO: Dataset: zara1               Batch: 7/8	Loss 87.2838 (87.6476)
2022-11-09 18:08:57,356:INFO: Dataset: zara1               Batch: 8/8	Loss 75.5853 (86.2446)
2022-11-09 18:08:58,374:INFO: Dataset: zara2               Batch:  1/18	Loss 85.0888 (85.0888)
2022-11-09 18:08:59,118:INFO: Dataset: zara2               Batch:  2/18	Loss 83.3264 (84.2660)
2022-11-09 18:08:59,870:INFO: Dataset: zara2               Batch:  3/18	Loss 83.2807 (83.9126)
2022-11-09 18:09:00,617:INFO: Dataset: zara2               Batch:  4/18	Loss 83.5585 (83.8307)
2022-11-09 18:09:01,367:INFO: Dataset: zara2               Batch:  5/18	Loss 83.6268 (83.7919)
2022-11-09 18:09:02,117:INFO: Dataset: zara2               Batch:  6/18	Loss 85.0989 (84.0058)
2022-11-09 18:09:02,866:INFO: Dataset: zara2               Batch:  7/18	Loss 82.9877 (83.8645)
2022-11-09 18:09:03,614:INFO: Dataset: zara2               Batch:  8/18	Loss 84.0814 (83.8901)
2022-11-09 18:09:04,362:INFO: Dataset: zara2               Batch:  9/18	Loss 83.1170 (83.8046)
2022-11-09 18:09:05,183:INFO: Dataset: zara2               Batch: 10/18	Loss 84.1399 (83.8399)
2022-11-09 18:09:05,933:INFO: Dataset: zara2               Batch: 11/18	Loss 83.7718 (83.8340)
2022-11-09 18:09:06,682:INFO: Dataset: zara2               Batch: 12/18	Loss 83.5224 (83.8092)
2022-11-09 18:09:07,431:INFO: Dataset: zara2               Batch: 13/18	Loss 84.2807 (83.8443)
2022-11-09 18:09:08,201:INFO: Dataset: zara2               Batch: 14/18	Loss 83.6820 (83.8337)
2022-11-09 18:09:08,952:INFO: Dataset: zara2               Batch: 15/18	Loss 82.6306 (83.7548)
2022-11-09 18:09:09,697:INFO: Dataset: zara2               Batch: 16/18	Loss 83.9181 (83.7658)
2022-11-09 18:09:10,446:INFO: Dataset: zara2               Batch: 17/18	Loss 82.7293 (83.7059)
2022-11-09 18:09:11,182:INFO: Dataset: zara2               Batch: 18/18	Loss 70.4517 (83.0705)
2022-11-09 18:09:11,251:INFO: - Computing ADE (validation o)
2022-11-09 18:09:11,530:INFO: 		 ADE on eth                       dataset:	 3.4793708324432373
2022-11-09 18:09:11,530:INFO: Average validation o:	ADE  3.4794	FDE  5.9028
2022-11-09 18:09:11,531:INFO: - Computing ADE (validation)
2022-11-09 18:09:11,804:INFO: 		 ADE on hotel                     dataset:	 0.9339386820793152
2022-11-09 18:09:12,123:INFO: 		 ADE on univ                      dataset:	 1.4859263896942139
2022-11-09 18:09:12,388:INFO: 		 ADE on zara1                     dataset:	 2.473797082901001
2022-11-09 18:09:12,763:INFO: 		 ADE on zara2                     dataset:	 1.4007441997528076
2022-11-09 18:09:12,763:INFO: Average validation:	ADE  1.4819	FDE  2.7012
2022-11-09 18:09:12,763:INFO: - Computing ADE (training)
2022-11-09 18:09:13,099:INFO: 		 ADE on hotel                     dataset:	 1.295864224433899
2022-11-09 18:09:13,722:INFO: 		 ADE on univ                      dataset:	 1.3711082935333252
2022-11-09 18:09:14,170:INFO: 		 ADE on zara1                     dataset:	 2.451911687850952
2022-11-09 18:09:14,884:INFO: 		 ADE on zara2                     dataset:	 1.6814367771148682
2022-11-09 18:09:14,885:INFO: Average training:	ADE  1.5011	FDE  2.7401
2022-11-09 18:09:14,894:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_261.pth.tar
2022-11-09 18:09:14,894:INFO: 
===> EPOCH: 262 (P3)
2022-11-09 18:09:14,895:INFO: - Computing loss (training)
2022-11-09 18:09:15,819:INFO: Dataset: hotel               Batch: 1/4	Loss 83.8702 (83.8702)
2022-11-09 18:09:16,555:INFO: Dataset: hotel               Batch: 2/4	Loss 82.9631 (83.4017)
2022-11-09 18:09:17,278:INFO: Dataset: hotel               Batch: 3/4	Loss 82.4402 (83.0852)
2022-11-09 18:09:17,963:INFO: Dataset: hotel               Batch: 4/4	Loss 49.8283 (77.5570)
2022-11-09 18:09:19,002:INFO: Dataset: univ                Batch:  1/15	Loss 82.6648 (82.6648)
2022-11-09 18:09:19,748:INFO: Dataset: univ                Batch:  2/15	Loss 82.4365 (82.5545)
2022-11-09 18:09:20,518:INFO: Dataset: univ                Batch:  3/15	Loss 80.6881 (81.8654)
2022-11-09 18:09:21,265:INFO: Dataset: univ                Batch:  4/15	Loss 82.7691 (82.0768)
2022-11-09 18:09:22,027:INFO: Dataset: univ                Batch:  5/15	Loss 81.8980 (82.0392)
2022-11-09 18:09:22,783:INFO: Dataset: univ                Batch:  6/15	Loss 82.9123 (82.1831)
2022-11-09 18:09:23,547:INFO: Dataset: univ                Batch:  7/15	Loss 81.9248 (82.1426)
2022-11-09 18:09:24,302:INFO: Dataset: univ                Batch:  8/15	Loss 82.8684 (82.2323)
2022-11-09 18:09:25,065:INFO: Dataset: univ                Batch:  9/15	Loss 80.6400 (82.0404)
2022-11-09 18:09:25,821:INFO: Dataset: univ                Batch: 10/15	Loss 81.6359 (82.0015)
2022-11-09 18:09:26,655:INFO: Dataset: univ                Batch: 11/15	Loss 82.2959 (82.0267)
2022-11-09 18:09:27,412:INFO: Dataset: univ                Batch: 12/15	Loss 84.7431 (82.2407)
2022-11-09 18:09:28,170:INFO: Dataset: univ                Batch: 13/15	Loss 82.9734 (82.2948)
2022-11-09 18:09:28,924:INFO: Dataset: univ                Batch: 14/15	Loss 82.0560 (82.2793)
2022-11-09 18:09:29,593:INFO: Dataset: univ                Batch: 15/15	Loss 15.9193 (81.4554)
2022-11-09 18:09:30,600:INFO: Dataset: zara1               Batch: 1/8	Loss 89.3910 (89.3910)
2022-11-09 18:09:31,336:INFO: Dataset: zara1               Batch: 2/8	Loss 87.5004 (88.3299)
2022-11-09 18:09:32,070:INFO: Dataset: zara1               Batch: 3/8	Loss 86.2109 (87.5878)
2022-11-09 18:09:32,810:INFO: Dataset: zara1               Batch: 4/8	Loss 86.9936 (87.4385)
2022-11-09 18:09:33,547:INFO: Dataset: zara1               Batch: 5/8	Loss 87.7861 (87.5036)
2022-11-09 18:09:34,283:INFO: Dataset: zara1               Batch: 6/8	Loss 87.2294 (87.4490)
2022-11-09 18:09:35,024:INFO: Dataset: zara1               Batch: 7/8	Loss 87.8868 (87.5163)
2022-11-09 18:09:35,746:INFO: Dataset: zara1               Batch: 8/8	Loss 75.0628 (86.0547)
2022-11-09 18:09:36,766:INFO: Dataset: zara2               Batch:  1/18	Loss 82.8909 (82.8909)
2022-11-09 18:09:37,516:INFO: Dataset: zara2               Batch:  2/18	Loss 83.1419 (83.0271)
2022-11-09 18:09:38,269:INFO: Dataset: zara2               Batch:  3/18	Loss 84.1557 (83.3552)
2022-11-09 18:09:39,017:INFO: Dataset: zara2               Batch:  4/18	Loss 84.4206 (83.6485)
2022-11-09 18:09:39,764:INFO: Dataset: zara2               Batch:  5/18	Loss 84.2219 (83.7638)
2022-11-09 18:09:40,513:INFO: Dataset: zara2               Batch:  6/18	Loss 84.1197 (83.8199)
2022-11-09 18:09:41,258:INFO: Dataset: zara2               Batch:  7/18	Loss 82.7516 (83.6555)
2022-11-09 18:09:42,004:INFO: Dataset: zara2               Batch:  8/18	Loss 82.7282 (83.5405)
2022-11-09 18:09:42,752:INFO: Dataset: zara2               Batch:  9/18	Loss 83.3043 (83.5151)
2022-11-09 18:09:43,497:INFO: Dataset: zara2               Batch: 10/18	Loss 83.1113 (83.4743)
2022-11-09 18:09:44,321:INFO: Dataset: zara2               Batch: 11/18	Loss 83.1931 (83.4485)
2022-11-09 18:09:45,069:INFO: Dataset: zara2               Batch: 12/18	Loss 82.8415 (83.3995)
2022-11-09 18:09:45,818:INFO: Dataset: zara2               Batch: 13/18	Loss 83.6282 (83.4148)
2022-11-09 18:09:46,570:INFO: Dataset: zara2               Batch: 14/18	Loss 83.8518 (83.4429)
2022-11-09 18:09:47,315:INFO: Dataset: zara2               Batch: 15/18	Loss 83.0890 (83.4190)
2022-11-09 18:09:48,064:INFO: Dataset: zara2               Batch: 16/18	Loss 83.1344 (83.4013)
2022-11-09 18:09:48,814:INFO: Dataset: zara2               Batch: 17/18	Loss 83.6606 (83.4144)
2022-11-09 18:09:49,553:INFO: Dataset: zara2               Batch: 18/18	Loss 70.9103 (82.7756)
2022-11-09 18:09:49,621:INFO: - Computing ADE (validation o)
2022-11-09 18:09:49,901:INFO: 		 ADE on eth                       dataset:	 3.4822723865509033
2022-11-09 18:09:49,901:INFO: Average validation o:	ADE  3.4823	FDE  5.9313
2022-11-09 18:09:49,902:INFO: - Computing ADE (validation)
2022-11-09 18:09:50,172:INFO: 		 ADE on hotel                     dataset:	 0.9521872401237488
2022-11-09 18:09:50,497:INFO: 		 ADE on univ                      dataset:	 1.4796531200408936
2022-11-09 18:09:50,763:INFO: 		 ADE on zara1                     dataset:	 2.4160449504852295
2022-11-09 18:09:51,132:INFO: 		 ADE on zara2                     dataset:	 1.4004077911376953
2022-11-09 18:09:51,132:INFO: Average validation:	ADE  1.4761	FDE  2.6944
2022-11-09 18:09:51,133:INFO: - Computing ADE (training)
2022-11-09 18:09:51,465:INFO: 		 ADE on hotel                     dataset:	 1.323520541191101
2022-11-09 18:09:52,097:INFO: 		 ADE on univ                      dataset:	 1.3669931888580322
2022-11-09 18:09:52,535:INFO: 		 ADE on zara1                     dataset:	 2.4374454021453857
2022-11-09 18:09:53,244:INFO: 		 ADE on zara2                     dataset:	 1.694990634918213
2022-11-09 18:09:53,244:INFO: Average training:	ADE  1.5007	FDE  2.7428
2022-11-09 18:09:53,253:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_262.pth.tar
2022-11-09 18:09:53,254:INFO: 
===> EPOCH: 263 (P3)
2022-11-09 18:09:53,254:INFO: - Computing loss (training)
2022-11-09 18:09:54,183:INFO: Dataset: hotel               Batch: 1/4	Loss 82.8754 (82.8754)
2022-11-09 18:09:54,918:INFO: Dataset: hotel               Batch: 2/4	Loss 82.6497 (82.7628)
2022-11-09 18:09:55,640:INFO: Dataset: hotel               Batch: 3/4	Loss 82.2997 (82.6165)
2022-11-09 18:09:56,333:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7226 (77.1045)
2022-11-09 18:09:57,344:INFO: Dataset: univ                Batch:  1/15	Loss 81.0910 (81.0910)
2022-11-09 18:09:58,089:INFO: Dataset: univ                Batch:  2/15	Loss 82.7322 (81.8602)
2022-11-09 18:09:58,840:INFO: Dataset: univ                Batch:  3/15	Loss 81.8412 (81.8537)
2022-11-09 18:09:59,590:INFO: Dataset: univ                Batch:  4/15	Loss 81.2469 (81.6952)
2022-11-09 18:10:00,352:INFO: Dataset: univ                Batch:  5/15	Loss 81.3432 (81.6159)
2022-11-09 18:10:01,108:INFO: Dataset: univ                Batch:  6/15	Loss 83.1907 (81.8782)
2022-11-09 18:10:01,854:INFO: Dataset: univ                Batch:  7/15	Loss 82.5907 (81.9667)
2022-11-09 18:10:02,606:INFO: Dataset: univ                Batch:  8/15	Loss 82.6721 (82.0522)
2022-11-09 18:10:03,371:INFO: Dataset: univ                Batch:  9/15	Loss 80.8184 (81.8995)
2022-11-09 18:10:04,122:INFO: Dataset: univ                Batch: 10/15	Loss 83.7790 (82.0741)
2022-11-09 18:10:04,956:INFO: Dataset: univ                Batch: 11/15	Loss 82.2401 (82.0890)
2022-11-09 18:10:05,704:INFO: Dataset: univ                Batch: 12/15	Loss 82.9485 (82.1560)
2022-11-09 18:10:06,467:INFO: Dataset: univ                Batch: 13/15	Loss 81.8136 (82.1287)
2022-11-09 18:10:07,222:INFO: Dataset: univ                Batch: 14/15	Loss 82.6551 (82.1671)
2022-11-09 18:10:07,884:INFO: Dataset: univ                Batch: 15/15	Loss 15.0661 (81.1177)
2022-11-09 18:10:08,916:INFO: Dataset: zara1               Batch: 1/8	Loss 86.6337 (86.6337)
2022-11-09 18:10:09,655:INFO: Dataset: zara1               Batch: 2/8	Loss 89.4455 (87.9545)
2022-11-09 18:10:10,401:INFO: Dataset: zara1               Batch: 3/8	Loss 86.5871 (87.5344)
2022-11-09 18:10:11,139:INFO: Dataset: zara1               Batch: 4/8	Loss 87.8261 (87.6108)
2022-11-09 18:10:11,876:INFO: Dataset: zara1               Batch: 5/8	Loss 87.5479 (87.5979)
2022-11-09 18:10:12,616:INFO: Dataset: zara1               Batch: 6/8	Loss 86.9205 (87.4892)
2022-11-09 18:10:13,354:INFO: Dataset: zara1               Batch: 7/8	Loss 88.0396 (87.5676)
2022-11-09 18:10:14,079:INFO: Dataset: zara1               Batch: 8/8	Loss 74.4930 (86.1431)
2022-11-09 18:10:15,091:INFO: Dataset: zara2               Batch:  1/18	Loss 83.5017 (83.5017)
2022-11-09 18:10:15,837:INFO: Dataset: zara2               Batch:  2/18	Loss 83.8314 (83.6698)
2022-11-09 18:10:16,583:INFO: Dataset: zara2               Batch:  3/18	Loss 82.9059 (83.4216)
2022-11-09 18:10:17,329:INFO: Dataset: zara2               Batch:  4/18	Loss 83.7798 (83.5085)
2022-11-09 18:10:18,076:INFO: Dataset: zara2               Batch:  5/18	Loss 82.0937 (83.2100)
2022-11-09 18:10:18,824:INFO: Dataset: zara2               Batch:  6/18	Loss 84.6535 (83.4406)
2022-11-09 18:10:19,571:INFO: Dataset: zara2               Batch:  7/18	Loss 83.1599 (83.3984)
2022-11-09 18:10:20,319:INFO: Dataset: zara2               Batch:  8/18	Loss 83.6191 (83.4241)
2022-11-09 18:10:21,064:INFO: Dataset: zara2               Batch:  9/18	Loss 83.3814 (83.4192)
2022-11-09 18:10:21,891:INFO: Dataset: zara2               Batch: 10/18	Loss 82.9384 (83.3747)
2022-11-09 18:10:22,640:INFO: Dataset: zara2               Batch: 11/18	Loss 84.2186 (83.4456)
2022-11-09 18:10:23,391:INFO: Dataset: zara2               Batch: 12/18	Loss 83.3306 (83.4363)
2022-11-09 18:10:24,138:INFO: Dataset: zara2               Batch: 13/18	Loss 82.9041 (83.3951)
2022-11-09 18:10:24,882:INFO: Dataset: zara2               Batch: 14/18	Loss 84.7696 (83.4979)
2022-11-09 18:10:25,628:INFO: Dataset: zara2               Batch: 15/18	Loss 82.7679 (83.4478)
2022-11-09 18:10:26,377:INFO: Dataset: zara2               Batch: 16/18	Loss 83.6238 (83.4586)
2022-11-09 18:10:27,126:INFO: Dataset: zara2               Batch: 17/18	Loss 83.4034 (83.4555)
2022-11-09 18:10:27,860:INFO: Dataset: zara2               Batch: 18/18	Loss 71.3833 (82.8667)
2022-11-09 18:10:27,930:INFO: - Computing ADE (validation o)
2022-11-09 18:10:28,207:INFO: 		 ADE on eth                       dataset:	 3.389336109161377
2022-11-09 18:10:28,207:INFO: Average validation o:	ADE  3.3893	FDE  5.7535
2022-11-09 18:10:28,207:INFO: - Computing ADE (validation)
2022-11-09 18:10:28,471:INFO: 		 ADE on hotel                     dataset:	 0.9637302160263062
2022-11-09 18:10:28,799:INFO: 		 ADE on univ                      dataset:	 1.4902150630950928
2022-11-09 18:10:29,073:INFO: 		 ADE on zara1                     dataset:	 2.4869160652160645
2022-11-09 18:10:29,442:INFO: 		 ADE on zara2                     dataset:	 1.3999468088150024
2022-11-09 18:10:29,443:INFO: Average validation:	ADE  1.4862	FDE  2.7260
2022-11-09 18:10:29,443:INFO: - Computing ADE (training)
2022-11-09 18:10:29,768:INFO: 		 ADE on hotel                     dataset:	 1.3136646747589111
2022-11-09 18:10:30,411:INFO: 		 ADE on univ                      dataset:	 1.370908498764038
2022-11-09 18:10:30,854:INFO: 		 ADE on zara1                     dataset:	 2.416910409927368
2022-11-09 18:10:31,546:INFO: 		 ADE on zara2                     dataset:	 1.6580381393432617
2022-11-09 18:10:31,547:INFO: Average training:	ADE  1.4944	FDE  2.7412
2022-11-09 18:10:31,557:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_263.pth.tar
2022-11-09 18:10:31,557:INFO: 
===> EPOCH: 264 (P3)
2022-11-09 18:10:31,557:INFO: - Computing loss (training)
2022-11-09 18:10:32,477:INFO: Dataset: hotel               Batch: 1/4	Loss 81.9330 (81.9330)
2022-11-09 18:10:33,215:INFO: Dataset: hotel               Batch: 2/4	Loss 82.9839 (82.4461)
2022-11-09 18:10:33,941:INFO: Dataset: hotel               Batch: 3/4	Loss 83.0884 (82.6555)
2022-11-09 18:10:34,629:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7283 (77.3483)
2022-11-09 18:10:35,654:INFO: Dataset: univ                Batch:  1/15	Loss 81.1994 (81.1994)
2022-11-09 18:10:36,401:INFO: Dataset: univ                Batch:  2/15	Loss 82.5342 (81.8530)
2022-11-09 18:10:37,148:INFO: Dataset: univ                Batch:  3/15	Loss 82.5295 (82.0617)
2022-11-09 18:10:37,909:INFO: Dataset: univ                Batch:  4/15	Loss 81.1315 (81.8071)
2022-11-09 18:10:38,656:INFO: Dataset: univ                Batch:  5/15	Loss 83.1984 (82.0605)
2022-11-09 18:10:39,411:INFO: Dataset: univ                Batch:  6/15	Loss 81.9006 (82.0338)
2022-11-09 18:10:40,156:INFO: Dataset: univ                Batch:  7/15	Loss 82.8355 (82.1415)
2022-11-09 18:10:40,910:INFO: Dataset: univ                Batch:  8/15	Loss 81.5970 (82.0751)
2022-11-09 18:10:41,667:INFO: Dataset: univ                Batch:  9/15	Loss 82.2872 (82.0998)
2022-11-09 18:10:42,426:INFO: Dataset: univ                Batch: 10/15	Loss 81.6582 (82.0517)
2022-11-09 18:10:43,266:INFO: Dataset: univ                Batch: 11/15	Loss 81.3175 (81.9821)
2022-11-09 18:10:44,021:INFO: Dataset: univ                Batch: 12/15	Loss 82.8400 (82.0496)
2022-11-09 18:10:44,773:INFO: Dataset: univ                Batch: 13/15	Loss 82.2067 (82.0608)
2022-11-09 18:10:45,528:INFO: Dataset: univ                Batch: 14/15	Loss 82.1001 (82.0636)
2022-11-09 18:10:46,191:INFO: Dataset: univ                Batch: 15/15	Loss 15.3348 (81.0517)
2022-11-09 18:10:47,207:INFO: Dataset: zara1               Batch: 1/8	Loss 87.6179 (87.6179)
2022-11-09 18:10:47,947:INFO: Dataset: zara1               Batch: 2/8	Loss 86.5865 (87.0812)
2022-11-09 18:10:48,686:INFO: Dataset: zara1               Batch: 3/8	Loss 87.1652 (87.1091)
2022-11-09 18:10:49,426:INFO: Dataset: zara1               Batch: 4/8	Loss 87.0082 (87.0854)
2022-11-09 18:10:50,164:INFO: Dataset: zara1               Batch: 5/8	Loss 86.9241 (87.0521)
2022-11-09 18:10:50,901:INFO: Dataset: zara1               Batch: 6/8	Loss 86.8577 (87.0202)
2022-11-09 18:10:51,639:INFO: Dataset: zara1               Batch: 7/8	Loss 87.4090 (87.0770)
2022-11-09 18:10:52,363:INFO: Dataset: zara1               Batch: 8/8	Loss 74.3572 (85.7247)
2022-11-09 18:10:53,380:INFO: Dataset: zara2               Batch:  1/18	Loss 84.1717 (84.1717)
2022-11-09 18:10:54,121:INFO: Dataset: zara2               Batch:  2/18	Loss 82.9548 (83.5435)
2022-11-09 18:10:54,863:INFO: Dataset: zara2               Batch:  3/18	Loss 82.9992 (83.3529)
2022-11-09 18:10:55,611:INFO: Dataset: zara2               Batch:  4/18	Loss 83.9965 (83.4832)
2022-11-09 18:10:56,357:INFO: Dataset: zara2               Batch:  5/18	Loss 83.8336 (83.5533)
2022-11-09 18:10:57,106:INFO: Dataset: zara2               Batch:  6/18	Loss 82.4305 (83.3740)
2022-11-09 18:10:57,850:INFO: Dataset: zara2               Batch:  7/18	Loss 83.6422 (83.4083)
2022-11-09 18:10:58,595:INFO: Dataset: zara2               Batch:  8/18	Loss 83.9716 (83.4790)
2022-11-09 18:10:59,410:INFO: Dataset: zara2               Batch:  9/18	Loss 82.5181 (83.3614)
2022-11-09 18:11:00,157:INFO: Dataset: zara2               Batch: 10/18	Loss 82.0196 (83.2106)
2022-11-09 18:11:00,902:INFO: Dataset: zara2               Batch: 11/18	Loss 83.8013 (83.2629)
2022-11-09 18:11:01,645:INFO: Dataset: zara2               Batch: 12/18	Loss 82.6687 (83.2085)
2022-11-09 18:11:02,393:INFO: Dataset: zara2               Batch: 13/18	Loss 82.6855 (83.1750)
2022-11-09 18:11:03,141:INFO: Dataset: zara2               Batch: 14/18	Loss 81.9715 (83.0893)
2022-11-09 18:11:03,884:INFO: Dataset: zara2               Batch: 15/18	Loss 83.3156 (83.1049)
2022-11-09 18:11:04,635:INFO: Dataset: zara2               Batch: 16/18	Loss 82.1652 (83.0505)
2022-11-09 18:11:05,381:INFO: Dataset: zara2               Batch: 17/18	Loss 82.9860 (83.0467)
2022-11-09 18:11:06,114:INFO: Dataset: zara2               Batch: 18/18	Loss 70.4729 (82.4667)
2022-11-09 18:11:06,184:INFO: - Computing ADE (validation o)
2022-11-09 18:11:06,462:INFO: 		 ADE on eth                       dataset:	 3.33451247215271
2022-11-09 18:11:06,462:INFO: Average validation o:	ADE  3.3345	FDE  5.6793
2022-11-09 18:11:06,463:INFO: - Computing ADE (validation)
2022-11-09 18:11:06,732:INFO: 		 ADE on hotel                     dataset:	 1.004833459854126
2022-11-09 18:11:07,053:INFO: 		 ADE on univ                      dataset:	 1.4547460079193115
2022-11-09 18:11:07,321:INFO: 		 ADE on zara1                     dataset:	 2.3577585220336914
2022-11-09 18:11:07,688:INFO: 		 ADE on zara2                     dataset:	 1.3869630098342896
2022-11-09 18:11:07,688:INFO: Average validation:	ADE  1.4577	FDE  2.6625
2022-11-09 18:11:07,689:INFO: - Computing ADE (training)
2022-11-09 18:11:08,012:INFO: 		 ADE on hotel                     dataset:	 1.3350733518600464
2022-11-09 18:11:08,648:INFO: 		 ADE on univ                      dataset:	 1.3489421606063843
2022-11-09 18:11:09,091:INFO: 		 ADE on zara1                     dataset:	 2.353011131286621
2022-11-09 18:11:09,780:INFO: 		 ADE on zara2                     dataset:	 1.6427842378616333
2022-11-09 18:11:09,780:INFO: Average training:	ADE  1.4722	FDE  2.6939
2022-11-09 18:11:09,790:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_264.pth.tar
2022-11-09 18:11:09,791:INFO: 
===> EPOCH: 265 (P3)
2022-11-09 18:11:09,791:INFO: - Computing loss (training)
2022-11-09 18:11:10,711:INFO: Dataset: hotel               Batch: 1/4	Loss 83.4801 (83.4801)
2022-11-09 18:11:11,446:INFO: Dataset: hotel               Batch: 2/4	Loss 82.4617 (82.9941)
2022-11-09 18:11:12,169:INFO: Dataset: hotel               Batch: 3/4	Loss 82.0392 (82.6763)
2022-11-09 18:11:12,856:INFO: Dataset: hotel               Batch: 4/4	Loss 50.2994 (76.9954)
2022-11-09 18:11:13,890:INFO: Dataset: univ                Batch:  1/15	Loss 82.5665 (82.5665)
2022-11-09 18:11:14,648:INFO: Dataset: univ                Batch:  2/15	Loss 80.8497 (81.6639)
2022-11-09 18:11:15,404:INFO: Dataset: univ                Batch:  3/15	Loss 80.9870 (81.4194)
2022-11-09 18:11:16,152:INFO: Dataset: univ                Batch:  4/15	Loss 82.5832 (81.6782)
2022-11-09 18:11:16,898:INFO: Dataset: univ                Batch:  5/15	Loss 82.9692 (81.9121)
2022-11-09 18:11:17,654:INFO: Dataset: univ                Batch:  6/15	Loss 81.6821 (81.8731)
2022-11-09 18:11:18,407:INFO: Dataset: univ                Batch:  7/15	Loss 82.2986 (81.9343)
2022-11-09 18:11:19,159:INFO: Dataset: univ                Batch:  8/15	Loss 81.9958 (81.9418)
2022-11-09 18:11:19,989:INFO: Dataset: univ                Batch:  9/15	Loss 81.8203 (81.9280)
2022-11-09 18:11:20,742:INFO: Dataset: univ                Batch: 10/15	Loss 82.3151 (81.9661)
2022-11-09 18:11:21,493:INFO: Dataset: univ                Batch: 11/15	Loss 81.0507 (81.8828)
2022-11-09 18:11:22,246:INFO: Dataset: univ                Batch: 12/15	Loss 80.8745 (81.8001)
2022-11-09 18:11:23,005:INFO: Dataset: univ                Batch: 13/15	Loss 80.2618 (81.6738)
2022-11-09 18:11:23,762:INFO: Dataset: univ                Batch: 14/15	Loss 82.3383 (81.7208)
2022-11-09 18:11:24,425:INFO: Dataset: univ                Batch: 15/15	Loss 15.0912 (80.8178)
2022-11-09 18:11:25,449:INFO: Dataset: zara1               Batch: 1/8	Loss 91.1889 (91.1889)
2022-11-09 18:11:26,195:INFO: Dataset: zara1               Batch: 2/8	Loss 86.3894 (88.7745)
2022-11-09 18:11:26,934:INFO: Dataset: zara1               Batch: 3/8	Loss 86.3678 (88.0089)
2022-11-09 18:11:27,671:INFO: Dataset: zara1               Batch: 4/8	Loss 92.9619 (89.1401)
2022-11-09 18:11:28,412:INFO: Dataset: zara1               Batch: 5/8	Loss 86.4966 (88.6083)
2022-11-09 18:11:29,158:INFO: Dataset: zara1               Batch: 6/8	Loss 85.3994 (87.9988)
2022-11-09 18:11:29,896:INFO: Dataset: zara1               Batch: 7/8	Loss 85.7152 (87.6894)
2022-11-09 18:11:30,621:INFO: Dataset: zara1               Batch: 8/8	Loss 75.7064 (86.2703)
2022-11-09 18:11:31,636:INFO: Dataset: zara2               Batch:  1/18	Loss 82.2984 (82.2984)
2022-11-09 18:11:32,373:INFO: Dataset: zara2               Batch:  2/18	Loss 83.6420 (82.9940)
2022-11-09 18:11:33,121:INFO: Dataset: zara2               Batch:  3/18	Loss 83.0191 (83.0019)
2022-11-09 18:11:33,864:INFO: Dataset: zara2               Batch:  4/18	Loss 83.2997 (83.0734)
2022-11-09 18:11:34,608:INFO: Dataset: zara2               Batch:  5/18	Loss 82.2345 (82.8933)
2022-11-09 18:11:35,354:INFO: Dataset: zara2               Batch:  6/18	Loss 82.6981 (82.8621)
2022-11-09 18:11:36,096:INFO: Dataset: zara2               Batch:  7/18	Loss 81.8112 (82.7090)
2022-11-09 18:11:36,842:INFO: Dataset: zara2               Batch:  8/18	Loss 85.0183 (82.9811)
2022-11-09 18:11:37,661:INFO: Dataset: zara2               Batch:  9/18	Loss 83.3765 (83.0301)
2022-11-09 18:11:38,409:INFO: Dataset: zara2               Batch: 10/18	Loss 81.7420 (82.9065)
2022-11-09 18:11:39,154:INFO: Dataset: zara2               Batch: 11/18	Loss 82.6134 (82.8801)
2022-11-09 18:11:39,899:INFO: Dataset: zara2               Batch: 12/18	Loss 83.7334 (82.9488)
2022-11-09 18:11:40,644:INFO: Dataset: zara2               Batch: 13/18	Loss 83.2450 (82.9699)
2022-11-09 18:11:41,386:INFO: Dataset: zara2               Batch: 14/18	Loss 83.0395 (82.9752)
2022-11-09 18:11:42,132:INFO: Dataset: zara2               Batch: 15/18	Loss 82.1885 (82.9285)
2022-11-09 18:11:42,878:INFO: Dataset: zara2               Batch: 16/18	Loss 82.4242 (82.8982)
2022-11-09 18:11:43,623:INFO: Dataset: zara2               Batch: 17/18	Loss 83.2046 (82.9173)
2022-11-09 18:11:44,355:INFO: Dataset: zara2               Batch: 18/18	Loss 70.3492 (82.3085)
2022-11-09 18:11:44,431:INFO: - Computing ADE (validation o)
2022-11-09 18:11:44,708:INFO: 		 ADE on eth                       dataset:	 3.251469373703003
2022-11-09 18:11:44,709:INFO: Average validation o:	ADE  3.2515	FDE  5.5332
2022-11-09 18:11:44,709:INFO: - Computing ADE (validation)
2022-11-09 18:11:44,978:INFO: 		 ADE on hotel                     dataset:	 1.3381699323654175
2022-11-09 18:11:45,294:INFO: 		 ADE on univ                      dataset:	 1.6253302097320557
2022-11-09 18:11:45,570:INFO: 		 ADE on zara1                     dataset:	 2.4125187397003174
2022-11-09 18:11:45,935:INFO: 		 ADE on zara2                     dataset:	 1.5982877016067505
2022-11-09 18:11:45,935:INFO: Average validation:	ADE  1.6454	FDE  2.9295
2022-11-09 18:11:45,936:INFO: - Computing ADE (training)
2022-11-09 18:11:46,262:INFO: 		 ADE on hotel                     dataset:	 1.6260405778884888
2022-11-09 18:11:46,887:INFO: 		 ADE on univ                      dataset:	 1.5112555027008057
2022-11-09 18:11:47,325:INFO: 		 ADE on zara1                     dataset:	 2.369065761566162
2022-11-09 18:11:48,033:INFO: 		 ADE on zara2                     dataset:	 1.7952470779418945
2022-11-09 18:11:48,033:INFO: Average training:	ADE  1.6265	FDE  2.9014
2022-11-09 18:11:48,043:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_265.pth.tar
2022-11-09 18:11:48,043:INFO: 
===> EPOCH: 266 (P3)
2022-11-09 18:11:48,044:INFO: - Computing loss (training)
2022-11-09 18:11:48,970:INFO: Dataset: hotel               Batch: 1/4	Loss 87.1021 (87.1021)
2022-11-09 18:11:49,706:INFO: Dataset: hotel               Batch: 2/4	Loss 85.5127 (86.2790)
2022-11-09 18:11:50,423:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8629 (85.1715)
2022-11-09 18:11:51,106:INFO: Dataset: hotel               Batch: 4/4	Loss 50.1613 (78.7976)
2022-11-09 18:11:52,141:INFO: Dataset: univ                Batch:  1/15	Loss 81.0276 (81.0276)
2022-11-09 18:11:52,877:INFO: Dataset: univ                Batch:  2/15	Loss 83.3613 (82.0235)
2022-11-09 18:11:53,626:INFO: Dataset: univ                Batch:  3/15	Loss 81.5002 (81.8524)
2022-11-09 18:11:54,375:INFO: Dataset: univ                Batch:  4/15	Loss 81.0396 (81.6569)
2022-11-09 18:11:55,131:INFO: Dataset: univ                Batch:  5/15	Loss 81.3862 (81.5992)
2022-11-09 18:11:55,889:INFO: Dataset: univ                Batch:  6/15	Loss 82.1876 (81.7058)
2022-11-09 18:11:56,638:INFO: Dataset: univ                Batch:  7/15	Loss 82.0627 (81.7569)
2022-11-09 18:11:57,466:INFO: Dataset: univ                Batch:  8/15	Loss 81.9936 (81.7858)
2022-11-09 18:11:58,223:INFO: Dataset: univ                Batch:  9/15	Loss 80.9853 (81.6932)
2022-11-09 18:11:58,976:INFO: Dataset: univ                Batch: 10/15	Loss 81.7584 (81.6998)
2022-11-09 18:11:59,738:INFO: Dataset: univ                Batch: 11/15	Loss 81.5708 (81.6871)
2022-11-09 18:12:00,491:INFO: Dataset: univ                Batch: 12/15	Loss 81.6108 (81.6808)
2022-11-09 18:12:01,252:INFO: Dataset: univ                Batch: 13/15	Loss 81.1632 (81.6401)
2022-11-09 18:12:02,003:INFO: Dataset: univ                Batch: 14/15	Loss 85.0729 (81.8487)
2022-11-09 18:12:02,668:INFO: Dataset: univ                Batch: 15/15	Loss 15.1204 (80.8811)
2022-11-09 18:12:03,677:INFO: Dataset: zara1               Batch: 1/8	Loss 85.8420 (85.8420)
2022-11-09 18:12:04,413:INFO: Dataset: zara1               Batch: 2/8	Loss 89.0230 (87.4221)
2022-11-09 18:12:05,154:INFO: Dataset: zara1               Batch: 3/8	Loss 87.3974 (87.4146)
2022-11-09 18:12:05,893:INFO: Dataset: zara1               Batch: 4/8	Loss 86.2682 (87.1126)
2022-11-09 18:12:06,634:INFO: Dataset: zara1               Batch: 5/8	Loss 87.9851 (87.3152)
2022-11-09 18:12:07,369:INFO: Dataset: zara1               Batch: 6/8	Loss 87.5652 (87.3596)
2022-11-09 18:12:08,114:INFO: Dataset: zara1               Batch: 7/8	Loss 86.6139 (87.2449)
2022-11-09 18:12:08,838:INFO: Dataset: zara1               Batch: 8/8	Loss 73.7396 (85.6598)
2022-11-09 18:12:09,864:INFO: Dataset: zara2               Batch:  1/18	Loss 82.9475 (82.9475)
2022-11-09 18:12:10,607:INFO: Dataset: zara2               Batch:  2/18	Loss 84.2511 (83.5948)
2022-11-09 18:12:11,350:INFO: Dataset: zara2               Batch:  3/18	Loss 82.6697 (83.2937)
2022-11-09 18:12:12,095:INFO: Dataset: zara2               Batch:  4/18	Loss 82.4146 (83.0685)
2022-11-09 18:12:12,849:INFO: Dataset: zara2               Batch:  5/18	Loss 82.6681 (82.9945)
2022-11-09 18:12:13,597:INFO: Dataset: zara2               Batch:  6/18	Loss 83.0102 (82.9970)
2022-11-09 18:12:14,343:INFO: Dataset: zara2               Batch:  7/18	Loss 82.9054 (82.9844)
2022-11-09 18:12:15,168:INFO: Dataset: zara2               Batch:  8/18	Loss 82.9111 (82.9754)
2022-11-09 18:12:15,914:INFO: Dataset: zara2               Batch:  9/18	Loss 82.1162 (82.8835)
2022-11-09 18:12:16,662:INFO: Dataset: zara2               Batch: 10/18	Loss 84.0101 (82.9861)
2022-11-09 18:12:17,410:INFO: Dataset: zara2               Batch: 11/18	Loss 83.4689 (83.0269)
2022-11-09 18:12:18,157:INFO: Dataset: zara2               Batch: 12/18	Loss 82.3494 (82.9671)
2022-11-09 18:12:18,906:INFO: Dataset: zara2               Batch: 13/18	Loss 82.6565 (82.9456)
2022-11-09 18:12:19,655:INFO: Dataset: zara2               Batch: 14/18	Loss 83.2676 (82.9702)
2022-11-09 18:12:20,404:INFO: Dataset: zara2               Batch: 15/18	Loss 82.4269 (82.9351)
2022-11-09 18:12:21,151:INFO: Dataset: zara2               Batch: 16/18	Loss 82.6603 (82.9177)
2022-11-09 18:12:21,899:INFO: Dataset: zara2               Batch: 17/18	Loss 82.2214 (82.8764)
2022-11-09 18:12:22,635:INFO: Dataset: zara2               Batch: 18/18	Loss 71.2789 (82.3492)
2022-11-09 18:12:22,705:INFO: - Computing ADE (validation o)
2022-11-09 18:12:22,980:INFO: 		 ADE on eth                       dataset:	 3.4475207328796387
2022-11-09 18:12:22,980:INFO: Average validation o:	ADE  3.4475	FDE  5.8538
2022-11-09 18:12:22,981:INFO: - Computing ADE (validation)
2022-11-09 18:12:23,256:INFO: 		 ADE on hotel                     dataset:	 1.3141015768051147
2022-11-09 18:12:23,575:INFO: 		 ADE on univ                      dataset:	 1.5833632946014404
2022-11-09 18:12:23,852:INFO: 		 ADE on zara1                     dataset:	 2.255470037460327
2022-11-09 18:12:24,220:INFO: 		 ADE on zara2                     dataset:	 1.538268804550171
2022-11-09 18:12:24,220:INFO: Average validation:	ADE  1.5912	FDE  2.8361
2022-11-09 18:12:24,221:INFO: - Computing ADE (training)
2022-11-09 18:12:24,559:INFO: 		 ADE on hotel                     dataset:	 1.593165636062622
2022-11-09 18:12:25,238:INFO: 		 ADE on univ                      dataset:	 1.4712393283843994
2022-11-09 18:12:25,683:INFO: 		 ADE on zara1                     dataset:	 2.358250617980957
2022-11-09 18:12:26,387:INFO: 		 ADE on zara2                     dataset:	 1.7824559211730957
2022-11-09 18:12:26,387:INFO: Average training:	ADE  1.5940	FDE  2.8475
2022-11-09 18:12:26,397:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_266.pth.tar
2022-11-09 18:12:26,397:INFO: 
===> EPOCH: 267 (P3)
2022-11-09 18:12:26,397:INFO: - Computing loss (training)
2022-11-09 18:12:27,317:INFO: Dataset: hotel               Batch: 1/4	Loss 86.0281 (86.0281)
2022-11-09 18:12:28,045:INFO: Dataset: hotel               Batch: 2/4	Loss 82.7302 (84.4233)
2022-11-09 18:12:28,770:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8613 (83.8918)
2022-11-09 18:12:29,450:INFO: Dataset: hotel               Batch: 4/4	Loss 49.5452 (77.7746)
2022-11-09 18:12:30,476:INFO: Dataset: univ                Batch:  1/15	Loss 81.4292 (81.4292)
2022-11-09 18:12:31,231:INFO: Dataset: univ                Batch:  2/15	Loss 81.5387 (81.4853)
2022-11-09 18:12:31,978:INFO: Dataset: univ                Batch:  3/15	Loss 81.4904 (81.4870)
2022-11-09 18:12:32,734:INFO: Dataset: univ                Batch:  4/15	Loss 81.2087 (81.4155)
2022-11-09 18:12:33,481:INFO: Dataset: univ                Batch:  5/15	Loss 82.3381 (81.5798)
2022-11-09 18:12:34,229:INFO: Dataset: univ                Batch:  6/15	Loss 83.0178 (81.8026)
2022-11-09 18:12:35,060:INFO: Dataset: univ                Batch:  7/15	Loss 81.9806 (81.8272)
2022-11-09 18:12:35,810:INFO: Dataset: univ                Batch:  8/15	Loss 82.7802 (81.9418)
2022-11-09 18:12:36,556:INFO: Dataset: univ                Batch:  9/15	Loss 81.4006 (81.8848)
2022-11-09 18:12:37,305:INFO: Dataset: univ                Batch: 10/15	Loss 81.8698 (81.8835)
2022-11-09 18:12:38,073:INFO: Dataset: univ                Batch: 11/15	Loss 80.9906 (81.7886)
2022-11-09 18:12:38,828:INFO: Dataset: univ                Batch: 12/15	Loss 81.8875 (81.7965)
2022-11-09 18:12:39,583:INFO: Dataset: univ                Batch: 13/15	Loss 80.8789 (81.7246)
2022-11-09 18:12:40,339:INFO: Dataset: univ                Batch: 14/15	Loss 81.3450 (81.6972)
2022-11-09 18:12:41,000:INFO: Dataset: univ                Batch: 15/15	Loss 15.8649 (80.8611)
2022-11-09 18:12:42,018:INFO: Dataset: zara1               Batch: 1/8	Loss 86.1903 (86.1903)
2022-11-09 18:12:42,758:INFO: Dataset: zara1               Batch: 2/8	Loss 87.7839 (86.9466)
2022-11-09 18:12:43,499:INFO: Dataset: zara1               Batch: 3/8	Loss 87.3216 (87.0550)
2022-11-09 18:12:44,238:INFO: Dataset: zara1               Batch: 4/8	Loss 85.4755 (86.6819)
2022-11-09 18:12:44,977:INFO: Dataset: zara1               Batch: 5/8	Loss 86.1690 (86.5795)
2022-11-09 18:12:45,729:INFO: Dataset: zara1               Batch: 6/8	Loss 85.9605 (86.4858)
2022-11-09 18:12:46,467:INFO: Dataset: zara1               Batch: 7/8	Loss 87.0508 (86.5700)
2022-11-09 18:12:47,192:INFO: Dataset: zara1               Batch: 8/8	Loss 74.9771 (85.3008)
2022-11-09 18:12:48,222:INFO: Dataset: zara2               Batch:  1/18	Loss 82.8928 (82.8928)
2022-11-09 18:12:48,971:INFO: Dataset: zara2               Batch:  2/18	Loss 82.4768 (82.6771)
2022-11-09 18:12:49,722:INFO: Dataset: zara2               Batch:  3/18	Loss 82.7088 (82.6876)
2022-11-09 18:12:50,468:INFO: Dataset: zara2               Batch:  4/18	Loss 81.9136 (82.4729)
2022-11-09 18:12:51,294:INFO: Dataset: zara2               Batch:  5/18	Loss 81.9845 (82.3795)
2022-11-09 18:12:52,040:INFO: Dataset: zara2               Batch:  6/18	Loss 83.4804 (82.5752)
2022-11-09 18:12:52,784:INFO: Dataset: zara2               Batch:  7/18	Loss 82.2390 (82.5234)
2022-11-09 18:12:53,531:INFO: Dataset: zara2               Batch:  8/18	Loss 81.9908 (82.4576)
2022-11-09 18:12:54,277:INFO: Dataset: zara2               Batch:  9/18	Loss 83.1882 (82.5429)
2022-11-09 18:12:55,025:INFO: Dataset: zara2               Batch: 10/18	Loss 83.8820 (82.6716)
2022-11-09 18:12:55,774:INFO: Dataset: zara2               Batch: 11/18	Loss 82.3391 (82.6403)
2022-11-09 18:12:56,518:INFO: Dataset: zara2               Batch: 12/18	Loss 82.8559 (82.6600)
2022-11-09 18:12:57,262:INFO: Dataset: zara2               Batch: 13/18	Loss 82.4700 (82.6442)
2022-11-09 18:12:58,009:INFO: Dataset: zara2               Batch: 14/18	Loss 84.4195 (82.7849)
2022-11-09 18:12:58,757:INFO: Dataset: zara2               Batch: 15/18	Loss 83.1574 (82.8090)
2022-11-09 18:12:59,505:INFO: Dataset: zara2               Batch: 16/18	Loss 83.0094 (82.8215)
2022-11-09 18:13:00,258:INFO: Dataset: zara2               Batch: 17/18	Loss 82.6205 (82.8103)
2022-11-09 18:13:00,993:INFO: Dataset: zara2               Batch: 18/18	Loss 70.2309 (82.1157)
2022-11-09 18:13:01,063:INFO: - Computing ADE (validation o)
2022-11-09 18:13:01,348:INFO: 		 ADE on eth                       dataset:	 3.3494749069213867
2022-11-09 18:13:01,348:INFO: Average validation o:	ADE  3.3495	FDE  5.6751
2022-11-09 18:13:01,349:INFO: - Computing ADE (validation)
2022-11-09 18:13:01,618:INFO: 		 ADE on hotel                     dataset:	 1.0967210531234741
2022-11-09 18:13:01,941:INFO: 		 ADE on univ                      dataset:	 1.444556474685669
2022-11-09 18:13:02,213:INFO: 		 ADE on zara1                     dataset:	 2.2325899600982666
2022-11-09 18:13:02,590:INFO: 		 ADE on zara2                     dataset:	 1.4134249687194824
2022-11-09 18:13:02,590:INFO: Average validation:	ADE  1.4599	FDE  2.6616
2022-11-09 18:13:02,591:INFO: - Computing ADE (training)
2022-11-09 18:13:02,914:INFO: 		 ADE on hotel                     dataset:	 1.3998267650604248
2022-11-09 18:13:03,602:INFO: 		 ADE on univ                      dataset:	 1.3616161346435547
2022-11-09 18:13:04,037:INFO: 		 ADE on zara1                     dataset:	 2.273017406463623
2022-11-09 18:13:04,746:INFO: 		 ADE on zara2                     dataset:	 1.6429194211959839
2022-11-09 18:13:04,746:INFO: Average training:	ADE  1.4778	FDE  2.6956
2022-11-09 18:13:04,756:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_267.pth.tar
2022-11-09 18:13:04,756:INFO: 
===> EPOCH: 268 (P3)
2022-11-09 18:13:04,756:INFO: - Computing loss (training)
2022-11-09 18:13:05,673:INFO: Dataset: hotel               Batch: 1/4	Loss 83.2557 (83.2557)
2022-11-09 18:13:06,403:INFO: Dataset: hotel               Batch: 2/4	Loss 82.6092 (82.9355)
2022-11-09 18:13:07,129:INFO: Dataset: hotel               Batch: 3/4	Loss 85.5349 (83.8241)
2022-11-09 18:13:07,808:INFO: Dataset: hotel               Batch: 4/4	Loss 50.1149 (78.0873)
2022-11-09 18:13:08,834:INFO: Dataset: univ                Batch:  1/15	Loss 80.8719 (80.8719)
2022-11-09 18:13:09,576:INFO: Dataset: univ                Batch:  2/15	Loss 81.0783 (80.9726)
2022-11-09 18:13:10,330:INFO: Dataset: univ                Batch:  3/15	Loss 80.8705 (80.9382)
2022-11-09 18:13:11,070:INFO: Dataset: univ                Batch:  4/15	Loss 82.8207 (81.3618)
2022-11-09 18:13:11,819:INFO: Dataset: univ                Batch:  5/15	Loss 81.2348 (81.3370)
2022-11-09 18:13:12,657:INFO: Dataset: univ                Batch:  6/15	Loss 82.0415 (81.4626)
2022-11-09 18:13:13,402:INFO: Dataset: univ                Batch:  7/15	Loss 81.8557 (81.5172)
2022-11-09 18:13:14,149:INFO: Dataset: univ                Batch:  8/15	Loss 81.5409 (81.5202)
2022-11-09 18:13:14,897:INFO: Dataset: univ                Batch:  9/15	Loss 81.3866 (81.5053)
2022-11-09 18:13:15,652:INFO: Dataset: univ                Batch: 10/15	Loss 80.2893 (81.3745)
2022-11-09 18:13:16,400:INFO: Dataset: univ                Batch: 11/15	Loss 83.5493 (81.5638)
2022-11-09 18:13:17,143:INFO: Dataset: univ                Batch: 12/15	Loss 85.4353 (81.8555)
2022-11-09 18:13:17,895:INFO: Dataset: univ                Batch: 13/15	Loss 81.3140 (81.8137)
2022-11-09 18:13:18,645:INFO: Dataset: univ                Batch: 14/15	Loss 81.0775 (81.7625)
2022-11-09 18:13:19,305:INFO: Dataset: univ                Batch: 15/15	Loss 15.2065 (80.7753)
2022-11-09 18:13:20,326:INFO: Dataset: zara1               Batch: 1/8	Loss 104.4438 (104.4438)
2022-11-09 18:13:21,063:INFO: Dataset: zara1               Batch: 2/8	Loss 86.0540 (95.8895)
2022-11-09 18:13:21,802:INFO: Dataset: zara1               Batch: 3/8	Loss 85.4774 (92.5052)
2022-11-09 18:13:22,539:INFO: Dataset: zara1               Batch: 4/8	Loss 91.0667 (92.1615)
2022-11-09 18:13:23,289:INFO: Dataset: zara1               Batch: 5/8	Loss 87.7595 (91.3095)
2022-11-09 18:13:24,028:INFO: Dataset: zara1               Batch: 6/8	Loss 88.5758 (90.8215)
2022-11-09 18:13:24,767:INFO: Dataset: zara1               Batch: 7/8	Loss 89.1019 (90.5847)
2022-11-09 18:13:25,488:INFO: Dataset: zara1               Batch: 8/8	Loss 75.8317 (88.7444)
2022-11-09 18:13:26,503:INFO: Dataset: zara2               Batch:  1/18	Loss 90.9889 (90.9889)
2022-11-09 18:13:27,242:INFO: Dataset: zara2               Batch:  2/18	Loss 84.2178 (87.4132)
2022-11-09 18:13:27,982:INFO: Dataset: zara2               Batch:  3/18	Loss 85.8091 (86.8558)
2022-11-09 18:13:28,731:INFO: Dataset: zara2               Batch:  4/18	Loss 82.3800 (85.6534)
2022-11-09 18:13:29,483:INFO: Dataset: zara2               Batch:  5/18	Loss 82.9762 (85.1345)
2022-11-09 18:13:30,327:INFO: Dataset: zara2               Batch:  6/18	Loss 83.2877 (84.8347)
2022-11-09 18:13:31,091:INFO: Dataset: zara2               Batch:  7/18	Loss 84.8640 (84.8389)
2022-11-09 18:13:31,852:INFO: Dataset: zara2               Batch:  8/18	Loss 86.0264 (84.9867)
2022-11-09 18:13:32,625:INFO: Dataset: zara2               Batch:  9/18	Loss 85.9376 (85.0900)
2022-11-09 18:13:33,418:INFO: Dataset: zara2               Batch: 10/18	Loss 83.2435 (84.9051)
2022-11-09 18:13:34,296:INFO: Dataset: zara2               Batch: 11/18	Loss 84.1661 (84.8325)
2022-11-09 18:13:35,128:INFO: Dataset: zara2               Batch: 12/18	Loss 84.7539 (84.8261)
2022-11-09 18:13:35,904:INFO: Dataset: zara2               Batch: 13/18	Loss 83.1449 (84.7001)
2022-11-09 18:13:36,737:INFO: Dataset: zara2               Batch: 14/18	Loss 83.9356 (84.6452)
2022-11-09 18:13:37,498:INFO: Dataset: zara2               Batch: 15/18	Loss 83.8241 (84.5947)
2022-11-09 18:13:38,306:INFO: Dataset: zara2               Batch: 16/18	Loss 84.1701 (84.5686)
2022-11-09 18:13:39,100:INFO: Dataset: zara2               Batch: 17/18	Loss 83.7606 (84.5201)
2022-11-09 18:13:39,896:INFO: Dataset: zara2               Batch: 18/18	Loss 71.7721 (83.9237)
2022-11-09 18:13:40,007:INFO: - Computing ADE (validation o)
2022-11-09 18:13:40,318:INFO: 		 ADE on eth                       dataset:	 3.305163860321045
2022-11-09 18:13:40,318:INFO: Average validation o:	ADE  3.3052	FDE  5.6339
2022-11-09 18:13:40,319:INFO: - Computing ADE (validation)
2022-11-09 18:13:40,589:INFO: 		 ADE on hotel                     dataset:	 1.154578447341919
2022-11-09 18:13:40,912:INFO: 		 ADE on univ                      dataset:	 1.4898029565811157
2022-11-09 18:13:41,179:INFO: 		 ADE on zara1                     dataset:	 2.1776912212371826
2022-11-09 18:13:41,559:INFO: 		 ADE on zara2                     dataset:	 1.435215711593628
2022-11-09 18:13:41,559:INFO: Average validation:	ADE  1.4914	FDE  2.7061
2022-11-09 18:13:41,559:INFO: - Computing ADE (training)
2022-11-09 18:13:41,890:INFO: 		 ADE on hotel                     dataset:	 1.4402326345443726
2022-11-09 18:13:42,562:INFO: 		 ADE on univ                      dataset:	 1.3825733661651611
2022-11-09 18:13:43,008:INFO: 		 ADE on zara1                     dataset:	 2.2553365230560303
2022-11-09 18:13:43,762:INFO: 		 ADE on zara2                     dataset:	 1.660453200340271
2022-11-09 18:13:43,762:INFO: Average training:	ADE  1.4961	FDE  2.7188
2022-11-09 18:13:43,772:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_268.pth.tar
2022-11-09 18:13:43,772:INFO: 
===> EPOCH: 269 (P3)
2022-11-09 18:13:43,773:INFO: - Computing loss (training)
2022-11-09 18:13:44,715:INFO: Dataset: hotel               Batch: 1/4	Loss 84.3811 (84.3811)
2022-11-09 18:13:45,458:INFO: Dataset: hotel               Batch: 2/4	Loss 82.9633 (83.6554)
2022-11-09 18:13:46,187:INFO: Dataset: hotel               Batch: 3/4	Loss 82.9298 (83.4181)
2022-11-09 18:13:46,913:INFO: Dataset: hotel               Batch: 4/4	Loss 50.9851 (77.8130)
2022-11-09 18:13:47,949:INFO: Dataset: univ                Batch:  1/15	Loss 83.7379 (83.7379)
2022-11-09 18:13:48,704:INFO: Dataset: univ                Batch:  2/15	Loss 81.6342 (82.7010)
2022-11-09 18:13:49,471:INFO: Dataset: univ                Batch:  3/15	Loss 81.7111 (82.3791)
2022-11-09 18:13:50,293:INFO: Dataset: univ                Batch:  4/15	Loss 81.5316 (82.1633)
2022-11-09 18:13:51,104:INFO: Dataset: univ                Batch:  5/15	Loss 84.7265 (82.6361)
2022-11-09 18:13:51,997:INFO: Dataset: univ                Batch:  6/15	Loss 82.2987 (82.5786)
2022-11-09 18:13:52,816:INFO: Dataset: univ                Batch:  7/15	Loss 81.9188 (82.4880)
2022-11-09 18:13:53,602:INFO: Dataset: univ                Batch:  8/15	Loss 81.8155 (82.4048)
2022-11-09 18:13:54,407:INFO: Dataset: univ                Batch:  9/15	Loss 81.9741 (82.3581)
2022-11-09 18:13:55,197:INFO: Dataset: univ                Batch: 10/15	Loss 81.2864 (82.2452)
2022-11-09 18:13:55,953:INFO: Dataset: univ                Batch: 11/15	Loss 81.1418 (82.1479)
2022-11-09 18:13:56,736:INFO: Dataset: univ                Batch: 12/15	Loss 82.7255 (82.1918)
2022-11-09 18:13:57,508:INFO: Dataset: univ                Batch: 13/15	Loss 81.5933 (82.1476)
2022-11-09 18:13:58,264:INFO: Dataset: univ                Batch: 14/15	Loss 81.6518 (82.1118)
2022-11-09 18:13:58,960:INFO: Dataset: univ                Batch: 15/15	Loss 15.6003 (81.3584)
2022-11-09 18:13:59,966:INFO: Dataset: zara1               Batch: 1/8	Loss 88.2522 (88.2522)
2022-11-09 18:14:00,709:INFO: Dataset: zara1               Batch: 2/8	Loss 85.1914 (86.5718)
2022-11-09 18:14:01,463:INFO: Dataset: zara1               Batch: 3/8	Loss 86.0001 (86.3702)
2022-11-09 18:14:02,216:INFO: Dataset: zara1               Batch: 4/8	Loss 84.8992 (86.0285)
2022-11-09 18:14:02,967:INFO: Dataset: zara1               Batch: 5/8	Loss 84.9586 (85.8319)
2022-11-09 18:14:03,712:INFO: Dataset: zara1               Batch: 6/8	Loss 86.4067 (85.9321)
2022-11-09 18:14:04,455:INFO: Dataset: zara1               Batch: 7/8	Loss 84.8699 (85.7894)
2022-11-09 18:14:05,175:INFO: Dataset: zara1               Batch: 8/8	Loss 73.4893 (84.4299)
2022-11-09 18:14:06,222:INFO: Dataset: zara2               Batch:  1/18	Loss 85.5289 (85.5289)
2022-11-09 18:14:06,978:INFO: Dataset: zara2               Batch:  2/18	Loss 85.0593 (85.2835)
2022-11-09 18:14:07,717:INFO: Dataset: zara2               Batch:  3/18	Loss 81.5887 (84.1137)
2022-11-09 18:14:08,538:INFO: Dataset: zara2               Batch:  4/18	Loss 82.1230 (83.6428)
2022-11-09 18:14:09,286:INFO: Dataset: zara2               Batch:  5/18	Loss 82.8421 (83.4813)
2022-11-09 18:14:10,033:INFO: Dataset: zara2               Batch:  6/18	Loss 81.9558 (83.2402)
2022-11-09 18:14:10,773:INFO: Dataset: zara2               Batch:  7/18	Loss 82.0162 (83.0701)
2022-11-09 18:14:11,509:INFO: Dataset: zara2               Batch:  8/18	Loss 83.0086 (83.0618)
2022-11-09 18:14:12,246:INFO: Dataset: zara2               Batch:  9/18	Loss 80.8579 (82.7989)
2022-11-09 18:14:12,986:INFO: Dataset: zara2               Batch: 10/18	Loss 82.3440 (82.7552)
2022-11-09 18:14:13,728:INFO: Dataset: zara2               Batch: 11/18	Loss 82.4434 (82.7283)
2022-11-09 18:14:14,467:INFO: Dataset: zara2               Batch: 12/18	Loss 82.4360 (82.7041)
2022-11-09 18:14:15,207:INFO: Dataset: zara2               Batch: 13/18	Loss 81.9459 (82.6479)
2022-11-09 18:14:15,947:INFO: Dataset: zara2               Batch: 14/18	Loss 83.1267 (82.6816)
2022-11-09 18:14:16,688:INFO: Dataset: zara2               Batch: 15/18	Loss 81.1741 (82.5819)
2022-11-09 18:14:17,428:INFO: Dataset: zara2               Batch: 16/18	Loss 81.9694 (82.5434)
2022-11-09 18:14:18,168:INFO: Dataset: zara2               Batch: 17/18	Loss 81.4718 (82.4751)
2022-11-09 18:14:18,893:INFO: Dataset: zara2               Batch: 18/18	Loss 70.9352 (81.9180)
2022-11-09 18:14:18,963:INFO: - Computing ADE (validation o)
2022-11-09 18:14:19,246:INFO: 		 ADE on eth                       dataset:	 3.145775079727173
2022-11-09 18:14:19,246:INFO: Average validation o:	ADE  3.1458	FDE  5.3039
2022-11-09 18:14:19,247:INFO: - Computing ADE (validation)
2022-11-09 18:14:19,524:INFO: 		 ADE on hotel                     dataset:	 1.3050200939178467
2022-11-09 18:14:19,847:INFO: 		 ADE on univ                      dataset:	 1.5716477632522583
2022-11-09 18:14:20,117:INFO: 		 ADE on zara1                     dataset:	 2.1352920532226562
2022-11-09 18:14:20,496:INFO: 		 ADE on zara2                     dataset:	 1.5407270193099976
2022-11-09 18:14:20,496:INFO: Average validation:	ADE  1.5785	FDE  2.8940
2022-11-09 18:14:20,497:INFO: - Computing ADE (training)
2022-11-09 18:14:20,831:INFO: 		 ADE on hotel                     dataset:	 1.5959439277648926
2022-11-09 18:14:21,455:INFO: 		 ADE on univ                      dataset:	 1.4807804822921753
2022-11-09 18:14:21,913:INFO: 		 ADE on zara1                     dataset:	 2.229830741882324
2022-11-09 18:14:22,615:INFO: 		 ADE on zara2                     dataset:	 1.7218636274337769
2022-11-09 18:14:22,615:INFO: Average training:	ADE  1.5804	FDE  2.8879
2022-11-09 18:14:22,625:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_269.pth.tar
2022-11-09 18:14:22,625:INFO: 
===> EPOCH: 270 (P3)
2022-11-09 18:14:22,625:INFO: - Computing loss (training)
2022-11-09 18:14:23,550:INFO: Dataset: hotel               Batch: 1/4	Loss 85.5601 (85.5601)
2022-11-09 18:14:24,284:INFO: Dataset: hotel               Batch: 2/4	Loss 84.4686 (85.0012)
2022-11-09 18:14:25,001:INFO: Dataset: hotel               Batch: 3/4	Loss 83.0838 (84.3459)
2022-11-09 18:14:25,682:INFO: Dataset: hotel               Batch: 4/4	Loss 50.3615 (78.6968)
2022-11-09 18:14:26,708:INFO: Dataset: univ                Batch:  1/15	Loss 80.7797 (80.7797)
2022-11-09 18:14:27,452:INFO: Dataset: univ                Batch:  2/15	Loss 81.1283 (80.9467)
2022-11-09 18:14:28,271:INFO: Dataset: univ                Batch:  3/15	Loss 80.6746 (80.8584)
2022-11-09 18:14:29,015:INFO: Dataset: univ                Batch:  4/15	Loss 80.2042 (80.6989)
2022-11-09 18:14:29,781:INFO: Dataset: univ                Batch:  5/15	Loss 80.0461 (80.5661)
2022-11-09 18:14:30,533:INFO: Dataset: univ                Batch:  6/15	Loss 80.0896 (80.4837)
2022-11-09 18:14:31,272:INFO: Dataset: univ                Batch:  7/15	Loss 85.1778 (81.0762)
2022-11-09 18:14:32,018:INFO: Dataset: univ                Batch:  8/15	Loss 82.0918 (81.2041)
2022-11-09 18:14:32,757:INFO: Dataset: univ                Batch:  9/15	Loss 81.4332 (81.2273)
2022-11-09 18:14:33,508:INFO: Dataset: univ                Batch: 10/15	Loss 82.7417 (81.3692)
2022-11-09 18:14:34,263:INFO: Dataset: univ                Batch: 11/15	Loss 81.2504 (81.3586)
2022-11-09 18:14:35,015:INFO: Dataset: univ                Batch: 12/15	Loss 81.3174 (81.3551)
2022-11-09 18:14:35,762:INFO: Dataset: univ                Batch: 13/15	Loss 81.5426 (81.3688)
2022-11-09 18:14:36,511:INFO: Dataset: univ                Batch: 14/15	Loss 81.3199 (81.3654)
2022-11-09 18:14:37,171:INFO: Dataset: univ                Batch: 15/15	Loss 15.1355 (80.4835)
2022-11-09 18:14:38,178:INFO: Dataset: zara1               Batch: 1/8	Loss 86.6647 (86.6647)
2022-11-09 18:14:38,932:INFO: Dataset: zara1               Batch: 2/8	Loss 85.4669 (86.0571)
2022-11-09 18:14:39,685:INFO: Dataset: zara1               Batch: 3/8	Loss 85.4045 (85.8435)
2022-11-09 18:14:40,434:INFO: Dataset: zara1               Batch: 4/8	Loss 86.0734 (85.8946)
2022-11-09 18:14:41,188:INFO: Dataset: zara1               Batch: 5/8	Loss 84.6848 (85.6303)
2022-11-09 18:14:41,937:INFO: Dataset: zara1               Batch: 6/8	Loss 86.2810 (85.7413)
2022-11-09 18:14:42,688:INFO: Dataset: zara1               Batch: 7/8	Loss 88.0331 (86.0671)
2022-11-09 18:14:43,424:INFO: Dataset: zara1               Batch: 8/8	Loss 72.2212 (84.3182)
2022-11-09 18:14:44,433:INFO: Dataset: zara2               Batch:  1/18	Loss 82.6291 (82.6291)
2022-11-09 18:14:45,174:INFO: Dataset: zara2               Batch:  2/18	Loss 81.6878 (82.1303)
2022-11-09 18:14:45,990:INFO: Dataset: zara2               Batch:  3/18	Loss 81.5918 (81.9567)
2022-11-09 18:14:46,732:INFO: Dataset: zara2               Batch:  4/18	Loss 81.6625 (81.8844)
2022-11-09 18:14:47,470:INFO: Dataset: zara2               Batch:  5/18	Loss 80.6433 (81.6374)
2022-11-09 18:14:48,213:INFO: Dataset: zara2               Batch:  6/18	Loss 81.9880 (81.6950)
2022-11-09 18:14:48,952:INFO: Dataset: zara2               Batch:  7/18	Loss 81.7874 (81.7097)
2022-11-09 18:14:49,689:INFO: Dataset: zara2               Batch:  8/18	Loss 83.9689 (82.0029)
2022-11-09 18:14:50,424:INFO: Dataset: zara2               Batch:  9/18	Loss 81.2436 (81.9150)
2022-11-09 18:14:51,166:INFO: Dataset: zara2               Batch: 10/18	Loss 81.7787 (81.9024)
2022-11-09 18:14:51,904:INFO: Dataset: zara2               Batch: 11/18	Loss 82.1347 (81.9227)
2022-11-09 18:14:52,646:INFO: Dataset: zara2               Batch: 12/18	Loss 82.6432 (81.9827)
2022-11-09 18:14:53,386:INFO: Dataset: zara2               Batch: 13/18	Loss 82.6918 (82.0372)
2022-11-09 18:14:54,126:INFO: Dataset: zara2               Batch: 14/18	Loss 81.2324 (81.9788)
2022-11-09 18:14:54,867:INFO: Dataset: zara2               Batch: 15/18	Loss 82.1633 (81.9910)
2022-11-09 18:14:55,611:INFO: Dataset: zara2               Batch: 16/18	Loss 81.5714 (81.9646)
2022-11-09 18:14:56,352:INFO: Dataset: zara2               Batch: 17/18	Loss 81.8511 (81.9581)
2022-11-09 18:14:57,084:INFO: Dataset: zara2               Batch: 18/18	Loss 70.3869 (81.4455)
2022-11-09 18:14:57,153:INFO: - Computing ADE (validation o)
2022-11-09 18:14:57,427:INFO: 		 ADE on eth                       dataset:	 3.01513934135437
2022-11-09 18:14:57,427:INFO: Average validation o:	ADE  3.0151	FDE  5.0551
2022-11-09 18:14:57,428:INFO: - Computing ADE (validation)
2022-11-09 18:14:57,703:INFO: 		 ADE on hotel                     dataset:	 1.2327256202697754
2022-11-09 18:14:58,025:INFO: 		 ADE on univ                      dataset:	 1.5032660961151123
2022-11-09 18:14:58,300:INFO: 		 ADE on zara1                     dataset:	 2.3515784740448
2022-11-09 18:14:58,671:INFO: 		 ADE on zara2                     dataset:	 1.4589574337005615
2022-11-09 18:14:58,671:INFO: Average validation:	ADE  1.5215	FDE  2.8226
2022-11-09 18:14:58,671:INFO: - Computing ADE (training)
2022-11-09 18:14:59,006:INFO: 		 ADE on hotel                     dataset:	 1.5145862102508545
2022-11-09 18:14:59,628:INFO: 		 ADE on univ                      dataset:	 1.4162850379943848
2022-11-09 18:15:00,071:INFO: 		 ADE on zara1                     dataset:	 2.1993439197540283
2022-11-09 18:15:00,807:INFO: 		 ADE on zara2                     dataset:	 1.6452349424362183
2022-11-09 18:15:00,807:INFO: Average training:	ADE  1.5152	FDE  2.8012
2022-11-09 18:15:00,818:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_270.pth.tar
2022-11-09 18:15:00,818:INFO: 
===> EPOCH: 271 (P3)
2022-11-09 18:15:00,818:INFO: - Computing loss (training)
2022-11-09 18:15:01,750:INFO: Dataset: hotel               Batch: 1/4	Loss 83.1199 (83.1199)
2022-11-09 18:15:02,489:INFO: Dataset: hotel               Batch: 2/4	Loss 82.7687 (82.9388)
2022-11-09 18:15:03,214:INFO: Dataset: hotel               Batch: 3/4	Loss 82.6249 (82.8329)
2022-11-09 18:15:03,904:INFO: Dataset: hotel               Batch: 4/4	Loss 49.2777 (76.7239)
2022-11-09 18:15:04,930:INFO: Dataset: univ                Batch:  1/15	Loss 81.7463 (81.7463)
2022-11-09 18:15:05,761:INFO: Dataset: univ                Batch:  2/15	Loss 80.3286 (81.0063)
2022-11-09 18:15:06,512:INFO: Dataset: univ                Batch:  3/15	Loss 80.9871 (80.9997)
2022-11-09 18:15:07,253:INFO: Dataset: univ                Batch:  4/15	Loss 80.8964 (80.9748)
2022-11-09 18:15:08,008:INFO: Dataset: univ                Batch:  5/15	Loss 80.3421 (80.8398)
2022-11-09 18:15:08,771:INFO: Dataset: univ                Batch:  6/15	Loss 80.1767 (80.7201)
2022-11-09 18:15:09,523:INFO: Dataset: univ                Batch:  7/15	Loss 79.9905 (80.6170)
2022-11-09 18:15:10,278:INFO: Dataset: univ                Batch:  8/15	Loss 80.8948 (80.6515)
2022-11-09 18:15:11,031:INFO: Dataset: univ                Batch:  9/15	Loss 80.3704 (80.6207)
2022-11-09 18:15:11,781:INFO: Dataset: univ                Batch: 10/15	Loss 80.9385 (80.6506)
2022-11-09 18:15:12,533:INFO: Dataset: univ                Batch: 11/15	Loss 80.8474 (80.6679)
2022-11-09 18:15:13,298:INFO: Dataset: univ                Batch: 12/15	Loss 80.6735 (80.6684)
2022-11-09 18:15:14,060:INFO: Dataset: univ                Batch: 13/15	Loss 80.5991 (80.6626)
2022-11-09 18:15:14,824:INFO: Dataset: univ                Batch: 14/15	Loss 80.1577 (80.6237)
2022-11-09 18:15:15,488:INFO: Dataset: univ                Batch: 15/15	Loss 15.0019 (79.5975)
2022-11-09 18:15:16,506:INFO: Dataset: zara1               Batch: 1/8	Loss 88.4786 (88.4786)
2022-11-09 18:15:17,242:INFO: Dataset: zara1               Batch: 2/8	Loss 83.1072 (85.7604)
2022-11-09 18:15:17,979:INFO: Dataset: zara1               Batch: 3/8	Loss 84.8757 (85.4639)
2022-11-09 18:15:18,718:INFO: Dataset: zara1               Batch: 4/8	Loss 89.2943 (86.3874)
2022-11-09 18:15:19,453:INFO: Dataset: zara1               Batch: 5/8	Loss 85.5338 (86.2319)
2022-11-09 18:15:20,188:INFO: Dataset: zara1               Batch: 6/8	Loss 83.8089 (85.8106)
2022-11-09 18:15:20,923:INFO: Dataset: zara1               Batch: 7/8	Loss 85.2226 (85.7261)
2022-11-09 18:15:21,644:INFO: Dataset: zara1               Batch: 8/8	Loss 73.5900 (84.4423)
2022-11-09 18:15:22,661:INFO: Dataset: zara2               Batch:  1/18	Loss 82.4613 (82.4613)
2022-11-09 18:15:23,478:INFO: Dataset: zara2               Batch:  2/18	Loss 82.2595 (82.3561)
2022-11-09 18:15:24,223:INFO: Dataset: zara2               Batch:  3/18	Loss 84.4115 (83.0477)
2022-11-09 18:15:24,961:INFO: Dataset: zara2               Batch:  4/18	Loss 81.4498 (82.6346)
2022-11-09 18:15:25,707:INFO: Dataset: zara2               Batch:  5/18	Loss 81.7469 (82.4662)
2022-11-09 18:15:26,457:INFO: Dataset: zara2               Batch:  6/18	Loss 82.1870 (82.4239)
2022-11-09 18:15:27,202:INFO: Dataset: zara2               Batch:  7/18	Loss 82.6442 (82.4555)
2022-11-09 18:15:27,944:INFO: Dataset: zara2               Batch:  8/18	Loss 82.2857 (82.4340)
2022-11-09 18:15:28,690:INFO: Dataset: zara2               Batch:  9/18	Loss 80.5332 (82.2215)
2022-11-09 18:15:29,435:INFO: Dataset: zara2               Batch: 10/18	Loss 82.2112 (82.2206)
2022-11-09 18:15:30,179:INFO: Dataset: zara2               Batch: 11/18	Loss 82.5082 (82.2472)
2022-11-09 18:15:30,927:INFO: Dataset: zara2               Batch: 12/18	Loss 80.6591 (82.1207)
2022-11-09 18:15:31,674:INFO: Dataset: zara2               Batch: 13/18	Loss 81.8841 (82.1026)
2022-11-09 18:15:32,419:INFO: Dataset: zara2               Batch: 14/18	Loss 81.1766 (82.0402)
2022-11-09 18:15:33,167:INFO: Dataset: zara2               Batch: 15/18	Loss 80.8284 (81.9641)
2022-11-09 18:15:33,923:INFO: Dataset: zara2               Batch: 16/18	Loss 82.7039 (82.0087)
2022-11-09 18:15:34,670:INFO: Dataset: zara2               Batch: 17/18	Loss 81.3365 (81.9678)
2022-11-09 18:15:35,410:INFO: Dataset: zara2               Batch: 18/18	Loss 69.9147 (81.4975)
2022-11-09 18:15:35,481:INFO: - Computing ADE (validation o)
2022-11-09 18:15:35,790:INFO: 		 ADE on eth                       dataset:	 3.054309844970703
2022-11-09 18:15:35,791:INFO: Average validation o:	ADE  3.0543	FDE  5.1124
2022-11-09 18:15:35,791:INFO: - Computing ADE (validation)
2022-11-09 18:15:36,071:INFO: 		 ADE on hotel                     dataset:	 1.3344529867172241
2022-11-09 18:15:36,412:INFO: 		 ADE on univ                      dataset:	 1.507464051246643
2022-11-09 18:15:36,686:INFO: 		 ADE on zara1                     dataset:	 2.057074546813965
2022-11-09 18:15:37,071:INFO: 		 ADE on zara2                     dataset:	 1.4190070629119873
2022-11-09 18:15:37,071:INFO: Average validation:	ADE  1.4975	FDE  2.7838
2022-11-09 18:15:37,072:INFO: - Computing ADE (training)
2022-11-09 18:15:37,424:INFO: 		 ADE on hotel                     dataset:	 1.5983190536499023
2022-11-09 18:15:38,148:INFO: 		 ADE on univ                      dataset:	 1.4119093418121338
2022-11-09 18:15:38,687:INFO: 		 ADE on zara1                     dataset:	 2.051710367202759
2022-11-09 18:15:39,423:INFO: 		 ADE on zara2                     dataset:	 1.6161680221557617
2022-11-09 18:15:39,423:INFO: Average training:	ADE  1.4989	FDE  2.7791
2022-11-09 18:15:39,434:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_271.pth.tar
2022-11-09 18:15:39,434:INFO: 
===> EPOCH: 272 (P3)
2022-11-09 18:15:39,434:INFO: - Computing loss (training)
2022-11-09 18:15:40,380:INFO: Dataset: hotel               Batch: 1/4	Loss 82.5771 (82.5771)
2022-11-09 18:15:41,153:INFO: Dataset: hotel               Batch: 2/4	Loss 83.0697 (82.8211)
2022-11-09 18:15:41,928:INFO: Dataset: hotel               Batch: 3/4	Loss 84.6785 (83.4342)
2022-11-09 18:15:42,633:INFO: Dataset: hotel               Batch: 4/4	Loss 50.4022 (77.4641)
2022-11-09 18:15:43,760:INFO: Dataset: univ                Batch:  1/15	Loss 82.2511 (82.2511)
2022-11-09 18:15:44,529:INFO: Dataset: univ                Batch:  2/15	Loss 80.5748 (81.4118)
2022-11-09 18:15:45,297:INFO: Dataset: univ                Batch:  3/15	Loss 80.2125 (80.9977)
2022-11-09 18:15:46,084:INFO: Dataset: univ                Batch:  4/15	Loss 80.9605 (80.9887)
2022-11-09 18:15:46,860:INFO: Dataset: univ                Batch:  5/15	Loss 81.5121 (81.0904)
2022-11-09 18:15:47,655:INFO: Dataset: univ                Batch:  6/15	Loss 80.5271 (80.9886)
2022-11-09 18:15:48,413:INFO: Dataset: univ                Batch:  7/15	Loss 80.0074 (80.8442)
2022-11-09 18:15:49,186:INFO: Dataset: univ                Batch:  8/15	Loss 79.8629 (80.7145)
2022-11-09 18:15:49,961:INFO: Dataset: univ                Batch:  9/15	Loss 80.0494 (80.6424)
2022-11-09 18:15:50,727:INFO: Dataset: univ                Batch: 10/15	Loss 82.7078 (80.8238)
2022-11-09 18:15:51,494:INFO: Dataset: univ                Batch: 11/15	Loss 80.0710 (80.7524)
2022-11-09 18:15:52,292:INFO: Dataset: univ                Batch: 12/15	Loss 80.8404 (80.7598)
2022-11-09 18:15:53,057:INFO: Dataset: univ                Batch: 13/15	Loss 80.2295 (80.7178)
2022-11-09 18:15:53,827:INFO: Dataset: univ                Batch: 14/15	Loss 81.9263 (80.7911)
2022-11-09 18:15:54,531:INFO: Dataset: univ                Batch: 15/15	Loss 14.9978 (79.7404)
2022-11-09 18:15:55,575:INFO: Dataset: zara1               Batch: 1/8	Loss 90.7417 (90.7417)
2022-11-09 18:15:56,370:INFO: Dataset: zara1               Batch: 2/8	Loss 86.4606 (88.4051)
2022-11-09 18:15:57,197:INFO: Dataset: zara1               Batch: 3/8	Loss 86.2168 (87.7118)
2022-11-09 18:15:57,995:INFO: Dataset: zara1               Batch: 4/8	Loss 88.4737 (87.8945)
2022-11-09 18:15:58,745:INFO: Dataset: zara1               Batch: 5/8	Loss 92.1954 (88.6929)
2022-11-09 18:15:59,535:INFO: Dataset: zara1               Batch: 6/8	Loss 84.9449 (88.1542)
2022-11-09 18:16:00,283:INFO: Dataset: zara1               Batch: 7/8	Loss 85.8743 (87.8321)
2022-11-09 18:16:01,016:INFO: Dataset: zara1               Batch: 8/8	Loss 72.7704 (86.1278)
2022-11-09 18:16:02,129:INFO: Dataset: zara2               Batch:  1/18	Loss 82.0289 (82.0289)
2022-11-09 18:16:02,875:INFO: Dataset: zara2               Batch:  2/18	Loss 82.1244 (82.0720)
2022-11-09 18:16:03,615:INFO: Dataset: zara2               Batch:  3/18	Loss 81.4536 (81.8571)
2022-11-09 18:16:04,360:INFO: Dataset: zara2               Batch:  4/18	Loss 81.6864 (81.8158)
2022-11-09 18:16:05,099:INFO: Dataset: zara2               Batch:  5/18	Loss 80.8031 (81.6179)
2022-11-09 18:16:05,834:INFO: Dataset: zara2               Batch:  6/18	Loss 81.8279 (81.6541)
2022-11-09 18:16:06,570:INFO: Dataset: zara2               Batch:  7/18	Loss 80.9260 (81.5559)
2022-11-09 18:16:07,307:INFO: Dataset: zara2               Batch:  8/18	Loss 81.5746 (81.5581)
2022-11-09 18:16:08,043:INFO: Dataset: zara2               Batch:  9/18	Loss 81.8789 (81.5955)
2022-11-09 18:16:08,783:INFO: Dataset: zara2               Batch: 10/18	Loss 80.9764 (81.5376)
2022-11-09 18:16:09,517:INFO: Dataset: zara2               Batch: 11/18	Loss 82.0631 (81.5864)
2022-11-09 18:16:10,252:INFO: Dataset: zara2               Batch: 12/18	Loss 81.4993 (81.5787)
2022-11-09 18:16:10,986:INFO: Dataset: zara2               Batch: 13/18	Loss 83.4778 (81.7269)
2022-11-09 18:16:11,721:INFO: Dataset: zara2               Batch: 14/18	Loss 80.2500 (81.6171)
2022-11-09 18:16:12,458:INFO: Dataset: zara2               Batch: 15/18	Loss 83.7367 (81.7483)
2022-11-09 18:16:13,197:INFO: Dataset: zara2               Batch: 16/18	Loss 80.9127 (81.6954)
2022-11-09 18:16:13,937:INFO: Dataset: zara2               Batch: 17/18	Loss 81.0481 (81.6599)
2022-11-09 18:16:14,663:INFO: Dataset: zara2               Batch: 18/18	Loss 69.7764 (81.1059)
2022-11-09 18:16:14,732:INFO: - Computing ADE (validation o)
2022-11-09 18:16:14,998:INFO: 		 ADE on eth                       dataset:	 2.8220512866973877
2022-11-09 18:16:14,999:INFO: Average validation o:	ADE  2.8221	FDE  4.6801
2022-11-09 18:16:14,999:INFO: - Computing ADE (validation)
2022-11-09 18:16:15,264:INFO: 		 ADE on hotel                     dataset:	 1.3074119091033936
2022-11-09 18:16:15,584:INFO: 		 ADE on univ                      dataset:	 1.4277547597885132
2022-11-09 18:16:15,863:INFO: 		 ADE on zara1                     dataset:	 2.032862424850464
2022-11-09 18:16:16,229:INFO: 		 ADE on zara2                     dataset:	 1.3787856101989746
2022-11-09 18:16:16,229:INFO: Average validation:	ADE  1.4384	FDE  2.6724
2022-11-09 18:16:16,230:INFO: - Computing ADE (training)
2022-11-09 18:16:16,564:INFO: 		 ADE on hotel                     dataset:	 1.5535194873809814
2022-11-09 18:16:17,202:INFO: 		 ADE on univ                      dataset:	 1.3603934049606323
2022-11-09 18:16:17,645:INFO: 		 ADE on zara1                     dataset:	 1.9761747121810913
2022-11-09 18:16:18,345:INFO: 		 ADE on zara2                     dataset:	 1.5402002334594727
2022-11-09 18:16:18,345:INFO: Average training:	ADE  1.4410	FDE  2.6696
2022-11-09 18:16:18,355:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_272.pth.tar
2022-11-09 18:16:18,355:INFO: 
===> EPOCH: 273 (P3)
2022-11-09 18:16:18,355:INFO: - Computing loss (training)
2022-11-09 18:16:19,275:INFO: Dataset: hotel               Batch: 1/4	Loss 84.6567 (84.6567)
2022-11-09 18:16:20,002:INFO: Dataset: hotel               Batch: 2/4	Loss 84.2109 (84.4297)
2022-11-09 18:16:20,718:INFO: Dataset: hotel               Batch: 3/4	Loss 83.1299 (84.0253)
2022-11-09 18:16:21,472:INFO: Dataset: hotel               Batch: 4/4	Loss 52.1808 (78.6479)
2022-11-09 18:16:22,471:INFO: Dataset: univ                Batch:  1/15	Loss 80.6416 (80.6416)
2022-11-09 18:16:23,217:INFO: Dataset: univ                Batch:  2/15	Loss 80.2464 (80.4400)
2022-11-09 18:16:23,960:INFO: Dataset: univ                Batch:  3/15	Loss 81.8047 (80.8506)
2022-11-09 18:16:24,695:INFO: Dataset: univ                Batch:  4/15	Loss 81.1834 (80.9286)
2022-11-09 18:16:25,444:INFO: Dataset: univ                Batch:  5/15	Loss 80.9332 (80.9296)
2022-11-09 18:16:26,183:INFO: Dataset: univ                Batch:  6/15	Loss 80.3613 (80.8415)
2022-11-09 18:16:26,930:INFO: Dataset: univ                Batch:  7/15	Loss 80.7837 (80.8329)
2022-11-09 18:16:27,679:INFO: Dataset: univ                Batch:  8/15	Loss 80.7697 (80.8246)
2022-11-09 18:16:28,426:INFO: Dataset: univ                Batch:  9/15	Loss 81.5200 (80.8985)
2022-11-09 18:16:29,180:INFO: Dataset: univ                Batch: 10/15	Loss 80.0271 (80.8065)
2022-11-09 18:16:29,927:INFO: Dataset: univ                Batch: 11/15	Loss 80.4423 (80.7719)
2022-11-09 18:16:30,684:INFO: Dataset: univ                Batch: 12/15	Loss 79.8236 (80.6878)
2022-11-09 18:16:31,432:INFO: Dataset: univ                Batch: 13/15	Loss 79.7954 (80.6172)
2022-11-09 18:16:32,189:INFO: Dataset: univ                Batch: 14/15	Loss 80.1433 (80.5798)
2022-11-09 18:16:32,840:INFO: Dataset: univ                Batch: 15/15	Loss 15.2857 (79.8434)
2022-11-09 18:16:33,843:INFO: Dataset: zara1               Batch: 1/8	Loss 88.2446 (88.2446)
2022-11-09 18:16:34,573:INFO: Dataset: zara1               Batch: 2/8	Loss 87.1426 (87.7135)
2022-11-09 18:16:35,301:INFO: Dataset: zara1               Batch: 3/8	Loss 84.4081 (86.5695)
2022-11-09 18:16:36,029:INFO: Dataset: zara1               Batch: 4/8	Loss 85.4259 (86.2681)
2022-11-09 18:16:36,770:INFO: Dataset: zara1               Batch: 5/8	Loss 84.6869 (85.9287)
2022-11-09 18:16:37,506:INFO: Dataset: zara1               Batch: 6/8	Loss 90.1741 (86.7344)
2022-11-09 18:16:38,239:INFO: Dataset: zara1               Batch: 7/8	Loss 85.8577 (86.6067)
2022-11-09 18:16:39,037:INFO: Dataset: zara1               Batch: 8/8	Loss 72.7196 (85.1157)
2022-11-09 18:16:40,044:INFO: Dataset: zara2               Batch:  1/18	Loss 82.2342 (82.2342)
2022-11-09 18:16:40,777:INFO: Dataset: zara2               Batch:  2/18	Loss 82.2063 (82.2201)
2022-11-09 18:16:41,513:INFO: Dataset: zara2               Batch:  3/18	Loss 81.3262 (81.9548)
2022-11-09 18:16:42,264:INFO: Dataset: zara2               Batch:  4/18	Loss 81.5655 (81.8540)
2022-11-09 18:16:43,013:INFO: Dataset: zara2               Batch:  5/18	Loss 82.7992 (82.0471)
2022-11-09 18:16:43,783:INFO: Dataset: zara2               Batch:  6/18	Loss 82.1541 (82.0652)
2022-11-09 18:16:44,518:INFO: Dataset: zara2               Batch:  7/18	Loss 81.2309 (81.9530)
2022-11-09 18:16:45,257:INFO: Dataset: zara2               Batch:  8/18	Loss 80.8801 (81.8155)
2022-11-09 18:16:46,001:INFO: Dataset: zara2               Batch:  9/18	Loss 82.2791 (81.8654)
2022-11-09 18:16:46,736:INFO: Dataset: zara2               Batch: 10/18	Loss 81.6399 (81.8412)
2022-11-09 18:16:47,471:INFO: Dataset: zara2               Batch: 11/18	Loss 80.8573 (81.7424)
2022-11-09 18:16:48,208:INFO: Dataset: zara2               Batch: 12/18	Loss 81.0351 (81.6842)
2022-11-09 18:16:48,943:INFO: Dataset: zara2               Batch: 13/18	Loss 82.6532 (81.7619)
2022-11-09 18:16:49,676:INFO: Dataset: zara2               Batch: 14/18	Loss 82.1513 (81.7919)
2022-11-09 18:16:50,412:INFO: Dataset: zara2               Batch: 15/18	Loss 80.9093 (81.7311)
2022-11-09 18:16:51,148:INFO: Dataset: zara2               Batch: 16/18	Loss 83.0981 (81.8143)
2022-11-09 18:16:51,885:INFO: Dataset: zara2               Batch: 17/18	Loss 82.8181 (81.8737)
2022-11-09 18:16:52,596:INFO: Dataset: zara2               Batch: 18/18	Loss 70.5439 (81.3961)
2022-11-09 18:16:52,662:INFO: - Computing ADE (validation o)
2022-11-09 18:16:52,949:INFO: 		 ADE on eth                       dataset:	 2.8603954315185547
2022-11-09 18:16:52,950:INFO: Average validation o:	ADE  2.8604	FDE  4.7574
2022-11-09 18:16:52,950:INFO: - Computing ADE (validation)
2022-11-09 18:16:53,226:INFO: 		 ADE on hotel                     dataset:	 1.2596076726913452
2022-11-09 18:16:53,539:INFO: 		 ADE on univ                      dataset:	 1.486051321029663
2022-11-09 18:16:53,817:INFO: 		 ADE on zara1                     dataset:	 2.244187116622925
2022-11-09 18:16:54,190:INFO: 		 ADE on zara2                     dataset:	 1.4307360649108887
2022-11-09 18:16:54,190:INFO: Average validation:	ADE  1.4974	FDE  2.7829
2022-11-09 18:16:54,190:INFO: - Computing ADE (training)
2022-11-09 18:16:54,516:INFO: 		 ADE on hotel                     dataset:	 1.5444676876068115
2022-11-09 18:16:55,136:INFO: 		 ADE on univ                      dataset:	 1.3989207744598389
2022-11-09 18:16:55,582:INFO: 		 ADE on zara1                     dataset:	 2.0877768993377686
2022-11-09 18:16:56,269:INFO: 		 ADE on zara2                     dataset:	 1.595689058303833
2022-11-09 18:16:56,270:INFO: Average training:	ADE  1.4865	FDE  2.7546
2022-11-09 18:16:56,279:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_273.pth.tar
2022-11-09 18:16:56,279:INFO: 
===> EPOCH: 274 (P3)
2022-11-09 18:16:56,280:INFO: - Computing loss (training)
2022-11-09 18:16:57,193:INFO: Dataset: hotel               Batch: 1/4	Loss 82.7571 (82.7571)
2022-11-09 18:16:57,919:INFO: Dataset: hotel               Batch: 2/4	Loss 81.4585 (82.1230)
2022-11-09 18:16:58,632:INFO: Dataset: hotel               Batch: 3/4	Loss 82.1125 (82.1195)
2022-11-09 18:16:59,306:INFO: Dataset: hotel               Batch: 4/4	Loss 55.6063 (77.9572)
2022-11-09 18:17:00,388:INFO: Dataset: univ                Batch:  1/15	Loss 84.0665 (84.0665)
2022-11-09 18:17:01,140:INFO: Dataset: univ                Batch:  2/15	Loss 79.9587 (81.8782)
2022-11-09 18:17:01,884:INFO: Dataset: univ                Batch:  3/15	Loss 80.2850 (81.3687)
2022-11-09 18:17:02,628:INFO: Dataset: univ                Batch:  4/15	Loss 80.4082 (81.1358)
2022-11-09 18:17:03,382:INFO: Dataset: univ                Batch:  5/15	Loss 79.6909 (80.8283)
2022-11-09 18:17:04,121:INFO: Dataset: univ                Batch:  6/15	Loss 83.5924 (81.2455)
2022-11-09 18:17:04,868:INFO: Dataset: univ                Batch:  7/15	Loss 79.9770 (81.0662)
2022-11-09 18:17:05,601:INFO: Dataset: univ                Batch:  8/15	Loss 84.2997 (81.4126)
2022-11-09 18:17:06,356:INFO: Dataset: univ                Batch:  9/15	Loss 80.6028 (81.3185)
2022-11-09 18:17:07,108:INFO: Dataset: univ                Batch: 10/15	Loss 82.0886 (81.4008)
2022-11-09 18:17:07,864:INFO: Dataset: univ                Batch: 11/15	Loss 82.2404 (81.4800)
2022-11-09 18:17:08,610:INFO: Dataset: univ                Batch: 12/15	Loss 80.9973 (81.4411)
2022-11-09 18:17:09,367:INFO: Dataset: univ                Batch: 13/15	Loss 81.6666 (81.4595)
2022-11-09 18:17:10,109:INFO: Dataset: univ                Batch: 14/15	Loss 81.3881 (81.4549)
2022-11-09 18:17:10,766:INFO: Dataset: univ                Batch: 15/15	Loss 14.8985 (80.5591)
2022-11-09 18:17:11,782:INFO: Dataset: zara1               Batch: 1/8	Loss 89.3883 (89.3883)
2022-11-09 18:17:12,514:INFO: Dataset: zara1               Batch: 2/8	Loss 97.2859 (93.3755)
2022-11-09 18:17:13,247:INFO: Dataset: zara1               Batch: 3/8	Loss 90.6064 (92.4853)
2022-11-09 18:17:13,976:INFO: Dataset: zara1               Batch: 4/8	Loss 85.4837 (90.8408)
2022-11-09 18:17:14,705:INFO: Dataset: zara1               Batch: 5/8	Loss 84.6334 (89.6931)
2022-11-09 18:17:15,509:INFO: Dataset: zara1               Batch: 6/8	Loss 92.9212 (90.1987)
2022-11-09 18:17:16,239:INFO: Dataset: zara1               Batch: 7/8	Loss 95.8170 (91.0142)
2022-11-09 18:17:16,954:INFO: Dataset: zara1               Batch: 8/8	Loss 73.5221 (89.0624)
2022-11-09 18:17:18,033:INFO: Dataset: zara2               Batch:  1/18	Loss 84.3763 (84.3763)
2022-11-09 18:17:18,769:INFO: Dataset: zara2               Batch:  2/18	Loss 88.0969 (86.4445)
2022-11-09 18:17:19,501:INFO: Dataset: zara2               Batch:  3/18	Loss 85.7827 (86.2219)
2022-11-09 18:17:20,233:INFO: Dataset: zara2               Batch:  4/18	Loss 89.5652 (87.1387)
2022-11-09 18:17:20,966:INFO: Dataset: zara2               Batch:  5/18	Loss 84.4861 (86.6183)
2022-11-09 18:17:21,703:INFO: Dataset: zara2               Batch:  6/18	Loss 83.9527 (86.1844)
2022-11-09 18:17:22,439:INFO: Dataset: zara2               Batch:  7/18	Loss 83.5622 (85.8276)
2022-11-09 18:17:23,173:INFO: Dataset: zara2               Batch:  8/18	Loss 83.9180 (85.5921)
2022-11-09 18:17:23,911:INFO: Dataset: zara2               Batch:  9/18	Loss 83.0073 (85.3257)
2022-11-09 18:17:24,648:INFO: Dataset: zara2               Batch: 10/18	Loss 83.3717 (85.1280)
2022-11-09 18:17:25,380:INFO: Dataset: zara2               Batch: 11/18	Loss 82.9639 (84.9203)
2022-11-09 18:17:26,113:INFO: Dataset: zara2               Batch: 12/18	Loss 83.1036 (84.7725)
2022-11-09 18:17:26,852:INFO: Dataset: zara2               Batch: 13/18	Loss 83.4206 (84.6691)
2022-11-09 18:17:27,588:INFO: Dataset: zara2               Batch: 14/18	Loss 83.0387 (84.5642)
2022-11-09 18:17:28,330:INFO: Dataset: zara2               Batch: 15/18	Loss 84.7463 (84.5764)
2022-11-09 18:17:29,064:INFO: Dataset: zara2               Batch: 16/18	Loss 83.4287 (84.5062)
2022-11-09 18:17:29,797:INFO: Dataset: zara2               Batch: 17/18	Loss 83.1951 (84.4204)
2022-11-09 18:17:30,519:INFO: Dataset: zara2               Batch: 18/18	Loss 71.2691 (83.8181)
2022-11-09 18:17:30,588:INFO: - Computing ADE (validation o)
2022-11-09 18:17:30,865:INFO: 		 ADE on eth                       dataset:	 2.868940591812134
2022-11-09 18:17:30,866:INFO: Average validation o:	ADE  2.8689	FDE  4.7557
2022-11-09 18:17:30,867:INFO: - Computing ADE (validation)
2022-11-09 18:17:31,135:INFO: 		 ADE on hotel                     dataset:	 1.1666339635849
2022-11-09 18:17:31,451:INFO: 		 ADE on univ                      dataset:	 1.4341682195663452
2022-11-09 18:17:31,735:INFO: 		 ADE on zara1                     dataset:	 2.2034292221069336
2022-11-09 18:17:32,121:INFO: 		 ADE on zara2                     dataset:	 1.3941069841384888
2022-11-09 18:17:32,121:INFO: Average validation:	ADE  1.4495	FDE  2.7151
2022-11-09 18:17:32,122:INFO: - Computing ADE (training)
2022-11-09 18:17:32,472:INFO: 		 ADE on hotel                     dataset:	 1.471001386642456
2022-11-09 18:17:33,116:INFO: 		 ADE on univ                      dataset:	 1.351971983909607
2022-11-09 18:17:33,558:INFO: 		 ADE on zara1                     dataset:	 2.090425968170166
2022-11-09 18:17:34,266:INFO: 		 ADE on zara2                     dataset:	 1.560125470161438
2022-11-09 18:17:34,267:INFO: Average training:	ADE  1.4443	FDE  2.6964
2022-11-09 18:17:34,276:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_274.pth.tar
2022-11-09 18:17:34,276:INFO: 
===> EPOCH: 275 (P3)
2022-11-09 18:17:34,277:INFO: - Computing loss (training)
2022-11-09 18:17:35,199:INFO: Dataset: hotel               Batch: 1/4	Loss 82.8294 (82.8294)
2022-11-09 18:17:35,933:INFO: Dataset: hotel               Batch: 2/4	Loss 83.0437 (82.9395)
2022-11-09 18:17:36,732:INFO: Dataset: hotel               Batch: 3/4	Loss 84.9527 (83.5654)
2022-11-09 18:17:37,419:INFO: Dataset: hotel               Batch: 4/4	Loss 50.0544 (77.6413)
2022-11-09 18:17:38,452:INFO: Dataset: univ                Batch:  1/15	Loss 81.6643 (81.6643)
2022-11-09 18:17:39,212:INFO: Dataset: univ                Batch:  2/15	Loss 80.5900 (81.1006)
2022-11-09 18:17:39,957:INFO: Dataset: univ                Batch:  3/15	Loss 83.5623 (81.8847)
2022-11-09 18:17:40,707:INFO: Dataset: univ                Batch:  4/15	Loss 81.0896 (81.6833)
2022-11-09 18:17:41,452:INFO: Dataset: univ                Batch:  5/15	Loss 82.9208 (81.9135)
2022-11-09 18:17:42,211:INFO: Dataset: univ                Batch:  6/15	Loss 81.0831 (81.7724)
2022-11-09 18:17:42,954:INFO: Dataset: univ                Batch:  7/15	Loss 81.1884 (81.6942)
2022-11-09 18:17:43,721:INFO: Dataset: univ                Batch:  8/15	Loss 80.5708 (81.5313)
2022-11-09 18:17:44,474:INFO: Dataset: univ                Batch:  9/15	Loss 80.9754 (81.4714)
2022-11-09 18:17:45,233:INFO: Dataset: univ                Batch: 10/15	Loss 80.7099 (81.3888)
2022-11-09 18:17:45,983:INFO: Dataset: univ                Batch: 11/15	Loss 82.2276 (81.4633)
2022-11-09 18:17:46,739:INFO: Dataset: univ                Batch: 12/15	Loss 81.3580 (81.4545)
2022-11-09 18:17:47,486:INFO: Dataset: univ                Batch: 13/15	Loss 80.9280 (81.4177)
2022-11-09 18:17:48,249:INFO: Dataset: univ                Batch: 14/15	Loss 80.3410 (81.3345)
2022-11-09 18:17:48,912:INFO: Dataset: univ                Batch: 15/15	Loss 14.9490 (80.4726)
2022-11-09 18:17:49,927:INFO: Dataset: zara1               Batch: 1/8	Loss 89.6653 (89.6653)
2022-11-09 18:17:50,660:INFO: Dataset: zara1               Batch: 2/8	Loss 93.9431 (91.8581)
2022-11-09 18:17:51,397:INFO: Dataset: zara1               Batch: 3/8	Loss 82.6302 (88.5741)
2022-11-09 18:17:52,128:INFO: Dataset: zara1               Batch: 4/8	Loss 83.0689 (87.3111)
2022-11-09 18:17:52,938:INFO: Dataset: zara1               Batch: 5/8	Loss 84.2662 (86.6855)
2022-11-09 18:17:53,672:INFO: Dataset: zara1               Batch: 6/8	Loss 88.1785 (86.9374)
2022-11-09 18:17:54,406:INFO: Dataset: zara1               Batch: 7/8	Loss 85.3673 (86.7068)
2022-11-09 18:17:55,122:INFO: Dataset: zara1               Batch: 8/8	Loss 72.1050 (85.1851)
2022-11-09 18:17:56,134:INFO: Dataset: zara2               Batch:  1/18	Loss 85.3161 (85.3161)
2022-11-09 18:17:56,869:INFO: Dataset: zara2               Batch:  2/18	Loss 82.9599 (84.1003)
2022-11-09 18:17:57,602:INFO: Dataset: zara2               Batch:  3/18	Loss 82.4046 (83.5631)
2022-11-09 18:17:58,343:INFO: Dataset: zara2               Batch:  4/18	Loss 82.0738 (83.1782)
2022-11-09 18:17:59,077:INFO: Dataset: zara2               Batch:  5/18	Loss 82.6929 (83.0875)
2022-11-09 18:17:59,818:INFO: Dataset: zara2               Batch:  6/18	Loss 82.4093 (82.9748)
2022-11-09 18:18:00,555:INFO: Dataset: zara2               Batch:  7/18	Loss 81.5291 (82.7644)
2022-11-09 18:18:01,294:INFO: Dataset: zara2               Batch:  8/18	Loss 82.7923 (82.7676)
2022-11-09 18:18:02,030:INFO: Dataset: zara2               Batch:  9/18	Loss 84.2667 (82.9507)
2022-11-09 18:18:02,764:INFO: Dataset: zara2               Batch: 10/18	Loss 81.1455 (82.7624)
2022-11-09 18:18:03,509:INFO: Dataset: zara2               Batch: 11/18	Loss 81.1632 (82.6164)
2022-11-09 18:18:04,244:INFO: Dataset: zara2               Batch: 12/18	Loss 81.3125 (82.5029)
2022-11-09 18:18:04,980:INFO: Dataset: zara2               Batch: 13/18	Loss 81.5393 (82.4262)
2022-11-09 18:18:05,717:INFO: Dataset: zara2               Batch: 14/18	Loss 83.5031 (82.5008)
2022-11-09 18:18:06,458:INFO: Dataset: zara2               Batch: 15/18	Loss 83.0752 (82.5370)
2022-11-09 18:18:07,198:INFO: Dataset: zara2               Batch: 16/18	Loss 81.6479 (82.4821)
2022-11-09 18:18:07,935:INFO: Dataset: zara2               Batch: 17/18	Loss 82.4428 (82.4797)
2022-11-09 18:18:08,662:INFO: Dataset: zara2               Batch: 18/18	Loss 69.8959 (81.8348)
2022-11-09 18:18:08,732:INFO: - Computing ADE (validation o)
2022-11-09 18:18:09,010:INFO: 		 ADE on eth                       dataset:	 2.948190927505493
2022-11-09 18:18:09,010:INFO: Average validation o:	ADE  2.9482	FDE  4.8783
2022-11-09 18:18:09,011:INFO: - Computing ADE (validation)
2022-11-09 18:18:09,280:INFO: 		 ADE on hotel                     dataset:	 1.3348802328109741
2022-11-09 18:18:09,601:INFO: 		 ADE on univ                      dataset:	 1.5114721059799194
2022-11-09 18:18:09,873:INFO: 		 ADE on zara1                     dataset:	 2.2368223667144775
2022-11-09 18:18:10,243:INFO: 		 ADE on zara2                     dataset:	 1.4369131326675415
2022-11-09 18:18:10,243:INFO: Average validation:	ADE  1.5166	FDE  2.8351
2022-11-09 18:18:10,244:INFO: - Computing ADE (training)
2022-11-09 18:18:10,577:INFO: 		 ADE on hotel                     dataset:	 1.5992400646209717
2022-11-09 18:18:11,245:INFO: 		 ADE on univ                      dataset:	 1.425484538078308
2022-11-09 18:18:11,681:INFO: 		 ADE on zara1                     dataset:	 2.0674519538879395
2022-11-09 18:18:12,398:INFO: 		 ADE on zara2                     dataset:	 1.5979069471359253
2022-11-09 18:18:12,398:INFO: Average training:	ADE  1.5058	FDE  2.8041
2022-11-09 18:18:12,408:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_275.pth.tar
2022-11-09 18:18:12,408:INFO: 
===> EPOCH: 276 (P3)
2022-11-09 18:18:12,408:INFO: - Computing loss (training)
2022-11-09 18:18:13,419:INFO: Dataset: hotel               Batch: 1/4	Loss 85.4586 (85.4586)
2022-11-09 18:18:14,144:INFO: Dataset: hotel               Batch: 2/4	Loss 84.8865 (85.1858)
2022-11-09 18:18:14,869:INFO: Dataset: hotel               Batch: 3/4	Loss 83.4928 (84.6043)
2022-11-09 18:18:15,548:INFO: Dataset: hotel               Batch: 4/4	Loss 49.6818 (78.5228)
2022-11-09 18:18:16,556:INFO: Dataset: univ                Batch:  1/15	Loss 81.7375 (81.7375)
2022-11-09 18:18:17,315:INFO: Dataset: univ                Batch:  2/15	Loss 80.5789 (81.1258)
2022-11-09 18:18:18,063:INFO: Dataset: univ                Batch:  3/15	Loss 80.4561 (80.9004)
2022-11-09 18:18:18,809:INFO: Dataset: univ                Batch:  4/15	Loss 81.8390 (81.1184)
2022-11-09 18:18:19,554:INFO: Dataset: univ                Batch:  5/15	Loss 80.4064 (80.9839)
2022-11-09 18:18:20,302:INFO: Dataset: univ                Batch:  6/15	Loss 81.3843 (81.0454)
2022-11-09 18:18:21,042:INFO: Dataset: univ                Batch:  7/15	Loss 80.6011 (80.9870)
2022-11-09 18:18:21,786:INFO: Dataset: univ                Batch:  8/15	Loss 80.8522 (80.9705)
2022-11-09 18:18:22,522:INFO: Dataset: univ                Batch:  9/15	Loss 81.9227 (81.0694)
2022-11-09 18:18:23,260:INFO: Dataset: univ                Batch: 10/15	Loss 83.1700 (81.2583)
2022-11-09 18:18:24,007:INFO: Dataset: univ                Batch: 11/15	Loss 80.3512 (81.1759)
2022-11-09 18:18:24,751:INFO: Dataset: univ                Batch: 12/15	Loss 80.5816 (81.1263)
2022-11-09 18:18:25,504:INFO: Dataset: univ                Batch: 13/15	Loss 81.0058 (81.1162)
2022-11-09 18:18:26,256:INFO: Dataset: univ                Batch: 14/15	Loss 80.5836 (81.0762)
2022-11-09 18:18:26,911:INFO: Dataset: univ                Batch: 15/15	Loss 15.0146 (80.2372)
2022-11-09 18:18:27,902:INFO: Dataset: zara1               Batch: 1/8	Loss 87.2200 (87.2200)
2022-11-09 18:18:28,643:INFO: Dataset: zara1               Batch: 2/8	Loss 84.5536 (85.7842)
2022-11-09 18:18:29,458:INFO: Dataset: zara1               Batch: 3/8	Loss 83.4973 (85.0167)
2022-11-09 18:18:30,190:INFO: Dataset: zara1               Batch: 4/8	Loss 83.6336 (84.6691)
2022-11-09 18:18:30,921:INFO: Dataset: zara1               Batch: 5/8	Loss 83.4860 (84.4394)
2022-11-09 18:18:31,665:INFO: Dataset: zara1               Batch: 6/8	Loss 85.1034 (84.5512)
2022-11-09 18:18:32,407:INFO: Dataset: zara1               Batch: 7/8	Loss 83.8002 (84.4373)
2022-11-09 18:18:33,126:INFO: Dataset: zara1               Batch: 8/8	Loss 71.8087 (83.1146)
2022-11-09 18:18:34,138:INFO: Dataset: zara2               Batch:  1/18	Loss 81.5612 (81.5612)
2022-11-09 18:18:34,876:INFO: Dataset: zara2               Batch:  2/18	Loss 82.2430 (81.9222)
2022-11-09 18:18:35,611:INFO: Dataset: zara2               Batch:  3/18	Loss 82.1452 (81.9979)
2022-11-09 18:18:36,350:INFO: Dataset: zara2               Batch:  4/18	Loss 81.1700 (81.7894)
2022-11-09 18:18:37,086:INFO: Dataset: zara2               Batch:  5/18	Loss 80.4340 (81.5252)
2022-11-09 18:18:37,826:INFO: Dataset: zara2               Batch:  6/18	Loss 83.3836 (81.8252)
2022-11-09 18:18:38,564:INFO: Dataset: zara2               Batch:  7/18	Loss 81.4245 (81.7685)
2022-11-09 18:18:39,303:INFO: Dataset: zara2               Batch:  8/18	Loss 80.0379 (81.5648)
2022-11-09 18:18:40,040:INFO: Dataset: zara2               Batch:  9/18	Loss 80.1819 (81.4091)
2022-11-09 18:18:40,773:INFO: Dataset: zara2               Batch: 10/18	Loss 84.9425 (81.8021)
2022-11-09 18:18:41,507:INFO: Dataset: zara2               Batch: 11/18	Loss 83.1045 (81.9283)
2022-11-09 18:18:42,245:INFO: Dataset: zara2               Batch: 12/18	Loss 83.0278 (82.0222)
2022-11-09 18:18:42,980:INFO: Dataset: zara2               Batch: 13/18	Loss 81.4259 (81.9719)
2022-11-09 18:18:43,717:INFO: Dataset: zara2               Batch: 14/18	Loss 80.2721 (81.8453)
2022-11-09 18:18:44,530:INFO: Dataset: zara2               Batch: 15/18	Loss 83.8434 (81.9797)
2022-11-09 18:18:45,267:INFO: Dataset: zara2               Batch: 16/18	Loss 80.7668 (81.8974)
2022-11-09 18:18:46,007:INFO: Dataset: zara2               Batch: 17/18	Loss 81.0364 (81.8501)
2022-11-09 18:18:46,731:INFO: Dataset: zara2               Batch: 18/18	Loss 70.2135 (81.3115)
2022-11-09 18:18:46,801:INFO: - Computing ADE (validation o)
2022-11-09 18:18:47,079:INFO: 		 ADE on eth                       dataset:	 2.799851417541504
2022-11-09 18:18:47,080:INFO: Average validation o:	ADE  2.7999	FDE  4.6335
2022-11-09 18:18:47,080:INFO: - Computing ADE (validation)
2022-11-09 18:18:47,350:INFO: 		 ADE on hotel                     dataset:	 1.2447495460510254
2022-11-09 18:18:47,666:INFO: 		 ADE on univ                      dataset:	 1.4498006105422974
2022-11-09 18:18:47,933:INFO: 		 ADE on zara1                     dataset:	 2.029675245285034
2022-11-09 18:18:48,304:INFO: 		 ADE on zara2                     dataset:	 1.3604382276535034
2022-11-09 18:18:48,304:INFO: Average validation:	ADE  1.4395	FDE  2.6637
2022-11-09 18:18:48,305:INFO: - Computing ADE (training)
2022-11-09 18:18:48,624:INFO: 		 ADE on hotel                     dataset:	 1.545803427696228
2022-11-09 18:18:49,251:INFO: 		 ADE on univ                      dataset:	 1.3538451194763184
2022-11-09 18:18:49,702:INFO: 		 ADE on zara1                     dataset:	 2.044161558151245
2022-11-09 18:18:50,394:INFO: 		 ADE on zara2                     dataset:	 1.5490127801895142
2022-11-09 18:18:50,395:INFO: Average training:	ADE  1.4423	FDE  2.6597
2022-11-09 18:18:50,404:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_276.pth.tar
2022-11-09 18:18:50,404:INFO: 
===> EPOCH: 277 (P3)
2022-11-09 18:18:50,405:INFO: - Computing loss (training)
2022-11-09 18:18:51,314:INFO: Dataset: hotel               Batch: 1/4	Loss 82.2254 (82.2254)
2022-11-09 18:18:52,042:INFO: Dataset: hotel               Batch: 2/4	Loss 83.1877 (82.7065)
2022-11-09 18:18:52,754:INFO: Dataset: hotel               Batch: 3/4	Loss 84.4567 (83.2826)
2022-11-09 18:18:53,429:INFO: Dataset: hotel               Batch: 4/4	Loss 49.3364 (77.7741)
2022-11-09 18:18:54,460:INFO: Dataset: univ                Batch:  1/15	Loss 80.0249 (80.0249)
2022-11-09 18:18:55,202:INFO: Dataset: univ                Batch:  2/15	Loss 80.0326 (80.0286)
2022-11-09 18:18:55,945:INFO: Dataset: univ                Batch:  3/15	Loss 81.3876 (80.4510)
2022-11-09 18:18:56,693:INFO: Dataset: univ                Batch:  4/15	Loss 79.8411 (80.2888)
2022-11-09 18:18:57,449:INFO: Dataset: univ                Batch:  5/15	Loss 80.6984 (80.3776)
2022-11-09 18:18:58,205:INFO: Dataset: univ                Batch:  6/15	Loss 81.3561 (80.5436)
2022-11-09 18:18:58,960:INFO: Dataset: univ                Batch:  7/15	Loss 79.9588 (80.4563)
2022-11-09 18:18:59,707:INFO: Dataset: univ                Batch:  8/15	Loss 79.4729 (80.3319)
2022-11-09 18:19:00,464:INFO: Dataset: univ                Batch:  9/15	Loss 79.6790 (80.2531)
2022-11-09 18:19:01,213:INFO: Dataset: univ                Batch: 10/15	Loss 80.2367 (80.2515)
2022-11-09 18:19:01,959:INFO: Dataset: univ                Batch: 11/15	Loss 81.0968 (80.3223)
2022-11-09 18:19:02,712:INFO: Dataset: univ                Batch: 12/15	Loss 79.8748 (80.2858)
2022-11-09 18:19:03,465:INFO: Dataset: univ                Batch: 13/15	Loss 79.7904 (80.2485)
2022-11-09 18:19:04,222:INFO: Dataset: univ                Batch: 14/15	Loss 79.9336 (80.2252)
2022-11-09 18:19:04,883:INFO: Dataset: univ                Batch: 15/15	Loss 15.2179 (79.2579)
2022-11-09 18:19:05,897:INFO: Dataset: zara1               Batch: 1/8	Loss 92.0478 (92.0478)
2022-11-09 18:19:06,708:INFO: Dataset: zara1               Batch: 2/8	Loss 91.4811 (91.7700)
2022-11-09 18:19:07,449:INFO: Dataset: zara1               Batch: 3/8	Loss 82.1894 (88.3407)
2022-11-09 18:19:08,183:INFO: Dataset: zara1               Batch: 4/8	Loss 83.6238 (87.1298)
2022-11-09 18:19:08,919:INFO: Dataset: zara1               Batch: 5/8	Loss 86.7256 (87.0491)
2022-11-09 18:19:09,654:INFO: Dataset: zara1               Batch: 6/8	Loss 87.9729 (87.1931)
2022-11-09 18:19:10,396:INFO: Dataset: zara1               Batch: 7/8	Loss 89.5942 (87.5625)
2022-11-09 18:19:11,120:INFO: Dataset: zara1               Batch: 8/8	Loss 71.8420 (85.8250)
2022-11-09 18:19:12,143:INFO: Dataset: zara2               Batch:  1/18	Loss 81.5470 (81.5470)
2022-11-09 18:19:12,892:INFO: Dataset: zara2               Batch:  2/18	Loss 83.2573 (82.4062)
2022-11-09 18:19:13,640:INFO: Dataset: zara2               Batch:  3/18	Loss 81.2095 (81.9752)
2022-11-09 18:19:14,385:INFO: Dataset: zara2               Batch:  4/18	Loss 81.6222 (81.8823)
2022-11-09 18:19:15,143:INFO: Dataset: zara2               Batch:  5/18	Loss 81.5323 (81.8112)
2022-11-09 18:19:15,894:INFO: Dataset: zara2               Batch:  6/18	Loss 80.3367 (81.5801)
2022-11-09 18:19:16,641:INFO: Dataset: zara2               Batch:  7/18	Loss 80.3054 (81.3977)
2022-11-09 18:19:17,386:INFO: Dataset: zara2               Batch:  8/18	Loss 80.9462 (81.3400)
2022-11-09 18:19:18,136:INFO: Dataset: zara2               Batch:  9/18	Loss 80.2186 (81.2265)
2022-11-09 18:19:18,883:INFO: Dataset: zara2               Batch: 10/18	Loss 82.3636 (81.3519)
2022-11-09 18:19:19,634:INFO: Dataset: zara2               Batch: 11/18	Loss 82.2032 (81.4223)
2022-11-09 18:19:20,380:INFO: Dataset: zara2               Batch: 12/18	Loss 81.3420 (81.4154)
2022-11-09 18:19:21,129:INFO: Dataset: zara2               Batch: 13/18	Loss 80.8633 (81.3752)
2022-11-09 18:19:21,876:INFO: Dataset: zara2               Batch: 14/18	Loss 80.8397 (81.3355)
2022-11-09 18:19:22,704:INFO: Dataset: zara2               Batch: 15/18	Loss 80.2587 (81.2536)
2022-11-09 18:19:23,455:INFO: Dataset: zara2               Batch: 16/18	Loss 80.4853 (81.2064)
2022-11-09 18:19:24,205:INFO: Dataset: zara2               Batch: 17/18	Loss 80.6302 (81.1751)
2022-11-09 18:19:24,943:INFO: Dataset: zara2               Batch: 18/18	Loss 69.3441 (80.6353)
2022-11-09 18:19:25,012:INFO: - Computing ADE (validation o)
2022-11-09 18:19:25,287:INFO: 		 ADE on eth                       dataset:	 2.7622692584991455
2022-11-09 18:19:25,287:INFO: Average validation o:	ADE  2.7623	FDE  4.5411
2022-11-09 18:19:25,288:INFO: - Computing ADE (validation)
2022-11-09 18:19:25,559:INFO: 		 ADE on hotel                     dataset:	 1.3278021812438965
2022-11-09 18:19:25,871:INFO: 		 ADE on univ                      dataset:	 1.4867397546768188
2022-11-09 18:19:26,149:INFO: 		 ADE on zara1                     dataset:	 2.048128604888916
2022-11-09 18:19:26,514:INFO: 		 ADE on zara2                     dataset:	 1.4223835468292236
2022-11-09 18:19:26,515:INFO: Average validation:	ADE  1.4871	FDE  2.7572
2022-11-09 18:19:26,515:INFO: - Computing ADE (training)
2022-11-09 18:19:26,847:INFO: 		 ADE on hotel                     dataset:	 1.6047204732894897
2022-11-09 18:19:27,482:INFO: 		 ADE on univ                      dataset:	 1.4084738492965698
2022-11-09 18:19:27,935:INFO: 		 ADE on zara1                     dataset:	 2.039188861846924
2022-11-09 18:19:28,636:INFO: 		 ADE on zara2                     dataset:	 1.5952142477035522
2022-11-09 18:19:28,637:INFO: Average training:	ADE  1.4916	FDE  2.7596
2022-11-09 18:19:28,647:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_277.pth.tar
2022-11-09 18:19:28,647:INFO: 
===> EPOCH: 278 (P3)
2022-11-09 18:19:28,647:INFO: - Computing loss (training)
2022-11-09 18:19:29,569:INFO: Dataset: hotel               Batch: 1/4	Loss 85.6262 (85.6262)
2022-11-09 18:19:30,302:INFO: Dataset: hotel               Batch: 2/4	Loss 86.3338 (86.0072)
2022-11-09 18:19:31,023:INFO: Dataset: hotel               Batch: 3/4	Loss 82.6603 (84.8668)
2022-11-09 18:19:31,713:INFO: Dataset: hotel               Batch: 4/4	Loss 50.4647 (79.1029)
2022-11-09 18:19:32,721:INFO: Dataset: univ                Batch:  1/15	Loss 80.6890 (80.6890)
2022-11-09 18:19:33,461:INFO: Dataset: univ                Batch:  2/15	Loss 83.8275 (82.2285)
2022-11-09 18:19:34,221:INFO: Dataset: univ                Batch:  3/15	Loss 79.5701 (81.2176)
2022-11-09 18:19:34,981:INFO: Dataset: univ                Batch:  4/15	Loss 79.6945 (80.7987)
2022-11-09 18:19:35,733:INFO: Dataset: univ                Batch:  5/15	Loss 80.8401 (80.8067)
2022-11-09 18:19:36,478:INFO: Dataset: univ                Batch:  6/15	Loss 80.4619 (80.7531)
2022-11-09 18:19:37,228:INFO: Dataset: univ                Batch:  7/15	Loss 80.7563 (80.7536)
2022-11-09 18:19:37,976:INFO: Dataset: univ                Batch:  8/15	Loss 80.6458 (80.7401)
2022-11-09 18:19:38,723:INFO: Dataset: univ                Batch:  9/15	Loss 80.4389 (80.7079)
2022-11-09 18:19:39,473:INFO: Dataset: univ                Batch: 10/15	Loss 80.5882 (80.6959)
2022-11-09 18:19:40,228:INFO: Dataset: univ                Batch: 11/15	Loss 80.5637 (80.6834)
2022-11-09 18:19:40,978:INFO: Dataset: univ                Batch: 12/15	Loss 80.1939 (80.6414)
2022-11-09 18:19:41,728:INFO: Dataset: univ                Batch: 13/15	Loss 80.1557 (80.6033)
2022-11-09 18:19:42,550:INFO: Dataset: univ                Batch: 14/15	Loss 81.6556 (80.6711)
2022-11-09 18:19:43,212:INFO: Dataset: univ                Batch: 15/15	Loss 14.8468 (79.5263)
2022-11-09 18:19:44,214:INFO: Dataset: zara1               Batch: 1/8	Loss 88.2301 (88.2301)
2022-11-09 18:19:44,947:INFO: Dataset: zara1               Batch: 2/8	Loss 84.2727 (86.1909)
2022-11-09 18:19:45,673:INFO: Dataset: zara1               Batch: 3/8	Loss 89.6116 (87.3606)
2022-11-09 18:19:46,401:INFO: Dataset: zara1               Batch: 4/8	Loss 83.3247 (86.3258)
2022-11-09 18:19:47,135:INFO: Dataset: zara1               Batch: 5/8	Loss 83.0150 (85.5718)
2022-11-09 18:19:47,867:INFO: Dataset: zara1               Batch: 6/8	Loss 85.6296 (85.5807)
2022-11-09 18:19:48,599:INFO: Dataset: zara1               Batch: 7/8	Loss 88.1630 (85.9634)
2022-11-09 18:19:49,313:INFO: Dataset: zara1               Batch: 8/8	Loss 73.1202 (84.4763)
2022-11-09 18:19:50,321:INFO: Dataset: zara2               Batch:  1/18	Loss 87.9088 (87.9088)
2022-11-09 18:19:51,055:INFO: Dataset: zara2               Batch:  2/18	Loss 84.8184 (86.3742)
2022-11-09 18:19:51,788:INFO: Dataset: zara2               Batch:  3/18	Loss 82.3561 (85.1358)
2022-11-09 18:19:52,522:INFO: Dataset: zara2               Batch:  4/18	Loss 82.0228 (84.3581)
2022-11-09 18:19:53,257:INFO: Dataset: zara2               Batch:  5/18	Loss 81.4142 (83.7970)
2022-11-09 18:19:53,999:INFO: Dataset: zara2               Batch:  6/18	Loss 81.9642 (83.5248)
2022-11-09 18:19:54,734:INFO: Dataset: zara2               Batch:  7/18	Loss 82.6649 (83.4044)
2022-11-09 18:19:55,470:INFO: Dataset: zara2               Batch:  8/18	Loss 85.4565 (83.6598)
2022-11-09 18:19:56,202:INFO: Dataset: zara2               Batch:  9/18	Loss 85.5809 (83.8958)
2022-11-09 18:19:56,936:INFO: Dataset: zara2               Batch: 10/18	Loss 82.8965 (83.7909)
2022-11-09 18:19:57,676:INFO: Dataset: zara2               Batch: 11/18	Loss 83.8445 (83.7953)
2022-11-09 18:19:58,426:INFO: Dataset: zara2               Batch: 12/18	Loss 80.9653 (83.5739)
2022-11-09 18:19:59,165:INFO: Dataset: zara2               Batch: 13/18	Loss 80.9562 (83.3868)
2022-11-09 18:19:59,981:INFO: Dataset: zara2               Batch: 14/18	Loss 82.0088 (83.2872)
2022-11-09 18:20:00,715:INFO: Dataset: zara2               Batch: 15/18	Loss 80.3428 (83.0769)
2022-11-09 18:20:01,453:INFO: Dataset: zara2               Batch: 16/18	Loss 80.7400 (82.9340)
2022-11-09 18:20:02,193:INFO: Dataset: zara2               Batch: 17/18	Loss 81.4091 (82.8419)
2022-11-09 18:20:02,919:INFO: Dataset: zara2               Batch: 18/18	Loss 69.9540 (82.1920)
2022-11-09 18:20:02,988:INFO: - Computing ADE (validation o)
2022-11-09 18:20:03,254:INFO: 		 ADE on eth                       dataset:	 2.791041851043701
2022-11-09 18:20:03,254:INFO: Average validation o:	ADE  2.7910	FDE  4.6187
2022-11-09 18:20:03,255:INFO: - Computing ADE (validation)
2022-11-09 18:20:03,520:INFO: 		 ADE on hotel                     dataset:	 1.187895655632019
2022-11-09 18:20:03,830:INFO: 		 ADE on univ                      dataset:	 1.4537426233291626
2022-11-09 18:20:04,099:INFO: 		 ADE on zara1                     dataset:	 2.247197151184082
2022-11-09 18:20:04,470:INFO: 		 ADE on zara2                     dataset:	 1.3977904319763184
2022-11-09 18:20:04,470:INFO: Average validation:	ADE  1.4648	FDE  2.7616
2022-11-09 18:20:04,471:INFO: - Computing ADE (training)
2022-11-09 18:20:04,804:INFO: 		 ADE on hotel                     dataset:	 1.4835535287857056
2022-11-09 18:20:05,416:INFO: 		 ADE on univ                      dataset:	 1.356427550315857
2022-11-09 18:20:05,853:INFO: 		 ADE on zara1                     dataset:	 2.077218532562256
2022-11-09 18:20:06,557:INFO: 		 ADE on zara2                     dataset:	 1.5586564540863037
2022-11-09 18:20:06,557:INFO: Average training:	ADE  1.4466	FDE  2.7129
2022-11-09 18:20:06,567:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_278.pth.tar
2022-11-09 18:20:06,567:INFO: 
===> EPOCH: 279 (P3)
2022-11-09 18:20:06,568:INFO: - Computing loss (training)
2022-11-09 18:20:07,490:INFO: Dataset: hotel               Batch: 1/4	Loss 81.6683 (81.6683)
2022-11-09 18:20:08,228:INFO: Dataset: hotel               Batch: 2/4	Loss 82.0947 (81.8724)
2022-11-09 18:20:08,954:INFO: Dataset: hotel               Batch: 3/4	Loss 82.6002 (82.1104)
2022-11-09 18:20:09,644:INFO: Dataset: hotel               Batch: 4/4	Loss 50.2866 (76.6105)
2022-11-09 18:20:10,690:INFO: Dataset: univ                Batch:  1/15	Loss 82.2066 (82.2066)
2022-11-09 18:20:11,437:INFO: Dataset: univ                Batch:  2/15	Loss 81.1758 (81.6933)
2022-11-09 18:20:12,179:INFO: Dataset: univ                Batch:  3/15	Loss 81.9043 (81.7613)
2022-11-09 18:20:12,933:INFO: Dataset: univ                Batch:  4/15	Loss 80.5399 (81.4460)
2022-11-09 18:20:13,687:INFO: Dataset: univ                Batch:  5/15	Loss 81.2745 (81.4112)
2022-11-09 18:20:14,432:INFO: Dataset: univ                Batch:  6/15	Loss 81.2396 (81.3848)
2022-11-09 18:20:15,177:INFO: Dataset: univ                Batch:  7/15	Loss 80.7285 (81.2998)
2022-11-09 18:20:15,919:INFO: Dataset: univ                Batch:  8/15	Loss 80.7161 (81.2311)
2022-11-09 18:20:16,675:INFO: Dataset: univ                Batch:  9/15	Loss 79.7172 (81.0488)
2022-11-09 18:20:17,424:INFO: Dataset: univ                Batch: 10/15	Loss 82.7865 (81.2245)
2022-11-09 18:20:18,177:INFO: Dataset: univ                Batch: 11/15	Loss 80.4161 (81.1529)
2022-11-09 18:20:18,946:INFO: Dataset: univ                Batch: 12/15	Loss 79.4732 (81.0087)
2022-11-09 18:20:19,797:INFO: Dataset: univ                Batch: 13/15	Loss 79.9216 (80.9184)
2022-11-09 18:20:20,639:INFO: Dataset: univ                Batch: 14/15	Loss 79.6866 (80.8254)
2022-11-09 18:20:21,453:INFO: Dataset: univ                Batch: 15/15	Loss 14.8496 (79.9156)
2022-11-09 18:20:22,510:INFO: Dataset: zara1               Batch: 1/8	Loss 93.5000 (93.5000)
2022-11-09 18:20:23,307:INFO: Dataset: zara1               Batch: 2/8	Loss 94.7303 (94.0974)
2022-11-09 18:20:24,123:INFO: Dataset: zara1               Batch: 3/8	Loss 84.3430 (90.6008)
2022-11-09 18:20:24,879:INFO: Dataset: zara1               Batch: 4/8	Loss 84.6445 (89.1038)
2022-11-09 18:20:25,700:INFO: Dataset: zara1               Batch: 5/8	Loss 83.6966 (87.9597)
2022-11-09 18:20:26,479:INFO: Dataset: zara1               Batch: 6/8	Loss 84.1415 (87.3149)
2022-11-09 18:20:27,301:INFO: Dataset: zara1               Batch: 7/8	Loss 85.6820 (87.0690)
2022-11-09 18:20:28,055:INFO: Dataset: zara1               Batch: 8/8	Loss 74.4489 (85.6543)
2022-11-09 18:20:29,173:INFO: Dataset: zara2               Batch:  1/18	Loss 88.7352 (88.7352)
2022-11-09 18:20:30,013:INFO: Dataset: zara2               Batch:  2/18	Loss 90.2003 (89.4754)
2022-11-09 18:20:30,770:INFO: Dataset: zara2               Batch:  3/18	Loss 84.6359 (87.8259)
2022-11-09 18:20:31,581:INFO: Dataset: zara2               Batch:  4/18	Loss 81.4207 (86.3084)
2022-11-09 18:20:32,325:INFO: Dataset: zara2               Batch:  5/18	Loss 81.3629 (85.3903)
2022-11-09 18:20:33,104:INFO: Dataset: zara2               Batch:  6/18	Loss 80.5183 (84.4615)
2022-11-09 18:20:33,893:INFO: Dataset: zara2               Batch:  7/18	Loss 82.0199 (84.1175)
2022-11-09 18:20:34,678:INFO: Dataset: zara2               Batch:  8/18	Loss 81.2607 (83.7544)
2022-11-09 18:20:35,472:INFO: Dataset: zara2               Batch:  9/18	Loss 83.1421 (83.6882)
2022-11-09 18:20:36,248:INFO: Dataset: zara2               Batch: 10/18	Loss 81.0778 (83.4183)
2022-11-09 18:20:37,092:INFO: Dataset: zara2               Batch: 11/18	Loss 82.5973 (83.3448)
2022-11-09 18:20:37,878:INFO: Dataset: zara2               Batch: 12/18	Loss 81.6908 (83.1940)
2022-11-09 18:20:38,709:INFO: Dataset: zara2               Batch: 13/18	Loss 81.2729 (83.0486)
2022-11-09 18:20:39,495:INFO: Dataset: zara2               Batch: 14/18	Loss 84.6723 (83.1590)
2022-11-09 18:20:40,259:INFO: Dataset: zara2               Batch: 15/18	Loss 82.4615 (83.1123)
2022-11-09 18:20:41,070:INFO: Dataset: zara2               Batch: 16/18	Loss 79.9212 (82.9130)
2022-11-09 18:20:41,829:INFO: Dataset: zara2               Batch: 17/18	Loss 81.0145 (82.8089)
2022-11-09 18:20:42,566:INFO: Dataset: zara2               Batch: 18/18	Loss 70.2658 (82.1765)
2022-11-09 18:20:42,634:INFO: - Computing ADE (validation o)
2022-11-09 18:20:42,922:INFO: 		 ADE on eth                       dataset:	 2.957592248916626
2022-11-09 18:20:42,923:INFO: Average validation o:	ADE  2.9576	FDE  4.9816
2022-11-09 18:20:42,923:INFO: - Computing ADE (validation)
2022-11-09 18:20:43,196:INFO: 		 ADE on hotel                     dataset:	 1.3610187768936157
2022-11-09 18:20:43,512:INFO: 		 ADE on univ                      dataset:	 1.4692999124526978
2022-11-09 18:20:43,785:INFO: 		 ADE on zara1                     dataset:	 1.879040241241455
2022-11-09 18:20:44,196:INFO: 		 ADE on zara2                     dataset:	 1.4007014036178589
2022-11-09 18:20:44,196:INFO: Average validation:	ADE  1.4620	FDE  2.6986
2022-11-09 18:20:44,197:INFO: - Computing ADE (training)
2022-11-09 18:20:44,563:INFO: 		 ADE on hotel                     dataset:	 1.5981483459472656
2022-11-09 18:20:45,239:INFO: 		 ADE on univ                      dataset:	 1.377611517906189
2022-11-09 18:20:45,762:INFO: 		 ADE on zara1                     dataset:	 2.0398874282836914
2022-11-09 18:20:46,521:INFO: 		 ADE on zara2                     dataset:	 1.5940803289413452
2022-11-09 18:20:46,521:INFO: Average training:	ADE  1.4694	FDE  2.7119
2022-11-09 18:20:46,531:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_279.pth.tar
2022-11-09 18:20:46,531:INFO: 
===> EPOCH: 280 (P3)
2022-11-09 18:20:46,532:INFO: - Computing loss (training)
2022-11-09 18:20:47,495:INFO: Dataset: hotel               Batch: 1/4	Loss 83.6381 (83.6381)
2022-11-09 18:20:48,272:INFO: Dataset: hotel               Batch: 2/4	Loss 84.9828 (84.3262)
2022-11-09 18:20:49,037:INFO: Dataset: hotel               Batch: 3/4	Loss 83.5246 (84.0649)
2022-11-09 18:20:49,752:INFO: Dataset: hotel               Batch: 4/4	Loss 50.3069 (78.4534)
2022-11-09 18:20:51,202:INFO: Dataset: univ                Batch:  1/15	Loss 80.7638 (80.7638)
2022-11-09 18:20:51,960:INFO: Dataset: univ                Batch:  2/15	Loss 81.4295 (81.0865)
2022-11-09 18:20:52,711:INFO: Dataset: univ                Batch:  3/15	Loss 80.2892 (80.8321)
2022-11-09 18:20:53,483:INFO: Dataset: univ                Batch:  4/15	Loss 80.3548 (80.7116)
2022-11-09 18:20:54,268:INFO: Dataset: univ                Batch:  5/15	Loss 80.6730 (80.7035)
2022-11-09 18:20:55,032:INFO: Dataset: univ                Batch:  6/15	Loss 80.6521 (80.6954)
2022-11-09 18:20:55,794:INFO: Dataset: univ                Batch:  7/15	Loss 80.6899 (80.6946)
2022-11-09 18:20:56,553:INFO: Dataset: univ                Batch:  8/15	Loss 80.1245 (80.6205)
2022-11-09 18:20:57,308:INFO: Dataset: univ                Batch:  9/15	Loss 80.1632 (80.5696)
2022-11-09 18:20:58,062:INFO: Dataset: univ                Batch: 10/15	Loss 80.6126 (80.5737)
2022-11-09 18:20:58,895:INFO: Dataset: univ                Batch: 11/15	Loss 80.1739 (80.5376)
2022-11-09 18:20:59,651:INFO: Dataset: univ                Batch: 12/15	Loss 81.5653 (80.6206)
2022-11-09 18:21:00,410:INFO: Dataset: univ                Batch: 13/15	Loss 79.9368 (80.5694)
2022-11-09 18:21:01,177:INFO: Dataset: univ                Batch: 14/15	Loss 80.3036 (80.5494)
2022-11-09 18:21:01,843:INFO: Dataset: univ                Batch: 15/15	Loss 14.9570 (79.7164)
2022-11-09 18:21:02,850:INFO: Dataset: zara1               Batch: 1/8	Loss 90.3483 (90.3483)
2022-11-09 18:21:03,597:INFO: Dataset: zara1               Batch: 2/8	Loss 87.4990 (88.8882)
2022-11-09 18:21:04,339:INFO: Dataset: zara1               Batch: 3/8	Loss 84.8863 (87.5396)
2022-11-09 18:21:05,093:INFO: Dataset: zara1               Batch: 4/8	Loss 83.1487 (86.4127)
2022-11-09 18:21:05,833:INFO: Dataset: zara1               Batch: 5/8	Loss 84.5340 (86.0474)
2022-11-09 18:21:06,574:INFO: Dataset: zara1               Batch: 6/8	Loss 83.3070 (85.6046)
2022-11-09 18:21:07,315:INFO: Dataset: zara1               Batch: 7/8	Loss 85.4370 (85.5820)
2022-11-09 18:21:08,041:INFO: Dataset: zara1               Batch: 8/8	Loss 75.3310 (84.3626)
2022-11-09 18:21:09,082:INFO: Dataset: zara2               Batch:  1/18	Loss 90.5882 (90.5882)
2022-11-09 18:21:09,827:INFO: Dataset: zara2               Batch:  2/18	Loss 86.8100 (88.6883)
2022-11-09 18:21:10,583:INFO: Dataset: zara2               Batch:  3/18	Loss 86.9435 (88.1224)
2022-11-09 18:21:11,326:INFO: Dataset: zara2               Batch:  4/18	Loss 81.8649 (86.6177)
2022-11-09 18:21:12,069:INFO: Dataset: zara2               Batch:  5/18	Loss 81.5099 (85.6081)
2022-11-09 18:21:12,821:INFO: Dataset: zara2               Batch:  6/18	Loss 80.5071 (84.8236)
2022-11-09 18:21:13,568:INFO: Dataset: zara2               Batch:  7/18	Loss 81.4591 (84.3638)
2022-11-09 18:21:14,314:INFO: Dataset: zara2               Batch:  8/18	Loss 82.5459 (84.1374)
2022-11-09 18:21:15,058:INFO: Dataset: zara2               Batch:  9/18	Loss 82.4432 (83.9463)
2022-11-09 18:21:15,805:INFO: Dataset: zara2               Batch: 10/18	Loss 84.9829 (84.0438)
2022-11-09 18:21:16,629:INFO: Dataset: zara2               Batch: 11/18	Loss 82.8293 (83.9320)
2022-11-09 18:21:17,370:INFO: Dataset: zara2               Batch: 12/18	Loss 85.1785 (84.0419)
2022-11-09 18:21:18,113:INFO: Dataset: zara2               Batch: 13/18	Loss 80.7611 (83.7589)
2022-11-09 18:21:18,860:INFO: Dataset: zara2               Batch: 14/18	Loss 80.2968 (83.5179)
2022-11-09 18:21:19,606:INFO: Dataset: zara2               Batch: 15/18	Loss 82.8644 (83.4734)
2022-11-09 18:21:20,364:INFO: Dataset: zara2               Batch: 16/18	Loss 80.7751 (83.3156)
2022-11-09 18:21:21,106:INFO: Dataset: zara2               Batch: 17/18	Loss 80.5038 (83.1367)
2022-11-09 18:21:21,837:INFO: Dataset: zara2               Batch: 18/18	Loss 69.1754 (82.3750)
2022-11-09 18:21:21,907:INFO: - Computing ADE (validation o)
2022-11-09 18:21:22,187:INFO: 		 ADE on eth                       dataset:	 2.956815004348755
2022-11-09 18:21:22,188:INFO: Average validation o:	ADE  2.9568	FDE  4.9637
2022-11-09 18:21:22,188:INFO: - Computing ADE (validation)
2022-11-09 18:21:22,458:INFO: 		 ADE on hotel                     dataset:	 1.2269212007522583
2022-11-09 18:21:22,769:INFO: 		 ADE on univ                      dataset:	 1.48005211353302
2022-11-09 18:21:23,039:INFO: 		 ADE on zara1                     dataset:	 2.179804563522339
2022-11-09 18:21:23,415:INFO: 		 ADE on zara2                     dataset:	 1.416701078414917
2022-11-09 18:21:23,415:INFO: Average validation:	ADE  1.4836	FDE  2.7206
2022-11-09 18:21:23,416:INFO: - Computing ADE (training)
2022-11-09 18:21:23,752:INFO: 		 ADE on hotel                     dataset:	 1.4915804862976074
2022-11-09 18:21:24,420:INFO: 		 ADE on univ                      dataset:	 1.3671985864639282
2022-11-09 18:21:24,854:INFO: 		 ADE on zara1                     dataset:	 2.108844757080078
2022-11-09 18:21:25,610:INFO: 		 ADE on zara2                     dataset:	 1.5980806350708008
2022-11-09 18:21:25,610:INFO: Average training:	ADE  1.4645	FDE  2.6841
2022-11-09 18:21:25,620:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_280.pth.tar
2022-11-09 18:21:25,621:INFO: 
===> EPOCH: 281 (P3)
2022-11-09 18:21:25,621:INFO: - Computing loss (training)
2022-11-09 18:21:26,551:INFO: Dataset: hotel               Batch: 1/4	Loss 84.5820 (84.5820)
2022-11-09 18:21:27,284:INFO: Dataset: hotel               Batch: 2/4	Loss 81.6265 (83.0724)
2022-11-09 18:21:28,004:INFO: Dataset: hotel               Batch: 3/4	Loss 81.7156 (82.6052)
2022-11-09 18:21:28,687:INFO: Dataset: hotel               Batch: 4/4	Loss 49.6803 (77.3059)
2022-11-09 18:21:29,714:INFO: Dataset: univ                Batch:  1/15	Loss 82.6113 (82.6113)
2022-11-09 18:21:30,463:INFO: Dataset: univ                Batch:  2/15	Loss 83.0700 (82.8446)
2022-11-09 18:21:31,217:INFO: Dataset: univ                Batch:  3/15	Loss 84.9107 (83.4849)
2022-11-09 18:21:31,972:INFO: Dataset: univ                Batch:  4/15	Loss 83.8527 (83.5816)
2022-11-09 18:21:32,726:INFO: Dataset: univ                Batch:  5/15	Loss 80.9994 (83.0489)
2022-11-09 18:21:33,475:INFO: Dataset: univ                Batch:  6/15	Loss 80.6214 (82.6747)
2022-11-09 18:21:34,233:INFO: Dataset: univ                Batch:  7/15	Loss 80.6299 (82.3708)
2022-11-09 18:21:34,990:INFO: Dataset: univ                Batch:  8/15	Loss 80.9433 (82.1788)
2022-11-09 18:21:35,741:INFO: Dataset: univ                Batch:  9/15	Loss 80.6605 (82.0097)
2022-11-09 18:21:36,570:INFO: Dataset: univ                Batch: 10/15	Loss 82.0127 (82.0100)
2022-11-09 18:21:37,324:INFO: Dataset: univ                Batch: 11/15	Loss 81.2047 (81.9361)
2022-11-09 18:21:38,086:INFO: Dataset: univ                Batch: 12/15	Loss 82.1191 (81.9506)
2022-11-09 18:21:38,839:INFO: Dataset: univ                Batch: 13/15	Loss 81.8219 (81.9410)
2022-11-09 18:21:39,587:INFO: Dataset: univ                Batch: 14/15	Loss 89.8468 (82.4395)
2022-11-09 18:21:40,246:INFO: Dataset: univ                Batch: 15/15	Loss 14.8819 (81.6583)
2022-11-09 18:21:41,262:INFO: Dataset: zara1               Batch: 1/8	Loss 87.0065 (87.0065)
2022-11-09 18:21:42,002:INFO: Dataset: zara1               Batch: 2/8	Loss 92.0644 (89.3619)
2022-11-09 18:21:42,742:INFO: Dataset: zara1               Batch: 3/8	Loss 84.4778 (87.8059)
2022-11-09 18:21:43,480:INFO: Dataset: zara1               Batch: 4/8	Loss 84.2917 (87.0036)
2022-11-09 18:21:44,217:INFO: Dataset: zara1               Batch: 5/8	Loss 84.9270 (86.5912)
2022-11-09 18:21:44,957:INFO: Dataset: zara1               Batch: 6/8	Loss 84.8540 (86.3583)
2022-11-09 18:21:45,696:INFO: Dataset: zara1               Batch: 7/8	Loss 86.7775 (86.4148)
2022-11-09 18:21:46,423:INFO: Dataset: zara1               Batch: 8/8	Loss 74.8216 (85.2372)
2022-11-09 18:21:47,435:INFO: Dataset: zara2               Batch:  1/18	Loss 85.0569 (85.0569)
2022-11-09 18:21:48,182:INFO: Dataset: zara2               Batch:  2/18	Loss 85.1524 (85.1047)
2022-11-09 18:21:48,920:INFO: Dataset: zara2               Batch:  3/18	Loss 82.9228 (84.3493)
2022-11-09 18:21:49,657:INFO: Dataset: zara2               Batch:  4/18	Loss 82.0577 (83.7710)
2022-11-09 18:21:50,399:INFO: Dataset: zara2               Batch:  5/18	Loss 82.5164 (83.5171)
2022-11-09 18:21:51,147:INFO: Dataset: zara2               Batch:  6/18	Loss 81.6901 (83.2422)
2022-11-09 18:21:51,888:INFO: Dataset: zara2               Batch:  7/18	Loss 81.0472 (82.9108)
2022-11-09 18:21:52,710:INFO: Dataset: zara2               Batch:  8/18	Loss 81.3194 (82.7192)
2022-11-09 18:21:53,454:INFO: Dataset: zara2               Batch:  9/18	Loss 81.2040 (82.5564)
2022-11-09 18:21:54,202:INFO: Dataset: zara2               Batch: 10/18	Loss 81.2941 (82.4379)
2022-11-09 18:21:54,950:INFO: Dataset: zara2               Batch: 11/18	Loss 82.4850 (82.4423)
2022-11-09 18:21:55,693:INFO: Dataset: zara2               Batch: 12/18	Loss 81.9578 (82.3982)
2022-11-09 18:21:56,437:INFO: Dataset: zara2               Batch: 13/18	Loss 82.0513 (82.3714)
2022-11-09 18:21:57,188:INFO: Dataset: zara2               Batch: 14/18	Loss 80.9619 (82.2786)
2022-11-09 18:21:57,935:INFO: Dataset: zara2               Batch: 15/18	Loss 81.3524 (82.2184)
2022-11-09 18:21:58,683:INFO: Dataset: zara2               Batch: 16/18	Loss 82.2645 (82.2212)
2022-11-09 18:21:59,430:INFO: Dataset: zara2               Batch: 17/18	Loss 80.3513 (82.1156)
2022-11-09 18:22:00,165:INFO: Dataset: zara2               Batch: 18/18	Loss 69.3806 (81.4924)
2022-11-09 18:22:00,237:INFO: - Computing ADE (validation o)
2022-11-09 18:22:00,519:INFO: 		 ADE on eth                       dataset:	 2.8749120235443115
2022-11-09 18:22:00,519:INFO: Average validation o:	ADE  2.8749	FDE  4.7831
2022-11-09 18:22:00,520:INFO: - Computing ADE (validation)
2022-11-09 18:22:00,794:INFO: 		 ADE on hotel                     dataset:	 1.1568745374679565
2022-11-09 18:22:01,119:INFO: 		 ADE on univ                      dataset:	 1.4021588563919067
2022-11-09 18:22:01,402:INFO: 		 ADE on zara1                     dataset:	 2.1567177772521973
2022-11-09 18:22:01,775:INFO: 		 ADE on zara2                     dataset:	 1.3599374294281006
2022-11-09 18:22:01,775:INFO: Average validation:	ADE  1.4171	FDE  2.6605
2022-11-09 18:22:01,775:INFO: - Computing ADE (training)
2022-11-09 18:22:02,127:INFO: 		 ADE on hotel                     dataset:	 1.447447657585144
2022-11-09 18:22:02,751:INFO: 		 ADE on univ                      dataset:	 1.3271441459655762
2022-11-09 18:22:03,208:INFO: 		 ADE on zara1                     dataset:	 2.084280490875244
2022-11-09 18:22:03,900:INFO: 		 ADE on zara2                     dataset:	 1.5274019241333008
2022-11-09 18:22:03,900:INFO: Average training:	ADE  1.4191	FDE  2.6503
2022-11-09 18:22:03,910:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_281.pth.tar
2022-11-09 18:22:03,910:INFO: 
===> EPOCH: 282 (P3)
2022-11-09 18:22:03,910:INFO: - Computing loss (training)
2022-11-09 18:22:04,838:INFO: Dataset: hotel               Batch: 1/4	Loss 81.9101 (81.9101)
2022-11-09 18:22:05,572:INFO: Dataset: hotel               Batch: 2/4	Loss 81.6274 (81.7723)
2022-11-09 18:22:06,294:INFO: Dataset: hotel               Batch: 3/4	Loss 83.8740 (82.4280)
2022-11-09 18:22:06,976:INFO: Dataset: hotel               Batch: 4/4	Loss 49.9803 (77.4196)
2022-11-09 18:22:08,022:INFO: Dataset: univ                Batch:  1/15	Loss 87.3909 (87.3909)
2022-11-09 18:22:08,763:INFO: Dataset: univ                Batch:  2/15	Loss 80.9298 (84.3222)
2022-11-09 18:22:09,504:INFO: Dataset: univ                Batch:  3/15	Loss 81.3557 (83.3631)
2022-11-09 18:22:10,247:INFO: Dataset: univ                Batch:  4/15	Loss 81.5137 (82.9033)
2022-11-09 18:22:10,995:INFO: Dataset: univ                Batch:  5/15	Loss 81.8783 (82.6938)
2022-11-09 18:22:11,748:INFO: Dataset: univ                Batch:  6/15	Loss 80.0513 (82.2375)
2022-11-09 18:22:12,495:INFO: Dataset: univ                Batch:  7/15	Loss 82.1997 (82.2319)
2022-11-09 18:22:13,243:INFO: Dataset: univ                Batch:  8/15	Loss 83.0124 (82.3314)
2022-11-09 18:22:14,072:INFO: Dataset: univ                Batch:  9/15	Loss 83.6802 (82.4916)
2022-11-09 18:22:14,823:INFO: Dataset: univ                Batch: 10/15	Loss 79.9060 (82.2194)
2022-11-09 18:22:15,578:INFO: Dataset: univ                Batch: 11/15	Loss 82.5997 (82.2538)
2022-11-09 18:22:16,339:INFO: Dataset: univ                Batch: 12/15	Loss 79.9115 (82.0354)
2022-11-09 18:22:17,094:INFO: Dataset: univ                Batch: 13/15	Loss 80.0393 (81.8741)
2022-11-09 18:22:17,856:INFO: Dataset: univ                Batch: 14/15	Loss 79.4940 (81.6935)
2022-11-09 18:22:18,519:INFO: Dataset: univ                Batch: 15/15	Loss 15.0972 (80.7183)
2022-11-09 18:22:19,533:INFO: Dataset: zara1               Batch: 1/8	Loss 84.1017 (84.1017)
2022-11-09 18:22:20,273:INFO: Dataset: zara1               Batch: 2/8	Loss 89.8492 (87.0572)
2022-11-09 18:22:21,014:INFO: Dataset: zara1               Batch: 3/8	Loss 89.8355 (88.0866)
2022-11-09 18:22:21,753:INFO: Dataset: zara1               Batch: 4/8	Loss 85.7868 (87.4940)
2022-11-09 18:22:22,506:INFO: Dataset: zara1               Batch: 5/8	Loss 83.8054 (86.8041)
2022-11-09 18:22:23,246:INFO: Dataset: zara1               Batch: 6/8	Loss 83.8093 (86.3287)
2022-11-09 18:22:23,989:INFO: Dataset: zara1               Batch: 7/8	Loss 91.6206 (87.1349)
2022-11-09 18:22:24,716:INFO: Dataset: zara1               Batch: 8/8	Loss 76.9553 (85.9937)
2022-11-09 18:22:25,740:INFO: Dataset: zara2               Batch:  1/18	Loss 90.4739 (90.4739)
2022-11-09 18:22:26,485:INFO: Dataset: zara2               Batch:  2/18	Loss 83.6277 (86.9050)
2022-11-09 18:22:27,230:INFO: Dataset: zara2               Batch:  3/18	Loss 82.6237 (85.4380)
2022-11-09 18:22:27,985:INFO: Dataset: zara2               Batch:  4/18	Loss 83.0636 (84.8461)
2022-11-09 18:22:28,739:INFO: Dataset: zara2               Batch:  5/18	Loss 81.8235 (84.2166)
2022-11-09 18:22:29,496:INFO: Dataset: zara2               Batch:  6/18	Loss 81.2235 (83.7560)
2022-11-09 18:22:30,348:INFO: Dataset: zara2               Batch:  7/18	Loss 81.8855 (83.4877)
2022-11-09 18:22:31,133:INFO: Dataset: zara2               Batch:  8/18	Loss 82.0787 (83.3250)
2022-11-09 18:22:31,914:INFO: Dataset: zara2               Batch:  9/18	Loss 80.4586 (82.9956)
2022-11-09 18:22:32,708:INFO: Dataset: zara2               Batch: 10/18	Loss 81.1309 (82.8302)
2022-11-09 18:22:33,589:INFO: Dataset: zara2               Batch: 11/18	Loss 81.4906 (82.7134)
2022-11-09 18:22:34,409:INFO: Dataset: zara2               Batch: 12/18	Loss 81.3978 (82.5974)
2022-11-09 18:22:35,220:INFO: Dataset: zara2               Batch: 13/18	Loss 81.6371 (82.5257)
2022-11-09 18:22:36,020:INFO: Dataset: zara2               Batch: 14/18	Loss 81.3733 (82.4433)
2022-11-09 18:22:36,850:INFO: Dataset: zara2               Batch: 15/18	Loss 80.3359 (82.3054)
2022-11-09 18:22:37,683:INFO: Dataset: zara2               Batch: 16/18	Loss 80.5606 (82.1952)
2022-11-09 18:22:38,470:INFO: Dataset: zara2               Batch: 17/18	Loss 80.6380 (82.1025)
2022-11-09 18:22:39,233:INFO: Dataset: zara2               Batch: 18/18	Loss 69.7233 (81.4394)
2022-11-09 18:22:39,310:INFO: - Computing ADE (validation o)
2022-11-09 18:22:39,621:INFO: 		 ADE on eth                       dataset:	 2.762293577194214
2022-11-09 18:22:39,621:INFO: Average validation o:	ADE  2.7623	FDE  4.5564
2022-11-09 18:22:39,622:INFO: - Computing ADE (validation)
2022-11-09 18:22:39,896:INFO: 		 ADE on hotel                     dataset:	 1.1865003108978271
2022-11-09 18:22:40,211:INFO: 		 ADE on univ                      dataset:	 1.3532371520996094
2022-11-09 18:22:40,483:INFO: 		 ADE on zara1                     dataset:	 1.856224536895752
2022-11-09 18:22:40,855:INFO: 		 ADE on zara2                     dataset:	 1.2564233541488647
2022-11-09 18:22:40,855:INFO: Average validation:	ADE  1.3378	FDE  2.4565
2022-11-09 18:22:40,856:INFO: - Computing ADE (training)
2022-11-09 18:22:41,191:INFO: 		 ADE on hotel                     dataset:	 1.4716732501983643
2022-11-09 18:22:41,922:INFO: 		 ADE on univ                      dataset:	 1.2613551616668701
2022-11-09 18:22:42,411:INFO: 		 ADE on zara1                     dataset:	 1.907768964767456
2022-11-09 18:22:43,147:INFO: 		 ADE on zara2                     dataset:	 1.4386591911315918
2022-11-09 18:22:43,147:INFO: Average training:	ADE  1.3439	FDE  2.4660
2022-11-09 18:22:43,159:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_282.pth.tar
2022-11-09 18:22:43,159:INFO: 
===> EPOCH: 283 (P3)
2022-11-09 18:22:43,160:INFO: - Computing loss (training)
2022-11-09 18:22:44,140:INFO: Dataset: hotel               Batch: 1/4	Loss 84.0961 (84.0961)
2022-11-09 18:22:44,882:INFO: Dataset: hotel               Batch: 2/4	Loss 84.2643 (84.1804)
2022-11-09 18:22:45,643:INFO: Dataset: hotel               Batch: 3/4	Loss 84.3630 (84.2447)
2022-11-09 18:22:46,364:INFO: Dataset: hotel               Batch: 4/4	Loss 50.7935 (78.5077)
2022-11-09 18:22:47,452:INFO: Dataset: univ                Batch:  1/15	Loss 79.9282 (79.9282)
2022-11-09 18:22:48,215:INFO: Dataset: univ                Batch:  2/15	Loss 83.7187 (81.7027)
2022-11-09 18:22:49,000:INFO: Dataset: univ                Batch:  3/15	Loss 79.3735 (80.8339)
2022-11-09 18:22:49,781:INFO: Dataset: univ                Batch:  4/15	Loss 81.6136 (81.0221)
2022-11-09 18:22:50,625:INFO: Dataset: univ                Batch:  5/15	Loss 81.7583 (81.1722)
2022-11-09 18:22:51,430:INFO: Dataset: univ                Batch:  6/15	Loss 81.0602 (81.1558)
2022-11-09 18:22:52,413:INFO: Dataset: univ                Batch:  7/15	Loss 81.0885 (81.1461)
2022-11-09 18:22:53,249:INFO: Dataset: univ                Batch:  8/15	Loss 79.9868 (81.0013)
2022-11-09 18:22:54,001:INFO: Dataset: univ                Batch:  9/15	Loss 79.8443 (80.8822)
2022-11-09 18:22:54,753:INFO: Dataset: univ                Batch: 10/15	Loss 80.0474 (80.7975)
2022-11-09 18:22:55,504:INFO: Dataset: univ                Batch: 11/15	Loss 79.8707 (80.7138)
2022-11-09 18:22:56,247:INFO: Dataset: univ                Batch: 12/15	Loss 80.7755 (80.7183)
2022-11-09 18:22:56,999:INFO: Dataset: univ                Batch: 13/15	Loss 80.0255 (80.6646)
2022-11-09 18:22:57,743:INFO: Dataset: univ                Batch: 14/15	Loss 80.6212 (80.6618)
2022-11-09 18:22:58,401:INFO: Dataset: univ                Batch: 15/15	Loss 15.0454 (79.7289)
2022-11-09 18:22:59,408:INFO: Dataset: zara1               Batch: 1/8	Loss 88.0137 (88.0137)
2022-11-09 18:23:00,147:INFO: Dataset: zara1               Batch: 2/8	Loss 87.9406 (87.9747)
2022-11-09 18:23:00,886:INFO: Dataset: zara1               Batch: 3/8	Loss 84.5623 (86.7884)
2022-11-09 18:23:01,620:INFO: Dataset: zara1               Batch: 4/8	Loss 84.9439 (86.3263)
2022-11-09 18:23:02,353:INFO: Dataset: zara1               Batch: 5/8	Loss 82.1199 (85.5248)
2022-11-09 18:23:03,083:INFO: Dataset: zara1               Batch: 6/8	Loss 91.2965 (86.4717)
2022-11-09 18:23:03,821:INFO: Dataset: zara1               Batch: 7/8	Loss 87.1305 (86.5698)
2022-11-09 18:23:04,539:INFO: Dataset: zara1               Batch: 8/8	Loss 72.3388 (85.2665)
2022-11-09 18:23:05,586:INFO: Dataset: zara2               Batch:  1/18	Loss 90.0969 (90.0969)
2022-11-09 18:23:06,331:INFO: Dataset: zara2               Batch:  2/18	Loss 82.3894 (86.2892)
2022-11-09 18:23:07,073:INFO: Dataset: zara2               Batch:  3/18	Loss 81.3349 (84.6214)
2022-11-09 18:23:07,813:INFO: Dataset: zara2               Batch:  4/18	Loss 85.5552 (84.8757)
2022-11-09 18:23:08,555:INFO: Dataset: zara2               Batch:  5/18	Loss 82.6213 (84.4761)
2022-11-09 18:23:09,380:INFO: Dataset: zara2               Batch:  6/18	Loss 82.0397 (84.0177)
2022-11-09 18:23:10,122:INFO: Dataset: zara2               Batch:  7/18	Loss 82.1330 (83.7560)
2022-11-09 18:23:10,866:INFO: Dataset: zara2               Batch:  8/18	Loss 82.7381 (83.6339)
2022-11-09 18:23:11,608:INFO: Dataset: zara2               Batch:  9/18	Loss 82.1239 (83.4743)
2022-11-09 18:23:12,349:INFO: Dataset: zara2               Batch: 10/18	Loss 82.0652 (83.3311)
2022-11-09 18:23:13,090:INFO: Dataset: zara2               Batch: 11/18	Loss 81.1852 (83.1382)
2022-11-09 18:23:13,834:INFO: Dataset: zara2               Batch: 12/18	Loss 80.7395 (82.9372)
2022-11-09 18:23:14,576:INFO: Dataset: zara2               Batch: 13/18	Loss 81.4964 (82.8324)
2022-11-09 18:23:15,321:INFO: Dataset: zara2               Batch: 14/18	Loss 84.3968 (82.9428)
2022-11-09 18:23:16,062:INFO: Dataset: zara2               Batch: 15/18	Loss 81.2906 (82.8228)
2022-11-09 18:23:16,806:INFO: Dataset: zara2               Batch: 16/18	Loss 81.5810 (82.7495)
2022-11-09 18:23:17,546:INFO: Dataset: zara2               Batch: 17/18	Loss 80.1625 (82.5915)
2022-11-09 18:23:18,273:INFO: Dataset: zara2               Batch: 18/18	Loss 69.7069 (82.0526)
2022-11-09 18:23:18,338:INFO: - Computing ADE (validation o)
2022-11-09 18:23:18,619:INFO: 		 ADE on eth                       dataset:	 2.8705475330352783
2022-11-09 18:23:18,619:INFO: Average validation o:	ADE  2.8705	FDE  4.7535
2022-11-09 18:23:18,620:INFO: - Computing ADE (validation)
2022-11-09 18:23:18,883:INFO: 		 ADE on hotel                     dataset:	 1.3689939975738525
2022-11-09 18:23:19,206:INFO: 		 ADE on univ                      dataset:	 1.505102515220642
2022-11-09 18:23:19,479:INFO: 		 ADE on zara1                     dataset:	 2.188143730163574
2022-11-09 18:23:19,846:INFO: 		 ADE on zara2                     dataset:	 1.457170009613037
2022-11-09 18:23:19,846:INFO: Average validation:	ADE  1.5198	FDE  2.8284
2022-11-09 18:23:19,847:INFO: - Computing ADE (training)
2022-11-09 18:23:20,183:INFO: 		 ADE on hotel                     dataset:	 1.5921293497085571
2022-11-09 18:23:20,834:INFO: 		 ADE on univ                      dataset:	 1.424598217010498
2022-11-09 18:23:21,292:INFO: 		 ADE on zara1                     dataset:	 2.0953900814056396
2022-11-09 18:23:21,979:INFO: 		 ADE on zara2                     dataset:	 1.640947937965393
2022-11-09 18:23:21,979:INFO: Average training:	ADE  1.5155	FDE  2.8118
2022-11-09 18:23:21,989:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_283.pth.tar
2022-11-09 18:23:21,989:INFO: 
===> EPOCH: 284 (P3)
2022-11-09 18:23:21,989:INFO: - Computing loss (training)
2022-11-09 18:23:22,904:INFO: Dataset: hotel               Batch: 1/4	Loss 83.6124 (83.6124)
2022-11-09 18:23:23,636:INFO: Dataset: hotel               Batch: 2/4	Loss 83.7817 (83.6962)
2022-11-09 18:23:24,357:INFO: Dataset: hotel               Batch: 3/4	Loss 83.4125 (83.5975)
2022-11-09 18:23:25,037:INFO: Dataset: hotel               Batch: 4/4	Loss 49.8170 (78.3834)
2022-11-09 18:23:26,035:INFO: Dataset: univ                Batch:  1/15	Loss 80.4336 (80.4336)
2022-11-09 18:23:26,776:INFO: Dataset: univ                Batch:  2/15	Loss 81.4767 (80.9623)
2022-11-09 18:23:27,520:INFO: Dataset: univ                Batch:  3/15	Loss 81.5570 (81.1600)
2022-11-09 18:23:28,266:INFO: Dataset: univ                Batch:  4/15	Loss 81.6572 (81.2844)
2022-11-09 18:23:29,003:INFO: Dataset: univ                Batch:  5/15	Loss 84.1153 (81.7983)
2022-11-09 18:23:29,836:INFO: Dataset: univ                Batch:  6/15	Loss 80.6824 (81.5901)
2022-11-09 18:23:30,581:INFO: Dataset: univ                Batch:  7/15	Loss 80.1556 (81.3750)
2022-11-09 18:23:31,333:INFO: Dataset: univ                Batch:  8/15	Loss 79.5186 (81.1219)
2022-11-09 18:23:32,078:INFO: Dataset: univ                Batch:  9/15	Loss 80.9986 (81.1086)
2022-11-09 18:23:32,827:INFO: Dataset: univ                Batch: 10/15	Loss 80.2929 (81.0231)
2022-11-09 18:23:33,580:INFO: Dataset: univ                Batch: 11/15	Loss 80.1037 (80.9306)
2022-11-09 18:23:34,334:INFO: Dataset: univ                Batch: 12/15	Loss 79.9509 (80.8458)
2022-11-09 18:23:35,092:INFO: Dataset: univ                Batch: 13/15	Loss 80.1024 (80.7851)
2022-11-09 18:23:35,843:INFO: Dataset: univ                Batch: 14/15	Loss 80.9244 (80.7949)
2022-11-09 18:23:36,502:INFO: Dataset: univ                Batch: 15/15	Loss 15.2548 (79.9904)
2022-11-09 18:23:37,508:INFO: Dataset: zara1               Batch: 1/8	Loss 87.9563 (87.9563)
2022-11-09 18:23:38,258:INFO: Dataset: zara1               Batch: 2/8	Loss 88.5427 (88.2233)
2022-11-09 18:23:38,990:INFO: Dataset: zara1               Batch: 3/8	Loss 88.1066 (88.1825)
2022-11-09 18:23:39,728:INFO: Dataset: zara1               Batch: 4/8	Loss 85.4059 (87.5370)
2022-11-09 18:23:40,466:INFO: Dataset: zara1               Batch: 5/8	Loss 84.4133 (86.8674)
2022-11-09 18:23:41,200:INFO: Dataset: zara1               Batch: 6/8	Loss 84.3839 (86.4504)
2022-11-09 18:23:41,955:INFO: Dataset: zara1               Batch: 7/8	Loss 85.5182 (86.3121)
2022-11-09 18:23:42,676:INFO: Dataset: zara1               Batch: 8/8	Loss 78.0041 (85.4026)
2022-11-09 18:23:43,715:INFO: Dataset: zara2               Batch:  1/18	Loss 89.8307 (89.8307)
2022-11-09 18:23:44,461:INFO: Dataset: zara2               Batch:  2/18	Loss 90.1167 (89.9675)
2022-11-09 18:23:45,211:INFO: Dataset: zara2               Batch:  3/18	Loss 84.5431 (88.2481)
2022-11-09 18:23:45,968:INFO: Dataset: zara2               Batch:  4/18	Loss 81.8832 (86.6592)
2022-11-09 18:23:46,792:INFO: Dataset: zara2               Batch:  5/18	Loss 81.1731 (85.4970)
2022-11-09 18:23:47,540:INFO: Dataset: zara2               Batch:  6/18	Loss 81.7685 (84.9301)
2022-11-09 18:23:48,289:INFO: Dataset: zara2               Batch:  7/18	Loss 82.0263 (84.5554)
2022-11-09 18:23:49,033:INFO: Dataset: zara2               Batch:  8/18	Loss 81.1839 (84.1178)
2022-11-09 18:23:49,783:INFO: Dataset: zara2               Batch:  9/18	Loss 83.4748 (84.0513)
2022-11-09 18:23:50,530:INFO: Dataset: zara2               Batch: 10/18	Loss 82.2826 (83.8808)
2022-11-09 18:23:51,280:INFO: Dataset: zara2               Batch: 11/18	Loss 82.7513 (83.7815)
2022-11-09 18:23:52,028:INFO: Dataset: zara2               Batch: 12/18	Loss 82.4847 (83.6713)
2022-11-09 18:23:52,773:INFO: Dataset: zara2               Batch: 13/18	Loss 82.8156 (83.5979)
2022-11-09 18:23:53,520:INFO: Dataset: zara2               Batch: 14/18	Loss 81.2147 (83.4270)
2022-11-09 18:23:54,268:INFO: Dataset: zara2               Batch: 15/18	Loss 81.5993 (83.2952)
2022-11-09 18:23:55,014:INFO: Dataset: zara2               Batch: 16/18	Loss 80.4438 (83.1084)
2022-11-09 18:23:55,761:INFO: Dataset: zara2               Batch: 17/18	Loss 80.7860 (82.9668)
2022-11-09 18:23:56,496:INFO: Dataset: zara2               Batch: 18/18	Loss 69.8263 (82.2912)
2022-11-09 18:23:56,563:INFO: - Computing ADE (validation o)
2022-11-09 18:23:56,854:INFO: 		 ADE on eth                       dataset:	 2.9926095008850098
2022-11-09 18:23:56,854:INFO: Average validation o:	ADE  2.9926	FDE  5.0555
2022-11-09 18:23:56,855:INFO: - Computing ADE (validation)
2022-11-09 18:23:57,125:INFO: 		 ADE on hotel                     dataset:	 1.107581615447998
2022-11-09 18:23:57,439:INFO: 		 ADE on univ                      dataset:	 1.3288494348526
2022-11-09 18:23:57,727:INFO: 		 ADE on zara1                     dataset:	 1.9187374114990234
2022-11-09 18:23:58,100:INFO: 		 ADE on zara2                     dataset:	 1.2947567701339722
2022-11-09 18:23:58,100:INFO: Average validation:	ADE  1.3385	FDE  2.4468
2022-11-09 18:23:58,101:INFO: - Computing ADE (training)
2022-11-09 18:23:58,424:INFO: 		 ADE on hotel                     dataset:	 1.3923025131225586
2022-11-09 18:23:59,068:INFO: 		 ADE on univ                      dataset:	 1.2621724605560303
2022-11-09 18:23:59,514:INFO: 		 ADE on zara1                     dataset:	 2.0558786392211914
2022-11-09 18:24:00,217:INFO: 		 ADE on zara2                     dataset:	 1.5225956439971924
2022-11-09 18:24:00,217:INFO: Average training:	ADE  1.3689	FDE  2.5092
2022-11-09 18:24:00,227:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_284.pth.tar
2022-11-09 18:24:00,227:INFO: 
===> EPOCH: 285 (P3)
2022-11-09 18:24:00,228:INFO: - Computing loss (training)
2022-11-09 18:24:01,160:INFO: Dataset: hotel               Batch: 1/4	Loss 81.9572 (81.9572)
2022-11-09 18:24:01,893:INFO: Dataset: hotel               Batch: 2/4	Loss 82.1463 (82.0496)
2022-11-09 18:24:02,614:INFO: Dataset: hotel               Batch: 3/4	Loss 82.9088 (82.3164)
2022-11-09 18:24:03,295:INFO: Dataset: hotel               Batch: 4/4	Loss 49.7110 (77.4127)
2022-11-09 18:24:04,332:INFO: Dataset: univ                Batch:  1/15	Loss 82.4305 (82.4305)
2022-11-09 18:24:05,076:INFO: Dataset: univ                Batch:  2/15	Loss 82.3095 (82.3753)
2022-11-09 18:24:05,823:INFO: Dataset: univ                Batch:  3/15	Loss 80.4099 (81.6920)
2022-11-09 18:24:06,571:INFO: Dataset: univ                Batch:  4/15	Loss 80.5058 (81.3779)
2022-11-09 18:24:07,400:INFO: Dataset: univ                Batch:  5/15	Loss 80.3925 (81.1753)
2022-11-09 18:24:08,139:INFO: Dataset: univ                Batch:  6/15	Loss 80.4624 (81.0687)
2022-11-09 18:24:08,880:INFO: Dataset: univ                Batch:  7/15	Loss 80.6314 (81.0086)
2022-11-09 18:24:09,625:INFO: Dataset: univ                Batch:  8/15	Loss 80.0317 (80.8832)
2022-11-09 18:24:10,370:INFO: Dataset: univ                Batch:  9/15	Loss 81.3977 (80.9407)
2022-11-09 18:24:11,120:INFO: Dataset: univ                Batch: 10/15	Loss 80.3262 (80.8749)
2022-11-09 18:24:11,872:INFO: Dataset: univ                Batch: 11/15	Loss 79.6648 (80.7579)
2022-11-09 18:24:12,618:INFO: Dataset: univ                Batch: 12/15	Loss 79.9206 (80.6883)
2022-11-09 18:24:13,366:INFO: Dataset: univ                Batch: 13/15	Loss 80.3703 (80.6634)
2022-11-09 18:24:14,115:INFO: Dataset: univ                Batch: 14/15	Loss 81.0861 (80.6939)
2022-11-09 18:24:14,766:INFO: Dataset: univ                Batch: 15/15	Loss 15.5602 (80.0241)
2022-11-09 18:24:15,793:INFO: Dataset: zara1               Batch: 1/8	Loss 88.2617 (88.2617)
2022-11-09 18:24:16,529:INFO: Dataset: zara1               Batch: 2/8	Loss 83.7440 (86.1426)
2022-11-09 18:24:17,266:INFO: Dataset: zara1               Batch: 3/8	Loss 87.2793 (86.4794)
2022-11-09 18:24:18,005:INFO: Dataset: zara1               Batch: 4/8	Loss 83.5262 (85.6631)
2022-11-09 18:24:18,742:INFO: Dataset: zara1               Batch: 5/8	Loss 82.8901 (85.1212)
2022-11-09 18:24:19,476:INFO: Dataset: zara1               Batch: 6/8	Loss 83.5175 (84.8572)
2022-11-09 18:24:20,216:INFO: Dataset: zara1               Batch: 7/8	Loss 88.5788 (85.3720)
2022-11-09 18:24:20,939:INFO: Dataset: zara1               Batch: 8/8	Loss 75.8678 (84.3665)
2022-11-09 18:24:21,955:INFO: Dataset: zara2               Batch:  1/18	Loss 83.0107 (83.0107)
2022-11-09 18:24:22,688:INFO: Dataset: zara2               Batch:  2/18	Loss 83.4234 (83.2188)
2022-11-09 18:24:23,502:INFO: Dataset: zara2               Batch:  3/18	Loss 81.3150 (82.6208)
2022-11-09 18:24:24,240:INFO: Dataset: zara2               Batch:  4/18	Loss 80.7509 (82.1771)
2022-11-09 18:24:24,978:INFO: Dataset: zara2               Batch:  5/18	Loss 80.1101 (81.7976)
2022-11-09 18:24:25,717:INFO: Dataset: zara2               Batch:  6/18	Loss 81.1602 (81.6865)
2022-11-09 18:24:26,455:INFO: Dataset: zara2               Batch:  7/18	Loss 79.7865 (81.4154)
2022-11-09 18:24:27,192:INFO: Dataset: zara2               Batch:  8/18	Loss 83.3358 (81.6552)
2022-11-09 18:24:27,931:INFO: Dataset: zara2               Batch:  9/18	Loss 81.7717 (81.6677)
2022-11-09 18:24:28,667:INFO: Dataset: zara2               Batch: 10/18	Loss 79.8173 (81.4633)
2022-11-09 18:24:29,405:INFO: Dataset: zara2               Batch: 11/18	Loss 81.1732 (81.4370)
2022-11-09 18:24:30,142:INFO: Dataset: zara2               Batch: 12/18	Loss 80.1543 (81.3214)
2022-11-09 18:24:30,883:INFO: Dataset: zara2               Batch: 13/18	Loss 80.2560 (81.2507)
2022-11-09 18:24:31,623:INFO: Dataset: zara2               Batch: 14/18	Loss 81.9816 (81.3020)
2022-11-09 18:24:32,361:INFO: Dataset: zara2               Batch: 15/18	Loss 81.6067 (81.3227)
2022-11-09 18:24:33,100:INFO: Dataset: zara2               Batch: 16/18	Loss 80.2414 (81.2532)
2022-11-09 18:24:33,838:INFO: Dataset: zara2               Batch: 17/18	Loss 81.6190 (81.2761)
2022-11-09 18:24:34,566:INFO: Dataset: zara2               Batch: 18/18	Loss 69.7378 (80.7382)
2022-11-09 18:24:34,631:INFO: - Computing ADE (validation o)
2022-11-09 18:24:34,914:INFO: 		 ADE on eth                       dataset:	 2.6532256603240967
2022-11-09 18:24:34,914:INFO: Average validation o:	ADE  2.6532	FDE  4.3348
2022-11-09 18:24:34,915:INFO: - Computing ADE (validation)
2022-11-09 18:24:35,181:INFO: 		 ADE on hotel                     dataset:	 1.2423350811004639
2022-11-09 18:24:35,508:INFO: 		 ADE on univ                      dataset:	 1.4157510995864868
2022-11-09 18:24:35,780:INFO: 		 ADE on zara1                     dataset:	 2.153463125228882
2022-11-09 18:24:36,150:INFO: 		 ADE on zara2                     dataset:	 1.3559397459030151
2022-11-09 18:24:36,150:INFO: Average validation:	ADE  1.4272	FDE  2.7134
2022-11-09 18:24:36,151:INFO: - Computing ADE (training)
2022-11-09 18:24:36,480:INFO: 		 ADE on hotel                     dataset:	 1.5070801973342896
2022-11-09 18:24:37,098:INFO: 		 ADE on univ                      dataset:	 1.3375358581542969
2022-11-09 18:24:37,546:INFO: 		 ADE on zara1                     dataset:	 1.9836559295654297
2022-11-09 18:24:38,266:INFO: 		 ADE on zara2                     dataset:	 1.5069990158081055
2022-11-09 18:24:38,266:INFO: Average training:	ADE  1.4174	FDE  2.6806
2022-11-09 18:24:38,277:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_285.pth.tar
2022-11-09 18:24:38,277:INFO: 
===> EPOCH: 286 (P3)
2022-11-09 18:24:38,277:INFO: - Computing loss (training)
2022-11-09 18:24:39,197:INFO: Dataset: hotel               Batch: 1/4	Loss 82.4270 (82.4270)
2022-11-09 18:24:39,927:INFO: Dataset: hotel               Batch: 2/4	Loss 82.6449 (82.5362)
2022-11-09 18:24:40,643:INFO: Dataset: hotel               Batch: 3/4	Loss 82.5434 (82.5385)
2022-11-09 18:24:41,323:INFO: Dataset: hotel               Batch: 4/4	Loss 49.9148 (76.9004)
2022-11-09 18:24:42,364:INFO: Dataset: univ                Batch:  1/15	Loss 80.4531 (80.4531)
2022-11-09 18:24:43,103:INFO: Dataset: univ                Batch:  2/15	Loss 81.6596 (81.0253)
2022-11-09 18:24:43,932:INFO: Dataset: univ                Batch:  3/15	Loss 79.9419 (80.6594)
2022-11-09 18:24:44,678:INFO: Dataset: univ                Batch:  4/15	Loss 79.8904 (80.4694)
2022-11-09 18:24:45,433:INFO: Dataset: univ                Batch:  5/15	Loss 79.5688 (80.2799)
2022-11-09 18:24:46,186:INFO: Dataset: univ                Batch:  6/15	Loss 80.7863 (80.3641)
2022-11-09 18:24:46,932:INFO: Dataset: univ                Batch:  7/15	Loss 79.6978 (80.2723)
2022-11-09 18:24:47,675:INFO: Dataset: univ                Batch:  8/15	Loss 80.9695 (80.3543)
2022-11-09 18:24:48,436:INFO: Dataset: univ                Batch:  9/15	Loss 79.6164 (80.2642)
2022-11-09 18:24:49,187:INFO: Dataset: univ                Batch: 10/15	Loss 79.9617 (80.2336)
2022-11-09 18:24:49,949:INFO: Dataset: univ                Batch: 11/15	Loss 79.9734 (80.2092)
2022-11-09 18:24:50,706:INFO: Dataset: univ                Batch: 12/15	Loss 79.2375 (80.1264)
2022-11-09 18:24:51,455:INFO: Dataset: univ                Batch: 13/15	Loss 81.8842 (80.2435)
2022-11-09 18:24:52,213:INFO: Dataset: univ                Batch: 14/15	Loss 81.1366 (80.3052)
2022-11-09 18:24:52,873:INFO: Dataset: univ                Batch: 15/15	Loss 15.1040 (79.2701)
2022-11-09 18:24:53,880:INFO: Dataset: zara1               Batch: 1/8	Loss 85.8524 (85.8524)
2022-11-09 18:24:54,610:INFO: Dataset: zara1               Batch: 2/8	Loss 84.5293 (85.2073)
2022-11-09 18:24:55,341:INFO: Dataset: zara1               Batch: 3/8	Loss 86.8799 (85.7648)
2022-11-09 18:24:56,074:INFO: Dataset: zara1               Batch: 4/8	Loss 83.8655 (85.3004)
2022-11-09 18:24:56,803:INFO: Dataset: zara1               Batch: 5/8	Loss 83.6062 (84.9663)
2022-11-09 18:24:57,539:INFO: Dataset: zara1               Batch: 6/8	Loss 85.3982 (85.0412)
2022-11-09 18:24:58,275:INFO: Dataset: zara1               Batch: 7/8	Loss 87.7584 (85.4562)
2022-11-09 18:24:59,070:INFO: Dataset: zara1               Batch: 8/8	Loss 76.0630 (84.5119)
2022-11-09 18:25:00,106:INFO: Dataset: zara2               Batch:  1/18	Loss 93.4367 (93.4367)
2022-11-09 18:25:00,851:INFO: Dataset: zara2               Batch:  2/18	Loss 83.8004 (88.5348)
2022-11-09 18:25:01,594:INFO: Dataset: zara2               Batch:  3/18	Loss 80.9142 (85.9653)
2022-11-09 18:25:02,334:INFO: Dataset: zara2               Batch:  4/18	Loss 80.6508 (84.5460)
2022-11-09 18:25:03,072:INFO: Dataset: zara2               Batch:  5/18	Loss 81.3740 (83.9034)
2022-11-09 18:25:03,815:INFO: Dataset: zara2               Batch:  6/18	Loss 80.0125 (83.1930)
2022-11-09 18:25:04,558:INFO: Dataset: zara2               Batch:  7/18	Loss 81.8764 (83.0142)
2022-11-09 18:25:05,302:INFO: Dataset: zara2               Batch:  8/18	Loss 81.3509 (82.7951)
2022-11-09 18:25:06,042:INFO: Dataset: zara2               Batch:  9/18	Loss 81.1724 (82.5986)
2022-11-09 18:25:06,784:INFO: Dataset: zara2               Batch: 10/18	Loss 81.3990 (82.4799)
2022-11-09 18:25:07,525:INFO: Dataset: zara2               Batch: 11/18	Loss 81.6620 (82.4026)
2022-11-09 18:25:08,268:INFO: Dataset: zara2               Batch: 12/18	Loss 81.1216 (82.3066)
2022-11-09 18:25:09,010:INFO: Dataset: zara2               Batch: 13/18	Loss 81.5257 (82.2479)
2022-11-09 18:25:09,750:INFO: Dataset: zara2               Batch: 14/18	Loss 83.0183 (82.3056)
2022-11-09 18:25:10,490:INFO: Dataset: zara2               Batch: 15/18	Loss 81.9176 (82.2785)
2022-11-09 18:25:11,228:INFO: Dataset: zara2               Batch: 16/18	Loss 82.4901 (82.2926)
2022-11-09 18:25:11,964:INFO: Dataset: zara2               Batch: 17/18	Loss 82.1698 (82.2855)
2022-11-09 18:25:12,693:INFO: Dataset: zara2               Batch: 18/18	Loss 70.0230 (81.6550)
2022-11-09 18:25:12,762:INFO: - Computing ADE (validation o)
2022-11-09 18:25:13,030:INFO: 		 ADE on eth                       dataset:	 2.671170949935913
2022-11-09 18:25:13,031:INFO: Average validation o:	ADE  2.6712	FDE  4.4180
2022-11-09 18:25:13,031:INFO: - Computing ADE (validation)
2022-11-09 18:25:13,302:INFO: 		 ADE on hotel                     dataset:	 1.3406929969787598
2022-11-09 18:25:13,622:INFO: 		 ADE on univ                      dataset:	 1.5082920789718628
2022-11-09 18:25:13,890:INFO: 		 ADE on zara1                     dataset:	 2.2767505645751953
2022-11-09 18:25:14,268:INFO: 		 ADE on zara2                     dataset:	 1.4512856006622314
2022-11-09 18:25:14,268:INFO: Average validation:	ADE  1.5229	FDE  2.9009
2022-11-09 18:25:14,269:INFO: - Computing ADE (training)
2022-11-09 18:25:14,601:INFO: 		 ADE on hotel                     dataset:	 1.593479871749878
2022-11-09 18:25:15,219:INFO: 		 ADE on univ                      dataset:	 1.4189082384109497
2022-11-09 18:25:15,675:INFO: 		 ADE on zara1                     dataset:	 2.054905414581299
2022-11-09 18:25:16,367:INFO: 		 ADE on zara2                     dataset:	 1.5815176963806152
2022-11-09 18:25:16,367:INFO: Average training:	ADE  1.4969	FDE  2.8345
2022-11-09 18:25:16,377:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_286.pth.tar
2022-11-09 18:25:16,377:INFO: 
===> EPOCH: 287 (P3)
2022-11-09 18:25:16,378:INFO: - Computing loss (training)
2022-11-09 18:25:17,317:INFO: Dataset: hotel               Batch: 1/4	Loss 86.3791 (86.3791)
2022-11-09 18:25:18,044:INFO: Dataset: hotel               Batch: 2/4	Loss 83.3156 (84.8940)
2022-11-09 18:25:18,760:INFO: Dataset: hotel               Batch: 3/4	Loss 82.2071 (84.0026)
2022-11-09 18:25:19,436:INFO: Dataset: hotel               Batch: 4/4	Loss 50.1521 (78.6883)
2022-11-09 18:25:20,474:INFO: Dataset: univ                Batch:  1/15	Loss 80.6439 (80.6439)
2022-11-09 18:25:21,299:INFO: Dataset: univ                Batch:  2/15	Loss 80.0454 (80.3426)
2022-11-09 18:25:22,040:INFO: Dataset: univ                Batch:  3/15	Loss 81.6167 (80.7550)
2022-11-09 18:25:22,779:INFO: Dataset: univ                Batch:  4/15	Loss 91.2111 (83.1210)
2022-11-09 18:25:23,531:INFO: Dataset: univ                Batch:  5/15	Loss 81.2737 (82.7271)
2022-11-09 18:25:24,275:INFO: Dataset: univ                Batch:  6/15	Loss 81.7745 (82.5762)
2022-11-09 18:25:25,026:INFO: Dataset: univ                Batch:  7/15	Loss 80.1513 (82.2186)
2022-11-09 18:25:25,777:INFO: Dataset: univ                Batch:  8/15	Loss 79.8771 (81.9054)
2022-11-09 18:25:26,528:INFO: Dataset: univ                Batch:  9/15	Loss 80.4921 (81.7402)
2022-11-09 18:25:27,280:INFO: Dataset: univ                Batch: 10/15	Loss 80.0368 (81.5615)
2022-11-09 18:25:28,022:INFO: Dataset: univ                Batch: 11/15	Loss 84.0238 (81.7564)
2022-11-09 18:25:28,770:INFO: Dataset: univ                Batch: 12/15	Loss 79.8521 (81.6010)
2022-11-09 18:25:29,509:INFO: Dataset: univ                Batch: 13/15	Loss 83.4213 (81.7230)
2022-11-09 18:25:30,257:INFO: Dataset: univ                Batch: 14/15	Loss 80.6270 (81.6460)
2022-11-09 18:25:30,911:INFO: Dataset: univ                Batch: 15/15	Loss 16.0942 (80.9688)
2022-11-09 18:25:31,911:INFO: Dataset: zara1               Batch: 1/8	Loss 89.6651 (89.6651)
2022-11-09 18:25:32,653:INFO: Dataset: zara1               Batch: 2/8	Loss 84.5558 (87.0305)
2022-11-09 18:25:33,388:INFO: Dataset: zara1               Batch: 3/8	Loss 85.2078 (86.4005)
2022-11-09 18:25:34,123:INFO: Dataset: zara1               Batch: 4/8	Loss 83.9888 (85.7792)
2022-11-09 18:25:34,855:INFO: Dataset: zara1               Batch: 5/8	Loss 84.8365 (85.5916)
2022-11-09 18:25:35,587:INFO: Dataset: zara1               Batch: 6/8	Loss 88.0153 (85.9548)
2022-11-09 18:25:36,401:INFO: Dataset: zara1               Batch: 7/8	Loss 86.0134 (85.9640)
2022-11-09 18:25:37,123:INFO: Dataset: zara1               Batch: 8/8	Loss 73.2049 (84.7351)
2022-11-09 18:25:38,137:INFO: Dataset: zara2               Batch:  1/18	Loss 88.7382 (88.7382)
2022-11-09 18:25:38,875:INFO: Dataset: zara2               Batch:  2/18	Loss 83.0752 (85.7958)
2022-11-09 18:25:39,617:INFO: Dataset: zara2               Batch:  3/18	Loss 82.5964 (84.7379)
2022-11-09 18:25:40,352:INFO: Dataset: zara2               Batch:  4/18	Loss 81.4429 (83.8677)
2022-11-09 18:25:41,091:INFO: Dataset: zara2               Batch:  5/18	Loss 81.7466 (83.4415)
2022-11-09 18:25:41,832:INFO: Dataset: zara2               Batch:  6/18	Loss 80.3714 (82.9278)
2022-11-09 18:25:42,573:INFO: Dataset: zara2               Batch:  7/18	Loss 83.9453 (83.0659)
2022-11-09 18:25:43,315:INFO: Dataset: zara2               Batch:  8/18	Loss 82.4231 (82.9861)
2022-11-09 18:25:44,055:INFO: Dataset: zara2               Batch:  9/18	Loss 81.6778 (82.8171)
2022-11-09 18:25:44,794:INFO: Dataset: zara2               Batch: 10/18	Loss 81.4794 (82.6713)
2022-11-09 18:25:45,534:INFO: Dataset: zara2               Batch: 11/18	Loss 80.9828 (82.5255)
2022-11-09 18:25:46,271:INFO: Dataset: zara2               Batch: 12/18	Loss 80.4934 (82.3499)
2022-11-09 18:25:47,012:INFO: Dataset: zara2               Batch: 13/18	Loss 80.1082 (82.1949)
2022-11-09 18:25:47,755:INFO: Dataset: zara2               Batch: 14/18	Loss 80.5323 (82.0792)
2022-11-09 18:25:48,497:INFO: Dataset: zara2               Batch: 15/18	Loss 81.1572 (82.0219)
2022-11-09 18:25:49,240:INFO: Dataset: zara2               Batch: 16/18	Loss 81.9101 (82.0151)
2022-11-09 18:25:49,981:INFO: Dataset: zara2               Batch: 17/18	Loss 80.3194 (81.9083)
2022-11-09 18:25:50,711:INFO: Dataset: zara2               Batch: 18/18	Loss 68.9752 (81.3268)
2022-11-09 18:25:50,779:INFO: - Computing ADE (validation o)
2022-11-09 18:25:51,054:INFO: 		 ADE on eth                       dataset:	 2.8702313899993896
2022-11-09 18:25:51,055:INFO: Average validation o:	ADE  2.8702	FDE  4.8316
2022-11-09 18:25:51,056:INFO: - Computing ADE (validation)
2022-11-09 18:25:51,328:INFO: 		 ADE on hotel                     dataset:	 1.3508769273757935
2022-11-09 18:25:51,643:INFO: 		 ADE on univ                      dataset:	 1.5083608627319336
2022-11-09 18:25:51,912:INFO: 		 ADE on zara1                     dataset:	 2.0381317138671875
2022-11-09 18:25:52,280:INFO: 		 ADE on zara2                     dataset:	 1.4434880018234253
2022-11-09 18:25:52,280:INFO: Average validation:	ADE  1.5067	FDE  2.7768
2022-11-09 18:25:52,281:INFO: - Computing ADE (training)
2022-11-09 18:25:52,615:INFO: 		 ADE on hotel                     dataset:	 1.634130835533142
2022-11-09 18:25:53,249:INFO: 		 ADE on univ                      dataset:	 1.4165877103805542
2022-11-09 18:25:53,691:INFO: 		 ADE on zara1                     dataset:	 2.057936191558838
2022-11-09 18:25:54,400:INFO: 		 ADE on zara2                     dataset:	 1.6381150484085083
2022-11-09 18:25:54,400:INFO: Average training:	ADE  1.5080	FDE  2.7743
2022-11-09 18:25:54,412:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_287.pth.tar
2022-11-09 18:25:54,412:INFO: 
===> EPOCH: 288 (P3)
2022-11-09 18:25:54,412:INFO: - Computing loss (training)
2022-11-09 18:25:55,429:INFO: Dataset: hotel               Batch: 1/4	Loss 84.6826 (84.6826)
2022-11-09 18:25:56,165:INFO: Dataset: hotel               Batch: 2/4	Loss 83.7091 (84.2123)
2022-11-09 18:25:56,963:INFO: Dataset: hotel               Batch: 3/4	Loss 83.5726 (83.9877)
2022-11-09 18:25:57,645:INFO: Dataset: hotel               Batch: 4/4	Loss 49.7691 (78.5705)
2022-11-09 18:25:58,704:INFO: Dataset: univ                Batch:  1/15	Loss 80.9443 (80.9443)
2022-11-09 18:25:59,442:INFO: Dataset: univ                Batch:  2/15	Loss 80.7159 (80.8428)
2022-11-09 18:26:00,184:INFO: Dataset: univ                Batch:  3/15	Loss 80.9062 (80.8636)
2022-11-09 18:26:00,944:INFO: Dataset: univ                Batch:  4/15	Loss 80.2265 (80.6817)
2022-11-09 18:26:01,692:INFO: Dataset: univ                Batch:  5/15	Loss 80.4619 (80.6366)
2022-11-09 18:26:02,445:INFO: Dataset: univ                Batch:  6/15	Loss 80.0397 (80.5391)
2022-11-09 18:26:03,193:INFO: Dataset: univ                Batch:  7/15	Loss 79.6141 (80.4030)
2022-11-09 18:26:03,943:INFO: Dataset: univ                Batch:  8/15	Loss 80.0364 (80.3566)
2022-11-09 18:26:04,692:INFO: Dataset: univ                Batch:  9/15	Loss 80.1218 (80.3298)
2022-11-09 18:26:05,445:INFO: Dataset: univ                Batch: 10/15	Loss 80.4318 (80.3404)
2022-11-09 18:26:06,197:INFO: Dataset: univ                Batch: 11/15	Loss 80.0361 (80.3137)
2022-11-09 18:26:06,954:INFO: Dataset: univ                Batch: 12/15	Loss 79.6553 (80.2567)
2022-11-09 18:26:07,707:INFO: Dataset: univ                Batch: 13/15	Loss 80.7188 (80.2923)
2022-11-09 18:26:08,457:INFO: Dataset: univ                Batch: 14/15	Loss 80.3202 (80.2942)
2022-11-09 18:26:09,117:INFO: Dataset: univ                Batch: 15/15	Loss 14.9705 (79.3964)
2022-11-09 18:26:10,125:INFO: Dataset: zara1               Batch: 1/8	Loss 89.5004 (89.5004)
2022-11-09 18:26:10,861:INFO: Dataset: zara1               Batch: 2/8	Loss 83.4217 (86.5244)
2022-11-09 18:26:11,594:INFO: Dataset: zara1               Batch: 3/8	Loss 87.4914 (86.8431)
2022-11-09 18:26:12,327:INFO: Dataset: zara1               Batch: 4/8	Loss 84.5423 (86.2406)
2022-11-09 18:26:13,138:INFO: Dataset: zara1               Batch: 5/8	Loss 82.3049 (85.4757)
2022-11-09 18:26:13,874:INFO: Dataset: zara1               Batch: 6/8	Loss 88.2977 (85.9529)
2022-11-09 18:26:14,610:INFO: Dataset: zara1               Batch: 7/8	Loss 86.3889 (86.0083)
2022-11-09 18:26:15,331:INFO: Dataset: zara1               Batch: 8/8	Loss 72.6703 (84.3235)
2022-11-09 18:26:16,358:INFO: Dataset: zara2               Batch:  1/18	Loss 89.6485 (89.6485)
2022-11-09 18:26:17,093:INFO: Dataset: zara2               Batch:  2/18	Loss 81.6314 (85.5662)
2022-11-09 18:26:17,831:INFO: Dataset: zara2               Batch:  3/18	Loss 81.2998 (84.1454)
2022-11-09 18:26:18,575:INFO: Dataset: zara2               Batch:  4/18	Loss 81.2430 (83.4660)
2022-11-09 18:26:19,318:INFO: Dataset: zara2               Batch:  5/18	Loss 80.7208 (82.9517)
2022-11-09 18:26:20,071:INFO: Dataset: zara2               Batch:  6/18	Loss 81.7942 (82.7600)
2022-11-09 18:26:20,813:INFO: Dataset: zara2               Batch:  7/18	Loss 80.5718 (82.4273)
2022-11-09 18:26:21,564:INFO: Dataset: zara2               Batch:  8/18	Loss 80.4479 (82.1836)
2022-11-09 18:26:22,325:INFO: Dataset: zara2               Batch:  9/18	Loss 80.6310 (82.0084)
2022-11-09 18:26:23,105:INFO: Dataset: zara2               Batch: 10/18	Loss 80.4256 (81.8521)
2022-11-09 18:26:23,869:INFO: Dataset: zara2               Batch: 11/18	Loss 82.3764 (81.8971)
2022-11-09 18:26:24,688:INFO: Dataset: zara2               Batch: 12/18	Loss 80.4250 (81.7795)
2022-11-09 18:26:25,538:INFO: Dataset: zara2               Batch: 13/18	Loss 80.4093 (81.6805)
2022-11-09 18:26:26,320:INFO: Dataset: zara2               Batch: 14/18	Loss 80.6262 (81.6132)
2022-11-09 18:26:27,103:INFO: Dataset: zara2               Batch: 15/18	Loss 80.4541 (81.5311)
2022-11-09 18:26:27,885:INFO: Dataset: zara2               Batch: 16/18	Loss 80.3784 (81.4611)
2022-11-09 18:26:28,696:INFO: Dataset: zara2               Batch: 17/18	Loss 80.0662 (81.3755)
2022-11-09 18:26:29,498:INFO: Dataset: zara2               Batch: 18/18	Loss 68.9329 (80.7317)
2022-11-09 18:26:29,572:INFO: - Computing ADE (validation o)
2022-11-09 18:26:29,894:INFO: 		 ADE on eth                       dataset:	 2.748108148574829
2022-11-09 18:26:29,895:INFO: Average validation o:	ADE  2.7481	FDE  4.5575
2022-11-09 18:26:29,895:INFO: - Computing ADE (validation)
2022-11-09 18:26:30,163:INFO: 		 ADE on hotel                     dataset:	 1.2396153211593628
2022-11-09 18:26:30,476:INFO: 		 ADE on univ                      dataset:	 1.441040277481079
2022-11-09 18:26:30,745:INFO: 		 ADE on zara1                     dataset:	 2.084644317626953
2022-11-09 18:26:31,138:INFO: 		 ADE on zara2                     dataset:	 1.347522258758545
2022-11-09 18:26:31,139:INFO: Average validation:	ADE  1.4331	FDE  2.6595
2022-11-09 18:26:31,140:INFO: - Computing ADE (training)
2022-11-09 18:26:31,516:INFO: 		 ADE on hotel                     dataset:	 1.5266841650009155
2022-11-09 18:26:32,148:INFO: 		 ADE on univ                      dataset:	 1.340100884437561
2022-11-09 18:26:32,629:INFO: 		 ADE on zara1                     dataset:	 1.9904885292053223
2022-11-09 18:26:33,383:INFO: 		 ADE on zara2                     dataset:	 1.5179266929626465
2022-11-09 18:26:33,384:INFO: Average training:	ADE  1.4224	FDE  2.6357
2022-11-09 18:26:33,393:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_288.pth.tar
2022-11-09 18:26:33,393:INFO: 
===> EPOCH: 289 (P3)
2022-11-09 18:26:33,394:INFO: - Computing loss (training)
2022-11-09 18:26:34,341:INFO: Dataset: hotel               Batch: 1/4	Loss 81.0814 (81.0814)
2022-11-09 18:26:35,206:INFO: Dataset: hotel               Batch: 2/4	Loss 81.6815 (81.3786)
2022-11-09 18:26:35,956:INFO: Dataset: hotel               Batch: 3/4	Loss 81.8710 (81.5427)
2022-11-09 18:26:36,680:INFO: Dataset: hotel               Batch: 4/4	Loss 50.3515 (76.3990)
2022-11-09 18:26:37,803:INFO: Dataset: univ                Batch:  1/15	Loss 80.9192 (80.9192)
2022-11-09 18:26:38,573:INFO: Dataset: univ                Batch:  2/15	Loss 80.7822 (80.8499)
2022-11-09 18:26:39,335:INFO: Dataset: univ                Batch:  3/15	Loss 79.8448 (80.5160)
2022-11-09 18:26:40,175:INFO: Dataset: univ                Batch:  4/15	Loss 80.3058 (80.4625)
2022-11-09 18:26:40,966:INFO: Dataset: univ                Batch:  5/15	Loss 79.7669 (80.3289)
2022-11-09 18:26:41,740:INFO: Dataset: univ                Batch:  6/15	Loss 82.2807 (80.6261)
2022-11-09 18:26:42,578:INFO: Dataset: univ                Batch:  7/15	Loss 79.5851 (80.4657)
2022-11-09 18:26:43,376:INFO: Dataset: univ                Batch:  8/15	Loss 80.4115 (80.4590)
2022-11-09 18:26:44,161:INFO: Dataset: univ                Batch:  9/15	Loss 79.5032 (80.3418)
2022-11-09 18:26:44,961:INFO: Dataset: univ                Batch: 10/15	Loss 79.5364 (80.2630)
2022-11-09 18:26:45,743:INFO: Dataset: univ                Batch: 11/15	Loss 82.7117 (80.4633)
2022-11-09 18:26:46,625:INFO: Dataset: univ                Batch: 12/15	Loss 80.2494 (80.4462)
2022-11-09 18:26:47,632:INFO: Dataset: univ                Batch: 13/15	Loss 80.1152 (80.4169)
2022-11-09 18:26:48,391:INFO: Dataset: univ                Batch: 14/15	Loss 79.5918 (80.3611)
2022-11-09 18:26:49,064:INFO: Dataset: univ                Batch: 15/15	Loss 14.8232 (79.3083)
2022-11-09 18:26:50,079:INFO: Dataset: zara1               Batch: 1/8	Loss 96.9929 (96.9929)
2022-11-09 18:26:50,824:INFO: Dataset: zara1               Batch: 2/8	Loss 86.9783 (91.7177)
2022-11-09 18:26:51,566:INFO: Dataset: zara1               Batch: 3/8	Loss 89.4552 (90.9472)
2022-11-09 18:26:52,311:INFO: Dataset: zara1               Batch: 4/8	Loss 81.2746 (88.5711)
2022-11-09 18:26:53,052:INFO: Dataset: zara1               Batch: 5/8	Loss 85.5036 (87.9763)
2022-11-09 18:26:53,891:INFO: Dataset: zara1               Batch: 6/8	Loss 82.5721 (87.1270)
2022-11-09 18:26:54,636:INFO: Dataset: zara1               Batch: 7/8	Loss 83.6665 (86.6285)
2022-11-09 18:26:55,362:INFO: Dataset: zara1               Batch: 8/8	Loss 75.3469 (85.3222)
2022-11-09 18:26:56,377:INFO: Dataset: zara2               Batch:  1/18	Loss 86.3817 (86.3817)
2022-11-09 18:26:57,127:INFO: Dataset: zara2               Batch:  2/18	Loss 96.3620 (91.3501)
2022-11-09 18:26:57,879:INFO: Dataset: zara2               Batch:  3/18	Loss 92.5823 (91.7683)
2022-11-09 18:26:58,633:INFO: Dataset: zara2               Batch:  4/18	Loss 87.4109 (90.7115)
2022-11-09 18:26:59,382:INFO: Dataset: zara2               Batch:  5/18	Loss 81.5616 (88.9952)
2022-11-09 18:27:00,132:INFO: Dataset: zara2               Batch:  6/18	Loss 81.2972 (87.6839)
2022-11-09 18:27:00,881:INFO: Dataset: zara2               Batch:  7/18	Loss 80.7741 (86.7230)
2022-11-09 18:27:01,626:INFO: Dataset: zara2               Batch:  8/18	Loss 80.4194 (85.9392)
2022-11-09 18:27:02,372:INFO: Dataset: zara2               Batch:  9/18	Loss 81.3872 (85.4397)
2022-11-09 18:27:03,116:INFO: Dataset: zara2               Batch: 10/18	Loss 82.8664 (85.1791)
2022-11-09 18:27:03,864:INFO: Dataset: zara2               Batch: 11/18	Loss 81.9048 (84.8772)
2022-11-09 18:27:04,609:INFO: Dataset: zara2               Batch: 12/18	Loss 82.4761 (84.6865)
2022-11-09 18:27:05,352:INFO: Dataset: zara2               Batch: 13/18	Loss 80.3450 (84.3359)
2022-11-09 18:27:06,095:INFO: Dataset: zara2               Batch: 14/18	Loss 81.7009 (84.1405)
2022-11-09 18:27:06,844:INFO: Dataset: zara2               Batch: 15/18	Loss 80.2388 (83.8949)
2022-11-09 18:27:07,590:INFO: Dataset: zara2               Batch: 16/18	Loss 81.1934 (83.7213)
2022-11-09 18:27:08,336:INFO: Dataset: zara2               Batch: 17/18	Loss 81.2415 (83.5679)
2022-11-09 18:27:09,068:INFO: Dataset: zara2               Batch: 18/18	Loss 70.0102 (82.9471)
2022-11-09 18:27:09,138:INFO: - Computing ADE (validation o)
2022-11-09 18:27:09,403:INFO: 		 ADE on eth                       dataset:	 2.9049274921417236
2022-11-09 18:27:09,403:INFO: Average validation o:	ADE  2.9049	FDE  4.8994
2022-11-09 18:27:09,404:INFO: - Computing ADE (validation)
2022-11-09 18:27:09,679:INFO: 		 ADE on hotel                     dataset:	 1.1250169277191162
2022-11-09 18:27:09,998:INFO: 		 ADE on univ                      dataset:	 1.316638469696045
2022-11-09 18:27:10,266:INFO: 		 ADE on zara1                     dataset:	 1.8296504020690918
2022-11-09 18:27:10,633:INFO: 		 ADE on zara2                     dataset:	 1.2797245979309082
2022-11-09 18:27:10,634:INFO: Average validation:	ADE  1.3224	FDE  2.4184
2022-11-09 18:27:10,634:INFO: - Computing ADE (training)
2022-11-09 18:27:10,978:INFO: 		 ADE on hotel                     dataset:	 1.414967656135559
2022-11-09 18:27:11,608:INFO: 		 ADE on univ                      dataset:	 1.2512379884719849
2022-11-09 18:27:12,048:INFO: 		 ADE on zara1                     dataset:	 1.9837642908096313
2022-11-09 18:27:12,750:INFO: 		 ADE on zara2                     dataset:	 1.4827231168746948
2022-11-09 18:27:12,750:INFO: Average training:	ADE  1.3491	FDE  2.4756
2022-11-09 18:27:12,759:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_289.pth.tar
2022-11-09 18:27:12,759:INFO: 
===> EPOCH: 290 (P3)
2022-11-09 18:27:12,760:INFO: - Computing loss (training)
2022-11-09 18:27:13,680:INFO: Dataset: hotel               Batch: 1/4	Loss 82.4817 (82.4817)
2022-11-09 18:27:14,417:INFO: Dataset: hotel               Batch: 2/4	Loss 82.6768 (82.5790)
2022-11-09 18:27:15,220:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8683 (82.6738)
2022-11-09 18:27:15,904:INFO: Dataset: hotel               Batch: 4/4	Loss 50.0740 (77.2548)
2022-11-09 18:27:16,936:INFO: Dataset: univ                Batch:  1/15	Loss 81.6239 (81.6239)
2022-11-09 18:27:17,674:INFO: Dataset: univ                Batch:  2/15	Loss 81.4017 (81.5165)
2022-11-09 18:27:18,425:INFO: Dataset: univ                Batch:  3/15	Loss 80.5974 (81.2117)
2022-11-09 18:27:19,176:INFO: Dataset: univ                Batch:  4/15	Loss 79.9785 (80.8904)
2022-11-09 18:27:19,920:INFO: Dataset: univ                Batch:  5/15	Loss 80.2577 (80.7680)
2022-11-09 18:27:20,675:INFO: Dataset: univ                Batch:  6/15	Loss 81.1793 (80.8336)
2022-11-09 18:27:21,428:INFO: Dataset: univ                Batch:  7/15	Loss 80.0667 (80.7186)
2022-11-09 18:27:22,181:INFO: Dataset: univ                Batch:  8/15	Loss 81.5762 (80.8305)
2022-11-09 18:27:22,932:INFO: Dataset: univ                Batch:  9/15	Loss 81.0818 (80.8600)
2022-11-09 18:27:23,689:INFO: Dataset: univ                Batch: 10/15	Loss 79.9234 (80.7585)
2022-11-09 18:27:24,443:INFO: Dataset: univ                Batch: 11/15	Loss 80.7152 (80.7546)
2022-11-09 18:27:25,196:INFO: Dataset: univ                Batch: 12/15	Loss 82.2952 (80.8827)
2022-11-09 18:27:25,954:INFO: Dataset: univ                Batch: 13/15	Loss 79.9394 (80.8099)
2022-11-09 18:27:26,758:INFO: Dataset: univ                Batch: 14/15	Loss 79.9208 (80.7374)
2022-11-09 18:27:27,438:INFO: Dataset: univ                Batch: 15/15	Loss 14.6495 (79.7038)
2022-11-09 18:27:28,486:INFO: Dataset: zara1               Batch: 1/8	Loss 83.5020 (83.5020)
2022-11-09 18:27:29,275:INFO: Dataset: zara1               Batch: 2/8	Loss 90.2520 (86.6835)
2022-11-09 18:27:30,142:INFO: Dataset: zara1               Batch: 3/8	Loss 83.8355 (85.6832)
2022-11-09 18:27:30,974:INFO: Dataset: zara1               Batch: 4/8	Loss 82.9479 (84.9497)
2022-11-09 18:27:31,835:INFO: Dataset: zara1               Batch: 5/8	Loss 84.1473 (84.7816)
2022-11-09 18:27:32,635:INFO: Dataset: zara1               Batch: 6/8	Loss 86.1141 (84.9834)
2022-11-09 18:27:33,537:INFO: Dataset: zara1               Batch: 7/8	Loss 83.8073 (84.8263)
2022-11-09 18:27:34,298:INFO: Dataset: zara1               Batch: 8/8	Loss 72.5430 (83.5786)
2022-11-09 18:27:35,405:INFO: Dataset: zara2               Batch:  1/18	Loss 83.3664 (83.3664)
2022-11-09 18:27:36,182:INFO: Dataset: zara2               Batch:  2/18	Loss 81.9167 (82.6992)
2022-11-09 18:27:36,947:INFO: Dataset: zara2               Batch:  3/18	Loss 82.3544 (82.5880)
2022-11-09 18:27:37,750:INFO: Dataset: zara2               Batch:  4/18	Loss 81.6060 (82.3250)
2022-11-09 18:27:38,627:INFO: Dataset: zara2               Batch:  5/18	Loss 80.8059 (82.0093)
2022-11-09 18:27:39,419:INFO: Dataset: zara2               Batch:  6/18	Loss 80.6286 (81.7608)
2022-11-09 18:27:40,179:INFO: Dataset: zara2               Batch:  7/18	Loss 79.7567 (81.4815)
2022-11-09 18:27:40,996:INFO: Dataset: zara2               Batch:  8/18	Loss 81.9867 (81.5522)
2022-11-09 18:27:42,027:INFO: Dataset: zara2               Batch:  9/18	Loss 82.5562 (81.6552)
2022-11-09 18:27:42,790:INFO: Dataset: zara2               Batch: 10/18	Loss 79.7510 (81.4472)
2022-11-09 18:27:43,541:INFO: Dataset: zara2               Batch: 11/18	Loss 81.8613 (81.4886)
2022-11-09 18:27:44,285:INFO: Dataset: zara2               Batch: 12/18	Loss 83.3975 (81.6611)
2022-11-09 18:27:45,038:INFO: Dataset: zara2               Batch: 13/18	Loss 82.0817 (81.6906)
2022-11-09 18:27:45,786:INFO: Dataset: zara2               Batch: 14/18	Loss 81.8734 (81.7033)
2022-11-09 18:27:46,561:INFO: Dataset: zara2               Batch: 15/18	Loss 80.9543 (81.6533)
2022-11-09 18:27:47,305:INFO: Dataset: zara2               Batch: 16/18	Loss 79.8333 (81.5320)
2022-11-09 18:27:48,051:INFO: Dataset: zara2               Batch: 17/18	Loss 79.9044 (81.4412)
2022-11-09 18:27:48,786:INFO: Dataset: zara2               Batch: 18/18	Loss 68.3310 (80.8321)
2022-11-09 18:27:48,853:INFO: - Computing ADE (validation o)
2022-11-09 18:27:49,135:INFO: 		 ADE on eth                       dataset:	 2.746100664138794
2022-11-09 18:27:49,135:INFO: Average validation o:	ADE  2.7461	FDE  4.5605
2022-11-09 18:27:49,136:INFO: - Computing ADE (validation)
2022-11-09 18:27:49,407:INFO: 		 ADE on hotel                     dataset:	 1.1419274806976318
2022-11-09 18:27:49,728:INFO: 		 ADE on univ                      dataset:	 1.32046377658844
2022-11-09 18:27:50,002:INFO: 		 ADE on zara1                     dataset:	 1.8972517251968384
2022-11-09 18:27:50,367:INFO: 		 ADE on zara2                     dataset:	 1.2577203512191772
2022-11-09 18:27:50,368:INFO: Average validation:	ADE  1.3212	FDE  2.4175
2022-11-09 18:27:50,369:INFO: - Computing ADE (training)
2022-11-09 18:27:50,698:INFO: 		 ADE on hotel                     dataset:	 1.4127471446990967
2022-11-09 18:27:51,329:INFO: 		 ADE on univ                      dataset:	 1.251639723777771
2022-11-09 18:27:51,782:INFO: 		 ADE on zara1                     dataset:	 1.9558053016662598
2022-11-09 18:27:52,488:INFO: 		 ADE on zara2                     dataset:	 1.4557307958602905
2022-11-09 18:27:52,489:INFO: Average training:	ADE  1.3420	FDE  2.4533
2022-11-09 18:27:52,499:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_290.pth.tar
2022-11-09 18:27:52,499:INFO: 
===> EPOCH: 291 (P3)
2022-11-09 18:27:52,499:INFO: - Computing loss (training)
2022-11-09 18:27:53,425:INFO: Dataset: hotel               Batch: 1/4	Loss 82.1883 (82.1883)
2022-11-09 18:27:54,236:INFO: Dataset: hotel               Batch: 2/4	Loss 81.5565 (81.8794)
2022-11-09 18:27:54,959:INFO: Dataset: hotel               Batch: 3/4	Loss 81.2984 (81.6737)
2022-11-09 18:27:55,643:INFO: Dataset: hotel               Batch: 4/4	Loss 51.4485 (76.5697)
2022-11-09 18:27:56,686:INFO: Dataset: univ                Batch:  1/15	Loss 81.0393 (81.0393)
2022-11-09 18:27:57,423:INFO: Dataset: univ                Batch:  2/15	Loss 81.7291 (81.3622)
2022-11-09 18:27:58,172:INFO: Dataset: univ                Batch:  3/15	Loss 81.0034 (81.2457)
2022-11-09 18:27:58,940:INFO: Dataset: univ                Batch:  4/15	Loss 79.9765 (80.9092)
2022-11-09 18:27:59,696:INFO: Dataset: univ                Batch:  5/15	Loss 79.7512 (80.6650)
2022-11-09 18:28:00,448:INFO: Dataset: univ                Batch:  6/15	Loss 79.8143 (80.5237)
2022-11-09 18:28:01,198:INFO: Dataset: univ                Batch:  7/15	Loss 80.9471 (80.5813)
2022-11-09 18:28:01,950:INFO: Dataset: univ                Batch:  8/15	Loss 80.5275 (80.5745)
2022-11-09 18:28:02,700:INFO: Dataset: univ                Batch:  9/15	Loss 80.0094 (80.5112)
2022-11-09 18:28:03,451:INFO: Dataset: univ                Batch: 10/15	Loss 79.5529 (80.4196)
2022-11-09 18:28:04,203:INFO: Dataset: univ                Batch: 11/15	Loss 80.4241 (80.4200)
2022-11-09 18:28:04,955:INFO: Dataset: univ                Batch: 12/15	Loss 79.7806 (80.3666)
2022-11-09 18:28:05,704:INFO: Dataset: univ                Batch: 13/15	Loss 82.1576 (80.5034)
2022-11-09 18:28:06,466:INFO: Dataset: univ                Batch: 14/15	Loss 79.3730 (80.4159)
2022-11-09 18:28:07,123:INFO: Dataset: univ                Batch: 15/15	Loss 14.9338 (79.7363)
2022-11-09 18:28:08,139:INFO: Dataset: zara1               Batch: 1/8	Loss 86.6622 (86.6622)
2022-11-09 18:28:08,879:INFO: Dataset: zara1               Batch: 2/8	Loss 85.6089 (86.0967)
2022-11-09 18:28:09,615:INFO: Dataset: zara1               Batch: 3/8	Loss 88.2125 (86.6874)
2022-11-09 18:28:10,349:INFO: Dataset: zara1               Batch: 4/8	Loss 86.2377 (86.5657)
2022-11-09 18:28:11,084:INFO: Dataset: zara1               Batch: 5/8	Loss 82.4892 (85.7143)
2022-11-09 18:28:11,895:INFO: Dataset: zara1               Batch: 6/8	Loss 82.2300 (85.1001)
2022-11-09 18:28:12,636:INFO: Dataset: zara1               Batch: 7/8	Loss 82.8377 (84.7520)
2022-11-09 18:28:13,357:INFO: Dataset: zara1               Batch: 8/8	Loss 71.9579 (83.2907)
2022-11-09 18:28:14,378:INFO: Dataset: zara2               Batch:  1/18	Loss 93.7682 (93.7682)
2022-11-09 18:28:15,120:INFO: Dataset: zara2               Batch:  2/18	Loss 83.5287 (88.1231)
2022-11-09 18:28:15,875:INFO: Dataset: zara2               Batch:  3/18	Loss 85.1670 (87.1530)
2022-11-09 18:28:16,617:INFO: Dataset: zara2               Batch:  4/18	Loss 82.9646 (86.0348)
2022-11-09 18:28:17,364:INFO: Dataset: zara2               Batch:  5/18	Loss 82.7083 (85.3401)
2022-11-09 18:28:18,108:INFO: Dataset: zara2               Batch:  6/18	Loss 80.5697 (84.5160)
2022-11-09 18:28:18,849:INFO: Dataset: zara2               Batch:  7/18	Loss 80.6038 (83.9327)
2022-11-09 18:28:19,588:INFO: Dataset: zara2               Batch:  8/18	Loss 80.7193 (83.5086)
2022-11-09 18:28:20,328:INFO: Dataset: zara2               Batch:  9/18	Loss 80.4560 (83.1606)
2022-11-09 18:28:21,074:INFO: Dataset: zara2               Batch: 10/18	Loss 80.8227 (82.9323)
2022-11-09 18:28:21,818:INFO: Dataset: zara2               Batch: 11/18	Loss 82.4507 (82.8916)
2022-11-09 18:28:22,566:INFO: Dataset: zara2               Batch: 12/18	Loss 83.2932 (82.9206)
2022-11-09 18:28:23,317:INFO: Dataset: zara2               Batch: 13/18	Loss 81.5202 (82.8154)
2022-11-09 18:28:24,063:INFO: Dataset: zara2               Batch: 14/18	Loss 80.1668 (82.6247)
2022-11-09 18:28:24,808:INFO: Dataset: zara2               Batch: 15/18	Loss 80.1862 (82.4658)
2022-11-09 18:28:25,551:INFO: Dataset: zara2               Batch: 16/18	Loss 80.8763 (82.3647)
2022-11-09 18:28:26,315:INFO: Dataset: zara2               Batch: 17/18	Loss 80.7998 (82.2658)
2022-11-09 18:28:27,075:INFO: Dataset: zara2               Batch: 18/18	Loss 69.4580 (81.6115)
2022-11-09 18:28:27,153:INFO: - Computing ADE (validation o)
2022-11-09 18:28:27,435:INFO: 		 ADE on eth                       dataset:	 2.762005567550659
2022-11-09 18:28:27,436:INFO: Average validation o:	ADE  2.7620	FDE  4.5894
2022-11-09 18:28:27,436:INFO: - Computing ADE (validation)
2022-11-09 18:28:27,709:INFO: 		 ADE on hotel                     dataset:	 1.3572564125061035
2022-11-09 18:28:28,038:INFO: 		 ADE on univ                      dataset:	 1.5119363069534302
2022-11-09 18:28:28,334:INFO: 		 ADE on zara1                     dataset:	 2.2068023681640625
2022-11-09 18:28:28,760:INFO: 		 ADE on zara2                     dataset:	 1.464113473892212
2022-11-09 18:28:28,760:INFO: Average validation:	ADE  1.5263	FDE  2.8576
2022-11-09 18:28:28,761:INFO: - Computing ADE (training)
2022-11-09 18:28:29,163:INFO: 		 ADE on hotel                     dataset:	 1.622783899307251
2022-11-09 18:28:29,796:INFO: 		 ADE on univ                      dataset:	 1.4356426000595093
2022-11-09 18:28:30,258:INFO: 		 ADE on zara1                     dataset:	 2.030939817428589
2022-11-09 18:28:31,002:INFO: 		 ADE on zara2                     dataset:	 1.5816845893859863
2022-11-09 18:28:31,002:INFO: Average training:	ADE  1.5080	FDE  2.8117
2022-11-09 18:28:31,012:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_291.pth.tar
2022-11-09 18:28:31,012:INFO: 
===> EPOCH: 292 (P3)
2022-11-09 18:28:31,012:INFO: - Computing loss (training)
2022-11-09 18:28:31,993:INFO: Dataset: hotel               Batch: 1/4	Loss 87.0685 (87.0685)
2022-11-09 18:28:32,774:INFO: Dataset: hotel               Batch: 2/4	Loss 85.1890 (86.1585)
2022-11-09 18:28:33,644:INFO: Dataset: hotel               Batch: 3/4	Loss 82.9617 (85.0639)
2022-11-09 18:28:34,351:INFO: Dataset: hotel               Batch: 4/4	Loss 50.0307 (78.9169)
2022-11-09 18:28:35,440:INFO: Dataset: univ                Batch:  1/15	Loss 83.0083 (83.0083)
2022-11-09 18:28:36,259:INFO: Dataset: univ                Batch:  2/15	Loss 79.8939 (81.3923)
2022-11-09 18:28:37,088:INFO: Dataset: univ                Batch:  3/15	Loss 80.5289 (81.0903)
2022-11-09 18:28:37,880:INFO: Dataset: univ                Batch:  4/15	Loss 80.3534 (80.8956)
2022-11-09 18:28:38,633:INFO: Dataset: univ                Batch:  5/15	Loss 82.2753 (81.1705)
2022-11-09 18:28:39,386:INFO: Dataset: univ                Batch:  6/15	Loss 84.2184 (81.6835)
2022-11-09 18:28:40,160:INFO: Dataset: univ                Batch:  7/15	Loss 81.8345 (81.7072)
2022-11-09 18:28:40,966:INFO: Dataset: univ                Batch:  8/15	Loss 79.6406 (81.4378)
2022-11-09 18:28:41,811:INFO: Dataset: univ                Batch:  9/15	Loss 79.4563 (81.1974)
2022-11-09 18:28:42,606:INFO: Dataset: univ                Batch: 10/15	Loss 80.1121 (81.0769)
2022-11-09 18:28:43,442:INFO: Dataset: univ                Batch: 11/15	Loss 80.7686 (81.0471)
2022-11-09 18:28:44,205:INFO: Dataset: univ                Batch: 12/15	Loss 79.7476 (80.9343)
2022-11-09 18:28:45,039:INFO: Dataset: univ                Batch: 13/15	Loss 80.2427 (80.8806)
2022-11-09 18:28:45,845:INFO: Dataset: univ                Batch: 14/15	Loss 80.7290 (80.8698)
2022-11-09 18:28:46,533:INFO: Dataset: univ                Batch: 15/15	Loss 14.8304 (79.7119)
2022-11-09 18:28:47,560:INFO: Dataset: zara1               Batch: 1/8	Loss 92.5482 (92.5482)
2022-11-09 18:28:48,355:INFO: Dataset: zara1               Batch: 2/8	Loss 84.5311 (88.3525)
2022-11-09 18:28:49,101:INFO: Dataset: zara1               Batch: 3/8	Loss 89.2303 (88.6428)
2022-11-09 18:28:49,837:INFO: Dataset: zara1               Batch: 4/8	Loss 85.2728 (87.8252)
2022-11-09 18:28:50,581:INFO: Dataset: zara1               Batch: 5/8	Loss 84.5908 (87.2230)
2022-11-09 18:28:51,450:INFO: Dataset: zara1               Batch: 6/8	Loss 84.0586 (86.7426)
2022-11-09 18:28:52,523:INFO: Dataset: zara1               Batch: 7/8	Loss 86.9882 (86.7742)
2022-11-09 18:28:53,247:INFO: Dataset: zara1               Batch: 8/8	Loss 72.6570 (85.1842)
2022-11-09 18:28:54,249:INFO: Dataset: zara2               Batch:  1/18	Loss 86.2309 (86.2309)
2022-11-09 18:28:54,989:INFO: Dataset: zara2               Batch:  2/18	Loss 91.0396 (88.4698)
2022-11-09 18:28:55,727:INFO: Dataset: zara2               Batch:  3/18	Loss 85.7856 (87.4923)
2022-11-09 18:28:56,466:INFO: Dataset: zara2               Batch:  4/18	Loss 81.3444 (85.9718)
2022-11-09 18:28:57,212:INFO: Dataset: zara2               Batch:  5/18	Loss 81.0326 (84.8906)
2022-11-09 18:28:57,951:INFO: Dataset: zara2               Batch:  6/18	Loss 81.9452 (84.4249)
2022-11-09 18:28:58,689:INFO: Dataset: zara2               Batch:  7/18	Loss 81.5705 (83.9685)
2022-11-09 18:28:59,456:INFO: Dataset: zara2               Batch:  8/18	Loss 82.5914 (83.8012)
2022-11-09 18:29:00,209:INFO: Dataset: zara2               Batch:  9/18	Loss 82.5130 (83.6692)
2022-11-09 18:29:00,987:INFO: Dataset: zara2               Batch: 10/18	Loss 81.7522 (83.4786)
2022-11-09 18:29:01,750:INFO: Dataset: zara2               Batch: 11/18	Loss 81.9977 (83.3330)
2022-11-09 18:29:02,639:INFO: Dataset: zara2               Batch: 12/18	Loss 81.3421 (83.1620)
2022-11-09 18:29:03,464:INFO: Dataset: zara2               Batch: 13/18	Loss 80.4773 (82.9406)
2022-11-09 18:29:04,288:INFO: Dataset: zara2               Batch: 14/18	Loss 82.4152 (82.9028)
2022-11-09 18:29:05,093:INFO: Dataset: zara2               Batch: 15/18	Loss 80.2354 (82.7122)
2022-11-09 18:29:05,885:INFO: Dataset: zara2               Batch: 16/18	Loss 81.6661 (82.6484)
2022-11-09 18:29:06,736:INFO: Dataset: zara2               Batch: 17/18	Loss 80.7311 (82.5356)
2022-11-09 18:29:07,523:INFO: Dataset: zara2               Batch: 18/18	Loss 69.9674 (81.9767)
2022-11-09 18:29:07,603:INFO: - Computing ADE (validation o)
2022-11-09 18:29:07,918:INFO: 		 ADE on eth                       dataset:	 3.006324529647827
2022-11-09 18:29:07,918:INFO: Average validation o:	ADE  3.0063	FDE  5.1580
2022-11-09 18:29:07,919:INFO: - Computing ADE (validation)
2022-11-09 18:29:08,186:INFO: 		 ADE on hotel                     dataset:	 1.1335147619247437
2022-11-09 18:29:08,511:INFO: 		 ADE on univ                      dataset:	 1.3319438695907593
2022-11-09 18:29:08,788:INFO: 		 ADE on zara1                     dataset:	 1.8273862600326538
2022-11-09 18:29:09,162:INFO: 		 ADE on zara2                     dataset:	 1.2921147346496582
2022-11-09 18:29:09,162:INFO: Average validation:	ADE  1.3353	FDE  2.4556
2022-11-09 18:29:09,163:INFO: - Computing ADE (training)
2022-11-09 18:29:09,522:INFO: 		 ADE on hotel                     dataset:	 1.404030442237854
2022-11-09 18:29:10,222:INFO: 		 ADE on univ                      dataset:	 1.2614604234695435
2022-11-09 18:29:10,701:INFO: 		 ADE on zara1                     dataset:	 2.013432502746582
2022-11-09 18:29:11,441:INFO: 		 ADE on zara2                     dataset:	 1.507429838180542
2022-11-09 18:29:11,442:INFO: Average training:	ADE  1.3629	FDE  2.5158
2022-11-09 18:29:11,452:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_292.pth.tar
2022-11-09 18:29:11,452:INFO: 
===> EPOCH: 293 (P3)
2022-11-09 18:29:11,452:INFO: - Computing loss (training)
2022-11-09 18:29:12,444:INFO: Dataset: hotel               Batch: 1/4	Loss 82.8876 (82.8876)
2022-11-09 18:29:13,301:INFO: Dataset: hotel               Batch: 2/4	Loss 81.0825 (82.0111)
2022-11-09 18:29:14,040:INFO: Dataset: hotel               Batch: 3/4	Loss 81.6656 (81.8963)
2022-11-09 18:29:14,780:INFO: Dataset: hotel               Batch: 4/4	Loss 49.3258 (76.0955)
2022-11-09 18:29:15,861:INFO: Dataset: univ                Batch:  1/15	Loss 81.6587 (81.6587)
2022-11-09 18:29:16,642:INFO: Dataset: univ                Batch:  2/15	Loss 82.3808 (82.0430)
2022-11-09 18:29:17,444:INFO: Dataset: univ                Batch:  3/15	Loss 82.4494 (82.1712)
2022-11-09 18:29:18,444:INFO: Dataset: univ                Batch:  4/15	Loss 80.3489 (81.7229)
2022-11-09 18:29:19,224:INFO: Dataset: univ                Batch:  5/15	Loss 80.0412 (81.3704)
2022-11-09 18:29:19,993:INFO: Dataset: univ                Batch:  6/15	Loss 80.3631 (81.2088)
2022-11-09 18:29:20,744:INFO: Dataset: univ                Batch:  7/15	Loss 80.4805 (81.1106)
2022-11-09 18:29:21,502:INFO: Dataset: univ                Batch:  8/15	Loss 81.4397 (81.1527)
2022-11-09 18:29:22,262:INFO: Dataset: univ                Batch:  9/15	Loss 79.3776 (80.9485)
2022-11-09 18:29:23,021:INFO: Dataset: univ                Batch: 10/15	Loss 80.1212 (80.8642)
2022-11-09 18:29:23,771:INFO: Dataset: univ                Batch: 11/15	Loss 81.5592 (80.9231)
2022-11-09 18:29:24,533:INFO: Dataset: univ                Batch: 12/15	Loss 80.5370 (80.8899)
2022-11-09 18:29:25,283:INFO: Dataset: univ                Batch: 13/15	Loss 82.3780 (80.9923)
2022-11-09 18:29:26,044:INFO: Dataset: univ                Batch: 14/15	Loss 80.5499 (80.9611)
2022-11-09 18:29:26,704:INFO: Dataset: univ                Batch: 15/15	Loss 15.4131 (80.3927)
2022-11-09 18:29:27,715:INFO: Dataset: zara1               Batch: 1/8	Loss 87.4054 (87.4054)
2022-11-09 18:29:28,455:INFO: Dataset: zara1               Batch: 2/8	Loss 86.4606 (86.9426)
2022-11-09 18:29:29,193:INFO: Dataset: zara1               Batch: 3/8	Loss 85.3373 (86.4526)
2022-11-09 18:29:29,938:INFO: Dataset: zara1               Batch: 4/8	Loss 82.8795 (85.4986)
2022-11-09 18:29:30,691:INFO: Dataset: zara1               Batch: 5/8	Loss 83.9117 (85.1923)
2022-11-09 18:29:31,507:INFO: Dataset: zara1               Batch: 6/8	Loss 84.3599 (85.0542)
2022-11-09 18:29:32,250:INFO: Dataset: zara1               Batch: 7/8	Loss 85.2620 (85.0857)
2022-11-09 18:29:32,974:INFO: Dataset: zara1               Batch: 8/8	Loss 73.4403 (83.8476)
2022-11-09 18:29:34,001:INFO: Dataset: zara2               Batch:  1/18	Loss 87.8985 (87.8985)
2022-11-09 18:29:34,753:INFO: Dataset: zara2               Batch:  2/18	Loss 82.2103 (84.8938)
2022-11-09 18:29:35,501:INFO: Dataset: zara2               Batch:  3/18	Loss 82.0836 (83.9677)
2022-11-09 18:29:36,249:INFO: Dataset: zara2               Batch:  4/18	Loss 80.9076 (83.2442)
2022-11-09 18:29:37,000:INFO: Dataset: zara2               Batch:  5/18	Loss 80.9909 (82.8027)
2022-11-09 18:29:37,752:INFO: Dataset: zara2               Batch:  6/18	Loss 80.6969 (82.4706)
2022-11-09 18:29:38,500:INFO: Dataset: zara2               Batch:  7/18	Loss 82.1785 (82.4281)
2022-11-09 18:29:39,246:INFO: Dataset: zara2               Batch:  8/18	Loss 81.4389 (82.2980)
2022-11-09 18:29:39,996:INFO: Dataset: zara2               Batch:  9/18	Loss 81.0537 (82.1608)
2022-11-09 18:29:40,745:INFO: Dataset: zara2               Batch: 10/18	Loss 80.6268 (82.0145)
2022-11-09 18:29:41,494:INFO: Dataset: zara2               Batch: 11/18	Loss 80.3042 (81.8586)
2022-11-09 18:29:42,247:INFO: Dataset: zara2               Batch: 12/18	Loss 82.9138 (81.9375)
2022-11-09 18:29:42,995:INFO: Dataset: zara2               Batch: 13/18	Loss 80.4100 (81.8107)
2022-11-09 18:29:43,747:INFO: Dataset: zara2               Batch: 14/18	Loss 80.7957 (81.7408)
2022-11-09 18:29:44,498:INFO: Dataset: zara2               Batch: 15/18	Loss 79.9735 (81.6221)
2022-11-09 18:29:45,255:INFO: Dataset: zara2               Batch: 16/18	Loss 82.8090 (81.6947)
2022-11-09 18:29:46,009:INFO: Dataset: zara2               Batch: 17/18	Loss 79.7844 (81.5851)
2022-11-09 18:29:46,747:INFO: Dataset: zara2               Batch: 18/18	Loss 68.5506 (80.9818)
2022-11-09 18:29:46,816:INFO: - Computing ADE (validation o)
2022-11-09 18:29:47,090:INFO: 		 ADE on eth                       dataset:	 2.7800331115722656
2022-11-09 18:29:47,090:INFO: Average validation o:	ADE  2.7800	FDE  4.6207
2022-11-09 18:29:47,091:INFO: - Computing ADE (validation)
2022-11-09 18:29:47,359:INFO: 		 ADE on hotel                     dataset:	 1.0914114713668823
2022-11-09 18:29:47,676:INFO: 		 ADE on univ                      dataset:	 1.3313535451889038
2022-11-09 18:29:47,939:INFO: 		 ADE on zara1                     dataset:	 1.981365442276001
2022-11-09 18:29:48,312:INFO: 		 ADE on zara2                     dataset:	 1.24779212474823
2022-11-09 18:29:48,312:INFO: Average validation:	ADE  1.3254	FDE  2.4302
2022-11-09 18:29:48,313:INFO: - Computing ADE (training)
2022-11-09 18:29:48,642:INFO: 		 ADE on hotel                     dataset:	 1.37571120262146
2022-11-09 18:29:49,273:INFO: 		 ADE on univ                      dataset:	 1.2322112321853638
2022-11-09 18:29:49,711:INFO: 		 ADE on zara1                     dataset:	 1.9698452949523926
2022-11-09 18:29:50,441:INFO: 		 ADE on zara2                     dataset:	 1.440163254737854
2022-11-09 18:29:50,442:INFO: Average training:	ADE  1.3251	FDE  2.4298
2022-11-09 18:29:50,452:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_293.pth.tar
2022-11-09 18:29:50,452:INFO: 
===> EPOCH: 294 (P3)
2022-11-09 18:29:50,452:INFO: - Computing loss (training)
2022-11-09 18:29:51,372:INFO: Dataset: hotel               Batch: 1/4	Loss 83.4669 (83.4669)
2022-11-09 18:29:52,107:INFO: Dataset: hotel               Batch: 2/4	Loss 81.9604 (82.7155)
2022-11-09 18:29:52,905:INFO: Dataset: hotel               Batch: 3/4	Loss 81.6679 (82.3624)
2022-11-09 18:29:53,589:INFO: Dataset: hotel               Batch: 4/4	Loss 52.7703 (76.9749)
2022-11-09 18:29:54,602:INFO: Dataset: univ                Batch:  1/15	Loss 80.6448 (80.6448)
2022-11-09 18:29:55,351:INFO: Dataset: univ                Batch:  2/15	Loss 80.5323 (80.5861)
2022-11-09 18:29:56,105:INFO: Dataset: univ                Batch:  3/15	Loss 79.8370 (80.3375)
2022-11-09 18:29:56,845:INFO: Dataset: univ                Batch:  4/15	Loss 81.7520 (80.6883)
2022-11-09 18:29:57,592:INFO: Dataset: univ                Batch:  5/15	Loss 79.9420 (80.5321)
2022-11-09 18:29:58,336:INFO: Dataset: univ                Batch:  6/15	Loss 80.6362 (80.5479)
2022-11-09 18:29:59,085:INFO: Dataset: univ                Batch:  7/15	Loss 79.9719 (80.4652)
2022-11-09 18:29:59,837:INFO: Dataset: univ                Batch:  8/15	Loss 79.9716 (80.3980)
2022-11-09 18:30:00,593:INFO: Dataset: univ                Batch:  9/15	Loss 79.6808 (80.3104)
2022-11-09 18:30:01,347:INFO: Dataset: univ                Batch: 10/15	Loss 79.9772 (80.2760)
2022-11-09 18:30:02,100:INFO: Dataset: univ                Batch: 11/15	Loss 79.8860 (80.2406)
2022-11-09 18:30:02,849:INFO: Dataset: univ                Batch: 12/15	Loss 79.4870 (80.1776)
2022-11-09 18:30:03,595:INFO: Dataset: univ                Batch: 13/15	Loss 80.1395 (80.1749)
2022-11-09 18:30:04,347:INFO: Dataset: univ                Batch: 14/15	Loss 79.6907 (80.1404)
2022-11-09 18:30:05,008:INFO: Dataset: univ                Batch: 15/15	Loss 14.9296 (79.3060)
2022-11-09 18:30:06,013:INFO: Dataset: zara1               Batch: 1/8	Loss 88.2248 (88.2248)
2022-11-09 18:30:06,767:INFO: Dataset: zara1               Batch: 2/8	Loss 88.6855 (88.4700)
2022-11-09 18:30:07,512:INFO: Dataset: zara1               Batch: 3/8	Loss 90.3693 (89.0273)
2022-11-09 18:30:08,260:INFO: Dataset: zara1               Batch: 4/8	Loss 82.1184 (87.3448)
2022-11-09 18:30:09,081:INFO: Dataset: zara1               Batch: 5/8	Loss 82.4506 (86.3458)
2022-11-09 18:30:09,831:INFO: Dataset: zara1               Batch: 6/8	Loss 82.6016 (85.6482)
2022-11-09 18:30:10,583:INFO: Dataset: zara1               Batch: 7/8	Loss 83.3507 (85.2930)
2022-11-09 18:30:11,316:INFO: Dataset: zara1               Batch: 8/8	Loss 71.2336 (83.7761)
2022-11-09 18:30:12,333:INFO: Dataset: zara2               Batch:  1/18	Loss 84.5975 (84.5975)
2022-11-09 18:30:13,084:INFO: Dataset: zara2               Batch:  2/18	Loss 83.4057 (84.0465)
2022-11-09 18:30:13,833:INFO: Dataset: zara2               Batch:  3/18	Loss 84.9727 (84.3698)
2022-11-09 18:30:14,587:INFO: Dataset: zara2               Batch:  4/18	Loss 83.0258 (84.0604)
2022-11-09 18:30:15,344:INFO: Dataset: zara2               Batch:  5/18	Loss 81.9572 (83.6521)
2022-11-09 18:30:16,096:INFO: Dataset: zara2               Batch:  6/18	Loss 81.6301 (83.3032)
2022-11-09 18:30:16,843:INFO: Dataset: zara2               Batch:  7/18	Loss 80.8279 (82.9459)
2022-11-09 18:30:17,592:INFO: Dataset: zara2               Batch:  8/18	Loss 79.5141 (82.5183)
2022-11-09 18:30:18,344:INFO: Dataset: zara2               Batch:  9/18	Loss 80.8150 (82.3329)
2022-11-09 18:30:19,098:INFO: Dataset: zara2               Batch: 10/18	Loss 82.0978 (82.3135)
2022-11-09 18:30:19,849:INFO: Dataset: zara2               Batch: 11/18	Loss 79.7725 (82.0933)
2022-11-09 18:30:20,601:INFO: Dataset: zara2               Batch: 12/18	Loss 80.4995 (81.9598)
2022-11-09 18:30:21,352:INFO: Dataset: zara2               Batch: 13/18	Loss 79.9513 (81.8188)
2022-11-09 18:30:22,101:INFO: Dataset: zara2               Batch: 14/18	Loss 79.8530 (81.6699)
2022-11-09 18:30:22,853:INFO: Dataset: zara2               Batch: 15/18	Loss 81.3920 (81.6489)
2022-11-09 18:30:23,606:INFO: Dataset: zara2               Batch: 16/18	Loss 80.6702 (81.5964)
2022-11-09 18:30:24,360:INFO: Dataset: zara2               Batch: 17/18	Loss 79.8000 (81.4984)
2022-11-09 18:30:25,100:INFO: Dataset: zara2               Batch: 18/18	Loss 68.6194 (80.9172)
2022-11-09 18:30:25,167:INFO: - Computing ADE (validation o)
2022-11-09 18:30:25,434:INFO: 		 ADE on eth                       dataset:	 2.6588141918182373
2022-11-09 18:30:25,434:INFO: Average validation o:	ADE  2.6588	FDE  4.3517
2022-11-09 18:30:25,435:INFO: - Computing ADE (validation)
2022-11-09 18:30:25,708:INFO: 		 ADE on hotel                     dataset:	 1.1844685077667236
2022-11-09 18:30:26,021:INFO: 		 ADE on univ                      dataset:	 1.329357385635376
2022-11-09 18:30:26,303:INFO: 		 ADE on zara1                     dataset:	 1.9088116884231567
2022-11-09 18:30:26,673:INFO: 		 ADE on zara2                     dataset:	 1.2468162775039673
2022-11-09 18:30:26,674:INFO: Average validation:	ADE  1.3248	FDE  2.4506
2022-11-09 18:30:26,674:INFO: - Computing ADE (training)
2022-11-09 18:30:27,012:INFO: 		 ADE on hotel                     dataset:	 1.4551936388015747
2022-11-09 18:30:27,649:INFO: 		 ADE on univ                      dataset:	 1.2619277238845825
2022-11-09 18:30:28,090:INFO: 		 ADE on zara1                     dataset:	 1.8572759628295898
2022-11-09 18:30:28,780:INFO: 		 ADE on zara2                     dataset:	 1.4076539278030396
2022-11-09 18:30:28,781:INFO: Average training:	ADE  1.3344	FDE  2.4655
2022-11-09 18:30:28,791:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_294.pth.tar
2022-11-09 18:30:28,791:INFO: 
===> EPOCH: 295 (P3)
2022-11-09 18:30:28,791:INFO: - Computing loss (training)
2022-11-09 18:30:29,804:INFO: Dataset: hotel               Batch: 1/4	Loss 81.3784 (81.3784)
2022-11-09 18:30:30,542:INFO: Dataset: hotel               Batch: 2/4	Loss 81.4378 (81.4085)
2022-11-09 18:30:31,266:INFO: Dataset: hotel               Batch: 3/4	Loss 81.9033 (81.5750)
2022-11-09 18:30:31,958:INFO: Dataset: hotel               Batch: 4/4	Loss 49.4201 (75.6361)
2022-11-09 18:30:33,006:INFO: Dataset: univ                Batch:  1/15	Loss 79.5753 (79.5753)
2022-11-09 18:30:33,767:INFO: Dataset: univ                Batch:  2/15	Loss 81.4221 (80.5026)
2022-11-09 18:30:34,524:INFO: Dataset: univ                Batch:  3/15	Loss 80.5572 (80.5208)
2022-11-09 18:30:35,274:INFO: Dataset: univ                Batch:  4/15	Loss 79.3121 (80.2250)
2022-11-09 18:30:36,030:INFO: Dataset: univ                Batch:  5/15	Loss 79.4701 (80.0803)
2022-11-09 18:30:36,784:INFO: Dataset: univ                Batch:  6/15	Loss 79.5388 (79.9922)
2022-11-09 18:30:37,538:INFO: Dataset: univ                Batch:  7/15	Loss 79.6693 (79.9463)
2022-11-09 18:30:38,291:INFO: Dataset: univ                Batch:  8/15	Loss 79.8196 (79.9303)
2022-11-09 18:30:39,040:INFO: Dataset: univ                Batch:  9/15	Loss 82.1021 (80.1445)
2022-11-09 18:30:39,797:INFO: Dataset: univ                Batch: 10/15	Loss 78.9907 (80.0268)
2022-11-09 18:30:40,551:INFO: Dataset: univ                Batch: 11/15	Loss 79.9189 (80.0173)
2022-11-09 18:30:41,303:INFO: Dataset: univ                Batch: 12/15	Loss 81.1399 (80.1028)
2022-11-09 18:30:42,057:INFO: Dataset: univ                Batch: 13/15	Loss 80.4230 (80.1251)
2022-11-09 18:30:42,830:INFO: Dataset: univ                Batch: 14/15	Loss 79.7328 (80.0935)
2022-11-09 18:30:43,501:INFO: Dataset: univ                Batch: 15/15	Loss 14.7797 (78.9885)
2022-11-09 18:30:44,517:INFO: Dataset: zara1               Batch: 1/8	Loss 86.6084 (86.6084)
2022-11-09 18:30:45,264:INFO: Dataset: zara1               Batch: 2/8	Loss 81.1483 (83.7236)
2022-11-09 18:30:46,085:INFO: Dataset: zara1               Batch: 3/8	Loss 85.1926 (84.2276)
2022-11-09 18:30:46,822:INFO: Dataset: zara1               Batch: 4/8	Loss 83.1703 (83.9654)
2022-11-09 18:30:47,558:INFO: Dataset: zara1               Batch: 5/8	Loss 81.8322 (83.5609)
2022-11-09 18:30:48,296:INFO: Dataset: zara1               Batch: 6/8	Loss 83.4842 (83.5493)
2022-11-09 18:30:49,032:INFO: Dataset: zara1               Batch: 7/8	Loss 87.2567 (84.0670)
2022-11-09 18:30:49,755:INFO: Dataset: zara1               Batch: 8/8	Loss 73.8573 (82.9386)
2022-11-09 18:30:50,792:INFO: Dataset: zara2               Batch:  1/18	Loss 85.4546 (85.4546)
2022-11-09 18:30:51,540:INFO: Dataset: zara2               Batch:  2/18	Loss 82.6110 (84.0079)
2022-11-09 18:30:52,290:INFO: Dataset: zara2               Batch:  3/18	Loss 81.6963 (83.2270)
2022-11-09 18:30:53,037:INFO: Dataset: zara2               Batch:  4/18	Loss 80.0394 (82.4255)
2022-11-09 18:30:53,784:INFO: Dataset: zara2               Batch:  5/18	Loss 81.6069 (82.2702)
2022-11-09 18:30:54,531:INFO: Dataset: zara2               Batch:  6/18	Loss 81.0152 (82.0537)
2022-11-09 18:30:55,282:INFO: Dataset: zara2               Batch:  7/18	Loss 80.1119 (81.7987)
2022-11-09 18:30:56,029:INFO: Dataset: zara2               Batch:  8/18	Loss 80.4631 (81.6281)
2022-11-09 18:30:56,774:INFO: Dataset: zara2               Batch:  9/18	Loss 82.1353 (81.6853)
2022-11-09 18:30:57,522:INFO: Dataset: zara2               Batch: 10/18	Loss 80.3291 (81.5526)
2022-11-09 18:30:58,273:INFO: Dataset: zara2               Batch: 11/18	Loss 80.0041 (81.4252)
2022-11-09 18:30:59,021:INFO: Dataset: zara2               Batch: 12/18	Loss 79.9286 (81.2934)
2022-11-09 18:30:59,767:INFO: Dataset: zara2               Batch: 13/18	Loss 81.5922 (81.3171)
2022-11-09 18:31:00,517:INFO: Dataset: zara2               Batch: 14/18	Loss 80.1172 (81.2276)
2022-11-09 18:31:01,263:INFO: Dataset: zara2               Batch: 15/18	Loss 80.2636 (81.1574)
2022-11-09 18:31:02,093:INFO: Dataset: zara2               Batch: 16/18	Loss 80.3754 (81.1073)
2022-11-09 18:31:02,843:INFO: Dataset: zara2               Batch: 17/18	Loss 80.1423 (81.0561)
2022-11-09 18:31:03,574:INFO: Dataset: zara2               Batch: 18/18	Loss 68.5774 (80.5569)
2022-11-09 18:31:03,638:INFO: - Computing ADE (validation o)
2022-11-09 18:31:03,930:INFO: 		 ADE on eth                       dataset:	 2.7498979568481445
2022-11-09 18:31:03,930:INFO: Average validation o:	ADE  2.7499	FDE  4.5363
2022-11-09 18:31:03,931:INFO: - Computing ADE (validation)
2022-11-09 18:31:04,202:INFO: 		 ADE on hotel                     dataset:	 1.3383750915527344
2022-11-09 18:31:04,524:INFO: 		 ADE on univ                      dataset:	 1.5175281763076782
2022-11-09 18:31:04,804:INFO: 		 ADE on zara1                     dataset:	 2.1019539833068848
2022-11-09 18:31:05,176:INFO: 		 ADE on zara2                     dataset:	 1.4677042961120605
2022-11-09 18:31:05,176:INFO: Average validation:	ADE  1.5234	FDE  2.8253
2022-11-09 18:31:05,177:INFO: - Computing ADE (training)
2022-11-09 18:31:05,514:INFO: 		 ADE on hotel                     dataset:	 1.6748915910720825
2022-11-09 18:31:06,152:INFO: 		 ADE on univ                      dataset:	 1.4373328685760498
2022-11-09 18:31:06,611:INFO: 		 ADE on zara1                     dataset:	 2.0306053161621094
2022-11-09 18:31:07,321:INFO: 		 ADE on zara2                     dataset:	 1.62643301486969
2022-11-09 18:31:07,321:INFO: Average training:	ADE  1.5196	FDE  2.8133
2022-11-09 18:31:07,331:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_295.pth.tar
2022-11-09 18:31:07,331:INFO: 
===> EPOCH: 296 (P3)
2022-11-09 18:31:07,331:INFO: - Computing loss (training)
2022-11-09 18:31:08,242:INFO: Dataset: hotel               Batch: 1/4	Loss 86.6056 (86.6056)
2022-11-09 18:31:08,972:INFO: Dataset: hotel               Batch: 2/4	Loss 83.5334 (85.0264)
2022-11-09 18:31:09,687:INFO: Dataset: hotel               Batch: 3/4	Loss 82.0437 (84.0447)
2022-11-09 18:31:10,367:INFO: Dataset: hotel               Batch: 4/4	Loss 48.9875 (78.4947)
2022-11-09 18:31:11,389:INFO: Dataset: univ                Batch:  1/15	Loss 80.0797 (80.0797)
2022-11-09 18:31:12,132:INFO: Dataset: univ                Batch:  2/15	Loss 80.5995 (80.3375)
2022-11-09 18:31:12,872:INFO: Dataset: univ                Batch:  3/15	Loss 82.6831 (81.0644)
2022-11-09 18:31:13,626:INFO: Dataset: univ                Batch:  4/15	Loss 80.5431 (80.9276)
2022-11-09 18:31:14,369:INFO: Dataset: univ                Batch:  5/15	Loss 82.8454 (81.2775)
2022-11-09 18:31:15,119:INFO: Dataset: univ                Batch:  6/15	Loss 86.8371 (82.1341)
2022-11-09 18:31:15,872:INFO: Dataset: univ                Batch:  7/15	Loss 79.4725 (81.7170)
2022-11-09 18:31:16,623:INFO: Dataset: univ                Batch:  8/15	Loss 79.4648 (81.4093)
2022-11-09 18:31:17,363:INFO: Dataset: univ                Batch:  9/15	Loss 82.2421 (81.4960)
2022-11-09 18:31:18,116:INFO: Dataset: univ                Batch: 10/15	Loss 79.9713 (81.3353)
2022-11-09 18:31:18,864:INFO: Dataset: univ                Batch: 11/15	Loss 80.3182 (81.2418)
2022-11-09 18:31:19,614:INFO: Dataset: univ                Batch: 12/15	Loss 80.0760 (81.1450)
2022-11-09 18:31:20,361:INFO: Dataset: univ                Batch: 13/15	Loss 80.6800 (81.1087)
2022-11-09 18:31:21,102:INFO: Dataset: univ                Batch: 14/15	Loss 81.2472 (81.1178)
2022-11-09 18:31:21,832:INFO: Dataset: univ                Batch: 15/15	Loss 14.9307 (80.3148)
2022-11-09 18:31:22,840:INFO: Dataset: zara1               Batch: 1/8	Loss 87.8512 (87.8512)
2022-11-09 18:31:23,586:INFO: Dataset: zara1               Batch: 2/8	Loss 84.0147 (85.7896)
2022-11-09 18:31:24,320:INFO: Dataset: zara1               Batch: 3/8	Loss 84.4564 (85.3755)
2022-11-09 18:31:25,064:INFO: Dataset: zara1               Batch: 4/8	Loss 83.0531 (84.7634)
2022-11-09 18:31:25,799:INFO: Dataset: zara1               Batch: 5/8	Loss 84.5975 (84.7298)
2022-11-09 18:31:26,530:INFO: Dataset: zara1               Batch: 6/8	Loss 84.4620 (84.6845)
2022-11-09 18:31:27,261:INFO: Dataset: zara1               Batch: 7/8	Loss 84.7999 (84.7007)
2022-11-09 18:31:27,982:INFO: Dataset: zara1               Batch: 8/8	Loss 71.1219 (83.3499)
2022-11-09 18:31:29,001:INFO: Dataset: zara2               Batch:  1/18	Loss 86.7614 (86.7614)
2022-11-09 18:31:29,745:INFO: Dataset: zara2               Batch:  2/18	Loss 83.2540 (85.0520)
2022-11-09 18:31:30,493:INFO: Dataset: zara2               Batch:  3/18	Loss 83.0488 (84.4295)
2022-11-09 18:31:31,236:INFO: Dataset: zara2               Batch:  4/18	Loss 81.1345 (83.6028)
2022-11-09 18:31:31,988:INFO: Dataset: zara2               Batch:  5/18	Loss 80.7274 (83.0062)
2022-11-09 18:31:32,737:INFO: Dataset: zara2               Batch:  6/18	Loss 80.4043 (82.6193)
2022-11-09 18:31:33,479:INFO: Dataset: zara2               Batch:  7/18	Loss 81.4822 (82.4506)
2022-11-09 18:31:34,225:INFO: Dataset: zara2               Batch:  8/18	Loss 84.6447 (82.7158)
2022-11-09 18:31:34,971:INFO: Dataset: zara2               Batch:  9/18	Loss 81.3087 (82.5703)
2022-11-09 18:31:35,716:INFO: Dataset: zara2               Batch: 10/18	Loss 82.9416 (82.6076)
2022-11-09 18:31:36,466:INFO: Dataset: zara2               Batch: 11/18	Loss 80.7861 (82.4516)
2022-11-09 18:31:37,215:INFO: Dataset: zara2               Batch: 12/18	Loss 80.6441 (82.3107)
2022-11-09 18:31:38,037:INFO: Dataset: zara2               Batch: 13/18	Loss 81.6410 (82.2556)
2022-11-09 18:31:38,785:INFO: Dataset: zara2               Batch: 14/18	Loss 80.8397 (82.1579)
2022-11-09 18:31:39,532:INFO: Dataset: zara2               Batch: 15/18	Loss 80.6380 (82.0568)
2022-11-09 18:31:40,279:INFO: Dataset: zara2               Batch: 16/18	Loss 80.7159 (81.9585)
2022-11-09 18:31:41,027:INFO: Dataset: zara2               Batch: 17/18	Loss 80.6612 (81.8781)
2022-11-09 18:31:41,764:INFO: Dataset: zara2               Batch: 18/18	Loss 68.9035 (81.3247)
2022-11-09 18:31:41,829:INFO: - Computing ADE (validation o)
2022-11-09 18:31:42,109:INFO: 		 ADE on eth                       dataset:	 2.875311851501465
2022-11-09 18:31:42,109:INFO: Average validation o:	ADE  2.8753	FDE  4.8374
2022-11-09 18:31:42,110:INFO: - Computing ADE (validation)
2022-11-09 18:31:42,384:INFO: 		 ADE on hotel                     dataset:	 1.0468672513961792
2022-11-09 18:31:42,715:INFO: 		 ADE on univ                      dataset:	 1.323784589767456
2022-11-09 18:31:42,989:INFO: 		 ADE on zara1                     dataset:	 1.9570863246917725
2022-11-09 18:31:43,367:INFO: 		 ADE on zara2                     dataset:	 1.2582427263259888
2022-11-09 18:31:43,367:INFO: Average validation:	ADE  1.3214	FDE  2.4231
2022-11-09 18:31:43,368:INFO: - Computing ADE (training)
2022-11-09 18:31:43,699:INFO: 		 ADE on hotel                     dataset:	 1.3715718984603882
2022-11-09 18:31:44,340:INFO: 		 ADE on univ                      dataset:	 1.236822485923767
2022-11-09 18:31:44,771:INFO: 		 ADE on zara1                     dataset:	 2.0263290405273438
2022-11-09 18:31:45,458:INFO: 		 ADE on zara2                     dataset:	 1.4606420993804932
2022-11-09 18:31:45,458:INFO: Average training:	ADE  1.3360	FDE  2.4507
2022-11-09 18:31:45,468:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_296.pth.tar
2022-11-09 18:31:45,468:INFO: 
===> EPOCH: 297 (P3)
2022-11-09 18:31:45,468:INFO: - Computing loss (training)
2022-11-09 18:31:46,385:INFO: Dataset: hotel               Batch: 1/4	Loss 87.8016 (87.8016)
2022-11-09 18:31:47,116:INFO: Dataset: hotel               Batch: 2/4	Loss 82.2447 (84.9768)
2022-11-09 18:31:47,835:INFO: Dataset: hotel               Batch: 3/4	Loss 81.7110 (83.8795)
2022-11-09 18:31:48,513:INFO: Dataset: hotel               Batch: 4/4	Loss 49.4848 (78.1168)
2022-11-09 18:31:49,553:INFO: Dataset: univ                Batch:  1/15	Loss 80.5856 (80.5856)
2022-11-09 18:31:50,300:INFO: Dataset: univ                Batch:  2/15	Loss 80.1461 (80.3563)
2022-11-09 18:31:51,036:INFO: Dataset: univ                Batch:  3/15	Loss 80.3260 (80.3468)
2022-11-09 18:31:51,773:INFO: Dataset: univ                Batch:  4/15	Loss 80.9287 (80.4921)
2022-11-09 18:31:52,514:INFO: Dataset: univ                Batch:  5/15	Loss 81.5695 (80.7088)
2022-11-09 18:31:53,264:INFO: Dataset: univ                Batch:  6/15	Loss 80.0351 (80.5927)
2022-11-09 18:31:53,999:INFO: Dataset: univ                Batch:  7/15	Loss 80.9880 (80.6393)
2022-11-09 18:31:54,752:INFO: Dataset: univ                Batch:  8/15	Loss 79.7817 (80.5193)
2022-11-09 18:31:55,505:INFO: Dataset: univ                Batch:  9/15	Loss 79.9003 (80.4461)
2022-11-09 18:31:56,248:INFO: Dataset: univ                Batch: 10/15	Loss 80.5917 (80.4593)
2022-11-09 18:31:56,993:INFO: Dataset: univ                Batch: 11/15	Loss 80.2465 (80.4413)
2022-11-09 18:31:57,748:INFO: Dataset: univ                Batch: 12/15	Loss 80.2929 (80.4283)
2022-11-09 18:31:58,497:INFO: Dataset: univ                Batch: 13/15	Loss 79.5903 (80.3611)
2022-11-09 18:31:59,329:INFO: Dataset: univ                Batch: 14/15	Loss 79.7477 (80.3136)
2022-11-09 18:31:59,988:INFO: Dataset: univ                Batch: 15/15	Loss 14.8118 (79.3079)
2022-11-09 18:32:01,008:INFO: Dataset: zara1               Batch: 1/8	Loss 85.1465 (85.1465)
2022-11-09 18:32:01,756:INFO: Dataset: zara1               Batch: 2/8	Loss 86.4123 (85.7720)
2022-11-09 18:32:02,500:INFO: Dataset: zara1               Batch: 3/8	Loss 84.8512 (85.4789)
2022-11-09 18:32:03,242:INFO: Dataset: zara1               Batch: 4/8	Loss 85.6389 (85.5161)
2022-11-09 18:32:03,986:INFO: Dataset: zara1               Batch: 5/8	Loss 83.6920 (85.1400)
2022-11-09 18:32:04,729:INFO: Dataset: zara1               Batch: 6/8	Loss 84.1285 (84.9688)
2022-11-09 18:32:05,472:INFO: Dataset: zara1               Batch: 7/8	Loss 83.1969 (84.7264)
2022-11-09 18:32:06,208:INFO: Dataset: zara1               Batch: 8/8	Loss 73.4759 (83.6487)
2022-11-09 18:32:07,220:INFO: Dataset: zara2               Batch:  1/18	Loss 88.8734 (88.8734)
2022-11-09 18:32:07,970:INFO: Dataset: zara2               Batch:  2/18	Loss 84.5525 (86.8137)
2022-11-09 18:32:08,719:INFO: Dataset: zara2               Batch:  3/18	Loss 83.6648 (85.8579)
2022-11-09 18:32:09,462:INFO: Dataset: zara2               Batch:  4/18	Loss 82.6087 (85.0259)
2022-11-09 18:32:10,210:INFO: Dataset: zara2               Batch:  5/18	Loss 80.4249 (84.1057)
2022-11-09 18:32:10,959:INFO: Dataset: zara2               Batch:  6/18	Loss 80.8767 (83.5894)
2022-11-09 18:32:11,703:INFO: Dataset: zara2               Batch:  7/18	Loss 80.9395 (83.1984)
2022-11-09 18:32:12,447:INFO: Dataset: zara2               Batch:  8/18	Loss 79.8504 (82.7657)
2022-11-09 18:32:13,193:INFO: Dataset: zara2               Batch:  9/18	Loss 80.7759 (82.5540)
2022-11-09 18:32:13,929:INFO: Dataset: zara2               Batch: 10/18	Loss 80.1259 (82.2921)
2022-11-09 18:32:14,673:INFO: Dataset: zara2               Batch: 11/18	Loss 80.3404 (82.1185)
2022-11-09 18:32:15,494:INFO: Dataset: zara2               Batch: 12/18	Loss 79.6885 (81.9189)
2022-11-09 18:32:16,241:INFO: Dataset: zara2               Batch: 13/18	Loss 80.3682 (81.7822)
2022-11-09 18:32:16,986:INFO: Dataset: zara2               Batch: 14/18	Loss 80.5711 (81.6935)
2022-11-09 18:32:17,733:INFO: Dataset: zara2               Batch: 15/18	Loss 81.6741 (81.6922)
2022-11-09 18:32:18,480:INFO: Dataset: zara2               Batch: 16/18	Loss 81.8997 (81.7057)
2022-11-09 18:32:19,229:INFO: Dataset: zara2               Batch: 17/18	Loss 79.6541 (81.5845)
2022-11-09 18:32:19,965:INFO: Dataset: zara2               Batch: 18/18	Loss 68.6271 (80.9183)
2022-11-09 18:32:20,030:INFO: - Computing ADE (validation o)
2022-11-09 18:32:20,313:INFO: 		 ADE on eth                       dataset:	 2.647005796432495
2022-11-09 18:32:20,314:INFO: Average validation o:	ADE  2.6470	FDE  4.3355
2022-11-09 18:32:20,314:INFO: - Computing ADE (validation)
2022-11-09 18:32:20,590:INFO: 		 ADE on hotel                     dataset:	 1.1394803524017334
2022-11-09 18:32:20,914:INFO: 		 ADE on univ                      dataset:	 1.3572763204574585
2022-11-09 18:32:21,186:INFO: 		 ADE on zara1                     dataset:	 2.0434939861297607
2022-11-09 18:32:21,567:INFO: 		 ADE on zara2                     dataset:	 1.2843677997589111
2022-11-09 18:32:21,568:INFO: Average validation:	ADE  1.3585	FDE  2.5209
2022-11-09 18:32:21,568:INFO: - Computing ADE (training)
2022-11-09 18:32:21,906:INFO: 		 ADE on hotel                     dataset:	 1.4476559162139893
2022-11-09 18:32:22,533:INFO: 		 ADE on univ                      dataset:	 1.2682952880859375
2022-11-09 18:32:22,974:INFO: 		 ADE on zara1                     dataset:	 1.947152018547058
2022-11-09 18:32:23,669:INFO: 		 ADE on zara2                     dataset:	 1.4499741792678833
2022-11-09 18:32:23,669:INFO: Average training:	ADE  1.3530	FDE  2.5065
2022-11-09 18:32:23,679:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_297.pth.tar
2022-11-09 18:32:23,679:INFO: 
===> EPOCH: 298 (P3)
2022-11-09 18:32:23,679:INFO: - Computing loss (training)
2022-11-09 18:32:24,596:INFO: Dataset: hotel               Batch: 1/4	Loss 81.4497 (81.4497)
2022-11-09 18:32:25,324:INFO: Dataset: hotel               Batch: 2/4	Loss 81.2300 (81.3427)
2022-11-09 18:32:26,041:INFO: Dataset: hotel               Batch: 3/4	Loss 81.3978 (81.3609)
2022-11-09 18:32:26,724:INFO: Dataset: hotel               Batch: 4/4	Loss 49.5419 (75.7779)
2022-11-09 18:32:27,754:INFO: Dataset: univ                Batch:  1/15	Loss 80.7269 (80.7269)
2022-11-09 18:32:28,508:INFO: Dataset: univ                Batch:  2/15	Loss 81.3726 (81.0748)
2022-11-09 18:32:29,250:INFO: Dataset: univ                Batch:  3/15	Loss 80.3364 (80.8306)
2022-11-09 18:32:30,001:INFO: Dataset: univ                Batch:  4/15	Loss 80.0648 (80.6365)
2022-11-09 18:32:30,743:INFO: Dataset: univ                Batch:  5/15	Loss 80.2131 (80.5575)
2022-11-09 18:32:31,502:INFO: Dataset: univ                Batch:  6/15	Loss 79.5734 (80.3777)
2022-11-09 18:32:32,243:INFO: Dataset: univ                Batch:  7/15	Loss 81.0127 (80.4628)
2022-11-09 18:32:32,996:INFO: Dataset: univ                Batch:  8/15	Loss 79.7381 (80.3665)
2022-11-09 18:32:33,744:INFO: Dataset: univ                Batch:  9/15	Loss 79.2934 (80.2468)
2022-11-09 18:32:34,491:INFO: Dataset: univ                Batch: 10/15	Loss 79.4920 (80.1761)
2022-11-09 18:32:35,239:INFO: Dataset: univ                Batch: 11/15	Loss 79.7050 (80.1350)
2022-11-09 18:32:35,986:INFO: Dataset: univ                Batch: 12/15	Loss 80.0313 (80.1261)
2022-11-09 18:32:36,731:INFO: Dataset: univ                Batch: 13/15	Loss 79.2910 (80.0645)
2022-11-09 18:32:37,552:INFO: Dataset: univ                Batch: 14/15	Loss 80.6230 (80.1020)
2022-11-09 18:32:38,208:INFO: Dataset: univ                Batch: 15/15	Loss 16.7338 (79.6185)
2022-11-09 18:32:39,228:INFO: Dataset: zara1               Batch: 1/8	Loss 91.5641 (91.5641)
2022-11-09 18:32:39,969:INFO: Dataset: zara1               Batch: 2/8	Loss 85.7823 (88.7558)
2022-11-09 18:32:40,711:INFO: Dataset: zara1               Batch: 3/8	Loss 89.0729 (88.8686)
2022-11-09 18:32:41,449:INFO: Dataset: zara1               Batch: 4/8	Loss 84.3892 (87.7607)
2022-11-09 18:32:42,187:INFO: Dataset: zara1               Batch: 5/8	Loss 81.8370 (86.5141)
2022-11-09 18:32:42,925:INFO: Dataset: zara1               Batch: 6/8	Loss 82.0284 (85.7602)
2022-11-09 18:32:43,666:INFO: Dataset: zara1               Batch: 7/8	Loss 83.4114 (85.4103)
2022-11-09 18:32:44,393:INFO: Dataset: zara1               Batch: 8/8	Loss 71.2879 (83.7602)
2022-11-09 18:32:45,407:INFO: Dataset: zara2               Batch:  1/18	Loss 83.1542 (83.1542)
2022-11-09 18:32:46,156:INFO: Dataset: zara2               Batch:  2/18	Loss 84.3155 (83.6702)
2022-11-09 18:32:46,899:INFO: Dataset: zara2               Batch:  3/18	Loss 85.3666 (84.2362)
2022-11-09 18:32:47,646:INFO: Dataset: zara2               Batch:  4/18	Loss 82.8623 (83.8894)
2022-11-09 18:32:48,387:INFO: Dataset: zara2               Batch:  5/18	Loss 82.4732 (83.5895)
2022-11-09 18:32:49,135:INFO: Dataset: zara2               Batch:  6/18	Loss 83.7485 (83.6126)
2022-11-09 18:32:49,873:INFO: Dataset: zara2               Batch:  7/18	Loss 80.4061 (83.1291)
2022-11-09 18:32:50,614:INFO: Dataset: zara2               Batch:  8/18	Loss 80.5351 (82.7925)
2022-11-09 18:32:51,354:INFO: Dataset: zara2               Batch:  9/18	Loss 79.6167 (82.4214)
2022-11-09 18:32:52,098:INFO: Dataset: zara2               Batch: 10/18	Loss 79.3225 (82.1175)
2022-11-09 18:32:52,842:INFO: Dataset: zara2               Batch: 11/18	Loss 81.0797 (82.0223)
2022-11-09 18:32:53,668:INFO: Dataset: zara2               Batch: 12/18	Loss 80.0382 (81.8677)
2022-11-09 18:32:54,414:INFO: Dataset: zara2               Batch: 13/18	Loss 79.0620 (81.6595)
2022-11-09 18:32:55,157:INFO: Dataset: zara2               Batch: 14/18	Loss 80.0185 (81.5355)
2022-11-09 18:32:55,904:INFO: Dataset: zara2               Batch: 15/18	Loss 80.3436 (81.4632)
2022-11-09 18:32:56,647:INFO: Dataset: zara2               Batch: 16/18	Loss 81.3862 (81.4581)
2022-11-09 18:32:57,391:INFO: Dataset: zara2               Batch: 17/18	Loss 82.4168 (81.5143)
2022-11-09 18:32:58,125:INFO: Dataset: zara2               Batch: 18/18	Loss 68.9023 (80.8950)
2022-11-09 18:32:58,192:INFO: - Computing ADE (validation o)
2022-11-09 18:32:58,469:INFO: 		 ADE on eth                       dataset:	 2.5440196990966797
2022-11-09 18:32:58,469:INFO: Average validation o:	ADE  2.5440	FDE  4.1196
2022-11-09 18:32:58,470:INFO: - Computing ADE (validation)
2022-11-09 18:32:58,737:INFO: 		 ADE on hotel                     dataset:	 1.1983544826507568
2022-11-09 18:32:59,051:INFO: 		 ADE on univ                      dataset:	 1.3498865365982056
2022-11-09 18:32:59,320:INFO: 		 ADE on zara1                     dataset:	 1.9981049299240112
2022-11-09 18:32:59,695:INFO: 		 ADE on zara2                     dataset:	 1.277139663696289
2022-11-09 18:32:59,695:INFO: Average validation:	ADE  1.3526	FDE  2.5413
2022-11-09 18:32:59,696:INFO: - Computing ADE (training)
2022-11-09 18:33:00,028:INFO: 		 ADE on hotel                     dataset:	 1.4979709386825562
2022-11-09 18:33:00,664:INFO: 		 ADE on univ                      dataset:	 1.2789894342422485
2022-11-09 18:33:01,102:INFO: 		 ADE on zara1                     dataset:	 1.887855887413025
2022-11-09 18:33:01,795:INFO: 		 ADE on zara2                     dataset:	 1.4160704612731934
2022-11-09 18:33:01,795:INFO: Average training:	ADE  1.3512	FDE  2.5330
2022-11-09 18:33:01,805:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_298.pth.tar
2022-11-09 18:33:01,805:INFO: 
===> EPOCH: 299 (P3)
2022-11-09 18:33:01,805:INFO: - Computing loss (training)
2022-11-09 18:33:02,739:INFO: Dataset: hotel               Batch: 1/4	Loss 82.4271 (82.4271)
2022-11-09 18:33:03,468:INFO: Dataset: hotel               Batch: 2/4	Loss 82.1289 (82.2769)
2022-11-09 18:33:04,182:INFO: Dataset: hotel               Batch: 3/4	Loss 82.7227 (82.4323)
2022-11-09 18:33:04,859:INFO: Dataset: hotel               Batch: 4/4	Loss 50.0439 (77.2621)
2022-11-09 18:33:05,873:INFO: Dataset: univ                Batch:  1/15	Loss 81.2357 (81.2357)
2022-11-09 18:33:06,610:INFO: Dataset: univ                Batch:  2/15	Loss 80.4136 (80.8572)
2022-11-09 18:33:07,361:INFO: Dataset: univ                Batch:  3/15	Loss 79.8837 (80.5034)
2022-11-09 18:33:08,102:INFO: Dataset: univ                Batch:  4/15	Loss 79.9237 (80.3712)
2022-11-09 18:33:08,854:INFO: Dataset: univ                Batch:  5/15	Loss 80.2747 (80.3512)
2022-11-09 18:33:09,596:INFO: Dataset: univ                Batch:  6/15	Loss 81.4937 (80.5325)
2022-11-09 18:33:10,344:INFO: Dataset: univ                Batch:  7/15	Loss 80.2458 (80.4917)
2022-11-09 18:33:11,080:INFO: Dataset: univ                Batch:  8/15	Loss 79.7139 (80.3995)
2022-11-09 18:33:11,821:INFO: Dataset: univ                Batch:  9/15	Loss 81.6583 (80.5292)
2022-11-09 18:33:12,574:INFO: Dataset: univ                Batch: 10/15	Loss 78.9635 (80.3602)
2022-11-09 18:33:13,330:INFO: Dataset: univ                Batch: 11/15	Loss 79.1465 (80.2432)
2022-11-09 18:33:14,079:INFO: Dataset: univ                Batch: 12/15	Loss 80.6390 (80.2762)
2022-11-09 18:33:14,911:INFO: Dataset: univ                Batch: 13/15	Loss 79.4811 (80.2096)
2022-11-09 18:33:15,673:INFO: Dataset: univ                Batch: 14/15	Loss 78.9522 (80.1102)
2022-11-09 18:33:16,325:INFO: Dataset: univ                Batch: 15/15	Loss 15.4109 (79.3621)
2022-11-09 18:33:17,336:INFO: Dataset: zara1               Batch: 1/8	Loss 91.6267 (91.6267)
2022-11-09 18:33:18,083:INFO: Dataset: zara1               Batch: 2/8	Loss 83.6773 (87.1660)
2022-11-09 18:33:18,827:INFO: Dataset: zara1               Batch: 3/8	Loss 82.2687 (85.4638)
2022-11-09 18:33:19,569:INFO: Dataset: zara1               Batch: 4/8	Loss 82.5667 (84.6721)
2022-11-09 18:33:20,306:INFO: Dataset: zara1               Batch: 5/8	Loss 82.3200 (84.1978)
2022-11-09 18:33:21,047:INFO: Dataset: zara1               Batch: 6/8	Loss 83.8565 (84.1413)
2022-11-09 18:33:21,787:INFO: Dataset: zara1               Batch: 7/8	Loss 84.0847 (84.1331)
2022-11-09 18:33:22,513:INFO: Dataset: zara1               Batch: 8/8	Loss 71.5393 (82.7677)
2022-11-09 18:33:23,536:INFO: Dataset: zara2               Batch:  1/18	Loss 83.1821 (83.1821)
2022-11-09 18:33:24,288:INFO: Dataset: zara2               Batch:  2/18	Loss 83.9760 (83.5888)
2022-11-09 18:33:25,072:INFO: Dataset: zara2               Batch:  3/18	Loss 81.9005 (83.0718)
2022-11-09 18:33:25,858:INFO: Dataset: zara2               Batch:  4/18	Loss 79.8699 (82.2587)
2022-11-09 18:33:26,619:INFO: Dataset: zara2               Batch:  5/18	Loss 80.9088 (82.0070)
2022-11-09 18:33:27,379:INFO: Dataset: zara2               Batch:  6/18	Loss 80.4051 (81.7435)
2022-11-09 18:33:28,142:INFO: Dataset: zara2               Batch:  7/18	Loss 80.4974 (81.5569)
2022-11-09 18:33:28,896:INFO: Dataset: zara2               Batch:  8/18	Loss 79.6128 (81.3126)
2022-11-09 18:33:29,643:INFO: Dataset: zara2               Batch:  9/18	Loss 80.4821 (81.2219)
2022-11-09 18:33:30,393:INFO: Dataset: zara2               Batch: 10/18	Loss 79.3689 (81.0198)
2022-11-09 18:33:31,145:INFO: Dataset: zara2               Batch: 11/18	Loss 79.5717 (80.8946)
2022-11-09 18:33:31,893:INFO: Dataset: zara2               Batch: 12/18	Loss 80.0305 (80.8162)
2022-11-09 18:33:32,725:INFO: Dataset: zara2               Batch: 13/18	Loss 79.7064 (80.7212)
2022-11-09 18:33:33,475:INFO: Dataset: zara2               Batch: 14/18	Loss 79.7267 (80.6490)
2022-11-09 18:33:34,234:INFO: Dataset: zara2               Batch: 15/18	Loss 81.1100 (80.6775)
2022-11-09 18:33:34,988:INFO: Dataset: zara2               Batch: 16/18	Loss 79.2131 (80.5705)
2022-11-09 18:33:35,740:INFO: Dataset: zara2               Batch: 17/18	Loss 85.7000 (80.8709)
2022-11-09 18:33:36,485:INFO: Dataset: zara2               Batch: 18/18	Loss 68.4201 (80.3007)
2022-11-09 18:33:36,554:INFO: - Computing ADE (validation o)
2022-11-09 18:33:36,848:INFO: 		 ADE on eth                       dataset:	 2.6835274696350098
2022-11-09 18:33:36,849:INFO: Average validation o:	ADE  2.6835	FDE  4.4356
2022-11-09 18:33:36,849:INFO: - Computing ADE (validation)
2022-11-09 18:33:37,119:INFO: 		 ADE on hotel                     dataset:	 1.2745065689086914
2022-11-09 18:33:37,447:INFO: 		 ADE on univ                      dataset:	 1.358644962310791
2022-11-09 18:33:37,725:INFO: 		 ADE on zara1                     dataset:	 1.8410687446594238
2022-11-09 18:33:38,098:INFO: 		 ADE on zara2                     dataset:	 1.3005083799362183
2022-11-09 18:33:38,098:INFO: Average validation:	ADE  1.3608	FDE  2.5063
2022-11-09 18:33:38,099:INFO: - Computing ADE (training)
2022-11-09 18:33:38,435:INFO: 		 ADE on hotel                     dataset:	 1.5506727695465088
2022-11-09 18:33:39,071:INFO: 		 ADE on univ                      dataset:	 1.3100862503051758
2022-11-09 18:33:39,525:INFO: 		 ADE on zara1                     dataset:	 1.8882287740707397
2022-11-09 18:33:40,272:INFO: 		 ADE on zara2                     dataset:	 1.4559251070022583
2022-11-09 18:33:40,272:INFO: Average training:	ADE  1.3826	FDE  2.5469
2022-11-09 18:33:40,283:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_299.pth.tar
2022-11-09 18:33:40,283:INFO: 
===> EPOCH: 300 (P3)
2022-11-09 18:33:40,284:INFO: - Computing loss (training)
2022-11-09 18:33:41,310:INFO: Dataset: hotel               Batch: 1/4	Loss 82.0676 (82.0676)
2022-11-09 18:33:42,067:INFO: Dataset: hotel               Batch: 2/4	Loss 81.7333 (81.8912)
2022-11-09 18:33:42,811:INFO: Dataset: hotel               Batch: 3/4	Loss 81.6051 (81.7944)
2022-11-09 18:33:43,539:INFO: Dataset: hotel               Batch: 4/4	Loss 48.8050 (75.9625)
2022-11-09 18:33:44,605:INFO: Dataset: univ                Batch:  1/15	Loss 83.6967 (83.6967)
2022-11-09 18:33:45,372:INFO: Dataset: univ                Batch:  2/15	Loss 82.7657 (83.2038)
2022-11-09 18:33:46,131:INFO: Dataset: univ                Batch:  3/15	Loss 79.5609 (81.8181)
2022-11-09 18:33:46,886:INFO: Dataset: univ                Batch:  4/15	Loss 82.3638 (81.9566)
2022-11-09 18:33:47,670:INFO: Dataset: univ                Batch:  5/15	Loss 81.0616 (81.7658)
2022-11-09 18:33:48,496:INFO: Dataset: univ                Batch:  6/15	Loss 83.1944 (82.0055)
2022-11-09 18:33:49,274:INFO: Dataset: univ                Batch:  7/15	Loss 82.1737 (82.0282)
2022-11-09 18:33:50,048:INFO: Dataset: univ                Batch:  8/15	Loss 79.1642 (81.6566)
2022-11-09 18:33:50,803:INFO: Dataset: univ                Batch:  9/15	Loss 79.9099 (81.4754)
2022-11-09 18:33:51,643:INFO: Dataset: univ                Batch: 10/15	Loss 81.6123 (81.4881)
2022-11-09 18:33:52,424:INFO: Dataset: univ                Batch: 11/15	Loss 79.2382 (81.2551)
2022-11-09 18:33:53,290:INFO: Dataset: univ                Batch: 12/15	Loss 79.6889 (81.1073)
2022-11-09 18:33:54,112:INFO: Dataset: univ                Batch: 13/15	Loss 80.0429 (81.0267)
2022-11-09 18:33:54,945:INFO: Dataset: univ                Batch: 14/15	Loss 81.6148 (81.0709)
2022-11-09 18:33:55,669:INFO: Dataset: univ                Batch: 15/15	Loss 14.8856 (80.1205)
2022-11-09 18:33:56,695:INFO: Dataset: zara1               Batch: 1/8	Loss 93.8192 (93.8192)
2022-11-09 18:33:57,474:INFO: Dataset: zara1               Batch: 2/8	Loss 82.3501 (87.8765)
2022-11-09 18:33:58,225:INFO: Dataset: zara1               Batch: 3/8	Loss 84.8627 (86.8479)
2022-11-09 18:33:58,976:INFO: Dataset: zara1               Batch: 4/8	Loss 82.6889 (85.7622)
2022-11-09 18:33:59,714:INFO: Dataset: zara1               Batch: 5/8	Loss 82.1464 (85.0940)
2022-11-09 18:34:00,458:INFO: Dataset: zara1               Batch: 6/8	Loss 83.4633 (84.8378)
2022-11-09 18:34:01,201:INFO: Dataset: zara1               Batch: 7/8	Loss 82.7005 (84.5444)
2022-11-09 18:34:01,928:INFO: Dataset: zara1               Batch: 8/8	Loss 71.4749 (83.2993)
2022-11-09 18:34:02,937:INFO: Dataset: zara2               Batch:  1/18	Loss 88.9765 (88.9765)
2022-11-09 18:34:03,678:INFO: Dataset: zara2               Batch:  2/18	Loss 94.3453 (91.6953)
2022-11-09 18:34:04,424:INFO: Dataset: zara2               Batch:  3/18	Loss 84.3832 (89.3354)
2022-11-09 18:34:05,165:INFO: Dataset: zara2               Batch:  4/18	Loss 79.8053 (86.9271)
2022-11-09 18:34:05,907:INFO: Dataset: zara2               Batch:  5/18	Loss 79.6686 (85.5958)
2022-11-09 18:34:06,656:INFO: Dataset: zara2               Batch:  6/18	Loss 80.4513 (84.7876)
2022-11-09 18:34:07,401:INFO: Dataset: zara2               Batch:  7/18	Loss 80.3861 (84.1901)
2022-11-09 18:34:08,172:INFO: Dataset: zara2               Batch:  8/18	Loss 80.4439 (83.6837)
2022-11-09 18:34:08,975:INFO: Dataset: zara2               Batch:  9/18	Loss 80.1514 (83.2855)
2022-11-09 18:34:09,793:INFO: Dataset: zara2               Batch: 10/18	Loss 81.8059 (83.1286)
2022-11-09 18:34:10,627:INFO: Dataset: zara2               Batch: 11/18	Loss 79.7335 (82.8444)
2022-11-09 18:34:11,463:INFO: Dataset: zara2               Batch: 12/18	Loss 80.0702 (82.6218)
2022-11-09 18:34:12,222:INFO: Dataset: zara2               Batch: 13/18	Loss 79.5448 (82.3855)
2022-11-09 18:34:12,976:INFO: Dataset: zara2               Batch: 14/18	Loss 80.7671 (82.2794)
2022-11-09 18:34:13,890:INFO: Dataset: zara2               Batch: 15/18	Loss 80.2037 (82.1366)
2022-11-09 18:34:14,912:INFO: Dataset: zara2               Batch: 16/18	Loss 80.0551 (81.9997)
2022-11-09 18:34:15,686:INFO: Dataset: zara2               Batch: 17/18	Loss 81.4022 (81.9621)
2022-11-09 18:34:16,439:INFO: Dataset: zara2               Batch: 18/18	Loss 68.6500 (81.2887)
2022-11-09 18:34:16,506:INFO: - Computing ADE (validation o)
2022-11-09 18:34:16,798:INFO: 		 ADE on eth                       dataset:	 2.732822895050049
2022-11-09 18:34:16,798:INFO: Average validation o:	ADE  2.7328	FDE  4.5889
2022-11-09 18:34:16,799:INFO: - Computing ADE (validation)
2022-11-09 18:34:17,082:INFO: 		 ADE on hotel                     dataset:	 1.2588481903076172
2022-11-09 18:34:17,399:INFO: 		 ADE on univ                      dataset:	 1.3529366254806519
2022-11-09 18:34:17,666:INFO: 		 ADE on zara1                     dataset:	 1.6412639617919922
2022-11-09 18:34:18,034:INFO: 		 ADE on zara2                     dataset:	 1.2634488344192505
2022-11-09 18:34:18,034:INFO: Average validation:	ADE  1.3317	FDE  2.4677
2022-11-09 18:34:18,035:INFO: - Computing ADE (training)
2022-11-09 18:34:18,370:INFO: 		 ADE on hotel                     dataset:	 1.5217598676681519
2022-11-09 18:34:19,010:INFO: 		 ADE on univ                      dataset:	 1.2628079652786255
2022-11-09 18:34:19,449:INFO: 		 ADE on zara1                     dataset:	 1.867656946182251
2022-11-09 18:34:20,205:INFO: 		 ADE on zara2                     dataset:	 1.4472196102142334
2022-11-09 18:34:20,205:INFO: Average training:	ADE  1.3454	FDE  2.4928
2022-11-09 18:34:20,216:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_300.pth.tar
2022-11-09 18:34:20,216:INFO: 
===> EPOCH: 301 (P3)
2022-11-09 18:34:20,216:INFO: - Computing loss (training)
2022-11-09 18:34:21,150:INFO: Dataset: hotel               Batch: 1/4	Loss 83.7899 (83.7899)
2022-11-09 18:34:21,906:INFO: Dataset: hotel               Batch: 2/4	Loss 83.0417 (83.4209)
2022-11-09 18:34:22,636:INFO: Dataset: hotel               Batch: 3/4	Loss 81.7068 (82.9094)
2022-11-09 18:34:23,327:INFO: Dataset: hotel               Batch: 4/4	Loss 52.1249 (77.7109)
2022-11-09 18:34:24,353:INFO: Dataset: univ                Batch:  1/15	Loss 79.4892 (79.4892)
2022-11-09 18:34:25,119:INFO: Dataset: univ                Batch:  2/15	Loss 80.3542 (79.9335)
2022-11-09 18:34:26,025:INFO: Dataset: univ                Batch:  3/15	Loss 78.4013 (79.3556)
2022-11-09 18:34:26,792:INFO: Dataset: univ                Batch:  4/15	Loss 80.2860 (79.5748)
2022-11-09 18:34:27,526:INFO: Dataset: univ                Batch:  5/15	Loss 82.3359 (80.0527)
2022-11-09 18:34:28,276:INFO: Dataset: univ                Batch:  6/15	Loss 80.9631 (80.2070)
2022-11-09 18:34:29,083:INFO: Dataset: univ                Batch:  7/15	Loss 80.4889 (80.2468)
2022-11-09 18:34:29,886:INFO: Dataset: univ                Batch:  8/15	Loss 80.2094 (80.2422)
2022-11-09 18:34:30,744:INFO: Dataset: univ                Batch:  9/15	Loss 79.4235 (80.1575)
2022-11-09 18:34:31,555:INFO: Dataset: univ                Batch: 10/15	Loss 81.8524 (80.3348)
2022-11-09 18:34:32,354:INFO: Dataset: univ                Batch: 11/15	Loss 79.1087 (80.2141)
2022-11-09 18:34:33,107:INFO: Dataset: univ                Batch: 12/15	Loss 80.5323 (80.2379)
2022-11-09 18:34:33,942:INFO: Dataset: univ                Batch: 13/15	Loss 80.7599 (80.2783)
2022-11-09 18:34:34,692:INFO: Dataset: univ                Batch: 14/15	Loss 80.8868 (80.3220)
2022-11-09 18:34:35,356:INFO: Dataset: univ                Batch: 15/15	Loss 14.9182 (79.5068)
2022-11-09 18:34:36,391:INFO: Dataset: zara1               Batch: 1/8	Loss 86.6652 (86.6652)
2022-11-09 18:34:37,132:INFO: Dataset: zara1               Batch: 2/8	Loss 86.6502 (86.6577)
2022-11-09 18:34:37,895:INFO: Dataset: zara1               Batch: 3/8	Loss 83.6957 (85.7173)
2022-11-09 18:34:38,659:INFO: Dataset: zara1               Batch: 4/8	Loss 81.8025 (84.6733)
2022-11-09 18:34:39,410:INFO: Dataset: zara1               Batch: 5/8	Loss 82.0596 (84.1593)
2022-11-09 18:34:40,148:INFO: Dataset: zara1               Batch: 6/8	Loss 83.2311 (84.0141)
2022-11-09 18:34:40,896:INFO: Dataset: zara1               Batch: 7/8	Loss 82.5039 (83.8038)
2022-11-09 18:34:41,628:INFO: Dataset: zara1               Batch: 8/8	Loss 71.4088 (82.6491)
2022-11-09 18:34:42,720:INFO: Dataset: zara2               Batch:  1/18	Loss 92.1173 (92.1173)
2022-11-09 18:34:43,542:INFO: Dataset: zara2               Batch:  2/18	Loss 87.7263 (89.8518)
2022-11-09 18:34:44,358:INFO: Dataset: zara2               Batch:  3/18	Loss 86.5135 (88.7433)
2022-11-09 18:34:45,166:INFO: Dataset: zara2               Batch:  4/18	Loss 81.7124 (87.0765)
2022-11-09 18:34:45,972:INFO: Dataset: zara2               Batch:  5/18	Loss 80.4646 (85.6605)
2022-11-09 18:34:46,789:INFO: Dataset: zara2               Batch:  6/18	Loss 80.7165 (84.9377)
2022-11-09 18:34:47,589:INFO: Dataset: zara2               Batch:  7/18	Loss 81.5487 (84.4565)
2022-11-09 18:34:48,475:INFO: Dataset: zara2               Batch:  8/18	Loss 81.1136 (84.0288)
2022-11-09 18:34:49,344:INFO: Dataset: zara2               Batch:  9/18	Loss 84.4122 (84.0705)
2022-11-09 18:34:50,194:INFO: Dataset: zara2               Batch: 10/18	Loss 80.3779 (83.6789)
2022-11-09 18:34:51,050:INFO: Dataset: zara2               Batch: 11/18	Loss 82.8641 (83.6071)
2022-11-09 18:34:51,885:INFO: Dataset: zara2               Batch: 12/18	Loss 81.6445 (83.4319)
2022-11-09 18:34:52,783:INFO: Dataset: zara2               Batch: 13/18	Loss 81.0812 (83.2523)
2022-11-09 18:34:53,541:INFO: Dataset: zara2               Batch: 14/18	Loss 79.6279 (82.9931)
2022-11-09 18:34:54,294:INFO: Dataset: zara2               Batch: 15/18	Loss 79.7364 (82.7810)
2022-11-09 18:34:55,045:INFO: Dataset: zara2               Batch: 16/18	Loss 80.5747 (82.6397)
2022-11-09 18:34:55,806:INFO: Dataset: zara2               Batch: 17/18	Loss 80.5515 (82.5187)
2022-11-09 18:34:56,547:INFO: Dataset: zara2               Batch: 18/18	Loss 70.6351 (81.9628)
2022-11-09 18:34:56,614:INFO: - Computing ADE (validation o)
2022-11-09 18:34:56,891:INFO: 		 ADE on eth                       dataset:	 2.660277843475342
2022-11-09 18:34:56,891:INFO: Average validation o:	ADE  2.6603	FDE  4.3991
2022-11-09 18:34:56,892:INFO: - Computing ADE (validation)
2022-11-09 18:34:57,163:INFO: 		 ADE on hotel                     dataset:	 1.2462303638458252
2022-11-09 18:34:57,492:INFO: 		 ADE on univ                      dataset:	 1.42232346534729
2022-11-09 18:34:57,765:INFO: 		 ADE on zara1                     dataset:	 2.134748697280884
2022-11-09 18:34:58,134:INFO: 		 ADE on zara2                     dataset:	 1.3838293552398682
2022-11-09 18:34:58,134:INFO: Average validation:	ADE  1.4400	FDE  2.6990
2022-11-09 18:34:58,135:INFO: - Computing ADE (training)
2022-11-09 18:34:58,475:INFO: 		 ADE on hotel                     dataset:	 1.5222065448760986
2022-11-09 18:34:59,096:INFO: 		 ADE on univ                      dataset:	 1.3546080589294434
2022-11-09 18:34:59,543:INFO: 		 ADE on zara1                     dataset:	 1.973301649093628
2022-11-09 18:35:00,268:INFO: 		 ADE on zara2                     dataset:	 1.504607915878296
2022-11-09 18:35:00,269:INFO: Average training:	ADE  1.4287	FDE  2.6705
2022-11-09 18:35:00,279:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_301.pth.tar
2022-11-09 18:35:00,279:INFO: 
===> EPOCH: 302 (P3)
2022-11-09 18:35:00,279:INFO: - Computing loss (training)
2022-11-09 18:35:01,195:INFO: Dataset: hotel               Batch: 1/4	Loss 82.9816 (82.9816)
2022-11-09 18:35:01,927:INFO: Dataset: hotel               Batch: 2/4	Loss 82.8890 (82.9376)
2022-11-09 18:35:02,658:INFO: Dataset: hotel               Batch: 3/4	Loss 82.2769 (82.7075)
2022-11-09 18:35:03,359:INFO: Dataset: hotel               Batch: 4/4	Loss 49.8208 (77.1107)
2022-11-09 18:35:04,405:INFO: Dataset: univ                Batch:  1/15	Loss 81.1151 (81.1151)
2022-11-09 18:35:05,173:INFO: Dataset: univ                Batch:  2/15	Loss 80.8481 (80.9708)
2022-11-09 18:35:05,929:INFO: Dataset: univ                Batch:  3/15	Loss 80.3193 (80.7430)
2022-11-09 18:35:06,680:INFO: Dataset: univ                Batch:  4/15	Loss 79.7549 (80.5094)
2022-11-09 18:35:07,431:INFO: Dataset: univ                Batch:  5/15	Loss 80.3500 (80.4799)
2022-11-09 18:35:08,181:INFO: Dataset: univ                Batch:  6/15	Loss 81.1407 (80.5838)
2022-11-09 18:35:08,937:INFO: Dataset: univ                Batch:  7/15	Loss 81.1670 (80.6659)
2022-11-09 18:35:09,701:INFO: Dataset: univ                Batch:  8/15	Loss 80.9177 (80.6956)
2022-11-09 18:35:10,468:INFO: Dataset: univ                Batch:  9/15	Loss 79.5662 (80.5705)
2022-11-09 18:35:11,237:INFO: Dataset: univ                Batch: 10/15	Loss 79.7271 (80.4853)
2022-11-09 18:35:12,001:INFO: Dataset: univ                Batch: 11/15	Loss 79.5350 (80.3962)
2022-11-09 18:35:12,876:INFO: Dataset: univ                Batch: 12/15	Loss 80.5718 (80.4103)
2022-11-09 18:35:13,711:INFO: Dataset: univ                Batch: 13/15	Loss 80.0019 (80.3799)
2022-11-09 18:35:14,550:INFO: Dataset: univ                Batch: 14/15	Loss 79.3481 (80.2985)
2022-11-09 18:35:15,307:INFO: Dataset: univ                Batch: 15/15	Loss 14.9706 (79.3759)
2022-11-09 18:35:16,409:INFO: Dataset: zara1               Batch: 1/8	Loss 85.7782 (85.7782)
2022-11-09 18:35:17,158:INFO: Dataset: zara1               Batch: 2/8	Loss 87.9167 (86.8518)
2022-11-09 18:35:17,968:INFO: Dataset: zara1               Batch: 3/8	Loss 84.5622 (86.0762)
2022-11-09 18:35:18,747:INFO: Dataset: zara1               Batch: 4/8	Loss 84.7149 (85.7593)
2022-11-09 18:35:19,489:INFO: Dataset: zara1               Batch: 5/8	Loss 83.3898 (85.3070)
2022-11-09 18:35:20,276:INFO: Dataset: zara1               Batch: 6/8	Loss 82.8531 (84.8764)
2022-11-09 18:35:21,072:INFO: Dataset: zara1               Batch: 7/8	Loss 85.0269 (84.8973)
2022-11-09 18:35:21,816:INFO: Dataset: zara1               Batch: 8/8	Loss 72.8542 (83.4268)
2022-11-09 18:35:22,896:INFO: Dataset: zara2               Batch:  1/18	Loss 84.5655 (84.5655)
2022-11-09 18:35:23,712:INFO: Dataset: zara2               Batch:  2/18	Loss 82.0721 (83.2965)
2022-11-09 18:35:24,573:INFO: Dataset: zara2               Batch:  3/18	Loss 83.7825 (83.4530)
2022-11-09 18:35:25,401:INFO: Dataset: zara2               Batch:  4/18	Loss 83.2529 (83.3974)
2022-11-09 18:35:26,227:INFO: Dataset: zara2               Batch:  5/18	Loss 81.9648 (83.1196)
2022-11-09 18:35:27,016:INFO: Dataset: zara2               Batch:  6/18	Loss 81.6716 (82.8803)
2022-11-09 18:35:27,837:INFO: Dataset: zara2               Batch:  7/18	Loss 80.6865 (82.5699)
2022-11-09 18:35:28,592:INFO: Dataset: zara2               Batch:  8/18	Loss 81.2174 (82.4089)
2022-11-09 18:35:29,365:INFO: Dataset: zara2               Batch:  9/18	Loss 80.1734 (82.1449)
2022-11-09 18:35:30,270:INFO: Dataset: zara2               Batch: 10/18	Loss 80.5497 (81.9810)
2022-11-09 18:35:31,060:INFO: Dataset: zara2               Batch: 11/18	Loss 80.2096 (81.8190)
2022-11-09 18:35:31,856:INFO: Dataset: zara2               Batch: 12/18	Loss 81.0536 (81.7581)
2022-11-09 18:35:32,952:INFO: Dataset: zara2               Batch: 13/18	Loss 81.5817 (81.7444)
2022-11-09 18:35:33,762:INFO: Dataset: zara2               Batch: 14/18	Loss 80.0925 (81.6139)
2022-11-09 18:35:34,531:INFO: Dataset: zara2               Batch: 15/18	Loss 81.0130 (81.5757)
2022-11-09 18:35:35,290:INFO: Dataset: zara2               Batch: 16/18	Loss 81.4708 (81.5692)
2022-11-09 18:35:36,043:INFO: Dataset: zara2               Batch: 17/18	Loss 79.2117 (81.4414)
2022-11-09 18:35:36,781:INFO: Dataset: zara2               Batch: 18/18	Loss 69.9879 (80.9207)
2022-11-09 18:35:36,851:INFO: - Computing ADE (validation o)
2022-11-09 18:35:37,131:INFO: 		 ADE on eth                       dataset:	 2.685199737548828
2022-11-09 18:35:37,132:INFO: Average validation o:	ADE  2.6852	FDE  4.4024
2022-11-09 18:35:37,133:INFO: - Computing ADE (validation)
2022-11-09 18:35:37,399:INFO: 		 ADE on hotel                     dataset:	 1.0731120109558105
2022-11-09 18:35:37,717:INFO: 		 ADE on univ                      dataset:	 1.2631449699401855
2022-11-09 18:35:37,986:INFO: 		 ADE on zara1                     dataset:	 1.825792670249939
2022-11-09 18:35:38,366:INFO: 		 ADE on zara2                     dataset:	 1.1759856939315796
2022-11-09 18:35:38,366:INFO: Average validation:	ADE  1.2535	FDE  2.2858
2022-11-09 18:35:38,367:INFO: - Computing ADE (training)
2022-11-09 18:35:38,712:INFO: 		 ADE on hotel                     dataset:	 1.3627568483352661
2022-11-09 18:35:39,359:INFO: 		 ADE on univ                      dataset:	 1.1847361326217651
2022-11-09 18:35:39,822:INFO: 		 ADE on zara1                     dataset:	 1.8544551134109497
2022-11-09 18:35:40,527:INFO: 		 ADE on zara2                     dataset:	 1.3663818836212158
2022-11-09 18:35:40,528:INFO: Average training:	ADE  1.2688	FDE  2.3158
2022-11-09 18:35:40,538:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_302.pth.tar
2022-11-09 18:35:40,538:INFO: 
===> EPOCH: 303 (P3)
2022-11-09 18:35:40,538:INFO: - Computing loss (training)
2022-11-09 18:35:41,466:INFO: Dataset: hotel               Batch: 1/4	Loss 82.3745 (82.3745)
2022-11-09 18:35:42,207:INFO: Dataset: hotel               Batch: 2/4	Loss 82.8859 (82.6278)
2022-11-09 18:35:42,934:INFO: Dataset: hotel               Batch: 3/4	Loss 83.1829 (82.8087)
2022-11-09 18:35:43,623:INFO: Dataset: hotel               Batch: 4/4	Loss 49.3272 (77.1107)
2022-11-09 18:35:44,657:INFO: Dataset: univ                Batch:  1/15	Loss 79.5172 (79.5172)
2022-11-09 18:35:45,409:INFO: Dataset: univ                Batch:  2/15	Loss 80.0906 (79.8100)
2022-11-09 18:35:46,159:INFO: Dataset: univ                Batch:  3/15	Loss 79.7803 (79.8001)
2022-11-09 18:35:46,908:INFO: Dataset: univ                Batch:  4/15	Loss 80.3277 (79.9348)
2022-11-09 18:35:47,658:INFO: Dataset: univ                Batch:  5/15	Loss 79.8801 (79.9243)
2022-11-09 18:35:48,412:INFO: Dataset: univ                Batch:  6/15	Loss 79.3961 (79.8328)
2022-11-09 18:35:49,169:INFO: Dataset: univ                Batch:  7/15	Loss 80.4404 (79.9282)
2022-11-09 18:35:49,920:INFO: Dataset: univ                Batch:  8/15	Loss 79.6092 (79.8890)
2022-11-09 18:35:50,672:INFO: Dataset: univ                Batch:  9/15	Loss 79.3070 (79.8250)
2022-11-09 18:35:51,501:INFO: Dataset: univ                Batch: 10/15	Loss 79.4381 (79.7850)
2022-11-09 18:35:52,253:INFO: Dataset: univ                Batch: 11/15	Loss 79.7435 (79.7813)
2022-11-09 18:35:53,022:INFO: Dataset: univ                Batch: 12/15	Loss 79.4562 (79.7535)
2022-11-09 18:35:53,776:INFO: Dataset: univ                Batch: 13/15	Loss 80.4546 (79.8058)
2022-11-09 18:35:54,531:INFO: Dataset: univ                Batch: 14/15	Loss 79.3258 (79.7721)
2022-11-09 18:35:55,192:INFO: Dataset: univ                Batch: 15/15	Loss 14.7885 (78.7651)
2022-11-09 18:35:56,210:INFO: Dataset: zara1               Batch: 1/8	Loss 87.6813 (87.6813)
2022-11-09 18:35:56,958:INFO: Dataset: zara1               Batch: 2/8	Loss 96.4292 (92.0019)
2022-11-09 18:35:57,694:INFO: Dataset: zara1               Batch: 3/8	Loss 86.4962 (90.2171)
2022-11-09 18:35:58,435:INFO: Dataset: zara1               Batch: 4/8	Loss 82.1677 (88.2782)
2022-11-09 18:35:59,174:INFO: Dataset: zara1               Batch: 5/8	Loss 82.0550 (86.9710)
2022-11-09 18:35:59,925:INFO: Dataset: zara1               Batch: 6/8	Loss 81.5450 (86.0291)
2022-11-09 18:36:00,667:INFO: Dataset: zara1               Batch: 7/8	Loss 82.8282 (85.6073)
2022-11-09 18:36:01,391:INFO: Dataset: zara1               Batch: 8/8	Loss 71.0483 (84.0135)
2022-11-09 18:36:02,400:INFO: Dataset: zara2               Batch:  1/18	Loss 88.4184 (88.4184)
2022-11-09 18:36:03,142:INFO: Dataset: zara2               Batch:  2/18	Loss 90.6054 (89.4774)
2022-11-09 18:36:03,887:INFO: Dataset: zara2               Batch:  3/18	Loss 84.9140 (87.9827)
2022-11-09 18:36:04,635:INFO: Dataset: zara2               Batch:  4/18	Loss 83.9306 (86.8864)
2022-11-09 18:36:05,381:INFO: Dataset: zara2               Batch:  5/18	Loss 80.5225 (85.6503)
2022-11-09 18:36:06,125:INFO: Dataset: zara2               Batch:  6/18	Loss 80.9899 (84.8813)
2022-11-09 18:36:06,866:INFO: Dataset: zara2               Batch:  7/18	Loss 79.9042 (84.2255)
2022-11-09 18:36:07,689:INFO: Dataset: zara2               Batch:  8/18	Loss 80.8711 (83.8544)
2022-11-09 18:36:08,437:INFO: Dataset: zara2               Batch:  9/18	Loss 80.3814 (83.5013)
2022-11-09 18:36:09,183:INFO: Dataset: zara2               Batch: 10/18	Loss 81.0486 (83.2663)
2022-11-09 18:36:09,926:INFO: Dataset: zara2               Batch: 11/18	Loss 79.9153 (82.9471)
2022-11-09 18:36:10,669:INFO: Dataset: zara2               Batch: 12/18	Loss 79.3372 (82.6316)
2022-11-09 18:36:11,410:INFO: Dataset: zara2               Batch: 13/18	Loss 80.4120 (82.4522)
2022-11-09 18:36:12,151:INFO: Dataset: zara2               Batch: 14/18	Loss 84.9336 (82.6353)
2022-11-09 18:36:12,899:INFO: Dataset: zara2               Batch: 15/18	Loss 83.4124 (82.6818)
2022-11-09 18:36:13,645:INFO: Dataset: zara2               Batch: 16/18	Loss 80.5488 (82.5478)
2022-11-09 18:36:14,391:INFO: Dataset: zara2               Batch: 17/18	Loss 79.8020 (82.3864)
2022-11-09 18:36:15,128:INFO: Dataset: zara2               Batch: 18/18	Loss 68.4889 (81.7776)
2022-11-09 18:36:15,199:INFO: - Computing ADE (validation o)
2022-11-09 18:36:15,477:INFO: 		 ADE on eth                       dataset:	 2.6527059078216553
2022-11-09 18:36:15,478:INFO: Average validation o:	ADE  2.6527	FDE  4.3380
2022-11-09 18:36:15,478:INFO: - Computing ADE (validation)
2022-11-09 18:36:15,748:INFO: 		 ADE on hotel                     dataset:	 1.3141570091247559
2022-11-09 18:36:16,062:INFO: 		 ADE on univ                      dataset:	 1.4514514207839966
2022-11-09 18:36:16,339:INFO: 		 ADE on zara1                     dataset:	 2.0248453617095947
2022-11-09 18:36:16,709:INFO: 		 ADE on zara2                     dataset:	 1.3876715898513794
2022-11-09 18:36:16,709:INFO: Average validation:	ADE  1.4539	FDE  2.7197
2022-11-09 18:36:16,710:INFO: - Computing ADE (training)
2022-11-09 18:36:17,043:INFO: 		 ADE on hotel                     dataset:	 1.624355435371399
2022-11-09 18:36:17,710:INFO: 		 ADE on univ                      dataset:	 1.3707029819488525
2022-11-09 18:36:18,152:INFO: 		 ADE on zara1                     dataset:	 1.9437792301177979
2022-11-09 18:36:18,851:INFO: 		 ADE on zara2                     dataset:	 1.5077533721923828
2022-11-09 18:36:18,852:INFO: Average training:	ADE  1.4415	FDE  2.6931
2022-11-09 18:36:18,861:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_303.pth.tar
2022-11-09 18:36:18,861:INFO: 
===> EPOCH: 304 (P3)
2022-11-09 18:36:18,862:INFO: - Computing loss (training)
2022-11-09 18:36:19,803:INFO: Dataset: hotel               Batch: 1/4	Loss 85.4219 (85.4219)
2022-11-09 18:36:20,557:INFO: Dataset: hotel               Batch: 2/4	Loss 87.5246 (86.4782)
2022-11-09 18:36:21,313:INFO: Dataset: hotel               Batch: 3/4	Loss 82.8723 (85.2991)
2022-11-09 18:36:22,108:INFO: Dataset: hotel               Batch: 4/4	Loss 50.0988 (79.3550)
2022-11-09 18:36:23,213:INFO: Dataset: univ                Batch:  1/15	Loss 80.7808 (80.7808)
2022-11-09 18:36:23,981:INFO: Dataset: univ                Batch:  2/15	Loss 80.1485 (80.4895)
2022-11-09 18:36:24,787:INFO: Dataset: univ                Batch:  3/15	Loss 79.8385 (80.2712)
2022-11-09 18:36:25,575:INFO: Dataset: univ                Batch:  4/15	Loss 80.5656 (80.3432)
2022-11-09 18:36:26,349:INFO: Dataset: univ                Batch:  5/15	Loss 82.7220 (80.7759)
2022-11-09 18:36:27,174:INFO: Dataset: univ                Batch:  6/15	Loss 81.5613 (80.8886)
2022-11-09 18:36:27,989:INFO: Dataset: univ                Batch:  7/15	Loss 79.6585 (80.6946)
2022-11-09 18:36:28,838:INFO: Dataset: univ                Batch:  8/15	Loss 79.8042 (80.5831)
2022-11-09 18:36:29,653:INFO: Dataset: univ                Batch:  9/15	Loss 80.9306 (80.6230)
2022-11-09 18:36:30,476:INFO: Dataset: univ                Batch: 10/15	Loss 79.2489 (80.4746)
2022-11-09 18:36:31,275:INFO: Dataset: univ                Batch: 11/15	Loss 80.1732 (80.4482)
2022-11-09 18:36:32,071:INFO: Dataset: univ                Batch: 12/15	Loss 80.5666 (80.4574)
2022-11-09 18:36:32,871:INFO: Dataset: univ                Batch: 13/15	Loss 79.6975 (80.3980)
2022-11-09 18:36:33,689:INFO: Dataset: univ                Batch: 14/15	Loss 79.3501 (80.3269)
2022-11-09 18:36:34,411:INFO: Dataset: univ                Batch: 15/15	Loss 15.2095 (79.5801)
2022-11-09 18:36:35,420:INFO: Dataset: zara1               Batch: 1/8	Loss 85.3545 (85.3545)
2022-11-09 18:36:36,234:INFO: Dataset: zara1               Batch: 2/8	Loss 84.2308 (84.7786)
2022-11-09 18:36:37,026:INFO: Dataset: zara1               Batch: 3/8	Loss 82.8298 (84.1468)
2022-11-09 18:36:37,777:INFO: Dataset: zara1               Batch: 4/8	Loss 83.1012 (83.9231)
2022-11-09 18:36:38,557:INFO: Dataset: zara1               Batch: 5/8	Loss 82.7252 (83.6940)
2022-11-09 18:36:39,345:INFO: Dataset: zara1               Batch: 6/8	Loss 83.2064 (83.6118)
2022-11-09 18:36:40,439:INFO: Dataset: zara1               Batch: 7/8	Loss 83.8141 (83.6413)
2022-11-09 18:36:41,206:INFO: Dataset: zara1               Batch: 8/8	Loss 71.1289 (82.3308)
2022-11-09 18:36:42,233:INFO: Dataset: zara2               Batch:  1/18	Loss 82.3619 (82.3619)
2022-11-09 18:36:42,998:INFO: Dataset: zara2               Batch:  2/18	Loss 81.6819 (81.9992)
2022-11-09 18:36:43,747:INFO: Dataset: zara2               Batch:  3/18	Loss 82.4928 (82.1636)
2022-11-09 18:36:44,495:INFO: Dataset: zara2               Batch:  4/18	Loss 82.8128 (82.3336)
2022-11-09 18:36:45,255:INFO: Dataset: zara2               Batch:  5/18	Loss 84.2297 (82.7197)
2022-11-09 18:36:46,100:INFO: Dataset: zara2               Batch:  6/18	Loss 80.6269 (82.3692)
2022-11-09 18:36:46,912:INFO: Dataset: zara2               Batch:  7/18	Loss 81.7226 (82.2817)
2022-11-09 18:36:47,705:INFO: Dataset: zara2               Batch:  8/18	Loss 80.5926 (82.0916)
2022-11-09 18:36:48,456:INFO: Dataset: zara2               Batch:  9/18	Loss 80.1042 (81.8585)
2022-11-09 18:36:49,211:INFO: Dataset: zara2               Batch: 10/18	Loss 80.7733 (81.7414)
2022-11-09 18:36:49,967:INFO: Dataset: zara2               Batch: 11/18	Loss 80.0262 (81.5729)
2022-11-09 18:36:50,722:INFO: Dataset: zara2               Batch: 12/18	Loss 79.9331 (81.4218)
2022-11-09 18:36:51,522:INFO: Dataset: zara2               Batch: 13/18	Loss 81.7428 (81.4473)
2022-11-09 18:36:52,294:INFO: Dataset: zara2               Batch: 14/18	Loss 80.5261 (81.3727)
2022-11-09 18:36:53,048:INFO: Dataset: zara2               Batch: 15/18	Loss 81.1959 (81.3611)
2022-11-09 18:36:53,839:INFO: Dataset: zara2               Batch: 16/18	Loss 80.9943 (81.3355)
2022-11-09 18:36:54,592:INFO: Dataset: zara2               Batch: 17/18	Loss 79.6660 (81.2391)
2022-11-09 18:36:55,333:INFO: Dataset: zara2               Batch: 18/18	Loss 69.4317 (80.6535)
2022-11-09 18:36:55,405:INFO: - Computing ADE (validation o)
2022-11-09 18:36:55,676:INFO: 		 ADE on eth                       dataset:	 2.717575788497925
2022-11-09 18:36:55,676:INFO: Average validation o:	ADE  2.7176	FDE  4.5136
2022-11-09 18:36:55,677:INFO: - Computing ADE (validation)
2022-11-09 18:36:55,951:INFO: 		 ADE on hotel                     dataset:	 1.0370208024978638
2022-11-09 18:36:56,280:INFO: 		 ADE on univ                      dataset:	 1.2566276788711548
2022-11-09 18:36:56,566:INFO: 		 ADE on zara1                     dataset:	 1.8127295970916748
2022-11-09 18:36:56,930:INFO: 		 ADE on zara2                     dataset:	 1.171566367149353
2022-11-09 18:36:56,930:INFO: Average validation:	ADE  1.2457	FDE  2.2662
2022-11-09 18:36:56,931:INFO: - Computing ADE (training)
2022-11-09 18:36:57,263:INFO: 		 ADE on hotel                     dataset:	 1.3555006980895996
2022-11-09 18:36:57,897:INFO: 		 ADE on univ                      dataset:	 1.1749002933502197
2022-11-09 18:36:58,348:INFO: 		 ADE on zara1                     dataset:	 1.8591618537902832
2022-11-09 18:36:59,043:INFO: 		 ADE on zara2                     dataset:	 1.3723140954971313
2022-11-09 18:36:59,044:INFO: Average training:	ADE  1.2632	FDE  2.3032
2022-11-09 18:36:59,054:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_304.pth.tar
2022-11-09 18:36:59,054:INFO: 
===> EPOCH: 305 (P3)
2022-11-09 18:36:59,054:INFO: - Computing loss (training)
2022-11-09 18:36:59,988:INFO: Dataset: hotel               Batch: 1/4	Loss 85.1159 (85.1159)
2022-11-09 18:37:00,727:INFO: Dataset: hotel               Batch: 2/4	Loss 85.8867 (85.5257)
2022-11-09 18:37:01,454:INFO: Dataset: hotel               Batch: 3/4	Loss 82.2482 (84.3846)
2022-11-09 18:37:02,143:INFO: Dataset: hotel               Batch: 4/4	Loss 48.7854 (78.3261)
2022-11-09 18:37:03,184:INFO: Dataset: univ                Batch:  1/15	Loss 79.0784 (79.0784)
2022-11-09 18:37:03,950:INFO: Dataset: univ                Batch:  2/15	Loss 79.7525 (79.4065)
2022-11-09 18:37:04,703:INFO: Dataset: univ                Batch:  3/15	Loss 79.6476 (79.4869)
2022-11-09 18:37:05,462:INFO: Dataset: univ                Batch:  4/15	Loss 80.3719 (79.7156)
2022-11-09 18:37:06,300:INFO: Dataset: univ                Batch:  5/15	Loss 79.5632 (79.6847)
2022-11-09 18:37:07,068:INFO: Dataset: univ                Batch:  6/15	Loss 80.5692 (79.8431)
2022-11-09 18:37:07,825:INFO: Dataset: univ                Batch:  7/15	Loss 79.9925 (79.8635)
2022-11-09 18:37:08,584:INFO: Dataset: univ                Batch:  8/15	Loss 79.4373 (79.8105)
2022-11-09 18:37:09,342:INFO: Dataset: univ                Batch:  9/15	Loss 79.6038 (79.7880)
2022-11-09 18:37:10,100:INFO: Dataset: univ                Batch: 10/15	Loss 79.1752 (79.7273)
2022-11-09 18:37:10,857:INFO: Dataset: univ                Batch: 11/15	Loss 80.2158 (79.7705)
2022-11-09 18:37:11,615:INFO: Dataset: univ                Batch: 12/15	Loss 79.8135 (79.7739)
2022-11-09 18:37:12,374:INFO: Dataset: univ                Batch: 13/15	Loss 79.8085 (79.7765)
2022-11-09 18:37:13,135:INFO: Dataset: univ                Batch: 14/15	Loss 80.2094 (79.8065)
2022-11-09 18:37:13,800:INFO: Dataset: univ                Batch: 15/15	Loss 14.8786 (79.0342)
2022-11-09 18:37:14,830:INFO: Dataset: zara1               Batch: 1/8	Loss 85.5936 (85.5936)
2022-11-09 18:37:15,571:INFO: Dataset: zara1               Batch: 2/8	Loss 87.4336 (86.3860)
2022-11-09 18:37:16,309:INFO: Dataset: zara1               Batch: 3/8	Loss 82.7550 (85.3534)
2022-11-09 18:37:17,049:INFO: Dataset: zara1               Batch: 4/8	Loss 85.2787 (85.3353)
2022-11-09 18:37:17,788:INFO: Dataset: zara1               Batch: 5/8	Loss 81.2756 (84.6213)
2022-11-09 18:37:18,528:INFO: Dataset: zara1               Batch: 6/8	Loss 82.5031 (84.2624)
2022-11-09 18:37:19,273:INFO: Dataset: zara1               Batch: 7/8	Loss 82.0590 (83.9432)
2022-11-09 18:37:19,998:INFO: Dataset: zara1               Batch: 8/8	Loss 71.4401 (82.6600)
2022-11-09 18:37:21,010:INFO: Dataset: zara2               Batch:  1/18	Loss 85.0868 (85.0868)
2022-11-09 18:37:21,751:INFO: Dataset: zara2               Batch:  2/18	Loss 86.0751 (85.5670)
2022-11-09 18:37:22,571:INFO: Dataset: zara2               Batch:  3/18	Loss 86.1802 (85.7675)
2022-11-09 18:37:23,316:INFO: Dataset: zara2               Batch:  4/18	Loss 79.6243 (84.2769)
2022-11-09 18:37:24,062:INFO: Dataset: zara2               Batch:  5/18	Loss 79.6934 (83.4382)
2022-11-09 18:37:24,811:INFO: Dataset: zara2               Batch:  6/18	Loss 80.1370 (82.9711)
2022-11-09 18:37:25,554:INFO: Dataset: zara2               Batch:  7/18	Loss 81.4423 (82.7310)
2022-11-09 18:37:26,296:INFO: Dataset: zara2               Batch:  8/18	Loss 80.1195 (82.4059)
2022-11-09 18:37:27,056:INFO: Dataset: zara2               Batch:  9/18	Loss 79.3965 (82.0267)
2022-11-09 18:37:27,805:INFO: Dataset: zara2               Batch: 10/18	Loss 79.7818 (81.7766)
2022-11-09 18:37:28,550:INFO: Dataset: zara2               Batch: 11/18	Loss 79.5789 (81.6052)
2022-11-09 18:37:29,297:INFO: Dataset: zara2               Batch: 12/18	Loss 79.0327 (81.3912)
2022-11-09 18:37:30,041:INFO: Dataset: zara2               Batch: 13/18	Loss 79.4453 (81.2454)
2022-11-09 18:37:30,788:INFO: Dataset: zara2               Batch: 14/18	Loss 80.8162 (81.2149)
2022-11-09 18:37:31,542:INFO: Dataset: zara2               Batch: 15/18	Loss 79.7674 (81.1266)
2022-11-09 18:37:32,289:INFO: Dataset: zara2               Batch: 16/18	Loss 79.6812 (81.0352)
2022-11-09 18:37:33,034:INFO: Dataset: zara2               Batch: 17/18	Loss 79.5508 (80.9493)
2022-11-09 18:37:33,766:INFO: Dataset: zara2               Batch: 18/18	Loss 69.0466 (80.3728)
2022-11-09 18:37:33,836:INFO: - Computing ADE (validation o)
2022-11-09 18:37:34,114:INFO: 		 ADE on eth                       dataset:	 2.41051983833313
2022-11-09 18:37:34,115:INFO: Average validation o:	ADE  2.4105	FDE  3.8581
2022-11-09 18:37:34,115:INFO: - Computing ADE (validation)
2022-11-09 18:37:34,386:INFO: 		 ADE on hotel                     dataset:	 1.1699533462524414
2022-11-09 18:37:34,699:INFO: 		 ADE on univ                      dataset:	 1.2835677862167358
2022-11-09 18:37:34,969:INFO: 		 ADE on zara1                     dataset:	 1.8685941696166992
2022-11-09 18:37:35,357:INFO: 		 ADE on zara2                     dataset:	 1.2036664485931396
2022-11-09 18:37:35,357:INFO: Average validation:	ADE  1.2821	FDE  2.3951
2022-11-09 18:37:35,358:INFO: - Computing ADE (training)
2022-11-09 18:37:35,698:INFO: 		 ADE on hotel                     dataset:	 1.4283610582351685
2022-11-09 18:37:36,332:INFO: 		 ADE on univ                      dataset:	 1.2195855379104614
2022-11-09 18:37:36,770:INFO: 		 ADE on zara1                     dataset:	 1.7954411506652832
2022-11-09 18:37:37,492:INFO: 		 ADE on zara2                     dataset:	 1.33394193649292
2022-11-09 18:37:37,493:INFO: Average training:	ADE  1.2848	FDE  2.3959
2022-11-09 18:37:37,503:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_305.pth.tar
2022-11-09 18:37:37,503:INFO: 
===> EPOCH: 306 (P3)
2022-11-09 18:37:37,503:INFO: - Computing loss (training)
2022-11-09 18:37:38,449:INFO: Dataset: hotel               Batch: 1/4	Loss 81.2918 (81.2918)
2022-11-09 18:37:39,192:INFO: Dataset: hotel               Batch: 2/4	Loss 81.5641 (81.4292)
2022-11-09 18:37:39,919:INFO: Dataset: hotel               Batch: 3/4	Loss 82.2649 (81.6894)
2022-11-09 18:37:40,608:INFO: Dataset: hotel               Batch: 4/4	Loss 48.7822 (76.3930)
2022-11-09 18:37:41,625:INFO: Dataset: univ                Batch:  1/15	Loss 79.8715 (79.8715)
2022-11-09 18:37:42,374:INFO: Dataset: univ                Batch:  2/15	Loss 79.2805 (79.5640)
2022-11-09 18:37:43,130:INFO: Dataset: univ                Batch:  3/15	Loss 79.5424 (79.5564)
2022-11-09 18:37:43,887:INFO: Dataset: univ                Batch:  4/15	Loss 79.0433 (79.4255)
2022-11-09 18:37:44,713:INFO: Dataset: univ                Batch:  5/15	Loss 78.8665 (79.3132)
2022-11-09 18:37:45,473:INFO: Dataset: univ                Batch:  6/15	Loss 78.9508 (79.2476)
2022-11-09 18:37:46,227:INFO: Dataset: univ                Batch:  7/15	Loss 79.2725 (79.2513)
2022-11-09 18:37:46,970:INFO: Dataset: univ                Batch:  8/15	Loss 82.9694 (79.6426)
2022-11-09 18:37:47,720:INFO: Dataset: univ                Batch:  9/15	Loss 79.2100 (79.5976)
2022-11-09 18:37:48,471:INFO: Dataset: univ                Batch: 10/15	Loss 80.0795 (79.6452)
2022-11-09 18:37:49,224:INFO: Dataset: univ                Batch: 11/15	Loss 80.1782 (79.6910)
2022-11-09 18:37:49,978:INFO: Dataset: univ                Batch: 12/15	Loss 78.9856 (79.6312)
2022-11-09 18:37:50,726:INFO: Dataset: univ                Batch: 13/15	Loss 79.8498 (79.6464)
2022-11-09 18:37:51,482:INFO: Dataset: univ                Batch: 14/15	Loss 79.3888 (79.6286)
2022-11-09 18:37:52,141:INFO: Dataset: univ                Batch: 15/15	Loss 14.9081 (78.8496)
2022-11-09 18:37:53,161:INFO: Dataset: zara1               Batch: 1/8	Loss 86.9748 (86.9748)
2022-11-09 18:37:53,900:INFO: Dataset: zara1               Batch: 2/8	Loss 83.4077 (85.0951)
2022-11-09 18:37:54,640:INFO: Dataset: zara1               Batch: 3/8	Loss 87.2650 (85.8042)
2022-11-09 18:37:55,376:INFO: Dataset: zara1               Batch: 4/8	Loss 94.1302 (87.8418)
2022-11-09 18:37:56,114:INFO: Dataset: zara1               Batch: 5/8	Loss 81.0992 (86.5718)
2022-11-09 18:37:56,850:INFO: Dataset: zara1               Batch: 6/8	Loss 90.7757 (87.2735)
2022-11-09 18:37:57,594:INFO: Dataset: zara1               Batch: 7/8	Loss 96.6881 (88.8360)
2022-11-09 18:37:58,328:INFO: Dataset: zara1               Batch: 8/8	Loss 70.7298 (86.7491)
2022-11-09 18:37:59,389:INFO: Dataset: zara2               Batch:  1/18	Loss 85.1327 (85.1327)
2022-11-09 18:38:00,177:INFO: Dataset: zara2               Batch:  2/18	Loss 80.2257 (82.5895)
2022-11-09 18:38:01,142:INFO: Dataset: zara2               Batch:  3/18	Loss 80.4972 (81.8297)
2022-11-09 18:38:01,960:INFO: Dataset: zara2               Batch:  4/18	Loss 79.7491 (81.3284)
2022-11-09 18:38:02,760:INFO: Dataset: zara2               Batch:  5/18	Loss 79.5316 (80.9508)
2022-11-09 18:38:03,590:INFO: Dataset: zara2               Batch:  6/18	Loss 79.3628 (80.6644)
2022-11-09 18:38:04,415:INFO: Dataset: zara2               Batch:  7/18	Loss 79.8615 (80.5490)
2022-11-09 18:38:05,182:INFO: Dataset: zara2               Batch:  8/18	Loss 81.2308 (80.6291)
2022-11-09 18:38:06,014:INFO: Dataset: zara2               Batch:  9/18	Loss 79.6504 (80.5293)
2022-11-09 18:38:06,803:INFO: Dataset: zara2               Batch: 10/18	Loss 79.9052 (80.4725)
2022-11-09 18:38:07,591:INFO: Dataset: zara2               Batch: 11/18	Loss 79.5948 (80.3891)
2022-11-09 18:38:08,378:INFO: Dataset: zara2               Batch: 12/18	Loss 81.2260 (80.4558)
2022-11-09 18:38:09,194:INFO: Dataset: zara2               Batch: 13/18	Loss 80.0545 (80.4264)
2022-11-09 18:38:09,955:INFO: Dataset: zara2               Batch: 14/18	Loss 80.3773 (80.4228)
2022-11-09 18:38:10,805:INFO: Dataset: zara2               Batch: 15/18	Loss 80.0787 (80.3996)
2022-11-09 18:38:11,596:INFO: Dataset: zara2               Batch: 16/18	Loss 79.3866 (80.3358)
2022-11-09 18:38:12,388:INFO: Dataset: zara2               Batch: 17/18	Loss 79.1410 (80.2662)
2022-11-09 18:38:13,173:INFO: Dataset: zara2               Batch: 18/18	Loss 68.7048 (79.7311)
2022-11-09 18:38:13,243:INFO: - Computing ADE (validation o)
2022-11-09 18:38:13,538:INFO: 		 ADE on eth                       dataset:	 2.797032356262207
2022-11-09 18:38:13,538:INFO: Average validation o:	ADE  2.7970	FDE  4.6906
2022-11-09 18:38:13,539:INFO: - Computing ADE (validation)
2022-11-09 18:38:13,844:INFO: 		 ADE on hotel                     dataset:	 1.3740415573120117
2022-11-09 18:38:14,183:INFO: 		 ADE on univ                      dataset:	 1.4048970937728882
2022-11-09 18:38:14,454:INFO: 		 ADE on zara1                     dataset:	 1.7412917613983154
2022-11-09 18:38:14,836:INFO: 		 ADE on zara2                     dataset:	 1.3734264373779297
2022-11-09 18:38:14,836:INFO: Average validation:	ADE  1.4112	FDE  2.6079
2022-11-09 18:38:14,837:INFO: - Computing ADE (training)
2022-11-09 18:38:15,199:INFO: 		 ADE on hotel                     dataset:	 1.6280362606048584
2022-11-09 18:38:15,878:INFO: 		 ADE on univ                      dataset:	 1.3482168912887573
2022-11-09 18:38:16,347:INFO: 		 ADE on zara1                     dataset:	 1.8845149278640747
2022-11-09 18:38:17,090:INFO: 		 ADE on zara2                     dataset:	 1.511440634727478
2022-11-09 18:38:17,091:INFO: Average training:	ADE  1.4226	FDE  2.6279
2022-11-09 18:38:17,100:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_306.pth.tar
2022-11-09 18:38:17,100:INFO: 
===> EPOCH: 307 (P3)
2022-11-09 18:38:17,101:INFO: - Computing loss (training)
2022-11-09 18:38:18,051:INFO: Dataset: hotel               Batch: 1/4	Loss 88.2204 (88.2204)
2022-11-09 18:38:18,861:INFO: Dataset: hotel               Batch: 2/4	Loss 84.3896 (86.2869)
2022-11-09 18:38:19,772:INFO: Dataset: hotel               Batch: 3/4	Loss 82.6264 (85.0861)
2022-11-09 18:38:20,465:INFO: Dataset: hotel               Batch: 4/4	Loss 50.2296 (79.2460)
2022-11-09 18:38:21,508:INFO: Dataset: univ                Batch:  1/15	Loss 81.7355 (81.7355)
2022-11-09 18:38:22,342:INFO: Dataset: univ                Batch:  2/15	Loss 82.0713 (81.8999)
2022-11-09 18:38:23,104:INFO: Dataset: univ                Batch:  3/15	Loss 81.2298 (81.6531)
2022-11-09 18:38:23,862:INFO: Dataset: univ                Batch:  4/15	Loss 79.6718 (81.1493)
2022-11-09 18:38:24,614:INFO: Dataset: univ                Batch:  5/15	Loss 80.8033 (81.0856)
2022-11-09 18:38:25,367:INFO: Dataset: univ                Batch:  6/15	Loss 80.0883 (80.9388)
2022-11-09 18:38:26,124:INFO: Dataset: univ                Batch:  7/15	Loss 79.9997 (80.8028)
2022-11-09 18:38:26,885:INFO: Dataset: univ                Batch:  8/15	Loss 80.7881 (80.8009)
2022-11-09 18:38:27,647:INFO: Dataset: univ                Batch:  9/15	Loss 80.9831 (80.8204)
2022-11-09 18:38:28,414:INFO: Dataset: univ                Batch: 10/15	Loss 79.6859 (80.6974)
2022-11-09 18:38:29,174:INFO: Dataset: univ                Batch: 11/15	Loss 79.6783 (80.6031)
2022-11-09 18:38:29,927:INFO: Dataset: univ                Batch: 12/15	Loss 82.2200 (80.7212)
2022-11-09 18:38:30,682:INFO: Dataset: univ                Batch: 13/15	Loss 82.5353 (80.8567)
2022-11-09 18:38:31,449:INFO: Dataset: univ                Batch: 14/15	Loss 79.3757 (80.7461)
2022-11-09 18:38:32,115:INFO: Dataset: univ                Batch: 15/15	Loss 14.7770 (79.7176)
2022-11-09 18:38:33,143:INFO: Dataset: zara1               Batch: 1/8	Loss 83.4301 (83.4301)
2022-11-09 18:38:33,891:INFO: Dataset: zara1               Batch: 2/8	Loss 82.1371 (82.8068)
2022-11-09 18:38:34,634:INFO: Dataset: zara1               Batch: 3/8	Loss 82.4379 (82.6852)
2022-11-09 18:38:35,377:INFO: Dataset: zara1               Batch: 4/8	Loss 82.4980 (82.6385)
2022-11-09 18:38:36,121:INFO: Dataset: zara1               Batch: 5/8	Loss 82.9720 (82.7012)
2022-11-09 18:38:36,866:INFO: Dataset: zara1               Batch: 6/8	Loss 86.3931 (83.2958)
2022-11-09 18:38:37,609:INFO: Dataset: zara1               Batch: 7/8	Loss 83.0511 (83.2593)
2022-11-09 18:38:38,347:INFO: Dataset: zara1               Batch: 8/8	Loss 71.1392 (82.0792)
2022-11-09 18:38:39,380:INFO: Dataset: zara2               Batch:  1/18	Loss 81.7177 (81.7177)
2022-11-09 18:38:40,211:INFO: Dataset: zara2               Batch:  2/18	Loss 80.8647 (81.3408)
2022-11-09 18:38:40,967:INFO: Dataset: zara2               Batch:  3/18	Loss 80.9074 (81.2101)
2022-11-09 18:38:41,719:INFO: Dataset: zara2               Batch:  4/18	Loss 80.3688 (81.0079)
2022-11-09 18:38:42,469:INFO: Dataset: zara2               Batch:  5/18	Loss 79.7049 (80.7408)
2022-11-09 18:38:43,218:INFO: Dataset: zara2               Batch:  6/18	Loss 78.9462 (80.4559)
2022-11-09 18:38:43,969:INFO: Dataset: zara2               Batch:  7/18	Loss 80.0455 (80.3999)
2022-11-09 18:38:44,711:INFO: Dataset: zara2               Batch:  8/18	Loss 81.1299 (80.4996)
2022-11-09 18:38:45,456:INFO: Dataset: zara2               Batch:  9/18	Loss 80.9956 (80.5560)
2022-11-09 18:38:46,204:INFO: Dataset: zara2               Batch: 10/18	Loss 80.9715 (80.5973)
2022-11-09 18:38:46,966:INFO: Dataset: zara2               Batch: 11/18	Loss 81.5522 (80.6838)
2022-11-09 18:38:47,749:INFO: Dataset: zara2               Batch: 12/18	Loss 80.3341 (80.6556)
2022-11-09 18:38:48,540:INFO: Dataset: zara2               Batch: 13/18	Loss 79.9942 (80.6104)
2022-11-09 18:38:49,304:INFO: Dataset: zara2               Batch: 14/18	Loss 79.2619 (80.5103)
2022-11-09 18:38:50,205:INFO: Dataset: zara2               Batch: 15/18	Loss 79.5996 (80.4516)
2022-11-09 18:38:51,062:INFO: Dataset: zara2               Batch: 16/18	Loss 80.6575 (80.4648)
2022-11-09 18:38:51,844:INFO: Dataset: zara2               Batch: 17/18	Loss 79.8851 (80.4291)
2022-11-09 18:38:52,666:INFO: Dataset: zara2               Batch: 18/18	Loss 69.2092 (79.9172)
2022-11-09 18:38:52,746:INFO: - Computing ADE (validation o)
2022-11-09 18:38:53,041:INFO: 		 ADE on eth                       dataset:	 2.671640157699585
2022-11-09 18:38:53,041:INFO: Average validation o:	ADE  2.6716	FDE  4.3962
2022-11-09 18:38:53,042:INFO: - Computing ADE (validation)
2022-11-09 18:38:53,322:INFO: 		 ADE on hotel                     dataset:	 1.2143299579620361
2022-11-09 18:38:53,647:INFO: 		 ADE on univ                      dataset:	 1.316731572151184
2022-11-09 18:38:53,957:INFO: 		 ADE on zara1                     dataset:	 1.741173267364502
2022-11-09 18:38:54,337:INFO: 		 ADE on zara2                     dataset:	 1.2734531164169312
2022-11-09 18:38:54,337:INFO: Average validation:	ADE  1.3199	FDE  2.4182
2022-11-09 18:38:54,338:INFO: - Computing ADE (training)
2022-11-09 18:38:54,675:INFO: 		 ADE on hotel                     dataset:	 1.4928406476974487
2022-11-09 18:38:55,321:INFO: 		 ADE on univ                      dataset:	 1.2607951164245605
2022-11-09 18:38:55,781:INFO: 		 ADE on zara1                     dataset:	 1.8304884433746338
2022-11-09 18:38:56,512:INFO: 		 ADE on zara2                     dataset:	 1.4175472259521484
2022-11-09 18:38:56,512:INFO: Average training:	ADE  1.3348	FDE  2.4466
2022-11-09 18:38:56,522:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_307.pth.tar
2022-11-09 18:38:56,522:INFO: 
===> EPOCH: 308 (P3)
2022-11-09 18:38:56,522:INFO: - Computing loss (training)
2022-11-09 18:38:57,511:INFO: Dataset: hotel               Batch: 1/4	Loss 82.1633 (82.1633)
2022-11-09 18:38:58,296:INFO: Dataset: hotel               Batch: 2/4	Loss 81.8687 (82.0189)
2022-11-09 18:38:59,054:INFO: Dataset: hotel               Batch: 3/4	Loss 82.1470 (82.0631)
2022-11-09 18:38:59,748:INFO: Dataset: hotel               Batch: 4/4	Loss 49.2161 (76.3430)
2022-11-09 18:39:00,982:INFO: Dataset: univ                Batch:  1/15	Loss 80.4390 (80.4390)
2022-11-09 18:39:01,806:INFO: Dataset: univ                Batch:  2/15	Loss 79.6468 (80.0389)
2022-11-09 18:39:02,575:INFO: Dataset: univ                Batch:  3/15	Loss 80.0777 (80.0520)
2022-11-09 18:39:03,402:INFO: Dataset: univ                Batch:  4/15	Loss 79.2553 (79.8476)
2022-11-09 18:39:04,233:INFO: Dataset: univ                Batch:  5/15	Loss 79.4417 (79.7574)
2022-11-09 18:39:04,992:INFO: Dataset: univ                Batch:  6/15	Loss 78.8858 (79.6132)
2022-11-09 18:39:05,766:INFO: Dataset: univ                Batch:  7/15	Loss 79.1179 (79.5426)
2022-11-09 18:39:06,557:INFO: Dataset: univ                Batch:  8/15	Loss 81.7397 (79.7682)
2022-11-09 18:39:07,322:INFO: Dataset: univ                Batch:  9/15	Loss 80.3810 (79.8385)
2022-11-09 18:39:08,119:INFO: Dataset: univ                Batch: 10/15	Loss 79.4015 (79.7988)
2022-11-09 18:39:08,897:INFO: Dataset: univ                Batch: 11/15	Loss 81.6446 (79.9447)
2022-11-09 18:39:09,761:INFO: Dataset: univ                Batch: 12/15	Loss 79.3352 (79.8962)
2022-11-09 18:39:10,523:INFO: Dataset: univ                Batch: 13/15	Loss 79.5709 (79.8716)
2022-11-09 18:39:11,337:INFO: Dataset: univ                Batch: 14/15	Loss 79.0676 (79.8130)
2022-11-09 18:39:12,020:INFO: Dataset: univ                Batch: 15/15	Loss 14.7497 (78.9959)
2022-11-09 18:39:13,114:INFO: Dataset: zara1               Batch: 1/8	Loss 85.5716 (85.5716)
2022-11-09 18:39:13,974:INFO: Dataset: zara1               Batch: 2/8	Loss 83.0175 (84.2666)
2022-11-09 18:39:14,732:INFO: Dataset: zara1               Batch: 3/8	Loss 81.3589 (83.2733)
2022-11-09 18:39:15,474:INFO: Dataset: zara1               Batch: 4/8	Loss 81.7283 (82.9365)
2022-11-09 18:39:16,222:INFO: Dataset: zara1               Batch: 5/8	Loss 84.3428 (83.2045)
2022-11-09 18:39:16,973:INFO: Dataset: zara1               Batch: 6/8	Loss 81.4299 (82.9215)
2022-11-09 18:39:17,711:INFO: Dataset: zara1               Batch: 7/8	Loss 81.6750 (82.7329)
2022-11-09 18:39:18,438:INFO: Dataset: zara1               Batch: 8/8	Loss 70.7434 (81.4203)
2022-11-09 18:39:19,536:INFO: Dataset: zara2               Batch:  1/18	Loss 92.3900 (92.3900)
2022-11-09 18:39:20,277:INFO: Dataset: zara2               Batch:  2/18	Loss 93.2595 (92.7857)
2022-11-09 18:39:21,018:INFO: Dataset: zara2               Batch:  3/18	Loss 87.0622 (90.9076)
2022-11-09 18:39:21,756:INFO: Dataset: zara2               Batch:  4/18	Loss 82.2413 (88.7002)
2022-11-09 18:39:22,494:INFO: Dataset: zara2               Batch:  5/18	Loss 79.4417 (86.8679)
2022-11-09 18:39:23,239:INFO: Dataset: zara2               Batch:  6/18	Loss 81.5437 (86.0187)
2022-11-09 18:39:23,981:INFO: Dataset: zara2               Batch:  7/18	Loss 81.9971 (85.4743)
2022-11-09 18:39:24,719:INFO: Dataset: zara2               Batch:  8/18	Loss 80.1245 (84.8103)
2022-11-09 18:39:25,457:INFO: Dataset: zara2               Batch:  9/18	Loss 79.5873 (84.2092)
2022-11-09 18:39:26,208:INFO: Dataset: zara2               Batch: 10/18	Loss 80.3347 (83.8623)
2022-11-09 18:39:26,948:INFO: Dataset: zara2               Batch: 11/18	Loss 83.1007 (83.7851)
2022-11-09 18:39:27,685:INFO: Dataset: zara2               Batch: 12/18	Loss 79.9748 (83.4572)
2022-11-09 18:39:28,425:INFO: Dataset: zara2               Batch: 13/18	Loss 83.1128 (83.4321)
2022-11-09 18:39:29,166:INFO: Dataset: zara2               Batch: 14/18	Loss 80.5208 (83.2273)
2022-11-09 18:39:29,905:INFO: Dataset: zara2               Batch: 15/18	Loss 80.5820 (83.0558)
2022-11-09 18:39:30,642:INFO: Dataset: zara2               Batch: 16/18	Loss 79.8228 (82.8372)
2022-11-09 18:39:31,383:INFO: Dataset: zara2               Batch: 17/18	Loss 80.7770 (82.7147)
2022-11-09 18:39:32,111:INFO: Dataset: zara2               Batch: 18/18	Loss 69.7934 (82.1187)
2022-11-09 18:39:32,184:INFO: - Computing ADE (validation o)
2022-11-09 18:39:32,464:INFO: 		 ADE on eth                       dataset:	 2.664769411087036
2022-11-09 18:39:32,464:INFO: Average validation o:	ADE  2.6648	FDE  4.4052
2022-11-09 18:39:32,465:INFO: - Computing ADE (validation)
2022-11-09 18:39:32,751:INFO: 		 ADE on hotel                     dataset:	 1.366611123085022
2022-11-09 18:39:33,068:INFO: 		 ADE on univ                      dataset:	 1.4989745616912842
2022-11-09 18:39:33,334:INFO: 		 ADE on zara1                     dataset:	 2.084139823913574
2022-11-09 18:39:33,707:INFO: 		 ADE on zara2                     dataset:	 1.4427653551101685
2022-11-09 18:39:33,708:INFO: Average validation:	ADE  1.5051	FDE  2.7816
2022-11-09 18:39:33,708:INFO: - Computing ADE (training)
2022-11-09 18:39:34,045:INFO: 		 ADE on hotel                     dataset:	 1.6411629915237427
2022-11-09 18:39:34,685:INFO: 		 ADE on univ                      dataset:	 1.4221231937408447
2022-11-09 18:39:35,145:INFO: 		 ADE on zara1                     dataset:	 2.018589973449707
2022-11-09 18:39:35,843:INFO: 		 ADE on zara2                     dataset:	 1.5883336067199707
2022-11-09 18:39:35,843:INFO: Average training:	ADE  1.4994	FDE  2.7677
2022-11-09 18:39:35,853:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_308.pth.tar
2022-11-09 18:39:35,853:INFO: 
===> EPOCH: 309 (P3)
2022-11-09 18:39:35,853:INFO: - Computing loss (training)
2022-11-09 18:39:36,771:INFO: Dataset: hotel               Batch: 1/4	Loss 84.5935 (84.5935)
2022-11-09 18:39:37,507:INFO: Dataset: hotel               Batch: 2/4	Loss 83.4412 (84.0311)
2022-11-09 18:39:38,228:INFO: Dataset: hotel               Batch: 3/4	Loss 83.5225 (83.8637)
2022-11-09 18:39:39,007:INFO: Dataset: hotel               Batch: 4/4	Loss 50.2043 (77.8690)
2022-11-09 18:39:40,033:INFO: Dataset: univ                Batch:  1/15	Loss 82.5902 (82.5902)
2022-11-09 18:39:40,774:INFO: Dataset: univ                Batch:  2/15	Loss 80.1533 (81.4388)
2022-11-09 18:39:41,527:INFO: Dataset: univ                Batch:  3/15	Loss 80.5376 (81.1307)
2022-11-09 18:39:42,280:INFO: Dataset: univ                Batch:  4/15	Loss 80.3718 (80.9366)
2022-11-09 18:39:43,028:INFO: Dataset: univ                Batch:  5/15	Loss 82.2587 (81.1928)
2022-11-09 18:39:43,783:INFO: Dataset: univ                Batch:  6/15	Loss 81.5519 (81.2514)
2022-11-09 18:39:44,527:INFO: Dataset: univ                Batch:  7/15	Loss 80.3695 (81.1332)
2022-11-09 18:39:45,288:INFO: Dataset: univ                Batch:  8/15	Loss 79.4213 (80.9016)
2022-11-09 18:39:46,049:INFO: Dataset: univ                Batch:  9/15	Loss 80.7809 (80.8875)
2022-11-09 18:39:46,795:INFO: Dataset: univ                Batch: 10/15	Loss 81.1329 (80.9084)
2022-11-09 18:39:47,552:INFO: Dataset: univ                Batch: 11/15	Loss 79.6556 (80.7959)
2022-11-09 18:39:48,308:INFO: Dataset: univ                Batch: 12/15	Loss 81.1578 (80.8268)
2022-11-09 18:39:49,065:INFO: Dataset: univ                Batch: 13/15	Loss 81.3429 (80.8672)
2022-11-09 18:39:49,821:INFO: Dataset: univ                Batch: 14/15	Loss 79.6697 (80.7789)
2022-11-09 18:39:50,480:INFO: Dataset: univ                Batch: 15/15	Loss 14.7261 (79.9776)
2022-11-09 18:39:51,498:INFO: Dataset: zara1               Batch: 1/8	Loss 90.8827 (90.8827)
2022-11-09 18:39:52,247:INFO: Dataset: zara1               Batch: 2/8	Loss 84.0731 (87.4508)
2022-11-09 18:39:52,997:INFO: Dataset: zara1               Batch: 3/8	Loss 82.9628 (86.0281)
2022-11-09 18:39:53,746:INFO: Dataset: zara1               Batch: 4/8	Loss 85.3626 (85.8643)
2022-11-09 18:39:54,497:INFO: Dataset: zara1               Batch: 5/8	Loss 83.5543 (85.3989)
2022-11-09 18:39:55,246:INFO: Dataset: zara1               Batch: 6/8	Loss 83.1459 (85.0549)
2022-11-09 18:39:56,001:INFO: Dataset: zara1               Batch: 7/8	Loss 85.2674 (85.0843)
2022-11-09 18:39:56,812:INFO: Dataset: zara1               Batch: 8/8	Loss 72.1745 (83.5351)
2022-11-09 18:39:57,837:INFO: Dataset: zara2               Batch:  1/18	Loss 83.1764 (83.1764)
2022-11-09 18:39:58,587:INFO: Dataset: zara2               Batch:  2/18	Loss 82.3925 (82.7804)
2022-11-09 18:39:59,338:INFO: Dataset: zara2               Batch:  3/18	Loss 84.3958 (83.2929)
2022-11-09 18:40:00,087:INFO: Dataset: zara2               Batch:  4/18	Loss 82.0098 (82.9704)
2022-11-09 18:40:00,833:INFO: Dataset: zara2               Batch:  5/18	Loss 81.8705 (82.7331)
2022-11-09 18:40:01,583:INFO: Dataset: zara2               Batch:  6/18	Loss 80.9640 (82.4279)
2022-11-09 18:40:02,332:INFO: Dataset: zara2               Batch:  7/18	Loss 80.6537 (82.1378)
2022-11-09 18:40:03,083:INFO: Dataset: zara2               Batch:  8/18	Loss 80.5861 (81.9441)
2022-11-09 18:40:03,840:INFO: Dataset: zara2               Batch:  9/18	Loss 81.1466 (81.8496)
2022-11-09 18:40:04,615:INFO: Dataset: zara2               Batch: 10/18	Loss 79.4170 (81.6027)
2022-11-09 18:40:05,379:INFO: Dataset: zara2               Batch: 11/18	Loss 82.0855 (81.6442)
2022-11-09 18:40:06,265:INFO: Dataset: zara2               Batch: 12/18	Loss 79.7032 (81.5037)
2022-11-09 18:40:07,074:INFO: Dataset: zara2               Batch: 13/18	Loss 79.7956 (81.3769)
2022-11-09 18:40:07,913:INFO: Dataset: zara2               Batch: 14/18	Loss 83.1608 (81.5065)
2022-11-09 18:40:08,794:INFO: Dataset: zara2               Batch: 15/18	Loss 82.8827 (81.5966)
2022-11-09 18:40:09,666:INFO: Dataset: zara2               Batch: 16/18	Loss 81.1614 (81.5696)
2022-11-09 18:40:10,481:INFO: Dataset: zara2               Batch: 17/18	Loss 80.6541 (81.5163)
2022-11-09 18:40:11,292:INFO: Dataset: zara2               Batch: 18/18	Loss 70.3873 (81.0067)
2022-11-09 18:40:11,371:INFO: - Computing ADE (validation o)
2022-11-09 18:40:11,695:INFO: 		 ADE on eth                       dataset:	 2.8083863258361816
2022-11-09 18:40:11,695:INFO: Average validation o:	ADE  2.8084	FDE  4.7055
2022-11-09 18:40:11,696:INFO: - Computing ADE (validation)
2022-11-09 18:40:11,982:INFO: 		 ADE on hotel                     dataset:	 1.0091801881790161
2022-11-09 18:40:12,298:INFO: 		 ADE on univ                      dataset:	 1.2646763324737549
2022-11-09 18:40:12,582:INFO: 		 ADE on zara1                     dataset:	 1.927001953125
2022-11-09 18:40:12,976:INFO: 		 ADE on zara2                     dataset:	 1.2034950256347656
2022-11-09 18:40:12,976:INFO: Average validation:	ADE  1.2667	FDE  2.3186
2022-11-09 18:40:12,977:INFO: - Computing ADE (training)
2022-11-09 18:40:13,342:INFO: 		 ADE on hotel                     dataset:	 1.307599663734436
2022-11-09 18:40:13,991:INFO: 		 ADE on univ                      dataset:	 1.1955256462097168
2022-11-09 18:40:14,465:INFO: 		 ADE on zara1                     dataset:	 1.9526562690734863
2022-11-09 18:40:15,168:INFO: 		 ADE on zara2                     dataset:	 1.405179500579834
2022-11-09 18:40:15,168:INFO: Average training:	ADE  1.2892	FDE  2.3600
2022-11-09 18:40:15,178:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_309.pth.tar
2022-11-09 18:40:15,178:INFO: 
===> EPOCH: 310 (P3)
2022-11-09 18:40:15,178:INFO: - Computing loss (training)
2022-11-09 18:40:16,109:INFO: Dataset: hotel               Batch: 1/4	Loss 84.7849 (84.7849)
2022-11-09 18:40:16,869:INFO: Dataset: hotel               Batch: 2/4	Loss 90.5804 (87.7869)
2022-11-09 18:40:17,745:INFO: Dataset: hotel               Batch: 3/4	Loss 81.6571 (85.6324)
2022-11-09 18:40:18,451:INFO: Dataset: hotel               Batch: 4/4	Loss 53.4505 (80.7499)
2022-11-09 18:40:19,583:INFO: Dataset: univ                Batch:  1/15	Loss 79.4326 (79.4326)
2022-11-09 18:40:20,336:INFO: Dataset: univ                Batch:  2/15	Loss 80.6038 (79.9943)
2022-11-09 18:40:21,093:INFO: Dataset: univ                Batch:  3/15	Loss 79.8988 (79.9639)
2022-11-09 18:40:21,838:INFO: Dataset: univ                Batch:  4/15	Loss 80.4807 (80.0907)
2022-11-09 18:40:22,576:INFO: Dataset: univ                Batch:  5/15	Loss 80.7469 (80.2084)
2022-11-09 18:40:23,337:INFO: Dataset: univ                Batch:  6/15	Loss 79.4387 (80.0725)
2022-11-09 18:40:24,086:INFO: Dataset: univ                Batch:  7/15	Loss 80.3225 (80.1093)
2022-11-09 18:40:24,834:INFO: Dataset: univ                Batch:  8/15	Loss 79.9379 (80.0871)
2022-11-09 18:40:25,588:INFO: Dataset: univ                Batch:  9/15	Loss 79.5726 (80.0243)
2022-11-09 18:40:26,354:INFO: Dataset: univ                Batch: 10/15	Loss 79.9097 (80.0118)
2022-11-09 18:40:27,130:INFO: Dataset: univ                Batch: 11/15	Loss 79.8847 (79.9993)
2022-11-09 18:40:27,891:INFO: Dataset: univ                Batch: 12/15	Loss 79.9666 (79.9966)
2022-11-09 18:40:28,651:INFO: Dataset: univ                Batch: 13/15	Loss 79.3762 (79.9511)
2022-11-09 18:40:29,437:INFO: Dataset: univ                Batch: 14/15	Loss 79.7722 (79.9378)
2022-11-09 18:40:30,116:INFO: Dataset: univ                Batch: 15/15	Loss 14.9050 (79.1705)
2022-11-09 18:40:31,138:INFO: Dataset: zara1               Batch: 1/8	Loss 82.7812 (82.7812)
2022-11-09 18:40:31,896:INFO: Dataset: zara1               Batch: 2/8	Loss 85.5447 (84.1801)
2022-11-09 18:40:32,642:INFO: Dataset: zara1               Batch: 3/8	Loss 83.0875 (83.7935)
2022-11-09 18:40:33,380:INFO: Dataset: zara1               Batch: 4/8	Loss 84.6067 (83.9832)
2022-11-09 18:40:34,124:INFO: Dataset: zara1               Batch: 5/8	Loss 83.2804 (83.8423)
2022-11-09 18:40:34,888:INFO: Dataset: zara1               Batch: 6/8	Loss 82.6534 (83.6370)
2022-11-09 18:40:35,631:INFO: Dataset: zara1               Batch: 7/8	Loss 83.1104 (83.5610)
2022-11-09 18:40:36,367:INFO: Dataset: zara1               Batch: 8/8	Loss 70.7705 (82.3426)
2022-11-09 18:40:37,497:INFO: Dataset: zara2               Batch:  1/18	Loss 83.6964 (83.6964)
2022-11-09 18:40:38,275:INFO: Dataset: zara2               Batch:  2/18	Loss 84.2580 (83.9764)

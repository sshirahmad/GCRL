2022-10-31 18:50:28,409:INFO: Initializing Training Set
2022-10-31 18:50:32,452:INFO: Initializing Validation Set
2022-10-31 18:50:32,925:INFO: Initializing Validation O Set
2022-10-31 18:50:35,405:INFO: 
===> EPOCH: 251 (P3)
2022-10-31 18:50:35,406:INFO: - Computing loss (training)
2022-10-31 18:50:42,891:INFO: Dataset: hotel               Batch: 1/8	Loss 5.5801 (5.5801)
2022-10-31 18:50:43,344:INFO: Dataset: hotel               Batch: 2/8	Loss 4.7765 (5.1387)
2022-10-31 18:50:43,722:INFO: Dataset: hotel               Batch: 3/8	Loss 5.4042 (5.2218)
2022-10-31 18:50:44,061:INFO: Dataset: hotel               Batch: 4/8	Loss 4.9466 (5.1507)
2022-10-31 18:50:44,341:INFO: Dataset: hotel               Batch: 5/8	Loss 4.6816 (5.0565)
2022-10-31 18:50:44,607:INFO: Dataset: hotel               Batch: 6/8	Loss 3.8733 (4.8540)
2022-10-31 18:50:44,896:INFO: Dataset: hotel               Batch: 7/8	Loss 5.5558 (4.9525)
2022-10-31 18:50:45,103:INFO: Dataset: hotel               Batch: 8/8	Loss 1.3216 (4.8375)
2022-10-31 18:51:11,804:INFO: Dataset: univ                Batch:  1/29	Loss 6.6795 (6.6795)
2022-10-31 18:51:12,089:INFO: Dataset: univ                Batch:  2/29	Loss 4.9182 (5.6298)
2022-10-31 18:51:12,357:INFO: Dataset: univ                Batch:  3/29	Loss 5.1173 (5.4557)
2022-10-31 18:51:12,622:INFO: Dataset: univ                Batch:  4/29	Loss 6.0940 (5.6058)
2022-10-31 18:51:12,891:INFO: Dataset: univ                Batch:  5/29	Loss 6.0027 (5.6863)
2022-10-31 18:51:13,148:INFO: Dataset: univ                Batch:  6/29	Loss 5.2126 (5.6110)
2022-10-31 18:51:13,405:INFO: Dataset: univ                Batch:  7/29	Loss 5.5066 (5.5959)
2022-10-31 18:51:13,676:INFO: Dataset: univ                Batch:  8/29	Loss 5.2466 (5.5544)
2022-10-31 18:51:13,942:INFO: Dataset: univ                Batch:  9/29	Loss 6.4621 (5.6428)
2022-10-31 18:51:14,253:INFO: Dataset: univ                Batch: 10/29	Loss 4.7992 (5.5581)
2022-10-31 18:51:14,531:INFO: Dataset: univ                Batch: 11/29	Loss 6.3359 (5.6330)
2022-10-31 18:51:14,792:INFO: Dataset: univ                Batch: 12/29	Loss 4.5867 (5.5459)
2022-10-31 18:51:15,070:INFO: Dataset: univ                Batch: 13/29	Loss 5.4201 (5.5365)
2022-10-31 18:51:15,334:INFO: Dataset: univ                Batch: 14/29	Loss 5.1027 (5.5016)
2022-10-31 18:51:15,598:INFO: Dataset: univ                Batch: 15/29	Loss 4.3887 (5.4241)
2022-10-31 18:51:15,875:INFO: Dataset: univ                Batch: 16/29	Loss 4.5440 (5.3679)
2022-10-31 18:51:16,151:INFO: Dataset: univ                Batch: 17/29	Loss 6.9388 (5.4513)
2022-10-31 18:51:16,430:INFO: Dataset: univ                Batch: 18/29	Loss 6.9431 (5.5304)
2022-10-31 18:51:16,710:INFO: Dataset: univ                Batch: 19/29	Loss 4.3242 (5.4582)
2022-10-31 18:51:16,993:INFO: Dataset: univ                Batch: 20/29	Loss 5.0967 (5.4412)
2022-10-31 18:51:17,260:INFO: Dataset: univ                Batch: 21/29	Loss 5.9511 (5.4630)
2022-10-31 18:51:17,519:INFO: Dataset: univ                Batch: 22/29	Loss 6.6608 (5.5112)
2022-10-31 18:51:17,793:INFO: Dataset: univ                Batch: 23/29	Loss 5.1156 (5.4944)
2022-10-31 18:51:18,063:INFO: Dataset: univ                Batch: 24/29	Loss 4.3476 (5.4438)
2022-10-31 18:51:18,355:INFO: Dataset: univ                Batch: 25/29	Loss 5.3863 (5.4415)
2022-10-31 18:51:18,661:INFO: Dataset: univ                Batch: 26/29	Loss 4.2258 (5.3942)
2022-10-31 18:51:18,941:INFO: Dataset: univ                Batch: 27/29	Loss 5.8605 (5.4115)
2022-10-31 18:51:19,234:INFO: Dataset: univ                Batch: 28/29	Loss 6.7026 (5.4509)
2022-10-31 18:51:19,428:INFO: Dataset: univ                Batch: 29/29	Loss 1.4991 (5.3979)

2022-11-09 22:18:25,209:INFO: Initializing Training Set
2022-11-09 22:18:29,223:INFO: Initializing Validation Set
2022-11-09 22:18:29,708:INFO: Initializing Validation O Set
2022-11-09 22:18:32,226:INFO: => loaded checkpoint './models/E4/P3/CRMF_epoch_333.pth.tar' (epoch 334)
2022-11-09 22:18:32,227:INFO: 
===> EPOCH: 334 (P3)
2022-11-09 22:18:32,227:INFO: - Computing loss (training)
2022-11-09 22:18:44,474:INFO: Dataset: hotel               Batch: 1/8	Loss 40.9259 (40.9259)
2022-11-09 22:18:46,173:INFO: Dataset: hotel               Batch: 2/8	Loss 41.1671 (41.0441)
2022-11-09 22:18:47,957:INFO: Dataset: hotel               Batch: 3/8	Loss 42.1601 (41.4234)
2022-11-09 22:18:49,978:INFO: Dataset: hotel               Batch: 4/8	Loss 41.0344 (41.3192)
2022-11-09 22:18:51,836:INFO: Dataset: hotel               Batch: 5/8	Loss 42.1455 (41.4800)
2022-11-09 22:18:53,688:INFO: Dataset: hotel               Batch: 6/8	Loss 40.5968 (41.3267)
2022-11-09 22:18:55,863:INFO: Dataset: hotel               Batch: 7/8	Loss 39.8025 (41.1119)
2022-11-09 22:18:57,847:INFO: Dataset: hotel               Batch: 8/8	Loss 8.5889 (39.9535)
2022-11-09 22:19:24,282:INFO: Dataset: univ                Batch:  1/29	Loss 39.7251 (39.7251)
2022-11-09 22:19:26,412:INFO: Dataset: univ                Batch:  2/29	Loss 39.7361 (39.7306)
2022-11-09 22:19:28,578:INFO: Dataset: univ                Batch:  3/29	Loss 39.5665 (39.6753)
2022-11-09 22:19:30,594:INFO: Dataset: univ                Batch:  4/29	Loss 39.7023 (39.6820)
2022-11-09 22:19:32,528:INFO: Dataset: univ                Batch:  5/29	Loss 39.3828 (39.6227)
2022-11-09 22:19:34,616:INFO: Dataset: univ                Batch:  6/29	Loss 39.9232 (39.6666)
2022-11-09 22:19:36,772:INFO: Dataset: univ                Batch:  7/29	Loss 39.3614 (39.6255)
2022-11-09 22:19:39,000:INFO: Dataset: univ                Batch:  8/29	Loss 40.1702 (39.6878)
2022-11-09 22:19:41,152:INFO: Dataset: univ                Batch:  9/29	Loss 39.3329 (39.6457)
2022-11-09 22:19:43,264:INFO: Dataset: univ                Batch: 10/29	Loss 39.5183 (39.6317)
2022-11-09 22:19:45,419:INFO: Dataset: univ                Batch: 11/29	Loss 40.3563 (39.6977)
2022-11-09 22:19:47,497:INFO: Dataset: univ                Batch: 12/29	Loss 40.0442 (39.7231)
2022-11-09 22:19:49,521:INFO: Dataset: univ                Batch: 13/29	Loss 40.2340 (39.7599)
2022-11-09 22:19:51,592:INFO: Dataset: univ                Batch: 14/29	Loss 39.5128 (39.7431)
2022-11-09 22:19:53,610:INFO: Dataset: univ                Batch: 15/29	Loss 40.2452 (39.7780)
2022-11-09 22:19:55,614:INFO: Dataset: univ                Batch: 16/29	Loss 39.2789 (39.7429)
2022-11-09 22:19:57,682:INFO: Dataset: univ                Batch: 17/29	Loss 39.3663 (39.7209)
2022-11-09 22:19:59,747:INFO: Dataset: univ                Batch: 18/29	Loss 39.7449 (39.7222)
2022-11-09 22:20:01,769:INFO: Dataset: univ                Batch: 19/29	Loss 39.9780 (39.7342)
2022-11-09 22:20:03,849:INFO: Dataset: univ                Batch: 20/29	Loss 39.6810 (39.7315)
2022-11-09 22:20:05,853:INFO: Dataset: univ                Batch: 21/29	Loss 39.3318 (39.7127)
2022-11-09 22:20:07,803:INFO: Dataset: univ                Batch: 22/29	Loss 39.1606 (39.6847)
2022-11-09 22:20:09,825:INFO: Dataset: univ                Batch: 23/29	Loss 39.5081 (39.6770)
2022-11-09 22:20:11,870:INFO: Dataset: univ                Batch: 24/29	Loss 39.1331 (39.6527)
2022-11-09 22:20:14,090:INFO: Dataset: univ                Batch: 25/29	Loss 40.2959 (39.6773)
2022-11-09 22:20:16,551:INFO: Dataset: univ                Batch: 26/29	Loss 39.0023 (39.6511)
2022-11-09 22:20:18,778:INFO: Dataset: univ                Batch: 27/29	Loss 39.0622 (39.6281)
2022-11-09 22:20:20,907:INFO: Dataset: univ                Batch: 28/29	Loss 41.5300 (39.6840)
2022-11-09 22:20:22,813:INFO: Dataset: univ                Batch: 29/29	Loss 14.6319 (39.3385)
2022-11-09 22:20:33,004:INFO: Dataset: zara1               Batch:  1/16	Loss 45.5786 (45.5786)
2022-11-09 22:20:35,113:INFO: Dataset: zara1               Batch:  2/16	Loss 44.4488 (45.0499)
2022-11-09 22:20:37,283:INFO: Dataset: zara1               Batch:  3/16	Loss 40.5804 (43.5049)
2022-11-09 22:20:39,466:INFO: Dataset: zara1               Batch:  4/16	Loss 40.7364 (42.8762)
2022-11-09 22:20:41,853:INFO: Dataset: zara1               Batch:  5/16	Loss 40.2045 (42.3716)
2022-11-09 22:20:44,343:INFO: Dataset: zara1               Batch:  6/16	Loss 41.1266 (42.1604)
2022-11-09 22:20:46,624:INFO: Dataset: zara1               Batch:  7/16	Loss 41.8309 (42.1189)
2022-11-09 22:20:48,786:INFO: Dataset: zara1               Batch:  8/16	Loss 40.7610 (41.9769)
2022-11-09 22:20:50,854:INFO: Dataset: zara1               Batch:  9/16	Loss 39.9300 (41.7679)
2022-11-09 22:20:52,510:INFO: Dataset: zara1               Batch: 10/16	Loss 39.9947 (41.5945)
2022-11-09 22:20:54,253:INFO: Dataset: zara1               Batch: 11/16	Loss 40.1844 (41.4641)
2022-11-09 22:20:55,889:INFO: Dataset: zara1               Batch: 12/16	Loss 40.4172 (41.3814)
2022-11-09 22:20:57,605:INFO: Dataset: zara1               Batch: 13/16	Loss 40.3589 (41.3095)
2022-11-09 22:20:59,291:INFO: Dataset: zara1               Batch: 14/16	Loss 40.1096 (41.2274)
2022-11-09 22:21:01,015:INFO: Dataset: zara1               Batch: 15/16	Loss 40.6978 (41.1929)
2022-11-09 22:21:02,642:INFO: Dataset: zara1               Batch: 16/16	Loss 29.1436 (40.6412)
2022-11-09 22:21:27,958:INFO: Dataset: zara2               Batch:  1/36	Loss 41.2786 (41.2786)
2022-11-09 22:21:29,670:INFO: Dataset: zara2               Batch:  2/36	Loss 39.8207 (40.5178)
2022-11-09 22:21:31,379:INFO: Dataset: zara2               Batch:  3/36	Loss 40.6520 (40.5691)
2022-11-09 22:21:33,038:INFO: Dataset: zara2               Batch:  4/36	Loss 40.6613 (40.5927)
2022-11-09 22:21:34,913:INFO: Dataset: zara2               Batch:  5/36	Loss 40.0622 (40.4851)
2022-11-09 22:21:36,865:INFO: Dataset: zara2               Batch:  6/36	Loss 40.5082 (40.4890)
2022-11-09 22:21:38,544:INFO: Dataset: zara2               Batch:  7/36	Loss 39.3612 (40.3170)
2022-11-09 22:21:40,316:INFO: Dataset: zara2               Batch:  8/36	Loss 39.2470 (40.1832)
2022-11-09 22:21:42,042:INFO: Dataset: zara2               Batch:  9/36	Loss 39.1301 (40.0727)
2022-11-09 22:21:43,784:INFO: Dataset: zara2               Batch: 10/36	Loss 39.6832 (40.0335)
2022-11-09 22:21:45,428:INFO: Dataset: zara2               Batch: 11/36	Loss 38.9035 (39.9361)
2022-11-09 22:21:47,114:INFO: Dataset: zara2               Batch: 12/36	Loss 39.1131 (39.8704)
2022-11-09 22:21:48,780:INFO: Dataset: zara2               Batch: 13/36	Loss 39.5668 (39.8499)
2022-11-09 22:21:50,508:INFO: Dataset: zara2               Batch: 14/36	Loss 39.2783 (39.8130)
2022-11-09 22:21:52,153:INFO: Dataset: zara2               Batch: 15/36	Loss 39.2326 (39.7720)
2022-11-09 22:21:53,860:INFO: Dataset: zara2               Batch: 16/36	Loss 39.4636 (39.7552)
2022-11-09 22:21:55,504:INFO: Dataset: zara2               Batch: 17/36	Loss 39.2354 (39.7229)
2022-11-09 22:21:57,209:INFO: Dataset: zara2               Batch: 18/36	Loss 39.5294 (39.7128)
2022-11-09 22:21:58,894:INFO: Dataset: zara2               Batch: 19/36	Loss 39.4522 (39.6994)
2022-11-09 22:22:00,633:INFO: Dataset: zara2               Batch: 20/36	Loss 39.0744 (39.6675)
2022-11-09 22:22:02,326:INFO: Dataset: zara2               Batch: 21/36	Loss 39.3202 (39.6508)
2022-11-09 22:22:04,062:INFO: Dataset: zara2               Batch: 22/36	Loss 39.6906 (39.6523)
2022-11-09 22:22:05,698:INFO: Dataset: zara2               Batch: 23/36	Loss 39.3063 (39.6361)
2022-11-09 22:22:07,402:INFO: Dataset: zara2               Batch: 24/36	Loss 39.7556 (39.6411)
2022-11-09 22:22:09,037:INFO: Dataset: zara2               Batch: 25/36	Loss 38.7834 (39.6092)
2022-11-09 22:22:10,763:INFO: Dataset: zara2               Batch: 26/36	Loss 38.6659 (39.5704)
2022-11-09 22:22:12,415:INFO: Dataset: zara2               Batch: 27/36	Loss 38.9189 (39.5471)
2022-11-09 22:22:14,130:INFO: Dataset: zara2               Batch: 28/36	Loss 39.3358 (39.5395)
2022-11-09 22:22:15,799:INFO: Dataset: zara2               Batch: 29/36	Loss 39.2584 (39.5300)
2022-11-09 22:22:17,705:INFO: Dataset: zara2               Batch: 30/36	Loss 39.4897 (39.5287)
2022-11-09 22:22:19,451:INFO: Dataset: zara2               Batch: 31/36	Loss 39.2918 (39.5215)
2022-11-09 22:22:21,171:INFO: Dataset: zara2               Batch: 32/36	Loss 39.0408 (39.5070)
2022-11-09 22:22:22,933:INFO: Dataset: zara2               Batch: 33/36	Loss 39.0983 (39.4942)
2022-11-09 22:22:24,894:INFO: Dataset: zara2               Batch: 34/36	Loss 39.3725 (39.4903)
2022-11-09 22:22:27,089:INFO: Dataset: zara2               Batch: 35/36	Loss 39.2675 (39.4843)
2022-11-09 22:22:29,063:INFO: Dataset: zara2               Batch: 36/36	Loss 28.9250 (39.2749)
2022-11-09 22:22:30,015:INFO: - Computing ADE (validation o)
2022-11-09 22:22:38,058:INFO: 		 ADE on eth                       dataset:	 2.357419013977051
2022-11-09 22:22:38,058:INFO: Average validation o:	ADE  2.3574	FDE  3.7497
2022-11-09 22:22:38,059:INFO: - Computing ADE (validation)
2022-11-09 22:22:46,858:INFO: 		 ADE on hotel                     dataset:	 1.1206063032150269
2022-11-09 22:22:55,768:INFO: 		 ADE on univ                      dataset:	 1.1463267803192139
2022-11-09 22:23:05,678:INFO: 		 ADE on zara1                     dataset:	 1.3969944715499878
2022-11-09 22:23:14,887:INFO: 		 ADE on zara2                     dataset:	 1.0119789838790894
2022-11-09 22:23:14,887:INFO: Average validation:	ADE  1.1102	FDE  2.0027
2022-11-09 22:23:14,892:INFO: - Computing ADE (training)
2022-11-09 22:23:24,033:INFO: 		 ADE on hotel                     dataset:	 1.372023344039917
2022-11-09 22:23:49,637:INFO: 		 ADE on univ                      dataset:	 1.068102478981018
2022-11-09 22:24:00,097:INFO: 		 ADE on zara1                     dataset:	 1.5266953706741333
2022-11-09 22:24:28,585:INFO: 		 ADE on zara2                     dataset:	 1.1539984941482544
2022-11-09 22:24:28,585:INFO: Average training:	ADE  1.1225	FDE  2.0213
2022-11-09 22:24:28,613:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_334.pth.tar
2022-11-09 22:24:28,613:INFO: 
===> EPOCH: 335 (P3)
2022-11-09 22:24:28,614:INFO: - Computing loss (training)
2022-11-09 22:24:37,849:INFO: Dataset: hotel               Batch: 1/8	Loss 40.4220 (40.4220)
2022-11-09 22:24:39,921:INFO: Dataset: hotel               Batch: 2/8	Loss 41.4232 (40.9494)
2022-11-09 22:24:41,937:INFO: Dataset: hotel               Batch: 3/8	Loss 40.5629 (40.8236)
2022-11-09 22:24:44,230:INFO: Dataset: hotel               Batch: 4/8	Loss 40.1107 (40.6329)
2022-11-09 22:24:46,427:INFO: Dataset: hotel               Batch: 5/8	Loss 40.3023 (40.5651)
2022-11-09 22:24:48,336:INFO: Dataset: hotel               Batch: 6/8	Loss 40.7009 (40.5886)
2022-11-09 22:24:50,838:INFO: Dataset: hotel               Batch: 7/8	Loss 40.4694 (40.5715)
2022-11-09 22:24:53,047:INFO: Dataset: hotel               Batch: 8/8	Loss 8.8040 (39.6914)
2022-11-09 22:25:20,028:INFO: Dataset: univ                Batch:  1/29	Loss 39.4993 (39.4993)
2022-11-09 22:25:22,721:INFO: Dataset: univ                Batch:  2/29	Loss 39.5555 (39.5266)
2022-11-09 22:25:24,917:INFO: Dataset: univ                Batch:  3/29	Loss 39.0710 (39.3738)
2022-11-09 22:25:27,045:INFO: Dataset: univ                Batch:  4/29	Loss 38.8970 (39.2488)
2022-11-09 22:25:29,093:INFO: Dataset: univ                Batch:  5/29	Loss 39.0737 (39.2141)
2022-11-09 22:25:31,397:INFO: Dataset: univ                Batch:  6/29	Loss 39.3053 (39.2302)
2022-11-09 22:25:33,354:INFO: Dataset: univ                Batch:  7/29	Loss 40.0685 (39.3267)
2022-11-09 22:25:35,492:INFO: Dataset: univ                Batch:  8/29	Loss 39.2080 (39.3112)
2022-11-09 22:25:37,423:INFO: Dataset: univ                Batch:  9/29	Loss 39.4486 (39.3253)
2022-11-09 22:25:39,462:INFO: Dataset: univ                Batch: 10/29	Loss 39.7650 (39.3652)
2022-11-09 22:25:41,504:INFO: Dataset: univ                Batch: 11/29	Loss 39.2985 (39.3587)
2022-11-09 22:25:43,277:INFO: Dataset: univ                Batch: 12/29	Loss 39.2851 (39.3530)
2022-11-09 22:25:45,047:INFO: Dataset: univ                Batch: 13/29	Loss 39.8237 (39.3898)
2022-11-09 22:25:46,729:INFO: Dataset: univ                Batch: 14/29	Loss 38.9227 (39.3563)
2022-11-09 22:25:48,388:INFO: Dataset: univ                Batch: 15/29	Loss 39.1772 (39.3426)
2022-11-09 22:25:50,020:INFO: Dataset: univ                Batch: 16/29	Loss 39.3878 (39.3453)
2022-11-09 22:25:51,655:INFO: Dataset: univ                Batch: 17/29	Loss 39.5714 (39.3574)
2022-11-09 22:25:53,295:INFO: Dataset: univ                Batch: 18/29	Loss 39.3468 (39.3568)
2022-11-09 22:25:55,012:INFO: Dataset: univ                Batch: 19/29	Loss 39.4616 (39.3618)
2022-11-09 22:25:57,046:INFO: Dataset: univ                Batch: 20/29	Loss 39.8551 (39.3847)
2022-11-09 22:25:58,963:INFO: Dataset: univ                Batch: 21/29	Loss 39.3457 (39.3826)
2022-11-09 22:26:00,704:INFO: Dataset: univ                Batch: 22/29	Loss 39.3038 (39.3786)
2022-11-09 22:26:02,549:INFO: Dataset: univ                Batch: 23/29	Loss 39.4245 (39.3807)
2022-11-09 22:26:04,272:INFO: Dataset: univ                Batch: 24/29	Loss 39.3372 (39.3787)
2022-11-09 22:26:06,263:INFO: Dataset: univ                Batch: 25/29	Loss 39.2619 (39.3741)
2022-11-09 22:26:08,171:INFO: Dataset: univ                Batch: 26/29	Loss 39.2508 (39.3695)
2022-11-09 22:26:09,807:INFO: Dataset: univ                Batch: 27/29	Loss 39.0109 (39.3560)
2022-11-09 22:26:11,445:INFO: Dataset: univ                Batch: 28/29	Loss 39.0101 (39.3414)
2022-11-09 22:26:13,043:INFO: Dataset: univ                Batch: 29/29	Loss 14.5863 (39.0528)
2022-11-09 22:26:24,516:INFO: Dataset: zara1               Batch:  1/16	Loss 43.6377 (43.6377)
2022-11-09 22:26:27,836:INFO: Dataset: zara1               Batch:  2/16	Loss 42.5968 (43.1034)
2022-11-09 22:26:29,730:INFO: Dataset: zara1               Batch:  3/16	Loss 41.1087 (42.3786)
2022-11-09 22:26:31,501:INFO: Dataset: zara1               Batch:  4/16	Loss 39.7815 (41.6108)
2022-11-09 22:26:33,528:INFO: Dataset: zara1               Batch:  5/16	Loss 41.1241 (41.5204)
2022-11-09 22:26:35,493:INFO: Dataset: zara1               Batch:  6/16	Loss 42.9171 (41.7519)
2022-11-09 22:26:37,656:INFO: Dataset: zara1               Batch:  7/16	Loss 39.7550 (41.4779)
2022-11-09 22:26:39,459:INFO: Dataset: zara1               Batch:  8/16	Loss 40.2155 (41.3144)
2022-11-09 22:26:41,222:INFO: Dataset: zara1               Batch:  9/16	Loss 40.0385 (41.1731)
2022-11-09 22:26:43,084:INFO: Dataset: zara1               Batch: 10/16	Loss 40.4475 (41.1077)
2022-11-09 22:26:44,770:INFO: Dataset: zara1               Batch: 11/16	Loss 40.0960 (41.0158)
2022-11-09 22:26:46,537:INFO: Dataset: zara1               Batch: 12/16	Loss 39.4080 (40.8601)
2022-11-09 22:26:48,718:INFO: Dataset: zara1               Batch: 13/16	Loss 39.6661 (40.7737)
2022-11-09 22:26:50,868:INFO: Dataset: zara1               Batch: 14/16	Loss 39.9654 (40.7231)
2022-11-09 22:26:52,773:INFO: Dataset: zara1               Batch: 15/16	Loss 39.8295 (40.6728)
2022-11-09 22:26:54,450:INFO: Dataset: zara1               Batch: 16/16	Loss 28.7694 (40.1215)
2022-11-09 22:27:20,486:INFO: Dataset: zara2               Batch:  1/36	Loss 41.4676 (41.4676)
2022-11-09 22:27:22,146:INFO: Dataset: zara2               Batch:  2/36	Loss 40.0422 (40.7412)
2022-11-09 22:27:23,839:INFO: Dataset: zara2               Batch:  3/36	Loss 39.7767 (40.4115)
2022-11-09 22:27:25,541:INFO: Dataset: zara2               Batch:  4/36	Loss 39.9678 (40.2985)
2022-11-09 22:27:27,282:INFO: Dataset: zara2               Batch:  5/36	Loss 39.2261 (40.0650)
2022-11-09 22:27:28,970:INFO: Dataset: zara2               Batch:  6/36	Loss 38.9244 (39.8914)
2022-11-09 22:27:30,696:INFO: Dataset: zara2               Batch:  7/36	Loss 39.3362 (39.8120)
2022-11-09 22:27:32,382:INFO: Dataset: zara2               Batch:  8/36	Loss 39.3668 (39.7566)
2022-11-09 22:27:34,097:INFO: Dataset: zara2               Batch:  9/36	Loss 38.9490 (39.6613)
2022-11-09 22:27:35,819:INFO: Dataset: zara2               Batch: 10/36	Loss 38.9832 (39.5820)
2022-11-09 22:27:37,525:INFO: Dataset: zara2               Batch: 11/36	Loss 39.4128 (39.5650)
2022-11-09 22:27:39,172:INFO: Dataset: zara2               Batch: 12/36	Loss 39.1828 (39.5307)
2022-11-09 22:27:40,845:INFO: Dataset: zara2               Batch: 13/36	Loss 39.0225 (39.4909)
2022-11-09 22:27:42,495:INFO: Dataset: zara2               Batch: 14/36	Loss 38.8933 (39.4385)
2022-11-09 22:27:44,174:INFO: Dataset: zara2               Batch: 15/36	Loss 39.0715 (39.4130)
2022-11-09 22:27:45,904:INFO: Dataset: zara2               Batch: 16/36	Loss 39.1059 (39.3937)
2022-11-09 22:27:47,659:INFO: Dataset: zara2               Batch: 17/36	Loss 39.1455 (39.3788)
2022-11-09 22:27:49,443:INFO: Dataset: zara2               Batch: 18/36	Loss 39.0281 (39.3604)
2022-11-09 22:27:51,100:INFO: Dataset: zara2               Batch: 19/36	Loss 38.7596 (39.3266)
2022-11-09 22:27:52,803:INFO: Dataset: zara2               Batch: 20/36	Loss 39.6830 (39.3435)
2022-11-09 22:27:54,469:INFO: Dataset: zara2               Batch: 21/36	Loss 39.4682 (39.3501)
2022-11-09 22:27:56,199:INFO: Dataset: zara2               Batch: 22/36	Loss 39.3778 (39.3514)
2022-11-09 22:27:57,862:INFO: Dataset: zara2               Batch: 23/36	Loss 40.5213 (39.4065)
2022-11-09 22:27:59,592:INFO: Dataset: zara2               Batch: 24/36	Loss 39.4868 (39.4095)
2022-11-09 22:28:01,279:INFO: Dataset: zara2               Batch: 25/36	Loss 39.4484 (39.4109)
2022-11-09 22:28:03,049:INFO: Dataset: zara2               Batch: 26/36	Loss 38.9126 (39.3938)
2022-11-09 22:28:04,747:INFO: Dataset: zara2               Batch: 27/36	Loss 38.8749 (39.3754)
2022-11-09 22:28:06,507:INFO: Dataset: zara2               Batch: 28/36	Loss 39.3639 (39.3751)
2022-11-09 22:28:08,147:INFO: Dataset: zara2               Batch: 29/36	Loss 39.7155 (39.3862)
2022-11-09 22:28:09,873:INFO: Dataset: zara2               Batch: 30/36	Loss 39.0371 (39.3741)
2022-11-09 22:28:11,592:INFO: Dataset: zara2               Batch: 31/36	Loss 39.1510 (39.3668)
2022-11-09 22:28:13,232:INFO: Dataset: zara2               Batch: 32/36	Loss 39.2805 (39.3642)
2022-11-09 22:28:14,947:INFO: Dataset: zara2               Batch: 33/36	Loss 38.9552 (39.3526)
2022-11-09 22:28:16,652:INFO: Dataset: zara2               Batch: 34/36	Loss 39.0462 (39.3430)
2022-11-09 22:28:18,412:INFO: Dataset: zara2               Batch: 35/36	Loss 38.9987 (39.3339)
2022-11-09 22:28:20,026:INFO: Dataset: zara2               Batch: 36/36	Loss 27.9436 (39.1023)
2022-11-09 22:28:20,962:INFO: - Computing ADE (validation o)
2022-11-09 22:28:29,277:INFO: 		 ADE on eth                       dataset:	 2.206915855407715
2022-11-09 22:28:29,279:INFO: Average validation o:	ADE  2.2069	FDE  3.4213
2022-11-09 22:28:29,280:INFO: - Computing ADE (validation)
2022-11-09 22:28:38,139:INFO: 		 ADE on hotel                     dataset:	 1.0957458019256592
2022-11-09 22:28:47,282:INFO: 		 ADE on univ                      dataset:	 1.1159114837646484
2022-11-09 22:28:56,300:INFO: 		 ADE on zara1                     dataset:	 1.358333706855774
2022-11-09 22:29:05,571:INFO: 		 ADE on zara2                     dataset:	 0.9736464023590088
2022-11-09 22:29:05,572:INFO: Average validation:	ADE  1.0767	FDE  1.9398
2022-11-09 22:29:05,572:INFO: - Computing ADE (training)
2022-11-09 22:29:14,816:INFO: 		 ADE on hotel                     dataset:	 1.3808329105377197
2022-11-09 22:29:39,466:INFO: 		 ADE on univ                      dataset:	 1.0539617538452148
2022-11-09 22:29:48,790:INFO: 		 ADE on zara1                     dataset:	 1.4449318647384644
2022-11-09 22:30:14,431:INFO: 		 ADE on zara2                     dataset:	 1.1206035614013672
2022-11-09 22:30:14,431:INFO: Average training:	ADE  1.1007	FDE  1.9821
2022-11-09 22:30:14,473:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_335.pth.tar
2022-11-09 22:30:14,474:INFO: 
===> EPOCH: 336 (P3)
2022-11-09 22:30:14,475:INFO: - Computing loss (training)
2022-11-09 22:30:25,298:INFO: Dataset: hotel               Batch: 1/8	Loss 40.8909 (40.8909)
2022-11-09 22:30:27,039:INFO: Dataset: hotel               Batch: 2/8	Loss 39.8356 (40.3120)
2022-11-09 22:30:28,847:INFO: Dataset: hotel               Batch: 3/8	Loss 39.6674 (40.0916)
2022-11-09 22:30:30,712:INFO: Dataset: hotel               Batch: 4/8	Loss 40.7336 (40.2563)
2022-11-09 22:30:32,797:INFO: Dataset: hotel               Batch: 5/8	Loss 41.7197 (40.5304)
2022-11-09 22:30:34,660:INFO: Dataset: hotel               Batch: 6/8	Loss 40.3302 (40.4969)
2022-11-09 22:30:36,407:INFO: Dataset: hotel               Batch: 7/8	Loss 41.0488 (40.5798)
2022-11-09 22:30:38,239:INFO: Dataset: hotel               Batch: 8/8	Loss 8.6704 (39.4853)
2022-11-09 22:31:07,239:INFO: Dataset: univ                Batch:  1/29	Loss 39.0275 (39.0275)
2022-11-09 22:31:08,952:INFO: Dataset: univ                Batch:  2/29	Loss 43.4377 (41.2840)
2022-11-09 22:31:10,606:INFO: Dataset: univ                Batch:  3/29	Loss 39.8640 (40.8301)
2022-11-09 22:31:12,217:INFO: Dataset: univ                Batch:  4/29	Loss 39.2706 (40.4505)
2022-11-09 22:31:13,851:INFO: Dataset: univ                Batch:  5/29	Loss 39.4632 (40.2524)
2022-11-09 22:31:15,540:INFO: Dataset: univ                Batch:  6/29	Loss 39.8647 (40.2041)
2022-11-09 22:31:17,204:INFO: Dataset: univ                Batch:  7/29	Loss 39.7523 (40.1427)
2022-11-09 22:31:18,877:INFO: Dataset: univ                Batch:  8/29	Loss 39.8963 (40.1158)
2022-11-09 22:31:20,924:INFO: Dataset: univ                Batch:  9/29	Loss 39.5004 (40.0507)
2022-11-09 22:31:22,501:INFO: Dataset: univ                Batch: 10/29	Loss 39.5377 (40.0020)
2022-11-09 22:31:24,145:INFO: Dataset: univ                Batch: 11/29	Loss 39.8267 (39.9876)
2022-11-09 22:31:25,947:INFO: Dataset: univ                Batch: 12/29	Loss 39.6376 (39.9617)
2022-11-09 22:31:27,760:INFO: Dataset: univ                Batch: 13/29	Loss 39.4713 (39.9236)
2022-11-09 22:31:30,102:INFO: Dataset: univ                Batch: 14/29	Loss 39.9017 (39.9221)
2022-11-09 22:31:32,108:INFO: Dataset: univ                Batch: 15/29	Loss 39.1788 (39.8712)
2022-11-09 22:31:34,999:INFO: Dataset: univ                Batch: 16/29	Loss 39.4143 (39.8439)
2022-11-09 22:31:36,731:INFO: Dataset: univ                Batch: 17/29	Loss 39.2439 (39.8096)
2022-11-09 22:31:38,560:INFO: Dataset: univ                Batch: 18/29	Loss 39.4022 (39.7839)
2022-11-09 22:31:40,329:INFO: Dataset: univ                Batch: 19/29	Loss 39.7682 (39.7831)
2022-11-09 22:31:41,974:INFO: Dataset: univ                Batch: 20/29	Loss 40.0070 (39.7936)
2022-11-09 22:31:43,916:INFO: Dataset: univ                Batch: 21/29	Loss 39.2079 (39.7598)
2022-11-09 22:31:45,953:INFO: Dataset: univ                Batch: 22/29	Loss 39.0877 (39.7267)
2022-11-09 22:31:48,770:INFO: Dataset: univ                Batch: 23/29	Loss 39.5751 (39.7203)
2022-11-09 22:31:51,024:INFO: Dataset: univ                Batch: 24/29	Loss 39.1721 (39.6981)
2022-11-09 22:31:53,423:INFO: Dataset: univ                Batch: 25/29	Loss 39.5787 (39.6937)
2022-11-09 22:31:55,134:INFO: Dataset: univ                Batch: 26/29	Loss 39.1754 (39.6698)
2022-11-09 22:31:56,817:INFO: Dataset: univ                Batch: 27/29	Loss 39.6047 (39.6674)
2022-11-09 22:31:59,079:INFO: Dataset: univ                Batch: 28/29	Loss 39.5208 (39.6618)
2022-11-09 22:32:00,854:INFO: Dataset: univ                Batch: 29/29	Loss 15.1314 (39.3816)
2022-11-09 22:32:11,655:INFO: Dataset: zara1               Batch:  1/16	Loss 40.7610 (40.7610)
2022-11-09 22:32:13,279:INFO: Dataset: zara1               Batch:  2/16	Loss 40.4837 (40.6412)
2022-11-09 22:32:15,011:INFO: Dataset: zara1               Batch:  3/16	Loss 42.9230 (41.3164)
2022-11-09 22:32:16,669:INFO: Dataset: zara1               Batch:  4/16	Loss 43.4510 (41.8770)
2022-11-09 22:32:18,442:INFO: Dataset: zara1               Batch:  5/16	Loss 40.8812 (41.6337)
2022-11-09 22:32:20,139:INFO: Dataset: zara1               Batch:  6/16	Loss 40.8270 (41.5186)
2022-11-09 22:32:22,966:INFO: Dataset: zara1               Batch:  7/16	Loss 39.7430 (41.2987)
2022-11-09 22:32:24,840:INFO: Dataset: zara1               Batch:  8/16	Loss 40.0226 (41.1627)
2022-11-09 22:32:26,663:INFO: Dataset: zara1               Batch:  9/16	Loss 39.3518 (40.9689)
2022-11-09 22:32:28,504:INFO: Dataset: zara1               Batch: 10/16	Loss 40.0694 (40.8792)
2022-11-09 22:32:30,709:INFO: Dataset: zara1               Batch: 11/16	Loss 40.2381 (40.8216)
2022-11-09 22:32:32,514:INFO: Dataset: zara1               Batch: 12/16	Loss 40.1748 (40.7764)
2022-11-09 22:32:34,430:INFO: Dataset: zara1               Batch: 13/16	Loss 40.2597 (40.7290)
2022-11-09 22:32:36,316:INFO: Dataset: zara1               Batch: 14/16	Loss 40.1209 (40.6850)
2022-11-09 22:32:38,208:INFO: Dataset: zara1               Batch: 15/16	Loss 39.9919 (40.6419)
2022-11-09 22:32:40,122:INFO: Dataset: zara1               Batch: 16/16	Loss 28.3762 (40.0932)
2022-11-09 22:33:06,797:INFO: Dataset: zara2               Batch:  1/36	Loss 40.3791 (40.3791)
2022-11-09 22:33:08,443:INFO: Dataset: zara2               Batch:  2/36	Loss 39.8950 (40.1195)
2022-11-09 22:33:10,104:INFO: Dataset: zara2               Batch:  3/36	Loss 39.1197 (39.8053)
2022-11-09 22:33:11,716:INFO: Dataset: zara2               Batch:  4/36	Loss 41.3671 (40.1854)
2022-11-09 22:33:13,411:INFO: Dataset: zara2               Batch:  5/36	Loss 39.5630 (40.0816)
2022-11-09 22:33:15,036:INFO: Dataset: zara2               Batch:  6/36	Loss 39.2899 (39.9494)
2022-11-09 22:33:16,756:INFO: Dataset: zara2               Batch:  7/36	Loss 39.3357 (39.8760)
2022-11-09 22:33:18,494:INFO: Dataset: zara2               Batch:  8/36	Loss 39.2123 (39.7858)
2022-11-09 22:33:20,608:INFO: Dataset: zara2               Batch:  9/36	Loss 39.3184 (39.7342)
2022-11-09 22:33:22,601:INFO: Dataset: zara2               Batch: 10/36	Loss 38.7683 (39.6423)
2022-11-09 22:33:24,244:INFO: Dataset: zara2               Batch: 11/36	Loss 39.2879 (39.6062)
2022-11-09 22:33:26,267:INFO: Dataset: zara2               Batch: 12/36	Loss 38.9934 (39.5514)
2022-11-09 22:33:28,115:INFO: Dataset: zara2               Batch: 13/36	Loss 38.9530 (39.5054)
2022-11-09 22:33:29,979:INFO: Dataset: zara2               Batch: 14/36	Loss 39.9087 (39.5323)
2022-11-09 22:33:32,112:INFO: Dataset: zara2               Batch: 15/36	Loss 39.0507 (39.5003)
2022-11-09 22:33:33,881:INFO: Dataset: zara2               Batch: 16/36	Loss 38.8588 (39.4583)
2022-11-09 22:33:35,696:INFO: Dataset: zara2               Batch: 17/36	Loss 38.9714 (39.4278)
2022-11-09 22:33:37,993:INFO: Dataset: zara2               Batch: 18/36	Loss 39.6798 (39.4418)
2022-11-09 22:33:40,693:INFO: Dataset: zara2               Batch: 19/36	Loss 39.9156 (39.4675)
2022-11-09 22:33:42,497:INFO: Dataset: zara2               Batch: 20/36	Loss 39.9074 (39.4854)
2022-11-09 22:33:44,501:INFO: Dataset: zara2               Batch: 21/36	Loss 39.3313 (39.4781)
2022-11-09 22:33:46,560:INFO: Dataset: zara2               Batch: 22/36	Loss 39.4855 (39.4784)
2022-11-09 22:33:48,619:INFO: Dataset: zara2               Batch: 23/36	Loss 39.0415 (39.4592)
2022-11-09 22:33:50,803:INFO: Dataset: zara2               Batch: 24/36	Loss 39.3248 (39.4535)
2022-11-09 22:33:52,994:INFO: Dataset: zara2               Batch: 25/36	Loss 39.2699 (39.4458)
2022-11-09 22:33:55,307:INFO: Dataset: zara2               Batch: 26/36	Loss 39.8671 (39.4626)
2022-11-09 22:33:57,319:INFO: Dataset: zara2               Batch: 27/36	Loss 39.0987 (39.4490)
2022-11-09 22:33:59,325:INFO: Dataset: zara2               Batch: 28/36	Loss 39.1543 (39.4399)
2022-11-09 22:34:01,340:INFO: Dataset: zara2               Batch: 29/36	Loss 39.0869 (39.4277)
2022-11-09 22:34:03,390:INFO: Dataset: zara2               Batch: 30/36	Loss 38.8958 (39.4092)
2022-11-09 22:34:05,305:INFO: Dataset: zara2               Batch: 31/36	Loss 39.2272 (39.4025)
2022-11-09 22:34:07,350:INFO: Dataset: zara2               Batch: 32/36	Loss 39.1161 (39.3948)
2022-11-09 22:34:09,464:INFO: Dataset: zara2               Batch: 33/36	Loss 39.7539 (39.4044)
2022-11-09 22:34:11,652:INFO: Dataset: zara2               Batch: 34/36	Loss 39.3386 (39.4023)
2022-11-09 22:34:13,689:INFO: Dataset: zara2               Batch: 35/36	Loss 39.0066 (39.3919)
2022-11-09 22:34:15,472:INFO: Dataset: zara2               Batch: 36/36	Loss 28.0523 (39.1857)
2022-11-09 22:34:16,472:INFO: - Computing ADE (validation o)
2022-11-09 22:34:25,413:INFO: 		 ADE on eth                       dataset:	 2.257892370223999
2022-11-09 22:34:25,413:INFO: Average validation o:	ADE  2.2579	FDE  3.4943
2022-11-09 22:34:25,414:INFO: - Computing ADE (validation)
2022-11-09 22:34:34,618:INFO: 		 ADE on hotel                     dataset:	 1.1231352090835571
2022-11-09 22:34:45,533:INFO: 		 ADE on univ                      dataset:	 1.1370295286178589
2022-11-09 22:34:55,668:INFO: 		 ADE on zara1                     dataset:	 1.4485069513320923
2022-11-09 22:35:05,642:INFO: 		 ADE on zara2                     dataset:	 1.0288362503051758
2022-11-09 22:35:05,642:INFO: Average validation:	ADE  1.1147	FDE  2.0275
2022-11-09 22:35:05,644:INFO: - Computing ADE (training)
2022-11-09 22:35:17,860:INFO: 		 ADE on hotel                     dataset:	 1.3801015615463257
2022-11-09 22:35:44,123:INFO: 		 ADE on univ                      dataset:	 1.0828320980072021
2022-11-09 22:35:55,458:INFO: 		 ADE on zara1                     dataset:	 1.4945294857025146
2022-11-09 22:36:23,044:INFO: 		 ADE on zara2                     dataset:	 1.1396936178207397
2022-11-09 22:36:23,044:INFO: Average training:	ADE  1.1282	FDE  2.0492
2022-11-09 22:36:23,094:INFO:  --> Model Saved in ./models/E4//P3/CRMF_epoch_336.pth.tar
2022-11-09 22:36:23,095:INFO: 
===> EPOCH: 337 (P3)
2022-11-09 22:36:23,096:INFO: - Computing loss (training)
2022-11-09 22:36:35,479:INFO: Dataset: hotel               Batch: 1/8	Loss 40.1286 (40.1286)
2022-11-09 22:36:37,960:INFO: Dataset: hotel               Batch: 2/8	Loss 40.4343 (40.2893)
2022-11-09 22:36:40,207:INFO: Dataset: hotel               Batch: 3/8	Loss 39.9856 (40.1859)
2022-11-09 22:36:42,377:INFO: Dataset: hotel               Batch: 4/8	Loss 40.4639 (40.2546)
2022-11-09 22:36:44,496:INFO: Dataset: hotel               Batch: 5/8	Loss 40.9115 (40.3808)
2022-11-09 22:36:46,822:INFO: Dataset: hotel               Batch: 6/8	Loss 41.4251 (40.5435)
2022-11-09 22:36:48,964:INFO: Dataset: hotel               Batch: 7/8	Loss 40.4264 (40.5268)
2022-11-09 22:36:50,918:INFO: Dataset: hotel               Batch: 8/8	Loss 8.7192 (39.5197)
2022-11-09 22:37:19,409:INFO: Dataset: univ                Batch:  1/29	Loss 39.6967 (39.6967)
2022-11-09 22:37:21,492:INFO: Dataset: univ                Batch:  2/29	Loss 39.3410 (39.4993)
2022-11-09 22:37:23,406:INFO: Dataset: univ                Batch:  3/29	Loss 39.1395 (39.3741)
2022-11-09 22:37:25,491:INFO: Dataset: univ                Batch:  4/29	Loss 39.1700 (39.3216)
2022-11-09 22:37:27,453:INFO: Dataset: univ                Batch:  5/29	Loss 39.5358 (39.3664)
2022-11-09 22:37:29,487:INFO: Dataset: univ                Batch:  6/29	Loss 40.4447 (39.5254)
2022-11-09 22:37:31,559:INFO: Dataset: univ                Batch:  7/29	Loss 39.2175 (39.4758)
2022-11-09 22:37:33,598:INFO: Dataset: univ                Batch:  8/29	Loss 40.3252 (39.5737)
2022-11-09 22:37:35,604:INFO: Dataset: univ                Batch:  9/29	Loss 39.3547 (39.5467)
2022-11-09 22:37:37,657:INFO: Dataset: univ                Batch: 10/29	Loss 39.2052 (39.5118)
2022-11-09 22:37:39,727:INFO: Dataset: univ                Batch: 11/29	Loss 39.7387 (39.5322)
2022-11-09 22:37:41,760:INFO: Dataset: univ                Batch: 12/29	Loss 39.4887 (39.5285)
2022-11-09 22:37:43,912:INFO: Dataset: univ                Batch: 13/29	Loss 39.7600 (39.5479)
2022-11-09 22:37:45,924:INFO: Dataset: univ                Batch: 14/29	Loss 39.6555 (39.5562)
2022-11-09 22:37:47,879:INFO: Dataset: univ                Batch: 15/29	Loss 39.2350 (39.5326)
2022-11-09 22:37:49,943:INFO: Dataset: univ                Batch: 16/29	Loss 39.0940 (39.5045)
2022-11-09 22:37:52,034:INFO: Dataset: univ                Batch: 17/29	Loss 39.6462 (39.5120)
2022-11-09 22:37:54,144:INFO: Dataset: univ                Batch: 18/29	Loss 39.2883 (39.4977)
2022-11-09 22:37:56,282:INFO: Dataset: univ                Batch: 19/29	Loss 39.0994 (39.4773)
2022-11-09 22:37:58,580:INFO: Dataset: univ                Batch: 20/29	Loss 39.3517 (39.4715)
2022-11-09 22:38:01,410:INFO: Dataset: univ                Batch: 21/29	Loss 40.4131 (39.5157)
2022-11-09 22:38:04,488:INFO: Dataset: univ                Batch: 22/29	Loss 39.0834 (39.4913)
2022-11-09 22:38:06,930:INFO: Dataset: univ                Batch: 23/29	Loss 39.1871 (39.4774)
2022-11-09 22:38:08,896:INFO: Dataset: univ                Batch: 24/29	Loss 38.9960 (39.4549)
2022-11-09 22:38:11,066:INFO: Dataset: univ                Batch: 25/29	Loss 38.8393 (39.4246)
2022-11-09 22:38:13,291:INFO: Dataset: univ                Batch: 26/29	Loss 39.4253 (39.4247)
2022-11-09 22:38:16,203:INFO: Dataset: univ                Batch: 27/29	Loss 39.4758 (39.4267)
2022-11-09 22:38:19,554:INFO: Dataset: univ                Batch: 28/29	Loss 39.8029 (39.4400)
2022-11-09 22:38:21,824:INFO: Dataset: univ                Batch: 29/29	Loss 14.5876 (39.1126)
2022-11-09 22:38:35,016:INFO: Dataset: zara1               Batch:  1/16	Loss 43.0740 (43.0740)
2022-11-09 22:38:37,655:INFO: Dataset: zara1               Batch:  2/16	Loss 43.7966 (43.4146)
2022-11-09 22:38:40,249:INFO: Dataset: zara1               Batch:  3/16	Loss 40.3772 (42.2487)
2022-11-09 22:38:43,740:INFO: Dataset: zara1               Batch:  4/16	Loss 40.5237 (41.8657)
2022-11-09 22:38:45,997:INFO: Dataset: zara1               Batch:  5/16	Loss 40.2242 (41.5421)
2022-11-09 22:38:48,204:INFO: Dataset: zara1               Batch:  6/16	Loss 40.4342 (41.3783)
2022-11-09 22:38:50,268:INFO: Dataset: zara1               Batch:  7/16	Loss 40.0861 (41.2092)
2022-11-09 22:38:52,781:INFO: Dataset: zara1               Batch:  8/16	Loss 39.6111 (41.0199)
2022-11-09 22:38:55,120:INFO: Dataset: zara1               Batch:  9/16	Loss 39.4914 (40.8444)
2022-11-09 22:38:57,231:INFO: Dataset: zara1               Batch: 10/16	Loss 40.4407 (40.8046)

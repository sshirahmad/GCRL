2022-11-12 19:41:37,026:INFO: Initializing Training Set
2022-11-12 19:41:41,869:INFO: Initializing Validation Set
2022-11-12 19:41:42,282:INFO: Initializing Validation O Set
2022-11-12 19:41:45,469:INFO: 
===> EPOCH: 1 (P1)
2022-11-12 19:41:45,470:INFO: - Computing loss (training)
2022-11-12 19:41:51,640:INFO: Dataset: hotel               Batch: 1/8	Loss 5.0817 (5.0817)
2022-11-12 19:41:51,980:INFO: Dataset: hotel               Batch: 2/8	Loss 5.6890 (5.3883)
2022-11-12 19:41:52,414:INFO: Dataset: hotel               Batch: 3/8	Loss 6.7130 (5.8494)
2022-11-12 19:41:52,685:INFO: Dataset: hotel               Batch: 4/8	Loss 5.4020 (5.7460)
2022-11-12 19:41:52,974:INFO: Dataset: hotel               Batch: 5/8	Loss 6.1834 (5.8288)
2022-11-12 19:41:53,124:INFO: Dataset: hotel               Batch: 6/8	Loss 5.4890 (5.7678)
2022-11-12 19:41:53,175:INFO: Dataset: hotel               Batch: 7/8	Loss 4.2549 (5.5270)
2022-11-12 19:41:53,195:INFO: Dataset: hotel               Batch: 8/8	Loss 1.3343 (5.3997)
2022-11-12 19:42:16,263:INFO: Dataset: univ                Batch:  1/29	Loss 7.2903 (7.2903)
2022-11-12 19:42:16,346:INFO: Dataset: univ                Batch:  2/29	Loss 6.3143 (6.7404)
2022-11-12 19:42:16,435:INFO: Dataset: univ                Batch:  3/29	Loss 7.6627 (7.0530)
2022-11-12 19:42:16,519:INFO: Dataset: univ                Batch:  4/29	Loss 6.9717 (7.0316)
2022-11-12 19:42:16,591:INFO: Dataset: univ                Batch:  5/29	Loss 5.3369 (6.6427)
2022-11-12 19:42:16,642:INFO: Dataset: univ                Batch:  6/29	Loss 6.5457 (6.6271)
2022-11-12 19:42:16,693:INFO: Dataset: univ                Batch:  7/29	Loss 4.1053 (6.2116)
2022-11-12 19:42:16,746:INFO: Dataset: univ                Batch:  8/29	Loss 5.5789 (6.1319)
2022-11-12 19:42:16,798:INFO: Dataset: univ                Batch:  9/29	Loss 7.1018 (6.2381)
2022-11-12 19:42:16,853:INFO: Dataset: univ                Batch: 10/29	Loss 6.2010 (6.2347)
2022-11-12 19:42:16,899:INFO: Dataset: univ                Batch: 11/29	Loss 6.6132 (6.2660)
2022-11-12 19:42:16,945:INFO: Dataset: univ                Batch: 12/29	Loss 5.4885 (6.1986)
2022-11-12 19:42:16,993:INFO: Dataset: univ                Batch: 13/29	Loss 5.1974 (6.1202)
2022-11-12 19:42:17,047:INFO: Dataset: univ                Batch: 14/29	Loss 5.9706 (6.1105)
2022-11-12 19:42:17,110:INFO: Dataset: univ                Batch: 15/29	Loss 4.0272 (5.9576)
2022-11-12 19:42:17,167:INFO: Dataset: univ                Batch: 16/29	Loss 6.0236 (5.9617)
2022-11-12 19:42:17,223:INFO: Dataset: univ                Batch: 17/29	Loss 5.3586 (5.9249)
2022-11-12 19:42:17,281:INFO: Dataset: univ                Batch: 18/29	Loss 5.9213 (5.9247)
2022-11-12 19:42:17,331:INFO: Dataset: univ                Batch: 19/29	Loss 5.7739 (5.9173)
2022-11-12 19:42:17,378:INFO: Dataset: univ                Batch: 20/29	Loss 5.2286 (5.8780)
2022-11-12 19:42:17,427:INFO: Dataset: univ                Batch: 21/29	Loss 5.5191 (5.8621)
2022-11-12 19:42:17,491:INFO: Dataset: univ                Batch: 22/29	Loss 4.2324 (5.7708)
2022-11-12 19:42:17,552:INFO: Dataset: univ                Batch: 23/29	Loss 5.5297 (5.7615)
2022-11-12 19:42:17,601:INFO: Dataset: univ                Batch: 24/29	Loss 4.4304 (5.7051)
2022-11-12 19:42:17,664:INFO: Dataset: univ                Batch: 25/29	Loss 5.0756 (5.6765)
2022-11-12 19:42:17,727:INFO: Dataset: univ                Batch: 26/29	Loss 5.2034 (5.6576)
2022-11-12 19:42:17,793:INFO: Dataset: univ                Batch: 27/29	Loss 5.1069 (5.6359)
2022-11-12 19:42:17,842:INFO: Dataset: univ                Batch: 28/29	Loss 5.4901 (5.6309)
2022-11-12 19:42:17,869:INFO: Dataset: univ                Batch: 29/29	Loss 2.4367 (5.5933)
2022-11-12 19:42:24,981:INFO: Dataset: zara1               Batch:  1/16	Loss 8.7781 (8.7781)
2022-11-12 19:42:25,345:INFO: Dataset: zara1               Batch:  2/16	Loss 7.5469 (8.2450)
2022-11-12 19:42:25,673:INFO: Dataset: zara1               Batch:  3/16	Loss 8.2994 (8.2618)
2022-11-12 19:42:25,972:INFO: Dataset: zara1               Batch:  4/16	Loss 8.8164 (8.3862)
2022-11-12 19:42:26,207:INFO: Dataset: zara1               Batch:  5/16	Loss 8.7669 (8.4549)
2022-11-12 19:42:26,432:INFO: Dataset: zara1               Batch:  6/16	Loss 7.7611 (8.3414)
2022-11-12 19:42:26,489:INFO: Dataset: zara1               Batch:  7/16	Loss 8.6687 (8.3921)
2022-11-12 19:42:26,544:INFO: Dataset: zara1               Batch:  8/16	Loss 8.1115 (8.3529)
2022-11-12 19:42:26,604:INFO: Dataset: zara1               Batch:  9/16	Loss 8.4664 (8.3659)
2022-11-12 19:42:26,667:INFO: Dataset: zara1               Batch: 10/16	Loss 8.1141 (8.3408)
2022-11-12 19:42:26,746:INFO: Dataset: zara1               Batch: 11/16	Loss 7.4343 (8.2666)
2022-11-12 19:42:26,806:INFO: Dataset: zara1               Batch: 12/16	Loss 7.2370 (8.1744)
2022-11-12 19:42:26,864:INFO: Dataset: zara1               Batch: 13/16	Loss 7.6729 (8.1370)
2022-11-12 19:42:26,939:INFO: Dataset: zara1               Batch: 14/16	Loss 7.2718 (8.0764)
2022-11-12 19:42:27,000:INFO: Dataset: zara1               Batch: 15/16	Loss 7.8561 (8.0642)
2022-11-12 19:42:27,041:INFO: Dataset: zara1               Batch: 16/16	Loss 4.1802 (7.8598)
2022-11-12 19:42:50,056:INFO: Dataset: zara2               Batch:  1/36	Loss 5.7260 (5.7260)
2022-11-12 19:42:50,117:INFO: Dataset: zara2               Batch:  2/36	Loss 5.1451 (5.4728)
2022-11-12 19:42:50,174:INFO: Dataset: zara2               Batch:  3/36	Loss 4.9195 (5.2832)
2022-11-12 19:42:50,235:INFO: Dataset: zara2               Batch:  4/36	Loss 5.6848 (5.3861)
2022-11-12 19:42:50,290:INFO: Dataset: zara2               Batch:  5/36	Loss 4.6715 (5.2317)
2022-11-12 19:42:50,348:INFO: Dataset: zara2               Batch:  6/36	Loss 5.8254 (5.3190)
2022-11-12 19:42:50,409:INFO: Dataset: zara2               Batch:  7/36	Loss 4.6068 (5.2072)
2022-11-12 19:42:50,464:INFO: Dataset: zara2               Batch:  8/36	Loss 5.4525 (5.2346)
2022-11-12 19:42:50,515:INFO: Dataset: zara2               Batch:  9/36	Loss 4.4887 (5.1618)
2022-11-12 19:42:50,574:INFO: Dataset: zara2               Batch: 10/36	Loss 4.7787 (5.1242)
2022-11-12 19:42:50,634:INFO: Dataset: zara2               Batch: 11/36	Loss 5.1864 (5.1298)
2022-11-12 19:42:50,678:INFO: Dataset: zara2               Batch: 12/36	Loss 4.7699 (5.1009)
2022-11-12 19:42:50,729:INFO: Dataset: zara2               Batch: 13/36	Loss 4.3498 (5.0456)
2022-11-12 19:42:50,785:INFO: Dataset: zara2               Batch: 14/36	Loss 5.1671 (5.0547)
2022-11-12 19:42:50,842:INFO: Dataset: zara2               Batch: 15/36	Loss 5.2372 (5.0651)
2022-11-12 19:42:50,894:INFO: Dataset: zara2               Batch: 16/36	Loss 5.0716 (5.0655)
2022-11-12 19:42:50,944:INFO: Dataset: zara2               Batch: 17/36	Loss 4.7137 (5.0455)
2022-11-12 19:42:50,997:INFO: Dataset: zara2               Batch: 18/36	Loss 4.1450 (4.9956)
2022-11-12 19:42:51,049:INFO: Dataset: zara2               Batch: 19/36	Loss 4.7226 (4.9818)
2022-11-12 19:42:51,096:INFO: Dataset: zara2               Batch: 20/36	Loss 4.2020 (4.9456)
2022-11-12 19:42:51,139:INFO: Dataset: zara2               Batch: 21/36	Loss 5.6375 (4.9779)
2022-11-12 19:42:51,181:INFO: Dataset: zara2               Batch: 22/36	Loss 4.6428 (4.9673)
2022-11-12 19:42:51,238:INFO: Dataset: zara2               Batch: 23/36	Loss 4.4319 (4.9435)
2022-11-12 19:42:51,290:INFO: Dataset: zara2               Batch: 24/36	Loss 4.2590 (4.9158)
2022-11-12 19:42:51,349:INFO: Dataset: zara2               Batch: 25/36	Loss 3.8185 (4.8672)
2022-11-12 19:42:51,403:INFO: Dataset: zara2               Batch: 26/36	Loss 3.7759 (4.8244)
2022-11-12 19:42:51,465:INFO: Dataset: zara2               Batch: 27/36	Loss 4.1999 (4.8019)
2022-11-12 19:42:51,518:INFO: Dataset: zara2               Batch: 28/36	Loss 4.1554 (4.7825)
2022-11-12 19:42:51,567:INFO: Dataset: zara2               Batch: 29/36	Loss 3.7370 (4.7392)
2022-11-12 19:42:51,619:INFO: Dataset: zara2               Batch: 30/36	Loss 4.0130 (4.7176)
2022-11-12 19:42:51,678:INFO: Dataset: zara2               Batch: 31/36	Loss 3.4756 (4.6771)
2022-11-12 19:42:51,730:INFO: Dataset: zara2               Batch: 32/36	Loss 3.5551 (4.6365)
2022-11-12 19:42:51,776:INFO: Dataset: zara2               Batch: 33/36	Loss 3.8125 (4.6110)
2022-11-12 19:42:51,829:INFO: Dataset: zara2               Batch: 34/36	Loss 3.7287 (4.5839)
2022-11-12 19:42:51,885:INFO: Dataset: zara2               Batch: 35/36	Loss 3.1309 (4.5451)
2022-11-12 19:42:51,930:INFO: Dataset: zara2               Batch: 36/36	Loss 3.2017 (4.5245)
2022-11-12 19:42:52,870:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_1.pth.tar
2022-11-12 19:42:52,870:INFO: 
===> EPOCH: 2 (P1)
2022-11-12 19:42:52,871:INFO: - Computing loss (training)
2022-11-12 19:42:59,018:INFO: Dataset: hotel               Batch: 1/8	Loss 5.7584 (5.7584)
2022-11-12 19:42:59,402:INFO: Dataset: hotel               Batch: 2/8	Loss 5.6182 (5.6869)
2022-11-12 19:42:59,703:INFO: Dataset: hotel               Batch: 3/8	Loss 4.8812 (5.4236)
2022-11-12 19:42:59,992:INFO: Dataset: hotel               Batch: 4/8	Loss 4.9969 (5.3192)
2022-11-12 19:43:00,260:INFO: Dataset: hotel               Batch: 5/8	Loss 6.4312 (5.5498)
2022-11-12 19:43:00,495:INFO: Dataset: hotel               Batch: 6/8	Loss 5.2222 (5.4935)
2022-11-12 19:43:00,553:INFO: Dataset: hotel               Batch: 7/8	Loss 5.7531 (5.5325)
2022-11-12 19:43:00,585:INFO: Dataset: hotel               Batch: 8/8	Loss 0.7568 (5.3750)
2022-11-12 19:43:23,646:INFO: Dataset: univ                Batch:  1/29	Loss 3.8786 (3.8786)
2022-11-12 19:43:23,711:INFO: Dataset: univ                Batch:  2/29	Loss 3.4873 (3.6791)
2022-11-12 19:43:23,798:INFO: Dataset: univ                Batch:  3/29	Loss 3.3695 (3.5805)
2022-11-12 19:43:23,872:INFO: Dataset: univ                Batch:  4/29	Loss 3.3308 (3.5164)
2022-11-12 19:43:23,958:INFO: Dataset: univ                Batch:  5/29	Loss 3.3228 (3.4776)
2022-11-12 19:43:24,027:INFO: Dataset: univ                Batch:  6/29	Loss 3.2794 (3.4449)
2022-11-12 19:43:24,092:INFO: Dataset: univ                Batch:  7/29	Loss 3.2480 (3.4179)
2022-11-12 19:43:24,153:INFO: Dataset: univ                Batch:  8/29	Loss 3.0910 (3.3805)
2022-11-12 19:43:24,218:INFO: Dataset: univ                Batch:  9/29	Loss 3.7163 (3.4179)
2022-11-12 19:43:24,285:INFO: Dataset: univ                Batch: 10/29	Loss 2.8580 (3.3546)
2022-11-12 19:43:24,337:INFO: Dataset: univ                Batch: 11/29	Loss 3.4623 (3.3643)
2022-11-12 19:43:24,412:INFO: Dataset: univ                Batch: 12/29	Loss 2.9626 (3.3270)
2022-11-12 19:43:24,475:INFO: Dataset: univ                Batch: 13/29	Loss 3.3290 (3.3272)
2022-11-12 19:43:24,528:INFO: Dataset: univ                Batch: 14/29	Loss 3.4344 (3.3344)
2022-11-12 19:43:24,583:INFO: Dataset: univ                Batch: 15/29	Loss 3.1344 (3.3210)
2022-11-12 19:43:24,642:INFO: Dataset: univ                Batch: 16/29	Loss 2.9416 (3.2970)
2022-11-12 19:43:24,702:INFO: Dataset: univ                Batch: 17/29	Loss 3.3426 (3.2996)
2022-11-12 19:43:24,759:INFO: Dataset: univ                Batch: 18/29	Loss 3.4371 (3.3072)
2022-11-12 19:43:24,815:INFO: Dataset: univ                Batch: 19/29	Loss 2.9136 (3.2856)
2022-11-12 19:43:24,876:INFO: Dataset: univ                Batch: 20/29	Loss 3.7498 (3.3062)
2022-11-12 19:43:24,933:INFO: Dataset: univ                Batch: 21/29	Loss 2.8275 (3.2814)
2022-11-12 19:43:24,988:INFO: Dataset: univ                Batch: 22/29	Loss 3.1227 (3.2743)
2022-11-12 19:43:25,040:INFO: Dataset: univ                Batch: 23/29	Loss 3.0922 (3.2654)
2022-11-12 19:43:25,093:INFO: Dataset: univ                Batch: 24/29	Loss 3.0358 (3.2566)
2022-11-12 19:43:25,154:INFO: Dataset: univ                Batch: 25/29	Loss 2.9201 (3.2427)
2022-11-12 19:43:25,224:INFO: Dataset: univ                Batch: 26/29	Loss 2.5179 (3.2156)
2022-11-12 19:43:25,276:INFO: Dataset: univ                Batch: 27/29	Loss 2.7486 (3.1962)
2022-11-12 19:43:25,337:INFO: Dataset: univ                Batch: 28/29	Loss 3.2171 (3.1969)
2022-11-12 19:43:25,372:INFO: Dataset: univ                Batch: 29/29	Loss 1.2351 (3.1755)
2022-11-12 19:43:32,399:INFO: Dataset: zara1               Batch:  1/16	Loss 3.8362 (3.8362)
2022-11-12 19:43:32,789:INFO: Dataset: zara1               Batch:  2/16	Loss 3.6770 (3.7633)
2022-11-12 19:43:33,093:INFO: Dataset: zara1               Batch:  3/16	Loss 3.7026 (3.7470)
2022-11-12 19:43:33,395:INFO: Dataset: zara1               Batch:  4/16	Loss 3.5102 (3.6928)
2022-11-12 19:43:33,642:INFO: Dataset: zara1               Batch:  5/16	Loss 3.4317 (3.6442)
2022-11-12 19:43:33,825:INFO: Dataset: zara1               Batch:  6/16	Loss 3.5299 (3.6256)
2022-11-12 19:43:33,880:INFO: Dataset: zara1               Batch:  7/16	Loss 3.4302 (3.5944)
2022-11-12 19:43:33,939:INFO: Dataset: zara1               Batch:  8/16	Loss 3.3975 (3.5718)
2022-11-12 19:43:34,014:INFO: Dataset: zara1               Batch:  9/16	Loss 3.3823 (3.5470)
2022-11-12 19:43:34,064:INFO: Dataset: zara1               Batch: 10/16	Loss 2.6444 (3.4576)
2022-11-12 19:43:34,117:INFO: Dataset: zara1               Batch: 11/16	Loss 3.0056 (3.4148)
2022-11-12 19:43:34,179:INFO: Dataset: zara1               Batch: 12/16	Loss 3.6190 (3.4318)
2022-11-12 19:43:34,237:INFO: Dataset: zara1               Batch: 13/16	Loss 3.0291 (3.4022)
2022-11-12 19:43:34,301:INFO: Dataset: zara1               Batch: 14/16	Loss 3.2498 (3.3908)
2022-11-12 19:43:34,359:INFO: Dataset: zara1               Batch: 15/16	Loss 3.3424 (3.3876)
2022-11-12 19:43:34,407:INFO: Dataset: zara1               Batch: 16/16	Loss 2.3637 (3.3472)
2022-11-12 19:43:58,071:INFO: Dataset: zara2               Batch:  1/36	Loss 2.3800 (2.3800)
2022-11-12 19:43:58,127:INFO: Dataset: zara2               Batch:  2/36	Loss 2.7409 (2.5307)
2022-11-12 19:43:58,194:INFO: Dataset: zara2               Batch:  3/36	Loss 3.2289 (2.7395)
2022-11-12 19:43:58,248:INFO: Dataset: zara2               Batch:  4/36	Loss 2.7241 (2.7358)
2022-11-12 19:43:58,308:INFO: Dataset: zara2               Batch:  5/36	Loss 2.3436 (2.6487)
2022-11-12 19:43:58,370:INFO: Dataset: zara2               Batch:  6/36	Loss 2.2355 (2.5747)
2022-11-12 19:43:58,423:INFO: Dataset: zara2               Batch:  7/36	Loss 2.2454 (2.5312)
2022-11-12 19:43:58,485:INFO: Dataset: zara2               Batch:  8/36	Loss 2.5443 (2.5329)
2022-11-12 19:43:58,546:INFO: Dataset: zara2               Batch:  9/36	Loss 2.4058 (2.5168)
2022-11-12 19:43:58,616:INFO: Dataset: zara2               Batch: 10/36	Loss 2.4414 (2.5078)
2022-11-12 19:43:58,666:INFO: Dataset: zara2               Batch: 11/36	Loss 1.9133 (2.4588)
2022-11-12 19:43:58,714:INFO: Dataset: zara2               Batch: 12/36	Loss 1.9433 (2.4179)
2022-11-12 19:43:58,768:INFO: Dataset: zara2               Batch: 13/36	Loss 2.2261 (2.4013)
2022-11-12 19:43:58,837:INFO: Dataset: zara2               Batch: 14/36	Loss 2.1163 (2.3817)
2022-11-12 19:43:58,884:INFO: Dataset: zara2               Batch: 15/36	Loss 2.1863 (2.3684)
2022-11-12 19:43:58,927:INFO: Dataset: zara2               Batch: 16/36	Loss 2.0571 (2.3507)
2022-11-12 19:43:58,973:INFO: Dataset: zara2               Batch: 17/36	Loss 2.0589 (2.3355)
2022-11-12 19:43:59,021:INFO: Dataset: zara2               Batch: 18/36	Loss 2.2094 (2.3278)
2022-11-12 19:43:59,078:INFO: Dataset: zara2               Batch: 19/36	Loss 1.9543 (2.3094)
2022-11-12 19:43:59,125:INFO: Dataset: zara2               Batch: 20/36	Loss 2.0167 (2.2943)
2022-11-12 19:43:59,183:INFO: Dataset: zara2               Batch: 21/36	Loss 1.9657 (2.2781)
2022-11-12 19:43:59,230:INFO: Dataset: zara2               Batch: 22/36	Loss 2.2753 (2.2780)
2022-11-12 19:43:59,283:INFO: Dataset: zara2               Batch: 23/36	Loss 2.2501 (2.2767)
2022-11-12 19:43:59,337:INFO: Dataset: zara2               Batch: 24/36	Loss 1.8581 (2.2587)
2022-11-12 19:43:59,384:INFO: Dataset: zara2               Batch: 25/36	Loss 1.8572 (2.2422)
2022-11-12 19:43:59,441:INFO: Dataset: zara2               Batch: 26/36	Loss 2.2232 (2.2415)
2022-11-12 19:43:59,495:INFO: Dataset: zara2               Batch: 27/36	Loss 2.3202 (2.2442)
2022-11-12 19:43:59,544:INFO: Dataset: zara2               Batch: 28/36	Loss 2.2389 (2.2439)
2022-11-12 19:43:59,594:INFO: Dataset: zara2               Batch: 29/36	Loss 1.7412 (2.2274)
2022-11-12 19:43:59,653:INFO: Dataset: zara2               Batch: 30/36	Loss 1.7956 (2.2132)
2022-11-12 19:43:59,699:INFO: Dataset: zara2               Batch: 31/36	Loss 2.0676 (2.2087)
2022-11-12 19:43:59,752:INFO: Dataset: zara2               Batch: 32/36	Loss 1.8127 (2.1957)
2022-11-12 19:43:59,803:INFO: Dataset: zara2               Batch: 33/36	Loss 1.8335 (2.1841)
2022-11-12 19:43:59,860:INFO: Dataset: zara2               Batch: 34/36	Loss 1.8872 (2.1755)
2022-11-12 19:43:59,909:INFO: Dataset: zara2               Batch: 35/36	Loss 1.7045 (2.1630)
2022-11-12 19:43:59,947:INFO: Dataset: zara2               Batch: 36/36	Loss 1.2800 (2.1488)
2022-11-12 19:44:00,935:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_2.pth.tar
2022-11-12 19:44:00,935:INFO: 
===> EPOCH: 3 (P2)
2022-11-12 19:44:00,936:INFO: - Computing loss (training)
2022-11-12 19:44:07,862:INFO: Dataset: hotel               Batch: 1/8	Loss 6.6640 (6.6640)
2022-11-12 19:44:08,260:INFO: Dataset: hotel               Batch: 2/8	Loss 7.0242 (6.8306)
2022-11-12 19:44:08,679:INFO: Dataset: hotel               Batch: 3/8	Loss 5.7611 (6.4858)
2022-11-12 19:44:08,947:INFO: Dataset: hotel               Batch: 4/8	Loss 5.6852 (6.3104)
2022-11-12 19:44:09,279:INFO: Dataset: hotel               Batch: 5/8	Loss 6.4329 (6.3339)
2022-11-12 19:44:09,547:INFO: Dataset: hotel               Batch: 6/8	Loss 5.9459 (6.2724)
2022-11-12 19:44:09,780:INFO: Dataset: hotel               Batch: 7/8	Loss 5.6033 (6.1746)
2022-11-12 19:44:09,881:INFO: Dataset: hotel               Batch: 8/8	Loss 1.1624 (6.0490)
2022-11-12 19:44:33,356:INFO: Dataset: univ                Batch:  1/29	Loss 8.0709 (8.0709)
2022-11-12 19:44:33,693:INFO: Dataset: univ                Batch:  2/29	Loss 7.9212 (7.9930)
2022-11-12 19:44:33,941:INFO: Dataset: univ                Batch:  3/29	Loss 6.8881 (7.6085)
2022-11-12 19:44:34,181:INFO: Dataset: univ                Batch:  4/29	Loss 5.5856 (7.0163)
2022-11-12 19:44:34,455:INFO: Dataset: univ                Batch:  5/29	Loss 8.4795 (7.2740)
2022-11-12 19:44:34,723:INFO: Dataset: univ                Batch:  6/29	Loss 7.2998 (7.2780)
2022-11-12 19:44:35,029:INFO: Dataset: univ                Batch:  7/29	Loss 6.1518 (7.1208)
2022-11-12 19:44:35,361:INFO: Dataset: univ                Batch:  8/29	Loss 6.5585 (7.0505)
2022-11-12 19:44:35,601:INFO: Dataset: univ                Batch:  9/29	Loss 7.1776 (7.0634)
2022-11-12 19:44:35,847:INFO: Dataset: univ                Batch: 10/29	Loss 7.4742 (7.1037)
2022-11-12 19:44:36,113:INFO: Dataset: univ                Batch: 11/29	Loss 8.0926 (7.1749)
2022-11-12 19:44:36,355:INFO: Dataset: univ                Batch: 12/29	Loss 6.3685 (7.1088)
2022-11-12 19:44:36,640:INFO: Dataset: univ                Batch: 13/29	Loss 5.7996 (7.0073)
2022-11-12 19:44:36,893:INFO: Dataset: univ                Batch: 14/29	Loss 5.8523 (6.9097)
2022-11-12 19:44:37,149:INFO: Dataset: univ                Batch: 15/29	Loss 4.4781 (6.7307)
2022-11-12 19:44:37,550:INFO: Dataset: univ                Batch: 16/29	Loss 5.6652 (6.6599)
2022-11-12 19:44:37,803:INFO: Dataset: univ                Batch: 17/29	Loss 4.8474 (6.5412)
2022-11-12 19:44:38,084:INFO: Dataset: univ                Batch: 18/29	Loss 6.2169 (6.5235)
2022-11-12 19:44:38,374:INFO: Dataset: univ                Batch: 19/29	Loss 5.7758 (6.4829)
2022-11-12 19:44:38,609:INFO: Dataset: univ                Batch: 20/29	Loss 5.9463 (6.4524)
2022-11-12 19:44:38,880:INFO: Dataset: univ                Batch: 21/29	Loss 4.8121 (6.3628)
2022-11-12 19:44:39,122:INFO: Dataset: univ                Batch: 22/29	Loss 5.6531 (6.3349)
2022-11-12 19:44:39,384:INFO: Dataset: univ                Batch: 23/29	Loss 4.1909 (6.2309)
2022-11-12 19:44:39,643:INFO: Dataset: univ                Batch: 24/29	Loss 4.9825 (6.1761)
2022-11-12 19:44:39,879:INFO: Dataset: univ                Batch: 25/29	Loss 5.0081 (6.1338)
2022-11-12 19:44:40,151:INFO: Dataset: univ                Batch: 26/29	Loss 5.4181 (6.1091)
2022-11-12 19:44:40,393:INFO: Dataset: univ                Batch: 27/29	Loss 3.9539 (6.0323)
2022-11-12 19:44:40,657:INFO: Dataset: univ                Batch: 28/29	Loss 4.1004 (5.9542)
2022-11-12 19:44:40,771:INFO: Dataset: univ                Batch: 29/29	Loss 1.6204 (5.8991)
2022-11-12 19:44:48,506:INFO: Dataset: zara1               Batch:  1/16	Loss 5.9982 (5.9982)
2022-11-12 19:44:48,943:INFO: Dataset: zara1               Batch:  2/16	Loss 6.9994 (6.4644)
2022-11-12 19:44:49,286:INFO: Dataset: zara1               Batch:  3/16	Loss 6.7647 (6.5503)
2022-11-12 19:44:49,696:INFO: Dataset: zara1               Batch:  4/16	Loss 6.9187 (6.6422)
2022-11-12 19:44:49,941:INFO: Dataset: zara1               Batch:  5/16	Loss 5.7945 (6.4889)
2022-11-12 19:44:50,220:INFO: Dataset: zara1               Batch:  6/16	Loss 6.5230 (6.4950)
2022-11-12 19:44:50,479:INFO: Dataset: zara1               Batch:  7/16	Loss 6.3996 (6.4815)
2022-11-12 19:44:50,755:INFO: Dataset: zara1               Batch:  8/16	Loss 6.4552 (6.4780)
2022-11-12 19:44:51,019:INFO: Dataset: zara1               Batch:  9/16	Loss 5.7255 (6.3827)
2022-11-12 19:44:51,283:INFO: Dataset: zara1               Batch: 10/16	Loss 5.1156 (6.2743)
2022-11-12 19:44:51,543:INFO: Dataset: zara1               Batch: 11/16	Loss 4.5602 (6.1216)
2022-11-12 19:44:51,804:INFO: Dataset: zara1               Batch: 12/16	Loss 5.7111 (6.0891)
2022-11-12 19:44:52,056:INFO: Dataset: zara1               Batch: 13/16	Loss 5.3676 (6.0344)
2022-11-12 19:44:52,315:INFO: Dataset: zara1               Batch: 14/16	Loss 4.8146 (5.9524)
2022-11-12 19:44:52,568:INFO: Dataset: zara1               Batch: 15/16	Loss 4.2120 (5.8312)
2022-11-12 19:44:52,771:INFO: Dataset: zara1               Batch: 16/16	Loss 3.2518 (5.7281)
2022-11-12 19:45:15,935:INFO: Dataset: zara2               Batch:  1/36	Loss 4.0710 (4.0710)
2022-11-12 19:45:16,204:INFO: Dataset: zara2               Batch:  2/36	Loss 3.2306 (3.6449)
2022-11-12 19:45:16,472:INFO: Dataset: zara2               Batch:  3/36	Loss 4.2755 (3.8570)
2022-11-12 19:45:16,739:INFO: Dataset: zara2               Batch:  4/36	Loss 3.7466 (3.8276)
2022-11-12 19:45:17,013:INFO: Dataset: zara2               Batch:  5/36	Loss 3.4913 (3.7604)
2022-11-12 19:45:17,281:INFO: Dataset: zara2               Batch:  6/36	Loss 3.6880 (3.7490)
2022-11-12 19:45:17,523:INFO: Dataset: zara2               Batch:  7/36	Loss 4.0880 (3.7955)
2022-11-12 19:45:17,762:INFO: Dataset: zara2               Batch:  8/36	Loss 4.0549 (3.8242)
2022-11-12 19:45:18,027:INFO: Dataset: zara2               Batch:  9/36	Loss 3.5234 (3.7878)
2022-11-12 19:45:18,283:INFO: Dataset: zara2               Batch: 10/36	Loss 3.6719 (3.7771)
2022-11-12 19:45:18,545:INFO: Dataset: zara2               Batch: 11/36	Loss 3.6337 (3.7660)
2022-11-12 19:45:18,804:INFO: Dataset: zara2               Batch: 12/36	Loss 3.0311 (3.7019)
2022-11-12 19:45:19,062:INFO: Dataset: zara2               Batch: 13/36	Loss 3.1768 (3.6587)
2022-11-12 19:45:19,326:INFO: Dataset: zara2               Batch: 14/36	Loss 2.8936 (3.6075)
2022-11-12 19:45:19,591:INFO: Dataset: zara2               Batch: 15/36	Loss 2.7089 (3.5441)
2022-11-12 19:45:19,892:INFO: Dataset: zara2               Batch: 16/36	Loss 2.9546 (3.5110)
2022-11-12 19:45:20,161:INFO: Dataset: zara2               Batch: 17/36	Loss 3.0298 (3.4818)
2022-11-12 19:45:20,431:INFO: Dataset: zara2               Batch: 18/36	Loss 2.9478 (3.4547)
2022-11-12 19:45:20,687:INFO: Dataset: zara2               Batch: 19/36	Loss 2.9767 (3.4292)
2022-11-12 19:45:20,973:INFO: Dataset: zara2               Batch: 20/36	Loss 3.0839 (3.4130)
2022-11-12 19:45:21,242:INFO: Dataset: zara2               Batch: 21/36	Loss 3.0086 (3.3952)
2022-11-12 19:45:21,494:INFO: Dataset: zara2               Batch: 22/36	Loss 2.9880 (3.3786)
2022-11-12 19:45:21,776:INFO: Dataset: zara2               Batch: 23/36	Loss 2.8798 (3.3559)
2022-11-12 19:45:22,048:INFO: Dataset: zara2               Batch: 24/36	Loss 2.5736 (3.3252)
2022-11-12 19:45:22,308:INFO: Dataset: zara2               Batch: 25/36	Loss 2.6601 (3.3000)
2022-11-12 19:45:22,561:INFO: Dataset: zara2               Batch: 26/36	Loss 2.5037 (3.2676)
2022-11-12 19:45:22,828:INFO: Dataset: zara2               Batch: 27/36	Loss 2.9769 (3.2577)
2022-11-12 19:45:23,122:INFO: Dataset: zara2               Batch: 28/36	Loss 2.8305 (3.2437)
2022-11-12 19:45:23,436:INFO: Dataset: zara2               Batch: 29/36	Loss 2.6698 (3.2257)
2022-11-12 19:45:23,689:INFO: Dataset: zara2               Batch: 30/36	Loss 2.8948 (3.2158)
2022-11-12 19:45:23,964:INFO: Dataset: zara2               Batch: 31/36	Loss 3.0636 (3.2105)
2022-11-12 19:45:24,250:INFO: Dataset: zara2               Batch: 32/36	Loss 2.8425 (3.2003)
2022-11-12 19:45:24,494:INFO: Dataset: zara2               Batch: 33/36	Loss 3.0694 (3.1961)
2022-11-12 19:45:24,764:INFO: Dataset: zara2               Batch: 34/36	Loss 2.3653 (3.1718)
2022-11-12 19:45:25,020:INFO: Dataset: zara2               Batch: 35/36	Loss 2.7011 (3.1605)
2022-11-12 19:45:25,211:INFO: Dataset: zara2               Batch: 36/36	Loss 1.6045 (3.1270)
2022-11-12 19:45:26,154:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_3.pth.tar
2022-11-12 19:45:26,154:INFO: 
===> EPOCH: 4 (P2)
2022-11-12 19:45:26,155:INFO: - Computing loss (training)
2022-11-12 19:45:32,619:INFO: Dataset: hotel               Batch: 1/8	Loss 4.8597 (4.8597)
2022-11-12 19:45:32,974:INFO: Dataset: hotel               Batch: 2/8	Loss 4.8852 (4.8726)
2022-11-12 19:45:33,359:INFO: Dataset: hotel               Batch: 3/8	Loss 4.8880 (4.8779)
2022-11-12 19:45:33,689:INFO: Dataset: hotel               Batch: 4/8	Loss 4.2220 (4.7091)
2022-11-12 19:45:34,022:INFO: Dataset: hotel               Batch: 5/8	Loss 3.5773 (4.4721)
2022-11-12 19:45:34,283:INFO: Dataset: hotel               Batch: 6/8	Loss 3.8439 (4.3625)
2022-11-12 19:45:34,527:INFO: Dataset: hotel               Batch: 7/8	Loss 4.7848 (4.4301)
2022-11-12 19:45:34,609:INFO: Dataset: hotel               Batch: 8/8	Loss 1.3305 (4.3442)
2022-11-12 19:45:57,808:INFO: Dataset: univ                Batch:  1/29	Loss 2.1999 (2.1999)
2022-11-12 19:45:58,055:INFO: Dataset: univ                Batch:  2/29	Loss 2.5783 (2.3581)
2022-11-12 19:45:58,296:INFO: Dataset: univ                Batch:  3/29	Loss 2.3467 (2.3539)
2022-11-12 19:45:58,533:INFO: Dataset: univ                Batch:  4/29	Loss 2.0470 (2.2667)
2022-11-12 19:45:58,790:INFO: Dataset: univ                Batch:  5/29	Loss 2.5215 (2.3116)
2022-11-12 19:45:59,086:INFO: Dataset: univ                Batch:  6/29	Loss 2.1414 (2.2824)
2022-11-12 19:45:59,350:INFO: Dataset: univ                Batch:  7/29	Loss 2.2590 (2.2789)
2022-11-12 19:45:59,578:INFO: Dataset: univ                Batch:  8/29	Loss 2.2229 (2.2717)
2022-11-12 19:45:59,839:INFO: Dataset: univ                Batch:  9/29	Loss 2.2904 (2.2737)
2022-11-12 19:46:00,097:INFO: Dataset: univ                Batch: 10/29	Loss 2.0561 (2.2493)
2022-11-12 19:46:00,365:INFO: Dataset: univ                Batch: 11/29	Loss 2.1364 (2.2384)
2022-11-12 19:46:00,659:INFO: Dataset: univ                Batch: 12/29	Loss 1.8879 (2.2061)
2022-11-12 19:46:00,915:INFO: Dataset: univ                Batch: 13/29	Loss 2.1347 (2.2010)
2022-11-12 19:46:01,189:INFO: Dataset: univ                Batch: 14/29	Loss 2.3474 (2.2111)
2022-11-12 19:46:01,450:INFO: Dataset: univ                Batch: 15/29	Loss 2.1838 (2.2094)
2022-11-12 19:46:01,694:INFO: Dataset: univ                Batch: 16/29	Loss 1.9484 (2.1928)
2022-11-12 19:46:01,948:INFO: Dataset: univ                Batch: 17/29	Loss 2.3788 (2.2032)
2022-11-12 19:46:02,198:INFO: Dataset: univ                Batch: 18/29	Loss 2.4922 (2.2173)
2022-11-12 19:46:02,452:INFO: Dataset: univ                Batch: 19/29	Loss 1.9510 (2.2027)
2022-11-12 19:46:02,693:INFO: Dataset: univ                Batch: 20/29	Loss 1.9658 (2.1915)
2022-11-12 19:46:02,941:INFO: Dataset: univ                Batch: 21/29	Loss 1.8874 (2.1753)
2022-11-12 19:46:03,216:INFO: Dataset: univ                Batch: 22/29	Loss 2.0952 (2.1717)
2022-11-12 19:46:03,463:INFO: Dataset: univ                Batch: 23/29	Loss 2.0033 (2.1643)
2022-11-12 19:46:03,735:INFO: Dataset: univ                Batch: 24/29	Loss 1.8176 (2.1492)
2022-11-12 19:46:03,993:INFO: Dataset: univ                Batch: 25/29	Loss 2.5557 (2.1637)
2022-11-12 19:46:04,252:INFO: Dataset: univ                Batch: 26/29	Loss 2.1222 (2.1622)
2022-11-12 19:46:04,505:INFO: Dataset: univ                Batch: 27/29	Loss 2.2226 (2.1642)
2022-11-12 19:46:04,758:INFO: Dataset: univ                Batch: 28/29	Loss 2.1163 (2.1626)
2022-11-12 19:46:04,877:INFO: Dataset: univ                Batch: 29/29	Loss 0.7330 (2.1453)
2022-11-12 19:46:12,600:INFO: Dataset: zara1               Batch:  1/16	Loss 2.3255 (2.3255)
2022-11-12 19:46:12,959:INFO: Dataset: zara1               Batch:  2/16	Loss 2.3233 (2.3245)
2022-11-12 19:46:13,391:INFO: Dataset: zara1               Batch:  3/16	Loss 2.6083 (2.4176)
2022-11-12 19:46:13,686:INFO: Dataset: zara1               Batch:  4/16	Loss 2.7200 (2.4781)
2022-11-12 19:46:14,023:INFO: Dataset: zara1               Batch:  5/16	Loss 2.3798 (2.4572)
2022-11-12 19:46:14,301:INFO: Dataset: zara1               Batch:  6/16	Loss 2.6054 (2.4800)
2022-11-12 19:46:14,575:INFO: Dataset: zara1               Batch:  7/16	Loss 2.1921 (2.4407)
2022-11-12 19:46:14,839:INFO: Dataset: zara1               Batch:  8/16	Loss 2.1981 (2.4104)
2022-11-12 19:46:15,113:INFO: Dataset: zara1               Batch:  9/16	Loss 2.1490 (2.3836)
2022-11-12 19:46:15,388:INFO: Dataset: zara1               Batch: 10/16	Loss 2.2795 (2.3720)
2022-11-12 19:46:15,638:INFO: Dataset: zara1               Batch: 11/16	Loss 2.4954 (2.3837)
2022-11-12 19:46:15,888:INFO: Dataset: zara1               Batch: 12/16	Loss 2.3956 (2.3848)
2022-11-12 19:46:16,179:INFO: Dataset: zara1               Batch: 13/16	Loss 2.3032 (2.3788)
2022-11-12 19:46:16,449:INFO: Dataset: zara1               Batch: 14/16	Loss 2.1691 (2.3628)
2022-11-12 19:46:16,723:INFO: Dataset: zara1               Batch: 15/16	Loss 2.3198 (2.3601)
2022-11-12 19:46:16,908:INFO: Dataset: zara1               Batch: 16/16	Loss 1.6498 (2.3257)
2022-11-12 19:46:40,577:INFO: Dataset: zara2               Batch:  1/36	Loss 1.7667 (1.7667)
2022-11-12 19:46:40,826:INFO: Dataset: zara2               Batch:  2/36	Loss 1.7824 (1.7742)
2022-11-12 19:46:41,082:INFO: Dataset: zara2               Batch:  3/36	Loss 1.9331 (1.8270)
2022-11-12 19:46:41,357:INFO: Dataset: zara2               Batch:  4/36	Loss 1.8659 (1.8368)
2022-11-12 19:46:41,617:INFO: Dataset: zara2               Batch:  5/36	Loss 2.1203 (1.8957)
2022-11-12 19:46:41,899:INFO: Dataset: zara2               Batch:  6/36	Loss 1.8020 (1.8793)
2022-11-12 19:46:42,176:INFO: Dataset: zara2               Batch:  7/36	Loss 1.8025 (1.8687)
2022-11-12 19:46:42,434:INFO: Dataset: zara2               Batch:  8/36	Loss 1.6050 (1.8381)
2022-11-12 19:46:42,684:INFO: Dataset: zara2               Batch:  9/36	Loss 1.8332 (1.8376)
2022-11-12 19:46:42,947:INFO: Dataset: zara2               Batch: 10/36	Loss 1.7740 (1.8317)
2022-11-12 19:46:43,192:INFO: Dataset: zara2               Batch: 11/36	Loss 1.8462 (1.8330)
2022-11-12 19:46:43,457:INFO: Dataset: zara2               Batch: 12/36	Loss 1.5962 (1.8125)
2022-11-12 19:46:43,725:INFO: Dataset: zara2               Batch: 13/36	Loss 1.7059 (1.8048)
2022-11-12 19:46:43,996:INFO: Dataset: zara2               Batch: 14/36	Loss 1.7147 (1.7983)
2022-11-12 19:46:44,246:INFO: Dataset: zara2               Batch: 15/36	Loss 1.4585 (1.7787)
2022-11-12 19:46:44,497:INFO: Dataset: zara2               Batch: 16/36	Loss 1.6027 (1.7680)
2022-11-12 19:46:44,753:INFO: Dataset: zara2               Batch: 17/36	Loss 1.6679 (1.7621)
2022-11-12 19:46:45,011:INFO: Dataset: zara2               Batch: 18/36	Loss 1.5795 (1.7520)
2022-11-12 19:46:45,247:INFO: Dataset: zara2               Batch: 19/36	Loss 1.4822 (1.7381)
2022-11-12 19:46:45,488:INFO: Dataset: zara2               Batch: 20/36	Loss 1.6706 (1.7346)
2022-11-12 19:46:45,735:INFO: Dataset: zara2               Batch: 21/36	Loss 1.4798 (1.7225)
2022-11-12 19:46:45,997:INFO: Dataset: zara2               Batch: 22/36	Loss 1.7928 (1.7252)
2022-11-12 19:46:46,266:INFO: Dataset: zara2               Batch: 23/36	Loss 1.4546 (1.7126)
2022-11-12 19:46:46,502:INFO: Dataset: zara2               Batch: 24/36	Loss 1.7123 (1.7126)
2022-11-12 19:46:46,747:INFO: Dataset: zara2               Batch: 25/36	Loss 1.6948 (1.7119)
2022-11-12 19:46:47,024:INFO: Dataset: zara2               Batch: 26/36	Loss 1.5189 (1.7046)
2022-11-12 19:46:47,281:INFO: Dataset: zara2               Batch: 27/36	Loss 1.3659 (1.6928)
2022-11-12 19:46:47,549:INFO: Dataset: zara2               Batch: 28/36	Loss 1.5790 (1.6886)
2022-11-12 19:46:47,852:INFO: Dataset: zara2               Batch: 29/36	Loss 1.3925 (1.6759)
2022-11-12 19:46:48,113:INFO: Dataset: zara2               Batch: 30/36	Loss 1.4198 (1.6661)
2022-11-12 19:46:48,386:INFO: Dataset: zara2               Batch: 31/36	Loss 1.6438 (1.6653)
2022-11-12 19:46:48,659:INFO: Dataset: zara2               Batch: 32/36	Loss 1.2842 (1.6527)
2022-11-12 19:46:48,925:INFO: Dataset: zara2               Batch: 33/36	Loss 1.4773 (1.6477)
2022-11-12 19:46:49,161:INFO: Dataset: zara2               Batch: 34/36	Loss 1.5463 (1.6448)
2022-11-12 19:46:49,408:INFO: Dataset: zara2               Batch: 35/36	Loss 1.6797 (1.6458)
2022-11-12 19:46:49,581:INFO: Dataset: zara2               Batch: 36/36	Loss 0.9530 (1.6301)
2022-11-12 19:46:50,551:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_4.pth.tar
2022-11-12 19:46:50,552:INFO: 
===> EPOCH: 5 (P3)
2022-11-12 19:46:50,553:INFO: - Computing loss (training)
2022-11-12 19:47:00,961:INFO: Dataset: hotel               Batch: 1/8	Loss 44.8266 (44.8266)
2022-11-12 19:47:02,275:INFO: Dataset: hotel               Batch: 2/8	Loss 45.7281 (45.2732)
2022-11-12 19:47:03,792:INFO: Dataset: hotel               Batch: 3/8	Loss 44.0288 (44.8635)
2022-11-12 19:47:05,377:INFO: Dataset: hotel               Batch: 4/8	Loss 43.3888 (44.4881)
2022-11-12 19:47:07,057:INFO: Dataset: hotel               Batch: 5/8	Loss 44.1360 (44.4245)
2022-11-12 19:47:08,680:INFO: Dataset: hotel               Batch: 6/8	Loss 43.7902 (44.3259)
2022-11-12 19:47:10,307:INFO: Dataset: hotel               Batch: 7/8	Loss 44.6592 (44.3723)
2022-11-12 19:47:11,761:INFO: Dataset: hotel               Batch: 8/8	Loss 9.2601 (43.1679)
2022-11-12 19:47:37,695:INFO: Dataset: univ                Batch:  1/29	Loss 43.0211 (43.0211)
2022-11-12 19:47:39,056:INFO: Dataset: univ                Batch:  2/29	Loss 42.8390 (42.9317)
2022-11-12 19:47:40,364:INFO: Dataset: univ                Batch:  3/29	Loss 42.5724 (42.8083)
2022-11-12 19:47:41,730:INFO: Dataset: univ                Batch:  4/29	Loss 44.9055 (43.2508)
2022-11-12 19:47:42,982:INFO: Dataset: univ                Batch:  5/29	Loss 41.6091 (42.8911)
2022-11-12 19:47:44,209:INFO: Dataset: univ                Batch:  6/29	Loss 42.1473 (42.7675)
2022-11-12 19:47:45,459:INFO: Dataset: univ                Batch:  7/29	Loss 43.4014 (42.8498)
2022-11-12 19:47:46,680:INFO: Dataset: univ                Batch:  8/29	Loss 42.9581 (42.8629)
2022-11-12 19:47:48,004:INFO: Dataset: univ                Batch:  9/29	Loss 42.6600 (42.8382)
2022-11-12 19:47:49,279:INFO: Dataset: univ                Batch: 10/29	Loss 41.7270 (42.7159)
2022-11-12 19:47:50,537:INFO: Dataset: univ                Batch: 11/29	Loss 42.3716 (42.6866)
2022-11-12 19:47:51,790:INFO: Dataset: univ                Batch: 12/29	Loss 41.7666 (42.6017)
2022-11-12 19:47:53,056:INFO: Dataset: univ                Batch: 13/29	Loss 42.6657 (42.6061)
2022-11-12 19:47:54,318:INFO: Dataset: univ                Batch: 14/29	Loss 42.4588 (42.5966)
2022-11-12 19:47:55,577:INFO: Dataset: univ                Batch: 15/29	Loss 42.1389 (42.5674)
2022-11-12 19:47:56,823:INFO: Dataset: univ                Batch: 16/29	Loss 42.6694 (42.5723)
2022-11-12 19:47:58,075:INFO: Dataset: univ                Batch: 17/29	Loss 41.5944 (42.5185)
2022-11-12 19:47:59,312:INFO: Dataset: univ                Batch: 18/29	Loss 40.7847 (42.4167)
2022-11-12 19:48:00,555:INFO: Dataset: univ                Batch: 19/29	Loss 42.1058 (42.4008)
2022-11-12 19:48:01,815:INFO: Dataset: univ                Batch: 20/29	Loss 41.5995 (42.3579)
2022-11-12 19:48:03,154:INFO: Dataset: univ                Batch: 21/29	Loss 41.1223 (42.3007)
2022-11-12 19:48:04,441:INFO: Dataset: univ                Batch: 22/29	Loss 40.9706 (42.2357)
2022-11-12 19:48:05,710:INFO: Dataset: univ                Batch: 23/29	Loss 40.1908 (42.1353)
2022-11-12 19:48:06,997:INFO: Dataset: univ                Batch: 24/29	Loss 40.5540 (42.0604)
2022-11-12 19:48:08,256:INFO: Dataset: univ                Batch: 25/29	Loss 41.5942 (42.0431)
2022-11-12 19:48:09,515:INFO: Dataset: univ                Batch: 26/29	Loss 41.0282 (42.0057)
2022-11-12 19:48:10,769:INFO: Dataset: univ                Batch: 27/29	Loss 40.6571 (41.9529)
2022-11-12 19:48:12,003:INFO: Dataset: univ                Batch: 28/29	Loss 41.6529 (41.9432)
2022-11-12 19:48:13,152:INFO: Dataset: univ                Batch: 29/29	Loss 15.3182 (41.5861)
2022-11-12 19:48:22,288:INFO: Dataset: zara1               Batch:  1/16	Loss 42.9920 (42.9920)
2022-11-12 19:48:23,543:INFO: Dataset: zara1               Batch:  2/16	Loss 43.2728 (43.1134)
2022-11-12 19:48:24,791:INFO: Dataset: zara1               Batch:  3/16	Loss 43.2285 (43.1530)
2022-11-12 19:48:26,026:INFO: Dataset: zara1               Batch:  4/16	Loss 43.6983 (43.2916)
2022-11-12 19:48:27,275:INFO: Dataset: zara1               Batch:  5/16	Loss 43.7701 (43.3840)
2022-11-12 19:48:28,528:INFO: Dataset: zara1               Batch:  6/16	Loss 43.7091 (43.4479)
2022-11-12 19:48:29,815:INFO: Dataset: zara1               Batch:  7/16	Loss 43.8492 (43.5055)
2022-11-12 19:48:31,143:INFO: Dataset: zara1               Batch:  8/16	Loss 43.4075 (43.4933)
2022-11-12 19:48:32,426:INFO: Dataset: zara1               Batch:  9/16	Loss 43.2844 (43.4710)
2022-11-12 19:48:33,682:INFO: Dataset: zara1               Batch: 10/16	Loss 42.8325 (43.4119)
2022-11-12 19:48:34,954:INFO: Dataset: zara1               Batch: 11/16	Loss 43.2242 (43.3946)
2022-11-12 19:48:36,306:INFO: Dataset: zara1               Batch: 12/16	Loss 42.9323 (43.3546)
2022-11-12 19:48:37,825:INFO: Dataset: zara1               Batch: 13/16	Loss 43.6665 (43.3789)
2022-11-12 19:48:39,243:INFO: Dataset: zara1               Batch: 14/16	Loss 43.9101 (43.4169)
2022-11-12 19:48:40,479:INFO: Dataset: zara1               Batch: 15/16	Loss 43.1140 (43.3923)
2022-11-12 19:48:41,712:INFO: Dataset: zara1               Batch: 16/16	Loss 30.6975 (42.8712)
2022-11-12 19:49:08,738:INFO: Dataset: zara2               Batch:  1/36	Loss 42.0632 (42.0632)
2022-11-12 19:49:10,055:INFO: Dataset: zara2               Batch:  2/36	Loss 42.1910 (42.1276)
2022-11-12 19:49:11,331:INFO: Dataset: zara2               Batch:  3/36	Loss 41.4289 (41.8893)
2022-11-12 19:49:12,688:INFO: Dataset: zara2               Batch:  4/36	Loss 40.5059 (41.5570)
2022-11-12 19:49:14,021:INFO: Dataset: zara2               Batch:  5/36	Loss 41.1764 (41.4948)
2022-11-12 19:49:15,449:INFO: Dataset: zara2               Batch:  6/36	Loss 41.2281 (41.4498)
2022-11-12 19:49:16,902:INFO: Dataset: zara2               Batch:  7/36	Loss 40.8974 (41.3599)
2022-11-12 19:49:18,272:INFO: Dataset: zara2               Batch:  8/36	Loss 41.9834 (41.4287)
2022-11-12 19:49:20,384:INFO: Dataset: zara2               Batch:  9/36	Loss 41.4779 (41.4339)
2022-11-12 19:49:21,756:INFO: Dataset: zara2               Batch: 10/36	Loss 41.8611 (41.4771)
2022-11-12 19:49:23,722:INFO: Dataset: zara2               Batch: 11/36	Loss 40.7287 (41.4094)
2022-11-12 19:49:26,056:INFO: Dataset: zara2               Batch: 12/36	Loss 41.4696 (41.4148)
2022-11-12 19:49:27,501:INFO: Dataset: zara2               Batch: 13/36	Loss 40.5647 (41.3521)
2022-11-12 19:49:28,830:INFO: Dataset: zara2               Batch: 14/36	Loss 41.0692 (41.3347)
2022-11-12 19:49:30,157:INFO: Dataset: zara2               Batch: 15/36	Loss 41.3174 (41.3338)
2022-11-12 19:49:31,441:INFO: Dataset: zara2               Batch: 16/36	Loss 40.7548 (41.3036)
2022-11-12 19:49:32,726:INFO: Dataset: zara2               Batch: 17/36	Loss 40.9730 (41.2828)
2022-11-12 19:49:33,922:INFO: Dataset: zara2               Batch: 18/36	Loss 41.8119 (41.3118)
2022-11-12 19:49:35,162:INFO: Dataset: zara2               Batch: 19/36	Loss 40.9453 (41.2932)
2022-11-12 19:49:36,398:INFO: Dataset: zara2               Batch: 20/36	Loss 40.9782 (41.2753)
2022-11-12 19:49:37,659:INFO: Dataset: zara2               Batch: 21/36	Loss 41.3168 (41.2774)
2022-11-12 19:49:38,911:INFO: Dataset: zara2               Batch: 22/36	Loss 41.1592 (41.2718)
2022-11-12 19:49:40,165:INFO: Dataset: zara2               Batch: 23/36	Loss 41.1684 (41.2675)
2022-11-12 19:49:41,411:INFO: Dataset: zara2               Batch: 24/36	Loss 41.5460 (41.2766)
2022-11-12 19:49:42,652:INFO: Dataset: zara2               Batch: 25/36	Loss 41.8047 (41.2957)
2022-11-12 19:49:43,859:INFO: Dataset: zara2               Batch: 26/36	Loss 41.4471 (41.3006)
2022-11-12 19:49:45,090:INFO: Dataset: zara2               Batch: 27/36	Loss 41.2506 (41.2987)
2022-11-12 19:49:46,316:INFO: Dataset: zara2               Batch: 28/36	Loss 41.3266 (41.2996)
2022-11-12 19:49:47,596:INFO: Dataset: zara2               Batch: 29/36	Loss 41.3706 (41.3024)
2022-11-12 19:49:48,836:INFO: Dataset: zara2               Batch: 30/36	Loss 41.8280 (41.3210)
2022-11-12 19:49:50,107:INFO: Dataset: zara2               Batch: 31/36	Loss 41.5753 (41.3278)
2022-11-12 19:49:51,353:INFO: Dataset: zara2               Batch: 32/36	Loss 40.3141 (41.2983)
2022-11-12 19:49:52,619:INFO: Dataset: zara2               Batch: 33/36	Loss 40.7529 (41.2815)
2022-11-12 19:49:53,864:INFO: Dataset: zara2               Batch: 34/36	Loss 40.5710 (41.2572)
2022-11-12 19:49:55,129:INFO: Dataset: zara2               Batch: 35/36	Loss 41.5218 (41.2639)
2022-11-12 19:49:56,356:INFO: Dataset: zara2               Batch: 36/36	Loss 29.6552 (41.0508)
2022-11-12 19:49:57,279:INFO: - Computing ADE (validation o)
2022-11-12 19:50:05,223:INFO: 		 ADE on eth                       dataset:	 3.379899501800537
2022-11-12 19:50:05,223:INFO: Average validation o:	ADE  3.3799	FDE  5.7328
2022-11-12 19:50:05,225:INFO: - Computing ADE (validation)
2022-11-12 19:50:13,731:INFO: 		 ADE on hotel                     dataset:	 0.9722310304641724
2022-11-12 19:50:23,898:INFO: 		 ADE on univ                      dataset:	 1.489396095275879
2022-11-12 19:50:33,661:INFO: 		 ADE on zara1                     dataset:	 2.49900221824646
2022-11-12 19:50:43,096:INFO: 		 ADE on zara2                     dataset:	 1.4204589128494263
2022-11-12 19:50:43,096:INFO: Average validation:	ADE  1.4945	FDE  2.7198
2022-11-12 19:50:43,098:INFO: - Computing ADE (training)
2022-11-12 19:50:53,448:INFO: 		 ADE on hotel                     dataset:	 1.3220760822296143
2022-11-12 19:51:21,086:INFO: 		 ADE on univ                      dataset:	 1.3794087171554565
2022-11-12 19:51:31,783:INFO: 		 ADE on zara1                     dataset:	 2.4301815032958984
2022-11-12 19:52:00,326:INFO: 		 ADE on zara2                     dataset:	 1.6737284660339355
2022-11-12 19:52:00,326:INFO: Average training:	ADE  1.5047	FDE  2.7444
2022-11-12 19:52:00,377:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_5.pth.tar
2022-11-12 19:52:00,378:INFO: 
===> EPOCH: 6 (P3)
2022-11-12 19:52:00,380:INFO: - Computing loss (training)
2022-11-12 19:52:10,488:INFO: Dataset: hotel               Batch: 1/8	Loss 40.1554 (40.1554)
2022-11-12 19:52:12,611:INFO: Dataset: hotel               Batch: 2/8	Loss 40.0426 (40.0967)
2022-11-12 19:52:14,463:INFO: Dataset: hotel               Batch: 3/8	Loss 40.1126 (40.1023)
2022-11-12 19:52:16,210:INFO: Dataset: hotel               Batch: 4/8	Loss 40.9851 (40.3154)
2022-11-12 19:52:17,915:INFO: Dataset: hotel               Batch: 5/8	Loss 41.3008 (40.5163)
2022-11-12 19:52:19,623:INFO: Dataset: hotel               Batch: 6/8	Loss 39.6711 (40.3664)
2022-11-12 19:52:21,483:INFO: Dataset: hotel               Batch: 7/8	Loss 40.8386 (40.4397)
2022-11-12 19:52:23,206:INFO: Dataset: hotel               Batch: 8/8	Loss 8.8292 (39.4389)
2022-11-12 19:52:50,584:INFO: Dataset: univ                Batch:  1/29	Loss 40.7317 (40.7317)
2022-11-12 19:52:52,340:INFO: Dataset: univ                Batch:  2/29	Loss 40.5692 (40.6469)
2022-11-12 19:52:54,092:INFO: Dataset: univ                Batch:  3/29	Loss 41.2581 (40.8308)
2022-11-12 19:52:55,933:INFO: Dataset: univ                Batch:  4/29	Loss 39.7155 (40.5157)
2022-11-12 19:52:57,569:INFO: Dataset: univ                Batch:  5/29	Loss 40.6655 (40.5466)
2022-11-12 19:52:59,299:INFO: Dataset: univ                Batch:  6/29	Loss 40.3869 (40.5186)
2022-11-12 19:53:00,990:INFO: Dataset: univ                Batch:  7/29	Loss 40.1793 (40.4716)
2022-11-12 19:53:02,653:INFO: Dataset: univ                Batch:  8/29	Loss 40.0329 (40.4131)
2022-11-12 19:53:04,287:INFO: Dataset: univ                Batch:  9/29	Loss 40.8375 (40.4577)
2022-11-12 19:53:05,836:INFO: Dataset: univ                Batch: 10/29	Loss 40.9509 (40.5003)
2022-11-12 19:53:07,446:INFO: Dataset: univ                Batch: 11/29	Loss 40.2228 (40.4731)
2022-11-12 19:53:09,035:INFO: Dataset: univ                Batch: 12/29	Loss 40.9513 (40.5127)
2022-11-12 19:53:10,740:INFO: Dataset: univ                Batch: 13/29	Loss 40.5424 (40.5146)
2022-11-12 19:53:12,418:INFO: Dataset: univ                Batch: 14/29	Loss 40.9156 (40.5427)
2022-11-12 19:53:14,169:INFO: Dataset: univ                Batch: 15/29	Loss 40.7654 (40.5559)
2022-11-12 19:53:15,778:INFO: Dataset: univ                Batch: 16/29	Loss 39.8729 (40.5089)
2022-11-12 19:53:17,436:INFO: Dataset: univ                Batch: 17/29	Loss 40.8845 (40.5275)
2022-11-12 19:53:19,162:INFO: Dataset: univ                Batch: 18/29	Loss 39.8063 (40.4783)
2022-11-12 19:53:20,795:INFO: Dataset: univ                Batch: 19/29	Loss 39.8061 (40.4381)
2022-11-12 19:53:22,565:INFO: Dataset: univ                Batch: 20/29	Loss 40.9845 (40.4648)
2022-11-12 19:53:24,164:INFO: Dataset: univ                Batch: 21/29	Loss 40.3578 (40.4594)
2022-11-12 19:53:25,823:INFO: Dataset: univ                Batch: 22/29	Loss 39.8806 (40.4314)
2022-11-12 19:53:27,423:INFO: Dataset: univ                Batch: 23/29	Loss 40.2027 (40.4212)
2022-11-12 19:53:29,101:INFO: Dataset: univ                Batch: 24/29	Loss 40.8096 (40.4358)
2022-11-12 19:53:30,745:INFO: Dataset: univ                Batch: 25/29	Loss 40.7665 (40.4476)
2022-11-12 19:53:32,475:INFO: Dataset: univ                Batch: 26/29	Loss 40.8911 (40.4615)
2022-11-12 19:53:34,345:INFO: Dataset: univ                Batch: 27/29	Loss 40.3468 (40.4573)
2022-11-12 19:53:36,170:INFO: Dataset: univ                Batch: 28/29	Loss 39.2255 (40.4083)
2022-11-12 19:53:37,710:INFO: Dataset: univ                Batch: 29/29	Loss 14.9588 (40.0743)
2022-11-12 19:53:47,826:INFO: Dataset: zara1               Batch:  1/16	Loss 41.3472 (41.3472)
2022-11-12 19:53:49,610:INFO: Dataset: zara1               Batch:  2/16	Loss 41.2741 (41.3040)
2022-11-12 19:53:51,294:INFO: Dataset: zara1               Batch:  3/16	Loss 40.8000 (41.1230)
2022-11-12 19:53:52,971:INFO: Dataset: zara1               Batch:  4/16	Loss 41.0000 (41.0903)
2022-11-12 19:53:54,608:INFO: Dataset: zara1               Batch:  5/16	Loss 40.6117 (40.9839)
2022-11-12 19:53:56,190:INFO: Dataset: zara1               Batch:  6/16	Loss 40.5089 (40.9041)
2022-11-12 19:53:57,862:INFO: Dataset: zara1               Batch:  7/16	Loss 40.5561 (40.8537)
2022-11-12 19:53:59,440:INFO: Dataset: zara1               Batch:  8/16	Loss 40.5747 (40.8231)
2022-11-12 19:54:01,249:INFO: Dataset: zara1               Batch:  9/16	Loss 40.3661 (40.7763)
2022-11-12 19:54:02,861:INFO: Dataset: zara1               Batch: 10/16	Loss 40.2289 (40.7309)
2022-11-12 19:54:04,541:INFO: Dataset: zara1               Batch: 11/16	Loss 40.3839 (40.7035)
2022-11-12 19:54:06,103:INFO: Dataset: zara1               Batch: 12/16	Loss 40.6628 (40.7001)
2022-11-12 19:54:07,688:INFO: Dataset: zara1               Batch: 13/16	Loss 40.2993 (40.6746)
2022-11-12 19:54:09,322:INFO: Dataset: zara1               Batch: 14/16	Loss 40.2563 (40.6402)
2022-11-12 19:54:10,887:INFO: Dataset: zara1               Batch: 15/16	Loss 40.4385 (40.6246)
2022-11-12 19:54:12,415:INFO: Dataset: zara1               Batch: 16/16	Loss 29.0416 (40.1125)
2022-11-12 19:54:38,075:INFO: Dataset: zara2               Batch:  1/36	Loss 40.0844 (40.0844)
2022-11-12 19:54:39,713:INFO: Dataset: zara2               Batch:  2/36	Loss 40.0217 (40.0489)
2022-11-12 19:54:41,382:INFO: Dataset: zara2               Batch:  3/36	Loss 39.7962 (39.9691)
2022-11-12 19:54:42,937:INFO: Dataset: zara2               Batch:  4/36	Loss 40.1123 (40.0025)
2022-11-12 19:54:44,496:INFO: Dataset: zara2               Batch:  5/36	Loss 39.8524 (39.9723)
2022-11-12 19:54:46,112:INFO: Dataset: zara2               Batch:  6/36	Loss 39.6140 (39.9114)
2022-11-12 19:54:47,684:INFO: Dataset: zara2               Batch:  7/36	Loss 39.9156 (39.9120)
2022-11-12 19:54:49,264:INFO: Dataset: zara2               Batch:  8/36	Loss 40.1914 (39.9456)
2022-11-12 19:54:50,833:INFO: Dataset: zara2               Batch:  9/36	Loss 39.8958 (39.9402)
2022-11-12 19:54:52,395:INFO: Dataset: zara2               Batch: 10/36	Loss 39.7597 (39.9214)
2022-11-12 19:54:53,973:INFO: Dataset: zara2               Batch: 11/36	Loss 39.7303 (39.9036)
2022-11-12 19:54:55,504:INFO: Dataset: zara2               Batch: 12/36	Loss 39.6408 (39.8826)
2022-11-12 19:54:57,075:INFO: Dataset: zara2               Batch: 13/36	Loss 39.9466 (39.8871)
2022-11-12 19:54:58,686:INFO: Dataset: zara2               Batch: 14/36	Loss 39.9730 (39.8929)
2022-11-12 19:55:00,364:INFO: Dataset: zara2               Batch: 15/36	Loss 40.1937 (39.9138)
2022-11-12 19:55:01,964:INFO: Dataset: zara2               Batch: 16/36	Loss 39.6519 (39.8988)
2022-11-12 19:55:03,610:INFO: Dataset: zara2               Batch: 17/36	Loss 39.7936 (39.8937)
2022-11-12 19:55:05,227:INFO: Dataset: zara2               Batch: 18/36	Loss 39.6713 (39.8807)
2022-11-12 19:55:06,880:INFO: Dataset: zara2               Batch: 19/36	Loss 39.8869 (39.8811)
2022-11-12 19:55:08,512:INFO: Dataset: zara2               Batch: 20/36	Loss 39.9751 (39.8853)
2022-11-12 19:55:10,129:INFO: Dataset: zara2               Batch: 21/36	Loss 39.8326 (39.8829)
2022-11-12 19:55:11,710:INFO: Dataset: zara2               Batch: 22/36	Loss 39.9682 (39.8867)
2022-11-12 19:55:13,286:INFO: Dataset: zara2               Batch: 23/36	Loss 39.6841 (39.8775)
2022-11-12 19:55:14,891:INFO: Dataset: zara2               Batch: 24/36	Loss 39.5407 (39.8635)
2022-11-12 19:55:16,471:INFO: Dataset: zara2               Batch: 25/36	Loss 39.6810 (39.8560)
2022-11-12 19:55:18,066:INFO: Dataset: zara2               Batch: 26/36	Loss 39.6929 (39.8498)
2022-11-12 19:55:19,697:INFO: Dataset: zara2               Batch: 27/36	Loss 39.6781 (39.8422)
2022-11-12 19:55:21,226:INFO: Dataset: zara2               Batch: 28/36	Loss 39.7666 (39.8399)
2022-11-12 19:55:22,710:INFO: Dataset: zara2               Batch: 29/36	Loss 39.6179 (39.8336)
2022-11-12 19:55:24,256:INFO: Dataset: zara2               Batch: 30/36	Loss 39.9498 (39.8372)
2022-11-12 19:55:25,897:INFO: Dataset: zara2               Batch: 31/36	Loss 39.9942 (39.8418)
2022-11-12 19:55:27,440:INFO: Dataset: zara2               Batch: 32/36	Loss 39.6937 (39.8373)
2022-11-12 19:55:28,982:INFO: Dataset: zara2               Batch: 33/36	Loss 39.7930 (39.8359)
2022-11-12 19:55:30,514:INFO: Dataset: zara2               Batch: 34/36	Loss 39.6303 (39.8303)
2022-11-12 19:55:32,169:INFO: Dataset: zara2               Batch: 35/36	Loss 39.5678 (39.8225)
2022-11-12 19:55:34,263:INFO: Dataset: zara2               Batch: 36/36	Loss 28.5377 (39.6397)
2022-11-12 19:55:35,280:INFO: - Computing ADE (validation o)
2022-11-12 19:55:44,483:INFO: 		 ADE on eth                       dataset:	 2.9519691467285156
2022-11-12 19:55:44,483:INFO: Average validation o:	ADE  2.9520	FDE  4.9789
2022-11-12 19:55:44,484:INFO: - Computing ADE (validation)
2022-11-12 19:55:54,043:INFO: 		 ADE on hotel                     dataset:	 1.7283092737197876
2022-11-12 19:56:03,770:INFO: 		 ADE on univ                      dataset:	 1.7369945049285889
2022-11-12 19:56:13,828:INFO: 		 ADE on zara1                     dataset:	 2.1085400581359863
2022-11-12 19:56:23,709:INFO: 		 ADE on zara2                     dataset:	 1.6639991998672485
2022-11-12 19:56:23,710:INFO: Average validation:	ADE  1.7313	FDE  3.2473
2022-11-12 19:56:23,711:INFO: - Computing ADE (training)
2022-11-12 19:56:33,634:INFO: 		 ADE on hotel                     dataset:	 1.944890022277832
2022-11-12 19:57:00,787:INFO: 		 ADE on univ                      dataset:	 1.6741007566452026
2022-11-12 19:57:10,906:INFO: 		 ADE on zara1                     dataset:	 2.1709229946136475
2022-11-12 19:57:37,865:INFO: 		 ADE on zara2                     dataset:	 1.859325885772705
2022-11-12 19:57:37,866:INFO: Average training:	ADE  1.7502	FDE  3.2805
2022-11-12 19:57:37,903:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_6.pth.tar
2022-11-12 19:57:37,903:INFO: 
===> EPOCH: 7 (P4)
2022-11-12 19:57:37,905:INFO: - Computing loss (training)
2022-11-12 19:57:47,636:INFO: Dataset: hotel               Batch: 1/8	Loss 41.1184 (41.1184)
2022-11-12 19:57:49,640:INFO: Dataset: hotel               Batch: 2/8	Loss 40.7920 (40.9416)
2022-11-12 19:57:51,564:INFO: Dataset: hotel               Batch: 3/8	Loss 40.6661 (40.8470)
2022-11-12 19:57:53,744:INFO: Dataset: hotel               Batch: 4/8	Loss 38.8584 (40.3487)
2022-11-12 19:57:55,553:INFO: Dataset: hotel               Batch: 5/8	Loss 42.1743 (40.6819)
2022-11-12 19:57:57,434:INFO: Dataset: hotel               Batch: 6/8	Loss 40.6940 (40.6838)
2022-11-12 19:57:59,214:INFO: Dataset: hotel               Batch: 7/8	Loss 40.3193 (40.6361)
2022-11-12 19:58:01,072:INFO: Dataset: hotel               Batch: 8/8	Loss 8.8201 (39.6287)
2022-11-12 19:58:30,300:INFO: Dataset: univ                Batch:  1/29	Loss 41.3694 (41.3694)
2022-11-12 19:58:32,013:INFO: Dataset: univ                Batch:  2/29	Loss 42.2346 (41.8399)
2022-11-12 19:58:33,571:INFO: Dataset: univ                Batch:  3/29	Loss 41.3151 (41.6706)
2022-11-12 19:58:35,158:INFO: Dataset: univ                Batch:  4/29	Loss 42.7924 (41.9021)
2022-11-12 19:58:36,821:INFO: Dataset: univ                Batch:  5/29	Loss 42.2261 (41.9638)
2022-11-12 19:58:38,632:INFO: Dataset: univ                Batch:  6/29	Loss 41.1225 (41.8143)
2022-11-12 19:58:40,447:INFO: Dataset: univ                Batch:  7/29	Loss 40.9025 (41.6663)
2022-11-12 19:58:42,003:INFO: Dataset: univ                Batch:  8/29	Loss 42.7684 (41.7879)
2022-11-12 19:58:43,510:INFO: Dataset: univ                Batch:  9/29	Loss 41.7514 (41.7843)
2022-11-12 19:58:45,013:INFO: Dataset: univ                Batch: 10/29	Loss 41.3331 (41.7435)
2022-11-12 19:58:46,668:INFO: Dataset: univ                Batch: 11/29	Loss 41.3295 (41.7081)
2022-11-12 19:58:48,491:INFO: Dataset: univ                Batch: 12/29	Loss 40.4151 (41.5986)
2022-11-12 19:58:50,330:INFO: Dataset: univ                Batch: 13/29	Loss 40.5760 (41.5218)
2022-11-12 19:58:51,993:INFO: Dataset: univ                Batch: 14/29	Loss 40.9275 (41.4818)
2022-11-12 19:58:53,515:INFO: Dataset: univ                Batch: 15/29	Loss 41.1196 (41.4590)
2022-11-12 19:58:55,056:INFO: Dataset: univ                Batch: 16/29	Loss 40.6056 (41.4070)
2022-11-12 19:58:56,741:INFO: Dataset: univ                Batch: 17/29	Loss 40.3175 (41.3447)
2022-11-12 19:58:58,511:INFO: Dataset: univ                Batch: 18/29	Loss 40.1475 (41.2742)
2022-11-12 19:59:00,344:INFO: Dataset: univ                Batch: 19/29	Loss 39.3674 (41.1700)
2022-11-12 19:59:02,019:INFO: Dataset: univ                Batch: 20/29	Loss 41.1837 (41.1706)
2022-11-12 19:59:03,587:INFO: Dataset: univ                Batch: 21/29	Loss 40.2953 (41.1324)
2022-11-12 19:59:05,155:INFO: Dataset: univ                Batch: 22/29	Loss 40.8572 (41.1203)
2022-11-12 19:59:06,824:INFO: Dataset: univ                Batch: 23/29	Loss 39.2664 (41.0340)
2022-11-12 19:59:08,706:INFO: Dataset: univ                Batch: 24/29	Loss 39.5177 (40.9726)
2022-11-12 19:59:10,506:INFO: Dataset: univ                Batch: 25/29	Loss 39.2763 (40.9022)
2022-11-12 19:59:12,183:INFO: Dataset: univ                Batch: 26/29	Loss 38.9102 (40.8203)
2022-11-12 19:59:13,747:INFO: Dataset: univ                Batch: 27/29	Loss 40.2381 (40.8012)
2022-11-12 19:59:15,279:INFO: Dataset: univ                Batch: 28/29	Loss 39.9921 (40.7777)
2022-11-12 19:59:16,797:INFO: Dataset: univ                Batch: 29/29	Loss 14.7642 (40.4424)
2022-11-12 19:59:27,764:INFO: Dataset: zara1               Batch:  1/16	Loss 44.0097 (44.0097)
2022-11-12 19:59:29,459:INFO: Dataset: zara1               Batch:  2/16	Loss 44.4889 (44.2661)
2022-11-12 19:59:31,212:INFO: Dataset: zara1               Batch:  3/16	Loss 44.6601 (44.4106)
2022-11-12 19:59:32,921:INFO: Dataset: zara1               Batch:  4/16	Loss 44.3428 (44.3931)
2022-11-12 19:59:34,529:INFO: Dataset: zara1               Batch:  5/16	Loss 43.6446 (44.2496)
2022-11-12 19:59:36,071:INFO: Dataset: zara1               Batch:  6/16	Loss 42.9125 (44.0130)
2022-11-12 19:59:37,689:INFO: Dataset: zara1               Batch:  7/16	Loss 43.4871 (43.9376)
2022-11-12 19:59:39,451:INFO: Dataset: zara1               Batch:  8/16	Loss 43.7349 (43.9137)
2022-11-12 19:59:41,137:INFO: Dataset: zara1               Batch:  9/16	Loss 42.9276 (43.7982)
2022-11-12 19:59:42,729:INFO: Dataset: zara1               Batch: 10/16	Loss 43.7863 (43.7970)
2022-11-12 19:59:44,216:INFO: Dataset: zara1               Batch: 11/16	Loss 42.6252 (43.6966)
2022-11-12 19:59:45,773:INFO: Dataset: zara1               Batch: 12/16	Loss 42.4945 (43.5980)
2022-11-12 19:59:47,343:INFO: Dataset: zara1               Batch: 13/16	Loss 42.8511 (43.5396)
2022-11-12 19:59:49,108:INFO: Dataset: zara1               Batch: 14/16	Loss 42.2225 (43.4383)
2022-11-12 19:59:50,911:INFO: Dataset: zara1               Batch: 15/16	Loss 43.4390 (43.4383)
2022-11-12 19:59:52,553:INFO: Dataset: zara1               Batch: 16/16	Loss 31.1467 (43.0049)
2022-11-12 20:00:20,112:INFO: Dataset: zara2               Batch:  1/36	Loss 40.8597 (40.8597)
2022-11-12 20:00:21,853:INFO: Dataset: zara2               Batch:  2/36	Loss 41.2694 (41.0834)
2022-11-12 20:00:23,424:INFO: Dataset: zara2               Batch:  3/36	Loss 40.4662 (40.8662)
2022-11-12 20:00:25,013:INFO: Dataset: zara2               Batch:  4/36	Loss 41.1359 (40.9266)
2022-11-12 20:00:26,502:INFO: Dataset: zara2               Batch:  5/36	Loss 41.1841 (40.9762)
2022-11-12 20:00:28,090:INFO: Dataset: zara2               Batch:  6/36	Loss 40.5362 (40.9010)
2022-11-12 20:00:29,801:INFO: Dataset: zara2               Batch:  7/36	Loss 41.1121 (40.9297)
2022-11-12 20:00:31,530:INFO: Dataset: zara2               Batch:  8/36	Loss 41.1450 (40.9588)
2022-11-12 20:00:33,330:INFO: Dataset: zara2               Batch:  9/36	Loss 40.4514 (40.9001)
2022-11-12 20:00:34,893:INFO: Dataset: zara2               Batch: 10/36	Loss 39.8258 (40.7992)
2022-11-12 20:00:36,481:INFO: Dataset: zara2               Batch: 11/36	Loss 40.0299 (40.7358)
2022-11-12 20:00:38,046:INFO: Dataset: zara2               Batch: 12/36	Loss 38.7069 (40.5589)
2022-11-12 20:00:39,917:INFO: Dataset: zara2               Batch: 13/36	Loss 39.8497 (40.5050)
2022-11-12 20:00:41,647:INFO: Dataset: zara2               Batch: 14/36	Loss 39.2483 (40.4158)
2022-11-12 20:00:43,342:INFO: Dataset: zara2               Batch: 15/36	Loss 39.7180 (40.3574)
2022-11-12 20:00:44,871:INFO: Dataset: zara2               Batch: 16/36	Loss 39.3985 (40.2949)
2022-11-12 20:00:46,449:INFO: Dataset: zara2               Batch: 17/36	Loss 39.0214 (40.2173)
2022-11-12 20:00:47,975:INFO: Dataset: zara2               Batch: 18/36	Loss 38.6721 (40.1286)
2022-11-12 20:00:49,881:INFO: Dataset: zara2               Batch: 19/36	Loss 38.8366 (40.0668)
2022-11-12 20:00:51,610:INFO: Dataset: zara2               Batch: 20/36	Loss 38.4559 (39.9891)
2022-11-12 20:00:53,331:INFO: Dataset: zara2               Batch: 21/36	Loss 38.7571 (39.9432)
2022-11-12 20:00:54,916:INFO: Dataset: zara2               Batch: 22/36	Loss 38.6787 (39.8751)
2022-11-12 20:00:56,463:INFO: Dataset: zara2               Batch: 23/36	Loss 38.0075 (39.7915)
2022-11-12 20:00:58,040:INFO: Dataset: zara2               Batch: 24/36	Loss 37.9749 (39.7222)
2022-11-12 20:00:59,763:INFO: Dataset: zara2               Batch: 25/36	Loss 38.0036 (39.6556)
2022-11-12 20:01:01,491:INFO: Dataset: zara2               Batch: 26/36	Loss 37.8786 (39.5811)
2022-11-12 20:01:03,327:INFO: Dataset: zara2               Batch: 27/36	Loss 37.8268 (39.5053)
2022-11-12 20:01:04,847:INFO: Dataset: zara2               Batch: 28/36	Loss 37.9814 (39.4589)
2022-11-12 20:01:06,420:INFO: Dataset: zara2               Batch: 29/36	Loss 37.8832 (39.4037)
2022-11-12 20:01:08,019:INFO: Dataset: zara2               Batch: 30/36	Loss 37.5904 (39.3436)
2022-11-12 20:01:09,864:INFO: Dataset: zara2               Batch: 31/36	Loss 37.6150 (39.2792)
2022-11-12 20:01:11,644:INFO: Dataset: zara2               Batch: 32/36	Loss 37.4827 (39.2269)
2022-11-12 20:01:13,388:INFO: Dataset: zara2               Batch: 33/36	Loss 37.2099 (39.1726)
2022-11-12 20:01:14,900:INFO: Dataset: zara2               Batch: 34/36	Loss 36.8959 (39.0911)
2022-11-12 20:01:16,474:INFO: Dataset: zara2               Batch: 35/36	Loss 37.1311 (39.0367)
2022-11-12 20:01:17,979:INFO: Dataset: zara2               Batch: 36/36	Loss 27.0003 (38.8318)
2022-11-12 20:01:19,024:INFO: - Computing ADE (validation)
2022-11-12 20:01:29,627:INFO: 		 ADE on hotel                     dataset:	 1.6478945016860962
2022-11-12 20:01:39,887:INFO: 		 ADE on univ                      dataset:	 1.7640339136123657
2022-11-12 20:01:50,194:INFO: 		 ADE on zara1                     dataset:	 2.3505806922912598
2022-11-12 20:02:00,735:INFO: 		 ADE on zara2                     dataset:	 1.7802906036376953
2022-11-12 20:02:00,737:INFO: Average validation:	ADE  1.7977	FDE  3.3043
2022-11-12 20:02:00,738:INFO: - Computing ADE (training)
2022-11-12 20:02:10,977:INFO: 		 ADE on hotel                     dataset:	 1.8745696544647217
2022-11-12 20:02:39,674:INFO: 		 ADE on univ                      dataset:	 1.6704654693603516
2022-11-12 20:02:50,704:INFO: 		 ADE on zara1                     dataset:	 2.2927896976470947
2022-11-12 20:03:20,159:INFO: 		 ADE on zara2                     dataset:	 1.9076982736587524
2022-11-12 20:03:20,159:INFO: Average training:	ADE  1.7635	FDE  3.2377
2022-11-12 20:03:20,199:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_7.pth.tar
2022-11-12 20:03:20,200:INFO: 
===> EPOCH: 8 (P4)
2022-11-12 20:03:20,201:INFO: - Computing loss (training)
2022-11-12 20:03:30,037:INFO: Dataset: hotel               Batch: 1/8	Loss 39.1734 (39.1734)
2022-11-12 20:03:32,149:INFO: Dataset: hotel               Batch: 2/8	Loss 39.6902 (39.4331)
2022-11-12 20:03:34,153:INFO: Dataset: hotel               Batch: 3/8	Loss 39.2678 (39.3775)
2022-11-12 20:03:36,117:INFO: Dataset: hotel               Batch: 4/8	Loss 39.9837 (39.5268)
2022-11-12 20:03:38,117:INFO: Dataset: hotel               Batch: 5/8	Loss 38.9163 (39.4024)
2022-11-12 20:03:39,970:INFO: Dataset: hotel               Batch: 6/8	Loss 38.0556 (39.1779)
2022-11-12 20:03:41,881:INFO: Dataset: hotel               Batch: 7/8	Loss 38.5140 (39.0878)
2022-11-12 20:03:43,834:INFO: Dataset: hotel               Batch: 8/8	Loss 8.3854 (37.9131)
2022-11-12 20:04:12,915:INFO: Dataset: univ                Batch:  1/29	Loss 39.0588 (39.0588)
2022-11-12 20:04:14,816:INFO: Dataset: univ                Batch:  2/29	Loss 38.0556 (38.5268)
2022-11-12 20:04:16,690:INFO: Dataset: univ                Batch:  3/29	Loss 38.5078 (38.5204)
2022-11-12 20:04:18,373:INFO: Dataset: univ                Batch:  4/29	Loss 38.5103 (38.5177)
2022-11-12 20:04:20,105:INFO: Dataset: univ                Batch:  5/29	Loss 38.6431 (38.5419)
2022-11-12 20:04:21,966:INFO: Dataset: univ                Batch:  6/29	Loss 38.5384 (38.5413)
2022-11-12 20:04:23,732:INFO: Dataset: univ                Batch:  7/29	Loss 38.2044 (38.4947)
2022-11-12 20:04:25,516:INFO: Dataset: univ                Batch:  8/29	Loss 38.6811 (38.5183)
2022-11-12 20:04:27,272:INFO: Dataset: univ                Batch:  9/29	Loss 37.8011 (38.4330)
2022-11-12 20:04:28,804:INFO: Dataset: univ                Batch: 10/29	Loss 38.2407 (38.4149)
2022-11-12 20:04:30,219:INFO: Dataset: univ                Batch: 11/29	Loss 38.0217 (38.3760)
2022-11-12 20:04:31,610:INFO: Dataset: univ                Batch: 12/29	Loss 38.1849 (38.3606)
2022-11-12 20:04:33,045:INFO: Dataset: univ                Batch: 13/29	Loss 37.8333 (38.3162)
2022-11-12 20:04:34,440:INFO: Dataset: univ                Batch: 14/29	Loss 37.3122 (38.2352)
2022-11-12 20:04:35,855:INFO: Dataset: univ                Batch: 15/29	Loss 37.1857 (38.1574)
2022-11-12 20:04:37,294:INFO: Dataset: univ                Batch: 16/29	Loss 37.5830 (38.1223)
2022-11-12 20:04:38,716:INFO: Dataset: univ                Batch: 17/29	Loss 37.4996 (38.0869)
2022-11-12 20:04:40,118:INFO: Dataset: univ                Batch: 18/29	Loss 37.5650 (38.0600)
2022-11-12 20:04:41,524:INFO: Dataset: univ                Batch: 19/29	Loss 37.3678 (38.0238)
2022-11-12 20:04:42,950:INFO: Dataset: univ                Batch: 20/29	Loss 37.5145 (37.9987)
2022-11-12 20:04:44,377:INFO: Dataset: univ                Batch: 21/29	Loss 37.1308 (37.9549)
2022-11-12 20:04:45,771:INFO: Dataset: univ                Batch: 22/29	Loss 37.6383 (37.9416)
2022-11-12 20:04:47,173:INFO: Dataset: univ                Batch: 23/29	Loss 37.4606 (37.9209)
2022-11-12 20:04:48,613:INFO: Dataset: univ                Batch: 24/29	Loss 37.0288 (37.8804)
2022-11-12 20:04:50,026:INFO: Dataset: univ                Batch: 25/29	Loss 36.9219 (37.8439)
2022-11-12 20:04:51,443:INFO: Dataset: univ                Batch: 26/29	Loss 36.3760 (37.7820)
2022-11-12 20:04:52,944:INFO: Dataset: univ                Batch: 27/29	Loss 36.9405 (37.7552)
2022-11-12 20:04:54,369:INFO: Dataset: univ                Batch: 28/29	Loss 36.3236 (37.7005)
2022-11-12 20:04:55,672:INFO: Dataset: univ                Batch: 29/29	Loss 13.9997 (37.4612)
2022-11-12 20:05:05,127:INFO: Dataset: zara1               Batch:  1/16	Loss 39.3407 (39.3407)
2022-11-12 20:05:06,535:INFO: Dataset: zara1               Batch:  2/16	Loss 38.9129 (39.1213)
2022-11-12 20:05:07,964:INFO: Dataset: zara1               Batch:  3/16	Loss 39.0366 (39.0942)
2022-11-12 20:05:09,396:INFO: Dataset: zara1               Batch:  4/16	Loss 38.9436 (39.0509)
2022-11-12 20:05:10,855:INFO: Dataset: zara1               Batch:  5/16	Loss 38.9568 (39.0344)
2022-11-12 20:05:12,291:INFO: Dataset: zara1               Batch:  6/16	Loss 38.8241 (39.0001)
2022-11-12 20:05:13,693:INFO: Dataset: zara1               Batch:  7/16	Loss 38.7660 (38.9611)
2022-11-12 20:05:15,115:INFO: Dataset: zara1               Batch:  8/16	Loss 38.7540 (38.9305)
2022-11-12 20:05:16,604:INFO: Dataset: zara1               Batch:  9/16	Loss 38.4441 (38.8762)
2022-11-12 20:05:18,106:INFO: Dataset: zara1               Batch: 10/16	Loss 38.3915 (38.8328)
2022-11-12 20:05:19,630:INFO: Dataset: zara1               Batch: 11/16	Loss 38.3942 (38.7956)
2022-11-12 20:05:21,209:INFO: Dataset: zara1               Batch: 12/16	Loss 38.3242 (38.7578)
2022-11-12 20:05:22,750:INFO: Dataset: zara1               Batch: 13/16	Loss 38.1399 (38.7134)
2022-11-12 20:05:24,282:INFO: Dataset: zara1               Batch: 14/16	Loss 37.9034 (38.6578)
2022-11-12 20:05:25,716:INFO: Dataset: zara1               Batch: 15/16	Loss 38.0249 (38.6187)
2022-11-12 20:05:27,123:INFO: Dataset: zara1               Batch: 16/16	Loss 27.1383 (38.0145)
2022-11-12 20:05:51,732:INFO: Dataset: zara2               Batch:  1/36	Loss 36.5398 (36.5398)
2022-11-12 20:05:53,159:INFO: Dataset: zara2               Batch:  2/36	Loss 36.6901 (36.6072)
2022-11-12 20:05:54,655:INFO: Dataset: zara2               Batch:  3/36	Loss 36.4116 (36.5489)
2022-11-12 20:05:56,466:INFO: Dataset: zara2               Batch:  4/36	Loss 36.3828 (36.5083)
2022-11-12 20:05:57,981:INFO: Dataset: zara2               Batch:  5/36	Loss 36.2567 (36.4649)
2022-11-12 20:05:59,421:INFO: Dataset: zara2               Batch:  6/36	Loss 36.2475 (36.4315)
2022-11-12 20:06:00,892:INFO: Dataset: zara2               Batch:  7/36	Loss 36.0405 (36.3783)
2022-11-12 20:06:02,344:INFO: Dataset: zara2               Batch:  8/36	Loss 36.2824 (36.3670)
2022-11-12 20:06:03,820:INFO: Dataset: zara2               Batch:  9/36	Loss 36.1315 (36.3387)
2022-11-12 20:06:05,254:INFO: Dataset: zara2               Batch: 10/36	Loss 35.9381 (36.2993)
2022-11-12 20:06:06,710:INFO: Dataset: zara2               Batch: 11/36	Loss 36.5877 (36.3240)
2022-11-12 20:06:08,278:INFO: Dataset: zara2               Batch: 12/36	Loss 36.0188 (36.2975)
2022-11-12 20:06:09,925:INFO: Dataset: zara2               Batch: 13/36	Loss 36.0819 (36.2815)
2022-11-12 20:06:11,482:INFO: Dataset: zara2               Batch: 14/36	Loss 35.9185 (36.2540)
2022-11-12 20:06:12,903:INFO: Dataset: zara2               Batch: 15/36	Loss 35.8081 (36.2239)
2022-11-12 20:06:14,353:INFO: Dataset: zara2               Batch: 16/36	Loss 35.6553 (36.1892)
2022-11-12 20:06:15,810:INFO: Dataset: zara2               Batch: 17/36	Loss 35.6738 (36.1543)
2022-11-12 20:06:17,236:INFO: Dataset: zara2               Batch: 18/36	Loss 35.7898 (36.1355)
2022-11-12 20:06:18,814:INFO: Dataset: zara2               Batch: 19/36	Loss 35.3561 (36.0946)
2022-11-12 20:06:20,467:INFO: Dataset: zara2               Batch: 20/36	Loss 35.3233 (36.0586)
2022-11-12 20:06:22,071:INFO: Dataset: zara2               Batch: 21/36	Loss 35.3576 (36.0246)
2022-11-12 20:06:23,483:INFO: Dataset: zara2               Batch: 22/36	Loss 35.4801 (35.9990)
2022-11-12 20:06:24,942:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1840 (35.9619)
2022-11-12 20:06:26,463:INFO: Dataset: zara2               Batch: 24/36	Loss 35.2324 (35.9373)
2022-11-12 20:06:27,926:INFO: Dataset: zara2               Batch: 25/36	Loss 35.0475 (35.8977)
2022-11-12 20:06:29,373:INFO: Dataset: zara2               Batch: 26/36	Loss 34.7075 (35.8559)
2022-11-12 20:06:30,751:INFO: Dataset: zara2               Batch: 27/36	Loss 34.8326 (35.8134)
2022-11-12 20:06:32,191:INFO: Dataset: zara2               Batch: 28/36	Loss 34.7809 (35.7738)
2022-11-12 20:06:33,572:INFO: Dataset: zara2               Batch: 29/36	Loss 34.8695 (35.7430)
2022-11-12 20:06:35,011:INFO: Dataset: zara2               Batch: 30/36	Loss 34.9648 (35.7163)
2022-11-12 20:06:36,433:INFO: Dataset: zara2               Batch: 31/36	Loss 34.7379 (35.6847)
2022-11-12 20:06:37,883:INFO: Dataset: zara2               Batch: 32/36	Loss 34.5028 (35.6486)
2022-11-12 20:06:39,371:INFO: Dataset: zara2               Batch: 33/36	Loss 34.2809 (35.6014)
2022-11-12 20:06:40,779:INFO: Dataset: zara2               Batch: 34/36	Loss 34.4666 (35.5670)
2022-11-12 20:06:42,159:INFO: Dataset: zara2               Batch: 35/36	Loss 34.3742 (35.5313)
2022-11-12 20:06:43,520:INFO: Dataset: zara2               Batch: 36/36	Loss 24.7432 (35.3494)
2022-11-12 20:06:44,447:INFO: - Computing ADE (validation)
2022-11-12 20:06:53,241:INFO: 		 ADE on hotel                     dataset:	 1.656113862991333
2022-11-12 20:07:02,055:INFO: 		 ADE on univ                      dataset:	 1.710977554321289
2022-11-12 20:07:10,715:INFO: 		 ADE on zara1                     dataset:	 1.985115885734558
2022-11-12 20:07:19,845:INFO: 		 ADE on zara2                     dataset:	 1.6513582468032837
2022-11-12 20:07:19,846:INFO: Average validation:	ADE  1.7020	FDE  3.1617
2022-11-12 20:07:19,848:INFO: - Computing ADE (training)
2022-11-12 20:07:28,770:INFO: 		 ADE on hotel                     dataset:	 1.9530059099197388
2022-11-12 20:07:53,702:INFO: 		 ADE on univ                      dataset:	 1.6507502794265747
2022-11-12 20:08:02,981:INFO: 		 ADE on zara1                     dataset:	 2.0599300861358643
2022-11-12 20:08:28,578:INFO: 		 ADE on zara2                     dataset:	 1.7873954772949219
2022-11-12 20:08:28,579:INFO: Average training:	ADE  1.7122	FDE  3.1781
2022-11-12 20:08:28,618:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_8.pth.tar
2022-11-12 20:08:28,618:INFO: 
===> EPOCH: 9 (P5)
2022-11-12 20:08:28,620:INFO: - Computing loss (training)
2022-11-12 20:08:34,882:INFO: Dataset: hotel               Batch: 1/8	Loss 17.7730 (17.7730)
2022-11-12 20:08:35,331:INFO: Dataset: hotel               Batch: 2/8	Loss 16.7499 (17.2475)
2022-11-12 20:08:35,603:INFO: Dataset: hotel               Batch: 3/8	Loss 15.5745 (16.6966)
2022-11-12 20:08:35,901:INFO: Dataset: hotel               Batch: 4/8	Loss 13.9016 (16.0091)
2022-11-12 20:08:36,188:INFO: Dataset: hotel               Batch: 5/8	Loss 11.8825 (15.2504)
2022-11-12 20:08:36,371:INFO: Dataset: hotel               Batch: 6/8	Loss 9.7936 (14.4176)
2022-11-12 20:08:36,377:INFO: Dataset: hotel               Batch: 7/8	Loss 8.3431 (13.5486)
2022-11-12 20:08:36,381:INFO: Dataset: hotel               Batch: 8/8	Loss 6.4044 (13.3224)
2022-11-12 20:08:59,510:INFO: Dataset: univ                Batch:  1/29	Loss 22.6586 (22.6586)
2022-11-12 20:08:59,516:INFO: Dataset: univ                Batch:  2/29	Loss 21.5242 (22.0538)
2022-11-12 20:08:59,522:INFO: Dataset: univ                Batch:  3/29	Loss 18.5698 (20.8701)
2022-11-12 20:08:59,532:INFO: Dataset: univ                Batch:  4/29	Loss 16.0718 (19.5686)
2022-11-12 20:08:59,545:INFO: Dataset: univ                Batch:  5/29	Loss 13.3652 (18.3063)
2022-11-12 20:08:59,551:INFO: Dataset: univ                Batch:  6/29	Loss 11.1883 (16.9897)
2022-11-12 20:08:59,558:INFO: Dataset: univ                Batch:  7/29	Loss 9.5579 (15.9740)
2022-11-12 20:08:59,565:INFO: Dataset: univ                Batch:  8/29	Loss 8.2170 (15.0426)
2022-11-12 20:08:59,572:INFO: Dataset: univ                Batch:  9/29	Loss 7.1222 (14.0843)
2022-11-12 20:08:59,580:INFO: Dataset: univ                Batch: 10/29	Loss 6.1268 (13.2001)
2022-11-12 20:08:59,585:INFO: Dataset: univ                Batch: 11/29	Loss 5.2186 (12.4761)
2022-11-12 20:08:59,592:INFO: Dataset: univ                Batch: 12/29	Loss 4.2506 (11.7995)
2022-11-12 20:08:59,598:INFO: Dataset: univ                Batch: 13/29	Loss 3.5299 (11.1639)
2022-11-12 20:08:59,604:INFO: Dataset: univ                Batch: 14/29	Loss 2.7530 (10.6119)
2022-11-12 20:08:59,610:INFO: Dataset: univ                Batch: 15/29	Loss 2.1587 (10.1225)
2022-11-12 20:08:59,614:INFO: Dataset: univ                Batch: 16/29	Loss 1.6999 (9.5802)
2022-11-12 20:08:59,620:INFO: Dataset: univ                Batch: 17/29	Loss 1.4025 (9.0928)
2022-11-12 20:08:59,626:INFO: Dataset: univ                Batch: 18/29	Loss 1.1783 (8.6615)
2022-11-12 20:08:59,632:INFO: Dataset: univ                Batch: 19/29	Loss 1.2216 (8.3080)
2022-11-12 20:08:59,638:INFO: Dataset: univ                Batch: 20/29	Loss 1.2524 (8.0027)
2022-11-12 20:08:59,643:INFO: Dataset: univ                Batch: 21/29	Loss 1.0463 (7.6684)
2022-11-12 20:08:59,648:INFO: Dataset: univ                Batch: 22/29	Loss 0.9736 (7.3884)
2022-11-12 20:08:59,653:INFO: Dataset: univ                Batch: 23/29	Loss 0.7085 (7.0688)
2022-11-12 20:08:59,658:INFO: Dataset: univ                Batch: 24/29	Loss 0.5456 (6.8044)
2022-11-12 20:08:59,663:INFO: Dataset: univ                Batch: 25/29	Loss 0.4397 (6.5176)
2022-11-12 20:08:59,668:INFO: Dataset: univ                Batch: 26/29	Loss 0.3588 (6.2419)
2022-11-12 20:08:59,672:INFO: Dataset: univ                Batch: 27/29	Loss 0.3394 (6.0479)
2022-11-12 20:08:59,677:INFO: Dataset: univ                Batch: 28/29	Loss 0.3381 (5.8391)
2022-11-12 20:08:59,682:INFO: Dataset: univ                Batch: 29/29	Loss 0.3564 (5.7641)
2022-11-12 20:09:06,702:INFO: Dataset: zara1               Batch:  1/16	Loss 30.9936 (30.9936)
2022-11-12 20:09:07,059:INFO: Dataset: zara1               Batch:  2/16	Loss 28.8740 (29.9680)
2022-11-12 20:09:07,450:INFO: Dataset: zara1               Batch:  3/16	Loss 26.2702 (28.7222)
2022-11-12 20:09:07,765:INFO: Dataset: zara1               Batch:  4/16	Loss 24.0977 (27.6786)
2022-11-12 20:09:08,015:INFO: Dataset: zara1               Batch:  5/16	Loss 22.2441 (26.7158)
2022-11-12 20:09:08,194:INFO: Dataset: zara1               Batch:  6/16	Loss 20.6991 (25.7808)
2022-11-12 20:09:08,197:INFO: Dataset: zara1               Batch:  7/16	Loss 19.4674 (24.8054)
2022-11-12 20:09:08,202:INFO: Dataset: zara1               Batch:  8/16	Loss 18.1179 (24.0224)
2022-11-12 20:09:08,209:INFO: Dataset: zara1               Batch:  9/16	Loss 16.4178 (23.1670)
2022-11-12 20:09:08,216:INFO: Dataset: zara1               Batch: 10/16	Loss 14.8495 (22.3971)
2022-11-12 20:09:08,222:INFO: Dataset: zara1               Batch: 11/16	Loss 12.5314 (21.4623)
2022-11-12 20:09:08,227:INFO: Dataset: zara1               Batch: 12/16	Loss 10.4935 (20.5201)
2022-11-12 20:09:08,247:INFO: Dataset: zara1               Batch: 13/16	Loss 8.5831 (19.5614)
2022-11-12 20:09:08,252:INFO: Dataset: zara1               Batch: 14/16	Loss 7.5101 (18.4521)
2022-11-12 20:09:08,269:INFO: Dataset: zara1               Batch: 15/16	Loss 6.2013 (17.4564)
2022-11-12 20:09:08,275:INFO: Dataset: zara1               Batch: 16/16	Loss 5.2716 (16.9498)
2022-11-12 20:09:31,174:INFO: Dataset: zara2               Batch:  1/36	Loss 31.8869 (31.8869)
2022-11-12 20:09:31,178:INFO: Dataset: zara2               Batch:  2/36	Loss 31.7232 (31.8067)
2022-11-12 20:09:31,186:INFO: Dataset: zara2               Batch:  3/36	Loss 27.8937 (30.5049)
2022-11-12 20:09:31,194:INFO: Dataset: zara2               Batch:  4/36	Loss 25.0569 (29.1569)
2022-11-12 20:09:31,213:INFO: Dataset: zara2               Batch:  5/36	Loss 21.9428 (27.8366)
2022-11-12 20:09:31,223:INFO: Dataset: zara2               Batch:  6/36	Loss 18.9904 (26.3564)
2022-11-12 20:09:31,229:INFO: Dataset: zara2               Batch:  7/36	Loss 16.0223 (24.8902)
2022-11-12 20:09:31,234:INFO: Dataset: zara2               Batch:  8/36	Loss 14.5346 (23.6239)
2022-11-12 20:09:31,239:INFO: Dataset: zara2               Batch:  9/36	Loss 13.4610 (22.4445)
2022-11-12 20:09:31,243:INFO: Dataset: zara2               Batch: 10/36	Loss 12.5707 (21.3604)
2022-11-12 20:09:31,248:INFO: Dataset: zara2               Batch: 11/36	Loss 11.8106 (20.5740)
2022-11-12 20:09:31,252:INFO: Dataset: zara2               Batch: 12/36	Loss 11.0473 (19.8273)
2022-11-12 20:09:31,257:INFO: Dataset: zara2               Batch: 13/36	Loss 10.1269 (19.1056)
2022-11-12 20:09:31,261:INFO: Dataset: zara2               Batch: 14/36	Loss 9.1755 (18.3902)
2022-11-12 20:09:31,267:INFO: Dataset: zara2               Batch: 15/36	Loss 8.2334 (17.6926)
2022-11-12 20:09:31,273:INFO: Dataset: zara2               Batch: 16/36	Loss 7.2910 (16.9986)
2022-11-12 20:09:31,277:INFO: Dataset: zara2               Batch: 17/36	Loss 6.0123 (16.3079)
2022-11-12 20:09:31,283:INFO: Dataset: zara2               Batch: 18/36	Loss 5.0579 (15.6808)
2022-11-12 20:09:31,288:INFO: Dataset: zara2               Batch: 19/36	Loss 4.1290 (15.0335)
2022-11-12 20:09:31,294:INFO: Dataset: zara2               Batch: 20/36	Loss 3.2471 (14.4543)
2022-11-12 20:09:31,298:INFO: Dataset: zara2               Batch: 21/36	Loss 2.6608 (13.7919)
2022-11-12 20:09:31,304:INFO: Dataset: zara2               Batch: 22/36	Loss 2.2524 (13.2628)
2022-11-12 20:09:31,307:INFO: Dataset: zara2               Batch: 23/36	Loss 1.9084 (12.7539)
2022-11-12 20:09:31,312:INFO: Dataset: zara2               Batch: 24/36	Loss 1.8857 (12.3361)
2022-11-12 20:09:31,318:INFO: Dataset: zara2               Batch: 25/36	Loss 1.8059 (11.9105)
2022-11-12 20:09:31,325:INFO: Dataset: zara2               Batch: 26/36	Loss 1.5979 (11.4921)
2022-11-12 20:09:31,331:INFO: Dataset: zara2               Batch: 27/36	Loss 1.1244 (11.0921)
2022-11-12 20:09:31,336:INFO: Dataset: zara2               Batch: 28/36	Loss 1.0236 (10.7080)
2022-11-12 20:09:31,344:INFO: Dataset: zara2               Batch: 29/36	Loss 0.9362 (10.4117)
2022-11-12 20:09:31,350:INFO: Dataset: zara2               Batch: 30/36	Loss 0.5392 (10.0744)
2022-11-12 20:09:31,356:INFO: Dataset: zara2               Batch: 31/36	Loss 0.4444 (9.7387)
2022-11-12 20:09:31,362:INFO: Dataset: zara2               Batch: 32/36	Loss 0.4004 (9.4837)
2022-11-12 20:09:31,367:INFO: Dataset: zara2               Batch: 33/36	Loss 0.3479 (9.2171)
2022-11-12 20:09:31,372:INFO: Dataset: zara2               Batch: 34/36	Loss 0.3264 (8.9770)
2022-11-12 20:09:31,377:INFO: Dataset: zara2               Batch: 35/36	Loss 0.3107 (8.7250)
2022-11-12 20:09:31,382:INFO: Dataset: zara2               Batch: 36/36	Loss 0.3084 (8.5858)
2022-11-12 20:09:32,335:INFO: - Computing loss (validation)
2022-11-12 20:09:38,505:INFO: Dataset: hotel               Batch: 1/3	Loss 27.9971 (27.9971)
2022-11-12 20:09:38,963:INFO: Dataset: hotel               Batch: 2/3	Loss 28.2306 (28.1182)
2022-11-12 20:09:39,305:INFO: Dataset: hotel               Batch: 3/3	Loss 28.5706 (28.1568)
2022-11-12 20:09:46,894:INFO: Dataset: univ                Batch: 1/6	Loss 17.6954 (17.6954)
2022-11-12 20:09:47,289:INFO: Dataset: univ                Batch: 2/6	Loss 17.6177 (17.6598)
2022-11-12 20:09:47,629:INFO: Dataset: univ                Batch: 3/6	Loss 17.6654 (17.6615)
2022-11-12 20:09:47,955:INFO: Dataset: univ                Batch: 4/6	Loss 17.6498 (17.6584)
2022-11-12 20:09:48,204:INFO: Dataset: univ                Batch: 5/6	Loss 17.6378 (17.6545)
2022-11-12 20:09:48,393:INFO: Dataset: univ                Batch: 6/6	Loss 17.6813 (17.6585)
2022-11-12 20:09:55,506:INFO: Dataset: zara1               Batch: 1/3	Loss 33.2955 (33.2955)
2022-11-12 20:09:55,830:INFO: Dataset: zara1               Batch: 2/3	Loss 33.4081 (33.3501)
2022-11-12 20:09:56,225:INFO: Dataset: zara1               Batch: 3/3	Loss 33.4537 (33.3761)
2022-11-12 20:10:03,938:INFO: Dataset: zara2               Batch:  1/10	Loss 0.2575 (0.2575)
2022-11-12 20:10:04,348:INFO: Dataset: zara2               Batch:  2/10	Loss 0.2492 (0.2535)
2022-11-12 20:10:04,662:INFO: Dataset: zara2               Batch:  3/10	Loss 0.2474 (0.2516)
2022-11-12 20:10:04,927:INFO: Dataset: zara2               Batch:  4/10	Loss 0.2605 (0.2538)
2022-11-12 20:10:05,190:INFO: Dataset: zara2               Batch:  5/10	Loss 0.2614 (0.2553)
2022-11-12 20:10:05,330:INFO: Dataset: zara2               Batch:  6/10	Loss 0.2559 (0.2554)
2022-11-12 20:10:05,333:INFO: Dataset: zara2               Batch:  7/10	Loss 0.2488 (0.2545)
2022-11-12 20:10:05,337:INFO: Dataset: zara2               Batch:  8/10	Loss 0.2545 (0.2545)
2022-11-12 20:10:05,340:INFO: Dataset: zara2               Batch:  9/10	Loss 0.2555 (0.2546)
2022-11-12 20:10:05,343:INFO: Dataset: zara2               Batch: 10/10	Loss 0.2569 (0.2548)
2022-11-12 20:10:06,248:INFO: - Computing ADE (validation o)
2022-11-12 20:10:14,138:INFO: 		 ADE on eth                       dataset:	 2.738586902618408
2022-11-12 20:10:14,138:INFO: Average validation o:	ADE  2.7386	FDE  4.5051
2022-11-12 20:10:14,176:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_9.pth.tar
2022-11-12 20:10:14,176:INFO: 
===> EPOCH: 10 (P5)
2022-11-12 20:10:14,177:INFO: - Computing loss (training)
2022-11-12 20:10:20,437:INFO: Dataset: hotel               Batch: 1/8	Loss 28.2168 (28.2168)
2022-11-12 20:10:20,836:INFO: Dataset: hotel               Batch: 2/8	Loss 26.7316 (27.4889)
2022-11-12 20:10:21,079:INFO: Dataset: hotel               Batch: 3/8	Loss 24.2273 (26.3526)
2022-11-12 20:10:21,378:INFO: Dataset: hotel               Batch: 4/8	Loss 21.6692 (25.1342)
2022-11-12 20:10:21,646:INFO: Dataset: hotel               Batch: 5/8	Loss 19.4615 (24.0948)
2022-11-12 20:10:21,895:INFO: Dataset: hotel               Batch: 6/8	Loss 17.4854 (22.9191)
2022-11-12 20:10:21,901:INFO: Dataset: hotel               Batch: 7/8	Loss 15.7175 (21.8232)
2022-11-12 20:10:21,907:INFO: Dataset: hotel               Batch: 8/8	Loss 14.1586 (21.6007)
2022-11-12 20:10:44,795:INFO: Dataset: univ                Batch:  1/29	Loss 12.3597 (12.3597)
2022-11-12 20:10:44,801:INFO: Dataset: univ                Batch:  2/29	Loss 12.0682 (12.2206)
2022-11-12 20:10:44,807:INFO: Dataset: univ                Batch:  3/29	Loss 11.5810 (12.0078)
2022-11-12 20:10:44,814:INFO: Dataset: univ                Batch:  4/29	Loss 11.0255 (11.8189)
2022-11-12 20:10:44,824:INFO: Dataset: univ                Batch:  5/29	Loss 10.3137 (11.4834)
2022-11-12 20:10:44,832:INFO: Dataset: univ                Batch:  6/29	Loss 9.6707 (11.1918)
2022-11-12 20:10:44,839:INFO: Dataset: univ                Batch:  7/29	Loss 8.9658 (10.8698)
2022-11-12 20:10:44,845:INFO: Dataset: univ                Batch:  8/29	Loss 8.3528 (10.5498)
2022-11-12 20:10:44,850:INFO: Dataset: univ                Batch:  9/29	Loss 7.7138 (10.2166)
2022-11-12 20:10:44,856:INFO: Dataset: univ                Batch: 10/29	Loss 7.1029 (9.8912)
2022-11-12 20:10:44,861:INFO: Dataset: univ                Batch: 11/29	Loss 6.5331 (9.6115)
2022-11-12 20:10:44,868:INFO: Dataset: univ                Batch: 12/29	Loss 5.9429 (9.3064)
2022-11-12 20:10:44,875:INFO: Dataset: univ                Batch: 13/29	Loss 5.3383 (9.0081)
2022-11-12 20:10:44,882:INFO: Dataset: univ                Batch: 14/29	Loss 4.7761 (8.6941)
2022-11-12 20:10:44,887:INFO: Dataset: univ                Batch: 15/29	Loss 4.1694 (8.4032)
2022-11-12 20:10:44,895:INFO: Dataset: univ                Batch: 16/29	Loss 3.4910 (8.1192)
2022-11-12 20:10:44,901:INFO: Dataset: univ                Batch: 17/29	Loss 2.9090 (7.7618)
2022-11-12 20:10:44,907:INFO: Dataset: univ                Batch: 18/29	Loss 2.3675 (7.3781)
2022-11-12 20:10:44,914:INFO: Dataset: univ                Batch: 19/29	Loss 1.8510 (7.0549)
2022-11-12 20:10:44,922:INFO: Dataset: univ                Batch: 20/29	Loss 1.4593 (6.7436)
2022-11-12 20:10:44,928:INFO: Dataset: univ                Batch: 21/29	Loss 1.1097 (6.4581)
2022-11-12 20:10:44,935:INFO: Dataset: univ                Batch: 22/29	Loss 0.9163 (6.1720)
2022-11-12 20:10:44,941:INFO: Dataset: univ                Batch: 23/29	Loss 0.6804 (5.9374)
2022-11-12 20:10:44,948:INFO: Dataset: univ                Batch: 24/29	Loss 0.6993 (5.7146)
2022-11-12 20:10:44,955:INFO: Dataset: univ                Batch: 25/29	Loss 0.6628 (5.5121)
2022-11-12 20:10:44,961:INFO: Dataset: univ                Batch: 26/29	Loss 0.4699 (5.3168)
2022-11-12 20:10:44,968:INFO: Dataset: univ                Batch: 27/29	Loss 0.4344 (5.1465)
2022-11-12 20:10:44,977:INFO: Dataset: univ                Batch: 28/29	Loss 0.3582 (4.9507)
2022-11-12 20:10:44,982:INFO: Dataset: univ                Batch: 29/29	Loss 0.3006 (4.8978)
2022-11-12 20:10:52,001:INFO: Dataset: zara1               Batch:  1/16	Loss 36.9623 (36.9623)
2022-11-12 20:10:52,345:INFO: Dataset: zara1               Batch:  2/16	Loss 34.3122 (35.7193)
2022-11-12 20:10:52,723:INFO: Dataset: zara1               Batch:  3/16	Loss 30.9804 (34.1304)
2022-11-12 20:10:53,037:INFO: Dataset: zara1               Batch:  4/16	Loss 27.8651 (32.4168)
2022-11-12 20:10:53,328:INFO: Dataset: zara1               Batch:  5/16	Loss 25.3012 (30.7337)
2022-11-12 20:10:53,492:INFO: Dataset: zara1               Batch:  6/16	Loss 22.9228 (29.3584)
2022-11-12 20:10:53,498:INFO: Dataset: zara1               Batch:  7/16	Loss 20.4659 (28.2130)
2022-11-12 20:10:53,504:INFO: Dataset: zara1               Batch:  8/16	Loss 18.3986 (27.1112)
2022-11-12 20:10:53,509:INFO: Dataset: zara1               Batch:  9/16	Loss 16.6165 (25.8449)
2022-11-12 20:10:53,512:INFO: Dataset: zara1               Batch: 10/16	Loss 14.7641 (24.9339)
2022-11-12 20:10:53,516:INFO: Dataset: zara1               Batch: 11/16	Loss 12.6168 (23.8330)
2022-11-12 20:10:53,520:INFO: Dataset: zara1               Batch: 12/16	Loss 10.9747 (22.6958)
2022-11-12 20:10:53,525:INFO: Dataset: zara1               Batch: 13/16	Loss 9.0015 (21.6722)
2022-11-12 20:10:53,528:INFO: Dataset: zara1               Batch: 14/16	Loss 7.1118 (20.5549)
2022-11-12 20:10:53,533:INFO: Dataset: zara1               Batch: 15/16	Loss 5.3005 (19.5385)
2022-11-12 20:10:53,538:INFO: Dataset: zara1               Batch: 16/16	Loss 3.6335 (18.7097)
2022-11-12 20:11:16,349:INFO: Dataset: zara2               Batch:  1/36	Loss 25.0919 (25.0919)
2022-11-12 20:11:16,354:INFO: Dataset: zara2               Batch:  2/36	Loss 26.3996 (25.6709)
2022-11-12 20:11:16,360:INFO: Dataset: zara2               Batch:  3/36	Loss 25.4601 (25.6012)
2022-11-12 20:11:16,364:INFO: Dataset: zara2               Batch:  4/36	Loss 23.6495 (25.1504)
2022-11-12 20:11:16,374:INFO: Dataset: zara2               Batch:  5/36	Loss 19.5345 (23.9899)
2022-11-12 20:11:16,397:INFO: Dataset: zara2               Batch:  6/36	Loss 17.5473 (22.9305)
2022-11-12 20:11:16,402:INFO: Dataset: zara2               Batch:  7/36	Loss 15.2441 (21.8160)
2022-11-12 20:11:16,406:INFO: Dataset: zara2               Batch:  8/36	Loss 13.2780 (20.6388)
2022-11-12 20:11:16,412:INFO: Dataset: zara2               Batch:  9/36	Loss 11.6673 (19.5622)
2022-11-12 20:11:16,420:INFO: Dataset: zara2               Batch: 10/36	Loss 10.3077 (18.6753)
2022-11-12 20:11:16,426:INFO: Dataset: zara2               Batch: 11/36	Loss 9.0507 (17.7664)
2022-11-12 20:11:16,431:INFO: Dataset: zara2               Batch: 12/36	Loss 8.1091 (17.1127)
2022-11-12 20:11:16,436:INFO: Dataset: zara2               Batch: 13/36	Loss 6.9690 (16.3578)
2022-11-12 20:11:16,440:INFO: Dataset: zara2               Batch: 14/36	Loss 6.1994 (15.6047)
2022-11-12 20:11:16,444:INFO: Dataset: zara2               Batch: 15/36	Loss 5.4533 (14.8582)
2022-11-12 20:11:16,450:INFO: Dataset: zara2               Batch: 16/36	Loss 4.6776 (14.0689)
2022-11-12 20:11:16,456:INFO: Dataset: zara2               Batch: 17/36	Loss 4.1086 (13.4265)
2022-11-12 20:11:16,460:INFO: Dataset: zara2               Batch: 18/36	Loss 3.4355 (12.8798)
2022-11-12 20:11:16,465:INFO: Dataset: zara2               Batch: 19/36	Loss 2.8984 (12.3947)
2022-11-12 20:11:16,470:INFO: Dataset: zara2               Batch: 20/36	Loss 2.5261 (11.8619)
2022-11-12 20:11:16,475:INFO: Dataset: zara2               Batch: 21/36	Loss 2.1445 (11.3615)
2022-11-12 20:11:16,480:INFO: Dataset: zara2               Batch: 22/36	Loss 1.7352 (10.9369)
2022-11-12 20:11:16,485:INFO: Dataset: zara2               Batch: 23/36	Loss 1.7160 (10.5201)
2022-11-12 20:11:16,490:INFO: Dataset: zara2               Batch: 24/36	Loss 1.5599 (10.1137)
2022-11-12 20:11:16,495:INFO: Dataset: zara2               Batch: 25/36	Loss 1.3520 (9.7277)
2022-11-12 20:11:16,501:INFO: Dataset: zara2               Batch: 26/36	Loss 1.1816 (9.4151)
2022-11-12 20:11:16,507:INFO: Dataset: zara2               Batch: 27/36	Loss 0.9750 (9.1031)
2022-11-12 20:11:16,515:INFO: Dataset: zara2               Batch: 28/36	Loss 0.7357 (8.8184)
2022-11-12 20:11:16,520:INFO: Dataset: zara2               Batch: 29/36	Loss 0.5620 (8.5273)
2022-11-12 20:11:16,527:INFO: Dataset: zara2               Batch: 30/36	Loss 0.4901 (8.2826)
2022-11-12 20:11:16,533:INFO: Dataset: zara2               Batch: 31/36	Loss 0.4254 (8.0142)
2022-11-12 20:11:16,540:INFO: Dataset: zara2               Batch: 32/36	Loss 0.3950 (7.7951)
2022-11-12 20:11:16,545:INFO: Dataset: zara2               Batch: 33/36	Loss 0.3079 (7.5703)
2022-11-12 20:11:16,550:INFO: Dataset: zara2               Batch: 34/36	Loss 0.3046 (7.3646)
2022-11-12 20:11:16,554:INFO: Dataset: zara2               Batch: 35/36	Loss 0.2915 (7.1629)
2022-11-12 20:11:16,560:INFO: Dataset: zara2               Batch: 36/36	Loss 0.2550 (7.0224)
2022-11-12 20:11:17,470:INFO: - Computing loss (validation)
2022-11-12 20:11:23,556:INFO: Dataset: hotel               Batch: 1/3	Loss 29.9886 (29.9886)
2022-11-12 20:11:23,911:INFO: Dataset: hotel               Batch: 2/3	Loss 29.7902 (29.8879)
2022-11-12 20:11:24,286:INFO: Dataset: hotel               Batch: 3/3	Loss 29.3097 (29.8583)
2022-11-12 20:11:31,815:INFO: Dataset: univ                Batch: 1/6	Loss 18.2691 (18.2691)
2022-11-12 20:11:32,167:INFO: Dataset: univ                Batch: 2/6	Loss 18.3299 (18.3006)
2022-11-12 20:11:32,524:INFO: Dataset: univ                Batch: 3/6	Loss 18.5092 (18.3667)
2022-11-12 20:11:32,834:INFO: Dataset: univ                Batch: 4/6	Loss 18.3836 (18.3711)
2022-11-12 20:11:33,064:INFO: Dataset: univ                Batch: 5/6	Loss 18.3048 (18.3591)
2022-11-12 20:11:33,269:INFO: Dataset: univ                Batch: 6/6	Loss 18.3889 (18.3633)
2022-11-12 20:11:40,365:INFO: Dataset: zara1               Batch: 1/3	Loss 32.8130 (32.8130)
2022-11-12 20:11:40,696:INFO: Dataset: zara1               Batch: 2/3	Loss 32.5375 (32.6683)
2022-11-12 20:11:41,129:INFO: Dataset: zara1               Batch: 3/3	Loss 32.7797 (32.6944)
2022-11-12 20:11:49,082:INFO: Dataset: zara2               Batch:  1/10	Loss 0.2499 (0.2499)
2022-11-12 20:11:49,519:INFO: Dataset: zara2               Batch:  2/10	Loss 0.2259 (0.2373)
2022-11-12 20:11:49,789:INFO: Dataset: zara2               Batch:  3/10	Loss 0.2084 (0.2279)
2022-11-12 20:11:50,091:INFO: Dataset: zara2               Batch:  4/10	Loss 0.2639 (0.2366)
2022-11-12 20:11:50,290:INFO: Dataset: zara2               Batch:  5/10	Loss 0.2229 (0.2336)
2022-11-12 20:11:50,531:INFO: Dataset: zara2               Batch:  6/10	Loss 0.2344 (0.2338)
2022-11-12 20:11:50,534:INFO: Dataset: zara2               Batch:  7/10	Loss 0.2522 (0.2363)
2022-11-12 20:11:50,537:INFO: Dataset: zara2               Batch:  8/10	Loss 0.2279 (0.2352)
2022-11-12 20:11:50,539:INFO: Dataset: zara2               Batch:  9/10	Loss 0.2545 (0.2374)
2022-11-12 20:11:50,541:INFO: Dataset: zara2               Batch: 10/10	Loss 0.2364 (0.2373)
2022-11-12 20:11:51,447:INFO: - Computing ADE (validation o)
2022-11-12 20:11:59,335:INFO: 		 ADE on eth                       dataset:	 2.7866687774658203
2022-11-12 20:11:59,335:INFO: Average validation o:	ADE  2.7867	FDE  4.5573
2022-11-12 20:11:59,373:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_10.pth.tar
2022-11-12 20:11:59,374:INFO: 
===> EPOCH: 11 (P6)
2022-11-12 20:11:59,375:INFO: - Computing loss (training)

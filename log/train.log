2022-11-07 17:29:29,208:INFO: Initializing Training Set
2022-11-07 17:29:33,086:INFO: Initializing Validation Set
2022-11-07 17:29:33,475:INFO: Initializing Validation O Set
2022-11-07 17:29:35,873:INFO: => loaded checkpoint './models/eth/CRMF_risk_erm_0.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 300, 100)_shuffle_true_seed_72/pretrain/P2/CRMF_epoch_250.pth.tar' (epoch 251)
2022-11-07 17:29:35,874:INFO: 
===> EPOCH: 251 (P4)
2022-11-07 17:29:35,874:INFO: - Computing loss (training)
2022-11-07 17:29:42,452:INFO: Dataset: hotel               Batch: 1/8	Loss 11.9224 (11.9224)
2022-11-07 17:29:43,040:INFO: Dataset: hotel               Batch: 2/8	Loss 13.3504 (12.6572)
2022-11-07 17:29:43,469:INFO: Dataset: hotel               Batch: 3/8	Loss 12.3708 (12.5624)
2022-11-07 17:29:43,849:INFO: Dataset: hotel               Batch: 4/8	Loss 11.0705 (12.1831)
2022-11-07 17:29:44,206:INFO: Dataset: hotel               Batch: 5/8	Loss 11.2723 (11.9874)
2022-11-07 17:29:44,565:INFO: Dataset: hotel               Batch: 6/8	Loss 11.1411 (11.8488)
2022-11-07 17:29:44,922:INFO: Dataset: hotel               Batch: 7/8	Loss 11.1555 (11.7472)
2022-11-07 17:29:45,173:INFO: Dataset: hotel               Batch: 8/8	Loss 3.1908 (11.5102)
2022-11-07 17:30:08,991:INFO: Dataset: univ                Batch:  1/29	Loss 13.8687 (13.8687)
2022-11-07 17:30:09,468:INFO: Dataset: univ                Batch:  2/29	Loss 10.6190 (12.0606)
2022-11-07 17:30:09,837:INFO: Dataset: univ                Batch:  3/29	Loss 11.4126 (11.8493)
2022-11-07 17:30:10,198:INFO: Dataset: univ                Batch:  4/29	Loss 12.7732 (12.0833)
2022-11-07 17:30:10,551:INFO: Dataset: univ                Batch:  5/29	Loss 12.0084 (12.0691)
2022-11-07 17:30:10,948:INFO: Dataset: univ                Batch:  6/29	Loss 13.8214 (12.3678)
2022-11-07 17:30:11,311:INFO: Dataset: univ                Batch:  7/29	Loss 11.8995 (12.2965)
2022-11-07 17:30:11,671:INFO: Dataset: univ                Batch:  8/29	Loss 11.0824 (12.1373)
2022-11-07 17:30:12,068:INFO: Dataset: univ                Batch:  9/29	Loss 12.5947 (12.1823)
2022-11-07 17:30:12,445:INFO: Dataset: univ                Batch: 10/29	Loss 12.4861 (12.2131)
2022-11-07 17:30:12,819:INFO: Dataset: univ                Batch: 11/29	Loss 12.1730 (12.2093)
2022-11-07 17:30:13,180:INFO: Dataset: univ                Batch: 12/29	Loss 10.8375 (12.0892)
2022-11-07 17:30:13,543:INFO: Dataset: univ                Batch: 13/29	Loss 10.8322 (11.9965)
2022-11-07 17:30:13,943:INFO: Dataset: univ                Batch: 14/29	Loss 10.6025 (11.8793)
2022-11-07 17:30:14,324:INFO: Dataset: univ                Batch: 15/29	Loss 11.6411 (11.8635)
2022-11-07 17:30:14,674:INFO: Dataset: univ                Batch: 16/29	Loss 11.3681 (11.8310)
2022-11-07 17:30:15,036:INFO: Dataset: univ                Batch: 17/29	Loss 11.1762 (11.7923)
2022-11-07 17:30:15,402:INFO: Dataset: univ                Batch: 18/29	Loss 11.8155 (11.7935)
2022-11-07 17:30:15,765:INFO: Dataset: univ                Batch: 19/29	Loss 12.8924 (11.8476)
2022-11-07 17:30:16,121:INFO: Dataset: univ                Batch: 20/29	Loss 11.8311 (11.8467)
2022-11-07 17:30:16,482:INFO: Dataset: univ                Batch: 21/29	Loss 12.1183 (11.8592)
2022-11-07 17:30:16,860:INFO: Dataset: univ                Batch: 22/29	Loss 10.1629 (11.7633)
2022-11-07 17:30:17,228:INFO: Dataset: univ                Batch: 23/29	Loss 11.0807 (11.7306)
2022-11-07 17:30:17,595:INFO: Dataset: univ                Batch: 24/29	Loss 10.3145 (11.6603)
2022-11-07 17:30:17,966:INFO: Dataset: univ                Batch: 25/29	Loss 11.4058 (11.6513)
2022-11-07 17:30:18,352:INFO: Dataset: univ                Batch: 26/29	Loss 9.7964 (11.5625)
2022-11-07 17:30:18,776:INFO: Dataset: univ                Batch: 27/29	Loss 12.6359 (11.5963)
2022-11-07 17:30:19,395:INFO: Dataset: univ                Batch: 28/29	Loss 10.6963 (11.5635)
2022-11-07 17:30:19,968:INFO: Dataset: univ                Batch: 29/29	Loss 4.6386 (11.4749)
2022-11-07 17:30:30,133:INFO: Dataset: zara1               Batch:  1/16	Loss 13.7661 (13.7661)
2022-11-07 17:30:30,733:INFO: Dataset: zara1               Batch:  2/16	Loss 13.0924 (13.4011)
2022-11-07 17:30:31,253:INFO: Dataset: zara1               Batch:  3/16	Loss 13.9798 (13.5636)
2022-11-07 17:30:31,696:INFO: Dataset: zara1               Batch:  4/16	Loss 12.8072 (13.4067)
2022-11-07 17:30:32,098:INFO: Dataset: zara1               Batch:  5/16	Loss 13.9484 (13.5053)
2022-11-07 17:30:32,504:INFO: Dataset: zara1               Batch:  6/16	Loss 13.2335 (13.4617)
2022-11-07 17:30:32,890:INFO: Dataset: zara1               Batch:  7/16	Loss 14.8613 (13.6673)
2022-11-07 17:30:33,391:INFO: Dataset: zara1               Batch:  8/16	Loss 13.7524 (13.6789)
2022-11-07 17:30:33,774:INFO: Dataset: zara1               Batch:  9/16	Loss 13.4732 (13.6550)
2022-11-07 17:30:34,180:INFO: Dataset: zara1               Batch: 10/16	Loss 12.9693 (13.5682)
2022-11-07 17:30:34,591:INFO: Dataset: zara1               Batch: 11/16	Loss 13.1026 (13.5209)
2022-11-07 17:30:34,985:INFO: Dataset: zara1               Batch: 12/16	Loss 12.9691 (13.4735)
2022-11-07 17:30:35,368:INFO: Dataset: zara1               Batch: 13/16	Loss 13.5098 (13.4762)
2022-11-07 17:30:35,737:INFO: Dataset: zara1               Batch: 14/16	Loss 12.7307 (13.4160)
2022-11-07 17:30:36,087:INFO: Dataset: zara1               Batch: 15/16	Loss 13.8165 (13.4402)
2022-11-07 17:30:36,438:INFO: Dataset: zara1               Batch: 16/16	Loss 9.2372 (13.2300)
2022-11-07 17:31:01,677:INFO: Dataset: zara2               Batch:  1/36	Loss 11.0680 (11.0680)
2022-11-07 17:31:02,017:INFO: Dataset: zara2               Batch:  2/36	Loss 10.8208 (10.9529)
2022-11-07 17:31:02,366:INFO: Dataset: zara2               Batch:  3/36	Loss 10.5108 (10.8165)
2022-11-07 17:31:02,708:INFO: Dataset: zara2               Batch:  4/36	Loss 11.5233 (11.0215)
2022-11-07 17:31:03,074:INFO: Dataset: zara2               Batch:  5/36	Loss 11.3946 (11.1001)
2022-11-07 17:31:03,434:INFO: Dataset: zara2               Batch:  6/36	Loss 11.0135 (11.0846)
2022-11-07 17:31:03,785:INFO: Dataset: zara2               Batch:  7/36	Loss 11.3222 (11.1153)
2022-11-07 17:31:04,160:INFO: Dataset: zara2               Batch:  8/36	Loss 11.9902 (11.2110)
2022-11-07 17:31:04,516:INFO: Dataset: zara2               Batch:  9/36	Loss 9.9638 (11.0687)
2022-11-07 17:31:04,870:INFO: Dataset: zara2               Batch: 10/36	Loss 10.6078 (11.0192)
2022-11-07 17:31:05,245:INFO: Dataset: zara2               Batch: 11/36	Loss 11.2453 (11.0457)
2022-11-07 17:31:05,602:INFO: Dataset: zara2               Batch: 12/36	Loss 11.0725 (11.0478)
2022-11-07 17:31:05,949:INFO: Dataset: zara2               Batch: 13/36	Loss 11.2794 (11.0637)
2022-11-07 17:31:06,293:INFO: Dataset: zara2               Batch: 14/36	Loss 10.7028 (11.0362)
2022-11-07 17:31:06,642:INFO: Dataset: zara2               Batch: 15/36	Loss 9.8831 (10.9553)
2022-11-07 17:31:07,001:INFO: Dataset: zara2               Batch: 16/36	Loss 10.6639 (10.9389)
2022-11-07 17:31:07,373:INFO: Dataset: zara2               Batch: 17/36	Loss 11.1138 (10.9493)
2022-11-07 17:31:07,737:INFO: Dataset: zara2               Batch: 18/36	Loss 10.7855 (10.9402)
2022-11-07 17:31:08,101:INFO: Dataset: zara2               Batch: 19/36	Loss 12.5627 (11.0270)
2022-11-07 17:31:08,450:INFO: Dataset: zara2               Batch: 20/36	Loss 11.0533 (11.0284)
2022-11-07 17:31:08,806:INFO: Dataset: zara2               Batch: 21/36	Loss 11.4343 (11.0472)
2022-11-07 17:31:09,164:INFO: Dataset: zara2               Batch: 22/36	Loss 11.3593 (11.0616)
2022-11-07 17:31:09,512:INFO: Dataset: zara2               Batch: 23/36	Loss 10.9401 (11.0562)
2022-11-07 17:31:09,860:INFO: Dataset: zara2               Batch: 24/36	Loss 10.0334 (11.0115)
2022-11-07 17:31:10,260:INFO: Dataset: zara2               Batch: 25/36	Loss 10.6423 (10.9969)
2022-11-07 17:31:10,639:INFO: Dataset: zara2               Batch: 26/36	Loss 10.9363 (10.9944)
2022-11-07 17:31:11,011:INFO: Dataset: zara2               Batch: 27/36	Loss 10.6463 (10.9815)
2022-11-07 17:31:11,385:INFO: Dataset: zara2               Batch: 28/36	Loss 10.2094 (10.9557)
2022-11-07 17:31:11,753:INFO: Dataset: zara2               Batch: 29/36	Loss 10.7811 (10.9494)
2022-11-07 17:31:12,134:INFO: Dataset: zara2               Batch: 30/36	Loss 10.8002 (10.9440)
2022-11-07 17:31:12,500:INFO: Dataset: zara2               Batch: 31/36	Loss 10.6121 (10.9329)
2022-11-07 17:31:12,864:INFO: Dataset: zara2               Batch: 32/36	Loss 11.1419 (10.9396)
2022-11-07 17:31:13,218:INFO: Dataset: zara2               Batch: 33/36	Loss 10.7097 (10.9325)
2022-11-07 17:31:13,570:INFO: Dataset: zara2               Batch: 34/36	Loss 10.2462 (10.9109)
2022-11-07 17:31:13,914:INFO: Dataset: zara2               Batch: 35/36	Loss 11.1057 (10.9166)
2022-11-07 17:31:14,217:INFO: Dataset: zara2               Batch: 36/36	Loss 6.6472 (10.8276)
2022-11-07 17:31:15,138:INFO: - Computing ADE (validation o)
2022-11-07 17:31:23,922:INFO: 		 ADE on eth                       dataset:	 2.814692735671997
2022-11-07 17:31:23,922:INFO: Average validation o:	ADE  2.8147	FDE  4.7997
2022-11-07 17:31:23,924:INFO: - Computing ADE (validation)
2022-11-07 17:31:32,442:INFO: 		 ADE on hotel                     dataset:	 0.9810593724250793
2022-11-07 17:31:41,386:INFO: 		 ADE on univ                      dataset:	 1.4798141717910767
2022-11-07 17:31:49,850:INFO: 		 ADE on zara1                     dataset:	 2.423785448074341
2022-11-07 17:32:00,305:INFO: 		 ADE on zara2                     dataset:	 1.4031414985656738
2022-11-07 17:32:00,305:INFO: Average validation:	ADE  1.4793	FDE  2.6870
2022-11-07 17:32:00,307:INFO: - Computing ADE (training)
2022-11-07 17:32:08,962:INFO: 		 ADE on hotel                     dataset:	 1.3352004289627075
2022-11-07 17:32:35,146:INFO: 		 ADE on univ                      dataset:	 1.3715673685073853
2022-11-07 17:32:45,755:INFO: 		 ADE on zara1                     dataset:	 2.418069362640381
2022-11-07 17:33:11,772:INFO: 		 ADE on zara2                     dataset:	 1.6810961961746216
2022-11-07 17:33:11,774:INFO: Average training:	ADE  1.5002	FDE  2.7318
2022-11-07 17:33:11,795:INFO:  --> Model Saved in ./models/eth/CRMF_risk_erm_0.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 300, 100)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_251.pth.tar
2022-11-07 17:33:11,796:INFO: 
===> EPOCH: 252 (P4)
2022-11-07 17:33:11,796:INFO: - Computing loss (training)
2022-11-07 17:33:18,213:INFO: Dataset: hotel               Batch: 1/8	Loss 10.7106 (10.7106)
2022-11-07 17:33:18,819:INFO: Dataset: hotel               Batch: 2/8	Loss 10.1941 (10.4499)
2022-11-07 17:33:19,284:INFO: Dataset: hotel               Batch: 3/8	Loss 11.3036 (10.7363)
2022-11-07 17:33:19,711:INFO: Dataset: hotel               Batch: 4/8	Loss 11.1490 (10.8370)
2022-11-07 17:33:20,090:INFO: Dataset: hotel               Batch: 5/8	Loss 10.6551 (10.7988)
2022-11-07 17:33:20,447:INFO: Dataset: hotel               Batch: 6/8	Loss 9.6777 (10.6131)
2022-11-07 17:33:20,805:INFO: Dataset: hotel               Batch: 7/8	Loss 11.5778 (10.7480)
2022-11-07 17:33:21,058:INFO: Dataset: hotel               Batch: 8/8	Loss 2.2856 (10.5135)

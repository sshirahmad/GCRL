2022-11-01 13:48:11,492:INFO: Initializing Training Set
2022-11-01 13:48:16,167:INFO: Initializing Validation Set
2022-11-01 13:48:16,631:INFO: Initializing Validation O Set
2022-11-01 13:48:19,371:INFO: 
===> EPOCH: 651 (P4)
2022-11-01 13:48:19,371:INFO: - Computing loss (training)
2022-11-01 13:48:27,581:INFO: Dataset: hotel               Batch: 1/8	Loss -952.5187 (-952.5187)
2022-11-01 13:48:28,519:INFO: Dataset: hotel               Batch: 2/8	Loss -1423.7811 (-1185.9270)
2022-11-01 13:48:29,312:INFO: Dataset: hotel               Batch: 3/8	Loss -1368.4583 (-1249.0242)
2022-11-01 13:48:30,129:INFO: Dataset: hotel               Batch: 4/8	Loss -1264.5905 (-1252.8615)
2022-11-01 13:48:30,927:INFO: Dataset: hotel               Batch: 5/8	Loss -1631.2828 (-1324.2617)
2022-11-01 13:48:31,703:INFO: Dataset: hotel               Batch: 6/8	Loss -1039.5420 (-1277.5569)
2022-11-01 13:48:32,497:INFO: Dataset: hotel               Batch: 7/8	Loss -1163.1376 (-1261.1671)
2022-11-01 13:48:33,086:INFO: Dataset: hotel               Batch: 8/8	Loss -697.6891 (-1247.7864)
2022-11-01 13:49:00,806:INFO: Dataset: univ                Batch:  1/29	Loss 6265.1562 (6265.1562)
2022-11-01 13:49:01,586:INFO: Dataset: univ                Batch:  2/29	Loss -2732.8184 (1941.8888)
2022-11-01 13:49:02,331:INFO: Dataset: univ                Batch:  3/29	Loss -2103.3804 (681.5190)
2022-11-01 13:49:03,168:INFO: Dataset: univ                Batch:  4/29	Loss -1606.8495 (178.5809)
2022-11-01 13:49:03,948:INFO: Dataset: univ                Batch:  5/29	Loss -4869.9224 (-782.3906)
2022-11-01 13:49:04,715:INFO: Dataset: univ                Batch:  6/29	Loss -2208.8604 (-997.8793)
2022-11-01 13:49:05,477:INFO: Dataset: univ                Batch:  7/29	Loss -2060.3228 (-1140.2404)
2022-11-01 13:49:06,245:INFO: Dataset: univ                Batch:  8/29	Loss -326.8777 (-1023.0797)
2022-11-01 13:49:06,958:INFO: Dataset: univ                Batch:  9/29	Loss -1795.4769 (-1088.8947)
2022-11-01 13:49:07,945:INFO: Dataset: univ                Batch: 10/29	Loss -2249.1060 (-1207.5509)
2022-11-01 13:49:08,870:INFO: Dataset: univ                Batch: 11/29	Loss -3118.2134 (-1399.8143)
2022-11-01 13:49:09,662:INFO: Dataset: univ                Batch: 12/29	Loss -3171.7544 (-1545.3359)
2022-11-01 13:49:10,433:INFO: Dataset: univ                Batch: 13/29	Loss -1932.8453 (-1575.0874)
2022-11-01 13:49:11,227:INFO: Dataset: univ                Batch: 14/29	Loss -1419.5354 (-1564.5827)
2022-11-01 13:49:12,015:INFO: Dataset: univ                Batch: 15/29	Loss -556.3624 (-1486.9919)
2022-11-01 13:49:12,752:INFO: Dataset: univ                Batch: 16/29	Loss 10900.1504 (-759.1514)
2022-11-01 13:49:13,516:INFO: Dataset: univ                Batch: 17/29	Loss -1070.5969 (-779.0415)
2022-11-01 13:49:14,278:INFO: Dataset: univ                Batch: 18/29	Loss -1531.0135 (-823.0572)
2022-11-01 13:49:14,995:INFO: Dataset: univ                Batch: 19/29	Loss -897.0621 (-826.7059)
2022-11-01 13:49:15,777:INFO: Dataset: univ                Batch: 20/29	Loss -1901.9181 (-878.5634)
2022-11-01 13:49:16,584:INFO: Dataset: univ                Batch: 21/29	Loss -726.8936 (-870.3177)
2022-11-01 13:49:17,355:INFO: Dataset: univ                Batch: 22/29	Loss -1700.2405 (-909.1682)
2022-11-01 13:49:18,144:INFO: Dataset: univ                Batch: 23/29	Loss -3287.2048 (-1018.3258)
2022-11-01 13:49:18,892:INFO: Dataset: univ                Batch: 24/29	Loss -2094.2556 (-1063.9768)
2022-11-01 13:49:19,644:INFO: Dataset: univ                Batch: 25/29	Loss -278.3280 (-1030.0028)
2022-11-01 13:49:20,432:INFO: Dataset: univ                Batch: 26/29	Loss -2465.8870 (-1080.6934)
2022-11-01 13:49:21,205:INFO: Dataset: univ                Batch: 27/29	Loss -1072.2438 (-1080.3713)
2022-11-01 13:49:21,989:INFO: Dataset: univ                Batch: 28/29	Loss -963.4294 (-1075.6921)
2022-11-01 13:49:22,670:INFO: Dataset: univ                Batch: 29/29	Loss -221.7572 (-1062.1761)
2022-11-01 13:49:32,429:INFO: Dataset: zara1               Batch:  1/16	Loss -1326.5144 (-1326.5144)
2022-11-01 13:49:33,532:INFO: Dataset: zara1               Batch:  2/16	Loss -1303.9432 (-1315.0939)
2022-11-01 13:49:34,557:INFO: Dataset: zara1               Batch:  3/16	Loss -1478.9259 (-1365.3297)
2022-11-01 13:49:35,371:INFO: Dataset: zara1               Batch:  4/16	Loss -1362.9708 (-1364.7954)
2022-11-01 13:49:36,116:INFO: Dataset: zara1               Batch:  5/16	Loss -1394.9176 (-1371.2249)
2022-11-01 13:49:36,928:INFO: Dataset: zara1               Batch:  6/16	Loss -1623.0454 (-1407.4580)
2022-11-01 13:49:37,682:INFO: Dataset: zara1               Batch:  7/16	Loss -6475.7642 (-2153.7117)
2022-11-01 13:49:38,434:INFO: Dataset: zara1               Batch:  8/16	Loss -1789.1677 (-2106.5858)
2022-11-01 13:49:39,207:INFO: Dataset: zara1               Batch:  9/16	Loss -1282.1371 (-2012.2071)
2022-11-01 13:49:40,043:INFO: Dataset: zara1               Batch: 10/16	Loss -916.3713 (-1875.2276)
2022-11-01 13:49:40,837:INFO: Dataset: zara1               Batch: 11/16	Loss -1343.5992 (-1822.1440)
2022-11-01 13:49:41,634:INFO: Dataset: zara1               Batch: 12/16	Loss 691.1617 (-1615.8535)
2022-11-01 13:49:42,420:INFO: Dataset: zara1               Batch: 13/16	Loss -1420.9318 (-1601.6392)
2022-11-01 13:49:43,237:INFO: Dataset: zara1               Batch: 14/16	Loss -1089.9506 (-1561.0148)
2022-11-01 13:49:44,016:INFO: Dataset: zara1               Batch: 15/16	Loss -1236.0149 (-1541.0696)
2022-11-01 13:49:44,778:INFO: Dataset: zara1               Batch: 16/16	Loss -1392.0564 (-1535.1875)
2022-11-01 13:50:12,713:INFO: Dataset: zara2               Batch:  1/36	Loss -1317.1934 (-1317.1934)
2022-11-01 13:50:13,507:INFO: Dataset: zara2               Batch:  2/36	Loss -1231.1285 (-1279.8640)
2022-11-01 13:50:14,300:INFO: Dataset: zara2               Batch:  3/36	Loss -1056.5862 (-1202.5079)
2022-11-01 13:50:15,079:INFO: Dataset: zara2               Batch:  4/36	Loss -1227.4990 (-1208.8840)
2022-11-01 13:50:15,853:INFO: Dataset: zara2               Batch:  5/36	Loss -812.4285 (-1132.4079)
2022-11-01 13:50:16,631:INFO: Dataset: zara2               Batch:  6/36	Loss -1090.4382 (-1124.8697)
2022-11-01 13:50:17,435:INFO: Dataset: zara2               Batch:  7/36	Loss -993.2841 (-1107.7547)
2022-11-01 13:50:18,217:INFO: Dataset: zara2               Batch:  8/36	Loss -1204.9454 (-1118.8123)
2022-11-01 13:50:18,979:INFO: Dataset: zara2               Batch:  9/36	Loss -1574.8469 (-1173.7526)
2022-11-01 13:50:19,812:INFO: Dataset: zara2               Batch: 10/36	Loss 488.1066 (-1014.4911)
2022-11-01 13:50:20,614:INFO: Dataset: zara2               Batch: 11/36	Loss -1725.0004 (-1082.2133)
2022-11-01 13:50:21,411:INFO: Dataset: zara2               Batch: 12/36	Loss -888.8369 (-1065.0391)
2022-11-01 13:50:22,225:INFO: Dataset: zara2               Batch: 13/36	Loss -185.2514 (-1000.6253)
2022-11-01 13:50:22,977:INFO: Dataset: zara2               Batch: 14/36	Loss -4321.2363 (-1267.2741)
2022-11-01 13:50:23,739:INFO: Dataset: zara2               Batch: 15/36	Loss -3093.8015 (-1384.5150)
2022-11-01 13:50:24,526:INFO: Dataset: zara2               Batch: 16/36	Loss -893.8652 (-1358.3373)
2022-11-01 13:50:25,323:INFO: Dataset: zara2               Batch: 17/36	Loss -1003.1082 (-1337.8720)
2022-11-01 13:50:26,146:INFO: Dataset: zara2               Batch: 18/36	Loss -580.8306 (-1300.4345)
2022-11-01 13:50:27,424:INFO: Dataset: zara2               Batch: 19/36	Loss -434.3596 (-1251.0994)
2022-11-01 13:50:28,272:INFO: Dataset: zara2               Batch: 20/36	Loss -1312.5115 (-1254.0456)
2022-11-01 13:50:29,091:INFO: Dataset: zara2               Batch: 21/36	Loss -1421.2909 (-1260.7889)
2022-11-01 13:50:30,244:INFO: Dataset: zara2               Batch: 22/36	Loss -1074.1688 (-1252.5264)
2022-11-01 13:50:31,118:INFO: Dataset: zara2               Batch: 23/36	Loss -458.3530 (-1215.8818)
2022-11-01 13:50:31,964:INFO: Dataset: zara2               Batch: 24/36	Loss -992.9166 (-1206.1007)
2022-11-01 13:50:32,853:INFO: Dataset: zara2               Batch: 25/36	Loss -1886.4436 (-1233.9130)
2022-11-01 13:50:33,669:INFO: Dataset: zara2               Batch: 26/36	Loss -1205.7706 (-1232.7891)
2022-11-01 13:50:34,497:INFO: Dataset: zara2               Batch: 27/36	Loss -710.7519 (-1213.6296)
2022-11-01 13:50:35,308:INFO: Dataset: zara2               Batch: 28/36	Loss -222.7454 (-1174.4909)
2022-11-01 13:50:36,069:INFO: Dataset: zara2               Batch: 29/36	Loss -1069.5858 (-1171.1270)
2022-11-01 13:50:36,838:INFO: Dataset: zara2               Batch: 30/36	Loss -1012.2946 (-1165.6120)
2022-11-01 13:50:37,598:INFO: Dataset: zara2               Batch: 31/36	Loss -1791.7849 (-1186.0437)
2022-11-01 13:50:38,375:INFO: Dataset: zara2               Batch: 32/36	Loss -1113.9104 (-1183.3257)
2022-11-01 13:50:39,253:INFO: Dataset: zara2               Batch: 33/36	Loss -1490.8560 (-1193.1680)
2022-11-01 13:50:40,062:INFO: Dataset: zara2               Batch: 34/36	Loss 7124.5630 (-960.4335)
2022-11-01 13:50:40,785:INFO: Dataset: zara2               Batch: 35/36	Loss -1694.8719 (-981.5095)
2022-11-01 13:50:41,513:INFO: Dataset: zara2               Batch: 36/36	Loss -237.3673 (-966.1321)
2022-11-01 13:50:42,576:INFO: - Computing ADE (validation o)
2022-11-01 13:50:53,402:INFO: 		 ADE on eth                       dataset:	 2.8631694316864014
2022-11-01 13:50:53,402:INFO: Average validation o:	ADE  2.8632	FDE  4.8356
2022-11-01 13:50:53,404:INFO: - Computing ADE (validation)
2022-11-01 13:51:04,060:INFO: 		 ADE on hotel                     dataset:	 0.9729705452919006
2022-11-01 13:51:14,589:INFO: 		 ADE on univ                      dataset:	 1.50714910030365
2022-11-01 13:51:26,679:INFO: 		 ADE on zara1                     dataset:	 2.578701972961426
2022-11-01 13:51:37,934:INFO: 		 ADE on zara2                     dataset:	 1.448851466178894
2022-11-01 13:51:37,934:INFO: Average validation:	ADE  1.5188	FDE  2.7397
2022-11-01 13:51:37,936:INFO: - Computing ADE (training)
2022-11-01 13:51:48,757:INFO: 		 ADE on hotel                     dataset:	 1.3220738172531128
2022-11-01 13:52:22,233:INFO: 		 ADE on univ                      dataset:	 1.4021716117858887
2022-11-01 13:52:34,936:INFO: 		 ADE on zara1                     dataset:	 2.4850614070892334
2022-11-01 13:53:05,707:INFO: 		 ADE on zara2                     dataset:	 1.7110339403152466
2022-11-01 13:53:05,707:INFO: Average training:	ADE  1.5318	FDE  2.7712
2022-11-01 13:53:05,726:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(200, 150, 300, 200)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_651.pth.tar
2022-11-01 13:53:05,727:INFO: 
===> EPOCH: 652 (P4)
2022-11-01 13:53:05,727:INFO: - Computing loss (training)

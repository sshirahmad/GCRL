2022-10-30 23:37:54,592:INFO: Initializing Training Set
2022-10-30 23:37:58,443:INFO: Initializing Validation Set
2022-10-30 23:37:58,879:INFO: Initializing Validation O Set
2022-10-30 23:38:01,288:INFO: 
===> EPOCH: 251 (P3)
2022-10-30 23:38:01,289:INFO: - Computing loss (training)
2022-10-30 23:38:09,275:INFO: Dataset: hotel               Batch: 1/8	Loss 42.1729 (42.1729)
2022-10-30 23:38:09,903:INFO: Dataset: hotel               Batch: 2/8	Loss 41.3810 (41.7379)
2022-10-30 23:38:10,299:INFO: Dataset: hotel               Batch: 3/8	Loss 41.9959 (41.8187)
2022-10-30 23:38:10,657:INFO: Dataset: hotel               Batch: 4/8	Loss 41.5521 (41.7498)
2022-10-30 23:38:11,019:INFO: Dataset: hotel               Batch: 5/8	Loss 41.2889 (41.6573)
2022-10-30 23:38:11,367:INFO: Dataset: hotel               Batch: 6/8	Loss 40.4862 (41.4568)
2022-10-30 23:38:11,717:INFO: Dataset: hotel               Batch: 7/8	Loss 42.1717 (41.5571)
2022-10-30 23:38:11,979:INFO: Dataset: hotel               Batch: 8/8	Loss 9.3034 (40.5359)
2022-10-30 23:38:34,362:INFO: Dataset: univ                Batch:  1/29	Loss 43.3085 (43.3085)
2022-10-30 23:38:34,731:INFO: Dataset: univ                Batch:  2/29	Loss 41.5503 (42.2606)
2022-10-30 23:38:35,083:INFO: Dataset: univ                Batch:  3/29	Loss 41.7476 (42.0864)
2022-10-30 23:38:35,448:INFO: Dataset: univ                Batch:  4/29	Loss 42.7576 (42.2443)
2022-10-30 23:38:35,796:INFO: Dataset: univ                Batch:  5/29	Loss 42.6397 (42.3244)
2022-10-30 23:38:36,139:INFO: Dataset: univ                Batch:  6/29	Loss 41.8479 (42.2487)
2022-10-30 23:38:36,472:INFO: Dataset: univ                Batch:  7/29	Loss 42.1432 (42.2335)
2022-10-30 23:38:36,812:INFO: Dataset: univ                Batch:  8/29	Loss 41.8880 (42.1924)
2022-10-30 23:38:37,182:INFO: Dataset: univ                Batch:  9/29	Loss 43.1115 (42.2819)
2022-10-30 23:38:37,523:INFO: Dataset: univ                Batch: 10/29	Loss 41.4236 (42.1957)
2022-10-30 23:38:37,863:INFO: Dataset: univ                Batch: 11/29	Loss 43.0030 (42.2734)
2022-10-30 23:38:38,205:INFO: Dataset: univ                Batch: 12/29	Loss 41.2197 (42.1858)
2022-10-30 23:38:38,543:INFO: Dataset: univ                Batch: 13/29	Loss 42.0521 (42.1757)
2022-10-30 23:38:38,882:INFO: Dataset: univ                Batch: 14/29	Loss 41.7314 (42.1400)
2022-10-30 23:38:39,216:INFO: Dataset: univ                Batch: 15/29	Loss 41.0150 (42.0617)
2022-10-30 23:38:39,597:INFO: Dataset: univ                Batch: 16/29	Loss 41.1813 (42.0054)
2022-10-30 23:38:39,953:INFO: Dataset: univ                Batch: 17/29	Loss 43.5673 (42.0883)
2022-10-30 23:38:40,308:INFO: Dataset: univ                Batch: 18/29	Loss 43.5868 (42.1678)
2022-10-30 23:38:40,665:INFO: Dataset: univ                Batch: 19/29	Loss 40.9569 (42.0954)
2022-10-30 23:38:41,030:INFO: Dataset: univ                Batch: 20/29	Loss 41.7230 (42.0778)
2022-10-30 23:38:41,371:INFO: Dataset: univ                Batch: 21/29	Loss 42.5912 (42.0998)
2022-10-30 23:38:41,749:INFO: Dataset: univ                Batch: 22/29	Loss 43.3206 (42.1489)
2022-10-30 23:38:42,098:INFO: Dataset: univ                Batch: 23/29	Loss 41.7503 (42.1319)
2022-10-30 23:38:42,436:INFO: Dataset: univ                Batch: 24/29	Loss 40.9785 (42.0811)
2022-10-30 23:38:42,775:INFO: Dataset: univ                Batch: 25/29	Loss 42.0454 (42.0797)
2022-10-30 23:38:43,125:INFO: Dataset: univ                Batch: 26/29	Loss 40.8636 (42.0323)
2022-10-30 23:38:43,485:INFO: Dataset: univ                Batch: 27/29	Loss 42.5150 (42.0502)
2022-10-30 23:38:43,825:INFO: Dataset: univ                Batch: 28/29	Loss 43.3455 (42.0897)
2022-10-30 23:38:44,115:INFO: Dataset: univ                Batch: 29/29	Loss 15.2443 (41.7297)
2022-10-30 23:38:51,552:INFO: Dataset: zara1               Batch:  1/16	Loss 45.8010 (45.8010)
2022-10-30 23:38:52,106:INFO: Dataset: zara1               Batch:  2/16	Loss 45.6835 (45.7402)
2022-10-30 23:38:52,561:INFO: Dataset: zara1               Batch:  3/16	Loss 45.5835 (45.6873)
2022-10-30 23:38:53,010:INFO: Dataset: zara1               Batch:  4/16	Loss 45.2279 (45.5576)
2022-10-30 23:38:53,373:INFO: Dataset: zara1               Batch:  5/16	Loss 45.8189 (45.6138)
2022-10-30 23:38:53,737:INFO: Dataset: zara1               Batch:  6/16	Loss 46.9273 (45.8030)
2022-10-30 23:38:54,091:INFO: Dataset: zara1               Batch:  7/16	Loss 46.2827 (45.8736)
2022-10-30 23:38:54,441:INFO: Dataset: zara1               Batch:  8/16	Loss 45.9768 (45.8870)
2022-10-30 23:38:54,789:INFO: Dataset: zara1               Batch:  9/16	Loss 44.7280 (45.7515)
2022-10-30 23:38:55,154:INFO: Dataset: zara1               Batch: 10/16	Loss 45.8101 (45.7577)
2022-10-30 23:38:55,504:INFO: Dataset: zara1               Batch: 11/16	Loss 44.2578 (45.6128)
2022-10-30 23:38:55,857:INFO: Dataset: zara1               Batch: 12/16	Loss 46.3566 (45.6674)
2022-10-30 23:38:56,211:INFO: Dataset: zara1               Batch: 13/16	Loss 45.5441 (45.6572)
2022-10-30 23:38:56,559:INFO: Dataset: zara1               Batch: 14/16	Loss 45.2339 (45.6276)
2022-10-30 23:38:56,903:INFO: Dataset: zara1               Batch: 15/16	Loss 45.5438 (45.6218)
2022-10-30 23:38:57,216:INFO: Dataset: zara1               Batch: 16/16	Loss 32.0865 (44.9237)
2022-10-30 23:39:19,693:INFO: Dataset: zara2               Batch:  1/36	Loss 42.8515 (42.8515)
2022-10-30 23:39:20,054:INFO: Dataset: zara2               Batch:  2/36	Loss 44.0194 (43.4375)
2022-10-30 23:39:20,413:INFO: Dataset: zara2               Batch:  3/36	Loss 43.8409 (43.5825)
2022-10-30 23:39:20,765:INFO: Dataset: zara2               Batch:  4/36	Loss 43.0692 (43.4583)
2022-10-30 23:39:21,104:INFO: Dataset: zara2               Batch:  5/36	Loss 41.7780 (43.0795)
2022-10-30 23:39:21,445:INFO: Dataset: zara2               Batch:  6/36	Loss 43.5974 (43.1702)
2022-10-30 23:39:21,808:INFO: Dataset: zara2               Batch:  7/36	Loss 43.8784 (43.2807)
2022-10-30 23:39:22,156:INFO: Dataset: zara2               Batch:  8/36	Loss 42.8091 (43.2213)
2022-10-30 23:39:22,524:INFO: Dataset: zara2               Batch:  9/36	Loss 42.7761 (43.1706)
2022-10-30 23:39:22,862:INFO: Dataset: zara2               Batch: 10/36	Loss 42.6566 (43.1255)
2022-10-30 23:39:23,220:INFO: Dataset: zara2               Batch: 11/36	Loss 42.8727 (43.1027)
2022-10-30 23:39:23,573:INFO: Dataset: zara2               Batch: 12/36	Loss 42.1399 (43.0201)
2022-10-30 23:39:23,918:INFO: Dataset: zara2               Batch: 13/36	Loss 43.0480 (43.0223)
2022-10-30 23:39:24,258:INFO: Dataset: zara2               Batch: 14/36	Loss 43.0411 (43.0238)
2022-10-30 23:39:24,600:INFO: Dataset: zara2               Batch: 15/36	Loss 42.6363 (42.9975)
2022-10-30 23:39:24,945:INFO: Dataset: zara2               Batch: 16/36	Loss 43.2628 (43.0156)
2022-10-30 23:39:25,299:INFO: Dataset: zara2               Batch: 17/36	Loss 43.2930 (43.0340)
2022-10-30 23:39:25,645:INFO: Dataset: zara2               Batch: 18/36	Loss 43.0151 (43.0329)
2022-10-30 23:39:26,000:INFO: Dataset: zara2               Batch: 19/36	Loss 42.5778 (43.0066)
2022-10-30 23:39:26,360:INFO: Dataset: zara2               Batch: 20/36	Loss 42.9169 (43.0011)
2022-10-30 23:39:26,728:INFO: Dataset: zara2               Batch: 21/36	Loss 42.8616 (42.9935)
2022-10-30 23:39:27,092:INFO: Dataset: zara2               Batch: 22/36	Loss 42.7431 (42.9829)
2022-10-30 23:39:27,448:INFO: Dataset: zara2               Batch: 23/36	Loss 43.9342 (43.0245)
2022-10-30 23:39:27,802:INFO: Dataset: zara2               Batch: 24/36	Loss 43.6456 (43.0489)
2022-10-30 23:39:28,147:INFO: Dataset: zara2               Batch: 25/36	Loss 43.8869 (43.0895)
2022-10-30 23:39:28,513:INFO: Dataset: zara2               Batch: 26/36	Loss 41.4664 (43.0224)
2022-10-30 23:39:28,874:INFO: Dataset: zara2               Batch: 27/36	Loss 43.9448 (43.0602)
2022-10-30 23:39:29,244:INFO: Dataset: zara2               Batch: 28/36	Loss 43.2718 (43.0681)
2022-10-30 23:39:29,589:INFO: Dataset: zara2               Batch: 29/36	Loss 42.6814 (43.0532)
2022-10-30 23:39:29,940:INFO: Dataset: zara2               Batch: 30/36	Loss 42.4499 (43.0297)
2022-10-30 23:39:30,307:INFO: Dataset: zara2               Batch: 31/36	Loss 42.7978 (43.0210)
2022-10-30 23:39:30,670:INFO: Dataset: zara2               Batch: 32/36	Loss 41.6873 (42.9779)
2022-10-30 23:39:31,016:INFO: Dataset: zara2               Batch: 33/36	Loss 43.3341 (42.9885)
2022-10-30 23:39:31,382:INFO: Dataset: zara2               Batch: 34/36	Loss 43.2935 (42.9983)
2022-10-30 23:39:31,749:INFO: Dataset: zara2               Batch: 35/36	Loss 43.4435 (43.0122)
2022-10-30 23:39:32,086:INFO: Dataset: zara2               Batch: 36/36	Loss 31.6710 (42.7778)
2022-10-30 23:39:32,952:INFO: - Computing ADE (validation o)
2022-10-30 23:39:41,183:INFO: 		 ADE on eth                       dataset:	 2.80476713180542
2022-10-30 23:39:41,183:INFO: Average validation o:	ADE  2.8048	FDE  4.7770
2022-10-30 23:39:41,184:INFO: - Computing ADE (validation)
2022-10-30 23:39:49,247:INFO: 		 ADE on hotel                     dataset:	 0.9058032631874084
2022-10-30 23:39:57,475:INFO: 		 ADE on univ                      dataset:	 1.4710614681243896
2022-10-30 23:40:05,577:INFO: 		 ADE on zara1                     dataset:	 2.5077462196350098
2022-10-30 23:40:13,962:INFO: 		 ADE on zara2                     dataset:	 1.3849512338638306
2022-10-30 23:40:13,962:INFO: Average validation:	ADE  1.4688	FDE  2.6880
2022-10-30 23:40:13,963:INFO: - Computing ADE (training)
2022-10-30 23:40:22,244:INFO: 		 ADE on hotel                     dataset:	 1.2620054483413696
2022-10-30 23:40:45,216:INFO: 		 ADE on univ                      dataset:	 1.3627936840057373
2022-10-30 23:40:53,825:INFO: 		 ADE on zara1                     dataset:	 2.4435744285583496
2022-10-30 23:41:16,920:INFO: 		 ADE on zara2                     dataset:	 1.6516779661178589
2022-10-30 23:41:16,920:INFO: Average training:	ADE  1.4877	FDE  2.7261
2022-10-30 23:41:16,938:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_251.pth.tar
2022-10-30 23:41:16,938:INFO: 
===> EPOCH: 252 (P3)
2022-10-30 23:41:16,939:INFO: - Computing loss (training)
2022-10-30 23:41:23,881:INFO: Dataset: hotel               Batch: 1/8	Loss 40.6604 (40.6604)
2022-10-30 23:41:24,492:INFO: Dataset: hotel               Batch: 2/8	Loss 41.4742 (41.0635)
2022-10-30 23:41:24,991:INFO: Dataset: hotel               Batch: 3/8	Loss 41.9026 (41.3501)
2022-10-30 23:41:25,381:INFO: Dataset: hotel               Batch: 4/8	Loss 42.1979 (41.5571)
2022-10-30 23:41:25,748:INFO: Dataset: hotel               Batch: 5/8	Loss 41.0409 (41.4543)
2022-10-30 23:41:26,116:INFO: Dataset: hotel               Batch: 6/8	Loss 42.2068 (41.5754)
2022-10-30 23:41:26,472:INFO: Dataset: hotel               Batch: 7/8	Loss 41.1305 (41.5200)
2022-10-30 23:41:26,728:INFO: Dataset: hotel               Batch: 8/8	Loss 9.4631 (40.3781)
2022-10-30 23:41:49,569:INFO: Dataset: univ                Batch:  1/29	Loss 40.9548 (40.9548)
2022-10-30 23:41:49,934:INFO: Dataset: univ                Batch:  2/29	Loss 41.4664 (41.2201)
2022-10-30 23:41:50,301:INFO: Dataset: univ                Batch:  3/29	Loss 41.5121 (41.3143)
2022-10-30 23:41:50,634:INFO: Dataset: univ                Batch:  4/29	Loss 41.4216 (41.3418)
2022-10-30 23:41:50,966:INFO: Dataset: univ                Batch:  5/29	Loss 44.1359 (41.8327)
2022-10-30 23:41:51,303:INFO: Dataset: univ                Batch:  6/29	Loss 41.2313 (41.7243)
2022-10-30 23:41:51,666:INFO: Dataset: univ                Batch:  7/29	Loss 42.1198 (41.7746)
2022-10-30 23:41:52,008:INFO: Dataset: univ                Batch:  8/29	Loss 43.3448 (41.9585)
2022-10-30 23:41:52,352:INFO: Dataset: univ                Batch:  9/29	Loss 41.9138 (41.9528)
2022-10-30 23:41:52,697:INFO: Dataset: univ                Batch: 10/29	Loss 40.9576 (41.8447)
2022-10-30 23:41:53,040:INFO: Dataset: univ                Batch: 11/29	Loss 42.6550 (41.9048)
2022-10-30 23:41:53,378:INFO: Dataset: univ                Batch: 12/29	Loss 42.1689 (41.9252)
2022-10-30 23:41:53,740:INFO: Dataset: univ                Batch: 13/29	Loss 41.2721 (41.8710)
2022-10-30 23:41:54,090:INFO: Dataset: univ                Batch: 14/29	Loss 42.4013 (41.9096)
2022-10-30 23:41:54,428:INFO: Dataset: univ                Batch: 15/29	Loss 41.5792 (41.8894)
2022-10-30 23:41:54,773:INFO: Dataset: univ                Batch: 16/29	Loss 42.3367 (41.9186)
2022-10-30 23:41:55,118:INFO: Dataset: univ                Batch: 17/29	Loss 43.5571 (41.9943)
2022-10-30 23:41:55,469:INFO: Dataset: univ                Batch: 18/29	Loss 42.3465 (42.0125)
2022-10-30 23:41:55,822:INFO: Dataset: univ                Batch: 19/29	Loss 41.1747 (41.9719)
2022-10-30 23:41:56,169:INFO: Dataset: univ                Batch: 20/29	Loss 42.3497 (41.9902)
2022-10-30 23:41:56,517:INFO: Dataset: univ                Batch: 21/29	Loss 42.0174 (41.9914)
2022-10-30 23:41:56,859:INFO: Dataset: univ                Batch: 22/29	Loss 43.5308 (42.0500)
2022-10-30 23:41:57,222:INFO: Dataset: univ                Batch: 23/29	Loss 40.8354 (41.9889)
2022-10-30 23:41:57,576:INFO: Dataset: univ                Batch: 24/29	Loss 41.3018 (41.9565)
2022-10-30 23:41:57,922:INFO: Dataset: univ                Batch: 25/29	Loss 43.3759 (42.0063)
2022-10-30 23:41:58,261:INFO: Dataset: univ                Batch: 26/29	Loss 41.0183 (41.9626)
2022-10-30 23:41:58,612:INFO: Dataset: univ                Batch: 27/29	Loss 42.5802 (41.9842)
2022-10-30 23:41:58,950:INFO: Dataset: univ                Batch: 28/29	Loss 41.9365 (41.9826)
2022-10-30 23:41:59,237:INFO: Dataset: univ                Batch: 29/29	Loss 16.0400 (41.6605)
2022-10-30 23:42:06,885:INFO: Dataset: zara1               Batch:  1/16	Loss 44.3394 (44.3394)
2022-10-30 23:42:07,465:INFO: Dataset: zara1               Batch:  2/16	Loss 44.3767 (44.3559)
2022-10-30 23:42:07,951:INFO: Dataset: zara1               Batch:  3/16	Loss 46.1676 (44.8984)
2022-10-30 23:42:08,331:INFO: Dataset: zara1               Batch:  4/16	Loss 44.6701 (44.8369)
2022-10-30 23:42:08,719:INFO: Dataset: zara1               Batch:  5/16	Loss 47.2476 (45.2664)
2022-10-30 23:42:09,096:INFO: Dataset: zara1               Batch:  6/16	Loss 46.8709 (45.4959)
2022-10-30 23:42:09,470:INFO: Dataset: zara1               Batch:  7/16	Loss 45.3604 (45.4752)
2022-10-30 23:42:09,832:INFO: Dataset: zara1               Batch:  8/16	Loss 45.4093 (45.4672)
2022-10-30 23:42:10,212:INFO: Dataset: zara1               Batch:  9/16	Loss 44.9898 (45.4233)
2022-10-30 23:42:10,556:INFO: Dataset: zara1               Batch: 10/16	Loss 45.3369 (45.4131)
2022-10-30 23:42:10,901:INFO: Dataset: zara1               Batch: 11/16	Loss 46.2252 (45.4825)
2022-10-30 23:42:11,284:INFO: Dataset: zara1               Batch: 12/16	Loss 45.5791 (45.4909)
2022-10-30 23:42:11,625:INFO: Dataset: zara1               Batch: 13/16	Loss 46.3941 (45.5536)
2022-10-30 23:42:12,006:INFO: Dataset: zara1               Batch: 14/16	Loss 45.2117 (45.5288)
2022-10-30 23:42:12,370:INFO: Dataset: zara1               Batch: 15/16	Loss 44.5382 (45.4541)
2022-10-30 23:42:12,705:INFO: Dataset: zara1               Batch: 16/16	Loss 33.5473 (45.0217)
2022-10-30 23:42:35,644:INFO: Dataset: zara2               Batch:  1/36	Loss 42.8884 (42.8884)
2022-10-30 23:42:35,993:INFO: Dataset: zara2               Batch:  2/36	Loss 41.8261 (42.3824)
2022-10-30 23:42:36,344:INFO: Dataset: zara2               Batch:  3/36	Loss 43.8253 (42.8800)
2022-10-30 23:42:36,705:INFO: Dataset: zara2               Batch:  4/36	Loss 42.9892 (42.9042)
2022-10-30 23:42:37,048:INFO: Dataset: zara2               Batch:  5/36	Loss 43.1660 (42.9534)
2022-10-30 23:42:37,392:INFO: Dataset: zara2               Batch:  6/36	Loss 41.4932 (42.7430)
2022-10-30 23:42:37,751:INFO: Dataset: zara2               Batch:  7/36	Loss 42.2142 (42.6543)
2022-10-30 23:42:38,091:INFO: Dataset: zara2               Batch:  8/36	Loss 43.1382 (42.7194)
2022-10-30 23:42:38,436:INFO: Dataset: zara2               Batch:  9/36	Loss 44.0597 (42.8655)
2022-10-30 23:42:38,802:INFO: Dataset: zara2               Batch: 10/36	Loss 43.5446 (42.9223)
2022-10-30 23:42:39,146:INFO: Dataset: zara2               Batch: 11/36	Loss 43.9002 (43.0130)
2022-10-30 23:42:39,516:INFO: Dataset: zara2               Batch: 12/36	Loss 42.9863 (43.0108)
2022-10-30 23:42:39,881:INFO: Dataset: zara2               Batch: 13/36	Loss 43.2515 (43.0280)
2022-10-30 23:42:40,234:INFO: Dataset: zara2               Batch: 14/36	Loss 43.1920 (43.0398)
2022-10-30 23:42:40,569:INFO: Dataset: zara2               Batch: 15/36	Loss 43.7550 (43.0926)
2022-10-30 23:42:40,918:INFO: Dataset: zara2               Batch: 16/36	Loss 42.3604 (43.0522)
2022-10-30 23:42:41,279:INFO: Dataset: zara2               Batch: 17/36	Loss 43.3862 (43.0710)
2022-10-30 23:42:41,626:INFO: Dataset: zara2               Batch: 18/36	Loss 44.1632 (43.1375)
2022-10-30 23:42:42,008:INFO: Dataset: zara2               Batch: 19/36	Loss 42.9902 (43.1300)
2022-10-30 23:42:42,354:INFO: Dataset: zara2               Batch: 20/36	Loss 42.8513 (43.1162)
2022-10-30 23:42:42,706:INFO: Dataset: zara2               Batch: 21/36	Loss 43.2062 (43.1196)
2022-10-30 23:42:43,048:INFO: Dataset: zara2               Batch: 22/36	Loss 43.0870 (43.1181)
2022-10-30 23:42:43,402:INFO: Dataset: zara2               Batch: 23/36	Loss 43.0566 (43.1156)
2022-10-30 23:42:43,746:INFO: Dataset: zara2               Batch: 24/36	Loss 42.8517 (43.1035)
2022-10-30 23:42:44,088:INFO: Dataset: zara2               Batch: 25/36	Loss 42.4961 (43.0769)
2022-10-30 23:42:44,450:INFO: Dataset: zara2               Batch: 26/36	Loss 42.8864 (43.0698)
2022-10-30 23:42:44,804:INFO: Dataset: zara2               Batch: 27/36	Loss 42.3146 (43.0431)
2022-10-30 23:42:45,149:INFO: Dataset: zara2               Batch: 28/36	Loss 43.0894 (43.0447)
2022-10-30 23:42:45,494:INFO: Dataset: zara2               Batch: 29/36	Loss 42.6306 (43.0303)
2022-10-30 23:42:45,842:INFO: Dataset: zara2               Batch: 30/36	Loss 43.2457 (43.0368)
2022-10-30 23:42:46,193:INFO: Dataset: zara2               Batch: 31/36	Loss 42.9137 (43.0327)
2022-10-30 23:42:46,545:INFO: Dataset: zara2               Batch: 32/36	Loss 43.4827 (43.0464)
2022-10-30 23:42:46,897:INFO: Dataset: zara2               Batch: 33/36	Loss 42.4176 (43.0274)
2022-10-30 23:42:47,244:INFO: Dataset: zara2               Batch: 34/36	Loss 43.4317 (43.0385)
2022-10-30 23:42:47,667:INFO: Dataset: zara2               Batch: 35/36	Loss 42.9174 (43.0347)
2022-10-30 23:42:48,004:INFO: Dataset: zara2               Batch: 36/36	Loss 29.7644 (42.7626)
2022-10-30 23:42:48,878:INFO: - Computing ADE (validation o)
2022-10-30 23:42:57,009:INFO: 		 ADE on eth                       dataset:	 2.780322313308716
2022-10-30 23:42:57,009:INFO: Average validation o:	ADE  2.7803	FDE  4.7560
2022-10-30 23:42:57,010:INFO: - Computing ADE (validation)
2022-10-30 23:43:05,082:INFO: 		 ADE on hotel                     dataset:	 0.9512308239936829
2022-10-30 23:43:13,381:INFO: 		 ADE on univ                      dataset:	 1.4998929500579834
2022-10-30 23:43:21,998:INFO: 		 ADE on zara1                     dataset:	 2.49312424659729
2022-10-30 23:43:30,812:INFO: 		 ADE on zara2                     dataset:	 1.4023925065994263
2022-10-30 23:43:30,812:INFO: Average validation:	ADE  1.4918	FDE  2.7494
2022-10-30 23:43:30,813:INFO: - Computing ADE (training)
2022-10-30 23:43:39,723:INFO: 		 ADE on hotel                     dataset:	 1.3163275718688965
2022-10-30 23:44:04,769:INFO: 		 ADE on univ                      dataset:	 1.373079538345337
2022-10-30 23:44:13,459:INFO: 		 ADE on zara1                     dataset:	 2.4109816551208496
2022-10-30 23:44:37,789:INFO: 		 ADE on zara2                     dataset:	 1.6602472066879272
2022-10-30 23:44:37,789:INFO: Average training:	ADE  1.4961	FDE  2.7557
2022-10-30 23:44:37,808:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_252.pth.tar
2022-10-30 23:44:37,808:INFO: 
===> EPOCH: 253 (P3)
2022-10-30 23:44:37,808:INFO: - Computing loss (training)
2022-10-30 23:44:45,629:INFO: Dataset: hotel               Batch: 1/8	Loss 42.3616 (42.3616)
2022-10-30 23:44:46,245:INFO: Dataset: hotel               Batch: 2/8	Loss 41.5399 (41.9546)
2022-10-30 23:44:46,766:INFO: Dataset: hotel               Batch: 3/8	Loss 41.7105 (41.8753)
2022-10-30 23:44:47,345:INFO: Dataset: hotel               Batch: 4/8	Loss 42.5647 (42.0342)
2022-10-30 23:44:47,921:INFO: Dataset: hotel               Batch: 5/8	Loss 40.4806 (41.6926)
2022-10-30 23:44:48,774:INFO: Dataset: hotel               Batch: 6/8	Loss 40.4796 (41.4898)
2022-10-30 23:44:49,785:INFO: Dataset: hotel               Batch: 7/8	Loss 41.8728 (41.5460)
2022-10-30 23:44:50,314:INFO: Dataset: hotel               Batch: 8/8	Loss 8.8756 (40.5978)
2022-10-30 23:45:15,964:INFO: Dataset: univ                Batch:  1/29	Loss 41.0076 (41.0076)
2022-10-30 23:45:16,477:INFO: Dataset: univ                Batch:  2/29	Loss 41.5449 (41.2825)
2022-10-30 23:45:16,980:INFO: Dataset: univ                Batch:  3/29	Loss 42.1799 (41.5696)
2022-10-30 23:45:17,488:INFO: Dataset: univ                Batch:  4/29	Loss 41.7862 (41.6226)
2022-10-30 23:45:17,973:INFO: Dataset: univ                Batch:  5/29	Loss 43.8564 (41.9913)
2022-10-30 23:45:18,476:INFO: Dataset: univ                Batch:  6/29	Loss 40.9322 (41.7895)
2022-10-30 23:45:18,960:INFO: Dataset: univ                Batch:  7/29	Loss 41.8409 (41.7974)
2022-10-30 23:45:19,477:INFO: Dataset: univ                Batch:  8/29	Loss 42.0791 (41.8358)
2022-10-30 23:45:19,967:INFO: Dataset: univ                Batch:  9/29	Loss 41.0229 (41.7291)
2022-10-30 23:45:20,475:INFO: Dataset: univ                Batch: 10/29	Loss 41.6666 (41.7229)
2022-10-30 23:45:20,959:INFO: Dataset: univ                Batch: 11/29	Loss 42.3406 (41.7730)
2022-10-30 23:45:21,475:INFO: Dataset: univ                Batch: 12/29	Loss 43.5351 (41.8947)
2022-10-30 23:45:21,973:INFO: Dataset: univ                Batch: 13/29	Loss 42.9397 (41.9662)
2022-10-30 23:45:22,508:INFO: Dataset: univ                Batch: 14/29	Loss 41.8369 (41.9567)
2022-10-30 23:45:23,014:INFO: Dataset: univ                Batch: 15/29	Loss 40.7279 (41.8690)
2022-10-30 23:45:23,483:INFO: Dataset: univ                Batch: 16/29	Loss 42.7887 (41.9289)
2022-10-30 23:45:24,014:INFO: Dataset: univ                Batch: 17/29	Loss 42.5179 (41.9612)
2022-10-30 23:45:24,507:INFO: Dataset: univ                Batch: 18/29	Loss 43.8194 (42.0508)
2022-10-30 23:45:24,982:INFO: Dataset: univ                Batch: 19/29	Loss 40.7194 (41.9686)
2022-10-30 23:45:25,457:INFO: Dataset: univ                Batch: 20/29	Loss 42.7764 (42.0086)
2022-10-30 23:45:25,911:INFO: Dataset: univ                Batch: 21/29	Loss 40.6062 (41.9296)
2022-10-30 23:45:26,353:INFO: Dataset: univ                Batch: 22/29	Loss 43.0642 (41.9746)
2022-10-30 23:45:26,802:INFO: Dataset: univ                Batch: 23/29	Loss 41.5562 (41.9567)
2022-10-30 23:45:27,291:INFO: Dataset: univ                Batch: 24/29	Loss 41.4656 (41.9355)
2022-10-30 23:45:27,770:INFO: Dataset: univ                Batch: 25/29	Loss 42.9085 (41.9712)
2022-10-30 23:45:28,254:INFO: Dataset: univ                Batch: 26/29	Loss 42.8199 (42.0049)
2022-10-30 23:45:28,733:INFO: Dataset: univ                Batch: 27/29	Loss 40.4809 (41.9395)
2022-10-30 23:45:29,200:INFO: Dataset: univ                Batch: 28/29	Loss 42.0534 (41.9442)
2022-10-30 23:45:29,575:INFO: Dataset: univ                Batch: 29/29	Loss 16.1020 (41.6320)
2022-10-30 23:45:38,169:INFO: Dataset: zara1               Batch:  1/16	Loss 45.1143 (45.1143)
2022-10-30 23:45:38,759:INFO: Dataset: zara1               Batch:  2/16	Loss 45.1623 (45.1381)
2022-10-30 23:45:39,306:INFO: Dataset: zara1               Batch:  3/16	Loss 44.6904 (44.9812)
2022-10-30 23:45:39,901:INFO: Dataset: zara1               Batch:  4/16	Loss 44.8144 (44.9404)
2022-10-30 23:45:40,395:INFO: Dataset: zara1               Batch:  5/16	Loss 45.9991 (45.1289)
2022-10-30 23:45:40,908:INFO: Dataset: zara1               Batch:  6/16	Loss 44.7568 (45.0706)
2022-10-30 23:45:41,422:INFO: Dataset: zara1               Batch:  7/16	Loss 45.4990 (45.1295)
2022-10-30 23:45:41,959:INFO: Dataset: zara1               Batch:  8/16	Loss 46.6434 (45.3110)
2022-10-30 23:45:42,452:INFO: Dataset: zara1               Batch:  9/16	Loss 45.4937 (45.3295)
2022-10-30 23:45:42,948:INFO: Dataset: zara1               Batch: 10/16	Loss 46.3837 (45.4373)
2022-10-30 23:45:43,419:INFO: Dataset: zara1               Batch: 11/16	Loss 44.9546 (45.3922)
2022-10-30 23:45:43,900:INFO: Dataset: zara1               Batch: 12/16	Loss 45.6286 (45.4088)
2022-10-30 23:45:44,394:INFO: Dataset: zara1               Batch: 13/16	Loss 46.1670 (45.4642)
2022-10-30 23:45:44,864:INFO: Dataset: zara1               Batch: 14/16	Loss 45.3053 (45.4542)
2022-10-30 23:45:45,395:INFO: Dataset: zara1               Batch: 15/16	Loss 46.6885 (45.5091)
2022-10-30 23:45:45,819:INFO: Dataset: zara1               Batch: 16/16	Loss 32.9692 (44.9745)
2022-10-30 23:46:10,885:INFO: Dataset: zara2               Batch:  1/36	Loss 41.8281 (41.8281)
2022-10-30 23:46:11,271:INFO: Dataset: zara2               Batch:  2/36	Loss 42.1396 (41.9887)
2022-10-30 23:46:11,627:INFO: Dataset: zara2               Batch:  3/36	Loss 43.0076 (42.3146)
2022-10-30 23:46:11,997:INFO: Dataset: zara2               Batch:  4/36	Loss 43.1066 (42.5048)
2022-10-30 23:46:12,352:INFO: Dataset: zara2               Batch:  5/36	Loss 44.2834 (42.8215)
2022-10-30 23:46:12,705:INFO: Dataset: zara2               Batch:  6/36	Loss 42.6912 (42.8011)
2022-10-30 23:46:13,131:INFO: Dataset: zara2               Batch:  7/36	Loss 42.5091 (42.7508)
2022-10-30 23:46:13,485:INFO: Dataset: zara2               Batch:  8/36	Loss 43.8214 (42.8698)
2022-10-30 23:46:13,832:INFO: Dataset: zara2               Batch:  9/36	Loss 43.5492 (42.9399)
2022-10-30 23:46:14,188:INFO: Dataset: zara2               Batch: 10/36	Loss 43.4378 (42.9913)
2022-10-30 23:46:14,549:INFO: Dataset: zara2               Batch: 11/36	Loss 42.7454 (42.9672)
2022-10-30 23:46:14,924:INFO: Dataset: zara2               Batch: 12/36	Loss 42.1971 (42.8950)
2022-10-30 23:46:15,285:INFO: Dataset: zara2               Batch: 13/36	Loss 43.1111 (42.9114)
2022-10-30 23:46:15,630:INFO: Dataset: zara2               Batch: 14/36	Loss 41.8632 (42.8238)
2022-10-30 23:46:15,981:INFO: Dataset: zara2               Batch: 15/36	Loss 43.3490 (42.8564)
2022-10-30 23:46:16,374:INFO: Dataset: zara2               Batch: 16/36	Loss 42.8782 (42.8578)
2022-10-30 23:46:16,798:INFO: Dataset: zara2               Batch: 17/36	Loss 42.2502 (42.8213)
2022-10-30 23:46:17,230:INFO: Dataset: zara2               Batch: 18/36	Loss 44.0093 (42.8739)
2022-10-30 23:46:17,670:INFO: Dataset: zara2               Batch: 19/36	Loss 42.2001 (42.8379)
2022-10-30 23:46:18,031:INFO: Dataset: zara2               Batch: 20/36	Loss 43.1646 (42.8550)
2022-10-30 23:46:18,403:INFO: Dataset: zara2               Batch: 21/36	Loss 43.4902 (42.8820)
2022-10-30 23:46:18,766:INFO: Dataset: zara2               Batch: 22/36	Loss 42.9406 (42.8850)
2022-10-30 23:46:19,117:INFO: Dataset: zara2               Batch: 23/36	Loss 42.8624 (42.8840)
2022-10-30 23:46:19,462:INFO: Dataset: zara2               Batch: 24/36	Loss 42.1550 (42.8509)
2022-10-30 23:46:19,807:INFO: Dataset: zara2               Batch: 25/36	Loss 43.6319 (42.8825)
2022-10-30 23:46:20,152:INFO: Dataset: zara2               Batch: 26/36	Loss 42.5446 (42.8712)
2022-10-30 23:46:20,510:INFO: Dataset: zara2               Batch: 27/36	Loss 42.5767 (42.8599)
2022-10-30 23:46:20,878:INFO: Dataset: zara2               Batch: 28/36	Loss 43.4112 (42.8774)
2022-10-30 23:46:21,247:INFO: Dataset: zara2               Batch: 29/36	Loss 42.9271 (42.8790)
2022-10-30 23:46:21,606:INFO: Dataset: zara2               Batch: 30/36	Loss 44.0130 (42.9124)
2022-10-30 23:46:21,949:INFO: Dataset: zara2               Batch: 31/36	Loss 42.5337 (42.8993)
2022-10-30 23:46:22,311:INFO: Dataset: zara2               Batch: 32/36	Loss 43.4986 (42.9186)
2022-10-30 23:46:22,664:INFO: Dataset: zara2               Batch: 33/36	Loss 42.3927 (42.9027)
2022-10-30 23:46:23,034:INFO: Dataset: zara2               Batch: 34/36	Loss 43.5310 (42.9208)
2022-10-30 23:46:23,393:INFO: Dataset: zara2               Batch: 35/36	Loss 42.2388 (42.9008)
2022-10-30 23:46:23,738:INFO: Dataset: zara2               Batch: 36/36	Loss 31.5192 (42.6543)
2022-10-30 23:46:24,622:INFO: - Computing ADE (validation o)
2022-10-30 23:46:32,915:INFO: 		 ADE on eth                       dataset:	 2.728210926055908
2022-10-30 23:46:32,916:INFO: Average validation o:	ADE  2.7282	FDE  4.6686
2022-10-30 23:46:32,916:INFO: - Computing ADE (validation)
2022-10-30 23:46:41,161:INFO: 		 ADE on hotel                     dataset:	 0.9861267805099487
2022-10-30 23:46:49,609:INFO: 		 ADE on univ                      dataset:	 1.509494423866272
2022-10-30 23:46:57,903:INFO: 		 ADE on zara1                     dataset:	 2.492202043533325
2022-10-30 23:47:06,563:INFO: 		 ADE on zara2                     dataset:	 1.4224125146865845
2022-10-30 23:47:06,563:INFO: Average validation:	ADE  1.5060	FDE  2.7807
2022-10-30 23:47:06,564:INFO: - Computing ADE (training)
2022-10-30 23:47:15,068:INFO: 		 ADE on hotel                     dataset:	 1.3385409116744995
2022-10-30 23:47:39,624:INFO: 		 ADE on univ                      dataset:	 1.3757808208465576
2022-10-30 23:47:49,891:INFO: 		 ADE on zara1                     dataset:	 2.37321400642395
2022-10-30 23:48:14,794:INFO: 		 ADE on zara2                     dataset:	 1.657118558883667
2022-10-30 23:48:14,794:INFO: Average training:	ADE  1.4955	FDE  2.7618
2022-10-30 23:48:14,807:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_253.pth.tar
2022-10-30 23:48:14,807:INFO: 
===> EPOCH: 254 (P3)
2022-10-30 23:48:14,808:INFO: - Computing loss (training)
2022-10-30 23:48:21,678:INFO: Dataset: hotel               Batch: 1/8	Loss 40.7076 (40.7076)
2022-10-30 23:48:22,253:INFO: Dataset: hotel               Batch: 2/8	Loss 40.4245 (40.5596)
2022-10-30 23:48:22,838:INFO: Dataset: hotel               Batch: 3/8	Loss 40.4940 (40.5390)
2022-10-30 23:48:23,227:INFO: Dataset: hotel               Batch: 4/8	Loss 43.9015 (41.3437)
2022-10-30 23:48:23,571:INFO: Dataset: hotel               Batch: 5/8	Loss 41.0245 (41.2797)
2022-10-30 23:48:23,924:INFO: Dataset: hotel               Batch: 6/8	Loss 42.7582 (41.5190)
2022-10-30 23:48:24,263:INFO: Dataset: hotel               Batch: 7/8	Loss 41.1239 (41.4617)
2022-10-30 23:48:24,524:INFO: Dataset: hotel               Batch: 8/8	Loss 9.6050 (40.5791)
2022-10-30 23:48:47,182:INFO: Dataset: univ                Batch:  1/29	Loss 42.6638 (42.6638)
2022-10-30 23:48:47,542:INFO: Dataset: univ                Batch:  2/29	Loss 42.5188 (42.5875)
2022-10-30 23:48:47,880:INFO: Dataset: univ                Batch:  3/29	Loss 41.2831 (42.1110)
2022-10-30 23:48:48,218:INFO: Dataset: univ                Batch:  4/29	Loss 41.6856 (41.9930)
2022-10-30 23:48:48,580:INFO: Dataset: univ                Batch:  5/29	Loss 43.0428 (42.1748)
2022-10-30 23:48:48,938:INFO: Dataset: univ                Batch:  6/29	Loss 41.9093 (42.1288)
2022-10-30 23:48:49,299:INFO: Dataset: univ                Batch:  7/29	Loss 42.5146 (42.1769)
2022-10-30 23:48:49,649:INFO: Dataset: univ                Batch:  8/29	Loss 43.7625 (42.3534)
2022-10-30 23:48:50,000:INFO: Dataset: univ                Batch:  9/29	Loss 43.9402 (42.5064)
2022-10-30 23:48:50,358:INFO: Dataset: univ                Batch: 10/29	Loss 40.4522 (42.2537)
2022-10-30 23:48:50,719:INFO: Dataset: univ                Batch: 11/29	Loss 41.9273 (42.2253)
2022-10-30 23:48:51,061:INFO: Dataset: univ                Batch: 12/29	Loss 42.2884 (42.2301)
2022-10-30 23:48:51,399:INFO: Dataset: univ                Batch: 13/29	Loss 40.7985 (42.1047)
2022-10-30 23:48:51,742:INFO: Dataset: univ                Batch: 14/29	Loss 42.1577 (42.1089)
2022-10-30 23:48:52,074:INFO: Dataset: univ                Batch: 15/29	Loss 40.9662 (42.0273)
2022-10-30 23:48:52,418:INFO: Dataset: univ                Batch: 16/29	Loss 40.8449 (41.9430)
2022-10-30 23:48:52,770:INFO: Dataset: univ                Batch: 17/29	Loss 41.7771 (41.9329)
2022-10-30 23:48:53,120:INFO: Dataset: univ                Batch: 18/29	Loss 41.1017 (41.8851)
2022-10-30 23:48:53,468:INFO: Dataset: univ                Batch: 19/29	Loss 42.2533 (41.9053)
2022-10-30 23:48:53,800:INFO: Dataset: univ                Batch: 20/29	Loss 41.6733 (41.8934)
2022-10-30 23:48:54,147:INFO: Dataset: univ                Batch: 21/29	Loss 41.2978 (41.8623)
2022-10-30 23:48:54,488:INFO: Dataset: univ                Batch: 22/29	Loss 41.6280 (41.8520)
2022-10-30 23:48:54,846:INFO: Dataset: univ                Batch: 23/29	Loss 41.4244 (41.8321)
2022-10-30 23:48:55,201:INFO: Dataset: univ                Batch: 24/29	Loss 41.3869 (41.8119)
2022-10-30 23:48:55,564:INFO: Dataset: univ                Batch: 25/29	Loss 42.9876 (41.8559)
2022-10-30 23:48:55,927:INFO: Dataset: univ                Batch: 26/29	Loss 41.7552 (41.8522)
2022-10-30 23:48:56,277:INFO: Dataset: univ                Batch: 27/29	Loss 42.9486 (41.8858)
2022-10-30 23:48:56,611:INFO: Dataset: univ                Batch: 28/29	Loss 43.9185 (41.9468)
2022-10-30 23:48:56,903:INFO: Dataset: univ                Batch: 29/29	Loss 15.4720 (41.6482)

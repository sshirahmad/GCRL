2022-11-18 18:10:22,740:INFO: Initializing Training Set
2022-11-18 18:10:24,299:INFO: Initializing Validation Set
2022-11-18 18:10:24,476:INFO: Initializing Validation O Set
2022-11-18 18:10:26,479:INFO: 
===> EPOCH: 301 (P4)
2022-11-18 18:10:26,479:INFO: - Computing loss (training)
2022-11-18 18:10:27,041:INFO: Dataset: hotel               Batch: 1/4	Loss 156.2124 (156.2124)
2022-11-18 18:10:27,408:INFO: Dataset: hotel               Batch: 2/4	Loss 153.6191 (154.9340)
2022-11-18 18:10:27,779:INFO: Dataset: hotel               Batch: 3/4	Loss 159.8451 (156.5295)
2022-11-18 18:10:28,107:INFO: Dataset: hotel               Batch: 4/4	Loss 96.2135 (146.4238)
2022-11-18 18:10:28,814:INFO: Dataset: univ                Batch:  1/15	Loss 155.1406 (155.1406)
2022-11-18 18:10:29,328:INFO: Dataset: univ                Batch:  2/15	Loss 155.4335 (155.2770)
2022-11-18 18:10:29,758:INFO: Dataset: univ                Batch:  3/15	Loss 159.1285 (156.5053)
2022-11-18 18:10:30,218:INFO: Dataset: univ                Batch:  4/15	Loss 151.7866 (155.1965)
2022-11-18 18:10:30,652:INFO: Dataset: univ                Batch:  5/15	Loss 154.8329 (155.1271)
2022-11-18 18:10:31,093:INFO: Dataset: univ                Batch:  6/15	Loss 152.8985 (154.7321)
2022-11-18 18:10:31,501:INFO: Dataset: univ                Batch:  7/15	Loss 156.1090 (154.8953)
2022-11-18 18:10:31,925:INFO: Dataset: univ                Batch:  8/15	Loss 155.1576 (154.9289)
2022-11-18 18:10:32,354:INFO: Dataset: univ                Batch:  9/15	Loss 154.6933 (154.9027)
2022-11-18 18:10:32,787:INFO: Dataset: univ                Batch: 10/15	Loss 153.9721 (154.8071)
2022-11-18 18:10:33,238:INFO: Dataset: univ                Batch: 11/15	Loss 151.7299 (154.5003)
2022-11-18 18:10:33,689:INFO: Dataset: univ                Batch: 12/15	Loss 152.0901 (154.2783)
2022-11-18 18:10:34,118:INFO: Dataset: univ                Batch: 13/15	Loss 155.2631 (154.3475)
2022-11-18 18:10:34,548:INFO: Dataset: univ                Batch: 14/15	Loss 152.5432 (154.2204)
2022-11-18 18:10:34,838:INFO: Dataset: univ                Batch: 15/15	Loss 28.5512 (152.7078)
2022-11-18 18:10:35,419:INFO: Dataset: zara1               Batch: 1/8	Loss 164.6818 (164.6818)
2022-11-18 18:10:35,790:INFO: Dataset: zara1               Batch: 2/8	Loss 162.3769 (163.6025)
2022-11-18 18:10:36,158:INFO: Dataset: zara1               Batch: 3/8	Loss 165.8405 (164.3140)
2022-11-18 18:10:36,526:INFO: Dataset: zara1               Batch: 4/8	Loss 162.4526 (163.8186)
2022-11-18 18:10:36,900:INFO: Dataset: zara1               Batch: 5/8	Loss 163.5292 (163.7503)
2022-11-18 18:10:37,270:INFO: Dataset: zara1               Batch: 6/8	Loss 164.7901 (163.9437)
2022-11-18 18:10:37,643:INFO: Dataset: zara1               Batch: 7/8	Loss 166.8876 (164.3243)
2022-11-18 18:10:37,997:INFO: Dataset: zara1               Batch: 8/8	Loss 142.4606 (161.8618)
2022-11-18 18:10:38,562:INFO: Dataset: zara2               Batch:  1/18	Loss 157.2296 (157.2296)
2022-11-18 18:10:38,937:INFO: Dataset: zara2               Batch:  2/18	Loss 155.6229 (156.3468)
2022-11-18 18:10:39,314:INFO: Dataset: zara2               Batch:  3/18	Loss 156.4270 (156.3732)
2022-11-18 18:10:39,691:INFO: Dataset: zara2               Batch:  4/18	Loss 151.4618 (155.0092)
2022-11-18 18:10:40,103:INFO: Dataset: zara2               Batch:  5/18	Loss 151.4027 (154.3111)
2022-11-18 18:10:40,527:INFO: Dataset: zara2               Batch:  6/18	Loss 154.4020 (154.3269)
2022-11-18 18:10:40,914:INFO: Dataset: zara2               Batch:  7/18	Loss 152.5013 (154.0851)
2022-11-18 18:10:41,303:INFO: Dataset: zara2               Batch:  8/18	Loss 156.0378 (154.3473)
2022-11-18 18:10:41,673:INFO: Dataset: zara2               Batch:  9/18	Loss 153.4472 (154.2560)
2022-11-18 18:10:42,057:INFO: Dataset: zara2               Batch: 10/18	Loss 151.2072 (153.9639)
2022-11-18 18:10:42,476:INFO: Dataset: zara2               Batch: 11/18	Loss 151.6721 (153.7584)
2022-11-18 18:10:42,854:INFO: Dataset: zara2               Batch: 12/18	Loss 151.4476 (153.5543)
2022-11-18 18:10:43,233:INFO: Dataset: zara2               Batch: 13/18	Loss 151.9890 (153.4351)
2022-11-18 18:10:43,614:INFO: Dataset: zara2               Batch: 14/18	Loss 149.2113 (153.1719)
2022-11-18 18:10:43,994:INFO: Dataset: zara2               Batch: 15/18	Loss 149.9272 (152.9730)
2022-11-18 18:10:44,374:INFO: Dataset: zara2               Batch: 16/18	Loss 151.6532 (152.8922)
2022-11-18 18:10:44,751:INFO: Dataset: zara2               Batch: 17/18	Loss 152.5417 (152.8688)
2022-11-18 18:10:45,114:INFO: Dataset: zara2               Batch: 18/18	Loss 131.3973 (151.7613)
2022-11-18 18:10:45,162:INFO: - Computing ADE (validation)
2022-11-18 18:10:45,409:INFO: 		 ADE on hotel                     dataset:	 1.2668178081512451
2022-11-18 18:10:45,860:INFO: 		 ADE on univ                      dataset:	 1.6570881605148315
2022-11-18 18:10:46,162:INFO: 		 ADE on zara1                     dataset:	 2.6312713623046875
2022-11-18 18:10:46,635:INFO: 		 ADE on zara2                     dataset:	 1.6715790033340454
2022-11-18 18:10:46,635:INFO: Average validation:	ADE  1.6977	FDE  2.8988
2022-11-18 18:10:46,636:INFO: - Computing ADE (training)
2022-11-18 18:10:47,033:INFO: 		 ADE on hotel                     dataset:	 1.566534161567688
2022-11-18 18:10:48,039:INFO: 		 ADE on univ                      dataset:	 1.5711497068405151
2022-11-18 18:10:48,608:INFO: 		 ADE on zara1                     dataset:	 2.552703857421875
2022-11-18 18:10:49,690:INFO: 		 ADE on zara2                     dataset:	 1.9077191352844238
2022-11-18 18:10:49,690:INFO: Average training:	ADE  1.7019	FDE  2.9215
2022-11-18 18:10:49,707:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_301.pth.tar
2022-11-18 18:10:49,707:INFO: 
===> EPOCH: 302 (P4)
2022-11-18 18:10:49,708:INFO: - Computing loss (training)
2022-11-18 18:10:50,251:INFO: Dataset: hotel               Batch: 1/4	Loss 145.8885 (145.8885)
2022-11-18 18:10:50,638:INFO: Dataset: hotel               Batch: 2/4	Loss 146.2493 (146.0676)
2022-11-18 18:10:51,011:INFO: Dataset: hotel               Batch: 3/4	Loss 148.9835 (146.9975)
2022-11-18 18:10:51,339:INFO: Dataset: hotel               Batch: 4/4	Loss 90.5361 (137.0162)
2022-11-18 18:10:52,023:INFO: Dataset: univ                Batch:  1/15	Loss 145.2553 (145.2553)
2022-11-18 18:10:52,455:INFO: Dataset: univ                Batch:  2/15	Loss 146.0065 (145.5979)
2022-11-18 18:10:52,898:INFO: Dataset: univ                Batch:  3/15	Loss 145.3272 (145.5049)
2022-11-18 18:10:53,357:INFO: Dataset: univ                Batch:  4/15	Loss 145.2478 (145.4376)
2022-11-18 18:10:53,792:INFO: Dataset: univ                Batch:  5/15	Loss 146.5879 (145.6522)
2022-11-18 18:10:54,321:INFO: Dataset: univ                Batch:  6/15	Loss 147.1831 (145.9127)
2022-11-18 18:10:54,760:INFO: Dataset: univ                Batch:  7/15	Loss 145.4007 (145.8466)
2022-11-18 18:10:55,196:INFO: Dataset: univ                Batch:  8/15	Loss 144.7177 (145.7114)
2022-11-18 18:10:55,635:INFO: Dataset: univ                Batch:  9/15	Loss 143.9705 (145.5191)
2022-11-18 18:10:56,083:INFO: Dataset: univ                Batch: 10/15	Loss 142.1861 (145.1601)
2022-11-18 18:10:56,532:INFO: Dataset: univ                Batch: 11/15	Loss 143.7411 (145.0282)
2022-11-18 18:10:56,983:INFO: Dataset: univ                Batch: 12/15	Loss 142.7122 (144.8274)
2022-11-18 18:10:57,423:INFO: Dataset: univ                Batch: 13/15	Loss 144.5861 (144.8085)
2022-11-18 18:10:57,860:INFO: Dataset: univ                Batch: 14/15	Loss 144.0098 (144.7547)
2022-11-18 18:10:58,158:INFO: Dataset: univ                Batch: 15/15	Loss 27.1154 (143.4223)
2022-11-18 18:10:58,721:INFO: Dataset: zara1               Batch: 1/8	Loss 158.3985 (158.3985)
2022-11-18 18:10:59,112:INFO: Dataset: zara1               Batch: 2/8	Loss 153.1502 (155.7911)
2022-11-18 18:10:59,559:INFO: Dataset: zara1               Batch: 3/8	Loss 155.0985 (155.5453)
2022-11-18 18:10:59,945:INFO: Dataset: zara1               Batch: 4/8	Loss 156.2219 (155.7090)
2022-11-18 18:11:00,329:INFO: Dataset: zara1               Batch: 5/8	Loss 156.3043 (155.8282)
2022-11-18 18:11:00,730:INFO: Dataset: zara1               Batch: 6/8	Loss 154.2623 (155.5835)
2022-11-18 18:11:01,143:INFO: Dataset: zara1               Batch: 7/8	Loss 156.9948 (155.8179)
2022-11-18 18:11:01,532:INFO: Dataset: zara1               Batch: 8/8	Loss 133.4777 (153.5134)
2022-11-18 18:11:02,247:INFO: Dataset: zara2               Batch:  1/18	Loss 143.7937 (143.7937)
2022-11-18 18:11:02,643:INFO: Dataset: zara2               Batch:  2/18	Loss 147.4810 (145.6214)
2022-11-18 18:11:03,024:INFO: Dataset: zara2               Batch:  3/18	Loss 144.3533 (145.2617)
2022-11-18 18:11:03,412:INFO: Dataset: zara2               Batch:  4/18	Loss 145.6613 (145.3637)
2022-11-18 18:11:03,802:INFO: Dataset: zara2               Batch:  5/18	Loss 144.2410 (145.1240)
2022-11-18 18:11:04,191:INFO: Dataset: zara2               Batch:  6/18	Loss 146.1809 (145.2940)
2022-11-18 18:11:04,575:INFO: Dataset: zara2               Batch:  7/18	Loss 145.3026 (145.2953)
2022-11-18 18:11:04,961:INFO: Dataset: zara2               Batch:  8/18	Loss 142.1368 (144.9162)
2022-11-18 18:11:05,346:INFO: Dataset: zara2               Batch:  9/18	Loss 143.0899 (144.7030)
2022-11-18 18:11:05,725:INFO: Dataset: zara2               Batch: 10/18	Loss 143.7488 (144.6048)
2022-11-18 18:11:06,114:INFO: Dataset: zara2               Batch: 11/18	Loss 144.3725 (144.5831)
2022-11-18 18:11:06,503:INFO: Dataset: zara2               Batch: 12/18	Loss 142.3383 (144.3829)
2022-11-18 18:11:06,889:INFO: Dataset: zara2               Batch: 13/18	Loss 144.4812 (144.3909)
2022-11-18 18:11:07,273:INFO: Dataset: zara2               Batch: 14/18	Loss 142.0553 (144.2241)
2022-11-18 18:11:07,656:INFO: Dataset: zara2               Batch: 15/18	Loss 142.7261 (144.1196)

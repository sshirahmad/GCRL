2022-10-31 15:04:29,438:INFO: Initializing Training Set
2022-10-31 15:04:33,387:INFO: Initializing Validation Set
2022-10-31 15:04:33,820:INFO: Initializing Validation O Set
2022-10-31 15:04:36,421:INFO: 
===> EPOCH: 251 (P3)
2022-10-31 15:04:36,421:INFO: - Computing loss (training)
2022-10-31 15:04:43,339:INFO: Dataset: hotel               Batch: 1/8	Loss -3.8845 (-3.8845)
2022-10-31 15:04:43,809:INFO: Dataset: hotel               Batch: 2/8	Loss -4.7122 (-4.3392)
2022-10-31 15:04:44,185:INFO: Dataset: hotel               Batch: 3/8	Loss -4.0735 (-4.2560)
2022-10-31 15:04:44,585:INFO: Dataset: hotel               Batch: 4/8	Loss -4.5634 (-4.3354)
2022-10-31 15:04:45,013:INFO: Dataset: hotel               Batch: 5/8	Loss -4.8223 (-4.4332)
2022-10-31 15:04:45,278:INFO: Dataset: hotel               Batch: 6/8	Loss -5.6374 (-4.6393)
2022-10-31 15:04:45,537:INFO: Dataset: hotel               Batch: 7/8	Loss -3.9503 (-4.5426)
2022-10-31 15:04:45,717:INFO: Dataset: hotel               Batch: 8/8	Loss -0.7652 (-4.4230)
2022-10-31 15:05:09,934:INFO: Dataset: univ                Batch:  1/29	Loss -2.7696 (-2.7696)
2022-10-31 15:05:10,237:INFO: Dataset: univ                Batch:  2/29	Loss -4.5505 (-3.8310)
2022-10-31 15:05:10,524:INFO: Dataset: univ                Batch:  3/29	Loss -4.3319 (-4.0011)
2022-10-31 15:05:10,800:INFO: Dataset: univ                Batch:  4/29	Loss -3.4153 (-3.8633)
2022-10-31 15:05:11,127:INFO: Dataset: univ                Batch:  5/29	Loss -3.4642 (-3.7824)
2022-10-31 15:05:11,446:INFO: Dataset: univ                Batch:  6/29	Loss -4.2734 (-3.8604)
2022-10-31 15:05:11,751:INFO: Dataset: univ                Batch:  7/29	Loss -3.9635 (-3.8753)
2022-10-31 15:05:12,007:INFO: Dataset: univ                Batch:  8/29	Loss -4.2570 (-3.9207)
2022-10-31 15:05:12,263:INFO: Dataset: univ                Batch:  9/29	Loss -3.0444 (-3.8353)
2022-10-31 15:05:12,527:INFO: Dataset: univ                Batch: 10/29	Loss -4.6460 (-3.9167)
2022-10-31 15:05:12,783:INFO: Dataset: univ                Batch: 11/29	Loss -3.1163 (-3.8397)
2022-10-31 15:05:13,044:INFO: Dataset: univ                Batch: 12/29	Loss -4.8747 (-3.9258)
2022-10-31 15:05:13,299:INFO: Dataset: univ                Batch: 13/29	Loss -4.0461 (-3.9348)
2022-10-31 15:05:13,568:INFO: Dataset: univ                Batch: 14/29	Loss -4.3683 (-3.9697)
2022-10-31 15:05:13,824:INFO: Dataset: univ                Batch: 15/29	Loss -5.0760 (-4.0467)
2022-10-31 15:05:14,086:INFO: Dataset: univ                Batch: 16/29	Loss -4.9241 (-4.1028)
2022-10-31 15:05:14,339:INFO: Dataset: univ                Batch: 17/29	Loss -2.5321 (-4.0194)
2022-10-31 15:05:14,599:INFO: Dataset: univ                Batch: 18/29	Loss -2.5288 (-3.9403)
2022-10-31 15:05:14,864:INFO: Dataset: univ                Batch: 19/29	Loss -5.1348 (-4.0118)
2022-10-31 15:05:15,127:INFO: Dataset: univ                Batch: 20/29	Loss -4.3686 (-4.0287)
2022-10-31 15:05:15,397:INFO: Dataset: univ                Batch: 21/29	Loss -3.5300 (-4.0073)
2022-10-31 15:05:15,659:INFO: Dataset: univ                Batch: 22/29	Loss -2.8043 (-3.9589)
2022-10-31 15:05:15,926:INFO: Dataset: univ                Batch: 23/29	Loss -4.3673 (-3.9763)
2022-10-31 15:05:16,180:INFO: Dataset: univ                Batch: 24/29	Loss -5.1108 (-4.0263)
2022-10-31 15:05:16,432:INFO: Dataset: univ                Batch: 25/29	Loss -4.0660 (-4.0278)
2022-10-31 15:05:16,697:INFO: Dataset: univ                Batch: 26/29	Loss -5.2204 (-4.0743)
2022-10-31 15:05:16,944:INFO: Dataset: univ                Batch: 27/29	Loss -3.6068 (-4.0570)
2022-10-31 15:05:17,209:INFO: Dataset: univ                Batch: 28/29	Loss -2.7652 (-4.0176)
2022-10-31 15:05:17,405:INFO: Dataset: univ                Batch: 29/29	Loss -2.0539 (-3.9912)
2022-10-31 15:05:24,952:INFO: Dataset: zara1               Batch:  1/16	Loss -0.3170 (-0.3170)
2022-10-31 15:05:25,441:INFO: Dataset: zara1               Batch:  2/16	Loss -0.3975 (-0.3587)
2022-10-31 15:05:25,808:INFO: Dataset: zara1               Batch:  3/16	Loss -0.5054 (-0.4082)
2022-10-31 15:05:26,226:INFO: Dataset: zara1               Batch:  4/16	Loss -0.8727 (-0.5393)
2022-10-31 15:05:26,582:INFO: Dataset: zara1               Batch:  5/16	Loss -0.3354 (-0.4955)
2022-10-31 15:05:26,839:INFO: Dataset: zara1               Batch:  6/16	Loss 0.7969 (-0.3093)
2022-10-31 15:05:27,116:INFO: Dataset: zara1               Batch:  7/16	Loss 0.1667 (-0.2392)
2022-10-31 15:05:27,398:INFO: Dataset: zara1               Batch:  8/16	Loss -0.1914 (-0.2330)
2022-10-31 15:05:27,675:INFO: Dataset: zara1               Batch:  9/16	Loss -1.3856 (-0.3678)
2022-10-31 15:05:27,928:INFO: Dataset: zara1               Batch: 10/16	Loss -0.2755 (-0.3579)
2022-10-31 15:05:28,205:INFO: Dataset: zara1               Batch: 11/16	Loss -1.8372 (-0.5008)
2022-10-31 15:05:28,471:INFO: Dataset: zara1               Batch: 12/16	Loss 0.2977 (-0.4422)
2022-10-31 15:05:28,726:INFO: Dataset: zara1               Batch: 13/16	Loss -0.5932 (-0.4548)
2022-10-31 15:05:28,989:INFO: Dataset: zara1               Batch: 14/16	Loss -0.8806 (-0.4845)
2022-10-31 15:05:29,262:INFO: Dataset: zara1               Batch: 15/16	Loss -0.5108 (-0.4863)
2022-10-31 15:05:29,523:INFO: Dataset: zara1               Batch: 16/16	Loss -1.0703 (-0.5164)
2022-10-31 15:05:53,470:INFO: Dataset: zara2               Batch:  1/36	Loss -3.2866 (-3.2866)
2022-10-31 15:05:53,714:INFO: Dataset: zara2               Batch:  2/36	Loss -2.1199 (-2.7012)
2022-10-31 15:05:54,028:INFO: Dataset: zara2               Batch:  3/36	Loss -2.3015 (-2.5576)
2022-10-31 15:05:54,325:INFO: Dataset: zara2               Batch:  4/36	Loss -3.1476 (-2.7003)
2022-10-31 15:05:54,621:INFO: Dataset: zara2               Batch:  5/36	Loss -4.3763 (-3.0781)
2022-10-31 15:05:54,885:INFO: Dataset: zara2               Batch:  6/36	Loss -2.4948 (-2.9760)
2022-10-31 15:05:55,144:INFO: Dataset: zara2               Batch:  7/36	Loss -2.2271 (-2.8591)
2022-10-31 15:05:55,400:INFO: Dataset: zara2               Batch:  8/36	Loss -3.2920 (-2.9137)
2022-10-31 15:05:55,648:INFO: Dataset: zara2               Batch:  9/36	Loss -3.3821 (-2.9670)
2022-10-31 15:05:55,908:INFO: Dataset: zara2               Batch: 10/36	Loss -3.4938 (-3.0132)
2022-10-31 15:05:56,169:INFO: Dataset: zara2               Batch: 11/36	Loss -3.2497 (-3.0345)
2022-10-31 15:05:56,475:INFO: Dataset: zara2               Batch: 12/36	Loss -3.9443 (-3.1126)
2022-10-31 15:05:56,793:INFO: Dataset: zara2               Batch: 13/36	Loss -3.0750 (-3.1096)
2022-10-31 15:05:57,103:INFO: Dataset: zara2               Batch: 14/36	Loss -3.0923 (-3.1082)
2022-10-31 15:05:57,367:INFO: Dataset: zara2               Batch: 15/36	Loss -3.4815 (-3.1336)
2022-10-31 15:05:57,632:INFO: Dataset: zara2               Batch: 16/36	Loss -2.8854 (-3.1166)
2022-10-31 15:05:57,939:INFO: Dataset: zara2               Batch: 17/36	Loss -2.8754 (-3.1007)
2022-10-31 15:05:58,199:INFO: Dataset: zara2               Batch: 18/36	Loss -3.1301 (-3.1023)
2022-10-31 15:05:58,474:INFO: Dataset: zara2               Batch: 19/36	Loss -3.5498 (-3.1282)
2022-10-31 15:05:58,724:INFO: Dataset: zara2               Batch: 20/36	Loss -3.2257 (-3.1341)
2022-10-31 15:05:58,995:INFO: Dataset: zara2               Batch: 21/36	Loss -3.2790 (-3.1421)
2022-10-31 15:05:59,261:INFO: Dataset: zara2               Batch: 22/36	Loss -3.4227 (-3.1539)
2022-10-31 15:05:59,515:INFO: Dataset: zara2               Batch: 23/36	Loss -2.2184 (-3.1130)
2022-10-31 15:05:59,789:INFO: Dataset: zara2               Batch: 24/36	Loss -2.5450 (-3.0907)
2022-10-31 15:06:00,070:INFO: Dataset: zara2               Batch: 25/36	Loss -2.2880 (-3.0518)
2022-10-31 15:06:00,330:INFO: Dataset: zara2               Batch: 26/36	Loss -4.6861 (-3.1194)
2022-10-31 15:06:00,587:INFO: Dataset: zara2               Batch: 27/36	Loss -2.2354 (-3.0831)
2022-10-31 15:06:00,848:INFO: Dataset: zara2               Batch: 28/36	Loss -2.8942 (-3.0761)
2022-10-31 15:06:01,101:INFO: Dataset: zara2               Batch: 29/36	Loss -3.4910 (-3.0921)
2022-10-31 15:06:01,359:INFO: Dataset: zara2               Batch: 30/36	Loss -3.7070 (-3.1161)
2022-10-31 15:06:01,622:INFO: Dataset: zara2               Batch: 31/36	Loss -3.3965 (-3.1266)
2022-10-31 15:06:01,881:INFO: Dataset: zara2               Batch: 32/36	Loss -4.4586 (-3.1696)
2022-10-31 15:06:02,156:INFO: Dataset: zara2               Batch: 33/36	Loss -2.8399 (-3.1598)
2022-10-31 15:06:02,412:INFO: Dataset: zara2               Batch: 34/36	Loss -2.8760 (-3.1507)
2022-10-31 15:06:02,660:INFO: Dataset: zara2               Batch: 35/36	Loss -2.7583 (-3.1384)
2022-10-31 15:06:02,887:INFO: Dataset: zara2               Batch: 36/36	Loss -1.5188 (-3.1050)
2022-10-31 15:06:03,762:INFO: - Computing ADE (validation o)
2022-10-31 15:06:11,882:INFO: 		 ADE on eth                       dataset:	 2.768281936645508
2022-10-31 15:06:11,882:INFO: Average validation o:	ADE  2.7683	FDE  4.7247
2022-10-31 15:06:11,883:INFO: - Computing ADE (validation)
2022-10-31 15:06:20,087:INFO: 		 ADE on hotel                     dataset:	 0.9317723512649536
2022-10-31 15:06:29,227:INFO: 		 ADE on univ                      dataset:	 1.4625113010406494
2022-10-31 15:06:37,467:INFO: 		 ADE on zara1                     dataset:	 2.4859259128570557
2022-10-31 15:06:46,272:INFO: 		 ADE on zara2                     dataset:	 1.3904520273208618
2022-10-31 15:06:46,272:INFO: Average validation:	ADE  1.4665	FDE  2.6854
2022-10-31 15:06:46,274:INFO: - Computing ADE (training)
2022-10-31 15:06:54,649:INFO: 		 ADE on hotel                     dataset:	 1.2791876792907715
2022-10-31 15:07:18,398:INFO: 		 ADE on univ                      dataset:	 1.3588037490844727
2022-10-31 15:07:27,120:INFO: 		 ADE on zara1                     dataset:	 2.4132261276245117
2022-10-31 15:07:51,751:INFO: 		 ADE on zara2                     dataset:	 1.6466763019561768
2022-10-31 15:07:51,751:INFO: Average training:	ADE  1.4824	FDE  2.7182
2022-10-31 15:07:51,769:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_251.pth.tar
2022-10-31 15:07:51,769:INFO: 
===> EPOCH: 252 (P3)
2022-10-31 15:07:51,770:INFO: - Computing loss (training)
2022-10-31 15:07:58,074:INFO: Dataset: hotel               Batch: 1/8	Loss -5.3858 (-5.3858)
2022-10-31 15:07:58,522:INFO: Dataset: hotel               Batch: 2/8	Loss -4.6337 (-5.0133)
2022-10-31 15:07:58,946:INFO: Dataset: hotel               Batch: 3/8	Loss -4.2493 (-4.7524)
2022-10-31 15:07:59,264:INFO: Dataset: hotel               Batch: 4/8	Loss -3.8958 (-4.5432)
2022-10-31 15:07:59,548:INFO: Dataset: hotel               Batch: 5/8	Loss -5.0606 (-4.6463)
2022-10-31 15:07:59,813:INFO: Dataset: hotel               Batch: 6/8	Loss -3.8871 (-4.5241)
2022-10-31 15:08:00,080:INFO: Dataset: hotel               Batch: 7/8	Loss -4.9914 (-4.5823)
2022-10-31 15:08:00,260:INFO: Dataset: hotel               Batch: 8/8	Loss -0.6402 (-4.4419)
2022-10-31 15:08:23,147:INFO: Dataset: univ                Batch:  1/29	Loss -5.2125 (-5.2125)
2022-10-31 15:08:23,411:INFO: Dataset: univ                Batch:  2/29	Loss -4.6710 (-4.9317)
2022-10-31 15:08:23,671:INFO: Dataset: univ                Batch:  3/29	Loss -4.6368 (-4.8365)
2022-10-31 15:08:23,938:INFO: Dataset: univ                Batch:  4/29	Loss -4.7327 (-4.8100)
2022-10-31 15:08:24,203:INFO: Dataset: univ                Batch:  5/29	Loss -2.0865 (-4.3315)
2022-10-31 15:08:24,474:INFO: Dataset: univ                Batch:  6/29	Loss -4.9455 (-4.4422)
2022-10-31 15:08:24,723:INFO: Dataset: univ                Batch:  7/29	Loss -4.0710 (-4.3950)
2022-10-31 15:08:24,977:INFO: Dataset: univ                Batch:  8/29	Loss -2.9211 (-4.2224)
2022-10-31 15:08:25,232:INFO: Dataset: univ                Batch:  9/29	Loss -4.3230 (-4.2350)
2022-10-31 15:08:25,493:INFO: Dataset: univ                Batch: 10/29	Loss -5.2140 (-4.3414)
2022-10-31 15:08:25,769:INFO: Dataset: univ                Batch: 11/29	Loss -3.4538 (-4.2756)
2022-10-31 15:08:26,030:INFO: Dataset: univ                Batch: 12/29	Loss -3.9709 (-4.2520)
2022-10-31 15:08:26,306:INFO: Dataset: univ                Batch: 13/29	Loss -4.9339 (-4.3087)
2022-10-31 15:08:26,584:INFO: Dataset: univ                Batch: 14/29	Loss -3.7878 (-4.2707)
2022-10-31 15:08:26,854:INFO: Dataset: univ                Batch: 15/29	Loss -4.6359 (-4.2930)
2022-10-31 15:08:27,122:INFO: Dataset: univ                Batch: 16/29	Loss -3.9135 (-4.2683)
2022-10-31 15:08:27,398:INFO: Dataset: univ                Batch: 17/29	Loss -2.7091 (-4.1963)
2022-10-31 15:08:27,653:INFO: Dataset: univ                Batch: 18/29	Loss -3.8278 (-4.1771)
2022-10-31 15:08:27,917:INFO: Dataset: univ                Batch: 19/29	Loss -5.0318 (-4.2186)
2022-10-31 15:08:28,170:INFO: Dataset: univ                Batch: 20/29	Loss -3.8647 (-4.2015)
2022-10-31 15:08:28,430:INFO: Dataset: univ                Batch: 21/29	Loss -4.2429 (-4.2033)
2022-10-31 15:08:28,700:INFO: Dataset: univ                Batch: 22/29	Loss -2.7759 (-4.1490)
2022-10-31 15:08:28,985:INFO: Dataset: univ                Batch: 23/29	Loss -5.3903 (-4.2115)
2022-10-31 15:08:29,290:INFO: Dataset: univ                Batch: 24/29	Loss -4.9068 (-4.2443)
2022-10-31 15:08:29,595:INFO: Dataset: univ                Batch: 25/29	Loss -3.0769 (-4.2033)
2022-10-31 15:08:30,056:INFO: Dataset: univ                Batch: 26/29	Loss -5.2196 (-4.2483)
2022-10-31 15:08:30,473:INFO: Dataset: univ                Batch: 27/29	Loss -3.8361 (-4.2338)
2022-10-31 15:08:30,877:INFO: Dataset: univ                Batch: 28/29	Loss -4.4487 (-4.2412)
2022-10-31 15:08:31,085:INFO: Dataset: univ                Batch: 29/29	Loss -1.3725 (-4.2056)
2022-10-31 15:08:39,063:INFO: Dataset: zara1               Batch:  1/16	Loss -2.2064 (-2.2064)
2022-10-31 15:08:39,580:INFO: Dataset: zara1               Batch:  2/16	Loss -1.9536 (-2.0945)
2022-10-31 15:08:39,996:INFO: Dataset: zara1               Batch:  3/16	Loss -0.3842 (-1.5823)
2022-10-31 15:08:40,395:INFO: Dataset: zara1               Batch:  4/16	Loss -1.6949 (-1.6126)
2022-10-31 15:08:40,805:INFO: Dataset: zara1               Batch:  5/16	Loss 0.5633 (-1.2250)
2022-10-31 15:08:41,181:INFO: Dataset: zara1               Batch:  6/16	Loss 0.2538 (-1.0134)
2022-10-31 15:08:41,568:INFO: Dataset: zara1               Batch:  7/16	Loss -1.1811 (-1.0390)
2022-10-31 15:08:41,939:INFO: Dataset: zara1               Batch:  8/16	Loss -1.1352 (-1.0507)
2022-10-31 15:08:42,320:INFO: Dataset: zara1               Batch:  9/16	Loss -1.6209 (-1.1032)
2022-10-31 15:08:42,702:INFO: Dataset: zara1               Batch: 10/16	Loss -1.1187 (-1.1050)
2022-10-31 15:08:43,073:INFO: Dataset: zara1               Batch: 11/16	Loss -0.4563 (-1.0496)
2022-10-31 15:08:43,478:INFO: Dataset: zara1               Batch: 12/16	Loss -0.7084 (-1.0199)
2022-10-31 15:08:43,844:INFO: Dataset: zara1               Batch: 13/16	Loss -0.2493 (-0.9664)
2022-10-31 15:08:44,198:INFO: Dataset: zara1               Batch: 14/16	Loss -7.1914 (-1.4187)
2022-10-31 15:08:44,562:INFO: Dataset: zara1               Batch: 15/16	Loss 1.5812 (-1.1926)
2022-10-31 15:08:44,911:INFO: Dataset: zara1               Batch: 16/16	Loss 5.1952 (-0.9606)
2022-10-31 15:09:09,560:INFO: Dataset: zara2               Batch:  1/36	Loss -2.8672 (-2.8672)
2022-10-31 15:09:09,837:INFO: Dataset: zara2               Batch:  2/36	Loss -4.5368 (-3.6625)
2022-10-31 15:09:10,095:INFO: Dataset: zara2               Batch:  3/36	Loss -2.7975 (-3.3641)
2022-10-31 15:09:10,364:INFO: Dataset: zara2               Batch:  4/36	Loss -3.2081 (-3.3296)
2022-10-31 15:09:10,699:INFO: Dataset: zara2               Batch:  5/36	Loss -3.4642 (-3.3549)
2022-10-31 15:09:11,002:INFO: Dataset: zara2               Batch:  6/36	Loss -4.7490 (-3.5558)
2022-10-31 15:09:11,437:INFO: Dataset: zara2               Batch:  7/36	Loss -4.2756 (-3.6765)
2022-10-31 15:09:11,865:INFO: Dataset: zara2               Batch:  8/36	Loss -3.4715 (-3.6489)
2022-10-31 15:09:12,189:INFO: Dataset: zara2               Batch:  9/36	Loss -2.8605 (-3.5630)
2022-10-31 15:09:12,446:INFO: Dataset: zara2               Batch: 10/36	Loss -2.9719 (-3.5135)
2022-10-31 15:09:12,708:INFO: Dataset: zara2               Batch: 11/36	Loss -2.3497 (-3.4056)
2022-10-31 15:09:12,979:INFO: Dataset: zara2               Batch: 12/36	Loss -3.6072 (-3.4217)
2022-10-31 15:09:13,255:INFO: Dataset: zara2               Batch: 13/36	Loss -3.2346 (-3.4084)
2022-10-31 15:09:13,544:INFO: Dataset: zara2               Batch: 14/36	Loss -3.1103 (-3.3871)
2022-10-31 15:09:13,806:INFO: Dataset: zara2               Batch: 15/36	Loss -2.6414 (-3.3319)
2022-10-31 15:09:14,067:INFO: Dataset: zara2               Batch: 16/36	Loss -3.9676 (-3.3670)
2022-10-31 15:09:14,362:INFO: Dataset: zara2               Batch: 17/36	Loss -3.0306 (-3.3481)
2022-10-31 15:09:14,668:INFO: Dataset: zara2               Batch: 18/36	Loss -2.4294 (-3.2921)
2022-10-31 15:09:14,957:INFO: Dataset: zara2               Batch: 19/36	Loss -3.5106 (-3.3032)
2022-10-31 15:09:15,238:INFO: Dataset: zara2               Batch: 20/36	Loss -3.7123 (-3.3236)
2022-10-31 15:09:15,622:INFO: Dataset: zara2               Batch: 21/36	Loss -2.9167 (-3.3082)
2022-10-31 15:09:15,967:INFO: Dataset: zara2               Batch: 22/36	Loss -4.1436 (-3.3457)
2022-10-31 15:09:16,319:INFO: Dataset: zara2               Batch: 23/36	Loss -3.4547 (-3.3501)
2022-10-31 15:09:16,675:INFO: Dataset: zara2               Batch: 24/36	Loss -3.5942 (-3.3614)
2022-10-31 15:09:17,016:INFO: Dataset: zara2               Batch: 25/36	Loss -3.8950 (-3.3847)
2022-10-31 15:09:17,349:INFO: Dataset: zara2               Batch: 26/36	Loss -3.6053 (-3.3929)
2022-10-31 15:09:17,671:INFO: Dataset: zara2               Batch: 27/36	Loss -3.9918 (-3.4141)
2022-10-31 15:09:18,003:INFO: Dataset: zara2               Batch: 28/36	Loss -3.3801 (-3.4129)
2022-10-31 15:09:18,346:INFO: Dataset: zara2               Batch: 29/36	Loss -3.7478 (-3.4246)
2022-10-31 15:09:18,682:INFO: Dataset: zara2               Batch: 30/36	Loss -2.5449 (-3.3981)
2022-10-31 15:09:19,031:INFO: Dataset: zara2               Batch: 31/36	Loss -3.5892 (-3.4044)
2022-10-31 15:09:19,366:INFO: Dataset: zara2               Batch: 32/36	Loss -3.4270 (-3.4051)
2022-10-31 15:09:19,740:INFO: Dataset: zara2               Batch: 33/36	Loss -4.1151 (-3.4266)
2022-10-31 15:09:20,095:INFO: Dataset: zara2               Batch: 34/36	Loss -3.6676 (-3.4332)
2022-10-31 15:09:20,428:INFO: Dataset: zara2               Batch: 35/36	Loss -3.4008 (-3.4322)
2022-10-31 15:09:20,763:INFO: Dataset: zara2               Batch: 36/36	Loss -3.5980 (-3.4356)
2022-10-31 15:09:21,776:INFO: - Computing ADE (validation o)
2022-10-31 15:09:30,974:INFO: 		 ADE on eth                       dataset:	 2.6707937717437744
2022-10-31 15:09:30,974:INFO: Average validation o:	ADE  2.6708	FDE  4.5647
2022-10-31 15:09:30,976:INFO: - Computing ADE (validation)
2022-10-31 15:09:39,904:INFO: 		 ADE on hotel                     dataset:	 1.201541781425476
2022-10-31 15:09:48,144:INFO: 		 ADE on univ                      dataset:	 1.5024216175079346
2022-10-31 15:09:56,216:INFO: 		 ADE on zara1                     dataset:	 2.230506420135498
2022-10-31 15:10:04,675:INFO: 		 ADE on zara2                     dataset:	 1.4554238319396973
2022-10-31 15:10:04,675:INFO: Average validation:	ADE  1.5110	FDE  2.7630
2022-10-31 15:10:04,676:INFO: - Computing ADE (training)
2022-10-31 15:10:13,078:INFO: 		 ADE on hotel                     dataset:	 1.492745041847229
2022-10-31 15:10:38,996:INFO: 		 ADE on univ                      dataset:	 1.405929684638977
2022-10-31 15:10:47,763:INFO: 		 ADE on zara1                     dataset:	 2.235630750656128
2022-10-31 15:11:11,560:INFO: 		 ADE on zara2                     dataset:	 1.6736645698547363
2022-10-31 15:11:11,560:INFO: Average training:	ADE  1.5154	FDE  2.7705
2022-10-31 15:11:11,574:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_252.pth.tar
2022-10-31 15:11:11,574:INFO: 
===> EPOCH: 253 (P3)
2022-10-31 15:11:11,575:INFO: - Computing loss (training)
2022-10-31 15:11:17,890:INFO: Dataset: hotel               Batch: 1/8	Loss -3.8803 (-3.8803)
2022-10-31 15:11:18,370:INFO: Dataset: hotel               Batch: 2/8	Loss -4.6411 (-4.2571)
2022-10-31 15:11:18,792:INFO: Dataset: hotel               Batch: 3/8	Loss -4.5191 (-4.3422)
2022-10-31 15:11:19,132:INFO: Dataset: hotel               Batch: 4/8	Loss -3.7135 (-4.1974)
2022-10-31 15:11:19,423:INFO: Dataset: hotel               Batch: 5/8	Loss -5.4846 (-4.4804)
2022-10-31 15:11:19,689:INFO: Dataset: hotel               Batch: 6/8	Loss -5.4390 (-4.6407)
2022-10-31 15:11:19,960:INFO: Dataset: hotel               Batch: 7/8	Loss -4.2139 (-4.5781)
2022-10-31 15:11:20,129:INFO: Dataset: hotel               Batch: 8/8	Loss -1.2211 (-4.4806)
2022-10-31 15:11:43,241:INFO: Dataset: univ                Batch:  1/29	Loss -5.3435 (-5.3435)
2022-10-31 15:11:43,507:INFO: Dataset: univ                Batch:  2/29	Loss -4.7649 (-5.0475)
2022-10-31 15:11:43,785:INFO: Dataset: univ                Batch:  3/29	Loss -4.2095 (-4.7794)
2022-10-31 15:11:44,059:INFO: Dataset: univ                Batch:  4/29	Loss -4.4028 (-4.6872)
2022-10-31 15:11:44,337:INFO: Dataset: univ                Batch:  5/29	Loss 2.7294 (-3.4631)
2022-10-31 15:11:44,595:INFO: Dataset: univ                Batch:  6/29	Loss -5.2796 (-3.8092)
2022-10-31 15:11:44,858:INFO: Dataset: univ                Batch:  7/29	Loss -4.4358 (-3.9058)
2022-10-31 15:11:45,131:INFO: Dataset: univ                Batch:  8/29	Loss -4.3466 (-3.9659)
2022-10-31 15:11:45,451:INFO: Dataset: univ                Batch:  9/29	Loss -5.2724 (-4.1374)
2022-10-31 15:11:45,726:INFO: Dataset: univ                Batch: 10/29	Loss -4.6633 (-4.1895)
2022-10-31 15:11:46,002:INFO: Dataset: univ                Batch: 11/29	Loss -4.0661 (-4.1795)
2022-10-31 15:11:46,277:INFO: Dataset: univ                Batch: 12/29	Loss -2.8018 (-4.0843)
2022-10-31 15:11:46,548:INFO: Dataset: univ                Batch: 13/29	Loss -3.5189 (-4.0457)
2022-10-31 15:11:46,814:INFO: Dataset: univ                Batch: 14/29	Loss -4.5010 (-4.0788)
2022-10-31 15:11:47,076:INFO: Dataset: univ                Batch: 15/29	Loss -5.6268 (-4.1894)
2022-10-31 15:11:47,332:INFO: Dataset: univ                Batch: 16/29	Loss -4.0794 (-4.1822)
2022-10-31 15:11:47,634:INFO: Dataset: univ                Batch: 17/29	Loss -4.0410 (-4.1745)
2022-10-31 15:11:47,904:INFO: Dataset: univ                Batch: 18/29	Loss -2.7678 (-4.1066)
2022-10-31 15:11:48,182:INFO: Dataset: univ                Batch: 19/29	Loss -5.5827 (-4.1977)
2022-10-31 15:11:48,465:INFO: Dataset: univ                Batch: 20/29	Loss -3.6526 (-4.1707)
2022-10-31 15:11:48,732:INFO: Dataset: univ                Batch: 21/29	Loss -5.7729 (-4.2610)
2022-10-31 15:11:49,002:INFO: Dataset: univ                Batch: 22/29	Loss -3.6138 (-4.2353)
2022-10-31 15:11:49,254:INFO: Dataset: univ                Batch: 23/29	Loss -4.6888 (-4.2547)
2022-10-31 15:11:49,539:INFO: Dataset: univ                Batch: 24/29	Loss -4.9990 (-4.2868)
2022-10-31 15:11:49,804:INFO: Dataset: univ                Batch: 25/29	Loss -3.3691 (-4.2532)
2022-10-31 15:11:50,078:INFO: Dataset: univ                Batch: 26/29	Loss -3.5734 (-4.2262)
2022-10-31 15:11:50,332:INFO: Dataset: univ                Batch: 27/29	Loss -5.9580 (-4.3005)
2022-10-31 15:11:50,615:INFO: Dataset: univ                Batch: 28/29	Loss -4.5556 (-4.3113)
2022-10-31 15:11:50,819:INFO: Dataset: univ                Batch: 29/29	Loss -1.2080 (-4.2738)
2022-10-31 15:11:58,742:INFO: Dataset: zara1               Batch:  1/16	Loss -1.3319 (-1.3319)
2022-10-31 15:11:59,205:INFO: Dataset: zara1               Batch:  2/16	Loss -1.2508 (-1.2918)
2022-10-31 15:11:59,584:INFO: Dataset: zara1               Batch:  3/16	Loss -1.8873 (-1.5005)
2022-10-31 15:11:59,898:INFO: Dataset: zara1               Batch:  4/16	Loss -1.6365 (-1.5337)
2022-10-31 15:12:00,195:INFO: Dataset: zara1               Batch:  5/16	Loss -0.3463 (-1.3223)
2022-10-31 15:12:00,455:INFO: Dataset: zara1               Batch:  6/16	Loss -1.8450 (-1.4041)
2022-10-31 15:12:00,708:INFO: Dataset: zara1               Batch:  7/16	Loss -1.0218 (-1.3516)
2022-10-31 15:12:00,968:INFO: Dataset: zara1               Batch:  8/16	Loss 0.1917 (-1.1666)
2022-10-31 15:12:01,225:INFO: Dataset: zara1               Batch:  9/16	Loss -2.9698 (-1.3498)
2022-10-31 15:12:01,480:INFO: Dataset: zara1               Batch: 10/16	Loss -0.6289 (-1.2761)
2022-10-31 15:12:01,756:INFO: Dataset: zara1               Batch: 11/16	Loss -1.1864 (-1.2677)
2022-10-31 15:12:02,009:INFO: Dataset: zara1               Batch: 12/16	Loss -1.2586 (-1.2671)
2022-10-31 15:12:02,273:INFO: Dataset: zara1               Batch: 13/16	Loss -0.1631 (-1.1864)
2022-10-31 15:12:02,521:INFO: Dataset: zara1               Batch: 14/16	Loss -1.3503 (-1.1967)
2022-10-31 15:12:02,787:INFO: Dataset: zara1               Batch: 15/16	Loss 0.0978 (-1.1391)
2022-10-31 15:12:03,024:INFO: Dataset: zara1               Batch: 16/16	Loss -0.5519 (-1.1141)
2022-10-31 15:12:27,562:INFO: Dataset: zara2               Batch:  1/36	Loss -4.1669 (-4.1669)
2022-10-31 15:12:27,861:INFO: Dataset: zara2               Batch:  2/36	Loss -4.3077 (-4.2395)
2022-10-31 15:12:28,120:INFO: Dataset: zara2               Batch:  3/36	Loss -3.7444 (-4.0811)
2022-10-31 15:12:28,403:INFO: Dataset: zara2               Batch:  4/36	Loss -3.7837 (-4.0097)
2022-10-31 15:12:28,671:INFO: Dataset: zara2               Batch:  5/36	Loss -2.3358 (-3.7116)
2022-10-31 15:12:28,932:INFO: Dataset: zara2               Batch:  6/36	Loss -3.9368 (-3.7470)
2022-10-31 15:12:29,188:INFO: Dataset: zara2               Batch:  7/36	Loss -3.7742 (-3.7517)
2022-10-31 15:12:29,456:INFO: Dataset: zara2               Batch:  8/36	Loss -2.5502 (-3.6181)
2022-10-31 15:12:29,719:INFO: Dataset: zara2               Batch:  9/36	Loss -2.9823 (-3.5525)
2022-10-31 15:12:29,967:INFO: Dataset: zara2               Batch: 10/36	Loss -5.6671 (-3.7709)
2022-10-31 15:12:30,230:INFO: Dataset: zara2               Batch: 11/36	Loss -3.9270 (-3.7862)
2022-10-31 15:12:30,493:INFO: Dataset: zara2               Batch: 12/36	Loss -4.1536 (-3.8206)
2022-10-31 15:12:30,764:INFO: Dataset: zara2               Batch: 13/36	Loss -3.3264 (-3.7832)
2022-10-31 15:12:31,022:INFO: Dataset: zara2               Batch: 14/36	Loss -4.3613 (-3.8315)
2022-10-31 15:12:31,274:INFO: Dataset: zara2               Batch: 15/36	Loss -3.2974 (-3.7984)
2022-10-31 15:12:31,531:INFO: Dataset: zara2               Batch: 16/36	Loss -3.6550 (-3.7892)
2022-10-31 15:12:31,789:INFO: Dataset: zara2               Batch: 17/36	Loss -4.3728 (-3.8242)
2022-10-31 15:12:32,049:INFO: Dataset: zara2               Batch: 18/36	Loss -2.5698 (-3.7687)
2022-10-31 15:12:32,298:INFO: Dataset: zara2               Batch: 19/36	Loss -4.3298 (-3.7986)
2022-10-31 15:12:32,561:INFO: Dataset: zara2               Batch: 20/36	Loss -3.2473 (-3.7698)
2022-10-31 15:12:32,832:INFO: Dataset: zara2               Batch: 21/36	Loss -2.0160 (-3.6952)
2022-10-31 15:12:33,099:INFO: Dataset: zara2               Batch: 22/36	Loss -3.7494 (-3.6979)
2022-10-31 15:12:33,359:INFO: Dataset: zara2               Batch: 23/36	Loss -3.6686 (-3.6966)
2022-10-31 15:12:33,629:INFO: Dataset: zara2               Batch: 24/36	Loss -4.4086 (-3.7290)
2022-10-31 15:12:33,887:INFO: Dataset: zara2               Batch: 25/36	Loss -2.7500 (-3.6893)
2022-10-31 15:12:34,138:INFO: Dataset: zara2               Batch: 26/36	Loss -3.8119 (-3.6934)
2022-10-31 15:12:34,423:INFO: Dataset: zara2               Batch: 27/36	Loss -3.7739 (-3.6965)
2022-10-31 15:12:34,690:INFO: Dataset: zara2               Batch: 28/36	Loss -3.3313 (-3.6849)
2022-10-31 15:12:34,951:INFO: Dataset: zara2               Batch: 29/36	Loss -3.7741 (-3.6878)
2022-10-31 15:12:35,201:INFO: Dataset: zara2               Batch: 30/36	Loss -2.1450 (-3.6423)
2022-10-31 15:12:35,481:INFO: Dataset: zara2               Batch: 31/36	Loss -3.7440 (-3.6458)
2022-10-31 15:12:35,739:INFO: Dataset: zara2               Batch: 32/36	Loss -3.3788 (-3.6372)
2022-10-31 15:12:35,992:INFO: Dataset: zara2               Batch: 33/36	Loss -4.0549 (-3.6499)
2022-10-31 15:12:36,282:INFO: Dataset: zara2               Batch: 34/36	Loss -3.4789 (-3.6449)
2022-10-31 15:12:36,542:INFO: Dataset: zara2               Batch: 35/36	Loss -3.6052 (-3.6437)
2022-10-31 15:12:36,766:INFO: Dataset: zara2               Batch: 36/36	Loss -1.9323 (-3.6067)
2022-10-31 15:12:37,636:INFO: - Computing ADE (validation o)
2022-10-31 15:12:45,735:INFO: 		 ADE on eth                       dataset:	 2.7533535957336426
2022-10-31 15:12:45,735:INFO: Average validation o:	ADE  2.7534	FDE  4.7905
2022-10-31 15:12:45,736:INFO: - Computing ADE (validation)
2022-10-31 15:12:53,808:INFO: 		 ADE on hotel                     dataset:	 1.2706561088562012
2022-10-31 15:13:02,062:INFO: 		 ADE on univ                      dataset:	 1.4612653255462646
2022-10-31 15:13:10,154:INFO: 		 ADE on zara1                     dataset:	 1.9782599210739136
2022-10-31 15:13:18,486:INFO: 		 ADE on zara2                     dataset:	 1.4419351816177368
2022-10-31 15:13:18,486:INFO: Average validation:	ADE  1.4738	FDE  2.7061
2022-10-31 15:13:18,487:INFO: - Computing ADE (training)
2022-10-31 15:13:26,908:INFO: 		 ADE on hotel                     dataset:	 1.5149390697479248
2022-10-31 15:13:50,762:INFO: 		 ADE on univ                      dataset:	 1.3791228532791138
2022-10-31 15:13:59,300:INFO: 		 ADE on zara1                     dataset:	 2.1779732704162598
2022-10-31 15:14:24,140:INFO: 		 ADE on zara2                     dataset:	 1.6842951774597168
2022-10-31 15:14:24,140:INFO: Average training:	ADE  1.4954	FDE  2.7585
2022-10-31 15:14:24,157:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 200)_shuffle_true_seed_72/pretrain/P3/CRMF_epoch_253.pth.tar
2022-10-31 15:14:24,157:INFO: 
===> EPOCH: 254 (P3)
2022-10-31 15:14:24,158:INFO: - Computing loss (training)

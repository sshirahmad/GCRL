2022-11-11 21:04:35,169:INFO: Initializing Training Set
2022-11-11 21:04:39,455:INFO: Initializing Validation Set
2022-11-11 21:04:39,943:INFO: Initializing Validation O Set
2022-11-11 21:04:43,255:INFO: 
===> EPOCH: 1 (P1)
2022-11-11 21:04:43,255:INFO: - Computing loss (training)
2022-11-11 21:04:50,184:INFO: Dataset: hotel               Batch: 1/4	Loss 4.0682 (4.0682)
2022-11-11 21:04:50,546:INFO: Dataset: hotel               Batch: 2/4	Loss 4.2428 (4.1524)
2022-11-11 21:04:50,920:INFO: Dataset: hotel               Batch: 3/4	Loss 6.2583 (4.8310)
2022-11-11 21:04:51,250:INFO: Dataset: hotel               Batch: 4/4	Loss 4.9760 (4.8555)
2022-11-11 21:05:15,928:INFO: Dataset: univ                Batch:  1/15	Loss 3.6033 (3.6033)
2022-11-11 21:05:15,976:INFO: Dataset: univ                Batch:  2/15	Loss 3.8669 (3.7057)
2022-11-11 21:05:16,035:INFO: Dataset: univ                Batch:  3/15	Loss 3.2308 (3.5415)
2022-11-11 21:05:16,089:INFO: Dataset: univ                Batch:  4/15	Loss 2.8464 (3.3670)
2022-11-11 21:05:16,160:INFO: Dataset: univ                Batch:  5/15	Loss 3.1048 (3.3115)
2022-11-11 21:05:16,217:INFO: Dataset: univ                Batch:  6/15	Loss 5.2885 (3.5607)
2022-11-11 21:05:16,264:INFO: Dataset: univ                Batch:  7/15	Loss 5.5213 (3.7776)
2022-11-11 21:05:16,310:INFO: Dataset: univ                Batch:  8/15	Loss 4.6183 (3.8491)
2022-11-11 21:05:16,352:INFO: Dataset: univ                Batch:  9/15	Loss 5.7803 (4.0278)
2022-11-11 21:05:16,409:INFO: Dataset: univ                Batch: 10/15	Loss 4.3241 (4.0517)
2022-11-11 21:05:16,446:INFO: Dataset: univ                Batch: 11/15	Loss 3.5742 (4.0231)
2022-11-11 21:05:16,478:INFO: Dataset: univ                Batch: 12/15	Loss 9.2680 (4.1999)
2022-11-11 21:05:16,513:INFO: Dataset: univ                Batch: 13/15	Loss 11.4519 (4.2624)
2022-11-11 21:05:16,546:INFO: Dataset: univ                Batch: 14/15	Loss 13.9649 (4.3406)
2022-11-11 21:05:16,559:INFO: Dataset: univ                Batch: 15/15	Loss 3.2368 (4.3394)
2022-11-11 21:05:23,660:INFO: Dataset: zara1               Batch: 1/8	Loss 6.9548 (6.9548)
2022-11-11 21:05:24,077:INFO: Dataset: zara1               Batch: 2/8	Loss 7.4993 (7.2075)
2022-11-11 21:05:24,393:INFO: Dataset: zara1               Batch: 3/8	Loss 6.2580 (6.9304)
2022-11-11 21:05:24,725:INFO: Dataset: zara1               Batch: 4/8	Loss 12.8825 (8.1073)
2022-11-11 21:05:24,939:INFO: Dataset: zara1               Batch: 5/8	Loss 8.7949 (8.2305)
2022-11-11 21:05:25,167:INFO: Dataset: zara1               Batch: 6/8	Loss 8.4955 (8.2899)
2022-11-11 21:05:25,201:INFO: Dataset: zara1               Batch: 7/8	Loss 9.7672 (8.5712)
2022-11-11 21:05:25,234:INFO: Dataset: zara1               Batch: 8/8	Loss 10.6020 (8.7775)
2022-11-11 21:05:48,017:INFO: Dataset: zara2               Batch:  1/18	Loss 10.7774 (10.7774)
2022-11-11 21:05:48,052:INFO: Dataset: zara2               Batch:  2/18	Loss 8.1757 (9.2853)
2022-11-11 21:05:48,089:INFO: Dataset: zara2               Batch:  3/18	Loss 7.3497 (8.5942)
2022-11-11 21:05:48,123:INFO: Dataset: zara2               Batch:  4/18	Loss 7.6209 (8.3918)
2022-11-11 21:05:48,163:INFO: Dataset: zara2               Batch:  5/18	Loss 5.3286 (7.8860)
2022-11-11 21:05:48,199:INFO: Dataset: zara2               Batch:  6/18	Loss 4.2587 (6.8588)
2022-11-11 21:05:48,238:INFO: Dataset: zara2               Batch:  7/18	Loss 3.4473 (6.1174)
2022-11-11 21:05:48,292:INFO: Dataset: zara2               Batch:  8/18	Loss 3.5479 (5.6350)
2022-11-11 21:05:48,329:INFO: Dataset: zara2               Batch:  9/18	Loss 2.5617 (5.2669)
2022-11-11 21:05:48,360:INFO: Dataset: zara2               Batch: 10/18	Loss 3.5893 (5.0285)
2022-11-11 21:05:48,392:INFO: Dataset: zara2               Batch: 11/18	Loss 4.2045 (4.9187)
2022-11-11 21:05:48,423:INFO: Dataset: zara2               Batch: 12/18	Loss 9.3997 (5.1607)
2022-11-11 21:05:48,459:INFO: Dataset: zara2               Batch: 13/18	Loss 6.8881 (5.2335)
2022-11-11 21:05:48,515:INFO: Dataset: zara2               Batch: 14/18	Loss 3.7096 (5.1457)
2022-11-11 21:05:48,552:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7552 (5.2395)
2022-11-11 21:05:48,584:INFO: Dataset: zara2               Batch: 16/18	Loss 7.5338 (5.3422)
2022-11-11 21:05:48,615:INFO: Dataset: zara2               Batch: 17/18	Loss 8.7026 (5.4691)
2022-11-11 21:05:48,642:INFO: Dataset: zara2               Batch: 18/18	Loss 5.4887 (5.4696)
2022-11-11 21:05:49,551:INFO:  --> Model Saved in ./models/E6//P1/CRMF_epoch_1.pth.tar
2022-11-11 21:05:49,551:INFO: 
===> EPOCH: 2 (P1)
2022-11-11 21:05:49,552:INFO: - Computing loss (training)
2022-11-11 21:05:55,682:INFO: Dataset: hotel               Batch: 1/4	Loss 3.5500 (3.5500)
2022-11-11 21:05:56,043:INFO: Dataset: hotel               Batch: 2/4	Loss 3.9641 (3.7498)
2022-11-11 21:05:56,407:INFO: Dataset: hotel               Batch: 3/4	Loss 5.9294 (4.4521)
2022-11-11 21:05:56,754:INFO: Dataset: hotel               Batch: 4/4	Loss 4.5372 (4.4665)
2022-11-11 21:06:19,832:INFO: Dataset: univ                Batch:  1/15	Loss 2.9493 (2.9493)
2022-11-11 21:06:19,895:INFO: Dataset: univ                Batch:  2/15	Loss 3.1571 (3.0300)
2022-11-11 21:06:19,948:INFO: Dataset: univ                Batch:  3/15	Loss 2.7273 (2.9254)
2022-11-11 21:06:19,996:INFO: Dataset: univ                Batch:  4/15	Loss 2.3830 (2.7892)
2022-11-11 21:06:20,050:INFO: Dataset: univ                Batch:  5/15	Loss 2.5857 (2.7461)
2022-11-11 21:06:20,124:INFO: Dataset: univ                Batch:  6/15	Loss 4.1676 (2.9253)
2022-11-11 21:06:20,167:INFO: Dataset: univ                Batch:  7/15	Loss 4.4177 (3.0904)
2022-11-11 21:06:20,211:INFO: Dataset: univ                Batch:  8/15	Loss 3.7188 (3.1438)
2022-11-11 21:06:20,256:INFO: Dataset: univ                Batch:  9/15	Loss 4.5290 (3.2720)
2022-11-11 21:06:20,307:INFO: Dataset: univ                Batch: 10/15	Loss 3.4764 (3.2885)
2022-11-11 21:06:20,352:INFO: Dataset: univ                Batch: 11/15	Loss 2.8667 (3.2632)
2022-11-11 21:06:20,383:INFO: Dataset: univ                Batch: 12/15	Loss 7.1161 (3.3931)
2022-11-11 21:06:20,415:INFO: Dataset: univ                Batch: 13/15	Loss 8.5268 (3.4373)
2022-11-11 21:06:20,449:INFO: Dataset: univ                Batch: 14/15	Loss 10.0406 (3.4906)
2022-11-11 21:06:20,460:INFO: Dataset: univ                Batch: 15/15	Loss 2.1952 (3.4891)
2022-11-11 21:06:27,438:INFO: Dataset: zara1               Batch: 1/8	Loss 5.0777 (5.0777)
2022-11-11 21:06:27,828:INFO: Dataset: zara1               Batch: 2/8	Loss 5.6944 (5.3638)
2022-11-11 21:06:28,154:INFO: Dataset: zara1               Batch: 3/8	Loss 4.5177 (5.1169)
2022-11-11 21:06:28,520:INFO: Dataset: zara1               Batch: 4/8	Loss 9.0070 (5.8861)
2022-11-11 21:06:28,752:INFO: Dataset: zara1               Batch: 5/8	Loss 6.6577 (6.0243)
2022-11-11 21:06:28,926:INFO: Dataset: zara1               Batch: 6/8	Loss 6.6509 (6.1649)
2022-11-11 21:06:28,962:INFO: Dataset: zara1               Batch: 7/8	Loss 6.5928 (6.2463)
2022-11-11 21:06:29,013:INFO: Dataset: zara1               Batch: 8/8	Loss 7.4915 (6.3728)
2022-11-11 21:06:51,496:INFO: Dataset: zara2               Batch:  1/18	Loss 7.0247 (7.0247)
2022-11-11 21:06:51,533:INFO: Dataset: zara2               Batch:  2/18	Loss 5.7334 (6.2842)
2022-11-11 21:06:51,579:INFO: Dataset: zara2               Batch:  3/18	Loss 5.2019 (5.8977)
2022-11-11 21:06:51,619:INFO: Dataset: zara2               Batch:  4/18	Loss 5.3280 (5.7792)
2022-11-11 21:06:51,660:INFO: Dataset: zara2               Batch:  5/18	Loss 4.1377 (5.5082)
2022-11-11 21:06:51,703:INFO: Dataset: zara2               Batch:  6/18	Loss 3.2717 (4.8748)
2022-11-11 21:06:51,785:INFO: Dataset: zara2               Batch:  7/18	Loss 2.8346 (4.4315)
2022-11-11 21:06:51,825:INFO: Dataset: zara2               Batch:  8/18	Loss 2.8398 (4.1326)
2022-11-11 21:06:51,860:INFO: Dataset: zara2               Batch:  9/18	Loss 2.2905 (3.9120)
2022-11-11 21:06:51,902:INFO: Dataset: zara2               Batch: 10/18	Loss 3.0210 (3.7854)
2022-11-11 21:06:51,966:INFO: Dataset: zara2               Batch: 11/18	Loss 3.3513 (3.7276)
2022-11-11 21:06:52,000:INFO: Dataset: zara2               Batch: 12/18	Loss 6.4107 (3.8724)
2022-11-11 21:06:52,033:INFO: Dataset: zara2               Batch: 13/18	Loss 4.9160 (3.9164)
2022-11-11 21:06:52,065:INFO: Dataset: zara2               Batch: 14/18	Loss 2.8831 (3.8569)
2022-11-11 21:06:52,096:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6543 (3.9034)
2022-11-11 21:06:52,128:INFO: Dataset: zara2               Batch: 16/18	Loss 5.0199 (3.9533)
2022-11-11 21:06:52,162:INFO: Dataset: zara2               Batch: 17/18	Loss 5.7418 (4.0209)
2022-11-11 21:06:52,204:INFO: Dataset: zara2               Batch: 18/18	Loss 4.1291 (4.0239)
2022-11-11 21:06:53,106:INFO:  --> Model Saved in ./models/E6//P1/CRMF_epoch_2.pth.tar
2022-11-11 21:06:53,107:INFO: 
===> EPOCH: 3 (P2)
2022-11-11 21:06:53,107:INFO: - Computing loss (training)
2022-11-11 21:06:59,529:INFO: Dataset: hotel               Batch: 1/4	Loss 4.9648 (4.9648)
2022-11-11 21:06:59,887:INFO: Dataset: hotel               Batch: 2/4	Loss 5.1206 (5.0400)
2022-11-11 21:07:00,284:INFO: Dataset: hotel               Batch: 3/4	Loss 7.7092 (5.9000)
2022-11-11 21:07:00,562:INFO: Dataset: hotel               Batch: 4/4	Loss 6.0173 (5.9198)
2022-11-11 21:07:23,850:INFO: Dataset: univ                Batch:  1/15	Loss 4.3699 (4.3699)
2022-11-11 21:07:24,072:INFO: Dataset: univ                Batch:  2/15	Loss 4.6330 (4.4721)
2022-11-11 21:07:24,269:INFO: Dataset: univ                Batch:  3/15	Loss 3.8333 (4.2513)
2022-11-11 21:07:24,468:INFO: Dataset: univ                Batch:  4/15	Loss 3.3746 (4.0311)
2022-11-11 21:07:24,669:INFO: Dataset: univ                Batch:  5/15	Loss 3.4702 (3.9125)
2022-11-11 21:07:24,857:INFO: Dataset: univ                Batch:  6/15	Loss 6.0521 (4.1822)
2022-11-11 21:07:25,062:INFO: Dataset: univ                Batch:  7/15	Loss 6.3785 (4.4251)
2022-11-11 21:07:25,257:INFO: Dataset: univ                Batch:  8/15	Loss 4.9521 (4.4699)
2022-11-11 21:07:25,447:INFO: Dataset: univ                Batch:  9/15	Loss 5.9382 (4.6058)
2022-11-11 21:07:25,635:INFO: Dataset: univ                Batch: 10/15	Loss 4.7033 (4.6137)
2022-11-11 21:07:25,826:INFO: Dataset: univ                Batch: 11/15	Loss 4.0197 (4.5781)
2022-11-11 21:07:26,025:INFO: Dataset: univ                Batch: 12/15	Loss 9.9757 (4.7601)
2022-11-11 21:07:26,219:INFO: Dataset: univ                Batch: 13/15	Loss 12.6121 (4.8277)
2022-11-11 21:07:26,406:INFO: Dataset: univ                Batch: 14/15	Loss 15.8840 (4.9169)
2022-11-11 21:07:26,452:INFO: Dataset: univ                Batch: 15/15	Loss 3.6149 (4.9154)
2022-11-11 21:07:33,692:INFO: Dataset: zara1               Batch: 1/8	Loss 6.6738 (6.6738)
2022-11-11 21:07:34,105:INFO: Dataset: zara1               Batch: 2/8	Loss 8.1200 (7.3448)
2022-11-11 21:07:34,447:INFO: Dataset: zara1               Batch: 3/8	Loss 7.0649 (7.2632)
2022-11-11 21:07:34,763:INFO: Dataset: zara1               Batch: 4/8	Loss 12.9827 (8.3941)
2022-11-11 21:07:35,110:INFO: Dataset: zara1               Batch: 5/8	Loss 8.8526 (8.4762)
2022-11-11 21:07:35,327:INFO: Dataset: zara1               Batch: 6/8	Loss 8.5259 (8.4874)
2022-11-11 21:07:35,515:INFO: Dataset: zara1               Batch: 7/8	Loss 9.8428 (8.7454)
2022-11-11 21:07:35,680:INFO: Dataset: zara1               Batch: 8/8	Loss 10.1006 (8.8831)
2022-11-11 21:07:58,443:INFO: Dataset: zara2               Batch:  1/18	Loss 11.3525 (11.3525)
2022-11-11 21:07:58,657:INFO: Dataset: zara2               Batch:  2/18	Loss 7.6193 (9.2115)
2022-11-11 21:07:58,934:INFO: Dataset: zara2               Batch:  3/18	Loss 6.1639 (8.1233)
2022-11-11 21:07:59,126:INFO: Dataset: zara2               Batch:  4/18	Loss 7.6690 (8.0288)
2022-11-11 21:07:59,337:INFO: Dataset: zara2               Batch:  5/18	Loss 4.9917 (7.5273)
2022-11-11 21:07:59,525:INFO: Dataset: zara2               Batch:  6/18	Loss 3.7698 (6.4633)
2022-11-11 21:07:59,719:INFO: Dataset: zara2               Batch:  7/18	Loss 3.2973 (5.7752)
2022-11-11 21:07:59,906:INFO: Dataset: zara2               Batch:  8/18	Loss 3.3379 (5.3176)
2022-11-11 21:08:00,117:INFO: Dataset: zara2               Batch:  9/18	Loss 2.6340 (4.9962)
2022-11-11 21:08:00,314:INFO: Dataset: zara2               Batch: 10/18	Loss 3.5158 (4.7859)
2022-11-11 21:08:00,532:INFO: Dataset: zara2               Batch: 11/18	Loss 3.9239 (4.6710)
2022-11-11 21:08:00,723:INFO: Dataset: zara2               Batch: 12/18	Loss 9.4656 (4.9299)
2022-11-11 21:08:00,934:INFO: Dataset: zara2               Batch: 13/18	Loss 6.3880 (4.9913)
2022-11-11 21:08:01,122:INFO: Dataset: zara2               Batch: 14/18	Loss 3.7633 (4.9206)
2022-11-11 21:08:01,313:INFO: Dataset: zara2               Batch: 15/18	Loss 6.0854 (4.9885)
2022-11-11 21:08:01,506:INFO: Dataset: zara2               Batch: 16/18	Loss 7.3447 (5.0939)
2022-11-11 21:08:01,703:INFO: Dataset: zara2               Batch: 17/18	Loss 9.2894 (5.2524)
2022-11-11 21:08:01,878:INFO: Dataset: zara2               Batch: 18/18	Loss 4.9453 (5.2437)
2022-11-11 21:08:02,785:INFO:  --> Model Saved in ./models/E6//P2/CRMF_epoch_3.pth.tar
2022-11-11 21:08:02,785:INFO: 
===> EPOCH: 4 (P2)
2022-11-11 21:08:02,786:INFO: - Computing loss (training)
2022-11-11 21:08:09,132:INFO: Dataset: hotel               Batch: 1/4	Loss 4.3058 (4.3058)
2022-11-11 21:08:09,613:INFO: Dataset: hotel               Batch: 2/4	Loss 4.6110 (4.4530)
2022-11-11 21:08:09,948:INFO: Dataset: hotel               Batch: 3/4	Loss 6.9843 (5.2686)
2022-11-11 21:08:10,124:INFO: Dataset: hotel               Batch: 4/4	Loss 5.3155 (5.2766)
2022-11-11 21:08:33,228:INFO: Dataset: univ                Batch:  1/15	Loss 3.1518 (3.1518)
2022-11-11 21:08:33,430:INFO: Dataset: univ                Batch:  2/15	Loss 3.2247 (3.1802)
2022-11-11 21:08:33,625:INFO: Dataset: univ                Batch:  3/15	Loss 2.7325 (3.0254)
2022-11-11 21:08:33,812:INFO: Dataset: univ                Batch:  4/15	Loss 2.3597 (2.8582)
2022-11-11 21:08:34,023:INFO: Dataset: univ                Batch:  5/15	Loss 2.5346 (2.7898)
2022-11-11 21:08:34,226:INFO: Dataset: univ                Batch:  6/15	Loss 3.6689 (2.9006)
2022-11-11 21:08:34,413:INFO: Dataset: univ                Batch:  7/15	Loss 4.0436 (3.0270)
2022-11-11 21:08:34,599:INFO: Dataset: univ                Batch:  8/15	Loss 3.2520 (3.0461)
2022-11-11 21:08:34,801:INFO: Dataset: univ                Batch:  9/15	Loss 3.5667 (3.0943)
2022-11-11 21:08:34,999:INFO: Dataset: univ                Batch: 10/15	Loss 3.1113 (3.0957)
2022-11-11 21:08:35,195:INFO: Dataset: univ                Batch: 11/15	Loss 2.5649 (3.0639)
2022-11-11 21:08:35,395:INFO: Dataset: univ                Batch: 12/15	Loss 6.3431 (3.1744)
2022-11-11 21:08:35,579:INFO: Dataset: univ                Batch: 13/15	Loss 7.5774 (3.2124)
2022-11-11 21:08:35,773:INFO: Dataset: univ                Batch: 14/15	Loss 8.9545 (3.2587)
2022-11-11 21:08:35,820:INFO: Dataset: univ                Batch: 15/15	Loss 1.6872 (3.2569)
2022-11-11 21:08:43,083:INFO: Dataset: zara1               Batch: 1/8	Loss 4.3401 (4.3401)
2022-11-11 21:08:43,473:INFO: Dataset: zara1               Batch: 2/8	Loss 5.1474 (4.7147)
2022-11-11 21:08:43,824:INFO: Dataset: zara1               Batch: 3/8	Loss 3.8763 (4.4701)
2022-11-11 21:08:44,159:INFO: Dataset: zara1               Batch: 4/8	Loss 7.2376 (5.0173)
2022-11-11 21:08:44,409:INFO: Dataset: zara1               Batch: 5/8	Loss 6.1416 (5.2187)
2022-11-11 21:08:44,623:INFO: Dataset: zara1               Batch: 6/8	Loss 5.3066 (5.2384)
2022-11-11 21:08:44,808:INFO: Dataset: zara1               Batch: 7/8	Loss 5.7369 (5.3333)
2022-11-11 21:08:44,998:INFO: Dataset: zara1               Batch: 8/8	Loss 5.3996 (5.3400)
2022-11-11 21:09:09,097:INFO: Dataset: zara2               Batch:  1/18	Loss 6.5317 (6.5317)
2022-11-11 21:09:10,017:INFO: Dataset: zara2               Batch:  2/18	Loss 4.7465 (5.5079)
2022-11-11 21:09:10,230:INFO: Dataset: zara2               Batch:  3/18	Loss 4.0458 (4.9858)
2022-11-11 21:09:10,422:INFO: Dataset: zara2               Batch:  4/18	Loss 5.1154 (5.0128)
2022-11-11 21:09:10,612:INFO: Dataset: zara2               Batch:  5/18	Loss 3.3511 (4.7384)
2022-11-11 21:09:10,799:INFO: Dataset: zara2               Batch:  6/18	Loss 2.6568 (4.1489)
2022-11-11 21:09:10,993:INFO: Dataset: zara2               Batch:  7/18	Loss 2.3354 (3.7548)
2022-11-11 21:09:11,186:INFO: Dataset: zara2               Batch:  8/18	Loss 2.2706 (3.4761)
2022-11-11 21:09:11,399:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0245 (3.3023)
2022-11-11 21:09:11,583:INFO: Dataset: zara2               Batch: 10/18	Loss 2.4635 (3.1831)
2022-11-11 21:09:11,786:INFO: Dataset: zara2               Batch: 11/18	Loss 2.8061 (3.1329)
2022-11-11 21:09:11,996:INFO: Dataset: zara2               Batch: 12/18	Loss 6.2584 (3.3016)
2022-11-11 21:09:12,187:INFO: Dataset: zara2               Batch: 13/18	Loss 4.3456 (3.3456)
2022-11-11 21:09:12,383:INFO: Dataset: zara2               Batch: 14/18	Loss 2.4931 (3.2965)
2022-11-11 21:09:12,580:INFO: Dataset: zara2               Batch: 15/18	Loss 3.9211 (3.3329)
2022-11-11 21:09:12,770:INFO: Dataset: zara2               Batch: 16/18	Loss 4.6162 (3.3903)
2022-11-11 21:09:12,969:INFO: Dataset: zara2               Batch: 17/18	Loss 6.4252 (3.5050)
2022-11-11 21:09:13,144:INFO: Dataset: zara2               Batch: 18/18	Loss 3.5537 (3.5063)
2022-11-11 21:09:14,074:INFO:  --> Model Saved in ./models/E6//P2/CRMF_epoch_4.pth.tar
2022-11-11 21:09:14,075:INFO: 
===> EPOCH: 5 (P3)
2022-11-11 21:09:14,075:INFO: - Computing loss (training)
2022-11-11 21:09:23,420:INFO: Dataset: hotel               Batch: 1/4	Loss 83.6754 (83.6754)
2022-11-11 21:09:24,834:INFO: Dataset: hotel               Batch: 2/4	Loss 82.6702 (83.1904)
2022-11-11 21:09:26,207:INFO: Dataset: hotel               Batch: 3/4	Loss 86.0883 (84.1242)
2022-11-11 21:09:27,548:INFO: Dataset: hotel               Batch: 4/4	Loss 53.2620 (78.9126)
2022-11-11 21:09:53,199:INFO: Dataset: univ                Batch:  1/15	Loss 81.0515 (81.0515)
2022-11-11 21:09:57,326:INFO: Dataset: univ                Batch:  2/15	Loss 80.6405 (80.8918)
2022-11-11 21:09:59,236:INFO: Dataset: univ                Batch:  3/15	Loss 78.8319 (80.1798)

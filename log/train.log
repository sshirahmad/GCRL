2022-11-24 20:52:07,316:INFO: Initializing Training Set
2022-11-24 20:52:08,836:INFO: Initializing Validation Set
2022-11-24 20:52:09,018:INFO: Initializing Validation O Set
2022-11-24 20:52:11,042:INFO: 
===> EPOCH: 1 (P1)
2022-11-24 20:52:11,043:INFO: - Computing loss (training)
2022-11-24 20:52:11,220:INFO: Dataset: hotel               Batch: 1/4	Loss 10.5628 (10.5628)
2022-11-24 20:52:11,240:INFO: Dataset: hotel               Batch: 2/4	Loss 11.6485 (11.0979)
2022-11-24 20:52:11,262:INFO: Dataset: hotel               Batch: 3/4	Loss 11.7483 (11.3161)
2022-11-24 20:52:11,276:INFO: Dataset: hotel               Batch: 4/4	Loss 8.0621 (10.7623)
2022-11-24 20:52:11,493:INFO: Dataset: univ                Batch:  1/15	Loss 13.1160 (13.1160)
2022-11-24 20:52:11,516:INFO: Dataset: univ                Batch:  2/15	Loss 11.3914 (12.2488)
2022-11-24 20:52:11,540:INFO: Dataset: univ                Batch:  3/15	Loss 11.2943 (11.9159)
2022-11-24 20:52:11,564:INFO: Dataset: univ                Batch:  4/15	Loss 14.7447 (12.5495)
2022-11-24 20:52:11,586:INFO: Dataset: univ                Batch:  5/15	Loss 12.3414 (12.5066)
2022-11-24 20:52:11,610:INFO: Dataset: univ                Batch:  6/15	Loss 12.0672 (12.4353)
2022-11-24 20:52:11,632:INFO: Dataset: univ                Batch:  7/15	Loss 12.2302 (12.4055)
2022-11-24 20:52:11,653:INFO: Dataset: univ                Batch:  8/15	Loss 10.4340 (12.1346)
2022-11-24 20:52:11,674:INFO: Dataset: univ                Batch:  9/15	Loss 10.8260 (11.9885)
2022-11-24 20:52:11,694:INFO: Dataset: univ                Batch: 10/15	Loss 14.8254 (12.2530)
2022-11-24 20:52:11,719:INFO: Dataset: univ                Batch: 11/15	Loss 13.2537 (12.3450)
2022-11-24 20:52:11,741:INFO: Dataset: univ                Batch: 12/15	Loss 10.6873 (12.2015)
2022-11-24 20:52:11,762:INFO: Dataset: univ                Batch: 13/15	Loss 11.5242 (12.1530)
2022-11-24 20:52:11,783:INFO: Dataset: univ                Batch: 14/15	Loss 14.0322 (12.2723)
2022-11-24 20:52:11,792:INFO: Dataset: univ                Batch: 15/15	Loss 2.4378 (12.1450)
2022-11-24 20:52:11,992:INFO: Dataset: zara1               Batch: 1/8	Loss 20.8493 (20.8493)
2022-11-24 20:52:12,012:INFO: Dataset: zara1               Batch: 2/8	Loss 17.0729 (18.8720)
2022-11-24 20:52:12,032:INFO: Dataset: zara1               Batch: 3/8	Loss 18.9090 (18.8854)
2022-11-24 20:52:12,053:INFO: Dataset: zara1               Batch: 4/8	Loss 21.4495 (19.5388)
2022-11-24 20:52:12,073:INFO: Dataset: zara1               Batch: 5/8	Loss 19.6890 (19.5687)
2022-11-24 20:52:12,094:INFO: Dataset: zara1               Batch: 6/8	Loss 18.9377 (19.4639)
2022-11-24 20:52:12,113:INFO: Dataset: zara1               Batch: 7/8	Loss 18.7934 (19.3704)
2022-11-24 20:52:12,130:INFO: Dataset: zara1               Batch: 8/8	Loss 17.4392 (19.1743)
2022-11-24 20:52:12,327:INFO: Dataset: zara2               Batch:  1/18	Loss 12.3043 (12.3043)
2022-11-24 20:52:12,351:INFO: Dataset: zara2               Batch:  2/18	Loss 14.6399 (13.4492)
2022-11-24 20:52:12,370:INFO: Dataset: zara2               Batch:  3/18	Loss 15.7849 (14.1936)
2022-11-24 20:52:12,389:INFO: Dataset: zara2               Batch:  4/18	Loss 12.7524 (13.8414)
2022-11-24 20:52:12,411:INFO: Dataset: zara2               Batch:  5/18	Loss 16.6156 (14.3197)
2022-11-24 20:52:12,432:INFO: Dataset: zara2               Batch:  6/18	Loss 15.2815 (14.4963)
2022-11-24 20:52:12,450:INFO: Dataset: zara2               Batch:  7/18	Loss 13.9192 (14.4198)
2022-11-24 20:52:12,469:INFO: Dataset: zara2               Batch:  8/18	Loss 14.8664 (14.4780)
2022-11-24 20:52:12,488:INFO: Dataset: zara2               Batch:  9/18	Loss 16.2086 (14.6611)
2022-11-24 20:52:12,507:INFO: Dataset: zara2               Batch: 10/18	Loss 15.5780 (14.7578)
2022-11-24 20:52:12,528:INFO: Dataset: zara2               Batch: 11/18	Loss 14.7225 (14.7548)
2022-11-24 20:52:12,547:INFO: Dataset: zara2               Batch: 12/18	Loss 15.3046 (14.8030)
2022-11-24 20:52:12,566:INFO: Dataset: zara2               Batch: 13/18	Loss 14.7653 (14.8003)
2022-11-24 20:52:12,585:INFO: Dataset: zara2               Batch: 14/18	Loss 13.9912 (14.7458)
2022-11-24 20:52:12,606:INFO: Dataset: zara2               Batch: 15/18	Loss 13.2163 (14.6409)
2022-11-24 20:52:12,626:INFO: Dataset: zara2               Batch: 16/18	Loss 14.3251 (14.6219)
2022-11-24 20:52:12,646:INFO: Dataset: zara2               Batch: 17/18	Loss 15.5334 (14.6807)
2022-11-24 20:52:12,664:INFO: Dataset: zara2               Batch: 18/18	Loss 12.6139 (14.5795)
2022-11-24 20:52:12,704:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_1.pth.tar
2022-11-24 20:52:12,704:INFO: 
===> EPOCH: 2 (P1)
2022-11-24 20:52:12,704:INFO: - Computing loss (training)
2022-11-24 20:52:12,876:INFO: Dataset: hotel               Batch: 1/4	Loss 11.8956 (11.8956)
2022-11-24 20:52:12,897:INFO: Dataset: hotel               Batch: 2/4	Loss 10.7998 (11.3045)
2022-11-24 20:52:12,916:INFO: Dataset: hotel               Batch: 3/4	Loss 12.5029 (11.7122)
2022-11-24 20:52:12,930:INFO: Dataset: hotel               Batch: 4/4	Loss 7.0739 (10.9595)
2022-11-24 20:52:13,150:INFO: Dataset: univ                Batch:  1/15	Loss 10.0930 (10.0930)
2022-11-24 20:52:13,172:INFO: Dataset: univ                Batch:  2/15	Loss 14.4201 (12.0424)
2022-11-24 20:52:13,193:INFO: Dataset: univ                Batch:  3/15	Loss 13.6541 (12.5533)
2022-11-24 20:52:13,217:INFO: Dataset: univ                Batch:  4/15	Loss 11.4829 (12.2860)
2022-11-24 20:52:13,239:INFO: Dataset: univ                Batch:  5/15	Loss 13.3095 (12.4717)
2022-11-24 20:52:13,263:INFO: Dataset: univ                Batch:  6/15	Loss 11.0439 (12.2379)
2022-11-24 20:52:13,283:INFO: Dataset: univ                Batch:  7/15	Loss 15.4538 (12.6274)
2022-11-24 20:52:13,303:INFO: Dataset: univ                Batch:  8/15	Loss 10.9114 (12.4041)
2022-11-24 20:52:13,324:INFO: Dataset: univ                Batch:  9/15	Loss 11.0443 (12.2467)
2022-11-24 20:52:13,344:INFO: Dataset: univ                Batch: 10/15	Loss 11.6270 (12.1804)
2022-11-24 20:52:13,365:INFO: Dataset: univ                Batch: 11/15	Loss 12.4538 (12.2037)
2022-11-24 20:52:13,388:INFO: Dataset: univ                Batch: 12/15	Loss 12.4333 (12.2218)
2022-11-24 20:52:13,409:INFO: Dataset: univ                Batch: 13/15	Loss 12.7839 (12.2673)
2022-11-24 20:52:13,430:INFO: Dataset: univ                Batch: 14/15	Loss 11.5041 (12.2099)
2022-11-24 20:52:13,439:INFO: Dataset: univ                Batch: 15/15	Loss 2.0118 (12.0553)
2022-11-24 20:52:13,642:INFO: Dataset: zara1               Batch: 1/8	Loss 20.7800 (20.7800)
2022-11-24 20:52:13,664:INFO: Dataset: zara1               Batch: 2/8	Loss 18.5795 (19.6523)
2022-11-24 20:52:13,683:INFO: Dataset: zara1               Batch: 3/8	Loss 19.1004 (19.4658)
2022-11-24 20:52:13,702:INFO: Dataset: zara1               Batch: 4/8	Loss 19.5822 (19.4946)
2022-11-24 20:52:13,720:INFO: Dataset: zara1               Batch: 5/8	Loss 19.1380 (19.4265)
2022-11-24 20:52:13,741:INFO: Dataset: zara1               Batch: 6/8	Loss 19.4908 (19.4376)
2022-11-24 20:52:13,759:INFO: Dataset: zara1               Batch: 7/8	Loss 18.3402 (19.2755)
2022-11-24 20:52:13,776:INFO: Dataset: zara1               Batch: 8/8	Loss 16.0714 (18.9265)
2022-11-24 20:52:13,971:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3592 (13.3592)
2022-11-24 20:52:14,025:INFO: Dataset: zara2               Batch:  2/18	Loss 14.5751 (13.9888)
2022-11-24 20:52:14,044:INFO: Dataset: zara2               Batch:  3/18	Loss 15.4481 (14.4317)
2022-11-24 20:52:14,063:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2679 (14.3894)
2022-11-24 20:52:14,082:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0990 (14.3286)
2022-11-24 20:52:14,105:INFO: Dataset: zara2               Batch:  6/18	Loss 14.9439 (14.4258)
2022-11-24 20:52:14,124:INFO: Dataset: zara2               Batch:  7/18	Loss 12.8832 (14.2144)
2022-11-24 20:52:14,143:INFO: Dataset: zara2               Batch:  8/18	Loss 17.1398 (14.5382)
2022-11-24 20:52:14,161:INFO: Dataset: zara2               Batch:  9/18	Loss 14.8436 (14.5693)
2022-11-24 20:52:14,180:INFO: Dataset: zara2               Batch: 10/18	Loss 13.6311 (14.4689)
2022-11-24 20:52:14,200:INFO: Dataset: zara2               Batch: 11/18	Loss 14.7338 (14.4942)
2022-11-24 20:52:14,220:INFO: Dataset: zara2               Batch: 12/18	Loss 13.6181 (14.4253)
2022-11-24 20:52:14,239:INFO: Dataset: zara2               Batch: 13/18	Loss 14.5452 (14.4339)
2022-11-24 20:52:14,258:INFO: Dataset: zara2               Batch: 14/18	Loss 15.5246 (14.5117)
2022-11-24 20:52:14,279:INFO: Dataset: zara2               Batch: 15/18	Loss 15.0770 (14.5451)
2022-11-24 20:52:14,298:INFO: Dataset: zara2               Batch: 16/18	Loss 13.2755 (14.4655)
2022-11-24 20:52:14,318:INFO: Dataset: zara2               Batch: 17/18	Loss 14.5540 (14.4703)
2022-11-24 20:52:14,336:INFO: Dataset: zara2               Batch: 18/18	Loss 12.1763 (14.3637)
2022-11-24 20:52:14,376:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_2.pth.tar
2022-11-24 20:52:14,376:INFO: 
===> EPOCH: 3 (P1)
2022-11-24 20:52:14,376:INFO: - Computing loss (training)
2022-11-24 20:52:14,554:INFO: Dataset: hotel               Batch: 1/4	Loss 10.5126 (10.5126)
2022-11-24 20:52:14,573:INFO: Dataset: hotel               Batch: 2/4	Loss 12.7883 (11.6200)
2022-11-24 20:52:14,593:INFO: Dataset: hotel               Batch: 3/4	Loss 11.0771 (11.4335)
2022-11-24 20:52:14,606:INFO: Dataset: hotel               Batch: 4/4	Loss 7.3381 (10.7204)
2022-11-24 20:52:14,801:INFO: Dataset: univ                Batch:  1/15	Loss 11.4122 (11.4122)
2022-11-24 20:52:14,825:INFO: Dataset: univ                Batch:  2/15	Loss 11.2909 (11.3515)
2022-11-24 20:52:14,846:INFO: Dataset: univ                Batch:  3/15	Loss 13.3283 (11.9574)
2022-11-24 20:52:14,869:INFO: Dataset: univ                Batch:  4/15	Loss 13.4663 (12.3280)
2022-11-24 20:52:14,892:INFO: Dataset: univ                Batch:  5/15	Loss 11.8047 (12.2187)
2022-11-24 20:52:14,915:INFO: Dataset: univ                Batch:  6/15	Loss 11.3547 (12.0699)
2022-11-24 20:52:14,937:INFO: Dataset: univ                Batch:  7/15	Loss 13.3956 (12.2673)
2022-11-24 20:52:14,957:INFO: Dataset: univ                Batch:  8/15	Loss 11.9843 (12.2334)
2022-11-24 20:52:14,978:INFO: Dataset: univ                Batch:  9/15	Loss 10.3390 (12.0129)
2022-11-24 20:52:14,999:INFO: Dataset: univ                Batch: 10/15	Loss 13.7550 (12.1761)
2022-11-24 20:52:15,023:INFO: Dataset: univ                Batch: 11/15	Loss 13.1260 (12.2569)
2022-11-24 20:52:15,047:INFO: Dataset: univ                Batch: 12/15	Loss 11.5200 (12.1934)
2022-11-24 20:52:15,069:INFO: Dataset: univ                Batch: 13/15	Loss 11.9613 (12.1750)
2022-11-24 20:52:15,091:INFO: Dataset: univ                Batch: 14/15	Loss 10.8069 (12.0781)
2022-11-24 20:52:15,101:INFO: Dataset: univ                Batch: 15/15	Loss 2.3469 (11.9407)
2022-11-24 20:52:15,299:INFO: Dataset: zara1               Batch: 1/8	Loss 19.7081 (19.7081)
2022-11-24 20:52:15,318:INFO: Dataset: zara1               Batch: 2/8	Loss 18.0973 (18.9419)
2022-11-24 20:52:15,338:INFO: Dataset: zara1               Batch: 3/8	Loss 19.2696 (19.0475)
2022-11-24 20:52:15,357:INFO: Dataset: zara1               Batch: 4/8	Loss 19.7808 (19.2229)
2022-11-24 20:52:15,376:INFO: Dataset: zara1               Batch: 5/8	Loss 18.6322 (19.1128)
2022-11-24 20:52:15,397:INFO: Dataset: zara1               Batch: 6/8	Loss 19.0521 (19.1028)
2022-11-24 20:52:15,416:INFO: Dataset: zara1               Batch: 7/8	Loss 18.8619 (19.0673)
2022-11-24 20:52:15,433:INFO: Dataset: zara1               Batch: 8/8	Loss 16.7938 (18.8471)
2022-11-24 20:52:15,708:INFO: Dataset: zara2               Batch:  1/18	Loss 16.5063 (16.5063)
2022-11-24 20:52:15,737:INFO: Dataset: zara2               Batch:  2/18	Loss 14.4506 (15.4816)
2022-11-24 20:52:15,756:INFO: Dataset: zara2               Batch:  3/18	Loss 12.7186 (14.4194)
2022-11-24 20:52:15,777:INFO: Dataset: zara2               Batch:  4/18	Loss 13.1049 (14.1174)
2022-11-24 20:52:15,797:INFO: Dataset: zara2               Batch:  5/18	Loss 15.3499 (14.3642)
2022-11-24 20:52:15,818:INFO: Dataset: zara2               Batch:  6/18	Loss 16.7662 (14.7410)
2022-11-24 20:52:15,836:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0436 (14.6475)
2022-11-24 20:52:15,855:INFO: Dataset: zara2               Batch:  8/18	Loss 13.1314 (14.4642)
2022-11-24 20:52:15,874:INFO: Dataset: zara2               Batch:  9/18	Loss 14.4285 (14.4603)
2022-11-24 20:52:15,893:INFO: Dataset: zara2               Batch: 10/18	Loss 14.4240 (14.4568)
2022-11-24 20:52:15,913:INFO: Dataset: zara2               Batch: 11/18	Loss 14.0627 (14.4178)
2022-11-24 20:52:15,933:INFO: Dataset: zara2               Batch: 12/18	Loss 13.5085 (14.3414)
2022-11-24 20:52:15,953:INFO: Dataset: zara2               Batch: 13/18	Loss 14.0730 (14.3194)
2022-11-24 20:52:15,974:INFO: Dataset: zara2               Batch: 14/18	Loss 14.7072 (14.3500)
2022-11-24 20:52:15,995:INFO: Dataset: zara2               Batch: 15/18	Loss 14.7547 (14.3801)
2022-11-24 20:52:16,015:INFO: Dataset: zara2               Batch: 16/18	Loss 13.3136 (14.3149)
2022-11-24 20:52:16,035:INFO: Dataset: zara2               Batch: 17/18	Loss 15.7714 (14.4066)
2022-11-24 20:52:16,053:INFO: Dataset: zara2               Batch: 18/18	Loss 11.0942 (14.2495)
2022-11-24 20:52:16,088:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_3.pth.tar
2022-11-24 20:52:16,088:INFO: 
===> EPOCH: 4 (P1)
2022-11-24 20:52:16,088:INFO: - Computing loss (training)
2022-11-24 20:52:16,269:INFO: Dataset: hotel               Batch: 1/4	Loss 11.3052 (11.3052)
2022-11-24 20:52:16,288:INFO: Dataset: hotel               Batch: 2/4	Loss 14.3988 (12.8821)
2022-11-24 20:52:16,309:INFO: Dataset: hotel               Batch: 3/4	Loss 10.2360 (11.9555)
2022-11-24 20:52:16,322:INFO: Dataset: hotel               Batch: 4/4	Loss 5.4752 (10.8954)
2022-11-24 20:52:16,535:INFO: Dataset: univ                Batch:  1/15	Loss 11.0326 (11.0326)
2022-11-24 20:52:16,556:INFO: Dataset: univ                Batch:  2/15	Loss 12.5687 (11.8103)
2022-11-24 20:52:16,579:INFO: Dataset: univ                Batch:  3/15	Loss 10.4081 (11.3050)
2022-11-24 20:52:16,600:INFO: Dataset: univ                Batch:  4/15	Loss 12.2272 (11.5190)
2022-11-24 20:52:16,623:INFO: Dataset: univ                Batch:  5/15	Loss 12.6225 (11.7482)
2022-11-24 20:52:16,645:INFO: Dataset: univ                Batch:  6/15	Loss 13.1334 (11.9770)
2022-11-24 20:52:16,665:INFO: Dataset: univ                Batch:  7/15	Loss 12.1077 (11.9965)
2022-11-24 20:52:16,686:INFO: Dataset: univ                Batch:  8/15	Loss 13.6496 (12.1943)
2022-11-24 20:52:16,706:INFO: Dataset: univ                Batch:  9/15	Loss 13.5027 (12.3327)
2022-11-24 20:52:16,726:INFO: Dataset: univ                Batch: 10/15	Loss 10.5578 (12.1425)
2022-11-24 20:52:16,748:INFO: Dataset: univ                Batch: 11/15	Loss 11.2501 (12.0570)
2022-11-24 20:52:16,769:INFO: Dataset: univ                Batch: 12/15	Loss 10.3076 (11.9086)
2022-11-24 20:52:16,790:INFO: Dataset: univ                Batch: 13/15	Loss 13.4119 (12.0189)
2022-11-24 20:52:16,812:INFO: Dataset: univ                Batch: 14/15	Loss 11.4668 (11.9789)
2022-11-24 20:52:16,820:INFO: Dataset: univ                Batch: 15/15	Loss 1.6530 (11.8179)
2022-11-24 20:52:17,042:INFO: Dataset: zara1               Batch: 1/8	Loss 19.1496 (19.1496)
2022-11-24 20:52:17,063:INFO: Dataset: zara1               Batch: 2/8	Loss 18.3005 (18.7496)
2022-11-24 20:52:17,082:INFO: Dataset: zara1               Batch: 3/8	Loss 18.6485 (18.7141)
2022-11-24 20:52:17,103:INFO: Dataset: zara1               Batch: 4/8	Loss 17.6730 (18.4679)
2022-11-24 20:52:17,123:INFO: Dataset: zara1               Batch: 5/8	Loss 19.0831 (18.5923)
2022-11-24 20:52:17,143:INFO: Dataset: zara1               Batch: 6/8	Loss 20.3177 (18.8985)
2022-11-24 20:52:17,162:INFO: Dataset: zara1               Batch: 7/8	Loss 19.6585 (19.0057)
2022-11-24 20:52:17,178:INFO: Dataset: zara1               Batch: 8/8	Loss 15.3575 (18.5698)
2022-11-24 20:52:17,391:INFO: Dataset: zara2               Batch:  1/18	Loss 16.3103 (16.3103)
2022-11-24 20:52:17,411:INFO: Dataset: zara2               Batch:  2/18	Loss 15.2363 (15.7341)
2022-11-24 20:52:17,432:INFO: Dataset: zara2               Batch:  3/18	Loss 13.1155 (14.8233)
2022-11-24 20:52:17,452:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3335 (14.4489)
2022-11-24 20:52:17,475:INFO: Dataset: zara2               Batch:  5/18	Loss 12.7567 (14.1057)
2022-11-24 20:52:17,496:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3961 (13.9658)
2022-11-24 20:52:17,514:INFO: Dataset: zara2               Batch:  7/18	Loss 15.3310 (14.1700)
2022-11-24 20:52:17,533:INFO: Dataset: zara2               Batch:  8/18	Loss 14.4472 (14.2016)
2022-11-24 20:52:17,552:INFO: Dataset: zara2               Batch:  9/18	Loss 14.9904 (14.2802)
2022-11-24 20:52:17,571:INFO: Dataset: zara2               Batch: 10/18	Loss 13.9275 (14.2417)
2022-11-24 20:52:17,591:INFO: Dataset: zara2               Batch: 11/18	Loss 14.7409 (14.2827)
2022-11-24 20:52:17,611:INFO: Dataset: zara2               Batch: 12/18	Loss 15.3729 (14.3705)
2022-11-24 20:52:17,630:INFO: Dataset: zara2               Batch: 13/18	Loss 14.2430 (14.3614)
2022-11-24 20:52:17,650:INFO: Dataset: zara2               Batch: 14/18	Loss 13.4907 (14.2976)
2022-11-24 20:52:17,670:INFO: Dataset: zara2               Batch: 15/18	Loss 13.1265 (14.2219)
2022-11-24 20:52:17,690:INFO: Dataset: zara2               Batch: 16/18	Loss 13.2244 (14.1554)
2022-11-24 20:52:17,710:INFO: Dataset: zara2               Batch: 17/18	Loss 13.4991 (14.1138)
2022-11-24 20:52:17,727:INFO: Dataset: zara2               Batch: 18/18	Loss 12.9666 (14.0576)
2022-11-24 20:52:17,762:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_4.pth.tar
2022-11-24 20:52:17,762:INFO: 
===> EPOCH: 5 (P1)
2022-11-24 20:52:17,763:INFO: - Computing loss (training)
2022-11-24 20:52:17,943:INFO: Dataset: hotel               Batch: 1/4	Loss 11.5773 (11.5773)
2022-11-24 20:52:17,965:INFO: Dataset: hotel               Batch: 2/4	Loss 9.7991 (10.7521)
2022-11-24 20:52:17,986:INFO: Dataset: hotel               Batch: 3/4	Loss 12.4085 (11.2771)
2022-11-24 20:52:18,000:INFO: Dataset: hotel               Batch: 4/4	Loss 6.7547 (10.5194)
2022-11-24 20:52:18,220:INFO: Dataset: univ                Batch:  1/15	Loss 11.9963 (11.9963)
2022-11-24 20:52:18,242:INFO: Dataset: univ                Batch:  2/15	Loss 13.1100 (12.5161)
2022-11-24 20:52:18,265:INFO: Dataset: univ                Batch:  3/15	Loss 11.5509 (12.1708)
2022-11-24 20:52:18,288:INFO: Dataset: univ                Batch:  4/15	Loss 11.6314 (12.0403)
2022-11-24 20:52:18,309:INFO: Dataset: univ                Batch:  5/15	Loss 11.2275 (11.8659)
2022-11-24 20:52:18,336:INFO: Dataset: univ                Batch:  6/15	Loss 12.6004 (11.9911)
2022-11-24 20:52:18,358:INFO: Dataset: univ                Batch:  7/15	Loss 11.0594 (11.8594)
2022-11-24 20:52:18,380:INFO: Dataset: univ                Batch:  8/15	Loss 12.4189 (11.9308)
2022-11-24 20:52:18,401:INFO: Dataset: univ                Batch:  9/15	Loss 12.4532 (11.9896)
2022-11-24 20:52:18,422:INFO: Dataset: univ                Batch: 10/15	Loss 10.7276 (11.8546)
2022-11-24 20:52:18,448:INFO: Dataset: univ                Batch: 11/15	Loss 11.0386 (11.7817)
2022-11-24 20:52:18,471:INFO: Dataset: univ                Batch: 12/15	Loss 12.6372 (11.8530)
2022-11-24 20:52:18,494:INFO: Dataset: univ                Batch: 13/15	Loss 10.6955 (11.7638)
2022-11-24 20:52:18,516:INFO: Dataset: univ                Batch: 14/15	Loss 11.8247 (11.7681)
2022-11-24 20:52:18,525:INFO: Dataset: univ                Batch: 15/15	Loss 2.8077 (11.6637)
2022-11-24 20:52:18,733:INFO: Dataset: zara1               Batch: 1/8	Loss 17.5081 (17.5081)
2022-11-24 20:52:18,757:INFO: Dataset: zara1               Batch: 2/8	Loss 18.8547 (18.1670)
2022-11-24 20:52:18,777:INFO: Dataset: zara1               Batch: 3/8	Loss 18.4710 (18.2625)
2022-11-24 20:52:18,797:INFO: Dataset: zara1               Batch: 4/8	Loss 17.4218 (18.0707)
2022-11-24 20:52:18,817:INFO: Dataset: zara1               Batch: 5/8	Loss 18.7999 (18.2087)
2022-11-24 20:52:18,839:INFO: Dataset: zara1               Batch: 6/8	Loss 19.8952 (18.4868)
2022-11-24 20:52:18,858:INFO: Dataset: zara1               Batch: 7/8	Loss 18.9793 (18.5610)
2022-11-24 20:52:18,876:INFO: Dataset: zara1               Batch: 8/8	Loss 16.6605 (18.3530)
2022-11-24 20:52:19,086:INFO: Dataset: zara2               Batch:  1/18	Loss 14.8273 (14.8273)
2022-11-24 20:52:19,110:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1300 (14.4819)
2022-11-24 20:52:19,129:INFO: Dataset: zara2               Batch:  3/18	Loss 16.0121 (15.0078)
2022-11-24 20:52:19,150:INFO: Dataset: zara2               Batch:  4/18	Loss 13.0244 (14.4960)
2022-11-24 20:52:19,172:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0815 (14.4051)
2022-11-24 20:52:19,194:INFO: Dataset: zara2               Batch:  6/18	Loss 15.2048 (14.5368)
2022-11-24 20:52:19,213:INFO: Dataset: zara2               Batch:  7/18	Loss 14.2937 (14.4992)
2022-11-24 20:52:19,232:INFO: Dataset: zara2               Batch:  8/18	Loss 12.9186 (14.2958)
2022-11-24 20:52:19,252:INFO: Dataset: zara2               Batch:  9/18	Loss 12.4702 (14.0887)
2022-11-24 20:52:19,271:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9037 (13.9652)
2022-11-24 20:52:19,291:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9731 (13.8678)
2022-11-24 20:52:19,312:INFO: Dataset: zara2               Batch: 12/18	Loss 14.3000 (13.9022)
2022-11-24 20:52:19,331:INFO: Dataset: zara2               Batch: 13/18	Loss 13.0665 (13.8405)
2022-11-24 20:52:19,351:INFO: Dataset: zara2               Batch: 14/18	Loss 14.1790 (13.8637)
2022-11-24 20:52:19,372:INFO: Dataset: zara2               Batch: 15/18	Loss 13.3535 (13.8286)
2022-11-24 20:52:19,392:INFO: Dataset: zara2               Batch: 16/18	Loss 12.9363 (13.7743)
2022-11-24 20:52:19,412:INFO: Dataset: zara2               Batch: 17/18	Loss 15.5425 (13.8694)
2022-11-24 20:52:19,430:INFO: Dataset: zara2               Batch: 18/18	Loss 13.5879 (13.8570)
2022-11-24 20:52:19,466:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_5.pth.tar
2022-11-24 20:52:19,466:INFO: 
===> EPOCH: 6 (P1)
2022-11-24 20:52:19,466:INFO: - Computing loss (training)
2022-11-24 20:52:19,635:INFO: Dataset: hotel               Batch: 1/4	Loss 13.8821 (13.8821)
2022-11-24 20:52:19,657:INFO: Dataset: hotel               Batch: 2/4	Loss 10.3149 (12.0941)
2022-11-24 20:52:19,676:INFO: Dataset: hotel               Batch: 3/4	Loss 9.9837 (11.3872)
2022-11-24 20:52:19,690:INFO: Dataset: hotel               Batch: 4/4	Loss 6.6808 (10.5180)
2022-11-24 20:52:19,896:INFO: Dataset: univ                Batch:  1/15	Loss 11.4687 (11.4687)
2022-11-24 20:52:19,918:INFO: Dataset: univ                Batch:  2/15	Loss 12.6200 (12.0479)
2022-11-24 20:52:19,943:INFO: Dataset: univ                Batch:  3/15	Loss 12.9295 (12.3554)
2022-11-24 20:52:19,963:INFO: Dataset: univ                Batch:  4/15	Loss 10.9628 (11.9893)
2022-11-24 20:52:19,986:INFO: Dataset: univ                Batch:  5/15	Loss 10.7974 (11.7573)
2022-11-24 20:52:20,009:INFO: Dataset: univ                Batch:  6/15	Loss 11.9175 (11.7841)
2022-11-24 20:52:20,030:INFO: Dataset: univ                Batch:  7/15	Loss 10.0005 (11.5242)
2022-11-24 20:52:20,050:INFO: Dataset: univ                Batch:  8/15	Loss 10.6071 (11.4109)
2022-11-24 20:52:20,070:INFO: Dataset: univ                Batch:  9/15	Loss 12.5779 (11.5341)
2022-11-24 20:52:20,091:INFO: Dataset: univ                Batch: 10/15	Loss 12.5763 (11.6348)
2022-11-24 20:52:20,115:INFO: Dataset: univ                Batch: 11/15	Loss 10.6123 (11.5349)
2022-11-24 20:52:20,136:INFO: Dataset: univ                Batch: 12/15	Loss 11.5232 (11.5339)
2022-11-24 20:52:20,157:INFO: Dataset: univ                Batch: 13/15	Loss 11.6884 (11.5455)
2022-11-24 20:52:20,177:INFO: Dataset: univ                Batch: 14/15	Loss 12.6861 (11.6212)
2022-11-24 20:52:20,185:INFO: Dataset: univ                Batch: 15/15	Loss 2.9951 (11.5411)
2022-11-24 20:52:20,404:INFO: Dataset: zara1               Batch: 1/8	Loss 17.9220 (17.9220)
2022-11-24 20:52:20,423:INFO: Dataset: zara1               Batch: 2/8	Loss 17.3868 (17.6517)
2022-11-24 20:52:20,442:INFO: Dataset: zara1               Batch: 3/8	Loss 19.5275 (18.3145)
2022-11-24 20:52:20,460:INFO: Dataset: zara1               Batch: 4/8	Loss 18.2699 (18.3031)
2022-11-24 20:52:20,479:INFO: Dataset: zara1               Batch: 5/8	Loss 19.6160 (18.5392)
2022-11-24 20:52:20,501:INFO: Dataset: zara1               Batch: 6/8	Loss 17.1599 (18.3130)
2022-11-24 20:52:20,519:INFO: Dataset: zara1               Batch: 7/8	Loss 19.0553 (18.4209)
2022-11-24 20:52:20,536:INFO: Dataset: zara1               Batch: 8/8	Loss 15.9353 (18.1855)
2022-11-24 20:52:20,733:INFO: Dataset: zara2               Batch:  1/18	Loss 15.3086 (15.3086)
2022-11-24 20:52:20,755:INFO: Dataset: zara2               Batch:  2/18	Loss 11.8161 (13.4896)
2022-11-24 20:52:20,775:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6634 (13.5487)
2022-11-24 20:52:20,794:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4238 (13.5177)
2022-11-24 20:52:20,815:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7141 (13.5592)
2022-11-24 20:52:20,838:INFO: Dataset: zara2               Batch:  6/18	Loss 12.4201 (13.3595)
2022-11-24 20:52:20,857:INFO: Dataset: zara2               Batch:  7/18	Loss 12.9596 (13.3025)
2022-11-24 20:52:20,876:INFO: Dataset: zara2               Batch:  8/18	Loss 12.1871 (13.1698)
2022-11-24 20:52:20,894:INFO: Dataset: zara2               Batch:  9/18	Loss 14.4373 (13.3075)
2022-11-24 20:52:20,913:INFO: Dataset: zara2               Batch: 10/18	Loss 14.2210 (13.3961)
2022-11-24 20:52:20,932:INFO: Dataset: zara2               Batch: 11/18	Loss 13.9456 (13.4456)
2022-11-24 20:52:20,954:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5020 (13.3718)
2022-11-24 20:52:20,974:INFO: Dataset: zara2               Batch: 13/18	Loss 16.7592 (13.6167)
2022-11-24 20:52:20,994:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7576 (13.5555)
2022-11-24 20:52:21,013:INFO: Dataset: zara2               Batch: 15/18	Loss 15.2510 (13.6750)
2022-11-24 20:52:21,034:INFO: Dataset: zara2               Batch: 16/18	Loss 14.2427 (13.7105)
2022-11-24 20:52:21,054:INFO: Dataset: zara2               Batch: 17/18	Loss 15.7551 (13.8282)
2022-11-24 20:52:21,071:INFO: Dataset: zara2               Batch: 18/18	Loss 11.8472 (13.7361)
2022-11-24 20:52:21,112:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_6.pth.tar
2022-11-24 20:52:21,112:INFO: 
===> EPOCH: 7 (P1)
2022-11-24 20:52:21,112:INFO: - Computing loss (training)
2022-11-24 20:52:21,287:INFO: Dataset: hotel               Batch: 1/4	Loss 12.1727 (12.1727)
2022-11-24 20:52:21,308:INFO: Dataset: hotel               Batch: 2/4	Loss 11.4720 (11.8109)
2022-11-24 20:52:21,328:INFO: Dataset: hotel               Batch: 3/4	Loss 10.5757 (11.4202)
2022-11-24 20:52:21,341:INFO: Dataset: hotel               Batch: 4/4	Loss 6.3432 (10.5361)
2022-11-24 20:52:21,549:INFO: Dataset: univ                Batch:  1/15	Loss 11.8572 (11.8572)
2022-11-24 20:52:21,592:INFO: Dataset: univ                Batch:  2/15	Loss 10.5135 (11.1545)
2022-11-24 20:52:21,614:INFO: Dataset: univ                Batch:  3/15	Loss 11.1380 (11.1490)
2022-11-24 20:52:21,634:INFO: Dataset: univ                Batch:  4/15	Loss 11.0863 (11.1327)
2022-11-24 20:52:21,655:INFO: Dataset: univ                Batch:  5/15	Loss 12.5305 (11.4174)
2022-11-24 20:52:21,678:INFO: Dataset: univ                Batch:  6/15	Loss 11.0277 (11.3545)
2022-11-24 20:52:21,699:INFO: Dataset: univ                Batch:  7/15	Loss 10.4034 (11.2131)
2022-11-24 20:52:21,719:INFO: Dataset: univ                Batch:  8/15	Loss 11.8957 (11.2950)
2022-11-24 20:52:21,739:INFO: Dataset: univ                Batch:  9/15	Loss 12.8308 (11.4754)
2022-11-24 20:52:21,760:INFO: Dataset: univ                Batch: 10/15	Loss 14.2818 (11.7242)
2022-11-24 20:52:21,781:INFO: Dataset: univ                Batch: 11/15	Loss 10.5188 (11.6137)
2022-11-24 20:52:21,803:INFO: Dataset: univ                Batch: 12/15	Loss 10.8198 (11.5425)
2022-11-24 20:52:21,823:INFO: Dataset: univ                Batch: 13/15	Loss 12.0214 (11.5789)
2022-11-24 20:52:21,845:INFO: Dataset: univ                Batch: 14/15	Loss 11.1433 (11.5465)
2022-11-24 20:52:21,854:INFO: Dataset: univ                Batch: 15/15	Loss 2.5341 (11.4256)
2022-11-24 20:52:22,051:INFO: Dataset: zara1               Batch: 1/8	Loss 16.6449 (16.6449)
2022-11-24 20:52:22,073:INFO: Dataset: zara1               Batch: 2/8	Loss 18.6225 (17.6337)
2022-11-24 20:52:22,092:INFO: Dataset: zara1               Batch: 3/8	Loss 19.3503 (18.2227)
2022-11-24 20:52:22,111:INFO: Dataset: zara1               Batch: 4/8	Loss 17.8160 (18.1209)
2022-11-24 20:52:22,131:INFO: Dataset: zara1               Batch: 5/8	Loss 17.5063 (17.9932)
2022-11-24 20:52:22,152:INFO: Dataset: zara1               Batch: 6/8	Loss 18.5385 (18.0807)
2022-11-24 20:52:22,170:INFO: Dataset: zara1               Batch: 7/8	Loss 18.2911 (18.1058)
2022-11-24 20:52:22,187:INFO: Dataset: zara1               Batch: 8/8	Loss 15.9256 (17.8820)
2022-11-24 20:52:22,389:INFO: Dataset: zara2               Batch:  1/18	Loss 12.7206 (12.7206)
2022-11-24 20:52:22,409:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3735 (13.0617)
2022-11-24 20:52:22,431:INFO: Dataset: zara2               Batch:  3/18	Loss 15.1145 (13.7091)
2022-11-24 20:52:22,452:INFO: Dataset: zara2               Batch:  4/18	Loss 13.6220 (13.6870)
2022-11-24 20:52:22,471:INFO: Dataset: zara2               Batch:  5/18	Loss 12.1753 (13.4005)
2022-11-24 20:52:22,492:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7773 (13.4633)
2022-11-24 20:52:22,511:INFO: Dataset: zara2               Batch:  7/18	Loss 14.6561 (13.6303)
2022-11-24 20:52:22,530:INFO: Dataset: zara2               Batch:  8/18	Loss 12.9563 (13.5484)
2022-11-24 20:52:22,549:INFO: Dataset: zara2               Batch:  9/18	Loss 12.8783 (13.4744)
2022-11-24 20:52:22,567:INFO: Dataset: zara2               Batch: 10/18	Loss 14.8081 (13.6006)
2022-11-24 20:52:22,589:INFO: Dataset: zara2               Batch: 11/18	Loss 13.9438 (13.6307)
2022-11-24 20:52:22,608:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1852 (13.5964)
2022-11-24 20:52:22,628:INFO: Dataset: zara2               Batch: 13/18	Loss 15.0180 (13.7065)
2022-11-24 20:52:22,647:INFO: Dataset: zara2               Batch: 14/18	Loss 11.7128 (13.5617)
2022-11-24 20:52:22,666:INFO: Dataset: zara2               Batch: 15/18	Loss 13.9082 (13.5844)
2022-11-24 20:52:22,686:INFO: Dataset: zara2               Batch: 16/18	Loss 14.4017 (13.6367)
2022-11-24 20:52:22,707:INFO: Dataset: zara2               Batch: 17/18	Loss 13.7365 (13.6428)
2022-11-24 20:52:22,725:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6471 (13.5392)
2022-11-24 20:52:22,766:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_7.pth.tar
2022-11-24 20:52:22,766:INFO: 
===> EPOCH: 8 (P1)
2022-11-24 20:52:22,767:INFO: - Computing loss (training)
2022-11-24 20:52:22,927:INFO: Dataset: hotel               Batch: 1/4	Loss 9.6770 (9.6770)
2022-11-24 20:52:22,946:INFO: Dataset: hotel               Batch: 2/4	Loss 12.4849 (11.0575)
2022-11-24 20:52:22,966:INFO: Dataset: hotel               Batch: 3/4	Loss 10.1622 (10.7558)
2022-11-24 20:52:22,979:INFO: Dataset: hotel               Batch: 4/4	Loss 7.8206 (10.2679)
2022-11-24 20:52:23,183:INFO: Dataset: univ                Batch:  1/15	Loss 13.2452 (13.2452)
2022-11-24 20:52:23,206:INFO: Dataset: univ                Batch:  2/15	Loss 11.7157 (12.4842)
2022-11-24 20:52:23,228:INFO: Dataset: univ                Batch:  3/15	Loss 11.5281 (12.1677)
2022-11-24 20:52:23,251:INFO: Dataset: univ                Batch:  4/15	Loss 9.8959 (11.5294)
2022-11-24 20:52:23,277:INFO: Dataset: univ                Batch:  5/15	Loss 12.7779 (11.7939)
2022-11-24 20:52:23,308:INFO: Dataset: univ                Batch:  6/15	Loss 9.8466 (11.4487)
2022-11-24 20:52:23,330:INFO: Dataset: univ                Batch:  7/15	Loss 10.6965 (11.3450)
2022-11-24 20:52:23,353:INFO: Dataset: univ                Batch:  8/15	Loss 11.0044 (11.3053)
2022-11-24 20:52:23,375:INFO: Dataset: univ                Batch:  9/15	Loss 12.2036 (11.4006)
2022-11-24 20:52:23,397:INFO: Dataset: univ                Batch: 10/15	Loss 10.3098 (11.2999)
2022-11-24 20:52:23,420:INFO: Dataset: univ                Batch: 11/15	Loss 10.1295 (11.1815)
2022-11-24 20:52:23,445:INFO: Dataset: univ                Batch: 12/15	Loss 11.0597 (11.1712)
2022-11-24 20:52:23,468:INFO: Dataset: univ                Batch: 13/15	Loss 13.3851 (11.3315)
2022-11-24 20:52:23,490:INFO: Dataset: univ                Batch: 14/15	Loss 11.7724 (11.3611)
2022-11-24 20:52:23,498:INFO: Dataset: univ                Batch: 15/15	Loss 2.9418 (11.2689)
2022-11-24 20:52:23,714:INFO: Dataset: zara1               Batch: 1/8	Loss 17.7063 (17.7063)
2022-11-24 20:52:23,734:INFO: Dataset: zara1               Batch: 2/8	Loss 17.3838 (17.5325)
2022-11-24 20:52:23,754:INFO: Dataset: zara1               Batch: 3/8	Loss 19.7619 (18.2383)
2022-11-24 20:52:23,773:INFO: Dataset: zara1               Batch: 4/8	Loss 18.4640 (18.2926)
2022-11-24 20:52:23,793:INFO: Dataset: zara1               Batch: 5/8	Loss 18.6153 (18.3623)
2022-11-24 20:52:23,816:INFO: Dataset: zara1               Batch: 6/8	Loss 18.3249 (18.3562)
2022-11-24 20:52:23,837:INFO: Dataset: zara1               Batch: 7/8	Loss 17.2293 (18.1770)
2022-11-24 20:52:23,854:INFO: Dataset: zara1               Batch: 8/8	Loss 14.4106 (17.8024)
2022-11-24 20:52:24,083:INFO: Dataset: zara2               Batch:  1/18	Loss 14.2810 (14.2810)
2022-11-24 20:52:24,104:INFO: Dataset: zara2               Batch:  2/18	Loss 14.4897 (14.3785)
2022-11-24 20:52:24,124:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9605 (14.2554)
2022-11-24 20:52:24,146:INFO: Dataset: zara2               Batch:  4/18	Loss 11.3489 (13.5350)
2022-11-24 20:52:24,171:INFO: Dataset: zara2               Batch:  5/18	Loss 14.3630 (13.6804)
2022-11-24 20:52:24,193:INFO: Dataset: zara2               Batch:  6/18	Loss 11.5156 (13.2991)
2022-11-24 20:52:24,212:INFO: Dataset: zara2               Batch:  7/18	Loss 13.0650 (13.2644)
2022-11-24 20:52:24,231:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7996 (13.3351)
2022-11-24 20:52:24,250:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7567 (13.2730)
2022-11-24 20:52:24,269:INFO: Dataset: zara2               Batch: 10/18	Loss 13.9452 (13.3415)
2022-11-24 20:52:24,288:INFO: Dataset: zara2               Batch: 11/18	Loss 15.8637 (13.5486)
2022-11-24 20:52:24,309:INFO: Dataset: zara2               Batch: 12/18	Loss 12.2015 (13.4379)
2022-11-24 20:52:24,329:INFO: Dataset: zara2               Batch: 13/18	Loss 14.9265 (13.5417)
2022-11-24 20:52:24,349:INFO: Dataset: zara2               Batch: 14/18	Loss 13.6689 (13.5503)
2022-11-24 20:52:24,370:INFO: Dataset: zara2               Batch: 15/18	Loss 13.3352 (13.5373)
2022-11-24 20:52:24,391:INFO: Dataset: zara2               Batch: 16/18	Loss 11.4827 (13.3971)
2022-11-24 20:52:24,411:INFO: Dataset: zara2               Batch: 17/18	Loss 14.5585 (13.4660)
2022-11-24 20:52:24,429:INFO: Dataset: zara2               Batch: 18/18	Loss 11.2795 (13.3684)
2022-11-24 20:52:24,472:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_8.pth.tar
2022-11-24 20:52:24,472:INFO: 
===> EPOCH: 9 (P1)
2022-11-24 20:52:24,473:INFO: - Computing loss (training)
2022-11-24 20:52:24,632:INFO: Dataset: hotel               Batch: 1/4	Loss 11.0367 (11.0367)
2022-11-24 20:52:24,653:INFO: Dataset: hotel               Batch: 2/4	Loss 11.9133 (11.4638)
2022-11-24 20:52:24,673:INFO: Dataset: hotel               Batch: 3/4	Loss 9.7941 (10.9490)
2022-11-24 20:52:24,687:INFO: Dataset: hotel               Batch: 4/4	Loss 7.1376 (10.2853)
2022-11-24 20:52:24,898:INFO: Dataset: univ                Batch:  1/15	Loss 10.4125 (10.4125)
2022-11-24 20:52:24,922:INFO: Dataset: univ                Batch:  2/15	Loss 12.1455 (11.2799)
2022-11-24 20:52:24,947:INFO: Dataset: univ                Batch:  3/15	Loss 11.0524 (11.2046)
2022-11-24 20:52:24,968:INFO: Dataset: univ                Batch:  4/15	Loss 13.5244 (11.7521)
2022-11-24 20:52:24,989:INFO: Dataset: univ                Batch:  5/15	Loss 10.5545 (11.5136)
2022-11-24 20:52:25,013:INFO: Dataset: univ                Batch:  6/15	Loss 11.0741 (11.4362)
2022-11-24 20:52:25,034:INFO: Dataset: univ                Batch:  7/15	Loss 11.5812 (11.4580)
2022-11-24 20:52:25,055:INFO: Dataset: univ                Batch:  8/15	Loss 11.4091 (11.4516)
2022-11-24 20:52:25,076:INFO: Dataset: univ                Batch:  9/15	Loss 10.8540 (11.3837)
2022-11-24 20:52:25,098:INFO: Dataset: univ                Batch: 10/15	Loss 10.1358 (11.2393)
2022-11-24 20:52:25,120:INFO: Dataset: univ                Batch: 11/15	Loss 11.3590 (11.2511)
2022-11-24 20:52:25,143:INFO: Dataset: univ                Batch: 12/15	Loss 11.6467 (11.2828)
2022-11-24 20:52:25,164:INFO: Dataset: univ                Batch: 13/15	Loss 11.7755 (11.3187)
2022-11-24 20:52:25,186:INFO: Dataset: univ                Batch: 14/15	Loss 11.2692 (11.3153)
2022-11-24 20:52:25,195:INFO: Dataset: univ                Batch: 15/15	Loss 1.5762 (11.1676)
2022-11-24 20:52:25,406:INFO: Dataset: zara1               Batch: 1/8	Loss 16.5501 (16.5501)
2022-11-24 20:52:25,425:INFO: Dataset: zara1               Batch: 2/8	Loss 18.7260 (17.5965)
2022-11-24 20:52:25,445:INFO: Dataset: zara1               Batch: 3/8	Loss 17.1235 (17.4216)
2022-11-24 20:52:25,464:INFO: Dataset: zara1               Batch: 4/8	Loss 16.6412 (17.2152)
2022-11-24 20:52:25,484:INFO: Dataset: zara1               Batch: 5/8	Loss 18.4193 (17.4624)
2022-11-24 20:52:25,505:INFO: Dataset: zara1               Batch: 6/8	Loss 16.0042 (17.2198)
2022-11-24 20:52:25,524:INFO: Dataset: zara1               Batch: 7/8	Loss 19.4394 (17.5531)
2022-11-24 20:52:25,541:INFO: Dataset: zara1               Batch: 8/8	Loss 15.9391 (17.3874)
2022-11-24 20:52:25,749:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6772 (13.6772)
2022-11-24 20:52:25,770:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4901 (13.5887)
2022-11-24 20:52:25,791:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7294 (13.6379)
2022-11-24 20:52:25,812:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3117 (13.5581)
2022-11-24 20:52:25,834:INFO: Dataset: zara2               Batch:  5/18	Loss 13.1857 (13.4789)
2022-11-24 20:52:25,863:INFO: Dataset: zara2               Batch:  6/18	Loss 14.1668 (13.5968)
2022-11-24 20:52:25,884:INFO: Dataset: zara2               Batch:  7/18	Loss 12.1807 (13.3978)
2022-11-24 20:52:25,904:INFO: Dataset: zara2               Batch:  8/18	Loss 13.0935 (13.3616)
2022-11-24 20:52:25,924:INFO: Dataset: zara2               Batch:  9/18	Loss 14.4282 (13.4769)
2022-11-24 20:52:25,944:INFO: Dataset: zara2               Batch: 10/18	Loss 13.8820 (13.5159)
2022-11-24 20:52:25,964:INFO: Dataset: zara2               Batch: 11/18	Loss 14.3652 (13.5889)
2022-11-24 20:52:25,987:INFO: Dataset: zara2               Batch: 12/18	Loss 12.1866 (13.4717)
2022-11-24 20:52:26,008:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5844 (13.3995)
2022-11-24 20:52:26,028:INFO: Dataset: zara2               Batch: 14/18	Loss 14.7288 (13.4985)
2022-11-24 20:52:26,051:INFO: Dataset: zara2               Batch: 15/18	Loss 13.1509 (13.4733)
2022-11-24 20:52:26,072:INFO: Dataset: zara2               Batch: 16/18	Loss 12.6219 (13.4211)
2022-11-24 20:52:26,094:INFO: Dataset: zara2               Batch: 17/18	Loss 12.4220 (13.3591)
2022-11-24 20:52:26,113:INFO: Dataset: zara2               Batch: 18/18	Loss 11.1332 (13.2447)
2022-11-24 20:52:26,155:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_9.pth.tar
2022-11-24 20:52:26,155:INFO: 
===> EPOCH: 10 (P1)
2022-11-24 20:52:26,156:INFO: - Computing loss (training)
2022-11-24 20:52:26,343:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9213 (9.9213)
2022-11-24 20:52:26,364:INFO: Dataset: hotel               Batch: 2/4	Loss 8.5752 (9.2676)
2022-11-24 20:52:26,385:INFO: Dataset: hotel               Batch: 3/4	Loss 12.9398 (10.5073)
2022-11-24 20:52:26,400:INFO: Dataset: hotel               Batch: 4/4	Loss 8.4051 (10.1467)
2022-11-24 20:52:26,664:INFO: Dataset: univ                Batch:  1/15	Loss 12.0255 (12.0255)
2022-11-24 20:52:26,692:INFO: Dataset: univ                Batch:  2/15	Loss 11.6838 (11.8532)
2022-11-24 20:52:26,718:INFO: Dataset: univ                Batch:  3/15	Loss 12.4608 (12.0510)
2022-11-24 20:52:26,741:INFO: Dataset: univ                Batch:  4/15	Loss 10.9613 (11.7496)
2022-11-24 20:52:26,763:INFO: Dataset: univ                Batch:  5/15	Loss 10.9967 (11.6025)
2022-11-24 20:52:26,789:INFO: Dataset: univ                Batch:  6/15	Loss 9.9512 (11.3002)
2022-11-24 20:52:26,811:INFO: Dataset: univ                Batch:  7/15	Loss 10.0794 (11.1207)
2022-11-24 20:52:26,832:INFO: Dataset: univ                Batch:  8/15	Loss 12.3846 (11.2669)
2022-11-24 20:52:26,854:INFO: Dataset: univ                Batch:  9/15	Loss 12.4559 (11.3961)
2022-11-24 20:52:26,876:INFO: Dataset: univ                Batch: 10/15	Loss 12.8720 (11.5433)
2022-11-24 20:52:26,898:INFO: Dataset: univ                Batch: 11/15	Loss 10.6345 (11.4554)
2022-11-24 20:52:26,923:INFO: Dataset: univ                Batch: 12/15	Loss 9.5911 (11.2890)
2022-11-24 20:52:26,971:INFO: Dataset: univ                Batch: 13/15	Loss 9.8369 (11.1701)
2022-11-24 20:52:27,013:INFO: Dataset: univ                Batch: 14/15	Loss 10.3319 (11.1112)
2022-11-24 20:52:27,030:INFO: Dataset: univ                Batch: 15/15	Loss 2.3268 (11.0058)
2022-11-24 20:52:27,315:INFO: Dataset: zara1               Batch: 1/8	Loss 18.3720 (18.3720)
2022-11-24 20:52:27,337:INFO: Dataset: zara1               Batch: 2/8	Loss 16.3083 (17.3970)
2022-11-24 20:52:27,358:INFO: Dataset: zara1               Batch: 3/8	Loss 17.1320 (17.3125)
2022-11-24 20:52:27,383:INFO: Dataset: zara1               Batch: 4/8	Loss 16.2519 (17.0543)
2022-11-24 20:52:27,407:INFO: Dataset: zara1               Batch: 5/8	Loss 18.8586 (17.3968)
2022-11-24 20:52:27,427:INFO: Dataset: zara1               Batch: 6/8	Loss 17.9854 (17.4938)
2022-11-24 20:52:27,446:INFO: Dataset: zara1               Batch: 7/8	Loss 17.3142 (17.4662)
2022-11-24 20:52:27,463:INFO: Dataset: zara1               Batch: 8/8	Loss 15.1575 (17.2487)
2022-11-24 20:52:27,710:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7893 (13.7893)
2022-11-24 20:52:27,737:INFO: Dataset: zara2               Batch:  2/18	Loss 13.0200 (13.4400)
2022-11-24 20:52:27,757:INFO: Dataset: zara2               Batch:  3/18	Loss 14.5474 (13.7987)
2022-11-24 20:52:27,776:INFO: Dataset: zara2               Batch:  4/18	Loss 11.3154 (13.2005)
2022-11-24 20:52:27,796:INFO: Dataset: zara2               Batch:  5/18	Loss 11.6305 (12.9090)
2022-11-24 20:52:27,818:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7176 (13.0523)
2022-11-24 20:52:27,837:INFO: Dataset: zara2               Batch:  7/18	Loss 12.7133 (13.0057)
2022-11-24 20:52:27,857:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7177 (13.0940)
2022-11-24 20:52:27,876:INFO: Dataset: zara2               Batch:  9/18	Loss 14.0542 (13.2106)
2022-11-24 20:52:27,895:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5204 (13.2386)
2022-11-24 20:52:27,914:INFO: Dataset: zara2               Batch: 11/18	Loss 14.6349 (13.3521)
2022-11-24 20:52:27,935:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5419 (13.2859)
2022-11-24 20:52:27,955:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9007 (13.2577)
2022-11-24 20:52:27,976:INFO: Dataset: zara2               Batch: 14/18	Loss 13.5684 (13.2792)
2022-11-24 20:52:27,997:INFO: Dataset: zara2               Batch: 15/18	Loss 14.3569 (13.3470)
2022-11-24 20:52:28,018:INFO: Dataset: zara2               Batch: 16/18	Loss 12.3007 (13.2816)
2022-11-24 20:52:28,038:INFO: Dataset: zara2               Batch: 17/18	Loss 12.3710 (13.2230)
2022-11-24 20:52:28,057:INFO: Dataset: zara2               Batch: 18/18	Loss 10.6195 (13.1081)
2022-11-24 20:52:28,098:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_10.pth.tar
2022-11-24 20:52:28,098:INFO: 
===> EPOCH: 11 (P1)
2022-11-24 20:52:28,099:INFO: - Computing loss (training)
2022-11-24 20:52:28,272:INFO: Dataset: hotel               Batch: 1/4	Loss 11.0695 (11.0695)
2022-11-24 20:52:28,293:INFO: Dataset: hotel               Batch: 2/4	Loss 10.9560 (11.0126)
2022-11-24 20:52:28,314:INFO: Dataset: hotel               Batch: 3/4	Loss 11.5720 (11.1928)
2022-11-24 20:52:28,329:INFO: Dataset: hotel               Batch: 4/4	Loss 5.9606 (10.2471)
2022-11-24 20:52:28,536:INFO: Dataset: univ                Batch:  1/15	Loss 10.4939 (10.4939)
2022-11-24 20:52:28,560:INFO: Dataset: univ                Batch:  2/15	Loss 11.5941 (11.0370)
2022-11-24 20:52:28,582:INFO: Dataset: univ                Batch:  3/15	Loss 10.5625 (10.8816)
2022-11-24 20:52:28,606:INFO: Dataset: univ                Batch:  4/15	Loss 13.3740 (11.4385)
2022-11-24 20:52:28,628:INFO: Dataset: univ                Batch:  5/15	Loss 11.1884 (11.3862)
2022-11-24 20:52:28,651:INFO: Dataset: univ                Batch:  6/15	Loss 9.7280 (11.1149)
2022-11-24 20:52:28,672:INFO: Dataset: univ                Batch:  7/15	Loss 10.4616 (11.0256)
2022-11-24 20:52:28,692:INFO: Dataset: univ                Batch:  8/15	Loss 10.0460 (10.8963)
2022-11-24 20:52:28,713:INFO: Dataset: univ                Batch:  9/15	Loss 9.9057 (10.7820)
2022-11-24 20:52:28,733:INFO: Dataset: univ                Batch: 10/15	Loss 11.9942 (10.8929)
2022-11-24 20:52:28,756:INFO: Dataset: univ                Batch: 11/15	Loss 9.5462 (10.7550)
2022-11-24 20:52:28,780:INFO: Dataset: univ                Batch: 12/15	Loss 12.6412 (10.9047)
2022-11-24 20:52:28,801:INFO: Dataset: univ                Batch: 13/15	Loss 11.1550 (10.9236)
2022-11-24 20:52:28,823:INFO: Dataset: univ                Batch: 14/15	Loss 10.5381 (10.8936)
2022-11-24 20:52:28,832:INFO: Dataset: univ                Batch: 15/15	Loss 2.6208 (10.7866)
2022-11-24 20:52:29,047:INFO: Dataset: zara1               Batch: 1/8	Loss 17.5076 (17.5076)
2022-11-24 20:52:29,068:INFO: Dataset: zara1               Batch: 2/8	Loss 16.8943 (17.1955)
2022-11-24 20:52:29,088:INFO: Dataset: zara1               Batch: 3/8	Loss 18.9850 (17.7639)
2022-11-24 20:52:29,108:INFO: Dataset: zara1               Batch: 4/8	Loss 16.6355 (17.4815)
2022-11-24 20:52:29,129:INFO: Dataset: zara1               Batch: 5/8	Loss 16.7104 (17.3242)
2022-11-24 20:52:29,148:INFO: Dataset: zara1               Batch: 6/8	Loss 17.1488 (17.2972)
2022-11-24 20:52:29,167:INFO: Dataset: zara1               Batch: 7/8	Loss 16.8677 (17.2393)
2022-11-24 20:52:29,184:INFO: Dataset: zara1               Batch: 8/8	Loss 15.1001 (17.0153)
2022-11-24 20:52:29,395:INFO: Dataset: zara2               Batch:  1/18	Loss 12.9262 (12.9262)
2022-11-24 20:52:29,416:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5414 (13.2230)
2022-11-24 20:52:29,441:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3102 (13.2529)
2022-11-24 20:52:29,460:INFO: Dataset: zara2               Batch:  4/18	Loss 12.4168 (13.0350)
2022-11-24 20:52:29,480:INFO: Dataset: zara2               Batch:  5/18	Loss 11.4670 (12.7633)
2022-11-24 20:52:29,502:INFO: Dataset: zara2               Batch:  6/18	Loss 12.8521 (12.7796)
2022-11-24 20:52:29,522:INFO: Dataset: zara2               Batch:  7/18	Loss 12.6433 (12.7601)
2022-11-24 20:52:29,542:INFO: Dataset: zara2               Batch:  8/18	Loss 12.2452 (12.7003)
2022-11-24 20:52:29,561:INFO: Dataset: zara2               Batch:  9/18	Loss 13.9978 (12.8457)
2022-11-24 20:52:29,581:INFO: Dataset: zara2               Batch: 10/18	Loss 14.0426 (12.9540)
2022-11-24 20:52:29,603:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0349 (12.9609)
2022-11-24 20:52:29,623:INFO: Dataset: zara2               Batch: 12/18	Loss 13.4666 (13.0089)
2022-11-24 20:52:29,643:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4249 (12.9643)
2022-11-24 20:52:29,663:INFO: Dataset: zara2               Batch: 14/18	Loss 13.0922 (12.9728)
2022-11-24 20:52:29,685:INFO: Dataset: zara2               Batch: 15/18	Loss 13.5099 (13.0050)
2022-11-24 20:52:29,706:INFO: Dataset: zara2               Batch: 16/18	Loss 12.7048 (12.9877)
2022-11-24 20:52:29,726:INFO: Dataset: zara2               Batch: 17/18	Loss 12.9422 (12.9850)
2022-11-24 20:52:29,745:INFO: Dataset: zara2               Batch: 18/18	Loss 10.8876 (12.8814)
2022-11-24 20:52:29,790:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_11.pth.tar
2022-11-24 20:52:29,790:INFO: 
===> EPOCH: 12 (P1)
2022-11-24 20:52:29,790:INFO: - Computing loss (training)
2022-11-24 20:52:29,954:INFO: Dataset: hotel               Batch: 1/4	Loss 12.1104 (12.1104)
2022-11-24 20:52:29,973:INFO: Dataset: hotel               Batch: 2/4	Loss 10.4903 (11.2596)
2022-11-24 20:52:29,992:INFO: Dataset: hotel               Batch: 3/4	Loss 10.0851 (10.8613)
2022-11-24 20:52:30,006:INFO: Dataset: hotel               Batch: 4/4	Loss 6.5322 (10.1360)
2022-11-24 20:52:30,220:INFO: Dataset: univ                Batch:  1/15	Loss 10.5794 (10.5794)
2022-11-24 20:52:30,242:INFO: Dataset: univ                Batch:  2/15	Loss 13.0078 (11.7629)
2022-11-24 20:52:30,265:INFO: Dataset: univ                Batch:  3/15	Loss 10.7041 (11.3610)
2022-11-24 20:52:30,287:INFO: Dataset: univ                Batch:  4/15	Loss 11.2782 (11.3408)
2022-11-24 20:52:30,308:INFO: Dataset: univ                Batch:  5/15	Loss 11.6740 (11.4083)
2022-11-24 20:52:30,330:INFO: Dataset: univ                Batch:  6/15	Loss 10.3075 (11.2190)
2022-11-24 20:52:30,350:INFO: Dataset: univ                Batch:  7/15	Loss 9.4722 (10.9601)
2022-11-24 20:52:30,371:INFO: Dataset: univ                Batch:  8/15	Loss 11.1912 (10.9895)
2022-11-24 20:52:30,391:INFO: Dataset: univ                Batch:  9/15	Loss 11.7818 (11.0770)
2022-11-24 20:52:30,411:INFO: Dataset: univ                Batch: 10/15	Loss 10.7182 (11.0387)
2022-11-24 20:52:30,434:INFO: Dataset: univ                Batch: 11/15	Loss 9.8478 (10.9301)
2022-11-24 20:52:30,455:INFO: Dataset: univ                Batch: 12/15	Loss 10.8770 (10.9254)
2022-11-24 20:52:30,477:INFO: Dataset: univ                Batch: 13/15	Loss 10.8885 (10.9227)
2022-11-24 20:52:30,497:INFO: Dataset: univ                Batch: 14/15	Loss 10.6137 (10.9016)
2022-11-24 20:52:30,506:INFO: Dataset: univ                Batch: 15/15	Loss 1.8859 (10.7897)
2022-11-24 20:52:30,711:INFO: Dataset: zara1               Batch: 1/8	Loss 18.1482 (18.1482)
2022-11-24 20:52:30,732:INFO: Dataset: zara1               Batch: 2/8	Loss 17.6486 (17.8703)
2022-11-24 20:52:30,752:INFO: Dataset: zara1               Batch: 3/8	Loss 16.3517 (17.3621)
2022-11-24 20:52:30,774:INFO: Dataset: zara1               Batch: 4/8	Loss 16.8874 (17.2577)
2022-11-24 20:52:30,796:INFO: Dataset: zara1               Batch: 5/8	Loss 17.8474 (17.3740)
2022-11-24 20:52:30,816:INFO: Dataset: zara1               Batch: 6/8	Loss 16.1817 (17.1681)
2022-11-24 20:52:30,836:INFO: Dataset: zara1               Batch: 7/8	Loss 16.4384 (17.0709)
2022-11-24 20:52:30,854:INFO: Dataset: zara1               Batch: 8/8	Loss 14.7730 (16.8254)
2022-11-24 20:52:31,065:INFO: Dataset: zara2               Batch:  1/18	Loss 11.9696 (11.9696)
2022-11-24 20:52:31,085:INFO: Dataset: zara2               Batch:  2/18	Loss 12.5347 (12.2496)
2022-11-24 20:52:31,107:INFO: Dataset: zara2               Batch:  3/18	Loss 14.9297 (13.1212)
2022-11-24 20:52:31,128:INFO: Dataset: zara2               Batch:  4/18	Loss 12.2681 (12.8835)
2022-11-24 20:52:31,148:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6473 (13.0330)
2022-11-24 20:52:31,171:INFO: Dataset: zara2               Batch:  6/18	Loss 12.7918 (12.9913)
2022-11-24 20:52:31,192:INFO: Dataset: zara2               Batch:  7/18	Loss 12.5640 (12.9266)
2022-11-24 20:52:31,211:INFO: Dataset: zara2               Batch:  8/18	Loss 12.0967 (12.8344)
2022-11-24 20:52:31,230:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6974 (12.9321)
2022-11-24 20:52:31,248:INFO: Dataset: zara2               Batch: 10/18	Loss 14.0219 (13.0397)
2022-11-24 20:52:31,269:INFO: Dataset: zara2               Batch: 11/18	Loss 12.6839 (13.0094)
2022-11-24 20:52:31,289:INFO: Dataset: zara2               Batch: 12/18	Loss 13.8222 (13.0786)
2022-11-24 20:52:31,309:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8833 (13.0635)
2022-11-24 20:52:31,328:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4125 (13.0180)
2022-11-24 20:52:31,349:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0547 (12.9533)
2022-11-24 20:52:31,369:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7249 (12.8675)
2022-11-24 20:52:31,390:INFO: Dataset: zara2               Batch: 17/18	Loss 12.2544 (12.8340)
2022-11-24 20:52:31,407:INFO: Dataset: zara2               Batch: 18/18	Loss 10.9921 (12.7506)
2022-11-24 20:52:31,443:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_12.pth.tar
2022-11-24 20:52:31,443:INFO: 
===> EPOCH: 13 (P1)
2022-11-24 20:52:31,443:INFO: - Computing loss (training)
2022-11-24 20:52:31,608:INFO: Dataset: hotel               Batch: 1/4	Loss 12.6153 (12.6153)
2022-11-24 20:52:31,629:INFO: Dataset: hotel               Batch: 2/4	Loss 10.0880 (11.3191)
2022-11-24 20:52:31,649:INFO: Dataset: hotel               Batch: 3/4	Loss 11.7926 (11.4675)
2022-11-24 20:52:31,664:INFO: Dataset: hotel               Batch: 4/4	Loss 4.9446 (10.2972)
2022-11-24 20:52:31,875:INFO: Dataset: univ                Batch:  1/15	Loss 8.6895 (8.6895)
2022-11-24 20:52:31,896:INFO: Dataset: univ                Batch:  2/15	Loss 12.3405 (10.3469)
2022-11-24 20:52:31,919:INFO: Dataset: univ                Batch:  3/15	Loss 9.9255 (10.1986)
2022-11-24 20:52:31,942:INFO: Dataset: univ                Batch:  4/15	Loss 11.4725 (10.5166)
2022-11-24 20:52:31,964:INFO: Dataset: univ                Batch:  5/15	Loss 10.2230 (10.4597)
2022-11-24 20:52:31,989:INFO: Dataset: univ                Batch:  6/15	Loss 10.3696 (10.4446)
2022-11-24 20:52:32,009:INFO: Dataset: univ                Batch:  7/15	Loss 10.9260 (10.5145)
2022-11-24 20:52:32,030:INFO: Dataset: univ                Batch:  8/15	Loss 11.0575 (10.5820)
2022-11-24 20:52:32,050:INFO: Dataset: univ                Batch:  9/15	Loss 11.4588 (10.6700)
2022-11-24 20:52:32,071:INFO: Dataset: univ                Batch: 10/15	Loss 10.6689 (10.6699)
2022-11-24 20:52:32,093:INFO: Dataset: univ                Batch: 11/15	Loss 9.9768 (10.6044)
2022-11-24 20:52:32,116:INFO: Dataset: univ                Batch: 12/15	Loss 13.1472 (10.8006)
2022-11-24 20:52:32,138:INFO: Dataset: univ                Batch: 13/15	Loss 9.8069 (10.7171)
2022-11-24 20:52:32,161:INFO: Dataset: univ                Batch: 14/15	Loss 10.9364 (10.7314)
2022-11-24 20:52:32,171:INFO: Dataset: univ                Batch: 15/15	Loss 2.1521 (10.6184)
2022-11-24 20:52:32,385:INFO: Dataset: zara1               Batch: 1/8	Loss 17.2678 (17.2678)
2022-11-24 20:52:32,405:INFO: Dataset: zara1               Batch: 2/8	Loss 15.0562 (16.2499)
2022-11-24 20:52:32,425:INFO: Dataset: zara1               Batch: 3/8	Loss 16.1823 (16.2264)
2022-11-24 20:52:32,446:INFO: Dataset: zara1               Batch: 4/8	Loss 17.4158 (16.4956)
2022-11-24 20:52:32,467:INFO: Dataset: zara1               Batch: 5/8	Loss 17.7810 (16.7561)
2022-11-24 20:52:32,488:INFO: Dataset: zara1               Batch: 6/8	Loss 16.8268 (16.7687)
2022-11-24 20:52:32,507:INFO: Dataset: zara1               Batch: 7/8	Loss 15.9329 (16.6586)
2022-11-24 20:52:32,524:INFO: Dataset: zara1               Batch: 8/8	Loss 15.0147 (16.4925)
2022-11-24 20:52:32,738:INFO: Dataset: zara2               Batch:  1/18	Loss 13.1040 (13.1040)
2022-11-24 20:52:32,760:INFO: Dataset: zara2               Batch:  2/18	Loss 12.7388 (12.9323)
2022-11-24 20:52:32,780:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5985 (13.1618)
2022-11-24 20:52:32,799:INFO: Dataset: zara2               Batch:  4/18	Loss 12.1974 (12.9437)
2022-11-24 20:52:32,822:INFO: Dataset: zara2               Batch:  5/18	Loss 12.8917 (12.9331)
2022-11-24 20:52:32,844:INFO: Dataset: zara2               Batch:  6/18	Loss 13.1821 (12.9734)
2022-11-24 20:52:32,863:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2327 (13.0048)
2022-11-24 20:52:32,881:INFO: Dataset: zara2               Batch:  8/18	Loss 12.5817 (12.9597)
2022-11-24 20:52:32,900:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7324 (12.9320)
2022-11-24 20:52:32,920:INFO: Dataset: zara2               Batch: 10/18	Loss 12.6207 (12.8989)
2022-11-24 20:52:32,941:INFO: Dataset: zara2               Batch: 11/18	Loss 11.0840 (12.7339)
2022-11-24 20:52:32,960:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7773 (12.7376)
2022-11-24 20:52:32,980:INFO: Dataset: zara2               Batch: 13/18	Loss 12.1686 (12.6932)
2022-11-24 20:52:32,999:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7570 (12.6976)
2022-11-24 20:52:33,019:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3065 (12.6700)
2022-11-24 20:52:33,038:INFO: Dataset: zara2               Batch: 16/18	Loss 12.3958 (12.6530)
2022-11-24 20:52:33,058:INFO: Dataset: zara2               Batch: 17/18	Loss 12.7429 (12.6580)
2022-11-24 20:52:33,077:INFO: Dataset: zara2               Batch: 18/18	Loss 10.2735 (12.5351)
2022-11-24 20:52:33,119:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_13.pth.tar
2022-11-24 20:52:33,119:INFO: 
===> EPOCH: 14 (P1)
2022-11-24 20:52:33,119:INFO: - Computing loss (training)
2022-11-24 20:52:33,312:INFO: Dataset: hotel               Batch: 1/4	Loss 10.3986 (10.3986)
2022-11-24 20:52:33,337:INFO: Dataset: hotel               Batch: 2/4	Loss 12.8031 (11.5433)
2022-11-24 20:52:33,359:INFO: Dataset: hotel               Batch: 3/4	Loss 10.7225 (11.2715)
2022-11-24 20:52:33,375:INFO: Dataset: hotel               Batch: 4/4	Loss 5.3578 (10.2339)
2022-11-24 20:52:33,585:INFO: Dataset: univ                Batch:  1/15	Loss 9.7193 (9.7193)
2022-11-24 20:52:33,612:INFO: Dataset: univ                Batch:  2/15	Loss 10.4517 (10.0924)
2022-11-24 20:52:33,633:INFO: Dataset: univ                Batch:  3/15	Loss 12.0126 (10.7115)
2022-11-24 20:52:33,654:INFO: Dataset: univ                Batch:  4/15	Loss 9.7856 (10.4760)
2022-11-24 20:52:33,676:INFO: Dataset: univ                Batch:  5/15	Loss 9.6569 (10.3036)
2022-11-24 20:52:33,699:INFO: Dataset: univ                Batch:  6/15	Loss 10.8847 (10.3961)
2022-11-24 20:52:33,720:INFO: Dataset: univ                Batch:  7/15	Loss 10.2966 (10.3816)
2022-11-24 20:52:33,741:INFO: Dataset: univ                Batch:  8/15	Loss 11.4283 (10.5032)
2022-11-24 20:52:33,761:INFO: Dataset: univ                Batch:  9/15	Loss 11.3954 (10.5986)
2022-11-24 20:52:33,782:INFO: Dataset: univ                Batch: 10/15	Loss 10.5265 (10.5914)
2022-11-24 20:52:33,805:INFO: Dataset: univ                Batch: 11/15	Loss 10.5374 (10.5866)
2022-11-24 20:52:33,826:INFO: Dataset: univ                Batch: 12/15	Loss 9.7220 (10.5146)
2022-11-24 20:52:33,847:INFO: Dataset: univ                Batch: 13/15	Loss 10.5625 (10.5181)
2022-11-24 20:52:33,868:INFO: Dataset: univ                Batch: 14/15	Loss 11.8971 (10.6084)
2022-11-24 20:52:33,877:INFO: Dataset: univ                Batch: 15/15	Loss 2.0850 (10.5030)
2022-11-24 20:52:34,090:INFO: Dataset: zara1               Batch: 1/8	Loss 15.5447 (15.5447)
2022-11-24 20:52:34,109:INFO: Dataset: zara1               Batch: 2/8	Loss 17.0637 (16.3141)
2022-11-24 20:52:34,128:INFO: Dataset: zara1               Batch: 3/8	Loss 17.5033 (16.7490)
2022-11-24 20:52:34,146:INFO: Dataset: zara1               Batch: 4/8	Loss 16.0348 (16.5653)
2022-11-24 20:52:34,165:INFO: Dataset: zara1               Batch: 5/8	Loss 15.9842 (16.4468)
2022-11-24 20:52:34,185:INFO: Dataset: zara1               Batch: 6/8	Loss 15.7555 (16.3313)
2022-11-24 20:52:34,204:INFO: Dataset: zara1               Batch: 7/8	Loss 18.6111 (16.6520)
2022-11-24 20:52:34,221:INFO: Dataset: zara1               Batch: 8/8	Loss 13.6345 (16.3439)
2022-11-24 20:52:34,423:INFO: Dataset: zara2               Batch:  1/18	Loss 12.8980 (12.8980)
2022-11-24 20:52:34,442:INFO: Dataset: zara2               Batch:  2/18	Loss 13.0722 (12.9929)
2022-11-24 20:52:34,461:INFO: Dataset: zara2               Batch:  3/18	Loss 12.8873 (12.9548)
2022-11-24 20:52:34,480:INFO: Dataset: zara2               Batch:  4/18	Loss 12.5006 (12.8424)
2022-11-24 20:52:34,501:INFO: Dataset: zara2               Batch:  5/18	Loss 11.1952 (12.4880)
2022-11-24 20:52:34,525:INFO: Dataset: zara2               Batch:  6/18	Loss 11.6681 (12.3544)
2022-11-24 20:52:34,544:INFO: Dataset: zara2               Batch:  7/18	Loss 11.3590 (12.2167)
2022-11-24 20:52:34,564:INFO: Dataset: zara2               Batch:  8/18	Loss 12.2435 (12.2199)
2022-11-24 20:52:34,584:INFO: Dataset: zara2               Batch:  9/18	Loss 11.4956 (12.1394)
2022-11-24 20:52:34,604:INFO: Dataset: zara2               Batch: 10/18	Loss 12.4344 (12.1688)
2022-11-24 20:52:34,623:INFO: Dataset: zara2               Batch: 11/18	Loss 12.1833 (12.1702)
2022-11-24 20:52:34,645:INFO: Dataset: zara2               Batch: 12/18	Loss 13.6386 (12.2835)
2022-11-24 20:52:34,665:INFO: Dataset: zara2               Batch: 13/18	Loss 11.9344 (12.2560)
2022-11-24 20:52:34,685:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1858 (12.2511)
2022-11-24 20:52:34,705:INFO: Dataset: zara2               Batch: 15/18	Loss 11.8568 (12.2247)
2022-11-24 20:52:34,727:INFO: Dataset: zara2               Batch: 16/18	Loss 12.9007 (12.2649)
2022-11-24 20:52:34,749:INFO: Dataset: zara2               Batch: 17/18	Loss 14.3123 (12.3917)
2022-11-24 20:52:34,768:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6061 (12.3547)
2022-11-24 20:52:34,804:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_14.pth.tar
2022-11-24 20:52:34,804:INFO: 
===> EPOCH: 15 (P1)
2022-11-24 20:52:34,805:INFO: - Computing loss (training)
2022-11-24 20:52:34,979:INFO: Dataset: hotel               Batch: 1/4	Loss 11.1054 (11.1054)
2022-11-24 20:52:35,000:INFO: Dataset: hotel               Batch: 2/4	Loss 9.8129 (10.4715)
2022-11-24 20:52:35,020:INFO: Dataset: hotel               Batch: 3/4	Loss 10.3287 (10.4242)
2022-11-24 20:52:35,034:INFO: Dataset: hotel               Batch: 4/4	Loss 7.6705 (9.9519)
2022-11-24 20:52:35,243:INFO: Dataset: univ                Batch:  1/15	Loss 10.7282 (10.7282)
2022-11-24 20:52:35,265:INFO: Dataset: univ                Batch:  2/15	Loss 9.3708 (10.0392)
2022-11-24 20:52:35,290:INFO: Dataset: univ                Batch:  3/15	Loss 8.8969 (9.6333)
2022-11-24 20:52:35,311:INFO: Dataset: univ                Batch:  4/15	Loss 11.5857 (10.1025)
2022-11-24 20:52:35,333:INFO: Dataset: univ                Batch:  5/15	Loss 11.5510 (10.3543)
2022-11-24 20:52:35,358:INFO: Dataset: univ                Batch:  6/15	Loss 10.8108 (10.4307)
2022-11-24 20:52:35,378:INFO: Dataset: univ                Batch:  7/15	Loss 11.4893 (10.5804)
2022-11-24 20:52:35,399:INFO: Dataset: univ                Batch:  8/15	Loss 8.7385 (10.3211)
2022-11-24 20:52:35,419:INFO: Dataset: univ                Batch:  9/15	Loss 10.5968 (10.3515)
2022-11-24 20:52:35,439:INFO: Dataset: univ                Batch: 10/15	Loss 11.8341 (10.4869)
2022-11-24 20:52:35,460:INFO: Dataset: univ                Batch: 11/15	Loss 10.8056 (10.5152)
2022-11-24 20:52:35,483:INFO: Dataset: univ                Batch: 12/15	Loss 8.9777 (10.3794)
2022-11-24 20:52:35,504:INFO: Dataset: univ                Batch: 13/15	Loss 10.6707 (10.4012)
2022-11-24 20:52:35,525:INFO: Dataset: univ                Batch: 14/15	Loss 11.0665 (10.4520)
2022-11-24 20:52:35,534:INFO: Dataset: univ                Batch: 15/15	Loss 2.2490 (10.3587)
2022-11-24 20:52:35,744:INFO: Dataset: zara1               Batch: 1/8	Loss 16.9027 (16.9027)
2022-11-24 20:52:35,763:INFO: Dataset: zara1               Batch: 2/8	Loss 16.5229 (16.7081)
2022-11-24 20:52:35,782:INFO: Dataset: zara1               Batch: 3/8	Loss 16.7898 (16.7349)
2022-11-24 20:52:35,801:INFO: Dataset: zara1               Batch: 4/8	Loss 15.6682 (16.4640)
2022-11-24 20:52:35,821:INFO: Dataset: zara1               Batch: 5/8	Loss 15.9638 (16.3588)
2022-11-24 20:52:35,843:INFO: Dataset: zara1               Batch: 6/8	Loss 15.7287 (16.2504)
2022-11-24 20:52:35,862:INFO: Dataset: zara1               Batch: 7/8	Loss 17.7213 (16.4381)
2022-11-24 20:52:35,879:INFO: Dataset: zara1               Batch: 8/8	Loss 12.7069 (16.0158)
2022-11-24 20:52:36,098:INFO: Dataset: zara2               Batch:  1/18	Loss 11.3603 (11.3603)
2022-11-24 20:52:36,168:INFO: Dataset: zara2               Batch:  2/18	Loss 11.9652 (11.6589)
2022-11-24 20:52:36,188:INFO: Dataset: zara2               Batch:  3/18	Loss 11.3112 (11.5442)
2022-11-24 20:52:36,207:INFO: Dataset: zara2               Batch:  4/18	Loss 12.4610 (11.7615)
2022-11-24 20:52:36,226:INFO: Dataset: zara2               Batch:  5/18	Loss 11.0653 (11.6219)
2022-11-24 20:52:36,248:INFO: Dataset: zara2               Batch:  6/18	Loss 12.1713 (11.7150)
2022-11-24 20:52:36,267:INFO: Dataset: zara2               Batch:  7/18	Loss 11.7889 (11.7265)
2022-11-24 20:52:36,286:INFO: Dataset: zara2               Batch:  8/18	Loss 12.3761 (11.7954)
2022-11-24 20:52:36,306:INFO: Dataset: zara2               Batch:  9/18	Loss 11.5661 (11.7696)
2022-11-24 20:52:36,326:INFO: Dataset: zara2               Batch: 10/18	Loss 12.8837 (11.8727)
2022-11-24 20:52:36,348:INFO: Dataset: zara2               Batch: 11/18	Loss 12.5112 (11.9316)
2022-11-24 20:52:36,367:INFO: Dataset: zara2               Batch: 12/18	Loss 13.6121 (12.0498)
2022-11-24 20:52:36,387:INFO: Dataset: zara2               Batch: 13/18	Loss 13.3961 (12.1590)
2022-11-24 20:52:36,407:INFO: Dataset: zara2               Batch: 14/18	Loss 13.3822 (12.2564)
2022-11-24 20:52:36,428:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5954 (12.2784)
2022-11-24 20:52:36,448:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8067 (12.2501)
2022-11-24 20:52:36,468:INFO: Dataset: zara2               Batch: 17/18	Loss 13.1511 (12.3007)
2022-11-24 20:52:36,485:INFO: Dataset: zara2               Batch: 18/18	Loss 10.6712 (12.2158)
2022-11-24 20:52:36,530:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_15.pth.tar
2022-11-24 20:52:36,530:INFO: 
===> EPOCH: 16 (P1)
2022-11-24 20:52:36,530:INFO: - Computing loss (training)
2022-11-24 20:52:36,713:INFO: Dataset: hotel               Batch: 1/4	Loss 8.4770 (8.4770)
2022-11-24 20:52:36,735:INFO: Dataset: hotel               Batch: 2/4	Loss 13.8154 (11.1084)
2022-11-24 20:52:36,756:INFO: Dataset: hotel               Batch: 3/4	Loss 9.5134 (10.5869)
2022-11-24 20:52:36,770:INFO: Dataset: hotel               Batch: 4/4	Loss 7.1186 (10.0012)
2022-11-24 20:52:36,983:INFO: Dataset: univ                Batch:  1/15	Loss 11.9004 (11.9004)
2022-11-24 20:52:37,010:INFO: Dataset: univ                Batch:  2/15	Loss 10.0510 (10.9338)
2022-11-24 20:52:37,033:INFO: Dataset: univ                Batch:  3/15	Loss 9.3312 (10.3721)
2022-11-24 20:52:37,055:INFO: Dataset: univ                Batch:  4/15	Loss 10.4661 (10.3953)
2022-11-24 20:52:37,079:INFO: Dataset: univ                Batch:  5/15	Loss 9.9032 (10.2940)
2022-11-24 20:52:37,103:INFO: Dataset: univ                Batch:  6/15	Loss 9.8911 (10.2257)
2022-11-24 20:52:37,125:INFO: Dataset: univ                Batch:  7/15	Loss 11.5359 (10.4156)
2022-11-24 20:52:37,147:INFO: Dataset: univ                Batch:  8/15	Loss 12.0744 (10.6090)
2022-11-24 20:52:37,169:INFO: Dataset: univ                Batch:  9/15	Loss 10.0748 (10.5467)
2022-11-24 20:52:37,191:INFO: Dataset: univ                Batch: 10/15	Loss 9.0297 (10.3902)
2022-11-24 20:52:37,216:INFO: Dataset: univ                Batch: 11/15	Loss 11.0199 (10.4422)
2022-11-24 20:52:37,238:INFO: Dataset: univ                Batch: 12/15	Loss 11.3563 (10.5128)
2022-11-24 20:52:37,261:INFO: Dataset: univ                Batch: 13/15	Loss 9.0810 (10.3951)
2022-11-24 20:52:37,283:INFO: Dataset: univ                Batch: 14/15	Loss 10.4244 (10.3973)
2022-11-24 20:52:37,292:INFO: Dataset: univ                Batch: 15/15	Loss 1.7421 (10.2804)
2022-11-24 20:52:37,500:INFO: Dataset: zara1               Batch: 1/8	Loss 16.8635 (16.8635)
2022-11-24 20:52:37,521:INFO: Dataset: zara1               Batch: 2/8	Loss 16.3969 (16.6342)
2022-11-24 20:52:37,540:INFO: Dataset: zara1               Batch: 3/8	Loss 15.6262 (16.3064)
2022-11-24 20:52:37,560:INFO: Dataset: zara1               Batch: 4/8	Loss 15.7046 (16.1810)
2022-11-24 20:52:37,579:INFO: Dataset: zara1               Batch: 5/8	Loss 16.6508 (16.2688)
2022-11-24 20:52:37,601:INFO: Dataset: zara1               Batch: 6/8	Loss 15.6242 (16.1666)
2022-11-24 20:52:37,620:INFO: Dataset: zara1               Batch: 7/8	Loss 16.7990 (16.2606)
2022-11-24 20:52:37,637:INFO: Dataset: zara1               Batch: 8/8	Loss 12.7203 (15.8526)
2022-11-24 20:52:37,839:INFO: Dataset: zara2               Batch:  1/18	Loss 12.3990 (12.3990)
2022-11-24 20:52:37,860:INFO: Dataset: zara2               Batch:  2/18	Loss 11.7310 (12.0729)
2022-11-24 20:52:37,882:INFO: Dataset: zara2               Batch:  3/18	Loss 11.4499 (11.8773)
2022-11-24 20:52:37,900:INFO: Dataset: zara2               Batch:  4/18	Loss 13.0996 (12.1902)
2022-11-24 20:52:37,921:INFO: Dataset: zara2               Batch:  5/18	Loss 12.7822 (12.2999)
2022-11-24 20:52:37,942:INFO: Dataset: zara2               Batch:  6/18	Loss 12.4213 (12.3218)
2022-11-24 20:52:37,961:INFO: Dataset: zara2               Batch:  7/18	Loss 13.1437 (12.4385)
2022-11-24 20:52:37,980:INFO: Dataset: zara2               Batch:  8/18	Loss 12.1317 (12.3963)
2022-11-24 20:52:37,998:INFO: Dataset: zara2               Batch:  9/18	Loss 11.7959 (12.3333)
2022-11-24 20:52:38,017:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3007 (12.4212)
2022-11-24 20:52:38,037:INFO: Dataset: zara2               Batch: 11/18	Loss 11.6769 (12.3515)
2022-11-24 20:52:38,057:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0423 (12.4138)
2022-11-24 20:52:38,076:INFO: Dataset: zara2               Batch: 13/18	Loss 11.2316 (12.3283)
2022-11-24 20:52:38,097:INFO: Dataset: zara2               Batch: 14/18	Loss 10.7048 (12.2018)
2022-11-24 20:52:38,118:INFO: Dataset: zara2               Batch: 15/18	Loss 10.8350 (12.1017)
2022-11-24 20:52:38,138:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2221 (12.1094)
2022-11-24 20:52:38,159:INFO: Dataset: zara2               Batch: 17/18	Loss 12.8257 (12.1571)
2022-11-24 20:52:38,176:INFO: Dataset: zara2               Batch: 18/18	Loss 10.1368 (12.0739)
2022-11-24 20:52:38,212:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_16.pth.tar
2022-11-24 20:52:38,212:INFO: 
===> EPOCH: 17 (P1)
2022-11-24 20:52:38,213:INFO: - Computing loss (training)
2022-11-24 20:52:38,381:INFO: Dataset: hotel               Batch: 1/4	Loss 9.4670 (9.4670)
2022-11-24 20:52:38,401:INFO: Dataset: hotel               Batch: 2/4	Loss 10.9894 (10.2228)
2022-11-24 20:52:38,421:INFO: Dataset: hotel               Batch: 3/4	Loss 11.6317 (10.6843)
2022-11-24 20:52:38,436:INFO: Dataset: hotel               Batch: 4/4	Loss 6.9079 (10.0565)
2022-11-24 20:52:38,664:INFO: Dataset: univ                Batch:  1/15	Loss 10.9391 (10.9391)
2022-11-24 20:52:38,686:INFO: Dataset: univ                Batch:  2/15	Loss 11.0463 (10.9955)
2022-11-24 20:52:38,709:INFO: Dataset: univ                Batch:  3/15	Loss 10.5874 (10.8560)
2022-11-24 20:52:38,731:INFO: Dataset: univ                Batch:  4/15	Loss 9.3957 (10.4709)
2022-11-24 20:52:38,756:INFO: Dataset: univ                Batch:  5/15	Loss 10.0431 (10.3803)
2022-11-24 20:52:38,779:INFO: Dataset: univ                Batch:  6/15	Loss 10.2757 (10.3623)
2022-11-24 20:52:38,801:INFO: Dataset: univ                Batch:  7/15	Loss 11.1536 (10.4731)
2022-11-24 20:52:38,822:INFO: Dataset: univ                Batch:  8/15	Loss 10.6219 (10.4916)
2022-11-24 20:52:38,844:INFO: Dataset: univ                Batch:  9/15	Loss 9.8093 (10.4205)
2022-11-24 20:52:38,865:INFO: Dataset: univ                Batch: 10/15	Loss 9.8689 (10.3634)
2022-11-24 20:52:38,890:INFO: Dataset: univ                Batch: 11/15	Loss 13.0062 (10.5557)
2022-11-24 20:52:38,912:INFO: Dataset: univ                Batch: 12/15	Loss 9.0393 (10.4174)
2022-11-24 20:52:38,934:INFO: Dataset: univ                Batch: 13/15	Loss 9.5881 (10.3571)
2022-11-24 20:52:38,957:INFO: Dataset: univ                Batch: 14/15	Loss 8.8608 (10.2449)
2022-11-24 20:52:38,967:INFO: Dataset: univ                Batch: 15/15	Loss 1.4683 (10.0980)
2022-11-24 20:52:39,171:INFO: Dataset: zara1               Batch: 1/8	Loss 16.8202 (16.8202)
2022-11-24 20:52:39,190:INFO: Dataset: zara1               Batch: 2/8	Loss 17.0261 (16.9261)
2022-11-24 20:52:39,211:INFO: Dataset: zara1               Batch: 3/8	Loss 14.8414 (16.2408)
2022-11-24 20:52:39,231:INFO: Dataset: zara1               Batch: 4/8	Loss 16.0812 (16.2027)
2022-11-24 20:52:39,251:INFO: Dataset: zara1               Batch: 5/8	Loss 15.1108 (15.9557)
2022-11-24 20:52:39,271:INFO: Dataset: zara1               Batch: 6/8	Loss 17.1164 (16.1532)
2022-11-24 20:52:39,290:INFO: Dataset: zara1               Batch: 7/8	Loss 14.9399 (16.0013)
2022-11-24 20:52:39,307:INFO: Dataset: zara1               Batch: 8/8	Loss 13.0170 (15.6761)
2022-11-24 20:52:39,528:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5523 (13.5523)
2022-11-24 20:52:39,548:INFO: Dataset: zara2               Batch:  2/18	Loss 12.4835 (12.9788)
2022-11-24 20:52:39,567:INFO: Dataset: zara2               Batch:  3/18	Loss 11.5325 (12.5363)
2022-11-24 20:52:39,588:INFO: Dataset: zara2               Batch:  4/18	Loss 11.5673 (12.2764)
2022-11-24 20:52:39,610:INFO: Dataset: zara2               Batch:  5/18	Loss 11.7266 (12.1683)
2022-11-24 20:52:39,632:INFO: Dataset: zara2               Batch:  6/18	Loss 12.4274 (12.2107)
2022-11-24 20:52:39,651:INFO: Dataset: zara2               Batch:  7/18	Loss 12.7046 (12.2866)
2022-11-24 20:52:39,670:INFO: Dataset: zara2               Batch:  8/18	Loss 11.9985 (12.2549)
2022-11-24 20:52:39,688:INFO: Dataset: zara2               Batch:  9/18	Loss 12.2170 (12.2511)
2022-11-24 20:52:39,707:INFO: Dataset: zara2               Batch: 10/18	Loss 10.7537 (12.0843)
2022-11-24 20:52:39,728:INFO: Dataset: zara2               Batch: 11/18	Loss 11.2927 (12.0119)
2022-11-24 20:52:39,747:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8860 (12.0811)
2022-11-24 20:52:39,767:INFO: Dataset: zara2               Batch: 13/18	Loss 11.7131 (12.0568)
2022-11-24 20:52:39,786:INFO: Dataset: zara2               Batch: 14/18	Loss 11.4982 (12.0190)
2022-11-24 20:52:39,807:INFO: Dataset: zara2               Batch: 15/18	Loss 11.2279 (11.9681)
2022-11-24 20:52:39,827:INFO: Dataset: zara2               Batch: 16/18	Loss 11.2676 (11.9212)
2022-11-24 20:52:39,848:INFO: Dataset: zara2               Batch: 17/18	Loss 12.7578 (11.9730)
2022-11-24 20:52:39,866:INFO: Dataset: zara2               Batch: 18/18	Loss 9.4464 (11.8661)
2022-11-24 20:52:39,901:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_17.pth.tar
2022-11-24 20:52:39,901:INFO: 
===> EPOCH: 18 (P1)
2022-11-24 20:52:39,902:INFO: - Computing loss (training)
2022-11-24 20:52:40,069:INFO: Dataset: hotel               Batch: 1/4	Loss 11.9598 (11.9598)
2022-11-24 20:52:40,091:INFO: Dataset: hotel               Batch: 2/4	Loss 9.7553 (10.8291)
2022-11-24 20:52:40,111:INFO: Dataset: hotel               Batch: 3/4	Loss 10.1205 (10.5978)
2022-11-24 20:52:40,124:INFO: Dataset: hotel               Batch: 4/4	Loss 6.5573 (9.9368)
2022-11-24 20:52:40,329:INFO: Dataset: univ                Batch:  1/15	Loss 9.2704 (9.2704)
2022-11-24 20:52:40,354:INFO: Dataset: univ                Batch:  2/15	Loss 9.1906 (9.2302)
2022-11-24 20:52:40,377:INFO: Dataset: univ                Batch:  3/15	Loss 8.7906 (9.0865)
2022-11-24 20:52:40,401:INFO: Dataset: univ                Batch:  4/15	Loss 9.1017 (9.0904)
2022-11-24 20:52:40,426:INFO: Dataset: univ                Batch:  5/15	Loss 8.9137 (9.0534)
2022-11-24 20:52:40,451:INFO: Dataset: univ                Batch:  6/15	Loss 10.8794 (9.3263)
2022-11-24 20:52:40,473:INFO: Dataset: univ                Batch:  7/15	Loss 9.6194 (9.3661)
2022-11-24 20:52:40,494:INFO: Dataset: univ                Batch:  8/15	Loss 11.2067 (9.5720)
2022-11-24 20:52:40,516:INFO: Dataset: univ                Batch:  9/15	Loss 10.9289 (9.7021)
2022-11-24 20:52:40,538:INFO: Dataset: univ                Batch: 10/15	Loss 10.4366 (9.7700)
2022-11-24 20:52:40,563:INFO: Dataset: univ                Batch: 11/15	Loss 9.8741 (9.7798)
2022-11-24 20:52:40,586:INFO: Dataset: univ                Batch: 12/15	Loss 11.1305 (9.8867)
2022-11-24 20:52:40,609:INFO: Dataset: univ                Batch: 13/15	Loss 10.1259 (9.9053)
2022-11-24 20:52:40,631:INFO: Dataset: univ                Batch: 14/15	Loss 12.3067 (10.0525)
2022-11-24 20:52:40,640:INFO: Dataset: univ                Batch: 15/15	Loss 1.9028 (9.9363)
2022-11-24 20:52:40,851:INFO: Dataset: zara1               Batch: 1/8	Loss 16.3413 (16.3413)
2022-11-24 20:52:40,871:INFO: Dataset: zara1               Batch: 2/8	Loss 15.5701 (15.9518)
2022-11-24 20:52:40,892:INFO: Dataset: zara1               Batch: 3/8	Loss 14.4945 (15.4311)
2022-11-24 20:52:40,914:INFO: Dataset: zara1               Batch: 4/8	Loss 15.2582 (15.3908)
2022-11-24 20:52:40,935:INFO: Dataset: zara1               Batch: 5/8	Loss 15.4498 (15.4021)
2022-11-24 20:52:40,955:INFO: Dataset: zara1               Batch: 6/8	Loss 16.6990 (15.5924)
2022-11-24 20:52:40,975:INFO: Dataset: zara1               Batch: 7/8	Loss 14.9419 (15.4929)
2022-11-24 20:52:40,992:INFO: Dataset: zara1               Batch: 8/8	Loss 14.3248 (15.3736)
2022-11-24 20:52:41,230:INFO: Dataset: zara2               Batch:  1/18	Loss 10.4550 (10.4550)
2022-11-24 20:52:41,252:INFO: Dataset: zara2               Batch:  2/18	Loss 11.6809 (11.1113)
2022-11-24 20:52:41,273:INFO: Dataset: zara2               Batch:  3/18	Loss 11.3112 (11.1657)
2022-11-24 20:52:41,298:INFO: Dataset: zara2               Batch:  4/18	Loss 12.4249 (11.4977)
2022-11-24 20:52:41,318:INFO: Dataset: zara2               Batch:  5/18	Loss 12.8795 (11.7644)
2022-11-24 20:52:41,341:INFO: Dataset: zara2               Batch:  6/18	Loss 10.8837 (11.6262)
2022-11-24 20:52:41,360:INFO: Dataset: zara2               Batch:  7/18	Loss 12.4294 (11.7458)
2022-11-24 20:52:41,380:INFO: Dataset: zara2               Batch:  8/18	Loss 12.2878 (11.8026)
2022-11-24 20:52:41,400:INFO: Dataset: zara2               Batch:  9/18	Loss 11.4562 (11.7625)
2022-11-24 20:52:41,419:INFO: Dataset: zara2               Batch: 10/18	Loss 11.6883 (11.7553)
2022-11-24 20:52:41,439:INFO: Dataset: zara2               Batch: 11/18	Loss 11.0966 (11.6957)
2022-11-24 20:52:41,462:INFO: Dataset: zara2               Batch: 12/18	Loss 11.7362 (11.6989)
2022-11-24 20:52:41,483:INFO: Dataset: zara2               Batch: 13/18	Loss 10.5221 (11.6132)
2022-11-24 20:52:41,504:INFO: Dataset: zara2               Batch: 14/18	Loss 11.6783 (11.6176)
2022-11-24 20:52:41,527:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3688 (11.6650)
2022-11-24 20:52:41,549:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9097 (11.6791)
2022-11-24 20:52:41,570:INFO: Dataset: zara2               Batch: 17/18	Loss 12.6640 (11.7376)
2022-11-24 20:52:41,590:INFO: Dataset: zara2               Batch: 18/18	Loss 9.5914 (11.6304)
2022-11-24 20:52:41,633:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_18.pth.tar
2022-11-24 20:52:41,633:INFO: 
===> EPOCH: 19 (P1)
2022-11-24 20:52:41,633:INFO: - Computing loss (training)
2022-11-24 20:52:41,805:INFO: Dataset: hotel               Batch: 1/4	Loss 9.4035 (9.4035)
2022-11-24 20:52:41,827:INFO: Dataset: hotel               Batch: 2/4	Loss 11.3245 (10.3617)
2022-11-24 20:52:41,848:INFO: Dataset: hotel               Batch: 3/4	Loss 11.9638 (10.8949)
2022-11-24 20:52:41,861:INFO: Dataset: hotel               Batch: 4/4	Loss 5.7765 (10.0171)
2022-11-24 20:52:42,065:INFO: Dataset: univ                Batch:  1/15	Loss 9.6118 (9.6118)
2022-11-24 20:52:42,089:INFO: Dataset: univ                Batch:  2/15	Loss 9.2158 (9.4177)
2022-11-24 20:52:42,112:INFO: Dataset: univ                Batch:  3/15	Loss 10.7343 (9.8565)
2022-11-24 20:52:42,137:INFO: Dataset: univ                Batch:  4/15	Loss 10.5932 (10.0255)
2022-11-24 20:52:42,159:INFO: Dataset: univ                Batch:  5/15	Loss 9.6360 (9.9522)
2022-11-24 20:52:42,183:INFO: Dataset: univ                Batch:  6/15	Loss 11.5349 (10.1885)
2022-11-24 20:52:42,204:INFO: Dataset: univ                Batch:  7/15	Loss 9.0368 (10.0113)
2022-11-24 20:52:42,226:INFO: Dataset: univ                Batch:  8/15	Loss 9.7816 (9.9826)
2022-11-24 20:52:42,248:INFO: Dataset: univ                Batch:  9/15	Loss 11.1362 (10.1124)
2022-11-24 20:52:42,269:INFO: Dataset: univ                Batch: 10/15	Loss 10.4281 (10.1445)
2022-11-24 20:52:42,296:INFO: Dataset: univ                Batch: 11/15	Loss 8.8830 (10.0176)
2022-11-24 20:52:42,320:INFO: Dataset: univ                Batch: 12/15	Loss 10.3975 (10.0481)
2022-11-24 20:52:42,344:INFO: Dataset: univ                Batch: 13/15	Loss 9.3861 (9.9987)
2022-11-24 20:52:42,367:INFO: Dataset: univ                Batch: 14/15	Loss 8.9507 (9.9168)
2022-11-24 20:52:42,375:INFO: Dataset: univ                Batch: 15/15	Loss 2.4669 (9.8402)
2022-11-24 20:52:42,592:INFO: Dataset: zara1               Batch: 1/8	Loss 14.8685 (14.8685)
2022-11-24 20:52:42,614:INFO: Dataset: zara1               Batch: 2/8	Loss 15.9080 (15.4002)
2022-11-24 20:52:42,633:INFO: Dataset: zara1               Batch: 3/8	Loss 14.7764 (15.1742)
2022-11-24 20:52:42,653:INFO: Dataset: zara1               Batch: 4/8	Loss 14.9965 (15.1394)
2022-11-24 20:52:42,674:INFO: Dataset: zara1               Batch: 5/8	Loss 15.5747 (15.2379)
2022-11-24 20:52:42,695:INFO: Dataset: zara1               Batch: 6/8	Loss 16.7293 (15.4972)
2022-11-24 20:52:42,715:INFO: Dataset: zara1               Batch: 7/8	Loss 14.5116 (15.3576)
2022-11-24 20:52:42,732:INFO: Dataset: zara1               Batch: 8/8	Loss 13.2037 (15.1331)
2022-11-24 20:52:42,942:INFO: Dataset: zara2               Batch:  1/18	Loss 11.3673 (11.3673)
2022-11-24 20:52:42,966:INFO: Dataset: zara2               Batch:  2/18	Loss 10.2765 (10.7882)
2022-11-24 20:52:42,987:INFO: Dataset: zara2               Batch:  3/18	Loss 12.4538 (11.3316)
2022-11-24 20:52:43,008:INFO: Dataset: zara2               Batch:  4/18	Loss 11.7459 (11.4232)
2022-11-24 20:52:43,031:INFO: Dataset: zara2               Batch:  5/18	Loss 12.4556 (11.6496)
2022-11-24 20:52:43,054:INFO: Dataset: zara2               Batch:  6/18	Loss 11.3049 (11.5901)
2022-11-24 20:52:43,073:INFO: Dataset: zara2               Batch:  7/18	Loss 12.0315 (11.6569)
2022-11-24 20:52:43,094:INFO: Dataset: zara2               Batch:  8/18	Loss 11.9408 (11.6947)
2022-11-24 20:52:43,114:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7473 (11.8232)
2022-11-24 20:52:43,134:INFO: Dataset: zara2               Batch: 10/18	Loss 12.5098 (11.8932)
2022-11-24 20:52:43,153:INFO: Dataset: zara2               Batch: 11/18	Loss 12.2648 (11.9305)
2022-11-24 20:52:43,174:INFO: Dataset: zara2               Batch: 12/18	Loss 11.9954 (11.9357)
2022-11-24 20:52:43,196:INFO: Dataset: zara2               Batch: 13/18	Loss 11.2605 (11.8814)
2022-11-24 20:52:43,217:INFO: Dataset: zara2               Batch: 14/18	Loss 10.3145 (11.7792)
2022-11-24 20:52:43,241:INFO: Dataset: zara2               Batch: 15/18	Loss 11.3163 (11.7483)
2022-11-24 20:52:43,263:INFO: Dataset: zara2               Batch: 16/18	Loss 10.2438 (11.6533)
2022-11-24 20:52:43,285:INFO: Dataset: zara2               Batch: 17/18	Loss 11.0730 (11.6198)
2022-11-24 20:52:43,304:INFO: Dataset: zara2               Batch: 18/18	Loss 9.4273 (11.5180)
2022-11-24 20:52:43,343:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_19.pth.tar
2022-11-24 20:52:43,343:INFO: 
===> EPOCH: 20 (P1)
2022-11-24 20:52:43,343:INFO: - Computing loss (training)
2022-11-24 20:52:43,513:INFO: Dataset: hotel               Batch: 1/4	Loss 8.9854 (8.9854)
2022-11-24 20:52:43,536:INFO: Dataset: hotel               Batch: 2/4	Loss 11.7042 (10.3480)
2022-11-24 20:52:43,556:INFO: Dataset: hotel               Batch: 3/4	Loss 10.9766 (10.5477)
2022-11-24 20:52:43,569:INFO: Dataset: hotel               Batch: 4/4	Loss 6.9519 (9.8931)
2022-11-24 20:52:43,829:INFO: Dataset: univ                Batch:  1/15	Loss 11.4150 (11.4150)
2022-11-24 20:52:43,851:INFO: Dataset: univ                Batch:  2/15	Loss 8.5063 (9.8381)
2022-11-24 20:52:43,874:INFO: Dataset: univ                Batch:  3/15	Loss 9.4178 (9.6932)
2022-11-24 20:52:43,895:INFO: Dataset: univ                Batch:  4/15	Loss 10.4116 (9.8631)
2022-11-24 20:52:43,916:INFO: Dataset: univ                Batch:  5/15	Loss 9.6999 (9.8270)
2022-11-24 20:52:43,941:INFO: Dataset: univ                Batch:  6/15	Loss 9.9794 (9.8515)
2022-11-24 20:52:43,962:INFO: Dataset: univ                Batch:  7/15	Loss 9.6502 (9.8241)
2022-11-24 20:52:43,984:INFO: Dataset: univ                Batch:  8/15	Loss 9.4428 (9.7777)
2022-11-24 20:52:44,005:INFO: Dataset: univ                Batch:  9/15	Loss 9.6689 (9.7651)
2022-11-24 20:52:44,026:INFO: Dataset: univ                Batch: 10/15	Loss 9.1288 (9.6997)
2022-11-24 20:52:44,049:INFO: Dataset: univ                Batch: 11/15	Loss 9.8070 (9.7093)
2022-11-24 20:52:44,076:INFO: Dataset: univ                Batch: 12/15	Loss 11.0274 (9.8172)
2022-11-24 20:52:44,099:INFO: Dataset: univ                Batch: 13/15	Loss 10.1173 (9.8410)
2022-11-24 20:52:44,122:INFO: Dataset: univ                Batch: 14/15	Loss 9.5409 (9.8204)
2022-11-24 20:52:44,131:INFO: Dataset: univ                Batch: 15/15	Loss 2.5999 (9.7359)
2022-11-24 20:52:44,351:INFO: Dataset: zara1               Batch: 1/8	Loss 15.3875 (15.3875)
2022-11-24 20:52:44,371:INFO: Dataset: zara1               Batch: 2/8	Loss 15.3625 (15.3752)
2022-11-24 20:52:44,395:INFO: Dataset: zara1               Batch: 3/8	Loss 14.3360 (15.0177)
2022-11-24 20:52:44,416:INFO: Dataset: zara1               Batch: 4/8	Loss 13.6852 (14.6941)
2022-11-24 20:52:44,436:INFO: Dataset: zara1               Batch: 5/8	Loss 15.9827 (14.9832)
2022-11-24 20:52:44,456:INFO: Dataset: zara1               Batch: 6/8	Loss 15.7035 (15.1127)
2022-11-24 20:52:44,475:INFO: Dataset: zara1               Batch: 7/8	Loss 14.4288 (15.0057)
2022-11-24 20:52:44,492:INFO: Dataset: zara1               Batch: 8/8	Loss 13.0355 (14.7859)
2022-11-24 20:52:44,756:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3350 (13.3350)
2022-11-24 20:52:44,775:INFO: Dataset: zara2               Batch:  2/18	Loss 11.4717 (12.3854)
2022-11-24 20:52:44,794:INFO: Dataset: zara2               Batch:  3/18	Loss 11.6009 (12.1372)
2022-11-24 20:52:44,813:INFO: Dataset: zara2               Batch:  4/18	Loss 11.4861 (11.9786)
2022-11-24 20:52:44,831:INFO: Dataset: zara2               Batch:  5/18	Loss 11.3391 (11.8672)
2022-11-24 20:52:44,854:INFO: Dataset: zara2               Batch:  6/18	Loss 12.0624 (11.9032)
2022-11-24 20:52:44,872:INFO: Dataset: zara2               Batch:  7/18	Loss 11.2866 (11.8152)
2022-11-24 20:52:44,891:INFO: Dataset: zara2               Batch:  8/18	Loss 10.3711 (11.6191)
2022-11-24 20:52:44,912:INFO: Dataset: zara2               Batch:  9/18	Loss 10.2299 (11.4689)
2022-11-24 20:52:44,931:INFO: Dataset: zara2               Batch: 10/18	Loss 11.5657 (11.4770)
2022-11-24 20:52:44,950:INFO: Dataset: zara2               Batch: 11/18	Loss 11.6110 (11.4884)
2022-11-24 20:52:44,971:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6165 (11.5686)
2022-11-24 20:52:44,992:INFO: Dataset: zara2               Batch: 13/18	Loss 10.9501 (11.5196)
2022-11-24 20:52:45,016:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2586 (11.5692)
2022-11-24 20:52:45,037:INFO: Dataset: zara2               Batch: 15/18	Loss 10.2824 (11.4785)
2022-11-24 20:52:45,059:INFO: Dataset: zara2               Batch: 16/18	Loss 10.0373 (11.3835)
2022-11-24 20:52:45,079:INFO: Dataset: zara2               Batch: 17/18	Loss 10.4647 (11.3261)
2022-11-24 20:52:45,097:INFO: Dataset: zara2               Batch: 18/18	Loss 10.7972 (11.3039)
2022-11-24 20:52:45,140:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_20.pth.tar
2022-11-24 20:52:45,140:INFO: 
===> EPOCH: 21 (P1)
2022-11-24 20:52:45,140:INFO: - Computing loss (training)
2022-11-24 20:52:45,323:INFO: Dataset: hotel               Batch: 1/4	Loss 9.0673 (9.0673)
2022-11-24 20:52:45,344:INFO: Dataset: hotel               Batch: 2/4	Loss 10.4811 (9.7808)
2022-11-24 20:52:45,364:INFO: Dataset: hotel               Batch: 3/4	Loss 12.3825 (10.6372)
2022-11-24 20:52:45,377:INFO: Dataset: hotel               Batch: 4/4	Loss 6.7776 (10.0414)
2022-11-24 20:52:45,577:INFO: Dataset: univ                Batch:  1/15	Loss 8.6770 (8.6770)
2022-11-24 20:52:45,600:INFO: Dataset: univ                Batch:  2/15	Loss 8.9251 (8.7991)
2022-11-24 20:52:45,626:INFO: Dataset: univ                Batch:  3/15	Loss 10.3114 (9.2443)
2022-11-24 20:52:45,647:INFO: Dataset: univ                Batch:  4/15	Loss 10.5004 (9.5568)
2022-11-24 20:52:45,670:INFO: Dataset: univ                Batch:  5/15	Loss 9.7815 (9.5987)
2022-11-24 20:52:45,693:INFO: Dataset: univ                Batch:  6/15	Loss 9.1461 (9.5235)
2022-11-24 20:52:45,714:INFO: Dataset: univ                Batch:  7/15	Loss 10.4385 (9.6481)
2022-11-24 20:52:45,735:INFO: Dataset: univ                Batch:  8/15	Loss 9.0045 (9.5667)
2022-11-24 20:52:45,757:INFO: Dataset: univ                Batch:  9/15	Loss 9.3238 (9.5386)
2022-11-24 20:52:45,777:INFO: Dataset: univ                Batch: 10/15	Loss 10.3181 (9.6177)
2022-11-24 20:52:45,799:INFO: Dataset: univ                Batch: 11/15	Loss 10.0540 (9.6572)
2022-11-24 20:52:45,823:INFO: Dataset: univ                Batch: 12/15	Loss 9.5863 (9.6508)
2022-11-24 20:52:45,845:INFO: Dataset: univ                Batch: 13/15	Loss 9.3643 (9.6281)
2022-11-24 20:52:45,867:INFO: Dataset: univ                Batch: 14/15	Loss 10.4973 (9.6887)
2022-11-24 20:52:45,876:INFO: Dataset: univ                Batch: 15/15	Loss 1.9879 (9.6037)
2022-11-24 20:52:46,097:INFO: Dataset: zara1               Batch: 1/8	Loss 14.9309 (14.9309)
2022-11-24 20:52:46,117:INFO: Dataset: zara1               Batch: 2/8	Loss 14.5848 (14.7522)
2022-11-24 20:52:46,139:INFO: Dataset: zara1               Batch: 3/8	Loss 14.6249 (14.7106)
2022-11-24 20:52:46,158:INFO: Dataset: zara1               Batch: 4/8	Loss 14.5915 (14.6775)
2022-11-24 20:52:46,177:INFO: Dataset: zara1               Batch: 5/8	Loss 15.4343 (14.8439)
2022-11-24 20:52:46,196:INFO: Dataset: zara1               Batch: 6/8	Loss 15.0589 (14.8796)
2022-11-24 20:52:46,215:INFO: Dataset: zara1               Batch: 7/8	Loss 15.1687 (14.9177)
2022-11-24 20:52:46,233:INFO: Dataset: zara1               Batch: 8/8	Loss 11.4713 (14.4896)
2022-11-24 20:52:46,436:INFO: Dataset: zara2               Batch:  1/18	Loss 12.2436 (12.2436)
2022-11-24 20:52:46,457:INFO: Dataset: zara2               Batch:  2/18	Loss 11.2558 (11.7388)
2022-11-24 20:52:46,479:INFO: Dataset: zara2               Batch:  3/18	Loss 10.8705 (11.4548)
2022-11-24 20:52:46,500:INFO: Dataset: zara2               Batch:  4/18	Loss 13.1084 (11.8605)
2022-11-24 20:52:46,521:INFO: Dataset: zara2               Batch:  5/18	Loss 9.9174 (11.4712)
2022-11-24 20:52:46,543:INFO: Dataset: zara2               Batch:  6/18	Loss 11.1565 (11.4140)
2022-11-24 20:52:46,562:INFO: Dataset: zara2               Batch:  7/18	Loss 10.8917 (11.3421)
2022-11-24 20:52:46,581:INFO: Dataset: zara2               Batch:  8/18	Loss 10.4765 (11.2318)
2022-11-24 20:52:46,599:INFO: Dataset: zara2               Batch:  9/18	Loss 10.7140 (11.1759)
2022-11-24 20:52:46,618:INFO: Dataset: zara2               Batch: 10/18	Loss 10.5395 (11.1104)
2022-11-24 20:52:46,637:INFO: Dataset: zara2               Batch: 11/18	Loss 11.0062 (11.1014)
2022-11-24 20:52:46,658:INFO: Dataset: zara2               Batch: 12/18	Loss 11.8215 (11.1655)
2022-11-24 20:52:46,678:INFO: Dataset: zara2               Batch: 13/18	Loss 12.0411 (11.2319)
2022-11-24 20:52:46,698:INFO: Dataset: zara2               Batch: 14/18	Loss 10.9837 (11.2137)
2022-11-24 20:52:46,721:INFO: Dataset: zara2               Batch: 15/18	Loss 10.1321 (11.1424)
2022-11-24 20:52:46,744:INFO: Dataset: zara2               Batch: 16/18	Loss 10.4347 (11.0999)
2022-11-24 20:52:46,765:INFO: Dataset: zara2               Batch: 17/18	Loss 12.6354 (11.1936)
2022-11-24 20:52:46,783:INFO: Dataset: zara2               Batch: 18/18	Loss 9.7047 (11.1251)
2022-11-24 20:52:46,824:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_21.pth.tar
2022-11-24 20:52:46,824:INFO: 
===> EPOCH: 22 (P1)
2022-11-24 20:52:46,824:INFO: - Computing loss (training)
2022-11-24 20:52:46,988:INFO: Dataset: hotel               Batch: 1/4	Loss 11.2065 (11.2065)
2022-11-24 20:52:47,008:INFO: Dataset: hotel               Batch: 2/4	Loss 9.7386 (10.4534)
2022-11-24 20:52:47,029:INFO: Dataset: hotel               Batch: 3/4	Loss 10.3780 (10.4274)
2022-11-24 20:52:47,042:INFO: Dataset: hotel               Batch: 4/4	Loss 7.0882 (9.9164)
2022-11-24 20:52:47,262:INFO: Dataset: univ                Batch:  1/15	Loss 10.3591 (10.3591)
2022-11-24 20:52:47,285:INFO: Dataset: univ                Batch:  2/15	Loss 9.1193 (9.7358)
2022-11-24 20:52:47,310:INFO: Dataset: univ                Batch:  3/15	Loss 10.3794 (9.9614)
2022-11-24 20:52:47,331:INFO: Dataset: univ                Batch:  4/15	Loss 10.9818 (10.2162)
2022-11-24 20:52:47,353:INFO: Dataset: univ                Batch:  5/15	Loss 10.2852 (10.2292)
2022-11-24 20:52:47,376:INFO: Dataset: univ                Batch:  6/15	Loss 9.1541 (10.0319)
2022-11-24 20:52:47,396:INFO: Dataset: univ                Batch:  7/15	Loss 10.5584 (10.1019)
2022-11-24 20:52:47,416:INFO: Dataset: univ                Batch:  8/15	Loss 9.5259 (10.0259)
2022-11-24 20:52:47,436:INFO: Dataset: univ                Batch:  9/15	Loss 9.0081 (9.9096)
2022-11-24 20:52:47,457:INFO: Dataset: univ                Batch: 10/15	Loss 8.4217 (9.7661)
2022-11-24 20:52:47,479:INFO: Dataset: univ                Batch: 11/15	Loss 9.6778 (9.7583)
2022-11-24 20:52:47,500:INFO: Dataset: univ                Batch: 12/15	Loss 10.2838 (9.7962)
2022-11-24 20:52:47,521:INFO: Dataset: univ                Batch: 13/15	Loss 9.4787 (9.7718)
2022-11-24 20:52:47,542:INFO: Dataset: univ                Batch: 14/15	Loss 8.1996 (9.6512)
2022-11-24 20:52:47,551:INFO: Dataset: univ                Batch: 15/15	Loss 1.1374 (9.5161)
2022-11-24 20:52:47,763:INFO: Dataset: zara1               Batch: 1/8	Loss 14.8886 (14.8886)
2022-11-24 20:52:47,785:INFO: Dataset: zara1               Batch: 2/8	Loss 13.6753 (14.2604)
2022-11-24 20:52:47,804:INFO: Dataset: zara1               Batch: 3/8	Loss 14.4198 (14.3159)
2022-11-24 20:52:47,823:INFO: Dataset: zara1               Batch: 4/8	Loss 15.0449 (14.4856)
2022-11-24 20:52:47,843:INFO: Dataset: zara1               Batch: 5/8	Loss 13.7021 (14.3371)
2022-11-24 20:52:47,865:INFO: Dataset: zara1               Batch: 6/8	Loss 15.4341 (14.5411)
2022-11-24 20:52:47,884:INFO: Dataset: zara1               Batch: 7/8	Loss 13.8357 (14.4326)
2022-11-24 20:52:47,901:INFO: Dataset: zara1               Batch: 8/8	Loss 12.7115 (14.2659)
2022-11-24 20:52:48,109:INFO: Dataset: zara2               Batch:  1/18	Loss 10.6983 (10.6983)
2022-11-24 20:52:48,131:INFO: Dataset: zara2               Batch:  2/18	Loss 10.4515 (10.5719)
2022-11-24 20:52:48,155:INFO: Dataset: zara2               Batch:  3/18	Loss 10.3626 (10.5018)
2022-11-24 20:52:48,174:INFO: Dataset: zara2               Batch:  4/18	Loss 11.9664 (10.8323)
2022-11-24 20:52:48,193:INFO: Dataset: zara2               Batch:  5/18	Loss 10.8176 (10.8293)
2022-11-24 20:52:48,217:INFO: Dataset: zara2               Batch:  6/18	Loss 10.2651 (10.7367)
2022-11-24 20:52:48,236:INFO: Dataset: zara2               Batch:  7/18	Loss 11.8085 (10.8890)
2022-11-24 20:52:48,255:INFO: Dataset: zara2               Batch:  8/18	Loss 10.6773 (10.8613)
2022-11-24 20:52:48,274:INFO: Dataset: zara2               Batch:  9/18	Loss 11.4093 (10.9206)
2022-11-24 20:52:48,293:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3012 (11.1390)
2022-11-24 20:52:48,312:INFO: Dataset: zara2               Batch: 11/18	Loss 9.8882 (11.0302)
2022-11-24 20:52:48,333:INFO: Dataset: zara2               Batch: 12/18	Loss 11.0538 (11.0322)
2022-11-24 20:52:48,353:INFO: Dataset: zara2               Batch: 13/18	Loss 11.5171 (11.0696)
2022-11-24 20:52:48,373:INFO: Dataset: zara2               Batch: 14/18	Loss 10.9358 (11.0610)
2022-11-24 20:52:48,394:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0622 (11.1304)
2022-11-24 20:52:48,416:INFO: Dataset: zara2               Batch: 16/18	Loss 11.0745 (11.1269)
2022-11-24 20:52:48,436:INFO: Dataset: zara2               Batch: 17/18	Loss 10.3057 (11.0765)
2022-11-24 20:52:48,455:INFO: Dataset: zara2               Batch: 18/18	Loss 7.9073 (10.9120)
2022-11-24 20:52:48,499:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_22.pth.tar
2022-11-24 20:52:48,499:INFO: 
===> EPOCH: 23 (P1)
2022-11-24 20:52:48,499:INFO: - Computing loss (training)
2022-11-24 20:52:48,670:INFO: Dataset: hotel               Batch: 1/4	Loss 9.5607 (9.5607)
2022-11-24 20:52:48,692:INFO: Dataset: hotel               Batch: 2/4	Loss 12.0831 (10.7786)
2022-11-24 20:52:48,713:INFO: Dataset: hotel               Batch: 3/4	Loss 9.9434 (10.4869)
2022-11-24 20:52:48,727:INFO: Dataset: hotel               Batch: 4/4	Loss 6.6841 (9.8297)
2022-11-24 20:52:48,987:INFO: Dataset: univ                Batch:  1/15	Loss 10.7676 (10.7676)
2022-11-24 20:52:49,013:INFO: Dataset: univ                Batch:  2/15	Loss 10.6998 (10.7332)
2022-11-24 20:52:49,036:INFO: Dataset: univ                Batch:  3/15	Loss 10.0119 (10.4831)
2022-11-24 20:52:49,058:INFO: Dataset: univ                Batch:  4/15	Loss 8.8370 (10.0189)
2022-11-24 20:52:49,079:INFO: Dataset: univ                Batch:  5/15	Loss 10.2609 (10.0637)
2022-11-24 20:52:49,105:INFO: Dataset: univ                Batch:  6/15	Loss 9.0575 (9.8657)
2022-11-24 20:52:49,127:INFO: Dataset: univ                Batch:  7/15	Loss 9.0277 (9.7426)
2022-11-24 20:52:49,149:INFO: Dataset: univ                Batch:  8/15	Loss 8.2805 (9.5440)
2022-11-24 20:52:49,170:INFO: Dataset: univ                Batch:  9/15	Loss 10.0152 (9.5972)
2022-11-24 20:52:49,192:INFO: Dataset: univ                Batch: 10/15	Loss 8.1386 (9.4444)
2022-11-24 20:52:49,214:INFO: Dataset: univ                Batch: 11/15	Loss 8.1934 (9.3265)
2022-11-24 20:52:49,238:INFO: Dataset: univ                Batch: 12/15	Loss 9.8796 (9.3701)
2022-11-24 20:52:49,263:INFO: Dataset: univ                Batch: 13/15	Loss 9.7081 (9.3939)
2022-11-24 20:52:49,287:INFO: Dataset: univ                Batch: 14/15	Loss 9.0740 (9.3705)
2022-11-24 20:52:49,295:INFO: Dataset: univ                Batch: 15/15	Loss 2.3428 (9.2925)
2022-11-24 20:52:49,499:INFO: Dataset: zara1               Batch: 1/8	Loss 14.2379 (14.2379)
2022-11-24 20:52:49,530:INFO: Dataset: zara1               Batch: 2/8	Loss 14.8767 (14.5491)
2022-11-24 20:52:49,550:INFO: Dataset: zara1               Batch: 3/8	Loss 14.8476 (14.6533)
2022-11-24 20:52:49,570:INFO: Dataset: zara1               Batch: 4/8	Loss 14.6280 (14.6472)
2022-11-24 20:52:49,589:INFO: Dataset: zara1               Batch: 5/8	Loss 13.0936 (14.3415)
2022-11-24 20:52:49,611:INFO: Dataset: zara1               Batch: 6/8	Loss 14.0517 (14.2858)
2022-11-24 20:52:49,630:INFO: Dataset: zara1               Batch: 7/8	Loss 14.1769 (14.2707)
2022-11-24 20:52:49,647:INFO: Dataset: zara1               Batch: 8/8	Loss 12.0470 (14.0225)
2022-11-24 20:52:49,873:INFO: Dataset: zara2               Batch:  1/18	Loss 12.5457 (12.5457)
2022-11-24 20:52:49,893:INFO: Dataset: zara2               Batch:  2/18	Loss 10.8064 (11.7079)
2022-11-24 20:52:49,914:INFO: Dataset: zara2               Batch:  3/18	Loss 10.9574 (11.4465)
2022-11-24 20:52:49,934:INFO: Dataset: zara2               Batch:  4/18	Loss 10.6368 (11.2429)
2022-11-24 20:52:49,956:INFO: Dataset: zara2               Batch:  5/18	Loss 10.5002 (11.1026)
2022-11-24 20:52:49,978:INFO: Dataset: zara2               Batch:  6/18	Loss 12.0886 (11.2633)
2022-11-24 20:52:49,997:INFO: Dataset: zara2               Batch:  7/18	Loss 11.7237 (11.3226)
2022-11-24 20:52:50,016:INFO: Dataset: zara2               Batch:  8/18	Loss 10.5883 (11.2297)
2022-11-24 20:52:50,035:INFO: Dataset: zara2               Batch:  9/18	Loss 11.1890 (11.2255)
2022-11-24 20:52:50,053:INFO: Dataset: zara2               Batch: 10/18	Loss 11.9316 (11.2933)
2022-11-24 20:52:50,074:INFO: Dataset: zara2               Batch: 11/18	Loss 9.7188 (11.1448)
2022-11-24 20:52:50,094:INFO: Dataset: zara2               Batch: 12/18	Loss 8.7911 (10.9388)
2022-11-24 20:52:50,113:INFO: Dataset: zara2               Batch: 13/18	Loss 11.5317 (10.9860)
2022-11-24 20:52:50,133:INFO: Dataset: zara2               Batch: 14/18	Loss 9.9606 (10.9136)
2022-11-24 20:52:50,152:INFO: Dataset: zara2               Batch: 15/18	Loss 9.9555 (10.8442)
2022-11-24 20:52:50,173:INFO: Dataset: zara2               Batch: 16/18	Loss 10.1895 (10.8057)
2022-11-24 20:52:50,193:INFO: Dataset: zara2               Batch: 17/18	Loss 10.8582 (10.8087)
2022-11-24 20:52:50,211:INFO: Dataset: zara2               Batch: 18/18	Loss 9.2376 (10.7310)
2022-11-24 20:52:50,248:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_23.pth.tar
2022-11-24 20:52:50,249:INFO: 
===> EPOCH: 24 (P1)
2022-11-24 20:52:50,249:INFO: - Computing loss (training)
2022-11-24 20:52:50,431:INFO: Dataset: hotel               Batch: 1/4	Loss 10.0008 (10.0008)
2022-11-24 20:52:50,453:INFO: Dataset: hotel               Batch: 2/4	Loss 10.0125 (10.0065)
2022-11-24 20:52:50,472:INFO: Dataset: hotel               Batch: 3/4	Loss 11.2234 (10.4083)
2022-11-24 20:52:50,486:INFO: Dataset: hotel               Batch: 4/4	Loss 7.0923 (9.8352)
2022-11-24 20:52:50,701:INFO: Dataset: univ                Batch:  1/15	Loss 9.0630 (9.0630)
2022-11-24 20:52:50,743:INFO: Dataset: univ                Batch:  2/15	Loss 9.9695 (9.4870)
2022-11-24 20:52:50,765:INFO: Dataset: univ                Batch:  3/15	Loss 9.4505 (9.4739)
2022-11-24 20:52:50,786:INFO: Dataset: univ                Batch:  4/15	Loss 9.5061 (9.4821)
2022-11-24 20:52:50,806:INFO: Dataset: univ                Batch:  5/15	Loss 8.7432 (9.3386)
2022-11-24 20:52:50,830:INFO: Dataset: univ                Batch:  6/15	Loss 8.6353 (9.2185)
2022-11-24 20:52:50,851:INFO: Dataset: univ                Batch:  7/15	Loss 8.9640 (9.1820)
2022-11-24 20:52:50,871:INFO: Dataset: univ                Batch:  8/15	Loss 7.8761 (9.0196)
2022-11-24 20:52:50,891:INFO: Dataset: univ                Batch:  9/15	Loss 11.4465 (9.2772)
2022-11-24 20:52:50,912:INFO: Dataset: univ                Batch: 10/15	Loss 10.2267 (9.3639)
2022-11-24 20:52:50,934:INFO: Dataset: univ                Batch: 11/15	Loss 8.8346 (9.3155)
2022-11-24 20:52:50,957:INFO: Dataset: univ                Batch: 12/15	Loss 10.4549 (9.4061)
2022-11-24 20:52:50,979:INFO: Dataset: univ                Batch: 13/15	Loss 9.4077 (9.4062)
2022-11-24 20:52:51,001:INFO: Dataset: univ                Batch: 14/15	Loss 8.6489 (9.3514)
2022-11-24 20:52:51,009:INFO: Dataset: univ                Batch: 15/15	Loss 1.5295 (9.2424)
2022-11-24 20:52:51,203:INFO: Dataset: zara1               Batch: 1/8	Loss 13.0917 (13.0917)
2022-11-24 20:52:51,223:INFO: Dataset: zara1               Batch: 2/8	Loss 13.8976 (13.5041)
2022-11-24 20:52:51,245:INFO: Dataset: zara1               Batch: 3/8	Loss 14.5120 (13.8061)
2022-11-24 20:52:51,264:INFO: Dataset: zara1               Batch: 4/8	Loss 14.4103 (13.9614)
2022-11-24 20:52:51,283:INFO: Dataset: zara1               Batch: 5/8	Loss 13.5664 (13.8816)
2022-11-24 20:52:51,303:INFO: Dataset: zara1               Batch: 6/8	Loss 14.5441 (13.9941)
2022-11-24 20:52:51,322:INFO: Dataset: zara1               Batch: 7/8	Loss 15.5894 (14.1999)
2022-11-24 20:52:51,339:INFO: Dataset: zara1               Batch: 8/8	Loss 10.6501 (13.8356)
2022-11-24 20:52:51,560:INFO: Dataset: zara2               Batch:  1/18	Loss 11.2198 (11.2198)
2022-11-24 20:52:51,582:INFO: Dataset: zara2               Batch:  2/18	Loss 9.7957 (10.5046)
2022-11-24 20:52:51,604:INFO: Dataset: zara2               Batch:  3/18	Loss 9.5972 (10.2115)
2022-11-24 20:52:51,623:INFO: Dataset: zara2               Batch:  4/18	Loss 11.4035 (10.5113)
2022-11-24 20:52:51,644:INFO: Dataset: zara2               Batch:  5/18	Loss 10.9627 (10.6024)
2022-11-24 20:52:51,665:INFO: Dataset: zara2               Batch:  6/18	Loss 10.0252 (10.5012)
2022-11-24 20:52:51,684:INFO: Dataset: zara2               Batch:  7/18	Loss 11.3418 (10.6251)
2022-11-24 20:52:51,703:INFO: Dataset: zara2               Batch:  8/18	Loss 11.7961 (10.7684)
2022-11-24 20:52:51,722:INFO: Dataset: zara2               Batch:  9/18	Loss 10.1105 (10.6957)
2022-11-24 20:52:51,741:INFO: Dataset: zara2               Batch: 10/18	Loss 10.0311 (10.6353)
2022-11-24 20:52:51,761:INFO: Dataset: zara2               Batch: 11/18	Loss 11.5557 (10.7122)
2022-11-24 20:52:51,781:INFO: Dataset: zara2               Batch: 12/18	Loss 10.5180 (10.6952)
2022-11-24 20:52:51,801:INFO: Dataset: zara2               Batch: 13/18	Loss 11.4303 (10.7511)
2022-11-24 20:52:51,820:INFO: Dataset: zara2               Batch: 14/18	Loss 10.4916 (10.7335)
2022-11-24 20:52:51,841:INFO: Dataset: zara2               Batch: 15/18	Loss 10.5021 (10.7178)
2022-11-24 20:52:51,863:INFO: Dataset: zara2               Batch: 16/18	Loss 10.2232 (10.6832)
2022-11-24 20:52:51,884:INFO: Dataset: zara2               Batch: 17/18	Loss 10.1254 (10.6487)
2022-11-24 20:52:51,902:INFO: Dataset: zara2               Batch: 18/18	Loss 9.4267 (10.5907)
2022-11-24 20:52:51,938:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_24.pth.tar
2022-11-24 20:52:51,938:INFO: 
===> EPOCH: 25 (P1)
2022-11-24 20:52:51,939:INFO: - Computing loss (training)
2022-11-24 20:52:52,104:INFO: Dataset: hotel               Batch: 1/4	Loss 12.1550 (12.1550)
2022-11-24 20:52:52,125:INFO: Dataset: hotel               Batch: 2/4	Loss 8.8952 (10.4324)
2022-11-24 20:52:52,144:INFO: Dataset: hotel               Batch: 3/4	Loss 9.7795 (10.2148)
2022-11-24 20:52:52,158:INFO: Dataset: hotel               Batch: 4/4	Loss 7.2440 (9.7249)
2022-11-24 20:52:52,379:INFO: Dataset: univ                Batch:  1/15	Loss 8.0083 (8.0083)
2022-11-24 20:52:52,402:INFO: Dataset: univ                Batch:  2/15	Loss 9.2167 (8.5851)
2022-11-24 20:52:52,425:INFO: Dataset: univ                Batch:  3/15	Loss 8.9848 (8.7206)
2022-11-24 20:52:52,450:INFO: Dataset: univ                Batch:  4/15	Loss 8.6703 (8.7081)
2022-11-24 20:52:52,473:INFO: Dataset: univ                Batch:  5/15	Loss 8.4925 (8.6653)
2022-11-24 20:52:52,496:INFO: Dataset: univ                Batch:  6/15	Loss 10.5677 (8.9243)
2022-11-24 20:52:52,517:INFO: Dataset: univ                Batch:  7/15	Loss 8.5871 (8.8751)
2022-11-24 20:52:52,538:INFO: Dataset: univ                Batch:  8/15	Loss 9.7960 (8.9875)
2022-11-24 20:52:52,559:INFO: Dataset: univ                Batch:  9/15	Loss 10.3626 (9.1338)
2022-11-24 20:52:52,580:INFO: Dataset: univ                Batch: 10/15	Loss 9.3259 (9.1518)
2022-11-24 20:52:52,604:INFO: Dataset: univ                Batch: 11/15	Loss 9.2104 (9.1571)
2022-11-24 20:52:52,626:INFO: Dataset: univ                Batch: 12/15	Loss 8.4225 (9.0909)
2022-11-24 20:52:52,648:INFO: Dataset: univ                Batch: 13/15	Loss 9.4341 (9.1199)
2022-11-24 20:52:52,670:INFO: Dataset: univ                Batch: 14/15	Loss 9.4701 (9.1443)
2022-11-24 20:52:52,679:INFO: Dataset: univ                Batch: 15/15	Loss 1.7321 (9.0347)
2022-11-24 20:52:52,981:INFO: Dataset: zara1               Batch: 1/8	Loss 12.7073 (12.7073)
2022-11-24 20:52:53,001:INFO: Dataset: zara1               Batch: 2/8	Loss 14.1142 (13.4347)
2022-11-24 20:52:53,020:INFO: Dataset: zara1               Batch: 3/8	Loss 12.8415 (13.2298)
2022-11-24 20:52:53,039:INFO: Dataset: zara1               Batch: 4/8	Loss 15.0133 (13.6639)
2022-11-24 20:52:53,057:INFO: Dataset: zara1               Batch: 5/8	Loss 13.8561 (13.7019)
2022-11-24 20:52:53,078:INFO: Dataset: zara1               Batch: 6/8	Loss 12.6484 (13.5056)
2022-11-24 20:52:53,097:INFO: Dataset: zara1               Batch: 7/8	Loss 13.9942 (13.5760)
2022-11-24 20:52:53,114:INFO: Dataset: zara1               Batch: 8/8	Loss 12.1787 (13.4282)
2022-11-24 20:52:53,328:INFO: Dataset: zara2               Batch:  1/18	Loss 10.1626 (10.1626)
2022-11-24 20:52:53,348:INFO: Dataset: zara2               Batch:  2/18	Loss 11.1669 (10.6719)
2022-11-24 20:52:53,373:INFO: Dataset: zara2               Batch:  3/18	Loss 10.0012 (10.4384)
2022-11-24 20:52:53,394:INFO: Dataset: zara2               Batch:  4/18	Loss 11.3774 (10.6623)
2022-11-24 20:52:53,414:INFO: Dataset: zara2               Batch:  5/18	Loss 10.4231 (10.6125)
2022-11-24 20:52:53,436:INFO: Dataset: zara2               Batch:  6/18	Loss 10.3489 (10.5662)
2022-11-24 20:52:53,455:INFO: Dataset: zara2               Batch:  7/18	Loss 9.9986 (10.4773)
2022-11-24 20:52:53,474:INFO: Dataset: zara2               Batch:  8/18	Loss 10.5551 (10.4874)
2022-11-24 20:52:53,493:INFO: Dataset: zara2               Batch:  9/18	Loss 8.6486 (10.2891)
2022-11-24 20:52:53,512:INFO: Dataset: zara2               Batch: 10/18	Loss 10.3412 (10.2945)
2022-11-24 20:52:53,533:INFO: Dataset: zara2               Batch: 11/18	Loss 9.8963 (10.2543)
2022-11-24 20:52:53,552:INFO: Dataset: zara2               Batch: 12/18	Loss 10.6032 (10.2809)
2022-11-24 20:52:53,572:INFO: Dataset: zara2               Batch: 13/18	Loss 10.6010 (10.3074)
2022-11-24 20:52:53,592:INFO: Dataset: zara2               Batch: 14/18	Loss 10.3983 (10.3143)
2022-11-24 20:52:53,614:INFO: Dataset: zara2               Batch: 15/18	Loss 11.7284 (10.4073)
2022-11-24 20:52:53,636:INFO: Dataset: zara2               Batch: 16/18	Loss 9.9468 (10.3782)
2022-11-24 20:52:53,656:INFO: Dataset: zara2               Batch: 17/18	Loss 10.7985 (10.4035)
2022-11-24 20:52:53,674:INFO: Dataset: zara2               Batch: 18/18	Loss 8.3151 (10.2995)
2022-11-24 20:52:53,715:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_25.pth.tar
2022-11-24 20:52:53,715:INFO: 
===> EPOCH: 26 (P1)
2022-11-24 20:52:53,716:INFO: - Computing loss (training)
2022-11-24 20:52:53,881:INFO: Dataset: hotel               Batch: 1/4	Loss 8.6868 (8.6868)
2022-11-24 20:52:53,903:INFO: Dataset: hotel               Batch: 2/4	Loss 10.8416 (9.7341)
2022-11-24 20:52:53,922:INFO: Dataset: hotel               Batch: 3/4	Loss 11.9661 (10.4330)
2022-11-24 20:52:53,936:INFO: Dataset: hotel               Batch: 4/4	Loss 6.8160 (9.8031)
2022-11-24 20:52:54,141:INFO: Dataset: univ                Batch:  1/15	Loss 10.3798 (10.3798)
2022-11-24 20:52:54,164:INFO: Dataset: univ                Batch:  2/15	Loss 9.0777 (9.7238)
2022-11-24 20:52:54,187:INFO: Dataset: univ                Batch:  3/15	Loss 9.3678 (9.6084)
2022-11-24 20:52:54,211:INFO: Dataset: univ                Batch:  4/15	Loss 10.0738 (9.7188)
2022-11-24 20:52:54,231:INFO: Dataset: univ                Batch:  5/15	Loss 8.5622 (9.4854)
2022-11-24 20:52:54,256:INFO: Dataset: univ                Batch:  6/15	Loss 8.5116 (9.3039)
2022-11-24 20:52:54,277:INFO: Dataset: univ                Batch:  7/15	Loss 9.8997 (9.3862)
2022-11-24 20:52:54,297:INFO: Dataset: univ                Batch:  8/15	Loss 9.0203 (9.3439)
2022-11-24 20:52:54,318:INFO: Dataset: univ                Batch:  9/15	Loss 8.9937 (9.3042)
2022-11-24 20:52:54,338:INFO: Dataset: univ                Batch: 10/15	Loss 8.2117 (9.1932)
2022-11-24 20:52:54,359:INFO: Dataset: univ                Batch: 11/15	Loss 8.3642 (9.1190)
2022-11-24 20:52:54,382:INFO: Dataset: univ                Batch: 12/15	Loss 8.9270 (9.1020)
2022-11-24 20:52:54,403:INFO: Dataset: univ                Batch: 13/15	Loss 7.4663 (8.9563)
2022-11-24 20:52:54,425:INFO: Dataset: univ                Batch: 14/15	Loss 10.3240 (9.0469)
2022-11-24 20:52:54,434:INFO: Dataset: univ                Batch: 15/15	Loss 1.8362 (8.9604)
2022-11-24 20:52:54,640:INFO: Dataset: zara1               Batch: 1/8	Loss 13.3924 (13.3924)
2022-11-24 20:52:54,663:INFO: Dataset: zara1               Batch: 2/8	Loss 13.3252 (13.3591)
2022-11-24 20:52:54,683:INFO: Dataset: zara1               Batch: 3/8	Loss 12.9398 (13.2244)
2022-11-24 20:52:54,702:INFO: Dataset: zara1               Batch: 4/8	Loss 13.4625 (13.2891)
2022-11-24 20:52:54,721:INFO: Dataset: zara1               Batch: 5/8	Loss 13.9846 (13.4095)
2022-11-24 20:52:54,743:INFO: Dataset: zara1               Batch: 6/8	Loss 12.4292 (13.2188)
2022-11-24 20:52:54,762:INFO: Dataset: zara1               Batch: 7/8	Loss 13.5982 (13.2747)
2022-11-24 20:52:54,779:INFO: Dataset: zara1               Batch: 8/8	Loss 11.2987 (13.0917)
2022-11-24 20:52:54,990:INFO: Dataset: zara2               Batch:  1/18	Loss 8.9169 (8.9169)
2022-11-24 20:52:55,010:INFO: Dataset: zara2               Batch:  2/18	Loss 10.9784 (9.8173)
2022-11-24 20:52:55,030:INFO: Dataset: zara2               Batch:  3/18	Loss 10.4615 (10.0440)
2022-11-24 20:52:55,053:INFO: Dataset: zara2               Batch:  4/18	Loss 10.2675 (10.0984)
2022-11-24 20:52:55,072:INFO: Dataset: zara2               Batch:  5/18	Loss 10.4290 (10.1635)
2022-11-24 20:52:55,094:INFO: Dataset: zara2               Batch:  6/18	Loss 10.6560 (10.2491)
2022-11-24 20:52:55,113:INFO: Dataset: zara2               Batch:  7/18	Loss 9.3615 (10.1296)
2022-11-24 20:52:55,132:INFO: Dataset: zara2               Batch:  8/18	Loss 10.6233 (10.1850)
2022-11-24 20:52:55,150:INFO: Dataset: zara2               Batch:  9/18	Loss 11.8275 (10.3496)
2022-11-24 20:52:55,169:INFO: Dataset: zara2               Batch: 10/18	Loss 11.4128 (10.4559)
2022-11-24 20:52:55,188:INFO: Dataset: zara2               Batch: 11/18	Loss 9.3654 (10.3620)
2022-11-24 20:52:55,209:INFO: Dataset: zara2               Batch: 12/18	Loss 9.5273 (10.2953)
2022-11-24 20:52:55,231:INFO: Dataset: zara2               Batch: 13/18	Loss 9.0130 (10.1950)
2022-11-24 20:52:55,253:INFO: Dataset: zara2               Batch: 14/18	Loss 9.6894 (10.1610)
2022-11-24 20:52:55,278:INFO: Dataset: zara2               Batch: 15/18	Loss 10.3795 (10.1750)
2022-11-24 20:52:55,302:INFO: Dataset: zara2               Batch: 16/18	Loss 10.0865 (10.1694)
2022-11-24 20:52:55,325:INFO: Dataset: zara2               Batch: 17/18	Loss 10.7280 (10.2025)
2022-11-24 20:52:55,346:INFO: Dataset: zara2               Batch: 18/18	Loss 9.1719 (10.1560)
2022-11-24 20:52:55,397:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_26.pth.tar
2022-11-24 20:52:55,397:INFO: 
===> EPOCH: 27 (P1)
2022-11-24 20:52:55,397:INFO: - Computing loss (training)
2022-11-24 20:52:55,583:INFO: Dataset: hotel               Batch: 1/4	Loss 11.7664 (11.7664)
2022-11-24 20:52:55,608:INFO: Dataset: hotel               Batch: 2/4	Loss 10.3113 (11.0336)
2022-11-24 20:52:55,636:INFO: Dataset: hotel               Batch: 3/4	Loss 10.4347 (10.8317)
2022-11-24 20:52:55,657:INFO: Dataset: hotel               Batch: 4/4	Loss 6.2235 (10.0292)
2022-11-24 20:52:55,898:INFO: Dataset: univ                Batch:  1/15	Loss 8.4361 (8.4361)
2022-11-24 20:52:55,926:INFO: Dataset: univ                Batch:  2/15	Loss 8.9710 (8.6909)
2022-11-24 20:52:55,953:INFO: Dataset: univ                Batch:  3/15	Loss 8.7340 (8.7044)
2022-11-24 20:52:55,980:INFO: Dataset: univ                Batch:  4/15	Loss 9.8069 (8.9694)
2022-11-24 20:52:56,008:INFO: Dataset: univ                Batch:  5/15	Loss 8.5449 (8.8818)
2022-11-24 20:52:56,036:INFO: Dataset: univ                Batch:  6/15	Loss 7.7935 (8.6933)
2022-11-24 20:52:56,061:INFO: Dataset: univ                Batch:  7/15	Loss 9.2398 (8.7714)
2022-11-24 20:52:56,085:INFO: Dataset: univ                Batch:  8/15	Loss 8.3560 (8.7208)
2022-11-24 20:52:56,110:INFO: Dataset: univ                Batch:  9/15	Loss 8.5426 (8.7008)
2022-11-24 20:52:56,135:INFO: Dataset: univ                Batch: 10/15	Loss 9.1759 (8.7471)
2022-11-24 20:52:56,162:INFO: Dataset: univ                Batch: 11/15	Loss 9.3931 (8.8034)
2022-11-24 20:52:56,187:INFO: Dataset: univ                Batch: 12/15	Loss 10.0571 (8.9107)
2022-11-24 20:52:56,212:INFO: Dataset: univ                Batch: 13/15	Loss 8.7900 (8.9008)
2022-11-24 20:52:56,237:INFO: Dataset: univ                Batch: 14/15	Loss 9.0503 (8.9124)
2022-11-24 20:52:56,248:INFO: Dataset: univ                Batch: 15/15	Loss 2.2231 (8.8388)
2022-11-24 20:52:56,487:INFO: Dataset: zara1               Batch: 1/8	Loss 14.0525 (14.0525)
2022-11-24 20:52:56,511:INFO: Dataset: zara1               Batch: 2/8	Loss 12.0977 (13.1331)
2022-11-24 20:52:56,536:INFO: Dataset: zara1               Batch: 3/8	Loss 12.2465 (12.8269)
2022-11-24 20:52:56,558:INFO: Dataset: zara1               Batch: 4/8	Loss 14.1157 (13.1128)
2022-11-24 20:52:56,584:INFO: Dataset: zara1               Batch: 5/8	Loss 12.9897 (13.0867)
2022-11-24 20:52:56,605:INFO: Dataset: zara1               Batch: 6/8	Loss 12.8791 (13.0543)
2022-11-24 20:52:56,624:INFO: Dataset: zara1               Batch: 7/8	Loss 13.1697 (13.0718)
2022-11-24 20:52:56,640:INFO: Dataset: zara1               Batch: 8/8	Loss 10.8578 (12.8423)
2022-11-24 20:52:56,864:INFO: Dataset: zara2               Batch:  1/18	Loss 10.2262 (10.2262)
2022-11-24 20:52:56,883:INFO: Dataset: zara2               Batch:  2/18	Loss 9.9300 (10.0760)
2022-11-24 20:52:56,902:INFO: Dataset: zara2               Batch:  3/18	Loss 9.4847 (9.8735)
2022-11-24 20:52:56,921:INFO: Dataset: zara2               Batch:  4/18	Loss 10.6880 (10.0682)
2022-11-24 20:52:56,941:INFO: Dataset: zara2               Batch:  5/18	Loss 10.1632 (10.0875)
2022-11-24 20:52:56,969:INFO: Dataset: zara2               Batch:  6/18	Loss 10.8578 (10.2144)
2022-11-24 20:52:56,998:INFO: Dataset: zara2               Batch:  7/18	Loss 9.5796 (10.1268)
2022-11-24 20:52:57,021:INFO: Dataset: zara2               Batch:  8/18	Loss 10.6365 (10.1948)
2022-11-24 20:52:57,042:INFO: Dataset: zara2               Batch:  9/18	Loss 9.4022 (10.1005)
2022-11-24 20:52:57,064:INFO: Dataset: zara2               Batch: 10/18	Loss 10.2176 (10.1132)
2022-11-24 20:52:57,086:INFO: Dataset: zara2               Batch: 11/18	Loss 9.6783 (10.0737)
2022-11-24 20:52:57,110:INFO: Dataset: zara2               Batch: 12/18	Loss 10.4379 (10.1039)
2022-11-24 20:52:57,133:INFO: Dataset: zara2               Batch: 13/18	Loss 9.3203 (10.0422)
2022-11-24 20:52:57,155:INFO: Dataset: zara2               Batch: 14/18	Loss 11.1655 (10.1235)
2022-11-24 20:52:57,178:INFO: Dataset: zara2               Batch: 15/18	Loss 9.3507 (10.0652)
2022-11-24 20:52:57,202:INFO: Dataset: zara2               Batch: 16/18	Loss 9.4422 (10.0238)
2022-11-24 20:52:57,226:INFO: Dataset: zara2               Batch: 17/18	Loss 9.7131 (10.0048)
2022-11-24 20:52:57,247:INFO: Dataset: zara2               Batch: 18/18	Loss 8.1981 (9.9152)
2022-11-24 20:52:57,290:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_27.pth.tar
2022-11-24 20:52:57,290:INFO: 
===> EPOCH: 28 (P1)
2022-11-24 20:52:57,291:INFO: - Computing loss (training)
2022-11-24 20:52:57,483:INFO: Dataset: hotel               Batch: 1/4	Loss 9.7284 (9.7284)
2022-11-24 20:52:57,510:INFO: Dataset: hotel               Batch: 2/4	Loss 9.9906 (9.8535)
2022-11-24 20:52:57,535:INFO: Dataset: hotel               Batch: 3/4	Loss 12.9022 (10.8417)
2022-11-24 20:52:57,555:INFO: Dataset: hotel               Batch: 4/4	Loss 5.6280 (9.8719)
2022-11-24 20:52:57,782:INFO: Dataset: univ                Batch:  1/15	Loss 8.4545 (8.4545)
2022-11-24 20:52:57,806:INFO: Dataset: univ                Batch:  2/15	Loss 7.7160 (8.0903)
2022-11-24 20:52:57,829:INFO: Dataset: univ                Batch:  3/15	Loss 8.0179 (8.0652)
2022-11-24 20:52:57,854:INFO: Dataset: univ                Batch:  4/15	Loss 8.3632 (8.1334)
2022-11-24 20:52:57,879:INFO: Dataset: univ                Batch:  5/15	Loss 9.2681 (8.3400)
2022-11-24 20:52:57,902:INFO: Dataset: univ                Batch:  6/15	Loss 9.1683 (8.4716)
2022-11-24 20:52:57,924:INFO: Dataset: univ                Batch:  7/15	Loss 9.7751 (8.6476)
2022-11-24 20:52:57,945:INFO: Dataset: univ                Batch:  8/15	Loss 8.6920 (8.6531)
2022-11-24 20:52:57,966:INFO: Dataset: univ                Batch:  9/15	Loss 8.7680 (8.6663)
2022-11-24 20:52:57,987:INFO: Dataset: univ                Batch: 10/15	Loss 9.0005 (8.7006)
2022-11-24 20:52:58,009:INFO: Dataset: univ                Batch: 11/15	Loss 9.8714 (8.7988)
2022-11-24 20:52:58,032:INFO: Dataset: univ                Batch: 12/15	Loss 9.9437 (8.8849)
2022-11-24 20:52:58,054:INFO: Dataset: univ                Batch: 13/15	Loss 7.6516 (8.7837)
2022-11-24 20:52:58,076:INFO: Dataset: univ                Batch: 14/15	Loss 8.6758 (8.7760)
2022-11-24 20:52:58,084:INFO: Dataset: univ                Batch: 15/15	Loss 1.8753 (8.6808)
2022-11-24 20:52:58,303:INFO: Dataset: zara1               Batch: 1/8	Loss 12.1919 (12.1919)
2022-11-24 20:52:58,326:INFO: Dataset: zara1               Batch: 2/8	Loss 12.7971 (12.5138)
2022-11-24 20:52:58,352:INFO: Dataset: zara1               Batch: 3/8	Loss 13.1766 (12.7397)
2022-11-24 20:52:58,376:INFO: Dataset: zara1               Batch: 4/8	Loss 13.3121 (12.8820)
2022-11-24 20:52:58,404:INFO: Dataset: zara1               Batch: 5/8	Loss 11.5649 (12.5979)
2022-11-24 20:52:58,428:INFO: Dataset: zara1               Batch: 6/8	Loss 13.2225 (12.6995)
2022-11-24 20:52:58,450:INFO: Dataset: zara1               Batch: 7/8	Loss 13.0188 (12.7448)
2022-11-24 20:52:58,470:INFO: Dataset: zara1               Batch: 8/8	Loss 11.4917 (12.6024)
2022-11-24 20:52:58,702:INFO: Dataset: zara2               Batch:  1/18	Loss 9.9191 (9.9191)
2022-11-24 20:52:58,733:INFO: Dataset: zara2               Batch:  2/18	Loss 9.7801 (9.8527)
2022-11-24 20:52:58,766:INFO: Dataset: zara2               Batch:  3/18	Loss 9.5200 (9.7484)
2022-11-24 20:52:58,790:INFO: Dataset: zara2               Batch:  4/18	Loss 8.9430 (9.5325)
2022-11-24 20:52:58,816:INFO: Dataset: zara2               Batch:  5/18	Loss 9.9369 (9.6044)
2022-11-24 20:52:58,843:INFO: Dataset: zara2               Batch:  6/18	Loss 9.8364 (9.6438)
2022-11-24 20:52:58,866:INFO: Dataset: zara2               Batch:  7/18	Loss 10.0743 (9.7006)
2022-11-24 20:52:58,889:INFO: Dataset: zara2               Batch:  8/18	Loss 9.5000 (9.6739)
2022-11-24 20:52:58,912:INFO: Dataset: zara2               Batch:  9/18	Loss 10.5497 (9.7799)
2022-11-24 20:52:58,935:INFO: Dataset: zara2               Batch: 10/18	Loss 8.8499 (9.6979)
2022-11-24 20:52:58,967:INFO: Dataset: zara2               Batch: 11/18	Loss 10.1717 (9.7412)
2022-11-24 20:52:58,993:INFO: Dataset: zara2               Batch: 12/18	Loss 9.6776 (9.7362)
2022-11-24 20:52:59,015:INFO: Dataset: zara2               Batch: 13/18	Loss 9.5164 (9.7205)
2022-11-24 20:52:59,037:INFO: Dataset: zara2               Batch: 14/18	Loss 10.3767 (9.7655)
2022-11-24 20:52:59,060:INFO: Dataset: zara2               Batch: 15/18	Loss 9.5787 (9.7537)
2022-11-24 20:52:59,082:INFO: Dataset: zara2               Batch: 16/18	Loss 10.8909 (9.8256)
2022-11-24 20:52:59,103:INFO: Dataset: zara2               Batch: 17/18	Loss 10.4601 (9.8618)
2022-11-24 20:52:59,122:INFO: Dataset: zara2               Batch: 18/18	Loss 8.5804 (9.8025)
2022-11-24 20:52:59,168:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_28.pth.tar
2022-11-24 20:52:59,168:INFO: 
===> EPOCH: 29 (P1)
2022-11-24 20:52:59,168:INFO: - Computing loss (training)
2022-11-24 20:52:59,369:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9688 (9.9688)
2022-11-24 20:52:59,405:INFO: Dataset: hotel               Batch: 2/4	Loss 10.2905 (10.1316)
2022-11-24 20:52:59,431:INFO: Dataset: hotel               Batch: 3/4	Loss 10.2639 (10.1760)
2022-11-24 20:52:59,451:INFO: Dataset: hotel               Batch: 4/4	Loss 7.8669 (9.7983)
2022-11-24 20:52:59,693:INFO: Dataset: univ                Batch:  1/15	Loss 8.7728 (8.7728)
2022-11-24 20:52:59,719:INFO: Dataset: univ                Batch:  2/15	Loss 8.0315 (8.3926)
2022-11-24 20:52:59,747:INFO: Dataset: univ                Batch:  3/15	Loss 9.3447 (8.6973)
2022-11-24 20:52:59,772:INFO: Dataset: univ                Batch:  4/15	Loss 8.9397 (8.7593)
2022-11-24 20:52:59,798:INFO: Dataset: univ                Batch:  5/15	Loss 7.6637 (8.5266)
2022-11-24 20:52:59,829:INFO: Dataset: univ                Batch:  6/15	Loss 9.2608 (8.6365)
2022-11-24 20:52:59,856:INFO: Dataset: univ                Batch:  7/15	Loss 8.9656 (8.6850)
2022-11-24 20:52:59,884:INFO: Dataset: univ                Batch:  8/15	Loss 8.6873 (8.6853)
2022-11-24 20:52:59,913:INFO: Dataset: univ                Batch:  9/15	Loss 7.8103 (8.5747)
2022-11-24 20:52:59,942:INFO: Dataset: univ                Batch: 10/15	Loss 8.6087 (8.5782)
2022-11-24 20:52:59,976:INFO: Dataset: univ                Batch: 11/15	Loss 8.6455 (8.5840)
2022-11-24 20:53:00,004:INFO: Dataset: univ                Batch: 12/15	Loss 9.7954 (8.6750)
2022-11-24 20:53:00,030:INFO: Dataset: univ                Batch: 13/15	Loss 8.8623 (8.6887)
2022-11-24 20:53:00,057:INFO: Dataset: univ                Batch: 14/15	Loss 7.8753 (8.6318)
2022-11-24 20:53:00,067:INFO: Dataset: univ                Batch: 15/15	Loss 1.8366 (8.5519)
2022-11-24 20:53:00,279:INFO: Dataset: zara1               Batch: 1/8	Loss 13.7415 (13.7415)
2022-11-24 20:53:00,299:INFO: Dataset: zara1               Batch: 2/8	Loss 12.9092 (13.3304)
2022-11-24 20:53:00,319:INFO: Dataset: zara1               Batch: 3/8	Loss 11.6883 (12.7757)
2022-11-24 20:53:00,341:INFO: Dataset: zara1               Batch: 4/8	Loss 13.3630 (12.9154)
2022-11-24 20:53:00,361:INFO: Dataset: zara1               Batch: 5/8	Loss 12.1294 (12.7571)
2022-11-24 20:53:00,381:INFO: Dataset: zara1               Batch: 6/8	Loss 12.5300 (12.7215)
2022-11-24 20:53:00,400:INFO: Dataset: zara1               Batch: 7/8	Loss 12.5937 (12.7030)
2022-11-24 20:53:00,417:INFO: Dataset: zara1               Batch: 8/8	Loss 10.1767 (12.4265)
2022-11-24 20:53:00,627:INFO: Dataset: zara2               Batch:  1/18	Loss 9.5382 (9.5382)
2022-11-24 20:53:00,648:INFO: Dataset: zara2               Batch:  2/18	Loss 10.1779 (9.8442)
2022-11-24 20:53:00,670:INFO: Dataset: zara2               Batch:  3/18	Loss 9.4158 (9.6913)
2022-11-24 20:53:00,692:INFO: Dataset: zara2               Batch:  4/18	Loss 10.1960 (9.8260)
2022-11-24 20:53:00,711:INFO: Dataset: zara2               Batch:  5/18	Loss 11.2831 (10.1220)
2022-11-24 20:53:00,735:INFO: Dataset: zara2               Batch:  6/18	Loss 9.7428 (10.0600)
2022-11-24 20:53:00,755:INFO: Dataset: zara2               Batch:  7/18	Loss 9.2640 (9.9375)
2022-11-24 20:53:00,774:INFO: Dataset: zara2               Batch:  8/18	Loss 10.4563 (9.9984)
2022-11-24 20:53:00,794:INFO: Dataset: zara2               Batch:  9/18	Loss 9.4032 (9.9358)
2022-11-24 20:53:00,813:INFO: Dataset: zara2               Batch: 10/18	Loss 10.2076 (9.9641)
2022-11-24 20:53:00,833:INFO: Dataset: zara2               Batch: 11/18	Loss 9.5801 (9.9277)
2022-11-24 20:53:00,854:INFO: Dataset: zara2               Batch: 12/18	Loss 8.8902 (9.8412)
2022-11-24 20:53:00,874:INFO: Dataset: zara2               Batch: 13/18	Loss 9.0600 (9.7730)
2022-11-24 20:53:00,895:INFO: Dataset: zara2               Batch: 14/18	Loss 9.7738 (9.7730)
2022-11-24 20:53:00,917:INFO: Dataset: zara2               Batch: 15/18	Loss 9.6057 (9.7624)
2022-11-24 20:53:00,938:INFO: Dataset: zara2               Batch: 16/18	Loss 9.5119 (9.7463)
2022-11-24 20:53:00,960:INFO: Dataset: zara2               Batch: 17/18	Loss 8.6333 (9.6755)
2022-11-24 20:53:00,978:INFO: Dataset: zara2               Batch: 18/18	Loss 9.0762 (9.6472)
2022-11-24 20:53:01,014:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_29.pth.tar
2022-11-24 20:53:01,015:INFO: 
===> EPOCH: 30 (P1)
2022-11-24 20:53:01,015:INFO: - Computing loss (training)
2022-11-24 20:53:01,187:INFO: Dataset: hotel               Batch: 1/4	Loss 8.0939 (8.0939)
2022-11-24 20:53:01,207:INFO: Dataset: hotel               Batch: 2/4	Loss 11.4441 (9.7441)
2022-11-24 20:53:01,226:INFO: Dataset: hotel               Batch: 3/4	Loss 11.5145 (10.3701)
2022-11-24 20:53:01,240:INFO: Dataset: hotel               Batch: 4/4	Loss 6.9738 (9.7742)
2022-11-24 20:53:01,456:INFO: Dataset: univ                Batch:  1/15	Loss 9.0972 (9.0972)
2022-11-24 20:53:01,479:INFO: Dataset: univ                Batch:  2/15	Loss 8.8565 (8.9773)
2022-11-24 20:53:01,503:INFO: Dataset: univ                Batch:  3/15	Loss 9.0791 (9.0137)
2022-11-24 20:53:01,528:INFO: Dataset: univ                Batch:  4/15	Loss 8.1127 (8.7772)
2022-11-24 20:53:01,549:INFO: Dataset: univ                Batch:  5/15	Loss 9.0025 (8.8188)
2022-11-24 20:53:01,572:INFO: Dataset: univ                Batch:  6/15	Loss 9.6420 (8.9482)
2022-11-24 20:53:01,594:INFO: Dataset: univ                Batch:  7/15	Loss 7.5375 (8.7247)
2022-11-24 20:53:01,615:INFO: Dataset: univ                Batch:  8/15	Loss 8.1109 (8.6431)
2022-11-24 20:53:01,636:INFO: Dataset: univ                Batch:  9/15	Loss 9.6024 (8.7432)
2022-11-24 20:53:01,658:INFO: Dataset: univ                Batch: 10/15	Loss 8.5168 (8.7208)
2022-11-24 20:53:01,683:INFO: Dataset: univ                Batch: 11/15	Loss 7.5999 (8.6115)
2022-11-24 20:53:01,705:INFO: Dataset: univ                Batch: 12/15	Loss 6.9868 (8.4622)
2022-11-24 20:53:01,728:INFO: Dataset: univ                Batch: 13/15	Loss 8.4022 (8.4572)
2022-11-24 20:53:01,750:INFO: Dataset: univ                Batch: 14/15	Loss 9.9615 (8.5543)
2022-11-24 20:53:01,758:INFO: Dataset: univ                Batch: 15/15	Loss 1.4748 (8.4614)
2022-11-24 20:53:01,949:INFO: Dataset: zara1               Batch: 1/8	Loss 11.2767 (11.2767)
2022-11-24 20:53:01,970:INFO: Dataset: zara1               Batch: 2/8	Loss 12.9241 (12.0736)
2022-11-24 20:53:01,992:INFO: Dataset: zara1               Batch: 3/8	Loss 12.4742 (12.2021)
2022-11-24 20:53:02,012:INFO: Dataset: zara1               Batch: 4/8	Loss 12.5939 (12.2925)
2022-11-24 20:53:02,033:INFO: Dataset: zara1               Batch: 5/8	Loss 12.0469 (12.2484)
2022-11-24 20:53:02,057:INFO: Dataset: zara1               Batch: 6/8	Loss 12.9028 (12.3541)
2022-11-24 20:53:02,081:INFO: Dataset: zara1               Batch: 7/8	Loss 12.8474 (12.4189)
2022-11-24 20:53:02,102:INFO: Dataset: zara1               Batch: 8/8	Loss 10.4269 (12.1830)
2022-11-24 20:53:02,341:INFO: Dataset: zara2               Batch:  1/18	Loss 9.7816 (9.7816)
2022-11-24 20:53:02,377:INFO: Dataset: zara2               Batch:  2/18	Loss 10.4023 (10.0793)
2022-11-24 20:53:02,398:INFO: Dataset: zara2               Batch:  3/18	Loss 9.8124 (9.9883)
2022-11-24 20:53:02,418:INFO: Dataset: zara2               Batch:  4/18	Loss 9.6095 (9.8890)
2022-11-24 20:53:02,437:INFO: Dataset: zara2               Batch:  5/18	Loss 9.1959 (9.7600)
2022-11-24 20:53:02,461:INFO: Dataset: zara2               Batch:  6/18	Loss 9.4940 (9.7169)
2022-11-24 20:53:02,481:INFO: Dataset: zara2               Batch:  7/18	Loss 10.9892 (9.9051)
2022-11-24 20:53:02,500:INFO: Dataset: zara2               Batch:  8/18	Loss 8.8546 (9.7651)
2022-11-24 20:53:02,519:INFO: Dataset: zara2               Batch:  9/18	Loss 8.3852 (9.5992)
2022-11-24 20:53:02,538:INFO: Dataset: zara2               Batch: 10/18	Loss 8.9934 (9.5224)
2022-11-24 20:53:02,557:INFO: Dataset: zara2               Batch: 11/18	Loss 9.4452 (9.5149)
2022-11-24 20:53:02,579:INFO: Dataset: zara2               Batch: 12/18	Loss 10.3817 (9.5867)
2022-11-24 20:53:02,598:INFO: Dataset: zara2               Batch: 13/18	Loss 9.8013 (9.6036)
2022-11-24 20:53:02,618:INFO: Dataset: zara2               Batch: 14/18	Loss 9.0323 (9.5670)
2022-11-24 20:53:02,639:INFO: Dataset: zara2               Batch: 15/18	Loss 8.3281 (9.4836)
2022-11-24 20:53:02,660:INFO: Dataset: zara2               Batch: 16/18	Loss 9.4490 (9.4813)
2022-11-24 20:53:02,680:INFO: Dataset: zara2               Batch: 17/18	Loss 9.3036 (9.4711)
2022-11-24 20:53:02,700:INFO: Dataset: zara2               Batch: 18/18	Loss 8.4073 (9.4231)
2022-11-24 20:53:02,740:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_30.pth.tar
2022-11-24 20:53:02,740:INFO: 
===> EPOCH: 31 (P1)
2022-11-24 20:53:02,741:INFO: - Computing loss (training)
2022-11-24 20:53:02,935:INFO: Dataset: hotel               Batch: 1/4	Loss 10.7548 (10.7548)
2022-11-24 20:53:02,958:INFO: Dataset: hotel               Batch: 2/4	Loss 11.7468 (11.2578)
2022-11-24 20:53:02,981:INFO: Dataset: hotel               Batch: 3/4	Loss 9.2522 (10.5914)
2022-11-24 20:53:02,996:INFO: Dataset: hotel               Batch: 4/4	Loss 6.6253 (9.9321)
2022-11-24 20:53:03,194:INFO: Dataset: univ                Batch:  1/15	Loss 9.8792 (9.8792)
2022-11-24 20:53:03,218:INFO: Dataset: univ                Batch:  2/15	Loss 8.0080 (8.8729)
2022-11-24 20:53:03,242:INFO: Dataset: univ                Batch:  3/15	Loss 8.4392 (8.7226)
2022-11-24 20:53:03,264:INFO: Dataset: univ                Batch:  4/15	Loss 7.9786 (8.5425)
2022-11-24 20:53:03,287:INFO: Dataset: univ                Batch:  5/15	Loss 8.5099 (8.5358)
2022-11-24 20:53:03,312:INFO: Dataset: univ                Batch:  6/15	Loss 9.0822 (8.6203)
2022-11-24 20:53:03,333:INFO: Dataset: univ                Batch:  7/15	Loss 8.2901 (8.5761)
2022-11-24 20:53:03,353:INFO: Dataset: univ                Batch:  8/15	Loss 8.5356 (8.5707)
2022-11-24 20:53:03,374:INFO: Dataset: univ                Batch:  9/15	Loss 8.8264 (8.5981)
2022-11-24 20:53:03,395:INFO: Dataset: univ                Batch: 10/15	Loss 8.9694 (8.6341)
2022-11-24 20:53:03,417:INFO: Dataset: univ                Batch: 11/15	Loss 8.5411 (8.6261)
2022-11-24 20:53:03,441:INFO: Dataset: univ                Batch: 12/15	Loss 7.4208 (8.5153)
2022-11-24 20:53:03,463:INFO: Dataset: univ                Batch: 13/15	Loss 8.0105 (8.4764)
2022-11-24 20:53:03,485:INFO: Dataset: univ                Batch: 14/15	Loss 8.1943 (8.4552)
2022-11-24 20:53:03,493:INFO: Dataset: univ                Batch: 15/15	Loss 1.3469 (8.3501)
2022-11-24 20:53:03,711:INFO: Dataset: zara1               Batch: 1/8	Loss 12.3740 (12.3740)
2022-11-24 20:53:03,736:INFO: Dataset: zara1               Batch: 2/8	Loss 12.7928 (12.5953)
2022-11-24 20:53:03,756:INFO: Dataset: zara1               Batch: 3/8	Loss 12.3338 (12.5108)
2022-11-24 20:53:03,776:INFO: Dataset: zara1               Batch: 4/8	Loss 11.6895 (12.2908)
2022-11-24 20:53:03,797:INFO: Dataset: zara1               Batch: 5/8	Loss 13.2253 (12.4641)
2022-11-24 20:53:03,819:INFO: Dataset: zara1               Batch: 6/8	Loss 10.8758 (12.1938)
2022-11-24 20:53:03,839:INFO: Dataset: zara1               Batch: 7/8	Loss 11.7675 (12.1390)
2022-11-24 20:53:03,857:INFO: Dataset: zara1               Batch: 8/8	Loss 10.5163 (11.9622)
2022-11-24 20:53:04,065:INFO: Dataset: zara2               Batch:  1/18	Loss 7.9897 (7.9897)
2022-11-24 20:53:04,088:INFO: Dataset: zara2               Batch:  2/18	Loss 9.9578 (9.0634)
2022-11-24 20:53:04,109:INFO: Dataset: zara2               Batch:  3/18	Loss 9.5873 (9.2282)
2022-11-24 20:53:04,130:INFO: Dataset: zara2               Batch:  4/18	Loss 9.2662 (9.2381)
2022-11-24 20:53:04,152:INFO: Dataset: zara2               Batch:  5/18	Loss 9.0189 (9.1986)
2022-11-24 20:53:04,175:INFO: Dataset: zara2               Batch:  6/18	Loss 9.3700 (9.2261)
2022-11-24 20:53:04,194:INFO: Dataset: zara2               Batch:  7/18	Loss 9.6881 (9.2920)
2022-11-24 20:53:04,214:INFO: Dataset: zara2               Batch:  8/18	Loss 9.6773 (9.3395)
2022-11-24 20:53:04,233:INFO: Dataset: zara2               Batch:  9/18	Loss 8.7069 (9.2678)
2022-11-24 20:53:04,253:INFO: Dataset: zara2               Batch: 10/18	Loss 8.5985 (9.2032)
2022-11-24 20:53:04,274:INFO: Dataset: zara2               Batch: 11/18	Loss 9.0453 (9.1882)
2022-11-24 20:53:04,295:INFO: Dataset: zara2               Batch: 12/18	Loss 10.1844 (9.2707)
2022-11-24 20:53:04,316:INFO: Dataset: zara2               Batch: 13/18	Loss 9.4783 (9.2865)
2022-11-24 20:53:04,338:INFO: Dataset: zara2               Batch: 14/18	Loss 10.1214 (9.3457)
2022-11-24 20:53:04,359:INFO: Dataset: zara2               Batch: 15/18	Loss 9.0200 (9.3220)
2022-11-24 20:53:04,381:INFO: Dataset: zara2               Batch: 16/18	Loss 9.7915 (9.3495)
2022-11-24 20:53:04,402:INFO: Dataset: zara2               Batch: 17/18	Loss 8.8931 (9.3226)
2022-11-24 20:53:04,420:INFO: Dataset: zara2               Batch: 18/18	Loss 7.4951 (9.2232)
2022-11-24 20:53:04,456:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_31.pth.tar
2022-11-24 20:53:04,456:INFO: 
===> EPOCH: 32 (P1)
2022-11-24 20:53:04,457:INFO: - Computing loss (training)
2022-11-24 20:53:04,639:INFO: Dataset: hotel               Batch: 1/4	Loss 10.4739 (10.4739)
2022-11-24 20:53:04,660:INFO: Dataset: hotel               Batch: 2/4	Loss 10.0060 (10.2337)
2022-11-24 20:53:04,680:INFO: Dataset: hotel               Batch: 3/4	Loss 11.0274 (10.5129)
2022-11-24 20:53:04,694:INFO: Dataset: hotel               Batch: 4/4	Loss 6.7532 (9.8978)
2022-11-24 20:53:04,919:INFO: Dataset: univ                Batch:  1/15	Loss 9.1303 (9.1303)
2022-11-24 20:53:04,940:INFO: Dataset: univ                Batch:  2/15	Loss 8.7324 (8.9309)
2022-11-24 20:53:04,964:INFO: Dataset: univ                Batch:  3/15	Loss 7.8053 (8.5278)
2022-11-24 20:53:04,985:INFO: Dataset: univ                Batch:  4/15	Loss 8.6799 (8.5697)
2022-11-24 20:53:05,009:INFO: Dataset: univ                Batch:  5/15	Loss 8.7994 (8.6127)
2022-11-24 20:53:05,032:INFO: Dataset: univ                Batch:  6/15	Loss 7.3900 (8.3902)
2022-11-24 20:53:05,053:INFO: Dataset: univ                Batch:  7/15	Loss 8.0485 (8.3381)
2022-11-24 20:53:05,073:INFO: Dataset: univ                Batch:  8/15	Loss 8.5338 (8.3617)
2022-11-24 20:53:05,095:INFO: Dataset: univ                Batch:  9/15	Loss 8.7338 (8.4023)
2022-11-24 20:53:05,116:INFO: Dataset: univ                Batch: 10/15	Loss 7.9392 (8.3557)
2022-11-24 20:53:05,141:INFO: Dataset: univ                Batch: 11/15	Loss 8.7899 (8.3959)
2022-11-24 20:53:05,163:INFO: Dataset: univ                Batch: 12/15	Loss 7.6305 (8.3257)
2022-11-24 20:53:05,186:INFO: Dataset: univ                Batch: 13/15	Loss 8.3416 (8.3269)
2022-11-24 20:53:05,207:INFO: Dataset: univ                Batch: 14/15	Loss 7.8369 (8.2920)
2022-11-24 20:53:05,217:INFO: Dataset: univ                Batch: 15/15	Loss 1.3145 (8.1918)
2022-11-24 20:53:05,416:INFO: Dataset: zara1               Batch: 1/8	Loss 11.2435 (11.2435)
2022-11-24 20:53:05,439:INFO: Dataset: zara1               Batch: 2/8	Loss 11.8977 (11.5740)
2022-11-24 20:53:05,459:INFO: Dataset: zara1               Batch: 3/8	Loss 12.1374 (11.7651)
2022-11-24 20:53:05,482:INFO: Dataset: zara1               Batch: 4/8	Loss 12.2610 (11.8795)
2022-11-24 20:53:05,503:INFO: Dataset: zara1               Batch: 5/8	Loss 11.5662 (11.8120)
2022-11-24 20:53:05,528:INFO: Dataset: zara1               Batch: 6/8	Loss 12.5864 (11.9441)
2022-11-24 20:53:05,550:INFO: Dataset: zara1               Batch: 7/8	Loss 11.1008 (11.8318)
2022-11-24 20:53:05,570:INFO: Dataset: zara1               Batch: 8/8	Loss 10.4047 (11.6801)
2022-11-24 20:53:05,863:INFO: Dataset: zara2               Batch:  1/18	Loss 9.1701 (9.1701)
2022-11-24 20:53:05,888:INFO: Dataset: zara2               Batch:  2/18	Loss 8.8800 (9.0284)
2022-11-24 20:53:05,910:INFO: Dataset: zara2               Batch:  3/18	Loss 8.4788 (8.8370)
2022-11-24 20:53:05,930:INFO: Dataset: zara2               Batch:  4/18	Loss 9.7465 (9.0618)
2022-11-24 20:53:05,950:INFO: Dataset: zara2               Batch:  5/18	Loss 8.9900 (9.0474)
2022-11-24 20:53:05,972:INFO: Dataset: zara2               Batch:  6/18	Loss 8.9606 (9.0331)
2022-11-24 20:53:05,991:INFO: Dataset: zara2               Batch:  7/18	Loss 9.3895 (9.0872)
2022-11-24 20:53:06,011:INFO: Dataset: zara2               Batch:  8/18	Loss 9.6615 (9.1594)
2022-11-24 20:53:06,031:INFO: Dataset: zara2               Batch:  9/18	Loss 8.0461 (9.0383)
2022-11-24 20:53:06,050:INFO: Dataset: zara2               Batch: 10/18	Loss 8.7525 (9.0062)
2022-11-24 20:53:06,072:INFO: Dataset: zara2               Batch: 11/18	Loss 10.1358 (9.1119)
2022-11-24 20:53:06,094:INFO: Dataset: zara2               Batch: 12/18	Loss 8.7465 (9.0816)
2022-11-24 20:53:06,115:INFO: Dataset: zara2               Batch: 13/18	Loss 9.6033 (9.1193)
2022-11-24 20:53:06,136:INFO: Dataset: zara2               Batch: 14/18	Loss 9.6326 (9.1551)
2022-11-24 20:53:06,158:INFO: Dataset: zara2               Batch: 15/18	Loss 10.1538 (9.2232)
2022-11-24 20:53:06,179:INFO: Dataset: zara2               Batch: 16/18	Loss 9.1270 (9.2170)
2022-11-24 20:53:06,200:INFO: Dataset: zara2               Batch: 17/18	Loss 8.1103 (9.1524)
2022-11-24 20:53:06,219:INFO: Dataset: zara2               Batch: 18/18	Loss 7.8509 (9.0965)
2022-11-24 20:53:06,265:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_32.pth.tar
2022-11-24 20:53:06,265:INFO: 
===> EPOCH: 33 (P1)
2022-11-24 20:53:06,265:INFO: - Computing loss (training)
2022-11-24 20:53:06,457:INFO: Dataset: hotel               Batch: 1/4	Loss 10.7710 (10.7710)
2022-11-24 20:53:06,478:INFO: Dataset: hotel               Batch: 2/4	Loss 9.4806 (10.1363)
2022-11-24 20:53:06,500:INFO: Dataset: hotel               Batch: 3/4	Loss 10.7257 (10.3231)
2022-11-24 20:53:06,513:INFO: Dataset: hotel               Batch: 4/4	Loss 7.1180 (9.7734)
2022-11-24 20:53:06,738:INFO: Dataset: univ                Batch:  1/15	Loss 8.2274 (8.2274)
2022-11-24 20:53:06,760:INFO: Dataset: univ                Batch:  2/15	Loss 8.1490 (8.1903)
2022-11-24 20:53:06,785:INFO: Dataset: univ                Batch:  3/15	Loss 8.1455 (8.1756)
2022-11-24 20:53:06,806:INFO: Dataset: univ                Batch:  4/15	Loss 7.9760 (8.1251)
2022-11-24 20:53:06,830:INFO: Dataset: univ                Batch:  5/15	Loss 7.5962 (8.0271)
2022-11-24 20:53:06,853:INFO: Dataset: univ                Batch:  6/15	Loss 8.0181 (8.0255)
2022-11-24 20:53:06,874:INFO: Dataset: univ                Batch:  7/15	Loss 7.9371 (8.0132)
2022-11-24 20:53:06,895:INFO: Dataset: univ                Batch:  8/15	Loss 9.2134 (8.1559)
2022-11-24 20:53:06,916:INFO: Dataset: univ                Batch:  9/15	Loss 8.6442 (8.2048)
2022-11-24 20:53:06,937:INFO: Dataset: univ                Batch: 10/15	Loss 8.8600 (8.2656)
2022-11-24 20:53:06,962:INFO: Dataset: univ                Batch: 11/15	Loss 6.5378 (8.0774)
2022-11-24 20:53:06,984:INFO: Dataset: univ                Batch: 12/15	Loss 8.7035 (8.1242)
2022-11-24 20:53:07,006:INFO: Dataset: univ                Batch: 13/15	Loss 8.3678 (8.1430)
2022-11-24 20:53:07,029:INFO: Dataset: univ                Batch: 14/15	Loss 9.3782 (8.2286)
2022-11-24 20:53:07,037:INFO: Dataset: univ                Batch: 15/15	Loss 1.1272 (8.1166)
2022-11-24 20:53:07,243:INFO: Dataset: zara1               Batch: 1/8	Loss 11.9104 (11.9104)
2022-11-24 20:53:07,264:INFO: Dataset: zara1               Batch: 2/8	Loss 11.2328 (11.5566)
2022-11-24 20:53:07,285:INFO: Dataset: zara1               Batch: 3/8	Loss 11.9970 (11.7007)
2022-11-24 20:53:07,307:INFO: Dataset: zara1               Batch: 4/8	Loss 12.4904 (11.8835)
2022-11-24 20:53:07,328:INFO: Dataset: zara1               Batch: 5/8	Loss 10.6753 (11.6345)
2022-11-24 20:53:07,349:INFO: Dataset: zara1               Batch: 6/8	Loss 10.8451 (11.5091)
2022-11-24 20:53:07,369:INFO: Dataset: zara1               Batch: 7/8	Loss 12.0003 (11.5824)
2022-11-24 20:53:07,387:INFO: Dataset: zara1               Batch: 8/8	Loss 9.5874 (11.3672)
2022-11-24 20:53:07,657:INFO: Dataset: zara2               Batch:  1/18	Loss 9.2629 (9.2629)
2022-11-24 20:53:07,677:INFO: Dataset: zara2               Batch:  2/18	Loss 9.0997 (9.1767)
2022-11-24 20:53:07,699:INFO: Dataset: zara2               Batch:  3/18	Loss 8.2643 (8.8922)
2022-11-24 20:53:07,720:INFO: Dataset: zara2               Batch:  4/18	Loss 9.1533 (8.9598)
2022-11-24 20:53:07,742:INFO: Dataset: zara2               Batch:  5/18	Loss 8.7714 (8.9212)
2022-11-24 20:53:07,765:INFO: Dataset: zara2               Batch:  6/18	Loss 7.4854 (8.6813)
2022-11-24 20:53:07,785:INFO: Dataset: zara2               Batch:  7/18	Loss 10.0603 (8.8612)
2022-11-24 20:53:07,805:INFO: Dataset: zara2               Batch:  8/18	Loss 9.1765 (8.9003)
2022-11-24 20:53:07,825:INFO: Dataset: zara2               Batch:  9/18	Loss 9.2549 (8.9412)
2022-11-24 20:53:07,844:INFO: Dataset: zara2               Batch: 10/18	Loss 9.1524 (8.9632)
2022-11-24 20:53:07,864:INFO: Dataset: zara2               Batch: 11/18	Loss 9.1421 (8.9784)
2022-11-24 20:53:07,886:INFO: Dataset: zara2               Batch: 12/18	Loss 9.3389 (9.0091)
2022-11-24 20:53:07,910:INFO: Dataset: zara2               Batch: 13/18	Loss 8.3940 (8.9621)
2022-11-24 20:53:07,932:INFO: Dataset: zara2               Batch: 14/18	Loss 9.4802 (8.9994)
2022-11-24 20:53:07,953:INFO: Dataset: zara2               Batch: 15/18	Loss 9.8232 (9.0507)
2022-11-24 20:53:07,974:INFO: Dataset: zara2               Batch: 16/18	Loss 8.3157 (9.0019)
2022-11-24 20:53:07,995:INFO: Dataset: zara2               Batch: 17/18	Loss 8.1231 (8.9490)
2022-11-24 20:53:08,014:INFO: Dataset: zara2               Batch: 18/18	Loss 7.4969 (8.8801)
2022-11-24 20:53:08,060:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_33.pth.tar
2022-11-24 20:53:08,060:INFO: 
===> EPOCH: 34 (P1)
2022-11-24 20:53:08,060:INFO: - Computing loss (training)
2022-11-24 20:53:08,239:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9436 (9.9436)
2022-11-24 20:53:08,260:INFO: Dataset: hotel               Batch: 2/4	Loss 10.5709 (10.2697)
2022-11-24 20:53:08,281:INFO: Dataset: hotel               Batch: 3/4	Loss 10.1875 (10.2435)
2022-11-24 20:53:08,294:INFO: Dataset: hotel               Batch: 4/4	Loss 7.5837 (9.7943)
2022-11-24 20:53:08,547:INFO: Dataset: univ                Batch:  1/15	Loss 7.6847 (7.6847)
2022-11-24 20:53:08,572:INFO: Dataset: univ                Batch:  2/15	Loss 7.6936 (7.6890)
2022-11-24 20:53:08,594:INFO: Dataset: univ                Batch:  3/15	Loss 9.1643 (8.1443)
2022-11-24 20:53:08,615:INFO: Dataset: univ                Batch:  4/15	Loss 8.3788 (8.2034)
2022-11-24 20:53:08,639:INFO: Dataset: univ                Batch:  5/15	Loss 7.8733 (8.1321)
2022-11-24 20:53:08,667:INFO: Dataset: univ                Batch:  6/15	Loss 7.2421 (7.9833)
2022-11-24 20:53:08,690:INFO: Dataset: univ                Batch:  7/15	Loss 8.9593 (8.1148)
2022-11-24 20:53:08,716:INFO: Dataset: univ                Batch:  8/15	Loss 8.1138 (8.1147)
2022-11-24 20:53:08,742:INFO: Dataset: univ                Batch:  9/15	Loss 8.4613 (8.1545)
2022-11-24 20:53:08,765:INFO: Dataset: univ                Batch: 10/15	Loss 7.6571 (8.0997)
2022-11-24 20:53:08,788:INFO: Dataset: univ                Batch: 11/15	Loss 7.9823 (8.0892)
2022-11-24 20:53:08,812:INFO: Dataset: univ                Batch: 12/15	Loss 8.5115 (8.1219)
2022-11-24 20:53:08,834:INFO: Dataset: univ                Batch: 13/15	Loss 7.6906 (8.0876)
2022-11-24 20:53:08,857:INFO: Dataset: univ                Batch: 14/15	Loss 8.8591 (8.1394)
2022-11-24 20:53:08,865:INFO: Dataset: univ                Batch: 15/15	Loss 1.7049 (8.0397)
2022-11-24 20:53:09,065:INFO: Dataset: zara1               Batch: 1/8	Loss 11.3132 (11.3132)
2022-11-24 20:53:09,087:INFO: Dataset: zara1               Batch: 2/8	Loss 11.1881 (11.2475)
2022-11-24 20:53:09,106:INFO: Dataset: zara1               Batch: 3/8	Loss 11.3373 (11.2779)
2022-11-24 20:53:09,125:INFO: Dataset: zara1               Batch: 4/8	Loss 11.0097 (11.2114)
2022-11-24 20:53:09,144:INFO: Dataset: zara1               Batch: 5/8	Loss 11.5166 (11.2695)
2022-11-24 20:53:09,165:INFO: Dataset: zara1               Batch: 6/8	Loss 11.1255 (11.2486)
2022-11-24 20:53:09,184:INFO: Dataset: zara1               Batch: 7/8	Loss 11.7812 (11.3137)
2022-11-24 20:53:09,201:INFO: Dataset: zara1               Batch: 8/8	Loss 9.9208 (11.1678)
2022-11-24 20:53:09,394:INFO: Dataset: zara2               Batch:  1/18	Loss 8.7624 (8.7624)
2022-11-24 20:53:09,452:INFO: Dataset: zara2               Batch:  2/18	Loss 8.5617 (8.6643)
2022-11-24 20:53:09,473:INFO: Dataset: zara2               Batch:  3/18	Loss 9.2252 (8.8544)
2022-11-24 20:53:09,494:INFO: Dataset: zara2               Batch:  4/18	Loss 8.9982 (8.8898)
2022-11-24 20:53:09,515:INFO: Dataset: zara2               Batch:  5/18	Loss 9.2427 (8.9607)
2022-11-24 20:53:09,538:INFO: Dataset: zara2               Batch:  6/18	Loss 8.7982 (8.9342)
2022-11-24 20:53:09,559:INFO: Dataset: zara2               Batch:  7/18	Loss 8.6718 (8.8975)
2022-11-24 20:53:09,579:INFO: Dataset: zara2               Batch:  8/18	Loss 10.2893 (9.0513)
2022-11-24 20:53:09,600:INFO: Dataset: zara2               Batch:  9/18	Loss 8.2831 (8.9728)
2022-11-24 20:53:09,621:INFO: Dataset: zara2               Batch: 10/18	Loss 8.5093 (8.9259)
2022-11-24 20:53:09,642:INFO: Dataset: zara2               Batch: 11/18	Loss 9.8571 (9.0047)
2022-11-24 20:53:09,662:INFO: Dataset: zara2               Batch: 12/18	Loss 9.7595 (9.0656)
2022-11-24 20:53:09,681:INFO: Dataset: zara2               Batch: 13/18	Loss 7.3946 (8.9398)
2022-11-24 20:53:09,701:INFO: Dataset: zara2               Batch: 14/18	Loss 7.9316 (8.8654)
2022-11-24 20:53:09,720:INFO: Dataset: zara2               Batch: 15/18	Loss 8.0057 (8.8094)
2022-11-24 20:53:09,741:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2868 (8.7786)
2022-11-24 20:53:09,761:INFO: Dataset: zara2               Batch: 17/18	Loss 9.0401 (8.7947)
2022-11-24 20:53:09,779:INFO: Dataset: zara2               Batch: 18/18	Loss 7.6246 (8.7402)
2022-11-24 20:53:09,815:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_34.pth.tar
2022-11-24 20:53:09,815:INFO: 
===> EPOCH: 35 (P1)
2022-11-24 20:53:09,815:INFO: - Computing loss (training)
2022-11-24 20:53:09,980:INFO: Dataset: hotel               Batch: 1/4	Loss 10.3560 (10.3560)
2022-11-24 20:53:10,001:INFO: Dataset: hotel               Batch: 2/4	Loss 9.6498 (9.9953)
2022-11-24 20:53:10,022:INFO: Dataset: hotel               Batch: 3/4	Loss 12.1809 (10.7134)
2022-11-24 20:53:10,036:INFO: Dataset: hotel               Batch: 4/4	Loss 5.9564 (9.8913)
2022-11-24 20:53:10,255:INFO: Dataset: univ                Batch:  1/15	Loss 8.9634 (8.9634)
2022-11-24 20:53:10,282:INFO: Dataset: univ                Batch:  2/15	Loss 7.1911 (8.0460)
2022-11-24 20:53:10,304:INFO: Dataset: univ                Batch:  3/15	Loss 8.5094 (8.1958)
2022-11-24 20:53:10,325:INFO: Dataset: univ                Batch:  4/15	Loss 8.2200 (8.2019)
2022-11-24 20:53:10,348:INFO: Dataset: univ                Batch:  5/15	Loss 7.9624 (8.1555)
2022-11-24 20:53:10,372:INFO: Dataset: univ                Batch:  6/15	Loss 8.1283 (8.1509)
2022-11-24 20:53:10,393:INFO: Dataset: univ                Batch:  7/15	Loss 8.0187 (8.1312)
2022-11-24 20:53:10,415:INFO: Dataset: univ                Batch:  8/15	Loss 7.6532 (8.0704)
2022-11-24 20:53:10,435:INFO: Dataset: univ                Batch:  9/15	Loss 8.4068 (8.1066)
2022-11-24 20:53:10,456:INFO: Dataset: univ                Batch: 10/15	Loss 7.7196 (8.0643)
2022-11-24 20:53:10,477:INFO: Dataset: univ                Batch: 11/15	Loss 8.4341 (8.0977)
2022-11-24 20:53:10,499:INFO: Dataset: univ                Batch: 12/15	Loss 7.4037 (8.0392)
2022-11-24 20:53:10,519:INFO: Dataset: univ                Batch: 13/15	Loss 8.3913 (8.0665)
2022-11-24 20:53:10,540:INFO: Dataset: univ                Batch: 14/15	Loss 6.9689 (7.9878)
2022-11-24 20:53:10,548:INFO: Dataset: univ                Batch: 15/15	Loss 1.2867 (7.9024)
2022-11-24 20:53:10,767:INFO: Dataset: zara1               Batch: 1/8	Loss 11.1321 (11.1321)
2022-11-24 20:53:10,790:INFO: Dataset: zara1               Batch: 2/8	Loss 11.1373 (11.1348)
2022-11-24 20:53:10,816:INFO: Dataset: zara1               Batch: 3/8	Loss 11.4902 (11.2568)
2022-11-24 20:53:10,837:INFO: Dataset: zara1               Batch: 4/8	Loss 11.4864 (11.3142)
2022-11-24 20:53:10,858:INFO: Dataset: zara1               Batch: 5/8	Loss 11.0478 (11.2620)
2022-11-24 20:53:10,882:INFO: Dataset: zara1               Batch: 6/8	Loss 10.5725 (11.1389)
2022-11-24 20:53:10,909:INFO: Dataset: zara1               Batch: 7/8	Loss 11.0081 (11.1186)
2022-11-24 20:53:10,941:INFO: Dataset: zara1               Batch: 8/8	Loss 9.3038 (10.9065)
2022-11-24 20:53:11,190:INFO: Dataset: zara2               Batch:  1/18	Loss 7.4371 (7.4371)
2022-11-24 20:53:11,225:INFO: Dataset: zara2               Batch:  2/18	Loss 9.3146 (8.2849)
2022-11-24 20:53:11,256:INFO: Dataset: zara2               Batch:  3/18	Loss 9.1112 (8.5612)
2022-11-24 20:53:11,287:INFO: Dataset: zara2               Batch:  4/18	Loss 9.3018 (8.7434)
2022-11-24 20:53:11,320:INFO: Dataset: zara2               Batch:  5/18	Loss 7.9532 (8.5978)
2022-11-24 20:53:11,352:INFO: Dataset: zara2               Batch:  6/18	Loss 8.2222 (8.5347)
2022-11-24 20:53:11,378:INFO: Dataset: zara2               Batch:  7/18	Loss 9.5490 (8.6892)
2022-11-24 20:53:11,401:INFO: Dataset: zara2               Batch:  8/18	Loss 9.0235 (8.7358)
2022-11-24 20:53:11,423:INFO: Dataset: zara2               Batch:  9/18	Loss 9.1808 (8.7841)
2022-11-24 20:53:11,445:INFO: Dataset: zara2               Batch: 10/18	Loss 8.0035 (8.7093)
2022-11-24 20:53:11,467:INFO: Dataset: zara2               Batch: 11/18	Loss 9.2171 (8.7561)
2022-11-24 20:53:11,491:INFO: Dataset: zara2               Batch: 12/18	Loss 8.8708 (8.7653)
2022-11-24 20:53:11,513:INFO: Dataset: zara2               Batch: 13/18	Loss 8.8903 (8.7762)
2022-11-24 20:53:11,535:INFO: Dataset: zara2               Batch: 14/18	Loss 8.3324 (8.7453)
2022-11-24 20:53:11,558:INFO: Dataset: zara2               Batch: 15/18	Loss 8.9013 (8.7560)
2022-11-24 20:53:11,581:INFO: Dataset: zara2               Batch: 16/18	Loss 7.7908 (8.6924)
2022-11-24 20:53:11,606:INFO: Dataset: zara2               Batch: 17/18	Loss 8.3538 (8.6740)
2022-11-24 20:53:11,630:INFO: Dataset: zara2               Batch: 18/18	Loss 7.1277 (8.5963)
2022-11-24 20:53:11,725:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_35.pth.tar
2022-11-24 20:53:11,725:INFO: 
===> EPOCH: 36 (P1)
2022-11-24 20:53:11,726:INFO: - Computing loss (training)
2022-11-24 20:53:11,905:INFO: Dataset: hotel               Batch: 1/4	Loss 10.3828 (10.3828)
2022-11-24 20:53:11,937:INFO: Dataset: hotel               Batch: 2/4	Loss 9.3463 (9.8833)
2022-11-24 20:53:11,962:INFO: Dataset: hotel               Batch: 3/4	Loss 11.0325 (10.2707)
2022-11-24 20:53:11,979:INFO: Dataset: hotel               Batch: 4/4	Loss 7.3631 (9.7643)
2022-11-24 20:53:12,210:INFO: Dataset: univ                Batch:  1/15	Loss 7.1769 (7.1769)
2022-11-24 20:53:12,235:INFO: Dataset: univ                Batch:  2/15	Loss 9.1640 (8.0766)
2022-11-24 20:53:12,258:INFO: Dataset: univ                Batch:  3/15	Loss 7.5262 (7.8930)
2022-11-24 20:53:12,284:INFO: Dataset: univ                Batch:  4/15	Loss 8.8914 (8.1385)
2022-11-24 20:53:12,306:INFO: Dataset: univ                Batch:  5/15	Loss 7.4360 (7.9957)
2022-11-24 20:53:12,334:INFO: Dataset: univ                Batch:  6/15	Loss 7.2420 (7.8697)
2022-11-24 20:53:12,357:INFO: Dataset: univ                Batch:  7/15	Loss 8.1272 (7.9078)
2022-11-24 20:53:12,380:INFO: Dataset: univ                Batch:  8/15	Loss 9.3490 (8.0751)
2022-11-24 20:53:12,404:INFO: Dataset: univ                Batch:  9/15	Loss 7.8388 (8.0471)
2022-11-24 20:53:12,427:INFO: Dataset: univ                Batch: 10/15	Loss 8.7389 (8.1136)
2022-11-24 20:53:12,452:INFO: Dataset: univ                Batch: 11/15	Loss 7.4204 (8.0434)
2022-11-24 20:53:12,476:INFO: Dataset: univ                Batch: 12/15	Loss 8.1903 (8.0550)
2022-11-24 20:53:12,500:INFO: Dataset: univ                Batch: 13/15	Loss 7.4765 (8.0083)
2022-11-24 20:53:12,523:INFO: Dataset: univ                Batch: 14/15	Loss 6.7693 (7.9132)
2022-11-24 20:53:12,532:INFO: Dataset: univ                Batch: 15/15	Loss 1.4140 (7.8306)
2022-11-24 20:53:12,735:INFO: Dataset: zara1               Batch: 1/8	Loss 10.9450 (10.9450)
2022-11-24 20:53:12,757:INFO: Dataset: zara1               Batch: 2/8	Loss 10.7788 (10.8564)
2022-11-24 20:53:12,776:INFO: Dataset: zara1               Batch: 3/8	Loss 11.3837 (11.0246)
2022-11-24 20:53:12,795:INFO: Dataset: zara1               Batch: 4/8	Loss 10.3221 (10.8488)
2022-11-24 20:53:12,814:INFO: Dataset: zara1               Batch: 5/8	Loss 10.3435 (10.7617)
2022-11-24 20:53:12,835:INFO: Dataset: zara1               Batch: 6/8	Loss 10.6591 (10.7450)
2022-11-24 20:53:12,854:INFO: Dataset: zara1               Batch: 7/8	Loss 10.4530 (10.7042)
2022-11-24 20:53:12,871:INFO: Dataset: zara1               Batch: 8/8	Loss 9.7024 (10.5861)
2022-11-24 20:53:13,084:INFO: Dataset: zara2               Batch:  1/18	Loss 9.5076 (9.5076)
2022-11-24 20:53:13,105:INFO: Dataset: zara2               Batch:  2/18	Loss 7.7728 (8.6005)
2022-11-24 20:53:13,144:INFO: Dataset: zara2               Batch:  3/18	Loss 8.9559 (8.7272)
2022-11-24 20:53:13,166:INFO: Dataset: zara2               Batch:  4/18	Loss 7.8435 (8.5001)
2022-11-24 20:53:13,192:INFO: Dataset: zara2               Batch:  5/18	Loss 7.6270 (8.3030)
2022-11-24 20:53:13,224:INFO: Dataset: zara2               Batch:  6/18	Loss 7.9634 (8.2430)
2022-11-24 20:53:13,248:INFO: Dataset: zara2               Batch:  7/18	Loss 8.2852 (8.2488)
2022-11-24 20:53:13,274:INFO: Dataset: zara2               Batch:  8/18	Loss 8.7546 (8.3121)
2022-11-24 20:53:13,304:INFO: Dataset: zara2               Batch:  9/18	Loss 7.7868 (8.2548)
2022-11-24 20:53:13,333:INFO: Dataset: zara2               Batch: 10/18	Loss 9.1100 (8.3378)
2022-11-24 20:53:13,359:INFO: Dataset: zara2               Batch: 11/18	Loss 9.4231 (8.4338)
2022-11-24 20:53:13,388:INFO: Dataset: zara2               Batch: 12/18	Loss 9.9213 (8.5649)
2022-11-24 20:53:13,420:INFO: Dataset: zara2               Batch: 13/18	Loss 8.4876 (8.5587)
2022-11-24 20:53:13,448:INFO: Dataset: zara2               Batch: 14/18	Loss 8.2177 (8.5317)
2022-11-24 20:53:13,473:INFO: Dataset: zara2               Batch: 15/18	Loss 7.5726 (8.4614)
2022-11-24 20:53:13,498:INFO: Dataset: zara2               Batch: 16/18	Loss 8.9222 (8.4915)
2022-11-24 20:53:13,524:INFO: Dataset: zara2               Batch: 17/18	Loss 9.6448 (8.5570)
2022-11-24 20:53:13,547:INFO: Dataset: zara2               Batch: 18/18	Loss 7.0349 (8.4832)
2022-11-24 20:53:13,605:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_36.pth.tar
2022-11-24 20:53:13,606:INFO: 
===> EPOCH: 37 (P1)
2022-11-24 20:53:13,606:INFO: - Computing loss (training)
2022-11-24 20:53:13,852:INFO: Dataset: hotel               Batch: 1/4	Loss 10.5285 (10.5285)
2022-11-24 20:53:13,883:INFO: Dataset: hotel               Batch: 2/4	Loss 10.8608 (10.6978)
2022-11-24 20:53:13,927:INFO: Dataset: hotel               Batch: 3/4	Loss 11.2409 (10.8806)
2022-11-24 20:53:13,953:INFO: Dataset: hotel               Batch: 4/4	Loss 5.6912 (10.0248)
2022-11-24 20:53:14,237:INFO: Dataset: univ                Batch:  1/15	Loss 8.6941 (8.6941)
2022-11-24 20:53:14,265:INFO: Dataset: univ                Batch:  2/15	Loss 7.4424 (8.0212)
2022-11-24 20:53:14,294:INFO: Dataset: univ                Batch:  3/15	Loss 6.8650 (7.6031)
2022-11-24 20:53:14,319:INFO: Dataset: univ                Batch:  4/15	Loss 7.3795 (7.5509)
2022-11-24 20:53:14,345:INFO: Dataset: univ                Batch:  5/15	Loss 7.2894 (7.5004)
2022-11-24 20:53:14,369:INFO: Dataset: univ                Batch:  6/15	Loss 7.6177 (7.5197)
2022-11-24 20:53:14,390:INFO: Dataset: univ                Batch:  7/15	Loss 7.5788 (7.5284)
2022-11-24 20:53:14,411:INFO: Dataset: univ                Batch:  8/15	Loss 8.4139 (7.6329)
2022-11-24 20:53:14,432:INFO: Dataset: univ                Batch:  9/15	Loss 7.7139 (7.6419)
2022-11-24 20:53:14,454:INFO: Dataset: univ                Batch: 10/15	Loss 8.4294 (7.7233)
2022-11-24 20:53:14,477:INFO: Dataset: univ                Batch: 11/15	Loss 7.6530 (7.7168)
2022-11-24 20:53:14,498:INFO: Dataset: univ                Batch: 12/15	Loss 8.8871 (7.8166)
2022-11-24 20:53:14,519:INFO: Dataset: univ                Batch: 13/15	Loss 7.1391 (7.7619)
2022-11-24 20:53:14,540:INFO: Dataset: univ                Batch: 14/15	Loss 7.7495 (7.7609)
2022-11-24 20:53:14,548:INFO: Dataset: univ                Batch: 15/15	Loss 1.5089 (7.6859)
2022-11-24 20:53:14,746:INFO: Dataset: zara1               Batch: 1/8	Loss 11.7724 (11.7724)
2022-11-24 20:53:14,769:INFO: Dataset: zara1               Batch: 2/8	Loss 10.9734 (11.3908)
2022-11-24 20:53:14,788:INFO: Dataset: zara1               Batch: 3/8	Loss 10.8423 (11.1928)
2022-11-24 20:53:14,808:INFO: Dataset: zara1               Batch: 4/8	Loss 10.8071 (11.0998)
2022-11-24 20:53:14,828:INFO: Dataset: zara1               Batch: 5/8	Loss 9.7783 (10.8344)
2022-11-24 20:53:14,849:INFO: Dataset: zara1               Batch: 6/8	Loss 9.4970 (10.6001)
2022-11-24 20:53:14,869:INFO: Dataset: zara1               Batch: 7/8	Loss 10.4055 (10.5745)
2022-11-24 20:53:14,886:INFO: Dataset: zara1               Batch: 8/8	Loss 8.9218 (10.3910)
2022-11-24 20:53:15,105:INFO: Dataset: zara2               Batch:  1/18	Loss 8.2554 (8.2554)
2022-11-24 20:53:15,130:INFO: Dataset: zara2               Batch:  2/18	Loss 8.9014 (8.5624)
2022-11-24 20:53:15,156:INFO: Dataset: zara2               Batch:  3/18	Loss 7.2697 (8.1236)
2022-11-24 20:53:15,180:INFO: Dataset: zara2               Batch:  4/18	Loss 7.7559 (8.0389)
2022-11-24 20:53:15,203:INFO: Dataset: zara2               Batch:  5/18	Loss 8.2584 (8.0847)
2022-11-24 20:53:15,230:INFO: Dataset: zara2               Batch:  6/18	Loss 9.1986 (8.2591)
2022-11-24 20:53:15,251:INFO: Dataset: zara2               Batch:  7/18	Loss 8.8552 (8.3286)
2022-11-24 20:53:15,272:INFO: Dataset: zara2               Batch:  8/18	Loss 9.0074 (8.4160)
2022-11-24 20:53:15,294:INFO: Dataset: zara2               Batch:  9/18	Loss 6.9583 (8.2559)
2022-11-24 20:53:15,316:INFO: Dataset: zara2               Batch: 10/18	Loss 7.7515 (8.2063)
2022-11-24 20:53:15,337:INFO: Dataset: zara2               Batch: 11/18	Loss 8.2952 (8.2147)
2022-11-24 20:53:15,360:INFO: Dataset: zara2               Batch: 12/18	Loss 7.9276 (8.1922)
2022-11-24 20:53:15,382:INFO: Dataset: zara2               Batch: 13/18	Loss 8.3379 (8.2033)
2022-11-24 20:53:15,402:INFO: Dataset: zara2               Batch: 14/18	Loss 9.2909 (8.2708)
2022-11-24 20:53:15,421:INFO: Dataset: zara2               Batch: 15/18	Loss 7.2937 (8.2078)
2022-11-24 20:53:15,442:INFO: Dataset: zara2               Batch: 16/18	Loss 9.3017 (8.2695)
2022-11-24 20:53:15,462:INFO: Dataset: zara2               Batch: 17/18	Loss 8.6055 (8.2893)
2022-11-24 20:53:15,479:INFO: Dataset: zara2               Batch: 18/18	Loss 6.9679 (8.2258)
2022-11-24 20:53:15,525:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_37.pth.tar
2022-11-24 20:53:15,525:INFO: 
===> EPOCH: 38 (P1)
2022-11-24 20:53:15,525:INFO: - Computing loss (training)
2022-11-24 20:53:15,702:INFO: Dataset: hotel               Batch: 1/4	Loss 10.2384 (10.2384)
2022-11-24 20:53:15,721:INFO: Dataset: hotel               Batch: 2/4	Loss 12.4093 (11.3986)
2022-11-24 20:53:15,742:INFO: Dataset: hotel               Batch: 3/4	Loss 9.0836 (10.6380)
2022-11-24 20:53:15,758:INFO: Dataset: hotel               Batch: 4/4	Loss 6.3071 (9.8896)
2022-11-24 20:53:15,980:INFO: Dataset: univ                Batch:  1/15	Loss 8.0009 (8.0009)
2022-11-24 20:53:16,003:INFO: Dataset: univ                Batch:  2/15	Loss 7.6190 (7.7910)
2022-11-24 20:53:16,027:INFO: Dataset: univ                Batch:  3/15	Loss 8.1290 (7.9078)
2022-11-24 20:53:16,048:INFO: Dataset: univ                Batch:  4/15	Loss 7.0398 (7.6779)
2022-11-24 20:53:16,071:INFO: Dataset: univ                Batch:  5/15	Loss 7.6322 (7.6693)
2022-11-24 20:53:16,095:INFO: Dataset: univ                Batch:  6/15	Loss 7.7410 (7.6811)
2022-11-24 20:53:16,116:INFO: Dataset: univ                Batch:  7/15	Loss 8.1131 (7.7458)
2022-11-24 20:53:16,136:INFO: Dataset: univ                Batch:  8/15	Loss 9.0122 (7.8960)
2022-11-24 20:53:16,157:INFO: Dataset: univ                Batch:  9/15	Loss 8.4251 (7.9524)
2022-11-24 20:53:16,177:INFO: Dataset: univ                Batch: 10/15	Loss 6.9145 (7.8375)
2022-11-24 20:53:16,198:INFO: Dataset: univ                Batch: 11/15	Loss 7.9714 (7.8507)
2022-11-24 20:53:16,220:INFO: Dataset: univ                Batch: 12/15	Loss 7.3066 (7.8036)
2022-11-24 20:53:16,241:INFO: Dataset: univ                Batch: 13/15	Loss 7.0738 (7.7443)
2022-11-24 20:53:16,261:INFO: Dataset: univ                Batch: 14/15	Loss 7.5384 (7.7299)
2022-11-24 20:53:16,269:INFO: Dataset: univ                Batch: 15/15	Loss 1.1343 (7.6361)
2022-11-24 20:53:16,490:INFO: Dataset: zara1               Batch: 1/8	Loss 10.6732 (10.6732)
2022-11-24 20:53:16,514:INFO: Dataset: zara1               Batch: 2/8	Loss 9.5789 (10.1313)
2022-11-24 20:53:16,538:INFO: Dataset: zara1               Batch: 3/8	Loss 10.9395 (10.3838)
2022-11-24 20:53:16,557:INFO: Dataset: zara1               Batch: 4/8	Loss 10.1166 (10.3168)
2022-11-24 20:53:16,578:INFO: Dataset: zara1               Batch: 5/8	Loss 10.0784 (10.2723)
2022-11-24 20:53:16,597:INFO: Dataset: zara1               Batch: 6/8	Loss 10.6494 (10.3335)
2022-11-24 20:53:16,616:INFO: Dataset: zara1               Batch: 7/8	Loss 10.2408 (10.3206)
2022-11-24 20:53:16,634:INFO: Dataset: zara1               Batch: 8/8	Loss 9.0746 (10.1921)
2022-11-24 20:53:16,861:INFO: Dataset: zara2               Batch:  1/18	Loss 8.2693 (8.2693)
2022-11-24 20:53:16,884:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2743 (7.7800)
2022-11-24 20:53:16,907:INFO: Dataset: zara2               Batch:  3/18	Loss 7.8435 (7.8000)
2022-11-24 20:53:16,931:INFO: Dataset: zara2               Batch:  4/18	Loss 8.0852 (7.8676)
2022-11-24 20:53:16,952:INFO: Dataset: zara2               Batch:  5/18	Loss 8.2923 (7.9517)
2022-11-24 20:53:16,974:INFO: Dataset: zara2               Batch:  6/18	Loss 8.5864 (8.0530)
2022-11-24 20:53:16,994:INFO: Dataset: zara2               Batch:  7/18	Loss 8.0678 (8.0554)
2022-11-24 20:53:17,015:INFO: Dataset: zara2               Batch:  8/18	Loss 8.6587 (8.1326)
2022-11-24 20:53:17,035:INFO: Dataset: zara2               Batch:  9/18	Loss 8.2258 (8.1422)
2022-11-24 20:53:17,054:INFO: Dataset: zara2               Batch: 10/18	Loss 7.2988 (8.0547)
2022-11-24 20:53:17,073:INFO: Dataset: zara2               Batch: 11/18	Loss 8.6554 (8.1048)
2022-11-24 20:53:17,094:INFO: Dataset: zara2               Batch: 12/18	Loss 8.7522 (8.1542)
2022-11-24 20:53:17,113:INFO: Dataset: zara2               Batch: 13/18	Loss 8.2124 (8.1582)
2022-11-24 20:53:17,132:INFO: Dataset: zara2               Batch: 14/18	Loss 6.8107 (8.0625)
2022-11-24 20:53:17,153:INFO: Dataset: zara2               Batch: 15/18	Loss 8.4946 (8.0884)
2022-11-24 20:53:17,173:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2525 (8.0993)
2022-11-24 20:53:17,193:INFO: Dataset: zara2               Batch: 17/18	Loss 8.9315 (8.1436)
2022-11-24 20:53:17,211:INFO: Dataset: zara2               Batch: 18/18	Loss 7.2005 (8.0989)
2022-11-24 20:53:17,254:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_38.pth.tar
2022-11-24 20:53:17,254:INFO: 
===> EPOCH: 39 (P1)
2022-11-24 20:53:17,255:INFO: - Computing loss (training)
2022-11-24 20:53:17,434:INFO: Dataset: hotel               Batch: 1/4	Loss 10.0277 (10.0277)
2022-11-24 20:53:17,454:INFO: Dataset: hotel               Batch: 2/4	Loss 11.1219 (10.5648)
2022-11-24 20:53:17,476:INFO: Dataset: hotel               Batch: 3/4	Loss 10.1313 (10.4257)
2022-11-24 20:53:17,490:INFO: Dataset: hotel               Batch: 4/4	Loss 7.1506 (9.9245)
2022-11-24 20:53:17,701:INFO: Dataset: univ                Batch:  1/15	Loss 7.6352 (7.6352)
2022-11-24 20:53:17,726:INFO: Dataset: univ                Batch:  2/15	Loss 6.8398 (7.2407)
2022-11-24 20:53:17,747:INFO: Dataset: univ                Batch:  3/15	Loss 7.0947 (7.1970)
2022-11-24 20:53:17,768:INFO: Dataset: univ                Batch:  4/15	Loss 7.4561 (7.2557)
2022-11-24 20:53:17,790:INFO: Dataset: univ                Batch:  5/15	Loss 6.9090 (7.1872)
2022-11-24 20:53:17,816:INFO: Dataset: univ                Batch:  6/15	Loss 7.8458 (7.2876)
2022-11-24 20:53:17,837:INFO: Dataset: univ                Batch:  7/15	Loss 8.6097 (7.4741)
2022-11-24 20:53:17,857:INFO: Dataset: univ                Batch:  8/15	Loss 6.9005 (7.3958)
2022-11-24 20:53:17,880:INFO: Dataset: univ                Batch:  9/15	Loss 7.4206 (7.3985)
2022-11-24 20:53:17,904:INFO: Dataset: univ                Batch: 10/15	Loss 8.4329 (7.4929)
2022-11-24 20:53:17,928:INFO: Dataset: univ                Batch: 11/15	Loss 7.7405 (7.5148)
2022-11-24 20:53:17,955:INFO: Dataset: univ                Batch: 12/15	Loss 8.7917 (7.6056)
2022-11-24 20:53:17,978:INFO: Dataset: univ                Batch: 13/15	Loss 6.9311 (7.5535)
2022-11-24 20:53:18,001:INFO: Dataset: univ                Batch: 14/15	Loss 8.0165 (7.5854)
2022-11-24 20:53:18,011:INFO: Dataset: univ                Batch: 15/15	Loss 1.2759 (7.5050)
2022-11-24 20:53:18,221:INFO: Dataset: zara1               Batch: 1/8	Loss 10.8971 (10.8971)
2022-11-24 20:53:18,243:INFO: Dataset: zara1               Batch: 2/8	Loss 10.4544 (10.6800)
2022-11-24 20:53:18,265:INFO: Dataset: zara1               Batch: 3/8	Loss 9.8260 (10.3957)
2022-11-24 20:53:18,288:INFO: Dataset: zara1               Batch: 4/8	Loss 11.0910 (10.5682)
2022-11-24 20:53:18,308:INFO: Dataset: zara1               Batch: 5/8	Loss 10.5550 (10.5653)
2022-11-24 20:53:18,329:INFO: Dataset: zara1               Batch: 6/8	Loss 9.8725 (10.4441)
2022-11-24 20:53:18,349:INFO: Dataset: zara1               Batch: 7/8	Loss 9.4061 (10.2874)
2022-11-24 20:53:18,366:INFO: Dataset: zara1               Batch: 8/8	Loss 8.9388 (10.1476)
2022-11-24 20:53:18,605:INFO: Dataset: zara2               Batch:  1/18	Loss 7.4346 (7.4346)
2022-11-24 20:53:18,626:INFO: Dataset: zara2               Batch:  2/18	Loss 8.0126 (7.7112)
2022-11-24 20:53:18,645:INFO: Dataset: zara2               Batch:  3/18	Loss 8.2849 (7.9164)
2022-11-24 20:53:18,665:INFO: Dataset: zara2               Batch:  4/18	Loss 8.4048 (8.0389)
2022-11-24 20:53:18,684:INFO: Dataset: zara2               Batch:  5/18	Loss 7.6663 (7.9638)
2022-11-24 20:53:18,707:INFO: Dataset: zara2               Batch:  6/18	Loss 7.7295 (7.9254)
2022-11-24 20:53:18,727:INFO: Dataset: zara2               Batch:  7/18	Loss 9.2386 (8.1013)
2022-11-24 20:53:18,745:INFO: Dataset: zara2               Batch:  8/18	Loss 7.4820 (8.0181)
2022-11-24 20:53:18,764:INFO: Dataset: zara2               Batch:  9/18	Loss 8.3445 (8.0559)
2022-11-24 20:53:18,783:INFO: Dataset: zara2               Batch: 10/18	Loss 8.0970 (8.0599)
2022-11-24 20:53:18,803:INFO: Dataset: zara2               Batch: 11/18	Loss 7.4927 (8.0138)
2022-11-24 20:53:18,828:INFO: Dataset: zara2               Batch: 12/18	Loss 7.9235 (8.0057)
2022-11-24 20:53:18,851:INFO: Dataset: zara2               Batch: 13/18	Loss 8.1904 (8.0194)
2022-11-24 20:53:18,875:INFO: Dataset: zara2               Batch: 14/18	Loss 8.2470 (8.0337)
2022-11-24 20:53:18,896:INFO: Dataset: zara2               Batch: 15/18	Loss 7.6155 (8.0016)
2022-11-24 20:53:18,918:INFO: Dataset: zara2               Batch: 16/18	Loss 7.8121 (7.9893)
2022-11-24 20:53:18,940:INFO: Dataset: zara2               Batch: 17/18	Loss 8.7340 (8.0331)
2022-11-24 20:53:18,959:INFO: Dataset: zara2               Batch: 18/18	Loss 6.8956 (7.9767)
2022-11-24 20:53:19,001:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_39.pth.tar
2022-11-24 20:53:19,001:INFO: 
===> EPOCH: 40 (P1)
2022-11-24 20:53:19,002:INFO: - Computing loss (training)
2022-11-24 20:53:19,179:INFO: Dataset: hotel               Batch: 1/4	Loss 10.5436 (10.5436)
2022-11-24 20:53:19,222:INFO: Dataset: hotel               Batch: 2/4	Loss 11.9997 (11.3010)
2022-11-24 20:53:19,247:INFO: Dataset: hotel               Batch: 3/4	Loss 9.0689 (10.5773)
2022-11-24 20:53:19,264:INFO: Dataset: hotel               Batch: 4/4	Loss 6.5266 (9.8559)
2022-11-24 20:53:19,610:INFO: Dataset: univ                Batch:  1/15	Loss 8.7580 (8.7580)
2022-11-24 20:53:19,651:INFO: Dataset: univ                Batch:  2/15	Loss 7.1456 (7.8806)
2022-11-24 20:53:19,696:INFO: Dataset: univ                Batch:  3/15	Loss 7.6858 (7.8184)
2022-11-24 20:53:19,752:INFO: Dataset: univ                Batch:  4/15	Loss 7.4358 (7.7232)
2022-11-24 20:53:19,797:INFO: Dataset: univ                Batch:  5/15	Loss 8.8734 (7.9383)
2022-11-24 20:53:19,846:INFO: Dataset: univ                Batch:  6/15	Loss 6.4280 (7.6665)
2022-11-24 20:53:19,890:INFO: Dataset: univ                Batch:  7/15	Loss 7.7181 (7.6736)
2022-11-24 20:53:19,935:INFO: Dataset: univ                Batch:  8/15	Loss 6.6632 (7.5337)
2022-11-24 20:53:19,972:INFO: Dataset: univ                Batch:  9/15	Loss 6.9571 (7.4691)
2022-11-24 20:53:20,005:INFO: Dataset: univ                Batch: 10/15	Loss 8.2547 (7.5430)
2022-11-24 20:53:20,037:INFO: Dataset: univ                Batch: 11/15	Loss 7.4054 (7.5305)
2022-11-24 20:53:20,069:INFO: Dataset: univ                Batch: 12/15	Loss 6.8401 (7.4739)
2022-11-24 20:53:20,099:INFO: Dataset: univ                Batch: 13/15	Loss 7.2386 (7.4555)
2022-11-24 20:53:20,121:INFO: Dataset: univ                Batch: 14/15	Loss 7.0514 (7.4253)
2022-11-24 20:53:20,130:INFO: Dataset: univ                Batch: 15/15	Loss 1.1949 (7.3461)
2022-11-24 20:53:20,327:INFO: Dataset: zara1               Batch: 1/8	Loss 9.7099 (9.7099)
2022-11-24 20:53:20,348:INFO: Dataset: zara1               Batch: 2/8	Loss 9.8792 (9.7995)
2022-11-24 20:53:20,371:INFO: Dataset: zara1               Batch: 3/8	Loss 9.4013 (9.6516)
2022-11-24 20:53:20,392:INFO: Dataset: zara1               Batch: 4/8	Loss 10.5206 (9.8556)
2022-11-24 20:53:20,414:INFO: Dataset: zara1               Batch: 5/8	Loss 11.0704 (10.0923)
2022-11-24 20:53:20,435:INFO: Dataset: zara1               Batch: 6/8	Loss 10.3624 (10.1364)
2022-11-24 20:53:20,455:INFO: Dataset: zara1               Batch: 7/8	Loss 9.9027 (10.1035)
2022-11-24 20:53:20,472:INFO: Dataset: zara1               Batch: 8/8	Loss 8.2001 (9.8801)
2022-11-24 20:53:20,668:INFO: Dataset: zara2               Batch:  1/18	Loss 7.4671 (7.4671)
2022-11-24 20:53:20,692:INFO: Dataset: zara2               Batch:  2/18	Loss 7.6777 (7.5670)
2022-11-24 20:53:20,712:INFO: Dataset: zara2               Batch:  3/18	Loss 8.0132 (7.7201)
2022-11-24 20:53:20,732:INFO: Dataset: zara2               Batch:  4/18	Loss 7.2337 (7.6076)
2022-11-24 20:53:20,752:INFO: Dataset: zara2               Batch:  5/18	Loss 7.4063 (7.5697)
2022-11-24 20:53:20,776:INFO: Dataset: zara2               Batch:  6/18	Loss 8.0614 (7.6603)
2022-11-24 20:53:20,795:INFO: Dataset: zara2               Batch:  7/18	Loss 8.5526 (7.7933)
2022-11-24 20:53:20,814:INFO: Dataset: zara2               Batch:  8/18	Loss 7.8493 (7.8005)
2022-11-24 20:53:20,833:INFO: Dataset: zara2               Batch:  9/18	Loss 8.1823 (7.8432)
2022-11-24 20:53:20,852:INFO: Dataset: zara2               Batch: 10/18	Loss 7.2455 (7.7791)
2022-11-24 20:53:20,871:INFO: Dataset: zara2               Batch: 11/18	Loss 7.6350 (7.7670)
2022-11-24 20:53:20,893:INFO: Dataset: zara2               Batch: 12/18	Loss 8.0050 (7.7868)
2022-11-24 20:53:20,913:INFO: Dataset: zara2               Batch: 13/18	Loss 7.2491 (7.7428)
2022-11-24 20:53:20,934:INFO: Dataset: zara2               Batch: 14/18	Loss 7.9864 (7.7600)
2022-11-24 20:53:20,955:INFO: Dataset: zara2               Batch: 15/18	Loss 7.9093 (7.7698)
2022-11-24 20:53:20,978:INFO: Dataset: zara2               Batch: 16/18	Loss 7.8865 (7.7770)
2022-11-24 20:53:20,999:INFO: Dataset: zara2               Batch: 17/18	Loss 8.6281 (7.8278)
2022-11-24 20:53:21,018:INFO: Dataset: zara2               Batch: 18/18	Loss 6.3258 (7.7523)
2022-11-24 20:53:21,054:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_40.pth.tar
2022-11-24 20:53:21,054:INFO: 
===> EPOCH: 41 (P1)
2022-11-24 20:53:21,054:INFO: - Computing loss (training)
2022-11-24 20:53:21,225:INFO: Dataset: hotel               Batch: 1/4	Loss 10.8041 (10.8041)
2022-11-24 20:53:21,248:INFO: Dataset: hotel               Batch: 2/4	Loss 9.7433 (10.2568)
2022-11-24 20:53:21,268:INFO: Dataset: hotel               Batch: 3/4	Loss 12.5475 (11.0412)
2022-11-24 20:53:21,282:INFO: Dataset: hotel               Batch: 4/4	Loss 5.5260 (10.0517)
2022-11-24 20:53:21,519:INFO: Dataset: univ                Batch:  1/15	Loss 7.1113 (7.1113)
2022-11-24 20:53:21,544:INFO: Dataset: univ                Batch:  2/15	Loss 7.2444 (7.1777)
2022-11-24 20:53:21,569:INFO: Dataset: univ                Batch:  3/15	Loss 7.3308 (7.2296)
2022-11-24 20:53:21,591:INFO: Dataset: univ                Batch:  4/15	Loss 7.4807 (7.2938)
2022-11-24 20:53:21,615:INFO: Dataset: univ                Batch:  5/15	Loss 7.3250 (7.2998)
2022-11-24 20:53:21,642:INFO: Dataset: univ                Batch:  6/15	Loss 7.9728 (7.4018)
2022-11-24 20:53:21,665:INFO: Dataset: univ                Batch:  7/15	Loss 8.6197 (7.5584)
2022-11-24 20:53:21,687:INFO: Dataset: univ                Batch:  8/15	Loss 7.5133 (7.5529)
2022-11-24 20:53:21,709:INFO: Dataset: univ                Batch:  9/15	Loss 7.2753 (7.5196)
2022-11-24 20:53:21,732:INFO: Dataset: univ                Batch: 10/15	Loss 6.9901 (7.4657)
2022-11-24 20:53:21,756:INFO: Dataset: univ                Batch: 11/15	Loss 7.2061 (7.4421)
2022-11-24 20:53:21,780:INFO: Dataset: univ                Batch: 12/15	Loss 7.7780 (7.4678)
2022-11-24 20:53:21,802:INFO: Dataset: univ                Batch: 13/15	Loss 7.3051 (7.4552)
2022-11-24 20:53:21,826:INFO: Dataset: univ                Batch: 14/15	Loss 6.8665 (7.4124)
2022-11-24 20:53:21,835:INFO: Dataset: univ                Batch: 15/15	Loss 1.1893 (7.3192)
2022-11-24 20:53:22,063:INFO: Dataset: zara1               Batch: 1/8	Loss 9.4258 (9.4258)
2022-11-24 20:53:22,084:INFO: Dataset: zara1               Batch: 2/8	Loss 9.6506 (9.5378)
2022-11-24 20:53:22,105:INFO: Dataset: zara1               Batch: 3/8	Loss 9.4925 (9.5222)
2022-11-24 20:53:22,126:INFO: Dataset: zara1               Batch: 4/8	Loss 9.9512 (9.6255)
2022-11-24 20:53:22,146:INFO: Dataset: zara1               Batch: 5/8	Loss 9.9887 (9.7039)
2022-11-24 20:53:22,168:INFO: Dataset: zara1               Batch: 6/8	Loss 10.1675 (9.7760)
2022-11-24 20:53:22,187:INFO: Dataset: zara1               Batch: 7/8	Loss 10.5972 (9.8873)
2022-11-24 20:53:22,204:INFO: Dataset: zara1               Batch: 8/8	Loss 8.3531 (9.7056)
2022-11-24 20:53:22,404:INFO: Dataset: zara2               Batch:  1/18	Loss 7.8739 (7.8739)
2022-11-24 20:53:22,426:INFO: Dataset: zara2               Batch:  2/18	Loss 7.9663 (7.9202)
2022-11-24 20:53:22,448:INFO: Dataset: zara2               Batch:  3/18	Loss 6.9283 (7.5873)
2022-11-24 20:53:22,471:INFO: Dataset: zara2               Batch:  4/18	Loss 8.5526 (7.8266)
2022-11-24 20:53:22,493:INFO: Dataset: zara2               Batch:  5/18	Loss 7.7756 (7.8165)
2022-11-24 20:53:22,515:INFO: Dataset: zara2               Batch:  6/18	Loss 8.2309 (7.8809)
2022-11-24 20:53:22,534:INFO: Dataset: zara2               Batch:  7/18	Loss 7.0465 (7.7503)
2022-11-24 20:53:22,553:INFO: Dataset: zara2               Batch:  8/18	Loss 7.4309 (7.7126)
2022-11-24 20:53:22,573:INFO: Dataset: zara2               Batch:  9/18	Loss 8.0202 (7.7486)
2022-11-24 20:53:22,592:INFO: Dataset: zara2               Batch: 10/18	Loss 8.9309 (7.8706)
2022-11-24 20:53:22,611:INFO: Dataset: zara2               Batch: 11/18	Loss 7.3999 (7.8319)
2022-11-24 20:53:22,633:INFO: Dataset: zara2               Batch: 12/18	Loss 8.1698 (7.8588)
2022-11-24 20:53:22,653:INFO: Dataset: zara2               Batch: 13/18	Loss 7.6717 (7.8435)
2022-11-24 20:53:22,673:INFO: Dataset: zara2               Batch: 14/18	Loss 6.6506 (7.7498)
2022-11-24 20:53:22,693:INFO: Dataset: zara2               Batch: 15/18	Loss 7.8118 (7.7535)
2022-11-24 20:53:22,713:INFO: Dataset: zara2               Batch: 16/18	Loss 8.2456 (7.7834)
2022-11-24 20:53:22,734:INFO: Dataset: zara2               Batch: 17/18	Loss 7.8550 (7.7880)
2022-11-24 20:53:22,752:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0749 (7.7025)
2022-11-24 20:53:22,787:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_41.pth.tar
2022-11-24 20:53:22,788:INFO: 
===> EPOCH: 42 (P1)
2022-11-24 20:53:22,788:INFO: - Computing loss (training)
2022-11-24 20:53:22,962:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9621 (9.9621)
2022-11-24 20:53:22,983:INFO: Dataset: hotel               Batch: 2/4	Loss 9.9808 (9.9721)
2022-11-24 20:53:23,007:INFO: Dataset: hotel               Batch: 3/4	Loss 11.3970 (10.4508)
2022-11-24 20:53:23,020:INFO: Dataset: hotel               Batch: 4/4	Loss 6.2203 (9.7755)
2022-11-24 20:53:23,229:INFO: Dataset: univ                Batch:  1/15	Loss 7.3468 (7.3468)
2022-11-24 20:53:23,252:INFO: Dataset: univ                Batch:  2/15	Loss 6.9705 (7.1578)
2022-11-24 20:53:23,278:INFO: Dataset: univ                Batch:  3/15	Loss 7.4113 (7.2378)
2022-11-24 20:53:23,299:INFO: Dataset: univ                Batch:  4/15	Loss 7.2965 (7.2516)
2022-11-24 20:53:23,322:INFO: Dataset: univ                Batch:  5/15	Loss 7.6748 (7.3373)
2022-11-24 20:53:23,346:INFO: Dataset: univ                Batch:  6/15	Loss 7.5671 (7.3755)
2022-11-24 20:53:23,367:INFO: Dataset: univ                Batch:  7/15	Loss 7.0333 (7.3235)
2022-11-24 20:53:23,388:INFO: Dataset: univ                Batch:  8/15	Loss 7.3095 (7.3220)
2022-11-24 20:53:23,410:INFO: Dataset: univ                Batch:  9/15	Loss 7.0815 (7.2966)
2022-11-24 20:53:23,431:INFO: Dataset: univ                Batch: 10/15	Loss 6.9985 (7.2650)
2022-11-24 20:53:23,452:INFO: Dataset: univ                Batch: 11/15	Loss 7.3738 (7.2746)
2022-11-24 20:53:23,476:INFO: Dataset: univ                Batch: 12/15	Loss 6.4780 (7.2029)
2022-11-24 20:53:23,498:INFO: Dataset: univ                Batch: 13/15	Loss 7.4656 (7.2221)
2022-11-24 20:53:23,520:INFO: Dataset: univ                Batch: 14/15	Loss 7.2853 (7.2264)
2022-11-24 20:53:23,529:INFO: Dataset: univ                Batch: 15/15	Loss 1.6430 (7.1473)
2022-11-24 20:53:23,722:INFO: Dataset: zara1               Batch: 1/8	Loss 9.7584 (9.7584)
2022-11-24 20:53:23,745:INFO: Dataset: zara1               Batch: 2/8	Loss 10.5362 (10.1481)
2022-11-24 20:53:23,767:INFO: Dataset: zara1               Batch: 3/8	Loss 9.6670 (10.0006)
2022-11-24 20:53:23,788:INFO: Dataset: zara1               Batch: 4/8	Loss 9.4429 (9.8476)
2022-11-24 20:53:23,810:INFO: Dataset: zara1               Batch: 5/8	Loss 9.3454 (9.7442)
2022-11-24 20:53:23,831:INFO: Dataset: zara1               Batch: 6/8	Loss 8.9847 (9.6118)
2022-11-24 20:53:23,850:INFO: Dataset: zara1               Batch: 7/8	Loss 9.6304 (9.6144)
2022-11-24 20:53:23,868:INFO: Dataset: zara1               Batch: 8/8	Loss 8.1922 (9.4490)
2022-11-24 20:53:24,070:INFO: Dataset: zara2               Batch:  1/18	Loss 7.5632 (7.5632)
2022-11-24 20:53:24,090:INFO: Dataset: zara2               Batch:  2/18	Loss 7.6765 (7.6235)
2022-11-24 20:53:24,111:INFO: Dataset: zara2               Batch:  3/18	Loss 7.6677 (7.6395)
2022-11-24 20:53:24,133:INFO: Dataset: zara2               Batch:  4/18	Loss 8.0654 (7.7443)
2022-11-24 20:53:24,153:INFO: Dataset: zara2               Batch:  5/18	Loss 7.7901 (7.7538)
2022-11-24 20:53:24,175:INFO: Dataset: zara2               Batch:  6/18	Loss 8.0826 (7.8080)
2022-11-24 20:53:24,194:INFO: Dataset: zara2               Batch:  7/18	Loss 7.3529 (7.7461)
2022-11-24 20:53:24,213:INFO: Dataset: zara2               Batch:  8/18	Loss 7.0462 (7.6590)
2022-11-24 20:53:24,232:INFO: Dataset: zara2               Batch:  9/18	Loss 7.8559 (7.6812)
2022-11-24 20:53:24,251:INFO: Dataset: zara2               Batch: 10/18	Loss 6.7645 (7.5888)
2022-11-24 20:53:24,271:INFO: Dataset: zara2               Batch: 11/18	Loss 7.3256 (7.5661)
2022-11-24 20:53:24,291:INFO: Dataset: zara2               Batch: 12/18	Loss 7.4469 (7.5568)
2022-11-24 20:53:24,311:INFO: Dataset: zara2               Batch: 13/18	Loss 7.4780 (7.5505)
2022-11-24 20:53:24,331:INFO: Dataset: zara2               Batch: 14/18	Loss 8.5271 (7.6175)
2022-11-24 20:53:24,352:INFO: Dataset: zara2               Batch: 15/18	Loss 7.0132 (7.5768)
2022-11-24 20:53:24,374:INFO: Dataset: zara2               Batch: 16/18	Loss 7.7903 (7.5904)
2022-11-24 20:53:24,394:INFO: Dataset: zara2               Batch: 17/18	Loss 7.4484 (7.5819)
2022-11-24 20:53:24,412:INFO: Dataset: zara2               Batch: 18/18	Loss 6.5832 (7.5304)
2022-11-24 20:53:24,454:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_42.pth.tar
2022-11-24 20:53:24,454:INFO: 
===> EPOCH: 43 (P1)
2022-11-24 20:53:24,455:INFO: - Computing loss (training)
2022-11-24 20:53:24,637:INFO: Dataset: hotel               Batch: 1/4	Loss 10.6125 (10.6125)
2022-11-24 20:53:24,657:INFO: Dataset: hotel               Batch: 2/4	Loss 11.1460 (10.8768)
2022-11-24 20:53:24,676:INFO: Dataset: hotel               Batch: 3/4	Loss 10.4997 (10.7590)
2022-11-24 20:53:24,689:INFO: Dataset: hotel               Batch: 4/4	Loss 5.8482 (9.9751)
2022-11-24 20:53:24,899:INFO: Dataset: univ                Batch:  1/15	Loss 7.3988 (7.3988)
2022-11-24 20:53:24,923:INFO: Dataset: univ                Batch:  2/15	Loss 7.5195 (7.4568)
2022-11-24 20:53:24,946:INFO: Dataset: univ                Batch:  3/15	Loss 6.7312 (7.2121)
2022-11-24 20:53:24,972:INFO: Dataset: univ                Batch:  4/15	Loss 7.3851 (7.2558)
2022-11-24 20:53:24,993:INFO: Dataset: univ                Batch:  5/15	Loss 7.3300 (7.2697)
2022-11-24 20:53:25,017:INFO: Dataset: univ                Batch:  6/15	Loss 7.4913 (7.3053)
2022-11-24 20:53:25,038:INFO: Dataset: univ                Batch:  7/15	Loss 7.4347 (7.3257)
2022-11-24 20:53:25,059:INFO: Dataset: univ                Batch:  8/15	Loss 7.2804 (7.3199)
2022-11-24 20:53:25,080:INFO: Dataset: univ                Batch:  9/15	Loss 7.1105 (7.2961)
2022-11-24 20:53:25,101:INFO: Dataset: univ                Batch: 10/15	Loss 7.3628 (7.3027)
2022-11-24 20:53:25,122:INFO: Dataset: univ                Batch: 11/15	Loss 7.2089 (7.2942)
2022-11-24 20:53:25,145:INFO: Dataset: univ                Batch: 12/15	Loss 7.9369 (7.3440)
2022-11-24 20:53:25,167:INFO: Dataset: univ                Batch: 13/15	Loss 6.0942 (7.2365)
2022-11-24 20:53:25,188:INFO: Dataset: univ                Batch: 14/15	Loss 6.9404 (7.2143)
2022-11-24 20:53:25,197:INFO: Dataset: univ                Batch: 15/15	Loss 1.8933 (7.1659)
2022-11-24 20:53:25,410:INFO: Dataset: zara1               Batch: 1/8	Loss 9.4471 (9.4471)
2022-11-24 20:53:25,430:INFO: Dataset: zara1               Batch: 2/8	Loss 9.2110 (9.3308)
2022-11-24 20:53:25,449:INFO: Dataset: zara1               Batch: 3/8	Loss 9.2994 (9.3213)
2022-11-24 20:53:25,469:INFO: Dataset: zara1               Batch: 4/8	Loss 9.1710 (9.2869)
2022-11-24 20:53:25,488:INFO: Dataset: zara1               Batch: 5/8	Loss 9.7969 (9.3781)
2022-11-24 20:53:25,509:INFO: Dataset: zara1               Batch: 6/8	Loss 9.5665 (9.4104)
2022-11-24 20:53:25,528:INFO: Dataset: zara1               Batch: 7/8	Loss 8.8058 (9.3223)
2022-11-24 20:53:25,545:INFO: Dataset: zara1               Batch: 8/8	Loss 8.1162 (9.1884)
2022-11-24 20:53:25,751:INFO: Dataset: zara2               Batch:  1/18	Loss 6.9157 (6.9157)
2022-11-24 20:53:25,770:INFO: Dataset: zara2               Batch:  2/18	Loss 7.5968 (7.2501)
2022-11-24 20:53:25,789:INFO: Dataset: zara2               Batch:  3/18	Loss 6.0951 (6.8508)
2022-11-24 20:53:25,811:INFO: Dataset: zara2               Batch:  4/18	Loss 7.2745 (6.9622)
2022-11-24 20:53:25,832:INFO: Dataset: zara2               Batch:  5/18	Loss 7.3010 (7.0262)
2022-11-24 20:53:25,854:INFO: Dataset: zara2               Batch:  6/18	Loss 7.4804 (7.1027)
2022-11-24 20:53:25,872:INFO: Dataset: zara2               Batch:  7/18	Loss 7.2102 (7.1180)
2022-11-24 20:53:25,891:INFO: Dataset: zara2               Batch:  8/18	Loss 7.3675 (7.1497)
2022-11-24 20:53:25,910:INFO: Dataset: zara2               Batch:  9/18	Loss 7.6037 (7.1994)
2022-11-24 20:53:25,929:INFO: Dataset: zara2               Batch: 10/18	Loss 7.6656 (7.2416)
2022-11-24 20:53:25,948:INFO: Dataset: zara2               Batch: 11/18	Loss 7.7249 (7.2846)
2022-11-24 20:53:25,968:INFO: Dataset: zara2               Batch: 12/18	Loss 7.1408 (7.2721)
2022-11-24 20:53:25,988:INFO: Dataset: zara2               Batch: 13/18	Loss 7.1043 (7.2579)
2022-11-24 20:53:26,008:INFO: Dataset: zara2               Batch: 14/18	Loss 7.4440 (7.2707)
2022-11-24 20:53:26,028:INFO: Dataset: zara2               Batch: 15/18	Loss 7.4712 (7.2845)
2022-11-24 20:53:26,049:INFO: Dataset: zara2               Batch: 16/18	Loss 8.3362 (7.3446)
2022-11-24 20:53:26,069:INFO: Dataset: zara2               Batch: 17/18	Loss 7.8872 (7.3744)
2022-11-24 20:53:26,086:INFO: Dataset: zara2               Batch: 18/18	Loss 6.8581 (7.3494)
2022-11-24 20:53:26,122:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_43.pth.tar
2022-11-24 20:53:26,122:INFO: 
===> EPOCH: 44 (P1)
2022-11-24 20:53:26,123:INFO: - Computing loss (training)
2022-11-24 20:53:26,289:INFO: Dataset: hotel               Batch: 1/4	Loss 9.4121 (9.4121)
2022-11-24 20:53:26,310:INFO: Dataset: hotel               Batch: 2/4	Loss 9.9909 (9.7119)
2022-11-24 20:53:26,330:INFO: Dataset: hotel               Batch: 3/4	Loss 12.4134 (10.6280)
2022-11-24 20:53:26,345:INFO: Dataset: hotel               Batch: 4/4	Loss 6.1322 (9.8925)
2022-11-24 20:53:26,564:INFO: Dataset: univ                Batch:  1/15	Loss 6.9220 (6.9220)
2022-11-24 20:53:26,587:INFO: Dataset: univ                Batch:  2/15	Loss 6.4856 (6.6907)
2022-11-24 20:53:26,608:INFO: Dataset: univ                Batch:  3/15	Loss 7.6628 (6.9930)
2022-11-24 20:53:26,630:INFO: Dataset: univ                Batch:  4/15	Loss 7.5609 (7.1311)
2022-11-24 20:53:26,654:INFO: Dataset: univ                Batch:  5/15	Loss 7.3358 (7.1720)
2022-11-24 20:53:26,678:INFO: Dataset: univ                Batch:  6/15	Loss 8.2814 (7.3376)
2022-11-24 20:53:26,699:INFO: Dataset: univ                Batch:  7/15	Loss 7.3264 (7.3360)
2022-11-24 20:53:26,720:INFO: Dataset: univ                Batch:  8/15	Loss 6.8233 (7.2712)
2022-11-24 20:53:26,740:INFO: Dataset: univ                Batch:  9/15	Loss 6.6042 (7.1948)
2022-11-24 20:53:26,762:INFO: Dataset: univ                Batch: 10/15	Loss 7.1339 (7.1884)
2022-11-24 20:53:26,785:INFO: Dataset: univ                Batch: 11/15	Loss 7.3975 (7.2086)
2022-11-24 20:53:26,808:INFO: Dataset: univ                Batch: 12/15	Loss 6.8415 (7.1788)
2022-11-24 20:53:26,830:INFO: Dataset: univ                Batch: 13/15	Loss 6.3548 (7.1110)
2022-11-24 20:53:26,853:INFO: Dataset: univ                Batch: 14/15	Loss 6.7411 (7.0843)
2022-11-24 20:53:26,862:INFO: Dataset: univ                Batch: 15/15	Loss 1.2176 (7.0042)
2022-11-24 20:53:27,059:INFO: Dataset: zara1               Batch: 1/8	Loss 9.3307 (9.3307)
2022-11-24 20:53:27,079:INFO: Dataset: zara1               Batch: 2/8	Loss 9.1419 (9.2477)
2022-11-24 20:53:27,103:INFO: Dataset: zara1               Batch: 3/8	Loss 9.2284 (9.2408)
2022-11-24 20:53:27,123:INFO: Dataset: zara1               Batch: 4/8	Loss 9.3951 (9.2767)
2022-11-24 20:53:27,143:INFO: Dataset: zara1               Batch: 5/8	Loss 9.6599 (9.3544)
2022-11-24 20:53:27,163:INFO: Dataset: zara1               Batch: 6/8	Loss 9.0531 (9.2983)
2022-11-24 20:53:27,183:INFO: Dataset: zara1               Batch: 7/8	Loss 9.6315 (9.3428)
2022-11-24 20:53:27,200:INFO: Dataset: zara1               Batch: 8/8	Loss 6.9538 (9.0901)
2022-11-24 20:53:27,407:INFO: Dataset: zara2               Batch:  1/18	Loss 7.6796 (7.6796)
2022-11-24 20:53:27,454:INFO: Dataset: zara2               Batch:  2/18	Loss 7.8524 (7.7695)
2022-11-24 20:53:27,477:INFO: Dataset: zara2               Batch:  3/18	Loss 7.8421 (7.7931)
2022-11-24 20:53:27,498:INFO: Dataset: zara2               Batch:  4/18	Loss 7.5495 (7.7358)
2022-11-24 20:53:27,517:INFO: Dataset: zara2               Batch:  5/18	Loss 8.0629 (7.8052)
2022-11-24 20:53:27,540:INFO: Dataset: zara2               Batch:  6/18	Loss 6.7163 (7.6231)
2022-11-24 20:53:27,560:INFO: Dataset: zara2               Batch:  7/18	Loss 6.5465 (7.4716)
2022-11-24 20:53:27,580:INFO: Dataset: zara2               Batch:  8/18	Loss 6.4616 (7.3437)
2022-11-24 20:53:27,599:INFO: Dataset: zara2               Batch:  9/18	Loss 8.2555 (7.4467)
2022-11-24 20:53:27,619:INFO: Dataset: zara2               Batch: 10/18	Loss 6.8817 (7.3889)
2022-11-24 20:53:27,640:INFO: Dataset: zara2               Batch: 11/18	Loss 6.4805 (7.3011)
2022-11-24 20:53:27,662:INFO: Dataset: zara2               Batch: 12/18	Loss 7.3518 (7.3052)
2022-11-24 20:53:27,683:INFO: Dataset: zara2               Batch: 13/18	Loss 7.5623 (7.3227)
2022-11-24 20:53:27,703:INFO: Dataset: zara2               Batch: 14/18	Loss 6.8741 (7.2903)
2022-11-24 20:53:27,723:INFO: Dataset: zara2               Batch: 15/18	Loss 7.7410 (7.3222)
2022-11-24 20:53:27,744:INFO: Dataset: zara2               Batch: 16/18	Loss 7.0188 (7.3037)
2022-11-24 20:53:27,765:INFO: Dataset: zara2               Batch: 17/18	Loss 6.8070 (7.2731)
2022-11-24 20:53:27,783:INFO: Dataset: zara2               Batch: 18/18	Loss 6.2925 (7.2294)
2022-11-24 20:53:27,823:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_44.pth.tar
2022-11-24 20:53:27,823:INFO: 
===> EPOCH: 45 (P1)
2022-11-24 20:53:27,823:INFO: - Computing loss (training)
2022-11-24 20:53:28,000:INFO: Dataset: hotel               Batch: 1/4	Loss 11.7363 (11.7363)
2022-11-24 20:53:28,020:INFO: Dataset: hotel               Batch: 2/4	Loss 9.9750 (10.8328)
2022-11-24 20:53:28,042:INFO: Dataset: hotel               Batch: 3/4	Loss 10.0431 (10.5725)
2022-11-24 20:53:28,056:INFO: Dataset: hotel               Batch: 4/4	Loss 5.7526 (9.7649)
2022-11-24 20:53:28,261:INFO: Dataset: univ                Batch:  1/15	Loss 6.2263 (6.2263)
2022-11-24 20:53:28,284:INFO: Dataset: univ                Batch:  2/15	Loss 6.5054 (6.3711)
2022-11-24 20:53:28,309:INFO: Dataset: univ                Batch:  3/15	Loss 7.0279 (6.5871)
2022-11-24 20:53:28,330:INFO: Dataset: univ                Batch:  4/15	Loss 7.2965 (6.7589)
2022-11-24 20:53:28,354:INFO: Dataset: univ                Batch:  5/15	Loss 7.5561 (6.9134)
2022-11-24 20:53:28,376:INFO: Dataset: univ                Batch:  6/15	Loss 7.4216 (6.9937)
2022-11-24 20:53:28,397:INFO: Dataset: univ                Batch:  7/15	Loss 6.5698 (6.9276)
2022-11-24 20:53:28,418:INFO: Dataset: univ                Batch:  8/15	Loss 6.8017 (6.9114)
2022-11-24 20:53:28,439:INFO: Dataset: univ                Batch:  9/15	Loss 6.7855 (6.8969)
2022-11-24 20:53:28,460:INFO: Dataset: univ                Batch: 10/15	Loss 6.4351 (6.8498)
2022-11-24 20:53:28,483:INFO: Dataset: univ                Batch: 11/15	Loss 7.2207 (6.8847)
2022-11-24 20:53:28,505:INFO: Dataset: univ                Batch: 12/15	Loss 6.8716 (6.8837)
2022-11-24 20:53:28,526:INFO: Dataset: univ                Batch: 13/15	Loss 6.5108 (6.8543)
2022-11-24 20:53:28,548:INFO: Dataset: univ                Batch: 14/15	Loss 7.8427 (6.9226)
2022-11-24 20:53:28,557:INFO: Dataset: univ                Batch: 15/15	Loss 1.3356 (6.8403)
2022-11-24 20:53:28,760:INFO: Dataset: zara1               Batch: 1/8	Loss 9.2160 (9.2160)
2022-11-24 20:53:28,782:INFO: Dataset: zara1               Batch: 2/8	Loss 8.7703 (9.0083)
2022-11-24 20:53:28,802:INFO: Dataset: zara1               Batch: 3/8	Loss 9.2295 (9.0890)
2022-11-24 20:53:28,823:INFO: Dataset: zara1               Batch: 4/8	Loss 9.1351 (9.1019)
2022-11-24 20:53:28,845:INFO: Dataset: zara1               Batch: 5/8	Loss 9.4268 (9.1663)
2022-11-24 20:53:28,866:INFO: Dataset: zara1               Batch: 6/8	Loss 8.7548 (9.0936)
2022-11-24 20:53:28,885:INFO: Dataset: zara1               Batch: 7/8	Loss 9.1711 (9.1034)
2022-11-24 20:53:28,902:INFO: Dataset: zara1               Batch: 8/8	Loss 8.8062 (9.0687)
2022-11-24 20:53:29,145:INFO: Dataset: zara2               Batch:  1/18	Loss 7.1879 (7.1879)
2022-11-24 20:53:29,167:INFO: Dataset: zara2               Batch:  2/18	Loss 7.2590 (7.2232)
2022-11-24 20:53:29,186:INFO: Dataset: zara2               Batch:  3/18	Loss 7.1393 (7.1945)
2022-11-24 20:53:29,204:INFO: Dataset: zara2               Batch:  4/18	Loss 7.0402 (7.1565)
2022-11-24 20:53:29,223:INFO: Dataset: zara2               Batch:  5/18	Loss 6.8648 (7.0991)
2022-11-24 20:53:29,244:INFO: Dataset: zara2               Batch:  6/18	Loss 7.8478 (7.2046)
2022-11-24 20:53:29,263:INFO: Dataset: zara2               Batch:  7/18	Loss 7.7984 (7.2847)
2022-11-24 20:53:29,282:INFO: Dataset: zara2               Batch:  8/18	Loss 7.3014 (7.2868)
2022-11-24 20:53:29,301:INFO: Dataset: zara2               Batch:  9/18	Loss 6.3793 (7.1825)
2022-11-24 20:53:29,319:INFO: Dataset: zara2               Batch: 10/18	Loss 6.9669 (7.1606)
2022-11-24 20:53:29,338:INFO: Dataset: zara2               Batch: 11/18	Loss 6.7793 (7.1236)
2022-11-24 20:53:29,359:INFO: Dataset: zara2               Batch: 12/18	Loss 7.0854 (7.1206)
2022-11-24 20:53:29,380:INFO: Dataset: zara2               Batch: 13/18	Loss 8.1645 (7.2054)
2022-11-24 20:53:29,400:INFO: Dataset: zara2               Batch: 14/18	Loss 6.5186 (7.1552)
2022-11-24 20:53:29,421:INFO: Dataset: zara2               Batch: 15/18	Loss 7.6258 (7.1879)
2022-11-24 20:53:29,441:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4140 (7.1436)
2022-11-24 20:53:29,463:INFO: Dataset: zara2               Batch: 17/18	Loss 7.3556 (7.1557)
2022-11-24 20:53:29,481:INFO: Dataset: zara2               Batch: 18/18	Loss 6.0546 (7.1034)
2022-11-24 20:53:29,523:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_45.pth.tar
2022-11-24 20:53:29,523:INFO: 
===> EPOCH: 46 (P1)
2022-11-24 20:53:29,523:INFO: - Computing loss (training)
2022-11-24 20:53:29,691:INFO: Dataset: hotel               Batch: 1/4	Loss 11.4949 (11.4949)
2022-11-24 20:53:29,714:INFO: Dataset: hotel               Batch: 2/4	Loss 10.5475 (11.0442)
2022-11-24 20:53:29,735:INFO: Dataset: hotel               Batch: 3/4	Loss 9.5141 (10.5195)
2022-11-24 20:53:29,748:INFO: Dataset: hotel               Batch: 4/4	Loss 5.9356 (9.7273)
2022-11-24 20:53:29,956:INFO: Dataset: univ                Batch:  1/15	Loss 6.8917 (6.8917)
2022-11-24 20:53:29,978:INFO: Dataset: univ                Batch:  2/15	Loss 7.1479 (7.0202)
2022-11-24 20:53:30,003:INFO: Dataset: univ                Batch:  3/15	Loss 7.0345 (7.0249)
2022-11-24 20:53:30,023:INFO: Dataset: univ                Batch:  4/15	Loss 6.7706 (6.9634)
2022-11-24 20:53:30,044:INFO: Dataset: univ                Batch:  5/15	Loss 7.3876 (7.0412)
2022-11-24 20:53:30,068:INFO: Dataset: univ                Batch:  6/15	Loss 6.9345 (7.0246)
2022-11-24 20:53:30,088:INFO: Dataset: univ                Batch:  7/15	Loss 7.7359 (7.1245)
2022-11-24 20:53:30,109:INFO: Dataset: univ                Batch:  8/15	Loss 7.3500 (7.1533)
2022-11-24 20:53:30,129:INFO: Dataset: univ                Batch:  9/15	Loss 6.3220 (7.0550)
2022-11-24 20:53:30,150:INFO: Dataset: univ                Batch: 10/15	Loss 6.3183 (6.9699)
2022-11-24 20:53:30,170:INFO: Dataset: univ                Batch: 11/15	Loss 6.8244 (6.9568)
2022-11-24 20:53:30,192:INFO: Dataset: univ                Batch: 12/15	Loss 6.5586 (6.9215)
2022-11-24 20:53:30,213:INFO: Dataset: univ                Batch: 13/15	Loss 6.9722 (6.9256)
2022-11-24 20:53:30,234:INFO: Dataset: univ                Batch: 14/15	Loss 6.8610 (6.9209)
2022-11-24 20:53:30,242:INFO: Dataset: univ                Batch: 15/15	Loss 1.0481 (6.8329)
2022-11-24 20:53:30,447:INFO: Dataset: zara1               Batch: 1/8	Loss 8.6627 (8.6627)
2022-11-24 20:53:30,468:INFO: Dataset: zara1               Batch: 2/8	Loss 9.0934 (8.8632)
2022-11-24 20:53:30,489:INFO: Dataset: zara1               Batch: 3/8	Loss 9.1332 (8.9528)
2022-11-24 20:53:30,510:INFO: Dataset: zara1               Batch: 4/8	Loss 8.4114 (8.8064)
2022-11-24 20:53:30,529:INFO: Dataset: zara1               Batch: 5/8	Loss 8.6067 (8.7633)
2022-11-24 20:53:30,548:INFO: Dataset: zara1               Batch: 6/8	Loss 8.8901 (8.7872)
2022-11-24 20:53:30,567:INFO: Dataset: zara1               Batch: 7/8	Loss 8.8008 (8.7890)
2022-11-24 20:53:30,584:INFO: Dataset: zara1               Batch: 8/8	Loss 7.6705 (8.6572)
2022-11-24 20:53:30,800:INFO: Dataset: zara2               Batch:  1/18	Loss 6.7588 (6.7588)
2022-11-24 20:53:30,820:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1997 (6.9772)
2022-11-24 20:53:30,840:INFO: Dataset: zara2               Batch:  3/18	Loss 6.4320 (6.7881)
2022-11-24 20:53:30,864:INFO: Dataset: zara2               Batch:  4/18	Loss 7.5190 (6.9726)
2022-11-24 20:53:30,883:INFO: Dataset: zara2               Batch:  5/18	Loss 7.6803 (7.1118)
2022-11-24 20:53:30,905:INFO: Dataset: zara2               Batch:  6/18	Loss 6.5317 (7.0189)
2022-11-24 20:53:30,923:INFO: Dataset: zara2               Batch:  7/18	Loss 6.9376 (7.0075)
2022-11-24 20:53:30,942:INFO: Dataset: zara2               Batch:  8/18	Loss 6.4607 (6.9373)
2022-11-24 20:53:30,961:INFO: Dataset: zara2               Batch:  9/18	Loss 7.9258 (7.0434)
2022-11-24 20:53:30,980:INFO: Dataset: zara2               Batch: 10/18	Loss 7.3785 (7.0837)
2022-11-24 20:53:31,001:INFO: Dataset: zara2               Batch: 11/18	Loss 6.9599 (7.0705)
2022-11-24 20:53:31,021:INFO: Dataset: zara2               Batch: 12/18	Loss 6.6972 (7.0393)
2022-11-24 20:53:31,040:INFO: Dataset: zara2               Batch: 13/18	Loss 6.8252 (7.0221)
2022-11-24 20:53:31,060:INFO: Dataset: zara2               Batch: 14/18	Loss 7.0541 (7.0243)
2022-11-24 20:53:31,081:INFO: Dataset: zara2               Batch: 15/18	Loss 7.4736 (7.0522)
2022-11-24 20:53:31,102:INFO: Dataset: zara2               Batch: 16/18	Loss 6.9194 (7.0439)
2022-11-24 20:53:31,122:INFO: Dataset: zara2               Batch: 17/18	Loss 7.1509 (7.0500)
2022-11-24 20:53:31,140:INFO: Dataset: zara2               Batch: 18/18	Loss 6.6204 (7.0263)
2022-11-24 20:53:31,182:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_46.pth.tar
2022-11-24 20:53:31,183:INFO: 
===> EPOCH: 47 (P1)
2022-11-24 20:53:31,183:INFO: - Computing loss (training)
2022-11-24 20:53:31,361:INFO: Dataset: hotel               Batch: 1/4	Loss 10.7402 (10.7402)
2022-11-24 20:53:31,381:INFO: Dataset: hotel               Batch: 2/4	Loss 10.6381 (10.6880)
2022-11-24 20:53:31,400:INFO: Dataset: hotel               Batch: 3/4	Loss 9.8079 (10.3913)
2022-11-24 20:53:31,414:INFO: Dataset: hotel               Batch: 4/4	Loss 6.2258 (9.6165)
2022-11-24 20:53:31,633:INFO: Dataset: univ                Batch:  1/15	Loss 7.2836 (7.2836)
2022-11-24 20:53:31,666:INFO: Dataset: univ                Batch:  2/15	Loss 6.9929 (7.1267)
2022-11-24 20:53:31,687:INFO: Dataset: univ                Batch:  3/15	Loss 6.5110 (6.9249)
2022-11-24 20:53:31,708:INFO: Dataset: univ                Batch:  4/15	Loss 6.0028 (6.6841)
2022-11-24 20:53:31,728:INFO: Dataset: univ                Batch:  5/15	Loss 6.6482 (6.6768)
2022-11-24 20:53:31,752:INFO: Dataset: univ                Batch:  6/15	Loss 6.6308 (6.6687)
2022-11-24 20:53:31,773:INFO: Dataset: univ                Batch:  7/15	Loss 6.9518 (6.7070)
2022-11-24 20:53:31,793:INFO: Dataset: univ                Batch:  8/15	Loss 7.0199 (6.7437)
2022-11-24 20:53:31,814:INFO: Dataset: univ                Batch:  9/15	Loss 7.4097 (6.8165)
2022-11-24 20:53:31,835:INFO: Dataset: univ                Batch: 10/15	Loss 6.1191 (6.7375)
2022-11-24 20:53:31,857:INFO: Dataset: univ                Batch: 11/15	Loss 7.6335 (6.8083)
2022-11-24 20:53:31,880:INFO: Dataset: univ                Batch: 12/15	Loss 6.2439 (6.7608)
2022-11-24 20:53:31,901:INFO: Dataset: univ                Batch: 13/15	Loss 6.7185 (6.7574)
2022-11-24 20:53:31,924:INFO: Dataset: univ                Batch: 14/15	Loss 6.7611 (6.7577)
2022-11-24 20:53:31,932:INFO: Dataset: univ                Batch: 15/15	Loss 1.3962 (6.7000)
2022-11-24 20:53:32,140:INFO: Dataset: zara1               Batch: 1/8	Loss 8.8482 (8.8482)
2022-11-24 20:53:32,159:INFO: Dataset: zara1               Batch: 2/8	Loss 9.2352 (9.0541)
2022-11-24 20:53:32,178:INFO: Dataset: zara1               Batch: 3/8	Loss 8.8419 (8.9924)
2022-11-24 20:53:32,197:INFO: Dataset: zara1               Batch: 4/8	Loss 8.6679 (8.9090)
2022-11-24 20:53:32,218:INFO: Dataset: zara1               Batch: 5/8	Loss 8.6451 (8.8538)
2022-11-24 20:53:32,237:INFO: Dataset: zara1               Batch: 6/8	Loss 7.8773 (8.6826)
2022-11-24 20:53:32,256:INFO: Dataset: zara1               Batch: 7/8	Loss 8.6870 (8.6832)
2022-11-24 20:53:32,273:INFO: Dataset: zara1               Batch: 8/8	Loss 7.3816 (8.5359)
2022-11-24 20:53:32,490:INFO: Dataset: zara2               Batch:  1/18	Loss 6.7148 (6.7148)
2022-11-24 20:53:32,511:INFO: Dataset: zara2               Batch:  2/18	Loss 7.1101 (6.9047)
2022-11-24 20:53:32,531:INFO: Dataset: zara2               Batch:  3/18	Loss 6.2394 (6.7066)
2022-11-24 20:53:32,550:INFO: Dataset: zara2               Batch:  4/18	Loss 7.2547 (6.8402)
2022-11-24 20:53:32,574:INFO: Dataset: zara2               Batch:  5/18	Loss 6.9204 (6.8556)
2022-11-24 20:53:32,596:INFO: Dataset: zara2               Batch:  6/18	Loss 6.9911 (6.8786)
2022-11-24 20:53:32,615:INFO: Dataset: zara2               Batch:  7/18	Loss 6.8731 (6.8777)
2022-11-24 20:53:32,634:INFO: Dataset: zara2               Batch:  8/18	Loss 7.1985 (6.9137)
2022-11-24 20:53:32,653:INFO: Dataset: zara2               Batch:  9/18	Loss 8.0717 (7.0350)
2022-11-24 20:53:32,671:INFO: Dataset: zara2               Batch: 10/18	Loss 7.4042 (7.0687)
2022-11-24 20:53:32,690:INFO: Dataset: zara2               Batch: 11/18	Loss 6.7162 (7.0377)
2022-11-24 20:53:32,710:INFO: Dataset: zara2               Batch: 12/18	Loss 6.9199 (7.0284)
2022-11-24 20:53:32,729:INFO: Dataset: zara2               Batch: 13/18	Loss 6.9190 (7.0203)
2022-11-24 20:53:32,749:INFO: Dataset: zara2               Batch: 14/18	Loss 5.9843 (6.9459)
2022-11-24 20:53:32,768:INFO: Dataset: zara2               Batch: 15/18	Loss 6.5199 (6.9195)
2022-11-24 20:53:32,790:INFO: Dataset: zara2               Batch: 16/18	Loss 6.8575 (6.9158)
2022-11-24 20:53:32,810:INFO: Dataset: zara2               Batch: 17/18	Loss 6.4785 (6.8906)
2022-11-24 20:53:32,829:INFO: Dataset: zara2               Batch: 18/18	Loss 5.6510 (6.8279)
2022-11-24 20:53:32,870:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_47.pth.tar
2022-11-24 20:53:32,870:INFO: 
===> EPOCH: 48 (P1)
2022-11-24 20:53:32,871:INFO: - Computing loss (training)
2022-11-24 20:53:33,038:INFO: Dataset: hotel               Batch: 1/4	Loss 10.2297 (10.2297)
2022-11-24 20:53:33,057:INFO: Dataset: hotel               Batch: 2/4	Loss 10.4066 (10.3143)
2022-11-24 20:53:33,076:INFO: Dataset: hotel               Batch: 3/4	Loss 10.2879 (10.3052)
2022-11-24 20:53:33,089:INFO: Dataset: hotel               Batch: 4/4	Loss 6.5163 (9.6504)
2022-11-24 20:53:33,309:INFO: Dataset: univ                Batch:  1/15	Loss 6.1124 (6.1124)
2022-11-24 20:53:33,330:INFO: Dataset: univ                Batch:  2/15	Loss 6.6110 (6.3514)
2022-11-24 20:53:33,352:INFO: Dataset: univ                Batch:  3/15	Loss 6.4800 (6.3937)
2022-11-24 20:53:33,374:INFO: Dataset: univ                Batch:  4/15	Loss 6.5134 (6.4242)
2022-11-24 20:53:33,397:INFO: Dataset: univ                Batch:  5/15	Loss 7.6851 (6.6497)
2022-11-24 20:53:33,422:INFO: Dataset: univ                Batch:  6/15	Loss 6.5320 (6.6287)
2022-11-24 20:53:33,443:INFO: Dataset: univ                Batch:  7/15	Loss 7.4666 (6.7371)
2022-11-24 20:53:33,464:INFO: Dataset: univ                Batch:  8/15	Loss 6.5194 (6.7090)
2022-11-24 20:53:33,484:INFO: Dataset: univ                Batch:  9/15	Loss 6.0690 (6.6375)
2022-11-24 20:53:33,505:INFO: Dataset: univ                Batch: 10/15	Loss 7.4950 (6.7203)
2022-11-24 20:53:33,526:INFO: Dataset: univ                Batch: 11/15	Loss 7.3594 (6.7738)
2022-11-24 20:53:33,549:INFO: Dataset: univ                Batch: 12/15	Loss 6.6932 (6.7665)
2022-11-24 20:53:33,570:INFO: Dataset: univ                Batch: 13/15	Loss 5.7811 (6.6900)
2022-11-24 20:53:33,591:INFO: Dataset: univ                Batch: 14/15	Loss 7.0732 (6.7168)
2022-11-24 20:53:33,600:INFO: Dataset: univ                Batch: 15/15	Loss 1.2399 (6.6309)
2022-11-24 20:53:33,819:INFO: Dataset: zara1               Batch: 1/8	Loss 8.1751 (8.1751)
2022-11-24 20:53:33,841:INFO: Dataset: zara1               Batch: 2/8	Loss 8.8264 (8.5073)
2022-11-24 20:53:33,861:INFO: Dataset: zara1               Batch: 3/8	Loss 8.6166 (8.5390)
2022-11-24 20:53:33,880:INFO: Dataset: zara1               Batch: 4/8	Loss 8.2304 (8.4545)
2022-11-24 20:53:33,899:INFO: Dataset: zara1               Batch: 5/8	Loss 8.0128 (8.3619)
2022-11-24 20:53:33,920:INFO: Dataset: zara1               Batch: 6/8	Loss 8.9087 (8.4588)
2022-11-24 20:53:33,939:INFO: Dataset: zara1               Batch: 7/8	Loss 8.7807 (8.5012)
2022-11-24 20:53:33,957:INFO: Dataset: zara1               Batch: 8/8	Loss 7.3600 (8.3823)
2022-11-24 20:53:34,184:INFO: Dataset: zara2               Batch:  1/18	Loss 6.4459 (6.4459)
2022-11-24 20:53:34,205:INFO: Dataset: zara2               Batch:  2/18	Loss 6.9847 (6.6937)
2022-11-24 20:53:34,226:INFO: Dataset: zara2               Batch:  3/18	Loss 6.0179 (6.4650)
2022-11-24 20:53:34,245:INFO: Dataset: zara2               Batch:  4/18	Loss 6.6664 (6.5153)
2022-11-24 20:53:34,264:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1194 (6.6296)
2022-11-24 20:53:34,288:INFO: Dataset: zara2               Batch:  6/18	Loss 6.3887 (6.5893)
2022-11-24 20:53:34,307:INFO: Dataset: zara2               Batch:  7/18	Loss 6.7138 (6.6083)
2022-11-24 20:53:34,325:INFO: Dataset: zara2               Batch:  8/18	Loss 7.4167 (6.7036)
2022-11-24 20:53:34,344:INFO: Dataset: zara2               Batch:  9/18	Loss 7.0889 (6.7433)
2022-11-24 20:53:34,363:INFO: Dataset: zara2               Batch: 10/18	Loss 6.9392 (6.7627)
2022-11-24 20:53:34,383:INFO: Dataset: zara2               Batch: 11/18	Loss 7.2190 (6.8025)
2022-11-24 20:53:34,404:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8639 (6.8074)
2022-11-24 20:53:34,423:INFO: Dataset: zara2               Batch: 13/18	Loss 6.2152 (6.7623)
2022-11-24 20:53:34,445:INFO: Dataset: zara2               Batch: 14/18	Loss 6.6963 (6.7573)
2022-11-24 20:53:34,466:INFO: Dataset: zara2               Batch: 15/18	Loss 6.3506 (6.7330)
2022-11-24 20:53:34,488:INFO: Dataset: zara2               Batch: 16/18	Loss 6.2942 (6.7080)
2022-11-24 20:53:34,508:INFO: Dataset: zara2               Batch: 17/18	Loss 6.2897 (6.6836)
2022-11-24 20:53:34,527:INFO: Dataset: zara2               Batch: 18/18	Loss 6.8714 (6.6925)
2022-11-24 20:53:34,569:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_48.pth.tar
2022-11-24 20:53:34,570:INFO: 
===> EPOCH: 49 (P1)
2022-11-24 20:53:34,570:INFO: - Computing loss (training)
2022-11-24 20:53:34,745:INFO: Dataset: hotel               Batch: 1/4	Loss 9.6909 (9.6909)
2022-11-24 20:53:34,765:INFO: Dataset: hotel               Batch: 2/4	Loss 10.7137 (10.2023)
2022-11-24 20:53:34,786:INFO: Dataset: hotel               Batch: 3/4	Loss 9.3614 (9.9176)
2022-11-24 20:53:34,799:INFO: Dataset: hotel               Batch: 4/4	Loss 7.5743 (9.5374)
2022-11-24 20:53:34,994:INFO: Dataset: univ                Batch:  1/15	Loss 6.6030 (6.6030)
2022-11-24 20:53:35,017:INFO: Dataset: univ                Batch:  2/15	Loss 7.2352 (6.9188)
2022-11-24 20:53:35,041:INFO: Dataset: univ                Batch:  3/15	Loss 6.6638 (6.8320)
2022-11-24 20:53:35,062:INFO: Dataset: univ                Batch:  4/15	Loss 6.8073 (6.8255)
2022-11-24 20:53:35,085:INFO: Dataset: univ                Batch:  5/15	Loss 6.5336 (6.7638)
2022-11-24 20:53:35,108:INFO: Dataset: univ                Batch:  6/15	Loss 6.5021 (6.7220)
2022-11-24 20:53:35,128:INFO: Dataset: univ                Batch:  7/15	Loss 5.9660 (6.6069)
2022-11-24 20:53:35,149:INFO: Dataset: univ                Batch:  8/15	Loss 6.1874 (6.5540)
2022-11-24 20:53:35,169:INFO: Dataset: univ                Batch:  9/15	Loss 6.1990 (6.5152)
2022-11-24 20:53:35,190:INFO: Dataset: univ                Batch: 10/15	Loss 6.8380 (6.5432)
2022-11-24 20:53:35,212:INFO: Dataset: univ                Batch: 11/15	Loss 6.0033 (6.4949)
2022-11-24 20:53:35,233:INFO: Dataset: univ                Batch: 12/15	Loss 7.7188 (6.5872)
2022-11-24 20:53:35,254:INFO: Dataset: univ                Batch: 13/15	Loss 6.7879 (6.6029)
2022-11-24 20:53:35,275:INFO: Dataset: univ                Batch: 14/15	Loss 6.4321 (6.5900)
2022-11-24 20:53:35,284:INFO: Dataset: univ                Batch: 15/15	Loss 1.1533 (6.5153)
2022-11-24 20:53:35,487:INFO: Dataset: zara1               Batch: 1/8	Loss 8.5790 (8.5790)
2022-11-24 20:53:35,506:INFO: Dataset: zara1               Batch: 2/8	Loss 8.2775 (8.4364)
2022-11-24 20:53:35,525:INFO: Dataset: zara1               Batch: 3/8	Loss 7.8797 (8.2358)
2022-11-24 20:53:35,545:INFO: Dataset: zara1               Batch: 4/8	Loss 9.0485 (8.4341)
2022-11-24 20:53:35,566:INFO: Dataset: zara1               Batch: 5/8	Loss 7.9096 (8.3254)
2022-11-24 20:53:35,586:INFO: Dataset: zara1               Batch: 6/8	Loss 7.9617 (8.2576)
2022-11-24 20:53:35,605:INFO: Dataset: zara1               Batch: 7/8	Loss 8.8167 (8.3443)
2022-11-24 20:53:35,622:INFO: Dataset: zara1               Batch: 8/8	Loss 6.9824 (8.1845)
2022-11-24 20:53:35,817:INFO: Dataset: zara2               Batch:  1/18	Loss 7.0638 (7.0638)
2022-11-24 20:53:35,838:INFO: Dataset: zara2               Batch:  2/18	Loss 6.7071 (6.8817)
2022-11-24 20:53:35,862:INFO: Dataset: zara2               Batch:  3/18	Loss 6.8495 (6.8711)
2022-11-24 20:53:35,881:INFO: Dataset: zara2               Batch:  4/18	Loss 6.1940 (6.7149)
2022-11-24 20:53:35,901:INFO: Dataset: zara2               Batch:  5/18	Loss 7.2286 (6.8244)
2022-11-24 20:53:35,925:INFO: Dataset: zara2               Batch:  6/18	Loss 7.2424 (6.8866)
2022-11-24 20:53:35,945:INFO: Dataset: zara2               Batch:  7/18	Loss 5.9894 (6.7547)
2022-11-24 20:53:35,964:INFO: Dataset: zara2               Batch:  8/18	Loss 6.7429 (6.7532)
2022-11-24 20:53:35,983:INFO: Dataset: zara2               Batch:  9/18	Loss 6.6211 (6.7391)
2022-11-24 20:53:36,002:INFO: Dataset: zara2               Batch: 10/18	Loss 6.2938 (6.6995)
2022-11-24 20:53:36,021:INFO: Dataset: zara2               Batch: 11/18	Loss 6.8229 (6.7099)
2022-11-24 20:53:36,042:INFO: Dataset: zara2               Batch: 12/18	Loss 6.3442 (6.6769)
2022-11-24 20:53:36,062:INFO: Dataset: zara2               Batch: 13/18	Loss 6.6147 (6.6722)
2022-11-24 20:53:36,081:INFO: Dataset: zara2               Batch: 14/18	Loss 6.6942 (6.6736)
2022-11-24 20:53:36,104:INFO: Dataset: zara2               Batch: 15/18	Loss 7.0429 (6.7005)
2022-11-24 20:53:36,126:INFO: Dataset: zara2               Batch: 16/18	Loss 6.2743 (6.6730)
2022-11-24 20:53:36,147:INFO: Dataset: zara2               Batch: 17/18	Loss 6.1792 (6.6459)
2022-11-24 20:53:36,165:INFO: Dataset: zara2               Batch: 18/18	Loss 5.7654 (6.6074)
2022-11-24 20:53:36,207:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_49.pth.tar
2022-11-24 20:53:36,208:INFO: 
===> EPOCH: 50 (P1)
2022-11-24 20:53:36,208:INFO: - Computing loss (training)
2022-11-24 20:53:36,387:INFO: Dataset: hotel               Batch: 1/4	Loss 10.1827 (10.1827)
2022-11-24 20:53:36,406:INFO: Dataset: hotel               Batch: 2/4	Loss 9.9210 (10.0509)
2022-11-24 20:53:36,427:INFO: Dataset: hotel               Batch: 3/4	Loss 11.4701 (10.4890)
2022-11-24 20:53:36,441:INFO: Dataset: hotel               Batch: 4/4	Loss 5.7966 (9.7275)
2022-11-24 20:53:36,648:INFO: Dataset: univ                Batch:  1/15	Loss 6.6978 (6.6978)
2022-11-24 20:53:36,670:INFO: Dataset: univ                Batch:  2/15	Loss 6.2847 (6.4793)
2022-11-24 20:53:36,692:INFO: Dataset: univ                Batch:  3/15	Loss 6.7906 (6.5830)
2022-11-24 20:53:36,715:INFO: Dataset: univ                Batch:  4/15	Loss 6.6484 (6.5981)
2022-11-24 20:53:36,735:INFO: Dataset: univ                Batch:  5/15	Loss 6.4511 (6.5712)
2022-11-24 20:53:36,761:INFO: Dataset: univ                Batch:  6/15	Loss 6.5224 (6.5632)
2022-11-24 20:53:36,782:INFO: Dataset: univ                Batch:  7/15	Loss 6.2366 (6.5121)
2022-11-24 20:53:36,802:INFO: Dataset: univ                Batch:  8/15	Loss 6.8904 (6.5566)
2022-11-24 20:53:36,823:INFO: Dataset: univ                Batch:  9/15	Loss 6.4824 (6.5481)
2022-11-24 20:53:36,844:INFO: Dataset: univ                Batch: 10/15	Loss 6.2805 (6.5202)
2022-11-24 20:53:36,866:INFO: Dataset: univ                Batch: 11/15	Loss 7.2851 (6.5838)
2022-11-24 20:53:36,888:INFO: Dataset: univ                Batch: 12/15	Loss 6.5303 (6.5797)
2022-11-24 20:53:36,909:INFO: Dataset: univ                Batch: 13/15	Loss 6.6119 (6.5820)
2022-11-24 20:53:36,930:INFO: Dataset: univ                Batch: 14/15	Loss 6.7434 (6.5944)
2022-11-24 20:53:36,938:INFO: Dataset: univ                Batch: 15/15	Loss 1.5196 (6.5302)
2022-11-24 20:53:37,166:INFO: Dataset: zara1               Batch: 1/8	Loss 7.8044 (7.8044)
2022-11-24 20:53:37,187:INFO: Dataset: zara1               Batch: 2/8	Loss 8.6199 (8.2095)
2022-11-24 20:53:37,206:INFO: Dataset: zara1               Batch: 3/8	Loss 8.0624 (8.1642)
2022-11-24 20:53:37,228:INFO: Dataset: zara1               Batch: 4/8	Loss 8.4578 (8.2474)
2022-11-24 20:53:37,247:INFO: Dataset: zara1               Batch: 5/8	Loss 8.0829 (8.2120)
2022-11-24 20:53:37,267:INFO: Dataset: zara1               Batch: 6/8	Loss 7.4801 (8.0905)
2022-11-24 20:53:37,286:INFO: Dataset: zara1               Batch: 7/8	Loss 8.0971 (8.0915)
2022-11-24 20:53:37,303:INFO: Dataset: zara1               Batch: 8/8	Loss 6.9492 (7.9706)
2022-11-24 20:53:37,523:INFO: Dataset: zara2               Batch:  1/18	Loss 6.3985 (6.3985)
2022-11-24 20:53:37,588:INFO: Dataset: zara2               Batch:  2/18	Loss 6.6282 (6.5234)
2022-11-24 20:53:37,607:INFO: Dataset: zara2               Batch:  3/18	Loss 7.0011 (6.6820)
2022-11-24 20:53:37,626:INFO: Dataset: zara2               Batch:  4/18	Loss 6.0844 (6.5311)
2022-11-24 20:53:37,645:INFO: Dataset: zara2               Batch:  5/18	Loss 7.1238 (6.6476)
2022-11-24 20:53:37,666:INFO: Dataset: zara2               Batch:  6/18	Loss 6.3629 (6.5974)
2022-11-24 20:53:37,684:INFO: Dataset: zara2               Batch:  7/18	Loss 6.7729 (6.6233)
2022-11-24 20:53:37,703:INFO: Dataset: zara2               Batch:  8/18	Loss 5.9090 (6.5329)
2022-11-24 20:53:37,722:INFO: Dataset: zara2               Batch:  9/18	Loss 7.3715 (6.6259)
2022-11-24 20:53:37,742:INFO: Dataset: zara2               Batch: 10/18	Loss 6.1302 (6.5798)
2022-11-24 20:53:37,763:INFO: Dataset: zara2               Batch: 11/18	Loss 6.5617 (6.5782)
2022-11-24 20:53:37,783:INFO: Dataset: zara2               Batch: 12/18	Loss 5.5982 (6.5062)
2022-11-24 20:53:37,802:INFO: Dataset: zara2               Batch: 13/18	Loss 7.3647 (6.5684)
2022-11-24 20:53:37,823:INFO: Dataset: zara2               Batch: 14/18	Loss 6.8788 (6.5909)
2022-11-24 20:53:37,843:INFO: Dataset: zara2               Batch: 15/18	Loss 6.7757 (6.6030)
2022-11-24 20:53:37,863:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4789 (6.5955)
2022-11-24 20:53:37,883:INFO: Dataset: zara2               Batch: 17/18	Loss 6.4160 (6.5851)
2022-11-24 20:53:37,901:INFO: Dataset: zara2               Batch: 18/18	Loss 5.2666 (6.5214)
2022-11-24 20:53:37,942:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_50.pth.tar
2022-11-24 20:53:37,942:INFO: 
===> EPOCH: 51 (P1)
2022-11-24 20:53:37,943:INFO: - Computing loss (training)
2022-11-24 20:53:38,124:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9274 (9.9274)
2022-11-24 20:53:38,144:INFO: Dataset: hotel               Batch: 2/4	Loss 12.4782 (11.1274)
2022-11-24 20:53:38,165:INFO: Dataset: hotel               Batch: 3/4	Loss 8.7707 (10.3456)
2022-11-24 20:53:38,179:INFO: Dataset: hotel               Batch: 4/4	Loss 5.7896 (9.5942)
2022-11-24 20:53:38,382:INFO: Dataset: univ                Batch:  1/15	Loss 6.4529 (6.4529)
2022-11-24 20:53:38,412:INFO: Dataset: univ                Batch:  2/15	Loss 6.6246 (6.5425)
2022-11-24 20:53:38,435:INFO: Dataset: univ                Batch:  3/15	Loss 6.4014 (6.4961)
2022-11-24 20:53:38,457:INFO: Dataset: univ                Batch:  4/15	Loss 6.0014 (6.3684)
2022-11-24 20:53:38,483:INFO: Dataset: univ                Batch:  5/15	Loss 6.3198 (6.3583)
2022-11-24 20:53:38,507:INFO: Dataset: univ                Batch:  6/15	Loss 6.5633 (6.3908)
2022-11-24 20:53:38,528:INFO: Dataset: univ                Batch:  7/15	Loss 6.8562 (6.4561)
2022-11-24 20:53:38,549:INFO: Dataset: univ                Batch:  8/15	Loss 6.7135 (6.4878)
2022-11-24 20:53:38,570:INFO: Dataset: univ                Batch:  9/15	Loss 7.1462 (6.5565)
2022-11-24 20:53:38,592:INFO: Dataset: univ                Batch: 10/15	Loss 5.9561 (6.4900)
2022-11-24 20:53:38,618:INFO: Dataset: univ                Batch: 11/15	Loss 6.3797 (6.4806)
2022-11-24 20:53:38,641:INFO: Dataset: univ                Batch: 12/15	Loss 5.8360 (6.4261)
2022-11-24 20:53:38,664:INFO: Dataset: univ                Batch: 13/15	Loss 6.3240 (6.4185)
2022-11-24 20:53:38,686:INFO: Dataset: univ                Batch: 14/15	Loss 6.2339 (6.4036)
2022-11-24 20:53:38,695:INFO: Dataset: univ                Batch: 15/15	Loss 1.4870 (6.3460)
2022-11-24 20:53:38,887:INFO: Dataset: zara1               Batch: 1/8	Loss 7.8491 (7.8491)
2022-11-24 20:53:38,910:INFO: Dataset: zara1               Batch: 2/8	Loss 7.5777 (7.7161)
2022-11-24 20:53:38,929:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9808 (7.8097)
2022-11-24 20:53:38,948:INFO: Dataset: zara1               Batch: 4/8	Loss 7.7869 (7.8043)
2022-11-24 20:53:38,967:INFO: Dataset: zara1               Batch: 5/8	Loss 8.4586 (7.9224)
2022-11-24 20:53:38,988:INFO: Dataset: zara1               Batch: 6/8	Loss 7.7632 (7.8950)
2022-11-24 20:53:39,007:INFO: Dataset: zara1               Batch: 7/8	Loss 7.2789 (7.8049)
2022-11-24 20:53:39,024:INFO: Dataset: zara1               Batch: 8/8	Loss 7.3254 (7.7605)
2022-11-24 20:53:39,327:INFO: Dataset: zara2               Batch:  1/18	Loss 6.4209 (6.4209)
2022-11-24 20:53:39,352:INFO: Dataset: zara2               Batch:  2/18	Loss 6.0178 (6.2135)
2022-11-24 20:53:39,371:INFO: Dataset: zara2               Batch:  3/18	Loss 6.4620 (6.3019)
2022-11-24 20:53:39,390:INFO: Dataset: zara2               Batch:  4/18	Loss 6.3258 (6.3073)
2022-11-24 20:53:39,409:INFO: Dataset: zara2               Batch:  5/18	Loss 6.0274 (6.2516)
2022-11-24 20:53:39,430:INFO: Dataset: zara2               Batch:  6/18	Loss 6.7165 (6.3274)
2022-11-24 20:53:39,449:INFO: Dataset: zara2               Batch:  7/18	Loss 6.7136 (6.3851)
2022-11-24 20:53:39,469:INFO: Dataset: zara2               Batch:  8/18	Loss 6.5017 (6.3996)
2022-11-24 20:53:39,488:INFO: Dataset: zara2               Batch:  9/18	Loss 6.0829 (6.3660)
2022-11-24 20:53:39,507:INFO: Dataset: zara2               Batch: 10/18	Loss 7.1212 (6.4402)
2022-11-24 20:53:39,528:INFO: Dataset: zara2               Batch: 11/18	Loss 6.3022 (6.4283)
2022-11-24 20:53:39,548:INFO: Dataset: zara2               Batch: 12/18	Loss 6.2592 (6.4142)
2022-11-24 20:53:39,569:INFO: Dataset: zara2               Batch: 13/18	Loss 6.6669 (6.4326)
2022-11-24 20:53:39,590:INFO: Dataset: zara2               Batch: 14/18	Loss 6.4163 (6.4316)
2022-11-24 20:53:39,611:INFO: Dataset: zara2               Batch: 15/18	Loss 6.6262 (6.4456)
2022-11-24 20:53:39,632:INFO: Dataset: zara2               Batch: 16/18	Loss 6.8817 (6.4742)
2022-11-24 20:53:39,652:INFO: Dataset: zara2               Batch: 17/18	Loss 6.3712 (6.4677)
2022-11-24 20:53:39,670:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1410 (6.4061)
2022-11-24 20:53:39,719:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_51.pth.tar
2022-11-24 20:53:39,719:INFO: 
===> EPOCH: 52 (P1)
2022-11-24 20:53:39,720:INFO: - Computing loss (training)
2022-11-24 20:53:39,894:INFO: Dataset: hotel               Batch: 1/4	Loss 9.9403 (9.9403)
2022-11-24 20:53:39,913:INFO: Dataset: hotel               Batch: 2/4	Loss 11.6823 (10.7795)
2022-11-24 20:53:39,932:INFO: Dataset: hotel               Batch: 3/4	Loss 10.1522 (10.5640)
2022-11-24 20:53:39,945:INFO: Dataset: hotel               Batch: 4/4	Loss 5.1341 (9.6185)
2022-11-24 20:53:40,156:INFO: Dataset: univ                Batch:  1/15	Loss 6.7251 (6.7251)
2022-11-24 20:53:40,182:INFO: Dataset: univ                Batch:  2/15	Loss 6.2628 (6.4643)
2022-11-24 20:53:40,203:INFO: Dataset: univ                Batch:  3/15	Loss 7.4049 (6.7493)
2022-11-24 20:53:40,223:INFO: Dataset: univ                Batch:  4/15	Loss 5.9097 (6.5150)
2022-11-24 20:53:40,245:INFO: Dataset: univ                Batch:  5/15	Loss 6.5681 (6.5265)
2022-11-24 20:53:40,269:INFO: Dataset: univ                Batch:  6/15	Loss 5.5572 (6.3545)
2022-11-24 20:53:40,289:INFO: Dataset: univ                Batch:  7/15	Loss 6.6650 (6.4033)
2022-11-24 20:53:40,310:INFO: Dataset: univ                Batch:  8/15	Loss 6.4097 (6.4041)
2022-11-24 20:53:40,330:INFO: Dataset: univ                Batch:  9/15	Loss 6.3507 (6.3978)
2022-11-24 20:53:40,350:INFO: Dataset: univ                Batch: 10/15	Loss 6.0109 (6.3583)
2022-11-24 20:53:40,374:INFO: Dataset: univ                Batch: 11/15	Loss 6.3183 (6.3544)
2022-11-24 20:53:40,395:INFO: Dataset: univ                Batch: 12/15	Loss 7.1155 (6.4141)
2022-11-24 20:53:40,416:INFO: Dataset: univ                Batch: 13/15	Loss 6.5391 (6.4234)
2022-11-24 20:53:40,437:INFO: Dataset: univ                Batch: 14/15	Loss 6.0606 (6.3954)
2022-11-24 20:53:40,446:INFO: Dataset: univ                Batch: 15/15	Loss 1.1148 (6.3213)
2022-11-24 20:53:40,671:INFO: Dataset: zara1               Batch: 1/8	Loss 7.9334 (7.9334)
2022-11-24 20:53:40,691:INFO: Dataset: zara1               Batch: 2/8	Loss 8.1536 (8.0417)
2022-11-24 20:53:40,710:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9086 (7.9976)
2022-11-24 20:53:40,731:INFO: Dataset: zara1               Batch: 4/8	Loss 7.5457 (7.8830)
2022-11-24 20:53:40,752:INFO: Dataset: zara1               Batch: 5/8	Loss 7.3252 (7.7698)
2022-11-24 20:53:40,772:INFO: Dataset: zara1               Batch: 6/8	Loss 7.3845 (7.7028)
2022-11-24 20:53:40,791:INFO: Dataset: zara1               Batch: 7/8	Loss 7.8748 (7.7261)
2022-11-24 20:53:40,809:INFO: Dataset: zara1               Batch: 8/8	Loss 7.3850 (7.6909)
2022-11-24 20:53:41,064:INFO: Dataset: zara2               Batch:  1/18	Loss 5.7601 (5.7601)
2022-11-24 20:53:41,083:INFO: Dataset: zara2               Batch:  2/18	Loss 6.3421 (6.0605)
2022-11-24 20:53:41,106:INFO: Dataset: zara2               Batch:  3/18	Loss 7.0626 (6.4116)
2022-11-24 20:53:41,126:INFO: Dataset: zara2               Batch:  4/18	Loss 6.3119 (6.3884)
2022-11-24 20:53:41,145:INFO: Dataset: zara2               Batch:  5/18	Loss 6.4441 (6.3994)
2022-11-24 20:53:41,169:INFO: Dataset: zara2               Batch:  6/18	Loss 6.5327 (6.4213)
2022-11-24 20:53:41,189:INFO: Dataset: zara2               Batch:  7/18	Loss 5.8139 (6.3350)
2022-11-24 20:53:41,208:INFO: Dataset: zara2               Batch:  8/18	Loss 6.5918 (6.3663)
2022-11-24 20:53:41,227:INFO: Dataset: zara2               Batch:  9/18	Loss 6.5049 (6.3833)
2022-11-24 20:53:41,246:INFO: Dataset: zara2               Batch: 10/18	Loss 6.7195 (6.4179)
2022-11-24 20:53:41,265:INFO: Dataset: zara2               Batch: 11/18	Loss 6.3108 (6.4079)
2022-11-24 20:53:41,286:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8193 (6.4373)
2022-11-24 20:53:41,306:INFO: Dataset: zara2               Batch: 13/18	Loss 6.4754 (6.4402)
2022-11-24 20:53:41,326:INFO: Dataset: zara2               Batch: 14/18	Loss 6.2532 (6.4267)
2022-11-24 20:53:41,347:INFO: Dataset: zara2               Batch: 15/18	Loss 6.8301 (6.4532)
2022-11-24 20:53:41,368:INFO: Dataset: zara2               Batch: 16/18	Loss 6.4307 (6.4518)
2022-11-24 20:53:41,388:INFO: Dataset: zara2               Batch: 17/18	Loss 5.7439 (6.4114)
2022-11-24 20:53:41,406:INFO: Dataset: zara2               Batch: 18/18	Loss 5.6017 (6.3756)
2022-11-24 20:53:41,449:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_52.pth.tar
2022-11-24 20:53:41,449:INFO: 
===> EPOCH: 53 (P1)
2022-11-24 20:53:41,449:INFO: - Computing loss (training)
2022-11-24 20:53:41,623:INFO: Dataset: hotel               Batch: 1/4	Loss 8.1058 (8.1058)
2022-11-24 20:53:41,645:INFO: Dataset: hotel               Batch: 2/4	Loss 10.9631 (9.5209)
2022-11-24 20:53:41,666:INFO: Dataset: hotel               Batch: 3/4	Loss 10.2072 (9.7467)
2022-11-24 20:53:41,679:INFO: Dataset: hotel               Batch: 4/4	Loss 7.2143 (9.3057)
2022-11-24 20:53:41,885:INFO: Dataset: univ                Batch:  1/15	Loss 6.0162 (6.0162)
2022-11-24 20:53:41,910:INFO: Dataset: univ                Batch:  2/15	Loss 6.8012 (6.4303)
2022-11-24 20:53:41,935:INFO: Dataset: univ                Batch:  3/15	Loss 6.2469 (6.3705)
2022-11-24 20:53:41,956:INFO: Dataset: univ                Batch:  4/15	Loss 6.5925 (6.4196)
2022-11-24 20:53:41,980:INFO: Dataset: univ                Batch:  5/15	Loss 5.9305 (6.3141)
2022-11-24 20:53:42,002:INFO: Dataset: univ                Batch:  6/15	Loss 5.8544 (6.2343)
2022-11-24 20:53:42,023:INFO: Dataset: univ                Batch:  7/15	Loss 6.5536 (6.2779)
2022-11-24 20:53:42,044:INFO: Dataset: univ                Batch:  8/15	Loss 6.3853 (6.2904)
2022-11-24 20:53:42,066:INFO: Dataset: univ                Batch:  9/15	Loss 5.9943 (6.2580)
2022-11-24 20:53:42,087:INFO: Dataset: univ                Batch: 10/15	Loss 6.1803 (6.2499)
2022-11-24 20:53:42,110:INFO: Dataset: univ                Batch: 11/15	Loss 6.4029 (6.2625)
2022-11-24 20:53:42,133:INFO: Dataset: univ                Batch: 12/15	Loss 6.2784 (6.2638)
2022-11-24 20:53:42,155:INFO: Dataset: univ                Batch: 13/15	Loss 6.1981 (6.2587)
2022-11-24 20:53:42,176:INFO: Dataset: univ                Batch: 14/15	Loss 6.5867 (6.2809)
2022-11-24 20:53:42,184:INFO: Dataset: univ                Batch: 15/15	Loss 1.2197 (6.2246)
2022-11-24 20:53:42,397:INFO: Dataset: zara1               Batch: 1/8	Loss 7.3913 (7.3913)
2022-11-24 20:53:42,418:INFO: Dataset: zara1               Batch: 2/8	Loss 7.5243 (7.4586)
2022-11-24 20:53:42,438:INFO: Dataset: zara1               Batch: 3/8	Loss 7.6069 (7.5089)
2022-11-24 20:53:42,459:INFO: Dataset: zara1               Batch: 4/8	Loss 7.7818 (7.5736)
2022-11-24 20:53:42,478:INFO: Dataset: zara1               Batch: 5/8	Loss 7.9507 (7.6387)
2022-11-24 20:53:42,497:INFO: Dataset: zara1               Batch: 6/8	Loss 8.0344 (7.7007)
2022-11-24 20:53:42,516:INFO: Dataset: zara1               Batch: 7/8	Loss 7.8744 (7.7256)
2022-11-24 20:53:42,533:INFO: Dataset: zara1               Batch: 8/8	Loss 6.5153 (7.5982)
2022-11-24 20:53:42,742:INFO: Dataset: zara2               Batch:  1/18	Loss 6.9513 (6.9513)
2022-11-24 20:53:42,788:INFO: Dataset: zara2               Batch:  2/18	Loss 5.9736 (6.4279)
2022-11-24 20:53:42,808:INFO: Dataset: zara2               Batch:  3/18	Loss 6.9231 (6.5789)
2022-11-24 20:53:42,827:INFO: Dataset: zara2               Batch:  4/18	Loss 6.1750 (6.4819)
2022-11-24 20:53:42,846:INFO: Dataset: zara2               Batch:  5/18	Loss 5.9379 (6.3750)
2022-11-24 20:53:42,869:INFO: Dataset: zara2               Batch:  6/18	Loss 5.5155 (6.2356)
2022-11-24 20:53:42,887:INFO: Dataset: zara2               Batch:  7/18	Loss 6.1094 (6.2178)
2022-11-24 20:53:42,906:INFO: Dataset: zara2               Batch:  8/18	Loss 6.1391 (6.2072)
2022-11-24 20:53:42,925:INFO: Dataset: zara2               Batch:  9/18	Loss 6.4184 (6.2317)
2022-11-24 20:53:42,944:INFO: Dataset: zara2               Batch: 10/18	Loss 7.3621 (6.3466)
2022-11-24 20:53:42,963:INFO: Dataset: zara2               Batch: 11/18	Loss 5.8653 (6.3027)
2022-11-24 20:53:42,984:INFO: Dataset: zara2               Batch: 12/18	Loss 6.8063 (6.3448)
2022-11-24 20:53:43,003:INFO: Dataset: zara2               Batch: 13/18	Loss 5.7651 (6.2982)
2022-11-24 20:53:43,024:INFO: Dataset: zara2               Batch: 14/18	Loss 6.3376 (6.3012)
2022-11-24 20:53:43,045:INFO: Dataset: zara2               Batch: 15/18	Loss 6.1381 (6.2896)
2022-11-24 20:53:43,065:INFO: Dataset: zara2               Batch: 16/18	Loss 6.2047 (6.2840)
2022-11-24 20:53:43,084:INFO: Dataset: zara2               Batch: 17/18	Loss 6.0841 (6.2718)
2022-11-24 20:53:43,102:INFO: Dataset: zara2               Batch: 18/18	Loss 5.7320 (6.2442)
2022-11-24 20:53:43,145:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_53.pth.tar
2022-11-24 20:53:43,145:INFO: 
===> EPOCH: 54 (P1)
2022-11-24 20:53:43,145:INFO: - Computing loss (training)
2022-11-24 20:53:43,325:INFO: Dataset: hotel               Batch: 1/4	Loss 9.0807 (9.0807)
2022-11-24 20:53:43,344:INFO: Dataset: hotel               Batch: 2/4	Loss 10.6771 (9.8194)
2022-11-24 20:53:43,364:INFO: Dataset: hotel               Batch: 3/4	Loss 9.8574 (9.8311)
2022-11-24 20:53:43,379:INFO: Dataset: hotel               Batch: 4/4	Loss 6.9681 (9.3854)
2022-11-24 20:53:43,594:INFO: Dataset: univ                Batch:  1/15	Loss 5.7357 (5.7357)
2022-11-24 20:53:43,619:INFO: Dataset: univ                Batch:  2/15	Loss 6.2772 (6.0033)
2022-11-24 20:53:43,640:INFO: Dataset: univ                Batch:  3/15	Loss 6.4994 (6.1698)
2022-11-24 20:53:43,662:INFO: Dataset: univ                Batch:  4/15	Loss 6.1034 (6.1534)
2022-11-24 20:53:43,685:INFO: Dataset: univ                Batch:  5/15	Loss 6.1418 (6.1512)
2022-11-24 20:53:43,709:INFO: Dataset: univ                Batch:  6/15	Loss 6.5349 (6.2169)
2022-11-24 20:53:43,730:INFO: Dataset: univ                Batch:  7/15	Loss 6.2827 (6.2262)
2022-11-24 20:53:43,750:INFO: Dataset: univ                Batch:  8/15	Loss 6.2828 (6.2329)
2022-11-24 20:53:43,771:INFO: Dataset: univ                Batch:  9/15	Loss 6.5510 (6.2669)
2022-11-24 20:53:43,792:INFO: Dataset: univ                Batch: 10/15	Loss 5.6636 (6.2026)
2022-11-24 20:53:43,816:INFO: Dataset: univ                Batch: 11/15	Loss 6.2772 (6.2096)
2022-11-24 20:53:43,838:INFO: Dataset: univ                Batch: 12/15	Loss 5.9073 (6.1838)
2022-11-24 20:53:43,859:INFO: Dataset: univ                Batch: 13/15	Loss 6.3038 (6.1931)
2022-11-24 20:53:43,881:INFO: Dataset: univ                Batch: 14/15	Loss 6.3021 (6.2012)
2022-11-24 20:53:43,889:INFO: Dataset: univ                Batch: 15/15	Loss 1.1870 (6.1409)
2022-11-24 20:53:44,103:INFO: Dataset: zara1               Batch: 1/8	Loss 7.6783 (7.6783)
2022-11-24 20:53:44,126:INFO: Dataset: zara1               Batch: 2/8	Loss 7.9098 (7.7911)
2022-11-24 20:53:44,146:INFO: Dataset: zara1               Batch: 3/8	Loss 7.9462 (7.8470)
2022-11-24 20:53:44,165:INFO: Dataset: zara1               Batch: 4/8	Loss 7.2980 (7.6869)
2022-11-24 20:53:44,184:INFO: Dataset: zara1               Batch: 5/8	Loss 7.6345 (7.6766)
2022-11-24 20:53:44,205:INFO: Dataset: zara1               Batch: 6/8	Loss 7.3649 (7.6238)
2022-11-24 20:53:44,224:INFO: Dataset: zara1               Batch: 7/8	Loss 7.6521 (7.6281)
2022-11-24 20:53:44,240:INFO: Dataset: zara1               Batch: 8/8	Loss 6.3780 (7.4978)
2022-11-24 20:53:44,445:INFO: Dataset: zara2               Batch:  1/18	Loss 6.5714 (6.5714)
2022-11-24 20:53:44,512:INFO: Dataset: zara2               Batch:  2/18	Loss 5.8801 (6.2366)
2022-11-24 20:53:44,532:INFO: Dataset: zara2               Batch:  3/18	Loss 5.8445 (6.0980)
2022-11-24 20:53:44,552:INFO: Dataset: zara2               Batch:  4/18	Loss 6.0044 (6.0732)
2022-11-24 20:53:44,571:INFO: Dataset: zara2               Batch:  5/18	Loss 5.9510 (6.0493)
2022-11-24 20:53:44,593:INFO: Dataset: zara2               Batch:  6/18	Loss 6.3577 (6.1020)
2022-11-24 20:53:44,612:INFO: Dataset: zara2               Batch:  7/18	Loss 5.7460 (6.0539)
2022-11-24 20:53:44,631:INFO: Dataset: zara2               Batch:  8/18	Loss 6.3920 (6.0950)
2022-11-24 20:53:44,650:INFO: Dataset: zara2               Batch:  9/18	Loss 5.5575 (6.0343)
2022-11-24 20:53:44,669:INFO: Dataset: zara2               Batch: 10/18	Loss 6.2404 (6.0547)
2022-11-24 20:53:44,691:INFO: Dataset: zara2               Batch: 11/18	Loss 6.0796 (6.0567)
2022-11-24 20:53:44,712:INFO: Dataset: zara2               Batch: 12/18	Loss 7.1978 (6.1409)
2022-11-24 20:53:44,732:INFO: Dataset: zara2               Batch: 13/18	Loss 6.4157 (6.1615)
2022-11-24 20:53:44,752:INFO: Dataset: zara2               Batch: 14/18	Loss 5.7734 (6.1354)
2022-11-24 20:53:44,772:INFO: Dataset: zara2               Batch: 15/18	Loss 6.0046 (6.1265)
2022-11-24 20:53:44,792:INFO: Dataset: zara2               Batch: 16/18	Loss 6.0705 (6.1231)
2022-11-24 20:53:44,812:INFO: Dataset: zara2               Batch: 17/18	Loss 6.0901 (6.1212)
2022-11-24 20:53:44,829:INFO: Dataset: zara2               Batch: 18/18	Loss 5.8257 (6.1067)
2022-11-24 20:53:44,872:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_54.pth.tar
2022-11-24 20:53:44,872:INFO: 
===> EPOCH: 55 (P1)
2022-11-24 20:53:44,872:INFO: - Computing loss (training)
2022-11-24 20:53:45,050:INFO: Dataset: hotel               Batch: 1/4	Loss 10.2297 (10.2297)
2022-11-24 20:53:45,071:INFO: Dataset: hotel               Batch: 2/4	Loss 8.6116 (9.4075)
2022-11-24 20:53:45,092:INFO: Dataset: hotel               Batch: 3/4	Loss 11.4960 (10.0670)
2022-11-24 20:53:45,105:INFO: Dataset: hotel               Batch: 4/4	Loss 6.1730 (9.3940)
2022-11-24 20:53:45,303:INFO: Dataset: univ                Batch:  1/15	Loss 6.2793 (6.2793)
2022-11-24 20:53:45,325:INFO: Dataset: univ                Batch:  2/15	Loss 6.0912 (6.1866)
2022-11-24 20:53:45,350:INFO: Dataset: univ                Batch:  3/15	Loss 5.6199 (5.9987)
2022-11-24 20:53:45,371:INFO: Dataset: univ                Batch:  4/15	Loss 5.7586 (5.9399)
2022-11-24 20:53:45,391:INFO: Dataset: univ                Batch:  5/15	Loss 6.4809 (6.0462)
2022-11-24 20:53:45,416:INFO: Dataset: univ                Batch:  6/15	Loss 6.4919 (6.1197)
2022-11-24 20:53:45,437:INFO: Dataset: univ                Batch:  7/15	Loss 5.7141 (6.0610)
2022-11-24 20:53:45,458:INFO: Dataset: univ                Batch:  8/15	Loss 6.7627 (6.1414)
2022-11-24 20:53:45,478:INFO: Dataset: univ                Batch:  9/15	Loss 6.2039 (6.1479)
2022-11-24 20:53:45,499:INFO: Dataset: univ                Batch: 10/15	Loss 6.2394 (6.1561)
2022-11-24 20:53:45,520:INFO: Dataset: univ                Batch: 11/15	Loss 6.3812 (6.1750)
2022-11-24 20:53:45,544:INFO: Dataset: univ                Batch: 12/15	Loss 6.5679 (6.2072)
2022-11-24 20:53:45,566:INFO: Dataset: univ                Batch: 13/15	Loss 6.2877 (6.2137)
2022-11-24 20:53:45,587:INFO: Dataset: univ                Batch: 14/15	Loss 5.7107 (6.1740)
2022-11-24 20:53:45,595:INFO: Dataset: univ                Batch: 15/15	Loss 1.0647 (6.1048)
2022-11-24 20:53:45,809:INFO: Dataset: zara1               Batch: 1/8	Loss 7.5849 (7.5849)
2022-11-24 20:53:45,832:INFO: Dataset: zara1               Batch: 2/8	Loss 7.4836 (7.5384)
2022-11-24 20:53:45,851:INFO: Dataset: zara1               Batch: 3/8	Loss 7.7072 (7.5941)
2022-11-24 20:53:45,869:INFO: Dataset: zara1               Batch: 4/8	Loss 7.2724 (7.5177)
2022-11-24 20:53:45,890:INFO: Dataset: zara1               Batch: 5/8	Loss 7.3561 (7.4871)
2022-11-24 20:53:45,912:INFO: Dataset: zara1               Batch: 6/8	Loss 7.3401 (7.4652)
2022-11-24 20:53:45,931:INFO: Dataset: zara1               Batch: 7/8	Loss 7.0232 (7.4030)
2022-11-24 20:53:45,948:INFO: Dataset: zara1               Batch: 8/8	Loss 5.9419 (7.2584)
2022-11-24 20:53:46,232:INFO: Dataset: zara2               Batch:  1/18	Loss 5.7933 (5.7933)
2022-11-24 20:53:46,255:INFO: Dataset: zara2               Batch:  2/18	Loss 5.2013 (5.4996)
2022-11-24 20:53:46,276:INFO: Dataset: zara2               Batch:  3/18	Loss 6.7038 (5.9334)
2022-11-24 20:53:46,296:INFO: Dataset: zara2               Batch:  4/18	Loss 5.7391 (5.8874)
2022-11-24 20:53:46,315:INFO: Dataset: zara2               Batch:  5/18	Loss 5.8899 (5.8879)
2022-11-24 20:53:46,336:INFO: Dataset: zara2               Batch:  6/18	Loss 6.9893 (6.0875)
2022-11-24 20:53:46,355:INFO: Dataset: zara2               Batch:  7/18	Loss 6.2761 (6.1166)
2022-11-24 20:53:46,374:INFO: Dataset: zara2               Batch:  8/18	Loss 5.7673 (6.0696)
2022-11-24 20:53:46,392:INFO: Dataset: zara2               Batch:  9/18	Loss 5.9820 (6.0611)
2022-11-24 20:53:46,412:INFO: Dataset: zara2               Batch: 10/18	Loss 5.8277 (6.0366)
2022-11-24 20:53:46,433:INFO: Dataset: zara2               Batch: 11/18	Loss 5.9457 (6.0281)
2022-11-24 20:53:46,454:INFO: Dataset: zara2               Batch: 12/18	Loss 5.8741 (6.0153)
2022-11-24 20:53:46,476:INFO: Dataset: zara2               Batch: 13/18	Loss 6.2481 (6.0340)
2022-11-24 20:53:46,497:INFO: Dataset: zara2               Batch: 14/18	Loss 6.6483 (6.0744)
2022-11-24 20:53:46,517:INFO: Dataset: zara2               Batch: 15/18	Loss 6.4037 (6.0949)
2022-11-24 20:53:46,538:INFO: Dataset: zara2               Batch: 16/18	Loss 5.5659 (6.0568)
2022-11-24 20:53:46,558:INFO: Dataset: zara2               Batch: 17/18	Loss 5.8580 (6.0457)
2022-11-24 20:53:46,576:INFO: Dataset: zara2               Batch: 18/18	Loss 4.8778 (5.9901)
2022-11-24 20:53:46,618:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_55.pth.tar
2022-11-24 20:53:46,618:INFO: 
===> EPOCH: 56 (P1)
2022-11-24 20:53:46,619:INFO: - Computing loss (training)
2022-11-24 20:53:46,782:INFO: Dataset: hotel               Batch: 1/4	Loss 10.0701 (10.0701)
2022-11-24 20:53:46,803:INFO: Dataset: hotel               Batch: 2/4	Loss 9.7868 (9.9260)
2022-11-24 20:53:46,824:INFO: Dataset: hotel               Batch: 3/4	Loss 9.9159 (9.9226)
2022-11-24 20:53:46,837:INFO: Dataset: hotel               Batch: 4/4	Loss 6.3434 (9.2568)
2022-11-24 20:53:47,042:INFO: Dataset: univ                Batch:  1/15	Loss 6.0854 (6.0854)
2022-11-24 20:53:47,110:INFO: Dataset: univ                Batch:  2/15	Loss 6.1479 (6.1172)
2022-11-24 20:53:47,131:INFO: Dataset: univ                Batch:  3/15	Loss 6.0726 (6.1018)
2022-11-24 20:53:47,151:INFO: Dataset: univ                Batch:  4/15	Loss 5.8592 (6.0407)
2022-11-24 20:53:47,171:INFO: Dataset: univ                Batch:  5/15	Loss 5.9134 (6.0135)
2022-11-24 20:53:47,196:INFO: Dataset: univ                Batch:  6/15	Loss 6.1146 (6.0313)
2022-11-24 20:53:47,216:INFO: Dataset: univ                Batch:  7/15	Loss 5.9412 (6.0187)
2022-11-24 20:53:47,237:INFO: Dataset: univ                Batch:  8/15	Loss 5.8195 (5.9933)
2022-11-24 20:53:47,258:INFO: Dataset: univ                Batch:  9/15	Loss 5.5663 (5.9421)
2022-11-24 20:53:47,280:INFO: Dataset: univ                Batch: 10/15	Loss 6.0411 (5.9524)
2022-11-24 20:53:47,302:INFO: Dataset: univ                Batch: 11/15	Loss 5.8412 (5.9426)
2022-11-24 20:53:47,324:INFO: Dataset: univ                Batch: 12/15	Loss 6.4802 (5.9849)
2022-11-24 20:53:47,346:INFO: Dataset: univ                Batch: 13/15	Loss 6.8654 (6.0507)
2022-11-24 20:53:47,367:INFO: Dataset: univ                Batch: 14/15	Loss 6.3066 (6.0694)
2022-11-24 20:53:47,375:INFO: Dataset: univ                Batch: 15/15	Loss 1.2918 (6.0089)
2022-11-24 20:53:47,579:INFO: Dataset: zara1               Batch: 1/8	Loss 7.6795 (7.6795)
2022-11-24 20:53:47,599:INFO: Dataset: zara1               Batch: 2/8	Loss 7.8355 (7.7551)
2022-11-24 20:53:47,619:INFO: Dataset: zara1               Batch: 3/8	Loss 7.3728 (7.6262)
2022-11-24 20:53:47,639:INFO: Dataset: zara1               Batch: 4/8	Loss 7.4932 (7.5937)
2022-11-24 20:53:47,659:INFO: Dataset: zara1               Batch: 5/8	Loss 7.4628 (7.5640)
2022-11-24 20:53:47,681:INFO: Dataset: zara1               Batch: 6/8	Loss 6.5616 (7.3679)
2022-11-24 20:53:47,700:INFO: Dataset: zara1               Batch: 7/8	Loss 7.8158 (7.4278)
2022-11-24 20:53:47,716:INFO: Dataset: zara1               Batch: 8/8	Loss 6.0761 (7.2727)
2022-11-24 20:53:47,932:INFO: Dataset: zara2               Batch:  1/18	Loss 6.0271 (6.0271)
2022-11-24 20:53:47,952:INFO: Dataset: zara2               Batch:  2/18	Loss 5.7573 (5.8810)
2022-11-24 20:53:47,971:INFO: Dataset: zara2               Batch:  3/18	Loss 5.7124 (5.8247)
2022-11-24 20:53:47,992:INFO: Dataset: zara2               Batch:  4/18	Loss 6.4295 (5.9827)
2022-11-24 20:53:48,014:INFO: Dataset: zara2               Batch:  5/18	Loss 5.6464 (5.9102)
2022-11-24 20:53:48,037:INFO: Dataset: zara2               Batch:  6/18	Loss 6.0577 (5.9362)
2022-11-24 20:53:48,055:INFO: Dataset: zara2               Batch:  7/18	Loss 6.1758 (5.9725)
2022-11-24 20:53:48,074:INFO: Dataset: zara2               Batch:  8/18	Loss 5.6688 (5.9364)
2022-11-24 20:53:48,093:INFO: Dataset: zara2               Batch:  9/18	Loss 6.0871 (5.9534)
2022-11-24 20:53:48,111:INFO: Dataset: zara2               Batch: 10/18	Loss 6.2103 (5.9783)
2022-11-24 20:53:48,130:INFO: Dataset: zara2               Batch: 11/18	Loss 6.5981 (6.0297)
2022-11-24 20:53:48,150:INFO: Dataset: zara2               Batch: 12/18	Loss 5.7715 (6.0065)
2022-11-24 20:53:48,169:INFO: Dataset: zara2               Batch: 13/18	Loss 5.7361 (5.9854)
2022-11-24 20:53:48,189:INFO: Dataset: zara2               Batch: 14/18	Loss 5.9671 (5.9840)
2022-11-24 20:53:48,209:INFO: Dataset: zara2               Batch: 15/18	Loss 5.9352 (5.9808)
2022-11-24 20:53:48,230:INFO: Dataset: zara2               Batch: 16/18	Loss 5.8773 (5.9749)
2022-11-24 20:53:48,250:INFO: Dataset: zara2               Batch: 17/18	Loss 5.7885 (5.9647)
2022-11-24 20:53:48,268:INFO: Dataset: zara2               Batch: 18/18	Loss 5.0893 (5.9190)
2022-11-24 20:53:48,310:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_56.pth.tar
2022-11-24 20:53:48,310:INFO: 
===> EPOCH: 57 (P1)
2022-11-24 20:53:48,310:INFO: - Computing loss (training)
2022-11-24 20:53:48,480:INFO: Dataset: hotel               Batch: 1/4	Loss 9.1906 (9.1906)
2022-11-24 20:53:48,500:INFO: Dataset: hotel               Batch: 2/4	Loss 9.6516 (9.4239)
2022-11-24 20:53:48,521:INFO: Dataset: hotel               Batch: 3/4	Loss 10.7044 (9.8528)
2022-11-24 20:53:48,534:INFO: Dataset: hotel               Batch: 4/4	Loss 6.2663 (9.2329)
2022-11-24 20:53:48,733:INFO: Dataset: univ                Batch:  1/15	Loss 5.4222 (5.4222)
2022-11-24 20:53:48,759:INFO: Dataset: univ                Batch:  2/15	Loss 5.7182 (5.5606)
2022-11-24 20:53:48,780:INFO: Dataset: univ                Batch:  3/15	Loss 5.4061 (5.5073)
2022-11-24 20:53:48,801:INFO: Dataset: univ                Batch:  4/15	Loss 5.9493 (5.6186)
2022-11-24 20:53:48,824:INFO: Dataset: univ                Batch:  5/15	Loss 5.7137 (5.6377)
2022-11-24 20:53:48,847:INFO: Dataset: univ                Batch:  6/15	Loss 6.0631 (5.7048)
2022-11-24 20:53:48,867:INFO: Dataset: univ                Batch:  7/15	Loss 6.4589 (5.8025)
2022-11-24 20:53:48,887:INFO: Dataset: univ                Batch:  8/15	Loss 6.4808 (5.8844)
2022-11-24 20:53:48,907:INFO: Dataset: univ                Batch:  9/15	Loss 5.6706 (5.8615)
2022-11-24 20:53:48,928:INFO: Dataset: univ                Batch: 10/15	Loss 6.0860 (5.8839)
2022-11-24 20:53:48,948:INFO: Dataset: univ                Batch: 11/15	Loss 6.0867 (5.9025)
2022-11-24 20:53:48,971:INFO: Dataset: univ                Batch: 12/15	Loss 6.5141 (5.9486)
2022-11-24 20:53:48,992:INFO: Dataset: univ                Batch: 13/15	Loss 6.0990 (5.9605)
2022-11-24 20:53:49,012:INFO: Dataset: univ                Batch: 14/15	Loss 5.9957 (5.9628)
2022-11-24 20:53:49,020:INFO: Dataset: univ                Batch: 15/15	Loss 1.1699 (5.9192)
2022-11-24 20:53:49,222:INFO: Dataset: zara1               Batch: 1/8	Loss 7.3343 (7.3343)
2022-11-24 20:53:49,242:INFO: Dataset: zara1               Batch: 2/8	Loss 7.2295 (7.2791)
2022-11-24 20:53:49,263:INFO: Dataset: zara1               Batch: 3/8	Loss 7.4396 (7.3351)
2022-11-24 20:53:49,282:INFO: Dataset: zara1               Batch: 4/8	Loss 7.4922 (7.3779)
2022-11-24 20:53:49,302:INFO: Dataset: zara1               Batch: 5/8	Loss 7.0729 (7.3130)
2022-11-24 20:53:49,322:INFO: Dataset: zara1               Batch: 6/8	Loss 7.4596 (7.3368)
2022-11-24 20:53:49,341:INFO: Dataset: zara1               Batch: 7/8	Loss 6.8888 (7.2726)
2022-11-24 20:53:49,358:INFO: Dataset: zara1               Batch: 8/8	Loss 5.9988 (7.1265)
2022-11-24 20:53:49,564:INFO: Dataset: zara2               Batch:  1/18	Loss 6.0052 (6.0052)
2022-11-24 20:53:49,583:INFO: Dataset: zara2               Batch:  2/18	Loss 5.6455 (5.8129)
2022-11-24 20:53:49,604:INFO: Dataset: zara2               Batch:  3/18	Loss 5.2096 (5.6228)
2022-11-24 20:53:49,623:INFO: Dataset: zara2               Batch:  4/18	Loss 6.2868 (5.7798)
2022-11-24 20:53:49,646:INFO: Dataset: zara2               Batch:  5/18	Loss 5.9793 (5.8200)
2022-11-24 20:53:49,668:INFO: Dataset: zara2               Batch:  6/18	Loss 5.8241 (5.8207)
2022-11-24 20:53:49,687:INFO: Dataset: zara2               Batch:  7/18	Loss 6.0734 (5.8550)
2022-11-24 20:53:49,706:INFO: Dataset: zara2               Batch:  8/18	Loss 5.6946 (5.8368)
2022-11-24 20:53:49,725:INFO: Dataset: zara2               Batch:  9/18	Loss 5.6459 (5.8162)
2022-11-24 20:53:49,744:INFO: Dataset: zara2               Batch: 10/18	Loss 6.0880 (5.8426)
2022-11-24 20:53:49,763:INFO: Dataset: zara2               Batch: 11/18	Loss 5.7425 (5.8333)
2022-11-24 20:53:49,783:INFO: Dataset: zara2               Batch: 12/18	Loss 5.2147 (5.7833)
2022-11-24 20:53:49,804:INFO: Dataset: zara2               Batch: 13/18	Loss 5.7003 (5.7771)
2022-11-24 20:53:49,824:INFO: Dataset: zara2               Batch: 14/18	Loss 5.6086 (5.7648)
2022-11-24 20:53:49,843:INFO: Dataset: zara2               Batch: 15/18	Loss 5.8258 (5.7685)
2022-11-24 20:53:49,864:INFO: Dataset: zara2               Batch: 16/18	Loss 6.1144 (5.7882)
2022-11-24 20:53:49,884:INFO: Dataset: zara2               Batch: 17/18	Loss 5.7718 (5.7873)
2022-11-24 20:53:49,902:INFO: Dataset: zara2               Batch: 18/18	Loss 5.9309 (5.7934)
2022-11-24 20:53:49,937:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_57.pth.tar
2022-11-24 20:53:49,937:INFO: 
===> EPOCH: 58 (P1)
2022-11-24 20:53:49,938:INFO: - Computing loss (training)
2022-11-24 20:53:50,096:INFO: Dataset: hotel               Batch: 1/4	Loss 9.3278 (9.3278)
2022-11-24 20:53:50,116:INFO: Dataset: hotel               Batch: 2/4	Loss 10.1248 (9.7337)
2022-11-24 20:53:50,136:INFO: Dataset: hotel               Batch: 3/4	Loss 10.2039 (9.8830)
2022-11-24 20:53:50,149:INFO: Dataset: hotel               Batch: 4/4	Loss 6.0164 (9.2300)
2022-11-24 20:53:50,358:INFO: Dataset: univ                Batch:  1/15	Loss 6.2897 (6.2897)
2022-11-24 20:53:50,381:INFO: Dataset: univ                Batch:  2/15	Loss 6.3966 (6.3429)
2022-11-24 20:53:50,404:INFO: Dataset: univ                Batch:  3/15	Loss 6.0119 (6.2235)
2022-11-24 20:53:50,425:INFO: Dataset: univ                Batch:  4/15	Loss 6.1109 (6.1972)
2022-11-24 20:53:50,450:INFO: Dataset: univ                Batch:  5/15	Loss 5.3996 (6.0194)
2022-11-24 20:53:50,473:INFO: Dataset: univ                Batch:  6/15	Loss 5.0166 (5.8443)
2022-11-24 20:53:50,494:INFO: Dataset: univ                Batch:  7/15	Loss 5.7708 (5.8330)
2022-11-24 20:53:50,515:INFO: Dataset: univ                Batch:  8/15	Loss 6.2309 (5.8819)
2022-11-24 20:53:50,536:INFO: Dataset: univ                Batch:  9/15	Loss 5.6332 (5.8535)
2022-11-24 20:53:50,557:INFO: Dataset: univ                Batch: 10/15	Loss 5.8870 (5.8567)
2022-11-24 20:53:50,579:INFO: Dataset: univ                Batch: 11/15	Loss 5.4763 (5.8252)
2022-11-24 20:53:50,602:INFO: Dataset: univ                Batch: 12/15	Loss 6.5050 (5.8784)
2022-11-24 20:53:50,624:INFO: Dataset: univ                Batch: 13/15	Loss 6.1824 (5.9023)
2022-11-24 20:53:50,647:INFO: Dataset: univ                Batch: 14/15	Loss 6.9155 (5.9635)
2022-11-24 20:53:50,656:INFO: Dataset: univ                Batch: 15/15	Loss 0.9468 (5.9038)
2022-11-24 20:53:50,859:INFO: Dataset: zara1               Batch: 1/8	Loss 7.0362 (7.0362)
2022-11-24 20:53:50,879:INFO: Dataset: zara1               Batch: 2/8	Loss 7.8140 (7.4091)
2022-11-24 20:53:50,898:INFO: Dataset: zara1               Batch: 3/8	Loss 7.2409 (7.3529)
2022-11-24 20:53:50,919:INFO: Dataset: zara1               Batch: 4/8	Loss 7.0974 (7.2876)
2022-11-24 20:53:50,941:INFO: Dataset: zara1               Batch: 5/8	Loss 7.4224 (7.3124)
2022-11-24 20:53:50,960:INFO: Dataset: zara1               Batch: 6/8	Loss 7.3239 (7.3142)
2022-11-24 20:53:50,980:INFO: Dataset: zara1               Batch: 7/8	Loss 7.2098 (7.2991)
2022-11-24 20:53:50,996:INFO: Dataset: zara1               Batch: 8/8	Loss 5.8716 (7.1354)
2022-11-24 20:53:51,194:INFO: Dataset: zara2               Batch:  1/18	Loss 5.8710 (5.8710)
2022-11-24 20:53:51,215:INFO: Dataset: zara2               Batch:  2/18	Loss 6.0069 (5.9386)
2022-11-24 20:53:51,237:INFO: Dataset: zara2               Batch:  3/18	Loss 5.2533 (5.7041)
2022-11-24 20:53:51,258:INFO: Dataset: zara2               Batch:  4/18	Loss 5.6921 (5.7012)
2022-11-24 20:53:51,278:INFO: Dataset: zara2               Batch:  5/18	Loss 5.0215 (5.5572)
2022-11-24 20:53:51,304:INFO: Dataset: zara2               Batch:  6/18	Loss 5.7458 (5.5912)
2022-11-24 20:53:51,323:INFO: Dataset: zara2               Batch:  7/18	Loss 6.3279 (5.6953)
2022-11-24 20:53:51,342:INFO: Dataset: zara2               Batch:  8/18	Loss 5.4544 (5.6666)
2022-11-24 20:53:51,361:INFO: Dataset: zara2               Batch:  9/18	Loss 5.7772 (5.6789)
2022-11-24 20:53:51,380:INFO: Dataset: zara2               Batch: 10/18	Loss 6.2188 (5.7320)
2022-11-24 20:53:51,400:INFO: Dataset: zara2               Batch: 11/18	Loss 6.1700 (5.7714)
2022-11-24 20:53:51,422:INFO: Dataset: zara2               Batch: 12/18	Loss 6.2076 (5.8037)
2022-11-24 20:53:51,442:INFO: Dataset: zara2               Batch: 13/18	Loss 5.4613 (5.7780)
2022-11-24 20:53:51,462:INFO: Dataset: zara2               Batch: 14/18	Loss 5.9366 (5.7889)
2022-11-24 20:53:51,482:INFO: Dataset: zara2               Batch: 15/18	Loss 4.9684 (5.7307)
2022-11-24 20:53:51,503:INFO: Dataset: zara2               Batch: 16/18	Loss 5.8077 (5.7356)
2022-11-24 20:53:51,523:INFO: Dataset: zara2               Batch: 17/18	Loss 6.1251 (5.7598)
2022-11-24 20:53:51,541:INFO: Dataset: zara2               Batch: 18/18	Loss 4.8337 (5.7121)
2022-11-24 20:53:51,583:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_58.pth.tar
2022-11-24 20:53:51,583:INFO: 
===> EPOCH: 59 (P1)
2022-11-24 20:53:51,584:INFO: - Computing loss (training)
2022-11-24 20:53:51,750:INFO: Dataset: hotel               Batch: 1/4	Loss 10.3638 (10.3638)
2022-11-24 20:53:51,769:INFO: Dataset: hotel               Batch: 2/4	Loss 10.2974 (10.3301)
2022-11-24 20:53:51,789:INFO: Dataset: hotel               Batch: 3/4	Loss 8.6957 (9.7766)
2022-11-24 20:53:51,802:INFO: Dataset: hotel               Batch: 4/4	Loss 6.4377 (9.1951)
2022-11-24 20:53:52,017:INFO: Dataset: univ                Batch:  1/15	Loss 5.6594 (5.6594)
2022-11-24 20:53:52,040:INFO: Dataset: univ                Batch:  2/15	Loss 5.8635 (5.7607)
2022-11-24 20:53:52,061:INFO: Dataset: univ                Batch:  3/15	Loss 5.9647 (5.8258)
2022-11-24 20:53:52,082:INFO: Dataset: univ                Batch:  4/15	Loss 5.7206 (5.7987)
2022-11-24 20:53:52,107:INFO: Dataset: univ                Batch:  5/15	Loss 5.7245 (5.7834)
2022-11-24 20:53:52,132:INFO: Dataset: univ                Batch:  6/15	Loss 5.5287 (5.7436)
2022-11-24 20:53:52,153:INFO: Dataset: univ                Batch:  7/15	Loss 6.1779 (5.8040)
2022-11-24 20:53:52,174:INFO: Dataset: univ                Batch:  8/15	Loss 5.3537 (5.7425)
2022-11-24 20:53:52,195:INFO: Dataset: univ                Batch:  9/15	Loss 6.4838 (5.8163)
2022-11-24 20:53:52,217:INFO: Dataset: univ                Batch: 10/15	Loss 6.5772 (5.8892)
2022-11-24 20:53:52,238:INFO: Dataset: univ                Batch: 11/15	Loss 5.2234 (5.8233)
2022-11-24 20:53:52,262:INFO: Dataset: univ                Batch: 12/15	Loss 5.7296 (5.8158)
2022-11-24 20:53:52,285:INFO: Dataset: univ                Batch: 13/15	Loss 5.7645 (5.8119)
2022-11-24 20:53:52,307:INFO: Dataset: univ                Batch: 14/15	Loss 6.3212 (5.8445)
2022-11-24 20:53:52,317:INFO: Dataset: univ                Batch: 15/15	Loss 0.9203 (5.7666)
2022-11-24 20:53:52,536:INFO: Dataset: zara1               Batch: 1/8	Loss 6.9209 (6.9209)
2022-11-24 20:53:52,555:INFO: Dataset: zara1               Batch: 2/8	Loss 6.6458 (6.7904)
2022-11-24 20:53:52,574:INFO: Dataset: zara1               Batch: 3/8	Loss 6.9661 (6.8476)
2022-11-24 20:53:52,593:INFO: Dataset: zara1               Batch: 4/8	Loss 6.9395 (6.8685)
2022-11-24 20:53:52,611:INFO: Dataset: zara1               Batch: 5/8	Loss 6.6553 (6.8238)
2022-11-24 20:53:52,632:INFO: Dataset: zara1               Batch: 6/8	Loss 6.9284 (6.8429)
2022-11-24 20:53:52,651:INFO: Dataset: zara1               Batch: 7/8	Loss 6.9454 (6.8581)
2022-11-24 20:53:52,668:INFO: Dataset: zara1               Batch: 8/8	Loss 5.7883 (6.7455)
2022-11-24 20:53:52,889:INFO: Dataset: zara2               Batch:  1/18	Loss 5.4334 (5.4334)
2022-11-24 20:53:52,910:INFO: Dataset: zara2               Batch:  2/18	Loss 5.8593 (5.6521)
2022-11-24 20:53:52,935:INFO: Dataset: zara2               Batch:  3/18	Loss 5.8496 (5.7175)
2022-11-24 20:53:52,957:INFO: Dataset: zara2               Batch:  4/18	Loss 5.9328 (5.7700)
2022-11-24 20:53:52,976:INFO: Dataset: zara2               Batch:  5/18	Loss 5.5887 (5.7312)
2022-11-24 20:53:52,999:INFO: Dataset: zara2               Batch:  6/18	Loss 5.3459 (5.6653)
2022-11-24 20:53:53,018:INFO: Dataset: zara2               Batch:  7/18	Loss 5.1792 (5.5921)
2022-11-24 20:53:53,037:INFO: Dataset: zara2               Batch:  8/18	Loss 5.4902 (5.5793)
2022-11-24 20:53:53,056:INFO: Dataset: zara2               Batch:  9/18	Loss 5.5367 (5.5744)
2022-11-24 20:53:53,075:INFO: Dataset: zara2               Batch: 10/18	Loss 5.4630 (5.5635)
2022-11-24 20:53:53,094:INFO: Dataset: zara2               Batch: 11/18	Loss 6.1583 (5.6175)
2022-11-24 20:53:53,115:INFO: Dataset: zara2               Batch: 12/18	Loss 5.3809 (5.5978)
2022-11-24 20:53:53,135:INFO: Dataset: zara2               Batch: 13/18	Loss 6.0663 (5.6315)
2022-11-24 20:53:53,156:INFO: Dataset: zara2               Batch: 14/18	Loss 5.2891 (5.6081)
2022-11-24 20:53:53,177:INFO: Dataset: zara2               Batch: 15/18	Loss 5.6044 (5.6078)
2022-11-24 20:53:53,198:INFO: Dataset: zara2               Batch: 16/18	Loss 5.9236 (5.6272)
2022-11-24 20:53:53,218:INFO: Dataset: zara2               Batch: 17/18	Loss 5.6594 (5.6290)
2022-11-24 20:53:53,236:INFO: Dataset: zara2               Batch: 18/18	Loss 4.8385 (5.5919)
2022-11-24 20:53:53,273:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_59.pth.tar
2022-11-24 20:53:53,273:INFO: 
===> EPOCH: 60 (P1)
2022-11-24 20:53:53,274:INFO: - Computing loss (training)
2022-11-24 20:53:53,442:INFO: Dataset: hotel               Batch: 1/4	Loss 10.8811 (10.8811)
2022-11-24 20:53:53,462:INFO: Dataset: hotel               Batch: 2/4	Loss 8.4662 (9.6969)
2022-11-24 20:53:53,482:INFO: Dataset: hotel               Batch: 3/4	Loss 9.7689 (9.7212)
2022-11-24 20:53:53,496:INFO: Dataset: hotel               Batch: 4/4	Loss 5.5925 (9.0131)
2022-11-24 20:53:53,702:INFO: Dataset: univ                Batch:  1/15	Loss 5.7269 (5.7269)
2022-11-24 20:53:53,726:INFO: Dataset: univ                Batch:  2/15	Loss 6.2080 (5.9578)
2022-11-24 20:53:53,748:INFO: Dataset: univ                Batch:  3/15	Loss 5.9727 (5.9630)
2022-11-24 20:53:53,771:INFO: Dataset: univ                Batch:  4/15	Loss 5.7435 (5.9107)
2022-11-24 20:53:53,794:INFO: Dataset: univ                Batch:  5/15	Loss 5.8635 (5.9018)
2022-11-24 20:53:53,818:INFO: Dataset: univ                Batch:  6/15	Loss 5.2800 (5.8007)
2022-11-24 20:53:53,839:INFO: Dataset: univ                Batch:  7/15	Loss 5.2746 (5.7167)
2022-11-24 20:53:53,859:INFO: Dataset: univ                Batch:  8/15	Loss 5.9150 (5.7429)
2022-11-24 20:53:53,880:INFO: Dataset: univ                Batch:  9/15	Loss 5.6658 (5.7345)
2022-11-24 20:53:53,900:INFO: Dataset: univ                Batch: 10/15	Loss 6.0302 (5.7637)
2022-11-24 20:53:53,922:INFO: Dataset: univ                Batch: 11/15	Loss 5.8598 (5.7724)
2022-11-24 20:53:53,945:INFO: Dataset: univ                Batch: 12/15	Loss 5.5501 (5.7533)
2022-11-24 20:53:53,967:INFO: Dataset: univ                Batch: 13/15	Loss 5.9633 (5.7701)
2022-11-24 20:53:53,988:INFO: Dataset: univ                Batch: 14/15	Loss 5.9731 (5.7835)
2022-11-24 20:53:53,996:INFO: Dataset: univ                Batch: 15/15	Loss 1.0022 (5.7205)
2022-11-24 20:53:54,219:INFO: Dataset: zara1               Batch: 1/8	Loss 6.6434 (6.6434)
2022-11-24 20:53:54,239:INFO: Dataset: zara1               Batch: 2/8	Loss 7.3791 (7.0062)
2022-11-24 20:53:54,258:INFO: Dataset: zara1               Batch: 3/8	Loss 6.7462 (6.9180)
2022-11-24 20:53:54,277:INFO: Dataset: zara1               Batch: 4/8	Loss 6.7639 (6.8735)
2022-11-24 20:53:54,296:INFO: Dataset: zara1               Batch: 5/8	Loss 7.0745 (6.9158)
2022-11-24 20:53:54,317:INFO: Dataset: zara1               Batch: 6/8	Loss 6.5203 (6.8423)
2022-11-24 20:53:54,336:INFO: Dataset: zara1               Batch: 7/8	Loss 6.5959 (6.8052)
2022-11-24 20:53:54,353:INFO: Dataset: zara1               Batch: 8/8	Loss 6.0875 (6.7319)
2022-11-24 20:53:54,568:INFO: Dataset: zara2               Batch:  1/18	Loss 5.1268 (5.1268)
2022-11-24 20:53:54,589:INFO: Dataset: zara2               Batch:  2/18	Loss 5.3765 (5.2616)
2022-11-24 20:53:54,610:INFO: Dataset: zara2               Batch:  3/18	Loss 6.1055 (5.5259)
2022-11-24 20:53:54,631:INFO: Dataset: zara2               Batch:  4/18	Loss 6.2124 (5.6953)
2022-11-24 20:53:54,651:INFO: Dataset: zara2               Batch:  5/18	Loss 6.5538 (5.8715)
2022-11-24 20:53:54,673:INFO: Dataset: zara2               Batch:  6/18	Loss 5.5017 (5.8066)
2022-11-24 20:53:54,692:INFO: Dataset: zara2               Batch:  7/18	Loss 5.8312 (5.8104)
2022-11-24 20:53:54,711:INFO: Dataset: zara2               Batch:  8/18	Loss 5.4513 (5.7603)
2022-11-24 20:53:54,730:INFO: Dataset: zara2               Batch:  9/18	Loss 5.7065 (5.7549)
2022-11-24 20:53:54,749:INFO: Dataset: zara2               Batch: 10/18	Loss 5.0315 (5.6821)
2022-11-24 20:53:54,769:INFO: Dataset: zara2               Batch: 11/18	Loss 5.0449 (5.6250)
2022-11-24 20:53:54,789:INFO: Dataset: zara2               Batch: 12/18	Loss 5.6963 (5.6312)
2022-11-24 20:53:54,808:INFO: Dataset: zara2               Batch: 13/18	Loss 5.4224 (5.6134)
2022-11-24 20:53:54,828:INFO: Dataset: zara2               Batch: 14/18	Loss 5.9731 (5.6374)
2022-11-24 20:53:54,849:INFO: Dataset: zara2               Batch: 15/18	Loss 5.4350 (5.6233)
2022-11-24 20:53:54,869:INFO: Dataset: zara2               Batch: 16/18	Loss 5.1514 (5.5965)
2022-11-24 20:53:54,889:INFO: Dataset: zara2               Batch: 17/18	Loss 6.0656 (5.6255)
2022-11-24 20:53:54,908:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6267 (5.5753)
2022-11-24 20:53:54,953:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_60.pth.tar
2022-11-24 20:53:54,953:INFO: 
===> EPOCH: 61 (P1)
2022-11-24 20:53:54,953:INFO: - Computing loss (training)
2022-11-24 20:53:55,137:INFO: Dataset: hotel               Batch: 1/4	Loss 9.5786 (9.5786)
2022-11-24 20:53:55,158:INFO: Dataset: hotel               Batch: 2/4	Loss 10.2779 (9.9682)
2022-11-24 20:53:55,178:INFO: Dataset: hotel               Batch: 3/4	Loss 8.9156 (9.6278)
2022-11-24 20:53:55,191:INFO: Dataset: hotel               Batch: 4/4	Loss 6.1419 (9.0576)
2022-11-24 20:53:55,396:INFO: Dataset: univ                Batch:  1/15	Loss 6.6108 (6.6108)
2022-11-24 20:53:55,421:INFO: Dataset: univ                Batch:  2/15	Loss 5.2747 (5.9044)
2022-11-24 20:53:55,443:INFO: Dataset: univ                Batch:  3/15	Loss 5.4255 (5.7445)
2022-11-24 20:53:55,464:INFO: Dataset: univ                Batch:  4/15	Loss 5.8262 (5.7659)
2022-11-24 20:53:55,489:INFO: Dataset: univ                Batch:  5/15	Loss 5.3212 (5.6726)
2022-11-24 20:53:55,511:INFO: Dataset: univ                Batch:  6/15	Loss 5.2679 (5.6021)
2022-11-24 20:53:55,532:INFO: Dataset: univ                Batch:  7/15	Loss 5.6585 (5.6100)
2022-11-24 20:53:55,552:INFO: Dataset: univ                Batch:  8/15	Loss 5.8365 (5.6377)
2022-11-24 20:53:55,573:INFO: Dataset: univ                Batch:  9/15	Loss 5.3104 (5.5971)
2022-11-24 20:53:55,593:INFO: Dataset: univ                Batch: 10/15	Loss 5.8607 (5.6234)
2022-11-24 20:53:55,616:INFO: Dataset: univ                Batch: 11/15	Loss 5.8910 (5.6445)
2022-11-24 20:53:55,637:INFO: Dataset: univ                Batch: 12/15	Loss 5.9827 (5.6713)
2022-11-24 20:53:55,658:INFO: Dataset: univ                Batch: 13/15	Loss 5.5658 (5.6635)
2022-11-24 20:53:55,681:INFO: Dataset: univ                Batch: 14/15	Loss 5.5717 (5.6561)
2022-11-24 20:53:55,689:INFO: Dataset: univ                Batch: 15/15	Loss 1.0555 (5.6040)
2022-11-24 20:53:55,894:INFO: Dataset: zara1               Batch: 1/8	Loss 6.9878 (6.9878)
2022-11-24 20:53:55,917:INFO: Dataset: zara1               Batch: 2/8	Loss 6.9726 (6.9806)
2022-11-24 20:53:55,939:INFO: Dataset: zara1               Batch: 3/8	Loss 6.7030 (6.8915)
2022-11-24 20:53:55,960:INFO: Dataset: zara1               Batch: 4/8	Loss 6.7600 (6.8540)
2022-11-24 20:53:55,981:INFO: Dataset: zara1               Batch: 5/8	Loss 6.5681 (6.7886)
2022-11-24 20:53:56,001:INFO: Dataset: zara1               Batch: 6/8	Loss 6.4980 (6.7415)
2022-11-24 20:53:56,020:INFO: Dataset: zara1               Batch: 7/8	Loss 6.0507 (6.6417)
2022-11-24 20:53:56,037:INFO: Dataset: zara1               Batch: 8/8	Loss 5.8403 (6.5581)
2022-11-24 20:53:56,257:INFO: Dataset: zara2               Batch:  1/18	Loss 4.7601 (4.7601)
2022-11-24 20:53:56,278:INFO: Dataset: zara2               Batch:  2/18	Loss 5.6254 (5.2053)
2022-11-24 20:53:56,300:INFO: Dataset: zara2               Batch:  3/18	Loss 5.4796 (5.2905)
2022-11-24 20:53:56,321:INFO: Dataset: zara2               Batch:  4/18	Loss 5.2304 (5.2758)
2022-11-24 20:53:56,340:INFO: Dataset: zara2               Batch:  5/18	Loss 6.1685 (5.4636)
2022-11-24 20:53:56,364:INFO: Dataset: zara2               Batch:  6/18	Loss 4.9070 (5.3747)
2022-11-24 20:53:56,384:INFO: Dataset: zara2               Batch:  7/18	Loss 5.0523 (5.3292)
2022-11-24 20:53:56,403:INFO: Dataset: zara2               Batch:  8/18	Loss 6.0035 (5.4163)
2022-11-24 20:53:56,422:INFO: Dataset: zara2               Batch:  9/18	Loss 5.6867 (5.4465)
2022-11-24 20:53:56,441:INFO: Dataset: zara2               Batch: 10/18	Loss 5.8191 (5.4843)
2022-11-24 20:53:56,460:INFO: Dataset: zara2               Batch: 11/18	Loss 5.9313 (5.5241)
2022-11-24 20:53:56,481:INFO: Dataset: zara2               Batch: 12/18	Loss 5.1377 (5.4893)
2022-11-24 20:53:56,501:INFO: Dataset: zara2               Batch: 13/18	Loss 5.3378 (5.4775)
2022-11-24 20:53:56,520:INFO: Dataset: zara2               Batch: 14/18	Loss 5.4168 (5.4734)
2022-11-24 20:53:56,540:INFO: Dataset: zara2               Batch: 15/18	Loss 5.6573 (5.4865)
2022-11-24 20:53:56,561:INFO: Dataset: zara2               Batch: 16/18	Loss 5.1750 (5.4663)
2022-11-24 20:53:56,581:INFO: Dataset: zara2               Batch: 17/18	Loss 4.7872 (5.4251)
2022-11-24 20:53:56,599:INFO: Dataset: zara2               Batch: 18/18	Loss 5.1185 (5.4111)
2022-11-24 20:53:56,642:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_61.pth.tar
2022-11-24 20:53:56,642:INFO: 
===> EPOCH: 62 (P1)
2022-11-24 20:53:56,642:INFO: - Computing loss (training)
2022-11-24 20:53:56,819:INFO: Dataset: hotel               Batch: 1/4	Loss 9.5497 (9.5497)
2022-11-24 20:53:56,839:INFO: Dataset: hotel               Batch: 2/4	Loss 10.1257 (9.8397)
2022-11-24 20:53:56,859:INFO: Dataset: hotel               Batch: 3/4	Loss 10.0611 (9.9141)
2022-11-24 20:53:56,873:INFO: Dataset: hotel               Batch: 4/4	Loss 5.4441 (9.1652)
2022-11-24 20:53:57,078:INFO: Dataset: univ                Batch:  1/15	Loss 6.2211 (6.2211)
2022-11-24 20:53:57,101:INFO: Dataset: univ                Batch:  2/15	Loss 5.8651 (6.0387)
2022-11-24 20:53:57,125:INFO: Dataset: univ                Batch:  3/15	Loss 5.9159 (5.9951)
2022-11-24 20:53:57,147:INFO: Dataset: univ                Batch:  4/15	Loss 5.4694 (5.8594)
2022-11-24 20:53:57,172:INFO: Dataset: univ                Batch:  5/15	Loss 5.4635 (5.7679)
2022-11-24 20:53:57,197:INFO: Dataset: univ                Batch:  6/15	Loss 5.9628 (5.7994)
2022-11-24 20:53:57,217:INFO: Dataset: univ                Batch:  7/15	Loss 5.8143 (5.8015)
2022-11-24 20:53:57,238:INFO: Dataset: univ                Batch:  8/15	Loss 5.5162 (5.7671)
2022-11-24 20:53:57,259:INFO: Dataset: univ                Batch:  9/15	Loss 5.2788 (5.7091)
2022-11-24 20:53:57,280:INFO: Dataset: univ                Batch: 10/15	Loss 5.3177 (5.6661)
2022-11-24 20:53:57,305:INFO: Dataset: univ                Batch: 11/15	Loss 5.7491 (5.6738)
2022-11-24 20:53:57,329:INFO: Dataset: univ                Batch: 12/15	Loss 5.2670 (5.6404)
2022-11-24 20:53:57,351:INFO: Dataset: univ                Batch: 13/15	Loss 5.3650 (5.6187)
2022-11-24 20:53:57,374:INFO: Dataset: univ                Batch: 14/15	Loss 5.8083 (5.6322)
2022-11-24 20:53:57,381:INFO: Dataset: univ                Batch: 15/15	Loss 0.9199 (5.5664)
2022-11-24 20:53:57,593:INFO: Dataset: zara1               Batch: 1/8	Loss 6.2346 (6.2346)
2022-11-24 20:53:57,615:INFO: Dataset: zara1               Batch: 2/8	Loss 6.4289 (6.3378)
2022-11-24 20:53:57,634:INFO: Dataset: zara1               Batch: 3/8	Loss 6.6193 (6.4337)
2022-11-24 20:53:57,656:INFO: Dataset: zara1               Batch: 4/8	Loss 6.2961 (6.4023)
2022-11-24 20:53:57,676:INFO: Dataset: zara1               Batch: 5/8	Loss 6.2625 (6.3741)
2022-11-24 20:53:57,696:INFO: Dataset: zara1               Batch: 6/8	Loss 6.5520 (6.4051)
2022-11-24 20:53:57,715:INFO: Dataset: zara1               Batch: 7/8	Loss 6.4135 (6.4063)
2022-11-24 20:53:57,731:INFO: Dataset: zara1               Batch: 8/8	Loss 5.7762 (6.3347)
2022-11-24 20:53:57,949:INFO: Dataset: zara2               Batch:  1/18	Loss 5.6312 (5.6312)
2022-11-24 20:53:57,969:INFO: Dataset: zara2               Batch:  2/18	Loss 5.6181 (5.6246)
2022-11-24 20:53:57,988:INFO: Dataset: zara2               Batch:  3/18	Loss 5.6131 (5.6206)
2022-11-24 20:53:58,011:INFO: Dataset: zara2               Batch:  4/18	Loss 5.3287 (5.5445)
2022-11-24 20:53:58,032:INFO: Dataset: zara2               Batch:  5/18	Loss 5.0081 (5.4263)
2022-11-24 20:53:58,054:INFO: Dataset: zara2               Batch:  6/18	Loss 6.1867 (5.5670)
2022-11-24 20:53:58,073:INFO: Dataset: zara2               Batch:  7/18	Loss 5.3254 (5.5379)
2022-11-24 20:53:58,092:INFO: Dataset: zara2               Batch:  8/18	Loss 5.0655 (5.4771)
2022-11-24 20:53:58,113:INFO: Dataset: zara2               Batch:  9/18	Loss 4.9834 (5.4254)
2022-11-24 20:53:58,132:INFO: Dataset: zara2               Batch: 10/18	Loss 5.6400 (5.4453)
2022-11-24 20:53:58,151:INFO: Dataset: zara2               Batch: 11/18	Loss 5.3884 (5.4403)
2022-11-24 20:53:58,173:INFO: Dataset: zara2               Batch: 12/18	Loss 4.9567 (5.3999)
2022-11-24 20:53:58,192:INFO: Dataset: zara2               Batch: 13/18	Loss 4.9238 (5.3678)
2022-11-24 20:53:58,213:INFO: Dataset: zara2               Batch: 14/18	Loss 5.8776 (5.3994)
2022-11-24 20:53:58,233:INFO: Dataset: zara2               Batch: 15/18	Loss 5.4761 (5.4043)
2022-11-24 20:53:58,254:INFO: Dataset: zara2               Batch: 16/18	Loss 5.1430 (5.3865)
2022-11-24 20:53:58,274:INFO: Dataset: zara2               Batch: 17/18	Loss 5.3184 (5.3826)
2022-11-24 20:53:58,293:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6025 (5.3510)
2022-11-24 20:53:58,334:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_62.pth.tar
2022-11-24 20:53:58,335:INFO: 
===> EPOCH: 63 (P1)
2022-11-24 20:53:58,335:INFO: - Computing loss (training)
2022-11-24 20:53:58,494:INFO: Dataset: hotel               Batch: 1/4	Loss 10.7725 (10.7725)
2022-11-24 20:53:58,514:INFO: Dataset: hotel               Batch: 2/4	Loss 9.4652 (10.1418)
2022-11-24 20:53:58,535:INFO: Dataset: hotel               Batch: 3/4	Loss 9.2292 (9.8390)
2022-11-24 20:53:58,549:INFO: Dataset: hotel               Batch: 4/4	Loss 5.5899 (9.1720)
2022-11-24 20:53:58,763:INFO: Dataset: univ                Batch:  1/15	Loss 5.5504 (5.5504)
2022-11-24 20:53:58,788:INFO: Dataset: univ                Batch:  2/15	Loss 5.4833 (5.5157)
2022-11-24 20:53:58,811:INFO: Dataset: univ                Batch:  3/15	Loss 6.0435 (5.6911)
2022-11-24 20:53:58,835:INFO: Dataset: univ                Batch:  4/15	Loss 5.5550 (5.6557)
2022-11-24 20:53:58,858:INFO: Dataset: univ                Batch:  5/15	Loss 5.5501 (5.6356)
2022-11-24 20:53:58,881:INFO: Dataset: univ                Batch:  6/15	Loss 4.8652 (5.4921)
2022-11-24 20:53:58,902:INFO: Dataset: univ                Batch:  7/15	Loss 5.3217 (5.4674)
2022-11-24 20:53:58,923:INFO: Dataset: univ                Batch:  8/15	Loss 5.7183 (5.4979)
2022-11-24 20:53:58,944:INFO: Dataset: univ                Batch:  9/15	Loss 6.2043 (5.5666)
2022-11-24 20:53:58,965:INFO: Dataset: univ                Batch: 10/15	Loss 5.6898 (5.5799)
2022-11-24 20:53:58,986:INFO: Dataset: univ                Batch: 11/15	Loss 5.6281 (5.5841)
2022-11-24 20:53:59,009:INFO: Dataset: univ                Batch: 12/15	Loss 4.8341 (5.5187)
2022-11-24 20:53:59,031:INFO: Dataset: univ                Batch: 13/15	Loss 5.4946 (5.5171)
2022-11-24 20:53:59,054:INFO: Dataset: univ                Batch: 14/15	Loss 5.3387 (5.5040)
2022-11-24 20:53:59,062:INFO: Dataset: univ                Batch: 15/15	Loss 0.9463 (5.4405)
2022-11-24 20:53:59,265:INFO: Dataset: zara1               Batch: 1/8	Loss 6.0011 (6.0011)
2022-11-24 20:53:59,296:INFO: Dataset: zara1               Batch: 2/8	Loss 6.1889 (6.0896)
2022-11-24 20:53:59,318:INFO: Dataset: zara1               Batch: 3/8	Loss 6.4445 (6.2189)
2022-11-24 20:53:59,337:INFO: Dataset: zara1               Batch: 4/8	Loss 6.1694 (6.2071)
2022-11-24 20:53:59,356:INFO: Dataset: zara1               Batch: 5/8	Loss 6.4812 (6.2605)
2022-11-24 20:53:59,377:INFO: Dataset: zara1               Batch: 6/8	Loss 6.4399 (6.2915)
2022-11-24 20:53:59,396:INFO: Dataset: zara1               Batch: 7/8	Loss 6.8258 (6.3644)
2022-11-24 20:53:59,413:INFO: Dataset: zara1               Batch: 8/8	Loss 5.7183 (6.2967)
2022-11-24 20:53:59,632:INFO: Dataset: zara2               Batch:  1/18	Loss 5.4649 (5.4649)
2022-11-24 20:53:59,653:INFO: Dataset: zara2               Batch:  2/18	Loss 4.6017 (5.0402)
2022-11-24 20:53:59,674:INFO: Dataset: zara2               Batch:  3/18	Loss 5.3017 (5.1284)
2022-11-24 20:53:59,696:INFO: Dataset: zara2               Batch:  4/18	Loss 5.1864 (5.1429)
2022-11-24 20:53:59,717:INFO: Dataset: zara2               Batch:  5/18	Loss 5.2087 (5.1560)
2022-11-24 20:53:59,738:INFO: Dataset: zara2               Batch:  6/18	Loss 5.5005 (5.2147)
2022-11-24 20:53:59,757:INFO: Dataset: zara2               Batch:  7/18	Loss 5.6132 (5.2651)
2022-11-24 20:53:59,777:INFO: Dataset: zara2               Batch:  8/18	Loss 5.8207 (5.3341)
2022-11-24 20:53:59,796:INFO: Dataset: zara2               Batch:  9/18	Loss 4.9897 (5.2937)
2022-11-24 20:53:59,815:INFO: Dataset: zara2               Batch: 10/18	Loss 4.7216 (5.2400)
2022-11-24 20:53:59,836:INFO: Dataset: zara2               Batch: 11/18	Loss 4.6416 (5.1854)
2022-11-24 20:53:59,856:INFO: Dataset: zara2               Batch: 12/18	Loss 5.5464 (5.2144)
2022-11-24 20:53:59,876:INFO: Dataset: zara2               Batch: 13/18	Loss 5.8672 (5.2626)
2022-11-24 20:53:59,895:INFO: Dataset: zara2               Batch: 14/18	Loss 5.8512 (5.3043)
2022-11-24 20:53:59,917:INFO: Dataset: zara2               Batch: 15/18	Loss 5.4216 (5.3123)
2022-11-24 20:53:59,938:INFO: Dataset: zara2               Batch: 16/18	Loss 5.3791 (5.3165)
2022-11-24 20:53:59,959:INFO: Dataset: zara2               Batch: 17/18	Loss 5.3255 (5.3171)
2022-11-24 20:53:59,977:INFO: Dataset: zara2               Batch: 18/18	Loss 4.1838 (5.2691)
2022-11-24 20:54:00,021:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_63.pth.tar
2022-11-24 20:54:00,021:INFO: 
===> EPOCH: 64 (P1)
2022-11-24 20:54:00,022:INFO: - Computing loss (training)
2022-11-24 20:54:00,216:INFO: Dataset: hotel               Batch: 1/4	Loss 8.6246 (8.6246)
2022-11-24 20:54:00,237:INFO: Dataset: hotel               Batch: 2/4	Loss 10.0092 (9.2843)
2022-11-24 20:54:00,258:INFO: Dataset: hotel               Batch: 3/4	Loss 9.0672 (9.2133)
2022-11-24 20:54:00,271:INFO: Dataset: hotel               Batch: 4/4	Loss 6.6553 (8.7813)
2022-11-24 20:54:00,503:INFO: Dataset: univ                Batch:  1/15	Loss 5.3079 (5.3079)
2022-11-24 20:54:00,525:INFO: Dataset: univ                Batch:  2/15	Loss 5.6192 (5.4639)
2022-11-24 20:54:00,550:INFO: Dataset: univ                Batch:  3/15	Loss 5.5621 (5.4957)
2022-11-24 20:54:00,572:INFO: Dataset: univ                Batch:  4/15	Loss 5.2685 (5.4329)
2022-11-24 20:54:00,593:INFO: Dataset: univ                Batch:  5/15	Loss 5.4458 (5.4356)
2022-11-24 20:54:00,617:INFO: Dataset: univ                Batch:  6/15	Loss 5.8116 (5.4932)
2022-11-24 20:54:00,639:INFO: Dataset: univ                Batch:  7/15	Loss 5.3125 (5.4655)
2022-11-24 20:54:00,661:INFO: Dataset: univ                Batch:  8/15	Loss 5.2991 (5.4438)
2022-11-24 20:54:00,682:INFO: Dataset: univ                Batch:  9/15	Loss 5.4598 (5.4456)
2022-11-24 20:54:00,703:INFO: Dataset: univ                Batch: 10/15	Loss 5.1337 (5.4147)
2022-11-24 20:54:00,726:INFO: Dataset: univ                Batch: 11/15	Loss 5.6725 (5.4387)
2022-11-24 20:54:00,749:INFO: Dataset: univ                Batch: 12/15	Loss 5.4159 (5.4368)
2022-11-24 20:54:00,771:INFO: Dataset: univ                Batch: 13/15	Loss 5.3145 (5.4274)
2022-11-24 20:54:00,795:INFO: Dataset: univ                Batch: 14/15	Loss 5.3608 (5.4231)
2022-11-24 20:54:00,804:INFO: Dataset: univ                Batch: 15/15	Loss 1.0119 (5.3790)
2022-11-24 20:54:00,996:INFO: Dataset: zara1               Batch: 1/8	Loss 6.0228 (6.0228)
2022-11-24 20:54:01,019:INFO: Dataset: zara1               Batch: 2/8	Loss 6.4815 (6.2449)
2022-11-24 20:54:01,038:INFO: Dataset: zara1               Batch: 3/8	Loss 6.6913 (6.3827)
2022-11-24 20:54:01,057:INFO: Dataset: zara1               Batch: 4/8	Loss 6.2482 (6.3512)
2022-11-24 20:54:01,076:INFO: Dataset: zara1               Batch: 5/8	Loss 6.0685 (6.2991)
2022-11-24 20:54:01,097:INFO: Dataset: zara1               Batch: 6/8	Loss 6.3583 (6.3099)
2022-11-24 20:54:01,116:INFO: Dataset: zara1               Batch: 7/8	Loss 5.9253 (6.2522)
2022-11-24 20:54:01,133:INFO: Dataset: zara1               Batch: 8/8	Loss 5.0561 (6.1137)
2022-11-24 20:54:01,345:INFO: Dataset: zara2               Batch:  1/18	Loss 5.5894 (5.5894)
2022-11-24 20:54:01,366:INFO: Dataset: zara2               Batch:  2/18	Loss 5.7671 (5.6862)
2022-11-24 20:54:01,386:INFO: Dataset: zara2               Batch:  3/18	Loss 4.9173 (5.4205)
2022-11-24 20:54:01,408:INFO: Dataset: zara2               Batch:  4/18	Loss 5.2367 (5.3717)
2022-11-24 20:54:01,427:INFO: Dataset: zara2               Batch:  5/18	Loss 5.0944 (5.3173)
2022-11-24 20:54:01,449:INFO: Dataset: zara2               Batch:  6/18	Loss 5.1612 (5.2908)
2022-11-24 20:54:01,468:INFO: Dataset: zara2               Batch:  7/18	Loss 5.5924 (5.3329)
2022-11-24 20:54:01,487:INFO: Dataset: zara2               Batch:  8/18	Loss 5.2845 (5.3271)
2022-11-24 20:54:01,505:INFO: Dataset: zara2               Batch:  9/18	Loss 5.2808 (5.3218)
2022-11-24 20:54:01,524:INFO: Dataset: zara2               Batch: 10/18	Loss 5.5528 (5.3476)
2022-11-24 20:54:01,544:INFO: Dataset: zara2               Batch: 11/18	Loss 5.7918 (5.3869)
2022-11-24 20:54:01,564:INFO: Dataset: zara2               Batch: 12/18	Loss 5.4564 (5.3928)
2022-11-24 20:54:01,584:INFO: Dataset: zara2               Batch: 13/18	Loss 5.2536 (5.3823)
2022-11-24 20:54:01,603:INFO: Dataset: zara2               Batch: 14/18	Loss 4.6262 (5.3348)
2022-11-24 20:54:01,624:INFO: Dataset: zara2               Batch: 15/18	Loss 5.3012 (5.3326)
2022-11-24 20:54:01,644:INFO: Dataset: zara2               Batch: 16/18	Loss 4.9505 (5.3091)
2022-11-24 20:54:01,665:INFO: Dataset: zara2               Batch: 17/18	Loss 5.0843 (5.2963)
2022-11-24 20:54:01,682:INFO: Dataset: zara2               Batch: 18/18	Loss 4.1945 (5.2478)
2022-11-24 20:54:01,718:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_64.pth.tar
2022-11-24 20:54:01,718:INFO: 
===> EPOCH: 65 (P1)
2022-11-24 20:54:01,718:INFO: - Computing loss (training)
2022-11-24 20:54:01,882:INFO: Dataset: hotel               Batch: 1/4	Loss 9.6541 (9.6541)
2022-11-24 20:54:01,903:INFO: Dataset: hotel               Batch: 2/4	Loss 11.0248 (10.3410)
2022-11-24 20:54:01,922:INFO: Dataset: hotel               Batch: 3/4	Loss 8.7067 (9.8040)
2022-11-24 20:54:01,935:INFO: Dataset: hotel               Batch: 4/4	Loss 4.9209 (8.9987)
2022-11-24 20:54:02,140:INFO: Dataset: univ                Batch:  1/15	Loss 5.5304 (5.5304)
2022-11-24 20:54:02,162:INFO: Dataset: univ                Batch:  2/15	Loss 5.8514 (5.7040)
2022-11-24 20:54:02,187:INFO: Dataset: univ                Batch:  3/15	Loss 5.4519 (5.6167)
2022-11-24 20:54:02,208:INFO: Dataset: univ                Batch:  4/15	Loss 5.2169 (5.5075)
2022-11-24 20:54:02,229:INFO: Dataset: univ                Batch:  5/15	Loss 5.6287 (5.5324)
2022-11-24 20:54:02,253:INFO: Dataset: univ                Batch:  6/15	Loss 5.0201 (5.4421)
2022-11-24 20:54:02,274:INFO: Dataset: univ                Batch:  7/15	Loss 5.2670 (5.4175)
2022-11-24 20:54:02,295:INFO: Dataset: univ                Batch:  8/15	Loss 5.5022 (5.4286)
2022-11-24 20:54:02,316:INFO: Dataset: univ                Batch:  9/15	Loss 5.5225 (5.4390)
2022-11-24 20:54:02,336:INFO: Dataset: univ                Batch: 10/15	Loss 5.4212 (5.4373)
2022-11-24 20:54:02,357:INFO: Dataset: univ                Batch: 11/15	Loss 5.4108 (5.4350)
2022-11-24 20:54:02,380:INFO: Dataset: univ                Batch: 12/15	Loss 5.2175 (5.4158)
2022-11-24 20:54:02,403:INFO: Dataset: univ                Batch: 13/15	Loss 5.0594 (5.3879)
2022-11-24 20:54:02,427:INFO: Dataset: univ                Batch: 14/15	Loss 5.3140 (5.3823)
2022-11-24 20:54:02,436:INFO: Dataset: univ                Batch: 15/15	Loss 1.1589 (5.3422)
2022-11-24 20:54:02,634:INFO: Dataset: zara1               Batch: 1/8	Loss 6.0674 (6.0674)
2022-11-24 20:54:02,654:INFO: Dataset: zara1               Batch: 2/8	Loss 6.3105 (6.1887)
2022-11-24 20:54:02,677:INFO: Dataset: zara1               Batch: 3/8	Loss 6.1690 (6.1821)
2022-11-24 20:54:02,696:INFO: Dataset: zara1               Batch: 4/8	Loss 6.1953 (6.1855)
2022-11-24 20:54:02,716:INFO: Dataset: zara1               Batch: 5/8	Loss 6.2360 (6.1961)
2022-11-24 20:54:02,736:INFO: Dataset: zara1               Batch: 6/8	Loss 6.0318 (6.1691)
2022-11-24 20:54:02,756:INFO: Dataset: zara1               Batch: 7/8	Loss 6.0471 (6.1490)
2022-11-24 20:54:02,773:INFO: Dataset: zara1               Batch: 8/8	Loss 5.2816 (6.0678)
2022-11-24 20:54:02,995:INFO: Dataset: zara2               Batch:  1/18	Loss 5.1231 (5.1231)
2022-11-24 20:54:03,015:INFO: Dataset: zara2               Batch:  2/18	Loss 4.9789 (5.0553)
2022-11-24 20:54:03,038:INFO: Dataset: zara2               Batch:  3/18	Loss 5.0284 (5.0463)
2022-11-24 20:54:03,057:INFO: Dataset: zara2               Batch:  4/18	Loss 5.0202 (5.0401)
2022-11-24 20:54:03,076:INFO: Dataset: zara2               Batch:  5/18	Loss 4.9373 (5.0220)
2022-11-24 20:54:03,101:INFO: Dataset: zara2               Batch:  6/18	Loss 5.5918 (5.1104)
2022-11-24 20:54:03,120:INFO: Dataset: zara2               Batch:  7/18	Loss 5.0569 (5.1024)
2022-11-24 20:54:03,139:INFO: Dataset: zara2               Batch:  8/18	Loss 5.3057 (5.1270)
2022-11-24 20:54:03,158:INFO: Dataset: zara2               Batch:  9/18	Loss 5.3276 (5.1482)
2022-11-24 20:54:03,178:INFO: Dataset: zara2               Batch: 10/18	Loss 5.0917 (5.1428)
2022-11-24 20:54:03,196:INFO: Dataset: zara2               Batch: 11/18	Loss 5.4654 (5.1704)
2022-11-24 20:54:03,217:INFO: Dataset: zara2               Batch: 12/18	Loss 5.1164 (5.1659)
2022-11-24 20:54:03,237:INFO: Dataset: zara2               Batch: 13/18	Loss 5.0562 (5.1580)
2022-11-24 20:54:03,257:INFO: Dataset: zara2               Batch: 14/18	Loss 5.5881 (5.1905)
2022-11-24 20:54:03,276:INFO: Dataset: zara2               Batch: 15/18	Loss 5.4342 (5.2066)
2022-11-24 20:54:03,296:INFO: Dataset: zara2               Batch: 16/18	Loss 4.8684 (5.1856)
2022-11-24 20:54:03,316:INFO: Dataset: zara2               Batch: 17/18	Loss 5.0378 (5.1775)
2022-11-24 20:54:03,334:INFO: Dataset: zara2               Batch: 18/18	Loss 4.4865 (5.1402)
2022-11-24 20:54:03,369:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_65.pth.tar
2022-11-24 20:54:03,369:INFO: 
===> EPOCH: 66 (P1)
2022-11-24 20:54:03,369:INFO: - Computing loss (training)
2022-11-24 20:54:03,560:INFO: Dataset: hotel               Batch: 1/4	Loss 9.7866 (9.7866)
2022-11-24 20:54:03,582:INFO: Dataset: hotel               Batch: 2/4	Loss 10.4277 (10.1094)
2022-11-24 20:54:03,601:INFO: Dataset: hotel               Batch: 3/4	Loss 8.5579 (9.5947)
2022-11-24 20:54:03,614:INFO: Dataset: hotel               Batch: 4/4	Loss 5.8743 (8.9959)
2022-11-24 20:54:03,824:INFO: Dataset: univ                Batch:  1/15	Loss 5.2534 (5.2534)
2022-11-24 20:54:03,846:INFO: Dataset: univ                Batch:  2/15	Loss 5.1277 (5.1915)
2022-11-24 20:54:03,871:INFO: Dataset: univ                Batch:  3/15	Loss 5.8529 (5.3842)
2022-11-24 20:54:03,891:INFO: Dataset: univ                Batch:  4/15	Loss 5.0864 (5.3084)
2022-11-24 20:54:03,914:INFO: Dataset: univ                Batch:  5/15	Loss 5.9281 (5.4319)
2022-11-24 20:54:03,939:INFO: Dataset: univ                Batch:  6/15	Loss 4.8768 (5.3278)
2022-11-24 20:54:03,960:INFO: Dataset: univ                Batch:  7/15	Loss 5.2985 (5.3240)
2022-11-24 20:54:03,981:INFO: Dataset: univ                Batch:  8/15	Loss 5.6465 (5.3635)
2022-11-24 20:54:04,002:INFO: Dataset: univ                Batch:  9/15	Loss 5.2613 (5.3515)
2022-11-24 20:54:04,023:INFO: Dataset: univ                Batch: 10/15	Loss 4.6563 (5.2689)
2022-11-24 20:54:04,044:INFO: Dataset: univ                Batch: 11/15	Loss 5.1117 (5.2533)
2022-11-24 20:54:04,066:INFO: Dataset: univ                Batch: 12/15	Loss 5.7731 (5.2956)
2022-11-24 20:54:04,088:INFO: Dataset: univ                Batch: 13/15	Loss 5.4898 (5.3093)
2022-11-24 20:54:04,109:INFO: Dataset: univ                Batch: 14/15	Loss 5.3929 (5.3151)
2022-11-24 20:54:04,117:INFO: Dataset: univ                Batch: 15/15	Loss 0.9799 (5.2719)
2022-11-24 20:54:04,331:INFO: Dataset: zara1               Batch: 1/8	Loss 5.9170 (5.9170)
2022-11-24 20:54:04,351:INFO: Dataset: zara1               Batch: 2/8	Loss 6.3228 (6.1434)
2022-11-24 20:54:04,370:INFO: Dataset: zara1               Batch: 3/8	Loss 6.2657 (6.1850)
2022-11-24 20:54:04,389:INFO: Dataset: zara1               Batch: 4/8	Loss 5.7267 (6.0718)
2022-11-24 20:54:04,408:INFO: Dataset: zara1               Batch: 5/8	Loss 5.8592 (6.0305)
2022-11-24 20:54:04,430:INFO: Dataset: zara1               Batch: 6/8	Loss 5.8742 (6.0054)
2022-11-24 20:54:04,448:INFO: Dataset: zara1               Batch: 7/8	Loss 6.1630 (6.0271)
2022-11-24 20:54:04,465:INFO: Dataset: zara1               Batch: 8/8	Loss 5.1596 (5.9330)
2022-11-24 20:54:04,672:INFO: Dataset: zara2               Batch:  1/18	Loss 4.4447 (4.4447)
2022-11-24 20:54:04,691:INFO: Dataset: zara2               Batch:  2/18	Loss 4.9687 (4.7122)
2022-11-24 20:54:04,711:INFO: Dataset: zara2               Batch:  3/18	Loss 4.9817 (4.8045)
2022-11-24 20:54:04,731:INFO: Dataset: zara2               Batch:  4/18	Loss 5.7471 (5.0459)
2022-11-24 20:54:04,751:INFO: Dataset: zara2               Batch:  5/18	Loss 5.2367 (5.0876)
2022-11-24 20:54:04,777:INFO: Dataset: zara2               Batch:  6/18	Loss 4.8801 (5.0502)
2022-11-24 20:54:04,796:INFO: Dataset: zara2               Batch:  7/18	Loss 4.8448 (5.0230)
2022-11-24 20:54:04,814:INFO: Dataset: zara2               Batch:  8/18	Loss 4.7570 (4.9913)
2022-11-24 20:54:04,833:INFO: Dataset: zara2               Batch:  9/18	Loss 5.0239 (4.9952)
2022-11-24 20:54:04,852:INFO: Dataset: zara2               Batch: 10/18	Loss 5.3788 (5.0357)
2022-11-24 20:54:04,871:INFO: Dataset: zara2               Batch: 11/18	Loss 5.3920 (5.0724)
2022-11-24 20:54:04,892:INFO: Dataset: zara2               Batch: 12/18	Loss 4.6009 (5.0308)
2022-11-24 20:54:04,912:INFO: Dataset: zara2               Batch: 13/18	Loss 5.1674 (5.0421)
2022-11-24 20:54:04,933:INFO: Dataset: zara2               Batch: 14/18	Loss 4.8064 (5.0241)
2022-11-24 20:54:04,953:INFO: Dataset: zara2               Batch: 15/18	Loss 4.8230 (5.0104)
2022-11-24 20:54:04,973:INFO: Dataset: zara2               Batch: 16/18	Loss 4.7924 (4.9954)
2022-11-24 20:54:04,994:INFO: Dataset: zara2               Batch: 17/18	Loss 5.8426 (5.0445)
2022-11-24 20:54:05,012:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6794 (5.0273)
2022-11-24 20:54:05,051:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_66.pth.tar
2022-11-24 20:54:05,051:INFO: 
===> EPOCH: 67 (P1)
2022-11-24 20:54:05,052:INFO: - Computing loss (training)
2022-11-24 20:54:05,223:INFO: Dataset: hotel               Batch: 1/4	Loss 9.3318 (9.3318)
2022-11-24 20:54:05,244:INFO: Dataset: hotel               Batch: 2/4	Loss 9.3594 (9.3467)
2022-11-24 20:54:05,265:INFO: Dataset: hotel               Batch: 3/4	Loss 9.9727 (9.5537)
2022-11-24 20:54:05,279:INFO: Dataset: hotel               Batch: 4/4	Loss 5.3594 (8.8565)
2022-11-24 20:54:05,554:INFO: Dataset: univ                Batch:  1/15	Loss 5.0358 (5.0358)
2022-11-24 20:54:05,577:INFO: Dataset: univ                Batch:  2/15	Loss 5.3255 (5.1796)
2022-11-24 20:54:05,601:INFO: Dataset: univ                Batch:  3/15	Loss 5.3379 (5.2286)
2022-11-24 20:54:05,622:INFO: Dataset: univ                Batch:  4/15	Loss 5.3726 (5.2667)
2022-11-24 20:54:05,642:INFO: Dataset: univ                Batch:  5/15	Loss 5.8560 (5.3798)
2022-11-24 20:54:05,665:INFO: Dataset: univ                Batch:  6/15	Loss 5.2581 (5.3596)
2022-11-24 20:54:05,685:INFO: Dataset: univ                Batch:  7/15	Loss 5.7961 (5.4177)
2022-11-24 20:54:05,706:INFO: Dataset: univ                Batch:  8/15	Loss 5.0482 (5.3671)
2022-11-24 20:54:05,727:INFO: Dataset: univ                Batch:  9/15	Loss 5.3564 (5.3658)
2022-11-24 20:54:05,747:INFO: Dataset: univ                Batch: 10/15	Loss 5.2247 (5.3528)
2022-11-24 20:54:05,770:INFO: Dataset: univ                Batch: 11/15	Loss 5.5907 (5.3722)
2022-11-24 20:54:05,791:INFO: Dataset: univ                Batch: 12/15	Loss 4.7330 (5.3146)
2022-11-24 20:54:05,814:INFO: Dataset: univ                Batch: 13/15	Loss 5.2790 (5.3115)
2022-11-24 20:54:05,837:INFO: Dataset: univ                Batch: 14/15	Loss 4.9060 (5.2807)
2022-11-24 20:54:05,846:INFO: Dataset: univ                Batch: 15/15	Loss 0.9232 (5.2173)
2022-11-24 20:54:06,058:INFO: Dataset: zara1               Batch: 1/8	Loss 5.6132 (5.6132)
2022-11-24 20:54:06,079:INFO: Dataset: zara1               Batch: 2/8	Loss 6.3963 (5.9712)
2022-11-24 20:54:06,098:INFO: Dataset: zara1               Batch: 3/8	Loss 6.1409 (6.0234)
2022-11-24 20:54:06,117:INFO: Dataset: zara1               Batch: 4/8	Loss 6.5960 (6.1588)
2022-11-24 20:54:06,135:INFO: Dataset: zara1               Batch: 5/8	Loss 5.7432 (6.0712)
2022-11-24 20:54:06,156:INFO: Dataset: zara1               Batch: 6/8	Loss 5.6674 (5.9984)
2022-11-24 20:54:06,175:INFO: Dataset: zara1               Batch: 7/8	Loss 6.2557 (6.0327)
2022-11-24 20:54:06,192:INFO: Dataset: zara1               Batch: 8/8	Loss 5.1957 (5.9428)
2022-11-24 20:54:06,407:INFO: Dataset: zara2               Batch:  1/18	Loss 4.9713 (4.9713)
2022-11-24 20:54:06,429:INFO: Dataset: zara2               Batch:  2/18	Loss 4.9024 (4.9368)
2022-11-24 20:54:06,450:INFO: Dataset: zara2               Batch:  3/18	Loss 5.2188 (5.0304)
2022-11-24 20:54:06,474:INFO: Dataset: zara2               Batch:  4/18	Loss 5.2068 (5.0788)
2022-11-24 20:54:06,493:INFO: Dataset: zara2               Batch:  5/18	Loss 5.1356 (5.0904)
2022-11-24 20:54:06,515:INFO: Dataset: zara2               Batch:  6/18	Loss 4.8928 (5.0566)
2022-11-24 20:54:06,534:INFO: Dataset: zara2               Batch:  7/18	Loss 4.8365 (5.0244)
2022-11-24 20:54:06,553:INFO: Dataset: zara2               Batch:  8/18	Loss 4.5256 (4.9674)
2022-11-24 20:54:06,572:INFO: Dataset: zara2               Batch:  9/18	Loss 4.6241 (4.9300)
2022-11-24 20:54:06,591:INFO: Dataset: zara2               Batch: 10/18	Loss 5.1143 (4.9475)
2022-11-24 20:54:06,612:INFO: Dataset: zara2               Batch: 11/18	Loss 4.9373 (4.9465)
2022-11-24 20:54:06,631:INFO: Dataset: zara2               Batch: 12/18	Loss 5.0056 (4.9521)
2022-11-24 20:54:06,652:INFO: Dataset: zara2               Batch: 13/18	Loss 5.1831 (4.9707)
2022-11-24 20:54:06,677:INFO: Dataset: zara2               Batch: 14/18	Loss 4.7832 (4.9581)
2022-11-24 20:54:06,700:INFO: Dataset: zara2               Batch: 15/18	Loss 4.8017 (4.9479)
2022-11-24 20:54:06,722:INFO: Dataset: zara2               Batch: 16/18	Loss 5.2802 (4.9671)
2022-11-24 20:54:06,742:INFO: Dataset: zara2               Batch: 17/18	Loss 5.1790 (4.9790)
2022-11-24 20:54:06,760:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6861 (4.9644)
2022-11-24 20:54:06,795:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_67.pth.tar
2022-11-24 20:54:06,795:INFO: 
===> EPOCH: 68 (P1)
2022-11-24 20:54:06,795:INFO: - Computing loss (training)
2022-11-24 20:54:06,983:INFO: Dataset: hotel               Batch: 1/4	Loss 8.6246 (8.6246)
2022-11-24 20:54:07,002:INFO: Dataset: hotel               Batch: 2/4	Loss 8.3960 (8.5127)
2022-11-24 20:54:07,023:INFO: Dataset: hotel               Batch: 3/4	Loss 10.9336 (9.2826)
2022-11-24 20:54:07,037:INFO: Dataset: hotel               Batch: 4/4	Loss 5.4502 (8.6456)
2022-11-24 20:54:07,254:INFO: Dataset: univ                Batch:  1/15	Loss 5.2614 (5.2614)
2022-11-24 20:54:07,278:INFO: Dataset: univ                Batch:  2/15	Loss 5.4452 (5.3545)
2022-11-24 20:54:07,301:INFO: Dataset: univ                Batch:  3/15	Loss 5.2117 (5.3052)
2022-11-24 20:54:07,326:INFO: Dataset: univ                Batch:  4/15	Loss 5.5210 (5.3573)
2022-11-24 20:54:07,347:INFO: Dataset: univ                Batch:  5/15	Loss 4.9612 (5.2754)
2022-11-24 20:54:07,372:INFO: Dataset: univ                Batch:  6/15	Loss 5.1698 (5.2569)
2022-11-24 20:54:07,393:INFO: Dataset: univ                Batch:  7/15	Loss 5.2892 (5.2617)
2022-11-24 20:54:07,415:INFO: Dataset: univ                Batch:  8/15	Loss 5.5166 (5.2926)
2022-11-24 20:54:07,436:INFO: Dataset: univ                Batch:  9/15	Loss 5.0699 (5.2648)
2022-11-24 20:54:07,458:INFO: Dataset: univ                Batch: 10/15	Loss 4.7759 (5.2119)
2022-11-24 20:54:07,480:INFO: Dataset: univ                Batch: 11/15	Loss 5.1044 (5.2023)
2022-11-24 20:54:07,503:INFO: Dataset: univ                Batch: 12/15	Loss 5.3314 (5.2131)
2022-11-24 20:54:07,525:INFO: Dataset: univ                Batch: 13/15	Loss 5.1302 (5.2066)
2022-11-24 20:54:07,546:INFO: Dataset: univ                Batch: 14/15	Loss 5.0388 (5.1944)
2022-11-24 20:54:07,556:INFO: Dataset: univ                Batch: 15/15	Loss 1.1905 (5.1570)
2022-11-24 20:54:07,764:INFO: Dataset: zara1               Batch: 1/8	Loss 6.1180 (6.1180)
2022-11-24 20:54:07,785:INFO: Dataset: zara1               Batch: 2/8	Loss 5.9313 (6.0242)
2022-11-24 20:54:07,806:INFO: Dataset: zara1               Batch: 3/8	Loss 6.0901 (6.0477)
2022-11-24 20:54:07,826:INFO: Dataset: zara1               Batch: 4/8	Loss 6.2321 (6.0905)
2022-11-24 20:54:07,847:INFO: Dataset: zara1               Batch: 5/8	Loss 5.2832 (5.9279)
2022-11-24 20:54:07,870:INFO: Dataset: zara1               Batch: 6/8	Loss 6.1837 (5.9705)
2022-11-24 20:54:07,890:INFO: Dataset: zara1               Batch: 7/8	Loss 5.8607 (5.9536)
2022-11-24 20:54:07,908:INFO: Dataset: zara1               Batch: 8/8	Loss 5.4345 (5.8940)
2022-11-24 20:54:08,114:INFO: Dataset: zara2               Batch:  1/18	Loss 4.8432 (4.8432)
2022-11-24 20:54:08,134:INFO: Dataset: zara2               Batch:  2/18	Loss 5.0898 (4.9662)
2022-11-24 20:54:08,153:INFO: Dataset: zara2               Batch:  3/18	Loss 5.0218 (4.9845)
2022-11-24 20:54:08,173:INFO: Dataset: zara2               Batch:  4/18	Loss 4.8301 (4.9453)
2022-11-24 20:54:08,195:INFO: Dataset: zara2               Batch:  5/18	Loss 4.9599 (4.9481)
2022-11-24 20:54:08,216:INFO: Dataset: zara2               Batch:  6/18	Loss 4.6968 (4.9086)
2022-11-24 20:54:08,235:INFO: Dataset: zara2               Batch:  7/18	Loss 4.8669 (4.9025)
2022-11-24 20:54:08,254:INFO: Dataset: zara2               Batch:  8/18	Loss 4.9720 (4.9117)
2022-11-24 20:54:08,273:INFO: Dataset: zara2               Batch:  9/18	Loss 4.8578 (4.9059)
2022-11-24 20:54:08,292:INFO: Dataset: zara2               Batch: 10/18	Loss 5.0518 (4.9214)
2022-11-24 20:54:08,312:INFO: Dataset: zara2               Batch: 11/18	Loss 5.7465 (5.0000)
2022-11-24 20:54:08,332:INFO: Dataset: zara2               Batch: 12/18	Loss 5.4707 (5.0386)
2022-11-24 20:54:08,351:INFO: Dataset: zara2               Batch: 13/18	Loss 4.3398 (4.9877)
2022-11-24 20:54:08,371:INFO: Dataset: zara2               Batch: 14/18	Loss 5.5478 (5.0231)
2022-11-24 20:54:08,395:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6446 (4.9950)
2022-11-24 20:54:08,416:INFO: Dataset: zara2               Batch: 16/18	Loss 5.0519 (4.9983)
2022-11-24 20:54:08,437:INFO: Dataset: zara2               Batch: 17/18	Loss 5.1210 (5.0061)
2022-11-24 20:54:08,455:INFO: Dataset: zara2               Batch: 18/18	Loss 4.0755 (4.9652)
2022-11-24 20:54:08,499:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_68.pth.tar
2022-11-24 20:54:08,499:INFO: 
===> EPOCH: 69 (P1)
2022-11-24 20:54:08,499:INFO: - Computing loss (training)
2022-11-24 20:54:08,670:INFO: Dataset: hotel               Batch: 1/4	Loss 9.6739 (9.6739)
2022-11-24 20:54:08,693:INFO: Dataset: hotel               Batch: 2/4	Loss 10.6454 (10.1923)
2022-11-24 20:54:08,718:INFO: Dataset: hotel               Batch: 3/4	Loss 9.4808 (9.9629)
2022-11-24 20:54:08,734:INFO: Dataset: hotel               Batch: 4/4	Loss 4.1586 (9.0287)
2022-11-24 20:54:08,956:INFO: Dataset: univ                Batch:  1/15	Loss 5.0716 (5.0716)
2022-11-24 20:54:08,979:INFO: Dataset: univ                Batch:  2/15	Loss 5.0798 (5.0759)
2022-11-24 20:54:09,002:INFO: Dataset: univ                Batch:  3/15	Loss 5.1956 (5.1143)
2022-11-24 20:54:09,028:INFO: Dataset: univ                Batch:  4/15	Loss 5.3261 (5.1672)
2022-11-24 20:54:09,052:INFO: Dataset: univ                Batch:  5/15	Loss 5.1610 (5.1660)
2022-11-24 20:54:09,075:INFO: Dataset: univ                Batch:  6/15	Loss 5.0899 (5.1550)
2022-11-24 20:54:09,097:INFO: Dataset: univ                Batch:  7/15	Loss 5.1801 (5.1587)
2022-11-24 20:54:09,119:INFO: Dataset: univ                Batch:  8/15	Loss 5.2739 (5.1745)
2022-11-24 20:54:09,141:INFO: Dataset: univ                Batch:  9/15	Loss 5.1792 (5.1750)
2022-11-24 20:54:09,163:INFO: Dataset: univ                Batch: 10/15	Loss 5.5269 (5.2111)
2022-11-24 20:54:09,188:INFO: Dataset: univ                Batch: 11/15	Loss 4.8234 (5.1740)
2022-11-24 20:54:09,211:INFO: Dataset: univ                Batch: 12/15	Loss 5.3425 (5.1865)
2022-11-24 20:54:09,234:INFO: Dataset: univ                Batch: 13/15	Loss 5.2811 (5.1936)
2022-11-24 20:54:09,256:INFO: Dataset: univ                Batch: 14/15	Loss 4.4552 (5.1380)
2022-11-24 20:54:09,265:INFO: Dataset: univ                Batch: 15/15	Loss 0.8927 (5.0795)
2022-11-24 20:54:09,487:INFO: Dataset: zara1               Batch: 1/8	Loss 5.7177 (5.7177)
2022-11-24 20:54:09,509:INFO: Dataset: zara1               Batch: 2/8	Loss 5.9944 (5.8561)
2022-11-24 20:54:09,528:INFO: Dataset: zara1               Batch: 3/8	Loss 5.6372 (5.7848)
2022-11-24 20:54:09,549:INFO: Dataset: zara1               Batch: 4/8	Loss 5.8142 (5.7929)
2022-11-24 20:54:09,569:INFO: Dataset: zara1               Batch: 5/8	Loss 5.6113 (5.7555)
2022-11-24 20:54:09,590:INFO: Dataset: zara1               Batch: 6/8	Loss 6.0028 (5.7956)
2022-11-24 20:54:09,609:INFO: Dataset: zara1               Batch: 7/8	Loss 5.8276 (5.8003)
2022-11-24 20:54:09,626:INFO: Dataset: zara1               Batch: 8/8	Loss 5.1128 (5.7240)
2022-11-24 20:54:09,850:INFO: Dataset: zara2               Batch:  1/18	Loss 5.0181 (5.0181)
2022-11-24 20:54:09,886:INFO: Dataset: zara2               Batch:  2/18	Loss 4.6616 (4.8344)
2022-11-24 20:54:09,909:INFO: Dataset: zara2               Batch:  3/18	Loss 5.1907 (4.9531)
2022-11-24 20:54:09,930:INFO: Dataset: zara2               Batch:  4/18	Loss 4.8539 (4.9279)
2022-11-24 20:54:09,950:INFO: Dataset: zara2               Batch:  5/18	Loss 5.0731 (4.9561)
2022-11-24 20:54:09,973:INFO: Dataset: zara2               Batch:  6/18	Loss 5.0042 (4.9651)
2022-11-24 20:54:09,993:INFO: Dataset: zara2               Batch:  7/18	Loss 4.7687 (4.9381)
2022-11-24 20:54:10,012:INFO: Dataset: zara2               Batch:  8/18	Loss 4.6548 (4.9061)
2022-11-24 20:54:10,032:INFO: Dataset: zara2               Batch:  9/18	Loss 5.2031 (4.9374)
2022-11-24 20:54:10,052:INFO: Dataset: zara2               Batch: 10/18	Loss 4.6827 (4.9104)
2022-11-24 20:54:10,077:INFO: Dataset: zara2               Batch: 11/18	Loss 4.7774 (4.9004)
2022-11-24 20:54:10,098:INFO: Dataset: zara2               Batch: 12/18	Loss 5.2315 (4.9268)
2022-11-24 20:54:10,120:INFO: Dataset: zara2               Batch: 13/18	Loss 4.2188 (4.8657)
2022-11-24 20:54:10,143:INFO: Dataset: zara2               Batch: 14/18	Loss 4.1526 (4.8157)
2022-11-24 20:54:10,164:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6810 (4.8068)
2022-11-24 20:54:10,185:INFO: Dataset: zara2               Batch: 16/18	Loss 4.4228 (4.7845)
2022-11-24 20:54:10,205:INFO: Dataset: zara2               Batch: 17/18	Loss 4.9088 (4.7916)
2022-11-24 20:54:10,224:INFO: Dataset: zara2               Batch: 18/18	Loss 4.5749 (4.7817)
2022-11-24 20:54:10,262:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_69.pth.tar
2022-11-24 20:54:10,262:INFO: 
===> EPOCH: 70 (P1)
2022-11-24 20:54:10,263:INFO: - Computing loss (training)
2022-11-24 20:54:10,440:INFO: Dataset: hotel               Batch: 1/4	Loss 8.9453 (8.9453)
2022-11-24 20:54:10,461:INFO: Dataset: hotel               Batch: 2/4	Loss 11.4565 (10.1826)
2022-11-24 20:54:10,482:INFO: Dataset: hotel               Batch: 3/4	Loss 7.8355 (9.3877)
2022-11-24 20:54:10,495:INFO: Dataset: hotel               Batch: 4/4	Loss 5.2848 (8.6570)
2022-11-24 20:54:10,747:INFO: Dataset: univ                Batch:  1/15	Loss 4.7957 (4.7957)
2022-11-24 20:54:10,769:INFO: Dataset: univ                Batch:  2/15	Loss 5.1896 (4.9988)
2022-11-24 20:54:10,791:INFO: Dataset: univ                Batch:  3/15	Loss 4.8270 (4.9395)
2022-11-24 20:54:10,816:INFO: Dataset: univ                Batch:  4/15	Loss 5.2088 (5.0037)
2022-11-24 20:54:10,837:INFO: Dataset: univ                Batch:  5/15	Loss 4.9369 (4.9900)
2022-11-24 20:54:10,862:INFO: Dataset: univ                Batch:  6/15	Loss 5.0107 (4.9936)
2022-11-24 20:54:10,884:INFO: Dataset: univ                Batch:  7/15	Loss 4.9292 (4.9844)
2022-11-24 20:54:10,906:INFO: Dataset: univ                Batch:  8/15	Loss 5.0506 (4.9925)
2022-11-24 20:54:10,934:INFO: Dataset: univ                Batch:  9/15	Loss 4.9522 (4.9880)
2022-11-24 20:54:10,956:INFO: Dataset: univ                Batch: 10/15	Loss 5.0610 (4.9949)
2022-11-24 20:54:10,981:INFO: Dataset: univ                Batch: 11/15	Loss 4.9326 (4.9885)
2022-11-24 20:54:11,004:INFO: Dataset: univ                Batch: 12/15	Loss 5.2121 (5.0066)
2022-11-24 20:54:11,028:INFO: Dataset: univ                Batch: 13/15	Loss 5.1877 (5.0196)
2022-11-24 20:54:11,050:INFO: Dataset: univ                Batch: 14/15	Loss 5.2093 (5.0327)
2022-11-24 20:54:11,059:INFO: Dataset: univ                Batch: 15/15	Loss 1.1883 (4.9925)
2022-11-24 20:54:11,255:INFO: Dataset: zara1               Batch: 1/8	Loss 5.4171 (5.4171)
2022-11-24 20:54:11,275:INFO: Dataset: zara1               Batch: 2/8	Loss 5.8768 (5.6626)
2022-11-24 20:54:11,294:INFO: Dataset: zara1               Batch: 3/8	Loss 5.4312 (5.5925)
2022-11-24 20:54:11,313:INFO: Dataset: zara1               Batch: 4/8	Loss 5.6028 (5.5949)
2022-11-24 20:54:11,333:INFO: Dataset: zara1               Batch: 5/8	Loss 5.5637 (5.5883)
2022-11-24 20:54:11,354:INFO: Dataset: zara1               Batch: 6/8	Loss 5.6706 (5.6013)
2022-11-24 20:54:11,373:INFO: Dataset: zara1               Batch: 7/8	Loss 6.0262 (5.6652)
2022-11-24 20:54:11,390:INFO: Dataset: zara1               Batch: 8/8	Loss 5.0921 (5.6073)
2022-11-24 20:54:11,625:INFO: Dataset: zara2               Batch:  1/18	Loss 4.3093 (4.3093)
2022-11-24 20:54:11,646:INFO: Dataset: zara2               Batch:  2/18	Loss 4.7010 (4.4899)
2022-11-24 20:54:11,670:INFO: Dataset: zara2               Batch:  3/18	Loss 4.6300 (4.5364)
2022-11-24 20:54:11,692:INFO: Dataset: zara2               Batch:  4/18	Loss 4.9154 (4.6184)
2022-11-24 20:54:11,714:INFO: Dataset: zara2               Batch:  5/18	Loss 5.2935 (4.7533)
2022-11-24 20:54:11,739:INFO: Dataset: zara2               Batch:  6/18	Loss 4.6067 (4.7279)
2022-11-24 20:54:11,758:INFO: Dataset: zara2               Batch:  7/18	Loss 4.5040 (4.6930)
2022-11-24 20:54:11,778:INFO: Dataset: zara2               Batch:  8/18	Loss 4.7159 (4.6960)
2022-11-24 20:54:11,798:INFO: Dataset: zara2               Batch:  9/18	Loss 4.4473 (4.6688)
2022-11-24 20:54:11,818:INFO: Dataset: zara2               Batch: 10/18	Loss 5.2190 (4.7210)
2022-11-24 20:54:11,839:INFO: Dataset: zara2               Batch: 11/18	Loss 4.7331 (4.7221)
2022-11-24 20:54:11,862:INFO: Dataset: zara2               Batch: 12/18	Loss 4.4833 (4.7013)
2022-11-24 20:54:11,883:INFO: Dataset: zara2               Batch: 13/18	Loss 4.6408 (4.6972)
2022-11-24 20:54:11,903:INFO: Dataset: zara2               Batch: 14/18	Loss 4.5709 (4.6881)
2022-11-24 20:54:11,925:INFO: Dataset: zara2               Batch: 15/18	Loss 5.0057 (4.7118)
2022-11-24 20:54:11,945:INFO: Dataset: zara2               Batch: 16/18	Loss 5.1046 (4.7372)
2022-11-24 20:54:11,965:INFO: Dataset: zara2               Batch: 17/18	Loss 4.4708 (4.7210)
2022-11-24 20:54:11,984:INFO: Dataset: zara2               Batch: 18/18	Loss 4.2008 (4.6979)
2022-11-24 20:54:12,029:INFO:  --> Model Saved in ./models/E1//P1/CRMF_epoch_70.pth.tar
2022-11-24 20:54:12,029:INFO: 
===> EPOCH: 71 (P1)
2022-11-24 20:54:12,030:INFO: - Computing loss (training)
2022-11-24 20:54:12,197:INFO: Dataset: hotel               Batch: 1/4	Loss 8.7881 (8.7881)
2022-11-24 20:54:12,217:INFO: Dataset: hotel               Batch: 2/4	Loss 10.2110 (9.4853)
2022-11-24 20:54:12,239:INFO: Dataset: hotel               Batch: 3/4	Loss 8.6392 (9.1842)
2022-11-24 20:54:12,253:INFO: Dataset: hotel               Batch: 4/4	Loss 5.5061 (8.5194)
2022-11-24 20:54:12,469:INFO: Dataset: univ                Batch:  1/15	Loss 4.6070 (4.6070)
2022-11-24 20:54:12,492:INFO: Dataset: univ                Batch:  2/15	Loss 5.0315 (4.8192)
2022-11-24 20:54:12,514:INFO: Dataset: univ                Batch:  3/15	Loss 4.7943 (4.8109)
2022-11-24 20:54:12,538:INFO: Dataset: univ                Batch:  4/15	Loss 5.1108 (4.8819)
2022-11-24 20:54:12,560:INFO: Dataset: univ                Batch:  5/15	Loss 5.5958 (5.0144)
2022-11-24 20:54:12,584:INFO: Dataset: univ                Batch:  6/15	Loss 4.4904 (4.9180)
2022-11-24 20:54:12,605:INFO: Dataset: univ                Batch:  7/15	Loss 4.9970 (4.9290)
2022-11-24 20:54:12,626:INFO: Dataset: univ                Batch:  8/15	Loss 5.2496 (4.9693)
2022-11-24 20:54:12,646:INFO: Dataset: univ                Batch:  9/15	Loss 5.3195 (5.0046)
2022-11-24 20:54:12,667:INFO: Dataset: univ                Batch: 10/15	Loss 5.1345 (5.0180)
2022-11-24 20:54:12,689:INFO: Dataset: univ                Batch: 11/15	Loss 5.0535 (5.0210)
2022-11-24 20:54:12,714:INFO: Dataset: univ                Batch: 12/15	Loss 4.9262 (5.0134)
2022-11-24 20:54:12,739:INFO: Dataset: univ                Batch: 13/15	Loss 5.1592 (5.0252)
2022-11-24 20:54:12,761:INFO: Dataset: univ                Batch: 14/15	Loss 5.3600 (5.0486)

2022-11-01 16:09:00,788:INFO: Initializing Training Set
2022-11-01 16:09:04,601:INFO: Initializing Validation Set
2022-11-01 16:09:05,046:INFO: Initializing Validation O Set
2022-11-01 16:09:07,538:INFO: => loaded checkpoint './models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(200, 150, 300, 300)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_653.pth.tar' (epoch 654)
2022-11-01 16:09:07,539:INFO: 
===> EPOCH: 654 (P4)
2022-11-01 16:09:07,539:INFO: - Computing loss (training)
2022-11-01 16:09:16,041:INFO: Dataset: hotel               Batch: 1/8	Loss -916.8528 (-916.8528)
2022-11-01 16:09:16,922:INFO: Dataset: hotel               Batch: 2/8	Loss -2565.8235 (-1733.5600)
2022-11-01 16:09:17,552:INFO: Dataset: hotel               Batch: 3/8	Loss -1598.0857 (-1686.7294)
2022-11-01 16:09:18,187:INFO: Dataset: hotel               Batch: 4/8	Loss -1938.0636 (-1748.6862)
2022-11-01 16:09:18,808:INFO: Dataset: hotel               Batch: 5/8	Loss -1098.0593 (-1625.9264)
2022-11-01 16:09:19,420:INFO: Dataset: hotel               Batch: 6/8	Loss -1076.5911 (-1535.8146)
2022-11-01 16:09:20,018:INFO: Dataset: hotel               Batch: 7/8	Loss -873.1407 (-1440.8910)
2022-11-01 16:09:20,541:INFO: Dataset: hotel               Batch: 8/8	Loss -758.4313 (-1424.6849)
2022-11-01 16:09:50,528:INFO: Dataset: univ                Batch:  1/29	Loss -455.9386 (-455.9386)
2022-11-01 16:09:51,327:INFO: Dataset: univ                Batch:  2/29	Loss -860.4031 (-650.2721)
2022-11-01 16:09:52,137:INFO: Dataset: univ                Batch:  3/29	Loss -1372.3798 (-875.2566)
2022-11-01 16:09:52,974:INFO: Dataset: univ                Batch:  4/29	Loss -1665.1282 (-1048.8547)
2022-11-01 16:09:53,708:INFO: Dataset: univ                Batch:  5/29	Loss -3847.7363 (-1581.6157)
2022-11-01 16:09:54,537:INFO: Dataset: univ                Batch:  6/29	Loss -1976.1031 (-1641.2087)
2022-11-01 16:09:55,362:INFO: Dataset: univ                Batch:  7/29	Loss -2627.7212 (-1773.3955)
2022-11-01 16:09:56,123:INFO: Dataset: univ                Batch:  8/29	Loss -949.0181 (-1654.6482)
2022-11-01 16:09:56,850:INFO: Dataset: univ                Batch:  9/29	Loss -2283.4387 (-1708.2267)
2022-11-01 16:09:57,588:INFO: Dataset: univ                Batch: 10/29	Loss -2030.1637 (-1741.1515)
2022-11-01 16:09:58,314:INFO: Dataset: univ                Batch: 11/29	Loss -4460.5410 (-2014.7944)
2022-11-01 16:09:59,201:INFO: Dataset: univ                Batch: 12/29	Loss -1040.8301 (-1934.8070)
2022-11-01 16:10:00,488:INFO: Dataset: univ                Batch: 13/29	Loss -3248.5693 (-2035.6724)
2022-11-01 16:10:01,315:INFO: Dataset: univ                Batch: 14/29	Loss -1234.3506 (-1981.5580)
2022-11-01 16:10:02,114:INFO: Dataset: univ                Batch: 15/29	Loss -1211.9658 (-1922.3316)
2022-11-01 16:10:02,855:INFO: Dataset: univ                Batch: 16/29	Loss -2244.9419 (-1941.2874)
2022-11-01 16:10:03,610:INFO: Dataset: univ                Batch: 17/29	Loss -648.2440 (-1858.7084)
2022-11-01 16:10:04,263:INFO: Dataset: univ                Batch: 18/29	Loss -982.5895 (-1807.4260)
2022-11-01 16:10:04,873:INFO: Dataset: univ                Batch: 19/29	Loss -1599.1362 (-1797.1564)
2022-11-01 16:10:05,529:INFO: Dataset: univ                Batch: 20/29	Loss -1816.4570 (-1798.0873)
2022-11-01 16:10:06,161:INFO: Dataset: univ                Batch: 21/29	Loss -1090.1467 (-1759.5993)
2022-11-01 16:10:06,802:INFO: Dataset: univ                Batch: 22/29	Loss -1094.6453 (-1728.4713)
2022-11-01 16:10:07,453:INFO: Dataset: univ                Batch: 23/29	Loss -3650.8735 (-1816.7142)
2022-11-01 16:10:08,216:INFO: Dataset: univ                Batch: 24/29	Loss -1661.1567 (-1810.1140)
2022-11-01 16:10:08,897:INFO: Dataset: univ                Batch: 25/29	Loss -1202.3082 (-1783.8305)
2022-11-01 16:10:09,625:INFO: Dataset: univ                Batch: 26/29	Loss -1836.7322 (-1785.6981)
2022-11-01 16:10:10,315:INFO: Dataset: univ                Batch: 27/29	Loss -1326.9995 (-1768.2125)
2022-11-01 16:10:10,983:INFO: Dataset: univ                Batch: 28/29	Loss -1021.8673 (-1738.3486)
2022-11-01 16:10:11,495:INFO: Dataset: univ                Batch: 29/29	Loss -563.3538 (-1719.7509)
2022-11-01 16:10:21,024:INFO: Dataset: zara1               Batch:  1/16	Loss -1619.9878 (-1619.9878)
2022-11-01 16:10:22,016:INFO: Dataset: zara1               Batch:  2/16	Loss -3043.0627 (-2340.0297)
2022-11-01 16:10:22,911:INFO: Dataset: zara1               Batch:  3/16	Loss -2512.0798 (-2392.7854)
2022-11-01 16:10:23,845:INFO: Dataset: zara1               Batch:  4/16	Loss -1748.4775 (-2246.8524)
2022-11-01 16:10:24,672:INFO: Dataset: zara1               Batch:  5/16	Loss -1185.5947 (-2020.3319)
2022-11-01 16:10:25,503:INFO: Dataset: zara1               Batch:  6/16	Loss -1367.1621 (-1926.3506)
2022-11-01 16:10:26,329:INFO: Dataset: zara1               Batch:  7/16	Loss -1025.8330 (-1793.7591)
2022-11-01 16:10:27,166:INFO: Dataset: zara1               Batch:  8/16	Loss -2140.8887 (-1838.6337)
2022-11-01 16:10:27,982:INFO: Dataset: zara1               Batch:  9/16	Loss -1731.3689 (-1826.3546)
2022-11-01 16:10:28,783:INFO: Dataset: zara1               Batch: 10/16	Loss -799.8631 (-1698.0432)
2022-11-01 16:10:29,818:INFO: Dataset: zara1               Batch: 11/16	Loss -6532.9102 (-2180.8093)
2022-11-01 16:10:30,573:INFO: Dataset: zara1               Batch: 12/16	Loss -1887.4578 (-2156.7312)
2022-11-01 16:10:31,292:INFO: Dataset: zara1               Batch: 13/16	Loss -1197.7246 (-2086.7973)
2022-11-01 16:10:32,083:INFO: Dataset: zara1               Batch: 14/16	Loss -800.3853 (-1984.6654)
2022-11-01 16:10:32,906:INFO: Dataset: zara1               Batch: 15/16	Loss -1500.3628 (-1954.9438)
2022-11-01 16:10:33,642:INFO: Dataset: zara1               Batch: 16/16	Loss -1479.2975 (-1936.1683)
2022-11-01 16:11:01,696:INFO: Dataset: zara2               Batch:  1/36	Loss -1140.9625 (-1140.9625)
2022-11-01 16:11:02,349:INFO: Dataset: zara2               Batch:  2/36	Loss -1084.5616 (-1116.4995)
2022-11-01 16:11:03,011:INFO: Dataset: zara2               Batch:  3/36	Loss -1140.3229 (-1124.7533)
2022-11-01 16:11:03,617:INFO: Dataset: zara2               Batch:  4/36	Loss -1095.5773 (-1117.3095)
2022-11-01 16:11:04,293:INFO: Dataset: zara2               Batch:  5/36	Loss -802.0686 (-1056.4997)
2022-11-01 16:11:05,058:INFO: Dataset: zara2               Batch:  6/36	Loss -620.5266 (-978.1939)
2022-11-01 16:11:05,844:INFO: Dataset: zara2               Batch:  7/36	Loss -1328.1533 (-1023.7123)
2022-11-01 16:11:06,598:INFO: Dataset: zara2               Batch:  8/36	Loss -759.6359 (-993.6676)
2022-11-01 16:11:07,340:INFO: Dataset: zara2               Batch:  9/36	Loss -1642.4828 (-1071.8330)
2022-11-01 16:11:07,974:INFO: Dataset: zara2               Batch: 10/36	Loss -636.6941 (-1030.1322)
2022-11-01 16:11:08,592:INFO: Dataset: zara2               Batch: 11/36	Loss -1791.6217 (-1102.7136)
2022-11-01 16:11:09,242:INFO: Dataset: zara2               Batch: 12/36	Loss -1320.4989 (-1122.0557)
2022-11-01 16:11:09,889:INFO: Dataset: zara2               Batch: 13/36	Loss -1023.1204 (-1114.8121)
2022-11-01 16:11:10,533:INFO: Dataset: zara2               Batch: 14/36	Loss -1621.6621 (-1155.5127)
2022-11-01 16:11:11,183:INFO: Dataset: zara2               Batch: 15/36	Loss -1444.2417 (-1174.0456)
2022-11-01 16:11:11,823:INFO: Dataset: zara2               Batch: 16/36	Loss -1967.1864 (-1216.3621)
2022-11-01 16:11:12,460:INFO: Dataset: zara2               Batch: 17/36	Loss -1073.1859 (-1208.1135)
2022-11-01 16:11:13,203:INFO: Dataset: zara2               Batch: 18/36	Loss -749.2100 (-1185.4196)
2022-11-01 16:11:13,936:INFO: Dataset: zara2               Batch: 19/36	Loss -1539.7653 (-1205.6046)
2022-11-01 16:11:14,674:INFO: Dataset: zara2               Batch: 20/36	Loss -1208.0841 (-1205.7235)
2022-11-01 16:11:15,468:INFO: Dataset: zara2               Batch: 21/36	Loss -420.1378 (-1174.0485)
2022-11-01 16:11:16,148:INFO: Dataset: zara2               Batch: 22/36	Loss -1152.0403 (-1173.0741)
2022-11-01 16:11:16,843:INFO: Dataset: zara2               Batch: 23/36	Loss -1151.7260 (-1172.0891)
2022-11-01 16:11:17,522:INFO: Dataset: zara2               Batch: 24/36	Loss -1410.7367 (-1182.5581)
2022-11-01 16:11:18,196:INFO: Dataset: zara2               Batch: 25/36	Loss -1181.2626 (-1182.5052)
2022-11-01 16:11:18,835:INFO: Dataset: zara2               Batch: 26/36	Loss -1379.3407 (-1190.3660)
2022-11-01 16:11:19,483:INFO: Dataset: zara2               Batch: 27/36	Loss -8928.4521 (-1474.3638)
2022-11-01 16:11:20,134:INFO: Dataset: zara2               Batch: 28/36	Loss -912.4787 (-1452.1700)
2022-11-01 16:11:20,806:INFO: Dataset: zara2               Batch: 29/36	Loss -529.5839 (-1422.5866)
2022-11-01 16:11:21,449:INFO: Dataset: zara2               Batch: 30/36	Loss -1140.0702 (-1412.7770)
2022-11-01 16:11:22,087:INFO: Dataset: zara2               Batch: 31/36	Loss -575.0358 (-1385.4418)
2022-11-01 16:11:22,738:INFO: Dataset: zara2               Batch: 32/36	Loss -1075.1204 (-1373.7489)
2022-11-01 16:11:23,412:INFO: Dataset: zara2               Batch: 33/36	Loss -2308.5989 (-1403.6681)
2022-11-01 16:11:24,076:INFO: Dataset: zara2               Batch: 34/36	Loss 1006.8157 (-1336.2215)
2022-11-01 16:11:24,699:INFO: Dataset: zara2               Batch: 35/36	Loss -2384.7617 (-1366.3113)
2022-11-01 16:11:25,336:INFO: Dataset: zara2               Batch: 36/36	Loss -342.7151 (-1345.1591)
2022-11-01 16:11:26,398:INFO: - Computing ADE (validation o)
2022-11-01 16:11:37,425:INFO: 		 ADE on eth                       dataset:	 2.8601372241973877
2022-11-01 16:11:37,425:INFO: Average validation o:	ADE  2.8601	FDE  4.8349
2022-11-01 16:11:37,427:INFO: - Computing ADE (validation)
2022-11-01 16:11:47,532:INFO: 		 ADE on hotel                     dataset:	 1.0227514505386353
2022-11-01 16:11:57,524:INFO: 		 ADE on univ                      dataset:	 1.567340612411499
2022-11-01 16:12:08,430:INFO: 		 ADE on zara1                     dataset:	 2.640019416809082
2022-11-01 16:12:18,671:INFO: 		 ADE on zara2                     dataset:	 1.4899559020996094
2022-11-01 16:12:18,672:INFO: Average validation:	ADE  1.5715	FDE  2.8502
2022-11-01 16:12:18,673:INFO: - Computing ADE (training)
2022-11-01 16:12:28,872:INFO: 		 ADE on hotel                     dataset:	 1.385929822921753
2022-11-01 16:12:57,758:INFO: 		 ADE on univ                      dataset:	 1.4379314184188843
2022-11-01 16:13:09,013:INFO: 		 ADE on zara1                     dataset:	 2.494180917739868
2022-11-01 16:13:39,564:INFO: 		 ADE on zara2                     dataset:	 1.7362725734710693
2022-11-01 16:13:39,564:INFO: Average training:	ADE  1.5645	FDE  2.8390
2022-11-01 16:13:39,585:INFO:  --> Model Saved in ./models/eth/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(200, 150, 300, 300)_shuffle_true_seed_72/pretrain/P4/CRMF_epoch_654.pth.tar
2022-11-01 16:13:39,585:INFO: 
===> EPOCH: 655 (P4)
2022-11-01 16:13:39,585:INFO: - Computing loss (training)
2022-11-01 16:13:47,671:INFO: Dataset: hotel               Batch: 1/8	Loss -2278.0840 (-2278.0840)
2022-11-01 16:13:48,570:INFO: Dataset: hotel               Batch: 2/8	Loss -1798.0217 (-2023.8044)
2022-11-01 16:13:49,243:INFO: Dataset: hotel               Batch: 3/8	Loss -2098.7148 (-2046.6343)
2022-11-01 16:13:49,898:INFO: Dataset: hotel               Batch: 4/8	Loss 242.4483 (-1503.4621)
2022-11-01 16:13:50,561:INFO: Dataset: hotel               Batch: 5/8	Loss -31979.1797 (-8095.9323)
2022-11-01 16:13:51,181:INFO: Dataset: hotel               Batch: 6/8	Loss -1412.0142 (-6967.8894)
2022-11-01 16:13:51,790:INFO: Dataset: hotel               Batch: 7/8	Loss 23.8449 (-6031.8375)
2022-11-01 16:13:52,287:INFO: Dataset: hotel               Batch: 8/8	Loss -225.0121 (-5832.6588)
2022-11-01 16:14:19,118:INFO: Dataset: univ                Batch:  1/29	Loss -1885.3276 (-1885.3276)
2022-11-01 16:14:19,748:INFO: Dataset: univ                Batch:  2/29	Loss -3866.4155 (-2968.2466)
2022-11-01 16:14:20,370:INFO: Dataset: univ                Batch:  3/29	Loss -4251.0020 (-3441.8793)
2022-11-01 16:14:20,968:INFO: Dataset: univ                Batch:  4/29	Loss -2209.7192 (-3146.7715)
2022-11-01 16:14:21,586:INFO: Dataset: univ                Batch:  5/29	Loss -2126.6260 (-2944.8823)
2022-11-01 16:14:22,237:INFO: Dataset: univ                Batch:  6/29	Loss -307.3979 (-2447.9907)
2022-11-01 16:14:22,890:INFO: Dataset: univ                Batch:  7/29	Loss -1335.9497 (-2284.2116)
2022-11-01 16:14:23,495:INFO: Dataset: univ                Batch:  8/29	Loss -185.6646 (-2008.7468)
2022-11-01 16:14:24,118:INFO: Dataset: univ                Batch:  9/29	Loss -1798.8135 (-1983.4666)
2022-11-01 16:14:24,771:INFO: Dataset: univ                Batch: 10/29	Loss -1131.4297 (-1908.3902)
2022-11-01 16:14:25,401:INFO: Dataset: univ                Batch: 11/29	Loss -449.9494 (-1765.8214)
2022-11-01 16:14:26,046:INFO: Dataset: univ                Batch: 12/29	Loss -1508.3508 (-1743.6919)
2022-11-01 16:14:26,685:INFO: Dataset: univ                Batch: 13/29	Loss -490.3109 (-1635.6917)
2022-11-01 16:14:27,306:INFO: Dataset: univ                Batch: 14/29	Loss -855.5875 (-1586.4266)
2022-11-01 16:14:27,929:INFO: Dataset: univ                Batch: 15/29	Loss -3134.4175 (-1682.7412)
2022-11-01 16:14:28,559:INFO: Dataset: univ                Batch: 16/29	Loss -900.8825 (-1633.3586)
2022-11-01 16:14:29,231:INFO: Dataset: univ                Batch: 17/29	Loss -2879.3291 (-1696.9116)
2022-11-01 16:14:29,859:INFO: Dataset: univ                Batch: 18/29	Loss -2154.4712 (-1723.3599)
2022-11-01 16:14:30,478:INFO: Dataset: univ                Batch: 19/29	Loss -642.4142 (-1669.3088)
2022-11-01 16:14:31,098:INFO: Dataset: univ                Batch: 20/29	Loss -1933.6807 (-1680.7331)
2022-11-01 16:14:31,691:INFO: Dataset: univ                Batch: 21/29	Loss 2877.8760 (-1498.6589)
2022-11-01 16:14:32,322:INFO: Dataset: univ                Batch: 22/29	Loss -2982.8384 (-1571.0534)
2022-11-01 16:14:32,931:INFO: Dataset: univ                Batch: 23/29	Loss -1859.4812 (-1583.0466)
2022-11-01 16:14:33,586:INFO: Dataset: univ                Batch: 24/29	Loss -1696.1111 (-1588.1386)
2022-11-01 16:14:34,203:INFO: Dataset: univ                Batch: 25/29	Loss -4922.8823 (-1726.0179)
2022-11-01 16:14:34,856:INFO: Dataset: univ                Batch: 26/29	Loss -662.1413 (-1680.6744)
2022-11-01 16:14:35,468:INFO: Dataset: univ                Batch: 27/29	Loss -2596.9795 (-1714.6032)
2022-11-01 16:14:36,090:INFO: Dataset: univ                Batch: 28/29	Loss -1793.9369 (-1717.2893)
2022-11-01 16:14:36,610:INFO: Dataset: univ                Batch: 29/29	Loss -2006.6862 (-1720.3201)
2022-11-01 16:14:45,829:INFO: Dataset: zara1               Batch:  1/16	Loss -1991.1082 (-1991.1082)
2022-11-01 16:14:46,672:INFO: Dataset: zara1               Batch:  2/16	Loss -3976.5713 (-3095.9893)
2022-11-01 16:14:47,347:INFO: Dataset: zara1               Batch:  3/16	Loss -2017.6328 (-2710.4479)
2022-11-01 16:14:47,992:INFO: Dataset: zara1               Batch:  4/16	Loss -2834.4500 (-2742.5600)
2022-11-01 16:14:48,608:INFO: Dataset: zara1               Batch:  5/16	Loss -1743.9001 (-2539.6577)
2022-11-01 16:14:49,243:INFO: Dataset: zara1               Batch:  6/16	Loss -1382.2974 (-2346.7643)
2022-11-01 16:14:49,910:INFO: Dataset: zara1               Batch:  7/16	Loss -2434.2473 (-2359.0914)
2022-11-01 16:14:50,539:INFO: Dataset: zara1               Batch:  8/16	Loss -1633.1387 (-2270.0663)
2022-11-01 16:14:51,164:INFO: Dataset: zara1               Batch:  9/16	Loss -1336.8339 (-2183.1559)
2022-11-01 16:14:51,770:INFO: Dataset: zara1               Batch: 10/16	Loss -3364.3860 (-2297.0326)
2022-11-01 16:14:52,435:INFO: Dataset: zara1               Batch: 11/16	Loss -1531.1075 (-2233.8754)
2022-11-01 16:14:53,085:INFO: Dataset: zara1               Batch: 12/16	Loss -2395.5969 (-2248.8374)
2022-11-01 16:14:53,725:INFO: Dataset: zara1               Batch: 13/16	Loss -1375.2783 (-2189.5661)
2022-11-01 16:14:54,330:INFO: Dataset: zara1               Batch: 14/16	Loss -2558.3958 (-2216.8547)
2022-11-01 16:14:54,966:INFO: Dataset: zara1               Batch: 15/16	Loss -1374.7646 (-2165.7627)
2022-11-01 16:14:55,616:INFO: Dataset: zara1               Batch: 16/16	Loss -1232.6069 (-2123.0340)
2022-11-01 16:15:22,245:INFO: Dataset: zara2               Batch:  1/36	Loss -1582.8544 (-1582.8544)
2022-11-01 16:15:22,983:INFO: Dataset: zara2               Batch:  2/36	Loss -1736.7523 (-1653.7170)
2022-11-01 16:15:23,667:INFO: Dataset: zara2               Batch:  3/36	Loss -845.3459 (-1384.2600)
2022-11-01 16:15:24,412:INFO: Dataset: zara2               Batch:  4/36	Loss -1449.7725 (-1399.2098)
2022-11-01 16:15:25,260:INFO: Dataset: zara2               Batch:  5/36	Loss -1891.0728 (-1486.2860)
2022-11-01 16:15:26,020:INFO: Dataset: zara2               Batch:  6/36	Loss -1185.1226 (-1431.4754)
2022-11-01 16:15:26,953:INFO: Dataset: zara2               Batch:  7/36	Loss -2387.4331 (-1558.0708)
2022-11-01 16:15:27,748:INFO: Dataset: zara2               Batch:  8/36	Loss -7345.7100 (-2332.5931)
2022-11-01 16:15:28,370:INFO: Dataset: zara2               Batch:  9/36	Loss -1273.9694 (-2208.2512)
2022-11-01 16:15:29,040:INFO: Dataset: zara2               Batch: 10/36	Loss -1355.9492 (-2121.7786)
2022-11-01 16:15:29,845:INFO: Dataset: zara2               Batch: 11/36	Loss -1348.8347 (-2045.7846)
2022-11-01 16:15:30,662:INFO: Dataset: zara2               Batch: 12/36	Loss -1467.0355 (-1997.0215)
2022-11-01 16:15:31,479:INFO: Dataset: zara2               Batch: 13/36	Loss -2132.1704 (-2007.3574)
2022-11-01 16:15:32,273:INFO: Dataset: zara2               Batch: 14/36	Loss -1487.0912 (-1976.4592)
2022-11-01 16:15:33,075:INFO: Dataset: zara2               Batch: 15/36	Loss 773.9855 (-1790.8792)
2022-11-01 16:15:33,865:INFO: Dataset: zara2               Batch: 16/36	Loss -2060.7075 (-1808.5793)
2022-11-01 16:15:34,693:INFO: Dataset: zara2               Batch: 17/36	Loss -8400.2871 (-2180.3263)
2022-11-01 16:15:35,446:INFO: Dataset: zara2               Batch: 18/36	Loss -1917.8507 (-2166.7190)
2022-11-01 16:15:36,294:INFO: Dataset: zara2               Batch: 19/36	Loss -2121.2202 (-2164.4899)
2022-11-01 16:15:37,057:INFO: Dataset: zara2               Batch: 20/36	Loss -297.6453 (-2056.5041)
2022-11-01 16:15:37,854:INFO: Dataset: zara2               Batch: 21/36	Loss -358.9418 (-1976.1853)
2022-11-01 16:15:38,730:INFO: Dataset: zara2               Batch: 22/36	Loss -1106.0948 (-1939.9799)
2022-11-01 16:15:39,553:INFO: Dataset: zara2               Batch: 23/36	Loss -2293.6165 (-1955.6650)
2022-11-01 16:15:40,373:INFO: Dataset: zara2               Batch: 24/36	Loss -2013.5001 (-1958.0265)
2022-11-01 16:15:41,167:INFO: Dataset: zara2               Batch: 25/36	Loss -1679.1774 (-1947.0874)
2022-11-01 16:15:41,923:INFO: Dataset: zara2               Batch: 26/36	Loss -980.2684 (-1909.5410)
2022-11-01 16:15:42,704:INFO: Dataset: zara2               Batch: 27/36	Loss -1383.4125 (-1890.0929)
2022-11-01 16:15:43,567:INFO: Dataset: zara2               Batch: 28/36	Loss -2749.8477 (-1920.3926)
2022-11-01 16:15:44,348:INFO: Dataset: zara2               Batch: 29/36	Loss -1169.7306 (-1890.7473)
2022-11-01 16:15:45,123:INFO: Dataset: zara2               Batch: 30/36	Loss -1035.5565 (-1862.9078)
2022-11-01 16:15:45,891:INFO: Dataset: zara2               Batch: 31/36	Loss -782.4037 (-1836.0030)
2022-11-01 16:15:46,653:INFO: Dataset: zara2               Batch: 32/36	Loss -2614.8486 (-1856.7530)
2022-11-01 16:15:47,412:INFO: Dataset: zara2               Batch: 33/36	Loss -1794.1069 (-1854.9518)
2022-11-01 16:15:48,207:INFO: Dataset: zara2               Batch: 34/36	Loss -1383.0302 (-1839.9272)
2022-11-01 16:15:49,064:INFO: Dataset: zara2               Batch: 35/36	Loss -1333.9059 (-1824.9764)
2022-11-01 16:15:50,436:INFO: Dataset: zara2               Batch: 36/36	Loss -577.8839 (-1798.9996)
2022-11-01 16:15:51,491:INFO: - Computing ADE (validation o)

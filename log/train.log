2022-11-24 21:18:09,460:INFO: Initializing Training Set
2022-11-24 21:18:11,016:INFO: Initializing Validation Set
2022-11-24 21:18:11,205:INFO: Initializing Validation O Set
2022-11-24 21:18:13,519:INFO: => loaded checkpoint './models/E1//P2/CRMF_epoch_250.pth.tar' (epoch 251)
2022-11-24 21:18:13,519:INFO: 
===> EPOCH: 251 (P3)
2022-11-24 21:18:13,520:INFO: - Computing loss (training)
2022-11-24 21:18:13,871:INFO: Dataset: hotel               Batch: 1/4	Loss 105.7204 (105.7204)
2022-11-24 21:18:14,076:INFO: Dataset: hotel               Batch: 2/4	Loss 106.7267 (106.2150)
2022-11-24 21:18:14,272:INFO: Dataset: hotel               Batch: 3/4	Loss 106.0929 (106.1730)
2022-11-24 21:18:14,401:INFO: Dataset: hotel               Batch: 4/4	Loss 66.2559 (99.4324)
2022-11-24 21:18:14,875:INFO: Dataset: univ                Batch:  1/15	Loss 103.7398 (103.7398)
2022-11-24 21:18:15,088:INFO: Dataset: univ                Batch:  2/15	Loss 103.8097 (103.7747)
2022-11-24 21:18:15,295:INFO: Dataset: univ                Batch:  3/15	Loss 107.0865 (104.8408)
2022-11-24 21:18:15,509:INFO: Dataset: univ                Batch:  4/15	Loss 104.3357 (104.7150)
2022-11-24 21:18:15,730:INFO: Dataset: univ                Batch:  5/15	Loss 105.7101 (104.9074)
2022-11-24 21:18:15,958:INFO: Dataset: univ                Batch:  6/15	Loss 101.1219 (104.2353)
2022-11-24 21:18:16,192:INFO: Dataset: univ                Batch:  7/15	Loss 102.5618 (103.9900)
2022-11-24 21:18:16,416:INFO: Dataset: univ                Batch:  8/15	Loss 104.1356 (104.0080)
2022-11-24 21:18:16,640:INFO: Dataset: univ                Batch:  9/15	Loss 102.7715 (103.8583)
2022-11-24 21:18:16,864:INFO: Dataset: univ                Batch: 10/15	Loss 103.0899 (103.7799)
2022-11-24 21:18:17,095:INFO: Dataset: univ                Batch: 11/15	Loss 103.1586 (103.7215)
2022-11-24 21:18:17,322:INFO: Dataset: univ                Batch: 12/15	Loss 103.4281 (103.6960)
2022-11-24 21:18:17,552:INFO: Dataset: univ                Batch: 13/15	Loss 104.2075 (103.7363)
2022-11-24 21:18:17,780:INFO: Dataset: univ                Batch: 14/15	Loss 106.3729 (103.9008)
2022-11-24 21:18:17,863:INFO: Dataset: univ                Batch: 15/15	Loss 18.9968 (102.8426)
2022-11-24 21:18:18,280:INFO: Dataset: zara1               Batch: 1/8	Loss 112.6414 (112.6414)
2022-11-24 21:18:18,474:INFO: Dataset: zara1               Batch: 2/8	Loss 114.3268 (113.4540)
2022-11-24 21:18:18,663:INFO: Dataset: zara1               Batch: 3/8	Loss 110.2913 (112.2841)
2022-11-24 21:18:18,853:INFO: Dataset: zara1               Batch: 4/8	Loss 114.8280 (112.8894)
2022-11-24 21:18:19,041:INFO: Dataset: zara1               Batch: 5/8	Loss 111.3316 (112.5623)
2022-11-24 21:18:19,231:INFO: Dataset: zara1               Batch: 6/8	Loss 113.0168 (112.6409)
2022-11-24 21:18:19,421:INFO: Dataset: zara1               Batch: 7/8	Loss 112.0502 (112.5523)
2022-11-24 21:18:19,585:INFO: Dataset: zara1               Batch: 8/8	Loss 98.7270 (110.9515)
2022-11-24 21:18:19,998:INFO: Dataset: zara2               Batch:  1/18	Loss 101.3481 (101.3481)
2022-11-24 21:18:20,186:INFO: Dataset: zara2               Batch:  2/18	Loss 100.9313 (101.1323)
2022-11-24 21:18:20,374:INFO: Dataset: zara2               Batch:  3/18	Loss 101.7806 (101.3582)
2022-11-24 21:18:20,564:INFO: Dataset: zara2               Batch:  4/18	Loss 100.8330 (101.2222)
2022-11-24 21:18:20,759:INFO: Dataset: zara2               Batch:  5/18	Loss 100.5396 (101.0853)
2022-11-24 21:18:20,948:INFO: Dataset: zara2               Batch:  6/18	Loss 102.7068 (101.3608)
2022-11-24 21:18:21,138:INFO: Dataset: zara2               Batch:  7/18	Loss 101.0536 (101.3169)
2022-11-24 21:18:21,325:INFO: Dataset: zara2               Batch:  8/18	Loss 102.2148 (101.4348)
2022-11-24 21:18:21,514:INFO: Dataset: zara2               Batch:  9/18	Loss 101.9332 (101.4918)
2022-11-24 21:18:21,708:INFO: Dataset: zara2               Batch: 10/18	Loss 101.7941 (101.5209)
2022-11-24 21:18:22,000:INFO: Dataset: zara2               Batch: 11/18	Loss 103.3816 (101.7068)
2022-11-24 21:18:22,196:INFO: Dataset: zara2               Batch: 12/18	Loss 100.1823 (101.5878)
2022-11-24 21:18:22,397:INFO: Dataset: zara2               Batch: 13/18	Loss 100.7049 (101.5135)
2022-11-24 21:18:22,594:INFO: Dataset: zara2               Batch: 14/18	Loss 100.4074 (101.4436)
2022-11-24 21:18:22,789:INFO: Dataset: zara2               Batch: 15/18	Loss 99.2717 (101.2972)
2022-11-24 21:18:22,986:INFO: Dataset: zara2               Batch: 16/18	Loss 101.6409 (101.3185)
2022-11-24 21:18:23,186:INFO: Dataset: zara2               Batch: 17/18	Loss 103.6525 (101.4477)
2022-11-24 21:18:23,363:INFO: Dataset: zara2               Batch: 18/18	Loss 87.2592 (100.6901)
2022-11-24 21:18:23,409:INFO: - Computing ADE (validation)
2022-11-24 21:18:23,680:INFO: 		 ADE on hotel                     dataset:	 1.1299011707305908
2022-11-24 21:18:24,047:INFO: 		 ADE on univ                      dataset:	 1.574519395828247
2022-11-24 21:18:24,318:INFO: 		 ADE on zara1                     dataset:	 2.5617010593414307
2022-11-24 21:18:24,798:INFO: 		 ADE on zara2                     dataset:	 1.5461816787719727
2022-11-24 21:18:24,799:INFO: Average validation:	ADE  1.5972	FDE  2.8051
2022-11-24 21:18:24,799:INFO: - Computing ADE (training)
2022-11-24 21:18:25,202:INFO: 		 ADE on hotel                     dataset:	 1.4429409503936768
2022-11-24 21:18:26,493:INFO: 		 ADE on univ                      dataset:	 1.4703242778778076
2022-11-24 21:18:27,138:INFO: 		 ADE on zara1                     dataset:	 2.5119733810424805
2022-11-24 21:18:28,464:INFO: 		 ADE on zara2                     dataset:	 1.8111460208892822
2022-11-24 21:18:28,464:INFO: Average training:	ADE  1.6052	FDE  2.8311
2022-11-24 21:18:28,465:INFO: - Computing ADE (validation o)
2022-11-24 21:18:28,715:INFO: 		 ADE on eth                       dataset:	 3.8447206020355225
2022-11-24 21:18:28,715:INFO: Average validation o:	ADE  3.8447	FDE  6.5905
2022-11-24 21:18:28,727:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_251.pth.tar
2022-11-24 21:18:28,727:INFO: 
===> EPOCH: 252 (P3)
2022-11-24 21:18:28,727:INFO: - Computing loss (training)
2022-11-24 21:18:29,117:INFO: Dataset: hotel               Batch: 1/4	Loss 104.1568 (104.1568)
2022-11-24 21:18:29,310:INFO: Dataset: hotel               Batch: 2/4	Loss 105.2187 (104.6928)
2022-11-24 21:18:29,505:INFO: Dataset: hotel               Batch: 3/4	Loss 104.7446 (104.7099)
2022-11-24 21:18:29,634:INFO: Dataset: hotel               Batch: 4/4	Loss 64.7006 (97.9537)
2022-11-24 21:18:30,083:INFO: Dataset: univ                Batch:  1/15	Loss 100.5547 (100.5547)
2022-11-24 21:18:30,287:INFO: Dataset: univ                Batch:  2/15	Loss 101.1097 (100.8329)
2022-11-24 21:18:30,511:INFO: Dataset: univ                Batch:  3/15	Loss 100.9851 (100.8846)
2022-11-24 21:18:30,719:INFO: Dataset: univ                Batch:  4/15	Loss 100.8295 (100.8714)
2022-11-24 21:18:30,924:INFO: Dataset: univ                Batch:  5/15	Loss 105.9937 (101.7918)
2022-11-24 21:18:31,131:INFO: Dataset: univ                Batch:  6/15	Loss 100.8124 (101.6306)
2022-11-24 21:18:31,335:INFO: Dataset: univ                Batch:  7/15	Loss 101.7233 (101.6457)
2022-11-24 21:18:31,539:INFO: Dataset: univ                Batch:  8/15	Loss 103.4158 (101.8438)
2022-11-24 21:18:31,747:INFO: Dataset: univ                Batch:  9/15	Loss 104.5312 (102.1418)
2022-11-24 21:18:31,954:INFO: Dataset: univ                Batch: 10/15	Loss 102.0053 (102.1280)
2022-11-24 21:18:32,164:INFO: Dataset: univ                Batch: 11/15	Loss 100.3597 (101.9550)
2022-11-24 21:18:32,369:INFO: Dataset: univ                Batch: 12/15	Loss 101.6895 (101.9338)
2022-11-24 21:18:32,571:INFO: Dataset: univ                Batch: 13/15	Loss 101.9767 (101.9370)
2022-11-24 21:18:32,776:INFO: Dataset: univ                Batch: 14/15	Loss 100.5918 (101.8327)

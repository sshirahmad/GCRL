2022-10-27 22:53:43,598:INFO: Initializing Training Set
2022-10-27 22:53:46,946:INFO: Initializing Validation Set
2022-10-27 22:53:47,345:INFO: Initializing Validation O Set
2022-10-27 22:53:49,928:INFO: => loaded checkpoint './models/eth/pretrain/P1/5.0/SSE_data_eth_irm[5.0]_epoch_99.pth.tar' (epoch 100)
2022-10-27 22:53:49,928:INFO: 
===> EPOCH: 100 (P1)
2022-10-27 22:53:49,929:INFO: - Computing loss (training)
2022-10-27 22:53:55,652:INFO: Dataset: hotel               Batch: 1/8	Loss 2.7333 (2.7333)
2022-10-27 22:53:56,293:INFO: Dataset: hotel               Batch: 2/8	Loss 3.1416 (2.9413)
2022-10-27 22:53:56,799:INFO: Dataset: hotel               Batch: 3/8	Loss 3.4140 (3.0886)
2022-10-27 22:53:57,302:INFO: Dataset: hotel               Batch: 4/8	Loss 3.5528 (3.2116)
2022-10-27 22:53:57,796:INFO: Dataset: hotel               Batch: 5/8	Loss 3.4658 (3.2690)
2022-10-27 22:53:58,316:INFO: Dataset: hotel               Batch: 6/8	Loss 2.9515 (3.2186)
2022-10-27 22:53:58,803:INFO: Dataset: hotel               Batch: 7/8	Loss 3.0183 (3.1915)
2022-10-27 22:53:59,022:INFO: Dataset: hotel               Batch: 8/8	Loss 0.8264 (3.1259)
2022-10-27 22:54:18,007:INFO: Dataset: univ                Batch:  1/29	Loss 2.2922 (2.2922)
2022-10-27 22:54:18,554:INFO: Dataset: univ                Batch:  2/29	Loss 1.8195 (2.0360)
2022-10-27 22:54:19,234:INFO: Dataset: univ                Batch:  3/29	Loss 2.0724 (2.0495)
2022-10-27 22:54:20,301:INFO: Dataset: univ                Batch:  4/29	Loss 2.1137 (2.0660)
2022-10-27 22:54:20,958:INFO: Dataset: univ                Batch:  5/29	Loss 2.5334 (2.1568)
2022-10-27 22:54:21,601:INFO: Dataset: univ                Batch:  6/29	Loss 1.9416 (2.1247)
2022-10-27 22:54:22,222:INFO: Dataset: univ                Batch:  7/29	Loss 2.1802 (2.1323)
2022-10-27 22:54:22,862:INFO: Dataset: univ                Batch:  8/29	Loss 2.8302 (2.2057)
2022-10-27 22:54:23,548:INFO: Dataset: univ                Batch:  9/29	Loss 1.8438 (2.1609)
2022-10-27 22:54:24,080:INFO: Dataset: univ                Batch: 10/29	Loss 2.1068 (2.1554)
2022-10-27 22:54:24,605:INFO: Dataset: univ                Batch: 11/29	Loss 2.2049 (2.1596)
2022-10-27 22:54:25,121:INFO: Dataset: univ                Batch: 12/29	Loss 4.4043 (2.3276)
2022-10-27 22:54:25,632:INFO: Dataset: univ                Batch: 13/29	Loss 2.1719 (2.3157)
2022-10-27 22:54:26,247:INFO: Dataset: univ                Batch: 14/29	Loss 1.8983 (2.2874)
2022-10-27 22:54:26,781:INFO: Dataset: univ                Batch: 15/29	Loss 2.0358 (2.2702)
2022-10-27 22:54:27,299:INFO: Dataset: univ                Batch: 16/29	Loss 1.7940 (2.2373)
2022-10-27 22:54:27,845:INFO: Dataset: univ                Batch: 17/29	Loss 2.3466 (2.2431)
2022-10-27 22:54:28,380:INFO: Dataset: univ                Batch: 18/29	Loss 3.1424 (2.2840)
2022-10-27 22:54:28,907:INFO: Dataset: univ                Batch: 19/29	Loss 2.2449 (2.2819)
2022-10-27 22:54:29,418:INFO: Dataset: univ                Batch: 20/29	Loss 1.9055 (2.2619)
2022-10-27 22:54:29,928:INFO: Dataset: univ                Batch: 21/29	Loss 2.5502 (2.2763)
2022-10-27 22:54:30,436:INFO: Dataset: univ                Batch: 22/29	Loss 1.6978 (2.2512)
2022-10-27 22:54:30,958:INFO: Dataset: univ                Batch: 23/29	Loss 2.8690 (2.2802)
2022-10-27 22:54:31,479:INFO: Dataset: univ                Batch: 24/29	Loss 2.4412 (2.2858)
2022-10-27 22:54:32,011:INFO: Dataset: univ                Batch: 25/29	Loss 2.2228 (2.2835)
2022-10-27 22:54:32,548:INFO: Dataset: univ                Batch: 26/29	Loss 1.6141 (2.2589)
2022-10-27 22:54:33,074:INFO: Dataset: univ                Batch: 27/29	Loss 2.6873 (2.2751)
2022-10-27 22:54:33,576:INFO: Dataset: univ                Batch: 28/29	Loss 1.9497 (2.2633)
2022-10-27 22:54:33,847:INFO: Dataset: univ                Batch: 29/29	Loss 0.7527 (2.2407)
2022-10-27 22:54:40,074:INFO: Dataset: zara1               Batch:  1/16	Loss 4.3435 (4.3435)
2022-10-27 22:54:40,855:INFO: Dataset: zara1               Batch:  2/16	Loss 5.0780 (4.6703)
2022-10-27 22:54:41,498:INFO: Dataset: zara1               Batch:  3/16	Loss 2.0158 (3.7227)
2022-10-27 22:54:42,046:INFO: Dataset: zara1               Batch:  4/16	Loss 3.5793 (3.6888)
2022-10-27 22:54:42,566:INFO: Dataset: zara1               Batch:  5/16	Loss 1.9526 (3.3590)
2022-10-27 22:54:43,106:INFO: Dataset: zara1               Batch:  6/16	Loss 1.5422 (3.0384)
2022-10-27 22:54:43,604:INFO: Dataset: zara1               Batch:  7/16	Loss 2.0356 (2.9202)
2022-10-27 22:54:44,149:INFO: Dataset: zara1               Batch:  8/16	Loss 1.6884 (2.7630)
2022-10-27 22:54:44,684:INFO: Dataset: zara1               Batch:  9/16	Loss 1.7096 (2.6533)
2022-10-27 22:54:45,186:INFO: Dataset: zara1               Batch: 10/16	Loss 2.7235 (2.6603)
2022-10-27 22:54:45,767:INFO: Dataset: zara1               Batch: 11/16	Loss 1.6998 (2.5745)
2022-10-27 22:54:46,387:INFO: Dataset: zara1               Batch: 12/16	Loss 1.7680 (2.5038)
2022-10-27 22:54:47,025:INFO: Dataset: zara1               Batch: 13/16	Loss 2.0044 (2.4695)
2022-10-27 22:54:47,839:INFO: Dataset: zara1               Batch: 14/16	Loss 1.8047 (2.4223)
2022-10-27 22:54:48,514:INFO: Dataset: zara1               Batch: 15/16	Loss 1.3052 (2.3441)
2022-10-27 22:54:49,018:INFO: Dataset: zara1               Batch: 16/16	Loss 1.3466 (2.2921)
2022-10-27 22:55:07,461:INFO: Dataset: zara2               Batch:  1/36	Loss 1.3449 (1.3449)
2022-10-27 22:55:07,945:INFO: Dataset: zara2               Batch:  2/36	Loss 1.4684 (1.3974)
2022-10-27 22:55:08,421:INFO: Dataset: zara2               Batch:  3/36	Loss 1.1881 (1.3298)
2022-10-27 22:55:08,895:INFO: Dataset: zara2               Batch:  4/36	Loss 2.6311 (1.6589)
2022-10-27 22:55:09,378:INFO: Dataset: zara2               Batch:  5/36	Loss 1.1846 (1.5610)
2022-10-27 22:55:09,877:INFO: Dataset: zara2               Batch:  6/36	Loss 1.4963 (1.5513)
2022-10-27 22:55:10,397:INFO: Dataset: zara2               Batch:  7/36	Loss 2.0298 (1.6220)
2022-10-27 22:55:10,896:INFO: Dataset: zara2               Batch:  8/36	Loss 2.4648 (1.7454)
2022-10-27 22:55:11,400:INFO: Dataset: zara2               Batch:  9/36	Loss 2.0578 (1.7754)
2022-10-27 22:55:11,921:INFO: Dataset: zara2               Batch: 10/36	Loss 1.7110 (1.7691)
2022-10-27 22:55:12,452:INFO: Dataset: zara2               Batch: 11/36	Loss 1.3666 (1.7300)
2022-10-27 22:55:12,960:INFO: Dataset: zara2               Batch: 12/36	Loss 1.3022 (1.6926)
2022-10-27 22:55:13,446:INFO: Dataset: zara2               Batch: 13/36	Loss 1.7859 (1.7000)
2022-10-27 22:55:13,950:INFO: Dataset: zara2               Batch: 14/36	Loss 1.2221 (1.6672)
2022-10-27 22:55:14,439:INFO: Dataset: zara2               Batch: 15/36	Loss 1.5454 (1.6589)
2022-10-27 22:55:14,961:INFO: Dataset: zara2               Batch: 16/36	Loss 2.2833 (1.6932)
2022-10-27 22:55:15,498:INFO: Dataset: zara2               Batch: 17/36	Loss 1.4922 (1.6817)
2022-10-27 22:55:16,025:INFO: Dataset: zara2               Batch: 18/36	Loss 0.9852 (1.6446)
2022-10-27 22:55:16,545:INFO: Dataset: zara2               Batch: 19/36	Loss 1.2357 (1.6237)
2022-10-27 22:55:17,059:INFO: Dataset: zara2               Batch: 20/36	Loss 1.1062 (1.5993)
2022-10-27 22:55:17,579:INFO: Dataset: zara2               Batch: 21/36	Loss 1.6275 (1.6006)
2022-10-27 22:55:18,079:INFO: Dataset: zara2               Batch: 22/36	Loss 2.4894 (1.6372)
2022-10-27 22:55:18,601:INFO: Dataset: zara2               Batch: 23/36	Loss 0.9679 (1.6086)
2022-10-27 22:55:19,106:INFO: Dataset: zara2               Batch: 24/36	Loss 1.1879 (1.5907)
2022-10-27 22:55:19,603:INFO: Dataset: zara2               Batch: 25/36	Loss 1.5384 (1.5886)
2022-10-27 22:55:20,098:INFO: Dataset: zara2               Batch: 26/36	Loss 1.6135 (1.5896)
2022-10-27 22:55:20,594:INFO: Dataset: zara2               Batch: 27/36	Loss 1.3124 (1.5788)
2022-10-27 22:55:21,101:INFO: Dataset: zara2               Batch: 28/36	Loss 1.2750 (1.5681)
2022-10-27 22:55:21,591:INFO: Dataset: zara2               Batch: 29/36	Loss 1.4069 (1.5634)
2022-10-27 22:55:22,080:INFO: Dataset: zara2               Batch: 30/36	Loss 1.1954 (1.5512)
2022-10-27 22:55:22,566:INFO: Dataset: zara2               Batch: 31/36	Loss 1.1117 (1.5372)
2022-10-27 22:55:23,057:INFO: Dataset: zara2               Batch: 32/36	Loss 1.0188 (1.5197)
2022-10-27 22:55:23,560:INFO: Dataset: zara2               Batch: 33/36	Loss 1.5432 (1.5205)
2022-10-27 22:55:24,068:INFO: Dataset: zara2               Batch: 34/36	Loss 1.1303 (1.5087)
2022-10-27 22:55:24,582:INFO: Dataset: zara2               Batch: 35/36	Loss 1.4301 (1.5062)
2022-10-27 22:55:24,967:INFO: Dataset: zara2               Batch: 36/36	Loss 1.1681 (1.5000)
2022-10-27 22:55:25,687:INFO: - Computing ADE (validation o)
2022-10-27 22:55:32,229:INFO: 		 ADE on eth                       dataset:	 1.6955641508102417
2022-10-27 22:55:32,230:INFO: Average validation o:	ADE  1.6956	FDE  2.8157
2022-10-27 22:55:32,231:INFO: - Computing ADE (validation)
2022-10-27 22:55:38,745:INFO: 		 ADE on hotel                     dataset:	 1.1259145736694336
2022-10-27 22:55:45,406:INFO: 		 ADE on univ                      dataset:	 1.2873071432113647
2022-10-27 22:55:51,977:INFO: 		 ADE on zara1                     dataset:	 1.0838764905929565
2022-10-27 22:55:58,829:INFO: 		 ADE on zara2                     dataset:	 0.9299728870391846
2022-10-27 22:55:58,829:INFO: Average validation:	ADE  1.1356	FDE  2.0931
2022-10-27 22:55:58,831:INFO: - Computing ADE (training)
2022-10-27 22:56:05,546:INFO: 		 ADE on hotel                     dataset:	 1.4961711168289185
2022-10-27 22:56:26,001:INFO: 		 ADE on univ                      dataset:	 1.211814284324646
2022-10-27 22:56:33,392:INFO: 		 ADE on zara1                     dataset:	 1.312929391860962
2022-10-27 22:56:52,093:INFO: 		 ADE on zara2                     dataset:	 1.0196534395217896
2022-10-27 22:56:52,093:INFO: Average training:	ADE  1.1865	FDE  2.1694
2022-10-27 22:56:52,110:INFO:  --> Model Saved in ./models/eth/pretrain/P1/5.0/SSE_data_eth_irm[5.0]_epoch_100.pth.tar
2022-10-27 22:56:52,110:INFO: 
===> EPOCH: 101 (P2)
2022-10-27 22:56:52,110:INFO: - Computing loss (training)
2022-10-27 22:56:58,923:INFO: Dataset: hotel               Batch: 1/8	Loss 46.5896 (46.5896)
2022-10-27 22:57:00,564:INFO: Dataset: hotel               Batch: 2/8	Loss 38.2067 (42.3789)
2022-10-27 22:57:02,179:INFO: Dataset: hotel               Batch: 3/8	Loss 38.6478 (41.1938)
2022-10-27 22:57:03,755:INFO: Dataset: hotel               Batch: 4/8	Loss 38.4645 (40.5409)
2022-10-27 22:57:05,321:INFO: Dataset: hotel               Batch: 5/8	Loss 39.3936 (40.3123)
2022-10-27 22:57:06,912:INFO: Dataset: hotel               Batch: 6/8	Loss 37.9569 (39.9178)
2022-10-27 22:57:08,505:INFO: Dataset: hotel               Batch: 7/8	Loss 37.3437 (39.5486)
2022-10-27 22:57:09,500:INFO: Dataset: hotel               Batch: 8/8	Loss 7.8207 (38.4603)
2022-10-27 22:57:29,221:INFO: Dataset: univ                Batch:  1/29	Loss 47.6874 (47.6874)
2022-10-27 22:57:30,848:INFO: Dataset: univ                Batch:  2/29	Loss 42.9851 (45.3314)
2022-10-27 22:57:32,478:INFO: Dataset: univ                Batch:  3/29	Loss 39.9237 (43.5429)
2022-10-27 22:57:34,242:INFO: Dataset: univ                Batch:  4/29	Loss 46.6163 (44.3970)
2022-10-27 22:57:35,884:INFO: Dataset: univ                Batch:  5/29	Loss 38.6941 (43.1603)
2022-10-27 22:57:37,516:INFO: Dataset: univ                Batch:  6/29	Loss 40.0439 (42.6819)
2022-10-27 22:57:39,152:INFO: Dataset: univ                Batch:  7/29	Loss 37.8773 (41.9809)
2022-10-27 22:57:40,827:INFO: Dataset: univ                Batch:  8/29	Loss 38.1941 (41.4879)
2022-10-27 22:57:42,503:INFO: Dataset: univ                Batch:  9/29	Loss 40.8475 (41.4245)
2022-10-27 22:57:44,463:INFO: Dataset: univ                Batch: 10/29	Loss 37.0023 (41.0024)
2022-10-27 22:57:46,443:INFO: Dataset: univ                Batch: 11/29	Loss 39.2630 (40.8037)
2022-10-27 22:57:48,275:INFO: Dataset: univ                Batch: 12/29	Loss 37.5356 (40.5239)
2022-10-27 22:57:50,940:INFO: Dataset: univ                Batch: 13/29	Loss 38.6750 (40.3619)
2022-10-27 22:57:53,054:INFO: Dataset: univ                Batch: 14/29	Loss 37.4164 (40.1608)
2022-10-27 22:57:54,814:INFO: Dataset: univ                Batch: 15/29	Loss 37.2332 (40.0048)
2022-10-27 22:57:56,483:INFO: Dataset: univ                Batch: 16/29	Loss 39.1825 (39.9511)
2022-10-27 22:57:58,631:INFO: Dataset: univ                Batch: 17/29	Loss 28.8489 (39.4398)
2022-10-27 22:58:00,457:INFO: Dataset: univ                Batch: 18/29	Loss 42.7554 (39.6035)
2022-10-27 22:58:02,314:INFO: Dataset: univ                Batch: 19/29	Loss 38.6449 (39.5515)
2022-10-27 22:58:04,032:INFO: Dataset: univ                Batch: 20/29	Loss 46.8574 (39.9174)
2022-10-27 22:58:05,675:INFO: Dataset: univ                Batch: 21/29	Loss 37.8787 (39.8193)
2022-10-27 22:58:07,365:INFO: Dataset: univ                Batch: 22/29	Loss 39.2890 (39.7986)
2022-10-27 22:58:09,149:INFO: Dataset: univ                Batch: 23/29	Loss 31.7399 (39.4840)
2022-10-27 22:58:10,820:INFO: Dataset: univ                Batch: 24/29	Loss 38.5045 (39.4462)
2022-10-27 22:58:12,459:INFO: Dataset: univ                Batch: 25/29	Loss 36.7186 (39.3394)
2022-10-27 22:58:14,134:INFO: Dataset: univ                Batch: 26/29	Loss 37.9082 (39.2911)
2022-10-27 22:58:15,822:INFO: Dataset: univ                Batch: 27/29	Loss 38.2520 (39.2464)
2022-10-27 22:58:18,003:INFO: Dataset: univ                Batch: 28/29	Loss 37.7013 (39.1952)
2022-10-27 22:58:19,434:INFO: Dataset: univ                Batch: 29/29	Loss 14.6027 (38.9307)
2022-10-27 22:58:27,320:INFO: Dataset: zara1               Batch:  1/16	Loss 36.5547 (36.5547)
2022-10-27 22:58:29,098:INFO: Dataset: zara1               Batch:  2/16	Loss 36.5276 (36.5384)
2022-10-27 22:58:30,671:INFO: Dataset: zara1               Batch:  3/16	Loss 36.4239 (36.4952)
2022-10-27 22:58:32,374:INFO: Dataset: zara1               Batch:  4/16	Loss 38.8123 (37.1231)
2022-10-27 22:58:33,963:INFO: Dataset: zara1               Batch:  5/16	Loss 52.2087 (39.9341)
2022-10-27 22:58:35,685:INFO: Dataset: zara1               Batch:  6/16	Loss 36.8420 (39.4587)
2022-10-27 22:58:37,252:INFO: Dataset: zara1               Batch:  7/16	Loss 38.4483 (39.3221)
2022-10-27 22:58:38,833:INFO: Dataset: zara1               Batch:  8/16	Loss 36.9270 (39.0347)
2022-10-27 22:58:40,448:INFO: Dataset: zara1               Batch:  9/16	Loss 36.5255 (38.7382)
2022-10-27 22:58:42,054:INFO: Dataset: zara1               Batch: 10/16	Loss 36.4451 (38.5221)
2022-10-27 22:58:43,662:INFO: Dataset: zara1               Batch: 11/16	Loss 37.2197 (38.4222)
2022-10-27 22:58:45,258:INFO: Dataset: zara1               Batch: 12/16	Loss 36.9452 (38.3198)
2022-10-27 22:58:46,890:INFO: Dataset: zara1               Batch: 13/16	Loss 36.9118 (38.2036)
2022-10-27 22:58:48,723:INFO: Dataset: zara1               Batch: 14/16	Loss 36.7691 (38.1099)
2022-10-27 22:58:50,354:INFO: Dataset: zara1               Batch: 15/16	Loss 36.4939 (38.0117)
2022-10-27 22:58:51,746:INFO: Dataset: zara1               Batch: 16/16	Loss 26.8423 (37.4767)
2022-10-27 22:59:13,088:INFO: Dataset: zara2               Batch:  1/36	Loss 37.4503 (37.4503)
2022-10-27 22:59:15,432:INFO: Dataset: zara2               Batch:  2/36	Loss 37.6889 (37.5646)
2022-10-27 22:59:17,591:INFO: Dataset: zara2               Batch:  3/36	Loss 39.2572 (38.1234)
2022-10-27 22:59:19,726:INFO: Dataset: zara2               Batch:  4/36	Loss 37.0548 (37.8616)
2022-10-27 22:59:21,679:INFO: Dataset: zara2               Batch:  5/36	Loss 40.4750 (38.3831)
2022-10-27 22:59:23,393:INFO: Dataset: zara2               Batch:  6/36	Loss 37.4277 (38.2357)
2022-10-27 22:59:25,365:INFO: Dataset: zara2               Batch:  7/36	Loss 37.5621 (38.1314)
2022-10-27 22:59:27,025:INFO: Dataset: zara2               Batch:  8/36	Loss 35.6414 (37.8523)
2022-10-27 22:59:28,872:INFO: Dataset: zara2               Batch:  9/36	Loss 35.5564 (37.6008)
2022-10-27 22:59:30,661:INFO: Dataset: zara2               Batch: 10/36	Loss 35.9239 (37.4274)
2022-10-27 22:59:32,460:INFO: Dataset: zara2               Batch: 11/36	Loss 52.0285 (38.6765)
2022-10-27 22:59:34,095:INFO: Dataset: zara2               Batch: 12/36	Loss 37.1926 (38.5562)
2022-10-27 22:59:35,759:INFO: Dataset: zara2               Batch: 13/36	Loss 37.1200 (38.4606)
2022-10-27 22:59:37,396:INFO: Dataset: zara2               Batch: 14/36	Loss 36.0424 (38.2763)
2022-10-27 22:59:39,142:INFO: Dataset: zara2               Batch: 15/36	Loss 36.0354 (38.1382)
2022-10-27 22:59:41,172:INFO: Dataset: zara2               Batch: 16/36	Loss 37.1829 (38.0751)
2022-10-27 22:59:42,946:INFO: Dataset: zara2               Batch: 17/36	Loss 36.0047 (37.9570)
2022-10-27 22:59:44,860:INFO: Dataset: zara2               Batch: 18/36	Loss 36.0239 (37.8424)
2022-10-27 22:59:46,951:INFO: Dataset: zara2               Batch: 19/36	Loss 36.0028 (37.7460)
2022-10-27 22:59:48,971:INFO: Dataset: zara2               Batch: 20/36	Loss 35.6634 (37.6452)
2022-10-27 22:59:50,918:INFO: Dataset: zara2               Batch: 21/36	Loss 36.0825 (37.5560)
2022-10-27 22:59:52,950:INFO: Dataset: zara2               Batch: 22/36	Loss 35.9798 (37.4941)
2022-10-27 22:59:54,997:INFO: Dataset: zara2               Batch: 23/36	Loss 36.0808 (37.4399)
2022-10-27 22:59:56,933:INFO: Dataset: zara2               Batch: 24/36	Loss 35.9859 (37.3866)
2022-10-27 22:59:58,547:INFO: Dataset: zara2               Batch: 25/36	Loss 37.3458 (37.3850)
2022-10-27 23:00:00,179:INFO: Dataset: zara2               Batch: 26/36	Loss 35.8022 (37.3284)
2022-10-27 23:00:01,961:INFO: Dataset: zara2               Batch: 27/36	Loss 35.8221 (37.2764)
2022-10-27 23:00:03,574:INFO: Dataset: zara2               Batch: 28/36	Loss 36.0181 (37.2319)
2022-10-27 23:00:05,252:INFO: Dataset: zara2               Batch: 29/36	Loss 35.9202 (37.1836)
2022-10-27 23:00:06,883:INFO: Dataset: zara2               Batch: 30/36	Loss 37.2692 (37.1863)
2022-10-27 23:00:08,509:INFO: Dataset: zara2               Batch: 31/36	Loss 35.9503 (37.1504)
2022-10-27 23:00:10,111:INFO: Dataset: zara2               Batch: 32/36	Loss 36.5114 (37.1303)
2022-10-27 23:00:12,079:INFO: Dataset: zara2               Batch: 33/36	Loss 36.3406 (37.1039)
2022-10-27 23:00:13,974:INFO: Dataset: zara2               Batch: 34/36	Loss 35.8248 (37.0664)
2022-10-27 23:00:15,805:INFO: Dataset: zara2               Batch: 35/36	Loss 35.9959 (37.0285)
2022-10-27 23:00:17,194:INFO: Dataset: zara2               Batch: 36/36	Loss 25.8036 (36.7538)
2022-10-27 23:00:17,935:INFO: - Computing ADE (validation o)
2022-10-27 23:00:24,699:INFO: 		 ADE on eth                       dataset:	 3.220338821411133
2022-10-27 23:00:24,700:INFO: Average validation o:	ADE  3.2203	FDE  5.3717
2022-10-27 23:00:24,701:INFO: - Computing ADE (validation)
2022-10-27 23:00:31,346:INFO: 		 ADE on hotel                     dataset:	 1.239866852760315
2022-10-27 23:00:38,217:INFO: 		 ADE on univ                      dataset:	 1.5484427213668823
2022-10-27 23:00:44,973:INFO: 		 ADE on zara1                     dataset:	 1.7434228658676147
2022-10-27 23:00:52,077:INFO: 		 ADE on zara2                     dataset:	 1.4347712993621826
2022-10-27 23:00:52,077:INFO: Average validation:	ADE  1.5012	FDE  2.6590
2022-10-27 23:00:52,078:INFO: - Computing ADE (training)
2022-10-27 23:00:59,148:INFO: 		 ADE on hotel                     dataset:	 1.535940408706665

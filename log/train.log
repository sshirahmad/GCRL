2022-11-13 23:29:40,965:INFO: Initializing Training Set
2022-11-13 23:29:44,938:INFO: Initializing Validation Set
2022-11-13 23:29:45,409:INFO: Initializing Validation O Set
2022-11-13 23:29:48,602:INFO: => loaded checkpoint './models/Ec/P3/CRMF_epoch_292.pth.tar' (epoch 293)
2022-11-13 23:29:48,603:INFO: 
===> EPOCH: 401 (P4)
2022-11-13 23:29:48,604:INFO: - Computing loss (training)
2022-11-13 23:29:57,561:INFO: Dataset: hotel               Batch:  1/15	Loss 25.6202 (25.6202)
2022-11-13 23:29:59,126:INFO: Dataset: hotel               Batch:  2/15	Loss 23.1821 (24.3091)
2022-11-13 23:30:00,669:INFO: Dataset: hotel               Batch:  3/15	Loss 21.5189 (23.4395)
2022-11-13 23:30:02,488:INFO: Dataset: hotel               Batch:  4/15	Loss 21.2826 (22.9108)
2022-11-13 23:30:04,014:INFO: Dataset: hotel               Batch:  5/15	Loss 21.9327 (22.7183)
2022-11-13 23:30:05,845:INFO: Dataset: hotel               Batch:  6/15	Loss 20.9808 (22.4183)
2022-11-13 23:30:07,357:INFO: Dataset: hotel               Batch:  7/15	Loss 20.7678 (22.1952)
2022-11-13 23:30:08,815:INFO: Dataset: hotel               Batch:  8/15	Loss 20.3786 (21.9104)
2022-11-13 23:30:10,225:INFO: Dataset: hotel               Batch:  9/15	Loss 20.3389 (21.7435)
2022-11-13 23:30:11,662:INFO: Dataset: hotel               Batch: 10/15	Loss 21.1269 (21.6801)
2022-11-13 23:30:13,170:INFO: Dataset: hotel               Batch: 11/15	Loss 20.4430 (21.5686)
2022-11-13 23:30:14,632:INFO: Dataset: hotel               Batch: 12/15	Loss 20.9612 (21.5202)
2022-11-13 23:30:16,143:INFO: Dataset: hotel               Batch: 13/15	Loss 20.5521 (21.4514)
2022-11-13 23:30:18,080:INFO: Dataset: hotel               Batch: 14/15	Loss 19.5276 (21.2967)
2022-11-13 23:30:19,498:INFO: Dataset: hotel               Batch: 15/15	Loss 8.7886 (20.9007)
2022-11-13 23:30:45,890:INFO: Dataset: univ                Batch:  1/57	Loss 20.9075 (20.9075)
2022-11-13 23:30:47,791:INFO: Dataset: univ                Batch:  2/57	Loss 21.0890 (20.9990)
2022-11-13 23:30:49,542:INFO: Dataset: univ                Batch:  3/57	Loss 21.0888 (21.0274)
2022-11-13 23:30:51,205:INFO: Dataset: univ                Batch:  4/57	Loss 21.2614 (21.0779)
2022-11-13 23:30:52,990:INFO: Dataset: univ                Batch:  5/57	Loss 20.4143 (20.9197)
2022-11-13 23:30:54,595:INFO: Dataset: univ                Batch:  6/57	Loss 20.5949 (20.8633)
2022-11-13 23:30:56,416:INFO: Dataset: univ                Batch:  7/57	Loss 20.8223 (20.8580)
2022-11-13 23:30:58,144:INFO: Dataset: univ                Batch:  8/57	Loss 20.6835 (20.8355)
2022-11-13 23:30:59,811:INFO: Dataset: univ                Batch:  9/57	Loss 20.6570 (20.8152)
2022-11-13 23:31:01,485:INFO: Dataset: univ                Batch: 10/57	Loss 20.8837 (20.8214)
2022-11-13 23:31:03,167:INFO: Dataset: univ                Batch: 11/57	Loss 21.1055 (20.8419)
2022-11-13 23:31:04,815:INFO: Dataset: univ                Batch: 12/57	Loss 20.2735 (20.7922)
2022-11-13 23:31:06,917:INFO: Dataset: univ                Batch: 13/57	Loss 20.3493 (20.7602)
2022-11-13 23:31:08,977:INFO: Dataset: univ                Batch: 14/57	Loss 20.0514 (20.7092)
2022-11-13 23:31:10,855:INFO: Dataset: univ                Batch: 15/57	Loss 19.8965 (20.6434)
2022-11-13 23:31:12,726:INFO: Dataset: univ                Batch: 16/57	Loss 20.2200 (20.6163)
2022-11-13 23:31:14,521:INFO: Dataset: univ                Batch: 17/57	Loss 20.9220 (20.6317)
2022-11-13 23:31:16,405:INFO: Dataset: univ                Batch: 18/57	Loss 20.4822 (20.6223)
2022-11-13 23:31:18,360:INFO: Dataset: univ                Batch: 19/57	Loss 19.9743 (20.5869)
2022-11-13 23:31:20,274:INFO: Dataset: univ                Batch: 20/57	Loss 20.3351 (20.5720)
2022-11-13 23:31:22,061:INFO: Dataset: univ                Batch: 21/57	Loss 20.9719 (20.5857)
2022-11-13 23:31:23,890:INFO: Dataset: univ                Batch: 22/57	Loss 20.1844 (20.5648)
2022-11-13 23:31:25,653:INFO: Dataset: univ                Batch: 23/57	Loss 19.9054 (20.5331)
2022-11-13 23:31:27,676:INFO: Dataset: univ                Batch: 24/57	Loss 20.4829 (20.5306)
2022-11-13 23:31:29,614:INFO: Dataset: univ                Batch: 25/57	Loss 19.9812 (20.5087)
2022-11-13 23:31:31,488:INFO: Dataset: univ                Batch: 26/57	Loss 19.9820 (20.4861)
2022-11-13 23:31:33,342:INFO: Dataset: univ                Batch: 27/57	Loss 20.4489 (20.4849)
2022-11-13 23:31:35,216:INFO: Dataset: univ                Batch: 28/57	Loss 20.4845 (20.4849)
2022-11-13 23:31:37,153:INFO: Dataset: univ                Batch: 29/57	Loss 19.9359 (20.4625)
2022-11-13 23:31:39,105:INFO: Dataset: univ                Batch: 30/57	Loss 20.4083 (20.4609)
2022-11-13 23:31:41,107:INFO: Dataset: univ                Batch: 31/57	Loss 20.5942 (20.4648)
2022-11-13 23:31:42,964:INFO: Dataset: univ                Batch: 32/57	Loss 20.4927 (20.4655)
2022-11-13 23:31:44,851:INFO: Dataset: univ                Batch: 33/57	Loss 19.7499 (20.4414)
2022-11-13 23:31:46,867:INFO: Dataset: univ                Batch: 34/57	Loss 19.5682 (20.4082)
2022-11-13 23:31:48,885:INFO: Dataset: univ                Batch: 35/57	Loss 20.3435 (20.4066)
2022-11-13 23:31:50,926:INFO: Dataset: univ                Batch: 36/57	Loss 20.1924 (20.4003)
2022-11-13 23:31:52,758:INFO: Dataset: univ                Batch: 37/57	Loss 20.4623 (20.4017)
2022-11-13 23:31:54,545:INFO: Dataset: univ                Batch: 38/57	Loss 20.0239 (20.3906)
2022-11-13 23:31:56,349:INFO: Dataset: univ                Batch: 39/57	Loss 19.7472 (20.3707)
2022-11-13 23:31:58,320:INFO: Dataset: univ                Batch: 40/57	Loss 20.4161 (20.3717)
2022-11-13 23:32:00,254:INFO: Dataset: univ                Batch: 41/57	Loss 20.1839 (20.3671)
2022-11-13 23:32:02,092:INFO: Dataset: univ                Batch: 42/57	Loss 20.0083 (20.3573)
2022-11-13 23:32:03,857:INFO: Dataset: univ                Batch: 43/57	Loss 20.4153 (20.3584)
2022-11-13 23:32:05,703:INFO: Dataset: univ                Batch: 44/57	Loss 20.0787 (20.3531)
2022-11-13 23:32:07,683:INFO: Dataset: univ                Batch: 45/57	Loss 20.1395 (20.3488)
2022-11-13 23:32:09,625:INFO: Dataset: univ                Batch: 46/57	Loss 20.5570 (20.3530)
2022-11-13 23:32:11,541:INFO: Dataset: univ                Batch: 47/57	Loss 19.7666 (20.3373)
2022-11-13 23:32:13,306:INFO: Dataset: univ                Batch: 48/57	Loss 20.1200 (20.3326)
2022-11-13 23:32:15,178:INFO: Dataset: univ                Batch: 49/57	Loss 20.0984 (20.3276)
2022-11-13 23:32:17,043:INFO: Dataset: univ                Batch: 50/57	Loss 20.0481 (20.3223)
2022-11-13 23:32:19,010:INFO: Dataset: univ                Batch: 51/57	Loss 19.8859 (20.3125)
2022-11-13 23:32:21,028:INFO: Dataset: univ                Batch: 52/57	Loss 19.9651 (20.3052)
2022-11-13 23:32:22,945:INFO: Dataset: univ                Batch: 53/57	Loss 20.3635 (20.3063)
2022-11-13 23:32:24,807:INFO: Dataset: univ                Batch: 54/57	Loss 19.9099 (20.2973)
2022-11-13 23:32:26,622:INFO: Dataset: univ                Batch: 55/57	Loss 20.2501 (20.2965)
2022-11-13 23:32:28,597:INFO: Dataset: univ                Batch: 56/57	Loss 19.9850 (20.2911)
2022-11-13 23:32:30,530:INFO: Dataset: univ                Batch: 57/57	Loss 15.2303 (20.2338)
2022-11-13 23:32:41,945:INFO: Dataset: zara1               Batch:  1/32	Loss 20.5581 (20.5581)
2022-11-13 23:32:43,848:INFO: Dataset: zara1               Batch:  2/32	Loss 20.5326 (20.5462)
2022-11-13 23:32:46,316:INFO: Dataset: zara1               Batch:  3/32	Loss 20.3900 (20.4960)
2022-11-13 23:32:48,218:INFO: Dataset: zara1               Batch:  4/32	Loss 20.1724 (20.3906)

2022-11-25 00:56:49,484:INFO: Initializing Training Set
2022-11-25 00:56:51,161:INFO: Initializing Validation Set
2022-11-25 00:56:51,350:INFO: Initializing Validation O Set
2022-11-25 00:56:53,519:INFO: => loaded checkpoint './models/E1//P3/CRMF_epoch_366.pth.tar' (epoch 367)
2022-11-25 00:56:53,520:INFO: 
===> EPOCH: 551 (P4)
2022-11-25 00:56:53,520:INFO: - Computing loss (training)
2022-11-25 00:56:53,737:INFO: Dataset: hotel               Batch: 1/4	Loss 31.2869 (31.2869)
2022-11-25 00:56:53,740:INFO: Dataset: hotel               Batch: 2/4	Loss 31.2982 (31.2926)
2022-11-25 00:56:53,742:INFO: Dataset: hotel               Batch: 3/4	Loss 31.2908 (31.2920)
2022-11-25 00:56:53,744:INFO: Dataset: hotel               Batch: 4/4	Loss 31.3025 (31.2938)
2022-11-25 00:56:54,000:INFO: Dataset: univ                Batch:  1/15	Loss 28.5383 (28.5383)
2022-11-25 00:56:54,007:INFO: Dataset: univ                Batch:  2/15	Loss 28.5482 (28.5430)
2022-11-25 00:56:54,008:INFO: Dataset: univ                Batch:  3/15	Loss 28.5376 (28.5412)
2022-11-25 00:56:54,010:INFO: Dataset: univ                Batch:  4/15	Loss 28.5458 (28.5423)
2022-11-25 00:56:54,046:INFO: Dataset: univ                Batch:  5/15	Loss 28.5403 (28.5419)
2022-11-25 00:56:54,054:INFO: Dataset: univ                Batch:  6/15	Loss 28.5379 (28.5412)
2022-11-25 00:56:54,056:INFO: Dataset: univ                Batch:  7/15	Loss 28.5323 (28.5399)
2022-11-25 00:56:54,057:INFO: Dataset: univ                Batch:  8/15	Loss 28.5315 (28.5386)
2022-11-25 00:56:54,058:INFO: Dataset: univ                Batch:  9/15	Loss 28.5164 (28.5361)
2022-11-25 00:56:54,059:INFO: Dataset: univ                Batch: 10/15	Loss 28.5217 (28.5347)
2022-11-25 00:56:54,060:INFO: Dataset: univ                Batch: 11/15	Loss 28.5100 (28.5325)
2022-11-25 00:56:54,062:INFO: Dataset: univ                Batch: 12/15	Loss 28.5007 (28.5299)
2022-11-25 00:56:54,063:INFO: Dataset: univ                Batch: 13/15	Loss 28.5079 (28.5282)
2022-11-25 00:56:54,064:INFO: Dataset: univ                Batch: 14/15	Loss 28.5027 (28.5264)
2022-11-25 00:56:54,065:INFO: Dataset: univ                Batch: 15/15	Loss 28.4794 (28.5258)
2022-11-25 00:56:54,308:INFO: Dataset: zara1               Batch: 1/8	Loss 35.1361 (35.1361)
2022-11-25 00:56:54,309:INFO: Dataset: zara1               Batch: 2/8	Loss 35.1347 (35.1354)
2022-11-25 00:56:54,315:INFO: Dataset: zara1               Batch: 3/8	Loss 35.1001 (35.1235)
2022-11-25 00:56:54,316:INFO: Dataset: zara1               Batch: 4/8	Loss 35.0964 (35.1170)
2022-11-25 00:56:54,317:INFO: Dataset: zara1               Batch: 5/8	Loss 35.1153 (35.1167)
2022-11-25 00:56:54,353:INFO: Dataset: zara1               Batch: 6/8	Loss 35.1136 (35.1162)
2022-11-25 00:56:54,355:INFO: Dataset: zara1               Batch: 7/8	Loss 35.1015 (35.1140)
2022-11-25 00:56:54,356:INFO: Dataset: zara1               Batch: 8/8	Loss 35.0847 (35.1104)
2022-11-25 00:56:54,653:INFO: Dataset: zara2               Batch:  1/18	Loss 29.1427 (29.1427)
2022-11-25 00:56:54,664:INFO: Dataset: zara2               Batch:  2/18	Loss 29.1384 (29.1404)
2022-11-25 00:56:54,670:INFO: Dataset: zara2               Batch:  3/18	Loss 29.1494 (29.1434)
2022-11-25 00:56:54,673:INFO: Dataset: zara2               Batch:  4/18	Loss 29.1317 (29.1404)
2022-11-25 00:56:54,731:INFO: Dataset: zara2               Batch:  5/18	Loss 29.1395 (29.1402)
2022-11-25 00:56:54,742:INFO: Dataset: zara2               Batch:  6/18	Loss 29.1336 (29.1392)
2022-11-25 00:56:54,744:INFO: Dataset: zara2               Batch:  7/18	Loss 29.1265 (29.1375)
2022-11-25 00:56:54,746:INFO: Dataset: zara2               Batch:  8/18	Loss 29.1160 (29.1346)
2022-11-25 00:56:54,748:INFO: Dataset: zara2               Batch:  9/18	Loss 29.1130 (29.1321)
2022-11-25 00:56:54,750:INFO: Dataset: zara2               Batch: 10/18	Loss 29.1183 (29.1306)
2022-11-25 00:56:54,754:INFO: Dataset: zara2               Batch: 11/18	Loss 29.1048 (29.1283)
2022-11-25 00:56:54,755:INFO: Dataset: zara2               Batch: 12/18	Loss 29.1180 (29.1275)
2022-11-25 00:56:54,757:INFO: Dataset: zara2               Batch: 13/18	Loss 29.1123 (29.1262)
2022-11-25 00:56:54,758:INFO: Dataset: zara2               Batch: 14/18	Loss 29.1012 (29.1242)
2022-11-25 00:56:54,759:INFO: Dataset: zara2               Batch: 15/18	Loss 29.1021 (29.1227)
2022-11-25 00:56:54,761:INFO: Dataset: zara2               Batch: 16/18	Loss 29.1120 (29.1221)
2022-11-25 00:56:54,764:INFO: Dataset: zara2               Batch: 17/18	Loss 29.1041 (29.1211)
2022-11-25 00:56:54,767:INFO: Dataset: zara2               Batch: 18/18	Loss 29.0955 (29.1199)
2022-11-25 00:56:54,817:INFO: - Computing ADE (validation o)
2022-11-25 00:56:55,088:INFO: 		 ADE on eth                       dataset:	 1.1820335388183594
2022-11-25 00:56:55,088:INFO: Average validation o:	ADE  1.1820	FDE  2.2592
2022-11-25 00:56:55,089:INFO: - Computing loss (validation)
2022-11-25 00:56:55,286:INFO: Dataset: hotel               Batch: 1/2	Loss 31.2808 (31.2808)
2022-11-25 00:56:55,287:INFO: Dataset: hotel               Batch: 2/2	Loss 31.2760 (31.2805)
2022-11-25 00:56:55,618:INFO: Dataset: univ                Batch: 1/3	Loss 28.3848 (28.3848)
2022-11-25 00:56:55,633:INFO: Dataset: univ                Batch: 2/3	Loss 28.3814 (28.3832)
2022-11-25 00:56:55,634:INFO: Dataset: univ                Batch: 3/3	Loss 28.4008 (28.3889)
2022-11-25 00:56:55,932:INFO: Dataset: zara1               Batch: 1/2	Loss 34.9043 (34.9043)
2022-11-25 00:56:55,937:INFO: Dataset: zara1               Batch: 2/2	Loss 34.9237 (34.9092)
2022-11-25 00:56:56,201:INFO: Dataset: zara2               Batch: 1/5	Loss 29.1027 (29.1027)
2022-11-25 00:56:56,204:INFO: Dataset: zara2               Batch: 2/5	Loss 29.0969 (29.0999)
2022-11-25 00:56:56,205:INFO: Dataset: zara2               Batch: 3/5	Loss 29.1059 (29.1017)
2022-11-25 00:56:56,205:INFO: Dataset: zara2               Batch: 4/5	Loss 29.1188 (29.1064)
2022-11-25 00:56:56,206:INFO: Dataset: zara2               Batch: 5/5	Loss 29.1190 (29.1089)
2022-11-25 00:56:56,262:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_551.pth.tar
2022-11-25 00:56:56,263:INFO: 
===> EPOCH: 552 (P4)
2022-11-25 00:56:56,263:INFO: - Computing loss (training)
2022-11-25 00:56:56,465:INFO: Dataset: hotel               Batch: 1/4	Loss 31.2415 (31.2415)
2022-11-25 00:56:56,467:INFO: Dataset: hotel               Batch: 2/4	Loss 31.2479 (31.2447)
2022-11-25 00:56:56,469:INFO: Dataset: hotel               Batch: 3/4	Loss 31.2594 (31.2498)
2022-11-25 00:56:56,471:INFO: Dataset: hotel               Batch: 4/4	Loss 31.2469 (31.2493)
2022-11-25 00:56:56,731:INFO: Dataset: univ                Batch:  1/15	Loss 28.4114 (28.4114)
2022-11-25 00:56:56,739:INFO: Dataset: univ                Batch:  2/15	Loss 28.4143 (28.4128)
2022-11-25 00:56:56,741:INFO: Dataset: univ                Batch:  3/15	Loss 28.4112 (28.4123)
2022-11-25 00:56:56,742:INFO: Dataset: univ                Batch:  4/15	Loss 28.4036 (28.4102)
2022-11-25 00:56:56,781:INFO: Dataset: univ                Batch:  5/15	Loss 28.3946 (28.4071)
2022-11-25 00:56:56,783:INFO: Dataset: univ                Batch:  6/15	Loss 28.4047 (28.4067)
2022-11-25 00:56:56,785:INFO: Dataset: univ                Batch:  7/15	Loss 28.3995 (28.4056)
2022-11-25 00:56:56,786:INFO: Dataset: univ                Batch:  8/15	Loss 28.3899 (28.4036)
2022-11-25 00:56:56,787:INFO: Dataset: univ                Batch:  9/15	Loss 28.3858 (28.4017)
2022-11-25 00:56:56,788:INFO: Dataset: univ                Batch: 10/15	Loss 28.3796 (28.3994)
2022-11-25 00:56:56,790:INFO: Dataset: univ                Batch: 11/15	Loss 28.3786 (28.3976)
2022-11-25 00:56:56,792:INFO: Dataset: univ                Batch: 12/15	Loss 28.3755 (28.3957)
2022-11-25 00:56:56,793:INFO: Dataset: univ                Batch: 13/15	Loss 28.3739 (28.3942)
2022-11-25 00:56:56,794:INFO: Dataset: univ                Batch: 14/15	Loss 28.3740 (28.3928)
2022-11-25 00:56:56,795:INFO: Dataset: univ                Batch: 15/15	Loss 28.3569 (28.3923)
2022-11-25 00:56:57,069:INFO: Dataset: zara1               Batch: 1/8	Loss 35.0285 (35.0285)
2022-11-25 00:56:57,073:INFO: Dataset: zara1               Batch: 2/8	Loss 35.0180 (35.0230)
2022-11-25 00:56:57,074:INFO: Dataset: zara1               Batch: 3/8	Loss 34.9952 (35.0129)
2022-11-25 00:56:57,075:INFO: Dataset: zara1               Batch: 4/8	Loss 35.0093 (35.0121)
2022-11-25 00:56:57,078:INFO: Dataset: zara1               Batch: 5/8	Loss 34.9908 (35.0078)
2022-11-25 00:56:57,106:INFO: Dataset: zara1               Batch: 6/8	Loss 35.0082 (35.0079)
2022-11-25 00:56:57,107:INFO: Dataset: zara1               Batch: 7/8	Loss 34.9916 (35.0053)
2022-11-25 00:56:57,108:INFO: Dataset: zara1               Batch: 8/8	Loss 34.9864 (35.0032)
2022-11-25 00:56:57,382:INFO: Dataset: zara2               Batch:  1/18	Loss 28.9961 (28.9961)
2022-11-25 00:56:57,385:INFO: Dataset: zara2               Batch:  2/18	Loss 28.9911 (28.9938)
2022-11-25 00:56:57,390:INFO: Dataset: zara2               Batch:  3/18	Loss 29.0224 (29.0045)
2022-11-25 00:56:57,391:INFO: Dataset: zara2               Batch:  4/18	Loss 28.9881 (29.0005)
2022-11-25 00:56:57,428:INFO: Dataset: zara2               Batch:  5/18	Loss 29.0267 (29.0059)
2022-11-25 00:56:57,432:INFO: Dataset: zara2               Batch:  6/18	Loss 28.9756 (29.0009)
2022-11-25 00:56:57,433:INFO: Dataset: zara2               Batch:  7/18	Loss 28.9854 (28.9985)
2022-11-25 00:56:57,434:INFO: Dataset: zara2               Batch:  8/18	Loss 28.9638 (28.9945)
2022-11-25 00:56:57,435:INFO: Dataset: zara2               Batch:  9/18	Loss 29.0004 (28.9951)
2022-11-25 00:56:57,436:INFO: Dataset: zara2               Batch: 10/18	Loss 28.9957 (28.9951)
2022-11-25 00:56:57,437:INFO: Dataset: zara2               Batch: 11/18	Loss 28.9577 (28.9918)
2022-11-25 00:56:57,439:INFO: Dataset: zara2               Batch: 12/18	Loss 28.9842 (28.9911)
2022-11-25 00:56:57,440:INFO: Dataset: zara2               Batch: 13/18	Loss 28.9698 (28.9893)
2022-11-25 00:56:57,441:INFO: Dataset: zara2               Batch: 14/18	Loss 28.9583 (28.9870)
2022-11-25 00:56:57,442:INFO: Dataset: zara2               Batch: 15/18	Loss 28.9552 (28.9850)
2022-11-25 00:56:57,442:INFO: Dataset: zara2               Batch: 16/18	Loss 28.9593 (28.9833)
2022-11-25 00:56:57,444:INFO: Dataset: zara2               Batch: 17/18	Loss 28.9644 (28.9822)
2022-11-25 00:56:57,445:INFO: Dataset: zara2               Batch: 18/18	Loss 28.9423 (28.9802)
2022-11-25 00:56:57,491:INFO: - Computing ADE (validation o)
2022-11-25 00:56:57,754:INFO: 		 ADE on eth                       dataset:	 1.1046117544174194
2022-11-25 00:56:57,755:INFO: Average validation o:	ADE  1.1046	FDE  2.1572
2022-11-25 00:56:57,755:INFO: - Computing loss (validation)
2022-11-25 00:56:57,954:INFO: Dataset: hotel               Batch: 1/2	Loss 31.2679 (31.2679)
2022-11-25 00:56:57,955:INFO: Dataset: hotel               Batch: 2/2	Loss 31.2407 (31.2662)
2022-11-25 00:56:58,223:INFO: Dataset: univ                Batch: 1/3	Loss 28.2517 (28.2517)
2022-11-25 00:56:58,227:INFO: Dataset: univ                Batch: 2/3	Loss 28.2492 (28.2505)
2022-11-25 00:56:58,227:INFO: Dataset: univ                Batch: 3/3	Loss 28.2533 (28.2515)
2022-11-25 00:56:58,474:INFO: Dataset: zara1               Batch: 1/2	Loss 34.8000 (34.8000)
2022-11-25 00:56:58,475:INFO: Dataset: zara1               Batch: 2/2	Loss 34.8295 (34.8070)
2022-11-25 00:56:58,742:INFO: Dataset: zara2               Batch: 1/5	Loss 28.9878 (28.9878)
2022-11-25 00:56:58,743:INFO: Dataset: zara2               Batch: 2/5	Loss 28.9728 (28.9802)
2022-11-25 00:56:58,745:INFO: Dataset: zara2               Batch: 3/5	Loss 28.9807 (28.9804)
2022-11-25 00:56:58,749:INFO: Dataset: zara2               Batch: 4/5	Loss 28.9674 (28.9772)
2022-11-25 00:56:58,750:INFO: Dataset: zara2               Batch: 5/5	Loss 28.9673 (28.9750)
2022-11-25 00:56:58,802:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_552.pth.tar
2022-11-25 00:56:58,802:INFO: 
===> EPOCH: 553 (P4)
2022-11-25 00:56:58,802:INFO: - Computing loss (training)
2022-11-25 00:56:59,003:INFO: Dataset: hotel               Batch: 1/4	Loss 31.2297 (31.2297)
2022-11-25 00:56:59,005:INFO: Dataset: hotel               Batch: 2/4	Loss 31.2269 (31.2283)
2022-11-25 00:56:59,007:INFO: Dataset: hotel               Batch: 3/4	Loss 31.2380 (31.2315)
2022-11-25 00:56:59,009:INFO: Dataset: hotel               Batch: 4/4	Loss 31.2308 (31.2314)
2022-11-25 00:56:59,264:INFO: Dataset: univ                Batch:  1/15	Loss 28.2776 (28.2776)
2022-11-25 00:56:59,268:INFO: Dataset: univ                Batch:  2/15	Loss 28.2726 (28.2752)
2022-11-25 00:56:59,269:INFO: Dataset: univ                Batch:  3/15	Loss 28.2739 (28.2748)
2022-11-25 00:56:59,272:INFO: Dataset: univ                Batch:  4/15	Loss 28.2633 (28.2722)
2022-11-25 00:56:59,339:INFO: Dataset: univ                Batch:  5/15	Loss 28.2625 (28.2702)
2022-11-25 00:56:59,346:INFO: Dataset: univ                Batch:  6/15	Loss 28.2716 (28.2705)
2022-11-25 00:56:59,347:INFO: Dataset: univ                Batch:  7/15	Loss 28.2619 (28.2693)
2022-11-25 00:56:59,348:INFO: Dataset: univ                Batch:  8/15	Loss 28.2430 (28.2657)
2022-11-25 00:56:59,349:INFO: Dataset: univ                Batch:  9/15	Loss 28.2511 (28.2641)
2022-11-25 00:56:59,350:INFO: Dataset: univ                Batch: 10/15	Loss 28.2634 (28.2640)
2022-11-25 00:56:59,352:INFO: Dataset: univ                Batch: 11/15	Loss 28.2396 (28.2619)
2022-11-25 00:56:59,354:INFO: Dataset: univ                Batch: 12/15	Loss 28.2511 (28.2609)
2022-11-25 00:56:59,355:INFO: Dataset: univ                Batch: 13/15	Loss 28.2437 (28.2596)
2022-11-25 00:56:59,356:INFO: Dataset: univ                Batch: 14/15	Loss 28.2300 (28.2573)
2022-11-25 00:56:59,357:INFO: Dataset: univ                Batch: 15/15	Loss 28.2066 (28.2567)
2022-11-25 00:56:59,627:INFO: Dataset: zara1               Batch: 1/8	Loss 34.8953 (34.8953)
2022-11-25 00:56:59,630:INFO: Dataset: zara1               Batch: 2/8	Loss 34.9024 (34.8991)
2022-11-25 00:56:59,631:INFO: Dataset: zara1               Batch: 3/8	Loss 34.9257 (34.9081)
2022-11-25 00:56:59,633:INFO: Dataset: zara1               Batch: 4/8	Loss 34.8946 (34.9050)
2022-11-25 00:56:59,635:INFO: Dataset: zara1               Batch: 5/8	Loss 34.9122 (34.9063)
2022-11-25 00:56:59,648:INFO: Dataset: zara1               Batch: 6/8	Loss 34.8774 (34.9016)
2022-11-25 00:56:59,649:INFO: Dataset: zara1               Batch: 7/8	Loss 34.8710 (34.8970)
2022-11-25 00:56:59,650:INFO: Dataset: zara1               Batch: 8/8	Loss 34.8874 (34.8960)
2022-11-25 00:56:59,902:INFO: Dataset: zara2               Batch:  1/18	Loss 28.8391 (28.8391)
2022-11-25 00:56:59,954:INFO: Dataset: zara2               Batch:  2/18	Loss 28.8388 (28.8390)
2022-11-25 00:56:59,955:INFO: Dataset: zara2               Batch:  3/18	Loss 28.8772 (28.8527)
2022-11-25 00:56:59,956:INFO: Dataset: zara2               Batch:  4/18	Loss 28.8316 (28.8471)
2022-11-25 00:56:59,957:INFO: Dataset: zara2               Batch:  5/18	Loss 28.8353 (28.8447)
2022-11-25 00:56:59,960:INFO: Dataset: zara2               Batch:  6/18	Loss 28.8417 (28.8443)
2022-11-25 00:56:59,961:INFO: Dataset: zara2               Batch:  7/18	Loss 28.8313 (28.8423)
2022-11-25 00:56:59,961:INFO: Dataset: zara2               Batch:  8/18	Loss 28.8423 (28.8423)
2022-11-25 00:56:59,962:INFO: Dataset: zara2               Batch:  9/18	Loss 28.8334 (28.8413)
2022-11-25 00:56:59,963:INFO: Dataset: zara2               Batch: 10/18	Loss 28.8003 (28.8373)
2022-11-25 00:56:59,964:INFO: Dataset: zara2               Batch: 11/18	Loss 28.8129 (28.8350)
2022-11-25 00:56:59,966:INFO: Dataset: zara2               Batch: 12/18	Loss 28.8145 (28.8333)
2022-11-25 00:56:59,966:INFO: Dataset: zara2               Batch: 13/18	Loss 28.7795 (28.8295)
2022-11-25 00:56:59,968:INFO: Dataset: zara2               Batch: 14/18	Loss 28.8040 (28.8276)
2022-11-25 00:56:59,969:INFO: Dataset: zara2               Batch: 15/18	Loss 28.7840 (28.8247)
2022-11-25 00:56:59,971:INFO: Dataset: zara2               Batch: 16/18	Loss 28.8088 (28.8237)
2022-11-25 00:56:59,972:INFO: Dataset: zara2               Batch: 17/18	Loss 28.7712 (28.8205)
2022-11-25 00:56:59,974:INFO: Dataset: zara2               Batch: 18/18	Loss 28.7939 (28.8194)
2022-11-25 00:57:00,025:INFO: - Computing ADE (validation o)
2022-11-25 00:57:00,294:INFO: 		 ADE on eth                       dataset:	 1.0977354049682617
2022-11-25 00:57:00,294:INFO: Average validation o:	ADE  1.0977	FDE  2.1741
2022-11-25 00:57:00,295:INFO: - Computing loss (validation)
2022-11-25 00:57:00,484:INFO: Dataset: hotel               Batch: 1/2	Loss 31.2516 (31.2516)
2022-11-25 00:57:00,485:INFO: Dataset: hotel               Batch: 2/2	Loss 31.2476 (31.2514)
2022-11-25 00:57:00,734:INFO: Dataset: univ                Batch: 1/3	Loss 28.1022 (28.1022)
2022-11-25 00:57:00,747:INFO: Dataset: univ                Batch: 2/3	Loss 28.0855 (28.0944)
2022-11-25 00:57:00,748:INFO: Dataset: univ                Batch: 3/3	Loss 28.0935 (28.0942)
2022-11-25 00:57:00,994:INFO: Dataset: zara1               Batch: 1/2	Loss 34.6893 (34.6893)
2022-11-25 00:57:01,016:INFO: Dataset: zara1               Batch: 2/2	Loss 34.6999 (34.6920)
2022-11-25 00:57:01,265:INFO: Dataset: zara2               Batch: 1/5	Loss 28.7923 (28.7923)
2022-11-25 00:57:01,284:INFO: Dataset: zara2               Batch: 2/5	Loss 28.8426 (28.8186)
2022-11-25 00:57:01,285:INFO: Dataset: zara2               Batch: 3/5	Loss 28.8140 (28.8170)
2022-11-25 00:57:01,285:INFO: Dataset: zara2               Batch: 4/5	Loss 28.8308 (28.8205)
2022-11-25 00:57:01,288:INFO: Dataset: zara2               Batch: 5/5	Loss 28.8097 (28.8183)
2022-11-25 00:57:01,343:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_553.pth.tar
2022-11-25 00:57:01,343:INFO: 
===> EPOCH: 554 (P4)
2022-11-25 00:57:01,343:INFO: - Computing loss (training)
2022-11-25 00:57:01,550:INFO: Dataset: hotel               Batch: 1/4	Loss 31.2141 (31.2141)
2022-11-25 00:57:01,552:INFO: Dataset: hotel               Batch: 2/4	Loss 31.2107 (31.2125)
2022-11-25 00:57:01,554:INFO: Dataset: hotel               Batch: 3/4	Loss 31.2159 (31.2135)
2022-11-25 00:57:01,555:INFO: Dataset: hotel               Batch: 4/4	Loss 31.2053 (31.2123)
2022-11-25 00:57:01,823:INFO: Dataset: univ                Batch:  1/15	Loss 28.1242 (28.1242)
2022-11-25 00:57:01,829:INFO: Dataset: univ                Batch:  2/15	Loss 28.1143 (28.1191)
2022-11-25 00:57:01,831:INFO: Dataset: univ                Batch:  3/15	Loss 28.1274 (28.1218)
2022-11-25 00:57:01,859:INFO: Dataset: univ                Batch:  4/15	Loss 28.1231 (28.1222)
2022-11-25 00:57:01,860:INFO: Dataset: univ                Batch:  5/15	Loss 28.0970 (28.1176)
2022-11-25 00:57:01,863:INFO: Dataset: univ                Batch:  6/15	Loss 28.1180 (28.1177)
2022-11-25 00:57:01,864:INFO: Dataset: univ                Batch:  7/15	Loss 28.1079 (28.1163)
2022-11-25 00:57:01,865:INFO: Dataset: univ                Batch:  8/15	Loss 28.0880 (28.1127)
2022-11-25 00:57:01,866:INFO: Dataset: univ                Batch:  9/15	Loss 28.0976 (28.1110)
2022-11-25 00:57:01,867:INFO: Dataset: univ                Batch: 10/15	Loss 28.0741 (28.1076)
2022-11-25 00:57:01,869:INFO: Dataset: univ                Batch: 11/15	Loss 28.1013 (28.1070)
2022-11-25 00:57:01,871:INFO: Dataset: univ                Batch: 12/15	Loss 28.0952 (28.1059)
2022-11-25 00:57:01,872:INFO: Dataset: univ                Batch: 13/15	Loss 28.0634 (28.1027)
2022-11-25 00:57:01,873:INFO: Dataset: univ                Batch: 14/15	Loss 28.0775 (28.1007)
2022-11-25 00:57:01,874:INFO: Dataset: univ                Batch: 15/15	Loss 28.0930 (28.1006)
2022-11-25 00:57:02,132:INFO: Dataset: zara1               Batch: 1/8	Loss 34.7927 (34.7927)
2022-11-25 00:57:02,139:INFO: Dataset: zara1               Batch: 2/8	Loss 34.7920 (34.7924)
2022-11-25 00:57:02,140:INFO: Dataset: zara1               Batch: 3/8	Loss 34.7693 (34.7852)
2022-11-25 00:57:02,141:INFO: Dataset: zara1               Batch: 4/8	Loss 34.7744 (34.7826)
2022-11-25 00:57:02,142:INFO: Dataset: zara1               Batch: 5/8	Loss 34.7829 (34.7827)
2022-11-25 00:57:02,183:INFO: Dataset: zara1               Batch: 6/8	Loss 34.7691 (34.7805)
2022-11-25 00:57:02,186:INFO: Dataset: zara1               Batch: 7/8	Loss 34.7641 (34.7780)
2022-11-25 00:57:02,190:INFO: Dataset: zara1               Batch: 8/8	Loss 34.7556 (34.7756)
2022-11-25 00:57:02,474:INFO: Dataset: zara2               Batch:  1/18	Loss 28.6725 (28.6725)
2022-11-25 00:57:02,477:INFO: Dataset: zara2               Batch:  2/18	Loss 28.6660 (28.6693)
2022-11-25 00:57:02,517:INFO: Dataset: zara2               Batch:  3/18	Loss 28.6738 (28.6710)
2022-11-25 00:57:02,518:INFO: Dataset: zara2               Batch:  4/18	Loss 28.6304 (28.6615)
2022-11-25 00:57:02,519:INFO: Dataset: zara2               Batch:  5/18	Loss 28.6299 (28.6553)
2022-11-25 00:57:02,545:INFO: Dataset: zara2               Batch:  6/18	Loss 28.6339 (28.6518)
2022-11-25 00:57:02,546:INFO: Dataset: zara2               Batch:  7/18	Loss 28.6601 (28.6530)
2022-11-25 00:57:02,547:INFO: Dataset: zara2               Batch:  8/18	Loss 28.6411 (28.6515)
2022-11-25 00:57:02,548:INFO: Dataset: zara2               Batch:  9/18	Loss 28.6706 (28.6536)
2022-11-25 00:57:02,549:INFO: Dataset: zara2               Batch: 10/18	Loss 28.5897 (28.6471)
2022-11-25 00:57:02,549:INFO: Dataset: zara2               Batch: 11/18	Loss 28.6173 (28.6441)
2022-11-25 00:57:02,551:INFO: Dataset: zara2               Batch: 12/18	Loss 28.6648 (28.6458)
2022-11-25 00:57:02,552:INFO: Dataset: zara2               Batch: 13/18	Loss 28.6085 (28.6427)
2022-11-25 00:57:02,553:INFO: Dataset: zara2               Batch: 14/18	Loss 28.5768 (28.6382)
2022-11-25 00:57:02,554:INFO: Dataset: zara2               Batch: 15/18	Loss 28.5777 (28.6345)
2022-11-25 00:57:02,555:INFO: Dataset: zara2               Batch: 16/18	Loss 28.5915 (28.6315)
2022-11-25 00:57:02,556:INFO: Dataset: zara2               Batch: 17/18	Loss 28.5738 (28.6280)
2022-11-25 00:57:02,558:INFO: Dataset: zara2               Batch: 18/18	Loss 28.5749 (28.6253)
2022-11-25 00:57:02,604:INFO: - Computing ADE (validation o)
2022-11-25 00:57:02,872:INFO: 		 ADE on eth                       dataset:	 1.1409118175506592
2022-11-25 00:57:02,872:INFO: Average validation o:	ADE  1.1409	FDE  2.1860
2022-11-25 00:57:02,873:INFO: - Computing loss (validation)
2022-11-25 00:57:03,066:INFO: Dataset: hotel               Batch: 1/2	Loss 31.2318 (31.2318)
2022-11-25 00:57:03,067:INFO: Dataset: hotel               Batch: 2/2	Loss 31.2564 (31.2333)
2022-11-25 00:57:03,315:INFO: Dataset: univ                Batch: 1/3	Loss 27.9164 (27.9164)
2022-11-25 00:57:03,317:INFO: Dataset: univ                Batch: 2/3	Loss 27.9034 (27.9103)
2022-11-25 00:57:03,317:INFO: Dataset: univ                Batch: 3/3	Loss 27.8933 (27.9044)
2022-11-25 00:57:03,562:INFO: Dataset: zara1               Batch: 1/2	Loss 34.5549 (34.5549)
2022-11-25 00:57:03,564:INFO: Dataset: zara1               Batch: 2/2	Loss 34.5412 (34.5511)
2022-11-25 00:57:03,800:INFO: Dataset: zara2               Batch: 1/5	Loss 28.6534 (28.6534)
2022-11-25 00:57:03,800:INFO: Dataset: zara2               Batch: 2/5	Loss 28.6468 (28.6503)
2022-11-25 00:57:03,801:INFO: Dataset: zara2               Batch: 3/5	Loss 28.6413 (28.6472)
2022-11-25 00:57:03,802:INFO: Dataset: zara2               Batch: 4/5	Loss 28.5938 (28.6340)
2022-11-25 00:57:03,803:INFO: Dataset: zara2               Batch: 5/5	Loss 28.6049 (28.6280)
2022-11-25 00:57:03,858:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_554.pth.tar
2022-11-25 00:57:03,858:INFO: 
===> EPOCH: 555 (P4)
2022-11-25 00:57:03,858:INFO: - Computing loss (training)
2022-11-25 00:57:04,051:INFO: Dataset: hotel               Batch: 1/4	Loss 31.2054 (31.2054)
2022-11-25 00:57:04,054:INFO: Dataset: hotel               Batch: 2/4	Loss 31.1951 (31.2003)
2022-11-25 00:57:04,068:INFO: Dataset: hotel               Batch: 3/4	Loss 31.1695 (31.1895)
2022-11-25 00:57:04,072:INFO: Dataset: hotel               Batch: 4/4	Loss 31.1879 (31.1892)
2022-11-25 00:57:04,329:INFO: Dataset: univ                Batch:  1/15	Loss 27.9521 (27.9521)
2022-11-25 00:57:04,385:INFO: Dataset: univ                Batch:  2/15	Loss 27.9445 (27.9486)
2022-11-25 00:57:04,386:INFO: Dataset: univ                Batch:  3/15	Loss 27.9392 (27.9453)
2022-11-25 00:57:04,388:INFO: Dataset: univ                Batch:  4/15	Loss 27.9182 (27.9386)
2022-11-25 00:57:04,390:INFO: Dataset: univ                Batch:  5/15	Loss 27.9281 (27.9366)
2022-11-25 00:57:04,393:INFO: Dataset: univ                Batch:  6/15	Loss 27.9279 (27.9352)
2022-11-25 00:57:04,394:INFO: Dataset: univ                Batch:  7/15	Loss 27.9033 (27.9310)
2022-11-25 00:57:04,395:INFO: Dataset: univ                Batch:  8/15	Loss 27.9146 (27.9289)
2022-11-25 00:57:04,396:INFO: Dataset: univ                Batch:  9/15	Loss 27.8899 (27.9245)
2022-11-25 00:57:04,397:INFO: Dataset: univ                Batch: 10/15	Loss 27.9138 (27.9234)
2022-11-25 00:57:04,399:INFO: Dataset: univ                Batch: 11/15	Loss 27.8925 (27.9206)
2022-11-25 00:57:04,401:INFO: Dataset: univ                Batch: 12/15	Loss 27.8840 (27.9176)
2022-11-25 00:57:04,402:INFO: Dataset: univ                Batch: 13/15	Loss 27.8677 (27.9138)
2022-11-25 00:57:04,404:INFO: Dataset: univ                Batch: 14/15	Loss 27.8706 (27.9107)
2022-11-25 00:57:04,405:INFO: Dataset: univ                Batch: 15/15	Loss 27.8880 (27.9102)
2022-11-25 00:57:04,674:INFO: Dataset: zara1               Batch: 1/8	Loss 34.6154 (34.6154)
2022-11-25 00:57:04,675:INFO: Dataset: zara1               Batch: 2/8	Loss 34.6327 (34.6240)
2022-11-25 00:57:04,689:INFO: Dataset: zara1               Batch: 3/8	Loss 34.6659 (34.6373)
2022-11-25 00:57:04,690:INFO: Dataset: zara1               Batch: 4/8	Loss 34.6357 (34.6369)
2022-11-25 00:57:04,691:INFO: Dataset: zara1               Batch: 5/8	Loss 34.6282 (34.6353)
2022-11-25 00:57:04,736:INFO: Dataset: zara1               Batch: 6/8	Loss 34.6383 (34.6358)
2022-11-25 00:57:04,737:INFO: Dataset: zara1               Batch: 7/8	Loss 34.6220 (34.6339)
2022-11-25 00:57:04,738:INFO: Dataset: zara1               Batch: 8/8	Loss 34.6043 (34.6303)
2022-11-25 00:57:04,984:INFO: Dataset: zara2               Batch:  1/18	Loss 28.4533 (28.4533)
2022-11-25 00:57:04,990:INFO: Dataset: zara2               Batch:  2/18	Loss 28.3766 (28.4126)
2022-11-25 00:57:04,999:INFO: Dataset: zara2               Batch:  3/18	Loss 28.4351 (28.4206)
2022-11-25 00:57:05,000:INFO: Dataset: zara2               Batch:  4/18	Loss 28.4197 (28.4204)
2022-11-25 00:57:05,027:INFO: Dataset: zara2               Batch:  5/18	Loss 28.4278 (28.4220)
2022-11-25 00:57:05,041:INFO: Dataset: zara2               Batch:  6/18	Loss 28.4537 (28.4279)
2022-11-25 00:57:05,042:INFO: Dataset: zara2               Batch:  7/18	Loss 28.3733 (28.4196)
2022-11-25 00:57:05,043:INFO: Dataset: zara2               Batch:  8/18	Loss 28.3411 (28.4100)
2022-11-25 00:57:05,044:INFO: Dataset: zara2               Batch:  9/18	Loss 28.3992 (28.4089)
2022-11-25 00:57:05,045:INFO: Dataset: zara2               Batch: 10/18	Loss 28.3902 (28.4070)
2022-11-25 00:57:05,046:INFO: Dataset: zara2               Batch: 11/18	Loss 28.3502 (28.4017)
2022-11-25 00:57:05,047:INFO: Dataset: zara2               Batch: 12/18	Loss 28.3947 (28.4012)
2022-11-25 00:57:05,048:INFO: Dataset: zara2               Batch: 13/18	Loss 28.3732 (28.3991)
2022-11-25 00:57:05,049:INFO: Dataset: zara2               Batch: 14/18	Loss 28.3673 (28.3967)
2022-11-25 00:57:05,050:INFO: Dataset: zara2               Batch: 15/18	Loss 28.3056 (28.3912)
2022-11-25 00:57:05,051:INFO: Dataset: zara2               Batch: 16/18	Loss 28.3565 (28.3890)
2022-11-25 00:57:05,052:INFO: Dataset: zara2               Batch: 17/18	Loss 28.3559 (28.3872)
2022-11-25 00:57:05,054:INFO: Dataset: zara2               Batch: 18/18	Loss 28.3318 (28.3850)
2022-11-25 00:57:05,098:INFO: - Computing ADE (validation o)
2022-11-25 00:57:05,373:INFO: 		 ADE on eth                       dataset:	 1.1863036155700684
2022-11-25 00:57:05,373:INFO: Average validation o:	ADE  1.1863	FDE  2.2031
2022-11-25 00:57:05,374:INFO: - Computing loss (validation)
2022-11-25 00:57:05,574:INFO: Dataset: hotel               Batch: 1/2	Loss 31.2184 (31.2184)
2022-11-25 00:57:05,574:INFO: Dataset: hotel               Batch: 2/2	Loss 31.1598 (31.2122)
2022-11-25 00:57:05,822:INFO: Dataset: univ                Batch: 1/3	Loss 27.6833 (27.6833)
2022-11-25 00:57:05,823:INFO: Dataset: univ                Batch: 2/3	Loss 27.6408 (27.6630)
2022-11-25 00:57:05,824:INFO: Dataset: univ                Batch: 3/3	Loss 27.6685 (27.6646)
2022-11-25 00:57:06,066:INFO: Dataset: zara1               Batch: 1/2	Loss 34.3624 (34.3624)
2022-11-25 00:57:06,066:INFO: Dataset: zara1               Batch: 2/2	Loss 34.3932 (34.3694)
2022-11-25 00:57:06,330:INFO: Dataset: zara2               Batch: 1/5	Loss 28.4002 (28.4002)
2022-11-25 00:57:06,332:INFO: Dataset: zara2               Batch: 2/5	Loss 28.3903 (28.3955)
2022-11-25 00:57:06,345:INFO: Dataset: zara2               Batch: 3/5	Loss 28.3889 (28.3933)
2022-11-25 00:57:06,345:INFO: Dataset: zara2               Batch: 4/5	Loss 28.3692 (28.3874)
2022-11-25 00:57:06,347:INFO: Dataset: zara2               Batch: 5/5	Loss 28.3776 (28.3856)
2022-11-25 00:57:06,399:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_555.pth.tar
2022-11-25 00:57:06,399:INFO: 
===> EPOCH: 556 (P4)
2022-11-25 00:57:06,400:INFO: - Computing loss (training)
2022-11-25 00:57:06,615:INFO: Dataset: hotel               Batch: 1/4	Loss 31.1526 (31.1526)
2022-11-25 00:57:06,616:INFO: Dataset: hotel               Batch: 2/4	Loss 31.1612 (31.1568)
2022-11-25 00:57:06,618:INFO: Dataset: hotel               Batch: 3/4	Loss 31.1742 (31.1625)
2022-11-25 00:57:06,620:INFO: Dataset: hotel               Batch: 4/4	Loss 31.1647 (31.1628)
2022-11-25 00:57:06,885:INFO: Dataset: univ                Batch:  1/15	Loss 27.7096 (27.7096)
2022-11-25 00:57:06,887:INFO: Dataset: univ                Batch:  2/15	Loss 27.7095 (27.7095)
2022-11-25 00:57:06,890:INFO: Dataset: univ                Batch:  3/15	Loss 27.7149 (27.7114)
2022-11-25 00:57:06,893:INFO: Dataset: univ                Batch:  4/15	Loss 27.6822 (27.7042)
2022-11-25 00:57:06,924:INFO: Dataset: univ                Batch:  5/15	Loss 27.6961 (27.7028)
2022-11-25 00:57:06,927:INFO: Dataset: univ                Batch:  6/15	Loss 27.6663 (27.6967)
2022-11-25 00:57:06,928:INFO: Dataset: univ                Batch:  7/15	Loss 27.6641 (27.6917)
2022-11-25 00:57:06,929:INFO: Dataset: univ                Batch:  8/15	Loss 27.6601 (27.6879)
2022-11-25 00:57:06,931:INFO: Dataset: univ                Batch:  9/15	Loss 27.6543 (27.6845)
2022-11-25 00:57:06,932:INFO: Dataset: univ                Batch: 10/15	Loss 27.6539 (27.6817)
2022-11-25 00:57:06,933:INFO: Dataset: univ                Batch: 11/15	Loss 27.6515 (27.6790)
2022-11-25 00:57:06,934:INFO: Dataset: univ                Batch: 12/15	Loss 27.6209 (27.6738)
2022-11-25 00:57:06,936:INFO: Dataset: univ                Batch: 13/15	Loss 27.6378 (27.6711)
2022-11-25 00:57:06,937:INFO: Dataset: univ                Batch: 14/15	Loss 27.6288 (27.6686)
2022-11-25 00:57:06,938:INFO: Dataset: univ                Batch: 15/15	Loss 27.5827 (27.6674)
2022-11-25 00:57:07,192:INFO: Dataset: zara1               Batch: 1/8	Loss 34.4850 (34.4850)
2022-11-25 00:57:07,195:INFO: Dataset: zara1               Batch: 2/8	Loss 34.4447 (34.4638)
2022-11-25 00:57:07,202:INFO: Dataset: zara1               Batch: 3/8	Loss 34.4623 (34.4633)
2022-11-25 00:57:07,205:INFO: Dataset: zara1               Batch: 4/8	Loss 34.4372 (34.4571)
2022-11-25 00:57:07,206:INFO: Dataset: zara1               Batch: 5/8	Loss 34.4145 (34.4493)
2022-11-25 00:57:07,252:INFO: Dataset: zara1               Batch: 6/8	Loss 34.4416 (34.4480)
2022-11-25 00:57:07,255:INFO: Dataset: zara1               Batch: 7/8	Loss 34.4510 (34.4484)
2022-11-25 00:57:07,259:INFO: Dataset: zara1               Batch: 8/8	Loss 34.4054 (34.4440)
2022-11-25 00:57:07,547:INFO: Dataset: zara2               Batch:  1/18	Loss 28.1229 (28.1229)
2022-11-25 00:57:07,549:INFO: Dataset: zara2               Batch:  2/18	Loss 28.1665 (28.1448)
2022-11-25 00:57:07,552:INFO: Dataset: zara2               Batch:  3/18	Loss 28.1323 (28.1410)
2022-11-25 00:57:07,553:INFO: Dataset: zara2               Batch:  4/18	Loss 28.1408 (28.1409)
2022-11-25 00:57:07,592:INFO: Dataset: zara2               Batch:  5/18	Loss 28.0923 (28.1314)
2022-11-25 00:57:07,597:INFO: Dataset: zara2               Batch:  6/18	Loss 28.0728 (28.1217)
2022-11-25 00:57:07,598:INFO: Dataset: zara2               Batch:  7/18	Loss 28.1162 (28.1209)
2022-11-25 00:57:07,599:INFO: Dataset: zara2               Batch:  8/18	Loss 28.1140 (28.1201)
2022-11-25 00:57:07,600:INFO: Dataset: zara2               Batch:  9/18	Loss 28.0715 (28.1147)
2022-11-25 00:57:07,601:INFO: Dataset: zara2               Batch: 10/18	Loss 28.0271 (28.1054)
2022-11-25 00:57:07,602:INFO: Dataset: zara2               Batch: 11/18	Loss 28.0509 (28.1006)
2022-11-25 00:57:07,603:INFO: Dataset: zara2               Batch: 12/18	Loss 28.0721 (28.0983)
2022-11-25 00:57:07,604:INFO: Dataset: zara2               Batch: 13/18	Loss 28.0025 (28.0915)
2022-11-25 00:57:07,605:INFO: Dataset: zara2               Batch: 14/18	Loss 28.0600 (28.0892)
2022-11-25 00:57:07,606:INFO: Dataset: zara2               Batch: 15/18	Loss 28.0241 (28.0849)
2022-11-25 00:57:07,607:INFO: Dataset: zara2               Batch: 16/18	Loss 27.9412 (28.0764)
2022-11-25 00:57:07,608:INFO: Dataset: zara2               Batch: 17/18	Loss 27.9729 (28.0700)
2022-11-25 00:57:07,609:INFO: Dataset: zara2               Batch: 18/18	Loss 27.9939 (28.0666)
2022-11-25 00:57:07,667:INFO: - Computing ADE (validation o)
2022-11-25 00:57:07,934:INFO: 		 ADE on eth                       dataset:	 1.2043230533599854
2022-11-25 00:57:07,934:INFO: Average validation o:	ADE  1.2043	FDE  2.3347
2022-11-25 00:57:07,935:INFO: - Computing loss (validation)
2022-11-25 00:57:08,125:INFO: Dataset: hotel               Batch: 1/2	Loss 31.1896 (31.1896)
2022-11-25 00:57:08,126:INFO: Dataset: hotel               Batch: 2/2	Loss 31.1753 (31.1886)
2022-11-25 00:57:08,389:INFO: Dataset: univ                Batch: 1/3	Loss 27.3771 (27.3771)
2022-11-25 00:57:08,390:INFO: Dataset: univ                Batch: 2/3	Loss 27.3359 (27.3571)
2022-11-25 00:57:08,390:INFO: Dataset: univ                Batch: 3/3	Loss 27.3264 (27.3479)
2022-11-25 00:57:08,636:INFO: Dataset: zara1               Batch: 1/2	Loss 34.1286 (34.1286)
2022-11-25 00:57:08,639:INFO: Dataset: zara1               Batch: 2/2	Loss 34.1176 (34.1259)
2022-11-25 00:57:08,912:INFO: Dataset: zara2               Batch: 1/5	Loss 28.0299 (28.0299)
2022-11-25 00:57:08,920:INFO: Dataset: zara2               Batch: 2/5	Loss 28.0840 (28.0559)
2022-11-25 00:57:08,924:INFO: Dataset: zara2               Batch: 3/5	Loss 28.0497 (28.0540)
2022-11-25 00:57:08,924:INFO: Dataset: zara2               Batch: 4/5	Loss 28.0928 (28.0639)
2022-11-25 00:57:08,925:INFO: Dataset: zara2               Batch: 5/5	Loss 28.0624 (28.0636)
2022-11-25 00:57:08,976:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_556.pth.tar
2022-11-25 00:57:08,977:INFO: 
===> EPOCH: 557 (P4)
2022-11-25 00:57:08,977:INFO: - Computing loss (training)
2022-11-25 00:57:09,188:INFO: Dataset: hotel               Batch: 1/4	Loss 31.1512 (31.1512)
2022-11-25 00:57:09,190:INFO: Dataset: hotel               Batch: 2/4	Loss 31.1389 (31.1453)
2022-11-25 00:57:09,192:INFO: Dataset: hotel               Batch: 3/4	Loss 31.1340 (31.1418)
2022-11-25 00:57:09,193:INFO: Dataset: hotel               Batch: 4/4	Loss 31.0974 (31.1343)
2022-11-25 00:57:09,442:INFO: Dataset: univ                Batch:  1/15	Loss 27.4014 (27.4014)
2022-11-25 00:57:09,446:INFO: Dataset: univ                Batch:  2/15	Loss 27.3740 (27.3871)
2022-11-25 00:57:09,547:INFO: Dataset: univ                Batch:  3/15	Loss 27.3805 (27.3849)
2022-11-25 00:57:09,552:INFO: Dataset: univ                Batch:  4/15	Loss 27.4059 (27.3910)
2022-11-25 00:57:09,553:INFO: Dataset: univ                Batch:  5/15	Loss 27.3423 (27.3813)
2022-11-25 00:57:09,557:INFO: Dataset: univ                Batch:  6/15	Loss 27.3490 (27.3761)
2022-11-25 00:57:09,558:INFO: Dataset: univ                Batch:  7/15	Loss 27.3452 (27.3716)
2022-11-25 00:57:09,559:INFO: Dataset: univ                Batch:  8/15	Loss 27.3560 (27.3697)
2022-11-25 00:57:09,560:INFO: Dataset: univ                Batch:  9/15	Loss 27.3499 (27.3672)
2022-11-25 00:57:09,562:INFO: Dataset: univ                Batch: 10/15	Loss 27.2776 (27.3589)
2022-11-25 00:57:09,563:INFO: Dataset: univ                Batch: 11/15	Loss 27.3240 (27.3556)
2022-11-25 00:57:09,564:INFO: Dataset: univ                Batch: 12/15	Loss 27.3091 (27.3517)
2022-11-25 00:57:09,565:INFO: Dataset: univ                Batch: 13/15	Loss 27.2445 (27.3440)
2022-11-25 00:57:09,567:INFO: Dataset: univ                Batch: 14/15	Loss 27.2890 (27.3396)
2022-11-25 00:57:09,568:INFO: Dataset: univ                Batch: 15/15	Loss 27.3277 (27.3394)
2022-11-25 00:57:09,833:INFO: Dataset: zara1               Batch: 1/8	Loss 34.2342 (34.2342)
2022-11-25 00:57:09,834:INFO: Dataset: zara1               Batch: 2/8	Loss 34.2249 (34.2291)
2022-11-25 00:57:09,835:INFO: Dataset: zara1               Batch: 3/8	Loss 34.1941 (34.2174)
2022-11-25 00:57:09,836:INFO: Dataset: zara1               Batch: 4/8	Loss 34.1875 (34.2093)
2022-11-25 00:57:09,839:INFO: Dataset: zara1               Batch: 5/8	Loss 34.1876 (34.2051)
2022-11-25 00:57:09,886:INFO: Dataset: zara1               Batch: 6/8	Loss 34.1678 (34.1985)
2022-11-25 00:57:09,890:INFO: Dataset: zara1               Batch: 7/8	Loss 34.1866 (34.1968)
2022-11-25 00:57:09,893:INFO: Dataset: zara1               Batch: 8/8	Loss 34.1459 (34.1908)
2022-11-25 00:57:10,157:INFO: Dataset: zara2               Batch:  1/18	Loss 27.6753 (27.6753)
2022-11-25 00:57:10,204:INFO: Dataset: zara2               Batch:  2/18	Loss 27.7531 (27.7144)
2022-11-25 00:57:10,205:INFO: Dataset: zara2               Batch:  3/18	Loss 27.7199 (27.7163)
2022-11-25 00:57:10,207:INFO: Dataset: zara2               Batch:  4/18	Loss 27.6986 (27.7120)
2022-11-25 00:57:10,208:INFO: Dataset: zara2               Batch:  5/18	Loss 27.7281 (27.7157)
2022-11-25 00:57:10,212:INFO: Dataset: zara2               Batch:  6/18	Loss 27.6778 (27.7097)
2022-11-25 00:57:10,213:INFO: Dataset: zara2               Batch:  7/18	Loss 27.6786 (27.7056)
2022-11-25 00:57:10,214:INFO: Dataset: zara2               Batch:  8/18	Loss 27.6797 (27.7021)
2022-11-25 00:57:10,215:INFO: Dataset: zara2               Batch:  9/18	Loss 27.5712 (27.6869)
2022-11-25 00:57:10,216:INFO: Dataset: zara2               Batch: 10/18	Loss 27.6001 (27.6775)
2022-11-25 00:57:10,217:INFO: Dataset: zara2               Batch: 11/18	Loss 27.5586 (27.6671)
2022-11-25 00:57:10,219:INFO: Dataset: zara2               Batch: 12/18	Loss 27.6199 (27.6631)
2022-11-25 00:57:10,220:INFO: Dataset: zara2               Batch: 13/18	Loss 27.6587 (27.6627)
2022-11-25 00:57:10,221:INFO: Dataset: zara2               Batch: 14/18	Loss 27.5787 (27.6571)
2022-11-25 00:57:10,223:INFO: Dataset: zara2               Batch: 15/18	Loss 27.5753 (27.6519)
2022-11-25 00:57:10,225:INFO: Dataset: zara2               Batch: 16/18	Loss 27.5191 (27.6433)
2022-11-25 00:57:10,226:INFO: Dataset: zara2               Batch: 17/18	Loss 27.5744 (27.6390)
2022-11-25 00:57:10,228:INFO: Dataset: zara2               Batch: 18/18	Loss 27.5443 (27.6340)
2022-11-25 00:57:10,275:INFO: - Computing ADE (validation o)
2022-11-25 00:57:10,551:INFO: 		 ADE on eth                       dataset:	 1.1072689294815063
2022-11-25 00:57:10,551:INFO: Average validation o:	ADE  1.1073	FDE  2.2047
2022-11-25 00:57:10,552:INFO: - Computing loss (validation)
2022-11-25 00:57:10,758:INFO: Dataset: hotel               Batch: 1/2	Loss 31.1582 (31.1582)
2022-11-25 00:57:10,758:INFO: Dataset: hotel               Batch: 2/2	Loss 31.1932 (31.1608)
2022-11-25 00:57:11,005:INFO: Dataset: univ                Batch: 1/3	Loss 26.8841 (26.8841)
2022-11-25 00:57:11,007:INFO: Dataset: univ                Batch: 2/3	Loss 26.9622 (26.9259)
2022-11-25 00:57:11,008:INFO: Dataset: univ                Batch: 3/3	Loss 26.8852 (26.9149)
2022-11-25 00:57:11,254:INFO: Dataset: zara1               Batch: 1/2	Loss 33.7962 (33.7962)
2022-11-25 00:57:11,255:INFO: Dataset: zara1               Batch: 2/2	Loss 33.7828 (33.7929)
2022-11-25 00:57:11,520:INFO: Dataset: zara2               Batch: 1/5	Loss 27.6572 (27.6572)
2022-11-25 00:57:11,521:INFO: Dataset: zara2               Batch: 2/5	Loss 27.6392 (27.6481)
2022-11-25 00:57:11,522:INFO: Dataset: zara2               Batch: 3/5	Loss 27.6823 (27.6602)
2022-11-25 00:57:11,525:INFO: Dataset: zara2               Batch: 4/5	Loss 27.5374 (27.6293)
2022-11-25 00:57:11,528:INFO: Dataset: zara2               Batch: 5/5	Loss 27.6038 (27.6241)
2022-11-25 00:57:11,588:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_557.pth.tar
2022-11-25 00:57:11,589:INFO: 
===> EPOCH: 558 (P4)
2022-11-25 00:57:11,589:INFO: - Computing loss (training)
2022-11-25 00:57:11,795:INFO: Dataset: hotel               Batch: 1/4	Loss 31.1025 (31.1025)
2022-11-25 00:57:11,797:INFO: Dataset: hotel               Batch: 2/4	Loss 31.0976 (31.1000)
2022-11-25 00:57:11,798:INFO: Dataset: hotel               Batch: 3/4	Loss 31.1049 (31.1015)
2022-11-25 00:57:11,800:INFO: Dataset: hotel               Batch: 4/4	Loss 31.1011 (31.1015)
2022-11-25 00:57:12,077:INFO: Dataset: univ                Batch:  1/15	Loss 26.9687 (26.9687)
2022-11-25 00:57:12,079:INFO: Dataset: univ                Batch:  2/15	Loss 26.9439 (26.9566)
2022-11-25 00:57:12,102:INFO: Dataset: univ                Batch:  3/15	Loss 26.9401 (26.9507)
2022-11-25 00:57:12,103:INFO: Dataset: univ                Batch:  4/15	Loss 26.9342 (26.9465)
2022-11-25 00:57:12,105:INFO: Dataset: univ                Batch:  5/15	Loss 26.9531 (26.9477)
2022-11-25 00:57:12,121:INFO: Dataset: univ                Batch:  6/15	Loss 26.9117 (26.9418)
2022-11-25 00:57:12,122:INFO: Dataset: univ                Batch:  7/15	Loss 26.9123 (26.9378)
2022-11-25 00:57:12,124:INFO: Dataset: univ                Batch:  8/15	Loss 26.9119 (26.9346)
2022-11-25 00:57:12,125:INFO: Dataset: univ                Batch:  9/15	Loss 26.8651 (26.9269)
2022-11-25 00:57:12,126:INFO: Dataset: univ                Batch: 10/15	Loss 26.8664 (26.9211)
2022-11-25 00:57:12,127:INFO: Dataset: univ                Batch: 11/15	Loss 26.8556 (26.9150)
2022-11-25 00:57:12,129:INFO: Dataset: univ                Batch: 12/15	Loss 26.8611 (26.9107)
2022-11-25 00:57:12,130:INFO: Dataset: univ                Batch: 13/15	Loss 26.7718 (26.8997)
2022-11-25 00:57:12,131:INFO: Dataset: univ                Batch: 14/15	Loss 26.8073 (26.8923)
2022-11-25 00:57:12,132:INFO: Dataset: univ                Batch: 15/15	Loss 26.7741 (26.8908)
2022-11-25 00:57:12,402:INFO: Dataset: zara1               Batch: 1/8	Loss 33.8581 (33.8581)
2022-11-25 00:57:12,405:INFO: Dataset: zara1               Batch: 2/8	Loss 33.8946 (33.8772)
2022-11-25 00:57:12,406:INFO: Dataset: zara1               Batch: 3/8	Loss 33.9313 (33.8941)
2022-11-25 00:57:12,408:INFO: Dataset: zara1               Batch: 4/8	Loss 33.8369 (33.8794)
2022-11-25 00:57:12,410:INFO: Dataset: zara1               Batch: 5/8	Loss 33.7883 (33.8607)
2022-11-25 00:57:12,446:INFO: Dataset: zara1               Batch: 6/8	Loss 33.8052 (33.8515)
2022-11-25 00:57:12,447:INFO: Dataset: zara1               Batch: 7/8	Loss 33.8753 (33.8547)
2022-11-25 00:57:12,449:INFO: Dataset: zara1               Batch: 8/8	Loss 33.7589 (33.8443)
2022-11-25 00:57:12,729:INFO: Dataset: zara2               Batch:  1/18	Loss 27.2235 (27.2235)
2022-11-25 00:57:12,784:INFO: Dataset: zara2               Batch:  2/18	Loss 27.1478 (27.1843)
2022-11-25 00:57:12,786:INFO: Dataset: zara2               Batch:  3/18	Loss 27.1430 (27.1713)
2022-11-25 00:57:12,788:INFO: Dataset: zara2               Batch:  4/18	Loss 27.2669 (27.1952)
2022-11-25 00:57:12,790:INFO: Dataset: zara2               Batch:  5/18	Loss 27.1478 (27.1865)
2022-11-25 00:57:12,794:INFO: Dataset: zara2               Batch:  6/18	Loss 27.0856 (27.1702)
2022-11-25 00:57:12,795:INFO: Dataset: zara2               Batch:  7/18	Loss 27.1457 (27.1669)
2022-11-25 00:57:12,796:INFO: Dataset: zara2               Batch:  8/18	Loss 27.1270 (27.1619)
2022-11-25 00:57:12,797:INFO: Dataset: zara2               Batch:  9/18	Loss 27.0667 (27.1504)
2022-11-25 00:57:12,798:INFO: Dataset: zara2               Batch: 10/18	Loss 26.8964 (27.1272)
2022-11-25 00:57:12,799:INFO: Dataset: zara2               Batch: 11/18	Loss 26.9080 (27.1079)
2022-11-25 00:57:12,801:INFO: Dataset: zara2               Batch: 12/18	Loss 26.9826 (27.0976)
2022-11-25 00:57:12,803:INFO: Dataset: zara2               Batch: 13/18	Loss 26.8337 (27.0808)
2022-11-25 00:57:12,805:INFO: Dataset: zara2               Batch: 14/18	Loss 26.9232 (27.0697)
2022-11-25 00:57:12,807:INFO: Dataset: zara2               Batch: 15/18	Loss 26.9787 (27.0632)
2022-11-25 00:57:12,809:INFO: Dataset: zara2               Batch: 16/18	Loss 26.8940 (27.0519)
2022-11-25 00:57:12,811:INFO: Dataset: zara2               Batch: 17/18	Loss 26.8294 (27.0376)
2022-11-25 00:57:12,812:INFO: Dataset: zara2               Batch: 18/18	Loss 26.8914 (27.0294)
2022-11-25 00:57:12,867:INFO: - Computing ADE (validation o)
2022-11-25 00:57:13,213:INFO: 		 ADE on eth                       dataset:	 1.1347630023956299
2022-11-25 00:57:13,213:INFO: Average validation o:	ADE  1.1348	FDE  2.1378
2022-11-25 00:57:13,214:INFO: - Computing loss (validation)
2022-11-25 00:57:13,445:INFO: Dataset: hotel               Batch: 1/2	Loss 31.1314 (31.1314)
2022-11-25 00:57:13,446:INFO: Dataset: hotel               Batch: 2/2	Loss 31.1181 (31.1307)
2022-11-25 00:57:13,770:INFO: Dataset: univ                Batch: 1/3	Loss 26.3186 (26.3186)
2022-11-25 00:57:13,775:INFO: Dataset: univ                Batch: 2/3	Loss 26.3254 (26.3217)
2022-11-25 00:57:13,783:INFO: Dataset: univ                Batch: 3/3	Loss 26.2484 (26.3007)
2022-11-25 00:57:14,097:INFO: Dataset: zara1               Batch: 1/2	Loss 33.3222 (33.3222)
2022-11-25 00:57:14,098:INFO: Dataset: zara1               Batch: 2/2	Loss 33.3412 (33.3269)
2022-11-25 00:57:14,450:INFO: Dataset: zara2               Batch: 1/5	Loss 26.9777 (26.9777)
2022-11-25 00:57:14,452:INFO: Dataset: zara2               Batch: 2/5	Loss 27.0075 (26.9927)
2022-11-25 00:57:14,454:INFO: Dataset: zara2               Batch: 3/5	Loss 27.0907 (27.0258)
2022-11-25 00:57:14,455:INFO: Dataset: zara2               Batch: 4/5	Loss 26.9431 (27.0078)
2022-11-25 00:57:14,457:INFO: Dataset: zara2               Batch: 5/5	Loss 26.9824 (27.0030)
2022-11-25 00:57:14,524:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_558.pth.tar
2022-11-25 00:57:14,524:INFO: 
===> EPOCH: 559 (P4)
2022-11-25 00:57:14,524:INFO: - Computing loss (training)
2022-11-25 00:57:14,752:INFO: Dataset: hotel               Batch: 1/4	Loss 31.0701 (31.0701)
2022-11-25 00:57:14,754:INFO: Dataset: hotel               Batch: 2/4	Loss 31.0633 (31.0668)
2022-11-25 00:57:14,756:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0838 (31.0723)
2022-11-25 00:57:14,758:INFO: Dataset: hotel               Batch: 4/4	Loss 31.0476 (31.0679)
2022-11-25 00:57:15,033:INFO: Dataset: univ                Batch:  1/15	Loss 26.3326 (26.3326)
2022-11-25 00:57:15,036:INFO: Dataset: univ                Batch:  2/15	Loss 26.3024 (26.3175)
2022-11-25 00:57:15,038:INFO: Dataset: univ                Batch:  3/15	Loss 26.3510 (26.3289)
2022-11-25 00:57:15,075:INFO: Dataset: univ                Batch:  4/15	Loss 26.3491 (26.3338)
2022-11-25 00:57:15,076:INFO: Dataset: univ                Batch:  5/15	Loss 26.3088 (26.3286)
2022-11-25 00:57:15,080:INFO: Dataset: univ                Batch:  6/15	Loss 26.3114 (26.3257)
2022-11-25 00:57:15,082:INFO: Dataset: univ                Batch:  7/15	Loss 26.2961 (26.3214)
2022-11-25 00:57:15,083:INFO: Dataset: univ                Batch:  8/15	Loss 26.2474 (26.3129)
2022-11-25 00:57:15,084:INFO: Dataset: univ                Batch:  9/15	Loss 26.1793 (26.2973)
2022-11-25 00:57:15,085:INFO: Dataset: univ                Batch: 10/15	Loss 26.2580 (26.2931)
2022-11-25 00:57:15,087:INFO: Dataset: univ                Batch: 11/15	Loss 26.2033 (26.2848)
2022-11-25 00:57:15,089:INFO: Dataset: univ                Batch: 12/15	Loss 26.1601 (26.2741)
2022-11-25 00:57:15,090:INFO: Dataset: univ                Batch: 13/15	Loss 26.1615 (26.2658)
2022-11-25 00:57:15,091:INFO: Dataset: univ                Batch: 14/15	Loss 26.0734 (26.2543)
2022-11-25 00:57:15,092:INFO: Dataset: univ                Batch: 15/15	Loss 26.0872 (26.2523)
2022-11-25 00:57:15,349:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4144 (33.4144)
2022-11-25 00:57:15,351:INFO: Dataset: zara1               Batch: 2/8	Loss 33.4069 (33.4106)
2022-11-25 00:57:15,353:INFO: Dataset: zara1               Batch: 3/8	Loss 33.3449 (33.3891)
2022-11-25 00:57:15,356:INFO: Dataset: zara1               Batch: 4/8	Loss 33.3252 (33.3725)
2022-11-25 00:57:15,357:INFO: Dataset: zara1               Batch: 5/8	Loss 33.3108 (33.3595)
2022-11-25 00:57:15,385:INFO: Dataset: zara1               Batch: 6/8	Loss 33.3048 (33.3501)
2022-11-25 00:57:15,386:INFO: Dataset: zara1               Batch: 7/8	Loss 33.2558 (33.3358)
2022-11-25 00:57:15,387:INFO: Dataset: zara1               Batch: 8/8	Loss 33.3632 (33.3386)
2022-11-25 00:57:15,645:INFO: Dataset: zara2               Batch:  1/18	Loss 26.3280 (26.3280)
2022-11-25 00:57:15,647:INFO: Dataset: zara2               Batch:  2/18	Loss 26.3473 (26.3371)
2022-11-25 00:57:15,648:INFO: Dataset: zara2               Batch:  3/18	Loss 26.1768 (26.2849)
2022-11-25 00:57:15,685:INFO: Dataset: zara2               Batch:  4/18	Loss 26.3011 (26.2887)
2022-11-25 00:57:15,686:INFO: Dataset: zara2               Batch:  5/18	Loss 26.2398 (26.2773)
2022-11-25 00:57:15,692:INFO: Dataset: zara2               Batch:  6/18	Loss 26.2547 (26.2734)
2022-11-25 00:57:15,694:INFO: Dataset: zara2               Batch:  7/18	Loss 26.1780 (26.2608)
2022-11-25 00:57:15,695:INFO: Dataset: zara2               Batch:  8/18	Loss 26.2375 (26.2575)
2022-11-25 00:57:15,696:INFO: Dataset: zara2               Batch:  9/18	Loss 26.2096 (26.2520)
2022-11-25 00:57:15,697:INFO: Dataset: zara2               Batch: 10/18	Loss 26.2352 (26.2502)
2022-11-25 00:57:15,698:INFO: Dataset: zara2               Batch: 11/18	Loss 26.0519 (26.2325)
2022-11-25 00:57:15,701:INFO: Dataset: zara2               Batch: 12/18	Loss 26.1304 (26.2246)
2022-11-25 00:57:15,702:INFO: Dataset: zara2               Batch: 13/18	Loss 26.0410 (26.2126)
2022-11-25 00:57:15,703:INFO: Dataset: zara2               Batch: 14/18	Loss 26.1703 (26.2097)
2022-11-25 00:57:15,704:INFO: Dataset: zara2               Batch: 15/18	Loss 25.9600 (26.1932)
2022-11-25 00:57:15,706:INFO: Dataset: zara2               Batch: 16/18	Loss 26.1043 (26.1872)
2022-11-25 00:57:15,708:INFO: Dataset: zara2               Batch: 17/18	Loss 25.9866 (26.1751)
2022-11-25 00:57:15,710:INFO: Dataset: zara2               Batch: 18/18	Loss 25.7565 (26.1567)
2022-11-25 00:57:15,765:INFO: - Computing ADE (validation o)
2022-11-25 00:57:16,088:INFO: 		 ADE on eth                       dataset:	 1.1660594940185547
2022-11-25 00:57:16,089:INFO: Average validation o:	ADE  1.1661	FDE  2.2860
2022-11-25 00:57:16,090:INFO: - Computing loss (validation)
2022-11-25 00:57:16,306:INFO: Dataset: hotel               Batch: 1/2	Loss 31.1074 (31.1074)
2022-11-25 00:57:16,308:INFO: Dataset: hotel               Batch: 2/2	Loss 31.1403 (31.1092)
2022-11-25 00:57:16,583:INFO: Dataset: univ                Batch: 1/3	Loss 25.4638 (25.4638)
2022-11-25 00:57:16,583:INFO: Dataset: univ                Batch: 2/3	Loss 25.3984 (25.4336)
2022-11-25 00:57:16,584:INFO: Dataset: univ                Batch: 3/3	Loss 25.3641 (25.4132)
2022-11-25 00:57:16,845:INFO: Dataset: zara1               Batch: 1/2	Loss 32.6617 (32.6617)
2022-11-25 00:57:16,846:INFO: Dataset: zara1               Batch: 2/2	Loss 32.6542 (32.6596)
2022-11-25 00:57:17,124:INFO: Dataset: zara2               Batch: 1/5	Loss 26.0807 (26.0807)
2022-11-25 00:57:17,126:INFO: Dataset: zara2               Batch: 2/5	Loss 26.1301 (26.1048)
2022-11-25 00:57:17,133:INFO: Dataset: zara2               Batch: 3/5	Loss 26.1456 (26.1182)
2022-11-25 00:57:17,134:INFO: Dataset: zara2               Batch: 4/5	Loss 26.1129 (26.1169)
2022-11-25 00:57:17,134:INFO: Dataset: zara2               Batch: 5/5	Loss 26.0394 (26.1035)
2022-11-25 00:57:17,192:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_559.pth.tar
2022-11-25 00:57:17,192:INFO: 
===> EPOCH: 560 (P4)
2022-11-25 00:57:17,192:INFO: - Computing loss (training)
2022-11-25 00:57:17,407:INFO: Dataset: hotel               Batch: 1/4	Loss 31.0501 (31.0501)
2022-11-25 00:57:17,409:INFO: Dataset: hotel               Batch: 2/4	Loss 31.0601 (31.0551)
2022-11-25 00:57:17,410:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0225 (31.0446)
2022-11-25 00:57:17,411:INFO: Dataset: hotel               Batch: 4/4	Loss 31.0672 (31.0484)
2022-11-25 00:57:17,685:INFO: Dataset: univ                Batch:  1/15	Loss 25.5270 (25.5270)
2022-11-25 00:57:17,690:INFO: Dataset: univ                Batch:  2/15	Loss 25.4570 (25.4921)
2022-11-25 00:57:17,691:INFO: Dataset: univ                Batch:  3/15	Loss 25.4382 (25.4749)
2022-11-25 00:57:17,719:INFO: Dataset: univ                Batch:  4/15	Loss 25.4553 (25.4699)
2022-11-25 00:57:17,721:INFO: Dataset: univ                Batch:  5/15	Loss 25.3684 (25.4500)
2022-11-25 00:57:17,725:INFO: Dataset: univ                Batch:  6/15	Loss 25.3810 (25.4387)
2022-11-25 00:57:17,727:INFO: Dataset: univ                Batch:  7/15	Loss 25.3175 (25.4220)
2022-11-25 00:57:17,728:INFO: Dataset: univ                Batch:  8/15	Loss 25.3252 (25.4093)
2022-11-25 00:57:17,729:INFO: Dataset: univ                Batch:  9/15	Loss 25.2957 (25.3968)
2022-11-25 00:57:17,730:INFO: Dataset: univ                Batch: 10/15	Loss 25.2841 (25.3857)
2022-11-25 00:57:17,732:INFO: Dataset: univ                Batch: 11/15	Loss 25.2548 (25.3727)
2022-11-25 00:57:17,734:INFO: Dataset: univ                Batch: 12/15	Loss 25.2015 (25.3595)
2022-11-25 00:57:17,735:INFO: Dataset: univ                Batch: 13/15	Loss 25.1777 (25.3442)
2022-11-25 00:57:17,736:INFO: Dataset: univ                Batch: 14/15	Loss 25.1651 (25.3311)
2022-11-25 00:57:17,737:INFO: Dataset: univ                Batch: 15/15	Loss 25.0460 (25.3277)
2022-11-25 00:57:17,991:INFO: Dataset: zara1               Batch: 1/8	Loss 32.6699 (32.6699)
2022-11-25 00:57:17,993:INFO: Dataset: zara1               Batch: 2/8	Loss 32.6463 (32.6571)
2022-11-25 00:57:17,995:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6382 (32.6504)
2022-11-25 00:57:17,998:INFO: Dataset: zara1               Batch: 4/8	Loss 32.6523 (32.6508)
2022-11-25 00:57:17,999:INFO: Dataset: zara1               Batch: 5/8	Loss 32.6002 (32.6409)
2022-11-25 00:57:18,057:INFO: Dataset: zara1               Batch: 6/8	Loss 32.5367 (32.6237)
2022-11-25 00:57:18,058:INFO: Dataset: zara1               Batch: 7/8	Loss 32.5538 (32.6139)
2022-11-25 00:57:18,059:INFO: Dataset: zara1               Batch: 8/8	Loss 32.5999 (32.6125)
2022-11-25 00:57:18,313:INFO: Dataset: zara2               Batch:  1/18	Loss 25.0663 (25.0663)
2022-11-25 00:57:18,316:INFO: Dataset: zara2               Batch:  2/18	Loss 25.1325 (25.0976)
2022-11-25 00:57:18,318:INFO: Dataset: zara2               Batch:  3/18	Loss 25.2422 (25.1467)
2022-11-25 00:57:18,322:INFO: Dataset: zara2               Batch:  4/18	Loss 25.1687 (25.1525)
2022-11-25 00:57:18,357:INFO: Dataset: zara2               Batch:  5/18	Loss 24.9805 (25.1207)
2022-11-25 00:57:18,362:INFO: Dataset: zara2               Batch:  6/18	Loss 25.1547 (25.1270)
2022-11-25 00:57:18,363:INFO: Dataset: zara2               Batch:  7/18	Loss 24.9684 (25.1039)
2022-11-25 00:57:18,364:INFO: Dataset: zara2               Batch:  8/18	Loss 25.0116 (25.0927)
2022-11-25 00:57:18,365:INFO: Dataset: zara2               Batch:  9/18	Loss 24.8636 (25.0660)
2022-11-25 00:57:18,366:INFO: Dataset: zara2               Batch: 10/18	Loss 25.0802 (25.0676)
2022-11-25 00:57:18,367:INFO: Dataset: zara2               Batch: 11/18	Loss 24.8884 (25.0519)
2022-11-25 00:57:18,369:INFO: Dataset: zara2               Batch: 12/18	Loss 24.8377 (25.0343)
2022-11-25 00:57:18,370:INFO: Dataset: zara2               Batch: 13/18	Loss 24.7969 (25.0165)
2022-11-25 00:57:18,371:INFO: Dataset: zara2               Batch: 14/18	Loss 24.7933 (25.0003)
2022-11-25 00:57:18,372:INFO: Dataset: zara2               Batch: 15/18	Loss 24.7363 (24.9830)
2022-11-25 00:57:18,373:INFO: Dataset: zara2               Batch: 16/18	Loss 24.5769 (24.9577)
2022-11-25 00:57:18,374:INFO: Dataset: zara2               Batch: 17/18	Loss 24.4529 (24.9268)
2022-11-25 00:57:18,376:INFO: Dataset: zara2               Batch: 18/18	Loss 24.3374 (24.9010)
2022-11-25 00:57:18,437:INFO: - Computing ADE (validation o)
2022-11-25 00:57:18,715:INFO: 		 ADE on eth                       dataset:	 1.074313759803772
2022-11-25 00:57:18,715:INFO: Average validation o:	ADE  1.0743	FDE  2.0955
2022-11-25 00:57:18,716:INFO: - Computing loss (validation)
2022-11-25 00:57:18,945:INFO: Dataset: hotel               Batch: 1/2	Loss 31.1221 (31.1221)
2022-11-25 00:57:18,946:INFO: Dataset: hotel               Batch: 2/2	Loss 31.1341 (31.1230)
2022-11-25 00:57:19,246:INFO: Dataset: univ                Batch: 1/3	Loss 24.1590 (24.1590)
2022-11-25 00:57:19,247:INFO: Dataset: univ                Batch: 2/3	Loss 24.1223 (24.1407)
2022-11-25 00:57:19,247:INFO: Dataset: univ                Batch: 3/3	Loss 24.1427 (24.1413)
2022-11-25 00:57:19,542:INFO: Dataset: zara1               Batch: 1/2	Loss 31.7106 (31.7106)
2022-11-25 00:57:19,544:INFO: Dataset: zara1               Batch: 2/2	Loss 31.7388 (31.7178)
2022-11-25 00:57:19,830:INFO: Dataset: zara2               Batch: 1/5	Loss 24.8219 (24.8219)
2022-11-25 00:57:19,832:INFO: Dataset: zara2               Batch: 2/5	Loss 24.7508 (24.7857)
2022-11-25 00:57:19,846:INFO: Dataset: zara2               Batch: 3/5	Loss 24.9222 (24.8328)
2022-11-25 00:57:19,848:INFO: Dataset: zara2               Batch: 4/5	Loss 24.7681 (24.8157)
2022-11-25 00:57:19,848:INFO: Dataset: zara2               Batch: 5/5	Loss 24.7930 (24.8113)
2022-11-25 00:57:19,904:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_560.pth.tar
2022-11-25 00:57:19,904:INFO: 
===> EPOCH: 561 (P4)
2022-11-25 00:57:19,905:INFO: - Computing loss (training)
2022-11-25 00:57:20,106:INFO: Dataset: hotel               Batch: 1/4	Loss 31.0724 (31.0724)
2022-11-25 00:57:20,109:INFO: Dataset: hotel               Batch: 2/4	Loss 31.0795 (31.0761)
2022-11-25 00:57:20,110:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0714 (31.0745)
2022-11-25 00:57:20,112:INFO: Dataset: hotel               Batch: 4/4	Loss 31.0803 (31.0754)
2022-11-25 00:57:20,394:INFO: Dataset: univ                Batch:  1/15	Loss 24.1955 (24.1955)
2022-11-25 00:57:20,396:INFO: Dataset: univ                Batch:  2/15	Loss 24.1941 (24.1948)
2022-11-25 00:57:20,400:INFO: Dataset: univ                Batch:  3/15	Loss 24.1345 (24.1764)
2022-11-25 00:57:20,401:INFO: Dataset: univ                Batch:  4/15	Loss 24.2517 (24.1960)
2022-11-25 00:57:20,437:INFO: Dataset: univ                Batch:  5/15	Loss 24.1109 (24.1803)
2022-11-25 00:57:20,440:INFO: Dataset: univ                Batch:  6/15	Loss 24.0815 (24.1636)
2022-11-25 00:57:20,442:INFO: Dataset: univ                Batch:  7/15	Loss 24.1155 (24.1561)
2022-11-25 00:57:20,443:INFO: Dataset: univ                Batch:  8/15	Loss 24.0304 (24.1406)
2022-11-25 00:57:20,444:INFO: Dataset: univ                Batch:  9/15	Loss 23.9256 (24.1159)
2022-11-25 00:57:20,445:INFO: Dataset: univ                Batch: 10/15	Loss 23.9647 (24.1008)
2022-11-25 00:57:20,448:INFO: Dataset: univ                Batch: 11/15	Loss 23.9593 (24.0871)
2022-11-25 00:57:20,449:INFO: Dataset: univ                Batch: 12/15	Loss 23.7936 (24.0629)
2022-11-25 00:57:20,450:INFO: Dataset: univ                Batch: 13/15	Loss 23.7520 (24.0404)
2022-11-25 00:57:20,451:INFO: Dataset: univ                Batch: 14/15	Loss 23.7003 (24.0156)
2022-11-25 00:57:20,452:INFO: Dataset: univ                Batch: 15/15	Loss 23.6470 (24.0112)
2022-11-25 00:57:20,717:INFO: Dataset: zara1               Batch: 1/8	Loss 31.6949 (31.6949)
2022-11-25 00:57:20,724:INFO: Dataset: zara1               Batch: 2/8	Loss 31.5775 (31.6348)
2022-11-25 00:57:20,728:INFO: Dataset: zara1               Batch: 3/8	Loss 31.7129 (31.6604)
2022-11-25 00:57:20,729:INFO: Dataset: zara1               Batch: 4/8	Loss 31.5385 (31.6288)
2022-11-25 00:57:20,730:INFO: Dataset: zara1               Batch: 5/8	Loss 31.6629 (31.6355)
2022-11-25 00:57:20,775:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6643 (31.6407)
2022-11-25 00:57:20,779:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4595 (31.6166)
2022-11-25 00:57:20,782:INFO: Dataset: zara1               Batch: 8/8	Loss 31.4401 (31.5981)
2022-11-25 00:57:21,041:INFO: Dataset: zara2               Batch:  1/18	Loss 23.5445 (23.5445)
2022-11-25 00:57:21,043:INFO: Dataset: zara2               Batch:  2/18	Loss 23.6285 (23.5882)
2022-11-25 00:57:21,047:INFO: Dataset: zara2               Batch:  3/18	Loss 23.3863 (23.5241)
2022-11-25 00:57:21,090:INFO: Dataset: zara2               Batch:  4/18	Loss 23.4006 (23.4928)
2022-11-25 00:57:21,092:INFO: Dataset: zara2               Batch:  5/18	Loss 23.3708 (23.4681)
2022-11-25 00:57:21,095:INFO: Dataset: zara2               Batch:  6/18	Loss 23.4602 (23.4668)
2022-11-25 00:57:21,096:INFO: Dataset: zara2               Batch:  7/18	Loss 23.3671 (23.4535)
2022-11-25 00:57:21,097:INFO: Dataset: zara2               Batch:  8/18	Loss 23.4933 (23.4584)
2022-11-25 00:57:21,098:INFO: Dataset: zara2               Batch:  9/18	Loss 23.4351 (23.4561)
2022-11-25 00:57:21,099:INFO: Dataset: zara2               Batch: 10/18	Loss 23.1392 (23.4215)
2022-11-25 00:57:21,101:INFO: Dataset: zara2               Batch: 11/18	Loss 22.8439 (23.3724)
2022-11-25 00:57:21,102:INFO: Dataset: zara2               Batch: 12/18	Loss 23.2190 (23.3592)
2022-11-25 00:57:21,103:INFO: Dataset: zara2               Batch: 13/18	Loss 23.0294 (23.3325)
2022-11-25 00:57:21,104:INFO: Dataset: zara2               Batch: 14/18	Loss 22.8174 (23.2970)
2022-11-25 00:57:21,105:INFO: Dataset: zara2               Batch: 15/18	Loss 22.7987 (23.2673)
2022-11-25 00:57:21,107:INFO: Dataset: zara2               Batch: 16/18	Loss 22.7397 (23.2319)
2022-11-25 00:57:21,108:INFO: Dataset: zara2               Batch: 17/18	Loss 22.3570 (23.1798)
2022-11-25 00:57:21,110:INFO: Dataset: zara2               Batch: 18/18	Loss 22.2389 (23.1369)
2022-11-25 00:57:21,158:INFO: - Computing ADE (validation o)
2022-11-25 00:57:21,424:INFO: 		 ADE on eth                       dataset:	 1.1169774532318115
2022-11-25 00:57:21,424:INFO: Average validation o:	ADE  1.1170	FDE  2.1972
2022-11-25 00:57:21,425:INFO: - Computing loss (validation)
2022-11-25 00:57:21,619:INFO: Dataset: hotel               Batch: 1/2	Loss 31.2401 (31.2401)
2022-11-25 00:57:21,637:INFO: Dataset: hotel               Batch: 2/2	Loss 31.2469 (31.2407)
2022-11-25 00:57:21,885:INFO: Dataset: univ                Batch: 1/3	Loss 22.3759 (22.3759)
2022-11-25 00:57:21,886:INFO: Dataset: univ                Batch: 2/3	Loss 22.3068 (22.3409)
2022-11-25 00:57:21,887:INFO: Dataset: univ                Batch: 3/3	Loss 22.4475 (22.3769)
2022-11-25 00:57:22,151:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4674 (30.4674)
2022-11-25 00:57:22,152:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4387 (30.4611)
2022-11-25 00:57:22,422:INFO: Dataset: zara2               Batch: 1/5	Loss 22.9690 (22.9690)
2022-11-25 00:57:22,425:INFO: Dataset: zara2               Batch: 2/5	Loss 22.9795 (22.9743)
2022-11-25 00:57:22,428:INFO: Dataset: zara2               Batch: 3/5	Loss 23.1393 (23.0241)
2022-11-25 00:57:22,429:INFO: Dataset: zara2               Batch: 4/5	Loss 22.8707 (22.9879)
2022-11-25 00:57:22,436:INFO: Dataset: zara2               Batch: 5/5	Loss 23.0053 (22.9912)
2022-11-25 00:57:22,514:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_561.pth.tar
2022-11-25 00:57:22,514:INFO: 
===> EPOCH: 562 (P4)
2022-11-25 00:57:22,515:INFO: - Computing loss (training)
2022-11-25 00:57:22,756:INFO: Dataset: hotel               Batch: 1/4	Loss 31.2268 (31.2268)
2022-11-25 00:57:22,768:INFO: Dataset: hotel               Batch: 2/4	Loss 31.2201 (31.2234)
2022-11-25 00:57:22,770:INFO: Dataset: hotel               Batch: 3/4	Loss 31.2380 (31.2285)
2022-11-25 00:57:22,773:INFO: Dataset: hotel               Batch: 4/4	Loss 31.2470 (31.2311)
2022-11-25 00:57:23,071:INFO: Dataset: univ                Batch:  1/15	Loss 22.5094 (22.5094)
2022-11-25 00:57:23,077:INFO: Dataset: univ                Batch:  2/15	Loss 22.4536 (22.4807)
2022-11-25 00:57:23,081:INFO: Dataset: univ                Batch:  3/15	Loss 22.3585 (22.4439)
2022-11-25 00:57:23,083:INFO: Dataset: univ                Batch:  4/15	Loss 22.3254 (22.4134)
2022-11-25 00:57:23,156:INFO: Dataset: univ                Batch:  5/15	Loss 22.3755 (22.4056)
2022-11-25 00:57:23,161:INFO: Dataset: univ                Batch:  6/15	Loss 22.2096 (22.3730)
2022-11-25 00:57:23,163:INFO: Dataset: univ                Batch:  7/15	Loss 22.2638 (22.3574)
2022-11-25 00:57:23,165:INFO: Dataset: univ                Batch:  8/15	Loss 22.1657 (22.3334)
2022-11-25 00:57:23,167:INFO: Dataset: univ                Batch:  9/15	Loss 22.2253 (22.3214)
2022-11-25 00:57:23,169:INFO: Dataset: univ                Batch: 10/15	Loss 22.1195 (22.3019)
2022-11-25 00:57:23,190:INFO: Dataset: univ                Batch: 11/15	Loss 22.0785 (22.2817)
2022-11-25 00:57:23,192:INFO: Dataset: univ                Batch: 12/15	Loss 22.0476 (22.2615)
2022-11-25 00:57:23,194:INFO: Dataset: univ                Batch: 13/15	Loss 21.9385 (22.2373)
2022-11-25 00:57:23,200:INFO: Dataset: univ                Batch: 14/15	Loss 21.7927 (22.2051)
2022-11-25 00:57:23,203:INFO: Dataset: univ                Batch: 15/15	Loss 21.5735 (22.1969)
2022-11-25 00:57:23,645:INFO: Dataset: zara1               Batch: 1/8	Loss 30.4575 (30.4575)
2022-11-25 00:57:23,647:INFO: Dataset: zara1               Batch: 2/8	Loss 30.4813 (30.4688)
2022-11-25 00:57:23,649:INFO: Dataset: zara1               Batch: 3/8	Loss 30.4288 (30.4563)
2022-11-25 00:57:23,651:INFO: Dataset: zara1               Batch: 4/8	Loss 30.4590 (30.4569)
2022-11-25 00:57:23,654:INFO: Dataset: zara1               Batch: 5/8	Loss 30.3591 (30.4392)
2022-11-25 00:57:23,677:INFO: Dataset: zara1               Batch: 6/8	Loss 30.3314 (30.4208)
2022-11-25 00:57:23,678:INFO: Dataset: zara1               Batch: 7/8	Loss 30.2138 (30.3919)
2022-11-25 00:57:23,679:INFO: Dataset: zara1               Batch: 8/8	Loss 30.1941 (30.3655)
2022-11-25 00:57:23,971:INFO: Dataset: zara2               Batch:  1/18	Loss 21.1915 (21.1915)
2022-11-25 00:57:23,973:INFO: Dataset: zara2               Batch:  2/18	Loss 20.9259 (21.0536)
2022-11-25 00:57:23,975:INFO: Dataset: zara2               Batch:  3/18	Loss 21.1257 (21.0794)
2022-11-25 00:57:23,976:INFO: Dataset: zara2               Batch:  4/18	Loss 21.3210 (21.1405)
2022-11-25 00:57:24,005:INFO: Dataset: zara2               Batch:  5/18	Loss 21.2359 (21.1604)
2022-11-25 00:57:24,010:INFO: Dataset: zara2               Batch:  6/18	Loss 21.0310 (21.1392)
2022-11-25 00:57:24,011:INFO: Dataset: zara2               Batch:  7/18	Loss 20.9068 (21.1092)
2022-11-25 00:57:24,012:INFO: Dataset: zara2               Batch:  8/18	Loss 20.5947 (21.0495)
2022-11-25 00:57:24,013:INFO: Dataset: zara2               Batch:  9/18	Loss 21.0775 (21.0526)
2022-11-25 00:57:24,014:INFO: Dataset: zara2               Batch: 10/18	Loss 20.7747 (21.0262)
2022-11-25 00:57:24,015:INFO: Dataset: zara2               Batch: 11/18	Loss 21.0794 (21.0311)
2022-11-25 00:57:24,017:INFO: Dataset: zara2               Batch: 12/18	Loss 21.0556 (21.0332)
2022-11-25 00:57:24,018:INFO: Dataset: zara2               Batch: 13/18	Loss 20.5910 (21.0036)
2022-11-25 00:57:24,019:INFO: Dataset: zara2               Batch: 14/18	Loss 20.4353 (20.9636)
2022-11-25 00:57:24,020:INFO: Dataset: zara2               Batch: 15/18	Loss 20.4007 (20.9279)
2022-11-25 00:57:24,020:INFO: Dataset: zara2               Batch: 16/18	Loss 20.5688 (20.9054)
2022-11-25 00:57:24,022:INFO: Dataset: zara2               Batch: 17/18	Loss 20.1745 (20.8604)
2022-11-25 00:57:24,024:INFO: Dataset: zara2               Batch: 18/18	Loss 20.3763 (20.8392)
2022-11-25 00:57:24,072:INFO: - Computing ADE (validation o)
2022-11-25 00:57:24,334:INFO: 		 ADE on eth                       dataset:	 1.071930170059204
2022-11-25 00:57:24,342:INFO: Average validation o:	ADE  1.0719	FDE  2.1093
2022-11-25 00:57:24,343:INFO: - Computing loss (validation)
2022-11-25 00:57:24,541:INFO: Dataset: hotel               Batch: 1/2	Loss 31.6130 (31.6130)
2022-11-25 00:57:24,542:INFO: Dataset: hotel               Batch: 2/2	Loss 31.8956 (31.6352)
2022-11-25 00:57:24,797:INFO: Dataset: univ                Batch: 1/3	Loss 20.0705 (20.0705)
2022-11-25 00:57:24,805:INFO: Dataset: univ                Batch: 2/3	Loss 20.2569 (20.1656)
2022-11-25 00:57:24,806:INFO: Dataset: univ                Batch: 3/3	Loss 20.0377 (20.1271)
2022-11-25 00:57:25,055:INFO: Dataset: zara1               Batch: 1/2	Loss 29.0614 (29.0614)
2022-11-25 00:57:25,056:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1562 (29.0836)
2022-11-25 00:57:25,318:INFO: Dataset: zara2               Batch: 1/5	Loss 20.6339 (20.6339)
2022-11-25 00:57:25,319:INFO: Dataset: zara2               Batch: 2/5	Loss 20.5211 (20.5779)
2022-11-25 00:57:25,321:INFO: Dataset: zara2               Batch: 3/5	Loss 20.5745 (20.5768)
2022-11-25 00:57:25,333:INFO: Dataset: zara2               Batch: 4/5	Loss 20.5638 (20.5736)
2022-11-25 00:57:25,333:INFO: Dataset: zara2               Batch: 5/5	Loss 20.8716 (20.6337)
2022-11-25 00:57:25,388:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_562.pth.tar
2022-11-25 00:57:25,388:INFO: 
===> EPOCH: 563 (P4)
2022-11-25 00:57:25,388:INFO: - Computing loss (training)
2022-11-25 00:57:25,588:INFO: Dataset: hotel               Batch: 1/4	Loss 31.6871 (31.6871)
2022-11-25 00:57:25,589:INFO: Dataset: hotel               Batch: 2/4	Loss 31.7430 (31.7159)
2022-11-25 00:57:25,592:INFO: Dataset: hotel               Batch: 3/4	Loss 31.6744 (31.7022)
2022-11-25 00:57:25,595:INFO: Dataset: hotel               Batch: 4/4	Loss 31.7698 (31.7131)
2022-11-25 00:57:25,845:INFO: Dataset: univ                Batch:  1/15	Loss 20.4108 (20.4108)
2022-11-25 00:57:25,847:INFO: Dataset: univ                Batch:  2/15	Loss 20.1665 (20.2830)
2022-11-25 00:57:25,849:INFO: Dataset: univ                Batch:  3/15	Loss 20.2175 (20.2603)
2022-11-25 00:57:25,853:INFO: Dataset: univ                Batch:  4/15	Loss 20.1276 (20.2261)
2022-11-25 00:57:25,886:INFO: Dataset: univ                Batch:  5/15	Loss 20.0813 (20.1986)
2022-11-25 00:57:25,897:INFO: Dataset: univ                Batch:  6/15	Loss 20.0559 (20.1738)
2022-11-25 00:57:25,898:INFO: Dataset: univ                Batch:  7/15	Loss 20.0308 (20.1519)
2022-11-25 00:57:25,899:INFO: Dataset: univ                Batch:  8/15	Loss 20.0443 (20.1371)
2022-11-25 00:57:25,900:INFO: Dataset: univ                Batch:  9/15	Loss 19.8300 (20.1059)
2022-11-25 00:57:25,901:INFO: Dataset: univ                Batch: 10/15	Loss 19.7535 (20.0690)
2022-11-25 00:57:25,902:INFO: Dataset: univ                Batch: 11/15	Loss 19.7584 (20.0395)
2022-11-25 00:57:25,904:INFO: Dataset: univ                Batch: 12/15	Loss 19.7379 (20.0133)
2022-11-25 00:57:25,905:INFO: Dataset: univ                Batch: 13/15	Loss 19.5353 (19.9751)
2022-11-25 00:57:25,906:INFO: Dataset: univ                Batch: 14/15	Loss 19.5713 (19.9449)
2022-11-25 00:57:25,907:INFO: Dataset: univ                Batch: 15/15	Loss 19.6174 (19.9404)
2022-11-25 00:57:26,172:INFO: Dataset: zara1               Batch: 1/8	Loss 29.4164 (29.4164)
2022-11-25 00:57:26,174:INFO: Dataset: zara1               Batch: 2/8	Loss 29.2644 (29.3464)
2022-11-25 00:57:26,179:INFO: Dataset: zara1               Batch: 3/8	Loss 29.4095 (29.3688)
2022-11-25 00:57:26,180:INFO: Dataset: zara1               Batch: 4/8	Loss 29.4056 (29.3774)
2022-11-25 00:57:26,181:INFO: Dataset: zara1               Batch: 5/8	Loss 29.3744 (29.3768)
2022-11-25 00:57:26,228:INFO: Dataset: zara1               Batch: 6/8	Loss 29.2433 (29.3538)
2022-11-25 00:57:26,231:INFO: Dataset: zara1               Batch: 7/8	Loss 29.2118 (29.3317)
2022-11-25 00:57:26,235:INFO: Dataset: zara1               Batch: 8/8	Loss 29.1091 (29.3052)
2022-11-25 00:57:26,521:INFO: Dataset: zara2               Batch:  1/18	Loss 18.9447 (18.9447)
2022-11-25 00:57:26,524:INFO: Dataset: zara2               Batch:  2/18	Loss 18.8795 (18.9129)
2022-11-25 00:57:26,525:INFO: Dataset: zara2               Batch:  3/18	Loss 18.6014 (18.8095)
2022-11-25 00:57:26,529:INFO: Dataset: zara2               Batch:  4/18	Loss 18.4878 (18.7286)
2022-11-25 00:57:26,556:INFO: Dataset: zara2               Batch:  5/18	Loss 18.6596 (18.7152)
2022-11-25 00:57:26,562:INFO: Dataset: zara2               Batch:  6/18	Loss 18.6238 (18.6988)
2022-11-25 00:57:26,563:INFO: Dataset: zara2               Batch:  7/18	Loss 18.4784 (18.6694)
2022-11-25 00:57:26,564:INFO: Dataset: zara2               Batch:  8/18	Loss 18.5239 (18.6523)
2022-11-25 00:57:26,565:INFO: Dataset: zara2               Batch:  9/18	Loss 18.4528 (18.6263)
2022-11-25 00:57:26,566:INFO: Dataset: zara2               Batch: 10/18	Loss 18.4819 (18.6121)
2022-11-25 00:57:26,567:INFO: Dataset: zara2               Batch: 11/18	Loss 18.1630 (18.5705)
2022-11-25 00:57:26,568:INFO: Dataset: zara2               Batch: 12/18	Loss 18.2422 (18.5444)
2022-11-25 00:57:26,569:INFO: Dataset: zara2               Batch: 13/18	Loss 18.6376 (18.5517)
2022-11-25 00:57:26,570:INFO: Dataset: zara2               Batch: 14/18	Loss 18.0849 (18.5187)
2022-11-25 00:57:26,571:INFO: Dataset: zara2               Batch: 15/18	Loss 17.8598 (18.4791)
2022-11-25 00:57:26,572:INFO: Dataset: zara2               Batch: 16/18	Loss 17.8548 (18.4413)
2022-11-25 00:57:26,574:INFO: Dataset: zara2               Batch: 17/18	Loss 17.8901 (18.4099)
2022-11-25 00:57:26,575:INFO: Dataset: zara2               Batch: 18/18	Loss 17.5715 (18.3716)
2022-11-25 00:57:26,636:INFO: - Computing ADE (validation o)
2022-11-25 00:57:26,904:INFO: 		 ADE on eth                       dataset:	 1.0639572143554688
2022-11-25 00:57:26,904:INFO: Average validation o:	ADE  1.0640	FDE  2.1319
2022-11-25 00:57:26,905:INFO: - Computing loss (validation)
2022-11-25 00:57:27,097:INFO: Dataset: hotel               Batch: 1/2	Loss 32.4965 (32.4965)
2022-11-25 00:57:27,101:INFO: Dataset: hotel               Batch: 2/2	Loss 32.6173 (32.5068)
2022-11-25 00:57:27,342:INFO: Dataset: univ                Batch: 1/3	Loss 17.5312 (17.5312)
2022-11-25 00:57:27,343:INFO: Dataset: univ                Batch: 2/3	Loss 17.9455 (17.7757)
2022-11-25 00:57:27,344:INFO: Dataset: univ                Batch: 3/3	Loss 17.9456 (17.8311)
2022-11-25 00:57:27,592:INFO: Dataset: zara1               Batch: 1/2	Loss 28.2357 (28.2357)
2022-11-25 00:57:27,592:INFO: Dataset: zara1               Batch: 2/2	Loss 28.2759 (28.2462)
2022-11-25 00:57:27,846:INFO: Dataset: zara2               Batch: 1/5	Loss 18.0748 (18.0748)
2022-11-25 00:57:27,847:INFO: Dataset: zara2               Batch: 2/5	Loss 17.9362 (18.0045)
2022-11-25 00:57:27,849:INFO: Dataset: zara2               Batch: 3/5	Loss 18.3560 (18.1283)
2022-11-25 00:57:27,850:INFO: Dataset: zara2               Batch: 4/5	Loss 18.4636 (18.2147)
2022-11-25 00:57:27,853:INFO: Dataset: zara2               Batch: 5/5	Loss 18.0835 (18.1896)
2022-11-25 00:57:27,909:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_563.pth.tar
2022-11-25 00:57:27,910:INFO: 
===> EPOCH: 564 (P4)
2022-11-25 00:57:27,910:INFO: - Computing loss (training)
2022-11-25 00:57:28,107:INFO: Dataset: hotel               Batch: 1/4	Loss 32.6600 (32.6600)
2022-11-25 00:57:28,115:INFO: Dataset: hotel               Batch: 2/4	Loss 32.7652 (32.7120)
2022-11-25 00:57:28,118:INFO: Dataset: hotel               Batch: 3/4	Loss 32.6090 (32.6773)
2022-11-25 00:57:28,120:INFO: Dataset: hotel               Batch: 4/4	Loss 33.0029 (32.7275)
2022-11-25 00:57:28,390:INFO: Dataset: univ                Batch:  1/15	Loss 17.9757 (17.9757)
2022-11-25 00:57:28,415:INFO: Dataset: univ                Batch:  2/15	Loss 18.0345 (18.0068)
2022-11-25 00:57:28,418:INFO: Dataset: univ                Batch:  3/15	Loss 18.0772 (18.0312)
2022-11-25 00:57:28,421:INFO: Dataset: univ                Batch:  4/15	Loss 17.8534 (17.9921)
2022-11-25 00:57:28,422:INFO: Dataset: univ                Batch:  5/15	Loss 17.9400 (17.9814)
2022-11-25 00:57:28,426:INFO: Dataset: univ                Batch:  6/15	Loss 17.8266 (17.9570)
2022-11-25 00:57:28,427:INFO: Dataset: univ                Batch:  7/15	Loss 17.8283 (17.9408)
2022-11-25 00:57:28,428:INFO: Dataset: univ                Batch:  8/15	Loss 17.7218 (17.9159)
2022-11-25 00:57:28,429:INFO: Dataset: univ                Batch:  9/15	Loss 17.7330 (17.8949)
2022-11-25 00:57:28,431:INFO: Dataset: univ                Batch: 10/15	Loss 17.6897 (17.8753)
2022-11-25 00:57:28,432:INFO: Dataset: univ                Batch: 11/15	Loss 17.6727 (17.8548)
2022-11-25 00:57:28,434:INFO: Dataset: univ                Batch: 12/15	Loss 17.3762 (17.8177)
2022-11-25 00:57:28,435:INFO: Dataset: univ                Batch: 13/15	Loss 17.4916 (17.7918)
2022-11-25 00:57:28,436:INFO: Dataset: univ                Batch: 14/15	Loss 17.4384 (17.7666)
2022-11-25 00:57:28,437:INFO: Dataset: univ                Batch: 15/15	Loss 17.4284 (17.7617)
2022-11-25 00:57:28,710:INFO: Dataset: zara1               Batch: 1/8	Loss 29.1210 (29.1210)
2022-11-25 00:57:28,711:INFO: Dataset: zara1               Batch: 2/8	Loss 29.0187 (29.0715)
2022-11-25 00:57:28,716:INFO: Dataset: zara1               Batch: 3/8	Loss 29.1943 (29.1123)
2022-11-25 00:57:28,717:INFO: Dataset: zara1               Batch: 4/8	Loss 29.1413 (29.1192)
2022-11-25 00:57:28,718:INFO: Dataset: zara1               Batch: 5/8	Loss 29.0693 (29.1078)
2022-11-25 00:57:28,757:INFO: Dataset: zara1               Batch: 6/8	Loss 29.0472 (29.0982)
2022-11-25 00:57:28,758:INFO: Dataset: zara1               Batch: 7/8	Loss 28.9756 (29.0795)
2022-11-25 00:57:28,759:INFO: Dataset: zara1               Batch: 8/8	Loss 28.9793 (29.0692)
2022-11-25 00:57:29,043:INFO: Dataset: zara2               Batch:  1/18	Loss 16.6806 (16.6806)
2022-11-25 00:57:29,044:INFO: Dataset: zara2               Batch:  2/18	Loss 16.5668 (16.6242)
2022-11-25 00:57:29,045:INFO: Dataset: zara2               Batch:  3/18	Loss 16.3818 (16.5504)
2022-11-25 00:57:29,046:INFO: Dataset: zara2               Batch:  4/18	Loss 16.6641 (16.5798)
2022-11-25 00:57:29,096:INFO: Dataset: zara2               Batch:  5/18	Loss 16.7247 (16.6090)
2022-11-25 00:57:29,105:INFO: Dataset: zara2               Batch:  6/18	Loss 16.5649 (16.6016)
2022-11-25 00:57:29,106:INFO: Dataset: zara2               Batch:  7/18	Loss 16.8271 (16.6330)
2022-11-25 00:57:29,107:INFO: Dataset: zara2               Batch:  8/18	Loss 16.7706 (16.6499)
2022-11-25 00:57:29,108:INFO: Dataset: zara2               Batch:  9/18	Loss 16.6292 (16.6477)
2022-11-25 00:57:29,109:INFO: Dataset: zara2               Batch: 10/18	Loss 16.3485 (16.6193)
2022-11-25 00:57:29,110:INFO: Dataset: zara2               Batch: 11/18	Loss 16.6182 (16.6192)
2022-11-25 00:57:29,111:INFO: Dataset: zara2               Batch: 12/18	Loss 16.4246 (16.6023)
2022-11-25 00:57:29,112:INFO: Dataset: zara2               Batch: 13/18	Loss 16.3431 (16.5847)
2022-11-25 00:57:29,113:INFO: Dataset: zara2               Batch: 14/18	Loss 16.6044 (16.5862)
2022-11-25 00:57:29,114:INFO: Dataset: zara2               Batch: 15/18	Loss 16.2377 (16.5626)
2022-11-25 00:57:29,115:INFO: Dataset: zara2               Batch: 16/18	Loss 16.0093 (16.5261)
2022-11-25 00:57:29,116:INFO: Dataset: zara2               Batch: 17/18	Loss 15.8068 (16.4883)
2022-11-25 00:57:29,118:INFO: Dataset: zara2               Batch: 18/18	Loss 16.1374 (16.4689)
2022-11-25 00:57:29,173:INFO: - Computing ADE (validation o)
2022-11-25 00:57:29,456:INFO: 		 ADE on eth                       dataset:	 1.0997236967086792
2022-11-25 00:57:29,457:INFO: Average validation o:	ADE  1.0997	FDE  2.2421
2022-11-25 00:57:29,457:INFO: - Computing loss (validation)
2022-11-25 00:57:29,651:INFO: Dataset: hotel               Batch: 1/2	Loss 33.7479 (33.7479)
2022-11-25 00:57:29,653:INFO: Dataset: hotel               Batch: 2/2	Loss 33.5133 (33.7335)
2022-11-25 00:57:29,930:INFO: Dataset: univ                Batch: 1/3	Loss 16.2264 (16.2264)
2022-11-25 00:57:29,934:INFO: Dataset: univ                Batch: 2/3	Loss 16.0967 (16.1671)
2022-11-25 00:57:29,935:INFO: Dataset: univ                Batch: 3/3	Loss 16.1302 (16.1549)
2022-11-25 00:57:30,184:INFO: Dataset: zara1               Batch: 1/2	Loss 28.4615 (28.4615)
2022-11-25 00:57:30,187:INFO: Dataset: zara1               Batch: 2/2	Loss 28.3270 (28.4226)
2022-11-25 00:57:30,458:INFO: Dataset: zara2               Batch: 1/5	Loss 16.2499 (16.2499)
2022-11-25 00:57:30,459:INFO: Dataset: zara2               Batch: 2/5	Loss 16.4653 (16.3594)
2022-11-25 00:57:30,460:INFO: Dataset: zara2               Batch: 3/5	Loss 16.4276 (16.3826)
2022-11-25 00:57:30,462:INFO: Dataset: zara2               Batch: 4/5	Loss 16.2457 (16.3488)
2022-11-25 00:57:30,462:INFO: Dataset: zara2               Batch: 5/5	Loss 16.3979 (16.3588)
2022-11-25 00:57:30,514:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_564.pth.tar
2022-11-25 00:57:30,515:INFO: 
===> EPOCH: 565 (P4)
2022-11-25 00:57:30,515:INFO: - Computing loss (training)
2022-11-25 00:57:30,713:INFO: Dataset: hotel               Batch: 1/4	Loss 34.0995 (34.0995)
2022-11-25 00:57:30,716:INFO: Dataset: hotel               Batch: 2/4	Loss 34.0471 (34.0720)
2022-11-25 00:57:30,718:INFO: Dataset: hotel               Batch: 3/4	Loss 34.1142 (34.0859)
2022-11-25 00:57:30,723:INFO: Dataset: hotel               Batch: 4/4	Loss 34.0026 (34.0718)
2022-11-25 00:57:30,977:INFO: Dataset: univ                Batch:  1/15	Loss 16.4804 (16.4804)
2022-11-25 00:57:30,979:INFO: Dataset: univ                Batch:  2/15	Loss 16.3437 (16.4119)
2022-11-25 00:57:30,982:INFO: Dataset: univ                Batch:  3/15	Loss 16.3358 (16.3862)
2022-11-25 00:57:31,046:INFO: Dataset: univ                Batch:  4/15	Loss 16.3954 (16.3886)
2022-11-25 00:57:31,047:INFO: Dataset: univ                Batch:  5/15	Loss 16.4160 (16.3939)
2022-11-25 00:57:31,051:INFO: Dataset: univ                Batch:  6/15	Loss 16.3013 (16.3785)
2022-11-25 00:57:31,052:INFO: Dataset: univ                Batch:  7/15	Loss 16.4047 (16.3822)
2022-11-25 00:57:31,053:INFO: Dataset: univ                Batch:  8/15	Loss 16.2260 (16.3618)
2022-11-25 00:57:31,054:INFO: Dataset: univ                Batch:  9/15	Loss 16.2907 (16.3545)
2022-11-25 00:57:31,055:INFO: Dataset: univ                Batch: 10/15	Loss 16.3057 (16.3498)
2022-11-25 00:57:31,057:INFO: Dataset: univ                Batch: 11/15	Loss 16.1444 (16.3331)
2022-11-25 00:57:31,058:INFO: Dataset: univ                Batch: 12/15	Loss 16.0210 (16.3042)
2022-11-25 00:57:31,060:INFO: Dataset: univ                Batch: 13/15	Loss 15.8770 (16.2729)
2022-11-25 00:57:31,061:INFO: Dataset: univ                Batch: 14/15	Loss 15.8742 (16.2467)
2022-11-25 00:57:31,062:INFO: Dataset: univ                Batch: 15/15	Loss 15.8848 (16.2412)
2022-11-25 00:57:31,356:INFO: Dataset: zara1               Batch: 1/8	Loss 29.6630 (29.6630)
2022-11-25 00:57:31,357:INFO: Dataset: zara1               Batch: 2/8	Loss 29.6354 (29.6483)
2022-11-25 00:57:31,358:INFO: Dataset: zara1               Batch: 3/8	Loss 29.7494 (29.6814)
2022-11-25 00:57:31,362:INFO: Dataset: zara1               Batch: 4/8	Loss 29.6842 (29.6821)
2022-11-25 00:57:31,363:INFO: Dataset: zara1               Batch: 5/8	Loss 29.8512 (29.7136)
2022-11-25 00:57:31,382:INFO: Dataset: zara1               Batch: 6/8	Loss 29.5512 (29.6876)
2022-11-25 00:57:31,384:INFO: Dataset: zara1               Batch: 7/8	Loss 29.6223 (29.6788)
2022-11-25 00:57:31,384:INFO: Dataset: zara1               Batch: 8/8	Loss 29.4623 (29.6560)
2022-11-25 00:57:31,651:INFO: Dataset: zara2               Batch:  1/18	Loss 15.6477 (15.6477)
2022-11-25 00:57:31,653:INFO: Dataset: zara2               Batch:  2/18	Loss 15.6749 (15.6611)
2022-11-25 00:57:31,698:INFO: Dataset: zara2               Batch:  3/18	Loss 15.5301 (15.6197)
2022-11-25 00:57:31,699:INFO: Dataset: zara2               Batch:  4/18	Loss 15.5596 (15.6047)
2022-11-25 00:57:31,700:INFO: Dataset: zara2               Batch:  5/18	Loss 15.5091 (15.5862)
2022-11-25 00:57:31,709:INFO: Dataset: zara2               Batch:  6/18	Loss 15.5354 (15.5771)
2022-11-25 00:57:31,710:INFO: Dataset: zara2               Batch:  7/18	Loss 15.6705 (15.5900)
2022-11-25 00:57:31,711:INFO: Dataset: zara2               Batch:  8/18	Loss 15.5799 (15.5886)
2022-11-25 00:57:31,712:INFO: Dataset: zara2               Batch:  9/18	Loss 15.6244 (15.5922)
2022-11-25 00:57:31,713:INFO: Dataset: zara2               Batch: 10/18	Loss 15.4912 (15.5820)
2022-11-25 00:57:31,714:INFO: Dataset: zara2               Batch: 11/18	Loss 15.4285 (15.5682)
2022-11-25 00:57:31,715:INFO: Dataset: zara2               Batch: 12/18	Loss 15.3968 (15.5538)
2022-11-25 00:57:31,716:INFO: Dataset: zara2               Batch: 13/18	Loss 15.4863 (15.5487)
2022-11-25 00:57:31,717:INFO: Dataset: zara2               Batch: 14/18	Loss 15.1677 (15.5214)
2022-11-25 00:57:31,718:INFO: Dataset: zara2               Batch: 15/18	Loss 15.0918 (15.4911)
2022-11-25 00:57:31,719:INFO: Dataset: zara2               Batch: 16/18	Loss 15.0917 (15.4666)
2022-11-25 00:57:31,720:INFO: Dataset: zara2               Batch: 17/18	Loss 15.3712 (15.4617)
2022-11-25 00:57:31,721:INFO: Dataset: zara2               Batch: 18/18	Loss 14.8407 (15.4345)
2022-11-25 00:57:31,769:INFO: - Computing ADE (validation o)
2022-11-25 00:57:32,065:INFO: 		 ADE on eth                       dataset:	 1.057612419128418
2022-11-25 00:57:32,065:INFO: Average validation o:	ADE  1.0576	FDE  2.1580
2022-11-25 00:57:32,066:INFO: - Computing loss (validation)
2022-11-25 00:57:32,324:INFO: Dataset: hotel               Batch: 1/2	Loss 34.7721 (34.7721)
2022-11-25 00:57:32,325:INFO: Dataset: hotel               Batch: 2/2	Loss 35.4425 (34.8316)
2022-11-25 00:57:32,669:INFO: Dataset: univ                Batch: 1/3	Loss 15.3504 (15.3504)
2022-11-25 00:57:32,670:INFO: Dataset: univ                Batch: 2/3	Loss 15.1824 (15.2755)
2022-11-25 00:57:32,675:INFO: Dataset: univ                Batch: 3/3	Loss 15.2036 (15.2531)
2022-11-25 00:57:33,011:INFO: Dataset: zara1               Batch: 1/2	Loss 29.0173 (29.0173)
2022-11-25 00:57:33,011:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1307 (29.0436)
2022-11-25 00:57:33,289:INFO: Dataset: zara2               Batch: 1/5	Loss 15.1977 (15.1977)
2022-11-25 00:57:33,291:INFO: Dataset: zara2               Batch: 2/5	Loss 15.2447 (15.2221)
2022-11-25 00:57:33,297:INFO: Dataset: zara2               Batch: 3/5	Loss 15.4260 (15.2904)
2022-11-25 00:57:33,312:INFO: Dataset: zara2               Batch: 4/5	Loss 15.2737 (15.2863)
2022-11-25 00:57:33,313:INFO: Dataset: zara2               Batch: 5/5	Loss 15.4795 (15.3257)
2022-11-25 00:57:33,374:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_565.pth.tar
2022-11-25 00:57:33,374:INFO: 
===> EPOCH: 566 (P4)
2022-11-25 00:57:33,375:INFO: - Computing loss (training)
2022-11-25 00:57:33,607:INFO: Dataset: hotel               Batch: 1/4	Loss 35.2012 (35.2012)
2022-11-25 00:57:33,609:INFO: Dataset: hotel               Batch: 2/4	Loss 35.3388 (35.2689)
2022-11-25 00:57:33,610:INFO: Dataset: hotel               Batch: 3/4	Loss 35.1172 (35.2174)
2022-11-25 00:57:33,611:INFO: Dataset: hotel               Batch: 4/4	Loss 34.9816 (35.1788)
2022-11-25 00:57:33,897:INFO: Dataset: univ                Batch:  1/15	Loss 15.5375 (15.5375)
2022-11-25 00:57:33,900:INFO: Dataset: univ                Batch:  2/15	Loss 15.5877 (15.5629)
2022-11-25 00:57:33,903:INFO: Dataset: univ                Batch:  3/15	Loss 15.6242 (15.5802)
2022-11-25 00:57:33,905:INFO: Dataset: univ                Batch:  4/15	Loss 15.5527 (15.5733)
2022-11-25 00:57:33,937:INFO: Dataset: univ                Batch:  5/15	Loss 15.5538 (15.5695)
2022-11-25 00:57:33,940:INFO: Dataset: univ                Batch:  6/15	Loss 15.5095 (15.5597)
2022-11-25 00:57:33,941:INFO: Dataset: univ                Batch:  7/15	Loss 15.4487 (15.5440)
2022-11-25 00:57:33,942:INFO: Dataset: univ                Batch:  8/15	Loss 15.4673 (15.5356)
2022-11-25 00:57:33,944:INFO: Dataset: univ                Batch:  9/15	Loss 15.4212 (15.5227)
2022-11-25 00:57:33,945:INFO: Dataset: univ                Batch: 10/15	Loss 15.3621 (15.5088)
2022-11-25 00:57:33,947:INFO: Dataset: univ                Batch: 11/15	Loss 15.2557 (15.4853)
2022-11-25 00:57:33,948:INFO: Dataset: univ                Batch: 12/15	Loss 15.2838 (15.4687)
2022-11-25 00:57:33,949:INFO: Dataset: univ                Batch: 13/15	Loss 15.2360 (15.4495)
2022-11-25 00:57:33,950:INFO: Dataset: univ                Batch: 14/15	Loss 15.1408 (15.4288)
2022-11-25 00:57:33,951:INFO: Dataset: univ                Batch: 15/15	Loss 14.8985 (15.4215)
2022-11-25 00:57:34,218:INFO: Dataset: zara1               Batch: 1/8	Loss 30.2222 (30.2222)
2022-11-25 00:57:34,219:INFO: Dataset: zara1               Batch: 2/8	Loss 30.3263 (30.2720)
2022-11-25 00:57:34,221:INFO: Dataset: zara1               Batch: 3/8	Loss 30.2730 (30.2723)
2022-11-25 00:57:34,223:INFO: Dataset: zara1               Batch: 4/8	Loss 30.3741 (30.2977)
2022-11-25 00:57:34,226:INFO: Dataset: zara1               Batch: 5/8	Loss 30.3456 (30.3074)
2022-11-25 00:57:34,269:INFO: Dataset: zara1               Batch: 6/8	Loss 30.1318 (30.2781)
2022-11-25 00:57:34,270:INFO: Dataset: zara1               Batch: 7/8	Loss 30.0236 (30.2420)
2022-11-25 00:57:34,271:INFO: Dataset: zara1               Batch: 8/8	Loss 29.8252 (30.1992)
2022-11-25 00:57:34,578:INFO: Dataset: zara2               Batch:  1/18	Loss 14.7993 (14.7993)
2022-11-25 00:57:34,582:INFO: Dataset: zara2               Batch:  2/18	Loss 14.7399 (14.7694)
2022-11-25 00:57:34,583:INFO: Dataset: zara2               Batch:  3/18	Loss 14.9550 (14.8346)
2022-11-25 00:57:34,584:INFO: Dataset: zara2               Batch:  4/18	Loss 15.1360 (14.9060)
2022-11-25 00:57:34,616:INFO: Dataset: zara2               Batch:  5/18	Loss 15.1035 (14.9473)
2022-11-25 00:57:34,620:INFO: Dataset: zara2               Batch:  6/18	Loss 14.8893 (14.9381)
2022-11-25 00:57:34,621:INFO: Dataset: zara2               Batch:  7/18	Loss 14.8286 (14.9239)
2022-11-25 00:57:34,622:INFO: Dataset: zara2               Batch:  8/18	Loss 15.1546 (14.9504)
2022-11-25 00:57:34,623:INFO: Dataset: zara2               Batch:  9/18	Loss 15.1534 (14.9740)
2022-11-25 00:57:34,624:INFO: Dataset: zara2               Batch: 10/18	Loss 14.8435 (14.9611)
2022-11-25 00:57:34,626:INFO: Dataset: zara2               Batch: 11/18	Loss 15.0516 (14.9698)
2022-11-25 00:57:34,627:INFO: Dataset: zara2               Batch: 12/18	Loss 14.9383 (14.9673)
2022-11-25 00:57:34,628:INFO: Dataset: zara2               Batch: 13/18	Loss 14.6901 (14.9457)
2022-11-25 00:57:34,629:INFO: Dataset: zara2               Batch: 14/18	Loss 14.9447 (14.9456)
2022-11-25 00:57:34,629:INFO: Dataset: zara2               Batch: 15/18	Loss 14.6673 (14.9285)
2022-11-25 00:57:34,630:INFO: Dataset: zara2               Batch: 16/18	Loss 14.4972 (14.9026)
2022-11-25 00:57:34,632:INFO: Dataset: zara2               Batch: 17/18	Loss 14.5692 (14.8855)
2022-11-25 00:57:34,633:INFO: Dataset: zara2               Batch: 18/18	Loss 14.4716 (14.8658)
2022-11-25 00:57:34,681:INFO: - Computing ADE (validation o)
2022-11-25 00:57:34,952:INFO: 		 ADE on eth                       dataset:	 1.0247653722763062
2022-11-25 00:57:34,952:INFO: Average validation o:	ADE  1.0248	FDE  2.0829
2022-11-25 00:57:34,953:INFO: - Computing loss (validation)
2022-11-25 00:57:35,166:INFO: Dataset: hotel               Batch: 1/2	Loss 35.5616 (35.5616)
2022-11-25 00:57:35,170:INFO: Dataset: hotel               Batch: 2/2	Loss 35.2408 (35.5376)
2022-11-25 00:57:35,465:INFO: Dataset: univ                Batch: 1/3	Loss 14.7860 (14.7860)
2022-11-25 00:57:35,469:INFO: Dataset: univ                Batch: 2/3	Loss 14.6924 (14.7414)
2022-11-25 00:57:35,470:INFO: Dataset: univ                Batch: 3/3	Loss 14.8079 (14.7630)
2022-11-25 00:57:35,719:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4457 (29.4457)
2022-11-25 00:57:35,720:INFO: Dataset: zara1               Batch: 2/2	Loss 29.4428 (29.4449)
2022-11-25 00:57:35,963:INFO: Dataset: zara2               Batch: 1/5	Loss 14.7712 (14.7712)
2022-11-25 00:57:35,972:INFO: Dataset: zara2               Batch: 2/5	Loss 14.5516 (14.6651)
2022-11-25 00:57:35,982:INFO: Dataset: zara2               Batch: 3/5	Loss 14.7005 (14.6776)
2022-11-25 00:57:35,985:INFO: Dataset: zara2               Batch: 4/5	Loss 14.8828 (14.7310)
2022-11-25 00:57:35,985:INFO: Dataset: zara2               Batch: 5/5	Loss 14.7019 (14.7252)
2022-11-25 00:57:36,044:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_566.pth.tar
2022-11-25 00:57:36,045:INFO: 
===> EPOCH: 567 (P4)
2022-11-25 00:57:36,045:INFO: - Computing loss (training)
2022-11-25 00:57:36,250:INFO: Dataset: hotel               Batch: 1/4	Loss 35.7232 (35.7232)
2022-11-25 00:57:36,252:INFO: Dataset: hotel               Batch: 2/4	Loss 35.5502 (35.6369)
2022-11-25 00:57:36,254:INFO: Dataset: hotel               Batch: 3/4	Loss 36.0069 (35.7519)
2022-11-25 00:57:36,256:INFO: Dataset: hotel               Batch: 4/4	Loss 36.1850 (35.8193)
2022-11-25 00:57:36,524:INFO: Dataset: univ                Batch:  1/15	Loss 15.0343 (15.0343)
2022-11-25 00:57:36,527:INFO: Dataset: univ                Batch:  2/15	Loss 15.0283 (15.0312)
2022-11-25 00:57:36,528:INFO: Dataset: univ                Batch:  3/15	Loss 15.0367 (15.0331)
2022-11-25 00:57:36,530:INFO: Dataset: univ                Batch:  4/15	Loss 15.1255 (15.0579)
2022-11-25 00:57:36,572:INFO: Dataset: univ                Batch:  5/15	Loss 15.0869 (15.0642)
2022-11-25 00:57:36,578:INFO: Dataset: univ                Batch:  6/15	Loss 15.0078 (15.0551)
2022-11-25 00:57:36,579:INFO: Dataset: univ                Batch:  7/15	Loss 15.0286 (15.0512)
2022-11-25 00:57:36,580:INFO: Dataset: univ                Batch:  8/15	Loss 14.9967 (15.0442)
2022-11-25 00:57:36,581:INFO: Dataset: univ                Batch:  9/15	Loss 14.8863 (15.0269)
2022-11-25 00:57:36,583:INFO: Dataset: univ                Batch: 10/15	Loss 14.9956 (15.0236)
2022-11-25 00:57:36,584:INFO: Dataset: univ                Batch: 11/15	Loss 14.8819 (15.0107)
2022-11-25 00:57:36,586:INFO: Dataset: univ                Batch: 12/15	Loss 14.8171 (14.9949)
2022-11-25 00:57:36,587:INFO: Dataset: univ                Batch: 13/15	Loss 14.7425 (14.9758)
2022-11-25 00:57:36,588:INFO: Dataset: univ                Batch: 14/15	Loss 14.5847 (14.9506)
2022-11-25 00:57:36,589:INFO: Dataset: univ                Batch: 15/15	Loss 14.7105 (14.9483)
2022-11-25 00:57:36,859:INFO: Dataset: zara1               Batch: 1/8	Loss 30.2763 (30.2763)
2022-11-25 00:57:36,862:INFO: Dataset: zara1               Batch: 2/8	Loss 30.7057 (30.4849)
2022-11-25 00:57:36,863:INFO: Dataset: zara1               Batch: 3/8	Loss 30.6071 (30.5282)
2022-11-25 00:57:36,865:INFO: Dataset: zara1               Batch: 4/8	Loss 30.7133 (30.5716)
2022-11-25 00:57:36,868:INFO: Dataset: zara1               Batch: 5/8	Loss 30.3021 (30.5226)
2022-11-25 00:57:36,920:INFO: Dataset: zara1               Batch: 6/8	Loss 30.3086 (30.4855)
2022-11-25 00:57:36,921:INFO: Dataset: zara1               Batch: 7/8	Loss 30.4033 (30.4746)
2022-11-25 00:57:36,922:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3790 (30.4638)
2022-11-25 00:57:37,201:INFO: Dataset: zara2               Batch:  1/18	Loss 14.5050 (14.5050)
2022-11-25 00:57:37,205:INFO: Dataset: zara2               Batch:  2/18	Loss 14.5856 (14.5434)
2022-11-25 00:57:37,207:INFO: Dataset: zara2               Batch:  3/18	Loss 14.3506 (14.4764)
2022-11-25 00:57:37,212:INFO: Dataset: zara2               Batch:  4/18	Loss 14.6405 (14.5176)
2022-11-25 00:57:37,234:INFO: Dataset: zara2               Batch:  5/18	Loss 14.6758 (14.5498)
2022-11-25 00:57:37,253:INFO: Dataset: zara2               Batch:  6/18	Loss 14.5529 (14.5503)
2022-11-25 00:57:37,254:INFO: Dataset: zara2               Batch:  7/18	Loss 14.5829 (14.5548)
2022-11-25 00:57:37,255:INFO: Dataset: zara2               Batch:  8/18	Loss 14.7035 (14.5733)
2022-11-25 00:57:37,256:INFO: Dataset: zara2               Batch:  9/18	Loss 14.7852 (14.5970)
2022-11-25 00:57:37,257:INFO: Dataset: zara2               Batch: 10/18	Loss 14.6742 (14.6045)
2022-11-25 00:57:37,258:INFO: Dataset: zara2               Batch: 11/18	Loss 14.4222 (14.5881)
2022-11-25 00:57:37,259:INFO: Dataset: zara2               Batch: 12/18	Loss 14.3476 (14.5681)
2022-11-25 00:57:37,260:INFO: Dataset: zara2               Batch: 13/18	Loss 14.4506 (14.5582)
2022-11-25 00:57:37,261:INFO: Dataset: zara2               Batch: 14/18	Loss 14.2465 (14.5364)
2022-11-25 00:57:37,262:INFO: Dataset: zara2               Batch: 15/18	Loss 14.3225 (14.5223)
2022-11-25 00:57:37,263:INFO: Dataset: zara2               Batch: 16/18	Loss 14.3391 (14.5099)
2022-11-25 00:57:37,264:INFO: Dataset: zara2               Batch: 17/18	Loss 14.1632 (14.4876)
2022-11-25 00:57:37,266:INFO: Dataset: zara2               Batch: 18/18	Loss 13.8873 (14.4616)
2022-11-25 00:57:37,311:INFO: - Computing ADE (validation o)
2022-11-25 00:57:37,580:INFO: 		 ADE on eth                       dataset:	 1.0427576303482056
2022-11-25 00:57:37,581:INFO: Average validation o:	ADE  1.0428	FDE  2.1302
2022-11-25 00:57:37,581:INFO: - Computing loss (validation)
2022-11-25 00:57:37,784:INFO: Dataset: hotel               Batch: 1/2	Loss 36.1256 (36.1256)
2022-11-25 00:57:37,786:INFO: Dataset: hotel               Batch: 2/2	Loss 35.0375 (36.0662)
2022-11-25 00:57:38,074:INFO: Dataset: univ                Batch: 1/3	Loss 14.4066 (14.4066)
2022-11-25 00:57:38,077:INFO: Dataset: univ                Batch: 2/3	Loss 14.3992 (14.4031)
2022-11-25 00:57:38,082:INFO: Dataset: univ                Batch: 3/3	Loss 14.4285 (14.4117)
2022-11-25 00:57:38,345:INFO: Dataset: zara1               Batch: 1/2	Loss 29.6798 (29.6798)
2022-11-25 00:57:38,348:INFO: Dataset: zara1               Batch: 2/2	Loss 29.6111 (29.6634)
2022-11-25 00:57:38,641:INFO: Dataset: zara2               Batch: 1/5	Loss 14.4290 (14.4290)
2022-11-25 00:57:38,642:INFO: Dataset: zara2               Batch: 2/5	Loss 14.1574 (14.2882)
2022-11-25 00:57:38,644:INFO: Dataset: zara2               Batch: 3/5	Loss 14.2394 (14.2725)
2022-11-25 00:57:38,644:INFO: Dataset: zara2               Batch: 4/5	Loss 14.2499 (14.2669)
2022-11-25 00:57:38,645:INFO: Dataset: zara2               Batch: 5/5	Loss 14.3477 (14.2815)
2022-11-25 00:57:38,698:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_567.pth.tar
2022-11-25 00:57:38,698:INFO: 
===> EPOCH: 568 (P4)
2022-11-25 00:57:38,698:INFO: - Computing loss (training)
2022-11-25 00:57:38,908:INFO: Dataset: hotel               Batch: 1/4	Loss 36.2795 (36.2795)
2022-11-25 00:57:38,927:INFO: Dataset: hotel               Batch: 2/4	Loss 36.1547 (36.2183)
2022-11-25 00:57:38,928:INFO: Dataset: hotel               Batch: 3/4	Loss 36.2958 (36.2451)
2022-11-25 00:57:38,929:INFO: Dataset: hotel               Batch: 4/4	Loss 36.3294 (36.2593)
2022-11-25 00:57:39,188:INFO: Dataset: univ                Batch:  1/15	Loss 14.6771 (14.6771)
2022-11-25 00:57:39,192:INFO: Dataset: univ                Batch:  2/15	Loss 14.6954 (14.6862)
2022-11-25 00:57:39,193:INFO: Dataset: univ                Batch:  3/15	Loss 14.7077 (14.6931)
2022-11-25 00:57:39,196:INFO: Dataset: univ                Batch:  4/15	Loss 14.7297 (14.7018)
2022-11-25 00:57:39,214:INFO: Dataset: univ                Batch:  5/15	Loss 14.7225 (14.7056)
2022-11-25 00:57:39,255:INFO: Dataset: univ                Batch:  6/15	Loss 14.7019 (14.7049)
2022-11-25 00:57:39,259:INFO: Dataset: univ                Batch:  7/15	Loss 14.6793 (14.7011)
2022-11-25 00:57:39,263:INFO: Dataset: univ                Batch:  8/15	Loss 14.6757 (14.6977)
2022-11-25 00:57:39,266:INFO: Dataset: univ                Batch:  9/15	Loss 14.5335 (14.6796)
2022-11-25 00:57:39,270:INFO: Dataset: univ                Batch: 10/15	Loss 14.5964 (14.6713)
2022-11-25 00:57:39,274:INFO: Dataset: univ                Batch: 11/15	Loss 14.5107 (14.6573)
2022-11-25 00:57:39,279:INFO: Dataset: univ                Batch: 12/15	Loss 14.4903 (14.6420)
2022-11-25 00:57:39,281:INFO: Dataset: univ                Batch: 13/15	Loss 14.3212 (14.6170)
2022-11-25 00:57:39,282:INFO: Dataset: univ                Batch: 14/15	Loss 14.3049 (14.5946)
2022-11-25 00:57:39,284:INFO: Dataset: univ                Batch: 15/15	Loss 14.1001 (14.5886)
2022-11-25 00:57:39,604:INFO: Dataset: zara1               Batch: 1/8	Loss 30.7150 (30.7150)
2022-11-25 00:57:39,608:INFO: Dataset: zara1               Batch: 2/8	Loss 30.9609 (30.8273)
2022-11-25 00:57:39,628:INFO: Dataset: zara1               Batch: 3/8	Loss 30.6863 (30.7763)
2022-11-25 00:57:39,630:INFO: Dataset: zara1               Batch: 4/8	Loss 30.6469 (30.7402)
2022-11-25 00:57:39,632:INFO: Dataset: zara1               Batch: 5/8	Loss 30.6288 (30.7189)
2022-11-25 00:57:39,690:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6797 (30.7122)
2022-11-25 00:57:39,692:INFO: Dataset: zara1               Batch: 7/8	Loss 30.3947 (30.6705)
2022-11-25 00:57:39,693:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4062 (30.6390)
2022-11-25 00:57:40,101:INFO: Dataset: zara2               Batch:  1/18	Loss 14.2318 (14.2318)
2022-11-25 00:57:40,103:INFO: Dataset: zara2               Batch:  2/18	Loss 14.2006 (14.2169)
2022-11-25 00:57:40,105:INFO: Dataset: zara2               Batch:  3/18	Loss 14.2804 (14.2391)
2022-11-25 00:57:40,108:INFO: Dataset: zara2               Batch:  4/18	Loss 14.3364 (14.2647)
2022-11-25 00:57:40,109:INFO: Dataset: zara2               Batch:  5/18	Loss 14.3447 (14.2813)
2022-11-25 00:57:40,118:INFO: Dataset: zara2               Batch:  6/18	Loss 14.2429 (14.2755)
2022-11-25 00:57:40,120:INFO: Dataset: zara2               Batch:  7/18	Loss 14.2629 (14.2737)
2022-11-25 00:57:40,121:INFO: Dataset: zara2               Batch:  8/18	Loss 14.1192 (14.2535)
2022-11-25 00:57:40,123:INFO: Dataset: zara2               Batch:  9/18	Loss 14.3826 (14.2689)
2022-11-25 00:57:40,125:INFO: Dataset: zara2               Batch: 10/18	Loss 14.2072 (14.2626)
2022-11-25 00:57:40,128:INFO: Dataset: zara2               Batch: 11/18	Loss 14.1098 (14.2489)
2022-11-25 00:57:40,131:INFO: Dataset: zara2               Batch: 12/18	Loss 14.1404 (14.2401)
2022-11-25 00:57:40,133:INFO: Dataset: zara2               Batch: 13/18	Loss 14.0535 (14.2245)
2022-11-25 00:57:40,136:INFO: Dataset: zara2               Batch: 14/18	Loss 14.1807 (14.2212)
2022-11-25 00:57:40,139:INFO: Dataset: zara2               Batch: 15/18	Loss 13.9067 (14.1993)
2022-11-25 00:57:40,142:INFO: Dataset: zara2               Batch: 16/18	Loss 13.8899 (14.1803)
2022-11-25 00:57:40,145:INFO: Dataset: zara2               Batch: 17/18	Loss 13.8677 (14.1607)
2022-11-25 00:57:40,148:INFO: Dataset: zara2               Batch: 18/18	Loss 13.8155 (14.1436)
2022-11-25 00:57:40,213:INFO: - Computing ADE (validation o)
2022-11-25 00:57:40,510:INFO: 		 ADE on eth                       dataset:	 1.0961132049560547
2022-11-25 00:57:40,510:INFO: Average validation o:	ADE  1.0961	FDE  2.2071
2022-11-25 00:57:40,510:INFO: - Computing loss (validation)
2022-11-25 00:57:40,718:INFO: Dataset: hotel               Batch: 1/2	Loss 36.5378 (36.5378)
2022-11-25 00:57:40,744:INFO: Dataset: hotel               Batch: 2/2	Loss 36.7925 (36.5570)
2022-11-25 00:57:40,994:INFO: Dataset: univ                Batch: 1/3	Loss 14.1359 (14.1359)
2022-11-25 00:57:40,996:INFO: Dataset: univ                Batch: 2/3	Loss 14.0659 (14.1043)
2022-11-25 00:57:40,999:INFO: Dataset: univ                Batch: 3/3	Loss 14.1129 (14.1072)
2022-11-25 00:57:41,261:INFO: Dataset: zara1               Batch: 1/2	Loss 29.8747 (29.8747)
2022-11-25 00:57:41,261:INFO: Dataset: zara1               Batch: 2/2	Loss 29.8035 (29.8562)
2022-11-25 00:57:41,545:INFO: Dataset: zara2               Batch: 1/5	Loss 13.8797 (13.8797)
2022-11-25 00:57:41,547:INFO: Dataset: zara2               Batch: 2/5	Loss 13.9049 (13.8925)
2022-11-25 00:57:41,547:INFO: Dataset: zara2               Batch: 3/5	Loss 13.8201 (13.8680)
2022-11-25 00:57:41,548:INFO: Dataset: zara2               Batch: 4/5	Loss 13.9998 (13.9007)
2022-11-25 00:57:41,549:INFO: Dataset: zara2               Batch: 5/5	Loss 13.9555 (13.9113)
2022-11-25 00:57:41,604:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_568.pth.tar
2022-11-25 00:57:41,605:INFO: 
===> EPOCH: 569 (P4)
2022-11-25 00:57:41,605:INFO: - Computing loss (training)
2022-11-25 00:57:41,811:INFO: Dataset: hotel               Batch: 1/4	Loss 36.4623 (36.4623)
2022-11-25 00:57:41,813:INFO: Dataset: hotel               Batch: 2/4	Loss 36.5695 (36.5158)
2022-11-25 00:57:41,822:INFO: Dataset: hotel               Batch: 3/4	Loss 36.9472 (36.6495)
2022-11-25 00:57:41,832:INFO: Dataset: hotel               Batch: 4/4	Loss 36.8750 (36.6849)
2022-11-25 00:57:42,083:INFO: Dataset: univ                Batch:  1/15	Loss 14.3256 (14.3256)
2022-11-25 00:57:42,114:INFO: Dataset: univ                Batch:  2/15	Loss 14.4130 (14.3705)
2022-11-25 00:57:42,115:INFO: Dataset: univ                Batch:  3/15	Loss 14.4300 (14.3892)
2022-11-25 00:57:42,117:INFO: Dataset: univ                Batch:  4/15	Loss 14.4378 (14.4023)
2022-11-25 00:57:42,118:INFO: Dataset: univ                Batch:  5/15	Loss 14.4168 (14.4051)
2022-11-25 00:57:42,121:INFO: Dataset: univ                Batch:  6/15	Loss 14.4436 (14.4114)
2022-11-25 00:57:42,122:INFO: Dataset: univ                Batch:  7/15	Loss 14.3001 (14.3988)
2022-11-25 00:57:42,123:INFO: Dataset: univ                Batch:  8/15	Loss 14.3837 (14.3970)
2022-11-25 00:57:42,124:INFO: Dataset: univ                Batch:  9/15	Loss 14.3207 (14.3886)
2022-11-25 00:57:42,125:INFO: Dataset: univ                Batch: 10/15	Loss 14.2597 (14.3769)
2022-11-25 00:57:42,127:INFO: Dataset: univ                Batch: 11/15	Loss 14.1585 (14.3579)
2022-11-25 00:57:42,128:INFO: Dataset: univ                Batch: 12/15	Loss 14.0821 (14.3363)
2022-11-25 00:57:42,129:INFO: Dataset: univ                Batch: 13/15	Loss 14.0698 (14.3154)
2022-11-25 00:57:42,131:INFO: Dataset: univ                Batch: 14/15	Loss 13.9534 (14.2933)
2022-11-25 00:57:42,133:INFO: Dataset: univ                Batch: 15/15	Loss 13.8121 (14.2882)
2022-11-25 00:57:42,400:INFO: Dataset: zara1               Batch: 1/8	Loss 30.9495 (30.9495)
2022-11-25 00:57:42,402:INFO: Dataset: zara1               Batch: 2/8	Loss 30.8119 (30.8799)
2022-11-25 00:57:42,404:INFO: Dataset: zara1               Batch: 3/8	Loss 31.0248 (30.9259)
2022-11-25 00:57:42,406:INFO: Dataset: zara1               Batch: 4/8	Loss 30.9160 (30.9233)
2022-11-25 00:57:42,407:INFO: Dataset: zara1               Batch: 5/8	Loss 30.6721 (30.8737)
2022-11-25 00:57:42,437:INFO: Dataset: zara1               Batch: 6/8	Loss 30.9745 (30.8890)
2022-11-25 00:57:42,438:INFO: Dataset: zara1               Batch: 7/8	Loss 30.6562 (30.8552)
2022-11-25 00:57:42,439:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5883 (30.8268)
2022-11-25 00:57:42,702:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8081 (13.8081)
2022-11-25 00:57:42,705:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9105 (13.8595)
2022-11-25 00:57:42,706:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0985 (13.9374)
2022-11-25 00:57:42,745:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9660 (13.9444)
2022-11-25 00:57:42,746:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0602 (13.9650)
2022-11-25 00:57:42,751:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0120 (13.9728)
2022-11-25 00:57:42,752:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0845 (13.9882)
2022-11-25 00:57:42,753:INFO: Dataset: zara2               Batch:  8/18	Loss 14.1348 (14.0080)
2022-11-25 00:57:42,754:INFO: Dataset: zara2               Batch:  9/18	Loss 14.0138 (14.0087)
2022-11-25 00:57:42,755:INFO: Dataset: zara2               Batch: 10/18	Loss 14.1140 (14.0196)
2022-11-25 00:57:42,756:INFO: Dataset: zara2               Batch: 11/18	Loss 14.0031 (14.0182)
2022-11-25 00:57:42,757:INFO: Dataset: zara2               Batch: 12/18	Loss 13.7066 (13.9907)
2022-11-25 00:57:42,758:INFO: Dataset: zara2               Batch: 13/18	Loss 13.9174 (13.9845)
2022-11-25 00:57:42,759:INFO: Dataset: zara2               Batch: 14/18	Loss 13.7737 (13.9689)
2022-11-25 00:57:42,760:INFO: Dataset: zara2               Batch: 15/18	Loss 13.6335 (13.9466)
2022-11-25 00:57:42,761:INFO: Dataset: zara2               Batch: 16/18	Loss 13.6570 (13.9277)
2022-11-25 00:57:42,763:INFO: Dataset: zara2               Batch: 17/18	Loss 13.5840 (13.9058)
2022-11-25 00:57:42,764:INFO: Dataset: zara2               Batch: 18/18	Loss 13.3935 (13.8783)
2022-11-25 00:57:42,813:INFO: - Computing ADE (validation o)
2022-11-25 00:57:43,087:INFO: 		 ADE on eth                       dataset:	 1.0447925329208374
2022-11-25 00:57:43,095:INFO: Average validation o:	ADE  1.0448	FDE  2.1382
2022-11-25 00:57:43,095:INFO: - Computing loss (validation)
2022-11-25 00:57:43,278:INFO: Dataset: hotel               Batch: 1/2	Loss 36.9779 (36.9779)
2022-11-25 00:57:43,282:INFO: Dataset: hotel               Batch: 2/2	Loss 37.6133 (37.0082)
2022-11-25 00:57:43,533:INFO: Dataset: univ                Batch: 1/3	Loss 13.8127 (13.8127)
2022-11-25 00:57:43,536:INFO: Dataset: univ                Batch: 2/3	Loss 13.8552 (13.8366)
2022-11-25 00:57:43,537:INFO: Dataset: univ                Batch: 3/3	Loss 13.8252 (13.8330)
2022-11-25 00:57:43,784:INFO: Dataset: zara1               Batch: 1/2	Loss 30.0376 (30.0376)
2022-11-25 00:57:43,785:INFO: Dataset: zara1               Batch: 2/2	Loss 30.0172 (30.0325)
2022-11-25 00:57:44,061:INFO: Dataset: zara2               Batch: 1/5	Loss 13.5831 (13.5831)
2022-11-25 00:57:44,061:INFO: Dataset: zara2               Batch: 2/5	Loss 13.5678 (13.5756)
2022-11-25 00:57:44,062:INFO: Dataset: zara2               Batch: 3/5	Loss 13.6460 (13.5998)
2022-11-25 00:57:44,062:INFO: Dataset: zara2               Batch: 4/5	Loss 13.6234 (13.6057)
2022-11-25 00:57:44,062:INFO: Dataset: zara2               Batch: 5/5	Loss 13.5785 (13.6002)
2022-11-25 00:57:44,118:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_569.pth.tar
2022-11-25 00:57:44,118:INFO: 
===> EPOCH: 570 (P4)
2022-11-25 00:57:44,118:INFO: - Computing loss (training)
2022-11-25 00:57:44,327:INFO: Dataset: hotel               Batch: 1/4	Loss 37.0275 (37.0275)
2022-11-25 00:57:44,331:INFO: Dataset: hotel               Batch: 2/4	Loss 36.8964 (36.9623)
2022-11-25 00:57:44,332:INFO: Dataset: hotel               Batch: 3/4	Loss 37.0851 (37.0015)
2022-11-25 00:57:44,334:INFO: Dataset: hotel               Batch: 4/4	Loss 37.3838 (37.0651)
2022-11-25 00:57:44,597:INFO: Dataset: univ                Batch:  1/15	Loss 14.1097 (14.1097)
2022-11-25 00:57:44,600:INFO: Dataset: univ                Batch:  2/15	Loss 14.1357 (14.1225)
2022-11-25 00:57:44,601:INFO: Dataset: univ                Batch:  3/15	Loss 14.1842 (14.1431)
2022-11-25 00:57:44,667:INFO: Dataset: univ                Batch:  4/15	Loss 14.1266 (14.1391)
2022-11-25 00:57:44,668:INFO: Dataset: univ                Batch:  5/15	Loss 14.1770 (14.1464)
2022-11-25 00:57:44,673:INFO: Dataset: univ                Batch:  6/15	Loss 14.1499 (14.1469)
2022-11-25 00:57:44,674:INFO: Dataset: univ                Batch:  7/15	Loss 14.1384 (14.1458)
2022-11-25 00:57:44,675:INFO: Dataset: univ                Batch:  8/15	Loss 14.0818 (14.1379)
2022-11-25 00:57:44,676:INFO: Dataset: univ                Batch:  9/15	Loss 14.0459 (14.1273)
2022-11-25 00:57:44,677:INFO: Dataset: univ                Batch: 10/15	Loss 13.9675 (14.1121)
2022-11-25 00:57:44,679:INFO: Dataset: univ                Batch: 11/15	Loss 13.9036 (14.0927)
2022-11-25 00:57:44,680:INFO: Dataset: univ                Batch: 12/15	Loss 13.7937 (14.0697)
2022-11-25 00:57:44,681:INFO: Dataset: univ                Batch: 13/15	Loss 13.7782 (14.0488)
2022-11-25 00:57:44,682:INFO: Dataset: univ                Batch: 14/15	Loss 13.6666 (14.0214)
2022-11-25 00:57:44,683:INFO: Dataset: univ                Batch: 15/15	Loss 13.5911 (14.0162)
2022-11-25 00:57:44,944:INFO: Dataset: zara1               Batch: 1/8	Loss 31.1553 (31.1553)
2022-11-25 00:57:44,947:INFO: Dataset: zara1               Batch: 2/8	Loss 31.0770 (31.1176)
2022-11-25 00:57:44,952:INFO: Dataset: zara1               Batch: 3/8	Loss 31.1480 (31.1276)
2022-11-25 00:57:44,953:INFO: Dataset: zara1               Batch: 4/8	Loss 31.0322 (31.1056)
2022-11-25 00:57:44,954:INFO: Dataset: zara1               Batch: 5/8	Loss 31.0776 (31.1004)
2022-11-25 00:57:44,980:INFO: Dataset: zara1               Batch: 6/8	Loss 30.8810 (31.0653)
2022-11-25 00:57:44,981:INFO: Dataset: zara1               Batch: 7/8	Loss 30.8516 (31.0385)
2022-11-25 00:57:44,982:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8902 (31.0240)
2022-11-25 00:57:45,239:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5246 (13.5246)
2022-11-25 00:57:45,241:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8175 (13.6717)
2022-11-25 00:57:45,244:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8222 (13.7185)
2022-11-25 00:57:45,247:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7746 (13.7320)
2022-11-25 00:57:45,280:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8954 (13.7636)
2022-11-25 00:57:45,284:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8192 (13.7731)
2022-11-25 00:57:45,285:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8901 (13.7881)
2022-11-25 00:57:45,286:INFO: Dataset: zara2               Batch:  8/18	Loss 14.0163 (13.8174)
2022-11-25 00:57:45,287:INFO: Dataset: zara2               Batch:  9/18	Loss 13.8064 (13.8161)
2022-11-25 00:57:45,288:INFO: Dataset: zara2               Batch: 10/18	Loss 13.7952 (13.8141)
2022-11-25 00:57:45,289:INFO: Dataset: zara2               Batch: 11/18	Loss 13.6929 (13.8037)
2022-11-25 00:57:45,291:INFO: Dataset: zara2               Batch: 12/18	Loss 13.6887 (13.7932)
2022-11-25 00:57:45,292:INFO: Dataset: zara2               Batch: 13/18	Loss 13.5731 (13.7747)
2022-11-25 00:57:45,293:INFO: Dataset: zara2               Batch: 14/18	Loss 13.5313 (13.7570)
2022-11-25 00:57:45,294:INFO: Dataset: zara2               Batch: 15/18	Loss 13.4472 (13.7343)
2022-11-25 00:57:45,295:INFO: Dataset: zara2               Batch: 16/18	Loss 13.5406 (13.7222)
2022-11-25 00:57:45,297:INFO: Dataset: zara2               Batch: 17/18	Loss 13.2548 (13.6940)
2022-11-25 00:57:45,298:INFO: Dataset: zara2               Batch: 18/18	Loss 13.3225 (13.6759)
2022-11-25 00:57:45,345:INFO: - Computing ADE (validation o)
2022-11-25 00:57:45,604:INFO: 		 ADE on eth                       dataset:	 1.0908323526382446
2022-11-25 00:57:45,604:INFO: Average validation o:	ADE  1.0908	FDE  2.1851
2022-11-25 00:57:45,605:INFO: - Computing loss (validation)
2022-11-25 00:57:45,806:INFO: Dataset: hotel               Batch: 1/2	Loss 37.4055 (37.4055)
2022-11-25 00:57:45,806:INFO: Dataset: hotel               Batch: 2/2	Loss 37.4777 (37.4126)
2022-11-25 00:57:46,060:INFO: Dataset: univ                Batch: 1/3	Loss 13.5916 (13.5916)
2022-11-25 00:57:46,062:INFO: Dataset: univ                Batch: 2/3	Loss 13.5718 (13.5811)
2022-11-25 00:57:46,063:INFO: Dataset: univ                Batch: 3/3	Loss 13.6676 (13.6039)
2022-11-25 00:57:46,325:INFO: Dataset: zara1               Batch: 1/2	Loss 30.1994 (30.1994)
2022-11-25 00:57:46,327:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1852 (30.1961)
2022-11-25 00:57:46,585:INFO: Dataset: zara2               Batch: 1/5	Loss 13.3001 (13.3001)
2022-11-25 00:57:46,587:INFO: Dataset: zara2               Batch: 2/5	Loss 13.4246 (13.3613)
2022-11-25 00:57:46,588:INFO: Dataset: zara2               Batch: 3/5	Loss 13.3716 (13.3646)
2022-11-25 00:57:46,589:INFO: Dataset: zara2               Batch: 4/5	Loss 13.3398 (13.3580)
2022-11-25 00:57:46,590:INFO: Dataset: zara2               Batch: 5/5	Loss 13.2710 (13.3416)
2022-11-25 00:57:46,644:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_570.pth.tar
2022-11-25 00:57:46,644:INFO: 
===> EPOCH: 571 (P4)
2022-11-25 00:57:46,645:INFO: - Computing loss (training)
2022-11-25 00:57:46,839:INFO: Dataset: hotel               Batch: 1/4	Loss 37.6068 (37.6068)
2022-11-25 00:57:46,843:INFO: Dataset: hotel               Batch: 2/4	Loss 37.3022 (37.4538)
2022-11-25 00:57:46,844:INFO: Dataset: hotel               Batch: 3/4	Loss 37.1563 (37.3512)
2022-11-25 00:57:46,855:INFO: Dataset: hotel               Batch: 4/4	Loss 37.5465 (37.3836)
2022-11-25 00:57:47,120:INFO: Dataset: univ                Batch:  1/15	Loss 13.8677 (13.8677)
2022-11-25 00:57:47,124:INFO: Dataset: univ                Batch:  2/15	Loss 13.9817 (13.9239)
2022-11-25 00:57:47,125:INFO: Dataset: univ                Batch:  3/15	Loss 13.9367 (13.9283)
2022-11-25 00:57:47,128:INFO: Dataset: univ                Batch:  4/15	Loss 13.9010 (13.9221)
2022-11-25 00:57:47,169:INFO: Dataset: univ                Batch:  5/15	Loss 13.9258 (13.9228)
2022-11-25 00:57:47,171:INFO: Dataset: univ                Batch:  6/15	Loss 13.9057 (13.9201)
2022-11-25 00:57:47,172:INFO: Dataset: univ                Batch:  7/15	Loss 13.9049 (13.9180)
2022-11-25 00:57:47,173:INFO: Dataset: univ                Batch:  8/15	Loss 13.8665 (13.9116)
2022-11-25 00:57:47,174:INFO: Dataset: univ                Batch:  9/15	Loss 13.8097 (13.9007)
2022-11-25 00:57:47,175:INFO: Dataset: univ                Batch: 10/15	Loss 13.7768 (13.8877)
2022-11-25 00:57:47,177:INFO: Dataset: univ                Batch: 11/15	Loss 13.6960 (13.8697)
2022-11-25 00:57:47,178:INFO: Dataset: univ                Batch: 12/15	Loss 13.6033 (13.8456)
2022-11-25 00:57:47,180:INFO: Dataset: univ                Batch: 13/15	Loss 13.5418 (13.8216)
2022-11-25 00:57:47,181:INFO: Dataset: univ                Batch: 14/15	Loss 13.4364 (13.7962)
2022-11-25 00:57:47,181:INFO: Dataset: univ                Batch: 15/15	Loss 13.2981 (13.7902)
2022-11-25 00:57:47,458:INFO: Dataset: zara1               Batch: 1/8	Loss 31.3364 (31.3364)
2022-11-25 00:57:47,461:INFO: Dataset: zara1               Batch: 2/8	Loss 31.2182 (31.2772)
2022-11-25 00:57:47,465:INFO: Dataset: zara1               Batch: 3/8	Loss 31.3346 (31.2950)
2022-11-25 00:57:47,466:INFO: Dataset: zara1               Batch: 4/8	Loss 31.4103 (31.3237)
2022-11-25 00:57:47,467:INFO: Dataset: zara1               Batch: 5/8	Loss 31.2413 (31.3088)
2022-11-25 00:57:47,501:INFO: Dataset: zara1               Batch: 6/8	Loss 31.0488 (31.2659)
2022-11-25 00:57:47,502:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0724 (31.2391)
2022-11-25 00:57:47,503:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8124 (31.1901)
2022-11-25 00:57:47,770:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6384 (13.6384)
2022-11-25 00:57:47,771:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6247 (13.6319)
2022-11-25 00:57:47,774:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6569 (13.6404)
2022-11-25 00:57:47,776:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7310 (13.6658)
2022-11-25 00:57:47,797:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7724 (13.6867)
2022-11-25 00:57:47,809:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7621 (13.7002)
2022-11-25 00:57:47,811:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7325 (13.7050)
2022-11-25 00:57:47,812:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6706 (13.7005)
2022-11-25 00:57:47,813:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6949 (13.6999)
2022-11-25 00:57:47,813:INFO: Dataset: zara2               Batch: 10/18	Loss 13.6560 (13.6954)
2022-11-25 00:57:47,815:INFO: Dataset: zara2               Batch: 11/18	Loss 13.6051 (13.6868)
2022-11-25 00:57:47,816:INFO: Dataset: zara2               Batch: 12/18	Loss 13.5375 (13.6736)
2022-11-25 00:57:47,817:INFO: Dataset: zara2               Batch: 13/18	Loss 13.4721 (13.6574)
2022-11-25 00:57:47,818:INFO: Dataset: zara2               Batch: 14/18	Loss 13.3121 (13.6343)
2022-11-25 00:57:47,819:INFO: Dataset: zara2               Batch: 15/18	Loss 13.1250 (13.6036)
2022-11-25 00:57:47,820:INFO: Dataset: zara2               Batch: 16/18	Loss 13.3520 (13.5877)
2022-11-25 00:57:47,821:INFO: Dataset: zara2               Batch: 17/18	Loss 13.0641 (13.5545)
2022-11-25 00:57:47,823:INFO: Dataset: zara2               Batch: 18/18	Loss 12.9470 (13.5261)
2022-11-25 00:57:47,872:INFO: - Computing ADE (validation o)
2022-11-25 00:57:48,135:INFO: 		 ADE on eth                       dataset:	 1.1046148538589478
2022-11-25 00:57:48,135:INFO: Average validation o:	ADE  1.1046	FDE  2.2032
2022-11-25 00:57:48,136:INFO: - Computing loss (validation)
2022-11-25 00:57:48,332:INFO: Dataset: hotel               Batch: 1/2	Loss 37.7372 (37.7372)
2022-11-25 00:57:48,334:INFO: Dataset: hotel               Batch: 2/2	Loss 38.0243 (37.7617)
2022-11-25 00:57:48,576:INFO: Dataset: univ                Batch: 1/3	Loss 13.4082 (13.4082)
2022-11-25 00:57:48,578:INFO: Dataset: univ                Batch: 2/3	Loss 13.3592 (13.3838)
2022-11-25 00:57:48,579:INFO: Dataset: univ                Batch: 3/3	Loss 13.4103 (13.3913)
2022-11-25 00:57:48,835:INFO: Dataset: zara1               Batch: 1/2	Loss 30.3598 (30.3598)
2022-11-25 00:57:48,850:INFO: Dataset: zara1               Batch: 2/2	Loss 30.3509 (30.3575)
2022-11-25 00:57:49,096:INFO: Dataset: zara2               Batch: 1/5	Loss 13.1168 (13.1168)
2022-11-25 00:57:49,096:INFO: Dataset: zara2               Batch: 2/5	Loss 13.0489 (13.0828)
2022-11-25 00:57:49,097:INFO: Dataset: zara2               Batch: 3/5	Loss 13.1306 (13.0988)
2022-11-25 00:57:49,098:INFO: Dataset: zara2               Batch: 4/5	Loss 13.1963 (13.1233)
2022-11-25 00:57:49,100:INFO: Dataset: zara2               Batch: 5/5	Loss 13.1508 (13.1290)
2022-11-25 00:57:49,154:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_571.pth.tar
2022-11-25 00:57:49,155:INFO: 
===> EPOCH: 572 (P4)
2022-11-25 00:57:49,155:INFO: - Computing loss (training)
2022-11-25 00:57:49,349:INFO: Dataset: hotel               Batch: 1/4	Loss 37.6738 (37.6738)
2022-11-25 00:57:49,352:INFO: Dataset: hotel               Batch: 2/4	Loss 37.5721 (37.6234)
2022-11-25 00:57:49,355:INFO: Dataset: hotel               Batch: 3/4	Loss 37.6754 (37.6401)
2022-11-25 00:57:49,358:INFO: Dataset: hotel               Batch: 4/4	Loss 37.8599 (37.6731)
2022-11-25 00:57:49,618:INFO: Dataset: univ                Batch:  1/15	Loss 13.6831 (13.6831)
2022-11-25 00:57:49,621:INFO: Dataset: univ                Batch:  2/15	Loss 13.7263 (13.7044)
2022-11-25 00:57:49,624:INFO: Dataset: univ                Batch:  3/15	Loss 13.7303 (13.7132)
2022-11-25 00:57:49,626:INFO: Dataset: univ                Batch:  4/15	Loss 13.8175 (13.7377)
2022-11-25 00:57:49,645:INFO: Dataset: univ                Batch:  5/15	Loss 13.7450 (13.7391)
2022-11-25 00:57:49,669:INFO: Dataset: univ                Batch:  6/15	Loss 13.7972 (13.7480)
2022-11-25 00:57:49,670:INFO: Dataset: univ                Batch:  7/15	Loss 13.7295 (13.7450)
2022-11-25 00:57:49,672:INFO: Dataset: univ                Batch:  8/15	Loss 13.6507 (13.7335)
2022-11-25 00:57:49,673:INFO: Dataset: univ                Batch:  9/15	Loss 13.6295 (13.7222)
2022-11-25 00:57:49,674:INFO: Dataset: univ                Batch: 10/15	Loss 13.5917 (13.7084)
2022-11-25 00:57:49,675:INFO: Dataset: univ                Batch: 11/15	Loss 13.4861 (13.6874)
2022-11-25 00:57:49,677:INFO: Dataset: univ                Batch: 12/15	Loss 13.3828 (13.6605)
2022-11-25 00:57:49,678:INFO: Dataset: univ                Batch: 13/15	Loss 13.2637 (13.6278)
2022-11-25 00:57:49,679:INFO: Dataset: univ                Batch: 14/15	Loss 13.1913 (13.5961)
2022-11-25 00:57:49,680:INFO: Dataset: univ                Batch: 15/15	Loss 13.0823 (13.5892)
2022-11-25 00:57:49,948:INFO: Dataset: zara1               Batch: 1/8	Loss 31.4732 (31.4732)
2022-11-25 00:57:49,951:INFO: Dataset: zara1               Batch: 2/8	Loss 31.5370 (31.5044)
2022-11-25 00:57:49,953:INFO: Dataset: zara1               Batch: 3/8	Loss 31.4980 (31.5022)
2022-11-25 00:57:49,954:INFO: Dataset: zara1               Batch: 4/8	Loss 31.5186 (31.5063)
2022-11-25 00:57:49,956:INFO: Dataset: zara1               Batch: 5/8	Loss 31.3583 (31.4742)
2022-11-25 00:57:49,969:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2680 (31.4417)
2022-11-25 00:57:49,970:INFO: Dataset: zara1               Batch: 7/8	Loss 31.1674 (31.4018)
2022-11-25 00:57:49,970:INFO: Dataset: zara1               Batch: 8/8	Loss 30.9691 (31.3574)
2022-11-25 00:57:50,227:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3849 (13.3849)
2022-11-25 00:57:50,230:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5458 (13.4628)
2022-11-25 00:57:50,231:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7393 (13.5591)
2022-11-25 00:57:50,263:INFO: Dataset: zara2               Batch:  4/18	Loss 13.6446 (13.5798)
2022-11-25 00:57:50,264:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5780 (13.5794)
2022-11-25 00:57:50,269:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5753 (13.5787)
2022-11-25 00:57:50,270:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5404 (13.5728)
2022-11-25 00:57:50,271:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5591 (13.5709)
2022-11-25 00:57:50,272:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5690 (13.5707)
2022-11-25 00:57:50,273:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4955 (13.5638)
2022-11-25 00:57:50,275:INFO: Dataset: zara2               Batch: 11/18	Loss 13.5762 (13.5648)
2022-11-25 00:57:50,277:INFO: Dataset: zara2               Batch: 12/18	Loss 13.4695 (13.5573)
2022-11-25 00:57:50,278:INFO: Dataset: zara2               Batch: 13/18	Loss 13.3721 (13.5411)
2022-11-25 00:57:50,279:INFO: Dataset: zara2               Batch: 14/18	Loss 13.1857 (13.5150)
2022-11-25 00:57:50,280:INFO: Dataset: zara2               Batch: 15/18	Loss 13.0930 (13.4837)
2022-11-25 00:57:50,282:INFO: Dataset: zara2               Batch: 16/18	Loss 13.0388 (13.4538)
2022-11-25 00:57:50,283:INFO: Dataset: zara2               Batch: 17/18	Loss 12.9463 (13.4270)
2022-11-25 00:57:50,285:INFO: Dataset: zara2               Batch: 18/18	Loss 13.0065 (13.4075)
2022-11-25 00:57:50,333:INFO: - Computing ADE (validation o)
2022-11-25 00:57:50,696:INFO: 		 ADE on eth                       dataset:	 1.0473071336746216
2022-11-25 00:57:50,696:INFO: Average validation o:	ADE  1.0473	FDE  2.1089
2022-11-25 00:57:50,697:INFO: - Computing loss (validation)
2022-11-25 00:57:50,931:INFO: Dataset: hotel               Batch: 1/2	Loss 38.0369 (38.0369)
2022-11-25 00:57:50,933:INFO: Dataset: hotel               Batch: 2/2	Loss 37.8290 (38.0220)
2022-11-25 00:57:51,186:INFO: Dataset: univ                Batch: 1/3	Loss 13.2037 (13.2037)
2022-11-25 00:57:51,189:INFO: Dataset: univ                Batch: 2/3	Loss 13.2160 (13.2102)
2022-11-25 00:57:51,189:INFO: Dataset: univ                Batch: 3/3	Loss 13.2407 (13.2189)
2022-11-25 00:57:51,430:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4877 (30.4877)
2022-11-25 00:57:51,433:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4811 (30.4860)
2022-11-25 00:57:51,682:INFO: Dataset: zara2               Batch: 1/5	Loss 12.9678 (12.9678)
2022-11-25 00:57:51,700:INFO: Dataset: zara2               Batch: 2/5	Loss 13.0309 (12.9984)
2022-11-25 00:57:51,701:INFO: Dataset: zara2               Batch: 3/5	Loss 12.9316 (12.9763)
2022-11-25 00:57:51,701:INFO: Dataset: zara2               Batch: 4/5	Loss 12.9475 (12.9695)
2022-11-25 00:57:51,702:INFO: Dataset: zara2               Batch: 5/5	Loss 12.8837 (12.9530)
2022-11-25 00:57:51,757:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_572.pth.tar
2022-11-25 00:57:51,757:INFO: 
===> EPOCH: 573 (P4)
2022-11-25 00:57:51,757:INFO: - Computing loss (training)
2022-11-25 00:57:51,968:INFO: Dataset: hotel               Batch: 1/4	Loss 37.8615 (37.8615)
2022-11-25 00:57:51,975:INFO: Dataset: hotel               Batch: 2/4	Loss 38.0611 (37.9618)
2022-11-25 00:57:51,976:INFO: Dataset: hotel               Batch: 3/4	Loss 37.7995 (37.9114)
2022-11-25 00:57:51,976:INFO: Dataset: hotel               Batch: 4/4	Loss 37.6465 (37.8684)
2022-11-25 00:57:52,235:INFO: Dataset: univ                Batch:  1/15	Loss 13.5206 (13.5206)
2022-11-25 00:57:52,246:INFO: Dataset: univ                Batch:  2/15	Loss 13.5930 (13.5584)
2022-11-25 00:57:52,248:INFO: Dataset: univ                Batch:  3/15	Loss 13.6189 (13.5784)
2022-11-25 00:57:52,249:INFO: Dataset: univ                Batch:  4/15	Loss 13.6140 (13.5873)
2022-11-25 00:57:52,276:INFO: Dataset: univ                Batch:  5/15	Loss 13.6306 (13.5956)
2022-11-25 00:57:52,285:INFO: Dataset: univ                Batch:  6/15	Loss 13.6486 (13.6041)
2022-11-25 00:57:52,286:INFO: Dataset: univ                Batch:  7/15	Loss 13.5593 (13.5976)
2022-11-25 00:57:52,287:INFO: Dataset: univ                Batch:  8/15	Loss 13.5159 (13.5875)
2022-11-25 00:57:52,288:INFO: Dataset: univ                Batch:  9/15	Loss 13.4778 (13.5766)
2022-11-25 00:57:52,289:INFO: Dataset: univ                Batch: 10/15	Loss 13.3919 (13.5579)
2022-11-25 00:57:52,290:INFO: Dataset: univ                Batch: 11/15	Loss 13.3099 (13.5324)
2022-11-25 00:57:52,292:INFO: Dataset: univ                Batch: 12/15	Loss 13.2208 (13.5074)
2022-11-25 00:57:52,293:INFO: Dataset: univ                Batch: 13/15	Loss 13.1302 (13.4808)
2022-11-25 00:57:52,294:INFO: Dataset: univ                Batch: 14/15	Loss 12.9911 (13.4490)
2022-11-25 00:57:52,295:INFO: Dataset: univ                Batch: 15/15	Loss 12.8384 (13.4417)
2022-11-25 00:57:52,570:INFO: Dataset: zara1               Batch: 1/8	Loss 31.5923 (31.5923)
2022-11-25 00:57:52,575:INFO: Dataset: zara1               Batch: 2/8	Loss 31.7581 (31.6709)
2022-11-25 00:57:52,576:INFO: Dataset: zara1               Batch: 3/8	Loss 31.6419 (31.6609)
2022-11-25 00:57:52,578:INFO: Dataset: zara1               Batch: 4/8	Loss 31.6944 (31.6692)
2022-11-25 00:57:52,581:INFO: Dataset: zara1               Batch: 5/8	Loss 31.4205 (31.6211)
2022-11-25 00:57:52,629:INFO: Dataset: zara1               Batch: 6/8	Loss 31.5416 (31.6088)
2022-11-25 00:57:52,630:INFO: Dataset: zara1               Batch: 7/8	Loss 31.2828 (31.5579)
2022-11-25 00:57:52,631:INFO: Dataset: zara1               Batch: 8/8	Loss 31.1120 (31.5100)
2022-11-25 00:57:52,912:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4453 (13.4453)
2022-11-25 00:57:52,947:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3725 (13.4078)
2022-11-25 00:57:52,949:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5016 (13.4406)
2022-11-25 00:57:52,952:INFO: Dataset: zara2               Batch:  4/18	Loss 13.6249 (13.4910)
2022-11-25 00:57:52,954:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4996 (13.4927)
2022-11-25 00:57:52,961:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5403 (13.5006)
2022-11-25 00:57:52,962:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4441 (13.4920)
2022-11-25 00:57:52,963:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4936 (13.4922)
2022-11-25 00:57:52,964:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5889 (13.5032)
2022-11-25 00:57:52,965:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4267 (13.4956)
2022-11-25 00:57:52,966:INFO: Dataset: zara2               Batch: 11/18	Loss 13.4046 (13.4868)
2022-11-25 00:57:52,967:INFO: Dataset: zara2               Batch: 12/18	Loss 13.3989 (13.4787)
2022-11-25 00:57:52,968:INFO: Dataset: zara2               Batch: 13/18	Loss 13.1665 (13.4572)
2022-11-25 00:57:52,969:INFO: Dataset: zara2               Batch: 14/18	Loss 13.1051 (13.4313)
2022-11-25 00:57:52,970:INFO: Dataset: zara2               Batch: 15/18	Loss 13.0093 (13.4029)
2022-11-25 00:57:52,971:INFO: Dataset: zara2               Batch: 16/18	Loss 13.0120 (13.3785)
2022-11-25 00:57:52,972:INFO: Dataset: zara2               Batch: 17/18	Loss 12.8369 (13.3458)
2022-11-25 00:57:52,973:INFO: Dataset: zara2               Batch: 18/18	Loss 12.7706 (13.3183)
2022-11-25 00:57:53,021:INFO: - Computing ADE (validation o)
2022-11-25 00:57:53,285:INFO: 		 ADE on eth                       dataset:	 1.071049690246582
2022-11-25 00:57:53,285:INFO: Average validation o:	ADE  1.0710	FDE  2.1832
2022-11-25 00:57:53,286:INFO: - Computing loss (validation)
2022-11-25 00:57:53,485:INFO: Dataset: hotel               Batch: 1/2	Loss 38.2423 (38.2423)
2022-11-25 00:57:53,498:INFO: Dataset: hotel               Batch: 2/2	Loss 38.3581 (38.2494)
2022-11-25 00:57:53,767:INFO: Dataset: univ                Batch: 1/3	Loss 13.0873 (13.0873)
2022-11-25 00:57:53,768:INFO: Dataset: univ                Batch: 2/3	Loss 13.1341 (13.1080)
2022-11-25 00:57:53,781:INFO: Dataset: univ                Batch: 3/3	Loss 13.0469 (13.0862)
2022-11-25 00:57:54,080:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5937 (30.5937)
2022-11-25 00:57:54,085:INFO: Dataset: zara1               Batch: 2/2	Loss 30.5897 (30.5927)
2022-11-25 00:57:54,345:INFO: Dataset: zara2               Batch: 1/5	Loss 12.8253 (12.8253)
2022-11-25 00:57:54,346:INFO: Dataset: zara2               Batch: 2/5	Loss 12.8638 (12.8452)
2022-11-25 00:57:54,347:INFO: Dataset: zara2               Batch: 3/5	Loss 12.8291 (12.8400)
2022-11-25 00:57:54,353:INFO: Dataset: zara2               Batch: 4/5	Loss 12.7657 (12.8219)
2022-11-25 00:57:54,354:INFO: Dataset: zara2               Batch: 5/5	Loss 12.7393 (12.8059)
2022-11-25 00:57:54,427:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_573.pth.tar
2022-11-25 00:57:54,427:INFO: 
===> EPOCH: 574 (P4)
2022-11-25 00:57:54,427:INFO: - Computing loss (training)
2022-11-25 00:57:54,633:INFO: Dataset: hotel               Batch: 1/4	Loss 38.0739 (38.0739)
2022-11-25 00:57:54,635:INFO: Dataset: hotel               Batch: 2/4	Loss 38.1138 (38.0938)
2022-11-25 00:57:54,638:INFO: Dataset: hotel               Batch: 3/4	Loss 37.8573 (38.0184)
2022-11-25 00:57:54,639:INFO: Dataset: hotel               Batch: 4/4	Loss 37.8627 (37.9909)
2022-11-25 00:57:54,898:INFO: Dataset: univ                Batch:  1/15	Loss 13.3824 (13.3824)
2022-11-25 00:57:54,900:INFO: Dataset: univ                Batch:  2/15	Loss 13.4697 (13.4265)
2022-11-25 00:57:54,904:INFO: Dataset: univ                Batch:  3/15	Loss 13.4914 (13.4480)
2022-11-25 00:57:54,925:INFO: Dataset: univ                Batch:  4/15	Loss 13.5096 (13.4628)
2022-11-25 00:57:54,926:INFO: Dataset: univ                Batch:  5/15	Loss 13.5380 (13.4773)
2022-11-25 00:57:54,930:INFO: Dataset: univ                Batch:  6/15	Loss 13.4804 (13.4778)
2022-11-25 00:57:54,931:INFO: Dataset: univ                Batch:  7/15	Loss 13.4794 (13.4780)
2022-11-25 00:57:54,932:INFO: Dataset: univ                Batch:  8/15	Loss 13.3985 (13.4692)
2022-11-25 00:57:54,933:INFO: Dataset: univ                Batch:  9/15	Loss 13.3357 (13.4543)
2022-11-25 00:57:54,934:INFO: Dataset: univ                Batch: 10/15	Loss 13.2590 (13.4344)
2022-11-25 00:57:54,936:INFO: Dataset: univ                Batch: 11/15	Loss 13.1745 (13.4120)
2022-11-25 00:57:54,938:INFO: Dataset: univ                Batch: 12/15	Loss 13.0689 (13.3843)
2022-11-25 00:57:54,939:INFO: Dataset: univ                Batch: 13/15	Loss 12.9745 (13.3509)
2022-11-25 00:57:54,940:INFO: Dataset: univ                Batch: 14/15	Loss 12.8913 (13.3188)
2022-11-25 00:57:54,941:INFO: Dataset: univ                Batch: 15/15	Loss 12.7826 (13.3121)
2022-11-25 00:57:55,220:INFO: Dataset: zara1               Batch: 1/8	Loss 31.8127 (31.8127)
2022-11-25 00:57:55,224:INFO: Dataset: zara1               Batch: 2/8	Loss 31.7985 (31.8050)
2022-11-25 00:57:55,225:INFO: Dataset: zara1               Batch: 3/8	Loss 31.7880 (31.7990)
2022-11-25 00:57:55,226:INFO: Dataset: zara1               Batch: 4/8	Loss 31.6581 (31.7638)
2022-11-25 00:57:55,227:INFO: Dataset: zara1               Batch: 5/8	Loss 31.7503 (31.7611)
2022-11-25 00:57:55,253:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6185 (31.7402)
2022-11-25 00:57:55,254:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3574 (31.6824)
2022-11-25 00:57:55,255:INFO: Dataset: zara1               Batch: 8/8	Loss 31.2069 (31.6349)
2022-11-25 00:57:55,524:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3735 (13.3735)
2022-11-25 00:57:55,526:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3581 (13.3655)
2022-11-25 00:57:55,528:INFO: Dataset: zara2               Batch:  3/18	Loss 13.4941 (13.4085)
2022-11-25 00:57:55,557:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4904 (13.4266)
2022-11-25 00:57:55,558:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4817 (13.4371)
2022-11-25 00:57:55,585:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5313 (13.4542)
2022-11-25 00:57:55,586:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5685 (13.4711)
2022-11-25 00:57:55,587:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3965 (13.4614)
2022-11-25 00:57:55,588:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3534 (13.4492)
2022-11-25 00:57:55,589:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4068 (13.4449)
2022-11-25 00:57:55,590:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3080 (13.4309)
2022-11-25 00:57:55,591:INFO: Dataset: zara2               Batch: 12/18	Loss 13.3155 (13.4203)
2022-11-25 00:57:55,592:INFO: Dataset: zara2               Batch: 13/18	Loss 13.1052 (13.3971)
2022-11-25 00:57:55,593:INFO: Dataset: zara2               Batch: 14/18	Loss 13.1501 (13.3784)
2022-11-25 00:57:55,595:INFO: Dataset: zara2               Batch: 15/18	Loss 12.9399 (13.3510)
2022-11-25 00:57:55,596:INFO: Dataset: zara2               Batch: 16/18	Loss 12.8360 (13.3162)
2022-11-25 00:57:55,597:INFO: Dataset: zara2               Batch: 17/18	Loss 12.7699 (13.2802)
2022-11-25 00:57:55,598:INFO: Dataset: zara2               Batch: 18/18	Loss 12.5400 (13.2449)
2022-11-25 00:57:55,645:INFO: - Computing ADE (validation o)
2022-11-25 00:57:55,906:INFO: 		 ADE on eth                       dataset:	 1.0869381427764893
2022-11-25 00:57:55,907:INFO: Average validation o:	ADE  1.0869	FDE  2.2121
2022-11-25 00:57:55,907:INFO: - Computing loss (validation)
2022-11-25 00:57:56,103:INFO: Dataset: hotel               Batch: 1/2	Loss 38.4114 (38.4114)
2022-11-25 00:57:56,106:INFO: Dataset: hotel               Batch: 2/2	Loss 38.1215 (38.3907)
2022-11-25 00:57:56,367:INFO: Dataset: univ                Batch: 1/3	Loss 12.9744 (12.9744)
2022-11-25 00:57:56,368:INFO: Dataset: univ                Batch: 2/3	Loss 12.9657 (12.9698)
2022-11-25 00:57:56,370:INFO: Dataset: univ                Batch: 3/3	Loss 12.9693 (12.9697)
2022-11-25 00:57:56,622:INFO: Dataset: zara1               Batch: 1/2	Loss 30.6832 (30.6832)
2022-11-25 00:57:56,623:INFO: Dataset: zara1               Batch: 2/2	Loss 30.6929 (30.6856)
2022-11-25 00:57:56,885:INFO: Dataset: zara2               Batch: 1/5	Loss 12.7410 (12.7410)
2022-11-25 00:57:56,887:INFO: Dataset: zara2               Batch: 2/5	Loss 12.6414 (12.6899)
2022-11-25 00:57:56,888:INFO: Dataset: zara2               Batch: 3/5	Loss 12.6955 (12.6917)
2022-11-25 00:57:56,890:INFO: Dataset: zara2               Batch: 4/5	Loss 12.6581 (12.6839)
2022-11-25 00:57:56,896:INFO: Dataset: zara2               Batch: 5/5	Loss 12.6916 (12.6854)
2022-11-25 00:57:56,951:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_574.pth.tar
2022-11-25 00:57:56,951:INFO: 
===> EPOCH: 575 (P4)
2022-11-25 00:57:56,951:INFO: - Computing loss (training)
2022-11-25 00:57:57,175:INFO: Dataset: hotel               Batch: 1/4	Loss 38.0523 (38.0523)
2022-11-25 00:57:57,177:INFO: Dataset: hotel               Batch: 2/4	Loss 37.9824 (38.0191)
2022-11-25 00:57:57,178:INFO: Dataset: hotel               Batch: 3/4	Loss 38.1599 (38.0662)
2022-11-25 00:57:57,181:INFO: Dataset: hotel               Batch: 4/4	Loss 37.8306 (38.0276)
2022-11-25 00:57:57,454:INFO: Dataset: univ                Batch:  1/15	Loss 13.2885 (13.2885)
2022-11-25 00:57:57,457:INFO: Dataset: univ                Batch:  2/15	Loss 13.3700 (13.3308)
2022-11-25 00:57:57,462:INFO: Dataset: univ                Batch:  3/15	Loss 13.3993 (13.3545)
2022-11-25 00:57:57,463:INFO: Dataset: univ                Batch:  4/15	Loss 13.4330 (13.3732)
2022-11-25 00:57:57,485:INFO: Dataset: univ                Batch:  5/15	Loss 13.4292 (13.3842)
2022-11-25 00:57:57,497:INFO: Dataset: univ                Batch:  6/15	Loss 13.4168 (13.3896)
2022-11-25 00:57:57,498:INFO: Dataset: univ                Batch:  7/15	Loss 13.3691 (13.3867)
2022-11-25 00:57:57,499:INFO: Dataset: univ                Batch:  8/15	Loss 13.2979 (13.3750)
2022-11-25 00:57:57,500:INFO: Dataset: univ                Batch:  9/15	Loss 13.2401 (13.3602)
2022-11-25 00:57:57,501:INFO: Dataset: univ                Batch: 10/15	Loss 13.1315 (13.3369)
2022-11-25 00:57:57,502:INFO: Dataset: univ                Batch: 11/15	Loss 13.0269 (13.3105)
2022-11-25 00:57:57,505:INFO: Dataset: univ                Batch: 12/15	Loss 12.9520 (13.2800)
2022-11-25 00:57:57,506:INFO: Dataset: univ                Batch: 13/15	Loss 12.8502 (13.2503)
2022-11-25 00:57:57,507:INFO: Dataset: univ                Batch: 14/15	Loss 12.6850 (13.2084)
2022-11-25 00:57:57,508:INFO: Dataset: univ                Batch: 15/15	Loss 12.5281 (13.1992)
2022-11-25 00:57:57,782:INFO: Dataset: zara1               Batch: 1/8	Loss 31.9483 (31.9483)
2022-11-25 00:57:57,786:INFO: Dataset: zara1               Batch: 2/8	Loss 31.8966 (31.9239)
2022-11-25 00:57:57,787:INFO: Dataset: zara1               Batch: 3/8	Loss 31.9697 (31.9388)
2022-11-25 00:57:57,797:INFO: Dataset: zara1               Batch: 4/8	Loss 31.8185 (31.9083)
2022-11-25 00:57:57,799:INFO: Dataset: zara1               Batch: 5/8	Loss 31.8014 (31.8884)
2022-11-25 00:57:57,880:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6772 (31.8549)
2022-11-25 00:57:57,883:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4812 (31.8066)
2022-11-25 00:57:57,887:INFO: Dataset: zara1               Batch: 8/8	Loss 31.2786 (31.7513)
2022-11-25 00:57:58,177:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3828 (13.3828)
2022-11-25 00:57:58,178:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3768 (13.3799)
2022-11-25 00:57:58,181:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3970 (13.3853)
2022-11-25 00:57:58,184:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4764 (13.4089)
2022-11-25 00:57:58,216:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5326 (13.4345)
2022-11-25 00:57:58,229:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5547 (13.4563)
2022-11-25 00:57:58,230:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4435 (13.4545)
2022-11-25 00:57:58,231:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4960 (13.4595)
2022-11-25 00:57:58,232:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4270 (13.4554)
2022-11-25 00:57:58,233:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3355 (13.4441)
2022-11-25 00:57:58,234:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3070 (13.4314)
2022-11-25 00:57:58,236:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1394 (13.4075)
2022-11-25 00:57:58,237:INFO: Dataset: zara2               Batch: 13/18	Loss 13.1177 (13.3858)
2022-11-25 00:57:58,238:INFO: Dataset: zara2               Batch: 14/18	Loss 12.9560 (13.3532)
2022-11-25 00:57:58,239:INFO: Dataset: zara2               Batch: 15/18	Loss 12.8715 (13.3212)
2022-11-25 00:57:58,240:INFO: Dataset: zara2               Batch: 16/18	Loss 12.7892 (13.2878)
2022-11-25 00:57:58,241:INFO: Dataset: zara2               Batch: 17/18	Loss 12.7132 (13.2549)
2022-11-25 00:57:58,242:INFO: Dataset: zara2               Batch: 18/18	Loss 12.5168 (13.2177)
2022-11-25 00:57:58,290:INFO: - Computing ADE (validation o)
2022-11-25 00:57:58,559:INFO: 		 ADE on eth                       dataset:	 1.071213722229004
2022-11-25 00:57:58,559:INFO: Average validation o:	ADE  1.0712	FDE  2.1671
2022-11-25 00:57:58,560:INFO: - Computing loss (validation)
2022-11-25 00:57:58,761:INFO: Dataset: hotel               Batch: 1/2	Loss 38.4427 (38.4427)
2022-11-25 00:57:58,761:INFO: Dataset: hotel               Batch: 2/2	Loss 39.0075 (38.4870)
2022-11-25 00:57:59,033:INFO: Dataset: univ                Batch: 1/3	Loss 12.8910 (12.8910)
2022-11-25 00:57:59,035:INFO: Dataset: univ                Batch: 2/3	Loss 12.9279 (12.9093)
2022-11-25 00:57:59,043:INFO: Dataset: univ                Batch: 3/3	Loss 12.8166 (12.8784)
2022-11-25 00:57:59,291:INFO: Dataset: zara1               Batch: 1/2	Loss 30.7407 (30.7407)
2022-11-25 00:57:59,291:INFO: Dataset: zara1               Batch: 2/2	Loss 30.7844 (30.7511)
2022-11-25 00:57:59,550:INFO: Dataset: zara2               Batch: 1/5	Loss 12.6301 (12.6301)
2022-11-25 00:57:59,551:INFO: Dataset: zara2               Batch: 2/5	Loss 12.5173 (12.5716)
2022-11-25 00:57:59,566:INFO: Dataset: zara2               Batch: 3/5	Loss 12.5955 (12.5794)
2022-11-25 00:57:59,566:INFO: Dataset: zara2               Batch: 4/5	Loss 12.5515 (12.5727)
2022-11-25 00:57:59,567:INFO: Dataset: zara2               Batch: 5/5	Loss 12.5869 (12.5756)
2022-11-25 00:57:59,622:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_575.pth.tar
2022-11-25 00:57:59,622:INFO: 
===> EPOCH: 576 (P4)
2022-11-25 00:57:59,623:INFO: - Computing loss (training)
2022-11-25 00:57:59,833:INFO: Dataset: hotel               Batch: 1/4	Loss 38.0229 (38.0229)
2022-11-25 00:57:59,835:INFO: Dataset: hotel               Batch: 2/4	Loss 38.0805 (38.0515)
2022-11-25 00:57:59,838:INFO: Dataset: hotel               Batch: 3/4	Loss 38.1000 (38.0669)
2022-11-25 00:57:59,840:INFO: Dataset: hotel               Batch: 4/4	Loss 37.9612 (38.0494)
2022-11-25 00:58:00,100:INFO: Dataset: univ                Batch:  1/15	Loss 13.2466 (13.2466)
2022-11-25 00:58:00,102:INFO: Dataset: univ                Batch:  2/15	Loss 13.3323 (13.2858)
2022-11-25 00:58:00,104:INFO: Dataset: univ                Batch:  3/15	Loss 13.3754 (13.3159)
2022-11-25 00:58:00,106:INFO: Dataset: univ                Batch:  4/15	Loss 13.3691 (13.3293)
2022-11-25 00:58:00,187:INFO: Dataset: univ                Batch:  5/15	Loss 13.3487 (13.3335)
2022-11-25 00:58:00,194:INFO: Dataset: univ                Batch:  6/15	Loss 13.3388 (13.3345)
2022-11-25 00:58:00,195:INFO: Dataset: univ                Batch:  7/15	Loss 13.2969 (13.3292)
2022-11-25 00:58:00,196:INFO: Dataset: univ                Batch:  8/15	Loss 13.2447 (13.3192)
2022-11-25 00:58:00,197:INFO: Dataset: univ                Batch:  9/15	Loss 13.1876 (13.3051)
2022-11-25 00:58:00,198:INFO: Dataset: univ                Batch: 10/15	Loss 13.0646 (13.2790)
2022-11-25 00:58:00,200:INFO: Dataset: univ                Batch: 11/15	Loss 12.9600 (13.2490)
2022-11-25 00:58:00,201:INFO: Dataset: univ                Batch: 12/15	Loss 12.8234 (13.2164)
2022-11-25 00:58:00,202:INFO: Dataset: univ                Batch: 13/15	Loss 12.7094 (13.1784)
2022-11-25 00:58:00,204:INFO: Dataset: univ                Batch: 14/15	Loss 12.5442 (13.1330)
2022-11-25 00:58:00,205:INFO: Dataset: univ                Batch: 15/15	Loss 12.4703 (13.1225)
2022-11-25 00:58:00,455:INFO: Dataset: zara1               Batch: 1/8	Loss 31.9691 (31.9691)
2022-11-25 00:58:00,467:INFO: Dataset: zara1               Batch: 2/8	Loss 32.1063 (32.0274)
2022-11-25 00:58:00,473:INFO: Dataset: zara1               Batch: 3/8	Loss 32.1073 (32.0523)
2022-11-25 00:58:00,474:INFO: Dataset: zara1               Batch: 4/8	Loss 32.0099 (32.0418)
2022-11-25 00:58:00,475:INFO: Dataset: zara1               Batch: 5/8	Loss 31.8314 (31.9998)
2022-11-25 00:58:00,532:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7718 (31.9599)
2022-11-25 00:58:00,536:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4690 (31.8870)
2022-11-25 00:58:00,539:INFO: Dataset: zara1               Batch: 8/8	Loss 31.2833 (31.8266)
2022-11-25 00:58:00,815:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3472 (13.3472)
2022-11-25 00:58:00,818:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3333 (13.3402)
2022-11-25 00:58:00,821:INFO: Dataset: zara2               Batch:  3/18	Loss 13.4036 (13.3611)
2022-11-25 00:58:00,822:INFO: Dataset: zara2               Batch:  4/18	Loss 13.5013 (13.3919)
2022-11-25 00:58:00,857:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5161 (13.4158)
2022-11-25 00:58:00,877:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4093 (13.4148)
2022-11-25 00:58:00,878:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4294 (13.4165)
2022-11-25 00:58:00,879:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4818 (13.4249)
2022-11-25 00:58:00,880:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3557 (13.4170)
2022-11-25 00:58:00,880:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2962 (13.4050)
2022-11-25 00:58:00,881:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3049 (13.3949)
2022-11-25 00:58:00,883:INFO: Dataset: zara2               Batch: 12/18	Loss 13.2135 (13.3800)
2022-11-25 00:58:00,884:INFO: Dataset: zara2               Batch: 13/18	Loss 13.0728 (13.3590)
2022-11-25 00:58:00,885:INFO: Dataset: zara2               Batch: 14/18	Loss 12.9673 (13.3313)
2022-11-25 00:58:00,885:INFO: Dataset: zara2               Batch: 15/18	Loss 12.8333 (13.2978)
2022-11-25 00:58:00,886:INFO: Dataset: zara2               Batch: 16/18	Loss 12.6550 (13.2603)
2022-11-25 00:58:00,887:INFO: Dataset: zara2               Batch: 17/18	Loss 12.5470 (13.2176)
2022-11-25 00:58:00,889:INFO: Dataset: zara2               Batch: 18/18	Loss 12.3599 (13.1777)
2022-11-25 00:58:00,934:INFO: - Computing ADE (validation o)
2022-11-25 00:58:01,198:INFO: 		 ADE on eth                       dataset:	 1.0435206890106201
2022-11-25 00:58:01,198:INFO: Average validation o:	ADE  1.0435	FDE  2.1532
2022-11-25 00:58:01,199:INFO: - Computing loss (validation)
2022-11-25 00:58:01,391:INFO: Dataset: hotel               Batch: 1/2	Loss 38.5348 (38.5348)
2022-11-25 00:58:01,394:INFO: Dataset: hotel               Batch: 2/2	Loss 38.1237 (38.5054)
2022-11-25 00:58:01,653:INFO: Dataset: univ                Batch: 1/3	Loss 12.7329 (12.7329)
2022-11-25 00:58:01,664:INFO: Dataset: univ                Batch: 2/3	Loss 12.8101 (12.7695)
2022-11-25 00:58:01,664:INFO: Dataset: univ                Batch: 3/3	Loss 12.8451 (12.7938)
2022-11-25 00:58:01,909:INFO: Dataset: zara1               Batch: 1/2	Loss 30.8274 (30.8274)
2022-11-25 00:58:01,909:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8520 (30.8329)
2022-11-25 00:58:02,166:INFO: Dataset: zara2               Batch: 1/5	Loss 12.5419 (12.5419)
2022-11-25 00:58:02,167:INFO: Dataset: zara2               Batch: 2/5	Loss 12.5677 (12.5544)
2022-11-25 00:58:02,169:INFO: Dataset: zara2               Batch: 3/5	Loss 12.4499 (12.5209)
2022-11-25 00:58:02,170:INFO: Dataset: zara2               Batch: 4/5	Loss 12.4449 (12.5015)
2022-11-25 00:58:02,170:INFO: Dataset: zara2               Batch: 5/5	Loss 12.4442 (12.4903)
2022-11-25 00:58:02,222:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_576.pth.tar
2022-11-25 00:58:02,222:INFO: 
===> EPOCH: 577 (P4)
2022-11-25 00:58:02,223:INFO: - Computing loss (training)
2022-11-25 00:58:02,434:INFO: Dataset: hotel               Batch: 1/4	Loss 37.8563 (37.8563)
2022-11-25 00:58:02,439:INFO: Dataset: hotel               Batch: 2/4	Loss 37.9937 (37.9222)
2022-11-25 00:58:02,444:INFO: Dataset: hotel               Batch: 3/4	Loss 38.1975 (38.0153)
2022-11-25 00:58:02,445:INFO: Dataset: hotel               Batch: 4/4	Loss 37.6572 (37.9562)
2022-11-25 00:58:02,708:INFO: Dataset: univ                Batch:  1/15	Loss 13.1951 (13.1951)
2022-11-25 00:58:02,711:INFO: Dataset: univ                Batch:  2/15	Loss 13.2709 (13.2368)
2022-11-25 00:58:02,788:INFO: Dataset: univ                Batch:  3/15	Loss 13.3148 (13.2614)
2022-11-25 00:58:02,790:INFO: Dataset: univ                Batch:  4/15	Loss 13.3662 (13.2853)
2022-11-25 00:58:02,791:INFO: Dataset: univ                Batch:  5/15	Loss 13.2562 (13.2796)
2022-11-25 00:58:02,795:INFO: Dataset: univ                Batch:  6/15	Loss 13.2593 (13.2760)
2022-11-25 00:58:02,796:INFO: Dataset: univ                Batch:  7/15	Loss 13.2158 (13.2669)
2022-11-25 00:58:02,798:INFO: Dataset: univ                Batch:  8/15	Loss 13.1775 (13.2544)
2022-11-25 00:58:02,799:INFO: Dataset: univ                Batch:  9/15	Loss 13.0741 (13.2322)
2022-11-25 00:58:02,800:INFO: Dataset: univ                Batch: 10/15	Loss 12.9427 (13.2005)
2022-11-25 00:58:02,801:INFO: Dataset: univ                Batch: 11/15	Loss 12.8719 (13.1698)
2022-11-25 00:58:02,803:INFO: Dataset: univ                Batch: 12/15	Loss 12.7082 (13.1300)
2022-11-25 00:58:02,804:INFO: Dataset: univ                Batch: 13/15	Loss 12.6139 (13.0926)
2022-11-25 00:58:02,805:INFO: Dataset: univ                Batch: 14/15	Loss 12.4312 (13.0460)
2022-11-25 00:58:02,806:INFO: Dataset: univ                Batch: 15/15	Loss 12.3247 (13.0352)
2022-11-25 00:58:03,073:INFO: Dataset: zara1               Batch: 1/8	Loss 32.1198 (32.1198)
2022-11-25 00:58:03,077:INFO: Dataset: zara1               Batch: 2/8	Loss 32.1714 (32.1462)
2022-11-25 00:58:03,078:INFO: Dataset: zara1               Batch: 3/8	Loss 32.1236 (32.1381)
2022-11-25 00:58:03,079:INFO: Dataset: zara1               Batch: 4/8	Loss 32.0829 (32.1250)
2022-11-25 00:58:03,081:INFO: Dataset: zara1               Batch: 5/8	Loss 31.9544 (32.0928)
2022-11-25 00:58:03,128:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8376 (32.0467)
2022-11-25 00:58:03,132:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5720 (31.9746)
2022-11-25 00:58:03,135:INFO: Dataset: zara1               Batch: 8/8	Loss 31.2980 (31.9026)
2022-11-25 00:58:03,427:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3449 (13.3449)
2022-11-25 00:58:03,429:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3542 (13.3494)
2022-11-25 00:58:03,431:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5164 (13.4061)
2022-11-25 00:58:03,432:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4877 (13.4262)
2022-11-25 00:58:03,483:INFO: Dataset: zara2               Batch:  5/18	Loss 13.3856 (13.4171)
2022-11-25 00:58:03,492:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5170 (13.4354)
2022-11-25 00:58:03,493:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4840 (13.4427)
2022-11-25 00:58:03,494:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4937 (13.4492)
2022-11-25 00:58:03,495:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4408 (13.4484)
2022-11-25 00:58:03,496:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3076 (13.4346)
2022-11-25 00:58:03,498:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1672 (13.4106)
2022-11-25 00:58:03,499:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1701 (13.3905)
2022-11-25 00:58:03,500:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9677 (13.3558)
2022-11-25 00:58:03,501:INFO: Dataset: zara2               Batch: 14/18	Loss 12.8447 (13.3183)
2022-11-25 00:58:03,502:INFO: Dataset: zara2               Batch: 15/18	Loss 12.8793 (13.2897)
2022-11-25 00:58:03,503:INFO: Dataset: zara2               Batch: 16/18	Loss 12.6457 (13.2479)
2022-11-25 00:58:03,504:INFO: Dataset: zara2               Batch: 17/18	Loss 12.6040 (13.2085)
2022-11-25 00:58:03,506:INFO: Dataset: zara2               Batch: 18/18	Loss 12.3365 (13.1626)
2022-11-25 00:58:03,550:INFO: - Computing ADE (validation o)
2022-11-25 00:58:03,818:INFO: 		 ADE on eth                       dataset:	 1.0650774240493774
2022-11-25 00:58:03,818:INFO: Average validation o:	ADE  1.0651	FDE  2.1617
2022-11-25 00:58:03,819:INFO: - Computing loss (validation)
2022-11-25 00:58:04,017:INFO: Dataset: hotel               Batch: 1/2	Loss 38.4141 (38.4141)
2022-11-25 00:58:04,018:INFO: Dataset: hotel               Batch: 2/2	Loss 38.9613 (38.4646)
2022-11-25 00:58:04,291:INFO: Dataset: univ                Batch: 1/3	Loss 12.7396 (12.7396)
2022-11-25 00:58:04,292:INFO: Dataset: univ                Batch: 2/3	Loss 12.7497 (12.7446)
2022-11-25 00:58:04,292:INFO: Dataset: univ                Batch: 3/3	Loss 12.7221 (12.7368)
2022-11-25 00:58:04,543:INFO: Dataset: zara1               Batch: 1/2	Loss 30.8655 (30.8655)
2022-11-25 00:58:04,545:INFO: Dataset: zara1               Batch: 2/2	Loss 30.9016 (30.8738)
2022-11-25 00:58:04,830:INFO: Dataset: zara2               Batch: 1/5	Loss 12.3514 (12.3514)
2022-11-25 00:58:04,831:INFO: Dataset: zara2               Batch: 2/5	Loss 12.4100 (12.3817)
2022-11-25 00:58:04,833:INFO: Dataset: zara2               Batch: 3/5	Loss 12.4257 (12.3972)
2022-11-25 00:58:04,834:INFO: Dataset: zara2               Batch: 4/5	Loss 12.4017 (12.3983)
2022-11-25 00:58:04,835:INFO: Dataset: zara2               Batch: 5/5	Loss 12.4716 (12.4134)
2022-11-25 00:58:04,889:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_577.pth.tar
2022-11-25 00:58:04,889:INFO: 
===> EPOCH: 578 (P4)
2022-11-25 00:58:04,889:INFO: - Computing loss (training)
2022-11-25 00:58:05,119:INFO: Dataset: hotel               Batch: 1/4	Loss 37.9356 (37.9356)
2022-11-25 00:58:05,120:INFO: Dataset: hotel               Batch: 2/4	Loss 37.6061 (37.7712)
2022-11-25 00:58:05,121:INFO: Dataset: hotel               Batch: 3/4	Loss 37.7574 (37.7665)
2022-11-25 00:58:05,123:INFO: Dataset: hotel               Batch: 4/4	Loss 37.9192 (37.7913)
2022-11-25 00:58:05,384:INFO: Dataset: univ                Batch:  1/15	Loss 13.1934 (13.1934)
2022-11-25 00:58:05,455:INFO: Dataset: univ                Batch:  2/15	Loss 13.2169 (13.2054)
2022-11-25 00:58:05,457:INFO: Dataset: univ                Batch:  3/15	Loss 13.2802 (13.2326)
2022-11-25 00:58:05,459:INFO: Dataset: univ                Batch:  4/15	Loss 13.3222 (13.2553)
2022-11-25 00:58:05,460:INFO: Dataset: univ                Batch:  5/15	Loss 13.2435 (13.2528)
2022-11-25 00:58:05,463:INFO: Dataset: univ                Batch:  6/15	Loss 13.2360 (13.2500)
2022-11-25 00:58:05,465:INFO: Dataset: univ                Batch:  7/15	Loss 13.2494 (13.2499)
2022-11-25 00:58:05,466:INFO: Dataset: univ                Batch:  8/15	Loss 13.0990 (13.2328)
2022-11-25 00:58:05,467:INFO: Dataset: univ                Batch:  9/15	Loss 13.0486 (13.2126)
2022-11-25 00:58:05,468:INFO: Dataset: univ                Batch: 10/15	Loss 12.8995 (13.1794)
2022-11-25 00:58:05,470:INFO: Dataset: univ                Batch: 11/15	Loss 12.7592 (13.1429)
2022-11-25 00:58:05,471:INFO: Dataset: univ                Batch: 12/15	Loss 12.6573 (13.1053)
2022-11-25 00:58:05,473:INFO: Dataset: univ                Batch: 13/15	Loss 12.4643 (13.0547)
2022-11-25 00:58:05,474:INFO: Dataset: univ                Batch: 14/15	Loss 12.3381 (13.0045)
2022-11-25 00:58:05,476:INFO: Dataset: univ                Batch: 15/15	Loss 12.1467 (12.9938)
2022-11-25 00:58:05,784:INFO: Dataset: zara1               Batch: 1/8	Loss 32.1854 (32.1854)
2022-11-25 00:58:05,788:INFO: Dataset: zara1               Batch: 2/8	Loss 32.2787 (32.2340)
2022-11-25 00:58:05,789:INFO: Dataset: zara1               Batch: 3/8	Loss 32.2509 (32.2397)
2022-11-25 00:58:05,790:INFO: Dataset: zara1               Batch: 4/8	Loss 32.1446 (32.2186)
2022-11-25 00:58:05,793:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0770 (32.1886)
2022-11-25 00:58:05,852:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8325 (32.1310)
2022-11-25 00:58:05,855:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5870 (32.0448)
2022-11-25 00:58:05,859:INFO: Dataset: zara1               Batch: 8/8	Loss 31.3417 (31.9648)
2022-11-25 00:58:06,147:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3294 (13.3294)
2022-11-25 00:58:06,154:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3923 (13.3610)
2022-11-25 00:58:06,156:INFO: Dataset: zara2               Batch:  3/18	Loss 13.4871 (13.4009)
2022-11-25 00:58:06,158:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4106 (13.4035)
2022-11-25 00:58:06,208:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4726 (13.4177)
2022-11-25 00:58:06,215:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4830 (13.4294)
2022-11-25 00:58:06,216:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4396 (13.4309)
2022-11-25 00:58:06,217:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4602 (13.4349)
2022-11-25 00:58:06,218:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3238 (13.4224)
2022-11-25 00:58:06,219:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3576 (13.4159)
2022-11-25 00:58:06,220:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2392 (13.3999)
2022-11-25 00:58:06,222:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1465 (13.3755)
2022-11-25 00:58:06,223:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9774 (13.3443)
2022-11-25 00:58:06,224:INFO: Dataset: zara2               Batch: 14/18	Loss 12.9698 (13.3188)
2022-11-25 00:58:06,225:INFO: Dataset: zara2               Batch: 15/18	Loss 12.6773 (13.2779)
2022-11-25 00:58:06,226:INFO: Dataset: zara2               Batch: 16/18	Loss 12.5318 (13.2321)
2022-11-25 00:58:06,227:INFO: Dataset: zara2               Batch: 17/18	Loss 12.5397 (13.1962)
2022-11-25 00:58:06,229:INFO: Dataset: zara2               Batch: 18/18	Loss 12.3720 (13.1605)
2022-11-25 00:58:06,276:INFO: - Computing ADE (validation o)
2022-11-25 00:58:06,543:INFO: 		 ADE on eth                       dataset:	 1.1281499862670898
2022-11-25 00:58:06,544:INFO: Average validation o:	ADE  1.1281	FDE  2.2653
2022-11-25 00:58:06,544:INFO: - Computing loss (validation)
2022-11-25 00:58:06,737:INFO: Dataset: hotel               Batch: 1/2	Loss 38.3920 (38.3920)
2022-11-25 00:58:06,738:INFO: Dataset: hotel               Batch: 2/2	Loss 37.9279 (38.3555)
2022-11-25 00:58:07,010:INFO: Dataset: univ                Batch: 1/3	Loss 12.5991 (12.5991)
2022-11-25 00:58:07,011:INFO: Dataset: univ                Batch: 2/3	Loss 12.8244 (12.7020)
2022-11-25 00:58:07,012:INFO: Dataset: univ                Batch: 3/3	Loss 12.6468 (12.6840)
2022-11-25 00:58:07,263:INFO: Dataset: zara1               Batch: 1/2	Loss 30.9224 (30.9224)
2022-11-25 00:58:07,266:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8617 (30.9067)
2022-11-25 00:58:07,525:INFO: Dataset: zara2               Batch: 1/5	Loss 12.3225 (12.3225)
2022-11-25 00:58:07,526:INFO: Dataset: zara2               Batch: 2/5	Loss 12.3501 (12.3365)
2022-11-25 00:58:07,528:INFO: Dataset: zara2               Batch: 3/5	Loss 12.3348 (12.3359)
2022-11-25 00:58:07,528:INFO: Dataset: zara2               Batch: 4/5	Loss 12.3614 (12.3424)
2022-11-25 00:58:07,530:INFO: Dataset: zara2               Batch: 5/5	Loss 12.3695 (12.3475)
2022-11-25 00:58:07,587:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_578.pth.tar
2022-11-25 00:58:07,587:INFO: 
===> EPOCH: 579 (P4)
2022-11-25 00:58:07,587:INFO: - Computing loss (training)
2022-11-25 00:58:07,793:INFO: Dataset: hotel               Batch: 1/4	Loss 37.5658 (37.5658)
2022-11-25 00:58:07,796:INFO: Dataset: hotel               Batch: 2/4	Loss 37.9125 (37.7340)
2022-11-25 00:58:07,798:INFO: Dataset: hotel               Batch: 3/4	Loss 37.4531 (37.6365)
2022-11-25 00:58:07,800:INFO: Dataset: hotel               Batch: 4/4	Loss 37.0959 (37.5416)
2022-11-25 00:58:08,065:INFO: Dataset: univ                Batch:  1/15	Loss 13.0639 (13.0639)
2022-11-25 00:58:08,068:INFO: Dataset: univ                Batch:  2/15	Loss 13.1772 (13.1190)
2022-11-25 00:58:08,072:INFO: Dataset: univ                Batch:  3/15	Loss 13.2883 (13.1766)
2022-11-25 00:58:08,073:INFO: Dataset: univ                Batch:  4/15	Loss 13.2212 (13.1869)
2022-11-25 00:58:08,097:INFO: Dataset: univ                Batch:  5/15	Loss 13.3470 (13.2165)
2022-11-25 00:58:08,101:INFO: Dataset: univ                Batch:  6/15	Loss 13.1889 (13.2118)
2022-11-25 00:58:08,102:INFO: Dataset: univ                Batch:  7/15	Loss 13.1537 (13.2034)
2022-11-25 00:58:08,103:INFO: Dataset: univ                Batch:  8/15	Loss 13.0379 (13.1831)
2022-11-25 00:58:08,105:INFO: Dataset: univ                Batch:  9/15	Loss 12.9536 (13.1576)
2022-11-25 00:58:08,106:INFO: Dataset: univ                Batch: 10/15	Loss 12.8826 (13.1307)
2022-11-25 00:58:08,107:INFO: Dataset: univ                Batch: 11/15	Loss 12.6978 (13.0929)
2022-11-25 00:58:08,108:INFO: Dataset: univ                Batch: 12/15	Loss 12.6234 (13.0548)
2022-11-25 00:58:08,109:INFO: Dataset: univ                Batch: 13/15	Loss 12.4101 (13.0032)
2022-11-25 00:58:08,111:INFO: Dataset: univ                Batch: 14/15	Loss 12.1917 (12.9460)
2022-11-25 00:58:08,112:INFO: Dataset: univ                Batch: 15/15	Loss 11.9889 (12.9341)
2022-11-25 00:58:08,359:INFO: Dataset: zara1               Batch: 1/8	Loss 32.2839 (32.2839)
2022-11-25 00:58:08,362:INFO: Dataset: zara1               Batch: 2/8	Loss 32.2917 (32.2880)
2022-11-25 00:58:08,364:INFO: Dataset: zara1               Batch: 3/8	Loss 32.3031 (32.2930)
2022-11-25 00:58:08,367:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2705 (32.2873)
2022-11-25 00:58:08,369:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0732 (32.2446)
2022-11-25 00:58:08,414:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9098 (32.1883)
2022-11-25 00:58:08,415:INFO: Dataset: zara1               Batch: 7/8	Loss 31.6374 (32.0981)
2022-11-25 00:58:08,415:INFO: Dataset: zara1               Batch: 8/8	Loss 31.3137 (32.0068)
2022-11-25 00:58:08,706:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4034 (13.4034)
2022-11-25 00:58:08,708:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4705 (13.4371)
2022-11-25 00:58:08,709:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5573 (13.4776)
2022-11-25 00:58:08,742:INFO: Dataset: zara2               Batch:  4/18	Loss 13.5458 (13.4956)
2022-11-25 00:58:08,743:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6205 (13.5249)
2022-11-25 00:58:08,747:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6490 (13.5451)
2022-11-25 00:58:08,748:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5201 (13.5414)
2022-11-25 00:58:08,749:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4725 (13.5331)
2022-11-25 00:58:08,749:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4191 (13.5207)
2022-11-25 00:58:08,750:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3227 (13.4996)
2022-11-25 00:58:08,751:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2522 (13.4761)
2022-11-25 00:58:08,753:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0911 (13.4475)
2022-11-25 00:58:08,754:INFO: Dataset: zara2               Batch: 13/18	Loss 13.1186 (13.4206)
2022-11-25 00:58:08,755:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6836 (13.3693)
2022-11-25 00:58:08,756:INFO: Dataset: zara2               Batch: 15/18	Loss 12.7792 (13.3298)
2022-11-25 00:58:08,758:INFO: Dataset: zara2               Batch: 16/18	Loss 12.5578 (13.2830)
2022-11-25 00:58:08,759:INFO: Dataset: zara2               Batch: 17/18	Loss 12.3492 (13.2227)
2022-11-25 00:58:08,761:INFO: Dataset: zara2               Batch: 18/18	Loss 12.2625 (13.1781)
2022-11-25 00:58:08,809:INFO: - Computing ADE (validation o)
2022-11-25 00:58:09,080:INFO: 		 ADE on eth                       dataset:	 1.0457366704940796
2022-11-25 00:58:09,080:INFO: Average validation o:	ADE  1.0457	FDE  2.1315
2022-11-25 00:58:09,080:INFO: - Computing loss (validation)
2022-11-25 00:58:09,275:INFO: Dataset: hotel               Batch: 1/2	Loss 38.3311 (38.3311)
2022-11-25 00:58:09,275:INFO: Dataset: hotel               Batch: 2/2	Loss 37.8024 (38.2805)
2022-11-25 00:58:09,524:INFO: Dataset: univ                Batch: 1/3	Loss 12.6179 (12.6179)
2022-11-25 00:58:09,525:INFO: Dataset: univ                Batch: 2/3	Loss 12.8181 (12.7134)
2022-11-25 00:58:09,526:INFO: Dataset: univ                Batch: 3/3	Loss 12.5966 (12.6776)
2022-11-25 00:58:09,802:INFO: Dataset: zara1               Batch: 1/2	Loss 30.8644 (30.8644)
2022-11-25 00:58:09,806:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8924 (30.8711)
2022-11-25 00:58:10,057:INFO: Dataset: zara2               Batch: 1/5	Loss 12.3573 (12.3573)
2022-11-25 00:58:10,059:INFO: Dataset: zara2               Batch: 2/5	Loss 12.3234 (12.3417)
2022-11-25 00:58:10,060:INFO: Dataset: zara2               Batch: 3/5	Loss 12.1643 (12.2831)
2022-11-25 00:58:10,061:INFO: Dataset: zara2               Batch: 4/5	Loss 12.2868 (12.2841)
2022-11-25 00:58:10,062:INFO: Dataset: zara2               Batch: 5/5	Loss 12.2714 (12.2814)
2022-11-25 00:58:10,132:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_579.pth.tar
2022-11-25 00:58:10,132:INFO: 
===> EPOCH: 580 (P4)
2022-11-25 00:58:10,132:INFO: - Computing loss (training)
2022-11-25 00:58:10,337:INFO: Dataset: hotel               Batch: 1/4	Loss 37.2976 (37.2976)
2022-11-25 00:58:10,351:INFO: Dataset: hotel               Batch: 2/4	Loss 37.2706 (37.2850)
2022-11-25 00:58:10,353:INFO: Dataset: hotel               Batch: 3/4	Loss 37.8559 (37.4741)
2022-11-25 00:58:10,355:INFO: Dataset: hotel               Batch: 4/4	Loss 36.3753 (37.2856)
2022-11-25 00:58:10,612:INFO: Dataset: univ                Batch:  1/15	Loss 13.1654 (13.1654)
2022-11-25 00:58:10,616:INFO: Dataset: univ                Batch:  2/15	Loss 13.2141 (13.1908)
2022-11-25 00:58:10,618:INFO: Dataset: univ                Batch:  3/15	Loss 13.2421 (13.2086)
2022-11-25 00:58:10,623:INFO: Dataset: univ                Batch:  4/15	Loss 13.2949 (13.2307)
2022-11-25 00:58:10,653:INFO: Dataset: univ                Batch:  5/15	Loss 13.2406 (13.2327)
2022-11-25 00:58:10,677:INFO: Dataset: univ                Batch:  6/15	Loss 13.2393 (13.2338)
2022-11-25 00:58:10,678:INFO: Dataset: univ                Batch:  7/15	Loss 13.1424 (13.2197)
2022-11-25 00:58:10,679:INFO: Dataset: univ                Batch:  8/15	Loss 13.0655 (13.1989)
2022-11-25 00:58:10,680:INFO: Dataset: univ                Batch:  9/15	Loss 12.9577 (13.1698)
2022-11-25 00:58:10,682:INFO: Dataset: univ                Batch: 10/15	Loss 12.8196 (13.1347)
2022-11-25 00:58:10,683:INFO: Dataset: univ                Batch: 11/15	Loss 12.6813 (13.0912)
2022-11-25 00:58:10,684:INFO: Dataset: univ                Batch: 12/15	Loss 12.5205 (13.0430)
2022-11-25 00:58:10,685:INFO: Dataset: univ                Batch: 13/15	Loss 12.3977 (12.9971)
2022-11-25 00:58:10,686:INFO: Dataset: univ                Batch: 14/15	Loss 12.1728 (12.9363)
2022-11-25 00:58:10,687:INFO: Dataset: univ                Batch: 15/15	Loss 12.0603 (12.9237)
2022-11-25 00:58:10,953:INFO: Dataset: zara1               Batch: 1/8	Loss 32.3031 (32.3031)
2022-11-25 00:58:10,965:INFO: Dataset: zara1               Batch: 2/8	Loss 32.3031 (32.3031)
2022-11-25 00:58:10,966:INFO: Dataset: zara1               Batch: 3/8	Loss 32.3233 (32.3099)
2022-11-25 00:58:10,967:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2764 (32.3021)
2022-11-25 00:58:10,968:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1387 (32.2719)
2022-11-25 00:58:11,028:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9523 (32.2216)
2022-11-25 00:58:11,032:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5678 (32.1459)
2022-11-25 00:58:11,036:INFO: Dataset: zara1               Batch: 8/8	Loss 31.3333 (32.0531)
2022-11-25 00:58:11,319:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3351 (13.3351)
2022-11-25 00:58:11,373:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4303 (13.3847)
2022-11-25 00:58:11,374:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6151 (13.4613)
2022-11-25 00:58:11,376:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4426 (13.4565)
2022-11-25 00:58:11,377:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4793 (13.4613)
2022-11-25 00:58:11,385:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5282 (13.4732)
2022-11-25 00:58:11,386:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6021 (13.4913)
2022-11-25 00:58:11,387:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3999 (13.4800)
2022-11-25 00:58:11,388:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4977 (13.4818)
2022-11-25 00:58:11,389:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2165 (13.4552)
2022-11-25 00:58:11,390:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2443 (13.4372)
2022-11-25 00:58:11,392:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0765 (13.4037)
2022-11-25 00:58:11,393:INFO: Dataset: zara2               Batch: 13/18	Loss 13.0233 (13.3785)
2022-11-25 00:58:11,394:INFO: Dataset: zara2               Batch: 14/18	Loss 12.9598 (13.3508)
2022-11-25 00:58:11,395:INFO: Dataset: zara2               Batch: 15/18	Loss 12.6412 (13.3070)
2022-11-25 00:58:11,396:INFO: Dataset: zara2               Batch: 16/18	Loss 12.6480 (13.2682)
2022-11-25 00:58:11,397:INFO: Dataset: zara2               Batch: 17/18	Loss 12.3491 (13.2090)
2022-11-25 00:58:11,398:INFO: Dataset: zara2               Batch: 18/18	Loss 12.3734 (13.1708)
2022-11-25 00:58:11,446:INFO: - Computing ADE (validation o)
2022-11-25 00:58:11,717:INFO: 		 ADE on eth                       dataset:	 1.0874444246292114
2022-11-25 00:58:11,717:INFO: Average validation o:	ADE  1.0874	FDE  2.2203
2022-11-25 00:58:11,718:INFO: - Computing loss (validation)
2022-11-25 00:58:11,915:INFO: Dataset: hotel               Batch: 1/2	Loss 38.0914 (38.0914)
2022-11-25 00:58:11,916:INFO: Dataset: hotel               Batch: 2/2	Loss 38.3734 (38.1135)
2022-11-25 00:58:12,194:INFO: Dataset: univ                Batch: 1/3	Loss 12.6510 (12.6510)
2022-11-25 00:58:12,196:INFO: Dataset: univ                Batch: 2/3	Loss 12.5643 (12.6102)
2022-11-25 00:58:12,200:INFO: Dataset: univ                Batch: 3/3	Loss 12.7229 (12.6402)
2022-11-25 00:58:12,475:INFO: Dataset: zara1               Batch: 1/2	Loss 30.8395 (30.8395)
2022-11-25 00:58:12,478:INFO: Dataset: zara1               Batch: 2/2	Loss 30.7572 (30.8178)
2022-11-25 00:58:12,745:INFO: Dataset: zara2               Batch: 1/5	Loss 12.2284 (12.2284)
2022-11-25 00:58:12,747:INFO: Dataset: zara2               Batch: 2/5	Loss 12.1251 (12.1783)
2022-11-25 00:58:12,748:INFO: Dataset: zara2               Batch: 3/5	Loss 12.2568 (12.2035)
2022-11-25 00:58:12,749:INFO: Dataset: zara2               Batch: 4/5	Loss 12.2889 (12.2245)
2022-11-25 00:58:12,750:INFO: Dataset: zara2               Batch: 5/5	Loss 12.2752 (12.2344)
2022-11-25 00:58:12,808:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_580.pth.tar
2022-11-25 00:58:12,808:INFO: 
===> EPOCH: 581 (P4)
2022-11-25 00:58:12,808:INFO: - Computing loss (training)
2022-11-25 00:58:13,031:INFO: Dataset: hotel               Batch: 1/4	Loss 37.1775 (37.1775)
2022-11-25 00:58:13,034:INFO: Dataset: hotel               Batch: 2/4	Loss 36.8336 (37.0063)
2022-11-25 00:58:13,037:INFO: Dataset: hotel               Batch: 3/4	Loss 36.8139 (36.9410)
2022-11-25 00:58:13,038:INFO: Dataset: hotel               Batch: 4/4	Loss 36.8315 (36.9238)
2022-11-25 00:58:13,307:INFO: Dataset: univ                Batch:  1/15	Loss 13.1588 (13.1588)
2022-11-25 00:58:13,311:INFO: Dataset: univ                Batch:  2/15	Loss 13.1940 (13.1756)
2022-11-25 00:58:13,313:INFO: Dataset: univ                Batch:  3/15	Loss 13.2800 (13.2116)
2022-11-25 00:58:13,314:INFO: Dataset: univ                Batch:  4/15	Loss 13.2875 (13.2309)
2022-11-25 00:58:13,362:INFO: Dataset: univ                Batch:  5/15	Loss 13.2373 (13.2322)
2022-11-25 00:58:13,369:INFO: Dataset: univ                Batch:  6/15	Loss 13.2030 (13.2275)
2022-11-25 00:58:13,370:INFO: Dataset: univ                Batch:  7/15	Loss 13.0944 (13.2086)
2022-11-25 00:58:13,371:INFO: Dataset: univ                Batch:  8/15	Loss 13.0417 (13.1878)
2022-11-25 00:58:13,373:INFO: Dataset: univ                Batch:  9/15	Loss 12.8731 (13.1531)
2022-11-25 00:58:13,374:INFO: Dataset: univ                Batch: 10/15	Loss 12.7617 (13.1178)
2022-11-25 00:58:13,375:INFO: Dataset: univ                Batch: 11/15	Loss 12.5547 (13.0700)
2022-11-25 00:58:13,377:INFO: Dataset: univ                Batch: 12/15	Loss 12.4416 (13.0201)
2022-11-25 00:58:13,378:INFO: Dataset: univ                Batch: 13/15	Loss 12.2501 (12.9619)
2022-11-25 00:58:13,379:INFO: Dataset: univ                Batch: 14/15	Loss 12.0661 (12.8988)
2022-11-25 00:58:13,380:INFO: Dataset: univ                Batch: 15/15	Loss 11.8610 (12.8861)
2022-11-25 00:58:13,686:INFO: Dataset: zara1               Batch: 1/8	Loss 32.3397 (32.3397)
2022-11-25 00:58:13,688:INFO: Dataset: zara1               Batch: 2/8	Loss 32.4155 (32.3772)
2022-11-25 00:58:13,689:INFO: Dataset: zara1               Batch: 3/8	Loss 32.3118 (32.3556)
2022-11-25 00:58:13,693:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2943 (32.3403)
2022-11-25 00:58:13,694:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1744 (32.3080)
2022-11-25 00:58:13,713:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9203 (32.2464)
2022-11-25 00:58:13,714:INFO: Dataset: zara1               Batch: 7/8	Loss 31.6181 (32.1665)
2022-11-25 00:58:13,714:INFO: Dataset: zara1               Batch: 8/8	Loss 31.2797 (32.0732)
2022-11-25 00:58:13,994:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3461 (13.3461)
2022-11-25 00:58:13,997:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4215 (13.3858)
2022-11-25 00:58:13,999:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8203 (13.5395)
2022-11-25 00:58:14,003:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7198 (13.5857)
2022-11-25 00:58:14,045:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6968 (13.6074)
2022-11-25 00:58:14,049:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5596 (13.5991)
2022-11-25 00:58:14,050:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6126 (13.6011)
2022-11-25 00:58:14,051:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5412 (13.5932)
2022-11-25 00:58:14,052:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4097 (13.5722)
2022-11-25 00:58:14,053:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2904 (13.5433)
2022-11-25 00:58:14,054:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2829 (13.5218)
2022-11-25 00:58:14,056:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0515 (13.4786)
2022-11-25 00:58:14,057:INFO: Dataset: zara2               Batch: 13/18	Loss 13.0428 (13.4423)
2022-11-25 00:58:14,058:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7614 (13.3945)
2022-11-25 00:58:14,058:INFO: Dataset: zara2               Batch: 15/18	Loss 12.7281 (13.3524)
2022-11-25 00:58:14,059:INFO: Dataset: zara2               Batch: 16/18	Loss 12.5422 (13.3027)
2022-11-25 00:58:14,060:INFO: Dataset: zara2               Batch: 17/18	Loss 12.3498 (13.2430)
2022-11-25 00:58:14,062:INFO: Dataset: zara2               Batch: 18/18	Loss 12.2876 (13.1997)
2022-11-25 00:58:14,108:INFO: - Computing ADE (validation o)
2022-11-25 00:58:14,372:INFO: 		 ADE on eth                       dataset:	 1.0886286497116089
2022-11-25 00:58:14,372:INFO: Average validation o:	ADE  1.0886	FDE  2.1927
2022-11-25 00:58:14,373:INFO: - Computing loss (validation)
2022-11-25 00:58:14,614:INFO: Dataset: hotel               Batch: 1/2	Loss 37.9980 (37.9980)
2022-11-25 00:58:14,615:INFO: Dataset: hotel               Batch: 2/2	Loss 37.3921 (37.9525)
2022-11-25 00:58:14,875:INFO: Dataset: univ                Batch: 1/3	Loss 12.5563 (12.5563)
2022-11-25 00:58:14,876:INFO: Dataset: univ                Batch: 2/3	Loss 12.5907 (12.5742)
2022-11-25 00:58:14,878:INFO: Dataset: univ                Batch: 3/3	Loss 12.8517 (12.6469)
2022-11-25 00:58:15,207:INFO: Dataset: zara1               Batch: 1/2	Loss 30.7515 (30.7515)
2022-11-25 00:58:15,214:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8068 (30.7663)
2022-11-25 00:58:15,528:INFO: Dataset: zara2               Batch: 1/5	Loss 12.2242 (12.2242)
2022-11-25 00:58:15,529:INFO: Dataset: zara2               Batch: 2/5	Loss 12.2171 (12.2204)
2022-11-25 00:58:15,529:INFO: Dataset: zara2               Batch: 3/5	Loss 12.1391 (12.1948)
2022-11-25 00:58:15,530:INFO: Dataset: zara2               Batch: 4/5	Loss 12.0821 (12.1640)
2022-11-25 00:58:15,542:INFO: Dataset: zara2               Batch: 5/5	Loss 12.1487 (12.1612)
2022-11-25 00:58:15,597:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_581.pth.tar
2022-11-25 00:58:15,597:INFO: 
===> EPOCH: 582 (P4)
2022-11-25 00:58:15,597:INFO: - Computing loss (training)
2022-11-25 00:58:15,833:INFO: Dataset: hotel               Batch: 1/4	Loss 36.9799 (36.9799)
2022-11-25 00:58:15,835:INFO: Dataset: hotel               Batch: 2/4	Loss 36.4837 (36.7210)
2022-11-25 00:58:15,839:INFO: Dataset: hotel               Batch: 3/4	Loss 36.3630 (36.5994)
2022-11-25 00:58:15,840:INFO: Dataset: hotel               Batch: 4/4	Loss 36.4560 (36.5746)
2022-11-25 00:58:16,154:INFO: Dataset: univ                Batch:  1/15	Loss 13.1561 (13.1561)
2022-11-25 00:58:16,223:INFO: Dataset: univ                Batch:  2/15	Loss 13.2387 (13.1988)
2022-11-25 00:58:16,225:INFO: Dataset: univ                Batch:  3/15	Loss 13.3849 (13.2603)
2022-11-25 00:58:16,227:INFO: Dataset: univ                Batch:  4/15	Loss 13.2959 (13.2689)
2022-11-25 00:58:16,228:INFO: Dataset: univ                Batch:  5/15	Loss 13.2901 (13.2729)
2022-11-25 00:58:16,232:INFO: Dataset: univ                Batch:  6/15	Loss 13.2638 (13.2714)
2022-11-25 00:58:16,233:INFO: Dataset: univ                Batch:  7/15	Loss 13.2296 (13.2661)
2022-11-25 00:58:16,234:INFO: Dataset: univ                Batch:  8/15	Loss 12.9843 (13.2324)
2022-11-25 00:58:16,235:INFO: Dataset: univ                Batch:  9/15	Loss 12.9450 (13.2023)
2022-11-25 00:58:16,236:INFO: Dataset: univ                Batch: 10/15	Loss 12.8023 (13.1632)
2022-11-25 00:58:16,240:INFO: Dataset: univ                Batch: 11/15	Loss 12.4960 (13.1104)
2022-11-25 00:58:16,241:INFO: Dataset: univ                Batch: 12/15	Loss 12.4290 (13.0535)
2022-11-25 00:58:16,242:INFO: Dataset: univ                Batch: 13/15	Loss 12.2270 (12.9961)
2022-11-25 00:58:16,244:INFO: Dataset: univ                Batch: 14/15	Loss 12.0039 (12.9188)
2022-11-25 00:58:16,246:INFO: Dataset: univ                Batch: 15/15	Loss 11.9579 (12.9062)
2022-11-25 00:58:16,554:INFO: Dataset: zara1               Batch: 1/8	Loss 32.3434 (32.3434)
2022-11-25 00:58:16,556:INFO: Dataset: zara1               Batch: 2/8	Loss 32.5138 (32.4327)
2022-11-25 00:58:16,557:INFO: Dataset: zara1               Batch: 3/8	Loss 32.4481 (32.4377)
2022-11-25 00:58:16,559:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2997 (32.4030)
2022-11-25 00:58:16,560:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1588 (32.3481)
2022-11-25 00:58:16,587:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8969 (32.2735)
2022-11-25 00:58:16,588:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5042 (32.1608)
2022-11-25 00:58:16,589:INFO: Dataset: zara1               Batch: 8/8	Loss 31.2709 (32.0639)
2022-11-25 00:58:16,879:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3861 (13.3861)
2022-11-25 00:58:16,881:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4341 (13.4099)
2022-11-25 00:58:16,883:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7785 (13.5455)
2022-11-25 00:58:16,885:INFO: Dataset: zara2               Batch:  4/18	Loss 13.6952 (13.5854)
2022-11-25 00:58:16,904:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7514 (13.6200)
2022-11-25 00:58:16,917:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7302 (13.6391)
2022-11-25 00:58:16,918:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6829 (13.6450)
2022-11-25 00:58:16,919:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4456 (13.6196)
2022-11-25 00:58:16,920:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3564 (13.5885)
2022-11-25 00:58:16,921:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3705 (13.5676)
2022-11-25 00:58:16,922:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2948 (13.5430)
2022-11-25 00:58:16,924:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1519 (13.5130)
2022-11-25 00:58:16,925:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9689 (13.4721)
2022-11-25 00:58:16,925:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6669 (13.4136)
2022-11-25 00:58:16,926:INFO: Dataset: zara2               Batch: 15/18	Loss 12.7006 (13.3636)
2022-11-25 00:58:16,927:INFO: Dataset: zara2               Batch: 16/18	Loss 12.5677 (13.3157)
2022-11-25 00:58:16,928:INFO: Dataset: zara2               Batch: 17/18	Loss 12.3424 (13.2539)
2022-11-25 00:58:16,930:INFO: Dataset: zara2               Batch: 18/18	Loss 12.0039 (13.1842)
2022-11-25 00:58:16,976:INFO: - Computing ADE (validation o)
2022-11-25 00:58:17,301:INFO: 		 ADE on eth                       dataset:	 1.0604331493377686
2022-11-25 00:58:17,302:INFO: Average validation o:	ADE  1.0604	FDE  2.1459
2022-11-25 00:58:17,303:INFO: - Computing loss (validation)
2022-11-25 00:58:17,561:INFO: Dataset: hotel               Batch: 1/2	Loss 37.6622 (37.6622)
2022-11-25 00:58:17,563:INFO: Dataset: hotel               Batch: 2/2	Loss 38.6261 (37.7346)
2022-11-25 00:58:17,861:INFO: Dataset: univ                Batch: 1/3	Loss 12.6223 (12.6223)
2022-11-25 00:58:17,862:INFO: Dataset: univ                Batch: 2/3	Loss 12.7693 (12.6901)
2022-11-25 00:58:17,863:INFO: Dataset: univ                Batch: 3/3	Loss 12.4631 (12.6119)
2022-11-25 00:58:18,109:INFO: Dataset: zara1               Batch: 1/2	Loss 30.7162 (30.7162)
2022-11-25 00:58:18,110:INFO: Dataset: zara1               Batch: 2/2	Loss 30.7826 (30.7312)
2022-11-25 00:58:18,363:INFO: Dataset: zara2               Batch: 1/5	Loss 12.1257 (12.1257)
2022-11-25 00:58:18,376:INFO: Dataset: zara2               Batch: 2/5	Loss 12.1279 (12.1268)
2022-11-25 00:58:18,377:INFO: Dataset: zara2               Batch: 3/5	Loss 12.1069 (12.1200)
2022-11-25 00:58:18,377:INFO: Dataset: zara2               Batch: 4/5	Loss 12.0783 (12.1095)
2022-11-25 00:58:18,378:INFO: Dataset: zara2               Batch: 5/5	Loss 12.1607 (12.1192)
2022-11-25 00:58:18,433:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_582.pth.tar
2022-11-25 00:58:18,434:INFO: 
===> EPOCH: 583 (P4)
2022-11-25 00:58:18,434:INFO: - Computing loss (training)
2022-11-25 00:58:18,636:INFO: Dataset: hotel               Batch: 1/4	Loss 36.3189 (36.3189)
2022-11-25 00:58:18,638:INFO: Dataset: hotel               Batch: 2/4	Loss 36.3821 (36.3518)
2022-11-25 00:58:18,641:INFO: Dataset: hotel               Batch: 3/4	Loss 35.7816 (36.1691)
2022-11-25 00:58:18,643:INFO: Dataset: hotel               Batch: 4/4	Loss 35.6868 (36.0940)
2022-11-25 00:58:18,891:INFO: Dataset: univ                Batch:  1/15	Loss 13.1939 (13.1939)
2022-11-25 00:58:18,893:INFO: Dataset: univ                Batch:  2/15	Loss 13.2753 (13.2384)
2022-11-25 00:58:18,898:INFO: Dataset: univ                Batch:  3/15	Loss 13.3554 (13.2757)
2022-11-25 00:58:18,899:INFO: Dataset: univ                Batch:  4/15	Loss 13.2785 (13.2764)
2022-11-25 00:58:18,955:INFO: Dataset: univ                Batch:  5/15	Loss 13.2602 (13.2733)
2022-11-25 00:58:18,965:INFO: Dataset: univ                Batch:  6/15	Loss 13.1860 (13.2570)
2022-11-25 00:58:18,966:INFO: Dataset: univ                Batch:  7/15	Loss 13.0939 (13.2348)
2022-11-25 00:58:18,967:INFO: Dataset: univ                Batch:  8/15	Loss 12.8917 (13.1925)
2022-11-25 00:58:18,968:INFO: Dataset: univ                Batch:  9/15	Loss 12.8991 (13.1606)
2022-11-25 00:58:18,969:INFO: Dataset: univ                Batch: 10/15	Loss 12.7682 (13.1207)
2022-11-25 00:58:18,970:INFO: Dataset: univ                Batch: 11/15	Loss 12.5428 (13.0679)
2022-11-25 00:58:18,972:INFO: Dataset: univ                Batch: 12/15	Loss 12.4272 (13.0175)
2022-11-25 00:58:18,973:INFO: Dataset: univ                Batch: 13/15	Loss 12.1748 (12.9546)
2022-11-25 00:58:18,974:INFO: Dataset: univ                Batch: 14/15	Loss 11.9478 (12.8819)
2022-11-25 00:58:18,975:INFO: Dataset: univ                Batch: 15/15	Loss 11.7588 (12.8646)
2022-11-25 00:58:19,235:INFO: Dataset: zara1               Batch: 1/8	Loss 32.3996 (32.3996)
2022-11-25 00:58:19,239:INFO: Dataset: zara1               Batch: 2/8	Loss 32.5800 (32.4911)
2022-11-25 00:58:19,240:INFO: Dataset: zara1               Batch: 3/8	Loss 32.4716 (32.4840)
2022-11-25 00:58:19,241:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2646 (32.4331)
2022-11-25 00:58:19,244:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2458 (32.3972)
2022-11-25 00:58:19,308:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8794 (32.3145)
2022-11-25 00:58:19,312:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5239 (32.1996)
2022-11-25 00:58:19,315:INFO: Dataset: zara1               Batch: 8/8	Loss 31.1460 (32.0887)
2022-11-25 00:58:19,609:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3570 (13.3570)
2022-11-25 00:58:19,613:INFO: Dataset: zara2               Batch:  2/18	Loss 13.7296 (13.5467)
2022-11-25 00:58:19,616:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6524 (13.5808)
2022-11-25 00:58:19,622:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7554 (13.6250)
2022-11-25 00:58:19,675:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6486 (13.6298)
2022-11-25 00:58:19,682:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6730 (13.6369)
2022-11-25 00:58:19,683:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7863 (13.6589)
2022-11-25 00:58:19,684:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6329 (13.6556)
2022-11-25 00:58:19,685:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4948 (13.6368)
2022-11-25 00:58:19,686:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3502 (13.6070)
2022-11-25 00:58:19,688:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3344 (13.5815)
2022-11-25 00:58:19,689:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1800 (13.5499)
2022-11-25 00:58:19,690:INFO: Dataset: zara2               Batch: 13/18	Loss 13.0855 (13.5138)
2022-11-25 00:58:19,691:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7893 (13.4634)
2022-11-25 00:58:19,692:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5845 (13.4071)
2022-11-25 00:58:19,693:INFO: Dataset: zara2               Batch: 16/18	Loss 12.3428 (13.3395)
2022-11-25 00:58:19,695:INFO: Dataset: zara2               Batch: 17/18	Loss 12.2579 (13.2660)
2022-11-25 00:58:19,696:INFO: Dataset: zara2               Batch: 18/18	Loss 12.0439 (13.2042)
2022-11-25 00:58:19,742:INFO: - Computing ADE (validation o)
2022-11-25 00:58:20,039:INFO: 		 ADE on eth                       dataset:	 1.1234184503555298
2022-11-25 00:58:20,039:INFO: Average validation o:	ADE  1.1234	FDE  2.2722
2022-11-25 00:58:20,040:INFO: - Computing loss (validation)
2022-11-25 00:58:20,231:INFO: Dataset: hotel               Batch: 1/2	Loss 37.5880 (37.5880)
2022-11-25 00:58:20,232:INFO: Dataset: hotel               Batch: 2/2	Loss 37.1668 (37.5492)
2022-11-25 00:58:20,495:INFO: Dataset: univ                Batch: 1/3	Loss 12.4775 (12.4775)
2022-11-25 00:58:20,515:INFO: Dataset: univ                Batch: 2/3	Loss 12.8015 (12.6243)
2022-11-25 00:58:20,516:INFO: Dataset: univ                Batch: 3/3	Loss 12.7307 (12.6545)
2022-11-25 00:58:20,771:INFO: Dataset: zara1               Batch: 1/2	Loss 30.6394 (30.6394)
2022-11-25 00:58:20,771:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8520 (30.6886)
2022-11-25 00:58:21,048:INFO: Dataset: zara2               Batch: 1/5	Loss 12.0003 (12.0003)
2022-11-25 00:58:21,052:INFO: Dataset: zara2               Batch: 2/5	Loss 12.0484 (12.0245)
2022-11-25 00:58:21,054:INFO: Dataset: zara2               Batch: 3/5	Loss 12.0450 (12.0319)
2022-11-25 00:58:21,059:INFO: Dataset: zara2               Batch: 4/5	Loss 12.1488 (12.0618)
2022-11-25 00:58:21,059:INFO: Dataset: zara2               Batch: 5/5	Loss 11.9595 (12.0419)
2022-11-25 00:58:21,114:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_583.pth.tar
2022-11-25 00:58:21,115:INFO: 
===> EPOCH: 584 (P4)
2022-11-25 00:58:21,115:INFO: - Computing loss (training)
2022-11-25 00:58:21,316:INFO: Dataset: hotel               Batch: 1/4	Loss 35.5945 (35.5945)
2022-11-25 00:58:21,319:INFO: Dataset: hotel               Batch: 2/4	Loss 36.1079 (35.8576)
2022-11-25 00:58:21,322:INFO: Dataset: hotel               Batch: 3/4	Loss 35.8203 (35.8446)
2022-11-25 00:58:21,337:INFO: Dataset: hotel               Batch: 4/4	Loss 34.9639 (35.6784)
2022-11-25 00:58:21,614:INFO: Dataset: univ                Batch:  1/15	Loss 13.1653 (13.1653)
2022-11-25 00:58:21,619:INFO: Dataset: univ                Batch:  2/15	Loss 13.3238 (13.2441)
2022-11-25 00:58:21,622:INFO: Dataset: univ                Batch:  3/15	Loss 13.4194 (13.3011)
2022-11-25 00:58:21,623:INFO: Dataset: univ                Batch:  4/15	Loss 13.4478 (13.3360)
2022-11-25 00:58:21,679:INFO: Dataset: univ                Batch:  5/15	Loss 13.3642 (13.3413)
2022-11-25 00:58:21,687:INFO: Dataset: univ                Batch:  6/15	Loss 13.2982 (13.3345)
2022-11-25 00:58:21,688:INFO: Dataset: univ                Batch:  7/15	Loss 13.1497 (13.3067)
2022-11-25 00:58:21,689:INFO: Dataset: univ                Batch:  8/15	Loss 13.0965 (13.2821)
2022-11-25 00:58:21,690:INFO: Dataset: univ                Batch:  9/15	Loss 12.9515 (13.2533)
2022-11-25 00:58:21,692:INFO: Dataset: univ                Batch: 10/15	Loss 12.6929 (13.1972)
2022-11-25 00:58:21,696:INFO: Dataset: univ                Batch: 11/15	Loss 12.5229 (13.1384)
2022-11-25 00:58:21,697:INFO: Dataset: univ                Batch: 12/15	Loss 12.4621 (13.0814)
2022-11-25 00:58:21,698:INFO: Dataset: univ                Batch: 13/15	Loss 12.0712 (13.0024)
2022-11-25 00:58:21,699:INFO: Dataset: univ                Batch: 14/15	Loss 11.8860 (12.9219)
2022-11-25 00:58:21,700:INFO: Dataset: univ                Batch: 15/15	Loss 11.8274 (12.9104)
2022-11-25 00:58:21,971:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4300 (32.4300)
2022-11-25 00:58:21,972:INFO: Dataset: zara1               Batch: 2/8	Loss 32.4713 (32.4487)
2022-11-25 00:58:21,973:INFO: Dataset: zara1               Batch: 3/8	Loss 32.5485 (32.4822)
2022-11-25 00:58:21,976:INFO: Dataset: zara1               Batch: 4/8	Loss 32.4415 (32.4719)
2022-11-25 00:58:21,977:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1475 (32.4045)
2022-11-25 00:58:22,035:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9056 (32.3282)
2022-11-25 00:58:22,039:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4595 (32.2050)
2022-11-25 00:58:22,042:INFO: Dataset: zara1               Batch: 8/8	Loss 31.1495 (32.1056)
2022-11-25 00:58:22,341:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5124 (13.5124)
2022-11-25 00:58:22,343:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5374 (13.5247)
2022-11-25 00:58:22,386:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8550 (13.6318)
2022-11-25 00:58:22,388:INFO: Dataset: zara2               Batch:  4/18	Loss 13.6498 (13.6367)
2022-11-25 00:58:22,389:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6663 (13.6429)
2022-11-25 00:58:22,392:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7395 (13.6588)
2022-11-25 00:58:22,393:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7133 (13.6664)
2022-11-25 00:58:22,394:INFO: Dataset: zara2               Batch:  8/18	Loss 13.8440 (13.6891)
2022-11-25 00:58:22,395:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5926 (13.6782)
2022-11-25 00:58:22,396:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4099 (13.6530)
2022-11-25 00:58:22,398:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3001 (13.6240)
2022-11-25 00:58:22,399:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1334 (13.5808)
2022-11-25 00:58:22,400:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8671 (13.5259)
2022-11-25 00:58:22,401:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7655 (13.4689)
2022-11-25 00:58:22,403:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5763 (13.4195)
2022-11-25 00:58:22,404:INFO: Dataset: zara2               Batch: 16/18	Loss 12.4160 (13.3578)
2022-11-25 00:58:22,406:INFO: Dataset: zara2               Batch: 17/18	Loss 12.2270 (13.2905)
2022-11-25 00:58:22,408:INFO: Dataset: zara2               Batch: 18/18	Loss 12.0307 (13.2301)
2022-11-25 00:58:22,454:INFO: - Computing ADE (validation o)
2022-11-25 00:58:22,746:INFO: 		 ADE on eth                       dataset:	 1.0539194345474243
2022-11-25 00:58:22,747:INFO: Average validation o:	ADE  1.0539	FDE  2.1218
2022-11-25 00:58:22,747:INFO: - Computing loss (validation)
2022-11-25 00:58:22,949:INFO: Dataset: hotel               Batch: 1/2	Loss 37.2295 (37.2295)
2022-11-25 00:58:22,950:INFO: Dataset: hotel               Batch: 2/2	Loss 37.7670 (37.2717)
2022-11-25 00:58:23,206:INFO: Dataset: univ                Batch: 1/3	Loss 12.5648 (12.5648)
2022-11-25 00:58:23,207:INFO: Dataset: univ                Batch: 2/3	Loss 12.6660 (12.6166)
2022-11-25 00:58:23,209:INFO: Dataset: univ                Batch: 3/3	Loss 12.6173 (12.6168)
2022-11-25 00:58:23,460:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5913 (30.5913)
2022-11-25 00:58:23,462:INFO: Dataset: zara1               Batch: 2/2	Loss 30.6329 (30.6014)
2022-11-25 00:58:23,711:INFO: Dataset: zara2               Batch: 1/5	Loss 12.0359 (12.0359)
2022-11-25 00:58:23,712:INFO: Dataset: zara2               Batch: 2/5	Loss 12.0441 (12.0402)
2022-11-25 00:58:23,713:INFO: Dataset: zara2               Batch: 3/5	Loss 12.0470 (12.0424)
2022-11-25 00:58:23,714:INFO: Dataset: zara2               Batch: 4/5	Loss 11.9746 (12.0245)
2022-11-25 00:58:23,715:INFO: Dataset: zara2               Batch: 5/5	Loss 11.9581 (12.0126)
2022-11-25 00:58:23,770:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_584.pth.tar
2022-11-25 00:58:23,770:INFO: 
===> EPOCH: 585 (P4)
2022-11-25 00:58:23,771:INFO: - Computing loss (training)
2022-11-25 00:58:23,975:INFO: Dataset: hotel               Batch: 1/4	Loss 34.8578 (34.8578)
2022-11-25 00:58:23,979:INFO: Dataset: hotel               Batch: 2/4	Loss 35.3430 (35.1097)
2022-11-25 00:58:23,980:INFO: Dataset: hotel               Batch: 3/4	Loss 35.4942 (35.2350)
2022-11-25 00:58:23,981:INFO: Dataset: hotel               Batch: 4/4	Loss 34.5263 (35.1060)
2022-11-25 00:58:24,234:INFO: Dataset: univ                Batch:  1/15	Loss 13.1884 (13.1884)
2022-11-25 00:58:24,237:INFO: Dataset: univ                Batch:  2/15	Loss 13.2776 (13.2337)
2022-11-25 00:58:24,240:INFO: Dataset: univ                Batch:  3/15	Loss 13.4056 (13.2905)
2022-11-25 00:58:24,292:INFO: Dataset: univ                Batch:  4/15	Loss 13.3385 (13.3034)
2022-11-25 00:58:24,293:INFO: Dataset: univ                Batch:  5/15	Loss 13.3321 (13.3095)
2022-11-25 00:58:24,297:INFO: Dataset: univ                Batch:  6/15	Loss 13.2749 (13.3037)
2022-11-25 00:58:24,298:INFO: Dataset: univ                Batch:  7/15	Loss 13.2555 (13.2972)
2022-11-25 00:58:24,299:INFO: Dataset: univ                Batch:  8/15	Loss 13.0220 (13.2615)
2022-11-25 00:58:24,301:INFO: Dataset: univ                Batch:  9/15	Loss 12.9049 (13.2214)
2022-11-25 00:58:24,302:INFO: Dataset: univ                Batch: 10/15	Loss 12.7613 (13.1743)
2022-11-25 00:58:24,303:INFO: Dataset: univ                Batch: 11/15	Loss 12.5145 (13.1116)
2022-11-25 00:58:24,304:INFO: Dataset: univ                Batch: 12/15	Loss 12.2671 (13.0383)
2022-11-25 00:58:24,305:INFO: Dataset: univ                Batch: 13/15	Loss 12.1402 (12.9784)
2022-11-25 00:58:24,306:INFO: Dataset: univ                Batch: 14/15	Loss 11.8272 (12.8979)
2022-11-25 00:58:24,307:INFO: Dataset: univ                Batch: 15/15	Loss 11.4565 (12.8747)
2022-11-25 00:58:24,588:INFO: Dataset: zara1               Batch: 1/8	Loss 32.5231 (32.5231)
2022-11-25 00:58:24,589:INFO: Dataset: zara1               Batch: 2/8	Loss 32.6104 (32.5696)
2022-11-25 00:58:24,592:INFO: Dataset: zara1               Batch: 3/8	Loss 32.5332 (32.5563)
2022-11-25 00:58:24,596:INFO: Dataset: zara1               Batch: 4/8	Loss 32.3361 (32.5043)
2022-11-25 00:58:24,597:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1383 (32.4282)
2022-11-25 00:58:24,673:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9227 (32.3433)
2022-11-25 00:58:24,677:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3953 (32.2035)
2022-11-25 00:58:24,680:INFO: Dataset: zara1               Batch: 8/8	Loss 31.0351 (32.0738)
2022-11-25 00:58:24,990:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4515 (13.4515)
2022-11-25 00:58:24,992:INFO: Dataset: zara2               Batch:  2/18	Loss 13.7087 (13.5749)
2022-11-25 00:58:25,026:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8455 (13.6643)
2022-11-25 00:58:25,027:INFO: Dataset: zara2               Batch:  4/18	Loss 13.8021 (13.7006)
2022-11-25 00:58:25,028:INFO: Dataset: zara2               Batch:  5/18	Loss 13.9220 (13.7436)
2022-11-25 00:58:25,030:INFO: Dataset: zara2               Batch:  6/18	Loss 14.1195 (13.8054)
2022-11-25 00:58:25,031:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6933 (13.7903)
2022-11-25 00:58:25,032:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6593 (13.7739)
2022-11-25 00:58:25,033:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3990 (13.7310)
2022-11-25 00:58:25,034:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4112 (13.7006)
2022-11-25 00:58:25,035:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2180 (13.6543)
2022-11-25 00:58:25,037:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1558 (13.6099)
2022-11-25 00:58:25,038:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8417 (13.5458)
2022-11-25 00:58:25,039:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7673 (13.4903)
2022-11-25 00:58:25,040:INFO: Dataset: zara2               Batch: 15/18	Loss 12.7159 (13.4353)
2022-11-25 00:58:25,041:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2743 (13.3617)
2022-11-25 00:58:25,043:INFO: Dataset: zara2               Batch: 17/18	Loss 12.1137 (13.2827)
2022-11-25 00:58:25,045:INFO: Dataset: zara2               Batch: 18/18	Loss 11.9698 (13.2241)
2022-11-25 00:58:25,092:INFO: - Computing ADE (validation o)
2022-11-25 00:58:25,421:INFO: 		 ADE on eth                       dataset:	 1.093056559562683
2022-11-25 00:58:25,421:INFO: Average validation o:	ADE  1.0931	FDE  2.2128
2022-11-25 00:58:25,422:INFO: - Computing loss (validation)
2022-11-25 00:58:25,634:INFO: Dataset: hotel               Batch: 1/2	Loss 37.0822 (37.0822)
2022-11-25 00:58:25,635:INFO: Dataset: hotel               Batch: 2/2	Loss 36.4271 (37.0420)
2022-11-25 00:58:25,946:INFO: Dataset: univ                Batch: 1/3	Loss 12.7156 (12.7156)
2022-11-25 00:58:25,947:INFO: Dataset: univ                Batch: 2/3	Loss 12.6068 (12.6599)
2022-11-25 00:58:25,948:INFO: Dataset: univ                Batch: 3/3	Loss 12.7677 (12.6954)
2022-11-25 00:58:26,218:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5901 (30.5901)
2022-11-25 00:58:26,221:INFO: Dataset: zara1               Batch: 2/2	Loss 30.5442 (30.5781)
2022-11-25 00:58:26,501:INFO: Dataset: zara2               Batch: 1/5	Loss 11.8150 (11.8150)
2022-11-25 00:58:26,503:INFO: Dataset: zara2               Batch: 2/5	Loss 11.9158 (11.8664)
2022-11-25 00:58:26,517:INFO: Dataset: zara2               Batch: 3/5	Loss 12.0016 (11.9098)
2022-11-25 00:58:26,517:INFO: Dataset: zara2               Batch: 4/5	Loss 11.9217 (11.9126)
2022-11-25 00:58:26,518:INFO: Dataset: zara2               Batch: 5/5	Loss 11.9382 (11.9178)
2022-11-25 00:58:26,575:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_585.pth.tar
2022-11-25 00:58:26,575:INFO: 
===> EPOCH: 586 (P4)
2022-11-25 00:58:26,575:INFO: - Computing loss (training)
2022-11-25 00:58:26,791:INFO: Dataset: hotel               Batch: 1/4	Loss 34.7418 (34.7418)
2022-11-25 00:58:26,794:INFO: Dataset: hotel               Batch: 2/4	Loss 34.1836 (34.4554)
2022-11-25 00:58:26,796:INFO: Dataset: hotel               Batch: 3/4	Loss 35.1460 (34.6794)
2022-11-25 00:58:26,800:INFO: Dataset: hotel               Batch: 4/4	Loss 34.8330 (34.7061)
2022-11-25 00:58:27,072:INFO: Dataset: univ                Batch:  1/15	Loss 13.3970 (13.3970)
2022-11-25 00:58:27,125:INFO: Dataset: univ                Batch:  2/15	Loss 13.3390 (13.3658)
2022-11-25 00:58:27,127:INFO: Dataset: univ                Batch:  3/15	Loss 13.4997 (13.4066)
2022-11-25 00:58:27,129:INFO: Dataset: univ                Batch:  4/15	Loss 13.4128 (13.4082)
2022-11-25 00:58:27,131:INFO: Dataset: univ                Batch:  5/15	Loss 13.3705 (13.3998)
2022-11-25 00:58:27,135:INFO: Dataset: univ                Batch:  6/15	Loss 13.3523 (13.3919)
2022-11-25 00:58:27,136:INFO: Dataset: univ                Batch:  7/15	Loss 13.1558 (13.3593)
2022-11-25 00:58:27,138:INFO: Dataset: univ                Batch:  8/15	Loss 13.0645 (13.3202)
2022-11-25 00:58:27,139:INFO: Dataset: univ                Batch:  9/15	Loss 12.9437 (13.2799)
2022-11-25 00:58:27,141:INFO: Dataset: univ                Batch: 10/15	Loss 12.9152 (13.2452)
2022-11-25 00:58:27,144:INFO: Dataset: univ                Batch: 11/15	Loss 12.4573 (13.1755)
2022-11-25 00:58:27,147:INFO: Dataset: univ                Batch: 12/15	Loss 12.4253 (13.1103)
2022-11-25 00:58:27,149:INFO: Dataset: univ                Batch: 13/15	Loss 12.0825 (13.0411)
2022-11-25 00:58:27,152:INFO: Dataset: univ                Batch: 14/15	Loss 11.8465 (12.9560)
2022-11-25 00:58:27,155:INFO: Dataset: univ                Batch: 15/15	Loss 11.6307 (12.9370)
2022-11-25 00:58:27,489:INFO: Dataset: zara1               Batch: 1/8	Loss 32.6010 (32.6010)
2022-11-25 00:58:27,492:INFO: Dataset: zara1               Batch: 2/8	Loss 32.6922 (32.6441)
2022-11-25 00:58:27,496:INFO: Dataset: zara1               Batch: 3/8	Loss 32.5479 (32.6132)
2022-11-25 00:58:27,498:INFO: Dataset: zara1               Batch: 4/8	Loss 32.3197 (32.5414)
2022-11-25 00:58:27,500:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2281 (32.4827)
2022-11-25 00:58:27,527:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9834 (32.3974)
2022-11-25 00:58:27,528:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3362 (32.2714)
2022-11-25 00:58:27,529:INFO: Dataset: zara1               Batch: 8/8	Loss 30.9795 (32.1361)
2022-11-25 00:58:27,832:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5242 (13.5242)
2022-11-25 00:58:27,835:INFO: Dataset: zara2               Batch:  2/18	Loss 13.7143 (13.6160)
2022-11-25 00:58:27,837:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7927 (13.6790)
2022-11-25 00:58:27,842:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7888 (13.7066)
2022-11-25 00:58:27,889:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2352 (13.8180)
2022-11-25 00:58:27,893:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7777 (13.8114)
2022-11-25 00:58:27,894:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8990 (13.8244)
2022-11-25 00:58:27,896:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6946 (13.8085)
2022-11-25 00:58:27,897:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6716 (13.7931)
2022-11-25 00:58:27,898:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3109 (13.7470)
2022-11-25 00:58:27,900:INFO: Dataset: zara2               Batch: 11/18	Loss 13.4078 (13.7184)
2022-11-25 00:58:27,901:INFO: Dataset: zara2               Batch: 12/18	Loss 13.1696 (13.6739)
2022-11-25 00:58:27,902:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8921 (13.6160)
2022-11-25 00:58:27,903:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6558 (13.5399)
2022-11-25 00:58:27,904:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5472 (13.4685)
2022-11-25 00:58:27,905:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1582 (13.3790)
2022-11-25 00:58:27,906:INFO: Dataset: zara2               Batch: 17/18	Loss 12.2737 (13.3149)
2022-11-25 00:58:27,908:INFO: Dataset: zara2               Batch: 18/18	Loss 11.9399 (13.2535)
2022-11-25 00:58:27,953:INFO: - Computing ADE (validation o)
2022-11-25 00:58:28,234:INFO: 		 ADE on eth                       dataset:	 1.0604865550994873
2022-11-25 00:58:28,234:INFO: Average validation o:	ADE  1.0605	FDE  2.1688
2022-11-25 00:58:28,235:INFO: - Computing loss (validation)
2022-11-25 00:58:28,438:INFO: Dataset: hotel               Batch: 1/2	Loss 36.7664 (36.7664)
2022-11-25 00:58:28,463:INFO: Dataset: hotel               Batch: 2/2	Loss 37.6708 (36.8220)
2022-11-25 00:58:28,715:INFO: Dataset: univ                Batch: 1/3	Loss 12.6673 (12.6673)
2022-11-25 00:58:28,716:INFO: Dataset: univ                Batch: 2/3	Loss 12.5444 (12.5996)
2022-11-25 00:58:28,716:INFO: Dataset: univ                Batch: 3/3	Loss 12.8120 (12.6561)
2022-11-25 00:58:28,989:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4367 (30.4367)
2022-11-25 00:58:28,990:INFO: Dataset: zara1               Batch: 2/2	Loss 30.6556 (30.4958)
2022-11-25 00:58:29,249:INFO: Dataset: zara2               Batch: 1/5	Loss 11.9274 (11.9274)
2022-11-25 00:58:29,250:INFO: Dataset: zara2               Batch: 2/5	Loss 11.9608 (11.9437)
2022-11-25 00:58:29,251:INFO: Dataset: zara2               Batch: 3/5	Loss 11.9090 (11.9320)
2022-11-25 00:58:29,254:INFO: Dataset: zara2               Batch: 4/5	Loss 11.8295 (11.9080)
2022-11-25 00:58:29,255:INFO: Dataset: zara2               Batch: 5/5	Loss 11.8120 (11.8897)
2022-11-25 00:58:29,390:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_586.pth.tar
2022-11-25 00:58:29,390:INFO: 
===> EPOCH: 587 (P4)
2022-11-25 00:58:29,391:INFO: - Computing loss (training)
2022-11-25 00:58:29,604:INFO: Dataset: hotel               Batch: 1/4	Loss 35.0142 (35.0142)
2022-11-25 00:58:29,606:INFO: Dataset: hotel               Batch: 2/4	Loss 33.5875 (34.3076)
2022-11-25 00:58:29,607:INFO: Dataset: hotel               Batch: 3/4	Loss 34.3107 (34.3086)
2022-11-25 00:58:29,614:INFO: Dataset: hotel               Batch: 4/4	Loss 33.5169 (34.1843)
2022-11-25 00:58:29,904:INFO: Dataset: univ                Batch:  1/15	Loss 13.3085 (13.3085)
2022-11-25 00:58:29,905:INFO: Dataset: univ                Batch:  2/15	Loss 13.4215 (13.3657)
2022-11-25 00:58:29,950:INFO: Dataset: univ                Batch:  3/15	Loss 13.4504 (13.3946)
2022-11-25 00:58:29,952:INFO: Dataset: univ                Batch:  4/15	Loss 13.4502 (13.4101)
2022-11-25 00:58:29,953:INFO: Dataset: univ                Batch:  5/15	Loss 13.4698 (13.4217)
2022-11-25 00:58:29,957:INFO: Dataset: univ                Batch:  6/15	Loss 13.2368 (13.3877)
2022-11-25 00:58:29,959:INFO: Dataset: univ                Batch:  7/15	Loss 13.1768 (13.3574)
2022-11-25 00:58:29,960:INFO: Dataset: univ                Batch:  8/15	Loss 13.0802 (13.3231)
2022-11-25 00:58:29,961:INFO: Dataset: univ                Batch:  9/15	Loss 12.8863 (13.2725)
2022-11-25 00:58:29,962:INFO: Dataset: univ                Batch: 10/15	Loss 12.7409 (13.2242)
2022-11-25 00:58:29,964:INFO: Dataset: univ                Batch: 11/15	Loss 12.5380 (13.1682)
2022-11-25 00:58:29,965:INFO: Dataset: univ                Batch: 12/15	Loss 12.2707 (13.0941)
2022-11-25 00:58:29,967:INFO: Dataset: univ                Batch: 13/15	Loss 12.1279 (13.0241)
2022-11-25 00:58:29,968:INFO: Dataset: univ                Batch: 14/15	Loss 11.8170 (12.9346)
2022-11-25 00:58:29,969:INFO: Dataset: univ                Batch: 15/15	Loss 11.5758 (12.9210)
2022-11-25 00:58:30,225:INFO: Dataset: zara1               Batch: 1/8	Loss 32.7086 (32.7086)
2022-11-25 00:58:30,227:INFO: Dataset: zara1               Batch: 2/8	Loss 32.6688 (32.6883)
2022-11-25 00:58:30,228:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6061 (32.6596)
2022-11-25 00:58:30,231:INFO: Dataset: zara1               Batch: 4/8	Loss 32.4180 (32.5970)
2022-11-25 00:58:30,233:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2711 (32.5362)
2022-11-25 00:58:30,249:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8502 (32.4113)
2022-11-25 00:58:30,250:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3465 (32.2627)
2022-11-25 00:58:30,251:INFO: Dataset: zara1               Batch: 8/8	Loss 31.0505 (32.1383)
2022-11-25 00:58:30,516:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7547 (13.7547)
2022-11-25 00:58:30,518:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8352 (13.7955)
2022-11-25 00:58:30,522:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8893 (13.8271)
2022-11-25 00:58:30,558:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9261 (13.8525)
2022-11-25 00:58:30,560:INFO: Dataset: zara2               Batch:  5/18	Loss 13.9946 (13.8787)
2022-11-25 00:58:30,564:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6168 (13.8327)
2022-11-25 00:58:30,565:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7804 (13.8245)
2022-11-25 00:58:30,566:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5520 (13.7905)
2022-11-25 00:58:30,568:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5287 (13.7619)
2022-11-25 00:58:30,569:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4892 (13.7355)
2022-11-25 00:58:30,571:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3605 (13.6980)
2022-11-25 00:58:30,572:INFO: Dataset: zara2               Batch: 12/18	Loss 13.2496 (13.6609)
2022-11-25 00:58:30,573:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8491 (13.5998)
2022-11-25 00:58:30,574:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6930 (13.5356)
2022-11-25 00:58:30,575:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5259 (13.4686)
2022-11-25 00:58:30,577:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2720 (13.3938)
2022-11-25 00:58:30,579:INFO: Dataset: zara2               Batch: 17/18	Loss 12.0468 (13.3125)
2022-11-25 00:58:30,580:INFO: Dataset: zara2               Batch: 18/18	Loss 11.9010 (13.2469)
2022-11-25 00:58:30,632:INFO: - Computing ADE (validation o)
2022-11-25 00:58:30,932:INFO: 		 ADE on eth                       dataset:	 1.074589729309082
2022-11-25 00:58:30,932:INFO: Average validation o:	ADE  1.0746	FDE  2.1519
2022-11-25 00:58:30,932:INFO: - Computing loss (validation)
2022-11-25 00:58:31,196:INFO: Dataset: hotel               Batch: 1/2	Loss 36.5269 (36.5269)
2022-11-25 00:58:31,198:INFO: Dataset: hotel               Batch: 2/2	Loss 37.8840 (36.6242)
2022-11-25 00:58:31,489:INFO: Dataset: univ                Batch: 1/3	Loss 12.7669 (12.7669)
2022-11-25 00:58:31,491:INFO: Dataset: univ                Batch: 2/3	Loss 12.7002 (12.7349)
2022-11-25 00:58:31,492:INFO: Dataset: univ                Batch: 3/3	Loss 12.7903 (12.7526)
2022-11-25 00:58:31,887:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4661 (30.4661)
2022-11-25 00:58:31,888:INFO: Dataset: zara1               Batch: 2/2	Loss 30.5843 (30.4942)
2022-11-25 00:58:32,208:INFO: Dataset: zara2               Batch: 1/5	Loss 11.8225 (11.8225)
2022-11-25 00:58:32,210:INFO: Dataset: zara2               Batch: 2/5	Loss 11.7418 (11.7814)
2022-11-25 00:58:32,211:INFO: Dataset: zara2               Batch: 3/5	Loss 11.7453 (11.7699)
2022-11-25 00:58:32,212:INFO: Dataset: zara2               Batch: 4/5	Loss 11.8809 (11.7994)
2022-11-25 00:58:32,214:INFO: Dataset: zara2               Batch: 5/5	Loss 11.7740 (11.7945)
2022-11-25 00:58:32,268:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_587.pth.tar
2022-11-25 00:58:32,268:INFO: 
===> EPOCH: 588 (P4)
2022-11-25 00:58:32,269:INFO: - Computing loss (training)
2022-11-25 00:58:32,463:INFO: Dataset: hotel               Batch: 1/4	Loss 34.3347 (34.3347)
2022-11-25 00:58:32,466:INFO: Dataset: hotel               Batch: 2/4	Loss 33.1692 (33.7689)
2022-11-25 00:58:32,468:INFO: Dataset: hotel               Batch: 3/4	Loss 33.9421 (33.8287)
2022-11-25 00:58:32,471:INFO: Dataset: hotel               Batch: 4/4	Loss 33.6787 (33.8032)
2022-11-25 00:58:32,740:INFO: Dataset: univ                Batch:  1/15	Loss 13.3987 (13.3987)
2022-11-25 00:58:32,742:INFO: Dataset: univ                Batch:  2/15	Loss 13.5398 (13.4712)
2022-11-25 00:58:32,770:INFO: Dataset: univ                Batch:  3/15	Loss 13.6185 (13.5199)
2022-11-25 00:58:32,773:INFO: Dataset: univ                Batch:  4/15	Loss 13.5535 (13.5287)
2022-11-25 00:58:32,777:INFO: Dataset: univ                Batch:  5/15	Loss 13.4143 (13.5065)
2022-11-25 00:58:32,789:INFO: Dataset: univ                Batch:  6/15	Loss 13.3211 (13.4742)
2022-11-25 00:58:32,790:INFO: Dataset: univ                Batch:  7/15	Loss 13.2835 (13.4468)
2022-11-25 00:58:32,792:INFO: Dataset: univ                Batch:  8/15	Loss 13.1389 (13.4120)
2022-11-25 00:58:32,793:INFO: Dataset: univ                Batch:  9/15	Loss 12.9520 (13.3674)
2022-11-25 00:58:32,794:INFO: Dataset: univ                Batch: 10/15	Loss 12.8317 (13.3122)
2022-11-25 00:58:32,795:INFO: Dataset: univ                Batch: 11/15	Loss 12.5272 (13.2488)
2022-11-25 00:58:32,797:INFO: Dataset: univ                Batch: 12/15	Loss 12.2925 (13.1734)
2022-11-25 00:58:32,798:INFO: Dataset: univ                Batch: 13/15	Loss 11.9930 (13.0783)
2022-11-25 00:58:32,799:INFO: Dataset: univ                Batch: 14/15	Loss 11.9227 (13.0044)
2022-11-25 00:58:32,800:INFO: Dataset: univ                Batch: 15/15	Loss 11.6826 (12.9867)
2022-11-25 00:58:33,068:INFO: Dataset: zara1               Batch: 1/8	Loss 32.8372 (32.8372)
2022-11-25 00:58:33,070:INFO: Dataset: zara1               Batch: 2/8	Loss 32.8934 (32.8646)
2022-11-25 00:58:33,072:INFO: Dataset: zara1               Batch: 3/8	Loss 32.9480 (32.8919)
2022-11-25 00:58:33,073:INFO: Dataset: zara1               Batch: 4/8	Loss 32.4397 (32.7671)
2022-11-25 00:58:33,076:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1247 (32.6337)
2022-11-25 00:58:33,105:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8376 (32.5084)
2022-11-25 00:58:33,106:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4236 (32.3479)
2022-11-25 00:58:33,107:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7548 (32.1492)
2022-11-25 00:58:33,369:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6011 (13.6011)
2022-11-25 00:58:33,374:INFO: Dataset: zara2               Batch:  2/18	Loss 13.7211 (13.6575)
2022-11-25 00:58:33,375:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7679 (13.6970)
2022-11-25 00:58:33,376:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7764 (13.7188)
2022-11-25 00:58:33,413:INFO: Dataset: zara2               Batch:  5/18	Loss 14.1161 (13.7973)
2022-11-25 00:58:33,429:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0384 (13.8403)
2022-11-25 00:58:33,430:INFO: Dataset: zara2               Batch:  7/18	Loss 14.2305 (13.8954)
2022-11-25 00:58:33,431:INFO: Dataset: zara2               Batch:  8/18	Loss 13.8309 (13.8872)
2022-11-25 00:58:33,431:INFO: Dataset: zara2               Batch:  9/18	Loss 13.8296 (13.8817)
2022-11-25 00:58:33,432:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3266 (13.8251)
2022-11-25 00:58:33,433:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1559 (13.7667)
2022-11-25 00:58:33,435:INFO: Dataset: zara2               Batch: 12/18	Loss 13.2940 (13.7273)
2022-11-25 00:58:33,436:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6419 (13.6441)
2022-11-25 00:58:33,436:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5294 (13.5659)
2022-11-25 00:58:33,437:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5016 (13.4968)
2022-11-25 00:58:33,438:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2018 (13.4143)
2022-11-25 00:58:33,439:INFO: Dataset: zara2               Batch: 17/18	Loss 12.0878 (13.3286)
2022-11-25 00:58:33,440:INFO: Dataset: zara2               Batch: 18/18	Loss 11.8753 (13.2541)
2022-11-25 00:58:33,490:INFO: - Computing ADE (validation o)
2022-11-25 00:58:33,758:INFO: 		 ADE on eth                       dataset:	 1.0752602815628052
2022-11-25 00:58:33,758:INFO: Average validation o:	ADE  1.0753	FDE  2.1678
2022-11-25 00:58:33,758:INFO: - Computing loss (validation)
2022-11-25 00:58:33,948:INFO: Dataset: hotel               Batch: 1/2	Loss 36.5902 (36.5902)
2022-11-25 00:58:33,950:INFO: Dataset: hotel               Batch: 2/2	Loss 35.0005 (36.4546)
2022-11-25 00:58:34,219:INFO: Dataset: univ                Batch: 1/3	Loss 12.8103 (12.8103)
2022-11-25 00:58:34,233:INFO: Dataset: univ                Batch: 2/3	Loss 12.7245 (12.7651)
2022-11-25 00:58:34,234:INFO: Dataset: univ                Batch: 3/3	Loss 12.7461 (12.7594)
2022-11-25 00:58:34,491:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4073 (30.4073)
2022-11-25 00:58:34,493:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4086 (30.4076)
2022-11-25 00:58:34,746:INFO: Dataset: zara2               Batch: 1/5	Loss 11.7484 (11.7484)
2022-11-25 00:58:34,747:INFO: Dataset: zara2               Batch: 2/5	Loss 11.7712 (11.7599)
2022-11-25 00:58:34,748:INFO: Dataset: zara2               Batch: 3/5	Loss 11.8679 (11.7955)
2022-11-25 00:58:34,749:INFO: Dataset: zara2               Batch: 4/5	Loss 11.8062 (11.7981)
2022-11-25 00:58:34,750:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5781 (11.7573)
2022-11-25 00:58:34,805:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_588.pth.tar
2022-11-25 00:58:34,805:INFO: 
===> EPOCH: 589 (P4)
2022-11-25 00:58:34,805:INFO: - Computing loss (training)
2022-11-25 00:58:35,012:INFO: Dataset: hotel               Batch: 1/4	Loss 32.9557 (32.9557)
2022-11-25 00:58:35,015:INFO: Dataset: hotel               Batch: 2/4	Loss 33.6063 (33.2885)
2022-11-25 00:58:35,017:INFO: Dataset: hotel               Batch: 3/4	Loss 33.8403 (33.4614)
2022-11-25 00:58:35,019:INFO: Dataset: hotel               Batch: 4/4	Loss 33.1440 (33.4086)
2022-11-25 00:58:35,287:INFO: Dataset: univ                Batch:  1/15	Loss 13.4069 (13.4069)
2022-11-25 00:58:35,289:INFO: Dataset: univ                Batch:  2/15	Loss 13.4599 (13.4325)
2022-11-25 00:58:35,290:INFO: Dataset: univ                Batch:  3/15	Loss 13.4358 (13.4336)
2022-11-25 00:58:35,294:INFO: Dataset: univ                Batch:  4/15	Loss 13.4662 (13.4428)
2022-11-25 00:58:35,347:INFO: Dataset: univ                Batch:  5/15	Loss 13.6949 (13.4899)
2022-11-25 00:58:35,355:INFO: Dataset: univ                Batch:  6/15	Loss 13.3454 (13.4659)
2022-11-25 00:58:35,356:INFO: Dataset: univ                Batch:  7/15	Loss 13.3389 (13.4479)
2022-11-25 00:58:35,357:INFO: Dataset: univ                Batch:  8/15	Loss 13.2669 (13.4281)
2022-11-25 00:58:35,358:INFO: Dataset: univ                Batch:  9/15	Loss 12.9113 (13.3745)
2022-11-25 00:58:35,360:INFO: Dataset: univ                Batch: 10/15	Loss 12.8534 (13.3258)
2022-11-25 00:58:35,362:INFO: Dataset: univ                Batch: 11/15	Loss 12.5221 (13.2499)
2022-11-25 00:58:35,363:INFO: Dataset: univ                Batch: 12/15	Loss 12.3029 (13.1664)
2022-11-25 00:58:35,364:INFO: Dataset: univ                Batch: 13/15	Loss 12.0919 (13.0940)
2022-11-25 00:58:35,365:INFO: Dataset: univ                Batch: 14/15	Loss 11.7327 (12.9917)
2022-11-25 00:58:35,366:INFO: Dataset: univ                Batch: 15/15	Loss 11.3682 (12.9668)
2022-11-25 00:58:35,617:INFO: Dataset: zara1               Batch: 1/8	Loss 32.7147 (32.7147)
2022-11-25 00:58:35,618:INFO: Dataset: zara1               Batch: 2/8	Loss 32.8418 (32.7823)
2022-11-25 00:58:35,620:INFO: Dataset: zara1               Batch: 3/8	Loss 32.9762 (32.8484)
2022-11-25 00:58:35,623:INFO: Dataset: zara1               Batch: 4/8	Loss 32.4520 (32.7343)
2022-11-25 00:58:35,625:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3796 (32.6637)
2022-11-25 00:58:35,661:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7368 (32.5175)
2022-11-25 00:58:35,662:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5811 (32.3990)
2022-11-25 00:58:35,663:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8153 (32.2315)
2022-11-25 00:58:35,923:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6628 (13.6628)
2022-11-25 00:58:35,926:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8765 (13.7763)
2022-11-25 00:58:35,927:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9196 (13.8240)
2022-11-25 00:58:35,929:INFO: Dataset: zara2               Batch:  4/18	Loss 13.8504 (13.8302)
2022-11-25 00:58:35,979:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7631 (13.8174)
2022-11-25 00:58:35,989:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9230 (13.8363)
2022-11-25 00:58:35,990:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7967 (13.8306)
2022-11-25 00:58:35,991:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7186 (13.8174)
2022-11-25 00:58:35,992:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4361 (13.7775)
2022-11-25 00:58:35,993:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5275 (13.7526)
2022-11-25 00:58:35,995:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3972 (13.7209)
2022-11-25 00:58:35,996:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9839 (13.6608)
2022-11-25 00:58:35,997:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9479 (13.6117)
2022-11-25 00:58:35,998:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6806 (13.5498)
2022-11-25 00:58:35,999:INFO: Dataset: zara2               Batch: 15/18	Loss 12.6416 (13.4889)
2022-11-25 00:58:36,000:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0560 (13.3886)
2022-11-25 00:58:36,001:INFO: Dataset: zara2               Batch: 17/18	Loss 12.1103 (13.3085)
2022-11-25 00:58:36,003:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6811 (13.2350)
2022-11-25 00:58:36,048:INFO: - Computing ADE (validation o)
2022-11-25 00:58:36,315:INFO: 		 ADE on eth                       dataset:	 1.068832516670227
2022-11-25 00:58:36,316:INFO: Average validation o:	ADE  1.0688	FDE  2.1476
2022-11-25 00:58:36,316:INFO: - Computing loss (validation)
2022-11-25 00:58:36,509:INFO: Dataset: hotel               Batch: 1/2	Loss 36.2717 (36.2717)
2022-11-25 00:58:36,511:INFO: Dataset: hotel               Batch: 2/2	Loss 37.3949 (36.3445)
2022-11-25 00:58:36,765:INFO: Dataset: univ                Batch: 1/3	Loss 12.8383 (12.8383)
2022-11-25 00:58:36,774:INFO: Dataset: univ                Batch: 2/3	Loss 12.8647 (12.8510)
2022-11-25 00:58:36,774:INFO: Dataset: univ                Batch: 3/3	Loss 12.7835 (12.8293)
2022-11-25 00:58:37,049:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4600 (30.4600)
2022-11-25 00:58:37,049:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4251 (30.4501)
2022-11-25 00:58:37,298:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6196 (11.6196)
2022-11-25 00:58:37,303:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6467 (11.6325)
2022-11-25 00:58:37,305:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6128 (11.6263)
2022-11-25 00:58:37,306:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6826 (11.6410)
2022-11-25 00:58:37,306:INFO: Dataset: zara2               Batch: 5/5	Loss 11.8154 (11.6743)
2022-11-25 00:58:37,358:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_589.pth.tar
2022-11-25 00:58:37,358:INFO: 
===> EPOCH: 590 (P4)
2022-11-25 00:58:37,358:INFO: - Computing loss (training)
2022-11-25 00:58:37,570:INFO: Dataset: hotel               Batch: 1/4	Loss 32.2950 (32.2950)
2022-11-25 00:58:37,573:INFO: Dataset: hotel               Batch: 2/4	Loss 33.9465 (33.1658)
2022-11-25 00:58:37,574:INFO: Dataset: hotel               Batch: 3/4	Loss 33.4832 (33.2750)
2022-11-25 00:58:37,575:INFO: Dataset: hotel               Batch: 4/4	Loss 32.5340 (33.1362)
2022-11-25 00:58:37,836:INFO: Dataset: univ                Batch:  1/15	Loss 13.4390 (13.4390)
2022-11-25 00:58:37,839:INFO: Dataset: univ                Batch:  2/15	Loss 13.7303 (13.5762)
2022-11-25 00:58:37,840:INFO: Dataset: univ                Batch:  3/15	Loss 13.6332 (13.5958)
2022-11-25 00:58:37,886:INFO: Dataset: univ                Batch:  4/15	Loss 13.5744 (13.5903)
2022-11-25 00:58:37,887:INFO: Dataset: univ                Batch:  5/15	Loss 13.5222 (13.5769)
2022-11-25 00:58:37,891:INFO: Dataset: univ                Batch:  6/15	Loss 13.5465 (13.5719)
2022-11-25 00:58:37,892:INFO: Dataset: univ                Batch:  7/15	Loss 13.2320 (13.5186)
2022-11-25 00:58:37,893:INFO: Dataset: univ                Batch:  8/15	Loss 13.2326 (13.4850)
2022-11-25 00:58:37,894:INFO: Dataset: univ                Batch:  9/15	Loss 13.0329 (13.4364)
2022-11-25 00:58:37,895:INFO: Dataset: univ                Batch: 10/15	Loss 12.8170 (13.3704)
2022-11-25 00:58:37,898:INFO: Dataset: univ                Batch: 11/15	Loss 12.4652 (13.2910)
2022-11-25 00:58:37,899:INFO: Dataset: univ                Batch: 12/15	Loss 12.3160 (13.2136)
2022-11-25 00:58:37,900:INFO: Dataset: univ                Batch: 13/15	Loss 12.0842 (13.1296)
2022-11-25 00:58:37,901:INFO: Dataset: univ                Batch: 14/15	Loss 11.8322 (13.0364)
2022-11-25 00:58:37,902:INFO: Dataset: univ                Batch: 15/15	Loss 11.5969 (13.0189)
2022-11-25 00:58:38,167:INFO: Dataset: zara1               Batch: 1/8	Loss 32.8670 (32.8670)
2022-11-25 00:58:38,176:INFO: Dataset: zara1               Batch: 2/8	Loss 32.9498 (32.9115)
2022-11-25 00:58:38,177:INFO: Dataset: zara1               Batch: 3/8	Loss 32.8746 (32.9001)
2022-11-25 00:58:38,178:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7268 (32.8600)
2022-11-25 00:58:38,181:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2656 (32.7264)
2022-11-25 00:58:38,228:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8706 (32.5860)
2022-11-25 00:58:38,231:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4353 (32.4143)
2022-11-25 00:58:38,235:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8028 (32.2303)
2022-11-25 00:58:38,524:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6169 (13.6169)
2022-11-25 00:58:38,526:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9351 (13.7705)
2022-11-25 00:58:38,527:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8794 (13.8026)
2022-11-25 00:58:38,529:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7262 (13.7841)
2022-11-25 00:58:38,591:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0006 (13.8310)
2022-11-25 00:58:38,600:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8092 (13.8277)
2022-11-25 00:58:38,601:INFO: Dataset: zara2               Batch:  7/18	Loss 14.2568 (13.8904)
2022-11-25 00:58:38,602:INFO: Dataset: zara2               Batch:  8/18	Loss 14.1051 (13.9186)
2022-11-25 00:58:38,603:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5920 (13.8827)
2022-11-25 00:58:38,604:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5088 (13.8479)
2022-11-25 00:58:38,606:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2322 (13.7913)
2022-11-25 00:58:38,607:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8379 (13.7124)
2022-11-25 00:58:38,608:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7521 (13.6498)
2022-11-25 00:58:38,609:INFO: Dataset: zara2               Batch: 14/18	Loss 12.8478 (13.5952)
2022-11-25 00:58:38,610:INFO: Dataset: zara2               Batch: 15/18	Loss 12.4286 (13.5158)
2022-11-25 00:58:38,611:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1935 (13.4320)
2022-11-25 00:58:38,612:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9182 (13.3426)
2022-11-25 00:58:38,614:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5528 (13.2503)
2022-11-25 00:58:38,659:INFO: - Computing ADE (validation o)
2022-11-25 00:58:38,928:INFO: 		 ADE on eth                       dataset:	 1.067009449005127
2022-11-25 00:58:38,929:INFO: Average validation o:	ADE  1.0670	FDE  2.1292
2022-11-25 00:58:38,929:INFO: - Computing loss (validation)
2022-11-25 00:58:39,122:INFO: Dataset: hotel               Batch: 1/2	Loss 36.3019 (36.3019)
2022-11-25 00:58:39,123:INFO: Dataset: hotel               Batch: 2/2	Loss 35.3777 (36.2136)
2022-11-25 00:58:39,379:INFO: Dataset: univ                Batch: 1/3	Loss 12.8227 (12.8227)
2022-11-25 00:58:39,380:INFO: Dataset: univ                Batch: 2/3	Loss 12.7695 (12.7950)
2022-11-25 00:58:39,387:INFO: Dataset: univ                Batch: 3/3	Loss 12.9230 (12.8370)
2022-11-25 00:58:39,633:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4298 (30.4298)
2022-11-25 00:58:39,636:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2205 (30.3760)
2022-11-25 00:58:39,899:INFO: Dataset: zara2               Batch: 1/5	Loss 11.7190 (11.7190)
2022-11-25 00:58:39,901:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6953 (11.7067)
2022-11-25 00:58:39,902:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5735 (11.6617)
2022-11-25 00:58:39,903:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5776 (11.6410)
2022-11-25 00:58:39,904:INFO: Dataset: zara2               Batch: 5/5	Loss 11.6587 (11.6444)
2022-11-25 00:58:39,961:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_590.pth.tar
2022-11-25 00:58:39,961:INFO: 
===> EPOCH: 591 (P4)
2022-11-25 00:58:39,961:INFO: - Computing loss (training)
2022-11-25 00:58:40,198:INFO: Dataset: hotel               Batch: 1/4	Loss 33.3674 (33.3674)
2022-11-25 00:58:40,200:INFO: Dataset: hotel               Batch: 2/4	Loss 32.8211 (33.0871)
2022-11-25 00:58:40,201:INFO: Dataset: hotel               Batch: 3/4	Loss 32.7437 (32.9750)
2022-11-25 00:58:40,203:INFO: Dataset: hotel               Batch: 4/4	Loss 32.1950 (32.8413)
2022-11-25 00:58:40,463:INFO: Dataset: univ                Batch:  1/15	Loss 13.4311 (13.4311)
2022-11-25 00:58:40,465:INFO: Dataset: univ                Batch:  2/15	Loss 13.7950 (13.5984)
2022-11-25 00:58:40,472:INFO: Dataset: univ                Batch:  3/15	Loss 13.6653 (13.6204)
2022-11-25 00:58:40,473:INFO: Dataset: univ                Batch:  4/15	Loss 13.5646 (13.6054)
2022-11-25 00:58:40,523:INFO: Dataset: univ                Batch:  5/15	Loss 13.5634 (13.5972)
2022-11-25 00:58:40,530:INFO: Dataset: univ                Batch:  6/15	Loss 13.6515 (13.6060)
2022-11-25 00:58:40,531:INFO: Dataset: univ                Batch:  7/15	Loss 13.3171 (13.5621)
2022-11-25 00:58:40,532:INFO: Dataset: univ                Batch:  8/15	Loss 13.1628 (13.5103)
2022-11-25 00:58:40,533:INFO: Dataset: univ                Batch:  9/15	Loss 12.9381 (13.4478)
2022-11-25 00:58:40,534:INFO: Dataset: univ                Batch: 10/15	Loss 12.7615 (13.3807)
2022-11-25 00:58:40,535:INFO: Dataset: univ                Batch: 11/15	Loss 12.6039 (13.3110)
2022-11-25 00:58:40,537:INFO: Dataset: univ                Batch: 12/15	Loss 12.2253 (13.2170)
2022-11-25 00:58:40,538:INFO: Dataset: univ                Batch: 13/15	Loss 12.0059 (13.1289)
2022-11-25 00:58:40,539:INFO: Dataset: univ                Batch: 14/15	Loss 11.6791 (13.0205)
2022-11-25 00:58:40,540:INFO: Dataset: univ                Batch: 15/15	Loss 11.4784 (12.9987)
2022-11-25 00:58:40,798:INFO: Dataset: zara1               Batch: 1/8	Loss 32.9323 (32.9323)
2022-11-25 00:58:40,807:INFO: Dataset: zara1               Batch: 2/8	Loss 32.9419 (32.9371)
2022-11-25 00:58:40,808:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0849 (32.9849)
2022-11-25 00:58:40,809:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7483 (32.9185)
2022-11-25 00:58:40,810:INFO: Dataset: zara1               Batch: 5/8	Loss 32.4715 (32.8310)
2022-11-25 00:58:40,846:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9311 (32.6477)
2022-11-25 00:58:40,850:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3961 (32.4682)
2022-11-25 00:58:40,854:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7921 (32.2988)
2022-11-25 00:58:41,145:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7625 (13.7625)
2022-11-25 00:58:41,146:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9793 (13.8769)
2022-11-25 00:58:41,148:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8697 (13.8746)
2022-11-25 00:58:41,151:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2667 (13.9693)
2022-11-25 00:58:41,194:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7898 (13.9315)
2022-11-25 00:58:41,201:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8946 (13.9260)
2022-11-25 00:58:41,202:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6738 (13.8903)
2022-11-25 00:58:41,203:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5927 (13.8507)
2022-11-25 00:58:41,204:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4214 (13.8076)
2022-11-25 00:58:41,205:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4243 (13.7712)
2022-11-25 00:58:41,206:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1259 (13.7113)
2022-11-25 00:58:41,208:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0377 (13.6568)
2022-11-25 00:58:41,209:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9193 (13.5934)
2022-11-25 00:58:41,210:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5762 (13.5180)
2022-11-25 00:58:41,211:INFO: Dataset: zara2               Batch: 15/18	Loss 12.4614 (13.4508)
2022-11-25 00:58:41,212:INFO: Dataset: zara2               Batch: 16/18	Loss 12.3936 (13.3884)
2022-11-25 00:58:41,213:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8337 (13.2901)
2022-11-25 00:58:41,214:INFO: Dataset: zara2               Batch: 18/18	Loss 11.7479 (13.2151)
2022-11-25 00:58:41,260:INFO: - Computing ADE (validation o)
2022-11-25 00:58:41,534:INFO: 		 ADE on eth                       dataset:	 1.0589128732681274
2022-11-25 00:58:41,534:INFO: Average validation o:	ADE  1.0589	FDE  2.1770
2022-11-25 00:58:41,535:INFO: - Computing loss (validation)
2022-11-25 00:58:41,739:INFO: Dataset: hotel               Batch: 1/2	Loss 35.9949 (35.9949)
2022-11-25 00:58:41,739:INFO: Dataset: hotel               Batch: 2/2	Loss 37.4780 (36.1012)
2022-11-25 00:58:41,995:INFO: Dataset: univ                Batch: 1/3	Loss 12.9628 (12.9628)
2022-11-25 00:58:41,996:INFO: Dataset: univ                Batch: 2/3	Loss 12.8888 (12.9276)
2022-11-25 00:58:41,996:INFO: Dataset: univ                Batch: 3/3	Loss 12.9398 (12.9319)
2022-11-25 00:58:42,259:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4928 (30.4928)
2022-11-25 00:58:42,260:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1856 (30.4177)
2022-11-25 00:58:42,538:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4095 (11.4095)
2022-11-25 00:58:42,553:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5598 (11.4909)
2022-11-25 00:58:42,554:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6067 (11.5299)
2022-11-25 00:58:42,554:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4956 (11.5218)
2022-11-25 00:58:42,554:INFO: Dataset: zara2               Batch: 5/5	Loss 11.7498 (11.5726)
2022-11-25 00:58:42,608:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_591.pth.tar
2022-11-25 00:58:42,608:INFO: 
===> EPOCH: 592 (P4)
2022-11-25 00:58:42,609:INFO: - Computing loss (training)
2022-11-25 00:58:42,827:INFO: Dataset: hotel               Batch: 1/4	Loss 33.1942 (33.1942)
2022-11-25 00:58:42,836:INFO: Dataset: hotel               Batch: 2/4	Loss 32.8586 (33.0260)
2022-11-25 00:58:42,837:INFO: Dataset: hotel               Batch: 3/4	Loss 32.6000 (32.8842)
2022-11-25 00:58:42,841:INFO: Dataset: hotel               Batch: 4/4	Loss 31.3072 (32.6325)
2022-11-25 00:58:43,093:INFO: Dataset: univ                Batch:  1/15	Loss 13.6604 (13.6604)
2022-11-25 00:58:43,110:INFO: Dataset: univ                Batch:  2/15	Loss 13.7487 (13.7030)
2022-11-25 00:58:43,112:INFO: Dataset: univ                Batch:  3/15	Loss 13.7111 (13.7056)
2022-11-25 00:58:43,113:INFO: Dataset: univ                Batch:  4/15	Loss 13.5737 (13.6713)
2022-11-25 00:58:43,149:INFO: Dataset: univ                Batch:  5/15	Loss 13.6910 (13.6751)
2022-11-25 00:58:43,156:INFO: Dataset: univ                Batch:  6/15	Loss 13.5032 (13.6476)
2022-11-25 00:58:43,158:INFO: Dataset: univ                Batch:  7/15	Loss 13.4599 (13.6215)
2022-11-25 00:58:43,159:INFO: Dataset: univ                Batch:  8/15	Loss 13.1391 (13.5594)
2022-11-25 00:58:43,160:INFO: Dataset: univ                Batch:  9/15	Loss 13.0008 (13.4988)
2022-11-25 00:58:43,161:INFO: Dataset: univ                Batch: 10/15	Loss 12.9336 (13.4509)
2022-11-25 00:58:43,163:INFO: Dataset: univ                Batch: 11/15	Loss 12.6251 (13.3856)
2022-11-25 00:58:43,164:INFO: Dataset: univ                Batch: 12/15	Loss 12.2835 (13.2980)
2022-11-25 00:58:43,166:INFO: Dataset: univ                Batch: 13/15	Loss 12.0248 (13.2013)
2022-11-25 00:58:43,167:INFO: Dataset: univ                Batch: 14/15	Loss 11.6910 (13.0900)
2022-11-25 00:58:43,168:INFO: Dataset: univ                Batch: 15/15	Loss 11.3022 (13.0678)
2022-11-25 00:58:43,422:INFO: Dataset: zara1               Batch: 1/8	Loss 32.9970 (32.9970)
2022-11-25 00:58:43,424:INFO: Dataset: zara1               Batch: 2/8	Loss 33.1151 (33.0562)
2022-11-25 00:58:43,426:INFO: Dataset: zara1               Batch: 3/8	Loss 32.8962 (33.0057)
2022-11-25 00:58:43,429:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5576 (32.9014)
2022-11-25 00:58:43,430:INFO: Dataset: zara1               Batch: 5/8	Loss 32.4203 (32.8081)
2022-11-25 00:58:43,444:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0572 (32.6925)
2022-11-25 00:58:43,445:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4396 (32.5255)
2022-11-25 00:58:43,446:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8774 (32.3216)
2022-11-25 00:58:43,711:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6515 (13.6515)
2022-11-25 00:58:43,712:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9342 (13.7889)
2022-11-25 00:58:43,716:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1252 (13.9059)
2022-11-25 00:58:43,717:INFO: Dataset: zara2               Batch:  4/18	Loss 14.0849 (13.9507)
2022-11-25 00:58:43,743:INFO: Dataset: zara2               Batch:  5/18	Loss 13.9177 (13.9443)
2022-11-25 00:58:43,749:INFO: Dataset: zara2               Batch:  6/18	Loss 14.3207 (14.0095)
2022-11-25 00:58:43,750:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0813 (14.0187)
2022-11-25 00:58:43,750:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7847 (13.9883)
2022-11-25 00:58:43,751:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5068 (13.9337)
2022-11-25 00:58:43,752:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1716 (13.8615)
2022-11-25 00:58:43,753:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2885 (13.8099)
2022-11-25 00:58:43,755:INFO: Dataset: zara2               Batch: 12/18	Loss 13.3255 (13.7721)
2022-11-25 00:58:43,756:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5525 (13.6775)
2022-11-25 00:58:43,756:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4121 (13.5889)
2022-11-25 00:58:43,757:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2753 (13.5003)
2022-11-25 00:58:43,758:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0520 (13.4071)
2022-11-25 00:58:43,759:INFO: Dataset: zara2               Batch: 17/18	Loss 12.1582 (13.3413)
2022-11-25 00:58:43,761:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6582 (13.2639)
2022-11-25 00:58:43,817:INFO: - Computing ADE (validation o)
2022-11-25 00:58:44,102:INFO: 		 ADE on eth                       dataset:	 1.0000108480453491
2022-11-25 00:58:44,102:INFO: Average validation o:	ADE  1.0000	FDE  2.0187
2022-11-25 00:58:44,103:INFO: - Computing loss (validation)
2022-11-25 00:58:44,297:INFO: Dataset: hotel               Batch: 1/2	Loss 36.2844 (36.2844)
2022-11-25 00:58:44,297:INFO: Dataset: hotel               Batch: 2/2	Loss 33.9196 (36.0342)
2022-11-25 00:58:44,555:INFO: Dataset: univ                Batch: 1/3	Loss 12.7984 (12.7984)
2022-11-25 00:58:44,557:INFO: Dataset: univ                Batch: 2/3	Loss 13.1374 (12.9509)
2022-11-25 00:58:44,560:INFO: Dataset: univ                Batch: 3/3	Loss 12.7808 (12.8934)
2022-11-25 00:58:44,812:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4691 (30.4691)
2022-11-25 00:58:44,813:INFO: Dataset: zara1               Batch: 2/2	Loss 30.3193 (30.4296)
2022-11-25 00:58:45,069:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4837 (11.4837)
2022-11-25 00:58:45,072:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6555 (11.5734)
2022-11-25 00:58:45,072:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5861 (11.5774)
2022-11-25 00:58:45,073:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5066 (11.5601)
2022-11-25 00:58:45,074:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4869 (11.5454)
2022-11-25 00:58:45,129:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_592.pth.tar
2022-11-25 00:58:45,129:INFO: 
===> EPOCH: 593 (P4)
2022-11-25 00:58:45,129:INFO: - Computing loss (training)
2022-11-25 00:58:45,335:INFO: Dataset: hotel               Batch: 1/4	Loss 33.1860 (33.1860)
2022-11-25 00:58:45,338:INFO: Dataset: hotel               Batch: 2/4	Loss 32.2981 (32.7536)
2022-11-25 00:58:45,341:INFO: Dataset: hotel               Batch: 3/4	Loss 31.8342 (32.4554)
2022-11-25 00:58:45,342:INFO: Dataset: hotel               Batch: 4/4	Loss 32.0995 (32.3935)
2022-11-25 00:58:45,609:INFO: Dataset: univ                Batch:  1/15	Loss 13.5419 (13.5419)
2022-11-25 00:58:45,618:INFO: Dataset: univ                Batch:  2/15	Loss 13.6305 (13.5847)
2022-11-25 00:58:45,619:INFO: Dataset: univ                Batch:  3/15	Loss 13.8199 (13.6615)
2022-11-25 00:58:45,621:INFO: Dataset: univ                Batch:  4/15	Loss 13.6928 (13.6691)
2022-11-25 00:58:45,675:INFO: Dataset: univ                Batch:  5/15	Loss 13.6209 (13.6589)
2022-11-25 00:58:45,682:INFO: Dataset: univ                Batch:  6/15	Loss 13.5939 (13.6482)
2022-11-25 00:58:45,683:INFO: Dataset: univ                Batch:  7/15	Loss 13.4037 (13.6142)
2022-11-25 00:58:45,685:INFO: Dataset: univ                Batch:  8/15	Loss 13.0848 (13.5419)
2022-11-25 00:58:45,686:INFO: Dataset: univ                Batch:  9/15	Loss 13.1521 (13.5044)
2022-11-25 00:58:45,687:INFO: Dataset: univ                Batch: 10/15	Loss 12.7067 (13.4196)
2022-11-25 00:58:45,689:INFO: Dataset: univ                Batch: 11/15	Loss 12.6239 (13.3545)
2022-11-25 00:58:45,690:INFO: Dataset: univ                Batch: 12/15	Loss 12.2237 (13.2658)
2022-11-25 00:58:45,691:INFO: Dataset: univ                Batch: 13/15	Loss 11.9351 (13.1668)
2022-11-25 00:58:45,692:INFO: Dataset: univ                Batch: 14/15	Loss 11.6087 (13.0552)
2022-11-25 00:58:45,693:INFO: Dataset: univ                Batch: 15/15	Loss 11.1716 (13.0260)
2022-11-25 00:58:45,981:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4290 (33.4290)
2022-11-25 00:58:45,986:INFO: Dataset: zara1               Batch: 2/8	Loss 33.3738 (33.4004)
2022-11-25 00:58:45,987:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0609 (33.2759)
2022-11-25 00:58:45,988:INFO: Dataset: zara1               Batch: 4/8	Loss 32.9187 (33.1735)
2022-11-25 00:58:45,990:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5152 (33.0439)
2022-11-25 00:58:46,048:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0452 (32.8716)
2022-11-25 00:58:46,051:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4440 (32.6608)
2022-11-25 00:58:46,055:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7683 (32.4616)
2022-11-25 00:58:46,345:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6165 (13.6165)
2022-11-25 00:58:46,348:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8790 (13.7603)
2022-11-25 00:58:46,350:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8576 (13.7910)
2022-11-25 00:58:46,390:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9495 (13.8274)
2022-11-25 00:58:46,391:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2270 (13.9062)
2022-11-25 00:58:46,395:INFO: Dataset: zara2               Batch:  6/18	Loss 14.2272 (13.9656)
2022-11-25 00:58:46,396:INFO: Dataset: zara2               Batch:  7/18	Loss 14.1630 (13.9940)
2022-11-25 00:58:46,397:INFO: Dataset: zara2               Batch:  8/18	Loss 13.8493 (13.9736)
2022-11-25 00:58:46,398:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3544 (13.8980)
2022-11-25 00:58:46,399:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1519 (13.8266)
2022-11-25 00:58:46,400:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9848 (13.7489)
2022-11-25 00:58:46,401:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0999 (13.6834)
2022-11-25 00:58:46,402:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8205 (13.6187)
2022-11-25 00:58:46,403:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5260 (13.5414)
2022-11-25 00:58:46,404:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1801 (13.4465)
2022-11-25 00:58:46,405:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9852 (13.3515)
2022-11-25 00:58:46,407:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8810 (13.2651)
2022-11-25 00:58:46,408:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6669 (13.1956)
2022-11-25 00:58:46,454:INFO: - Computing ADE (validation o)
2022-11-25 00:58:46,727:INFO: 		 ADE on eth                       dataset:	 1.03386390209198
2022-11-25 00:58:46,727:INFO: Average validation o:	ADE  1.0339	FDE  2.1156
2022-11-25 00:58:46,728:INFO: - Computing loss (validation)
2022-11-25 00:58:46,921:INFO: Dataset: hotel               Batch: 1/2	Loss 35.9591 (35.9591)
2022-11-25 00:58:46,924:INFO: Dataset: hotel               Batch: 2/2	Loss 35.8034 (35.9474)
2022-11-25 00:58:47,192:INFO: Dataset: univ                Batch: 1/3	Loss 12.8351 (12.8351)
2022-11-25 00:58:47,194:INFO: Dataset: univ                Batch: 2/3	Loss 12.9914 (12.9140)
2022-11-25 00:58:47,197:INFO: Dataset: univ                Batch: 3/3	Loss 13.1670 (12.9886)
2022-11-25 00:58:47,452:INFO: Dataset: zara1               Batch: 1/2	Loss 30.3596 (30.3596)
2022-11-25 00:58:47,452:INFO: Dataset: zara1               Batch: 2/2	Loss 30.6852 (30.4350)
2022-11-25 00:58:47,713:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5288 (11.5288)
2022-11-25 00:58:47,715:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5080 (11.5177)
2022-11-25 00:58:47,716:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4886 (11.5081)
2022-11-25 00:58:47,721:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4653 (11.4981)
2022-11-25 00:58:47,722:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4552 (11.4901)
2022-11-25 00:58:47,776:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_593.pth.tar
2022-11-25 00:58:47,776:INFO: 
===> EPOCH: 594 (P4)
2022-11-25 00:58:47,777:INFO: - Computing loss (training)
2022-11-25 00:58:47,985:INFO: Dataset: hotel               Batch: 1/4	Loss 32.4398 (32.4398)
2022-11-25 00:58:48,002:INFO: Dataset: hotel               Batch: 2/4	Loss 32.0737 (32.2610)
2022-11-25 00:58:48,003:INFO: Dataset: hotel               Batch: 3/4	Loss 32.1483 (32.2239)
2022-11-25 00:58:48,004:INFO: Dataset: hotel               Batch: 4/4	Loss 32.3858 (32.2495)
2022-11-25 00:58:48,334:INFO: Dataset: univ                Batch:  1/15	Loss 13.6244 (13.6244)
2022-11-25 00:58:48,336:INFO: Dataset: univ                Batch:  2/15	Loss 13.6809 (13.6508)
2022-11-25 00:58:48,337:INFO: Dataset: univ                Batch:  3/15	Loss 13.6208 (13.6403)
2022-11-25 00:58:48,338:INFO: Dataset: univ                Batch:  4/15	Loss 13.9148 (13.7117)
2022-11-25 00:58:48,342:INFO: Dataset: univ                Batch:  5/15	Loss 13.5598 (13.6787)
2022-11-25 00:58:48,350:INFO: Dataset: univ                Batch:  6/15	Loss 13.5674 (13.6604)
2022-11-25 00:58:48,352:INFO: Dataset: univ                Batch:  7/15	Loss 13.5344 (13.6422)
2022-11-25 00:58:48,353:INFO: Dataset: univ                Batch:  8/15	Loss 13.3459 (13.6065)
2022-11-25 00:58:48,354:INFO: Dataset: univ                Batch:  9/15	Loss 13.0594 (13.5529)
2022-11-25 00:58:48,355:INFO: Dataset: univ                Batch: 10/15	Loss 12.7353 (13.4723)
2022-11-25 00:58:48,357:INFO: Dataset: univ                Batch: 11/15	Loss 12.6236 (13.3992)
2022-11-25 00:58:48,358:INFO: Dataset: univ                Batch: 12/15	Loss 12.2796 (13.3109)
2022-11-25 00:58:48,360:INFO: Dataset: univ                Batch: 13/15	Loss 11.9922 (13.2105)
2022-11-25 00:58:48,361:INFO: Dataset: univ                Batch: 14/15	Loss 11.6457 (13.0945)
2022-11-25 00:58:48,363:INFO: Dataset: univ                Batch: 15/15	Loss 11.4100 (13.0707)
2022-11-25 00:58:48,621:INFO: Dataset: zara1               Batch: 1/8	Loss 33.1041 (33.1041)
2022-11-25 00:58:48,622:INFO: Dataset: zara1               Batch: 2/8	Loss 33.0705 (33.0864)
2022-11-25 00:58:48,623:INFO: Dataset: zara1               Batch: 3/8	Loss 32.9686 (33.0478)
2022-11-25 00:58:48,624:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8151 (32.9850)
2022-11-25 00:58:48,627:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3505 (32.8551)
2022-11-25 00:58:48,671:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0453 (32.7199)
2022-11-25 00:58:48,675:INFO: Dataset: zara1               Batch: 7/8	Loss 31.5203 (32.5452)
2022-11-25 00:58:48,678:INFO: Dataset: zara1               Batch: 8/8	Loss 31.1295 (32.3879)
2022-11-25 00:58:48,950:INFO: Dataset: zara2               Batch:  1/18	Loss 14.1185 (14.1185)
2022-11-25 00:58:49,002:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1932 (14.1578)
2022-11-25 00:58:49,003:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9571 (14.0843)
2022-11-25 00:58:49,004:INFO: Dataset: zara2               Batch:  4/18	Loss 14.1727 (14.1075)
2022-11-25 00:58:49,006:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0990 (14.1058)
2022-11-25 00:58:49,009:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0133 (14.0888)
2022-11-25 00:58:49,010:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8535 (14.0560)
2022-11-25 00:58:49,011:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5736 (13.9994)
2022-11-25 00:58:49,012:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5408 (13.9503)
2022-11-25 00:58:49,014:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1763 (13.8701)
2022-11-25 00:58:49,016:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1984 (13.8044)
2022-11-25 00:58:49,017:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0655 (13.7416)
2022-11-25 00:58:49,018:INFO: Dataset: zara2               Batch: 13/18	Loss 13.0697 (13.6914)
2022-11-25 00:58:49,019:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3290 (13.5997)
2022-11-25 00:58:49,021:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1066 (13.5003)
2022-11-25 00:58:49,022:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9752 (13.4112)
2022-11-25 00:58:49,024:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7760 (13.3238)
2022-11-25 00:58:49,026:INFO: Dataset: zara2               Batch: 18/18	Loss 11.7787 (13.2604)
2022-11-25 00:58:49,074:INFO: - Computing ADE (validation o)
2022-11-25 00:58:49,335:INFO: 		 ADE on eth                       dataset:	 1.0773601531982422
2022-11-25 00:58:49,335:INFO: Average validation o:	ADE  1.0774	FDE  2.1878
2022-11-25 00:58:49,336:INFO: - Computing loss (validation)
2022-11-25 00:58:49,528:INFO: Dataset: hotel               Batch: 1/2	Loss 35.9416 (35.9416)
2022-11-25 00:58:49,529:INFO: Dataset: hotel               Batch: 2/2	Loss 35.2829 (35.8809)
2022-11-25 00:58:49,798:INFO: Dataset: univ                Batch: 1/3	Loss 12.9699 (12.9699)
2022-11-25 00:58:49,810:INFO: Dataset: univ                Batch: 2/3	Loss 13.2220 (13.0976)
2022-11-25 00:58:49,810:INFO: Dataset: univ                Batch: 3/3	Loss 12.7215 (12.9745)
2022-11-25 00:58:50,061:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4509 (30.4509)
2022-11-25 00:58:50,063:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2172 (30.3817)
2022-11-25 00:58:50,337:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4865 (11.4865)
2022-11-25 00:58:50,337:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5188 (11.5033)
2022-11-25 00:58:50,339:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4134 (11.4736)
2022-11-25 00:58:50,340:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5038 (11.4811)
2022-11-25 00:58:50,341:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4525 (11.4756)
2022-11-25 00:58:50,395:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_594.pth.tar
2022-11-25 00:58:50,395:INFO: 
===> EPOCH: 595 (P4)
2022-11-25 00:58:50,395:INFO: - Computing loss (training)
2022-11-25 00:58:50,608:INFO: Dataset: hotel               Batch: 1/4	Loss 32.0759 (32.0759)
2022-11-25 00:58:50,610:INFO: Dataset: hotel               Batch: 2/4	Loss 31.4420 (31.7566)
2022-11-25 00:58:50,622:INFO: Dataset: hotel               Batch: 3/4	Loss 32.5608 (32.0337)
2022-11-25 00:58:50,624:INFO: Dataset: hotel               Batch: 4/4	Loss 32.2875 (32.0786)
2022-11-25 00:58:50,879:INFO: Dataset: univ                Batch:  1/15	Loss 13.6644 (13.6644)
2022-11-25 00:58:50,881:INFO: Dataset: univ                Batch:  2/15	Loss 13.6459 (13.6548)
2022-11-25 00:58:50,885:INFO: Dataset: univ                Batch:  3/15	Loss 13.8272 (13.7117)
2022-11-25 00:58:50,887:INFO: Dataset: univ                Batch:  4/15	Loss 13.9572 (13.7697)
2022-11-25 00:58:50,914:INFO: Dataset: univ                Batch:  5/15	Loss 13.6019 (13.7340)
2022-11-25 00:58:50,921:INFO: Dataset: univ                Batch:  6/15	Loss 13.5145 (13.6954)
2022-11-25 00:58:50,922:INFO: Dataset: univ                Batch:  7/15	Loss 13.4061 (13.6556)
2022-11-25 00:58:50,923:INFO: Dataset: univ                Batch:  8/15	Loss 13.3140 (13.6132)
2022-11-25 00:58:50,924:INFO: Dataset: univ                Batch:  9/15	Loss 13.1444 (13.5655)
2022-11-25 00:58:50,925:INFO: Dataset: univ                Batch: 10/15	Loss 12.7284 (13.4736)
2022-11-25 00:58:50,927:INFO: Dataset: univ                Batch: 11/15	Loss 12.5245 (13.3854)
2022-11-25 00:58:50,928:INFO: Dataset: univ                Batch: 12/15	Loss 12.1340 (13.2776)
2022-11-25 00:58:50,930:INFO: Dataset: univ                Batch: 13/15	Loss 11.8259 (13.1663)
2022-11-25 00:58:50,931:INFO: Dataset: univ                Batch: 14/15	Loss 11.5716 (13.0522)
2022-11-25 00:58:50,932:INFO: Dataset: univ                Batch: 15/15	Loss 11.2733 (13.0296)
2022-11-25 00:58:51,197:INFO: Dataset: zara1               Batch: 1/8	Loss 33.3663 (33.3663)
2022-11-25 00:58:51,204:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2847 (33.3256)
2022-11-25 00:58:51,209:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0129 (33.2247)
2022-11-25 00:58:51,210:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7330 (33.1142)
2022-11-25 00:58:51,211:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5935 (33.0187)
2022-11-25 00:58:51,268:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0162 (32.8576)
2022-11-25 00:58:51,271:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3645 (32.6486)
2022-11-25 00:58:51,275:INFO: Dataset: zara1               Batch: 8/8	Loss 31.1249 (32.5059)
2022-11-25 00:58:51,561:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7844 (13.7844)
2022-11-25 00:58:51,563:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9859 (13.8833)
2022-11-25 00:58:51,565:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1263 (13.9594)
2022-11-25 00:58:51,569:INFO: Dataset: zara2               Batch:  4/18	Loss 14.4594 (14.0800)
2022-11-25 00:58:51,619:INFO: Dataset: zara2               Batch:  5/18	Loss 14.4062 (14.1449)
2022-11-25 00:58:51,629:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8027 (14.0888)
2022-11-25 00:58:51,630:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8790 (14.0571)
2022-11-25 00:58:51,631:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5238 (13.9961)
2022-11-25 00:58:51,631:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3970 (13.9293)
2022-11-25 00:58:51,632:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5929 (13.8960)
2022-11-25 00:58:51,633:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1206 (13.8269)
2022-11-25 00:58:51,635:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0123 (13.7595)
2022-11-25 00:58:51,636:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6597 (13.6790)
2022-11-25 00:58:51,636:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5349 (13.5971)
2022-11-25 00:58:51,637:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2828 (13.5164)
2022-11-25 00:58:51,638:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9464 (13.4102)
2022-11-25 00:58:51,639:INFO: Dataset: zara2               Batch: 17/18	Loss 11.5091 (13.2929)
2022-11-25 00:58:51,640:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5632 (13.2125)
2022-11-25 00:58:51,689:INFO: - Computing ADE (validation o)
2022-11-25 00:58:51,954:INFO: 		 ADE on eth                       dataset:	 1.0896505117416382
2022-11-25 00:58:51,954:INFO: Average validation o:	ADE  1.0897	FDE  2.1966
2022-11-25 00:58:51,955:INFO: - Computing loss (validation)
2022-11-25 00:58:52,147:INFO: Dataset: hotel               Batch: 1/2	Loss 36.1065 (36.1065)
2022-11-25 00:58:52,168:INFO: Dataset: hotel               Batch: 2/2	Loss 33.5052 (35.8934)
2022-11-25 00:58:52,431:INFO: Dataset: univ                Batch: 1/3	Loss 12.9155 (12.9155)
2022-11-25 00:58:52,433:INFO: Dataset: univ                Batch: 2/3	Loss 13.0763 (12.9927)
2022-11-25 00:58:52,433:INFO: Dataset: univ                Batch: 3/3	Loss 13.0980 (13.0252)
2022-11-25 00:58:52,693:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4946 (30.4946)
2022-11-25 00:58:52,693:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4658 (30.4874)
2022-11-25 00:58:52,962:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6744 (11.6744)
2022-11-25 00:58:52,964:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4222 (11.5485)
2022-11-25 00:58:52,972:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3226 (11.4770)
2022-11-25 00:58:52,973:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2833 (11.4284)
2022-11-25 00:58:52,973:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3719 (11.4176)
2022-11-25 00:58:53,031:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_595.pth.tar
2022-11-25 00:58:53,031:INFO: 
===> EPOCH: 596 (P4)
2022-11-25 00:58:53,032:INFO: - Computing loss (training)
2022-11-25 00:58:53,243:INFO: Dataset: hotel               Batch: 1/4	Loss 31.7696 (31.7696)
2022-11-25 00:58:53,246:INFO: Dataset: hotel               Batch: 2/4	Loss 32.2037 (31.9903)
2022-11-25 00:58:53,250:INFO: Dataset: hotel               Batch: 3/4	Loss 31.5009 (31.8290)
2022-11-25 00:58:53,251:INFO: Dataset: hotel               Batch: 4/4	Loss 33.1376 (32.0534)
2022-11-25 00:58:53,513:INFO: Dataset: univ                Batch:  1/15	Loss 13.5018 (13.5018)
2022-11-25 00:58:53,516:INFO: Dataset: univ                Batch:  2/15	Loss 13.7195 (13.6036)
2022-11-25 00:58:53,518:INFO: Dataset: univ                Batch:  3/15	Loss 13.7788 (13.6614)
2022-11-25 00:58:53,521:INFO: Dataset: univ                Batch:  4/15	Loss 13.8573 (13.7098)
2022-11-25 00:58:53,562:INFO: Dataset: univ                Batch:  5/15	Loss 14.1035 (13.7789)
2022-11-25 00:58:53,570:INFO: Dataset: univ                Batch:  6/15	Loss 13.8013 (13.7826)
2022-11-25 00:58:53,571:INFO: Dataset: univ                Batch:  7/15	Loss 13.5162 (13.7478)
2022-11-25 00:58:53,572:INFO: Dataset: univ                Batch:  8/15	Loss 13.3101 (13.6918)
2022-11-25 00:58:53,573:INFO: Dataset: univ                Batch:  9/15	Loss 12.8350 (13.5879)
2022-11-25 00:58:53,574:INFO: Dataset: univ                Batch: 10/15	Loss 12.6734 (13.4865)
2022-11-25 00:58:53,577:INFO: Dataset: univ                Batch: 11/15	Loss 12.5169 (13.3975)
2022-11-25 00:58:53,578:INFO: Dataset: univ                Batch: 12/15	Loss 12.1501 (13.2883)
2022-11-25 00:58:53,579:INFO: Dataset: univ                Batch: 13/15	Loss 11.9121 (13.1762)
2022-11-25 00:58:53,580:INFO: Dataset: univ                Batch: 14/15	Loss 11.4619 (13.0545)
2022-11-25 00:58:53,581:INFO: Dataset: univ                Batch: 15/15	Loss 11.1140 (13.0248)
2022-11-25 00:58:53,853:INFO: Dataset: zara1               Batch: 1/8	Loss 33.1346 (33.1346)
2022-11-25 00:58:53,857:INFO: Dataset: zara1               Batch: 2/8	Loss 33.4294 (33.2735)
2022-11-25 00:58:53,858:INFO: Dataset: zara1               Batch: 3/8	Loss 33.2734 (33.2735)
2022-11-25 00:58:53,859:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8155 (33.1596)
2022-11-25 00:58:53,861:INFO: Dataset: zara1               Batch: 5/8	Loss 32.6968 (33.0659)
2022-11-25 00:58:53,893:INFO: Dataset: zara1               Batch: 6/8	Loss 32.1073 (32.9133)
2022-11-25 00:58:53,894:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3970 (32.6813)
2022-11-25 00:58:53,895:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5342 (32.4542)
2022-11-25 00:58:54,164:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7564 (13.7564)
2022-11-25 00:58:54,166:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8513 (13.8042)
2022-11-25 00:58:54,235:INFO: Dataset: zara2               Batch:  3/18	Loss 14.2627 (13.9585)
2022-11-25 00:58:54,236:INFO: Dataset: zara2               Batch:  4/18	Loss 14.0996 (13.9971)
2022-11-25 00:58:54,237:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2686 (14.0518)
2022-11-25 00:58:54,240:INFO: Dataset: zara2               Batch:  6/18	Loss 14.1657 (14.0720)
2022-11-25 00:58:54,241:INFO: Dataset: zara2               Batch:  7/18	Loss 14.3046 (14.1060)
2022-11-25 00:58:54,242:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7165 (14.0537)
2022-11-25 00:58:54,243:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6714 (14.0120)
2022-11-25 00:58:54,244:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4548 (13.9561)
2022-11-25 00:58:54,246:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0949 (13.8794)
2022-11-25 00:58:54,247:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8007 (13.7837)
2022-11-25 00:58:54,248:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6714 (13.7006)
2022-11-25 00:58:54,249:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3125 (13.5905)
2022-11-25 00:58:54,251:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3134 (13.5123)
2022-11-25 00:58:54,253:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7919 (13.4086)
2022-11-25 00:58:54,254:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8389 (13.3080)
2022-11-25 00:58:54,256:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5094 (13.2113)
2022-11-25 00:58:54,306:INFO: - Computing ADE (validation o)
2022-11-25 00:58:54,578:INFO: 		 ADE on eth                       dataset:	 1.0759021043777466
2022-11-25 00:58:54,579:INFO: Average validation o:	ADE  1.0759	FDE  2.1956
2022-11-25 00:58:54,579:INFO: - Computing loss (validation)
2022-11-25 00:58:54,776:INFO: Dataset: hotel               Batch: 1/2	Loss 36.0353 (36.0353)
2022-11-25 00:58:54,777:INFO: Dataset: hotel               Batch: 2/2	Loss 33.4876 (35.8266)
2022-11-25 00:58:55,044:INFO: Dataset: univ                Batch: 1/3	Loss 13.0760 (13.0760)
2022-11-25 00:58:55,047:INFO: Dataset: univ                Batch: 2/3	Loss 13.1118 (13.0935)
2022-11-25 00:58:55,048:INFO: Dataset: univ                Batch: 3/3	Loss 12.8278 (12.9955)
2022-11-25 00:58:55,300:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4131 (30.4131)
2022-11-25 00:58:55,300:INFO: Dataset: zara1               Batch: 2/2	Loss 30.3126 (30.3885)
2022-11-25 00:58:55,562:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3613 (11.3613)
2022-11-25 00:58:55,568:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3597 (11.3605)
2022-11-25 00:58:55,572:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4468 (11.3887)
2022-11-25 00:58:55,573:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3869 (11.3883)
2022-11-25 00:58:55,576:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5656 (11.4217)
2022-11-25 00:58:55,630:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_596.pth.tar
2022-11-25 00:58:55,631:INFO: 
===> EPOCH: 597 (P4)
2022-11-25 00:58:55,631:INFO: - Computing loss (training)
2022-11-25 00:58:55,838:INFO: Dataset: hotel               Batch: 1/4	Loss 32.6474 (32.6474)
2022-11-25 00:58:55,842:INFO: Dataset: hotel               Batch: 2/4	Loss 32.4539 (32.5477)
2022-11-25 00:58:55,843:INFO: Dataset: hotel               Batch: 3/4	Loss 31.3579 (32.1617)
2022-11-25 00:58:55,845:INFO: Dataset: hotel               Batch: 4/4	Loss 30.5966 (31.9078)
2022-11-25 00:58:56,101:INFO: Dataset: univ                Batch:  1/15	Loss 13.6859 (13.6859)
2022-11-25 00:58:56,102:INFO: Dataset: univ                Batch:  2/15	Loss 13.8985 (13.7895)
2022-11-25 00:58:56,104:INFO: Dataset: univ                Batch:  3/15	Loss 13.8675 (13.8180)
2022-11-25 00:58:56,105:INFO: Dataset: univ                Batch:  4/15	Loss 13.9603 (13.8522)
2022-11-25 00:58:56,148:INFO: Dataset: univ                Batch:  5/15	Loss 13.7505 (13.8329)
2022-11-25 00:58:56,153:INFO: Dataset: univ                Batch:  6/15	Loss 13.6622 (13.8031)
2022-11-25 00:58:56,154:INFO: Dataset: univ                Batch:  7/15	Loss 13.4994 (13.7638)
2022-11-25 00:58:56,155:INFO: Dataset: univ                Batch:  8/15	Loss 13.2015 (13.6915)
2022-11-25 00:58:56,156:INFO: Dataset: univ                Batch:  9/15	Loss 12.8032 (13.5857)
2022-11-25 00:58:56,157:INFO: Dataset: univ                Batch: 10/15	Loss 12.6922 (13.4959)
2022-11-25 00:58:56,159:INFO: Dataset: univ                Batch: 11/15	Loss 12.2309 (13.3821)
2022-11-25 00:58:56,161:INFO: Dataset: univ                Batch: 12/15	Loss 12.0821 (13.2718)
2022-11-25 00:58:56,162:INFO: Dataset: univ                Batch: 13/15	Loss 11.9359 (13.1721)
2022-11-25 00:58:56,164:INFO: Dataset: univ                Batch: 14/15	Loss 11.4744 (13.0381)
2022-11-25 00:58:56,165:INFO: Dataset: univ                Batch: 15/15	Loss 11.1420 (13.0137)
2022-11-25 00:58:56,430:INFO: Dataset: zara1               Batch: 1/8	Loss 33.2389 (33.2389)
2022-11-25 00:58:56,433:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2990 (33.2685)
2022-11-25 00:58:56,435:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0524 (33.1844)
2022-11-25 00:58:56,435:INFO: Dataset: zara1               Batch: 4/8	Loss 33.1180 (33.1688)
2022-11-25 00:58:56,436:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5170 (33.0481)
2022-11-25 00:58:56,492:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0196 (32.8938)
2022-11-25 00:58:56,496:INFO: Dataset: zara1               Batch: 7/8	Loss 31.6100 (32.7191)
2022-11-25 00:58:56,499:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8189 (32.5241)
2022-11-25 00:58:56,772:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7135 (13.7135)
2022-11-25 00:58:56,773:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8541 (13.7844)
2022-11-25 00:58:56,775:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1778 (13.9177)
2022-11-25 00:58:56,776:INFO: Dataset: zara2               Batch:  4/18	Loss 14.4136 (14.0349)
2022-11-25 00:58:56,805:INFO: Dataset: zara2               Batch:  5/18	Loss 13.9266 (14.0115)
2022-11-25 00:58:56,810:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9098 (13.9944)
2022-11-25 00:58:56,811:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0776 (14.0061)
2022-11-25 00:58:56,812:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7412 (13.9723)
2022-11-25 00:58:56,813:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5560 (13.9238)
2022-11-25 00:58:56,813:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3789 (13.8648)
2022-11-25 00:58:56,816:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2384 (13.8077)
2022-11-25 00:58:56,817:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9744 (13.7349)
2022-11-25 00:58:56,817:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6949 (13.6585)
2022-11-25 00:58:56,818:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6234 (13.5912)
2022-11-25 00:58:56,819:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1246 (13.4912)
2022-11-25 00:58:56,820:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1841 (13.4108)
2022-11-25 00:58:56,822:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7117 (13.2957)
2022-11-25 00:58:56,823:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5103 (13.2133)
2022-11-25 00:58:56,868:INFO: - Computing ADE (validation o)
2022-11-25 00:58:57,137:INFO: 		 ADE on eth                       dataset:	 1.1273530721664429
2022-11-25 00:58:57,137:INFO: Average validation o:	ADE  1.1274	FDE  2.2602
2022-11-25 00:58:57,138:INFO: - Computing loss (validation)
2022-11-25 00:58:57,331:INFO: Dataset: hotel               Batch: 1/2	Loss 36.0067 (36.0067)
2022-11-25 00:58:57,331:INFO: Dataset: hotel               Batch: 2/2	Loss 33.3907 (35.7657)
2022-11-25 00:58:57,598:INFO: Dataset: univ                Batch: 1/3	Loss 13.0513 (13.0513)
2022-11-25 00:58:57,598:INFO: Dataset: univ                Batch: 2/3	Loss 12.8636 (12.9617)
2022-11-25 00:58:57,599:INFO: Dataset: univ                Batch: 3/3	Loss 13.1514 (13.0199)
2022-11-25 00:58:57,841:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4934 (30.4934)
2022-11-25 00:58:57,842:INFO: Dataset: zara1               Batch: 2/2	Loss 30.5845 (30.5177)
2022-11-25 00:58:58,088:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3704 (11.3704)
2022-11-25 00:58:58,088:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4338 (11.4029)
2022-11-25 00:58:58,089:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3497 (11.3861)
2022-11-25 00:58:58,090:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4015 (11.3902)
2022-11-25 00:58:58,091:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3865 (11.3895)
2022-11-25 00:58:58,145:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_597.pth.tar
2022-11-25 00:58:58,145:INFO: 
===> EPOCH: 598 (P4)
2022-11-25 00:58:58,145:INFO: - Computing loss (training)
2022-11-25 00:58:58,357:INFO: Dataset: hotel               Batch: 1/4	Loss 33.2350 (33.2350)
2022-11-25 00:58:58,360:INFO: Dataset: hotel               Batch: 2/4	Loss 31.1775 (32.2207)
2022-11-25 00:58:58,366:INFO: Dataset: hotel               Batch: 3/4	Loss 30.9492 (31.7955)
2022-11-25 00:58:58,370:INFO: Dataset: hotel               Batch: 4/4	Loss 31.6810 (31.7781)
2022-11-25 00:58:58,640:INFO: Dataset: univ                Batch:  1/15	Loss 13.7254 (13.7254)
2022-11-25 00:58:58,642:INFO: Dataset: univ                Batch:  2/15	Loss 13.7487 (13.7368)
2022-11-25 00:58:58,645:INFO: Dataset: univ                Batch:  3/15	Loss 13.9562 (13.8133)
2022-11-25 00:58:58,681:INFO: Dataset: univ                Batch:  4/15	Loss 13.7056 (13.7856)
2022-11-25 00:58:58,683:INFO: Dataset: univ                Batch:  5/15	Loss 13.6925 (13.7666)
2022-11-25 00:58:58,687:INFO: Dataset: univ                Batch:  6/15	Loss 13.6381 (13.7449)
2022-11-25 00:58:58,688:INFO: Dataset: univ                Batch:  7/15	Loss 13.3319 (13.6870)
2022-11-25 00:58:58,689:INFO: Dataset: univ                Batch:  8/15	Loss 13.2145 (13.6332)
2022-11-25 00:58:58,690:INFO: Dataset: univ                Batch:  9/15	Loss 13.0649 (13.5688)
2022-11-25 00:58:58,692:INFO: Dataset: univ                Batch: 10/15	Loss 12.5998 (13.4728)
2022-11-25 00:58:58,693:INFO: Dataset: univ                Batch: 11/15	Loss 12.4785 (13.3803)
2022-11-25 00:58:58,694:INFO: Dataset: univ                Batch: 12/15	Loss 12.1725 (13.2813)
2022-11-25 00:58:58,696:INFO: Dataset: univ                Batch: 13/15	Loss 11.9304 (13.1892)
2022-11-25 00:58:58,697:INFO: Dataset: univ                Batch: 14/15	Loss 11.4707 (13.0700)
2022-11-25 00:58:58,698:INFO: Dataset: univ                Batch: 15/15	Loss 11.0827 (13.0428)
2022-11-25 00:58:58,947:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4911 (33.4911)
2022-11-25 00:58:58,948:INFO: Dataset: zara1               Batch: 2/8	Loss 33.4620 (33.4770)
2022-11-25 00:58:58,949:INFO: Dataset: zara1               Batch: 3/8	Loss 33.2751 (33.4068)
2022-11-25 00:58:58,950:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8768 (33.2620)
2022-11-25 00:58:58,951:INFO: Dataset: zara1               Batch: 5/8	Loss 32.6901 (33.1438)
2022-11-25 00:58:58,980:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8533 (32.9001)
2022-11-25 00:58:58,981:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4098 (32.6930)
2022-11-25 00:58:58,982:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6151 (32.4644)
2022-11-25 00:58:59,244:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8615 (13.8615)
2022-11-25 00:58:59,249:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1624 (14.0110)
2022-11-25 00:58:59,250:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8750 (13.9622)
2022-11-25 00:58:59,304:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2009 (14.0266)
2022-11-25 00:58:59,305:INFO: Dataset: zara2               Batch:  5/18	Loss 13.9262 (14.0055)
2022-11-25 00:58:59,309:INFO: Dataset: zara2               Batch:  6/18	Loss 14.4007 (14.0706)
2022-11-25 00:58:59,310:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8368 (14.0372)
2022-11-25 00:58:59,311:INFO: Dataset: zara2               Batch:  8/18	Loss 13.8009 (14.0079)
2022-11-25 00:58:59,312:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6191 (13.9635)
2022-11-25 00:58:59,313:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3024 (13.8895)
2022-11-25 00:58:59,316:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0804 (13.8126)
2022-11-25 00:58:59,317:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8745 (13.7327)
2022-11-25 00:58:59,318:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5568 (13.6405)
2022-11-25 00:58:59,319:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5610 (13.5632)
2022-11-25 00:58:59,320:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3379 (13.4734)
2022-11-25 00:58:59,321:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8541 (13.3670)
2022-11-25 00:58:59,323:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8138 (13.2586)
2022-11-25 00:58:59,324:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6211 (13.1760)
2022-11-25 00:58:59,375:INFO: - Computing ADE (validation o)
2022-11-25 00:58:59,654:INFO: 		 ADE on eth                       dataset:	 1.036212682723999
2022-11-25 00:58:59,654:INFO: Average validation o:	ADE  1.0362	FDE  2.1280
2022-11-25 00:58:59,655:INFO: - Computing loss (validation)
2022-11-25 00:58:59,853:INFO: Dataset: hotel               Batch: 1/2	Loss 35.9089 (35.9089)
2022-11-25 00:58:59,853:INFO: Dataset: hotel               Batch: 2/2	Loss 34.2903 (35.7597)
2022-11-25 00:59:00,102:INFO: Dataset: univ                Batch: 1/3	Loss 13.3610 (13.3610)
2022-11-25 00:59:00,104:INFO: Dataset: univ                Batch: 2/3	Loss 13.0093 (13.1631)
2022-11-25 00:59:00,105:INFO: Dataset: univ                Batch: 3/3	Loss 12.8171 (13.0383)
2022-11-25 00:59:00,351:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5326 (30.5326)
2022-11-25 00:59:00,353:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2933 (30.4703)
2022-11-25 00:59:00,597:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4454 (11.4454)
2022-11-25 00:59:00,598:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3452 (11.3998)
2022-11-25 00:59:00,599:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5087 (11.4346)
2022-11-25 00:59:00,601:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3552 (11.4162)
2022-11-25 00:59:00,602:INFO: Dataset: zara2               Batch: 5/5	Loss 11.1877 (11.3711)
2022-11-25 00:59:00,658:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_598.pth.tar
2022-11-25 00:59:00,659:INFO: 
===> EPOCH: 599 (P4)
2022-11-25 00:59:00,659:INFO: - Computing loss (training)
2022-11-25 00:59:00,864:INFO: Dataset: hotel               Batch: 1/4	Loss 31.8679 (31.8679)
2022-11-25 00:59:00,868:INFO: Dataset: hotel               Batch: 2/4	Loss 32.5513 (32.1990)
2022-11-25 00:59:00,869:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0219 (31.8048)
2022-11-25 00:59:00,871:INFO: Dataset: hotel               Batch: 4/4	Loss 31.2439 (31.7100)
2022-11-25 00:59:01,127:INFO: Dataset: univ                Batch:  1/15	Loss 13.6363 (13.6363)
2022-11-25 00:59:01,142:INFO: Dataset: univ                Batch:  2/15	Loss 13.7320 (13.6816)
2022-11-25 00:59:01,143:INFO: Dataset: univ                Batch:  3/15	Loss 13.6687 (13.6771)
2022-11-25 00:59:01,146:INFO: Dataset: univ                Batch:  4/15	Loss 13.9075 (13.7352)
2022-11-25 00:59:01,195:INFO: Dataset: univ                Batch:  5/15	Loss 13.8311 (13.7541)
2022-11-25 00:59:01,209:INFO: Dataset: univ                Batch:  6/15	Loss 13.5065 (13.7137)
2022-11-25 00:59:01,210:INFO: Dataset: univ                Batch:  7/15	Loss 13.4592 (13.6772)
2022-11-25 00:59:01,211:INFO: Dataset: univ                Batch:  8/15	Loss 13.1842 (13.6134)
2022-11-25 00:59:01,212:INFO: Dataset: univ                Batch:  9/15	Loss 12.9399 (13.5341)
2022-11-25 00:59:01,213:INFO: Dataset: univ                Batch: 10/15	Loss 12.7881 (13.4603)
2022-11-25 00:59:01,215:INFO: Dataset: univ                Batch: 11/15	Loss 12.5422 (13.3696)
2022-11-25 00:59:01,216:INFO: Dataset: univ                Batch: 12/15	Loss 12.0431 (13.2534)
2022-11-25 00:59:01,217:INFO: Dataset: univ                Batch: 13/15	Loss 11.8718 (13.1437)
2022-11-25 00:59:01,218:INFO: Dataset: univ                Batch: 14/15	Loss 11.5082 (13.0265)
2022-11-25 00:59:01,219:INFO: Dataset: univ                Batch: 15/15	Loss 11.1133 (12.9968)
2022-11-25 00:59:01,495:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4357 (33.4357)
2022-11-25 00:59:01,507:INFO: Dataset: zara1               Batch: 2/8	Loss 33.5894 (33.5144)
2022-11-25 00:59:01,508:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0905 (33.3666)
2022-11-25 00:59:01,509:INFO: Dataset: zara1               Batch: 4/8	Loss 32.9029 (33.2449)
2022-11-25 00:59:01,510:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5604 (33.1000)
2022-11-25 00:59:01,568:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0464 (32.9302)
2022-11-25 00:59:01,571:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3416 (32.7064)
2022-11-25 00:59:01,575:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8247 (32.5123)
2022-11-25 00:59:01,875:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5361 (13.5361)
2022-11-25 00:59:01,878:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0096 (13.7671)
2022-11-25 00:59:01,882:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0238 (13.8496)
2022-11-25 00:59:01,885:INFO: Dataset: zara2               Batch:  4/18	Loss 14.1560 (13.9247)
2022-11-25 00:59:01,947:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0888 (13.9571)
2022-11-25 00:59:01,954:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0740 (13.9760)
2022-11-25 00:59:01,955:INFO: Dataset: zara2               Batch:  7/18	Loss 14.1000 (13.9931)
2022-11-25 00:59:01,956:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7857 (13.9638)
2022-11-25 00:59:01,957:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6364 (13.9305)
2022-11-25 00:59:01,958:INFO: Dataset: zara2               Batch: 10/18	Loss 13.6030 (13.8959)
2022-11-25 00:59:01,960:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1913 (13.8344)
2022-11-25 00:59:01,961:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8174 (13.7596)
2022-11-25 00:59:01,962:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5237 (13.6676)
2022-11-25 00:59:01,963:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6482 (13.5947)
2022-11-25 00:59:01,964:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2691 (13.5010)
2022-11-25 00:59:01,965:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0790 (13.3997)
2022-11-25 00:59:01,966:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9351 (13.3211)
2022-11-25 00:59:01,968:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5692 (13.2415)
2022-11-25 00:59:02,014:INFO: - Computing ADE (validation o)
2022-11-25 00:59:02,289:INFO: 		 ADE on eth                       dataset:	 1.0656983852386475
2022-11-25 00:59:02,289:INFO: Average validation o:	ADE  1.0657	FDE  2.1841
2022-11-25 00:59:02,290:INFO: - Computing loss (validation)
2022-11-25 00:59:02,481:INFO: Dataset: hotel               Batch: 1/2	Loss 35.5349 (35.5349)
2022-11-25 00:59:02,482:INFO: Dataset: hotel               Batch: 2/2	Loss 38.3808 (35.6903)
2022-11-25 00:59:02,737:INFO: Dataset: univ                Batch: 1/3	Loss 12.8740 (12.8740)
2022-11-25 00:59:02,738:INFO: Dataset: univ                Batch: 2/3	Loss 13.1294 (12.9967)
2022-11-25 00:59:02,739:INFO: Dataset: univ                Batch: 3/3	Loss 13.1097 (13.0314)
2022-11-25 00:59:02,995:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4876 (30.4876)
2022-11-25 00:59:02,996:INFO: Dataset: zara1               Batch: 2/2	Loss 30.5632 (30.5085)
2022-11-25 00:59:03,253:INFO: Dataset: zara2               Batch: 1/5	Loss 11.2571 (11.2571)
2022-11-25 00:59:03,254:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4813 (11.3689)
2022-11-25 00:59:03,257:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3454 (11.3610)
2022-11-25 00:59:03,258:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2799 (11.3394)
2022-11-25 00:59:03,269:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4707 (11.3647)
2022-11-25 00:59:03,326:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_599.pth.tar
2022-11-25 00:59:03,326:INFO: 
===> EPOCH: 600 (P4)
2022-11-25 00:59:03,326:INFO: - Computing loss (training)
2022-11-25 00:59:03,539:INFO: Dataset: hotel               Batch: 1/4	Loss 31.3594 (31.3594)
2022-11-25 00:59:03,542:INFO: Dataset: hotel               Batch: 2/4	Loss 31.6034 (31.4769)
2022-11-25 00:59:03,544:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0704 (31.3355)
2022-11-25 00:59:03,550:INFO: Dataset: hotel               Batch: 4/4	Loss 32.6096 (31.5608)
2022-11-25 00:59:03,802:INFO: Dataset: univ                Batch:  1/15	Loss 13.6944 (13.6944)
2022-11-25 00:59:03,805:INFO: Dataset: univ                Batch:  2/15	Loss 13.8935 (13.7951)
2022-11-25 00:59:03,806:INFO: Dataset: univ                Batch:  3/15	Loss 13.9114 (13.8312)
2022-11-25 00:59:03,810:INFO: Dataset: univ                Batch:  4/15	Loss 13.9243 (13.8542)
2022-11-25 00:59:03,846:INFO: Dataset: univ                Batch:  5/15	Loss 13.5856 (13.7986)
2022-11-25 00:59:03,853:INFO: Dataset: univ                Batch:  6/15	Loss 13.3609 (13.7231)
2022-11-25 00:59:03,855:INFO: Dataset: univ                Batch:  7/15	Loss 13.4985 (13.6905)
2022-11-25 00:59:03,856:INFO: Dataset: univ                Batch:  8/15	Loss 13.1576 (13.6198)
2022-11-25 00:59:03,857:INFO: Dataset: univ                Batch:  9/15	Loss 13.0927 (13.5664)
2022-11-25 00:59:03,858:INFO: Dataset: univ                Batch: 10/15	Loss 12.6011 (13.4669)
2022-11-25 00:59:03,860:INFO: Dataset: univ                Batch: 11/15	Loss 12.4243 (13.3701)
2022-11-25 00:59:03,861:INFO: Dataset: univ                Batch: 12/15	Loss 12.1260 (13.2791)
2022-11-25 00:59:03,862:INFO: Dataset: univ                Batch: 13/15	Loss 11.7474 (13.1541)
2022-11-25 00:59:03,864:INFO: Dataset: univ                Batch: 14/15	Loss 11.3776 (13.0243)
2022-11-25 00:59:03,865:INFO: Dataset: univ                Batch: 15/15	Loss 11.3868 (13.0044)
2022-11-25 00:59:04,124:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4653 (33.4653)
2022-11-25 00:59:04,136:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2656 (33.3594)
2022-11-25 00:59:04,137:INFO: Dataset: zara1               Batch: 3/8	Loss 33.3320 (33.3504)
2022-11-25 00:59:04,138:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8953 (33.2282)
2022-11-25 00:59:04,140:INFO: Dataset: zara1               Batch: 5/8	Loss 32.4748 (33.0769)
2022-11-25 00:59:04,188:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9940 (32.9038)
2022-11-25 00:59:04,191:INFO: Dataset: zara1               Batch: 7/8	Loss 31.7148 (32.7274)
2022-11-25 00:59:04,195:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6954 (32.5339)
2022-11-25 00:59:04,486:INFO: Dataset: zara2               Batch:  1/18	Loss 14.2373 (14.2373)
2022-11-25 00:59:04,489:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9314 (14.0860)
2022-11-25 00:59:04,492:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1824 (14.1197)
2022-11-25 00:59:04,493:INFO: Dataset: zara2               Batch:  4/18	Loss 14.1325 (14.1231)
2022-11-25 00:59:04,547:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0459 (14.1070)
2022-11-25 00:59:04,557:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9136 (14.0744)
2022-11-25 00:59:04,558:INFO: Dataset: zara2               Batch:  7/18	Loss 14.1406 (14.0834)
2022-11-25 00:59:04,559:INFO: Dataset: zara2               Batch:  8/18	Loss 13.9229 (14.0632)
2022-11-25 00:59:04,560:INFO: Dataset: zara2               Batch:  9/18	Loss 13.9648 (14.0521)
2022-11-25 00:59:04,561:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3987 (13.9873)
2022-11-25 00:59:04,562:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3431 (13.9291)
2022-11-25 00:59:04,564:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8007 (13.8324)
2022-11-25 00:59:04,565:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7577 (13.7469)
2022-11-25 00:59:04,566:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4764 (13.6686)
2022-11-25 00:59:04,567:INFO: Dataset: zara2               Batch: 15/18	Loss 11.9439 (13.5507)
2022-11-25 00:59:04,568:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0970 (13.4532)
2022-11-25 00:59:04,569:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6286 (13.3369)
2022-11-25 00:59:04,571:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5582 (13.2475)
2022-11-25 00:59:04,617:INFO: - Computing ADE (validation o)
2022-11-25 00:59:04,876:INFO: 		 ADE on eth                       dataset:	 1.0146054029464722
2022-11-25 00:59:04,876:INFO: Average validation o:	ADE  1.0146	FDE  2.0753
2022-11-25 00:59:04,877:INFO: - Computing loss (validation)
2022-11-25 00:59:05,073:INFO: Dataset: hotel               Batch: 1/2	Loss 35.5710 (35.5710)
2022-11-25 00:59:05,073:INFO: Dataset: hotel               Batch: 2/2	Loss 36.3682 (35.6227)
2022-11-25 00:59:05,332:INFO: Dataset: univ                Batch: 1/3	Loss 12.9266 (12.9266)
2022-11-25 00:59:05,333:INFO: Dataset: univ                Batch: 2/3	Loss 12.9233 (12.9250)
2022-11-25 00:59:05,336:INFO: Dataset: univ                Batch: 3/3	Loss 13.1397 (12.9929)
2022-11-25 00:59:05,602:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5491 (30.5491)
2022-11-25 00:59:05,603:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4371 (30.5203)
2022-11-25 00:59:05,870:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4340 (11.4340)
2022-11-25 00:59:05,872:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4031 (11.4180)
2022-11-25 00:59:05,874:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2648 (11.3667)
2022-11-25 00:59:05,874:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4877 (11.3982)
2022-11-25 00:59:05,881:INFO: Dataset: zara2               Batch: 5/5	Loss 11.2760 (11.3753)
2022-11-25 00:59:05,941:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_600.pth.tar
2022-11-25 00:59:05,941:INFO: 
===> EPOCH: 601 (P4)
2022-11-25 00:59:05,941:INFO: - Computing loss (training)
2022-11-25 00:59:06,144:INFO: Dataset: hotel               Batch: 1/4	Loss 31.7328 (31.7328)
2022-11-25 00:59:06,145:INFO: Dataset: hotel               Batch: 2/4	Loss 31.9491 (31.8349)
2022-11-25 00:59:06,147:INFO: Dataset: hotel               Batch: 3/4	Loss 31.3550 (31.6779)
2022-11-25 00:59:06,149:INFO: Dataset: hotel               Batch: 4/4	Loss 30.2880 (31.4487)
2022-11-25 00:59:06,415:INFO: Dataset: univ                Batch:  1/15	Loss 13.7133 (13.7133)
2022-11-25 00:59:06,418:INFO: Dataset: univ                Batch:  2/15	Loss 13.9410 (13.8234)
2022-11-25 00:59:06,421:INFO: Dataset: univ                Batch:  3/15	Loss 13.7632 (13.8030)
2022-11-25 00:59:06,466:INFO: Dataset: univ                Batch:  4/15	Loss 13.6868 (13.7766)
2022-11-25 00:59:06,467:INFO: Dataset: univ                Batch:  5/15	Loss 13.7800 (13.7773)
2022-11-25 00:59:06,470:INFO: Dataset: univ                Batch:  6/15	Loss 13.4312 (13.7209)
2022-11-25 00:59:06,471:INFO: Dataset: univ                Batch:  7/15	Loss 13.2170 (13.6495)
2022-11-25 00:59:06,472:INFO: Dataset: univ                Batch:  8/15	Loss 13.1306 (13.5891)
2022-11-25 00:59:06,473:INFO: Dataset: univ                Batch:  9/15	Loss 12.9727 (13.5219)
2022-11-25 00:59:06,474:INFO: Dataset: univ                Batch: 10/15	Loss 12.7825 (13.4481)
2022-11-25 00:59:06,475:INFO: Dataset: univ                Batch: 11/15	Loss 12.3843 (13.3465)
2022-11-25 00:59:06,477:INFO: Dataset: univ                Batch: 12/15	Loss 11.9659 (13.2252)
2022-11-25 00:59:06,478:INFO: Dataset: univ                Batch: 13/15	Loss 11.9194 (13.1289)
2022-11-25 00:59:06,479:INFO: Dataset: univ                Batch: 14/15	Loss 11.3626 (13.0099)
2022-11-25 00:59:06,480:INFO: Dataset: univ                Batch: 15/15	Loss 11.0969 (12.9831)
2022-11-25 00:59:06,742:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4505 (33.4505)
2022-11-25 00:59:06,744:INFO: Dataset: zara1               Batch: 2/8	Loss 33.3186 (33.3885)
2022-11-25 00:59:06,757:INFO: Dataset: zara1               Batch: 3/8	Loss 33.2297 (33.3346)
2022-11-25 00:59:06,758:INFO: Dataset: zara1               Batch: 4/8	Loss 33.1120 (33.2842)
2022-11-25 00:59:06,759:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5233 (33.1284)
2022-11-25 00:59:06,824:INFO: Dataset: zara1               Batch: 6/8	Loss 32.1139 (32.9646)
2022-11-25 00:59:06,827:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4386 (32.7327)
2022-11-25 00:59:06,831:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5722 (32.5246)
2022-11-25 00:59:07,180:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7422 (13.7422)
2022-11-25 00:59:07,181:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9635 (13.8553)
2022-11-25 00:59:07,182:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1122 (13.9425)
2022-11-25 00:59:07,183:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2798 (14.0282)
2022-11-25 00:59:07,184:INFO: Dataset: zara2               Batch:  5/18	Loss 14.1384 (14.0497)
2022-11-25 00:59:07,187:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9846 (14.0383)
2022-11-25 00:59:07,188:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0073 (14.0336)
2022-11-25 00:59:07,189:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5454 (13.9725)
2022-11-25 00:59:07,190:INFO: Dataset: zara2               Batch:  9/18	Loss 13.7999 (13.9517)
2022-11-25 00:59:07,191:INFO: Dataset: zara2               Batch: 10/18	Loss 13.8421 (13.9395)
2022-11-25 00:59:07,192:INFO: Dataset: zara2               Batch: 11/18	Loss 13.4599 (13.8956)
2022-11-25 00:59:07,194:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8495 (13.8023)
2022-11-25 00:59:07,195:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9019 (13.7292)
2022-11-25 00:59:07,197:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5091 (13.6342)
2022-11-25 00:59:07,198:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3168 (13.5531)
2022-11-25 00:59:07,200:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7632 (13.4393)
2022-11-25 00:59:07,202:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7432 (13.3367)
2022-11-25 00:59:07,203:INFO: Dataset: zara2               Batch: 18/18	Loss 11.2805 (13.2320)
2022-11-25 00:59:07,249:INFO: - Computing ADE (validation o)
2022-11-25 00:59:07,518:INFO: 		 ADE on eth                       dataset:	 1.0691604614257812
2022-11-25 00:59:07,519:INFO: Average validation o:	ADE  1.0692	FDE  2.1586
2022-11-25 00:59:07,519:INFO: - Computing loss (validation)
2022-11-25 00:59:07,711:INFO: Dataset: hotel               Batch: 1/2	Loss 35.5879 (35.5879)
2022-11-25 00:59:07,712:INFO: Dataset: hotel               Batch: 2/2	Loss 35.7746 (35.6020)
2022-11-25 00:59:07,974:INFO: Dataset: univ                Batch: 1/3	Loss 13.0631 (13.0631)
2022-11-25 00:59:07,985:INFO: Dataset: univ                Batch: 2/3	Loss 13.1003 (13.0822)
2022-11-25 00:59:07,985:INFO: Dataset: univ                Batch: 3/3	Loss 12.7051 (12.9527)
2022-11-25 00:59:08,233:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4548 (30.4548)
2022-11-25 00:59:08,233:INFO: Dataset: zara1               Batch: 2/2	Loss 30.7947 (30.5335)
2022-11-25 00:59:08,505:INFO: Dataset: zara2               Batch: 1/5	Loss 11.1594 (11.1594)
2022-11-25 00:59:08,507:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3935 (11.2740)
2022-11-25 00:59:08,517:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5347 (11.3596)
2022-11-25 00:59:08,517:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3858 (11.3660)
2022-11-25 00:59:08,518:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3551 (11.3638)
2022-11-25 00:59:08,573:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_601.pth.tar
2022-11-25 00:59:08,573:INFO: 
===> EPOCH: 602 (P4)
2022-11-25 00:59:08,573:INFO: - Computing loss (training)
2022-11-25 00:59:08,777:INFO: Dataset: hotel               Batch: 1/4	Loss 31.6816 (31.6816)
2022-11-25 00:59:08,779:INFO: Dataset: hotel               Batch: 2/4	Loss 31.6410 (31.6622)
2022-11-25 00:59:08,783:INFO: Dataset: hotel               Batch: 3/4	Loss 31.5617 (31.6280)
2022-11-25 00:59:08,785:INFO: Dataset: hotel               Batch: 4/4	Loss 30.1049 (31.3568)
2022-11-25 00:59:09,039:INFO: Dataset: univ                Batch:  1/15	Loss 13.5798 (13.5798)
2022-11-25 00:59:09,041:INFO: Dataset: univ                Batch:  2/15	Loss 13.7943 (13.6900)
2022-11-25 00:59:09,044:INFO: Dataset: univ                Batch:  3/15	Loss 13.8641 (13.7451)
2022-11-25 00:59:09,048:INFO: Dataset: univ                Batch:  4/15	Loss 13.7499 (13.7463)
2022-11-25 00:59:09,064:INFO: Dataset: univ                Batch:  5/15	Loss 13.7009 (13.7374)
2022-11-25 00:59:09,073:INFO: Dataset: univ                Batch:  6/15	Loss 13.4726 (13.6921)
2022-11-25 00:59:09,074:INFO: Dataset: univ                Batch:  7/15	Loss 13.3800 (13.6491)
2022-11-25 00:59:09,075:INFO: Dataset: univ                Batch:  8/15	Loss 13.1654 (13.5868)
2022-11-25 00:59:09,076:INFO: Dataset: univ                Batch:  9/15	Loss 13.0121 (13.5309)
2022-11-25 00:59:09,077:INFO: Dataset: univ                Batch: 10/15	Loss 12.5263 (13.4301)
2022-11-25 00:59:09,078:INFO: Dataset: univ                Batch: 11/15	Loss 12.3759 (13.3385)
2022-11-25 00:59:09,080:INFO: Dataset: univ                Batch: 12/15	Loss 12.0332 (13.2491)
2022-11-25 00:59:09,081:INFO: Dataset: univ                Batch: 13/15	Loss 11.7939 (13.1468)
2022-11-25 00:59:09,082:INFO: Dataset: univ                Batch: 14/15	Loss 11.2931 (13.0112)
2022-11-25 00:59:09,082:INFO: Dataset: univ                Batch: 15/15	Loss 11.0339 (12.9861)
2022-11-25 00:59:09,359:INFO: Dataset: zara1               Batch: 1/8	Loss 33.3660 (33.3660)
2022-11-25 00:59:09,360:INFO: Dataset: zara1               Batch: 2/8	Loss 33.3377 (33.3526)
2022-11-25 00:59:09,361:INFO: Dataset: zara1               Batch: 3/8	Loss 33.8805 (33.5261)
2022-11-25 00:59:09,364:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8931 (33.3695)
2022-11-25 00:59:09,365:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3835 (33.1654)
2022-11-25 00:59:09,397:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8623 (32.9262)
2022-11-25 00:59:09,400:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3170 (32.6658)
2022-11-25 00:59:09,403:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7050 (32.4965)
2022-11-25 00:59:09,681:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6733 (13.6733)
2022-11-25 00:59:09,688:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0445 (13.8705)
2022-11-25 00:59:09,691:INFO: Dataset: zara2               Batch:  3/18	Loss 14.2215 (13.9923)
2022-11-25 00:59:09,743:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9504 (13.9820)
2022-11-25 00:59:09,744:INFO: Dataset: zara2               Batch:  5/18	Loss 14.1927 (14.0210)
2022-11-25 00:59:09,747:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0946 (14.0331)
2022-11-25 00:59:09,748:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7132 (13.9899)
2022-11-25 00:59:09,749:INFO: Dataset: zara2               Batch:  8/18	Loss 13.9601 (13.9861)
2022-11-25 00:59:09,750:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6794 (13.9482)
2022-11-25 00:59:09,751:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5766 (13.9094)
2022-11-25 00:59:09,752:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3337 (13.8566)
2022-11-25 00:59:09,754:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0559 (13.7917)
2022-11-25 00:59:09,755:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9167 (13.7269)
2022-11-25 00:59:09,756:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5657 (13.6426)
2022-11-25 00:59:09,757:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2040 (13.5419)
2022-11-25 00:59:09,758:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8022 (13.4334)
2022-11-25 00:59:09,760:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6165 (13.3215)
2022-11-25 00:59:09,762:INFO: Dataset: zara2               Batch: 18/18	Loss 11.1704 (13.2191)
2022-11-25 00:59:09,814:INFO: - Computing ADE (validation o)
2022-11-25 00:59:10,090:INFO: 		 ADE on eth                       dataset:	 1.0485793352127075
2022-11-25 00:59:10,090:INFO: Average validation o:	ADE  1.0486	FDE  2.1427
2022-11-25 00:59:10,091:INFO: - Computing loss (validation)
2022-11-25 00:59:10,280:INFO: Dataset: hotel               Batch: 1/2	Loss 35.5878 (35.5878)
2022-11-25 00:59:10,281:INFO: Dataset: hotel               Batch: 2/2	Loss 35.8929 (35.6076)
2022-11-25 00:59:10,541:INFO: Dataset: univ                Batch: 1/3	Loss 12.7724 (12.7724)
2022-11-25 00:59:10,543:INFO: Dataset: univ                Batch: 2/3	Loss 13.0182 (12.9072)
2022-11-25 00:59:10,546:INFO: Dataset: univ                Batch: 3/3	Loss 13.2825 (13.0086)
2022-11-25 00:59:10,790:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5496 (30.5496)
2022-11-25 00:59:10,793:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4877 (30.5335)
2022-11-25 00:59:11,042:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3839 (11.3839)
2022-11-25 00:59:11,056:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3467 (11.3657)
2022-11-25 00:59:11,057:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4302 (11.3872)
2022-11-25 00:59:11,057:INFO: Dataset: zara2               Batch: 4/5	Loss 11.1982 (11.3401)
2022-11-25 00:59:11,058:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3536 (11.3427)
2022-11-25 00:59:11,114:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_602.pth.tar
2022-11-25 00:59:11,114:INFO: 
===> EPOCH: 603 (P4)
2022-11-25 00:59:11,114:INFO: - Computing loss (training)
2022-11-25 00:59:11,317:INFO: Dataset: hotel               Batch: 1/4	Loss 33.1170 (33.1170)
2022-11-25 00:59:11,325:INFO: Dataset: hotel               Batch: 2/4	Loss 32.4812 (32.7887)
2022-11-25 00:59:11,326:INFO: Dataset: hotel               Batch: 3/4	Loss 29.5226 (31.7240)
2022-11-25 00:59:11,328:INFO: Dataset: hotel               Batch: 4/4	Loss 29.3073 (31.3318)
2022-11-25 00:59:11,609:INFO: Dataset: univ                Batch:  1/15	Loss 13.6536 (13.6536)
2022-11-25 00:59:11,613:INFO: Dataset: univ                Batch:  2/15	Loss 13.7979 (13.7240)
2022-11-25 00:59:11,615:INFO: Dataset: univ                Batch:  3/15	Loss 13.6283 (13.6890)
2022-11-25 00:59:11,618:INFO: Dataset: univ                Batch:  4/15	Loss 13.9210 (13.7436)
2022-11-25 00:59:11,687:INFO: Dataset: univ                Batch:  5/15	Loss 13.8225 (13.7584)
2022-11-25 00:59:11,693:INFO: Dataset: univ                Batch:  6/15	Loss 13.4803 (13.7146)
2022-11-25 00:59:11,694:INFO: Dataset: univ                Batch:  7/15	Loss 13.4014 (13.6699)
2022-11-25 00:59:11,695:INFO: Dataset: univ                Batch:  8/15	Loss 13.0535 (13.5956)
2022-11-25 00:59:11,696:INFO: Dataset: univ                Batch:  9/15	Loss 13.0356 (13.5376)
2022-11-25 00:59:11,697:INFO: Dataset: univ                Batch: 10/15	Loss 12.8528 (13.4730)
2022-11-25 00:59:11,700:INFO: Dataset: univ                Batch: 11/15	Loss 12.3270 (13.3704)
2022-11-25 00:59:11,701:INFO: Dataset: univ                Batch: 12/15	Loss 11.9946 (13.2581)
2022-11-25 00:59:11,702:INFO: Dataset: univ                Batch: 13/15	Loss 11.5986 (13.1274)
2022-11-25 00:59:11,703:INFO: Dataset: univ                Batch: 14/15	Loss 11.3380 (12.9849)
2022-11-25 00:59:11,704:INFO: Dataset: univ                Batch: 15/15	Loss 10.9773 (12.9547)
2022-11-25 00:59:11,972:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4313 (33.4313)
2022-11-25 00:59:11,974:INFO: Dataset: zara1               Batch: 2/8	Loss 33.5803 (33.5053)
2022-11-25 00:59:11,977:INFO: Dataset: zara1               Batch: 3/8	Loss 33.3707 (33.4633)
2022-11-25 00:59:11,979:INFO: Dataset: zara1               Batch: 4/8	Loss 33.1073 (33.3673)
2022-11-25 00:59:11,983:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5823 (33.2032)
2022-11-25 00:59:12,023:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0639 (33.0108)
2022-11-25 00:59:12,026:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3633 (32.7761)
2022-11-25 00:59:12,030:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4247 (32.4927)
2022-11-25 00:59:12,311:INFO: Dataset: zara2               Batch:  1/18	Loss 14.0294 (14.0294)
2022-11-25 00:59:12,313:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1709 (14.0997)
2022-11-25 00:59:12,316:INFO: Dataset: zara2               Batch:  3/18	Loss 14.2412 (14.1469)
2022-11-25 00:59:12,318:INFO: Dataset: zara2               Batch:  4/18	Loss 14.3817 (14.2040)
2022-11-25 00:59:12,340:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0902 (14.1796)
2022-11-25 00:59:12,345:INFO: Dataset: zara2               Batch:  6/18	Loss 14.5896 (14.2424)
2022-11-25 00:59:12,346:INFO: Dataset: zara2               Batch:  7/18	Loss 13.9671 (14.2048)
2022-11-25 00:59:12,347:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3781 (14.1072)
2022-11-25 00:59:12,348:INFO: Dataset: zara2               Batch:  9/18	Loss 13.8323 (14.0774)
2022-11-25 00:59:12,349:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5030 (14.0132)
2022-11-25 00:59:12,350:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9525 (13.9172)
2022-11-25 00:59:12,351:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6761 (13.8171)
2022-11-25 00:59:12,352:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7237 (13.7286)
2022-11-25 00:59:12,353:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5600 (13.6408)
2022-11-25 00:59:12,354:INFO: Dataset: zara2               Batch: 15/18	Loss 11.9200 (13.5295)
2022-11-25 00:59:12,355:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7614 (13.4174)
2022-11-25 00:59:12,356:INFO: Dataset: zara2               Batch: 17/18	Loss 12.1777 (13.3400)
2022-11-25 00:59:12,357:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4215 (13.2519)
2022-11-25 00:59:12,405:INFO: - Computing ADE (validation o)
2022-11-25 00:59:12,678:INFO: 		 ADE on eth                       dataset:	 1.0546278953552246
2022-11-25 00:59:12,678:INFO: Average validation o:	ADE  1.0546	FDE  2.1082
2022-11-25 00:59:12,678:INFO: - Computing loss (validation)
2022-11-25 00:59:12,887:INFO: Dataset: hotel               Batch: 1/2	Loss 35.2167 (35.2167)
2022-11-25 00:59:12,888:INFO: Dataset: hotel               Batch: 2/2	Loss 39.5543 (35.4832)
2022-11-25 00:59:13,146:INFO: Dataset: univ                Batch: 1/3	Loss 12.8393 (12.8393)
2022-11-25 00:59:13,148:INFO: Dataset: univ                Batch: 2/3	Loss 12.9081 (12.8731)
2022-11-25 00:59:13,152:INFO: Dataset: univ                Batch: 3/3	Loss 13.0438 (12.9225)
2022-11-25 00:59:13,394:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5199 (30.5199)
2022-11-25 00:59:13,395:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2640 (30.4540)
2022-11-25 00:59:13,663:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3669 (11.3669)
2022-11-25 00:59:13,673:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3414 (11.3547)
2022-11-25 00:59:13,673:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4091 (11.3723)
2022-11-25 00:59:13,675:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4223 (11.3848)
2022-11-25 00:59:13,675:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3177 (11.3705)
2022-11-25 00:59:13,728:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_603.pth.tar
2022-11-25 00:59:13,728:INFO: 
===> EPOCH: 604 (P4)
2022-11-25 00:59:13,728:INFO: - Computing loss (training)
2022-11-25 00:59:13,938:INFO: Dataset: hotel               Batch: 1/4	Loss 31.6036 (31.6036)
2022-11-25 00:59:13,939:INFO: Dataset: hotel               Batch: 2/4	Loss 31.4161 (31.5099)
2022-11-25 00:59:13,942:INFO: Dataset: hotel               Batch: 3/4	Loss 29.2518 (30.7452)
2022-11-25 00:59:13,944:INFO: Dataset: hotel               Batch: 4/4	Loss 32.9362 (31.1267)
2022-11-25 00:59:14,219:INFO: Dataset: univ                Batch:  1/15	Loss 13.6603 (13.6603)
2022-11-25 00:59:14,221:INFO: Dataset: univ                Batch:  2/15	Loss 13.7504 (13.7052)
2022-11-25 00:59:14,224:INFO: Dataset: univ                Batch:  3/15	Loss 13.6692 (13.6936)
2022-11-25 00:59:14,225:INFO: Dataset: univ                Batch:  4/15	Loss 13.6126 (13.6733)
2022-11-25 00:59:14,271:INFO: Dataset: univ                Batch:  5/15	Loss 13.6898 (13.6767)
2022-11-25 00:59:14,281:INFO: Dataset: univ                Batch:  6/15	Loss 13.4605 (13.6406)
2022-11-25 00:59:14,282:INFO: Dataset: univ                Batch:  7/15	Loss 13.3720 (13.6050)
2022-11-25 00:59:14,283:INFO: Dataset: univ                Batch:  8/15	Loss 13.1455 (13.5535)
2022-11-25 00:59:14,284:INFO: Dataset: univ                Batch:  9/15	Loss 12.9217 (13.4803)
2022-11-25 00:59:14,286:INFO: Dataset: univ                Batch: 10/15	Loss 12.6504 (13.3984)
2022-11-25 00:59:14,287:INFO: Dataset: univ                Batch: 11/15	Loss 12.2693 (13.3056)
2022-11-25 00:59:14,288:INFO: Dataset: univ                Batch: 12/15	Loss 12.1193 (13.2100)
2022-11-25 00:59:14,290:INFO: Dataset: univ                Batch: 13/15	Loss 11.6917 (13.0958)
2022-11-25 00:59:14,291:INFO: Dataset: univ                Batch: 14/15	Loss 11.3514 (12.9592)
2022-11-25 00:59:14,292:INFO: Dataset: univ                Batch: 15/15	Loss 11.1679 (12.9391)
2022-11-25 00:59:14,560:INFO: Dataset: zara1               Batch: 1/8	Loss 33.2551 (33.2551)
2022-11-25 00:59:14,561:INFO: Dataset: zara1               Batch: 2/8	Loss 33.5963 (33.4216)
2022-11-25 00:59:14,562:INFO: Dataset: zara1               Batch: 3/8	Loss 33.3235 (33.3893)
2022-11-25 00:59:14,563:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8214 (33.2413)
2022-11-25 00:59:14,566:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5284 (33.1227)
2022-11-25 00:59:14,608:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8636 (32.9208)
2022-11-25 00:59:14,611:INFO: Dataset: zara1               Batch: 7/8	Loss 31.4006 (32.6969)
2022-11-25 00:59:14,615:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6967 (32.4905)
2022-11-25 00:59:14,878:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7955 (13.7955)
2022-11-25 00:59:14,880:INFO: Dataset: zara2               Batch:  2/18	Loss 14.4539 (14.1445)
2022-11-25 00:59:14,886:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0088 (14.0994)
2022-11-25 00:59:14,888:INFO: Dataset: zara2               Batch:  4/18	Loss 14.6281 (14.2372)
2022-11-25 00:59:14,932:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2500 (14.2400)
2022-11-25 00:59:14,934:INFO: Dataset: zara2               Batch:  6/18	Loss 14.2530 (14.2422)
2022-11-25 00:59:14,935:INFO: Dataset: zara2               Batch:  7/18	Loss 14.1386 (14.2269)
2022-11-25 00:59:14,937:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7404 (14.1658)
2022-11-25 00:59:14,938:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4454 (14.0947)
2022-11-25 00:59:14,939:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2844 (14.0030)
2022-11-25 00:59:14,940:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1753 (13.9340)
2022-11-25 00:59:14,942:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7830 (13.8444)
2022-11-25 00:59:14,943:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6313 (13.7512)
2022-11-25 00:59:14,944:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5010 (13.6565)
2022-11-25 00:59:14,945:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3545 (13.5770)
2022-11-25 00:59:14,946:INFO: Dataset: zara2               Batch: 16/18	Loss 11.6073 (13.4709)
2022-11-25 00:59:14,948:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7645 (13.3721)
2022-11-25 00:59:14,949:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3194 (13.2781)
2022-11-25 00:59:15,009:INFO: - Computing ADE (validation o)
2022-11-25 00:59:15,278:INFO: 		 ADE on eth                       dataset:	 1.0449432134628296
2022-11-25 00:59:15,278:INFO: Average validation o:	ADE  1.0449	FDE  2.1537
2022-11-25 00:59:15,279:INFO: - Computing loss (validation)
2022-11-25 00:59:15,476:INFO: Dataset: hotel               Batch: 1/2	Loss 35.4647 (35.4647)
2022-11-25 00:59:15,476:INFO: Dataset: hotel               Batch: 2/2	Loss 36.2549 (35.5295)
2022-11-25 00:59:15,723:INFO: Dataset: univ                Batch: 1/3	Loss 12.9051 (12.9051)
2022-11-25 00:59:15,725:INFO: Dataset: univ                Batch: 2/3	Loss 12.9966 (12.9491)
2022-11-25 00:59:15,727:INFO: Dataset: univ                Batch: 3/3	Loss 12.9798 (12.9589)
2022-11-25 00:59:15,976:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5111 (30.5111)
2022-11-25 00:59:15,978:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4646 (30.4986)
2022-11-25 00:59:16,265:INFO: Dataset: zara2               Batch: 1/5	Loss 11.2164 (11.2164)
2022-11-25 00:59:16,265:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4145 (11.3228)
2022-11-25 00:59:16,266:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3500 (11.3320)
2022-11-25 00:59:16,266:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3954 (11.3478)
2022-11-25 00:59:16,267:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3289 (11.3441)
2022-11-25 00:59:16,321:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_604.pth.tar
2022-11-25 00:59:16,322:INFO: 
===> EPOCH: 605 (P4)
2022-11-25 00:59:16,322:INFO: - Computing loss (training)
2022-11-25 00:59:16,534:INFO: Dataset: hotel               Batch: 1/4	Loss 32.1386 (32.1386)
2022-11-25 00:59:16,538:INFO: Dataset: hotel               Batch: 2/4	Loss 31.9593 (32.0466)
2022-11-25 00:59:16,540:INFO: Dataset: hotel               Batch: 3/4	Loss 30.3943 (31.5285)
2022-11-25 00:59:16,546:INFO: Dataset: hotel               Batch: 4/4	Loss 29.4143 (31.1575)
2022-11-25 00:59:16,806:INFO: Dataset: univ                Batch:  1/15	Loss 13.4018 (13.4018)
2022-11-25 00:59:16,807:INFO: Dataset: univ                Batch:  2/15	Loss 14.0250 (13.6963)
2022-11-25 00:59:16,810:INFO: Dataset: univ                Batch:  3/15	Loss 13.8055 (13.7343)
2022-11-25 00:59:16,842:INFO: Dataset: univ                Batch:  4/15	Loss 13.7972 (13.7496)
2022-11-25 00:59:16,844:INFO: Dataset: univ                Batch:  5/15	Loss 13.8852 (13.7774)
2022-11-25 00:59:16,847:INFO: Dataset: univ                Batch:  6/15	Loss 13.5784 (13.7440)
2022-11-25 00:59:16,849:INFO: Dataset: univ                Batch:  7/15	Loss 13.3000 (13.6752)
2022-11-25 00:59:16,850:INFO: Dataset: univ                Batch:  8/15	Loss 13.1090 (13.6005)
2022-11-25 00:59:16,851:INFO: Dataset: univ                Batch:  9/15	Loss 12.8617 (13.5194)
2022-11-25 00:59:16,852:INFO: Dataset: univ                Batch: 10/15	Loss 12.5687 (13.4162)
2022-11-25 00:59:16,854:INFO: Dataset: univ                Batch: 11/15	Loss 12.2504 (13.2954)
2022-11-25 00:59:16,855:INFO: Dataset: univ                Batch: 12/15	Loss 12.0653 (13.2051)
2022-11-25 00:59:16,856:INFO: Dataset: univ                Batch: 13/15	Loss 11.6274 (13.0843)
2022-11-25 00:59:16,857:INFO: Dataset: univ                Batch: 14/15	Loss 11.2857 (12.9445)
2022-11-25 00:59:16,858:INFO: Dataset: univ                Batch: 15/15	Loss 11.2555 (12.9274)
2022-11-25 00:59:17,124:INFO: Dataset: zara1               Batch: 1/8	Loss 33.3637 (33.3637)
2022-11-25 00:59:17,125:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2425 (33.3078)
2022-11-25 00:59:17,127:INFO: Dataset: zara1               Batch: 3/8	Loss 33.1834 (33.2702)
2022-11-25 00:59:17,128:INFO: Dataset: zara1               Batch: 4/8	Loss 33.1426 (33.2422)
2022-11-25 00:59:17,129:INFO: Dataset: zara1               Batch: 5/8	Loss 32.7808 (33.1467)
2022-11-25 00:59:17,171:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8174 (32.9268)
2022-11-25 00:59:17,175:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3823 (32.7191)
2022-11-25 00:59:17,178:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7974 (32.5350)
2022-11-25 00:59:17,446:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9058 (13.9058)
2022-11-25 00:59:17,448:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9060 (13.9059)
2022-11-25 00:59:17,451:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9169 (13.9096)
2022-11-25 00:59:17,452:INFO: Dataset: zara2               Batch:  4/18	Loss 13.8296 (13.8882)
2022-11-25 00:59:17,472:INFO: Dataset: zara2               Batch:  5/18	Loss 14.3459 (13.9705)
2022-11-25 00:59:17,493:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8803 (13.9563)
2022-11-25 00:59:17,494:INFO: Dataset: zara2               Batch:  7/18	Loss 13.9170 (13.9499)
2022-11-25 00:59:17,495:INFO: Dataset: zara2               Batch:  8/18	Loss 13.9655 (13.9519)
2022-11-25 00:59:17,496:INFO: Dataset: zara2               Batch:  9/18	Loss 13.7682 (13.9297)
2022-11-25 00:59:17,497:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3496 (13.8715)
2022-11-25 00:59:17,498:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3887 (13.8261)
2022-11-25 00:59:17,499:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7885 (13.7405)
2022-11-25 00:59:17,500:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8383 (13.6719)
2022-11-25 00:59:17,501:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4289 (13.5791)
2022-11-25 00:59:17,502:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1903 (13.4819)
2022-11-25 00:59:17,503:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2684 (13.4038)
2022-11-25 00:59:17,504:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9149 (13.3130)
2022-11-25 00:59:17,506:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4876 (13.2146)
2022-11-25 00:59:17,555:INFO: - Computing ADE (validation o)
2022-11-25 00:59:17,822:INFO: 		 ADE on eth                       dataset:	 1.0905903577804565
2022-11-25 00:59:17,823:INFO: Average validation o:	ADE  1.0906	FDE  2.2068
2022-11-25 00:59:17,823:INFO: - Computing loss (validation)
2022-11-25 00:59:18,020:INFO: Dataset: hotel               Batch: 1/2	Loss 35.4496 (35.4496)
2022-11-25 00:59:18,021:INFO: Dataset: hotel               Batch: 2/2	Loss 35.0194 (35.4143)
2022-11-25 00:59:18,300:INFO: Dataset: univ                Batch: 1/3	Loss 12.8241 (12.8241)
2022-11-25 00:59:18,302:INFO: Dataset: univ                Batch: 2/3	Loss 13.0636 (12.9376)
2022-11-25 00:59:18,303:INFO: Dataset: univ                Batch: 3/3	Loss 12.7944 (12.8928)
2022-11-25 00:59:18,542:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4486 (30.4486)
2022-11-25 00:59:18,543:INFO: Dataset: zara1               Batch: 2/2	Loss 30.8429 (30.5361)
2022-11-25 00:59:18,782:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3827 (11.3827)
2022-11-25 00:59:18,783:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3624 (11.3717)
2022-11-25 00:59:18,784:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2475 (11.3317)
2022-11-25 00:59:18,785:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2179 (11.3058)
2022-11-25 00:59:18,786:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5533 (11.3546)
2022-11-25 00:59:18,843:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_605.pth.tar
2022-11-25 00:59:18,843:INFO: 
===> EPOCH: 606 (P4)
2022-11-25 00:59:18,843:INFO: - Computing loss (training)
2022-11-25 00:59:19,043:INFO: Dataset: hotel               Batch: 1/4	Loss 31.3493 (31.3493)
2022-11-25 00:59:19,046:INFO: Dataset: hotel               Batch: 2/4	Loss 31.5597 (31.4542)
2022-11-25 00:59:19,048:INFO: Dataset: hotel               Batch: 3/4	Loss 32.3036 (31.7351)
2022-11-25 00:59:19,050:INFO: Dataset: hotel               Batch: 4/4	Loss 27.0567 (30.9389)
2022-11-25 00:59:19,315:INFO: Dataset: univ                Batch:  1/15	Loss 13.4252 (13.4252)
2022-11-25 00:59:19,317:INFO: Dataset: univ                Batch:  2/15	Loss 13.7528 (13.5836)
2022-11-25 00:59:19,324:INFO: Dataset: univ                Batch:  3/15	Loss 13.7166 (13.6280)
2022-11-25 00:59:19,327:INFO: Dataset: univ                Batch:  4/15	Loss 13.7440 (13.6562)
2022-11-25 00:59:19,388:INFO: Dataset: univ                Batch:  5/15	Loss 13.7062 (13.6666)
2022-11-25 00:59:19,397:INFO: Dataset: univ                Batch:  6/15	Loss 13.6600 (13.6655)
2022-11-25 00:59:19,398:INFO: Dataset: univ                Batch:  7/15	Loss 13.2659 (13.6131)
2022-11-25 00:59:19,399:INFO: Dataset: univ                Batch:  8/15	Loss 13.1977 (13.5643)
2022-11-25 00:59:19,400:INFO: Dataset: univ                Batch:  9/15	Loss 12.7001 (13.4660)
2022-11-25 00:59:19,401:INFO: Dataset: univ                Batch: 10/15	Loss 12.7751 (13.4051)
2022-11-25 00:59:19,402:INFO: Dataset: univ                Batch: 11/15	Loss 12.3579 (13.3250)
2022-11-25 00:59:19,404:INFO: Dataset: univ                Batch: 12/15	Loss 11.8722 (13.1820)
2022-11-25 00:59:19,405:INFO: Dataset: univ                Batch: 13/15	Loss 11.7593 (13.0681)
2022-11-25 00:59:19,406:INFO: Dataset: univ                Batch: 14/15	Loss 11.1630 (12.9099)
2022-11-25 00:59:19,407:INFO: Dataset: univ                Batch: 15/15	Loss 10.9480 (12.8872)
2022-11-25 00:59:19,677:INFO: Dataset: zara1               Batch: 1/8	Loss 33.4326 (33.4326)
2022-11-25 00:59:19,681:INFO: Dataset: zara1               Batch: 2/8	Loss 33.5040 (33.4688)
2022-11-25 00:59:19,682:INFO: Dataset: zara1               Batch: 3/8	Loss 33.4040 (33.4451)
2022-11-25 00:59:19,688:INFO: Dataset: zara1               Batch: 4/8	Loss 33.0548 (33.3529)
2022-11-25 00:59:19,690:INFO: Dataset: zara1               Batch: 5/8	Loss 32.6890 (33.2232)
2022-11-25 00:59:19,730:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7061 (32.9647)
2022-11-25 00:59:19,734:INFO: Dataset: zara1               Batch: 7/8	Loss 31.1840 (32.6690)
2022-11-25 00:59:19,737:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7840 (32.4805)
2022-11-25 00:59:20,032:INFO: Dataset: zara2               Batch:  1/18	Loss 14.1791 (14.1791)
2022-11-25 00:59:20,034:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5603 (13.8778)
2022-11-25 00:59:20,036:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9414 (13.9000)
2022-11-25 00:59:20,037:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2776 (13.9923)
2022-11-25 00:59:20,091:INFO: Dataset: zara2               Batch:  5/18	Loss 14.1012 (14.0149)
2022-11-25 00:59:20,098:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9875 (14.0102)
2022-11-25 00:59:20,099:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8493 (13.9871)
2022-11-25 00:59:20,100:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7604 (13.9555)
2022-11-25 00:59:20,101:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5393 (13.9084)
2022-11-25 00:59:20,102:INFO: Dataset: zara2               Batch: 10/18	Loss 13.4419 (13.8630)
2022-11-25 00:59:20,103:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0447 (13.7945)
2022-11-25 00:59:20,105:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0520 (13.7314)
2022-11-25 00:59:20,106:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8308 (13.6559)
2022-11-25 00:59:20,107:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4659 (13.5764)
2022-11-25 00:59:20,108:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3425 (13.5051)
2022-11-25 00:59:20,109:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7523 (13.3874)
2022-11-25 00:59:20,110:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6559 (13.2971)
2022-11-25 00:59:20,112:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5786 (13.2175)
2022-11-25 00:59:20,159:INFO: - Computing ADE (validation o)
2022-11-25 00:59:20,428:INFO: 		 ADE on eth                       dataset:	 1.0674382448196411
2022-11-25 00:59:20,428:INFO: Average validation o:	ADE  1.0674	FDE  2.1635
2022-11-25 00:59:20,428:INFO: - Computing loss (validation)
2022-11-25 00:59:20,622:INFO: Dataset: hotel               Batch: 1/2	Loss 35.2417 (35.2417)
2022-11-25 00:59:20,623:INFO: Dataset: hotel               Batch: 2/2	Loss 37.0930 (35.3807)
2022-11-25 00:59:20,891:INFO: Dataset: univ                Batch: 1/3	Loss 12.9539 (12.9539)
2022-11-25 00:59:20,893:INFO: Dataset: univ                Batch: 2/3	Loss 12.8312 (12.8917)
2022-11-25 00:59:20,893:INFO: Dataset: univ                Batch: 3/3	Loss 12.9720 (12.9142)
2022-11-25 00:59:21,131:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5227 (30.5227)
2022-11-25 00:59:21,132:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2823 (30.4678)
2022-11-25 00:59:21,381:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3076 (11.3076)
2022-11-25 00:59:21,383:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4348 (11.3729)
2022-11-25 00:59:21,396:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2802 (11.3424)
2022-11-25 00:59:21,397:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3215 (11.3371)
2022-11-25 00:59:21,398:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4328 (11.3549)
2022-11-25 00:59:21,451:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_606.pth.tar
2022-11-25 00:59:21,451:INFO: 
===> EPOCH: 607 (P4)
2022-11-25 00:59:21,451:INFO: - Computing loss (training)
2022-11-25 00:59:21,666:INFO: Dataset: hotel               Batch: 1/4	Loss 31.4041 (31.4041)
2022-11-25 00:59:21,681:INFO: Dataset: hotel               Batch: 2/4	Loss 32.4519 (31.9355)
2022-11-25 00:59:21,683:INFO: Dataset: hotel               Batch: 3/4	Loss 29.7141 (31.1833)
2022-11-25 00:59:21,684:INFO: Dataset: hotel               Batch: 4/4	Loss 29.3785 (30.8833)
2022-11-25 00:59:21,950:INFO: Dataset: univ                Batch:  1/15	Loss 13.4009 (13.4009)
2022-11-25 00:59:21,952:INFO: Dataset: univ                Batch:  2/15	Loss 13.8382 (13.6100)
2022-11-25 00:59:21,954:INFO: Dataset: univ                Batch:  3/15	Loss 13.9062 (13.7065)
2022-11-25 00:59:21,955:INFO: Dataset: univ                Batch:  4/15	Loss 13.7414 (13.7157)
2022-11-25 00:59:21,977:INFO: Dataset: univ                Batch:  5/15	Loss 13.5711 (13.6863)
2022-11-25 00:59:21,981:INFO: Dataset: univ                Batch:  6/15	Loss 13.4458 (13.6436)
2022-11-25 00:59:21,982:INFO: Dataset: univ                Batch:  7/15	Loss 13.2687 (13.5924)
2022-11-25 00:59:21,983:INFO: Dataset: univ                Batch:  8/15	Loss 13.1962 (13.5489)
2022-11-25 00:59:21,984:INFO: Dataset: univ                Batch:  9/15	Loss 12.8566 (13.4730)
2022-11-25 00:59:21,985:INFO: Dataset: univ                Batch: 10/15	Loss 12.5461 (13.3712)
2022-11-25 00:59:21,986:INFO: Dataset: univ                Batch: 11/15	Loss 12.2402 (13.2595)
2022-11-25 00:59:21,988:INFO: Dataset: univ                Batch: 12/15	Loss 11.9728 (13.1613)
2022-11-25 00:59:21,989:INFO: Dataset: univ                Batch: 13/15	Loss 11.8449 (13.0766)
2022-11-25 00:59:21,990:INFO: Dataset: univ                Batch: 14/15	Loss 11.3967 (12.9669)
2022-11-25 00:59:21,991:INFO: Dataset: univ                Batch: 15/15	Loss 11.0354 (12.9440)
2022-11-25 00:59:22,235:INFO: Dataset: zara1               Batch: 1/8	Loss 33.6799 (33.6799)
2022-11-25 00:59:22,238:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2625 (33.4595)
2022-11-25 00:59:22,241:INFO: Dataset: zara1               Batch: 3/8	Loss 33.3312 (33.4115)
2022-11-25 00:59:22,243:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7392 (33.2356)
2022-11-25 00:59:22,245:INFO: Dataset: zara1               Batch: 5/8	Loss 32.4724 (33.0719)
2022-11-25 00:59:22,274:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7509 (32.8794)
2022-11-25 00:59:22,275:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3625 (32.6460)
2022-11-25 00:59:22,276:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5840 (32.4322)
2022-11-25 00:59:22,534:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8586 (13.8586)
2022-11-25 00:59:22,592:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1245 (13.9866)
2022-11-25 00:59:22,593:INFO: Dataset: zara2               Batch:  3/18	Loss 14.4298 (14.1330)
2022-11-25 00:59:22,594:INFO: Dataset: zara2               Batch:  4/18	Loss 14.1142 (14.1283)
2022-11-25 00:59:22,595:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0131 (14.1052)
2022-11-25 00:59:22,598:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9949 (14.0857)
2022-11-25 00:59:22,599:INFO: Dataset: zara2               Batch:  7/18	Loss 14.3294 (14.1238)
2022-11-25 00:59:22,600:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6796 (14.0681)
2022-11-25 00:59:22,601:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6945 (14.0258)
2022-11-25 00:59:22,602:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3028 (13.9561)
2022-11-25 00:59:22,604:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1423 (13.8861)
2022-11-25 00:59:22,604:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0804 (13.8176)
2022-11-25 00:59:22,605:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4768 (13.7095)
2022-11-25 00:59:22,607:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4423 (13.6317)
2022-11-25 00:59:22,608:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5780 (13.5690)
2022-11-25 00:59:22,610:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7826 (13.4571)
2022-11-25 00:59:22,611:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7731 (13.3612)
2022-11-25 00:59:22,613:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3371 (13.2518)
2022-11-25 00:59:22,658:INFO: - Computing ADE (validation o)
2022-11-25 00:59:22,926:INFO: 		 ADE on eth                       dataset:	 1.0848395824432373
2022-11-25 00:59:22,926:INFO: Average validation o:	ADE  1.0848	FDE  2.2051
2022-11-25 00:59:22,927:INFO: - Computing loss (validation)
2022-11-25 00:59:23,120:INFO: Dataset: hotel               Batch: 1/2	Loss 35.5173 (35.5173)
2022-11-25 00:59:23,120:INFO: Dataset: hotel               Batch: 2/2	Loss 33.6261 (35.3624)
2022-11-25 00:59:23,394:INFO: Dataset: univ                Batch: 1/3	Loss 12.9046 (12.9046)
2022-11-25 00:59:23,396:INFO: Dataset: univ                Batch: 2/3	Loss 12.8465 (12.8739)
2022-11-25 00:59:23,397:INFO: Dataset: univ                Batch: 3/3	Loss 12.7348 (12.8301)
2022-11-25 00:59:23,646:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4389 (30.4389)
2022-11-25 00:59:23,646:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4170 (30.4338)
2022-11-25 00:59:23,919:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4841 (11.4841)
2022-11-25 00:59:23,924:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3891 (11.4382)
2022-11-25 00:59:23,925:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2035 (11.3583)
2022-11-25 00:59:23,926:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4139 (11.3722)
2022-11-25 00:59:23,926:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3747 (11.3727)
2022-11-25 00:59:23,984:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_607.pth.tar
2022-11-25 00:59:23,984:INFO: 
===> EPOCH: 608 (P4)
2022-11-25 00:59:23,984:INFO: - Computing loss (training)
2022-11-25 00:59:24,199:INFO: Dataset: hotel               Batch: 1/4	Loss 31.0561 (31.0561)
2022-11-25 00:59:24,202:INFO: Dataset: hotel               Batch: 2/4	Loss 31.1956 (31.1240)
2022-11-25 00:59:24,204:INFO: Dataset: hotel               Batch: 3/4	Loss 31.2169 (31.1558)
2022-11-25 00:59:24,207:INFO: Dataset: hotel               Batch: 4/4	Loss 28.9569 (30.7961)
2022-11-25 00:59:24,471:INFO: Dataset: univ                Batch:  1/15	Loss 13.3536 (13.3536)
2022-11-25 00:59:24,472:INFO: Dataset: univ                Batch:  2/15	Loss 13.7371 (13.5394)
2022-11-25 00:59:24,474:INFO: Dataset: univ                Batch:  3/15	Loss 13.9573 (13.6696)
2022-11-25 00:59:24,477:INFO: Dataset: univ                Batch:  4/15	Loss 13.6889 (13.6744)
2022-11-25 00:59:24,509:INFO: Dataset: univ                Batch:  5/15	Loss 13.6028 (13.6605)
2022-11-25 00:59:24,517:INFO: Dataset: univ                Batch:  6/15	Loss 13.5282 (13.6420)
2022-11-25 00:59:24,518:INFO: Dataset: univ                Batch:  7/15	Loss 13.2866 (13.5886)
2022-11-25 00:59:24,519:INFO: Dataset: univ                Batch:  8/15	Loss 13.1970 (13.5424)
2022-11-25 00:59:24,520:INFO: Dataset: univ                Batch:  9/15	Loss 12.6299 (13.4347)
2022-11-25 00:59:24,522:INFO: Dataset: univ                Batch: 10/15	Loss 12.5631 (13.3462)
2022-11-25 00:59:24,523:INFO: Dataset: univ                Batch: 11/15	Loss 12.1849 (13.2373)
2022-11-25 00:59:24,525:INFO: Dataset: univ                Batch: 12/15	Loss 11.9401 (13.1251)
2022-11-25 00:59:24,526:INFO: Dataset: univ                Batch: 13/15	Loss 11.5871 (13.0008)
2022-11-25 00:59:24,527:INFO: Dataset: univ                Batch: 14/15	Loss 11.1942 (12.8751)
2022-11-25 00:59:24,528:INFO: Dataset: univ                Batch: 15/15	Loss 10.7333 (12.8441)
2022-11-25 00:59:24,793:INFO: Dataset: zara1               Batch: 1/8	Loss 33.5719 (33.5719)
2022-11-25 00:59:24,795:INFO: Dataset: zara1               Batch: 2/8	Loss 33.4376 (33.5025)
2022-11-25 00:59:24,801:INFO: Dataset: zara1               Batch: 3/8	Loss 33.2934 (33.4286)
2022-11-25 00:59:24,802:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8844 (33.3017)
2022-11-25 00:59:24,803:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5168 (33.1482)
2022-11-25 00:59:24,856:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6685 (32.8929)
2022-11-25 00:59:24,859:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0798 (32.5965)
2022-11-25 00:59:24,863:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6750 (32.3639)
2022-11-25 00:59:25,145:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7042 (13.7042)
2022-11-25 00:59:25,147:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0251 (13.8753)
2022-11-25 00:59:25,152:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0499 (13.9308)
2022-11-25 00:59:25,153:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2469 (14.0075)
2022-11-25 00:59:25,189:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2569 (14.0561)
2022-11-25 00:59:25,195:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7252 (14.0035)
2022-11-25 00:59:25,196:INFO: Dataset: zara2               Batch:  7/18	Loss 14.1796 (14.0278)
2022-11-25 00:59:25,197:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5839 (13.9728)
2022-11-25 00:59:25,198:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4627 (13.9129)
2022-11-25 00:59:25,199:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3776 (13.8570)
2022-11-25 00:59:25,201:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0188 (13.7893)
2022-11-25 00:59:25,202:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7829 (13.7086)
2022-11-25 00:59:25,203:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5357 (13.6131)
2022-11-25 00:59:25,204:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5945 (13.5303)
2022-11-25 00:59:25,205:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2965 (13.4507)
2022-11-25 00:59:25,206:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8335 (13.3524)
2022-11-25 00:59:25,208:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7762 (13.2621)
2022-11-25 00:59:25,210:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5227 (13.1793)
2022-11-25 00:59:25,257:INFO: - Computing ADE (validation o)
2022-11-25 00:59:25,532:INFO: 		 ADE on eth                       dataset:	 1.0817807912826538
2022-11-25 00:59:25,533:INFO: Average validation o:	ADE  1.0818	FDE  2.1939
2022-11-25 00:59:25,533:INFO: - Computing loss (validation)
2022-11-25 00:59:25,729:INFO: Dataset: hotel               Batch: 1/2	Loss 35.2941 (35.2941)
2022-11-25 00:59:25,730:INFO: Dataset: hotel               Batch: 2/2	Loss 35.6654 (35.3169)
2022-11-25 00:59:26,000:INFO: Dataset: univ                Batch: 1/3	Loss 13.0248 (13.0248)
2022-11-25 00:59:26,000:INFO: Dataset: univ                Batch: 2/3	Loss 12.7382 (12.8835)
2022-11-25 00:59:26,001:INFO: Dataset: univ                Batch: 3/3	Loss 12.8761 (12.8812)
2022-11-25 00:59:26,242:INFO: Dataset: zara1               Batch: 1/2	Loss 30.5562 (30.5562)
2022-11-25 00:59:26,244:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2537 (30.4745)
2022-11-25 00:59:26,494:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3019 (11.3019)
2022-11-25 00:59:26,495:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3243 (11.3132)
2022-11-25 00:59:26,497:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3695 (11.3314)
2022-11-25 00:59:26,509:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4292 (11.3541)
2022-11-25 00:59:26,513:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3315 (11.3497)
2022-11-25 00:59:26,567:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_608.pth.tar
2022-11-25 00:59:26,567:INFO: 
===> EPOCH: 609 (P4)
2022-11-25 00:59:26,567:INFO: - Computing loss (training)
2022-11-25 00:59:26,783:INFO: Dataset: hotel               Batch: 1/4	Loss 30.4200 (30.4200)
2022-11-25 00:59:26,784:INFO: Dataset: hotel               Batch: 2/4	Loss 30.4677 (30.4432)
2022-11-25 00:59:26,801:INFO: Dataset: hotel               Batch: 3/4	Loss 31.3338 (30.7471)
2022-11-25 00:59:26,803:INFO: Dataset: hotel               Batch: 4/4	Loss 30.4314 (30.6938)
2022-11-25 00:59:27,063:INFO: Dataset: univ                Batch:  1/15	Loss 13.7071 (13.7071)
2022-11-25 00:59:27,117:INFO: Dataset: univ                Batch:  2/15	Loss 13.7356 (13.7217)
2022-11-25 00:59:27,118:INFO: Dataset: univ                Batch:  3/15	Loss 13.6703 (13.7047)
2022-11-25 00:59:27,120:INFO: Dataset: univ                Batch:  4/15	Loss 13.5968 (13.6777)
2022-11-25 00:59:27,121:INFO: Dataset: univ                Batch:  5/15	Loss 13.4460 (13.6333)
2022-11-25 00:59:27,126:INFO: Dataset: univ                Batch:  6/15	Loss 13.4947 (13.6107)
2022-11-25 00:59:27,127:INFO: Dataset: univ                Batch:  7/15	Loss 13.2838 (13.5605)
2022-11-25 00:59:27,128:INFO: Dataset: univ                Batch:  8/15	Loss 12.8183 (13.4629)
2022-11-25 00:59:27,130:INFO: Dataset: univ                Batch:  9/15	Loss 12.9703 (13.4158)
2022-11-25 00:59:27,131:INFO: Dataset: univ                Batch: 10/15	Loss 12.7843 (13.3535)
2022-11-25 00:59:27,132:INFO: Dataset: univ                Batch: 11/15	Loss 12.0645 (13.2396)
2022-11-25 00:59:27,133:INFO: Dataset: univ                Batch: 12/15	Loss 12.0423 (13.1406)
2022-11-25 00:59:27,134:INFO: Dataset: univ                Batch: 13/15	Loss 11.4187 (13.0018)
2022-11-25 00:59:27,136:INFO: Dataset: univ                Batch: 14/15	Loss 11.1103 (12.8527)
2022-11-25 00:59:27,137:INFO: Dataset: univ                Batch: 15/15	Loss 10.4954 (12.8225)
2022-11-25 00:59:27,389:INFO: Dataset: zara1               Batch: 1/8	Loss 33.8657 (33.8657)
2022-11-25 00:59:27,392:INFO: Dataset: zara1               Batch: 2/8	Loss 33.7104 (33.7863)
2022-11-25 00:59:27,394:INFO: Dataset: zara1               Batch: 3/8	Loss 33.6135 (33.7297)
2022-11-25 00:59:27,396:INFO: Dataset: zara1               Batch: 4/8	Loss 33.2300 (33.5964)
2022-11-25 00:59:27,398:INFO: Dataset: zara1               Batch: 5/8	Loss 32.5455 (33.3902)
2022-11-25 00:59:27,432:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9034 (33.1532)
2022-11-25 00:59:27,433:INFO: Dataset: zara1               Batch: 7/8	Loss 31.2288 (32.8623)
2022-11-25 00:59:27,434:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5112 (32.6148)
2022-11-25 00:59:27,703:INFO: Dataset: zara2               Batch:  1/18	Loss 14.3280 (14.3280)
2022-11-25 00:59:27,706:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1463 (14.2357)
2022-11-25 00:59:27,749:INFO: Dataset: zara2               Batch:  3/18	Loss 15.0782 (14.4878)
2022-11-25 00:59:27,753:INFO: Dataset: zara2               Batch:  4/18	Loss 14.7153 (14.5460)
2022-11-25 00:59:27,756:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2442 (14.4879)
2022-11-25 00:59:27,765:INFO: Dataset: zara2               Batch:  6/18	Loss 14.2719 (14.4503)
2022-11-25 00:59:27,766:INFO: Dataset: zara2               Batch:  7/18	Loss 13.9274 (14.3762)
2022-11-25 00:59:27,767:INFO: Dataset: zara2               Batch:  8/18	Loss 13.8854 (14.3132)
2022-11-25 00:59:27,768:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5830 (14.2265)
2022-11-25 00:59:27,768:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0823 (14.1090)
2022-11-25 00:59:27,769:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0571 (14.0149)
2022-11-25 00:59:27,771:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7606 (13.9185)
2022-11-25 00:59:27,772:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6424 (13.8182)
2022-11-25 00:59:27,773:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4340 (13.7280)
2022-11-25 00:59:27,773:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0002 (13.5973)
2022-11-25 00:59:27,774:INFO: Dataset: zara2               Batch: 16/18	Loss 11.6787 (13.4826)
2022-11-25 00:59:27,775:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7747 (13.3700)
2022-11-25 00:59:27,777:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4647 (13.2701)
2022-11-25 00:59:27,822:INFO: - Computing ADE (validation o)
2022-11-25 00:59:28,095:INFO: 		 ADE on eth                       dataset:	 1.0855969190597534
2022-11-25 00:59:28,096:INFO: Average validation o:	ADE  1.0856	FDE  2.1765
2022-11-25 00:59:28,096:INFO: - Computing loss (validation)
2022-11-25 00:59:28,287:INFO: Dataset: hotel               Batch: 1/2	Loss 35.6240 (35.6240)
2022-11-25 00:59:28,289:INFO: Dataset: hotel               Batch: 2/2	Loss 31.4672 (35.2693)
2022-11-25 00:59:28,537:INFO: Dataset: univ                Batch: 1/3	Loss 12.7657 (12.7657)
2022-11-25 00:59:28,543:INFO: Dataset: univ                Batch: 2/3	Loss 13.0996 (12.9171)
2022-11-25 00:59:28,544:INFO: Dataset: univ                Batch: 3/3	Loss 12.9929 (12.9419)
2022-11-25 00:59:28,799:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4971 (30.4971)
2022-11-25 00:59:28,800:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1046 (30.3886)
2022-11-25 00:59:29,080:INFO: Dataset: zara2               Batch: 1/5	Loss 11.2690 (11.2690)
2022-11-25 00:59:29,081:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4075 (11.3376)
2022-11-25 00:59:29,083:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3553 (11.3438)
2022-11-25 00:59:29,084:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2991 (11.3323)
2022-11-25 00:59:29,089:INFO: Dataset: zara2               Batch: 5/5	Loss 11.1963 (11.3062)
2022-11-25 00:59:29,144:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_609.pth.tar
2022-11-25 00:59:29,144:INFO: 
===> EPOCH: 610 (P4)
2022-11-25 00:59:29,144:INFO: - Computing loss (training)
2022-11-25 00:59:29,372:INFO: Dataset: hotel               Batch: 1/4	Loss 31.0733 (31.0733)
2022-11-25 00:59:29,373:INFO: Dataset: hotel               Batch: 2/4	Loss 30.8535 (30.9613)
2022-11-25 00:59:29,375:INFO: Dataset: hotel               Batch: 3/4	Loss 30.2457 (30.7356)
2022-11-25 00:59:29,377:INFO: Dataset: hotel               Batch: 4/4	Loss 29.9285 (30.6004)
2022-11-25 00:59:29,641:INFO: Dataset: univ                Batch:  1/15	Loss 13.5080 (13.5080)
2022-11-25 00:59:29,645:INFO: Dataset: univ                Batch:  2/15	Loss 13.7130 (13.6025)
2022-11-25 00:59:29,655:INFO: Dataset: univ                Batch:  3/15	Loss 14.1091 (13.7676)
2022-11-25 00:59:29,656:INFO: Dataset: univ                Batch:  4/15	Loss 13.8309 (13.7830)
2022-11-25 00:59:29,679:INFO: Dataset: univ                Batch:  5/15	Loss 13.6136 (13.7478)
2022-11-25 00:59:29,685:INFO: Dataset: univ                Batch:  6/15	Loss 13.6457 (13.7305)
2022-11-25 00:59:29,686:INFO: Dataset: univ                Batch:  7/15	Loss 13.3024 (13.6683)
2022-11-25 00:59:29,687:INFO: Dataset: univ                Batch:  8/15	Loss 13.0558 (13.5857)
2022-11-25 00:59:29,688:INFO: Dataset: univ                Batch:  9/15	Loss 12.9194 (13.5157)
2022-11-25 00:59:29,689:INFO: Dataset: univ                Batch: 10/15	Loss 12.5977 (13.4285)
2022-11-25 00:59:29,690:INFO: Dataset: univ                Batch: 11/15	Loss 12.4564 (13.3399)
2022-11-25 00:59:29,692:INFO: Dataset: univ                Batch: 12/15	Loss 11.9369 (13.2308)
2022-11-25 00:59:29,693:INFO: Dataset: univ                Batch: 13/15	Loss 11.6553 (13.1151)
2022-11-25 00:59:29,694:INFO: Dataset: univ                Batch: 14/15	Loss 11.3706 (12.9882)
2022-11-25 00:59:29,695:INFO: Dataset: univ                Batch: 15/15	Loss 10.8422 (12.9581)
2022-11-25 00:59:29,951:INFO: Dataset: zara1               Batch: 1/8	Loss 33.2039 (33.2039)
2022-11-25 00:59:29,968:INFO: Dataset: zara1               Batch: 2/8	Loss 33.4394 (33.3212)
2022-11-25 00:59:29,969:INFO: Dataset: zara1               Batch: 3/8	Loss 33.1247 (33.2586)
2022-11-25 00:59:29,970:INFO: Dataset: zara1               Batch: 4/8	Loss 32.6130 (33.0970)
2022-11-25 00:59:29,973:INFO: Dataset: zara1               Batch: 5/8	Loss 32.4894 (32.9758)
2022-11-25 00:59:30,028:INFO: Dataset: zara1               Batch: 6/8	Loss 32.1205 (32.8414)
2022-11-25 00:59:30,032:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0432 (32.5726)
2022-11-25 00:59:30,035:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4588 (32.3490)
2022-11-25 00:59:30,340:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7957 (13.7957)
2022-11-25 00:59:30,379:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6367 (13.7194)
2022-11-25 00:59:30,381:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0685 (13.8395)
2022-11-25 00:59:30,382:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2551 (13.9415)
2022-11-25 00:59:30,383:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8875 (13.9302)
2022-11-25 00:59:30,386:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0637 (13.9542)
2022-11-25 00:59:30,387:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7757 (13.9304)
2022-11-25 00:59:30,388:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6860 (13.8995)
2022-11-25 00:59:30,389:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4850 (13.8553)
2022-11-25 00:59:30,390:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1427 (13.7861)
2022-11-25 00:59:30,391:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9658 (13.7217)
2022-11-25 00:59:30,393:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7125 (13.6422)
2022-11-25 00:59:30,394:INFO: Dataset: zara2               Batch: 13/18	Loss 12.3589 (13.5551)
2022-11-25 00:59:30,396:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2911 (13.4669)
2022-11-25 00:59:30,397:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5817 (13.4129)
2022-11-25 00:59:30,399:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0417 (13.3400)
2022-11-25 00:59:30,400:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7205 (13.2560)
2022-11-25 00:59:30,402:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4460 (13.1662)
2022-11-25 00:59:30,450:INFO: - Computing ADE (validation o)
2022-11-25 00:59:30,718:INFO: 		 ADE on eth                       dataset:	 1.0843183994293213
2022-11-25 00:59:30,718:INFO: Average validation o:	ADE  1.0843	FDE  2.1851
2022-11-25 00:59:30,719:INFO: - Computing loss (validation)
2022-11-25 00:59:30,916:INFO: Dataset: hotel               Batch: 1/2	Loss 35.2391 (35.2391)
2022-11-25 00:59:30,917:INFO: Dataset: hotel               Batch: 2/2	Loss 34.1943 (35.1464)
2022-11-25 00:59:31,177:INFO: Dataset: univ                Batch: 1/3	Loss 13.0061 (13.0061)
2022-11-25 00:59:31,179:INFO: Dataset: univ                Batch: 2/3	Loss 12.8474 (12.9215)
2022-11-25 00:59:31,180:INFO: Dataset: univ                Batch: 3/3	Loss 12.7883 (12.8817)
2022-11-25 00:59:31,424:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4381 (30.4381)
2022-11-25 00:59:31,425:INFO: Dataset: zara1               Batch: 2/2	Loss 30.3365 (30.4106)
2022-11-25 00:59:31,673:INFO: Dataset: zara2               Batch: 1/5	Loss 11.2070 (11.2070)
2022-11-25 00:59:31,681:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5984 (11.3999)
2022-11-25 00:59:31,682:INFO: Dataset: zara2               Batch: 3/5	Loss 11.1057 (11.3074)
2022-11-25 00:59:31,682:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4096 (11.3322)
2022-11-25 00:59:31,685:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3499 (11.3357)
2022-11-25 00:59:31,738:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_610.pth.tar
2022-11-25 00:59:31,738:INFO: 
===> EPOCH: 611 (P4)
2022-11-25 00:59:31,739:INFO: - Computing loss (training)
2022-11-25 00:59:31,947:INFO: Dataset: hotel               Batch: 1/4	Loss 31.3495 (31.3495)
2022-11-25 00:59:31,949:INFO: Dataset: hotel               Batch: 2/4	Loss 29.8194 (30.5420)
2022-11-25 00:59:31,951:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0539 (30.7167)
2022-11-25 00:59:31,952:INFO: Dataset: hotel               Batch: 4/4	Loss 29.0092 (30.4284)
2022-11-25 00:59:32,221:INFO: Dataset: univ                Batch:  1/15	Loss 13.4121 (13.4121)
2022-11-25 00:59:32,222:INFO: Dataset: univ                Batch:  2/15	Loss 13.7063 (13.5552)
2022-11-25 00:59:32,262:INFO: Dataset: univ                Batch:  3/15	Loss 13.7869 (13.6339)
2022-11-25 00:59:32,263:INFO: Dataset: univ                Batch:  4/15	Loss 13.6403 (13.6357)
2022-11-25 00:59:32,264:INFO: Dataset: univ                Batch:  5/15	Loss 13.7414 (13.6563)
2022-11-25 00:59:32,273:INFO: Dataset: univ                Batch:  6/15	Loss 13.3724 (13.6106)
2022-11-25 00:59:32,274:INFO: Dataset: univ                Batch:  7/15	Loss 13.3706 (13.5759)
2022-11-25 00:59:32,275:INFO: Dataset: univ                Batch:  8/15	Loss 13.0768 (13.5156)
2022-11-25 00:59:32,276:INFO: Dataset: univ                Batch:  9/15	Loss 12.8835 (13.4422)
2022-11-25 00:59:32,277:INFO: Dataset: univ                Batch: 10/15	Loss 12.4515 (13.3405)
2022-11-25 00:59:32,278:INFO: Dataset: univ                Batch: 11/15	Loss 12.3311 (13.2513)
2022-11-25 00:59:32,280:INFO: Dataset: univ                Batch: 12/15	Loss 11.9547 (13.1427)
2022-11-25 00:59:32,282:INFO: Dataset: univ                Batch: 13/15	Loss 11.5949 (13.0238)
2022-11-25 00:59:32,283:INFO: Dataset: univ                Batch: 14/15	Loss 11.3102 (12.9063)
2022-11-25 00:59:32,284:INFO: Dataset: univ                Batch: 15/15	Loss 11.1198 (12.8880)
2022-11-25 00:59:32,544:INFO: Dataset: zara1               Batch: 1/8	Loss 33.3636 (33.3636)
2022-11-25 00:59:32,546:INFO: Dataset: zara1               Batch: 2/8	Loss 33.3108 (33.3364)
2022-11-25 00:59:32,548:INFO: Dataset: zara1               Batch: 3/8	Loss 33.5037 (33.3927)
2022-11-25 00:59:32,552:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5838 (33.1729)
2022-11-25 00:59:32,555:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3524 (32.9950)
2022-11-25 00:59:32,612:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8112 (32.7961)
2022-11-25 00:59:32,616:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0954 (32.5533)
2022-11-25 00:59:32,619:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7710 (32.3413)
2022-11-25 00:59:32,915:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7896 (13.7896)
2022-11-25 00:59:32,917:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8189 (13.8037)
2022-11-25 00:59:32,919:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0681 (13.8894)
2022-11-25 00:59:32,921:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2717 (13.9922)
2022-11-25 00:59:32,975:INFO: Dataset: zara2               Batch:  5/18	Loss 14.3679 (14.0600)
2022-11-25 00:59:32,983:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7423 (14.0083)
2022-11-25 00:59:32,984:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5976 (13.9458)
2022-11-25 00:59:32,985:INFO: Dataset: zara2               Batch:  8/18	Loss 13.8760 (13.9374)
2022-11-25 00:59:32,986:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3790 (13.8803)
2022-11-25 00:59:32,987:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3049 (13.8256)
2022-11-25 00:59:32,989:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3616 (13.7815)
2022-11-25 00:59:32,990:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7796 (13.6990)
2022-11-25 00:59:32,991:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7544 (13.6336)
2022-11-25 00:59:32,992:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2065 (13.5331)
2022-11-25 00:59:32,993:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2123 (13.4470)
2022-11-25 00:59:32,994:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8254 (13.3445)
2022-11-25 00:59:32,995:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7640 (13.2519)
2022-11-25 00:59:32,997:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4961 (13.1608)
2022-11-25 00:59:33,042:INFO: - Computing ADE (validation o)
2022-11-25 00:59:33,314:INFO: 		 ADE on eth                       dataset:	 1.0206304788589478
2022-11-25 00:59:33,322:INFO: Average validation o:	ADE  1.0206	FDE  2.1111
2022-11-25 00:59:33,322:INFO: - Computing loss (validation)
2022-11-25 00:59:33,523:INFO: Dataset: hotel               Batch: 1/2	Loss 35.1976 (35.1976)
2022-11-25 00:59:33,526:INFO: Dataset: hotel               Batch: 2/2	Loss 34.2938 (35.1297)
2022-11-25 00:59:33,793:INFO: Dataset: univ                Batch: 1/3	Loss 12.9141 (12.9141)
2022-11-25 00:59:33,797:INFO: Dataset: univ                Batch: 2/3	Loss 12.5550 (12.7169)
2022-11-25 00:59:33,797:INFO: Dataset: univ                Batch: 3/3	Loss 12.7850 (12.7357)
2022-11-25 00:59:34,048:INFO: Dataset: zara1               Batch: 1/2	Loss 30.3493 (30.3493)
2022-11-25 00:59:34,049:INFO: Dataset: zara1               Batch: 2/2	Loss 30.4794 (30.3815)
2022-11-25 00:59:34,294:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4045 (11.4045)
2022-11-25 00:59:34,297:INFO: Dataset: zara2               Batch: 2/5	Loss 11.2674 (11.3374)
2022-11-25 00:59:34,297:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4703 (11.3834)
2022-11-25 00:59:34,298:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3149 (11.3671)
2022-11-25 00:59:34,300:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3944 (11.3728)
2022-11-25 00:59:34,355:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_611.pth.tar
2022-11-25 00:59:34,355:INFO: 
===> EPOCH: 612 (P4)
2022-11-25 00:59:34,355:INFO: - Computing loss (training)
2022-11-25 00:59:34,575:INFO: Dataset: hotel               Batch: 1/4	Loss 30.5256 (30.5256)
2022-11-25 00:59:34,577:INFO: Dataset: hotel               Batch: 2/4	Loss 31.4035 (30.9614)
2022-11-25 00:59:34,579:INFO: Dataset: hotel               Batch: 3/4	Loss 29.5321 (30.4888)
2022-11-25 00:59:34,582:INFO: Dataset: hotel               Batch: 4/4	Loss 29.8278 (30.3789)
2022-11-25 00:59:34,855:INFO: Dataset: univ                Batch:  1/15	Loss 13.4094 (13.4094)
2022-11-25 00:59:34,859:INFO: Dataset: univ                Batch:  2/15	Loss 13.7155 (13.5604)
2022-11-25 00:59:34,861:INFO: Dataset: univ                Batch:  3/15	Loss 13.5561 (13.5589)
2022-11-25 00:59:34,916:INFO: Dataset: univ                Batch:  4/15	Loss 13.6566 (13.5844)
2022-11-25 00:59:34,917:INFO: Dataset: univ                Batch:  5/15	Loss 13.6342 (13.5941)
2022-11-25 00:59:34,920:INFO: Dataset: univ                Batch:  6/15	Loss 13.3494 (13.5557)
2022-11-25 00:59:34,921:INFO: Dataset: univ                Batch:  7/15	Loss 13.4307 (13.5385)
2022-11-25 00:59:34,922:INFO: Dataset: univ                Batch:  8/15	Loss 12.9608 (13.4614)
2022-11-25 00:59:34,923:INFO: Dataset: univ                Batch:  9/15	Loss 12.6277 (13.3599)
2022-11-25 00:59:34,925:INFO: Dataset: univ                Batch: 10/15	Loss 12.4612 (13.2611)
2022-11-25 00:59:34,927:INFO: Dataset: univ                Batch: 11/15	Loss 12.1195 (13.1541)
2022-11-25 00:59:34,928:INFO: Dataset: univ                Batch: 12/15	Loss 11.8520 (13.0415)
2022-11-25 00:59:34,929:INFO: Dataset: univ                Batch: 13/15	Loss 11.5449 (12.9213)
2022-11-25 00:59:34,930:INFO: Dataset: univ                Batch: 14/15	Loss 11.2713 (12.8082)
2022-11-25 00:59:34,931:INFO: Dataset: univ                Batch: 15/15	Loss 11.0078 (12.7848)
2022-11-25 00:59:35,185:INFO: Dataset: zara1               Batch: 1/8	Loss 33.2269 (33.2269)
2022-11-25 00:59:35,188:INFO: Dataset: zara1               Batch: 2/8	Loss 33.5663 (33.4045)
2022-11-25 00:59:35,189:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0050 (33.2726)
2022-11-25 00:59:35,192:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8807 (33.1802)
2022-11-25 00:59:35,193:INFO: Dataset: zara1               Batch: 5/8	Loss 32.4308 (33.0302)
2022-11-25 00:59:35,226:INFO: Dataset: zara1               Batch: 6/8	Loss 31.9800 (32.8581)
2022-11-25 00:59:35,227:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0068 (32.6022)
2022-11-25 00:59:35,228:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5001 (32.3488)
2022-11-25 00:59:35,497:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7791 (13.7791)
2022-11-25 00:59:35,582:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9447 (13.8655)
2022-11-25 00:59:35,585:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6250 (13.7920)
2022-11-25 00:59:35,590:INFO: Dataset: zara2               Batch:  4/18	Loss 14.3476 (13.9292)
2022-11-25 00:59:35,593:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2518 (14.0022)
2022-11-25 00:59:35,599:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8103 (13.9722)
2022-11-25 00:59:35,600:INFO: Dataset: zara2               Batch:  7/18	Loss 14.2682 (14.0135)
2022-11-25 00:59:35,601:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3783 (13.9269)
2022-11-25 00:59:35,602:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5390 (13.8848)
2022-11-25 00:59:35,603:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3918 (13.8354)
2022-11-25 00:59:35,605:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8977 (13.7452)
2022-11-25 00:59:35,606:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8894 (13.6667)
2022-11-25 00:59:35,607:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6634 (13.5936)
2022-11-25 00:59:35,608:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1672 (13.4958)
2022-11-25 00:59:35,610:INFO: Dataset: zara2               Batch: 15/18	Loss 12.5539 (13.4365)
2022-11-25 00:59:35,611:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9211 (13.3367)
2022-11-25 00:59:35,613:INFO: Dataset: zara2               Batch: 17/18	Loss 11.5587 (13.2367)
2022-11-25 00:59:35,614:INFO: Dataset: zara2               Batch: 18/18	Loss 11.1885 (13.1388)
2022-11-25 00:59:35,659:INFO: - Computing ADE (validation o)
2022-11-25 00:59:35,920:INFO: 		 ADE on eth                       dataset:	 1.093318223953247
2022-11-25 00:59:35,920:INFO: Average validation o:	ADE  1.0933	FDE  2.1998
2022-11-25 00:59:35,921:INFO: - Computing loss (validation)
2022-11-25 00:59:36,114:INFO: Dataset: hotel               Batch: 1/2	Loss 35.2343 (35.2343)
2022-11-25 00:59:36,114:INFO: Dataset: hotel               Batch: 2/2	Loss 33.5499 (35.0848)
2022-11-25 00:59:36,368:INFO: Dataset: univ                Batch: 1/3	Loss 12.8260 (12.8260)
2022-11-25 00:59:36,369:INFO: Dataset: univ                Batch: 2/3	Loss 12.7658 (12.7961)
2022-11-25 00:59:36,371:INFO: Dataset: univ                Batch: 3/3	Loss 12.6465 (12.7557)
2022-11-25 00:59:36,615:INFO: Dataset: zara1               Batch: 1/2	Loss 30.4139 (30.4139)
2022-11-25 00:59:36,617:INFO: Dataset: zara1               Batch: 2/2	Loss 30.0634 (30.3271)
2022-11-25 00:59:36,872:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3878 (11.3878)
2022-11-25 00:59:36,885:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4125 (11.4004)
2022-11-25 00:59:36,891:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2976 (11.3645)
2022-11-25 00:59:36,892:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3318 (11.3561)
2022-11-25 00:59:36,895:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4689 (11.3790)
2022-11-25 00:59:36,948:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_612.pth.tar
2022-11-25 00:59:36,948:INFO: 
===> EPOCH: 613 (P4)
2022-11-25 00:59:36,948:INFO: - Computing loss (training)
2022-11-25 00:59:37,159:INFO: Dataset: hotel               Batch: 1/4	Loss 30.8986 (30.8986)
2022-11-25 00:59:37,162:INFO: Dataset: hotel               Batch: 2/4	Loss 32.2366 (31.5611)
2022-11-25 00:59:37,174:INFO: Dataset: hotel               Batch: 3/4	Loss 29.6077 (30.8871)
2022-11-25 00:59:37,175:INFO: Dataset: hotel               Batch: 4/4	Loss 27.5173 (30.3003)
2022-11-25 00:59:37,434:INFO: Dataset: univ                Batch:  1/15	Loss 13.3711 (13.3711)
2022-11-25 00:59:37,437:INFO: Dataset: univ                Batch:  2/15	Loss 13.7450 (13.5481)
2022-11-25 00:59:37,441:INFO: Dataset: univ                Batch:  3/15	Loss 13.3795 (13.4887)
2022-11-25 00:59:37,468:INFO: Dataset: univ                Batch:  4/15	Loss 13.5813 (13.5118)
2022-11-25 00:59:37,470:INFO: Dataset: univ                Batch:  5/15	Loss 13.6328 (13.5355)
2022-11-25 00:59:37,474:INFO: Dataset: univ                Batch:  6/15	Loss 13.3469 (13.5051)
2022-11-25 00:59:37,475:INFO: Dataset: univ                Batch:  7/15	Loss 13.2417 (13.4657)
2022-11-25 00:59:37,476:INFO: Dataset: univ                Batch:  8/15	Loss 13.0650 (13.4159)
2022-11-25 00:59:37,477:INFO: Dataset: univ                Batch:  9/15	Loss 12.9210 (13.3643)
2022-11-25 00:59:37,478:INFO: Dataset: univ                Batch: 10/15	Loss 12.4181 (13.2715)
2022-11-25 00:59:37,480:INFO: Dataset: univ                Batch: 11/15	Loss 12.1731 (13.1618)
2022-11-25 00:59:37,481:INFO: Dataset: univ                Batch: 12/15	Loss 11.8754 (13.0487)
2022-11-25 00:59:37,482:INFO: Dataset: univ                Batch: 13/15	Loss 11.5032 (12.9262)
2022-11-25 00:59:37,483:INFO: Dataset: univ                Batch: 14/15	Loss 11.3034 (12.8102)
2022-11-25 00:59:37,484:INFO: Dataset: univ                Batch: 15/15	Loss 10.9546 (12.7828)
2022-11-25 00:59:37,756:INFO: Dataset: zara1               Batch: 1/8	Loss 33.3074 (33.3074)
2022-11-25 00:59:37,757:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2441 (33.2753)
2022-11-25 00:59:37,759:INFO: Dataset: zara1               Batch: 3/8	Loss 33.0811 (33.2161)
2022-11-25 00:59:37,761:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5653 (33.0534)
2022-11-25 00:59:37,764:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3968 (32.9269)
2022-11-25 00:59:37,816:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8452 (32.7584)
2022-11-25 00:59:37,819:INFO: Dataset: zara1               Batch: 7/8	Loss 31.3311 (32.5420)
2022-11-25 00:59:37,823:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5963 (32.3249)
2022-11-25 00:59:38,097:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9163 (13.9163)
2022-11-25 00:59:38,098:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9000 (13.9083)
2022-11-25 00:59:38,101:INFO: Dataset: zara2               Batch:  3/18	Loss 14.0413 (13.9514)
2022-11-25 00:59:38,102:INFO: Dataset: zara2               Batch:  4/18	Loss 13.8729 (13.9329)
2022-11-25 00:59:38,146:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8889 (13.9249)
2022-11-25 00:59:38,153:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9410 (13.9277)
2022-11-25 00:59:38,154:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0535 (13.9433)
2022-11-25 00:59:38,155:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5634 (13.8959)
2022-11-25 00:59:38,156:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3020 (13.8291)
2022-11-25 00:59:38,156:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1293 (13.7639)
2022-11-25 00:59:38,158:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1680 (13.7120)
2022-11-25 00:59:38,159:INFO: Dataset: zara2               Batch: 12/18	Loss 13.2215 (13.6724)
2022-11-25 00:59:38,160:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6434 (13.5929)
2022-11-25 00:59:38,161:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2794 (13.5102)
2022-11-25 00:59:38,162:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1236 (13.4204)
2022-11-25 00:59:38,163:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9562 (13.3313)
2022-11-25 00:59:38,164:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7570 (13.2421)
2022-11-25 00:59:38,166:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6273 (13.1722)
2022-11-25 00:59:38,215:INFO: - Computing ADE (validation o)
2022-11-25 00:59:38,485:INFO: 		 ADE on eth                       dataset:	 1.1057374477386475
2022-11-25 00:59:38,486:INFO: Average validation o:	ADE  1.1057	FDE  2.2504
2022-11-25 00:59:38,486:INFO: - Computing loss (validation)
2022-11-25 00:59:38,680:INFO: Dataset: hotel               Batch: 1/2	Loss 35.0874 (35.0874)
2022-11-25 00:59:38,684:INFO: Dataset: hotel               Batch: 2/2	Loss 35.1940 (35.0957)
2022-11-25 00:59:38,963:INFO: Dataset: univ                Batch: 1/3	Loss 12.6993 (12.6993)
2022-11-25 00:59:38,964:INFO: Dataset: univ                Batch: 2/3	Loss 12.5930 (12.6418)
2022-11-25 00:59:38,964:INFO: Dataset: univ                Batch: 3/3	Loss 12.6623 (12.6484)
2022-11-25 00:59:39,207:INFO: Dataset: zara1               Batch: 1/2	Loss 30.3277 (30.3277)
2022-11-25 00:59:39,207:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2624 (30.3086)
2022-11-25 00:59:39,493:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4755 (11.4755)
2022-11-25 00:59:39,495:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3366 (11.4048)
2022-11-25 00:59:39,497:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4235 (11.4112)
2022-11-25 00:59:39,497:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4242 (11.4145)
2022-11-25 00:59:39,499:INFO: Dataset: zara2               Batch: 5/5	Loss 11.2961 (11.3906)
2022-11-25 00:59:39,556:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_613.pth.tar
2022-11-25 00:59:39,556:INFO: 
===> EPOCH: 614 (P4)
2022-11-25 00:59:39,556:INFO: - Computing loss (training)
2022-11-25 00:59:39,761:INFO: Dataset: hotel               Batch: 1/4	Loss 29.0667 (29.0667)
2022-11-25 00:59:39,763:INFO: Dataset: hotel               Batch: 2/4	Loss 30.6399 (29.8662)
2022-11-25 00:59:39,764:INFO: Dataset: hotel               Batch: 3/4	Loss 31.4920 (30.3866)
2022-11-25 00:59:39,766:INFO: Dataset: hotel               Batch: 4/4	Loss 29.6814 (30.2656)
2022-11-25 00:59:40,017:INFO: Dataset: univ                Batch:  1/15	Loss 13.5132 (13.5132)
2022-11-25 00:59:40,018:INFO: Dataset: univ                Batch:  2/15	Loss 13.4778 (13.4945)
2022-11-25 00:59:40,021:INFO: Dataset: univ                Batch:  3/15	Loss 13.5598 (13.5154)
2022-11-25 00:59:40,025:INFO: Dataset: univ                Batch:  4/15	Loss 13.5259 (13.5181)
2022-11-25 00:59:40,083:INFO: Dataset: univ                Batch:  5/15	Loss 13.4074 (13.4953)
2022-11-25 00:59:40,090:INFO: Dataset: univ                Batch:  6/15	Loss 13.1327 (13.4248)
2022-11-25 00:59:40,091:INFO: Dataset: univ                Batch:  7/15	Loss 13.4111 (13.4230)
2022-11-25 00:59:40,092:INFO: Dataset: univ                Batch:  8/15	Loss 12.9572 (13.3604)
2022-11-25 00:59:40,093:INFO: Dataset: univ                Batch:  9/15	Loss 12.9126 (13.3170)
2022-11-25 00:59:40,094:INFO: Dataset: univ                Batch: 10/15	Loss 12.2332 (13.1938)
2022-11-25 00:59:40,096:INFO: Dataset: univ                Batch: 11/15	Loss 12.1543 (13.0957)
2022-11-25 00:59:40,097:INFO: Dataset: univ                Batch: 12/15	Loss 11.8704 (12.9971)
2022-11-25 00:59:40,099:INFO: Dataset: univ                Batch: 13/15	Loss 11.4454 (12.8817)
2022-11-25 00:59:40,100:INFO: Dataset: univ                Batch: 14/15	Loss 11.1973 (12.7586)
2022-11-25 00:59:40,101:INFO: Dataset: univ                Batch: 15/15	Loss 11.1872 (12.7393)
2022-11-25 00:59:40,356:INFO: Dataset: zara1               Batch: 1/8	Loss 33.2630 (33.2630)
2022-11-25 00:59:40,358:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2268 (33.2437)
2022-11-25 00:59:40,360:INFO: Dataset: zara1               Batch: 3/8	Loss 33.3056 (33.2643)
2022-11-25 00:59:40,363:INFO: Dataset: zara1               Batch: 4/8	Loss 33.0772 (33.2201)
2022-11-25 00:59:40,364:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2737 (33.0146)
2022-11-25 00:59:40,411:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7483 (32.7824)
2022-11-25 00:59:40,415:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0113 (32.5366)
2022-11-25 00:59:40,418:INFO: Dataset: zara1               Batch: 8/8	Loss 30.1845 (32.3249)
2022-11-25 00:59:40,703:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6061 (13.6061)
2022-11-25 00:59:40,705:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0077 (13.8184)
2022-11-25 00:59:40,747:INFO: Dataset: zara2               Batch:  3/18	Loss 14.5914 (14.0602)
2022-11-25 00:59:40,748:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9248 (14.0257)
2022-11-25 00:59:40,749:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8417 (13.9894)
2022-11-25 00:59:40,754:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0711 (14.0032)
2022-11-25 00:59:40,755:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5604 (13.9356)
2022-11-25 00:59:40,756:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7256 (13.9075)
2022-11-25 00:59:40,757:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3032 (13.8395)
2022-11-25 00:59:40,758:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3811 (13.7964)
2022-11-25 00:59:40,760:INFO: Dataset: zara2               Batch: 11/18	Loss 12.7449 (13.6926)
2022-11-25 00:59:40,761:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9231 (13.6296)
2022-11-25 00:59:40,762:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6272 (13.5494)
2022-11-25 00:59:40,763:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6362 (13.4856)
2022-11-25 00:59:40,765:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3184 (13.4037)
2022-11-25 00:59:40,766:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8064 (13.3003)
2022-11-25 00:59:40,768:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6261 (13.2070)
2022-11-25 00:59:40,770:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3436 (13.1278)
2022-11-25 00:59:40,818:INFO: - Computing ADE (validation o)
2022-11-25 00:59:41,090:INFO: 		 ADE on eth                       dataset:	 1.0785675048828125
2022-11-25 00:59:41,091:INFO: Average validation o:	ADE  1.0786	FDE  2.1500
2022-11-25 00:59:41,091:INFO: - Computing loss (validation)
2022-11-25 00:59:41,285:INFO: Dataset: hotel               Batch: 1/2	Loss 35.0019 (35.0019)
2022-11-25 00:59:41,286:INFO: Dataset: hotel               Batch: 2/2	Loss 35.2255 (35.0195)
2022-11-25 00:59:41,541:INFO: Dataset: univ                Batch: 1/3	Loss 12.6761 (12.6761)
2022-11-25 00:59:41,542:INFO: Dataset: univ                Batch: 2/3	Loss 12.7409 (12.7087)
2022-11-25 00:59:41,543:INFO: Dataset: univ                Batch: 3/3	Loss 12.5819 (12.6695)
2022-11-25 00:59:41,789:INFO: Dataset: zara1               Batch: 1/2	Loss 30.3659 (30.3659)
2022-11-25 00:59:41,792:INFO: Dataset: zara1               Batch: 2/2	Loss 30.2299 (30.3323)
2022-11-25 00:59:42,041:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4479 (11.4479)
2022-11-25 00:59:42,042:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3338 (11.3901)
2022-11-25 00:59:42,046:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4611 (11.4136)
2022-11-25 00:59:42,047:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4850 (11.4319)
2022-11-25 00:59:42,047:INFO: Dataset: zara2               Batch: 5/5	Loss 11.1878 (11.3861)
2022-11-25 00:59:42,103:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_614.pth.tar
2022-11-25 00:59:42,103:INFO: 
===> EPOCH: 615 (P4)
2022-11-25 00:59:42,103:INFO: - Computing loss (training)
2022-11-25 00:59:42,316:INFO: Dataset: hotel               Batch: 1/4	Loss 30.5997 (30.5997)
2022-11-25 00:59:42,318:INFO: Dataset: hotel               Batch: 2/4	Loss 29.2947 (29.9692)
2022-11-25 00:59:42,319:INFO: Dataset: hotel               Batch: 3/4	Loss 30.9635 (30.3122)
2022-11-25 00:59:42,322:INFO: Dataset: hotel               Batch: 4/4	Loss 29.4100 (30.1622)
2022-11-25 00:59:42,580:INFO: Dataset: univ                Batch:  1/15	Loss 13.2768 (13.2768)
2022-11-25 00:59:42,582:INFO: Dataset: univ                Batch:  2/15	Loss 13.3987 (13.3375)
2022-11-25 00:59:42,585:INFO: Dataset: univ                Batch:  3/15	Loss 13.5100 (13.3964)
2022-11-25 00:59:42,588:INFO: Dataset: univ                Batch:  4/15	Loss 13.4318 (13.4058)
2022-11-25 00:59:42,608:INFO: Dataset: univ                Batch:  5/15	Loss 13.7347 (13.4669)
2022-11-25 00:59:42,637:INFO: Dataset: univ                Batch:  6/15	Loss 13.2616 (13.4319)
2022-11-25 00:59:42,638:INFO: Dataset: univ                Batch:  7/15	Loss 13.0834 (13.3829)
2022-11-25 00:59:42,639:INFO: Dataset: univ                Batch:  8/15	Loss 12.8612 (13.3191)
2022-11-25 00:59:42,640:INFO: Dataset: univ                Batch:  9/15	Loss 12.6715 (13.2497)
2022-11-25 00:59:42,641:INFO: Dataset: univ                Batch: 10/15	Loss 12.3312 (13.1532)
2022-11-25 00:59:42,642:INFO: Dataset: univ                Batch: 11/15	Loss 12.0763 (13.0546)
2022-11-25 00:59:42,644:INFO: Dataset: univ                Batch: 12/15	Loss 12.1117 (12.9846)
2022-11-25 00:59:42,645:INFO: Dataset: univ                Batch: 13/15	Loss 11.4889 (12.8598)
2022-11-25 00:59:42,646:INFO: Dataset: univ                Batch: 14/15	Loss 11.4029 (12.7616)
2022-11-25 00:59:42,647:INFO: Dataset: univ                Batch: 15/15	Loss 10.9241 (12.7364)
2022-11-25 00:59:42,914:INFO: Dataset: zara1               Batch: 1/8	Loss 33.3092 (33.3092)
2022-11-25 00:59:42,917:INFO: Dataset: zara1               Batch: 2/8	Loss 33.3497 (33.3287)
2022-11-25 00:59:42,918:INFO: Dataset: zara1               Batch: 3/8	Loss 32.8920 (33.1800)
2022-11-25 00:59:42,920:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7391 (33.0698)
2022-11-25 00:59:42,923:INFO: Dataset: zara1               Batch: 5/8	Loss 32.1900 (32.8826)
2022-11-25 00:59:42,937:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8451 (32.7018)
2022-11-25 00:59:42,938:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0853 (32.4708)
2022-11-25 00:59:42,938:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4714 (32.2393)
2022-11-25 00:59:43,204:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4706 (13.4706)
2022-11-25 00:59:43,206:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1106 (13.7939)
2022-11-25 00:59:43,208:INFO: Dataset: zara2               Batch:  3/18	Loss 14.1789 (13.9263)
2022-11-25 00:59:43,212:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9509 (13.9330)
2022-11-25 00:59:43,237:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7412 (13.8986)
2022-11-25 00:59:43,245:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4910 (13.8406)
2022-11-25 00:59:43,246:INFO: Dataset: zara2               Batch:  7/18	Loss 13.9557 (13.8571)
2022-11-25 00:59:43,247:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6435 (13.8304)
2022-11-25 00:59:43,248:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6367 (13.8079)
2022-11-25 00:59:43,249:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3378 (13.7604)
2022-11-25 00:59:43,250:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9015 (13.6740)
2022-11-25 00:59:43,252:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9351 (13.6188)
2022-11-25 00:59:43,253:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7946 (13.5536)
2022-11-25 00:59:43,254:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2813 (13.4597)
2022-11-25 00:59:43,255:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3771 (13.3909)
2022-11-25 00:59:43,256:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7503 (13.2976)
2022-11-25 00:59:43,257:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8537 (13.2144)
2022-11-25 00:59:43,258:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6454 (13.1306)
2022-11-25 00:59:43,318:INFO: - Computing ADE (validation o)
2022-11-25 00:59:43,599:INFO: 		 ADE on eth                       dataset:	 1.0902377367019653
2022-11-25 00:59:43,599:INFO: Average validation o:	ADE  1.0902	FDE  2.2307
2022-11-25 00:59:43,599:INFO: - Computing loss (validation)
2022-11-25 00:59:43,792:INFO: Dataset: hotel               Batch: 1/2	Loss 35.0423 (35.0423)
2022-11-25 00:59:43,794:INFO: Dataset: hotel               Batch: 2/2	Loss 33.9703 (34.9399)
2022-11-25 00:59:44,069:INFO: Dataset: univ                Batch: 1/3	Loss 12.8315 (12.8315)
2022-11-25 00:59:44,073:INFO: Dataset: univ                Batch: 2/3	Loss 12.4265 (12.6201)
2022-11-25 00:59:44,074:INFO: Dataset: univ                Batch: 3/3	Loss 12.4933 (12.5814)
2022-11-25 00:59:44,330:INFO: Dataset: zara1               Batch: 1/2	Loss 30.2968 (30.2968)
2022-11-25 00:59:44,331:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1634 (30.2591)
2022-11-25 00:59:44,593:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5716 (11.5716)
2022-11-25 00:59:44,596:INFO: Dataset: zara2               Batch: 2/5	Loss 11.2866 (11.4356)
2022-11-25 00:59:44,596:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3676 (11.4125)
2022-11-25 00:59:44,597:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3995 (11.4093)
2022-11-25 00:59:44,598:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4538 (11.4179)
2022-11-25 00:59:44,653:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_615.pth.tar
2022-11-25 00:59:44,654:INFO: 
===> EPOCH: 616 (P4)
2022-11-25 00:59:44,654:INFO: - Computing loss (training)
2022-11-25 00:59:44,864:INFO: Dataset: hotel               Batch: 1/4	Loss 30.4710 (30.4710)
2022-11-25 00:59:44,865:INFO: Dataset: hotel               Batch: 2/4	Loss 29.6392 (30.0571)
2022-11-25 00:59:44,868:INFO: Dataset: hotel               Batch: 3/4	Loss 29.8095 (29.9748)
2022-11-25 00:59:44,869:INFO: Dataset: hotel               Batch: 4/4	Loss 30.1448 (30.0037)
2022-11-25 00:59:45,132:INFO: Dataset: univ                Batch:  1/15	Loss 13.3270 (13.3270)
2022-11-25 00:59:45,134:INFO: Dataset: univ                Batch:  2/15	Loss 13.5246 (13.4205)
2022-11-25 00:59:45,137:INFO: Dataset: univ                Batch:  3/15	Loss 13.5494 (13.4631)
2022-11-25 00:59:45,139:INFO: Dataset: univ                Batch:  4/15	Loss 13.6145 (13.5008)
2022-11-25 00:59:45,195:INFO: Dataset: univ                Batch:  5/15	Loss 13.3869 (13.4784)
2022-11-25 00:59:45,203:INFO: Dataset: univ                Batch:  6/15	Loss 13.4518 (13.4739)
2022-11-25 00:59:45,204:INFO: Dataset: univ                Batch:  7/15	Loss 13.1346 (13.4178)
2022-11-25 00:59:45,205:INFO: Dataset: univ                Batch:  8/15	Loss 12.8128 (13.3359)
2022-11-25 00:59:45,206:INFO: Dataset: univ                Batch:  9/15	Loss 12.5815 (13.2522)
2022-11-25 00:59:45,207:INFO: Dataset: univ                Batch: 10/15	Loss 12.4083 (13.1763)
2022-11-25 00:59:45,209:INFO: Dataset: univ                Batch: 11/15	Loss 12.1191 (13.0724)
2022-11-25 00:59:45,210:INFO: Dataset: univ                Batch: 12/15	Loss 11.8943 (12.9774)
2022-11-25 00:59:45,211:INFO: Dataset: univ                Batch: 13/15	Loss 11.4227 (12.8511)
2022-11-25 00:59:45,212:INFO: Dataset: univ                Batch: 14/15	Loss 10.9347 (12.7179)
2022-11-25 00:59:45,213:INFO: Dataset: univ                Batch: 15/15	Loss 10.6284 (12.6936)
2022-11-25 00:59:45,500:INFO: Dataset: zara1               Batch: 1/8	Loss 32.9913 (32.9913)
2022-11-25 00:59:45,502:INFO: Dataset: zara1               Batch: 2/8	Loss 33.2859 (33.1433)
2022-11-25 00:59:45,505:INFO: Dataset: zara1               Batch: 3/8	Loss 33.2816 (33.1831)
2022-11-25 00:59:45,506:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5260 (32.9973)
2022-11-25 00:59:45,509:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0192 (32.8217)
2022-11-25 00:59:45,556:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7685 (32.6405)
2022-11-25 00:59:45,559:INFO: Dataset: zara1               Batch: 7/8	Loss 31.1851 (32.4305)
2022-11-25 00:59:45,563:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7946 (32.2506)
2022-11-25 00:59:45,847:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9694 (13.9694)
2022-11-25 00:59:45,851:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6022 (13.7891)
2022-11-25 00:59:45,852:INFO: Dataset: zara2               Batch:  3/18	Loss 14.2632 (13.9416)
2022-11-25 00:59:45,857:INFO: Dataset: zara2               Batch:  4/18	Loss 14.1765 (13.9982)
2022-11-25 00:59:45,911:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8908 (13.9769)
2022-11-25 00:59:45,917:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7513 (13.9435)
2022-11-25 00:59:45,918:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6698 (13.9035)
2022-11-25 00:59:45,919:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5451 (13.8596)
2022-11-25 00:59:45,920:INFO: Dataset: zara2               Batch:  9/18	Loss 13.0577 (13.7702)
2022-11-25 00:59:45,921:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5185 (13.7446)
2022-11-25 00:59:45,922:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0206 (13.6787)
2022-11-25 00:59:45,924:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0561 (13.6276)
2022-11-25 00:59:45,925:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6406 (13.5466)
2022-11-25 00:59:45,925:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2800 (13.4660)
2022-11-25 00:59:45,926:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1277 (13.3812)
2022-11-25 00:59:45,927:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9389 (13.2891)
2022-11-25 00:59:45,928:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6107 (13.1940)
2022-11-25 00:59:45,930:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5912 (13.1256)
2022-11-25 00:59:45,978:INFO: - Computing ADE (validation o)
2022-11-25 00:59:46,262:INFO: 		 ADE on eth                       dataset:	 1.093768835067749
2022-11-25 00:59:46,263:INFO: Average validation o:	ADE  1.0938	FDE  2.2037
2022-11-25 00:59:46,263:INFO: - Computing loss (validation)
2022-11-25 00:59:46,458:INFO: Dataset: hotel               Batch: 1/2	Loss 34.9868 (34.9868)
2022-11-25 00:59:46,460:INFO: Dataset: hotel               Batch: 2/2	Loss 33.3369 (34.8629)
2022-11-25 00:59:46,730:INFO: Dataset: univ                Batch: 1/3	Loss 12.8180 (12.8180)
2022-11-25 00:59:46,733:INFO: Dataset: univ                Batch: 2/3	Loss 12.5183 (12.6557)
2022-11-25 00:59:46,733:INFO: Dataset: univ                Batch: 3/3	Loss 12.5349 (12.6212)
2022-11-25 00:59:46,974:INFO: Dataset: zara1               Batch: 1/2	Loss 30.1373 (30.1373)
2022-11-25 00:59:46,975:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1913 (30.1507)
2022-11-25 00:59:47,248:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4259 (11.4259)
2022-11-25 00:59:47,250:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3776 (11.4021)
2022-11-25 00:59:47,250:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5787 (11.4622)
2022-11-25 00:59:47,251:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2743 (11.4164)
2022-11-25 00:59:47,252:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4652 (11.4253)
2022-11-25 00:59:47,305:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_616.pth.tar
2022-11-25 00:59:47,305:INFO: 
===> EPOCH: 617 (P4)
2022-11-25 00:59:47,306:INFO: - Computing loss (training)
2022-11-25 00:59:47,516:INFO: Dataset: hotel               Batch: 1/4	Loss 30.8959 (30.8959)
2022-11-25 00:59:47,523:INFO: Dataset: hotel               Batch: 2/4	Loss 29.4406 (30.1717)
2022-11-25 00:59:47,528:INFO: Dataset: hotel               Batch: 3/4	Loss 28.6972 (29.6975)
2022-11-25 00:59:47,529:INFO: Dataset: hotel               Batch: 4/4	Loss 30.9274 (29.9133)
2022-11-25 00:59:47,791:INFO: Dataset: univ                Batch:  1/15	Loss 13.3784 (13.3784)
2022-11-25 00:59:47,842:INFO: Dataset: univ                Batch:  2/15	Loss 13.4476 (13.4126)
2022-11-25 00:59:47,845:INFO: Dataset: univ                Batch:  3/15	Loss 13.3479 (13.3911)
2022-11-25 00:59:47,846:INFO: Dataset: univ                Batch:  4/15	Loss 13.4098 (13.3959)
2022-11-25 00:59:47,847:INFO: Dataset: univ                Batch:  5/15	Loss 13.5093 (13.4176)
2022-11-25 00:59:47,850:INFO: Dataset: univ                Batch:  6/15	Loss 13.4645 (13.4249)
2022-11-25 00:59:47,851:INFO: Dataset: univ                Batch:  7/15	Loss 13.1498 (13.3887)
2022-11-25 00:59:47,853:INFO: Dataset: univ                Batch:  8/15	Loss 12.7771 (13.3077)
2022-11-25 00:59:47,854:INFO: Dataset: univ                Batch:  9/15	Loss 12.6159 (13.2189)
2022-11-25 00:59:47,855:INFO: Dataset: univ                Batch: 10/15	Loss 12.3286 (13.1271)
2022-11-25 00:59:47,856:INFO: Dataset: univ                Batch: 11/15	Loss 12.3329 (13.0578)
2022-11-25 00:59:47,858:INFO: Dataset: univ                Batch: 12/15	Loss 11.5826 (12.9199)
2022-11-25 00:59:47,859:INFO: Dataset: univ                Batch: 13/15	Loss 11.4902 (12.8036)
2022-11-25 00:59:47,861:INFO: Dataset: univ                Batch: 14/15	Loss 11.2399 (12.6950)
2022-11-25 00:59:47,863:INFO: Dataset: univ                Batch: 15/15	Loss 11.0402 (12.6746)
2022-11-25 00:59:48,120:INFO: Dataset: zara1               Batch: 1/8	Loss 33.1467 (33.1467)
2022-11-25 00:59:48,134:INFO: Dataset: zara1               Batch: 2/8	Loss 33.1479 (33.1472)
2022-11-25 00:59:48,135:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6856 (32.9898)
2022-11-25 00:59:48,136:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5166 (32.8663)
2022-11-25 00:59:48,137:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2768 (32.7553)
2022-11-25 00:59:48,212:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8981 (32.6255)
2022-11-25 00:59:48,216:INFO: Dataset: zara1               Batch: 7/8	Loss 31.2730 (32.4275)
2022-11-25 00:59:48,219:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5035 (32.2189)
2022-11-25 00:59:48,515:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7483 (13.7483)
2022-11-25 00:59:48,518:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6626 (13.7039)
2022-11-25 00:59:48,519:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8784 (13.7619)
2022-11-25 00:59:48,523:INFO: Dataset: zara2               Batch:  4/18	Loss 14.0537 (13.8408)
2022-11-25 00:59:48,561:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8184 (13.8367)
2022-11-25 00:59:48,567:INFO: Dataset: zara2               Batch:  6/18	Loss 14.0251 (13.8718)
2022-11-25 00:59:48,568:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7066 (13.8492)
2022-11-25 00:59:48,569:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4137 (13.7949)
2022-11-25 00:59:48,570:INFO: Dataset: zara2               Batch:  9/18	Loss 13.6030 (13.7773)
2022-11-25 00:59:48,571:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2085 (13.7175)
2022-11-25 00:59:48,574:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9185 (13.6355)
2022-11-25 00:59:48,575:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9165 (13.5812)
2022-11-25 00:59:48,576:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9565 (13.5259)
2022-11-25 00:59:48,577:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5315 (13.4599)
2022-11-25 00:59:48,578:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1554 (13.3765)
2022-11-25 00:59:48,579:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0423 (13.2898)
2022-11-25 00:59:48,580:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9183 (13.2100)
2022-11-25 00:59:48,582:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3692 (13.1205)
2022-11-25 00:59:48,626:INFO: - Computing ADE (validation o)
2022-11-25 00:59:48,904:INFO: 		 ADE on eth                       dataset:	 1.0400609970092773
2022-11-25 00:59:48,904:INFO: Average validation o:	ADE  1.0401	FDE  2.1196
2022-11-25 00:59:48,905:INFO: - Computing loss (validation)
2022-11-25 00:59:49,104:INFO: Dataset: hotel               Batch: 1/2	Loss 34.9169 (34.9169)
2022-11-25 00:59:49,105:INFO: Dataset: hotel               Batch: 2/2	Loss 32.7175 (34.7742)
2022-11-25 00:59:49,369:INFO: Dataset: univ                Batch: 1/3	Loss 12.3885 (12.3885)
2022-11-25 00:59:49,371:INFO: Dataset: univ                Batch: 2/3	Loss 12.6638 (12.5237)
2022-11-25 00:59:49,372:INFO: Dataset: univ                Batch: 3/3	Loss 12.4886 (12.5122)
2022-11-25 00:59:49,622:INFO: Dataset: zara1               Batch: 1/2	Loss 30.1567 (30.1567)
2022-11-25 00:59:49,624:INFO: Dataset: zara1               Batch: 2/2	Loss 30.1312 (30.1503)
2022-11-25 00:59:49,906:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6091 (11.6091)
2022-11-25 00:59:49,907:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4238 (11.5202)
2022-11-25 00:59:49,908:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4974 (11.5126)
2022-11-25 00:59:49,909:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3111 (11.4648)
2022-11-25 00:59:49,910:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4307 (11.4578)
2022-11-25 00:59:49,963:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_617.pth.tar
2022-11-25 00:59:49,963:INFO: 
===> EPOCH: 618 (P4)
2022-11-25 00:59:49,963:INFO: - Computing loss (training)
2022-11-25 00:59:50,171:INFO: Dataset: hotel               Batch: 1/4	Loss 30.3582 (30.3582)
2022-11-25 00:59:50,172:INFO: Dataset: hotel               Batch: 2/4	Loss 29.4227 (29.8982)
2022-11-25 00:59:50,182:INFO: Dataset: hotel               Batch: 3/4	Loss 30.7347 (30.1801)
2022-11-25 00:59:50,184:INFO: Dataset: hotel               Batch: 4/4	Loss 27.4470 (29.7474)
2022-11-25 00:59:50,466:INFO: Dataset: univ                Batch:  1/15	Loss 13.2037 (13.2037)
2022-11-25 00:59:50,471:INFO: Dataset: univ                Batch:  2/15	Loss 13.2990 (13.2520)
2022-11-25 00:59:50,528:INFO: Dataset: univ                Batch:  3/15	Loss 13.2854 (13.2628)
2022-11-25 00:59:50,529:INFO: Dataset: univ                Batch:  4/15	Loss 13.5133 (13.3252)
2022-11-25 00:59:50,530:INFO: Dataset: univ                Batch:  5/15	Loss 13.2864 (13.3182)
2022-11-25 00:59:50,533:INFO: Dataset: univ                Batch:  6/15	Loss 13.2544 (13.3077)
2022-11-25 00:59:50,534:INFO: Dataset: univ                Batch:  7/15	Loss 13.1425 (13.2863)
2022-11-25 00:59:50,535:INFO: Dataset: univ                Batch:  8/15	Loss 12.7526 (13.2193)
2022-11-25 00:59:50,536:INFO: Dataset: univ                Batch:  9/15	Loss 12.4568 (13.1250)
2022-11-25 00:59:50,537:INFO: Dataset: univ                Batch: 10/15	Loss 12.4810 (13.0627)
2022-11-25 00:59:50,538:INFO: Dataset: univ                Batch: 11/15	Loss 11.9763 (12.9657)
2022-11-25 00:59:50,540:INFO: Dataset: univ                Batch: 12/15	Loss 11.7878 (12.8638)
2022-11-25 00:59:50,541:INFO: Dataset: univ                Batch: 13/15	Loss 11.5050 (12.7440)
2022-11-25 00:59:50,543:INFO: Dataset: univ                Batch: 14/15	Loss 11.1162 (12.6247)
2022-11-25 00:59:50,543:INFO: Dataset: univ                Batch: 15/15	Loss 11.8045 (12.6192)
2022-11-25 00:59:50,815:INFO: Dataset: zara1               Batch: 1/8	Loss 33.0757 (33.0757)
2022-11-25 00:59:50,827:INFO: Dataset: zara1               Batch: 2/8	Loss 32.9960 (33.0349)
2022-11-25 00:59:50,828:INFO: Dataset: zara1               Batch: 3/8	Loss 32.9068 (32.9959)
2022-11-25 00:59:50,829:INFO: Dataset: zara1               Batch: 4/8	Loss 32.8399 (32.9554)
2022-11-25 00:59:50,835:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3583 (32.8380)
2022-11-25 00:59:50,878:INFO: Dataset: zara1               Batch: 6/8	Loss 31.7552 (32.6319)
2022-11-25 00:59:50,882:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0608 (32.3986)
2022-11-25 00:59:50,885:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4385 (32.1892)
2022-11-25 00:59:51,160:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9722 (13.9722)
2022-11-25 00:59:51,211:INFO: Dataset: zara2               Batch:  2/18	Loss 14.1013 (14.0327)
2022-11-25 00:59:51,212:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8373 (13.9673)
2022-11-25 00:59:51,214:INFO: Dataset: zara2               Batch:  4/18	Loss 14.3363 (14.0564)
2022-11-25 00:59:51,215:INFO: Dataset: zara2               Batch:  5/18	Loss 14.2942 (14.1010)
2022-11-25 00:59:51,218:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8108 (14.0496)
2022-11-25 00:59:51,219:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5597 (13.9771)
2022-11-25 00:59:51,220:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5011 (13.9160)
2022-11-25 00:59:51,221:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3488 (13.8603)
2022-11-25 00:59:51,222:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1007 (13.7878)
2022-11-25 00:59:51,223:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8959 (13.7070)
2022-11-25 00:59:51,225:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0018 (13.6492)
2022-11-25 00:59:51,226:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4948 (13.5647)
2022-11-25 00:59:51,227:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4284 (13.4827)
2022-11-25 00:59:51,229:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0925 (13.3948)
2022-11-25 00:59:51,230:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8502 (13.2923)
2022-11-25 00:59:51,232:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6527 (13.1998)
2022-11-25 00:59:51,233:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3467 (13.1119)
2022-11-25 00:59:51,279:INFO: - Computing ADE (validation o)
2022-11-25 00:59:51,549:INFO: 		 ADE on eth                       dataset:	 1.0137748718261719
2022-11-25 00:59:51,549:INFO: Average validation o:	ADE  1.0138	FDE  2.0938
2022-11-25 00:59:51,550:INFO: - Computing loss (validation)
2022-11-25 00:59:51,753:INFO: Dataset: hotel               Batch: 1/2	Loss 35.0780 (35.0780)
2022-11-25 00:59:51,755:INFO: Dataset: hotel               Batch: 2/2	Loss 32.3253 (34.8995)
2022-11-25 00:59:52,019:INFO: Dataset: univ                Batch: 1/3	Loss 12.5445 (12.5445)
2022-11-25 00:59:52,025:INFO: Dataset: univ                Batch: 2/3	Loss 12.5027 (12.5225)
2022-11-25 00:59:52,026:INFO: Dataset: univ                Batch: 3/3	Loss 12.3594 (12.4715)
2022-11-25 00:59:52,266:INFO: Dataset: zara1               Batch: 1/2	Loss 30.1788 (30.1788)
2022-11-25 00:59:52,268:INFO: Dataset: zara1               Batch: 2/2	Loss 29.7044 (30.0629)
2022-11-25 00:59:52,533:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4264 (11.4264)
2022-11-25 00:59:52,536:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4882 (11.4572)
2022-11-25 00:59:52,536:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5460 (11.4866)
2022-11-25 00:59:52,538:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4115 (11.4688)
2022-11-25 00:59:52,538:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3844 (11.4520)
2022-11-25 00:59:52,592:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_618.pth.tar
2022-11-25 00:59:52,592:INFO: 
===> EPOCH: 619 (P4)
2022-11-25 00:59:52,593:INFO: - Computing loss (training)
2022-11-25 00:59:52,802:INFO: Dataset: hotel               Batch: 1/4	Loss 30.2814 (30.2814)
2022-11-25 00:59:52,804:INFO: Dataset: hotel               Batch: 2/4	Loss 29.7306 (29.9863)
2022-11-25 00:59:52,806:INFO: Dataset: hotel               Batch: 3/4	Loss 31.0996 (30.3724)
2022-11-25 00:59:52,809:INFO: Dataset: hotel               Batch: 4/4	Loss 27.0639 (29.8705)
2022-11-25 00:59:53,118:INFO: Dataset: univ                Batch:  1/15	Loss 13.2127 (13.2127)
2022-11-25 00:59:53,119:INFO: Dataset: univ                Batch:  2/15	Loss 13.3774 (13.2949)
2022-11-25 00:59:53,121:INFO: Dataset: univ                Batch:  3/15	Loss 13.3667 (13.3193)
2022-11-25 00:59:53,122:INFO: Dataset: univ                Batch:  4/15	Loss 13.3881 (13.3369)
2022-11-25 00:59:53,123:INFO: Dataset: univ                Batch:  5/15	Loss 13.3117 (13.3322)
2022-11-25 00:59:53,126:INFO: Dataset: univ                Batch:  6/15	Loss 13.0490 (13.2811)
2022-11-25 00:59:53,128:INFO: Dataset: univ                Batch:  7/15	Loss 13.1173 (13.2559)
2022-11-25 00:59:53,129:INFO: Dataset: univ                Batch:  8/15	Loss 12.9548 (13.2220)
2022-11-25 00:59:53,130:INFO: Dataset: univ                Batch:  9/15	Loss 12.5741 (13.1488)
2022-11-25 00:59:53,131:INFO: Dataset: univ                Batch: 10/15	Loss 12.4665 (13.0790)
2022-11-25 00:59:53,132:INFO: Dataset: univ                Batch: 11/15	Loss 11.9947 (12.9700)
2022-11-25 00:59:53,134:INFO: Dataset: univ                Batch: 12/15	Loss 11.7003 (12.8558)
2022-11-25 00:59:53,135:INFO: Dataset: univ                Batch: 13/15	Loss 11.3372 (12.7342)
2022-11-25 00:59:53,137:INFO: Dataset: univ                Batch: 14/15	Loss 11.0720 (12.6101)
2022-11-25 00:59:53,139:INFO: Dataset: univ                Batch: 15/15	Loss 10.8175 (12.5806)
2022-11-25 00:59:53,408:INFO: Dataset: zara1               Batch: 1/8	Loss 32.9859 (32.9859)
2022-11-25 00:59:53,411:INFO: Dataset: zara1               Batch: 2/8	Loss 33.0204 (33.0024)
2022-11-25 00:59:53,413:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6773 (32.8882)
2022-11-25 00:59:53,415:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7517 (32.8520)
2022-11-25 00:59:53,418:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3238 (32.7590)
2022-11-25 00:59:53,445:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8058 (32.6015)
2022-11-25 00:59:53,447:INFO: Dataset: zara1               Batch: 7/8	Loss 31.2258 (32.4168)
2022-11-25 00:59:53,448:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3112 (32.1852)
2022-11-25 00:59:53,720:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4770 (13.4770)
2022-11-25 00:59:53,721:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6144 (13.5484)
2022-11-25 00:59:53,722:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9335 (13.6671)
2022-11-25 00:59:53,726:INFO: Dataset: zara2               Batch:  4/18	Loss 14.2701 (13.8216)
2022-11-25 00:59:53,767:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6460 (13.7877)
2022-11-25 00:59:53,776:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6352 (13.7639)
2022-11-25 00:59:53,777:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8588 (13.7774)
2022-11-25 00:59:53,778:INFO: Dataset: zara2               Batch:  8/18	Loss 13.7287 (13.7719)
2022-11-25 00:59:53,779:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3134 (13.7208)
2022-11-25 00:59:53,780:INFO: Dataset: zara2               Batch: 10/18	Loss 13.5090 (13.7023)
2022-11-25 00:59:53,781:INFO: Dataset: zara2               Batch: 11/18	Loss 13.2330 (13.6569)
2022-11-25 00:59:53,782:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6937 (13.5748)
2022-11-25 00:59:53,783:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4410 (13.4961)
2022-11-25 00:59:53,784:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6948 (13.4439)
2022-11-25 00:59:53,785:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1521 (13.3535)
2022-11-25 00:59:53,786:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1868 (13.2757)
2022-11-25 00:59:53,787:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6201 (13.1791)
2022-11-25 00:59:53,789:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4571 (13.0877)
2022-11-25 00:59:53,843:INFO: - Computing ADE (validation o)
2022-11-25 00:59:54,109:INFO: 		 ADE on eth                       dataset:	 1.0280570983886719
2022-11-25 00:59:54,109:INFO: Average validation o:	ADE  1.0281	FDE  2.1137
2022-11-25 00:59:54,110:INFO: - Computing loss (validation)
2022-11-25 00:59:54,301:INFO: Dataset: hotel               Batch: 1/2	Loss 34.9245 (34.9245)
2022-11-25 00:59:54,302:INFO: Dataset: hotel               Batch: 2/2	Loss 31.3059 (34.6898)
2022-11-25 00:59:54,561:INFO: Dataset: univ                Batch: 1/3	Loss 12.2458 (12.2458)
2022-11-25 00:59:54,563:INFO: Dataset: univ                Batch: 2/3	Loss 12.3990 (12.3163)
2022-11-25 00:59:54,573:INFO: Dataset: univ                Batch: 3/3	Loss 12.7982 (12.4460)
2022-11-25 00:59:54,818:INFO: Dataset: zara1               Batch: 1/2	Loss 30.0715 (30.0715)
2022-11-25 00:59:54,822:INFO: Dataset: zara1               Batch: 2/2	Loss 30.0569 (30.0678)
2022-11-25 00:59:55,067:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5332 (11.5332)
2022-11-25 00:59:55,069:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4949 (11.5145)
2022-11-25 00:59:55,069:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4615 (11.4969)
2022-11-25 00:59:55,071:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3466 (11.4561)
2022-11-25 00:59:55,071:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5203 (11.4687)
2022-11-25 00:59:55,124:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_619.pth.tar
2022-11-25 00:59:55,125:INFO: 
===> EPOCH: 620 (P4)
2022-11-25 00:59:55,125:INFO: - Computing loss (training)
2022-11-25 00:59:55,345:INFO: Dataset: hotel               Batch: 1/4	Loss 29.1246 (29.1246)
2022-11-25 00:59:55,346:INFO: Dataset: hotel               Batch: 2/4	Loss 30.7775 (29.9753)
2022-11-25 00:59:55,348:INFO: Dataset: hotel               Batch: 3/4	Loss 28.9821 (29.6421)
2022-11-25 00:59:55,349:INFO: Dataset: hotel               Batch: 4/4	Loss 29.4553 (29.6073)
2022-11-25 00:59:55,620:INFO: Dataset: univ                Batch:  1/15	Loss 13.1075 (13.1075)
2022-11-25 00:59:55,623:INFO: Dataset: univ                Batch:  2/15	Loss 13.3516 (13.2283)
2022-11-25 00:59:55,625:INFO: Dataset: univ                Batch:  3/15	Loss 13.3318 (13.2642)
2022-11-25 00:59:55,628:INFO: Dataset: univ                Batch:  4/15	Loss 13.1098 (13.2224)
2022-11-25 00:59:55,683:INFO: Dataset: univ                Batch:  5/15	Loss 13.6874 (13.3013)
2022-11-25 00:59:55,687:INFO: Dataset: univ                Batch:  6/15	Loss 12.9730 (13.2455)
2022-11-25 00:59:55,688:INFO: Dataset: univ                Batch:  7/15	Loss 12.8895 (13.1920)
2022-11-25 00:59:55,689:INFO: Dataset: univ                Batch:  8/15	Loss 12.8033 (13.1452)
2022-11-25 00:59:55,690:INFO: Dataset: univ                Batch:  9/15	Loss 12.7970 (13.1084)
2022-11-25 00:59:55,691:INFO: Dataset: univ                Batch: 10/15	Loss 12.1125 (12.9953)
2022-11-25 00:59:55,694:INFO: Dataset: univ                Batch: 11/15	Loss 12.2570 (12.9284)
2022-11-25 00:59:55,695:INFO: Dataset: univ                Batch: 12/15	Loss 11.7644 (12.8418)
2022-11-25 00:59:55,696:INFO: Dataset: univ                Batch: 13/15	Loss 11.2962 (12.7135)
2022-11-25 00:59:55,697:INFO: Dataset: univ                Batch: 14/15	Loss 11.1491 (12.6182)
2022-11-25 00:59:55,698:INFO: Dataset: univ                Batch: 15/15	Loss 10.6833 (12.5873)
2022-11-25 00:59:55,957:INFO: Dataset: zara1               Batch: 1/8	Loss 32.9567 (32.9567)
2022-11-25 00:59:55,959:INFO: Dataset: zara1               Batch: 2/8	Loss 32.8705 (32.9126)
2022-11-25 00:59:55,961:INFO: Dataset: zara1               Batch: 3/8	Loss 32.7894 (32.8716)
2022-11-25 00:59:55,964:INFO: Dataset: zara1               Batch: 4/8	Loss 32.7429 (32.8386)
2022-11-25 00:59:55,966:INFO: Dataset: zara1               Batch: 5/8	Loss 32.2165 (32.7184)
2022-11-25 00:59:56,032:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0154 (32.6151)
2022-11-25 00:59:56,036:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0342 (32.3701)
2022-11-25 00:59:56,039:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2460 (32.1644)
2022-11-25 00:59:56,315:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8441 (13.8441)
2022-11-25 00:59:56,322:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8436 (13.8439)
2022-11-25 00:59:56,323:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9767 (13.8862)
2022-11-25 00:59:56,324:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7260 (13.8445)
2022-11-25 00:59:56,375:INFO: Dataset: zara2               Batch:  5/18	Loss 13.9116 (13.8580)
2022-11-25 00:59:56,383:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7003 (13.8305)
2022-11-25 00:59:56,384:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6058 (13.7991)
2022-11-25 00:59:56,385:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5755 (13.7690)
2022-11-25 00:59:56,386:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2136 (13.7039)
2022-11-25 00:59:56,386:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3341 (13.6673)
2022-11-25 00:59:56,389:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1505 (13.6187)
2022-11-25 00:59:56,389:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7131 (13.5413)
2022-11-25 00:59:56,390:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6366 (13.4746)
2022-11-25 00:59:56,391:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3149 (13.3941)
2022-11-25 00:59:56,392:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2434 (13.3200)
2022-11-25 00:59:56,393:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0872 (13.2334)
2022-11-25 00:59:56,394:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7856 (13.1514)
2022-11-25 00:59:56,396:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3690 (13.0598)
2022-11-25 00:59:56,442:INFO: - Computing ADE (validation o)
2022-11-25 00:59:56,706:INFO: 		 ADE on eth                       dataset:	 1.1009989976882935
2022-11-25 00:59:56,706:INFO: Average validation o:	ADE  1.1010	FDE  2.2465
2022-11-25 00:59:56,707:INFO: - Computing loss (validation)
2022-11-25 00:59:56,906:INFO: Dataset: hotel               Batch: 1/2	Loss 34.4393 (34.4393)
2022-11-25 00:59:56,906:INFO: Dataset: hotel               Batch: 2/2	Loss 38.7813 (34.7357)
2022-11-25 00:59:57,184:INFO: Dataset: univ                Batch: 1/3	Loss 12.4044 (12.4044)
2022-11-25 00:59:57,187:INFO: Dataset: univ                Batch: 2/3	Loss 12.3540 (12.3796)
2022-11-25 00:59:57,187:INFO: Dataset: univ                Batch: 3/3	Loss 12.8881 (12.4996)
2022-11-25 00:59:57,428:INFO: Dataset: zara1               Batch: 1/2	Loss 30.1132 (30.1132)
2022-11-25 00:59:57,429:INFO: Dataset: zara1               Batch: 2/2	Loss 29.6868 (30.0036)
2022-11-25 00:59:57,668:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3747 (11.3747)
2022-11-25 00:59:57,685:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4292 (11.4006)
2022-11-25 00:59:57,686:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4739 (11.4260)
2022-11-25 00:59:57,686:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4614 (11.4349)
2022-11-25 00:59:57,687:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4859 (11.4449)
2022-11-25 00:59:57,741:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_620.pth.tar
2022-11-25 00:59:57,742:INFO: 
===> EPOCH: 621 (P4)
2022-11-25 00:59:57,742:INFO: - Computing loss (training)
2022-11-25 00:59:57,945:INFO: Dataset: hotel               Batch: 1/4	Loss 29.6767 (29.6767)
2022-11-25 00:59:57,962:INFO: Dataset: hotel               Batch: 2/4	Loss 29.1154 (29.3839)
2022-11-25 00:59:57,968:INFO: Dataset: hotel               Batch: 3/4	Loss 30.5672 (29.7908)
2022-11-25 00:59:57,969:INFO: Dataset: hotel               Batch: 4/4	Loss 29.0710 (29.6730)
2022-11-25 00:59:58,235:INFO: Dataset: univ                Batch:  1/15	Loss 13.2842 (13.2842)
2022-11-25 00:59:58,299:INFO: Dataset: univ                Batch:  2/15	Loss 13.0591 (13.1682)
2022-11-25 00:59:58,307:INFO: Dataset: univ                Batch:  3/15	Loss 13.2366 (13.1905)
2022-11-25 00:59:58,308:INFO: Dataset: univ                Batch:  4/15	Loss 13.7987 (13.3275)
2022-11-25 00:59:58,309:INFO: Dataset: univ                Batch:  5/15	Loss 13.6218 (13.3795)
2022-11-25 00:59:58,338:INFO: Dataset: univ                Batch:  6/15	Loss 13.1142 (13.3348)
2022-11-25 00:59:58,339:INFO: Dataset: univ                Batch:  7/15	Loss 12.7610 (13.2516)
2022-11-25 00:59:58,340:INFO: Dataset: univ                Batch:  8/15	Loss 12.8630 (13.2005)
2022-11-25 00:59:58,341:INFO: Dataset: univ                Batch:  9/15	Loss 12.4453 (13.1131)
2022-11-25 00:59:58,342:INFO: Dataset: univ                Batch: 10/15	Loss 12.0752 (12.9974)
2022-11-25 00:59:58,343:INFO: Dataset: univ                Batch: 11/15	Loss 12.2907 (12.9395)
2022-11-25 00:59:58,346:INFO: Dataset: univ                Batch: 12/15	Loss 11.6082 (12.8165)
2022-11-25 00:59:58,347:INFO: Dataset: univ                Batch: 13/15	Loss 11.4507 (12.6968)
2022-11-25 00:59:58,348:INFO: Dataset: univ                Batch: 14/15	Loss 11.0763 (12.5716)
2022-11-25 00:59:58,349:INFO: Dataset: univ                Batch: 15/15	Loss 10.8182 (12.5492)
2022-11-25 00:59:58,602:INFO: Dataset: zara1               Batch: 1/8	Loss 32.7517 (32.7517)
2022-11-25 00:59:58,603:INFO: Dataset: zara1               Batch: 2/8	Loss 33.0180 (32.8819)
2022-11-25 00:59:58,605:INFO: Dataset: zara1               Batch: 3/8	Loss 32.9145 (32.8931)
2022-11-25 00:59:58,609:INFO: Dataset: zara1               Batch: 4/8	Loss 32.3913 (32.7562)
2022-11-25 00:59:58,609:INFO: Dataset: zara1               Batch: 5/8	Loss 32.3832 (32.6739)
2022-11-25 00:59:58,634:INFO: Dataset: zara1               Batch: 6/8	Loss 31.5711 (32.4938)
2022-11-25 00:59:58,635:INFO: Dataset: zara1               Batch: 7/8	Loss 31.2193 (32.3174)
2022-11-25 00:59:58,636:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2862 (32.0865)
2022-11-25 00:59:58,925:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7705 (13.7705)
2022-11-25 00:59:58,930:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9542 (13.8584)
2022-11-25 00:59:58,931:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6581 (13.7888)
2022-11-25 00:59:58,933:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9941 (13.8389)
2022-11-25 00:59:58,975:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8885 (13.8491)
2022-11-25 00:59:58,983:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7519 (13.8327)
2022-11-25 00:59:58,984:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5511 (13.7904)
2022-11-25 00:59:58,985:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4523 (13.7403)
2022-11-25 00:59:58,986:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3572 (13.7001)
2022-11-25 00:59:58,987:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1441 (13.6415)
2022-11-25 00:59:58,988:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9200 (13.5725)
2022-11-25 00:59:58,990:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7981 (13.5060)
2022-11-25 00:59:58,991:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7569 (13.4510)
2022-11-25 00:59:58,992:INFO: Dataset: zara2               Batch: 14/18	Loss 12.8299 (13.4030)
2022-11-25 00:59:58,992:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0662 (13.3113)
2022-11-25 00:59:58,993:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9365 (13.2127)
2022-11-25 00:59:58,995:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6993 (13.1396)
2022-11-25 00:59:58,996:INFO: Dataset: zara2               Batch: 18/18	Loss 11.7982 (13.0751)
2022-11-25 00:59:59,044:INFO: - Computing ADE (validation o)
2022-11-25 00:59:59,317:INFO: 		 ADE on eth                       dataset:	 1.0565475225448608
2022-11-25 00:59:59,317:INFO: Average validation o:	ADE  1.0565	FDE  2.1867
2022-11-25 00:59:59,317:INFO: - Computing loss (validation)
2022-11-25 00:59:59,509:INFO: Dataset: hotel               Batch: 1/2	Loss 34.2325 (34.2325)
2022-11-25 00:59:59,510:INFO: Dataset: hotel               Batch: 2/2	Loss 38.6487 (34.5490)
2022-11-25 00:59:59,757:INFO: Dataset: univ                Batch: 1/3	Loss 12.3588 (12.3588)
2022-11-25 00:59:59,758:INFO: Dataset: univ                Batch: 2/3	Loss 12.3154 (12.3361)
2022-11-25 00:59:59,759:INFO: Dataset: univ                Batch: 3/3	Loss 12.4769 (12.3795)
2022-11-25 01:00:00,020:INFO: Dataset: zara1               Batch: 1/2	Loss 29.9569 (29.9569)
2022-11-25 01:00:00,021:INFO: Dataset: zara1               Batch: 2/2	Loss 29.9653 (29.9589)
2022-11-25 01:00:00,297:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4300 (11.4300)
2022-11-25 01:00:00,298:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5453 (11.4881)
2022-11-25 01:00:00,299:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5065 (11.4941)
2022-11-25 01:00:00,300:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4323 (11.4792)
2022-11-25 01:00:00,301:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5193 (11.4873)
2022-11-25 01:00:00,358:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_621.pth.tar
2022-11-25 01:00:00,358:INFO: 
===> EPOCH: 622 (P4)
2022-11-25 01:00:00,359:INFO: - Computing loss (training)
2022-11-25 01:00:00,564:INFO: Dataset: hotel               Batch: 1/4	Loss 31.3297 (31.3297)
2022-11-25 01:00:00,567:INFO: Dataset: hotel               Batch: 2/4	Loss 29.2352 (30.2607)
2022-11-25 01:00:00,568:INFO: Dataset: hotel               Batch: 3/4	Loss 27.8792 (29.5160)
2022-11-25 01:00:00,570:INFO: Dataset: hotel               Batch: 4/4	Loss 28.5655 (29.3555)
2022-11-25 01:00:00,830:INFO: Dataset: univ                Batch:  1/15	Loss 13.0287 (13.0287)
2022-11-25 01:00:00,932:INFO: Dataset: univ                Batch:  2/15	Loss 13.0210 (13.0250)
2022-11-25 01:00:00,936:INFO: Dataset: univ                Batch:  3/15	Loss 13.3014 (13.1205)
2022-11-25 01:00:00,937:INFO: Dataset: univ                Batch:  4/15	Loss 13.1694 (13.1335)
2022-11-25 01:00:00,938:INFO: Dataset: univ                Batch:  5/15	Loss 13.3052 (13.1677)
2022-11-25 01:00:00,940:INFO: Dataset: univ                Batch:  6/15	Loss 13.0735 (13.1516)
2022-11-25 01:00:00,941:INFO: Dataset: univ                Batch:  7/15	Loss 13.0148 (13.1315)
2022-11-25 01:00:00,942:INFO: Dataset: univ                Batch:  8/15	Loss 12.7787 (13.0882)
2022-11-25 01:00:00,943:INFO: Dataset: univ                Batch:  9/15	Loss 12.6894 (13.0479)
2022-11-25 01:00:00,944:INFO: Dataset: univ                Batch: 10/15	Loss 12.1490 (12.9593)
2022-11-25 01:00:00,947:INFO: Dataset: univ                Batch: 11/15	Loss 11.9450 (12.8667)
2022-11-25 01:00:00,948:INFO: Dataset: univ                Batch: 12/15	Loss 11.7091 (12.7730)
2022-11-25 01:00:00,949:INFO: Dataset: univ                Batch: 13/15	Loss 11.4785 (12.6755)
2022-11-25 01:00:00,950:INFO: Dataset: univ                Batch: 14/15	Loss 10.9993 (12.5602)
2022-11-25 01:00:00,952:INFO: Dataset: univ                Batch: 15/15	Loss 10.7492 (12.5356)
2022-11-25 01:00:01,222:INFO: Dataset: zara1               Batch: 1/8	Loss 33.0127 (33.0127)
2022-11-25 01:00:01,223:INFO: Dataset: zara1               Batch: 2/8	Loss 32.8544 (32.9380)
2022-11-25 01:00:01,224:INFO: Dataset: zara1               Batch: 3/8	Loss 32.7915 (32.8852)
2022-11-25 01:00:01,225:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5266 (32.7952)
2022-11-25 01:00:01,227:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0796 (32.6418)
2022-11-25 01:00:01,258:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6440 (32.4884)
2022-11-25 01:00:01,262:INFO: Dataset: zara1               Batch: 7/8	Loss 31.2868 (32.3092)
2022-11-25 01:00:01,265:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2771 (32.0921)
2022-11-25 01:00:01,597:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8480 (13.8480)
2022-11-25 01:00:01,598:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0348 (13.9422)
2022-11-25 01:00:01,600:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9101 (13.9314)
2022-11-25 01:00:01,601:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7236 (13.8759)
2022-11-25 01:00:01,602:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5580 (13.8155)
2022-11-25 01:00:01,605:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6152 (13.7835)
2022-11-25 01:00:01,606:INFO: Dataset: zara2               Batch:  7/18	Loss 14.0179 (13.8162)
2022-11-25 01:00:01,607:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3132 (13.7536)
2022-11-25 01:00:01,608:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3473 (13.7058)
2022-11-25 01:00:01,609:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0765 (13.6451)
2022-11-25 01:00:01,610:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8568 (13.5668)
2022-11-25 01:00:01,612:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9942 (13.5226)
2022-11-25 01:00:01,613:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6340 (13.4568)
2022-11-25 01:00:01,614:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4347 (13.3806)
2022-11-25 01:00:01,616:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2906 (13.3040)
2022-11-25 01:00:01,618:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1965 (13.2352)
2022-11-25 01:00:01,619:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7994 (13.1443)
2022-11-25 01:00:01,621:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3923 (13.0757)
2022-11-25 01:00:01,670:INFO: - Computing ADE (validation o)
2022-11-25 01:00:01,942:INFO: 		 ADE on eth                       dataset:	 1.0925358533859253
2022-11-25 01:00:01,942:INFO: Average validation o:	ADE  1.0925	FDE  2.1839
2022-11-25 01:00:01,943:INFO: - Computing loss (validation)
2022-11-25 01:00:02,137:INFO: Dataset: hotel               Batch: 1/2	Loss 34.5286 (34.5286)
2022-11-25 01:00:02,138:INFO: Dataset: hotel               Batch: 2/2	Loss 35.2538 (34.5757)
2022-11-25 01:00:02,400:INFO: Dataset: univ                Batch: 1/3	Loss 12.2522 (12.2522)
2022-11-25 01:00:02,401:INFO: Dataset: univ                Batch: 2/3	Loss 12.4754 (12.3554)
2022-11-25 01:00:02,403:INFO: Dataset: univ                Batch: 3/3	Loss 12.3523 (12.3542)
2022-11-25 01:00:02,650:INFO: Dataset: zara1               Batch: 1/2	Loss 29.9522 (29.9522)
2022-11-25 01:00:02,653:INFO: Dataset: zara1               Batch: 2/2	Loss 29.7101 (29.8915)
2022-11-25 01:00:02,901:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4521 (11.4521)
2022-11-25 01:00:02,902:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5101 (11.4810)
2022-11-25 01:00:02,902:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5414 (11.5006)
2022-11-25 01:00:02,913:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4879 (11.4976)
2022-11-25 01:00:02,913:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4770 (11.4935)
2022-11-25 01:00:02,969:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_622.pth.tar
2022-11-25 01:00:02,969:INFO: 
===> EPOCH: 623 (P4)
2022-11-25 01:00:02,969:INFO: - Computing loss (training)
2022-11-25 01:00:03,180:INFO: Dataset: hotel               Batch: 1/4	Loss 29.5828 (29.5828)
2022-11-25 01:00:03,182:INFO: Dataset: hotel               Batch: 2/4	Loss 29.1705 (29.3737)
2022-11-25 01:00:03,194:INFO: Dataset: hotel               Batch: 3/4	Loss 29.7314 (29.4880)
2022-11-25 01:00:03,196:INFO: Dataset: hotel               Batch: 4/4	Loss 28.8671 (29.3824)
2022-11-25 01:00:03,463:INFO: Dataset: univ                Batch:  1/15	Loss 13.0626 (13.0626)
2022-11-25 01:00:03,466:INFO: Dataset: univ                Batch:  2/15	Loss 13.1171 (13.0902)
2022-11-25 01:00:03,468:INFO: Dataset: univ                Batch:  3/15	Loss 13.4449 (13.2041)
2022-11-25 01:00:03,469:INFO: Dataset: univ                Batch:  4/15	Loss 13.2401 (13.2137)
2022-11-25 01:00:03,531:INFO: Dataset: univ                Batch:  5/15	Loss 13.2506 (13.2210)
2022-11-25 01:00:03,537:INFO: Dataset: univ                Batch:  6/15	Loss 13.2242 (13.2214)
2022-11-25 01:00:03,538:INFO: Dataset: univ                Batch:  7/15	Loss 12.8098 (13.1593)
2022-11-25 01:00:03,539:INFO: Dataset: univ                Batch:  8/15	Loss 12.6870 (13.0996)
2022-11-25 01:00:03,540:INFO: Dataset: univ                Batch:  9/15	Loss 12.4452 (13.0276)
2022-11-25 01:00:03,541:INFO: Dataset: univ                Batch: 10/15	Loss 12.1786 (12.9461)
2022-11-25 01:00:03,544:INFO: Dataset: univ                Batch: 11/15	Loss 12.1055 (12.8788)
2022-11-25 01:00:03,545:INFO: Dataset: univ                Batch: 12/15	Loss 11.5308 (12.7628)
2022-11-25 01:00:03,546:INFO: Dataset: univ                Batch: 13/15	Loss 11.3061 (12.6471)
2022-11-25 01:00:03,547:INFO: Dataset: univ                Batch: 14/15	Loss 10.9881 (12.5355)
2022-11-25 01:00:03,548:INFO: Dataset: univ                Batch: 15/15	Loss 10.4505 (12.5090)
2022-11-25 01:00:03,821:INFO: Dataset: zara1               Batch: 1/8	Loss 32.6717 (32.6717)
2022-11-25 01:00:03,822:INFO: Dataset: zara1               Batch: 2/8	Loss 33.0681 (32.8595)
2022-11-25 01:00:03,823:INFO: Dataset: zara1               Batch: 3/8	Loss 32.8431 (32.8542)
2022-11-25 01:00:03,825:INFO: Dataset: zara1               Batch: 4/8	Loss 32.4803 (32.7639)
2022-11-25 01:00:03,828:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0572 (32.6139)
2022-11-25 01:00:03,897:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6378 (32.4596)
2022-11-25 01:00:03,901:INFO: Dataset: zara1               Batch: 7/8	Loss 31.1428 (32.2833)
2022-11-25 01:00:03,910:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3077 (32.0545)
2022-11-25 01:00:04,185:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5216 (13.5216)
2022-11-25 01:00:04,187:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5707 (13.5469)
2022-11-25 01:00:04,190:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9385 (13.6742)
2022-11-25 01:00:04,192:INFO: Dataset: zara2               Batch:  4/18	Loss 14.1772 (13.7959)
2022-11-25 01:00:04,213:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4763 (13.7284)
2022-11-25 01:00:04,219:INFO: Dataset: zara2               Batch:  6/18	Loss 13.9837 (13.7683)
2022-11-25 01:00:04,220:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8509 (13.7795)
2022-11-25 01:00:04,221:INFO: Dataset: zara2               Batch:  8/18	Loss 13.9832 (13.8053)
2022-11-25 01:00:04,222:INFO: Dataset: zara2               Batch:  9/18	Loss 13.1668 (13.7363)
2022-11-25 01:00:04,223:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1996 (13.6895)
2022-11-25 01:00:04,224:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8507 (13.6131)
2022-11-25 01:00:04,225:INFO: Dataset: zara2               Batch: 12/18	Loss 12.4121 (13.5143)
2022-11-25 01:00:04,226:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6837 (13.4499)
2022-11-25 01:00:04,227:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2911 (13.3694)
2022-11-25 01:00:04,228:INFO: Dataset: zara2               Batch: 15/18	Loss 12.4795 (13.3127)
2022-11-25 01:00:04,229:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0720 (13.2344)
2022-11-25 01:00:04,230:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7607 (13.1380)
2022-11-25 01:00:04,231:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5458 (13.0588)
2022-11-25 01:00:04,278:INFO: - Computing ADE (validation o)
2022-11-25 01:00:04,554:INFO: 		 ADE on eth                       dataset:	 1.028026819229126
2022-11-25 01:00:04,554:INFO: Average validation o:	ADE  1.0280	FDE  2.0864
2022-11-25 01:00:04,554:INFO: - Computing loss (validation)
2022-11-25 01:00:04,752:INFO: Dataset: hotel               Batch: 1/2	Loss 34.8362 (34.8362)
2022-11-25 01:00:04,753:INFO: Dataset: hotel               Batch: 2/2	Loss 31.3952 (34.5191)
2022-11-25 01:00:05,013:INFO: Dataset: univ                Batch: 1/3	Loss 12.3422 (12.3422)
2022-11-25 01:00:05,014:INFO: Dataset: univ                Batch: 2/3	Loss 12.4001 (12.3719)
2022-11-25 01:00:05,015:INFO: Dataset: univ                Batch: 3/3	Loss 12.1825 (12.3037)
2022-11-25 01:00:05,269:INFO: Dataset: zara1               Batch: 1/2	Loss 29.8329 (29.8329)
2022-11-25 01:00:05,270:INFO: Dataset: zara1               Batch: 2/2	Loss 29.6892 (29.7959)
2022-11-25 01:00:05,538:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4970 (11.4970)
2022-11-25 01:00:05,539:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3334 (11.4134)
2022-11-25 01:00:05,539:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4486 (11.4255)
2022-11-25 01:00:05,541:INFO: Dataset: zara2               Batch: 4/5	Loss 11.7375 (11.5028)
2022-11-25 01:00:05,542:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5228 (11.5068)
2022-11-25 01:00:05,593:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_623.pth.tar
2022-11-25 01:00:05,593:INFO: 
===> EPOCH: 624 (P4)
2022-11-25 01:00:05,594:INFO: - Computing loss (training)
2022-11-25 01:00:05,798:INFO: Dataset: hotel               Batch: 1/4	Loss 32.2238 (32.2238)
2022-11-25 01:00:05,801:INFO: Dataset: hotel               Batch: 2/4	Loss 29.3877 (30.7682)
2022-11-25 01:00:05,803:INFO: Dataset: hotel               Batch: 3/4	Loss 27.4023 (29.5849)
2022-11-25 01:00:05,806:INFO: Dataset: hotel               Batch: 4/4	Loss 27.6996 (29.2914)
2022-11-25 01:00:06,132:INFO: Dataset: univ                Batch:  1/15	Loss 13.0470 (13.0470)
2022-11-25 01:00:06,134:INFO: Dataset: univ                Batch:  2/15	Loss 12.8286 (12.9404)
2022-11-25 01:00:06,137:INFO: Dataset: univ                Batch:  3/15	Loss 13.3923 (13.0877)
2022-11-25 01:00:06,139:INFO: Dataset: univ                Batch:  4/15	Loss 13.4394 (13.1777)
2022-11-25 01:00:06,140:INFO: Dataset: univ                Batch:  5/15	Loss 13.0168 (13.1454)
2022-11-25 01:00:06,143:INFO: Dataset: univ                Batch:  6/15	Loss 13.2159 (13.1569)
2022-11-25 01:00:06,144:INFO: Dataset: univ                Batch:  7/15	Loss 12.9535 (13.1286)
2022-11-25 01:00:06,145:INFO: Dataset: univ                Batch:  8/15	Loss 12.5287 (13.0555)
2022-11-25 01:00:06,146:INFO: Dataset: univ                Batch:  9/15	Loss 12.3290 (12.9764)
2022-11-25 01:00:06,147:INFO: Dataset: univ                Batch: 10/15	Loss 12.3025 (12.9159)
2022-11-25 01:00:06,148:INFO: Dataset: univ                Batch: 11/15	Loss 11.6273 (12.7893)
2022-11-25 01:00:06,150:INFO: Dataset: univ                Batch: 12/15	Loss 11.5653 (12.6820)
2022-11-25 01:00:06,152:INFO: Dataset: univ                Batch: 13/15	Loss 11.4472 (12.5857)
2022-11-25 01:00:06,153:INFO: Dataset: univ                Batch: 14/15	Loss 11.0036 (12.4875)
2022-11-25 01:00:06,155:INFO: Dataset: univ                Batch: 15/15	Loss 10.4782 (12.4544)
2022-11-25 01:00:06,422:INFO: Dataset: zara1               Batch: 1/8	Loss 32.8864 (32.8864)
2022-11-25 01:00:06,426:INFO: Dataset: zara1               Batch: 2/8	Loss 32.8702 (32.8787)
2022-11-25 01:00:06,427:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6855 (32.8161)
2022-11-25 01:00:06,430:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2552 (32.6793)
2022-11-25 01:00:06,433:INFO: Dataset: zara1               Batch: 5/8	Loss 31.8849 (32.5051)
2022-11-25 01:00:06,457:INFO: Dataset: zara1               Batch: 6/8	Loss 31.5536 (32.3496)
2022-11-25 01:00:06,458:INFO: Dataset: zara1               Batch: 7/8	Loss 31.1201 (32.1645)
2022-11-25 01:00:06,459:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7716 (32.0282)
2022-11-25 01:00:06,729:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9722 (13.9722)
2022-11-25 01:00:06,732:INFO: Dataset: zara2               Batch:  2/18	Loss 13.9180 (13.9450)
2022-11-25 01:00:06,734:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7551 (13.8857)
2022-11-25 01:00:06,737:INFO: Dataset: zara2               Batch:  4/18	Loss 14.0869 (13.9309)
2022-11-25 01:00:06,781:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6987 (13.8852)
2022-11-25 01:00:06,786:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6139 (13.8390)
2022-11-25 01:00:06,787:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8208 (13.8362)
2022-11-25 01:00:06,788:INFO: Dataset: zara2               Batch:  8/18	Loss 13.1818 (13.7547)
2022-11-25 01:00:06,789:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3600 (13.7106)
2022-11-25 01:00:06,790:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3211 (13.6695)
2022-11-25 01:00:06,792:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9280 (13.6034)
2022-11-25 01:00:06,793:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7532 (13.5292)
2022-11-25 01:00:06,794:INFO: Dataset: zara2               Batch: 13/18	Loss 12.2794 (13.4331)
2022-11-25 01:00:06,795:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4653 (13.3614)
2022-11-25 01:00:06,796:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1891 (13.2811)
2022-11-25 01:00:06,797:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1455 (13.2048)
2022-11-25 01:00:06,798:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9398 (13.1267)
2022-11-25 01:00:06,800:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4904 (13.0518)
2022-11-25 01:00:06,846:INFO: - Computing ADE (validation o)
2022-11-25 01:00:07,113:INFO: 		 ADE on eth                       dataset:	 1.0709041357040405
2022-11-25 01:00:07,113:INFO: Average validation o:	ADE  1.0709	FDE  2.1260
2022-11-25 01:00:07,114:INFO: - Computing loss (validation)
2022-11-25 01:00:07,313:INFO: Dataset: hotel               Batch: 1/2	Loss 34.4732 (34.4732)
2022-11-25 01:00:07,313:INFO: Dataset: hotel               Batch: 2/2	Loss 33.8493 (34.4328)
2022-11-25 01:00:07,569:INFO: Dataset: univ                Batch: 1/3	Loss 12.4989 (12.4989)
2022-11-25 01:00:07,570:INFO: Dataset: univ                Batch: 2/3	Loss 12.3614 (12.4296)
2022-11-25 01:00:07,571:INFO: Dataset: univ                Batch: 3/3	Loss 12.1342 (12.3243)
2022-11-25 01:00:07,841:INFO: Dataset: zara1               Batch: 1/2	Loss 29.7945 (29.7945)
2022-11-25 01:00:07,842:INFO: Dataset: zara1               Batch: 2/2	Loss 29.7931 (29.7942)
2022-11-25 01:00:08,118:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6677 (11.6677)
2022-11-25 01:00:08,121:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5932 (11.6307)
2022-11-25 01:00:08,121:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4796 (11.5839)
2022-11-25 01:00:08,122:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3426 (11.5253)
2022-11-25 01:00:08,123:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4195 (11.5026)
2022-11-25 01:00:08,179:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_624.pth.tar
2022-11-25 01:00:08,179:INFO: 
===> EPOCH: 625 (P4)
2022-11-25 01:00:08,179:INFO: - Computing loss (training)
2022-11-25 01:00:08,394:INFO: Dataset: hotel               Batch: 1/4	Loss 28.4514 (28.4514)
2022-11-25 01:00:08,397:INFO: Dataset: hotel               Batch: 2/4	Loss 29.8654 (29.1686)
2022-11-25 01:00:08,399:INFO: Dataset: hotel               Batch: 3/4	Loss 29.2920 (29.2108)
2022-11-25 01:00:08,401:INFO: Dataset: hotel               Batch: 4/4	Loss 28.7423 (29.1311)
2022-11-25 01:00:08,678:INFO: Dataset: univ                Batch:  1/15	Loss 12.7497 (12.7497)
2022-11-25 01:00:08,679:INFO: Dataset: univ                Batch:  2/15	Loss 13.2245 (12.9825)
2022-11-25 01:00:08,680:INFO: Dataset: univ                Batch:  3/15	Loss 13.1497 (13.0394)
2022-11-25 01:00:08,682:INFO: Dataset: univ                Batch:  4/15	Loss 13.2614 (13.0963)
2022-11-25 01:00:08,739:INFO: Dataset: univ                Batch:  5/15	Loss 13.2108 (13.1172)
2022-11-25 01:00:08,746:INFO: Dataset: univ                Batch:  6/15	Loss 13.2310 (13.1349)
2022-11-25 01:00:08,747:INFO: Dataset: univ                Batch:  7/15	Loss 12.5382 (13.0442)
2022-11-25 01:00:08,748:INFO: Dataset: univ                Batch:  8/15	Loss 12.8368 (13.0212)
2022-11-25 01:00:08,749:INFO: Dataset: univ                Batch:  9/15	Loss 12.5744 (12.9731)
2022-11-25 01:00:08,750:INFO: Dataset: univ                Batch: 10/15	Loss 12.1340 (12.8932)
2022-11-25 01:00:08,752:INFO: Dataset: univ                Batch: 11/15	Loss 11.7947 (12.7866)
2022-11-25 01:00:08,753:INFO: Dataset: univ                Batch: 12/15	Loss 11.6200 (12.6923)
2022-11-25 01:00:08,754:INFO: Dataset: univ                Batch: 13/15	Loss 11.4806 (12.6033)
2022-11-25 01:00:08,755:INFO: Dataset: univ                Batch: 14/15	Loss 11.0435 (12.4966)
2022-11-25 01:00:08,756:INFO: Dataset: univ                Batch: 15/15	Loss 10.7598 (12.4751)
2022-11-25 01:00:09,018:INFO: Dataset: zara1               Batch: 1/8	Loss 32.7296 (32.7296)
2022-11-25 01:00:09,026:INFO: Dataset: zara1               Batch: 2/8	Loss 32.8029 (32.7678)
2022-11-25 01:00:09,027:INFO: Dataset: zara1               Batch: 3/8	Loss 32.7321 (32.7554)
2022-11-25 01:00:09,033:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2422 (32.6295)
2022-11-25 01:00:09,034:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0605 (32.5257)
2022-11-25 01:00:09,074:INFO: Dataset: zara1               Batch: 6/8	Loss 31.8793 (32.4248)
2022-11-25 01:00:09,078:INFO: Dataset: zara1               Batch: 7/8	Loss 30.8401 (32.2011)
2022-11-25 01:00:09,082:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5255 (32.0124)
2022-11-25 01:00:09,368:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4290 (13.4290)
2022-11-25 01:00:09,370:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8136 (13.6213)
2022-11-25 01:00:09,372:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8032 (13.6819)
2022-11-25 01:00:09,418:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7403 (13.6957)
2022-11-25 01:00:09,420:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6630 (13.6899)
2022-11-25 01:00:09,423:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7546 (13.7015)
2022-11-25 01:00:09,424:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5355 (13.6764)
2022-11-25 01:00:09,425:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3359 (13.6318)
2022-11-25 01:00:09,426:INFO: Dataset: zara2               Batch:  9/18	Loss 13.4351 (13.6111)
2022-11-25 01:00:09,426:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2163 (13.5685)
2022-11-25 01:00:09,428:INFO: Dataset: zara2               Batch: 11/18	Loss 13.4637 (13.5587)
2022-11-25 01:00:09,429:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8871 (13.4989)
2022-11-25 01:00:09,430:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5525 (13.4243)
2022-11-25 01:00:09,431:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3383 (13.3438)
2022-11-25 01:00:09,432:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2736 (13.2752)
2022-11-25 01:00:09,433:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1173 (13.2055)
2022-11-25 01:00:09,435:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8778 (13.1163)
2022-11-25 01:00:09,436:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4729 (13.0375)
2022-11-25 01:00:09,485:INFO: - Computing ADE (validation o)
2022-11-25 01:00:09,760:INFO: 		 ADE on eth                       dataset:	 1.0509378910064697
2022-11-25 01:00:09,760:INFO: Average validation o:	ADE  1.0509	FDE  2.1441
2022-11-25 01:00:09,761:INFO: - Computing loss (validation)
2022-11-25 01:00:09,960:INFO: Dataset: hotel               Batch: 1/2	Loss 34.1794 (34.1794)
2022-11-25 01:00:09,961:INFO: Dataset: hotel               Batch: 2/2	Loss 36.3645 (34.3434)
2022-11-25 01:00:10,207:INFO: Dataset: univ                Batch: 1/3	Loss 11.9461 (11.9461)
2022-11-25 01:00:10,209:INFO: Dataset: univ                Batch: 2/3	Loss 12.4968 (12.2088)
2022-11-25 01:00:10,210:INFO: Dataset: univ                Batch: 3/3	Loss 12.2233 (12.2133)
2022-11-25 01:00:10,451:INFO: Dataset: zara1               Batch: 1/2	Loss 29.8299 (29.8299)
2022-11-25 01:00:10,451:INFO: Dataset: zara1               Batch: 2/2	Loss 29.6508 (29.7901)
2022-11-25 01:00:10,696:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4899 (11.4899)
2022-11-25 01:00:10,697:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5374 (11.5141)
2022-11-25 01:00:10,699:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4745 (11.5012)
2022-11-25 01:00:10,700:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6721 (11.5467)
2022-11-25 01:00:10,701:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4366 (11.5232)
2022-11-25 01:00:10,754:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_625.pth.tar
2022-11-25 01:00:10,754:INFO: 
===> EPOCH: 626 (P4)
2022-11-25 01:00:10,754:INFO: - Computing loss (training)
2022-11-25 01:00:10,951:INFO: Dataset: hotel               Batch: 1/4	Loss 30.0845 (30.0845)
2022-11-25 01:00:10,954:INFO: Dataset: hotel               Batch: 2/4	Loss 29.4161 (29.7503)
2022-11-25 01:00:10,957:INFO: Dataset: hotel               Batch: 3/4	Loss 28.3861 (29.3042)
2022-11-25 01:00:10,974:INFO: Dataset: hotel               Batch: 4/4	Loss 27.3457 (28.9735)
2022-11-25 01:00:11,235:INFO: Dataset: univ                Batch:  1/15	Loss 13.1275 (13.1275)
2022-11-25 01:00:11,238:INFO: Dataset: univ                Batch:  2/15	Loss 13.1515 (13.1399)
2022-11-25 01:00:11,239:INFO: Dataset: univ                Batch:  3/15	Loss 13.1108 (13.1297)
2022-11-25 01:00:11,243:INFO: Dataset: univ                Batch:  4/15	Loss 13.0243 (13.1030)
2022-11-25 01:00:11,287:INFO: Dataset: univ                Batch:  5/15	Loss 12.9527 (13.0725)
2022-11-25 01:00:11,292:INFO: Dataset: univ                Batch:  6/15	Loss 12.7773 (13.0209)
2022-11-25 01:00:11,293:INFO: Dataset: univ                Batch:  7/15	Loss 12.7495 (12.9841)
2022-11-25 01:00:11,295:INFO: Dataset: univ                Batch:  8/15	Loss 12.8821 (12.9726)
2022-11-25 01:00:11,296:INFO: Dataset: univ                Batch:  9/15	Loss 12.5307 (12.9270)
2022-11-25 01:00:11,297:INFO: Dataset: univ                Batch: 10/15	Loss 11.9682 (12.8363)
2022-11-25 01:00:11,299:INFO: Dataset: univ                Batch: 11/15	Loss 11.7277 (12.7388)
2022-11-25 01:00:11,300:INFO: Dataset: univ                Batch: 12/15	Loss 11.5921 (12.6477)
2022-11-25 01:00:11,301:INFO: Dataset: univ                Batch: 13/15	Loss 11.2866 (12.5428)
2022-11-25 01:00:11,302:INFO: Dataset: univ                Batch: 14/15	Loss 10.8708 (12.4161)
2022-11-25 01:00:11,304:INFO: Dataset: univ                Batch: 15/15	Loss 10.3516 (12.3890)
2022-11-25 01:00:11,560:INFO: Dataset: zara1               Batch: 1/8	Loss 32.7131 (32.7131)
2022-11-25 01:00:11,562:INFO: Dataset: zara1               Batch: 2/8	Loss 32.9626 (32.8278)
2022-11-25 01:00:11,564:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6035 (32.7542)
2022-11-25 01:00:11,567:INFO: Dataset: zara1               Batch: 4/8	Loss 32.1563 (32.5985)
2022-11-25 01:00:11,568:INFO: Dataset: zara1               Batch: 5/8	Loss 32.0204 (32.4898)
2022-11-25 01:00:11,576:INFO: Dataset: zara1               Batch: 6/8	Loss 31.6191 (32.3461)
2022-11-25 01:00:11,577:INFO: Dataset: zara1               Batch: 7/8	Loss 30.9700 (32.1587)
2022-11-25 01:00:11,578:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5934 (31.9972)
2022-11-25 01:00:11,848:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3836 (13.3836)
2022-11-25 01:00:11,851:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6726 (13.5265)
2022-11-25 01:00:11,903:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8348 (13.6321)
2022-11-25 01:00:11,904:INFO: Dataset: zara2               Batch:  4/18	Loss 13.8537 (13.6870)
2022-11-25 01:00:11,905:INFO: Dataset: zara2               Batch:  5/18	Loss 14.1514 (13.7750)
2022-11-25 01:00:11,913:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5258 (13.7317)
2022-11-25 01:00:11,914:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5743 (13.7106)
2022-11-25 01:00:11,915:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6432 (13.7009)
2022-11-25 01:00:11,915:INFO: Dataset: zara2               Batch:  9/18	Loss 13.5973 (13.6891)
2022-11-25 01:00:11,916:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2163 (13.6443)
2022-11-25 01:00:11,917:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9831 (13.5800)
2022-11-25 01:00:11,919:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7587 (13.4999)
2022-11-25 01:00:11,920:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6502 (13.4325)
2022-11-25 01:00:11,920:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4834 (13.3671)
2022-11-25 01:00:11,921:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1485 (13.2827)
2022-11-25 01:00:11,922:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1356 (13.2089)
2022-11-25 01:00:11,923:INFO: Dataset: zara2               Batch: 17/18	Loss 11.4870 (13.1016)
2022-11-25 01:00:11,925:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6342 (13.0307)
2022-11-25 01:00:11,972:INFO: - Computing ADE (validation o)
2022-11-25 01:00:12,241:INFO: 		 ADE on eth                       dataset:	 1.062352180480957
2022-11-25 01:00:12,241:INFO: Average validation o:	ADE  1.0624	FDE  2.1532
2022-11-25 01:00:12,241:INFO: - Computing loss (validation)
2022-11-25 01:00:12,431:INFO: Dataset: hotel               Batch: 1/2	Loss 34.6134 (34.6134)
2022-11-25 01:00:12,432:INFO: Dataset: hotel               Batch: 2/2	Loss 31.5944 (34.3661)
2022-11-25 01:00:12,716:INFO: Dataset: univ                Batch: 1/3	Loss 12.1569 (12.1569)
2022-11-25 01:00:12,717:INFO: Dataset: univ                Batch: 2/3	Loss 12.2601 (12.2073)
2022-11-25 01:00:12,724:INFO: Dataset: univ                Batch: 3/3	Loss 12.2657 (12.2276)
2022-11-25 01:00:12,972:INFO: Dataset: zara1               Batch: 1/2	Loss 29.6735 (29.6735)
2022-11-25 01:00:12,973:INFO: Dataset: zara1               Batch: 2/2	Loss 29.8044 (29.7034)
2022-11-25 01:00:13,225:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3697 (11.3697)
2022-11-25 01:00:13,226:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5455 (11.4565)
2022-11-25 01:00:13,229:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5736 (11.4954)
2022-11-25 01:00:13,229:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5713 (11.5161)
2022-11-25 01:00:13,230:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5063 (11.5142)
2022-11-25 01:00:13,283:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_626.pth.tar
2022-11-25 01:00:13,284:INFO: 
===> EPOCH: 627 (P4)
2022-11-25 01:00:13,284:INFO: - Computing loss (training)
2022-11-25 01:00:13,476:INFO: Dataset: hotel               Batch: 1/4	Loss 30.0885 (30.0885)
2022-11-25 01:00:13,479:INFO: Dataset: hotel               Batch: 2/4	Loss 29.3247 (29.7120)
2022-11-25 01:00:13,481:INFO: Dataset: hotel               Batch: 3/4	Loss 28.3768 (29.2753)
2022-11-25 01:00:13,483:INFO: Dataset: hotel               Batch: 4/4	Loss 27.7256 (29.0259)
2022-11-25 01:00:13,745:INFO: Dataset: univ                Batch:  1/15	Loss 12.9504 (12.9504)
2022-11-25 01:00:13,747:INFO: Dataset: univ                Batch:  2/15	Loss 13.0027 (12.9769)
2022-11-25 01:00:13,751:INFO: Dataset: univ                Batch:  3/15	Loss 12.8314 (12.9301)
2022-11-25 01:00:13,752:INFO: Dataset: univ                Batch:  4/15	Loss 13.0914 (12.9714)
2022-11-25 01:00:13,811:INFO: Dataset: univ                Batch:  5/15	Loss 13.2089 (13.0148)
2022-11-25 01:00:13,819:INFO: Dataset: univ                Batch:  6/15	Loss 12.9023 (12.9958)
2022-11-25 01:00:13,821:INFO: Dataset: univ                Batch:  7/15	Loss 13.0837 (13.0066)
2022-11-25 01:00:13,822:INFO: Dataset: univ                Batch:  8/15	Loss 12.6132 (12.9581)
2022-11-25 01:00:13,823:INFO: Dataset: univ                Batch:  9/15	Loss 12.5687 (12.9202)
2022-11-25 01:00:13,824:INFO: Dataset: univ                Batch: 10/15	Loss 12.1307 (12.8374)
2022-11-25 01:00:13,826:INFO: Dataset: univ                Batch: 11/15	Loss 11.8219 (12.7399)
2022-11-25 01:00:13,827:INFO: Dataset: univ                Batch: 12/15	Loss 11.5618 (12.6499)
2022-11-25 01:00:13,829:INFO: Dataset: univ                Batch: 13/15	Loss 11.3664 (12.5462)
2022-11-25 01:00:13,830:INFO: Dataset: univ                Batch: 14/15	Loss 10.9267 (12.4308)
2022-11-25 01:00:13,831:INFO: Dataset: univ                Batch: 15/15	Loss 10.3386 (12.3965)
2022-11-25 01:00:14,106:INFO: Dataset: zara1               Batch: 1/8	Loss 32.7175 (32.7175)
2022-11-25 01:00:14,107:INFO: Dataset: zara1               Batch: 2/8	Loss 32.9230 (32.8193)
2022-11-25 01:00:14,108:INFO: Dataset: zara1               Batch: 3/8	Loss 32.6523 (32.7593)
2022-11-25 01:00:14,109:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2592 (32.6219)
2022-11-25 01:00:14,112:INFO: Dataset: zara1               Batch: 5/8	Loss 31.7733 (32.4424)
2022-11-25 01:00:14,208:INFO: Dataset: zara1               Batch: 6/8	Loss 31.4126 (32.2516)
2022-11-25 01:00:14,212:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0017 (32.0868)
2022-11-25 01:00:14,215:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5397 (31.9166)
2022-11-25 01:00:14,485:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8918 (13.8918)
2022-11-25 01:00:14,486:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3785 (13.6448)
2022-11-25 01:00:14,495:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8957 (13.7325)
2022-11-25 01:00:14,505:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7963 (13.7475)
2022-11-25 01:00:14,563:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6790 (13.7336)
2022-11-25 01:00:14,585:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5851 (13.7098)
2022-11-25 01:00:14,586:INFO: Dataset: zara2               Batch:  7/18	Loss 13.7897 (13.7221)
2022-11-25 01:00:14,587:INFO: Dataset: zara2               Batch:  8/18	Loss 13.6496 (13.7126)
2022-11-25 01:00:14,588:INFO: Dataset: zara2               Batch:  9/18	Loss 13.1839 (13.6493)
2022-11-25 01:00:14,589:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0525 (13.5945)
2022-11-25 01:00:14,590:INFO: Dataset: zara2               Batch: 11/18	Loss 12.7811 (13.5221)
2022-11-25 01:00:14,591:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8468 (13.4662)
2022-11-25 01:00:14,592:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5084 (13.3927)
2022-11-25 01:00:14,593:INFO: Dataset: zara2               Batch: 14/18	Loss 12.7206 (13.3445)
2022-11-25 01:00:14,594:INFO: Dataset: zara2               Batch: 15/18	Loss 12.4041 (13.2774)
2022-11-25 01:00:14,595:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9568 (13.2014)
2022-11-25 01:00:14,596:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7497 (13.1086)
2022-11-25 01:00:14,597:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4843 (13.0294)
2022-11-25 01:00:14,651:INFO: - Computing ADE (validation o)
2022-11-25 01:00:14,922:INFO: 		 ADE on eth                       dataset:	 1.0956802368164062
2022-11-25 01:00:14,923:INFO: Average validation o:	ADE  1.0957	FDE  2.2211
2022-11-25 01:00:14,923:INFO: - Computing loss (validation)
2022-11-25 01:00:15,115:INFO: Dataset: hotel               Batch: 1/2	Loss 34.0811 (34.0811)
2022-11-25 01:00:15,116:INFO: Dataset: hotel               Batch: 2/2	Loss 36.2321 (34.2353)
2022-11-25 01:00:15,390:INFO: Dataset: univ                Batch: 1/3	Loss 11.9120 (11.9120)
2022-11-25 01:00:15,393:INFO: Dataset: univ                Batch: 2/3	Loss 12.4622 (12.1539)
2022-11-25 01:00:15,394:INFO: Dataset: univ                Batch: 3/3	Loss 12.1084 (12.1381)
2022-11-25 01:00:15,656:INFO: Dataset: zara1               Batch: 1/2	Loss 29.6918 (29.6918)
2022-11-25 01:00:15,657:INFO: Dataset: zara1               Batch: 2/2	Loss 29.7798 (29.7147)
2022-11-25 01:00:15,906:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6225 (11.6225)
2022-11-25 01:00:15,907:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5137 (11.5686)
2022-11-25 01:00:15,909:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4752 (11.5346)
2022-11-25 01:00:15,910:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5225 (11.5315)
2022-11-25 01:00:15,921:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5413 (11.5336)
2022-11-25 01:00:15,974:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_627.pth.tar
2022-11-25 01:00:15,974:INFO: 
===> EPOCH: 628 (P4)
2022-11-25 01:00:15,974:INFO: - Computing loss (training)
2022-11-25 01:00:16,181:INFO: Dataset: hotel               Batch: 1/4	Loss 30.6725 (30.6725)
2022-11-25 01:00:16,183:INFO: Dataset: hotel               Batch: 2/4	Loss 28.4860 (29.6390)
2022-11-25 01:00:16,184:INFO: Dataset: hotel               Batch: 3/4	Loss 26.3102 (28.5102)
2022-11-25 01:00:16,185:INFO: Dataset: hotel               Batch: 4/4	Loss 30.1019 (28.7643)
2022-11-25 01:00:16,443:INFO: Dataset: univ                Batch:  1/15	Loss 12.8917 (12.8917)
2022-11-25 01:00:16,445:INFO: Dataset: univ                Batch:  2/15	Loss 12.9170 (12.9039)
2022-11-25 01:00:16,455:INFO: Dataset: univ                Batch:  3/15	Loss 13.1290 (12.9773)
2022-11-25 01:00:16,456:INFO: Dataset: univ                Batch:  4/15	Loss 13.1658 (13.0291)
2022-11-25 01:00:16,483:INFO: Dataset: univ                Batch:  5/15	Loss 13.1182 (13.0459)
2022-11-25 01:00:16,526:INFO: Dataset: univ                Batch:  6/15	Loss 12.8983 (13.0212)
2022-11-25 01:00:16,527:INFO: Dataset: univ                Batch:  7/15	Loss 12.6212 (12.9606)
2022-11-25 01:00:16,528:INFO: Dataset: univ                Batch:  8/15	Loss 12.6276 (12.9192)
2022-11-25 01:00:16,529:INFO: Dataset: univ                Batch:  9/15	Loss 12.3355 (12.8526)
2022-11-25 01:00:16,532:INFO: Dataset: univ                Batch: 10/15	Loss 11.8120 (12.7407)
2022-11-25 01:00:16,536:INFO: Dataset: univ                Batch: 11/15	Loss 11.9245 (12.6645)
2022-11-25 01:00:16,541:INFO: Dataset: univ                Batch: 12/15	Loss 11.4978 (12.5584)
2022-11-25 01:00:16,542:INFO: Dataset: univ                Batch: 13/15	Loss 11.0813 (12.4432)
2022-11-25 01:00:16,544:INFO: Dataset: univ                Batch: 14/15	Loss 10.8970 (12.3366)
2022-11-25 01:00:16,545:INFO: Dataset: univ                Batch: 15/15	Loss 10.6723 (12.3116)
2022-11-25 01:00:16,812:INFO: Dataset: zara1               Batch: 1/8	Loss 32.5370 (32.5370)
2022-11-25 01:00:16,815:INFO: Dataset: zara1               Batch: 2/8	Loss 32.6434 (32.5874)
2022-11-25 01:00:16,816:INFO: Dataset: zara1               Batch: 3/8	Loss 32.8646 (32.6835)
2022-11-25 01:00:16,817:INFO: Dataset: zara1               Batch: 4/8	Loss 32.5739 (32.6575)
2022-11-25 01:00:16,819:INFO: Dataset: zara1               Batch: 5/8	Loss 31.8058 (32.4843)
2022-11-25 01:00:16,884:INFO: Dataset: zara1               Batch: 6/8	Loss 31.3847 (32.3175)
2022-11-25 01:00:16,888:INFO: Dataset: zara1               Batch: 7/8	Loss 30.6672 (32.0933)
2022-11-25 01:00:16,891:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4902 (31.9060)
2022-11-25 01:00:17,177:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6280 (13.6280)
2022-11-25 01:00:17,178:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6197 (13.6238)
2022-11-25 01:00:17,182:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6860 (13.6451)
2022-11-25 01:00:17,183:INFO: Dataset: zara2               Batch:  4/18	Loss 13.6079 (13.6362)
2022-11-25 01:00:17,239:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7247 (13.6540)
2022-11-25 01:00:17,246:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7559 (13.6698)
2022-11-25 01:00:17,248:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4862 (13.6451)
2022-11-25 01:00:17,249:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4292 (13.6211)
2022-11-25 01:00:17,250:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2427 (13.5827)
2022-11-25 01:00:17,251:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1822 (13.5416)
2022-11-25 01:00:17,253:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0126 (13.4938)
2022-11-25 01:00:17,254:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8752 (13.4409)
2022-11-25 01:00:17,255:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4755 (13.3618)
2022-11-25 01:00:17,256:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5305 (13.3019)
2022-11-25 01:00:17,257:INFO: Dataset: zara2               Batch: 15/18	Loss 12.4074 (13.2408)
2022-11-25 01:00:17,258:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1928 (13.1786)
2022-11-25 01:00:17,259:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8986 (13.1122)
2022-11-25 01:00:17,261:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4333 (13.0322)
2022-11-25 01:00:17,307:INFO: - Computing ADE (validation o)
2022-11-25 01:00:17,569:INFO: 		 ADE on eth                       dataset:	 1.0552177429199219
2022-11-25 01:00:17,569:INFO: Average validation o:	ADE  1.0552	FDE  2.1511
2022-11-25 01:00:17,570:INFO: - Computing loss (validation)
2022-11-25 01:00:17,757:INFO: Dataset: hotel               Batch: 1/2	Loss 34.1492 (34.1492)
2022-11-25 01:00:17,758:INFO: Dataset: hotel               Batch: 2/2	Loss 38.8341 (34.4211)
2022-11-25 01:00:18,014:INFO: Dataset: univ                Batch: 1/3	Loss 12.0585 (12.0585)
2022-11-25 01:00:18,021:INFO: Dataset: univ                Batch: 2/3	Loss 12.1686 (12.1153)
2022-11-25 01:00:18,023:INFO: Dataset: univ                Batch: 3/3	Loss 12.2800 (12.1645)
2022-11-25 01:00:18,279:INFO: Dataset: zara1               Batch: 1/2	Loss 29.5592 (29.5592)
2022-11-25 01:00:18,280:INFO: Dataset: zara1               Batch: 2/2	Loss 29.9403 (29.6462)
2022-11-25 01:00:18,547:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6155 (11.6155)
2022-11-25 01:00:18,570:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4205 (11.5163)
2022-11-25 01:00:18,570:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4975 (11.5104)
2022-11-25 01:00:18,571:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5305 (11.5156)
2022-11-25 01:00:18,571:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4725 (11.5076)
2022-11-25 01:00:18,628:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_628.pth.tar
2022-11-25 01:00:18,628:INFO: 
===> EPOCH: 629 (P4)
2022-11-25 01:00:18,629:INFO: - Computing loss (training)
2022-11-25 01:00:18,837:INFO: Dataset: hotel               Batch: 1/4	Loss 29.7745 (29.7745)
2022-11-25 01:00:18,845:INFO: Dataset: hotel               Batch: 2/4	Loss 30.3451 (30.0531)
2022-11-25 01:00:18,846:INFO: Dataset: hotel               Batch: 3/4	Loss 28.1363 (29.4102)
2022-11-25 01:00:18,847:INFO: Dataset: hotel               Batch: 4/4	Loss 26.4316 (28.9386)
2022-11-25 01:00:19,120:INFO: Dataset: univ                Batch:  1/15	Loss 12.7467 (12.7467)
2022-11-25 01:00:19,128:INFO: Dataset: univ                Batch:  2/15	Loss 12.9734 (12.8611)
2022-11-25 01:00:19,130:INFO: Dataset: univ                Batch:  3/15	Loss 13.0651 (12.9319)
2022-11-25 01:00:19,132:INFO: Dataset: univ                Batch:  4/15	Loss 12.9759 (12.9443)
2022-11-25 01:00:19,178:INFO: Dataset: univ                Batch:  5/15	Loss 13.3172 (13.0143)
2022-11-25 01:00:19,184:INFO: Dataset: univ                Batch:  6/15	Loss 12.7861 (12.9745)
2022-11-25 01:00:19,185:INFO: Dataset: univ                Batch:  7/15	Loss 12.5377 (12.9086)
2022-11-25 01:00:19,187:INFO: Dataset: univ                Batch:  8/15	Loss 12.5751 (12.8724)
2022-11-25 01:00:19,188:INFO: Dataset: univ                Batch:  9/15	Loss 12.5204 (12.8360)
2022-11-25 01:00:19,189:INFO: Dataset: univ                Batch: 10/15	Loss 11.9976 (12.7520)
2022-11-25 01:00:19,191:INFO: Dataset: univ                Batch: 11/15	Loss 11.8995 (12.6839)
2022-11-25 01:00:19,192:INFO: Dataset: univ                Batch: 12/15	Loss 11.6332 (12.5996)
2022-11-25 01:00:19,193:INFO: Dataset: univ                Batch: 13/15	Loss 11.3801 (12.5154)
2022-11-25 01:00:19,194:INFO: Dataset: univ                Batch: 14/15	Loss 11.0474 (12.4136)
2022-11-25 01:00:19,195:INFO: Dataset: univ                Batch: 15/15	Loss 11.0262 (12.3953)
2022-11-25 01:00:19,455:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4452 (32.4452)
2022-11-25 01:00:19,457:INFO: Dataset: zara1               Batch: 2/8	Loss 32.9166 (32.6672)
2022-11-25 01:00:19,459:INFO: Dataset: zara1               Batch: 3/8	Loss 32.5345 (32.6199)
2022-11-25 01:00:19,462:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2015 (32.5136)
2022-11-25 01:00:19,464:INFO: Dataset: zara1               Batch: 5/8	Loss 31.9380 (32.4151)
2022-11-25 01:00:19,477:INFO: Dataset: zara1               Batch: 6/8	Loss 31.3385 (32.2278)
2022-11-25 01:00:19,478:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7931 (31.9998)
2022-11-25 01:00:19,479:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5159 (31.8233)
2022-11-25 01:00:19,749:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8667 (13.8667)
2022-11-25 01:00:19,754:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3109 (13.5750)
2022-11-25 01:00:19,755:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8356 (13.6680)
2022-11-25 01:00:19,759:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7799 (13.6956)
2022-11-25 01:00:19,760:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5016 (13.6573)
2022-11-25 01:00:19,802:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7258 (13.6679)
2022-11-25 01:00:19,803:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5820 (13.6549)
2022-11-25 01:00:19,804:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5937 (13.6468)
2022-11-25 01:00:19,805:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2240 (13.6000)
2022-11-25 01:00:19,806:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0637 (13.5434)
2022-11-25 01:00:19,809:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0326 (13.5003)
2022-11-25 01:00:19,813:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7571 (13.4385)
2022-11-25 01:00:19,814:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6577 (13.3754)
2022-11-25 01:00:19,815:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4937 (13.3096)
2022-11-25 01:00:19,816:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3385 (13.2473)
2022-11-25 01:00:19,817:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9179 (13.1668)
2022-11-25 01:00:19,818:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8716 (13.0945)
2022-11-25 01:00:19,819:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6481 (13.0321)
2022-11-25 01:00:19,870:INFO: - Computing ADE (validation o)
2022-11-25 01:00:20,136:INFO: 		 ADE on eth                       dataset:	 1.0663890838623047
2022-11-25 01:00:20,136:INFO: Average validation o:	ADE  1.0664	FDE  2.1566
2022-11-25 01:00:20,137:INFO: - Computing loss (validation)
2022-11-25 01:00:20,332:INFO: Dataset: hotel               Batch: 1/2	Loss 34.4221 (34.4221)
2022-11-25 01:00:20,333:INFO: Dataset: hotel               Batch: 2/2	Loss 29.7902 (34.0585)
2022-11-25 01:00:20,587:INFO: Dataset: univ                Batch: 1/3	Loss 12.0196 (12.0196)
2022-11-25 01:00:20,589:INFO: Dataset: univ                Batch: 2/3	Loss 12.3063 (12.1542)
2022-11-25 01:00:20,590:INFO: Dataset: univ                Batch: 3/3	Loss 11.9232 (12.0739)
2022-11-25 01:00:20,878:INFO: Dataset: zara1               Batch: 1/2	Loss 29.6014 (29.6014)
2022-11-25 01:00:20,879:INFO: Dataset: zara1               Batch: 2/2	Loss 29.7903 (29.6464)
2022-11-25 01:00:21,124:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6385 (11.6385)
2022-11-25 01:00:21,126:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5306 (11.5865)
2022-11-25 01:00:21,133:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6190 (11.5973)
2022-11-25 01:00:21,134:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5442 (11.5850)
2022-11-25 01:00:21,134:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4139 (11.5512)
2022-11-25 01:00:21,186:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_629.pth.tar
2022-11-25 01:00:21,187:INFO: 
===> EPOCH: 630 (P4)
2022-11-25 01:00:21,187:INFO: - Computing loss (training)
2022-11-25 01:00:21,390:INFO: Dataset: hotel               Batch: 1/4	Loss 28.4319 (28.4319)
2022-11-25 01:00:21,393:INFO: Dataset: hotel               Batch: 2/4	Loss 29.7240 (29.0991)
2022-11-25 01:00:21,396:INFO: Dataset: hotel               Batch: 3/4	Loss 28.4195 (28.8826)
2022-11-25 01:00:21,399:INFO: Dataset: hotel               Batch: 4/4	Loss 26.4554 (28.4664)
2022-11-25 01:00:21,661:INFO: Dataset: univ                Batch:  1/15	Loss 12.7078 (12.7078)
2022-11-25 01:00:21,663:INFO: Dataset: univ                Batch:  2/15	Loss 12.8927 (12.8040)
2022-11-25 01:00:21,691:INFO: Dataset: univ                Batch:  3/15	Loss 13.1813 (12.9400)
2022-11-25 01:00:21,692:INFO: Dataset: univ                Batch:  4/15	Loss 12.8830 (12.9253)
2022-11-25 01:00:21,693:INFO: Dataset: univ                Batch:  5/15	Loss 12.9054 (12.9211)
2022-11-25 01:00:21,698:INFO: Dataset: univ                Batch:  6/15	Loss 13.2181 (12.9727)
2022-11-25 01:00:21,699:INFO: Dataset: univ                Batch:  7/15	Loss 12.7220 (12.9366)
2022-11-25 01:00:21,700:INFO: Dataset: univ                Batch:  8/15	Loss 12.4037 (12.8702)
2022-11-25 01:00:21,701:INFO: Dataset: univ                Batch:  9/15	Loss 12.3352 (12.8132)
2022-11-25 01:00:21,702:INFO: Dataset: univ                Batch: 10/15	Loss 11.8610 (12.7092)
2022-11-25 01:00:21,703:INFO: Dataset: univ                Batch: 11/15	Loss 11.6752 (12.6083)
2022-11-25 01:00:21,705:INFO: Dataset: univ                Batch: 12/15	Loss 11.3459 (12.4992)
2022-11-25 01:00:21,706:INFO: Dataset: univ                Batch: 13/15	Loss 11.2224 (12.4044)
2022-11-25 01:00:21,707:INFO: Dataset: univ                Batch: 14/15	Loss 10.7167 (12.2810)
2022-11-25 01:00:21,708:INFO: Dataset: univ                Batch: 15/15	Loss 10.2472 (12.2525)
2022-11-25 01:00:21,972:INFO: Dataset: zara1               Batch: 1/8	Loss 32.8738 (32.8738)
2022-11-25 01:00:21,978:INFO: Dataset: zara1               Batch: 2/8	Loss 32.7429 (32.8130)
2022-11-25 01:00:21,987:INFO: Dataset: zara1               Batch: 3/8	Loss 32.8443 (32.8230)
2022-11-25 01:00:21,989:INFO: Dataset: zara1               Batch: 4/8	Loss 32.3388 (32.7108)
2022-11-25 01:00:21,990:INFO: Dataset: zara1               Batch: 5/8	Loss 31.9478 (32.5533)
2022-11-25 01:00:22,060:INFO: Dataset: zara1               Batch: 6/8	Loss 31.3806 (32.3753)
2022-11-25 01:00:22,063:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7934 (32.1467)
2022-11-25 01:00:22,067:INFO: Dataset: zara1               Batch: 8/8	Loss 30.4787 (31.9728)
2022-11-25 01:00:22,376:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7413 (13.7413)
2022-11-25 01:00:22,377:INFO: Dataset: zara2               Batch:  2/18	Loss 13.8397 (13.7915)
2022-11-25 01:00:22,380:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6464 (13.7450)
2022-11-25 01:00:22,381:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9025 (13.7838)
2022-11-25 01:00:22,430:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7476 (13.7763)
2022-11-25 01:00:22,438:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8363 (13.7854)
2022-11-25 01:00:22,438:INFO: Dataset: zara2               Batch:  7/18	Loss 13.3132 (13.7136)
2022-11-25 01:00:22,439:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2903 (13.6604)
2022-11-25 01:00:22,440:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2720 (13.6145)
2022-11-25 01:00:22,441:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9029 (13.5392)
2022-11-25 01:00:22,443:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1126 (13.4977)
2022-11-25 01:00:22,444:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5881 (13.4176)
2022-11-25 01:00:22,445:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7092 (13.3642)
2022-11-25 01:00:22,446:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1072 (13.2725)
2022-11-25 01:00:22,447:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2798 (13.2012)
2022-11-25 01:00:22,448:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1036 (13.1290)
2022-11-25 01:00:22,449:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8817 (13.0540)
2022-11-25 01:00:22,451:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6628 (12.9942)
2022-11-25 01:00:22,499:INFO: - Computing ADE (validation o)
2022-11-25 01:00:22,773:INFO: 		 ADE on eth                       dataset:	 1.0681684017181396
2022-11-25 01:00:22,773:INFO: Average validation o:	ADE  1.0682	FDE  2.1560
2022-11-25 01:00:22,774:INFO: - Computing loss (validation)
2022-11-25 01:00:22,969:INFO: Dataset: hotel               Batch: 1/2	Loss 34.2726 (34.2726)
2022-11-25 01:00:22,970:INFO: Dataset: hotel               Batch: 2/2	Loss 33.0587 (34.1814)
2022-11-25 01:00:23,215:INFO: Dataset: univ                Batch: 1/3	Loss 12.2789 (12.2789)
2022-11-25 01:00:23,217:INFO: Dataset: univ                Batch: 2/3	Loss 12.1997 (12.2388)
2022-11-25 01:00:23,219:INFO: Dataset: univ                Batch: 3/3	Loss 11.7887 (12.0806)
2022-11-25 01:00:23,481:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4339 (29.4339)
2022-11-25 01:00:23,481:INFO: Dataset: zara1               Batch: 2/2	Loss 29.9682 (29.5507)
2022-11-25 01:00:23,735:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4197 (11.4197)
2022-11-25 01:00:23,736:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5153 (11.4681)
2022-11-25 01:00:23,736:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5636 (11.4990)
2022-11-25 01:00:23,738:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6559 (11.5392)
2022-11-25 01:00:23,740:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4337 (11.5184)
2022-11-25 01:00:23,796:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_630.pth.tar
2022-11-25 01:00:23,797:INFO: 
===> EPOCH: 631 (P4)
2022-11-25 01:00:23,797:INFO: - Computing loss (training)
2022-11-25 01:00:24,010:INFO: Dataset: hotel               Batch: 1/4	Loss 30.0171 (30.0171)
2022-11-25 01:00:24,014:INFO: Dataset: hotel               Batch: 2/4	Loss 27.7336 (28.8832)
2022-11-25 01:00:24,016:INFO: Dataset: hotel               Batch: 3/4	Loss 28.2835 (28.6937)
2022-11-25 01:00:24,022:INFO: Dataset: hotel               Batch: 4/4	Loss 28.2020 (28.6126)
2022-11-25 01:00:24,279:INFO: Dataset: univ                Batch:  1/15	Loss 12.8614 (12.8614)
2022-11-25 01:00:24,281:INFO: Dataset: univ                Batch:  2/15	Loss 12.6645 (12.7590)
2022-11-25 01:00:24,285:INFO: Dataset: univ                Batch:  3/15	Loss 12.9345 (12.8136)
2022-11-25 01:00:24,324:INFO: Dataset: univ                Batch:  4/15	Loss 12.6814 (12.7778)
2022-11-25 01:00:24,325:INFO: Dataset: univ                Batch:  5/15	Loss 12.9669 (12.8144)
2022-11-25 01:00:24,361:INFO: Dataset: univ                Batch:  6/15	Loss 12.8117 (12.8140)
2022-11-25 01:00:24,362:INFO: Dataset: univ                Batch:  7/15	Loss 12.8560 (12.8195)
2022-11-25 01:00:24,364:INFO: Dataset: univ                Batch:  8/15	Loss 12.6723 (12.8011)
2022-11-25 01:00:24,365:INFO: Dataset: univ                Batch:  9/15	Loss 12.2644 (12.7467)
2022-11-25 01:00:24,366:INFO: Dataset: univ                Batch: 10/15	Loss 12.1594 (12.6865)
2022-11-25 01:00:24,367:INFO: Dataset: univ                Batch: 11/15	Loss 11.7012 (12.6005)
2022-11-25 01:00:24,369:INFO: Dataset: univ                Batch: 12/15	Loss 11.5310 (12.5104)
2022-11-25 01:00:24,370:INFO: Dataset: univ                Batch: 13/15	Loss 11.2168 (12.4165)
2022-11-25 01:00:24,371:INFO: Dataset: univ                Batch: 14/15	Loss 10.9281 (12.2988)
2022-11-25 01:00:24,372:INFO: Dataset: univ                Batch: 15/15	Loss 11.2567 (12.2875)
2022-11-25 01:00:24,666:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4037 (32.4037)
2022-11-25 01:00:24,670:INFO: Dataset: zara1               Batch: 2/8	Loss 32.5859 (32.4938)
2022-11-25 01:00:24,671:INFO: Dataset: zara1               Batch: 3/8	Loss 32.4886 (32.4920)
2022-11-25 01:00:24,673:INFO: Dataset: zara1               Batch: 4/8	Loss 32.3790 (32.4616)
2022-11-25 01:00:24,675:INFO: Dataset: zara1               Batch: 5/8	Loss 31.5545 (32.2977)
2022-11-25 01:00:24,700:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2335 (32.1221)
2022-11-25 01:00:24,701:INFO: Dataset: zara1               Batch: 7/8	Loss 30.9746 (31.9682)
2022-11-25 01:00:24,702:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6955 (31.8490)
2022-11-25 01:00:24,999:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4263 (13.4263)
2022-11-25 01:00:25,000:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5343 (13.4810)
2022-11-25 01:00:25,001:INFO: Dataset: zara2               Batch:  3/18	Loss 13.8248 (13.6043)
2022-11-25 01:00:25,003:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9308 (13.6877)
2022-11-25 01:00:25,023:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5745 (13.6662)
2022-11-25 01:00:25,052:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5217 (13.6444)
2022-11-25 01:00:25,053:INFO: Dataset: zara2               Batch:  7/18	Loss 13.8029 (13.6654)
2022-11-25 01:00:25,054:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5730 (13.6531)
2022-11-25 01:00:25,055:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3870 (13.6229)
2022-11-25 01:00:25,056:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0367 (13.5578)
2022-11-25 01:00:25,057:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8646 (13.4915)
2022-11-25 01:00:25,058:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5609 (13.4200)
2022-11-25 01:00:25,059:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8422 (13.3729)
2022-11-25 01:00:25,060:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4383 (13.3051)
2022-11-25 01:00:25,061:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2880 (13.2373)
2022-11-25 01:00:25,062:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1087 (13.1726)
2022-11-25 01:00:25,063:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6021 (13.0731)
2022-11-25 01:00:25,064:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4539 (12.9880)
2022-11-25 01:00:25,113:INFO: - Computing ADE (validation o)
2022-11-25 01:00:25,380:INFO: 		 ADE on eth                       dataset:	 1.0393829345703125
2022-11-25 01:00:25,380:INFO: Average validation o:	ADE  1.0394	FDE  2.0933
2022-11-25 01:00:25,381:INFO: - Computing loss (validation)
2022-11-25 01:00:25,579:INFO: Dataset: hotel               Batch: 1/2	Loss 33.4914 (33.4914)
2022-11-25 01:00:25,579:INFO: Dataset: hotel               Batch: 2/2	Loss 42.1484 (33.9937)
2022-11-25 01:00:25,856:INFO: Dataset: univ                Batch: 1/3	Loss 12.0413 (12.0413)
2022-11-25 01:00:25,859:INFO: Dataset: univ                Batch: 2/3	Loss 12.0273 (12.0347)
2022-11-25 01:00:25,859:INFO: Dataset: univ                Batch: 3/3	Loss 11.8288 (11.9677)
2022-11-25 01:00:26,105:INFO: Dataset: zara1               Batch: 1/2	Loss 29.6580 (29.6580)
2022-11-25 01:00:26,108:INFO: Dataset: zara1               Batch: 2/2	Loss 29.2101 (29.5471)
2022-11-25 01:00:26,369:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5733 (11.5733)
2022-11-25 01:00:26,369:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4650 (11.5138)
2022-11-25 01:00:26,371:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4521 (11.4920)
2022-11-25 01:00:26,372:INFO: Dataset: zara2               Batch: 4/5	Loss 11.7329 (11.5551)
2022-11-25 01:00:26,374:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4877 (11.5415)
2022-11-25 01:00:26,435:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_631.pth.tar
2022-11-25 01:00:26,435:INFO: 
===> EPOCH: 632 (P4)
2022-11-25 01:00:26,435:INFO: - Computing loss (training)
2022-11-25 01:00:26,638:INFO: Dataset: hotel               Batch: 1/4	Loss 29.4535 (29.4535)
2022-11-25 01:00:26,640:INFO: Dataset: hotel               Batch: 2/4	Loss 28.0673 (28.8140)
2022-11-25 01:00:26,642:INFO: Dataset: hotel               Batch: 3/4	Loss 28.3983 (28.6732)
2022-11-25 01:00:26,645:INFO: Dataset: hotel               Batch: 4/4	Loss 26.8941 (28.3634)
2022-11-25 01:00:26,910:INFO: Dataset: univ                Batch:  1/15	Loss 12.6251 (12.6251)
2022-11-25 01:00:26,967:INFO: Dataset: univ                Batch:  2/15	Loss 12.8671 (12.7399)
2022-11-25 01:00:26,968:INFO: Dataset: univ                Batch:  3/15	Loss 12.8026 (12.7615)
2022-11-25 01:00:26,969:INFO: Dataset: univ                Batch:  4/15	Loss 13.0874 (12.8400)
2022-11-25 01:00:26,970:INFO: Dataset: univ                Batch:  5/15	Loss 13.2299 (12.9098)
2022-11-25 01:00:26,975:INFO: Dataset: univ                Batch:  6/15	Loss 12.5795 (12.8521)
2022-11-25 01:00:26,976:INFO: Dataset: univ                Batch:  7/15	Loss 12.4773 (12.7958)
2022-11-25 01:00:26,978:INFO: Dataset: univ                Batch:  8/15	Loss 12.2949 (12.7357)
2022-11-25 01:00:26,979:INFO: Dataset: univ                Batch:  9/15	Loss 12.0652 (12.6677)
2022-11-25 01:00:26,980:INFO: Dataset: univ                Batch: 10/15	Loss 12.0898 (12.6166)
2022-11-25 01:00:26,982:INFO: Dataset: univ                Batch: 11/15	Loss 11.7083 (12.5277)
2022-11-25 01:00:26,983:INFO: Dataset: univ                Batch: 12/15	Loss 11.4880 (12.4358)
2022-11-25 01:00:26,984:INFO: Dataset: univ                Batch: 13/15	Loss 11.0245 (12.3199)
2022-11-25 01:00:26,986:INFO: Dataset: univ                Batch: 14/15	Loss 10.7301 (12.2019)
2022-11-25 01:00:26,988:INFO: Dataset: univ                Batch: 15/15	Loss 10.2007 (12.1769)
2022-11-25 01:00:27,256:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4995 (32.4995)
2022-11-25 01:00:27,257:INFO: Dataset: zara1               Batch: 2/8	Loss 32.7493 (32.6203)
2022-11-25 01:00:27,259:INFO: Dataset: zara1               Batch: 3/8	Loss 32.4488 (32.5604)
2022-11-25 01:00:27,261:INFO: Dataset: zara1               Batch: 4/8	Loss 32.1346 (32.4572)
2022-11-25 01:00:27,263:INFO: Dataset: zara1               Batch: 5/8	Loss 31.8811 (32.3410)
2022-11-25 01:00:27,308:INFO: Dataset: zara1               Batch: 6/8	Loss 31.1555 (32.1617)
2022-11-25 01:00:27,311:INFO: Dataset: zara1               Batch: 7/8	Loss 30.6934 (31.9575)
2022-11-25 01:00:27,315:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6360 (31.8163)
2022-11-25 01:00:27,603:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9483 (13.9483)
2022-11-25 01:00:27,604:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0102 (13.9796)
2022-11-25 01:00:27,605:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7164 (13.8956)
2022-11-25 01:00:27,609:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9198 (13.9011)
2022-11-25 01:00:27,641:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6327 (13.8460)
2022-11-25 01:00:27,648:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4685 (13.7812)
2022-11-25 01:00:27,649:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5865 (13.7535)
2022-11-25 01:00:27,650:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3759 (13.7045)
2022-11-25 01:00:27,651:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2839 (13.6622)
2022-11-25 01:00:27,652:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0636 (13.5973)
2022-11-25 01:00:27,653:INFO: Dataset: zara2               Batch: 11/18	Loss 12.7288 (13.5162)
2022-11-25 01:00:27,655:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8183 (13.4578)
2022-11-25 01:00:27,656:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7423 (13.4016)
2022-11-25 01:00:27,657:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3382 (13.3337)
2022-11-25 01:00:27,658:INFO: Dataset: zara2               Batch: 15/18	Loss 11.8005 (13.2283)
2022-11-25 01:00:27,659:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9849 (13.1457)
2022-11-25 01:00:27,660:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9620 (13.0809)
2022-11-25 01:00:27,661:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6321 (13.0060)
2022-11-25 01:00:27,710:INFO: - Computing ADE (validation o)
2022-11-25 01:00:27,989:INFO: 		 ADE on eth                       dataset:	 1.0864804983139038
2022-11-25 01:00:27,989:INFO: Average validation o:	ADE  1.0865	FDE  2.1966
2022-11-25 01:00:27,990:INFO: - Computing loss (validation)
2022-11-25 01:00:28,191:INFO: Dataset: hotel               Batch: 1/2	Loss 34.0457 (34.0457)
2022-11-25 01:00:28,192:INFO: Dataset: hotel               Batch: 2/2	Loss 34.1318 (34.0530)
2022-11-25 01:00:28,441:INFO: Dataset: univ                Batch: 1/3	Loss 11.9501 (11.9501)
2022-11-25 01:00:28,453:INFO: Dataset: univ                Batch: 2/3	Loss 12.0742 (12.0112)
2022-11-25 01:00:28,453:INFO: Dataset: univ                Batch: 3/3	Loss 11.9259 (11.9840)
2022-11-25 01:00:28,704:INFO: Dataset: zara1               Batch: 1/2	Loss 29.5676 (29.5676)
2022-11-25 01:00:28,705:INFO: Dataset: zara1               Batch: 2/2	Loss 29.2921 (29.4897)
2022-11-25 01:00:28,957:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4229 (11.4229)
2022-11-25 01:00:28,958:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3736 (11.3988)
2022-11-25 01:00:28,959:INFO: Dataset: zara2               Batch: 3/5	Loss 11.7686 (11.5134)
2022-11-25 01:00:28,960:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6004 (11.5356)
2022-11-25 01:00:28,961:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4925 (11.5270)
2022-11-25 01:00:29,013:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_632.pth.tar
2022-11-25 01:00:29,014:INFO: 
===> EPOCH: 633 (P4)
2022-11-25 01:00:29,014:INFO: - Computing loss (training)
2022-11-25 01:00:29,221:INFO: Dataset: hotel               Batch: 1/4	Loss 29.5058 (29.5058)
2022-11-25 01:00:29,228:INFO: Dataset: hotel               Batch: 2/4	Loss 28.0236 (28.7595)
2022-11-25 01:00:29,229:INFO: Dataset: hotel               Batch: 3/4	Loss 27.5294 (28.3540)
2022-11-25 01:00:29,230:INFO: Dataset: hotel               Batch: 4/4	Loss 28.6785 (28.4084)
2022-11-25 01:00:29,508:INFO: Dataset: univ                Batch:  1/15	Loss 12.7973 (12.7973)
2022-11-25 01:00:29,510:INFO: Dataset: univ                Batch:  2/15	Loss 12.8798 (12.8417)
2022-11-25 01:00:29,514:INFO: Dataset: univ                Batch:  3/15	Loss 12.7161 (12.8004)
2022-11-25 01:00:29,516:INFO: Dataset: univ                Batch:  4/15	Loss 12.7947 (12.7990)
2022-11-25 01:00:29,571:INFO: Dataset: univ                Batch:  5/15	Loss 12.8850 (12.8160)
2022-11-25 01:00:29,578:INFO: Dataset: univ                Batch:  6/15	Loss 12.7667 (12.8078)
2022-11-25 01:00:29,579:INFO: Dataset: univ                Batch:  7/15	Loss 13.0058 (12.8343)
2022-11-25 01:00:29,580:INFO: Dataset: univ                Batch:  8/15	Loss 12.2327 (12.7562)
2022-11-25 01:00:29,581:INFO: Dataset: univ                Batch:  9/15	Loss 12.4129 (12.7188)
2022-11-25 01:00:29,582:INFO: Dataset: univ                Batch: 10/15	Loss 11.8630 (12.6362)
2022-11-25 01:00:29,583:INFO: Dataset: univ                Batch: 11/15	Loss 11.5216 (12.5244)
2022-11-25 01:00:29,585:INFO: Dataset: univ                Batch: 12/15	Loss 11.3613 (12.4179)
2022-11-25 01:00:29,586:INFO: Dataset: univ                Batch: 13/15	Loss 11.1104 (12.3229)
2022-11-25 01:00:29,587:INFO: Dataset: univ                Batch: 14/15	Loss 10.7689 (12.2115)
2022-11-25 01:00:29,588:INFO: Dataset: univ                Batch: 15/15	Loss 10.8672 (12.1969)
2022-11-25 01:00:29,868:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4081 (32.4081)
2022-11-25 01:00:29,869:INFO: Dataset: zara1               Batch: 2/8	Loss 32.7033 (32.5538)
2022-11-25 01:00:29,870:INFO: Dataset: zara1               Batch: 3/8	Loss 32.0990 (32.3929)
2022-11-25 01:00:29,873:INFO: Dataset: zara1               Batch: 4/8	Loss 32.0831 (32.3233)
2022-11-25 01:00:29,874:INFO: Dataset: zara1               Batch: 5/8	Loss 31.7042 (32.1840)
2022-11-25 01:00:29,917:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2436 (32.0295)
2022-11-25 01:00:29,918:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0873 (31.8907)
2022-11-25 01:00:29,919:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5828 (31.7420)
2022-11-25 01:00:30,206:INFO: Dataset: zara2               Batch:  1/18	Loss 13.9390 (13.9390)
2022-11-25 01:00:30,284:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5766 (13.7598)
2022-11-25 01:00:30,285:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9823 (13.8318)
2022-11-25 01:00:30,286:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7546 (13.8123)
2022-11-25 01:00:30,287:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4391 (13.7435)
2022-11-25 01:00:30,290:INFO: Dataset: zara2               Batch:  6/18	Loss 13.6134 (13.7216)
2022-11-25 01:00:30,291:INFO: Dataset: zara2               Batch:  7/18	Loss 13.5778 (13.7008)
2022-11-25 01:00:30,292:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4451 (13.6693)
2022-11-25 01:00:30,293:INFO: Dataset: zara2               Batch:  9/18	Loss 13.0343 (13.6027)
2022-11-25 01:00:30,294:INFO: Dataset: zara2               Batch: 10/18	Loss 12.8631 (13.5222)
2022-11-25 01:00:30,296:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8719 (13.4654)
2022-11-25 01:00:30,297:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6236 (13.3968)
2022-11-25 01:00:30,298:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6352 (13.3372)
2022-11-25 01:00:30,299:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4838 (13.2809)
2022-11-25 01:00:30,301:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0016 (13.1949)
2022-11-25 01:00:30,302:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9602 (13.1249)
2022-11-25 01:00:30,304:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7168 (13.0377)
2022-11-25 01:00:30,305:INFO: Dataset: zara2               Batch: 18/18	Loss 11.8884 (12.9811)
2022-11-25 01:00:30,352:INFO: - Computing ADE (validation o)
2022-11-25 01:00:30,630:INFO: 		 ADE on eth                       dataset:	 1.063319444656372
2022-11-25 01:00:30,630:INFO: Average validation o:	ADE  1.0633	FDE  2.1736
2022-11-25 01:00:30,631:INFO: - Computing loss (validation)
2022-11-25 01:00:30,826:INFO: Dataset: hotel               Batch: 1/2	Loss 33.7681 (33.7681)
2022-11-25 01:00:30,827:INFO: Dataset: hotel               Batch: 2/2	Loss 35.8104 (33.8797)
2022-11-25 01:00:31,108:INFO: Dataset: univ                Batch: 1/3	Loss 11.7942 (11.7942)
2022-11-25 01:00:31,109:INFO: Dataset: univ                Batch: 2/3	Loss 11.7750 (11.7843)
2022-11-25 01:00:31,111:INFO: Dataset: univ                Batch: 3/3	Loss 12.1074 (11.8762)
2022-11-25 01:00:31,357:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4393 (29.4393)
2022-11-25 01:00:31,358:INFO: Dataset: zara1               Batch: 2/2	Loss 29.5718 (29.4730)
2022-11-25 01:00:31,601:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5271 (11.5271)
2022-11-25 01:00:31,604:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6456 (11.5872)
2022-11-25 01:00:31,607:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4803 (11.5547)
2022-11-25 01:00:31,610:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6034 (11.5666)
2022-11-25 01:00:31,610:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5287 (11.5592)
2022-11-25 01:00:31,666:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_633.pth.tar
2022-11-25 01:00:31,666:INFO: 
===> EPOCH: 634 (P4)
2022-11-25 01:00:31,666:INFO: - Computing loss (training)
2022-11-25 01:00:31,877:INFO: Dataset: hotel               Batch: 1/4	Loss 28.2092 (28.2092)
2022-11-25 01:00:31,878:INFO: Dataset: hotel               Batch: 2/4	Loss 28.2834 (28.2468)
2022-11-25 01:00:31,883:INFO: Dataset: hotel               Batch: 3/4	Loss 27.9803 (28.1541)
2022-11-25 01:00:31,886:INFO: Dataset: hotel               Batch: 4/4	Loss 27.9786 (28.1238)
2022-11-25 01:00:32,156:INFO: Dataset: univ                Batch:  1/15	Loss 12.3688 (12.3688)
2022-11-25 01:00:32,158:INFO: Dataset: univ                Batch:  2/15	Loss 12.8301 (12.5789)
2022-11-25 01:00:32,162:INFO: Dataset: univ                Batch:  3/15	Loss 12.8408 (12.6642)
2022-11-25 01:00:32,170:INFO: Dataset: univ                Batch:  4/15	Loss 12.8258 (12.7008)
2022-11-25 01:00:32,214:INFO: Dataset: univ                Batch:  5/15	Loss 12.9868 (12.7508)
2022-11-25 01:00:32,220:INFO: Dataset: univ                Batch:  6/15	Loss 12.7019 (12.7422)
2022-11-25 01:00:32,221:INFO: Dataset: univ                Batch:  7/15	Loss 12.5101 (12.7092)
2022-11-25 01:00:32,222:INFO: Dataset: univ                Batch:  8/15	Loss 12.1239 (12.6305)
2022-11-25 01:00:32,224:INFO: Dataset: univ                Batch:  9/15	Loss 12.1446 (12.5775)
2022-11-25 01:00:32,225:INFO: Dataset: univ                Batch: 10/15	Loss 12.1115 (12.5340)
2022-11-25 01:00:32,226:INFO: Dataset: univ                Batch: 11/15	Loss 11.7066 (12.4586)
2022-11-25 01:00:32,228:INFO: Dataset: univ                Batch: 12/15	Loss 11.2111 (12.3351)
2022-11-25 01:00:32,229:INFO: Dataset: univ                Batch: 13/15	Loss 10.9579 (12.2243)
2022-11-25 01:00:32,230:INFO: Dataset: univ                Batch: 14/15	Loss 10.7575 (12.1150)
2022-11-25 01:00:32,231:INFO: Dataset: univ                Batch: 15/15	Loss 10.8632 (12.1012)
2022-11-25 01:00:32,504:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4537 (32.4537)
2022-11-25 01:00:32,506:INFO: Dataset: zara1               Batch: 2/8	Loss 32.3497 (32.4002)
2022-11-25 01:00:32,512:INFO: Dataset: zara1               Batch: 3/8	Loss 32.2362 (32.3496)
2022-11-25 01:00:32,513:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2938 (32.3369)
2022-11-25 01:00:32,515:INFO: Dataset: zara1               Batch: 5/8	Loss 31.7477 (32.2203)
2022-11-25 01:00:32,604:INFO: Dataset: zara1               Batch: 6/8	Loss 31.1878 (32.0420)
2022-11-25 01:00:32,608:INFO: Dataset: zara1               Batch: 7/8	Loss 31.0824 (31.8944)
2022-11-25 01:00:32,612:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5511 (31.7509)
2022-11-25 01:00:32,896:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6305 (13.6305)
2022-11-25 01:00:32,899:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6517 (13.6412)
2022-11-25 01:00:32,900:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5089 (13.5950)
2022-11-25 01:00:32,903:INFO: Dataset: zara2               Batch:  4/18	Loss 13.5411 (13.5806)
2022-11-25 01:00:32,955:INFO: Dataset: zara2               Batch:  5/18	Loss 13.8563 (13.6390)
2022-11-25 01:00:32,962:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7245 (13.6536)
2022-11-25 01:00:32,963:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2874 (13.6034)
2022-11-25 01:00:32,964:INFO: Dataset: zara2               Batch:  8/18	Loss 13.1991 (13.5556)
2022-11-25 01:00:32,965:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3702 (13.5343)
2022-11-25 01:00:32,966:INFO: Dataset: zara2               Batch: 10/18	Loss 12.7801 (13.4590)
2022-11-25 01:00:32,968:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0370 (13.4221)
2022-11-25 01:00:32,969:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6575 (13.3552)
2022-11-25 01:00:32,970:INFO: Dataset: zara2               Batch: 13/18	Loss 12.9544 (13.3224)
2022-11-25 01:00:32,971:INFO: Dataset: zara2               Batch: 14/18	Loss 12.6158 (13.2689)
2022-11-25 01:00:32,972:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1243 (13.2000)
2022-11-25 01:00:32,973:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9583 (13.1226)
2022-11-25 01:00:32,975:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9280 (13.0597)
2022-11-25 01:00:32,976:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4655 (12.9830)
2022-11-25 01:00:33,024:INFO: - Computing ADE (validation o)
2022-11-25 01:00:33,295:INFO: 		 ADE on eth                       dataset:	 1.0860199928283691
2022-11-25 01:00:33,295:INFO: Average validation o:	ADE  1.0860	FDE  2.1990
2022-11-25 01:00:33,296:INFO: - Computing loss (validation)
2022-11-25 01:00:33,494:INFO: Dataset: hotel               Batch: 1/2	Loss 33.8718 (33.8718)
2022-11-25 01:00:33,495:INFO: Dataset: hotel               Batch: 2/2	Loss 34.3064 (33.8985)
2022-11-25 01:00:33,766:INFO: Dataset: univ                Batch: 1/3	Loss 11.7337 (11.7337)
2022-11-25 01:00:33,767:INFO: Dataset: univ                Batch: 2/3	Loss 12.0541 (11.8775)
2022-11-25 01:00:33,769:INFO: Dataset: univ                Batch: 3/3	Loss 11.8276 (11.8611)
2022-11-25 01:00:34,016:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4127 (29.4127)
2022-11-25 01:00:34,016:INFO: Dataset: zara1               Batch: 2/2	Loss 29.4743 (29.4293)
2022-11-25 01:00:34,273:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5425 (11.5425)
2022-11-25 01:00:34,278:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6423 (11.5900)
2022-11-25 01:00:34,282:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5689 (11.5832)
2022-11-25 01:00:34,285:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4809 (11.5578)
2022-11-25 01:00:34,285:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5382 (11.5539)
2022-11-25 01:00:34,342:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_634.pth.tar
2022-11-25 01:00:34,342:INFO: 
===> EPOCH: 635 (P4)
2022-11-25 01:00:34,342:INFO: - Computing loss (training)
2022-11-25 01:00:34,567:INFO: Dataset: hotel               Batch: 1/4	Loss 29.2026 (29.2026)
2022-11-25 01:00:34,570:INFO: Dataset: hotel               Batch: 2/4	Loss 27.3427 (28.2814)
2022-11-25 01:00:34,571:INFO: Dataset: hotel               Batch: 3/4	Loss 27.7291 (28.1019)
2022-11-25 01:00:34,574:INFO: Dataset: hotel               Batch: 4/4	Loss 28.3931 (28.1507)
2022-11-25 01:00:34,834:INFO: Dataset: univ                Batch:  1/15	Loss 12.4740 (12.4740)
2022-11-25 01:00:34,835:INFO: Dataset: univ                Batch:  2/15	Loss 12.4073 (12.4406)
2022-11-25 01:00:34,839:INFO: Dataset: univ                Batch:  3/15	Loss 12.5762 (12.4897)
2022-11-25 01:00:34,899:INFO: Dataset: univ                Batch:  4/15	Loss 12.8412 (12.5702)
2022-11-25 01:00:34,900:INFO: Dataset: univ                Batch:  5/15	Loss 12.9769 (12.6522)
2022-11-25 01:00:34,905:INFO: Dataset: univ                Batch:  6/15	Loss 12.5066 (12.6291)
2022-11-25 01:00:34,906:INFO: Dataset: univ                Batch:  7/15	Loss 12.4342 (12.6003)
2022-11-25 01:00:34,907:INFO: Dataset: univ                Batch:  8/15	Loss 12.5546 (12.5957)
2022-11-25 01:00:34,908:INFO: Dataset: univ                Batch:  9/15	Loss 12.3257 (12.5655)
2022-11-25 01:00:34,909:INFO: Dataset: univ                Batch: 10/15	Loss 12.0344 (12.5204)
2022-11-25 01:00:34,911:INFO: Dataset: univ                Batch: 11/15	Loss 11.5915 (12.4345)
2022-11-25 01:00:34,912:INFO: Dataset: univ                Batch: 12/15	Loss 11.2846 (12.3376)
2022-11-25 01:00:34,913:INFO: Dataset: univ                Batch: 13/15	Loss 11.2828 (12.2654)
2022-11-25 01:00:34,914:INFO: Dataset: univ                Batch: 14/15	Loss 10.5994 (12.1488)
2022-11-25 01:00:34,915:INFO: Dataset: univ                Batch: 15/15	Loss 10.1817 (12.1221)
2022-11-25 01:00:35,173:INFO: Dataset: zara1               Batch: 1/8	Loss 32.5022 (32.5022)
2022-11-25 01:00:35,182:INFO: Dataset: zara1               Batch: 2/8	Loss 32.1777 (32.3430)
2022-11-25 01:00:35,193:INFO: Dataset: zara1               Batch: 3/8	Loss 32.7669 (32.4762)
2022-11-25 01:00:35,194:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2862 (32.4289)
2022-11-25 01:00:35,195:INFO: Dataset: zara1               Batch: 5/8	Loss 31.4247 (32.2323)
2022-11-25 01:00:35,251:INFO: Dataset: zara1               Batch: 6/8	Loss 31.1980 (32.0351)
2022-11-25 01:00:35,255:INFO: Dataset: zara1               Batch: 7/8	Loss 30.6866 (31.8479)
2022-11-25 01:00:35,258:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3231 (31.6818)
2022-11-25 01:00:35,537:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4723 (13.4723)
2022-11-25 01:00:35,543:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3585 (13.4114)
2022-11-25 01:00:35,549:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5860 (13.4704)
2022-11-25 01:00:35,550:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9217 (13.5872)
2022-11-25 01:00:35,627:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7063 (13.6122)
2022-11-25 01:00:35,634:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4567 (13.5877)
2022-11-25 01:00:35,634:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6503 (13.5967)
2022-11-25 01:00:35,635:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2979 (13.5614)
2022-11-25 01:00:35,636:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3064 (13.5316)
2022-11-25 01:00:35,637:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9167 (13.4691)
2022-11-25 01:00:35,639:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8491 (13.4064)
2022-11-25 01:00:35,640:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0615 (13.3776)
2022-11-25 01:00:35,641:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5978 (13.3157)
2022-11-25 01:00:35,642:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4065 (13.2502)
2022-11-25 01:00:35,643:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2505 (13.1843)
2022-11-25 01:00:35,644:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9470 (13.1006)
2022-11-25 01:00:35,645:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9082 (13.0389)
2022-11-25 01:00:35,647:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4848 (12.9626)
2022-11-25 01:00:35,693:INFO: - Computing ADE (validation o)
2022-11-25 01:00:35,957:INFO: 		 ADE on eth                       dataset:	 1.0697312355041504
2022-11-25 01:00:35,958:INFO: Average validation o:	ADE  1.0697	FDE  2.1660
2022-11-25 01:00:35,958:INFO: - Computing loss (validation)
2022-11-25 01:00:36,162:INFO: Dataset: hotel               Batch: 1/2	Loss 34.0034 (34.0034)
2022-11-25 01:00:36,163:INFO: Dataset: hotel               Batch: 2/2	Loss 32.6016 (33.9077)
2022-11-25 01:00:36,409:INFO: Dataset: univ                Batch: 1/3	Loss 11.6936 (11.6936)
2022-11-25 01:00:36,410:INFO: Dataset: univ                Batch: 2/3	Loss 11.6446 (11.6695)
2022-11-25 01:00:36,412:INFO: Dataset: univ                Batch: 3/3	Loss 11.9472 (11.7538)
2022-11-25 01:00:36,678:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4580 (29.4580)
2022-11-25 01:00:36,679:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1682 (29.3834)
2022-11-25 01:00:36,929:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5165 (11.5165)
2022-11-25 01:00:36,931:INFO: Dataset: zara2               Batch: 2/5	Loss 11.7429 (11.6319)
2022-11-25 01:00:36,940:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4435 (11.5678)
2022-11-25 01:00:36,946:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6717 (11.5947)
2022-11-25 01:00:36,946:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4490 (11.5655)
2022-11-25 01:00:37,003:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_635.pth.tar
2022-11-25 01:00:37,003:INFO: 
===> EPOCH: 636 (P4)
2022-11-25 01:00:37,003:INFO: - Computing loss (training)
2022-11-25 01:00:37,212:INFO: Dataset: hotel               Batch: 1/4	Loss 28.4958 (28.4958)
2022-11-25 01:00:37,214:INFO: Dataset: hotel               Batch: 2/4	Loss 27.9746 (28.2358)
2022-11-25 01:00:37,221:INFO: Dataset: hotel               Batch: 3/4	Loss 29.0070 (28.4965)
2022-11-25 01:00:37,223:INFO: Dataset: hotel               Batch: 4/4	Loss 26.0904 (28.1093)
2022-11-25 01:00:37,524:INFO: Dataset: univ                Batch:  1/15	Loss 12.3565 (12.3565)
2022-11-25 01:00:37,525:INFO: Dataset: univ                Batch:  2/15	Loss 12.5976 (12.4765)
2022-11-25 01:00:37,526:INFO: Dataset: univ                Batch:  3/15	Loss 12.7207 (12.5563)
2022-11-25 01:00:37,528:INFO: Dataset: univ                Batch:  4/15	Loss 12.6308 (12.5731)
2022-11-25 01:00:37,529:INFO: Dataset: univ                Batch:  5/15	Loss 12.6615 (12.5903)
2022-11-25 01:00:37,534:INFO: Dataset: univ                Batch:  6/15	Loss 12.6390 (12.5976)
2022-11-25 01:00:37,535:INFO: Dataset: univ                Batch:  7/15	Loss 12.3924 (12.5681)
2022-11-25 01:00:37,536:INFO: Dataset: univ                Batch:  8/15	Loss 12.1862 (12.5216)
2022-11-25 01:00:37,537:INFO: Dataset: univ                Batch:  9/15	Loss 12.1440 (12.4815)
2022-11-25 01:00:37,538:INFO: Dataset: univ                Batch: 10/15	Loss 12.0099 (12.4370)
2022-11-25 01:00:37,540:INFO: Dataset: univ                Batch: 11/15	Loss 11.7446 (12.3733)
2022-11-25 01:00:37,541:INFO: Dataset: univ                Batch: 12/15	Loss 11.2772 (12.2793)
2022-11-25 01:00:37,543:INFO: Dataset: univ                Batch: 13/15	Loss 10.9898 (12.1876)
2022-11-25 01:00:37,544:INFO: Dataset: univ                Batch: 14/15	Loss 10.7331 (12.0913)
2022-11-25 01:00:37,545:INFO: Dataset: univ                Batch: 15/15	Loss 10.4963 (12.0772)
2022-11-25 01:00:37,821:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4400 (32.4400)
2022-11-25 01:00:37,822:INFO: Dataset: zara1               Batch: 2/8	Loss 32.3485 (32.3935)
2022-11-25 01:00:37,824:INFO: Dataset: zara1               Batch: 3/8	Loss 32.2798 (32.3565)
2022-11-25 01:00:37,828:INFO: Dataset: zara1               Batch: 4/8	Loss 32.0131 (32.2649)
2022-11-25 01:00:37,830:INFO: Dataset: zara1               Batch: 5/8	Loss 31.6762 (32.1544)
2022-11-25 01:00:37,845:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2847 (32.0174)
2022-11-25 01:00:37,846:INFO: Dataset: zara1               Batch: 7/8	Loss 30.8676 (31.8503)
2022-11-25 01:00:37,847:INFO: Dataset: zara1               Batch: 8/8	Loss 30.0323 (31.6389)
2022-11-25 01:00:38,107:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8211 (13.8211)
2022-11-25 01:00:38,108:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6120 (13.7177)
2022-11-25 01:00:38,143:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7405 (13.7252)
2022-11-25 01:00:38,145:INFO: Dataset: zara2               Batch:  4/18	Loss 13.5518 (13.6790)
2022-11-25 01:00:38,146:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4987 (13.6400)
2022-11-25 01:00:38,161:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5811 (13.6299)
2022-11-25 01:00:38,162:INFO: Dataset: zara2               Batch:  7/18	Loss 13.0385 (13.5447)
2022-11-25 01:00:38,162:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4987 (13.5389)
2022-11-25 01:00:38,163:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2520 (13.5023)
2022-11-25 01:00:38,164:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0311 (13.4553)
2022-11-25 01:00:38,165:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0831 (13.4225)
2022-11-25 01:00:38,167:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8215 (13.3732)
2022-11-25 01:00:38,168:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8136 (13.3309)
2022-11-25 01:00:38,169:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1790 (13.2472)
2022-11-25 01:00:38,170:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1074 (13.1819)
2022-11-25 01:00:38,171:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0502 (13.1142)
2022-11-25 01:00:38,172:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9661 (13.0535)
2022-11-25 01:00:38,173:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4874 (12.9810)
2022-11-25 01:00:38,221:INFO: - Computing ADE (validation o)
2022-11-25 01:00:38,508:INFO: 		 ADE on eth                       dataset:	 1.0649679899215698
2022-11-25 01:00:38,508:INFO: Average validation o:	ADE  1.0650	FDE  2.1570
2022-11-25 01:00:38,509:INFO: - Computing loss (validation)
2022-11-25 01:00:38,705:INFO: Dataset: hotel               Batch: 1/2	Loss 33.6867 (33.6867)
2022-11-25 01:00:38,706:INFO: Dataset: hotel               Batch: 2/2	Loss 37.0304 (33.9264)
2022-11-25 01:00:38,962:INFO: Dataset: univ                Batch: 1/3	Loss 11.6797 (11.6797)
2022-11-25 01:00:38,963:INFO: Dataset: univ                Batch: 2/3	Loss 12.2122 (11.9222)
2022-11-25 01:00:38,965:INFO: Dataset: univ                Batch: 3/3	Loss 11.6371 (11.8241)
2022-11-25 01:00:39,204:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4019 (29.4019)
2022-11-25 01:00:39,206:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1406 (29.3347)
2022-11-25 01:00:39,477:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4897 (11.4897)
2022-11-25 01:00:39,479:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5604 (11.5262)
2022-11-25 01:00:39,487:INFO: Dataset: zara2               Batch: 3/5	Loss 11.7237 (11.5908)
2022-11-25 01:00:39,487:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4318 (11.5543)
2022-11-25 01:00:39,488:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3950 (11.5238)
2022-11-25 01:00:39,544:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_636.pth.tar
2022-11-25 01:00:39,544:INFO: 
===> EPOCH: 637 (P4)
2022-11-25 01:00:39,544:INFO: - Computing loss (training)
2022-11-25 01:00:39,754:INFO: Dataset: hotel               Batch: 1/4	Loss 27.9236 (27.9236)
2022-11-25 01:00:39,756:INFO: Dataset: hotel               Batch: 2/4	Loss 30.2136 (29.1224)
2022-11-25 01:00:39,758:INFO: Dataset: hotel               Batch: 3/4	Loss 27.3210 (28.5314)
2022-11-25 01:00:39,760:INFO: Dataset: hotel               Batch: 4/4	Loss 26.2206 (28.1534)
2022-11-25 01:00:40,019:INFO: Dataset: univ                Batch:  1/15	Loss 12.6666 (12.6666)
2022-11-25 01:00:40,021:INFO: Dataset: univ                Batch:  2/15	Loss 12.7854 (12.7304)
2022-11-25 01:00:40,022:INFO: Dataset: univ                Batch:  3/15	Loss 12.5145 (12.6546)
2022-11-25 01:00:40,024:INFO: Dataset: univ                Batch:  4/15	Loss 12.7358 (12.6743)
2022-11-25 01:00:40,037:INFO: Dataset: univ                Batch:  5/15	Loss 12.5853 (12.6565)
2022-11-25 01:00:40,043:INFO: Dataset: univ                Batch:  6/15	Loss 12.4572 (12.6248)
2022-11-25 01:00:40,044:INFO: Dataset: univ                Batch:  7/15	Loss 12.3591 (12.5863)
2022-11-25 01:00:40,045:INFO: Dataset: univ                Batch:  8/15	Loss 12.3380 (12.5584)
2022-11-25 01:00:40,046:INFO: Dataset: univ                Batch:  9/15	Loss 12.1409 (12.5128)
2022-11-25 01:00:40,047:INFO: Dataset: univ                Batch: 10/15	Loss 11.7891 (12.4462)
2022-11-25 01:00:40,049:INFO: Dataset: univ                Batch: 11/15	Loss 11.5436 (12.3654)
2022-11-25 01:00:40,051:INFO: Dataset: univ                Batch: 12/15	Loss 11.5868 (12.3091)
2022-11-25 01:00:40,052:INFO: Dataset: univ                Batch: 13/15	Loss 10.9429 (12.1989)
2022-11-25 01:00:40,053:INFO: Dataset: univ                Batch: 14/15	Loss 10.5987 (12.0937)
2022-11-25 01:00:40,054:INFO: Dataset: univ                Batch: 15/15	Loss 10.4919 (12.0735)
2022-11-25 01:00:40,316:INFO: Dataset: zara1               Batch: 1/8	Loss 32.3982 (32.3982)
2022-11-25 01:00:40,321:INFO: Dataset: zara1               Batch: 2/8	Loss 32.3349 (32.3660)
2022-11-25 01:00:40,322:INFO: Dataset: zara1               Batch: 3/8	Loss 31.8595 (32.1995)
2022-11-25 01:00:40,323:INFO: Dataset: zara1               Batch: 4/8	Loss 31.7285 (32.0794)
2022-11-25 01:00:40,326:INFO: Dataset: zara1               Batch: 5/8	Loss 31.5537 (31.9773)
2022-11-25 01:00:40,333:INFO: Dataset: zara1               Batch: 6/8	Loss 32.0472 (31.9890)
2022-11-25 01:00:40,334:INFO: Dataset: zara1               Batch: 7/8	Loss 30.4344 (31.7726)
2022-11-25 01:00:40,334:INFO: Dataset: zara1               Batch: 8/8	Loss 30.6415 (31.6595)
2022-11-25 01:00:40,609:INFO: Dataset: zara2               Batch:  1/18	Loss 13.3559 (13.3559)
2022-11-25 01:00:40,616:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3465 (13.3509)
2022-11-25 01:00:40,617:INFO: Dataset: zara2               Batch:  3/18	Loss 13.9013 (13.5360)
2022-11-25 01:00:40,620:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3007 (13.4769)
2022-11-25 01:00:40,687:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5759 (13.4959)
2022-11-25 01:00:40,694:INFO: Dataset: zara2               Batch:  6/18	Loss 13.8349 (13.5521)
2022-11-25 01:00:40,695:INFO: Dataset: zara2               Batch:  7/18	Loss 13.1708 (13.4953)
2022-11-25 01:00:40,696:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2957 (13.4712)
2022-11-25 01:00:40,697:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2661 (13.4493)
2022-11-25 01:00:40,698:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9239 (13.3970)
2022-11-25 01:00:40,700:INFO: Dataset: zara2               Batch: 11/18	Loss 12.6761 (13.3311)
2022-11-25 01:00:40,701:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0902 (13.3096)
2022-11-25 01:00:40,702:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4040 (13.2365)
2022-11-25 01:00:40,703:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4304 (13.1796)
2022-11-25 01:00:40,704:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3778 (13.1281)
2022-11-25 01:00:40,705:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2600 (13.0803)
2022-11-25 01:00:40,707:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6806 (12.9970)
2022-11-25 01:00:40,708:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6136 (12.9268)
2022-11-25 01:00:40,755:INFO: - Computing ADE (validation o)
2022-11-25 01:00:41,028:INFO: 		 ADE on eth                       dataset:	 1.052889108657837
2022-11-25 01:00:41,028:INFO: Average validation o:	ADE  1.0529	FDE  2.1620
2022-11-25 01:00:41,028:INFO: - Computing loss (validation)
2022-11-25 01:00:41,235:INFO: Dataset: hotel               Batch: 1/2	Loss 33.5239 (33.5239)
2022-11-25 01:00:41,236:INFO: Dataset: hotel               Batch: 2/2	Loss 35.4816 (33.6842)
2022-11-25 01:00:41,494:INFO: Dataset: univ                Batch: 1/3	Loss 11.9518 (11.9518)
2022-11-25 01:00:41,496:INFO: Dataset: univ                Batch: 2/3	Loss 11.6285 (11.7740)
2022-11-25 01:00:41,498:INFO: Dataset: univ                Batch: 3/3	Loss 11.5140 (11.6925)
2022-11-25 01:00:41,744:INFO: Dataset: zara1               Batch: 1/2	Loss 29.4985 (29.4985)
2022-11-25 01:00:41,745:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1179 (29.3994)
2022-11-25 01:00:41,994:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5101 (11.5101)
2022-11-25 01:00:41,995:INFO: Dataset: zara2               Batch: 2/5	Loss 11.7042 (11.6062)
2022-11-25 01:00:41,996:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4885 (11.5670)
2022-11-25 01:00:42,005:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5656 (11.5667)
2022-11-25 01:00:42,006:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5012 (11.5543)
2022-11-25 01:00:42,073:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_637.pth.tar
2022-11-25 01:00:42,073:INFO: 
===> EPOCH: 638 (P4)
2022-11-25 01:00:42,073:INFO: - Computing loss (training)
2022-11-25 01:00:42,277:INFO: Dataset: hotel               Batch: 1/4	Loss 26.8575 (26.8575)
2022-11-25 01:00:42,280:INFO: Dataset: hotel               Batch: 2/4	Loss 28.6863 (27.8064)
2022-11-25 01:00:42,289:INFO: Dataset: hotel               Batch: 3/4	Loss 27.3764 (27.6662)
2022-11-25 01:00:42,291:INFO: Dataset: hotel               Batch: 4/4	Loss 28.3372 (27.7804)
2022-11-25 01:00:42,561:INFO: Dataset: univ                Batch:  1/15	Loss 12.5569 (12.5569)
2022-11-25 01:00:42,563:INFO: Dataset: univ                Batch:  2/15	Loss 12.2102 (12.3778)
2022-11-25 01:00:42,603:INFO: Dataset: univ                Batch:  3/15	Loss 12.3107 (12.3559)
2022-11-25 01:00:42,604:INFO: Dataset: univ                Batch:  4/15	Loss 12.6476 (12.4247)
2022-11-25 01:00:42,605:INFO: Dataset: univ                Batch:  5/15	Loss 12.5945 (12.4576)
2022-11-25 01:00:42,625:INFO: Dataset: univ                Batch:  6/15	Loss 12.5868 (12.4805)
2022-11-25 01:00:42,626:INFO: Dataset: univ                Batch:  7/15	Loss 12.5521 (12.4911)
2022-11-25 01:00:42,627:INFO: Dataset: univ                Batch:  8/15	Loss 11.9606 (12.4218)
2022-11-25 01:00:42,628:INFO: Dataset: univ                Batch:  9/15	Loss 11.8817 (12.3637)
2022-11-25 01:00:42,629:INFO: Dataset: univ                Batch: 10/15	Loss 11.7079 (12.2982)
2022-11-25 01:00:42,630:INFO: Dataset: univ                Batch: 11/15	Loss 11.4167 (12.2199)
2022-11-25 01:00:42,632:INFO: Dataset: univ                Batch: 12/15	Loss 11.4406 (12.1580)
2022-11-25 01:00:42,633:INFO: Dataset: univ                Batch: 13/15	Loss 11.2677 (12.0909)
2022-11-25 01:00:42,634:INFO: Dataset: univ                Batch: 14/15	Loss 10.6692 (11.9871)
2022-11-25 01:00:42,635:INFO: Dataset: univ                Batch: 15/15	Loss 10.8978 (11.9748)
2022-11-25 01:00:42,899:INFO: Dataset: zara1               Batch: 1/8	Loss 32.0119 (32.0119)
2022-11-25 01:00:42,902:INFO: Dataset: zara1               Batch: 2/8	Loss 32.2525 (32.1282)
2022-11-25 01:00:42,910:INFO: Dataset: zara1               Batch: 3/8	Loss 32.3350 (32.1960)
2022-11-25 01:00:42,914:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2795 (32.2196)
2022-11-25 01:00:42,921:INFO: Dataset: zara1               Batch: 5/8	Loss 31.6422 (32.0965)
2022-11-25 01:00:42,972:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2342 (31.9284)
2022-11-25 01:00:42,976:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7324 (31.7551)
2022-11-25 01:00:42,979:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2663 (31.6015)
2022-11-25 01:00:43,271:INFO: Dataset: zara2               Batch:  1/18	Loss 13.8555 (13.8555)
2022-11-25 01:00:43,274:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3293 (13.6083)
2022-11-25 01:00:43,275:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3800 (13.5414)
2022-11-25 01:00:43,278:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7145 (13.5791)
2022-11-25 01:00:43,338:INFO: Dataset: zara2               Batch:  5/18	Loss 14.0466 (13.6815)
2022-11-25 01:00:43,345:INFO: Dataset: zara2               Batch:  6/18	Loss 13.2451 (13.6080)
2022-11-25 01:00:43,346:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4857 (13.5889)
2022-11-25 01:00:43,347:INFO: Dataset: zara2               Batch:  8/18	Loss 13.0015 (13.5155)
2022-11-25 01:00:43,348:INFO: Dataset: zara2               Batch:  9/18	Loss 13.1105 (13.4691)
2022-11-25 01:00:43,349:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9761 (13.4145)
2022-11-25 01:00:43,351:INFO: Dataset: zara2               Batch: 11/18	Loss 12.6987 (13.3476)
2022-11-25 01:00:43,352:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6474 (13.2897)
2022-11-25 01:00:43,353:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4353 (13.2240)
2022-11-25 01:00:43,353:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3436 (13.1591)
2022-11-25 01:00:43,354:INFO: Dataset: zara2               Batch: 15/18	Loss 12.4540 (13.1133)
2022-11-25 01:00:43,355:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0948 (13.0476)
2022-11-25 01:00:43,357:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8673 (12.9828)
2022-11-25 01:00:43,358:INFO: Dataset: zara2               Batch: 18/18	Loss 11.9151 (12.9376)
2022-11-25 01:00:43,405:INFO: - Computing ADE (validation o)
2022-11-25 01:00:43,677:INFO: 		 ADE on eth                       dataset:	 1.0438182353973389
2022-11-25 01:00:43,678:INFO: Average validation o:	ADE  1.0438	FDE  2.1354
2022-11-25 01:00:43,678:INFO: - Computing loss (validation)
2022-11-25 01:00:43,868:INFO: Dataset: hotel               Batch: 1/2	Loss 33.4108 (33.4108)
2022-11-25 01:00:43,869:INFO: Dataset: hotel               Batch: 2/2	Loss 38.5103 (33.7415)
2022-11-25 01:00:44,123:INFO: Dataset: univ                Batch: 1/3	Loss 11.4599 (11.4599)
2022-11-25 01:00:44,124:INFO: Dataset: univ                Batch: 2/3	Loss 11.5290 (11.4940)
2022-11-25 01:00:44,126:INFO: Dataset: univ                Batch: 3/3	Loss 12.2013 (11.6724)
2022-11-25 01:00:44,380:INFO: Dataset: zara1               Batch: 1/2	Loss 29.1968 (29.1968)
2022-11-25 01:00:44,381:INFO: Dataset: zara1               Batch: 2/2	Loss 29.5616 (29.2777)
2022-11-25 01:00:44,640:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5150 (11.5150)
2022-11-25 01:00:44,641:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4771 (11.4961)
2022-11-25 01:00:44,642:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6384 (11.5463)
2022-11-25 01:00:44,651:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5697 (11.5523)
2022-11-25 01:00:44,651:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5234 (11.5469)
2022-11-25 01:00:44,704:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_638.pth.tar
2022-11-25 01:00:44,704:INFO: 
===> EPOCH: 639 (P4)
2022-11-25 01:00:44,705:INFO: - Computing loss (training)
2022-11-25 01:00:44,905:INFO: Dataset: hotel               Batch: 1/4	Loss 27.5480 (27.5480)
2022-11-25 01:00:44,907:INFO: Dataset: hotel               Batch: 2/4	Loss 28.6917 (28.1090)
2022-11-25 01:00:44,909:INFO: Dataset: hotel               Batch: 3/4	Loss 27.2310 (27.8296)
2022-11-25 01:00:44,911:INFO: Dataset: hotel               Batch: 4/4	Loss 28.0478 (27.8687)
2022-11-25 01:00:45,182:INFO: Dataset: univ                Batch:  1/15	Loss 12.4886 (12.4886)
2022-11-25 01:00:45,183:INFO: Dataset: univ                Batch:  2/15	Loss 12.5339 (12.5094)
2022-11-25 01:00:45,186:INFO: Dataset: univ                Batch:  3/15	Loss 12.3839 (12.4671)
2022-11-25 01:00:45,188:INFO: Dataset: univ                Batch:  4/15	Loss 12.6302 (12.5065)
2022-11-25 01:00:45,210:INFO: Dataset: univ                Batch:  5/15	Loss 12.5740 (12.5200)
2022-11-25 01:00:45,225:INFO: Dataset: univ                Batch:  6/15	Loss 12.4603 (12.5089)
2022-11-25 01:00:45,226:INFO: Dataset: univ                Batch:  7/15	Loss 12.2198 (12.4691)
2022-11-25 01:00:45,227:INFO: Dataset: univ                Batch:  8/15	Loss 11.9313 (12.4015)
2022-11-25 01:00:45,228:INFO: Dataset: univ                Batch:  9/15	Loss 11.9377 (12.3497)
2022-11-25 01:00:45,229:INFO: Dataset: univ                Batch: 10/15	Loss 11.9439 (12.3096)
2022-11-25 01:00:45,230:INFO: Dataset: univ                Batch: 11/15	Loss 11.6192 (12.2458)
2022-11-25 01:00:45,232:INFO: Dataset: univ                Batch: 12/15	Loss 11.3063 (12.1652)
2022-11-25 01:00:45,233:INFO: Dataset: univ                Batch: 13/15	Loss 10.8381 (12.0579)
2022-11-25 01:00:45,234:INFO: Dataset: univ                Batch: 14/15	Loss 10.8050 (11.9656)
2022-11-25 01:00:45,235:INFO: Dataset: univ                Batch: 15/15	Loss 10.0529 (11.9413)
2022-11-25 01:00:45,515:INFO: Dataset: zara1               Batch: 1/8	Loss 32.5989 (32.5989)
2022-11-25 01:00:45,518:INFO: Dataset: zara1               Batch: 2/8	Loss 32.3805 (32.4843)
2022-11-25 01:00:45,519:INFO: Dataset: zara1               Batch: 3/8	Loss 31.6976 (32.2065)
2022-11-25 01:00:45,520:INFO: Dataset: zara1               Batch: 4/8	Loss 31.6847 (32.0809)
2022-11-25 01:00:45,522:INFO: Dataset: zara1               Batch: 5/8	Loss 31.5113 (31.9649)
2022-11-25 01:00:45,563:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2589 (31.8415)
2022-11-25 01:00:45,566:INFO: Dataset: zara1               Batch: 7/8	Loss 30.8328 (31.6996)
2022-11-25 01:00:45,570:INFO: Dataset: zara1               Batch: 8/8	Loss 30.0849 (31.5219)
2022-11-25 01:00:45,838:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5831 (13.5831)
2022-11-25 01:00:45,840:INFO: Dataset: zara2               Batch:  2/18	Loss 13.5760 (13.5795)
2022-11-25 01:00:45,844:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3008 (13.4878)
2022-11-25 01:00:45,845:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4115 (13.4677)
2022-11-25 01:00:45,860:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7625 (13.5275)
2022-11-25 01:00:45,866:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3341 (13.4953)
2022-11-25 01:00:45,867:INFO: Dataset: zara2               Batch:  7/18	Loss 13.6330 (13.5159)
2022-11-25 01:00:45,868:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2298 (13.4790)
2022-11-25 01:00:45,869:INFO: Dataset: zara2               Batch:  9/18	Loss 12.8192 (13.3984)
2022-11-25 01:00:45,870:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9787 (13.3585)
2022-11-25 01:00:45,871:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8771 (13.3150)
2022-11-25 01:00:45,872:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0215 (13.2914)
2022-11-25 01:00:45,873:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4010 (13.2124)
2022-11-25 01:00:45,874:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1291 (13.1322)
2022-11-25 01:00:45,875:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1198 (13.0622)
2022-11-25 01:00:45,876:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7962 (12.9819)
2022-11-25 01:00:45,877:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8098 (12.9149)
2022-11-25 01:00:45,878:INFO: Dataset: zara2               Batch: 18/18	Loss 11.1420 (12.8378)
2022-11-25 01:00:45,926:INFO: - Computing ADE (validation o)
2022-11-25 01:00:46,198:INFO: 		 ADE on eth                       dataset:	 1.0279276371002197
2022-11-25 01:00:46,198:INFO: Average validation o:	ADE  1.0279	FDE  2.1003
2022-11-25 01:00:46,199:INFO: - Computing loss (validation)
2022-11-25 01:00:46,397:INFO: Dataset: hotel               Batch: 1/2	Loss 33.9101 (33.9101)
2022-11-25 01:00:46,398:INFO: Dataset: hotel               Batch: 2/2	Loss 38.4549 (34.2203)
2022-11-25 01:00:46,653:INFO: Dataset: univ                Batch: 1/3	Loss 11.7454 (11.7454)
2022-11-25 01:00:46,657:INFO: Dataset: univ                Batch: 2/3	Loss 11.9796 (11.8629)
2022-11-25 01:00:46,657:INFO: Dataset: univ                Batch: 3/3	Loss 12.1425 (11.9458)
2022-11-25 01:00:46,903:INFO: Dataset: zara1               Batch: 1/2	Loss 29.7645 (29.7645)
2022-11-25 01:00:46,905:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1679 (29.6187)
2022-11-25 01:00:47,172:INFO: Dataset: zara2               Batch: 1/5	Loss 11.1104 (11.1104)
2022-11-25 01:00:47,174:INFO: Dataset: zara2               Batch: 2/5	Loss 11.0136 (11.0622)
2022-11-25 01:00:47,177:INFO: Dataset: zara2               Batch: 3/5	Loss 11.0567 (11.0604)
2022-11-25 01:00:47,192:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2210 (11.0990)
2022-11-25 01:00:47,193:INFO: Dataset: zara2               Batch: 5/5	Loss 11.1805 (11.1166)
2022-11-25 01:00:47,247:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_639.pth.tar
2022-11-25 01:00:47,247:INFO: 
===> EPOCH: 640 (P4)
2022-11-25 01:00:47,247:INFO: - Computing loss (training)
2022-11-25 01:00:47,453:INFO: Dataset: hotel               Batch: 1/4	Loss 28.0212 (28.0212)
2022-11-25 01:00:47,462:INFO: Dataset: hotel               Batch: 2/4	Loss 30.7074 (29.3706)
2022-11-25 01:00:47,466:INFO: Dataset: hotel               Batch: 3/4	Loss 26.5907 (28.4705)
2022-11-25 01:00:47,473:INFO: Dataset: hotel               Batch: 4/4	Loss 26.4784 (28.1341)
2022-11-25 01:00:47,744:INFO: Dataset: univ                Batch:  1/15	Loss 12.7988 (12.7988)
2022-11-25 01:00:47,803:INFO: Dataset: univ                Batch:  2/15	Loss 12.6873 (12.7432)
2022-11-25 01:00:47,807:INFO: Dataset: univ                Batch:  3/15	Loss 13.4173 (12.9403)
2022-11-25 01:00:47,808:INFO: Dataset: univ                Batch:  4/15	Loss 12.9182 (12.9341)
2022-11-25 01:00:47,809:INFO: Dataset: univ                Batch:  5/15	Loss 12.9271 (12.9327)
2022-11-25 01:00:47,812:INFO: Dataset: univ                Batch:  6/15	Loss 12.5866 (12.8795)
2022-11-25 01:00:47,814:INFO: Dataset: univ                Batch:  7/15	Loss 12.5794 (12.8381)
2022-11-25 01:00:47,815:INFO: Dataset: univ                Batch:  8/15	Loss 12.1757 (12.7465)
2022-11-25 01:00:47,816:INFO: Dataset: univ                Batch:  9/15	Loss 11.9814 (12.6652)
2022-11-25 01:00:47,817:INFO: Dataset: univ                Batch: 10/15	Loss 11.7642 (12.5724)
2022-11-25 01:00:47,818:INFO: Dataset: univ                Batch: 11/15	Loss 11.1058 (12.4233)
2022-11-25 01:00:47,820:INFO: Dataset: univ                Batch: 12/15	Loss 10.9760 (12.2931)
2022-11-25 01:00:47,821:INFO: Dataset: univ                Batch: 13/15	Loss 10.7496 (12.1701)
2022-11-25 01:00:47,823:INFO: Dataset: univ                Batch: 14/15	Loss 10.4549 (12.0499)
2022-11-25 01:00:47,825:INFO: Dataset: univ                Batch: 15/15	Loss 11.0609 (12.0389)
2022-11-25 01:00:48,082:INFO: Dataset: zara1               Batch: 1/8	Loss 32.4763 (32.4763)
2022-11-25 01:00:48,084:INFO: Dataset: zara1               Batch: 2/8	Loss 31.9575 (32.2247)
2022-11-25 01:00:48,087:INFO: Dataset: zara1               Batch: 3/8	Loss 31.6117 (32.0440)
2022-11-25 01:00:48,089:INFO: Dataset: zara1               Batch: 4/8	Loss 31.8706 (32.0006)
2022-11-25 01:00:48,097:INFO: Dataset: zara1               Batch: 5/8	Loss 31.3520 (31.8643)
2022-11-25 01:00:48,149:INFO: Dataset: zara1               Batch: 6/8	Loss 31.1345 (31.7322)
2022-11-25 01:00:48,150:INFO: Dataset: zara1               Batch: 7/8	Loss 30.5182 (31.5485)
2022-11-25 01:00:48,151:INFO: Dataset: zara1               Batch: 8/8	Loss 30.7477 (31.4663)
2022-11-25 01:00:48,436:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5230 (13.5230)
2022-11-25 01:00:48,468:INFO: Dataset: zara2               Batch:  2/18	Loss 14.0155 (13.7651)
2022-11-25 01:00:48,470:INFO: Dataset: zara2               Batch:  3/18	Loss 13.7413 (13.7582)
2022-11-25 01:00:48,471:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9128 (13.7998)
2022-11-25 01:00:48,472:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5227 (13.7459)
2022-11-25 01:00:48,481:INFO: Dataset: zara2               Batch:  6/18	Loss 13.2236 (13.6566)
2022-11-25 01:00:48,482:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4458 (13.6255)
2022-11-25 01:00:48,483:INFO: Dataset: zara2               Batch:  8/18	Loss 13.5602 (13.6183)
2022-11-25 01:00:48,484:INFO: Dataset: zara2               Batch:  9/18	Loss 13.3049 (13.5814)
2022-11-25 01:00:48,485:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0165 (13.5243)
2022-11-25 01:00:48,486:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8279 (13.4640)
2022-11-25 01:00:48,488:INFO: Dataset: zara2               Batch: 12/18	Loss 13.2085 (13.4428)
2022-11-25 01:00:48,489:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6439 (13.3740)
2022-11-25 01:00:48,489:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3875 (13.3092)
2022-11-25 01:00:48,490:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0092 (13.2231)
2022-11-25 01:00:48,491:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1229 (13.1494)
2022-11-25 01:00:48,493:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7892 (13.0714)
2022-11-25 01:00:48,495:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6145 (12.9914)
2022-11-25 01:00:48,539:INFO: - Computing ADE (validation o)
2022-11-25 01:00:48,809:INFO: 		 ADE on eth                       dataset:	 1.042822241783142
2022-11-25 01:00:48,809:INFO: Average validation o:	ADE  1.0428	FDE  2.1394
2022-11-25 01:00:48,809:INFO: - Computing loss (validation)
2022-11-25 01:00:49,007:INFO: Dataset: hotel               Batch: 1/2	Loss 33.4081 (33.4081)
2022-11-25 01:00:49,008:INFO: Dataset: hotel               Batch: 2/2	Loss 36.7458 (33.6359)
2022-11-25 01:00:49,262:INFO: Dataset: univ                Batch: 1/3	Loss 11.5610 (11.5610)
2022-11-25 01:00:49,264:INFO: Dataset: univ                Batch: 2/3	Loss 11.6110 (11.5846)
2022-11-25 01:00:49,265:INFO: Dataset: univ                Batch: 3/3	Loss 11.5033 (11.5593)
2022-11-25 01:00:49,512:INFO: Dataset: zara1               Batch: 1/2	Loss 29.1623 (29.1623)
2022-11-25 01:00:49,513:INFO: Dataset: zara1               Batch: 2/2	Loss 28.9486 (29.1114)
2022-11-25 01:00:49,784:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5346 (11.5346)
2022-11-25 01:00:49,788:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6921 (11.6137)
2022-11-25 01:00:49,790:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5037 (11.5782)
2022-11-25 01:00:49,791:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5313 (11.5665)
2022-11-25 01:00:49,792:INFO: Dataset: zara2               Batch: 5/5	Loss 11.6743 (11.5881)
2022-11-25 01:00:49,846:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_640.pth.tar
2022-11-25 01:00:49,846:INFO: 
===> EPOCH: 641 (P4)
2022-11-25 01:00:49,847:INFO: - Computing loss (training)
2022-11-25 01:00:50,053:INFO: Dataset: hotel               Batch: 1/4	Loss 27.6253 (27.6253)
2022-11-25 01:00:50,055:INFO: Dataset: hotel               Batch: 2/4	Loss 28.8582 (28.2559)
2022-11-25 01:00:50,060:INFO: Dataset: hotel               Batch: 3/4	Loss 29.6601 (28.7020)
2022-11-25 01:00:50,062:INFO: Dataset: hotel               Batch: 4/4	Loss 22.3776 (27.7091)
2022-11-25 01:00:50,310:INFO: Dataset: univ                Batch:  1/15	Loss 12.4766 (12.4766)
2022-11-25 01:00:50,312:INFO: Dataset: univ                Batch:  2/15	Loss 12.7504 (12.6028)
2022-11-25 01:00:50,316:INFO: Dataset: univ                Batch:  3/15	Loss 12.2314 (12.4653)
2022-11-25 01:00:50,367:INFO: Dataset: univ                Batch:  4/15	Loss 12.2901 (12.4185)
2022-11-25 01:00:50,368:INFO: Dataset: univ                Batch:  5/15	Loss 12.4830 (12.4310)
2022-11-25 01:00:50,371:INFO: Dataset: univ                Batch:  6/15	Loss 12.4261 (12.4302)
2022-11-25 01:00:50,372:INFO: Dataset: univ                Batch:  7/15	Loss 12.1296 (12.3864)
2022-11-25 01:00:50,373:INFO: Dataset: univ                Batch:  8/15	Loss 12.0714 (12.3483)
2022-11-25 01:00:50,374:INFO: Dataset: univ                Batch:  9/15	Loss 11.8170 (12.2900)
2022-11-25 01:00:50,375:INFO: Dataset: univ                Batch: 10/15	Loss 11.7218 (12.2336)
2022-11-25 01:00:50,377:INFO: Dataset: univ                Batch: 11/15	Loss 11.3423 (12.1473)
2022-11-25 01:00:50,379:INFO: Dataset: univ                Batch: 12/15	Loss 11.0687 (12.0493)
2022-11-25 01:00:50,380:INFO: Dataset: univ                Batch: 13/15	Loss 11.0890 (11.9799)
2022-11-25 01:00:50,381:INFO: Dataset: univ                Batch: 14/15	Loss 10.5503 (11.8802)
2022-11-25 01:00:50,382:INFO: Dataset: univ                Batch: 15/15	Loss 10.5428 (11.8622)
2022-11-25 01:00:50,650:INFO: Dataset: zara1               Batch: 1/8	Loss 32.0375 (32.0375)
2022-11-25 01:00:50,654:INFO: Dataset: zara1               Batch: 2/8	Loss 32.1865 (32.1136)
2022-11-25 01:00:50,655:INFO: Dataset: zara1               Batch: 3/8	Loss 31.9676 (32.0621)
2022-11-25 01:00:50,659:INFO: Dataset: zara1               Batch: 4/8	Loss 31.7965 (31.9919)
2022-11-25 01:00:50,659:INFO: Dataset: zara1               Batch: 5/8	Loss 31.3316 (31.8706)
2022-11-25 01:00:50,699:INFO: Dataset: zara1               Batch: 6/8	Loss 31.0279 (31.7334)
2022-11-25 01:00:50,702:INFO: Dataset: zara1               Batch: 7/8	Loss 30.8764 (31.6023)
2022-11-25 01:00:50,706:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3730 (31.4807)
2022-11-25 01:00:50,996:INFO: Dataset: zara2               Batch:  1/18	Loss 13.7088 (13.7088)
2022-11-25 01:00:50,999:INFO: Dataset: zara2               Batch:  2/18	Loss 13.1605 (13.4454)
2022-11-25 01:00:51,002:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5471 (13.4818)
2022-11-25 01:00:51,063:INFO: Dataset: zara2               Batch:  4/18	Loss 13.2809 (13.4277)
2022-11-25 01:00:51,064:INFO: Dataset: zara2               Batch:  5/18	Loss 13.7271 (13.4863)
2022-11-25 01:00:51,067:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3462 (13.4602)
2022-11-25 01:00:51,068:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2095 (13.4233)
2022-11-25 01:00:51,069:INFO: Dataset: zara2               Batch:  8/18	Loss 13.4439 (13.4262)
2022-11-25 01:00:51,070:INFO: Dataset: zara2               Batch:  9/18	Loss 12.9859 (13.3776)
2022-11-25 01:00:51,071:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0694 (13.3455)
2022-11-25 01:00:51,072:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9920 (13.3153)
2022-11-25 01:00:51,074:INFO: Dataset: zara2               Batch: 12/18	Loss 12.9151 (13.2818)
2022-11-25 01:00:51,075:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5819 (13.2290)
2022-11-25 01:00:51,076:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3956 (13.1694)
2022-11-25 01:00:51,077:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0678 (13.0985)
2022-11-25 01:00:51,078:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0550 (13.0336)
2022-11-25 01:00:51,080:INFO: Dataset: zara2               Batch: 17/18	Loss 12.1488 (12.9783)
2022-11-25 01:00:51,082:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5526 (12.9024)
2022-11-25 01:00:51,129:INFO: - Computing ADE (validation o)
2022-11-25 01:00:51,404:INFO: 		 ADE on eth                       dataset:	 1.054299235343933
2022-11-25 01:00:51,411:INFO: Average validation o:	ADE  1.0543	FDE  2.1598
2022-11-25 01:00:51,412:INFO: - Computing loss (validation)
2022-11-25 01:00:51,609:INFO: Dataset: hotel               Batch: 1/2	Loss 33.7526 (33.7526)
2022-11-25 01:00:51,610:INFO: Dataset: hotel               Batch: 2/2	Loss 31.6982 (33.5843)
2022-11-25 01:00:51,859:INFO: Dataset: univ                Batch: 1/3	Loss 11.5985 (11.5985)
2022-11-25 01:00:51,868:INFO: Dataset: univ                Batch: 2/3	Loss 11.3452 (11.4622)
2022-11-25 01:00:51,869:INFO: Dataset: univ                Batch: 3/3	Loss 11.4778 (11.4672)
2022-11-25 01:00:52,132:INFO: Dataset: zara1               Batch: 1/2	Loss 29.1508 (29.1508)
2022-11-25 01:00:52,135:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1441 (29.1492)
2022-11-25 01:00:52,392:INFO: Dataset: zara2               Batch: 1/5	Loss 11.5559 (11.5559)
2022-11-25 01:00:52,393:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6425 (11.5979)
2022-11-25 01:00:52,396:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5922 (11.5960)
2022-11-25 01:00:52,396:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4706 (11.5650)
2022-11-25 01:00:52,396:INFO: Dataset: zara2               Batch: 5/5	Loss 11.6764 (11.5852)
2022-11-25 01:00:52,453:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_641.pth.tar
2022-11-25 01:00:52,453:INFO: 
===> EPOCH: 642 (P4)
2022-11-25 01:00:52,454:INFO: - Computing loss (training)
2022-11-25 01:00:52,661:INFO: Dataset: hotel               Batch: 1/4	Loss 27.8293 (27.8293)
2022-11-25 01:00:52,664:INFO: Dataset: hotel               Batch: 2/4	Loss 27.9072 (27.8691)
2022-11-25 01:00:52,666:INFO: Dataset: hotel               Batch: 3/4	Loss 28.0548 (27.9297)
2022-11-25 01:00:52,668:INFO: Dataset: hotel               Batch: 4/4	Loss 25.5285 (27.5084)
2022-11-25 01:00:52,933:INFO: Dataset: univ                Batch:  1/15	Loss 12.3643 (12.3643)
2022-11-25 01:00:52,935:INFO: Dataset: univ                Batch:  2/15	Loss 12.3769 (12.3702)
2022-11-25 01:00:52,937:INFO: Dataset: univ                Batch:  3/15	Loss 12.6477 (12.4677)
2022-11-25 01:00:52,940:INFO: Dataset: univ                Batch:  4/15	Loss 12.4441 (12.4618)
2022-11-25 01:00:52,976:INFO: Dataset: univ                Batch:  5/15	Loss 12.2615 (12.4193)
2022-11-25 01:00:52,980:INFO: Dataset: univ                Batch:  6/15	Loss 12.3532 (12.4083)
2022-11-25 01:00:52,981:INFO: Dataset: univ                Batch:  7/15	Loss 12.2533 (12.3862)
2022-11-25 01:00:52,982:INFO: Dataset: univ                Batch:  8/15	Loss 11.9735 (12.3345)
2022-11-25 01:00:52,984:INFO: Dataset: univ                Batch:  9/15	Loss 11.6022 (12.2458)
2022-11-25 01:00:52,985:INFO: Dataset: univ                Batch: 10/15	Loss 11.5516 (12.1811)
2022-11-25 01:00:52,986:INFO: Dataset: univ                Batch: 11/15	Loss 11.1469 (12.0847)
2022-11-25 01:00:52,988:INFO: Dataset: univ                Batch: 12/15	Loss 11.1845 (12.0089)
2022-11-25 01:00:52,989:INFO: Dataset: univ                Batch: 13/15	Loss 10.9495 (11.9201)
2022-11-25 01:00:52,990:INFO: Dataset: univ                Batch: 14/15	Loss 10.8838 (11.8464)
2022-11-25 01:00:52,991:INFO: Dataset: univ                Batch: 15/15	Loss 9.7835 (11.8153)
2022-11-25 01:00:53,247:INFO: Dataset: zara1               Batch: 1/8	Loss 31.9699 (31.9699)
2022-11-25 01:00:53,249:INFO: Dataset: zara1               Batch: 2/8	Loss 31.6327 (31.8140)
2022-11-25 01:00:53,251:INFO: Dataset: zara1               Batch: 3/8	Loss 31.8770 (31.8354)
2022-11-25 01:00:53,255:INFO: Dataset: zara1               Batch: 4/8	Loss 31.6529 (31.7882)
2022-11-25 01:00:53,256:INFO: Dataset: zara1               Batch: 5/8	Loss 31.6844 (31.7689)
2022-11-25 01:00:53,279:INFO: Dataset: zara1               Batch: 6/8	Loss 31.1825 (31.6786)
2022-11-25 01:00:53,280:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7944 (31.5665)
2022-11-25 01:00:53,281:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3172 (31.4173)
2022-11-25 01:00:53,550:INFO: Dataset: zara2               Batch:  1/18	Loss 13.5067 (13.5067)
2022-11-25 01:00:53,552:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4589 (13.4820)
2022-11-25 01:00:53,553:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5773 (13.5110)
2022-11-25 01:00:53,555:INFO: Dataset: zara2               Batch:  4/18	Loss 13.1830 (13.4340)
2022-11-25 01:00:53,614:INFO: Dataset: zara2               Batch:  5/18	Loss 13.2735 (13.4027)
2022-11-25 01:00:53,622:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3065 (13.3845)
2022-11-25 01:00:53,623:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4410 (13.3926)
2022-11-25 01:00:53,624:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2414 (13.3730)
2022-11-25 01:00:53,625:INFO: Dataset: zara2               Batch:  9/18	Loss 13.1765 (13.3478)
2022-11-25 01:00:53,627:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1428 (13.3292)
2022-11-25 01:00:53,629:INFO: Dataset: zara2               Batch: 11/18	Loss 13.3469 (13.3307)
2022-11-25 01:00:53,630:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6718 (13.2793)
2022-11-25 01:00:53,631:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7203 (13.2340)
2022-11-25 01:00:53,632:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4442 (13.1799)
2022-11-25 01:00:53,633:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2480 (13.1169)
2022-11-25 01:00:53,634:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2241 (13.0581)
2022-11-25 01:00:53,635:INFO: Dataset: zara2               Batch: 17/18	Loss 11.5507 (12.9614)
2022-11-25 01:00:53,637:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4057 (12.8897)
2022-11-25 01:00:53,686:INFO: - Computing ADE (validation o)
2022-11-25 01:00:53,957:INFO: 		 ADE on eth                       dataset:	 1.0809122323989868
2022-11-25 01:00:53,957:INFO: Average validation o:	ADE  1.0809	FDE  2.1636
2022-11-25 01:00:53,957:INFO: - Computing loss (validation)
2022-11-25 01:00:54,153:INFO: Dataset: hotel               Batch: 1/2	Loss 33.3861 (33.3861)
2022-11-25 01:00:54,154:INFO: Dataset: hotel               Batch: 2/2	Loss 36.0642 (33.5689)
2022-11-25 01:00:54,412:INFO: Dataset: univ                Batch: 1/3	Loss 11.3383 (11.3383)
2022-11-25 01:00:54,413:INFO: Dataset: univ                Batch: 2/3	Loss 11.5958 (11.4608)
2022-11-25 01:00:54,415:INFO: Dataset: univ                Batch: 3/3	Loss 11.6839 (11.5298)
2022-11-25 01:00:54,670:INFO: Dataset: zara1               Batch: 1/2	Loss 29.1405 (29.1405)
2022-11-25 01:00:54,673:INFO: Dataset: zara1               Batch: 2/2	Loss 28.7010 (29.0274)
2022-11-25 01:00:54,938:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3980 (11.3980)
2022-11-25 01:00:54,940:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6745 (11.5415)
2022-11-25 01:00:54,940:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6122 (11.5654)
2022-11-25 01:00:54,942:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5629 (11.5648)
2022-11-25 01:00:54,945:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4746 (11.5465)
2022-11-25 01:00:54,999:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_642.pth.tar
2022-11-25 01:00:54,999:INFO: 
===> EPOCH: 643 (P4)
2022-11-25 01:00:55,000:INFO: - Computing loss (training)
2022-11-25 01:00:55,205:INFO: Dataset: hotel               Batch: 1/4	Loss 27.6931 (27.6931)
2022-11-25 01:00:55,209:INFO: Dataset: hotel               Batch: 2/4	Loss 27.9696 (27.8347)
2022-11-25 01:00:55,210:INFO: Dataset: hotel               Batch: 3/4	Loss 27.3823 (27.6791)
2022-11-25 01:00:55,215:INFO: Dataset: hotel               Batch: 4/4	Loss 26.8144 (27.5239)
2022-11-25 01:00:55,477:INFO: Dataset: univ                Batch:  1/15	Loss 12.5050 (12.5050)
2022-11-25 01:00:55,480:INFO: Dataset: univ                Batch:  2/15	Loss 12.6817 (12.6013)
2022-11-25 01:00:55,484:INFO: Dataset: univ                Batch:  3/15	Loss 12.4764 (12.5590)
2022-11-25 01:00:55,485:INFO: Dataset: univ                Batch:  4/15	Loss 12.6795 (12.5891)
2022-11-25 01:00:55,511:INFO: Dataset: univ                Batch:  5/15	Loss 11.9797 (12.4432)
2022-11-25 01:00:55,515:INFO: Dataset: univ                Batch:  6/15	Loss 12.3836 (12.4325)
2022-11-25 01:00:55,516:INFO: Dataset: univ                Batch:  7/15	Loss 11.9813 (12.3665)
2022-11-25 01:00:55,517:INFO: Dataset: univ                Batch:  8/15	Loss 11.6358 (12.2671)
2022-11-25 01:00:55,518:INFO: Dataset: univ                Batch:  9/15	Loss 11.9696 (12.2355)
2022-11-25 01:00:55,519:INFO: Dataset: univ                Batch: 10/15	Loss 11.5950 (12.1695)
2022-11-25 01:00:55,520:INFO: Dataset: univ                Batch: 11/15	Loss 11.4660 (12.1040)
2022-11-25 01:00:55,522:INFO: Dataset: univ                Batch: 12/15	Loss 11.0921 (12.0243)
2022-11-25 01:00:55,523:INFO: Dataset: univ                Batch: 13/15	Loss 10.9768 (11.9411)
2022-11-25 01:00:55,524:INFO: Dataset: univ                Batch: 14/15	Loss 10.5528 (11.8457)
2022-11-25 01:00:55,525:INFO: Dataset: univ                Batch: 15/15	Loss 10.5638 (11.8302)
2022-11-25 01:00:55,792:INFO: Dataset: zara1               Batch: 1/8	Loss 31.7201 (31.7201)
2022-11-25 01:00:55,793:INFO: Dataset: zara1               Batch: 2/8	Loss 31.7696 (31.7447)
2022-11-25 01:00:55,794:INFO: Dataset: zara1               Batch: 3/8	Loss 31.8134 (31.7666)
2022-11-25 01:00:55,795:INFO: Dataset: zara1               Batch: 4/8	Loss 31.9469 (31.8165)
2022-11-25 01:00:55,797:INFO: Dataset: zara1               Batch: 5/8	Loss 31.7129 (31.7944)
2022-11-25 01:00:55,828:INFO: Dataset: zara1               Batch: 6/8	Loss 31.2154 (31.7007)
2022-11-25 01:00:55,830:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7354 (31.5687)
2022-11-25 01:00:55,831:INFO: Dataset: zara1               Batch: 8/8	Loss 29.8149 (31.3675)
2022-11-25 01:00:56,119:INFO: Dataset: zara2               Batch:  1/18	Loss 13.1501 (13.1501)
2022-11-25 01:00:56,120:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3353 (13.2396)
2022-11-25 01:00:56,125:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5010 (13.3262)
2022-11-25 01:00:56,126:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3903 (13.3440)
2022-11-25 01:00:56,179:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4970 (13.3758)
2022-11-25 01:00:56,197:INFO: Dataset: zara2               Batch:  6/18	Loss 13.2260 (13.3518)
2022-11-25 01:00:56,198:INFO: Dataset: zara2               Batch:  7/18	Loss 13.3371 (13.3496)
2022-11-25 01:00:56,199:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2647 (13.3393)
2022-11-25 01:00:56,200:INFO: Dataset: zara2               Batch:  9/18	Loss 13.0011 (13.3017)
2022-11-25 01:00:56,201:INFO: Dataset: zara2               Batch: 10/18	Loss 13.3521 (13.3067)
2022-11-25 01:00:56,202:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8234 (13.2637)
2022-11-25 01:00:56,203:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6205 (13.2114)
2022-11-25 01:00:56,204:INFO: Dataset: zara2               Batch: 13/18	Loss 13.1362 (13.2055)
2022-11-25 01:00:56,205:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3452 (13.1463)
2022-11-25 01:00:56,206:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3281 (13.0891)
2022-11-25 01:00:56,207:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0500 (13.0259)
2022-11-25 01:00:56,208:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8631 (12.9567)
2022-11-25 01:00:56,210:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6592 (12.8960)
2022-11-25 01:00:56,258:INFO: - Computing ADE (validation o)
2022-11-25 01:00:56,542:INFO: 		 ADE on eth                       dataset:	 1.090322732925415
2022-11-25 01:00:56,542:INFO: Average validation o:	ADE  1.0903	FDE  2.2258
2022-11-25 01:00:56,543:INFO: - Computing loss (validation)
2022-11-25 01:00:56,737:INFO: Dataset: hotel               Batch: 1/2	Loss 33.7738 (33.7738)
2022-11-25 01:00:56,738:INFO: Dataset: hotel               Batch: 2/2	Loss 29.3660 (33.4127)
2022-11-25 01:00:57,003:INFO: Dataset: univ                Batch: 1/3	Loss 11.2731 (11.2731)
2022-11-25 01:00:57,005:INFO: Dataset: univ                Batch: 2/3	Loss 11.4817 (11.3721)
2022-11-25 01:00:57,009:INFO: Dataset: univ                Batch: 3/3	Loss 11.4050 (11.3829)
2022-11-25 01:00:57,256:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8372 (28.8372)
2022-11-25 01:00:57,257:INFO: Dataset: zara1               Batch: 2/2	Loss 29.8149 (29.0541)
2022-11-25 01:00:57,522:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4946 (11.4946)
2022-11-25 01:00:57,534:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6030 (11.5491)
2022-11-25 01:00:57,534:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6073 (11.5694)
2022-11-25 01:00:57,535:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6280 (11.5837)
2022-11-25 01:00:57,536:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5448 (11.5766)
2022-11-25 01:00:57,590:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_643.pth.tar
2022-11-25 01:00:57,590:INFO: 
===> EPOCH: 644 (P4)
2022-11-25 01:00:57,590:INFO: - Computing loss (training)
2022-11-25 01:00:57,796:INFO: Dataset: hotel               Batch: 1/4	Loss 26.3510 (26.3510)
2022-11-25 01:00:57,797:INFO: Dataset: hotel               Batch: 2/4	Loss 27.1702 (26.7557)
2022-11-25 01:00:57,799:INFO: Dataset: hotel               Batch: 3/4	Loss 27.7945 (27.0913)
2022-11-25 01:00:57,801:INFO: Dataset: hotel               Batch: 4/4	Loss 28.1882 (27.2968)
2022-11-25 01:00:58,053:INFO: Dataset: univ                Batch:  1/15	Loss 12.0817 (12.0817)
2022-11-25 01:00:58,055:INFO: Dataset: univ                Batch:  2/15	Loss 12.6194 (12.3383)
2022-11-25 01:00:58,110:INFO: Dataset: univ                Batch:  3/15	Loss 12.1478 (12.2693)
2022-11-25 01:00:58,113:INFO: Dataset: univ                Batch:  4/15	Loss 12.2856 (12.2736)
2022-11-25 01:00:58,114:INFO: Dataset: univ                Batch:  5/15	Loss 12.4437 (12.3038)
2022-11-25 01:00:58,117:INFO: Dataset: univ                Batch:  6/15	Loss 12.2069 (12.2866)
2022-11-25 01:00:58,119:INFO: Dataset: univ                Batch:  7/15	Loss 12.2166 (12.2766)
2022-11-25 01:00:58,120:INFO: Dataset: univ                Batch:  8/15	Loss 11.8909 (12.2282)
2022-11-25 01:00:58,121:INFO: Dataset: univ                Batch:  9/15	Loss 11.6743 (12.1650)
2022-11-25 01:00:58,122:INFO: Dataset: univ                Batch: 10/15	Loss 11.4474 (12.0923)
2022-11-25 01:00:58,124:INFO: Dataset: univ                Batch: 11/15	Loss 11.0816 (12.0104)
2022-11-25 01:00:58,126:INFO: Dataset: univ                Batch: 12/15	Loss 11.0237 (11.9296)
2022-11-25 01:00:58,127:INFO: Dataset: univ                Batch: 13/15	Loss 10.8452 (11.8431)
2022-11-25 01:00:58,128:INFO: Dataset: univ                Batch: 14/15	Loss 10.4936 (11.7418)
2022-11-25 01:00:58,130:INFO: Dataset: univ                Batch: 15/15	Loss 9.8522 (11.7217)
2022-11-25 01:00:58,392:INFO: Dataset: zara1               Batch: 1/8	Loss 32.5459 (32.5459)
2022-11-25 01:00:58,394:INFO: Dataset: zara1               Batch: 2/8	Loss 31.6026 (32.0842)
2022-11-25 01:00:58,397:INFO: Dataset: zara1               Batch: 3/8	Loss 31.8893 (32.0160)
2022-11-25 01:00:58,399:INFO: Dataset: zara1               Batch: 4/8	Loss 31.7340 (31.9488)
2022-11-25 01:00:58,401:INFO: Dataset: zara1               Batch: 5/8	Loss 31.3628 (31.8384)
2022-11-25 01:00:58,463:INFO: Dataset: zara1               Batch: 6/8	Loss 30.8479 (31.6732)
2022-11-25 01:00:58,466:INFO: Dataset: zara1               Batch: 7/8	Loss 30.3883 (31.4693)
2022-11-25 01:00:58,470:INFO: Dataset: zara1               Batch: 8/8	Loss 29.9629 (31.3020)
2022-11-25 01:00:58,762:INFO: Dataset: zara2               Batch:  1/18	Loss 13.6480 (13.6480)
2022-11-25 01:00:58,764:INFO: Dataset: zara2               Batch:  2/18	Loss 13.7676 (13.7024)
2022-11-25 01:00:58,767:INFO: Dataset: zara2               Batch:  3/18	Loss 13.4179 (13.6164)
2022-11-25 01:00:58,786:INFO: Dataset: zara2               Batch:  4/18	Loss 13.7188 (13.6416)
2022-11-25 01:00:58,787:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5974 (13.6337)
2022-11-25 01:00:58,797:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5045 (13.6143)
2022-11-25 01:00:58,798:INFO: Dataset: zara2               Batch:  7/18	Loss 12.9832 (13.5184)
2022-11-25 01:00:58,799:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3497 (13.4980)
2022-11-25 01:00:58,800:INFO: Dataset: zara2               Batch:  9/18	Loss 12.8828 (13.4319)
2022-11-25 01:00:58,801:INFO: Dataset: zara2               Batch: 10/18	Loss 12.7475 (13.3603)
2022-11-25 01:00:58,801:INFO: Dataset: zara2               Batch: 11/18	Loss 12.8666 (13.3137)
2022-11-25 01:00:58,803:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5608 (13.2542)
2022-11-25 01:00:58,804:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5391 (13.1979)
2022-11-25 01:00:58,805:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1114 (13.1135)
2022-11-25 01:00:58,806:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3579 (13.0580)
2022-11-25 01:00:58,806:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1170 (13.0023)
2022-11-25 01:00:58,807:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9861 (12.9411)
2022-11-25 01:00:58,809:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4976 (12.8693)
2022-11-25 01:00:58,856:INFO: - Computing ADE (validation o)
2022-11-25 01:00:59,137:INFO: 		 ADE on eth                       dataset:	 1.0541942119598389
2022-11-25 01:00:59,138:INFO: Average validation o:	ADE  1.0542	FDE  2.1599
2022-11-25 01:00:59,138:INFO: - Computing loss (validation)
2022-11-25 01:00:59,338:INFO: Dataset: hotel               Batch: 1/2	Loss 33.7747 (33.7747)
2022-11-25 01:00:59,338:INFO: Dataset: hotel               Batch: 2/2	Loss 30.1783 (33.5170)
2022-11-25 01:00:59,597:INFO: Dataset: univ                Batch: 1/3	Loss 11.4595 (11.4595)
2022-11-25 01:00:59,600:INFO: Dataset: univ                Batch: 2/3	Loss 11.3849 (11.4196)
2022-11-25 01:00:59,601:INFO: Dataset: univ                Batch: 3/3	Loss 11.3837 (11.4074)
2022-11-25 01:00:59,838:INFO: Dataset: zara1               Batch: 1/2	Loss 29.0304 (29.0304)
2022-11-25 01:00:59,839:INFO: Dataset: zara1               Batch: 2/2	Loss 28.7029 (28.9482)
2022-11-25 01:01:00,086:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4844 (11.4844)
2022-11-25 01:01:00,087:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6340 (11.5552)
2022-11-25 01:01:00,088:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5432 (11.5514)
2022-11-25 01:01:00,090:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5431 (11.5492)
2022-11-25 01:01:00,091:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4687 (11.5345)
2022-11-25 01:01:00,146:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_644.pth.tar
2022-11-25 01:01:00,146:INFO: 
===> EPOCH: 645 (P4)
2022-11-25 01:01:00,146:INFO: - Computing loss (training)
2022-11-25 01:01:00,363:INFO: Dataset: hotel               Batch: 1/4	Loss 27.5989 (27.5989)
2022-11-25 01:01:00,366:INFO: Dataset: hotel               Batch: 2/4	Loss 29.6463 (28.6413)
2022-11-25 01:01:00,369:INFO: Dataset: hotel               Batch: 3/4	Loss 25.4263 (27.6334)
2022-11-25 01:01:00,371:INFO: Dataset: hotel               Batch: 4/4	Loss 26.9457 (27.5246)
2022-11-25 01:01:00,634:INFO: Dataset: univ                Batch:  1/15	Loss 11.8042 (11.8042)
2022-11-25 01:01:00,637:INFO: Dataset: univ                Batch:  2/15	Loss 12.1038 (11.9454)
2022-11-25 01:01:00,700:INFO: Dataset: univ                Batch:  3/15	Loss 12.0844 (11.9925)
2022-11-25 01:01:00,701:INFO: Dataset: univ                Batch:  4/15	Loss 12.6564 (12.1471)
2022-11-25 01:01:00,702:INFO: Dataset: univ                Batch:  5/15	Loss 12.4985 (12.2166)
2022-11-25 01:01:00,705:INFO: Dataset: univ                Batch:  6/15	Loss 12.5118 (12.2662)
2022-11-25 01:01:00,706:INFO: Dataset: univ                Batch:  7/15	Loss 12.0021 (12.2251)
2022-11-25 01:01:00,708:INFO: Dataset: univ                Batch:  8/15	Loss 11.9919 (12.1939)
2022-11-25 01:01:00,709:INFO: Dataset: univ                Batch:  9/15	Loss 11.6431 (12.1269)
2022-11-25 01:01:00,710:INFO: Dataset: univ                Batch: 10/15	Loss 11.5648 (12.0720)
2022-11-25 01:01:00,712:INFO: Dataset: univ                Batch: 11/15	Loss 11.7113 (12.0434)
2022-11-25 01:01:00,713:INFO: Dataset: univ                Batch: 12/15	Loss 10.9282 (11.9487)
2022-11-25 01:01:00,714:INFO: Dataset: univ                Batch: 13/15	Loss 10.7201 (11.8463)
2022-11-25 01:01:00,715:INFO: Dataset: univ                Batch: 14/15	Loss 10.5065 (11.7564)
2022-11-25 01:01:00,717:INFO: Dataset: univ                Batch: 15/15	Loss 10.6399 (11.7425)
2022-11-25 01:01:01,001:INFO: Dataset: zara1               Batch: 1/8	Loss 31.8903 (31.8903)
2022-11-25 01:01:01,003:INFO: Dataset: zara1               Batch: 2/8	Loss 31.5152 (31.6913)
2022-11-25 01:01:01,005:INFO: Dataset: zara1               Batch: 3/8	Loss 31.7895 (31.7220)
2022-11-25 01:01:01,011:INFO: Dataset: zara1               Batch: 4/8	Loss 31.6346 (31.6989)
2022-11-25 01:01:01,012:INFO: Dataset: zara1               Batch: 5/8	Loss 31.0633 (31.5520)
2022-11-25 01:01:01,036:INFO: Dataset: zara1               Batch: 6/8	Loss 31.4001 (31.5278)
2022-11-25 01:01:01,038:INFO: Dataset: zara1               Batch: 7/8	Loss 30.6255 (31.3998)
2022-11-25 01:01:01,040:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2588 (31.2791)
2022-11-25 01:01:01,310:INFO: Dataset: zara2               Batch:  1/18	Loss 12.8099 (12.8099)
2022-11-25 01:01:01,311:INFO: Dataset: zara2               Batch:  2/18	Loss 13.0877 (12.9571)
2022-11-25 01:01:01,314:INFO: Dataset: zara2               Batch:  3/18	Loss 13.0245 (12.9791)
2022-11-25 01:01:01,317:INFO: Dataset: zara2               Batch:  4/18	Loss 13.8899 (13.1991)
2022-11-25 01:01:01,387:INFO: Dataset: zara2               Batch:  5/18	Loss 13.3569 (13.2310)
2022-11-25 01:01:01,394:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3380 (13.2500)
2022-11-25 01:01:01,395:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2161 (13.2448)
2022-11-25 01:01:01,396:INFO: Dataset: zara2               Batch:  8/18	Loss 13.1199 (13.2278)
2022-11-25 01:01:01,397:INFO: Dataset: zara2               Batch:  9/18	Loss 13.0109 (13.2037)
2022-11-25 01:01:01,398:INFO: Dataset: zara2               Batch: 10/18	Loss 13.1010 (13.1937)
2022-11-25 01:01:01,400:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9007 (13.1665)
2022-11-25 01:01:01,401:INFO: Dataset: zara2               Batch: 12/18	Loss 13.0559 (13.1574)
2022-11-25 01:01:01,402:INFO: Dataset: zara2               Batch: 13/18	Loss 12.8121 (13.1304)
2022-11-25 01:01:01,402:INFO: Dataset: zara2               Batch: 14/18	Loss 12.4183 (13.0801)
2022-11-25 01:01:01,403:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2436 (13.0263)
2022-11-25 01:01:01,404:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8127 (12.9483)
2022-11-25 01:01:01,406:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9973 (12.8938)
2022-11-25 01:01:01,407:INFO: Dataset: zara2               Batch: 18/18	Loss 11.7005 (12.8393)
2022-11-25 01:01:01,455:INFO: - Computing ADE (validation o)
2022-11-25 01:01:01,729:INFO: 		 ADE on eth                       dataset:	 1.0534812211990356
2022-11-25 01:01:01,729:INFO: Average validation o:	ADE  1.0535	FDE  2.1409
2022-11-25 01:01:01,730:INFO: - Computing loss (validation)
2022-11-25 01:01:01,926:INFO: Dataset: hotel               Batch: 1/2	Loss 33.4022 (33.4022)
2022-11-25 01:01:01,928:INFO: Dataset: hotel               Batch: 2/2	Loss 32.6846 (33.3533)
2022-11-25 01:01:02,193:INFO: Dataset: univ                Batch: 1/3	Loss 11.4471 (11.4471)
2022-11-25 01:01:02,201:INFO: Dataset: univ                Batch: 2/3	Loss 11.6259 (11.5327)
2022-11-25 01:01:02,202:INFO: Dataset: univ                Batch: 3/3	Loss 11.3642 (11.4782)
2022-11-25 01:01:02,454:INFO: Dataset: zara1               Batch: 1/2	Loss 29.1462 (29.1462)
2022-11-25 01:01:02,457:INFO: Dataset: zara1               Batch: 2/2	Loss 28.7869 (29.0584)
2022-11-25 01:01:02,728:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4401 (11.4401)
2022-11-25 01:01:02,730:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4457 (11.4431)
2022-11-25 01:01:02,731:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2637 (11.3872)
2022-11-25 01:01:02,733:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3980 (11.3897)
2022-11-25 01:01:02,733:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4086 (11.3936)
2022-11-25 01:01:02,787:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_645.pth.tar
2022-11-25 01:01:02,787:INFO: 
===> EPOCH: 646 (P4)
2022-11-25 01:01:02,787:INFO: - Computing loss (training)
2022-11-25 01:01:02,988:INFO: Dataset: hotel               Batch: 1/4	Loss 27.8345 (27.8345)
2022-11-25 01:01:02,989:INFO: Dataset: hotel               Batch: 2/4	Loss 27.7591 (27.7966)
2022-11-25 01:01:02,990:INFO: Dataset: hotel               Batch: 3/4	Loss 26.6535 (27.4264)
2022-11-25 01:01:02,993:INFO: Dataset: hotel               Batch: 4/4	Loss 25.8835 (27.1720)
2022-11-25 01:01:03,248:INFO: Dataset: univ                Batch:  1/15	Loss 12.5035 (12.5035)
2022-11-25 01:01:03,252:INFO: Dataset: univ                Batch:  2/15	Loss 12.5119 (12.5078)
2022-11-25 01:01:03,255:INFO: Dataset: univ                Batch:  3/15	Loss 12.3067 (12.4407)
2022-11-25 01:01:03,257:INFO: Dataset: univ                Batch:  4/15	Loss 12.4965 (12.4538)
2022-11-25 01:01:03,335:INFO: Dataset: univ                Batch:  5/15	Loss 12.4358 (12.4502)
2022-11-25 01:01:03,340:INFO: Dataset: univ                Batch:  6/15	Loss 12.5791 (12.4679)
2022-11-25 01:01:03,341:INFO: Dataset: univ                Batch:  7/15	Loss 11.9726 (12.3966)
2022-11-25 01:01:03,343:INFO: Dataset: univ                Batch:  8/15	Loss 12.0722 (12.3554)
2022-11-25 01:01:03,344:INFO: Dataset: univ                Batch:  9/15	Loss 11.7054 (12.2879)
2022-11-25 01:01:03,345:INFO: Dataset: univ                Batch: 10/15	Loss 11.3176 (12.1893)
2022-11-25 01:01:03,347:INFO: Dataset: univ                Batch: 11/15	Loss 11.2189 (12.1019)
2022-11-25 01:01:03,348:INFO: Dataset: univ                Batch: 12/15	Loss 10.6932 (11.9878)
2022-11-25 01:01:03,349:INFO: Dataset: univ                Batch: 13/15	Loss 10.5096 (11.8765)
2022-11-25 01:01:03,350:INFO: Dataset: univ                Batch: 14/15	Loss 10.2314 (11.7573)
2022-11-25 01:01:03,351:INFO: Dataset: univ                Batch: 15/15	Loss 10.5411 (11.7432)
2022-11-25 01:01:03,612:INFO: Dataset: zara1               Batch: 1/8	Loss 32.1570 (32.1570)
2022-11-25 01:01:03,616:INFO: Dataset: zara1               Batch: 2/8	Loss 32.3826 (32.2645)
2022-11-25 01:01:03,617:INFO: Dataset: zara1               Batch: 3/8	Loss 32.0376 (32.1845)
2022-11-25 01:01:03,618:INFO: Dataset: zara1               Batch: 4/8	Loss 32.2843 (32.2121)
2022-11-25 01:01:03,621:INFO: Dataset: zara1               Batch: 5/8	Loss 31.5627 (32.0665)
2022-11-25 01:01:03,675:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6197 (31.8123)
2022-11-25 01:01:03,679:INFO: Dataset: zara1               Batch: 7/8	Loss 29.9776 (31.5298)
2022-11-25 01:01:03,682:INFO: Dataset: zara1               Batch: 8/8	Loss 30.1036 (31.3662)
2022-11-25 01:01:03,965:INFO: Dataset: zara2               Batch:  1/18	Loss 13.2507 (13.2507)
2022-11-25 01:01:03,976:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4830 (13.3585)
2022-11-25 01:01:04,007:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6977 (13.4779)
2022-11-25 01:01:04,008:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9189 (13.5802)
2022-11-25 01:01:04,009:INFO: Dataset: zara2               Batch:  5/18	Loss 13.2663 (13.5126)
2022-11-25 01:01:04,025:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4275 (13.4985)
2022-11-25 01:01:04,026:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2643 (13.4633)
2022-11-25 01:01:04,027:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2647 (13.4372)
2022-11-25 01:01:04,028:INFO: Dataset: zara2               Batch:  9/18	Loss 13.0321 (13.3960)
2022-11-25 01:01:04,029:INFO: Dataset: zara2               Batch: 10/18	Loss 12.8240 (13.3391)
2022-11-25 01:01:04,030:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0434 (13.3094)
2022-11-25 01:01:04,032:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5429 (13.2495)
2022-11-25 01:01:04,033:INFO: Dataset: zara2               Batch: 13/18	Loss 12.6849 (13.2074)
2022-11-25 01:01:04,034:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5136 (13.1637)
2022-11-25 01:01:04,034:INFO: Dataset: zara2               Batch: 15/18	Loss 11.9796 (13.0783)
2022-11-25 01:01:04,035:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1928 (13.0257)
2022-11-25 01:01:04,036:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9762 (12.9640)
2022-11-25 01:01:04,038:INFO: Dataset: zara2               Batch: 18/18	Loss 11.7632 (12.8987)
2022-11-25 01:01:04,086:INFO: - Computing ADE (validation o)
2022-11-25 01:01:04,358:INFO: 		 ADE on eth                       dataset:	 1.0518560409545898
2022-11-25 01:01:04,358:INFO: Average validation o:	ADE  1.0519	FDE  2.1374
2022-11-25 01:01:04,359:INFO: - Computing loss (validation)
2022-11-25 01:01:04,577:INFO: Dataset: hotel               Batch: 1/2	Loss 33.9301 (33.9301)
2022-11-25 01:01:04,578:INFO: Dataset: hotel               Batch: 2/2	Loss 29.6768 (33.5237)
2022-11-25 01:01:04,829:INFO: Dataset: univ                Batch: 1/3	Loss 11.5698 (11.5698)
2022-11-25 01:01:04,833:INFO: Dataset: univ                Batch: 2/3	Loss 11.0383 (11.2853)
2022-11-25 01:01:04,833:INFO: Dataset: univ                Batch: 3/3	Loss 11.1410 (11.2367)
2022-11-25 01:01:05,082:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8843 (28.8843)
2022-11-25 01:01:05,083:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6312 (28.8208)
2022-11-25 01:01:05,329:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4609 (11.4609)
2022-11-25 01:01:05,336:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5342 (11.4988)
2022-11-25 01:01:05,337:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6924 (11.5638)
2022-11-25 01:01:05,339:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6340 (11.5816)
2022-11-25 01:01:05,341:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5105 (11.5673)
2022-11-25 01:01:05,402:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_646.pth.tar
2022-11-25 01:01:05,403:INFO: 
===> EPOCH: 647 (P4)
2022-11-25 01:01:05,403:INFO: - Computing loss (training)
2022-11-25 01:01:05,606:INFO: Dataset: hotel               Batch: 1/4	Loss 26.8997 (26.8997)
2022-11-25 01:01:05,609:INFO: Dataset: hotel               Batch: 2/4	Loss 27.6473 (27.2646)
2022-11-25 01:01:05,621:INFO: Dataset: hotel               Batch: 3/4	Loss 27.2409 (27.2567)
2022-11-25 01:01:05,623:INFO: Dataset: hotel               Batch: 4/4	Loss 28.3883 (27.4448)
2022-11-25 01:01:05,883:INFO: Dataset: univ                Batch:  1/15	Loss 11.9656 (11.9656)
2022-11-25 01:01:05,886:INFO: Dataset: univ                Batch:  2/15	Loss 12.5144 (12.2123)
2022-11-25 01:01:05,888:INFO: Dataset: univ                Batch:  3/15	Loss 12.1143 (12.1790)
2022-11-25 01:01:05,891:INFO: Dataset: univ                Batch:  4/15	Loss 12.1078 (12.1619)
2022-11-25 01:01:05,926:INFO: Dataset: univ                Batch:  5/15	Loss 12.1522 (12.1600)
2022-11-25 01:01:05,933:INFO: Dataset: univ                Batch:  6/15	Loss 12.1247 (12.1535)
2022-11-25 01:01:05,934:INFO: Dataset: univ                Batch:  7/15	Loss 11.9256 (12.1189)
2022-11-25 01:01:05,935:INFO: Dataset: univ                Batch:  8/15	Loss 11.8124 (12.0778)
2022-11-25 01:01:05,936:INFO: Dataset: univ                Batch:  9/15	Loss 11.8603 (12.0533)
2022-11-25 01:01:05,937:INFO: Dataset: univ                Batch: 10/15	Loss 11.2922 (11.9768)
2022-11-25 01:01:05,939:INFO: Dataset: univ                Batch: 11/15	Loss 11.5181 (11.9384)
2022-11-25 01:01:05,940:INFO: Dataset: univ                Batch: 12/15	Loss 11.0294 (11.8617)
2022-11-25 01:01:05,941:INFO: Dataset: univ                Batch: 13/15	Loss 10.7678 (11.7772)
2022-11-25 01:01:05,942:INFO: Dataset: univ                Batch: 14/15	Loss 10.4615 (11.6747)
2022-11-25 01:01:05,943:INFO: Dataset: univ                Batch: 15/15	Loss 10.5099 (11.6587)
2022-11-25 01:01:06,211:INFO: Dataset: zara1               Batch: 1/8	Loss 31.4005 (31.4005)
2022-11-25 01:01:06,215:INFO: Dataset: zara1               Batch: 2/8	Loss 31.8989 (31.6778)
2022-11-25 01:01:06,216:INFO: Dataset: zara1               Batch: 3/8	Loss 31.5206 (31.6233)
2022-11-25 01:01:06,217:INFO: Dataset: zara1               Batch: 4/8	Loss 31.2367 (31.5235)
2022-11-25 01:01:06,218:INFO: Dataset: zara1               Batch: 5/8	Loss 31.3934 (31.4963)
2022-11-25 01:01:06,263:INFO: Dataset: zara1               Batch: 6/8	Loss 30.9613 (31.4101)
2022-11-25 01:01:06,266:INFO: Dataset: zara1               Batch: 7/8	Loss 30.5394 (31.2819)
2022-11-25 01:01:06,270:INFO: Dataset: zara1               Batch: 8/8	Loss 29.5887 (31.1134)
2022-11-25 01:01:06,545:INFO: Dataset: zara2               Batch:  1/18	Loss 13.0903 (13.0903)
2022-11-25 01:01:06,550:INFO: Dataset: zara2               Batch:  2/18	Loss 13.0428 (13.0678)
2022-11-25 01:01:06,586:INFO: Dataset: zara2               Batch:  3/18	Loss 13.1000 (13.0788)
2022-11-25 01:01:06,587:INFO: Dataset: zara2               Batch:  4/18	Loss 13.5209 (13.1945)
2022-11-25 01:01:06,588:INFO: Dataset: zara2               Batch:  5/18	Loss 13.6069 (13.2771)
2022-11-25 01:01:06,601:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5287 (13.3171)
2022-11-25 01:01:06,602:INFO: Dataset: zara2               Batch:  7/18	Loss 13.1354 (13.2895)
2022-11-25 01:01:06,603:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3019 (13.2913)
2022-11-25 01:01:06,604:INFO: Dataset: zara2               Batch:  9/18	Loss 13.0488 (13.2648)
2022-11-25 01:01:06,605:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9977 (13.2385)
2022-11-25 01:01:06,605:INFO: Dataset: zara2               Batch: 11/18	Loss 12.5193 (13.1711)
2022-11-25 01:01:06,607:INFO: Dataset: zara2               Batch: 12/18	Loss 12.7650 (13.1397)
2022-11-25 01:01:06,608:INFO: Dataset: zara2               Batch: 13/18	Loss 12.2793 (13.0710)
2022-11-25 01:01:06,609:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5322 (13.0352)
2022-11-25 01:01:06,609:INFO: Dataset: zara2               Batch: 15/18	Loss 11.9578 (12.9624)
2022-11-25 01:01:06,610:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0996 (12.9103)
2022-11-25 01:01:06,611:INFO: Dataset: zara2               Batch: 17/18	Loss 12.0838 (12.8669)
2022-11-25 01:01:06,613:INFO: Dataset: zara2               Batch: 18/18	Loss 12.0674 (12.8277)
2022-11-25 01:01:06,668:INFO: - Computing ADE (validation o)
2022-11-25 01:01:06,947:INFO: 		 ADE on eth                       dataset:	 1.0546789169311523
2022-11-25 01:01:06,947:INFO: Average validation o:	ADE  1.0547	FDE  2.1370
2022-11-25 01:01:06,948:INFO: - Computing loss (validation)
2022-11-25 01:01:07,140:INFO: Dataset: hotel               Batch: 1/2	Loss 33.1910 (33.1910)
2022-11-25 01:01:07,140:INFO: Dataset: hotel               Batch: 2/2	Loss 34.6247 (33.2986)
2022-11-25 01:01:07,413:INFO: Dataset: univ                Batch: 1/3	Loss 11.1507 (11.1507)
2022-11-25 01:01:07,414:INFO: Dataset: univ                Batch: 2/3	Loss 11.2170 (11.1820)
2022-11-25 01:01:07,416:INFO: Dataset: univ                Batch: 3/3	Loss 11.2183 (11.1944)
2022-11-25 01:01:07,667:INFO: Dataset: zara1               Batch: 1/2	Loss 28.9790 (28.9790)
2022-11-25 01:01:07,668:INFO: Dataset: zara1               Batch: 2/2	Loss 28.7229 (28.9098)
2022-11-25 01:01:07,917:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4274 (11.4274)
2022-11-25 01:01:07,918:INFO: Dataset: zara2               Batch: 2/5	Loss 11.6055 (11.5144)
2022-11-25 01:01:07,925:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5550 (11.5280)
2022-11-25 01:01:07,926:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6336 (11.5542)
2022-11-25 01:01:07,926:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5558 (11.5545)
2022-11-25 01:01:07,996:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_647.pth.tar
2022-11-25 01:01:07,996:INFO: 
===> EPOCH: 648 (P4)
2022-11-25 01:01:07,996:INFO: - Computing loss (training)
2022-11-25 01:01:08,216:INFO: Dataset: hotel               Batch: 1/4	Loss 26.7458 (26.7458)
2022-11-25 01:01:08,217:INFO: Dataset: hotel               Batch: 2/4	Loss 28.4030 (27.5306)
2022-11-25 01:01:08,219:INFO: Dataset: hotel               Batch: 3/4	Loss 27.0250 (27.3594)
2022-11-25 01:01:08,234:INFO: Dataset: hotel               Batch: 4/4	Loss 25.6096 (27.0616)
2022-11-25 01:01:08,478:INFO: Dataset: univ                Batch:  1/15	Loss 12.1188 (12.1188)
2022-11-25 01:01:08,482:INFO: Dataset: univ                Batch:  2/15	Loss 12.0424 (12.0796)
2022-11-25 01:01:08,483:INFO: Dataset: univ                Batch:  3/15	Loss 12.4052 (12.1899)
2022-11-25 01:01:08,486:INFO: Dataset: univ                Batch:  4/15	Loss 12.1627 (12.1833)
2022-11-25 01:01:08,500:INFO: Dataset: univ                Batch:  5/15	Loss 12.1795 (12.1825)
2022-11-25 01:01:08,521:INFO: Dataset: univ                Batch:  6/15	Loss 11.9977 (12.1500)
2022-11-25 01:01:08,522:INFO: Dataset: univ                Batch:  7/15	Loss 11.7753 (12.0927)
2022-11-25 01:01:08,524:INFO: Dataset: univ                Batch:  8/15	Loss 11.7638 (12.0503)
2022-11-25 01:01:08,525:INFO: Dataset: univ                Batch:  9/15	Loss 11.5575 (11.9928)
2022-11-25 01:01:08,526:INFO: Dataset: univ                Batch: 10/15	Loss 11.3962 (11.9309)
2022-11-25 01:01:08,527:INFO: Dataset: univ                Batch: 11/15	Loss 11.4132 (11.8874)
2022-11-25 01:01:08,529:INFO: Dataset: univ                Batch: 12/15	Loss 10.8882 (11.7984)
2022-11-25 01:01:08,530:INFO: Dataset: univ                Batch: 13/15	Loss 10.4590 (11.6929)
2022-11-25 01:01:08,531:INFO: Dataset: univ                Batch: 14/15	Loss 10.6480 (11.6233)
2022-11-25 01:01:08,532:INFO: Dataset: univ                Batch: 15/15	Loss 9.8100 (11.5983)
2022-11-25 01:01:08,799:INFO: Dataset: zara1               Batch: 1/8	Loss 31.4995 (31.4995)
2022-11-25 01:01:08,803:INFO: Dataset: zara1               Batch: 2/8	Loss 31.9298 (31.7211)
2022-11-25 01:01:08,805:INFO: Dataset: zara1               Batch: 3/8	Loss 31.1498 (31.5057)
2022-11-25 01:01:08,806:INFO: Dataset: zara1               Batch: 4/8	Loss 31.8768 (31.5979)
2022-11-25 01:01:08,807:INFO: Dataset: zara1               Batch: 5/8	Loss 30.7401 (31.4317)
2022-11-25 01:01:08,825:INFO: Dataset: zara1               Batch: 6/8	Loss 30.8061 (31.3246)
2022-11-25 01:01:08,826:INFO: Dataset: zara1               Batch: 7/8	Loss 30.4836 (31.2091)
2022-11-25 01:01:08,827:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5693 (31.1505)
2022-11-25 01:01:09,096:INFO: Dataset: zara2               Batch:  1/18	Loss 13.2809 (13.2809)
2022-11-25 01:01:09,098:INFO: Dataset: zara2               Batch:  2/18	Loss 13.1401 (13.2097)
2022-11-25 01:01:09,100:INFO: Dataset: zara2               Batch:  3/18	Loss 13.6559 (13.3518)
2022-11-25 01:01:09,101:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3622 (13.3545)
2022-11-25 01:01:09,124:INFO: Dataset: zara2               Batch:  5/18	Loss 13.1613 (13.3157)
2022-11-25 01:01:09,129:INFO: Dataset: zara2               Batch:  6/18	Loss 13.2832 (13.3104)
2022-11-25 01:01:09,130:INFO: Dataset: zara2               Batch:  7/18	Loss 13.0139 (13.2707)
2022-11-25 01:01:09,131:INFO: Dataset: zara2               Batch:  8/18	Loss 13.3379 (13.2789)
2022-11-25 01:01:09,132:INFO: Dataset: zara2               Batch:  9/18	Loss 13.1190 (13.2622)
2022-11-25 01:01:09,133:INFO: Dataset: zara2               Batch: 10/18	Loss 12.7643 (13.2140)
2022-11-25 01:01:09,134:INFO: Dataset: zara2               Batch: 11/18	Loss 12.7879 (13.1743)
2022-11-25 01:01:09,135:INFO: Dataset: zara2               Batch: 12/18	Loss 12.8452 (13.1490)
2022-11-25 01:01:09,136:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5892 (13.1078)
2022-11-25 01:01:09,137:INFO: Dataset: zara2               Batch: 14/18	Loss 12.0382 (13.0348)
2022-11-25 01:01:09,138:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0594 (12.9672)
2022-11-25 01:01:09,139:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9033 (12.8999)
2022-11-25 01:01:09,140:INFO: Dataset: zara2               Batch: 17/18	Loss 12.0615 (12.8472)
2022-11-25 01:01:09,142:INFO: Dataset: zara2               Batch: 18/18	Loss 11.7141 (12.7852)
2022-11-25 01:01:09,190:INFO: - Computing ADE (validation o)
2022-11-25 01:01:09,468:INFO: 		 ADE on eth                       dataset:	 1.076229214668274
2022-11-25 01:01:09,468:INFO: Average validation o:	ADE  1.0762	FDE  2.1312
2022-11-25 01:01:09,469:INFO: - Computing loss (validation)
2022-11-25 01:01:09,663:INFO: Dataset: hotel               Batch: 1/2	Loss 33.2603 (33.2603)
2022-11-25 01:01:09,664:INFO: Dataset: hotel               Batch: 2/2	Loss 35.2103 (33.3867)
2022-11-25 01:01:09,915:INFO: Dataset: univ                Batch: 1/3	Loss 11.2424 (11.2424)
2022-11-25 01:01:09,916:INFO: Dataset: univ                Batch: 2/3	Loss 11.1115 (11.1788)
2022-11-25 01:01:09,923:INFO: Dataset: univ                Batch: 3/3	Loss 11.2385 (11.1965)
2022-11-25 01:01:10,179:INFO: Dataset: zara1               Batch: 1/2	Loss 28.9198 (28.9198)
2022-11-25 01:01:10,180:INFO: Dataset: zara1               Batch: 2/2	Loss 28.5696 (28.8410)
2022-11-25 01:01:10,434:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4963 (11.4963)
2022-11-25 01:01:10,437:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4440 (11.4701)
2022-11-25 01:01:10,438:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5598 (11.5006)
2022-11-25 01:01:10,442:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5290 (11.5080)
2022-11-25 01:01:10,442:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4750 (11.5008)
2022-11-25 01:01:10,495:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_648.pth.tar
2022-11-25 01:01:10,495:INFO: 
===> EPOCH: 649 (P4)
2022-11-25 01:01:10,495:INFO: - Computing loss (training)
2022-11-25 01:01:10,704:INFO: Dataset: hotel               Batch: 1/4	Loss 23.5254 (23.5254)
2022-11-25 01:01:10,706:INFO: Dataset: hotel               Batch: 2/4	Loss 26.4775 (25.0015)
2022-11-25 01:01:10,708:INFO: Dataset: hotel               Batch: 3/4	Loss 32.1098 (27.3785)
2022-11-25 01:01:10,711:INFO: Dataset: hotel               Batch: 4/4	Loss 26.5181 (27.2275)
2022-11-25 01:01:10,964:INFO: Dataset: univ                Batch:  1/15	Loss 11.8940 (11.8940)
2022-11-25 01:01:10,967:INFO: Dataset: univ                Batch:  2/15	Loss 11.7915 (11.8405)
2022-11-25 01:01:11,028:INFO: Dataset: univ                Batch:  3/15	Loss 12.0866 (11.9259)
2022-11-25 01:01:11,029:INFO: Dataset: univ                Batch:  4/15	Loss 12.0745 (11.9630)
2022-11-25 01:01:11,030:INFO: Dataset: univ                Batch:  5/15	Loss 11.8547 (11.9413)
2022-11-25 01:01:11,034:INFO: Dataset: univ                Batch:  6/15	Loss 12.2618 (11.9885)
2022-11-25 01:01:11,035:INFO: Dataset: univ                Batch:  7/15	Loss 11.9201 (11.9781)
2022-11-25 01:01:11,036:INFO: Dataset: univ                Batch:  8/15	Loss 11.8146 (11.9591)
2022-11-25 01:01:11,037:INFO: Dataset: univ                Batch:  9/15	Loss 11.6496 (11.9279)
2022-11-25 01:01:11,038:INFO: Dataset: univ                Batch: 10/15	Loss 11.4351 (11.8800)
2022-11-25 01:01:11,041:INFO: Dataset: univ                Batch: 11/15	Loss 11.4116 (11.8386)
2022-11-25 01:01:11,042:INFO: Dataset: univ                Batch: 12/15	Loss 11.0665 (11.7770)
2022-11-25 01:01:11,043:INFO: Dataset: univ                Batch: 13/15	Loss 11.2764 (11.7417)
2022-11-25 01:01:11,044:INFO: Dataset: univ                Batch: 14/15	Loss 10.2896 (11.6263)
2022-11-25 01:01:11,046:INFO: Dataset: univ                Batch: 15/15	Loss 10.4351 (11.6091)
2022-11-25 01:01:11,320:INFO: Dataset: zara1               Batch: 1/8	Loss 31.7256 (31.7256)
2022-11-25 01:01:11,322:INFO: Dataset: zara1               Batch: 2/8	Loss 31.4543 (31.5887)
2022-11-25 01:01:11,323:INFO: Dataset: zara1               Batch: 3/8	Loss 31.6469 (31.6096)
2022-11-25 01:01:11,327:INFO: Dataset: zara1               Batch: 4/8	Loss 31.5106 (31.5838)
2022-11-25 01:01:11,328:INFO: Dataset: zara1               Batch: 5/8	Loss 31.1772 (31.4999)
2022-11-25 01:01:11,388:INFO: Dataset: zara1               Batch: 6/8	Loss 30.4856 (31.3143)
2022-11-25 01:01:11,391:INFO: Dataset: zara1               Batch: 7/8	Loss 30.6145 (31.2186)
2022-11-25 01:01:11,394:INFO: Dataset: zara1               Batch: 8/8	Loss 30.1119 (31.0963)
2022-11-25 01:01:11,694:INFO: Dataset: zara2               Batch:  1/18	Loss 12.8876 (12.8876)
2022-11-25 01:01:11,727:INFO: Dataset: zara2               Batch:  2/18	Loss 13.3790 (13.1495)
2022-11-25 01:01:11,729:INFO: Dataset: zara2               Batch:  3/18	Loss 13.2015 (13.1661)
2022-11-25 01:01:11,730:INFO: Dataset: zara2               Batch:  4/18	Loss 13.4802 (13.2451)
2022-11-25 01:01:11,732:INFO: Dataset: zara2               Batch:  5/18	Loss 13.5459 (13.3083)
2022-11-25 01:01:11,763:INFO: Dataset: zara2               Batch:  6/18	Loss 13.4800 (13.3371)
2022-11-25 01:01:11,764:INFO: Dataset: zara2               Batch:  7/18	Loss 13.3098 (13.3337)
2022-11-25 01:01:11,765:INFO: Dataset: zara2               Batch:  8/18	Loss 13.1388 (13.3139)
2022-11-25 01:01:11,766:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7530 (13.2503)
2022-11-25 01:01:11,767:INFO: Dataset: zara2               Batch: 10/18	Loss 12.8239 (13.2092)
2022-11-25 01:01:11,768:INFO: Dataset: zara2               Batch: 11/18	Loss 12.5800 (13.1461)
2022-11-25 01:01:11,769:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5894 (13.0974)
2022-11-25 01:01:11,770:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5723 (13.0570)
2022-11-25 01:01:11,771:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1299 (12.9812)
2022-11-25 01:01:11,772:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3738 (12.9409)
2022-11-25 01:01:11,773:INFO: Dataset: zara2               Batch: 16/18	Loss 11.9713 (12.8838)
2022-11-25 01:01:11,774:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6894 (12.8150)
2022-11-25 01:01:11,775:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5473 (12.7548)
2022-11-25 01:01:11,822:INFO: - Computing ADE (validation o)
2022-11-25 01:01:12,089:INFO: 		 ADE on eth                       dataset:	 1.0342788696289062
2022-11-25 01:01:12,089:INFO: Average validation o:	ADE  1.0343	FDE  2.0962
2022-11-25 01:01:12,089:INFO: - Computing loss (validation)
2022-11-25 01:01:12,279:INFO: Dataset: hotel               Batch: 1/2	Loss 33.6125 (33.6125)
2022-11-25 01:01:12,279:INFO: Dataset: hotel               Batch: 2/2	Loss 29.1022 (33.2431)
2022-11-25 01:01:12,548:INFO: Dataset: univ                Batch: 1/3	Loss 11.1610 (11.1610)
2022-11-25 01:01:12,550:INFO: Dataset: univ                Batch: 2/3	Loss 11.1842 (11.1723)
2022-11-25 01:01:12,553:INFO: Dataset: univ                Batch: 3/3	Loss 10.9253 (11.0883)
2022-11-25 01:01:12,801:INFO: Dataset: zara1               Batch: 1/2	Loss 28.9932 (28.9932)
2022-11-25 01:01:12,805:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6256 (28.8809)
2022-11-25 01:01:13,056:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4193 (11.4193)
2022-11-25 01:01:13,059:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4014 (11.4100)
2022-11-25 01:01:13,059:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6122 (11.4783)
2022-11-25 01:01:13,060:INFO: Dataset: zara2               Batch: 4/5	Loss 11.6031 (11.5101)
2022-11-25 01:01:13,061:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4749 (11.5029)
2022-11-25 01:01:13,114:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_649.pth.tar
2022-11-25 01:01:13,114:INFO: 
===> EPOCH: 650 (P4)
2022-11-25 01:01:13,114:INFO: - Computing loss (training)
2022-11-25 01:01:13,316:INFO: Dataset: hotel               Batch: 1/4	Loss 26.4142 (26.4142)
2022-11-25 01:01:13,320:INFO: Dataset: hotel               Batch: 2/4	Loss 27.4990 (26.9796)
2022-11-25 01:01:13,321:INFO: Dataset: hotel               Batch: 3/4	Loss 27.0805 (27.0132)
2022-11-25 01:01:13,324:INFO: Dataset: hotel               Batch: 4/4	Loss 27.1171 (27.0299)
2022-11-25 01:01:13,607:INFO: Dataset: univ                Batch:  1/15	Loss 11.8308 (11.8308)
2022-11-25 01:01:13,608:INFO: Dataset: univ                Batch:  2/15	Loss 11.9678 (11.8997)
2022-11-25 01:01:13,612:INFO: Dataset: univ                Batch:  3/15	Loss 12.1577 (11.9836)
2022-11-25 01:01:13,613:INFO: Dataset: univ                Batch:  4/15	Loss 11.7397 (11.9221)
2022-11-25 01:01:13,671:INFO: Dataset: univ                Batch:  5/15	Loss 12.0480 (11.9497)
2022-11-25 01:01:13,678:INFO: Dataset: univ                Batch:  6/15	Loss 12.2323 (11.9919)
2022-11-25 01:01:13,679:INFO: Dataset: univ                Batch:  7/15	Loss 11.8404 (11.9677)
2022-11-25 01:01:13,680:INFO: Dataset: univ                Batch:  8/15	Loss 11.5799 (11.9196)
2022-11-25 01:01:13,681:INFO: Dataset: univ                Batch:  9/15	Loss 11.8510 (11.9127)
2022-11-25 01:01:13,682:INFO: Dataset: univ                Batch: 10/15	Loss 11.4241 (11.8673)
2022-11-25 01:01:13,685:INFO: Dataset: univ                Batch: 11/15	Loss 11.1389 (11.8048)
2022-11-25 01:01:13,686:INFO: Dataset: univ                Batch: 12/15	Loss 10.7721 (11.7130)
2022-11-25 01:01:13,687:INFO: Dataset: univ                Batch: 13/15	Loss 10.4940 (11.6137)
2022-11-25 01:01:13,688:INFO: Dataset: univ                Batch: 14/15	Loss 10.3982 (11.5255)
2022-11-25 01:01:13,689:INFO: Dataset: univ                Batch: 15/15	Loss 10.2637 (11.5083)
2022-11-25 01:01:13,963:INFO: Dataset: zara1               Batch: 1/8	Loss 31.5354 (31.5354)
2022-11-25 01:01:13,966:INFO: Dataset: zara1               Batch: 2/8	Loss 31.8177 (31.6760)
2022-11-25 01:01:13,967:INFO: Dataset: zara1               Batch: 3/8	Loss 31.4907 (31.6149)
2022-11-25 01:01:13,969:INFO: Dataset: zara1               Batch: 4/8	Loss 31.5899 (31.6078)
2022-11-25 01:01:13,972:INFO: Dataset: zara1               Batch: 5/8	Loss 31.1938 (31.5227)
2022-11-25 01:01:14,005:INFO: Dataset: zara1               Batch: 6/8	Loss 30.2528 (31.3426)
2022-11-25 01:01:14,006:INFO: Dataset: zara1               Batch: 7/8	Loss 30.3600 (31.2047)
2022-11-25 01:01:14,007:INFO: Dataset: zara1               Batch: 8/8	Loss 30.3727 (31.1123)
2022-11-25 01:01:14,311:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4546 (13.4546)
2022-11-25 01:01:14,313:INFO: Dataset: zara2               Batch:  2/18	Loss 13.1378 (13.3038)
2022-11-25 01:01:14,315:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5328 (13.3801)
2022-11-25 01:01:14,347:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3855 (13.3815)
2022-11-25 01:01:14,348:INFO: Dataset: zara2               Batch:  5/18	Loss 13.0322 (13.3050)
2022-11-25 01:01:14,353:INFO: Dataset: zara2               Batch:  6/18	Loss 13.0906 (13.2675)
2022-11-25 01:01:14,354:INFO: Dataset: zara2               Batch:  7/18	Loss 13.1707 (13.2540)
2022-11-25 01:01:14,355:INFO: Dataset: zara2               Batch:  8/18	Loss 12.9113 (13.2100)
2022-11-25 01:01:14,356:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2862 (13.2184)
2022-11-25 01:01:14,357:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0822 (13.2050)
2022-11-25 01:01:14,358:INFO: Dataset: zara2               Batch: 11/18	Loss 12.7245 (13.1579)
2022-11-25 01:01:14,359:INFO: Dataset: zara2               Batch: 12/18	Loss 12.4945 (13.1050)
2022-11-25 01:01:14,360:INFO: Dataset: zara2               Batch: 13/18	Loss 12.2189 (13.0358)
2022-11-25 01:01:14,361:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1219 (12.9720)
2022-11-25 01:01:14,362:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2471 (12.9261)
2022-11-25 01:01:14,364:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1346 (12.8766)
2022-11-25 01:01:14,365:INFO: Dataset: zara2               Batch: 17/18	Loss 11.9873 (12.8206)
2022-11-25 01:01:14,367:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6804 (12.7719)
2022-11-25 01:01:14,414:INFO: - Computing ADE (validation o)
2022-11-25 01:01:14,680:INFO: 		 ADE on eth                       dataset:	 1.0207459926605225
2022-11-25 01:01:14,680:INFO: Average validation o:	ADE  1.0207	FDE  2.0909
2022-11-25 01:01:14,681:INFO: - Computing loss (validation)
2022-11-25 01:01:14,875:INFO: Dataset: hotel               Batch: 1/2	Loss 33.5017 (33.5017)
2022-11-25 01:01:14,878:INFO: Dataset: hotel               Batch: 2/2	Loss 29.4158 (33.2925)
2022-11-25 01:01:15,130:INFO: Dataset: univ                Batch: 1/3	Loss 10.9674 (10.9674)
2022-11-25 01:01:15,134:INFO: Dataset: univ                Batch: 2/3	Loss 11.0174 (10.9913)
2022-11-25 01:01:15,135:INFO: Dataset: univ                Batch: 3/3	Loss 11.4026 (11.1118)
2022-11-25 01:01:15,387:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8987 (28.8987)
2022-11-25 01:01:15,388:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6444 (28.8341)
2022-11-25 01:01:15,653:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4811 (11.4811)
2022-11-25 01:01:15,664:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4424 (11.4618)
2022-11-25 01:01:15,665:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4631 (11.4622)
2022-11-25 01:01:15,666:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4349 (11.4558)
2022-11-25 01:01:15,666:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4869 (11.4619)
2022-11-25 01:01:15,721:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_650.pth.tar
2022-11-25 01:01:15,721:INFO: 
===> EPOCH: 651 (P4)
2022-11-25 01:01:15,721:INFO: - Computing loss (training)
2022-11-25 01:01:15,921:INFO: Dataset: hotel               Batch: 1/4	Loss 27.8400 (27.8400)
2022-11-25 01:01:15,924:INFO: Dataset: hotel               Batch: 2/4	Loss 28.1096 (27.9709)
2022-11-25 01:01:15,942:INFO: Dataset: hotel               Batch: 3/4	Loss 24.7490 (26.8867)
2022-11-25 01:01:15,944:INFO: Dataset: hotel               Batch: 4/4	Loss 27.9311 (27.0631)
2022-11-25 01:01:16,205:INFO: Dataset: univ                Batch:  1/15	Loss 11.6839 (11.6839)
2022-11-25 01:01:16,208:INFO: Dataset: univ                Batch:  2/15	Loss 12.0545 (11.8686)
2022-11-25 01:01:16,211:INFO: Dataset: univ                Batch:  3/15	Loss 11.9732 (11.9029)
2022-11-25 01:01:16,212:INFO: Dataset: univ                Batch:  4/15	Loss 12.0236 (11.9319)
2022-11-25 01:01:16,244:INFO: Dataset: univ                Batch:  5/15	Loss 12.2053 (11.9878)
2022-11-25 01:01:16,261:INFO: Dataset: univ                Batch:  6/15	Loss 11.9369 (11.9790)
2022-11-25 01:01:16,262:INFO: Dataset: univ                Batch:  7/15	Loss 11.9911 (11.9806)
2022-11-25 01:01:16,263:INFO: Dataset: univ                Batch:  8/15	Loss 11.7274 (11.9479)
2022-11-25 01:01:16,264:INFO: Dataset: univ                Batch:  9/15	Loss 11.5243 (11.9008)
2022-11-25 01:01:16,265:INFO: Dataset: univ                Batch: 10/15	Loss 11.4307 (11.8560)
2022-11-25 01:01:16,266:INFO: Dataset: univ                Batch: 11/15	Loss 11.2055 (11.7979)
2022-11-25 01:01:16,268:INFO: Dataset: univ                Batch: 12/15	Loss 11.1320 (11.7447)
2022-11-25 01:01:16,269:INFO: Dataset: univ                Batch: 13/15	Loss 10.8401 (11.6747)
2022-11-25 01:01:16,270:INFO: Dataset: univ                Batch: 14/15	Loss 10.0694 (11.5559)
2022-11-25 01:01:16,271:INFO: Dataset: univ                Batch: 15/15	Loss 10.0497 (11.5363)
2022-11-25 01:01:16,529:INFO: Dataset: zara1               Batch: 1/8	Loss 31.0177 (31.0177)
2022-11-25 01:01:16,532:INFO: Dataset: zara1               Batch: 2/8	Loss 31.3800 (31.2126)
2022-11-25 01:01:16,549:INFO: Dataset: zara1               Batch: 3/8	Loss 31.1932 (31.2064)
2022-11-25 01:01:16,550:INFO: Dataset: zara1               Batch: 4/8	Loss 31.2067 (31.2065)
2022-11-25 01:01:16,551:INFO: Dataset: zara1               Batch: 5/8	Loss 31.2086 (31.2068)
2022-11-25 01:01:16,603:INFO: Dataset: zara1               Batch: 6/8	Loss 30.7839 (31.1321)
2022-11-25 01:01:16,606:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7364 (31.0758)
2022-11-25 01:01:16,610:INFO: Dataset: zara1               Batch: 8/8	Loss 30.8946 (31.0575)
2022-11-25 01:01:16,896:INFO: Dataset: zara2               Batch:  1/18	Loss 12.9335 (12.9335)
2022-11-25 01:01:16,898:INFO: Dataset: zara2               Batch:  2/18	Loss 13.6311 (13.2642)
2022-11-25 01:01:16,900:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3424 (13.2891)
2022-11-25 01:01:16,902:INFO: Dataset: zara2               Batch:  4/18	Loss 12.7635 (13.1546)
2022-11-25 01:01:16,930:INFO: Dataset: zara2               Batch:  5/18	Loss 13.4274 (13.2085)
2022-11-25 01:01:16,935:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3755 (13.2366)
2022-11-25 01:01:16,936:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2340 (13.2362)
2022-11-25 01:01:16,937:INFO: Dataset: zara2               Batch:  8/18	Loss 12.6593 (13.1619)
2022-11-25 01:01:16,938:INFO: Dataset: zara2               Batch:  9/18	Loss 12.6290 (13.0950)
2022-11-25 01:01:16,939:INFO: Dataset: zara2               Batch: 10/18	Loss 12.4754 (13.0385)
2022-11-25 01:01:16,940:INFO: Dataset: zara2               Batch: 11/18	Loss 13.0106 (13.0359)
2022-11-25 01:01:16,941:INFO: Dataset: zara2               Batch: 12/18	Loss 12.3839 (12.9716)
2022-11-25 01:01:16,942:INFO: Dataset: zara2               Batch: 13/18	Loss 12.7077 (12.9522)
2022-11-25 01:01:16,943:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3437 (12.9057)
2022-11-25 01:01:16,944:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0318 (12.8523)
2022-11-25 01:01:16,945:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1063 (12.8043)
2022-11-25 01:01:16,946:INFO: Dataset: zara2               Batch: 17/18	Loss 12.0391 (12.7597)
2022-11-25 01:01:16,947:INFO: Dataset: zara2               Batch: 18/18	Loss 11.8157 (12.7195)
2022-11-25 01:01:16,995:INFO: - Computing ADE (validation o)
2022-11-25 01:01:17,259:INFO: 		 ADE on eth                       dataset:	 1.082122564315796
2022-11-25 01:01:17,259:INFO: Average validation o:	ADE  1.0821	FDE  2.1461
2022-11-25 01:01:17,260:INFO: - Computing loss (validation)
2022-11-25 01:01:17,453:INFO: Dataset: hotel               Batch: 1/2	Loss 32.9929 (32.9929)
2022-11-25 01:01:17,453:INFO: Dataset: hotel               Batch: 2/2	Loss 36.0486 (33.2223)
2022-11-25 01:01:17,714:INFO: Dataset: univ                Batch: 1/3	Loss 10.8592 (10.8592)
2022-11-25 01:01:17,725:INFO: Dataset: univ                Batch: 2/3	Loss 11.0805 (10.9666)
2022-11-25 01:01:17,725:INFO: Dataset: univ                Batch: 3/3	Loss 11.0660 (10.9949)
2022-11-25 01:01:17,974:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8530 (28.8530)
2022-11-25 01:01:17,982:INFO: Dataset: zara1               Batch: 2/2	Loss 28.7591 (28.8295)
2022-11-25 01:01:18,219:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4387 (11.4387)
2022-11-25 01:01:18,219:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3970 (11.4190)
2022-11-25 01:01:18,220:INFO: Dataset: zara2               Batch: 3/5	Loss 11.7096 (11.5136)
2022-11-25 01:01:18,222:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3053 (11.4584)
2022-11-25 01:01:18,223:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5796 (11.4827)
2022-11-25 01:01:18,276:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_651.pth.tar
2022-11-25 01:01:18,276:INFO: 
===> EPOCH: 652 (P4)
2022-11-25 01:01:18,276:INFO: - Computing loss (training)
2022-11-25 01:01:18,491:INFO: Dataset: hotel               Batch: 1/4	Loss 25.6343 (25.6343)
2022-11-25 01:01:18,502:INFO: Dataset: hotel               Batch: 2/4	Loss 25.8690 (25.7519)
2022-11-25 01:01:18,503:INFO: Dataset: hotel               Batch: 3/4	Loss 29.1767 (26.9249)
2022-11-25 01:01:18,504:INFO: Dataset: hotel               Batch: 4/4	Loss 26.5815 (26.8619)
2022-11-25 01:01:18,773:INFO: Dataset: univ                Batch:  1/15	Loss 11.8694 (11.8694)
2022-11-25 01:01:18,776:INFO: Dataset: univ                Batch:  2/15	Loss 11.4790 (11.6628)
2022-11-25 01:01:18,841:INFO: Dataset: univ                Batch:  3/15	Loss 12.2260 (11.8429)
2022-11-25 01:01:18,842:INFO: Dataset: univ                Batch:  4/15	Loss 11.9520 (11.8703)
2022-11-25 01:01:18,843:INFO: Dataset: univ                Batch:  5/15	Loss 12.0333 (11.9034)
2022-11-25 01:01:18,846:INFO: Dataset: univ                Batch:  6/15	Loss 12.0920 (11.9348)
2022-11-25 01:01:18,847:INFO: Dataset: univ                Batch:  7/15	Loss 12.1702 (11.9661)
2022-11-25 01:01:18,848:INFO: Dataset: univ                Batch:  8/15	Loss 11.8306 (11.9494)
2022-11-25 01:01:18,850:INFO: Dataset: univ                Batch:  9/15	Loss 11.4765 (11.8968)
2022-11-25 01:01:18,851:INFO: Dataset: univ                Batch: 10/15	Loss 11.4581 (11.8512)
2022-11-25 01:01:18,853:INFO: Dataset: univ                Batch: 11/15	Loss 10.6413 (11.7350)
2022-11-25 01:01:18,854:INFO: Dataset: univ                Batch: 12/15	Loss 11.0872 (11.6828)
2022-11-25 01:01:18,855:INFO: Dataset: univ                Batch: 13/15	Loss 10.3427 (11.5777)
2022-11-25 01:01:18,856:INFO: Dataset: univ                Batch: 14/15	Loss 10.4950 (11.5002)
2022-11-25 01:01:18,858:INFO: Dataset: univ                Batch: 15/15	Loss 9.8102 (11.4710)
2022-11-25 01:01:19,141:INFO: Dataset: zara1               Batch: 1/8	Loss 31.5437 (31.5437)
2022-11-25 01:01:19,145:INFO: Dataset: zara1               Batch: 2/8	Loss 31.9120 (31.7150)
2022-11-25 01:01:19,147:INFO: Dataset: zara1               Batch: 3/8	Loss 31.0686 (31.4938)
2022-11-25 01:01:19,150:INFO: Dataset: zara1               Batch: 4/8	Loss 31.6126 (31.5237)
2022-11-25 01:01:19,151:INFO: Dataset: zara1               Batch: 5/8	Loss 31.0029 (31.4205)
2022-11-25 01:01:19,179:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6897 (31.2972)
2022-11-25 01:01:19,180:INFO: Dataset: zara1               Batch: 7/8	Loss 30.5906 (31.1952)
2022-11-25 01:01:19,181:INFO: Dataset: zara1               Batch: 8/8	Loss 29.6087 (31.0140)
2022-11-25 01:01:19,457:INFO: Dataset: zara2               Batch:  1/18	Loss 12.8938 (12.8938)
2022-11-25 01:01:19,458:INFO: Dataset: zara2               Batch:  2/18	Loss 13.2090 (13.0542)
2022-11-25 01:01:19,459:INFO: Dataset: zara2               Batch:  3/18	Loss 12.9616 (13.0257)
2022-11-25 01:01:19,463:INFO: Dataset: zara2               Batch:  4/18	Loss 13.2488 (13.0783)
2022-11-25 01:01:19,481:INFO: Dataset: zara2               Batch:  5/18	Loss 13.0842 (13.0796)
2022-11-25 01:01:19,488:INFO: Dataset: zara2               Batch:  6/18	Loss 13.3473 (13.1273)
2022-11-25 01:01:19,489:INFO: Dataset: zara2               Batch:  7/18	Loss 13.3814 (13.1628)
2022-11-25 01:01:19,490:INFO: Dataset: zara2               Batch:  8/18	Loss 12.8618 (13.1236)
2022-11-25 01:01:19,491:INFO: Dataset: zara2               Batch:  9/18	Loss 12.8595 (13.0961)
2022-11-25 01:01:19,492:INFO: Dataset: zara2               Batch: 10/18	Loss 12.6540 (13.0510)
2022-11-25 01:01:19,493:INFO: Dataset: zara2               Batch: 11/18	Loss 13.1052 (13.0556)
2022-11-25 01:01:19,495:INFO: Dataset: zara2               Batch: 12/18	Loss 12.2824 (12.9895)
2022-11-25 01:01:19,496:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5254 (12.9509)
2022-11-25 01:01:19,497:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3359 (12.9073)
2022-11-25 01:01:19,498:INFO: Dataset: zara2               Batch: 15/18	Loss 12.1893 (12.8599)
2022-11-25 01:01:19,499:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0877 (12.8100)
2022-11-25 01:01:19,500:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8900 (12.7569)
2022-11-25 01:01:19,502:INFO: Dataset: zara2               Batch: 18/18	Loss 11.4296 (12.6915)
2022-11-25 01:01:19,552:INFO: - Computing ADE (validation o)
2022-11-25 01:01:19,821:INFO: 		 ADE on eth                       dataset:	 1.0386751890182495
2022-11-25 01:01:19,822:INFO: Average validation o:	ADE  1.0387	FDE  2.1311
2022-11-25 01:01:19,823:INFO: - Computing loss (validation)
2022-11-25 01:01:20,023:INFO: Dataset: hotel               Batch: 1/2	Loss 33.1551 (33.1551)
2022-11-25 01:01:20,023:INFO: Dataset: hotel               Batch: 2/2	Loss 34.6177 (33.2499)
2022-11-25 01:01:20,281:INFO: Dataset: univ                Batch: 1/3	Loss 10.7085 (10.7085)
2022-11-25 01:01:20,282:INFO: Dataset: univ                Batch: 2/3	Loss 11.1535 (10.9265)
2022-11-25 01:01:20,297:INFO: Dataset: univ                Batch: 3/3	Loss 11.2624 (11.0234)
2022-11-25 01:01:20,594:INFO: Dataset: zara1               Batch: 1/2	Loss 28.7051 (28.7051)
2022-11-25 01:01:20,601:INFO: Dataset: zara1               Batch: 2/2	Loss 28.9895 (28.7737)
2022-11-25 01:01:20,913:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3369 (11.3369)
2022-11-25 01:01:20,914:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3967 (11.3663)
2022-11-25 01:01:20,915:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4358 (11.3907)
2022-11-25 01:01:20,915:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5517 (11.4309)
2022-11-25 01:01:20,915:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4287 (11.4305)
2022-11-25 01:01:20,983:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_652.pth.tar
2022-11-25 01:01:20,983:INFO: 
===> EPOCH: 653 (P4)
2022-11-25 01:01:20,984:INFO: - Computing loss (training)
2022-11-25 01:01:21,245:INFO: Dataset: hotel               Batch: 1/4	Loss 26.0781 (26.0781)
2022-11-25 01:01:21,249:INFO: Dataset: hotel               Batch: 2/4	Loss 27.1935 (26.6142)
2022-11-25 01:01:21,251:INFO: Dataset: hotel               Batch: 3/4	Loss 29.3044 (27.5422)
2022-11-25 01:01:21,254:INFO: Dataset: hotel               Batch: 4/4	Loss 24.0403 (26.9601)
2022-11-25 01:01:21,554:INFO: Dataset: univ                Batch:  1/15	Loss 11.8550 (11.8550)
2022-11-25 01:01:21,556:INFO: Dataset: univ                Batch:  2/15	Loss 11.8031 (11.8282)
2022-11-25 01:01:21,557:INFO: Dataset: univ                Batch:  3/15	Loss 11.8485 (11.8353)
2022-11-25 01:01:21,561:INFO: Dataset: univ                Batch:  4/15	Loss 12.1799 (11.9195)
2022-11-25 01:01:21,581:INFO: Dataset: univ                Batch:  5/15	Loss 12.1194 (11.9589)
2022-11-25 01:01:21,601:INFO: Dataset: univ                Batch:  6/15	Loss 12.1745 (11.9955)
2022-11-25 01:01:21,602:INFO: Dataset: univ                Batch:  7/15	Loss 11.8824 (11.9785)
2022-11-25 01:01:21,603:INFO: Dataset: univ                Batch:  8/15	Loss 11.4976 (11.9199)
2022-11-25 01:01:21,604:INFO: Dataset: univ                Batch:  9/15	Loss 11.8999 (11.9176)
2022-11-25 01:01:21,605:INFO: Dataset: univ                Batch: 10/15	Loss 11.0872 (11.8262)
2022-11-25 01:01:21,606:INFO: Dataset: univ                Batch: 11/15	Loss 10.8102 (11.7201)
2022-11-25 01:01:21,608:INFO: Dataset: univ                Batch: 12/15	Loss 10.8329 (11.6412)
2022-11-25 01:01:21,609:INFO: Dataset: univ                Batch: 13/15	Loss 10.5683 (11.5628)
2022-11-25 01:01:21,610:INFO: Dataset: univ                Batch: 14/15	Loss 10.3179 (11.4686)
2022-11-25 01:01:21,611:INFO: Dataset: univ                Batch: 15/15	Loss 10.0433 (11.4519)
2022-11-25 01:01:21,887:INFO: Dataset: zara1               Batch: 1/8	Loss 31.3450 (31.3450)
2022-11-25 01:01:21,890:INFO: Dataset: zara1               Batch: 2/8	Loss 31.4372 (31.3924)
2022-11-25 01:01:21,892:INFO: Dataset: zara1               Batch: 3/8	Loss 31.4616 (31.4156)
2022-11-25 01:01:21,893:INFO: Dataset: zara1               Batch: 4/8	Loss 31.4394 (31.4223)
2022-11-25 01:01:21,895:INFO: Dataset: zara1               Batch: 5/8	Loss 31.0175 (31.3342)
2022-11-25 01:01:21,917:INFO: Dataset: zara1               Batch: 6/8	Loss 30.3637 (31.1712)
2022-11-25 01:01:21,918:INFO: Dataset: zara1               Batch: 7/8	Loss 30.7071 (31.1036)
2022-11-25 01:01:21,918:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2632 (31.0155)
2022-11-25 01:01:22,216:INFO: Dataset: zara2               Batch:  1/18	Loss 13.2263 (13.2263)
2022-11-25 01:01:22,217:INFO: Dataset: zara2               Batch:  2/18	Loss 13.1798 (13.2038)
2022-11-25 01:01:22,251:INFO: Dataset: zara2               Batch:  3/18	Loss 13.0957 (13.1685)
2022-11-25 01:01:22,252:INFO: Dataset: zara2               Batch:  4/18	Loss 12.9016 (13.1003)
2022-11-25 01:01:22,253:INFO: Dataset: zara2               Batch:  5/18	Loss 13.0929 (13.0988)
2022-11-25 01:01:22,258:INFO: Dataset: zara2               Batch:  6/18	Loss 13.5121 (13.1605)
2022-11-25 01:01:22,259:INFO: Dataset: zara2               Batch:  7/18	Loss 13.2964 (13.1797)
2022-11-25 01:01:22,261:INFO: Dataset: zara2               Batch:  8/18	Loss 13.0223 (13.1591)
2022-11-25 01:01:22,262:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7919 (13.1199)
2022-11-25 01:01:22,263:INFO: Dataset: zara2               Batch: 10/18	Loss 12.8056 (13.0850)
2022-11-25 01:01:22,265:INFO: Dataset: zara2               Batch: 11/18	Loss 12.7210 (13.0528)
2022-11-25 01:01:22,267:INFO: Dataset: zara2               Batch: 12/18	Loss 12.3305 (12.9899)
2022-11-25 01:01:22,268:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4056 (12.9399)
2022-11-25 01:01:22,269:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1486 (12.8808)
2022-11-25 01:01:22,271:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0541 (12.8254)
2022-11-25 01:01:22,273:INFO: Dataset: zara2               Batch: 16/18	Loss 12.2541 (12.7887)
2022-11-25 01:01:22,275:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6770 (12.7285)
2022-11-25 01:01:22,277:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3915 (12.6697)
2022-11-25 01:01:22,335:INFO: - Computing ADE (validation o)
2022-11-25 01:01:22,621:INFO: 		 ADE on eth                       dataset:	 1.1010323762893677
2022-11-25 01:01:22,622:INFO: Average validation o:	ADE  1.1010	FDE  2.2056
2022-11-25 01:01:22,622:INFO: - Computing loss (validation)
2022-11-25 01:01:22,866:INFO: Dataset: hotel               Batch: 1/2	Loss 33.4370 (33.4370)
2022-11-25 01:01:22,867:INFO: Dataset: hotel               Batch: 2/2	Loss 28.9146 (33.1128)
2022-11-25 01:01:23,161:INFO: Dataset: univ                Batch: 1/3	Loss 10.6917 (10.6917)
2022-11-25 01:01:23,163:INFO: Dataset: univ                Batch: 2/3	Loss 11.1323 (10.8901)
2022-11-25 01:01:23,163:INFO: Dataset: univ                Batch: 3/3	Loss 10.9640 (10.9132)
2022-11-25 01:01:23,461:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8641 (28.8641)
2022-11-25 01:01:23,462:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6005 (28.7938)
2022-11-25 01:01:23,755:INFO: Dataset: zara2               Batch: 1/5	Loss 11.6003 (11.6003)
2022-11-25 01:01:23,755:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4154 (11.5146)
2022-11-25 01:01:23,756:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4995 (11.5095)
2022-11-25 01:01:23,758:INFO: Dataset: zara2               Batch: 4/5	Loss 11.3552 (11.4710)
2022-11-25 01:01:23,759:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3363 (11.4440)
2022-11-25 01:01:23,820:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_653.pth.tar
2022-11-25 01:01:23,820:INFO: 
===> EPOCH: 654 (P4)
2022-11-25 01:01:23,821:INFO: - Computing loss (training)
2022-11-25 01:01:24,055:INFO: Dataset: hotel               Batch: 1/4	Loss 26.9556 (26.9556)
2022-11-25 01:01:24,059:INFO: Dataset: hotel               Batch: 2/4	Loss 26.9473 (26.9514)
2022-11-25 01:01:24,062:INFO: Dataset: hotel               Batch: 3/4	Loss 28.0518 (27.3188)
2022-11-25 01:01:24,063:INFO: Dataset: hotel               Batch: 4/4	Loss 24.3389 (26.7881)
2022-11-25 01:01:24,396:INFO: Dataset: univ                Batch:  1/15	Loss 11.6956 (11.6956)
2022-11-25 01:01:24,398:INFO: Dataset: univ                Batch:  2/15	Loss 11.7812 (11.7362)
2022-11-25 01:01:24,400:INFO: Dataset: univ                Batch:  3/15	Loss 11.7825 (11.7512)
2022-11-25 01:01:24,405:INFO: Dataset: univ                Batch:  4/15	Loss 11.8779 (11.7804)
2022-11-25 01:01:24,429:INFO: Dataset: univ                Batch:  5/15	Loss 12.0490 (11.8334)
2022-11-25 01:01:24,462:INFO: Dataset: univ                Batch:  6/15	Loss 11.9113 (11.8459)
2022-11-25 01:01:24,463:INFO: Dataset: univ                Batch:  7/15	Loss 11.9843 (11.8658)
2022-11-25 01:01:24,464:INFO: Dataset: univ                Batch:  8/15	Loss 11.5420 (11.8245)
2022-11-25 01:01:24,466:INFO: Dataset: univ                Batch:  9/15	Loss 11.3735 (11.7677)
2022-11-25 01:01:24,467:INFO: Dataset: univ                Batch: 10/15	Loss 11.1065 (11.7047)
2022-11-25 01:01:24,468:INFO: Dataset: univ                Batch: 11/15	Loss 10.6816 (11.6186)
2022-11-25 01:01:24,470:INFO: Dataset: univ                Batch: 12/15	Loss 10.6770 (11.5315)
2022-11-25 01:01:24,471:INFO: Dataset: univ                Batch: 13/15	Loss 10.8543 (11.4839)
2022-11-25 01:01:24,472:INFO: Dataset: univ                Batch: 14/15	Loss 10.0233 (11.3784)
2022-11-25 01:01:24,474:INFO: Dataset: univ                Batch: 15/15	Loss 10.0132 (11.3598)
2022-11-25 01:01:24,734:INFO: Dataset: zara1               Batch: 1/8	Loss 31.8356 (31.8356)
2022-11-25 01:01:24,735:INFO: Dataset: zara1               Batch: 2/8	Loss 31.0163 (31.3968)
2022-11-25 01:01:24,741:INFO: Dataset: zara1               Batch: 3/8	Loss 31.5557 (31.4484)
2022-11-25 01:01:24,750:INFO: Dataset: zara1               Batch: 4/8	Loss 31.1591 (31.3752)
2022-11-25 01:01:24,752:INFO: Dataset: zara1               Batch: 5/8	Loss 30.5942 (31.2323)
2022-11-25 01:01:24,820:INFO: Dataset: zara1               Batch: 6/8	Loss 31.1270 (31.2143)
2022-11-25 01:01:24,824:INFO: Dataset: zara1               Batch: 7/8	Loss 29.9928 (31.0320)
2022-11-25 01:01:24,827:INFO: Dataset: zara1               Batch: 8/8	Loss 30.5278 (30.9757)
2022-11-25 01:01:25,101:INFO: Dataset: zara2               Batch:  1/18	Loss 13.4873 (13.4873)
2022-11-25 01:01:25,103:INFO: Dataset: zara2               Batch:  2/18	Loss 13.1104 (13.2829)
2022-11-25 01:01:25,107:INFO: Dataset: zara2               Batch:  3/18	Loss 13.5722 (13.3790)
2022-11-25 01:01:25,108:INFO: Dataset: zara2               Batch:  4/18	Loss 13.9595 (13.5062)
2022-11-25 01:01:25,144:INFO: Dataset: zara2               Batch:  5/18	Loss 13.1714 (13.4402)
2022-11-25 01:01:25,151:INFO: Dataset: zara2               Batch:  6/18	Loss 12.9084 (13.3483)
2022-11-25 01:01:25,153:INFO: Dataset: zara2               Batch:  7/18	Loss 12.9396 (13.2878)
2022-11-25 01:01:25,154:INFO: Dataset: zara2               Batch:  8/18	Loss 13.2547 (13.2839)
2022-11-25 01:01:25,155:INFO: Dataset: zara2               Batch:  9/18	Loss 12.9596 (13.2452)
2022-11-25 01:01:25,156:INFO: Dataset: zara2               Batch: 10/18	Loss 12.6650 (13.1818)
2022-11-25 01:01:25,156:INFO: Dataset: zara2               Batch: 11/18	Loss 12.4186 (13.1110)
2022-11-25 01:01:25,158:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6775 (13.0768)
2022-11-25 01:01:25,159:INFO: Dataset: zara2               Batch: 13/18	Loss 12.1284 (12.9988)
2022-11-25 01:01:25,160:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3386 (12.9533)
2022-11-25 01:01:25,161:INFO: Dataset: zara2               Batch: 15/18	Loss 11.7590 (12.8649)
2022-11-25 01:01:25,161:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8306 (12.8027)
2022-11-25 01:01:25,162:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6962 (12.7390)
2022-11-25 01:01:25,164:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5715 (12.6843)
2022-11-25 01:01:25,213:INFO: - Computing ADE (validation o)
2022-11-25 01:01:25,478:INFO: 		 ADE on eth                       dataset:	 1.0381410121917725
2022-11-25 01:01:25,478:INFO: Average validation o:	ADE  1.0381	FDE  2.1619
2022-11-25 01:01:25,479:INFO: - Computing loss (validation)
2022-11-25 01:01:25,679:INFO: Dataset: hotel               Batch: 1/2	Loss 33.1217 (33.1217)
2022-11-25 01:01:25,680:INFO: Dataset: hotel               Batch: 2/2	Loss 34.0072 (33.1973)
2022-11-25 01:01:25,953:INFO: Dataset: univ                Batch: 1/3	Loss 10.9162 (10.9162)
2022-11-25 01:01:25,965:INFO: Dataset: univ                Batch: 2/3	Loss 11.0122 (10.9647)
2022-11-25 01:01:25,965:INFO: Dataset: univ                Batch: 3/3	Loss 10.8463 (10.9264)
2022-11-25 01:01:26,217:INFO: Dataset: zara1               Batch: 1/2	Loss 28.7969 (28.7969)
2022-11-25 01:01:26,217:INFO: Dataset: zara1               Batch: 2/2	Loss 28.7198 (28.7741)
2022-11-25 01:01:26,502:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3866 (11.3866)
2022-11-25 01:01:26,503:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3041 (11.3444)
2022-11-25 01:01:26,504:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2666 (11.3199)
2022-11-25 01:01:26,506:INFO: Dataset: zara2               Batch: 4/5	Loss 11.5160 (11.3679)
2022-11-25 01:01:26,507:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4777 (11.3890)
2022-11-25 01:01:26,560:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_654.pth.tar
2022-11-25 01:01:26,560:INFO: 
===> EPOCH: 655 (P4)
2022-11-25 01:01:26,561:INFO: - Computing loss (training)
2022-11-25 01:01:26,805:INFO: Dataset: hotel               Batch: 1/4	Loss 29.0028 (29.0028)
2022-11-25 01:01:26,807:INFO: Dataset: hotel               Batch: 2/4	Loss 26.5592 (27.7347)
2022-11-25 01:01:26,809:INFO: Dataset: hotel               Batch: 3/4	Loss 26.7239 (27.3999)
2022-11-25 01:01:26,810:INFO: Dataset: hotel               Batch: 4/4	Loss 24.2851 (26.8780)
2022-11-25 01:01:27,126:INFO: Dataset: univ                Batch:  1/15	Loss 11.7414 (11.7414)
2022-11-25 01:01:27,137:INFO: Dataset: univ                Batch:  2/15	Loss 12.1013 (11.9172)
2022-11-25 01:01:27,181:INFO: Dataset: univ                Batch:  3/15	Loss 11.6412 (11.8266)
2022-11-25 01:01:27,183:INFO: Dataset: univ                Batch:  4/15	Loss 11.6531 (11.7844)
2022-11-25 01:01:27,184:INFO: Dataset: univ                Batch:  5/15	Loss 11.8723 (11.8030)
2022-11-25 01:01:27,187:INFO: Dataset: univ                Batch:  6/15	Loss 11.8024 (11.8029)
2022-11-25 01:01:27,188:INFO: Dataset: univ                Batch:  7/15	Loss 11.8992 (11.8177)
2022-11-25 01:01:27,189:INFO: Dataset: univ                Batch:  8/15	Loss 11.9200 (11.8298)
2022-11-25 01:01:27,190:INFO: Dataset: univ                Batch:  9/15	Loss 11.3411 (11.7672)
2022-11-25 01:01:27,192:INFO: Dataset: univ                Batch: 10/15	Loss 11.2186 (11.7125)
2022-11-25 01:01:27,193:INFO: Dataset: univ                Batch: 11/15	Loss 11.0124 (11.6451)
2022-11-25 01:01:27,195:INFO: Dataset: univ                Batch: 12/15	Loss 10.8734 (11.5807)
2022-11-25 01:01:27,196:INFO: Dataset: univ                Batch: 13/15	Loss 10.5981 (11.5051)
2022-11-25 01:01:27,197:INFO: Dataset: univ                Batch: 14/15	Loss 9.9073 (11.3820)
2022-11-25 01:01:27,199:INFO: Dataset: univ                Batch: 15/15	Loss 10.4304 (11.3693)
2022-11-25 01:01:27,460:INFO: Dataset: zara1               Batch: 1/8	Loss 31.2762 (31.2762)
2022-11-25 01:01:27,462:INFO: Dataset: zara1               Batch: 2/8	Loss 31.7758 (31.5286)
2022-11-25 01:01:27,463:INFO: Dataset: zara1               Batch: 3/8	Loss 30.8933 (31.3253)
2022-11-25 01:01:27,465:INFO: Dataset: zara1               Batch: 4/8	Loss 31.2447 (31.3056)
2022-11-25 01:01:27,467:INFO: Dataset: zara1               Batch: 5/8	Loss 30.8671 (31.2269)
2022-11-25 01:01:27,484:INFO: Dataset: zara1               Batch: 6/8	Loss 30.9259 (31.1766)
2022-11-25 01:01:27,485:INFO: Dataset: zara1               Batch: 7/8	Loss 30.2207 (31.0195)
2022-11-25 01:01:27,486:INFO: Dataset: zara1               Batch: 8/8	Loss 30.2088 (30.9231)
2022-11-25 01:01:27,742:INFO: Dataset: zara2               Batch:  1/18	Loss 12.5997 (12.5997)
2022-11-25 01:01:27,744:INFO: Dataset: zara2               Batch:  2/18	Loss 13.4126 (12.9981)
2022-11-25 01:01:27,747:INFO: Dataset: zara2               Batch:  3/18	Loss 13.0682 (13.0237)
2022-11-25 01:01:27,749:INFO: Dataset: zara2               Batch:  4/18	Loss 13.3192 (13.0989)
2022-11-25 01:01:27,784:INFO: Dataset: zara2               Batch:  5/18	Loss 13.1788 (13.1140)
2022-11-25 01:01:27,789:INFO: Dataset: zara2               Batch:  6/18	Loss 13.0784 (13.1077)
2022-11-25 01:01:27,790:INFO: Dataset: zara2               Batch:  7/18	Loss 13.3443 (13.1378)
2022-11-25 01:01:27,790:INFO: Dataset: zara2               Batch:  8/18	Loss 12.5426 (13.0654)
2022-11-25 01:01:27,791:INFO: Dataset: zara2               Batch:  9/18	Loss 12.5557 (13.0108)
2022-11-25 01:01:27,792:INFO: Dataset: zara2               Batch: 10/18	Loss 12.6537 (12.9769)
2022-11-25 01:01:27,794:INFO: Dataset: zara2               Batch: 11/18	Loss 12.6961 (12.9531)
2022-11-25 01:01:27,795:INFO: Dataset: zara2               Batch: 12/18	Loss 12.3437 (12.9033)
2022-11-25 01:01:27,796:INFO: Dataset: zara2               Batch: 13/18	Loss 12.3380 (12.8610)
2022-11-25 01:01:27,797:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5074 (12.8333)
2022-11-25 01:01:27,798:INFO: Dataset: zara2               Batch: 15/18	Loss 12.2052 (12.7890)
2022-11-25 01:01:27,799:INFO: Dataset: zara2               Batch: 16/18	Loss 12.0760 (12.7397)
2022-11-25 01:01:27,800:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6022 (12.6781)
2022-11-25 01:01:27,802:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6951 (12.6261)
2022-11-25 01:01:27,850:INFO: - Computing ADE (validation o)
2022-11-25 01:01:28,112:INFO: 		 ADE on eth                       dataset:	 1.028612494468689
2022-11-25 01:01:28,113:INFO: Average validation o:	ADE  1.0286	FDE  2.1200
2022-11-25 01:01:28,113:INFO: - Computing loss (validation)
2022-11-25 01:01:28,307:INFO: Dataset: hotel               Batch: 1/2	Loss 32.5121 (32.5121)
2022-11-25 01:01:28,308:INFO: Dataset: hotel               Batch: 2/2	Loss 42.1885 (33.0735)
2022-11-25 01:01:28,571:INFO: Dataset: univ                Batch: 1/3	Loss 11.0514 (11.0514)
2022-11-25 01:01:28,574:INFO: Dataset: univ                Batch: 2/3	Loss 10.5540 (10.7870)
2022-11-25 01:01:28,575:INFO: Dataset: univ                Batch: 3/3	Loss 10.7980 (10.7905)
2022-11-25 01:01:28,842:INFO: Dataset: zara1               Batch: 1/2	Loss 28.6763 (28.6763)
2022-11-25 01:01:28,842:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1117 (28.7827)
2022-11-25 01:01:29,109:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3851 (11.3851)
2022-11-25 01:01:29,111:INFO: Dataset: zara2               Batch: 2/5	Loss 11.5251 (11.4561)
2022-11-25 01:01:29,117:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2938 (11.3978)
2022-11-25 01:01:29,118:INFO: Dataset: zara2               Batch: 4/5	Loss 11.4406 (11.4081)
2022-11-25 01:01:29,118:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4577 (11.4181)
2022-11-25 01:01:29,174:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_655.pth.tar
2022-11-25 01:01:29,174:INFO: 
===> EPOCH: 656 (P4)
2022-11-25 01:01:29,175:INFO: - Computing loss (training)
2022-11-25 01:01:29,377:INFO: Dataset: hotel               Batch: 1/4	Loss 28.1061 (28.1061)
2022-11-25 01:01:29,381:INFO: Dataset: hotel               Batch: 2/4	Loss 25.0882 (26.5264)
2022-11-25 01:01:29,384:INFO: Dataset: hotel               Batch: 3/4	Loss 26.1842 (26.4066)
2022-11-25 01:01:29,386:INFO: Dataset: hotel               Batch: 4/4	Loss 27.7535 (26.6465)
2022-11-25 01:01:29,648:INFO: Dataset: univ                Batch:  1/15	Loss 11.7153 (11.7153)
2022-11-25 01:01:29,651:INFO: Dataset: univ                Batch:  2/15	Loss 11.9658 (11.8457)
2022-11-25 01:01:29,652:INFO: Dataset: univ                Batch:  3/15	Loss 11.7004 (11.7943)
2022-11-25 01:01:29,654:INFO: Dataset: univ                Batch:  4/15	Loss 11.9601 (11.8319)
2022-11-25 01:01:29,694:INFO: Dataset: univ                Batch:  5/15	Loss 11.6946 (11.8012)
2022-11-25 01:01:29,698:INFO: Dataset: univ                Batch:  6/15	Loss 11.5772 (11.7645)
2022-11-25 01:01:29,700:INFO: Dataset: univ                Batch:  7/15	Loss 11.5929 (11.7401)
2022-11-25 01:01:29,701:INFO: Dataset: univ                Batch:  8/15	Loss 11.3304 (11.6885)
2022-11-25 01:01:29,702:INFO: Dataset: univ                Batch:  9/15	Loss 11.3935 (11.6549)
2022-11-25 01:01:29,704:INFO: Dataset: univ                Batch: 10/15	Loss 11.3703 (11.6301)
2022-11-25 01:01:29,706:INFO: Dataset: univ                Batch: 11/15	Loss 10.8539 (11.5568)
2022-11-25 01:01:29,708:INFO: Dataset: univ                Batch: 12/15	Loss 10.7610 (11.4868)
2022-11-25 01:01:29,709:INFO: Dataset: univ                Batch: 13/15	Loss 10.3242 (11.3890)
2022-11-25 01:01:29,710:INFO: Dataset: univ                Batch: 14/15	Loss 10.1875 (11.2927)
2022-11-25 01:01:29,711:INFO: Dataset: univ                Batch: 15/15	Loss 9.7813 (11.2760)
2022-11-25 01:01:29,995:INFO: Dataset: zara1               Batch: 1/8	Loss 31.0180 (31.0180)
2022-11-25 01:01:29,997:INFO: Dataset: zara1               Batch: 2/8	Loss 30.9609 (30.9915)
2022-11-25 01:01:29,999:INFO: Dataset: zara1               Batch: 3/8	Loss 31.4019 (31.1209)
2022-11-25 01:01:30,002:INFO: Dataset: zara1               Batch: 4/8	Loss 31.2639 (31.1562)
2022-11-25 01:01:30,004:INFO: Dataset: zara1               Batch: 5/8	Loss 31.5799 (31.2444)
2022-11-25 01:01:30,072:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6512 (31.1431)
2022-11-25 01:01:30,076:INFO: Dataset: zara1               Batch: 7/8	Loss 30.2411 (31.0279)
2022-11-25 01:01:30,079:INFO: Dataset: zara1               Batch: 8/8	Loss 30.1097 (30.9341)
2022-11-25 01:01:30,418:INFO: Dataset: zara2               Batch:  1/18	Loss 12.8224 (12.8224)
2022-11-25 01:01:30,419:INFO: Dataset: zara2               Batch:  2/18	Loss 12.9954 (12.9094)
2022-11-25 01:01:30,422:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3419 (13.0644)
2022-11-25 01:01:30,451:INFO: Dataset: zara2               Batch:  4/18	Loss 13.2511 (13.1096)
2022-11-25 01:01:30,452:INFO: Dataset: zara2               Batch:  5/18	Loss 13.0260 (13.0925)
2022-11-25 01:01:30,456:INFO: Dataset: zara2               Batch:  6/18	Loss 13.0938 (13.0927)
2022-11-25 01:01:30,458:INFO: Dataset: zara2               Batch:  7/18	Loss 13.0979 (13.0934)
2022-11-25 01:01:30,459:INFO: Dataset: zara2               Batch:  8/18	Loss 13.0139 (13.0836)
2022-11-25 01:01:30,460:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7029 (13.0390)
2022-11-25 01:01:30,461:INFO: Dataset: zara2               Batch: 10/18	Loss 13.2070 (13.0542)
2022-11-25 01:01:30,463:INFO: Dataset: zara2               Batch: 11/18	Loss 12.4400 (13.0003)
2022-11-25 01:01:30,464:INFO: Dataset: zara2               Batch: 12/18	Loss 12.5864 (12.9687)
2022-11-25 01:01:30,465:INFO: Dataset: zara2               Batch: 13/18	Loss 12.2806 (12.9157)
2022-11-25 01:01:30,466:INFO: Dataset: zara2               Batch: 14/18	Loss 11.9817 (12.8446)
2022-11-25 01:01:30,467:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0843 (12.7932)
2022-11-25 01:01:30,469:INFO: Dataset: zara2               Batch: 16/18	Loss 12.1058 (12.7506)
2022-11-25 01:01:30,470:INFO: Dataset: zara2               Batch: 17/18	Loss 11.5153 (12.6767)
2022-11-25 01:01:30,472:INFO: Dataset: zara2               Batch: 18/18	Loss 11.9270 (12.6440)
2022-11-25 01:01:30,530:INFO: - Computing ADE (validation o)
2022-11-25 01:01:30,838:INFO: 		 ADE on eth                       dataset:	 1.0747885704040527
2022-11-25 01:01:30,838:INFO: Average validation o:	ADE  1.0748	FDE  2.1850
2022-11-25 01:01:30,839:INFO: - Computing loss (validation)
2022-11-25 01:01:31,068:INFO: Dataset: hotel               Batch: 1/2	Loss 33.2742 (33.2742)
2022-11-25 01:01:31,068:INFO: Dataset: hotel               Batch: 2/2	Loss 30.9083 (33.1127)
2022-11-25 01:01:31,399:INFO: Dataset: univ                Batch: 1/3	Loss 10.5690 (10.5690)
2022-11-25 01:01:31,401:INFO: Dataset: univ                Batch: 2/3	Loss 11.0636 (10.7837)
2022-11-25 01:01:31,404:INFO: Dataset: univ                Batch: 3/3	Loss 10.8434 (10.8018)
2022-11-25 01:01:31,791:INFO: Dataset: zara1               Batch: 1/2	Loss 28.5213 (28.5213)
2022-11-25 01:01:31,792:INFO: Dataset: zara1               Batch: 2/2	Loss 29.4584 (28.7533)
2022-11-25 01:01:32,119:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3486 (11.3486)
2022-11-25 01:01:32,122:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4767 (11.4129)
2022-11-25 01:01:32,123:INFO: Dataset: zara2               Batch: 3/5	Loss 11.2578 (11.3603)
2022-11-25 01:01:32,123:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2637 (11.3374)
2022-11-25 01:01:32,133:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5477 (11.3817)
2022-11-25 01:01:32,188:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_656.pth.tar
2022-11-25 01:01:32,188:INFO: 
===> EPOCH: 657 (P4)
2022-11-25 01:01:32,189:INFO: - Computing loss (training)
2022-11-25 01:01:32,392:INFO: Dataset: hotel               Batch: 1/4	Loss 27.6388 (27.6388)
2022-11-25 01:01:32,394:INFO: Dataset: hotel               Batch: 2/4	Loss 26.8618 (27.2483)
2022-11-25 01:01:32,397:INFO: Dataset: hotel               Batch: 3/4	Loss 26.7797 (27.0826)
2022-11-25 01:01:32,400:INFO: Dataset: hotel               Batch: 4/4	Loss 24.7926 (26.6808)
2022-11-25 01:01:32,685:INFO: Dataset: univ                Batch:  1/15	Loss 11.6297 (11.6297)
2022-11-25 01:01:32,686:INFO: Dataset: univ                Batch:  2/15	Loss 11.6750 (11.6536)
2022-11-25 01:01:32,689:INFO: Dataset: univ                Batch:  3/15	Loss 12.1257 (11.7997)
2022-11-25 01:01:32,692:INFO: Dataset: univ                Batch:  4/15	Loss 11.5943 (11.7434)
2022-11-25 01:01:32,721:INFO: Dataset: univ                Batch:  5/15	Loss 11.7695 (11.7483)
2022-11-25 01:01:32,726:INFO: Dataset: univ                Batch:  6/15	Loss 11.6573 (11.7336)
2022-11-25 01:01:32,727:INFO: Dataset: univ                Batch:  7/15	Loss 11.9867 (11.7682)
2022-11-25 01:01:32,728:INFO: Dataset: univ                Batch:  8/15	Loss 11.1410 (11.6825)
2022-11-25 01:01:32,729:INFO: Dataset: univ                Batch:  9/15	Loss 11.3718 (11.6468)
2022-11-25 01:01:32,730:INFO: Dataset: univ                Batch: 10/15	Loss 11.2162 (11.6090)
2022-11-25 01:01:32,732:INFO: Dataset: univ                Batch: 11/15	Loss 11.1446 (11.5682)
2022-11-25 01:01:32,734:INFO: Dataset: univ                Batch: 12/15	Loss 10.7400 (11.4994)
2022-11-25 01:01:32,735:INFO: Dataset: univ                Batch: 13/15	Loss 10.5331 (11.4231)
2022-11-25 01:01:32,737:INFO: Dataset: univ                Batch: 14/15	Loss 10.1944 (11.3298)
2022-11-25 01:01:32,745:INFO: Dataset: univ                Batch: 15/15	Loss 9.9957 (11.3080)
2022-11-25 01:01:33,003:INFO: Dataset: zara1               Batch: 1/8	Loss 31.7759 (31.7759)
2022-11-25 01:01:33,006:INFO: Dataset: zara1               Batch: 2/8	Loss 31.1996 (31.4734)
2022-11-25 01:01:33,009:INFO: Dataset: zara1               Batch: 3/8	Loss 31.7319 (31.5704)
2022-11-25 01:01:33,010:INFO: Dataset: zara1               Batch: 4/8	Loss 30.9070 (31.4056)
2022-11-25 01:01:33,012:INFO: Dataset: zara1               Batch: 5/8	Loss 30.2859 (31.1953)
2022-11-25 01:01:33,032:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6088 (31.1039)
2022-11-25 01:01:33,033:INFO: Dataset: zara1               Batch: 7/8	Loss 30.2440 (30.9701)
2022-11-25 01:01:33,034:INFO: Dataset: zara1               Batch: 8/8	Loss 30.0445 (30.8741)
2022-11-25 01:01:33,302:INFO: Dataset: zara2               Batch:  1/18	Loss 12.6407 (12.6407)
2022-11-25 01:01:33,303:INFO: Dataset: zara2               Batch:  2/18	Loss 12.9313 (12.7842)
2022-11-25 01:01:33,307:INFO: Dataset: zara2               Batch:  3/18	Loss 12.8569 (12.8067)
2022-11-25 01:01:33,308:INFO: Dataset: zara2               Batch:  4/18	Loss 12.9282 (12.8388)
2022-11-25 01:01:33,329:INFO: Dataset: zara2               Batch:  5/18	Loss 13.0537 (12.8800)
2022-11-25 01:01:33,337:INFO: Dataset: zara2               Batch:  6/18	Loss 13.7488 (13.0081)
2022-11-25 01:01:33,338:INFO: Dataset: zara2               Batch:  7/18	Loss 13.4204 (13.0683)
2022-11-25 01:01:33,339:INFO: Dataset: zara2               Batch:  8/18	Loss 12.7087 (13.0274)
2022-11-25 01:01:33,340:INFO: Dataset: zara2               Batch:  9/18	Loss 12.7653 (12.9978)
2022-11-25 01:01:33,341:INFO: Dataset: zara2               Batch: 10/18	Loss 13.0335 (13.0014)
2022-11-25 01:01:33,342:INFO: Dataset: zara2               Batch: 11/18	Loss 12.9253 (12.9943)
2022-11-25 01:01:33,344:INFO: Dataset: zara2               Batch: 12/18	Loss 12.4019 (12.9485)
2022-11-25 01:01:33,345:INFO: Dataset: zara2               Batch: 13/18	Loss 12.2674 (12.8913)
2022-11-25 01:01:33,346:INFO: Dataset: zara2               Batch: 14/18	Loss 12.1040 (12.8311)
2022-11-25 01:01:33,346:INFO: Dataset: zara2               Batch: 15/18	Loss 11.9942 (12.7794)
2022-11-25 01:01:33,348:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7466 (12.7171)
2022-11-25 01:01:33,349:INFO: Dataset: zara2               Batch: 17/18	Loss 11.8458 (12.6687)
2022-11-25 01:01:33,350:INFO: Dataset: zara2               Batch: 18/18	Loss 11.2948 (12.5997)
2022-11-25 01:01:33,400:INFO: - Computing ADE (validation o)
2022-11-25 01:01:33,676:INFO: 		 ADE on eth                       dataset:	 1.045169711112976
2022-11-25 01:01:33,676:INFO: Average validation o:	ADE  1.0452	FDE  2.1658
2022-11-25 01:01:33,677:INFO: - Computing loss (validation)
2022-11-25 01:01:33,872:INFO: Dataset: hotel               Batch: 1/2	Loss 33.0021 (33.0021)
2022-11-25 01:01:33,872:INFO: Dataset: hotel               Batch: 2/2	Loss 33.4489 (33.0281)
2022-11-25 01:01:34,166:INFO: Dataset: univ                Batch: 1/3	Loss 10.4121 (10.4121)
2022-11-25 01:01:34,166:INFO: Dataset: univ                Batch: 2/3	Loss 11.0207 (10.7053)
2022-11-25 01:01:34,170:INFO: Dataset: univ                Batch: 3/3	Loss 10.7260 (10.7117)
2022-11-25 01:01:34,429:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8163 (28.8163)
2022-11-25 01:01:34,429:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6269 (28.7639)
2022-11-25 01:01:34,677:INFO: Dataset: zara2               Batch: 1/5	Loss 11.4953 (11.4953)
2022-11-25 01:01:34,681:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4389 (11.4665)
2022-11-25 01:01:34,682:INFO: Dataset: zara2               Batch: 3/5	Loss 11.3002 (11.4108)
2022-11-25 01:01:34,682:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2708 (11.3763)
2022-11-25 01:01:34,684:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4689 (11.3941)
2022-11-25 01:01:34,742:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_657.pth.tar
2022-11-25 01:01:34,742:INFO: 
===> EPOCH: 658 (P4)
2022-11-25 01:01:34,742:INFO: - Computing loss (training)
2022-11-25 01:01:34,946:INFO: Dataset: hotel               Batch: 1/4	Loss 24.9514 (24.9514)
2022-11-25 01:01:34,957:INFO: Dataset: hotel               Batch: 2/4	Loss 28.1380 (26.5183)
2022-11-25 01:01:34,965:INFO: Dataset: hotel               Batch: 3/4	Loss 26.3225 (26.4546)
2022-11-25 01:01:34,966:INFO: Dataset: hotel               Batch: 4/4	Loss 26.8285 (26.5192)
2022-11-25 01:01:35,222:INFO: Dataset: univ                Batch:  1/15	Loss 12.0044 (12.0044)
2022-11-25 01:01:35,227:INFO: Dataset: univ                Batch:  2/15	Loss 11.4440 (11.7119)
2022-11-25 01:01:35,228:INFO: Dataset: univ                Batch:  3/15	Loss 11.5630 (11.6614)
2022-11-25 01:01:35,231:INFO: Dataset: univ                Batch:  4/15	Loss 11.4570 (11.6070)
2022-11-25 01:01:35,275:INFO: Dataset: univ                Batch:  5/15	Loss 11.7761 (11.6395)
2022-11-25 01:01:35,282:INFO: Dataset: univ                Batch:  6/15	Loss 11.5867 (11.6315)
2022-11-25 01:01:35,283:INFO: Dataset: univ                Batch:  7/15	Loss 11.5959 (11.6264)
2022-11-25 01:01:35,284:INFO: Dataset: univ                Batch:  8/15	Loss 11.3417 (11.5914)
2022-11-25 01:01:35,285:INFO: Dataset: univ                Batch:  9/15	Loss 11.3543 (11.5665)
2022-11-25 01:01:35,286:INFO: Dataset: univ                Batch: 10/15	Loss 11.2299 (11.5321)
2022-11-25 01:01:35,288:INFO: Dataset: univ                Batch: 11/15	Loss 10.7141 (11.4558)
2022-11-25 01:01:35,289:INFO: Dataset: univ                Batch: 12/15	Loss 10.6085 (11.3831)
2022-11-25 01:01:35,290:INFO: Dataset: univ                Batch: 13/15	Loss 10.4210 (11.3019)
2022-11-25 01:01:35,291:INFO: Dataset: univ                Batch: 14/15	Loss 10.4620 (11.2442)
2022-11-25 01:01:35,292:INFO: Dataset: univ                Batch: 15/15	Loss 9.9227 (11.2244)
2022-11-25 01:01:35,572:INFO: Dataset: zara1               Batch: 1/8	Loss 31.0329 (31.0329)
2022-11-25 01:01:35,576:INFO: Dataset: zara1               Batch: 2/8	Loss 31.3833 (31.1913)
2022-11-25 01:01:35,577:INFO: Dataset: zara1               Batch: 3/8	Loss 31.5107 (31.3021)
2022-11-25 01:01:35,578:INFO: Dataset: zara1               Batch: 4/8	Loss 31.0393 (31.2352)
2022-11-25 01:01:35,582:INFO: Dataset: zara1               Batch: 5/8	Loss 30.5442 (31.1040)
2022-11-25 01:01:35,621:INFO: Dataset: zara1               Batch: 6/8	Loss 30.8864 (31.0672)
2022-11-25 01:01:35,622:INFO: Dataset: zara1               Batch: 7/8	Loss 30.3758 (30.9723)
2022-11-25 01:01:35,624:INFO: Dataset: zara1               Batch: 8/8	Loss 29.7834 (30.8459)
2022-11-25 01:01:35,892:INFO: Dataset: zara2               Batch:  1/18	Loss 12.7738 (12.7738)
2022-11-25 01:01:35,893:INFO: Dataset: zara2               Batch:  2/18	Loss 13.0359 (12.8972)
2022-11-25 01:01:35,906:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3092 (13.0301)
2022-11-25 01:01:35,907:INFO: Dataset: zara2               Batch:  4/18	Loss 13.0600 (13.0371)
2022-11-25 01:01:35,950:INFO: Dataset: zara2               Batch:  5/18	Loss 13.2232 (13.0719)
2022-11-25 01:01:35,958:INFO: Dataset: zara2               Batch:  6/18	Loss 13.2178 (13.0948)
2022-11-25 01:01:35,960:INFO: Dataset: zara2               Batch:  7/18	Loss 12.7306 (13.0424)
2022-11-25 01:01:35,961:INFO: Dataset: zara2               Batch:  8/18	Loss 12.6574 (12.9927)
2022-11-25 01:01:35,962:INFO: Dataset: zara2               Batch:  9/18	Loss 12.8685 (12.9790)
2022-11-25 01:01:35,963:INFO: Dataset: zara2               Batch: 10/18	Loss 12.7772 (12.9582)
2022-11-25 01:01:35,965:INFO: Dataset: zara2               Batch: 11/18	Loss 12.4507 (12.9081)
2022-11-25 01:01:35,966:INFO: Dataset: zara2               Batch: 12/18	Loss 12.3034 (12.8569)
2022-11-25 01:01:35,967:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4786 (12.8270)
2022-11-25 01:01:35,968:INFO: Dataset: zara2               Batch: 14/18	Loss 12.2172 (12.7804)
2022-11-25 01:01:35,969:INFO: Dataset: zara2               Batch: 15/18	Loss 12.3985 (12.7583)
2022-11-25 01:01:35,970:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8181 (12.7034)
2022-11-25 01:01:35,971:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6108 (12.6399)
2022-11-25 01:01:35,973:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5716 (12.5890)
2022-11-25 01:01:36,021:INFO: - Computing ADE (validation o)
2022-11-25 01:01:36,293:INFO: 		 ADE on eth                       dataset:	 1.0975264310836792
2022-11-25 01:01:36,293:INFO: Average validation o:	ADE  1.0975	FDE  2.2310
2022-11-25 01:01:36,294:INFO: - Computing loss (validation)
2022-11-25 01:01:36,488:INFO: Dataset: hotel               Batch: 1/2	Loss 33.2008 (33.2008)
2022-11-25 01:01:36,490:INFO: Dataset: hotel               Batch: 2/2	Loss 30.8700 (33.0337)
2022-11-25 01:01:36,772:INFO: Dataset: univ                Batch: 1/3	Loss 10.6603 (10.6603)
2022-11-25 01:01:36,773:INFO: Dataset: univ                Batch: 2/3	Loss 10.9701 (10.8076)
2022-11-25 01:01:36,774:INFO: Dataset: univ                Batch: 3/3	Loss 10.5736 (10.7287)
2022-11-25 01:01:37,014:INFO: Dataset: zara1               Batch: 1/2	Loss 28.5339 (28.5339)
2022-11-25 01:01:37,017:INFO: Dataset: zara1               Batch: 2/2	Loss 29.1124 (28.6828)
2022-11-25 01:01:37,278:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3338 (11.3338)
2022-11-25 01:01:37,279:INFO: Dataset: zara2               Batch: 2/5	Loss 11.2342 (11.2871)
2022-11-25 01:01:37,281:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4342 (11.3352)
2022-11-25 01:01:37,282:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2861 (11.3228)
2022-11-25 01:01:37,283:INFO: Dataset: zara2               Batch: 5/5	Loss 11.5010 (11.3562)
2022-11-25 01:01:37,339:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_658.pth.tar
2022-11-25 01:01:37,339:INFO: 
===> EPOCH: 659 (P4)
2022-11-25 01:01:37,339:INFO: - Computing loss (training)
2022-11-25 01:01:37,556:INFO: Dataset: hotel               Batch: 1/4	Loss 25.8498 (25.8498)
2022-11-25 01:01:37,560:INFO: Dataset: hotel               Batch: 2/4	Loss 28.2338 (27.0108)
2022-11-25 01:01:37,562:INFO: Dataset: hotel               Batch: 3/4	Loss 26.1273 (26.7243)
2022-11-25 01:01:37,565:INFO: Dataset: hotel               Batch: 4/4	Loss 26.0834 (26.6127)
2022-11-25 01:01:37,868:INFO: Dataset: univ                Batch:  1/15	Loss 11.5459 (11.5459)
2022-11-25 01:01:37,870:INFO: Dataset: univ                Batch:  2/15	Loss 11.7706 (11.6570)
2022-11-25 01:01:37,873:INFO: Dataset: univ                Batch:  3/15	Loss 11.9494 (11.7479)
2022-11-25 01:01:37,874:INFO: Dataset: univ                Batch:  4/15	Loss 11.4979 (11.6781)
2022-11-25 01:01:37,875:INFO: Dataset: univ                Batch:  5/15	Loss 11.8874 (11.7162)
2022-11-25 01:01:37,878:INFO: Dataset: univ                Batch:  6/15	Loss 11.5967 (11.6979)
2022-11-25 01:01:37,879:INFO: Dataset: univ                Batch:  7/15	Loss 11.6185 (11.6861)
2022-11-25 01:01:37,880:INFO: Dataset: univ                Batch:  8/15	Loss 11.3501 (11.6450)
2022-11-25 01:01:37,881:INFO: Dataset: univ                Batch:  9/15	Loss 11.0939 (11.5807)
2022-11-25 01:01:37,882:INFO: Dataset: univ                Batch: 10/15	Loss 11.1119 (11.5355)
2022-11-25 01:01:37,884:INFO: Dataset: univ                Batch: 11/15	Loss 10.7545 (11.4644)
2022-11-25 01:01:37,886:INFO: Dataset: univ                Batch: 12/15	Loss 10.7119 (11.4027)
2022-11-25 01:01:37,888:INFO: Dataset: univ                Batch: 13/15	Loss 10.3279 (11.3210)
2022-11-25 01:01:37,890:INFO: Dataset: univ                Batch: 14/15	Loss 10.3110 (11.2472)
2022-11-25 01:01:37,891:INFO: Dataset: univ                Batch: 15/15	Loss 10.3718 (11.2352)
2022-11-25 01:01:38,155:INFO: Dataset: zara1               Batch: 1/8	Loss 31.3683 (31.3683)
2022-11-25 01:01:38,157:INFO: Dataset: zara1               Batch: 2/8	Loss 31.6061 (31.4922)
2022-11-25 01:01:38,158:INFO: Dataset: zara1               Batch: 3/8	Loss 31.1248 (31.3742)
2022-11-25 01:01:38,160:INFO: Dataset: zara1               Batch: 4/8	Loss 30.9407 (31.2590)
2022-11-25 01:01:38,161:INFO: Dataset: zara1               Batch: 5/8	Loss 30.6851 (31.1437)
2022-11-25 01:01:38,220:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6930 (31.0702)
2022-11-25 01:01:38,223:INFO: Dataset: zara1               Batch: 7/8	Loss 30.5631 (31.0006)
2022-11-25 01:01:38,227:INFO: Dataset: zara1               Batch: 8/8	Loss 29.4714 (30.8147)
2022-11-25 01:01:38,493:INFO: Dataset: zara2               Batch:  1/18	Loss 13.1678 (13.1678)
2022-11-25 01:01:38,495:INFO: Dataset: zara2               Batch:  2/18	Loss 12.8807 (13.0343)
2022-11-25 01:01:38,497:INFO: Dataset: zara2               Batch:  3/18	Loss 12.9629 (13.0080)
2022-11-25 01:01:38,551:INFO: Dataset: zara2               Batch:  4/18	Loss 12.7126 (12.9364)
2022-11-25 01:01:38,552:INFO: Dataset: zara2               Batch:  5/18	Loss 12.9474 (12.9385)
2022-11-25 01:01:38,556:INFO: Dataset: zara2               Batch:  6/18	Loss 13.0715 (12.9620)
2022-11-25 01:01:38,557:INFO: Dataset: zara2               Batch:  7/18	Loss 13.3406 (13.0178)
2022-11-25 01:01:38,558:INFO: Dataset: zara2               Batch:  8/18	Loss 12.8120 (12.9913)
2022-11-25 01:01:38,559:INFO: Dataset: zara2               Batch:  9/18	Loss 12.8524 (12.9756)
2022-11-25 01:01:38,560:INFO: Dataset: zara2               Batch: 10/18	Loss 12.5615 (12.9394)
2022-11-25 01:01:38,561:INFO: Dataset: zara2               Batch: 11/18	Loss 12.4748 (12.8971)
2022-11-25 01:01:38,562:INFO: Dataset: zara2               Batch: 12/18	Loss 12.6225 (12.8739)
2022-11-25 01:01:38,563:INFO: Dataset: zara2               Batch: 13/18	Loss 12.5087 (12.8495)
2022-11-25 01:01:38,564:INFO: Dataset: zara2               Batch: 14/18	Loss 12.3671 (12.8164)
2022-11-25 01:01:38,565:INFO: Dataset: zara2               Batch: 15/18	Loss 11.9508 (12.7593)
2022-11-25 01:01:38,567:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7306 (12.6979)
2022-11-25 01:01:38,568:INFO: Dataset: zara2               Batch: 17/18	Loss 11.5465 (12.6275)
2022-11-25 01:01:38,570:INFO: Dataset: zara2               Batch: 18/18	Loss 11.3088 (12.5603)
2022-11-25 01:01:38,616:INFO: - Computing ADE (validation o)
2022-11-25 01:01:38,880:INFO: 		 ADE on eth                       dataset:	 1.0571982860565186
2022-11-25 01:01:38,880:INFO: Average validation o:	ADE  1.0572	FDE  2.1453
2022-11-25 01:01:38,881:INFO: - Computing loss (validation)
2022-11-25 01:01:39,072:INFO: Dataset: hotel               Batch: 1/2	Loss 32.8739 (32.8739)
2022-11-25 01:01:39,073:INFO: Dataset: hotel               Batch: 2/2	Loss 33.0044 (32.8842)
2022-11-25 01:01:39,327:INFO: Dataset: univ                Batch: 1/3	Loss 10.2444 (10.2444)
2022-11-25 01:01:39,327:INFO: Dataset: univ                Batch: 2/3	Loss 10.9369 (10.5598)
2022-11-25 01:01:39,328:INFO: Dataset: univ                Batch: 3/3	Loss 10.7489 (10.6169)
2022-11-25 01:01:39,605:INFO: Dataset: zara1               Batch: 1/2	Loss 28.5659 (28.5659)
2022-11-25 01:01:39,609:INFO: Dataset: zara1               Batch: 2/2	Loss 29.2276 (28.7298)
2022-11-25 01:01:39,864:INFO: Dataset: zara2               Batch: 1/5	Loss 11.2811 (11.2811)
2022-11-25 01:01:39,866:INFO: Dataset: zara2               Batch: 2/5	Loss 11.4509 (11.3681)
2022-11-25 01:01:39,874:INFO: Dataset: zara2               Batch: 3/5	Loss 11.5806 (11.4401)
2022-11-25 01:01:39,874:INFO: Dataset: zara2               Batch: 4/5	Loss 11.1763 (11.3719)
2022-11-25 01:01:39,875:INFO: Dataset: zara2               Batch: 5/5	Loss 11.4290 (11.3836)
2022-11-25 01:01:39,932:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_659.pth.tar
2022-11-25 01:01:39,932:INFO: 
===> EPOCH: 660 (P4)
2022-11-25 01:01:39,933:INFO: - Computing loss (training)
2022-11-25 01:01:40,138:INFO: Dataset: hotel               Batch: 1/4	Loss 24.9190 (24.9190)
2022-11-25 01:01:40,141:INFO: Dataset: hotel               Batch: 2/4	Loss 26.3494 (25.6462)
2022-11-25 01:01:40,150:INFO: Dataset: hotel               Batch: 3/4	Loss 26.2034 (25.8317)
2022-11-25 01:01:40,151:INFO: Dataset: hotel               Batch: 4/4	Loss 29.1060 (26.4191)
2022-11-25 01:01:40,417:INFO: Dataset: univ                Batch:  1/15	Loss 11.3348 (11.3348)
2022-11-25 01:01:40,419:INFO: Dataset: univ                Batch:  2/15	Loss 11.4337 (11.3829)
2022-11-25 01:01:40,422:INFO: Dataset: univ                Batch:  3/15	Loss 11.7203 (11.4867)
2022-11-25 01:01:40,426:INFO: Dataset: univ                Batch:  4/15	Loss 11.8529 (11.5698)
2022-11-25 01:01:40,469:INFO: Dataset: univ                Batch:  5/15	Loss 11.6468 (11.5850)
2022-11-25 01:01:40,472:INFO: Dataset: univ                Batch:  6/15	Loss 11.3652 (11.5489)
2022-11-25 01:01:40,474:INFO: Dataset: univ                Batch:  7/15	Loss 11.3714 (11.5239)
2022-11-25 01:01:40,475:INFO: Dataset: univ                Batch:  8/15	Loss 11.2059 (11.4855)
2022-11-25 01:01:40,476:INFO: Dataset: univ                Batch:  9/15	Loss 11.2186 (11.4570)
2022-11-25 01:01:40,477:INFO: Dataset: univ                Batch: 10/15	Loss 10.9344 (11.4048)
2022-11-25 01:01:40,479:INFO: Dataset: univ                Batch: 11/15	Loss 11.3649 (11.4016)
2022-11-25 01:01:40,480:INFO: Dataset: univ                Batch: 12/15	Loss 10.2571 (11.2946)
2022-11-25 01:01:40,482:INFO: Dataset: univ                Batch: 13/15	Loss 10.4991 (11.2279)
2022-11-25 01:01:40,483:INFO: Dataset: univ                Batch: 14/15	Loss 10.1820 (11.1512)
2022-11-25 01:01:40,484:INFO: Dataset: univ                Batch: 15/15	Loss 9.9025 (11.1305)
2022-11-25 01:01:40,763:INFO: Dataset: zara1               Batch: 1/8	Loss 31.3439 (31.3439)
2022-11-25 01:01:40,765:INFO: Dataset: zara1               Batch: 2/8	Loss 30.9876 (31.1634)
2022-11-25 01:01:40,767:INFO: Dataset: zara1               Batch: 3/8	Loss 30.8444 (31.0523)
2022-11-25 01:01:40,770:INFO: Dataset: zara1               Batch: 4/8	Loss 30.9562 (31.0256)
2022-11-25 01:01:40,771:INFO: Dataset: zara1               Batch: 5/8	Loss 30.8123 (30.9829)
2022-11-25 01:01:40,801:INFO: Dataset: zara1               Batch: 6/8	Loss 30.6371 (30.9185)
2022-11-25 01:01:40,802:INFO: Dataset: zara1               Batch: 7/8	Loss 30.4402 (30.8533)
2022-11-25 01:01:40,803:INFO: Dataset: zara1               Batch: 8/8	Loss 29.9828 (30.7520)
2022-11-25 01:01:41,061:INFO: Dataset: zara2               Batch:  1/18	Loss 12.9524 (12.9524)
2022-11-25 01:01:41,065:INFO: Dataset: zara2               Batch:  2/18	Loss 13.2790 (13.1212)
2022-11-25 01:01:41,120:INFO: Dataset: zara2               Batch:  3/18	Loss 13.3414 (13.1937)
2022-11-25 01:01:41,121:INFO: Dataset: zara2               Batch:  4/18	Loss 13.1958 (13.1943)
2022-11-25 01:01:41,123:INFO: Dataset: zara2               Batch:  5/18	Loss 13.1342 (13.1829)
2022-11-25 01:01:41,125:INFO: Dataset: zara2               Batch:  6/18	Loss 13.1750 (13.1816)
2022-11-25 01:01:41,126:INFO: Dataset: zara2               Batch:  7/18	Loss 13.1745 (13.1806)
2022-11-25 01:01:41,127:INFO: Dataset: zara2               Batch:  8/18	Loss 12.6110 (13.1073)
2022-11-25 01:01:41,128:INFO: Dataset: zara2               Batch:  9/18	Loss 12.6410 (13.0536)
2022-11-25 01:01:41,129:INFO: Dataset: zara2               Batch: 10/18	Loss 12.7266 (13.0199)
2022-11-25 01:01:41,131:INFO: Dataset: zara2               Batch: 11/18	Loss 12.5095 (12.9695)
2022-11-25 01:01:41,131:INFO: Dataset: zara2               Batch: 12/18	Loss 12.4805 (12.9283)
2022-11-25 01:01:41,132:INFO: Dataset: zara2               Batch: 13/18	Loss 12.1104 (12.8549)
2022-11-25 01:01:41,133:INFO: Dataset: zara2               Batch: 14/18	Loss 11.8924 (12.7852)
2022-11-25 01:01:41,135:INFO: Dataset: zara2               Batch: 15/18	Loss 12.0433 (12.7372)
2022-11-25 01:01:41,136:INFO: Dataset: zara2               Batch: 16/18	Loss 11.7077 (12.6622)
2022-11-25 01:01:41,138:INFO: Dataset: zara2               Batch: 17/18	Loss 11.6417 (12.6011)
2022-11-25 01:01:41,139:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6650 (12.5564)
2022-11-25 01:01:41,189:INFO: - Computing ADE (validation o)
2022-11-25 01:01:41,477:INFO: 		 ADE on eth                       dataset:	 1.0473742485046387
2022-11-25 01:01:41,477:INFO: Average validation o:	ADE  1.0474	FDE  2.1300
2022-11-25 01:01:41,478:INFO: - Computing loss (validation)
2022-11-25 01:01:41,676:INFO: Dataset: hotel               Batch: 1/2	Loss 33.3461 (33.3461)
2022-11-25 01:01:41,676:INFO: Dataset: hotel               Batch: 2/2	Loss 28.5548 (32.9700)
2022-11-25 01:01:41,939:INFO: Dataset: univ                Batch: 1/3	Loss 10.5606 (10.5606)
2022-11-25 01:01:41,941:INFO: Dataset: univ                Batch: 2/3	Loss 10.8197 (10.6837)
2022-11-25 01:01:41,942:INFO: Dataset: univ                Batch: 3/3	Loss 10.5083 (10.6221)
2022-11-25 01:01:42,192:INFO: Dataset: zara1               Batch: 1/2	Loss 28.8562 (28.8562)
2022-11-25 01:01:42,195:INFO: Dataset: zara1               Batch: 2/2	Loss 28.1404 (28.6629)
2022-11-25 01:01:42,456:INFO: Dataset: zara2               Batch: 1/5	Loss 11.2268 (11.2268)
2022-11-25 01:01:42,458:INFO: Dataset: zara2               Batch: 2/5	Loss 11.1776 (11.2026)
2022-11-25 01:01:42,468:INFO: Dataset: zara2               Batch: 3/5	Loss 11.6587 (11.3522)
2022-11-25 01:01:42,469:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2438 (11.3259)
2022-11-25 01:01:42,469:INFO: Dataset: zara2               Batch: 5/5	Loss 11.3732 (11.3351)
2022-11-25 01:01:42,523:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_660.pth.tar
2022-11-25 01:01:42,524:INFO: 
===> EPOCH: 661 (P4)
2022-11-25 01:01:42,524:INFO: - Computing loss (training)
2022-11-25 01:01:42,733:INFO: Dataset: hotel               Batch: 1/4	Loss 28.8357 (28.8357)
2022-11-25 01:01:42,736:INFO: Dataset: hotel               Batch: 2/4	Loss 21.9941 (25.4877)
2022-11-25 01:01:42,737:INFO: Dataset: hotel               Batch: 3/4	Loss 28.0443 (26.3304)
2022-11-25 01:01:42,740:INFO: Dataset: hotel               Batch: 4/4	Loss 27.4682 (26.5211)
2022-11-25 01:01:43,004:INFO: Dataset: univ                Batch:  1/15	Loss 11.2344 (11.2344)
2022-11-25 01:01:43,005:INFO: Dataset: univ                Batch:  2/15	Loss 11.2095 (11.2218)
2022-11-25 01:01:43,011:INFO: Dataset: univ                Batch:  3/15	Loss 11.7609 (11.3882)
2022-11-25 01:01:43,017:INFO: Dataset: univ                Batch:  4/15	Loss 12.1323 (11.5618)
2022-11-25 01:01:43,051:INFO: Dataset: univ                Batch:  5/15	Loss 11.3545 (11.5195)
2022-11-25 01:01:43,057:INFO: Dataset: univ                Batch:  6/15	Loss 11.5160 (11.5189)
2022-11-25 01:01:43,058:INFO: Dataset: univ                Batch:  7/15	Loss 11.8197 (11.5554)
2022-11-25 01:01:43,059:INFO: Dataset: univ                Batch:  8/15	Loss 11.0432 (11.4872)
2022-11-25 01:01:43,060:INFO: Dataset: univ                Batch:  9/15	Loss 11.1394 (11.4492)
2022-11-25 01:01:43,061:INFO: Dataset: univ                Batch: 10/15	Loss 11.0124 (11.4026)
2022-11-25 01:01:43,062:INFO: Dataset: univ                Batch: 11/15	Loss 10.7337 (11.3387)
2022-11-25 01:01:43,064:INFO: Dataset: univ                Batch: 12/15	Loss 10.7166 (11.2822)
2022-11-25 01:01:43,065:INFO: Dataset: univ                Batch: 13/15	Loss 10.7569 (11.2427)
2022-11-25 01:01:43,067:INFO: Dataset: univ                Batch: 14/15	Loss 10.1261 (11.1571)
2022-11-25 01:01:43,068:INFO: Dataset: univ                Batch: 15/15	Loss 10.9632 (11.1542)
2022-11-25 01:01:43,334:INFO: Dataset: zara1               Batch: 1/8	Loss 31.3170 (31.3170)
2022-11-25 01:01:43,344:INFO: Dataset: zara1               Batch: 2/8	Loss 30.9410 (31.1282)
2022-11-25 01:01:43,347:INFO: Dataset: zara1               Batch: 3/8	Loss 30.9122 (31.0590)
2022-11-25 01:01:43,349:INFO: Dataset: zara1               Batch: 4/8	Loss 31.1832 (31.0887)
2022-11-25 01:01:43,350:INFO: Dataset: zara1               Batch: 5/8	Loss 31.1214 (31.0958)
2022-11-25 01:01:43,381:INFO: Dataset: zara1               Batch: 6/8	Loss 30.4911 (30.9946)
2022-11-25 01:01:43,382:INFO: Dataset: zara1               Batch: 7/8	Loss 30.0461 (30.8491)
2022-11-25 01:01:43,383:INFO: Dataset: zara1               Batch: 8/8	Loss 29.4731 (30.7152)
2022-11-25 01:01:43,657:INFO: Dataset: zara2               Batch:  1/18	Loss 12.6318 (12.6318)
2022-11-25 01:01:43,658:INFO: Dataset: zara2               Batch:  2/18	Loss 12.6024 (12.6172)
2022-11-25 01:01:43,677:INFO: Dataset: zara2               Batch:  3/18	Loss 13.0597 (12.7546)
2022-11-25 01:01:43,679:INFO: Dataset: zara2               Batch:  4/18	Loss 12.9674 (12.8061)
2022-11-25 01:01:43,680:INFO: Dataset: zara2               Batch:  5/18	Loss 13.0358 (12.8477)
2022-11-25 01:01:43,693:INFO: Dataset: zara2               Batch:  6/18	Loss 13.1122 (12.8955)
2022-11-25 01:01:43,694:INFO: Dataset: zara2               Batch:  7/18	Loss 12.8846 (12.8940)
2022-11-25 01:01:43,695:INFO: Dataset: zara2               Batch:  8/18	Loss 12.8729 (12.8912)
2022-11-25 01:01:43,696:INFO: Dataset: zara2               Batch:  9/18	Loss 13.2381 (12.9323)
2022-11-25 01:01:43,697:INFO: Dataset: zara2               Batch: 10/18	Loss 12.6264 (12.9026)
2022-11-25 01:01:43,698:INFO: Dataset: zara2               Batch: 11/18	Loss 12.0515 (12.8165)
2022-11-25 01:01:43,699:INFO: Dataset: zara2               Batch: 12/18	Loss 12.1011 (12.7576)
2022-11-25 01:01:43,700:INFO: Dataset: zara2               Batch: 13/18	Loss 12.2996 (12.7216)
2022-11-25 01:01:43,701:INFO: Dataset: zara2               Batch: 14/18	Loss 12.5127 (12.7072)
2022-11-25 01:01:43,702:INFO: Dataset: zara2               Batch: 15/18	Loss 11.8729 (12.6558)
2022-11-25 01:01:43,703:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8695 (12.6035)
2022-11-25 01:01:43,704:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7316 (12.5548)
2022-11-25 01:01:43,705:INFO: Dataset: zara2               Batch: 18/18	Loss 11.5968 (12.5083)
2022-11-25 01:01:43,754:INFO: - Computing ADE (validation o)
2022-11-25 01:01:44,027:INFO: 		 ADE on eth                       dataset:	 1.0489819049835205
2022-11-25 01:01:44,028:INFO: Average validation o:	ADE  1.0490	FDE  2.1409
2022-11-25 01:01:44,028:INFO: - Computing loss (validation)
2022-11-25 01:01:44,238:INFO: Dataset: hotel               Batch: 1/2	Loss 33.0373 (33.0373)
2022-11-25 01:01:44,239:INFO: Dataset: hotel               Batch: 2/2	Loss 30.3392 (32.8807)
2022-11-25 01:01:44,495:INFO: Dataset: univ                Batch: 1/3	Loss 10.7961 (10.7961)
2022-11-25 01:01:44,498:INFO: Dataset: univ                Batch: 2/3	Loss 10.3931 (10.5836)
2022-11-25 01:01:44,501:INFO: Dataset: univ                Batch: 3/3	Loss 10.4047 (10.5264)
2022-11-25 01:01:44,767:INFO: Dataset: zara1               Batch: 1/2	Loss 28.6770 (28.6770)
2022-11-25 01:01:44,770:INFO: Dataset: zara1               Batch: 2/2	Loss 28.6319 (28.6657)
2022-11-25 01:01:45,025:INFO: Dataset: zara2               Batch: 1/5	Loss 11.3746 (11.3746)
2022-11-25 01:01:45,027:INFO: Dataset: zara2               Batch: 2/5	Loss 11.3656 (11.3699)
2022-11-25 01:01:45,029:INFO: Dataset: zara2               Batch: 3/5	Loss 11.4986 (11.4137)
2022-11-25 01:01:45,030:INFO: Dataset: zara2               Batch: 4/5	Loss 11.2639 (11.3750)
2022-11-25 01:01:45,045:INFO: Dataset: zara2               Batch: 5/5	Loss 11.2858 (11.3579)
2022-11-25 01:01:45,103:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_661.pth.tar
2022-11-25 01:01:45,103:INFO: 
===> EPOCH: 662 (P4)
2022-11-25 01:01:45,104:INFO: - Computing loss (training)
2022-11-25 01:01:45,312:INFO: Dataset: hotel               Batch: 1/4	Loss 24.3472 (24.3472)
2022-11-25 01:01:45,313:INFO: Dataset: hotel               Batch: 2/4	Loss 26.7288 (25.4918)
2022-11-25 01:01:45,314:INFO: Dataset: hotel               Batch: 3/4	Loss 27.1607 (26.0693)
2022-11-25 01:01:45,316:INFO: Dataset: hotel               Batch: 4/4	Loss 27.7609 (26.3549)
2022-11-25 01:01:45,577:INFO: Dataset: univ                Batch:  1/15	Loss 11.4983 (11.4983)
2022-11-25 01:01:45,582:INFO: Dataset: univ                Batch:  2/15	Loss 11.6786 (11.5826)
2022-11-25 01:01:45,644:INFO: Dataset: univ                Batch:  3/15	Loss 11.6516 (11.6054)
2022-11-25 01:01:45,647:INFO: Dataset: univ                Batch:  4/15	Loss 11.4647 (11.5714)
2022-11-25 01:01:45,648:INFO: Dataset: univ                Batch:  5/15	Loss 11.3852 (11.5332)
2022-11-25 01:01:45,651:INFO: Dataset: univ                Batch:  6/15	Loss 11.5001 (11.5278)
2022-11-25 01:01:45,652:INFO: Dataset: univ                Batch:  7/15	Loss 11.6151 (11.5402)
2022-11-25 01:01:45,653:INFO: Dataset: univ                Batch:  8/15	Loss 11.3399 (11.5153)
2022-11-25 01:01:45,654:INFO: Dataset: univ                Batch:  9/15	Loss 10.9221 (11.4504)
2022-11-25 01:01:45,655:INFO: Dataset: univ                Batch: 10/15	Loss 10.6808 (11.3643)
2022-11-25 01:01:45,658:INFO: Dataset: univ                Batch: 11/15	Loss 10.7576 (11.3129)
2022-11-25 01:01:45,659:INFO: Dataset: univ                Batch: 12/15	Loss 10.6772 (11.2629)
2022-11-25 01:01:45,660:INFO: Dataset: univ                Batch: 13/15	Loss 10.4130 (11.2043)
2022-11-25 01:01:45,661:INFO: Dataset: univ                Batch: 14/15	Loss 10.2131 (11.1375)
2022-11-25 01:01:45,663:INFO: Dataset: univ                Batch: 15/15	Loss 9.9247 (11.1199)
2022-11-25 01:01:45,937:INFO: Dataset: zara1               Batch: 1/8	Loss 31.4353 (31.4353)
2022-11-25 01:01:45,938:INFO: Dataset: zara1               Batch: 2/8	Loss 31.0194 (31.2224)
2022-11-25 01:01:45,942:INFO: Dataset: zara1               Batch: 3/8	Loss 31.4077 (31.2833)
2022-11-25 01:01:45,943:INFO: Dataset: zara1               Batch: 4/8	Loss 30.3839 (31.0656)
2022-11-25 01:01:45,944:INFO: Dataset: zara1               Batch: 5/8	Loss 30.5007 (30.9592)
2022-11-25 01:01:45,977:INFO: Dataset: zara1               Batch: 6/8	Loss 30.5561 (30.8937)
2022-11-25 01:01:45,978:INFO: Dataset: zara1               Batch: 7/8	Loss 30.4197 (30.8310)
2022-11-25 01:01:45,979:INFO: Dataset: zara1               Batch: 8/8	Loss 29.4645 (30.6678)
2022-11-25 01:01:46,272:INFO: Dataset: zara2               Batch:  1/18	Loss 12.9866 (12.9866)
2022-11-25 01:01:46,275:INFO: Dataset: zara2               Batch:  2/18	Loss 12.8572 (12.9226)
2022-11-25 01:01:46,314:INFO: Dataset: zara2               Batch:  3/18	Loss 12.8904 (12.9130)
2022-11-25 01:01:46,315:INFO: Dataset: zara2               Batch:  4/18	Loss 12.9790 (12.9315)
2022-11-25 01:01:46,316:INFO: Dataset: zara2               Batch:  5/18	Loss 12.8117 (12.9065)
2022-11-25 01:01:46,320:INFO: Dataset: zara2               Batch:  6/18	Loss 12.4822 (12.8434)
2022-11-25 01:01:46,321:INFO: Dataset: zara2               Batch:  7/18	Loss 13.1716 (12.8912)
2022-11-25 01:01:46,322:INFO: Dataset: zara2               Batch:  8/18	Loss 12.9144 (12.8944)
2022-11-25 01:01:46,323:INFO: Dataset: zara2               Batch:  9/18	Loss 12.3148 (12.8387)
2022-11-25 01:01:46,324:INFO: Dataset: zara2               Batch: 10/18	Loss 12.9455 (12.8502)
2022-11-25 01:01:46,326:INFO: Dataset: zara2               Batch: 11/18	Loss 12.4628 (12.8161)
2022-11-25 01:01:46,327:INFO: Dataset: zara2               Batch: 12/18	Loss 12.2000 (12.7667)
2022-11-25 01:01:46,328:INFO: Dataset: zara2               Batch: 13/18	Loss 12.4912 (12.7452)
2022-11-25 01:01:46,328:INFO: Dataset: zara2               Batch: 14/18	Loss 12.0566 (12.6953)
2022-11-25 01:01:46,330:INFO: Dataset: zara2               Batch: 15/18	Loss 11.8533 (12.6459)
2022-11-25 01:01:46,332:INFO: Dataset: zara2               Batch: 16/18	Loss 11.8069 (12.5922)
2022-11-25 01:01:46,333:INFO: Dataset: zara2               Batch: 17/18	Loss 11.7940 (12.5444)
2022-11-25 01:01:46,335:INFO: Dataset: zara2               Batch: 18/18	Loss 11.6451 (12.5002)
2022-11-25 01:01:46,382:INFO: - Computing ADE (validation o)
2022-11-25 01:01:46,647:INFO: 		 ADE on eth                       dataset:	 1.088891863822937
2022-11-25 01:01:46,647:INFO: Average validation o:	ADE  1.0889	FDE  2.1629
2022-11-25 01:01:46,648:INFO: - Computing loss (validation)
2022-11-25 01:01:46,847:INFO: Dataset: hotel               Batch: 1/2	Loss 32.9912 (32.9912)
2022-11-25 01:01:46,849:INFO: Dataset: hotel               Batch: 2/2	Loss 31.8750 (32.8884)
2022-11-25 01:01:47,137:INFO: Dataset: univ                Batch: 1/3	Loss 10.4994 (10.4994)
2022-11-25 01:01:47,139:INFO: Dataset: univ                Batch: 2/3	Loss 10.5543 (10.5276)
2022-11-25 01:01:47,140:INFO: Dataset: univ                Batch: 3/3	Loss 10.6420 (10.5616)
2022-11-25 01:01:47,389:INFO: Dataset: zara1               Batch: 1/2	Loss 28.7733 (28.7733)
2022-11-25 01:01:47,396:INFO: Dataset: zara1               Batch: 2/2	Loss 28.1408 (28.6086)

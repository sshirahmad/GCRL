2022-10-26 23:13:50,750:INFO: Initializing Training Set
2022-10-26 23:13:54,277:INFO: Initializing Validation Set
2022-10-26 23:13:56,334:INFO: 
===> EPOCH: 2 (P2)
2022-10-26 23:13:56,335:INFO: - Computing loss (training)
2022-10-26 23:14:03,379:INFO: Dataset: hotel               Batch: 1/8	Loss 8365.5059 (8365.5059)
2022-10-26 23:14:05,001:INFO: Dataset: hotel               Batch: 2/8	Loss 24026.0586 (16344.9303)
2022-10-26 23:14:06,542:INFO: Dataset: hotel               Batch: 3/8	Loss 65167.9805 (31552.1099)
2022-10-26 23:14:08,145:INFO: Dataset: hotel               Batch: 4/8	Loss -4600.3105 (21969.5406)
2022-10-26 23:14:09,717:INFO: Dataset: hotel               Batch: 5/8	Loss 9578.8252 (19172.3828)
2022-10-26 23:14:11,362:INFO: Dataset: hotel               Batch: 6/8	Loss 10509.5723 (17798.8446)
2022-10-26 23:14:12,936:INFO: Dataset: hotel               Batch: 7/8	Loss 39999.1641 (20811.0996)
2022-10-26 23:14:13,906:INFO: Dataset: hotel               Batch: 8/8	Loss 1821.9677 (20285.0155)
2022-10-26 23:14:35,334:INFO: Dataset: univ                Batch:  1/29	Loss 15387.6465 (15387.6465)
2022-10-26 23:14:36,924:INFO: Dataset: univ                Batch:  2/29	Loss 35399.1562 (25557.2064)
2022-10-26 23:14:38,503:INFO: Dataset: univ                Batch:  3/29	Loss 7589.2188 (19672.2783)
2022-10-26 23:14:40,130:INFO: Dataset: univ                Batch:  4/29	Loss 11882.9668 (17465.0507)
2022-10-26 23:14:41,729:INFO: Dataset: univ                Batch:  5/29	Loss 62433.6484 (26576.6433)
2022-10-26 23:14:43,315:INFO: Dataset: univ                Batch:  6/29	Loss 20153.2988 (25543.7650)
2022-10-26 23:14:44,911:INFO: Dataset: univ                Batch:  7/29	Loss 27972.4023 (25883.3607)
2022-10-26 23:14:46,528:INFO: Dataset: univ                Batch:  8/29	Loss 21081.1270 (25304.7590)
2022-10-26 23:14:48,108:INFO: Dataset: univ                Batch:  9/29	Loss 20429.5801 (24858.1440)
2022-10-26 23:14:49,785:INFO: Dataset: univ                Batch: 10/29	Loss 119281.4922 (34863.2670)
2022-10-26 23:14:51,349:INFO: Dataset: univ                Batch: 11/29	Loss 7110.8042 (32386.5886)
2022-10-26 23:14:52,957:INFO: Dataset: univ                Batch: 12/29	Loss 4717.2817 (29951.3914)
2022-10-26 23:14:54,562:INFO: Dataset: univ                Batch: 13/29	Loss 5284.9727 (27775.0912)
2022-10-26 23:14:56,173:INFO: Dataset: univ                Batch: 14/29	Loss 36854.7500 (28402.3187)
2022-10-26 23:14:57,773:INFO: Dataset: univ                Batch: 15/29	Loss 6762.8218 (27100.1120)
2022-10-26 23:14:59,379:INFO: Dataset: univ                Batch: 16/29	Loss 6791.1758 (25813.9816)
2022-10-26 23:15:00,947:INFO: Dataset: univ                Batch: 17/29	Loss 6618.8677 (24716.4674)
2022-10-26 23:15:02,563:INFO: Dataset: univ                Batch: 18/29	Loss 4472.4121 (23424.5168)
2022-10-26 23:15:04,148:INFO: Dataset: univ                Batch: 19/29	Loss 5007.2153 (22667.4802)
2022-10-26 23:15:05,765:INFO: Dataset: univ                Batch: 20/29	Loss 3241.7488 (21605.0032)
2022-10-26 23:15:07,327:INFO: Dataset: univ                Batch: 21/29	Loss 13939.6738 (21274.7029)
2022-10-26 23:15:08,917:INFO: Dataset: univ                Batch: 22/29	Loss 14868.3193 (20997.6020)
2022-10-26 23:15:10,554:INFO: Dataset: univ                Batch: 23/29	Loss 4441.3604 (20295.2986)
2022-10-26 23:15:12,150:INFO: Dataset: univ                Batch: 24/29	Loss 9310.1992 (19879.8983)
2022-10-26 23:15:13,794:INFO: Dataset: univ                Batch: 25/29	Loss 3550.4033 (19192.7508)
2022-10-26 23:15:15,405:INFO: Dataset: univ                Batch: 26/29	Loss 4094.0974 (18568.7422)
2022-10-26 23:15:17,135:INFO: Dataset: univ                Batch: 27/29	Loss 8332.9219 (18184.1371)
2022-10-26 23:15:18,954:INFO: Dataset: univ                Batch: 28/29	Loss 14475.7861 (18039.3618)
2022-10-26 23:15:20,122:INFO: Dataset: univ                Batch: 29/29	Loss 458.1570 (17786.9163)
2022-10-26 23:15:28,473:INFO: Dataset: zara1               Batch:  1/16	Loss 15453.0498 (15453.0498)
2022-10-26 23:15:30,359:INFO: Dataset: zara1               Batch:  2/16	Loss 1980.9767 (9080.3679)
2022-10-26 23:15:32,948:INFO: Dataset: zara1               Batch:  3/16	Loss 5570.9883 (7826.3496)
2022-10-26 23:15:34,694:INFO: Dataset: zara1               Batch:  4/16	Loss 34379.8008 (14739.6742)
2022-10-26 23:15:36,504:INFO: Dataset: zara1               Batch:  5/16	Loss 19396.0215 (15654.7091)
2022-10-26 23:15:38,249:INFO: Dataset: zara1               Batch:  6/16	Loss 9878.0361 (14712.3606)
2022-10-26 23:15:40,107:INFO: Dataset: zara1               Batch:  7/16	Loss 10473.4463 (14172.6840)
2022-10-26 23:15:41,761:INFO: Dataset: zara1               Batch:  8/16	Loss 9065.4766 (13619.2715)
2022-10-26 23:15:43,307:INFO: Dataset: zara1               Batch:  9/16	Loss 2680.6179 (12495.0210)
2022-10-26 23:15:44,902:INFO: Dataset: zara1               Batch: 10/16	Loss 4737.9644 (11696.1214)
2022-10-26 23:15:46,530:INFO: Dataset: zara1               Batch: 11/16	Loss 12650.3799 (11778.6597)
2022-10-26 23:15:48,151:INFO: Dataset: zara1               Batch: 12/16	Loss 2487.5955 (10985.5987)
2022-10-26 23:15:49,729:INFO: Dataset: zara1               Batch: 13/16	Loss 131711.2812 (19692.1515)
2022-10-26 23:15:51,301:INFO: Dataset: zara1               Batch: 14/16	Loss 2149.5549 (18453.3565)
2022-10-26 23:15:52,905:INFO: Dataset: zara1               Batch: 15/16	Loss 23073.9805 (18798.7520)
2022-10-26 23:15:54,301:INFO: Dataset: zara1               Batch: 16/16	Loss 611.6960 (17898.9713)
2022-10-26 23:16:14,337:INFO: Dataset: zara2               Batch:  1/36	Loss 1877.7699 (1877.7699)
2022-10-26 23:16:15,914:INFO: Dataset: zara2               Batch:  2/36	Loss 4049.2673 (2845.6373)
2022-10-26 23:16:17,489:INFO: Dataset: zara2               Batch:  3/36	Loss 12628.6270 (5914.8105)
2022-10-26 23:16:19,077:INFO: Dataset: zara2               Batch:  4/36	Loss 8501.2080 (6502.6281)
2022-10-26 23:16:20,646:INFO: Dataset: zara2               Batch:  5/36	Loss 1391.3315 (5470.4800)
2022-10-26 23:16:22,232:INFO: Dataset: zara2               Batch:  6/36	Loss 2985.5618 (5048.8423)
2022-10-26 23:16:23,792:INFO: Dataset: zara2               Batch:  7/36	Loss 2537.7717 (4722.0741)
2022-10-26 23:16:25,362:INFO: Dataset: zara2               Batch:  8/36	Loss 3554.4038 (4564.2093)
2022-10-26 23:16:26,895:INFO: Dataset: zara2               Batch:  9/36	Loss 2384.3992 (4359.8055)
2022-10-26 23:16:28,464:INFO: Dataset: zara2               Batch: 10/36	Loss 4091.8811 (4333.2113)
2022-10-26 23:16:30,023:INFO: Dataset: zara2               Batch: 11/36	Loss 5051.2466 (4404.9351)
2022-10-26 23:16:31,608:INFO: Dataset: zara2               Batch: 12/36	Loss 16190.8398 (5480.7035)
2022-10-26 23:16:33,192:INFO: Dataset: zara2               Batch: 13/36	Loss 1395.0826 (5128.6247)
2022-10-26 23:16:34,806:INFO: Dataset: zara2               Batch: 14/36	Loss 35188.8789 (7501.8026)
2022-10-26 23:16:36,410:INFO: Dataset: zara2               Batch: 15/36	Loss 3853.6594 (7275.2460)
2022-10-26 23:16:37,989:INFO: Dataset: zara2               Batch: 16/36	Loss 3102.2241 (7015.1136)
2022-10-26 23:16:39,562:INFO: Dataset: zara2               Batch: 17/36	Loss 2415.2874 (6763.5352)
2022-10-26 23:16:41,148:INFO: Dataset: zara2               Batch: 18/36	Loss 3574.6223 (6573.0892)
2022-10-26 23:16:42,758:INFO: Dataset: zara2               Batch: 19/36	Loss 17921.5508 (7249.3938)
2022-10-26 23:16:44,389:INFO: Dataset: zara2               Batch: 20/36	Loss 5511.7646 (7154.0830)
2022-10-26 23:16:46,008:INFO: Dataset: zara2               Batch: 21/36	Loss 1665.0668 (6892.0406)
2022-10-26 23:16:47,616:INFO: Dataset: zara2               Batch: 22/36	Loss 12265.3691 (7165.5763)
2022-10-26 23:16:49,260:INFO: Dataset: zara2               Batch: 23/36	Loss 1363.9266 (6905.6246)
2022-10-26 23:16:50,861:INFO: Dataset: zara2               Batch: 24/36	Loss 17847.4727 (7379.9667)
2022-10-26 23:16:52,430:INFO: Dataset: zara2               Batch: 25/36	Loss 1773.5490 (7173.4906)
2022-10-26 23:16:54,015:INFO: Dataset: zara2               Batch: 26/36	Loss 4231.1719 (7060.6564)
2022-10-26 23:16:55,602:INFO: Dataset: zara2               Batch: 27/36	Loss 1411.0513 (6863.8489)
2022-10-26 23:16:57,214:INFO: Dataset: zara2               Batch: 28/36	Loss 3084.5659 (6743.5555)
2022-10-26 23:16:58,802:INFO: Dataset: zara2               Batch: 29/36	Loss 4291.9663 (6658.2612)
2022-10-26 23:17:00,414:INFO: Dataset: zara2               Batch: 30/36	Loss 7000.8315 (6669.3227)
2022-10-26 23:17:02,245:INFO: Dataset: zara2               Batch: 31/36	Loss 852.8098 (6503.5499)
2022-10-26 23:17:03,815:INFO: Dataset: zara2               Batch: 32/36	Loss 5137.8188 (6456.8733)
2022-10-26 23:17:05,408:INFO: Dataset: zara2               Batch: 33/36	Loss 1415.0995 (6303.3534)
2022-10-26 23:17:07,040:INFO: Dataset: zara2               Batch: 34/36	Loss 1517.2585 (6159.5045)
2022-10-26 23:17:08,590:INFO: Dataset: zara2               Batch: 35/36	Loss 1870.8110 (6045.6325)
2022-10-26 23:17:09,997:INFO: Dataset: zara2               Batch: 36/36	Loss 158.0416 (5913.2615)
2022-10-26 23:17:10,792:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_2.pth.tar
2022-10-26 23:17:10,793:INFO: 
===> EPOCH: 3 (P2)
2022-10-26 23:17:10,793:INFO: - Computing loss (training)
2022-10-26 23:17:17,794:INFO: Dataset: hotel               Batch: 1/8	Loss 4537.8770 (4537.8770)
2022-10-26 23:17:19,377:INFO: Dataset: hotel               Batch: 2/8	Loss 273.9267 (2385.7889)
2022-10-26 23:17:20,971:INFO: Dataset: hotel               Batch: 3/8	Loss 463.5183 (1761.3570)
2022-10-26 23:17:22,577:INFO: Dataset: hotel               Batch: 4/8	Loss 1483.1978 (1695.7087)
2022-10-26 23:17:24,237:INFO: Dataset: hotel               Batch: 5/8	Loss 434.8754 (1441.0938)
2022-10-26 23:17:26,070:INFO: Dataset: hotel               Batch: 6/8	Loss 1472.7450 (1446.5386)
2022-10-26 23:17:27,994:INFO: Dataset: hotel               Batch: 7/8	Loss 309.7213 (1277.0269)
2022-10-26 23:17:29,122:INFO: Dataset: hotel               Batch: 8/8	Loss 12.2914 (1231.9769)
2022-10-26 23:17:51,707:INFO: Dataset: univ                Batch:  1/29	Loss 1221.6646 (1221.6646)
2022-10-26 23:17:53,848:INFO: Dataset: univ                Batch:  2/29	Loss 2163.3093 (1719.0802)
2022-10-26 23:17:55,810:INFO: Dataset: univ                Batch:  3/29	Loss 4417.7710 (2669.5131)
2022-10-26 23:17:57,801:INFO: Dataset: univ                Batch:  4/29	Loss 1386.6208 (2358.9990)
2022-10-26 23:17:59,860:INFO: Dataset: univ                Batch:  5/29	Loss 897.5569 (2060.0427)
2022-10-26 23:18:01,907:INFO: Dataset: univ                Batch:  6/29	Loss 630.6235 (1851.8600)
2022-10-26 23:18:03,858:INFO: Dataset: univ                Batch:  7/29	Loss 464.9979 (1680.0962)
2022-10-26 23:18:05,874:INFO: Dataset: univ                Batch:  8/29	Loss 1365.2894 (1641.3930)
2022-10-26 23:18:07,777:INFO: Dataset: univ                Batch:  9/29	Loss 743.4208 (1558.0042)
2022-10-26 23:18:09,543:INFO: Dataset: univ                Batch: 10/29	Loss 478.6341 (1457.8594)
2022-10-26 23:18:11,177:INFO: Dataset: univ                Batch: 11/29	Loss 651.2695 (1372.3949)
2022-10-26 23:18:12,862:INFO: Dataset: univ                Batch: 12/29	Loss 1109.5325 (1348.0991)
2022-10-26 23:18:14,870:INFO: Dataset: univ                Batch: 13/29	Loss 806.3995 (1302.3670)
2022-10-26 23:18:16,503:INFO: Dataset: univ                Batch: 14/29	Loss 721.9324 (1256.8321)
2022-10-26 23:18:18,101:INFO: Dataset: univ                Batch: 15/29	Loss 455.5374 (1210.5538)
2022-10-26 23:18:19,674:INFO: Dataset: univ                Batch: 16/29	Loss 1724.5165 (1235.1809)
2022-10-26 23:18:21,341:INFO: Dataset: univ                Batch: 17/29	Loss 271.7939 (1183.5302)
2022-10-26 23:18:23,698:INFO: Dataset: univ                Batch: 18/29	Loss 386.7632 (1138.2137)
2022-10-26 23:18:25,381:INFO: Dataset: univ                Batch: 19/29	Loss 615.2049 (1111.2180)
2022-10-26 23:18:27,193:INFO: Dataset: univ                Batch: 20/29	Loss 981.5652 (1105.2685)
2022-10-26 23:18:29,044:INFO: Dataset: univ                Batch: 21/29	Loss 221.3954 (1066.7059)
2022-10-26 23:18:30,852:INFO: Dataset: univ                Batch: 22/29	Loss 430.4019 (1033.3487)
2022-10-26 23:18:32,529:INFO: Dataset: univ                Batch: 23/29	Loss 855.9501 (1025.8291)
2022-10-26 23:18:34,144:INFO: Dataset: univ                Batch: 24/29	Loss -406.4877 (964.9764)
2022-10-26 23:18:35,782:INFO: Dataset: univ                Batch: 25/29	Loss 1215.2208 (975.2006)
2022-10-26 23:18:37,487:INFO: Dataset: univ                Batch: 26/29	Loss 661.4308 (962.9928)
2022-10-26 23:18:39,119:INFO: Dataset: univ                Batch: 27/29	Loss 5478.5928 (1119.7938)
2022-10-26 23:18:40,728:INFO: Dataset: univ                Batch: 28/29	Loss 1207.7740 (1122.9012)
2022-10-26 23:18:41,804:INFO: Dataset: univ                Batch: 29/29	Loss 163.6412 (1112.2185)
2022-10-26 23:18:50,756:INFO: Dataset: zara1               Batch:  1/16	Loss 151.6636 (151.6636)
2022-10-26 23:18:52,348:INFO: Dataset: zara1               Batch:  2/16	Loss 1090.7504 (659.0734)
2022-10-26 23:18:53,969:INFO: Dataset: zara1               Batch:  3/16	Loss 252.8553 (528.1069)
2022-10-26 23:18:55,597:INFO: Dataset: zara1               Batch:  4/16	Loss 995.7172 (647.1523)
2022-10-26 23:18:57,209:INFO: Dataset: zara1               Batch:  5/16	Loss 1535.7942 (842.1166)
2022-10-26 23:18:58,808:INFO: Dataset: zara1               Batch:  6/16	Loss 1468.0421 (938.1537)
2022-10-26 23:19:00,404:INFO: Dataset: zara1               Batch:  7/16	Loss 1862.2511 (1073.0506)
2022-10-26 23:19:02,015:INFO: Dataset: zara1               Batch:  8/16	Loss 480.8275 (1002.3195)
2022-10-26 23:19:03,612:INFO: Dataset: zara1               Batch:  9/16	Loss 556.9348 (957.3353)
2022-10-26 23:19:05,195:INFO: Dataset: zara1               Batch: 10/16	Loss 647.9880 (929.1895)
2022-10-26 23:19:06,858:INFO: Dataset: zara1               Batch: 11/16	Loss 711.6711 (907.3406)
2022-10-26 23:19:08,445:INFO: Dataset: zara1               Batch: 12/16	Loss 692.8925 (891.8009)
2022-10-26 23:19:10,061:INFO: Dataset: zara1               Batch: 13/16	Loss 1419.1270 (933.0611)
2022-10-26 23:19:11,660:INFO: Dataset: zara1               Batch: 14/16	Loss 692.8250 (915.3655)
2022-10-26 23:19:13,299:INFO: Dataset: zara1               Batch: 15/16	Loss 3508.6704 (1093.2815)
2022-10-26 23:19:14,727:INFO: Dataset: zara1               Batch: 16/16	Loss 484.6590 (1068.2960)
2022-10-26 23:19:35,274:INFO: Dataset: zara2               Batch:  1/36	Loss 3909.5479 (3909.5479)
2022-10-26 23:19:36,847:INFO: Dataset: zara2               Batch:  2/36	Loss 1146.8553 (2479.6559)
2022-10-26 23:19:38,454:INFO: Dataset: zara2               Batch:  3/36	Loss 7259.7319 (4109.9134)
2022-10-26 23:19:40,107:INFO: Dataset: zara2               Batch:  4/36	Loss 3315.2822 (3909.6914)
2022-10-26 23:19:41,730:INFO: Dataset: zara2               Batch:  5/36	Loss 517.2726 (3094.0500)
2022-10-26 23:19:43,295:INFO: Dataset: zara2               Batch:  6/36	Loss -3883.1311 (1863.5915)
2022-10-26 23:19:44,910:INFO: Dataset: zara2               Batch:  7/36	Loss 2946.8965 (2029.0713)
2022-10-26 23:19:46,492:INFO: Dataset: zara2               Batch:  8/36	Loss 2660.6973 (2118.2686)
2022-10-26 23:19:48,114:INFO: Dataset: zara2               Batch:  9/36	Loss 1642.6841 (2054.7589)
2022-10-26 23:19:49,715:INFO: Dataset: zara2               Batch: 10/36	Loss 584.5450 (1909.8778)
2022-10-26 23:19:51,387:INFO: Dataset: zara2               Batch: 11/36	Loss 1202.8933 (1850.7508)
2022-10-26 23:19:52,993:INFO: Dataset: zara2               Batch: 12/36	Loss 3726.5442 (1983.9697)
2022-10-26 23:19:54,610:INFO: Dataset: zara2               Batch: 13/36	Loss 340.6517 (1868.2021)
2022-10-26 23:19:56,215:INFO: Dataset: zara2               Batch: 14/36	Loss 242.5185 (1762.4720)
2022-10-26 23:19:57,803:INFO: Dataset: zara2               Batch: 15/36	Loss 2965.3918 (1854.5087)
2022-10-26 23:19:59,419:INFO: Dataset: zara2               Batch: 16/36	Loss 1199.8380 (1817.8623)
2022-10-26 23:20:01,005:INFO: Dataset: zara2               Batch: 17/36	Loss 731.9828 (1745.3971)
2022-10-26 23:20:02,577:INFO: Dataset: zara2               Batch: 18/36	Loss 500.0250 (1688.8986)
2022-10-26 23:20:04,175:INFO: Dataset: zara2               Batch: 19/36	Loss 607.9349 (1634.0910)
2022-10-26 23:20:05,768:INFO: Dataset: zara2               Batch: 20/36	Loss 426.1910 (1586.2569)
2022-10-26 23:20:07,353:INFO: Dataset: zara2               Batch: 21/36	Loss 581.2839 (1533.7312)
2022-10-26 23:20:08,919:INFO: Dataset: zara2               Batch: 22/36	Loss 346.0121 (1491.6881)
2022-10-26 23:20:10,528:INFO: Dataset: zara2               Batch: 23/36	Loss 561.1259 (1455.4729)
2022-10-26 23:20:12,144:INFO: Dataset: zara2               Batch: 24/36	Loss 386.0659 (1411.0904)
2022-10-26 23:20:13,782:INFO: Dataset: zara2               Batch: 25/36	Loss 623.2718 (1376.4814)
2022-10-26 23:20:15,375:INFO: Dataset: zara2               Batch: 26/36	Loss 219.0451 (1324.7729)
2022-10-26 23:20:17,076:INFO: Dataset: zara2               Batch: 27/36	Loss 4243.0610 (1433.1864)
2022-10-26 23:20:18,674:INFO: Dataset: zara2               Batch: 28/36	Loss 1840.9254 (1446.6349)
2022-10-26 23:20:20,438:INFO: Dataset: zara2               Batch: 29/36	Loss 385.8588 (1411.5127)
2022-10-26 23:20:22,184:INFO: Dataset: zara2               Batch: 30/36	Loss 633.5237 (1385.9873)
2022-10-26 23:20:23,913:INFO: Dataset: zara2               Batch: 31/36	Loss 520.8528 (1356.2796)
2022-10-26 23:20:25,518:INFO: Dataset: zara2               Batch: 32/36	Loss 527.6611 (1331.5721)
2022-10-26 23:20:27,364:INFO: Dataset: zara2               Batch: 33/36	Loss 1633.5790 (1341.5171)
2022-10-26 23:20:29,139:INFO: Dataset: zara2               Batch: 34/36	Loss 431.5219 (1317.5450)
2022-10-26 23:20:30,882:INFO: Dataset: zara2               Batch: 35/36	Loss 117.4554 (1283.1990)
2022-10-26 23:20:32,344:INFO: Dataset: zara2               Batch: 36/36	Loss 1319.8326 (1283.8591)
2022-10-26 23:20:33,164:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_3.pth.tar
2022-10-26 23:20:33,164:INFO: 
===> EPOCH: 4 (P2)
2022-10-26 23:20:33,165:INFO: - Computing loss (training)
2022-10-26 23:20:40,499:INFO: Dataset: hotel               Batch: 1/8	Loss 276.6944 (276.6944)
2022-10-26 23:20:42,120:INFO: Dataset: hotel               Batch: 2/8	Loss 3060.4333 (1642.3022)
2022-10-26 23:20:43,750:INFO: Dataset: hotel               Batch: 3/8	Loss 448.0483 (1262.1378)
2022-10-26 23:20:45,361:INFO: Dataset: hotel               Batch: 4/8	Loss 1369.9326 (1289.3456)
2022-10-26 23:20:46,965:INFO: Dataset: hotel               Batch: 5/8	Loss 441.2858 (1121.0409)
2022-10-26 23:20:48,552:INFO: Dataset: hotel               Batch: 6/8	Loss 339.6975 (989.5648)
2022-10-26 23:20:50,176:INFO: Dataset: hotel               Batch: 7/8	Loss 576.6123 (930.0861)
2022-10-26 23:20:51,291:INFO: Dataset: hotel               Batch: 8/8	Loss 171.9355 (901.0803)
2022-10-26 23:21:13,300:INFO: Dataset: univ                Batch:  1/29	Loss 458.7004 (458.7004)
2022-10-26 23:21:15,641:INFO: Dataset: univ                Batch:  2/29	Loss 222.6447 (336.4752)
2022-10-26 23:21:17,680:INFO: Dataset: univ                Batch:  3/29	Loss 1000.3425 (558.6022)
2022-10-26 23:21:19,377:INFO: Dataset: univ                Batch:  4/29	Loss 572.7586 (562.2408)
2022-10-26 23:21:21,088:INFO: Dataset: univ                Batch:  5/29	Loss 426.3164 (533.0739)
2022-10-26 23:21:22,799:INFO: Dataset: univ                Batch:  6/29	Loss 880.7347 (591.8683)
2022-10-26 23:21:24,524:INFO: Dataset: univ                Batch:  7/29	Loss 533.7792 (583.7402)
2022-10-26 23:21:26,260:INFO: Dataset: univ                Batch:  8/29	Loss 526.5483 (576.6592)
2022-10-26 23:21:27,987:INFO: Dataset: univ                Batch:  9/29	Loss 375.8387 (550.1363)
2022-10-26 23:21:29,893:INFO: Dataset: univ                Batch: 10/29	Loss 294.4749 (524.1462)
2022-10-26 23:21:31,724:INFO: Dataset: univ                Batch: 11/29	Loss 364.9961 (509.5487)
2022-10-26 23:21:33,622:INFO: Dataset: univ                Batch: 12/29	Loss 218.1109 (484.6746)
2022-10-26 23:21:35,655:INFO: Dataset: univ                Batch: 13/29	Loss 1468.6572 (563.5473)
2022-10-26 23:21:37,539:INFO: Dataset: univ                Batch: 14/29	Loss 469.7467 (556.1990)
2022-10-26 23:21:39,627:INFO: Dataset: univ                Batch: 15/29	Loss 606.5093 (559.5438)
2022-10-26 23:21:41,348:INFO: Dataset: univ                Batch: 16/29	Loss 240.9996 (541.7358)
2022-10-26 23:21:43,477:INFO: Dataset: univ                Batch: 17/29	Loss 1656.3615 (604.1682)
2022-10-26 23:21:45,556:INFO: Dataset: univ                Batch: 18/29	Loss 1663.7094 (662.9130)
2022-10-26 23:21:47,911:INFO: Dataset: univ                Batch: 19/29	Loss 4696.2505 (845.3845)
2022-10-26 23:21:50,045:INFO: Dataset: univ                Batch: 20/29	Loss 526.8555 (831.0793)
2022-10-26 23:21:52,122:INFO: Dataset: univ                Batch: 21/29	Loss 2272.2983 (899.1927)
2022-10-26 23:21:54,241:INFO: Dataset: univ                Batch: 22/29	Loss 629.7408 (886.4290)
2022-10-26 23:21:56,223:INFO: Dataset: univ                Batch: 23/29	Loss 184.1140 (854.9820)
2022-10-26 23:21:57,852:INFO: Dataset: univ                Batch: 24/29	Loss 280.8830 (827.6609)
2022-10-26 23:21:59,531:INFO: Dataset: univ                Batch: 25/29	Loss 182.8026 (804.6365)
2022-10-26 23:22:01,256:INFO: Dataset: univ                Batch: 26/29	Loss 689.5119 (799.9226)
2022-10-26 23:22:02,922:INFO: Dataset: univ                Batch: 27/29	Loss 285.2401 (780.4438)
2022-10-26 23:22:04,605:INFO: Dataset: univ                Batch: 28/29	Loss 236.4729 (759.5239)
2022-10-26 23:22:05,712:INFO: Dataset: univ                Batch: 29/29	Loss 249.9503 (752.2794)
2022-10-26 23:22:13,608:INFO: Dataset: zara1               Batch:  1/16	Loss 1010.7836 (1010.7836)
2022-10-26 23:22:15,242:INFO: Dataset: zara1               Batch:  2/16	Loss 223.8306 (620.7286)
2022-10-26 23:22:16,838:INFO: Dataset: zara1               Batch:  3/16	Loss 225.2584 (475.8318)
2022-10-26 23:22:18,378:INFO: Dataset: zara1               Batch:  4/16	Loss 507.2951 (483.1998)
2022-10-26 23:22:19,974:INFO: Dataset: zara1               Batch:  5/16	Loss 507.3704 (488.1152)
2022-10-26 23:22:21,596:INFO: Dataset: zara1               Batch:  6/16	Loss 141.1217 (433.9743)
2022-10-26 23:22:23,218:INFO: Dataset: zara1               Batch:  7/16	Loss 105.8291 (380.2566)
2022-10-26 23:22:24,813:INFO: Dataset: zara1               Batch:  8/16	Loss 770.0878 (427.4103)
2022-10-26 23:22:26,408:INFO: Dataset: zara1               Batch:  9/16	Loss 1332.7249 (518.2815)
2022-10-26 23:22:29,052:INFO: Dataset: zara1               Batch: 10/16	Loss 383.3536 (503.1147)
2022-10-26 23:22:30,901:INFO: Dataset: zara1               Batch: 11/16	Loss 291.5798 (483.7534)
2022-10-26 23:22:32,489:INFO: Dataset: zara1               Batch: 12/16	Loss 394.5494 (476.3868)
2022-10-26 23:22:34,109:INFO: Dataset: zara1               Batch: 13/16	Loss 1867.1613 (591.4731)
2022-10-26 23:22:36,047:INFO: Dataset: zara1               Batch: 14/16	Loss 221.5820 (566.0390)
2022-10-26 23:22:37,765:INFO: Dataset: zara1               Batch: 15/16	Loss 255.7483 (542.4188)
2022-10-26 23:22:39,162:INFO: Dataset: zara1               Batch: 16/16	Loss 92.9797 (524.9144)
2022-10-26 23:22:59,563:INFO: Dataset: zara2               Batch:  1/36	Loss 234.0828 (234.0828)
2022-10-26 23:23:01,159:INFO: Dataset: zara2               Batch:  2/36	Loss 325.4372 (280.0225)
2022-10-26 23:23:02,749:INFO: Dataset: zara2               Batch:  3/36	Loss 81.6391 (216.7417)
2022-10-26 23:23:04,343:INFO: Dataset: zara2               Batch:  4/36	Loss 516.5966 (290.2659)
2022-10-26 23:23:05,954:INFO: Dataset: zara2               Batch:  5/36	Loss 150.6493 (259.5438)
2022-10-26 23:23:07,558:INFO: Dataset: zara2               Batch:  6/36	Loss 1010.2412 (397.2541)
2022-10-26 23:23:09,172:INFO: Dataset: zara2               Batch:  7/36	Loss 818.2819 (455.0087)
2022-10-26 23:23:10,775:INFO: Dataset: zara2               Batch:  8/36	Loss 489.5869 (458.8729)
2022-10-26 23:23:12,387:INFO: Dataset: zara2               Batch:  9/36	Loss 522.1010 (465.4489)
2022-10-26 23:23:13,965:INFO: Dataset: zara2               Batch: 10/36	Loss 279.2022 (446.0482)
2022-10-26 23:23:15,569:INFO: Dataset: zara2               Batch: 11/36	Loss 966.4709 (492.9110)
2022-10-26 23:23:17,186:INFO: Dataset: zara2               Batch: 12/36	Loss 141.7858 (464.3725)
2022-10-26 23:23:18,797:INFO: Dataset: zara2               Batch: 13/36	Loss 439.0978 (462.2859)
2022-10-26 23:23:20,406:INFO: Dataset: zara2               Batch: 14/36	Loss 486.6575 (463.9302)
2022-10-26 23:23:22,011:INFO: Dataset: zara2               Batch: 15/36	Loss 68.7232 (443.3093)
2022-10-26 23:23:23,664:INFO: Dataset: zara2               Batch: 16/36	Loss 940.9941 (476.3060)
2022-10-26 23:23:25,266:INFO: Dataset: zara2               Batch: 17/36	Loss 243.1836 (464.5485)
2022-10-26 23:23:26,860:INFO: Dataset: zara2               Batch: 18/36	Loss 64.4561 (445.8439)
2022-10-26 23:23:28,465:INFO: Dataset: zara2               Batch: 19/36	Loss 201.0062 (431.9823)
2022-10-26 23:23:30,072:INFO: Dataset: zara2               Batch: 20/36	Loss 79.1340 (415.4656)
2022-10-26 23:23:31,703:INFO: Dataset: zara2               Batch: 21/36	Loss 337.4070 (411.8267)
2022-10-26 23:23:33,322:INFO: Dataset: zara2               Batch: 22/36	Loss 165.1135 (400.5820)
2022-10-26 23:23:34,924:INFO: Dataset: zara2               Batch: 23/36	Loss 141.2919 (390.1186)
2022-10-26 23:23:36,563:INFO: Dataset: zara2               Batch: 24/36	Loss 317.9381 (387.0593)
2022-10-26 23:23:38,140:INFO: Dataset: zara2               Batch: 25/36	Loss 383.6707 (386.9254)
2022-10-26 23:23:39,773:INFO: Dataset: zara2               Batch: 26/36	Loss -536.7902 (346.3226)
2022-10-26 23:23:41,388:INFO: Dataset: zara2               Batch: 27/36	Loss 1596.4926 (396.3294)
2022-10-26 23:23:42,995:INFO: Dataset: zara2               Batch: 28/36	Loss 199.3774 (389.9220)
2022-10-26 23:23:44,616:INFO: Dataset: zara2               Batch: 29/36	Loss 417.0908 (391.0250)
2022-10-26 23:23:46,177:INFO: Dataset: zara2               Batch: 30/36	Loss 274.4753 (387.2297)
2022-10-26 23:23:47,772:INFO: Dataset: zara2               Batch: 31/36	Loss 939.9187 (404.9664)
2022-10-26 23:23:49,371:INFO: Dataset: zara2               Batch: 32/36	Loss 83.9236 (395.5030)
2022-10-26 23:23:50,988:INFO: Dataset: zara2               Batch: 33/36	Loss 3036.9019 (478.0467)
2022-10-26 23:23:52,586:INFO: Dataset: zara2               Batch: 34/36	Loss 337.2963 (473.6153)
2022-10-26 23:23:54,215:INFO: Dataset: zara2               Batch: 35/36	Loss 236.7822 (466.3478)
2022-10-26 23:23:55,598:INFO: Dataset: zara2               Batch: 36/36	Loss 643.3817 (469.8013)
2022-10-26 23:23:56,353:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_4.pth.tar
2022-10-26 23:23:56,353:INFO: 
===> EPOCH: 5 (P2)
2022-10-26 23:23:56,354:INFO: - Computing loss (training)
2022-10-26 23:24:03,429:INFO: Dataset: hotel               Batch: 1/8	Loss -390.8075 (-390.8075)
2022-10-26 23:24:05,031:INFO: Dataset: hotel               Batch: 2/8	Loss 511.3825 (53.6861)
2022-10-26 23:24:06,629:INFO: Dataset: hotel               Batch: 3/8	Loss 101.0441 (69.5234)
2022-10-26 23:24:08,238:INFO: Dataset: hotel               Batch: 4/8	Loss -29.3000 (42.4868)
2022-10-26 23:24:09,847:INFO: Dataset: hotel               Batch: 5/8	Loss 252.0679 (84.4030)
2022-10-26 23:24:11,455:INFO: Dataset: hotel               Batch: 6/8	Loss 391.1299 (135.5242)
2022-10-26 23:24:13,079:INFO: Dataset: hotel               Batch: 7/8	Loss 80.3180 (128.1533)
2022-10-26 23:24:14,311:INFO: Dataset: hotel               Batch: 8/8	Loss 21.4958 (124.7763)
2022-10-26 23:24:35,818:INFO: Dataset: univ                Batch:  1/29	Loss 59.4300 (59.4300)
2022-10-26 23:24:37,932:INFO: Dataset: univ                Batch:  2/29	Loss -233.0432 (-87.4480)
2022-10-26 23:24:40,074:INFO: Dataset: univ                Batch:  3/29	Loss -14.6676 (-62.5808)
2022-10-26 23:24:42,066:INFO: Dataset: univ                Batch:  4/29	Loss 61.2756 (-33.5213)
2022-10-26 23:24:44,120:INFO: Dataset: univ                Batch:  5/29	Loss 670.9111 (111.1997)
2022-10-26 23:24:46,141:INFO: Dataset: univ                Batch:  6/29	Loss 199.8214 (125.8003)
2022-10-26 23:24:48,429:INFO: Dataset: univ                Batch:  7/29	Loss 174.8070 (132.6348)
2022-10-26 23:24:50,403:INFO: Dataset: univ                Batch:  8/29	Loss 355.4929 (165.0098)
2022-10-26 23:24:52,330:INFO: Dataset: univ                Batch:  9/29	Loss 1529.4456 (316.7591)
2022-10-26 23:24:54,353:INFO: Dataset: univ                Batch: 10/29	Loss 281.1316 (312.1107)
2022-10-26 23:24:56,470:INFO: Dataset: univ                Batch: 11/29	Loss 448.8925 (325.0507)
2022-10-26 23:24:58,631:INFO: Dataset: univ                Batch: 12/29	Loss 170.6419 (311.3660)
2022-10-26 23:25:01,063:INFO: Dataset: univ                Batch: 13/29	Loss -39.1370 (284.5698)
2022-10-26 23:25:03,185:INFO: Dataset: univ                Batch: 14/29	Loss -384.9277 (235.8612)
2022-10-26 23:25:04,966:INFO: Dataset: univ                Batch: 15/29	Loss 104.2111 (226.6078)
2022-10-26 23:25:06,584:INFO: Dataset: univ                Batch: 16/29	Loss 127.2145 (220.3665)
2022-10-26 23:25:08,201:INFO: Dataset: univ                Batch: 17/29	Loss 147.7132 (216.0353)
2022-10-26 23:25:09,864:INFO: Dataset: univ                Batch: 18/29	Loss -368.2437 (181.5775)
2022-10-26 23:25:11,498:INFO: Dataset: univ                Batch: 19/29	Loss 185.7096 (181.8079)
2022-10-26 23:25:13,129:INFO: Dataset: univ                Batch: 20/29	Loss 283.5088 (186.8978)
2022-10-26 23:25:14,735:INFO: Dataset: univ                Batch: 21/29	Loss 162.4279 (185.5580)
2022-10-26 23:25:16,346:INFO: Dataset: univ                Batch: 22/29	Loss 422.5533 (195.8881)
2022-10-26 23:25:17,988:INFO: Dataset: univ                Batch: 23/29	Loss 179.7078 (195.1532)
2022-10-26 23:25:19,596:INFO: Dataset: univ                Batch: 24/29	Loss 270.4531 (198.1693)
2022-10-26 23:25:21,225:INFO: Dataset: univ                Batch: 25/29	Loss 245.0571 (199.8307)
2022-10-26 23:25:22,887:INFO: Dataset: univ                Batch: 26/29	Loss 79.9924 (194.8509)
2022-10-26 23:25:24,579:INFO: Dataset: univ                Batch: 27/29	Loss 191.7392 (194.7141)
2022-10-26 23:25:26,206:INFO: Dataset: univ                Batch: 28/29	Loss 195.0120 (194.7244)
2022-10-26 23:25:27,326:INFO: Dataset: univ                Batch: 29/29	Loss 216.7009 (195.0087)
2022-10-26 23:25:35,232:INFO: Dataset: zara1               Batch:  1/16	Loss -205.6424 (-205.6424)
2022-10-26 23:25:36,972:INFO: Dataset: zara1               Batch:  2/16	Loss 464.7304 (135.1304)
2022-10-26 23:25:38,574:INFO: Dataset: zara1               Batch:  3/16	Loss 99.5583 (123.1420)
2022-10-26 23:25:40,164:INFO: Dataset: zara1               Batch:  4/16	Loss 171.1737 (135.2492)
2022-10-26 23:25:41,751:INFO: Dataset: zara1               Batch:  5/16	Loss 204.2172 (146.6646)
2022-10-26 23:25:43,350:INFO: Dataset: zara1               Batch:  6/16	Loss 211.7918 (158.3635)
2022-10-26 23:25:44,930:INFO: Dataset: zara1               Batch:  7/16	Loss 916.4830 (271.4885)
2022-10-26 23:25:46,494:INFO: Dataset: zara1               Batch:  8/16	Loss 106.7375 (252.0751)
2022-10-26 23:25:48,097:INFO: Dataset: zara1               Batch:  9/16	Loss 316.9601 (260.8337)
2022-10-26 23:25:49,703:INFO: Dataset: zara1               Batch: 10/16	Loss 742.5286 (310.4316)
2022-10-26 23:25:51,316:INFO: Dataset: zara1               Batch: 11/16	Loss 116.9123 (294.0861)
2022-10-26 23:25:53,170:INFO: Dataset: zara1               Batch: 12/16	Loss 511.5376 (310.7432)
2022-10-26 23:25:55,013:INFO: Dataset: zara1               Batch: 13/16	Loss 256.4281 (306.2025)
2022-10-26 23:25:56,795:INFO: Dataset: zara1               Batch: 14/16	Loss 344.2036 (308.9683)
2022-10-26 23:25:58,488:INFO: Dataset: zara1               Batch: 15/16	Loss 169.9935 (298.8999)
2022-10-26 23:26:00,074:INFO: Dataset: zara1               Batch: 16/16	Loss 218.3690 (295.5939)
2022-10-26 23:26:25,946:INFO: Dataset: zara2               Batch:  1/36	Loss 334.4767 (334.4767)
2022-10-26 23:26:28,403:INFO: Dataset: zara2               Batch:  2/36	Loss 142.8330 (244.8630)
2022-10-26 23:26:30,714:INFO: Dataset: zara2               Batch:  3/36	Loss 307.2209 (264.5676)
2022-10-26 23:26:32,934:INFO: Dataset: zara2               Batch:  4/36	Loss 401.9301 (296.3136)
2022-10-26 23:26:35,140:INFO: Dataset: zara2               Batch:  5/36	Loss 160.4468 (268.5955)
2022-10-26 23:26:37,169:INFO: Dataset: zara2               Batch:  6/36	Loss 350.7727 (280.8768)
2022-10-26 23:26:39,087:INFO: Dataset: zara2               Batch:  7/36	Loss 134.3789 (261.1657)
2022-10-26 23:26:40,958:INFO: Dataset: zara2               Batch:  8/36	Loss 231.1488 (257.4250)
2022-10-26 23:26:42,644:INFO: Dataset: zara2               Batch:  9/36	Loss -22.9870 (224.0158)
2022-10-26 23:26:44,366:INFO: Dataset: zara2               Batch: 10/36	Loss 271.3908 (228.7533)
2022-10-26 23:26:46,684:INFO: Dataset: zara2               Batch: 11/36	Loss 433.8071 (251.8780)
2022-10-26 23:26:48,764:INFO: Dataset: zara2               Batch: 12/36	Loss 86.3399 (238.8371)
2022-10-26 23:26:50,979:INFO: Dataset: zara2               Batch: 13/36	Loss 202.8250 (236.0104)
2022-10-26 23:26:53,109:INFO: Dataset: zara2               Batch: 14/36	Loss 190.9481 (232.4167)
2022-10-26 23:26:55,269:INFO: Dataset: zara2               Batch: 15/36	Loss 130.5664 (225.8893)
2022-10-26 23:26:57,055:INFO: Dataset: zara2               Batch: 16/36	Loss 348.1550 (233.4637)
2022-10-26 23:26:58,892:INFO: Dataset: zara2               Batch: 17/36	Loss 973.6055 (278.5601)
2022-10-26 23:27:00,651:INFO: Dataset: zara2               Batch: 18/36	Loss -3.1701 (262.2081)
2022-10-26 23:27:02,300:INFO: Dataset: zara2               Batch: 19/36	Loss 118.8716 (255.5176)
2022-10-26 23:27:03,931:INFO: Dataset: zara2               Batch: 20/36	Loss 172.7666 (251.4323)
2022-10-26 23:27:05,591:INFO: Dataset: zara2               Batch: 21/36	Loss 60.3350 (244.1925)
2022-10-26 23:27:07,280:INFO: Dataset: zara2               Batch: 22/36	Loss 130.7177 (239.0471)
2022-10-26 23:27:08,995:INFO: Dataset: zara2               Batch: 23/36	Loss 82.9906 (232.6643)
2022-10-26 23:27:11,323:INFO: Dataset: zara2               Batch: 24/36	Loss 66.7229 (225.3156)
2022-10-26 23:27:13,546:INFO: Dataset: zara2               Batch: 25/36	Loss 166.6924 (222.9361)
2022-10-26 23:27:15,346:INFO: Dataset: zara2               Batch: 26/36	Loss 110.1246 (218.7092)
2022-10-26 23:27:17,170:INFO: Dataset: zara2               Batch: 27/36	Loss 125.9391 (215.2016)
2022-10-26 23:27:18,914:INFO: Dataset: zara2               Batch: 28/36	Loss 59.7288 (209.6325)
2022-10-26 23:27:20,681:INFO: Dataset: zara2               Batch: 29/36	Loss 182.3203 (208.7312)
2022-10-26 23:27:22,446:INFO: Dataset: zara2               Batch: 30/36	Loss 501.2690 (220.5271)
2022-10-26 23:27:24,211:INFO: Dataset: zara2               Batch: 31/36	Loss -143.4439 (207.6811)
2022-10-26 23:27:25,916:INFO: Dataset: zara2               Batch: 32/36	Loss 754.8322 (225.5579)
2022-10-26 23:27:27,581:INFO: Dataset: zara2               Batch: 33/36	Loss 1277.0725 (254.0990)
2022-10-26 23:27:29,304:INFO: Dataset: zara2               Batch: 34/36	Loss 283.5309 (255.0156)
2022-10-26 23:27:30,955:INFO: Dataset: zara2               Batch: 35/36	Loss 133.9710 (252.2329)
2022-10-26 23:27:32,480:INFO: Dataset: zara2               Batch: 36/36	Loss 62.1623 (248.0538)
2022-10-26 23:27:33,324:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_5.pth.tar
2022-10-26 23:27:33,324:INFO: 
===> EPOCH: 6 (P2)
2022-10-26 23:27:33,325:INFO: - Computing loss (training)
2022-10-26 23:27:41,952:INFO: Dataset: hotel               Batch: 1/8	Loss 60.5797 (60.5797)
2022-10-26 23:27:43,813:INFO: Dataset: hotel               Batch: 2/8	Loss 251.3599 (144.9801)
2022-10-26 23:27:45,941:INFO: Dataset: hotel               Batch: 3/8	Loss 62.4662 (120.0402)
2022-10-26 23:27:48,016:INFO: Dataset: hotel               Batch: 4/8	Loss 110.9653 (117.6374)
2022-10-26 23:27:50,111:INFO: Dataset: hotel               Batch: 5/8	Loss 393.4784 (172.0728)
2022-10-26 23:27:52,150:INFO: Dataset: hotel               Batch: 6/8	Loss 59.3416 (154.2447)
2022-10-26 23:27:54,362:INFO: Dataset: hotel               Batch: 7/8	Loss 142.4374 (152.4664)
2022-10-26 23:27:55,625:INFO: Dataset: hotel               Batch: 8/8	Loss 12.5769 (148.5909)
2022-10-26 23:28:17,988:INFO: Dataset: univ                Batch:  1/29	Loss 111.2177 (111.2177)
2022-10-26 23:28:20,020:INFO: Dataset: univ                Batch:  2/29	Loss 132.1264 (121.7366)
2022-10-26 23:28:22,219:INFO: Dataset: univ                Batch:  3/29	Loss 58.7907 (102.5204)
2022-10-26 23:28:24,380:INFO: Dataset: univ                Batch:  4/29	Loss 157.5046 (116.3944)
2022-10-26 23:28:26,683:INFO: Dataset: univ                Batch:  5/29	Loss 280.2098 (149.4681)
2022-10-26 23:28:29,168:INFO: Dataset: univ                Batch:  6/29	Loss 117.0447 (143.2344)
2022-10-26 23:28:31,443:INFO: Dataset: univ                Batch:  7/29	Loss 109.9677 (139.2793)
2022-10-26 23:28:33,792:INFO: Dataset: univ                Batch:  8/29	Loss 119.1368 (137.2215)
2022-10-26 23:28:36,041:INFO: Dataset: univ                Batch:  9/29	Loss 28.6611 (125.8879)
2022-10-26 23:28:38,402:INFO: Dataset: univ                Batch: 10/29	Loss 83.8209 (122.2686)
2022-10-26 23:28:40,835:INFO: Dataset: univ                Batch: 11/29	Loss 150.5054 (125.2534)
2022-10-26 23:28:42,999:INFO: Dataset: univ                Batch: 12/29	Loss 171.7038 (129.1684)
2022-10-26 23:28:45,154:INFO: Dataset: univ                Batch: 13/29	Loss 268.2442 (139.3137)
2022-10-26 23:28:47,531:INFO: Dataset: univ                Batch: 14/29	Loss 137.9077 (139.2278)
2022-10-26 23:28:49,954:INFO: Dataset: univ                Batch: 15/29	Loss 277.9698 (146.3089)
2022-10-26 23:28:52,214:INFO: Dataset: univ                Batch: 16/29	Loss 98.0409 (143.2783)
2022-10-26 23:28:54,429:INFO: Dataset: univ                Batch: 17/29	Loss -95.7047 (127.2527)
2022-10-26 23:28:56,699:INFO: Dataset: univ                Batch: 18/29	Loss 538.3965 (151.9709)
2022-10-26 23:28:59,084:INFO: Dataset: univ                Batch: 19/29	Loss 80.4336 (148.0470)
2022-10-26 23:29:01,440:INFO: Dataset: univ                Batch: 20/29	Loss 101.3940 (145.5092)
2022-10-26 23:29:03,633:INFO: Dataset: univ                Batch: 21/29	Loss -19.1557 (137.3499)
2022-10-26 23:29:05,795:INFO: Dataset: univ                Batch: 22/29	Loss 81.0014 (134.7155)
2022-10-26 23:29:08,083:INFO: Dataset: univ                Batch: 23/29	Loss 154.4371 (135.4784)
2022-10-26 23:29:10,518:INFO: Dataset: univ                Batch: 24/29	Loss 237.6136 (139.8170)
2022-10-26 23:29:12,701:INFO: Dataset: univ                Batch: 25/29	Loss 107.2810 (138.4713)
2022-10-26 23:29:14,862:INFO: Dataset: univ                Batch: 26/29	Loss 145.3670 (138.6929)
2022-10-26 23:29:17,182:INFO: Dataset: univ                Batch: 27/29	Loss 190.4015 (140.4456)
2022-10-26 23:29:19,598:INFO: Dataset: univ                Batch: 28/29	Loss 50.8526 (137.3294)
2022-10-26 23:29:21,212:INFO: Dataset: univ                Batch: 29/29	Loss 75.7119 (136.4913)
2022-10-26 23:29:30,483:INFO: Dataset: zara1               Batch:  1/16	Loss 43.6254 (43.6254)
2022-10-26 23:29:32,625:INFO: Dataset: zara1               Batch:  2/16	Loss 88.8053 (66.2154)
2022-10-26 23:29:34,828:INFO: Dataset: zara1               Batch:  3/16	Loss 90.7637 (74.9779)
2022-10-26 23:29:37,109:INFO: Dataset: zara1               Batch:  4/16	Loss 33.9794 (65.1961)
2022-10-26 23:29:39,478:INFO: Dataset: zara1               Batch:  5/16	Loss 113.1031 (74.7456)
2022-10-26 23:29:41,852:INFO: Dataset: zara1               Batch:  6/16	Loss 89.2507 (77.5455)
2022-10-26 23:29:44,082:INFO: Dataset: zara1               Batch:  7/16	Loss 504.4070 (136.2698)
2022-10-26 23:29:46,364:INFO: Dataset: zara1               Batch:  8/16	Loss 194.1964 (142.8578)
2022-10-26 23:29:48,480:INFO: Dataset: zara1               Batch:  9/16	Loss 157.4737 (144.5641)
2022-10-26 23:29:50,574:INFO: Dataset: zara1               Batch: 10/16	Loss 35.2721 (134.8332)
2022-10-26 23:29:52,596:INFO: Dataset: zara1               Batch: 11/16	Loss 182.5383 (139.0952)
2022-10-26 23:29:54,487:INFO: Dataset: zara1               Batch: 12/16	Loss 78.7638 (134.1854)
2022-10-26 23:29:56,440:INFO: Dataset: zara1               Batch: 13/16	Loss 63.5972 (127.9297)
2022-10-26 23:29:58,163:INFO: Dataset: zara1               Batch: 14/16	Loss 140.5213 (128.8401)
2022-10-26 23:30:00,773:INFO: Dataset: zara1               Batch: 15/16	Loss 459.7723 (150.9862)
2022-10-26 23:30:02,400:INFO: Dataset: zara1               Batch: 16/16	Loss 344.4011 (157.2977)
2022-10-26 23:30:25,749:INFO: Dataset: zara2               Batch:  1/36	Loss 87.5021 (87.5021)
2022-10-26 23:30:27,925:INFO: Dataset: zara2               Batch:  2/36	Loss 42.5522 (65.3794)
2022-10-26 23:30:30,147:INFO: Dataset: zara2               Batch:  3/36	Loss 276.2380 (136.6864)
2022-10-26 23:30:31,961:INFO: Dataset: zara2               Batch:  4/36	Loss 22.0994 (110.7526)
2022-10-26 23:30:33,755:INFO: Dataset: zara2               Batch:  5/36	Loss 30.3509 (93.5164)
2022-10-26 23:30:35,569:INFO: Dataset: zara2               Batch:  6/36	Loss 333.9323 (136.1628)
2022-10-26 23:30:37,211:INFO: Dataset: zara2               Batch:  7/36	Loss 154.6828 (139.0494)
2022-10-26 23:30:38,876:INFO: Dataset: zara2               Batch:  8/36	Loss 96.6304 (133.6660)
2022-10-26 23:30:41,171:INFO: Dataset: zara2               Batch:  9/36	Loss 70.2305 (126.5602)
2022-10-26 23:30:43,138:INFO: Dataset: zara2               Batch: 10/36	Loss 84.8198 (122.3327)
2022-10-26 23:30:45,180:INFO: Dataset: zara2               Batch: 11/36	Loss 196.1298 (129.9988)
2022-10-26 23:30:47,331:INFO: Dataset: zara2               Batch: 12/36	Loss 442.8332 (155.4530)
2022-10-26 23:30:49,708:INFO: Dataset: zara2               Batch: 13/36	Loss 94.2999 (150.5901)
2022-10-26 23:30:52,090:INFO: Dataset: zara2               Batch: 14/36	Loss 254.3913 (158.0300)
2022-10-26 23:30:54,169:INFO: Dataset: zara2               Batch: 15/36	Loss 13.4874 (149.2327)
2022-10-26 23:30:56,468:INFO: Dataset: zara2               Batch: 16/36	Loss -24.1465 (137.5047)
2022-10-26 23:30:58,843:INFO: Dataset: zara2               Batch: 17/36	Loss 57.2593 (133.2227)
2022-10-26 23:31:01,187:INFO: Dataset: zara2               Batch: 18/36	Loss 362.1949 (145.9177)
2022-10-26 23:31:03,552:INFO: Dataset: zara2               Batch: 19/36	Loss 70.5283 (141.7759)
2022-10-26 23:31:05,818:INFO: Dataset: zara2               Batch: 20/36	Loss -88.9179 (129.4991)
2022-10-26 23:31:08,005:INFO: Dataset: zara2               Batch: 21/36	Loss 86.0133 (127.4794)
2022-10-26 23:31:10,344:INFO: Dataset: zara2               Batch: 22/36	Loss 189.9083 (130.4619)
2022-10-26 23:31:12,708:INFO: Dataset: zara2               Batch: 23/36	Loss 158.2735 (131.7922)
2022-10-26 23:31:14,837:INFO: Dataset: zara2               Batch: 24/36	Loss 90.7593 (129.7933)
2022-10-26 23:31:17,177:INFO: Dataset: zara2               Batch: 25/36	Loss 68.9722 (127.3397)
2022-10-26 23:31:19,264:INFO: Dataset: zara2               Batch: 26/36	Loss -560.2765 (100.9774)
2022-10-26 23:31:21,366:INFO: Dataset: zara2               Batch: 27/36	Loss 179.8118 (103.8715)
2022-10-26 23:31:23,221:INFO: Dataset: zara2               Batch: 28/36	Loss 58.2313 (102.3676)
2022-10-26 23:31:25,170:INFO: Dataset: zara2               Batch: 29/36	Loss -234.4112 (91.2894)
2022-10-26 23:31:26,903:INFO: Dataset: zara2               Batch: 30/36	Loss 61.0567 (90.0839)
2022-10-26 23:31:28,570:INFO: Dataset: zara2               Batch: 31/36	Loss 56.0689 (89.0614)
2022-10-26 23:31:30,235:INFO: Dataset: zara2               Batch: 32/36	Loss 38.6601 (87.1847)
2022-10-26 23:31:31,889:INFO: Dataset: zara2               Batch: 33/36	Loss 187.7571 (90.2057)
2022-10-26 23:31:33,606:INFO: Dataset: zara2               Batch: 34/36	Loss 169.4087 (92.3012)
2022-10-26 23:31:35,289:INFO: Dataset: zara2               Batch: 35/36	Loss 76.1326 (91.7889)
2022-10-26 23:31:36,945:INFO: Dataset: zara2               Batch: 36/36	Loss 28.5832 (90.5768)
2022-10-26 23:31:37,743:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_6.pth.tar
2022-10-26 23:31:37,743:INFO: 
===> EPOCH: 7 (P2)
2022-10-26 23:31:37,744:INFO: - Computing loss (training)
2022-10-26 23:31:45,485:INFO: Dataset: hotel               Batch: 1/8	Loss 31.1871 (31.1871)
2022-10-26 23:31:47,151:INFO: Dataset: hotel               Batch: 2/8	Loss 77.3518 (53.7472)
2022-10-26 23:31:48,814:INFO: Dataset: hotel               Batch: 3/8	Loss 0.8563 (37.2703)
2022-10-26 23:31:51,305:INFO: Dataset: hotel               Batch: 4/8	Loss 64.3257 (43.8427)
2022-10-26 23:31:53,468:INFO: Dataset: hotel               Batch: 5/8	Loss 122.6791 (60.0824)
2022-10-26 23:31:55,454:INFO: Dataset: hotel               Batch: 6/8	Loss 96.7505 (65.9631)
2022-10-26 23:31:57,659:INFO: Dataset: hotel               Batch: 7/8	Loss 24.1873 (60.3362)
2022-10-26 23:31:58,869:INFO: Dataset: hotel               Batch: 8/8	Loss 8.3567 (58.7590)
2022-10-26 23:32:21,012:INFO: Dataset: univ                Batch:  1/29	Loss 216.6510 (216.6510)
2022-10-26 23:32:22,762:INFO: Dataset: univ                Batch:  2/29	Loss 74.5912 (141.7413)
2022-10-26 23:32:24,512:INFO: Dataset: univ                Batch:  3/29	Loss 70.1432 (116.9574)
2022-10-26 23:32:26,190:INFO: Dataset: univ                Batch:  4/29	Loss 81.7270 (108.2317)
2022-10-26 23:32:27,908:INFO: Dataset: univ                Batch:  5/29	Loss 75.7082 (101.6822)
2022-10-26 23:32:29,626:INFO: Dataset: univ                Batch:  6/29	Loss 544.9799 (187.3707)
2022-10-26 23:32:31,313:INFO: Dataset: univ                Batch:  7/29	Loss 88.2602 (172.9703)
2022-10-26 23:32:32,999:INFO: Dataset: univ                Batch:  8/29	Loss 10.8463 (152.4966)
2022-10-26 23:32:34,724:INFO: Dataset: univ                Batch:  9/29	Loss 322.0366 (172.4308)
2022-10-26 23:32:36,361:INFO: Dataset: univ                Batch: 10/29	Loss 34.8905 (159.2296)
2022-10-26 23:32:38,083:INFO: Dataset: univ                Batch: 11/29	Loss 135.3869 (156.8976)
2022-10-26 23:32:39,713:INFO: Dataset: univ                Batch: 12/29	Loss 20.9770 (147.5103)
2022-10-26 23:32:41,432:INFO: Dataset: univ                Batch: 13/29	Loss 85.6834 (142.4200)
2022-10-26 23:32:43,132:INFO: Dataset: univ                Batch: 14/29	Loss 165.1455 (143.9067)
2022-10-26 23:32:44,867:INFO: Dataset: univ                Batch: 15/29	Loss 419.7323 (166.0810)
2022-10-26 23:32:46,514:INFO: Dataset: univ                Batch: 16/29	Loss 120.4593 (163.5597)
2022-10-26 23:32:48,205:INFO: Dataset: univ                Batch: 17/29	Loss 81.8912 (160.0485)
2022-10-26 23:32:49,887:INFO: Dataset: univ                Batch: 18/29	Loss 144.3010 (159.1880)
2022-10-26 23:32:51,651:INFO: Dataset: univ                Batch: 19/29	Loss 103.7458 (156.0446)
2022-10-26 23:32:53,316:INFO: Dataset: univ                Batch: 20/29	Loss 124.0109 (154.4479)
2022-10-26 23:32:55,045:INFO: Dataset: univ                Batch: 21/29	Loss 216.7799 (157.3846)
2022-10-26 23:32:56,709:INFO: Dataset: univ                Batch: 22/29	Loss 122.5427 (155.5970)
2022-10-26 23:32:58,439:INFO: Dataset: univ                Batch: 23/29	Loss 158.3518 (155.7176)
2022-10-26 23:33:00,079:INFO: Dataset: univ                Batch: 24/29	Loss -19.6847 (149.1574)
2022-10-26 23:33:01,782:INFO: Dataset: univ                Batch: 25/29	Loss 118.8924 (147.9692)
2022-10-26 23:33:03,521:INFO: Dataset: univ                Batch: 26/29	Loss 657.9185 (168.1676)
2022-10-26 23:33:05,298:INFO: Dataset: univ                Batch: 27/29	Loss 166.5220 (168.1063)
2022-10-26 23:33:06,971:INFO: Dataset: univ                Batch: 28/29	Loss 20.8971 (163.0112)
2022-10-26 23:33:08,125:INFO: Dataset: univ                Batch: 29/29	Loss -758.2040 (153.7126)
2022-10-26 23:33:18,003:INFO: Dataset: zara1               Batch:  1/16	Loss 55.1173 (55.1173)
2022-10-26 23:33:20,360:INFO: Dataset: zara1               Batch:  2/16	Loss 86.4816 (68.7654)
2022-10-26 23:33:22,729:INFO: Dataset: zara1               Batch:  3/16	Loss 308.2028 (157.2119)
2022-10-26 23:33:25,018:INFO: Dataset: zara1               Batch:  4/16	Loss 60.3828 (134.9727)
2022-10-26 23:33:27,270:INFO: Dataset: zara1               Batch:  5/16	Loss 76.4734 (122.4505)
2022-10-26 23:33:29,602:INFO: Dataset: zara1               Batch:  6/16	Loss 128.9882 (123.6032)
2022-10-26 23:33:31,889:INFO: Dataset: zara1               Batch:  7/16	Loss 148.9874 (126.6069)
2022-10-26 23:33:34,315:INFO: Dataset: zara1               Batch:  8/16	Loss 134.3511 (127.5324)
2022-10-26 23:33:36,423:INFO: Dataset: zara1               Batch:  9/16	Loss 183.0994 (133.0993)
2022-10-26 23:33:38,609:INFO: Dataset: zara1               Batch: 10/16	Loss 76.3674 (128.5435)
2022-10-26 23:33:40,914:INFO: Dataset: zara1               Batch: 11/16	Loss 36.5880 (120.3965)
2022-10-26 23:33:43,208:INFO: Dataset: zara1               Batch: 12/16	Loss 191.6315 (128.1702)
2022-10-26 23:33:45,391:INFO: Dataset: zara1               Batch: 13/16	Loss -27.7945 (117.0370)
2022-10-26 23:33:47,555:INFO: Dataset: zara1               Batch: 14/16	Loss 43.5942 (111.8999)
2022-10-26 23:33:49,869:INFO: Dataset: zara1               Batch: 15/16	Loss 117.6180 (112.2589)
2022-10-26 23:33:51,817:INFO: Dataset: zara1               Batch: 16/16	Loss 35.1219 (108.1990)
2022-10-26 23:34:14,118:INFO: Dataset: zara2               Batch:  1/36	Loss -465.8220 (-465.8220)
2022-10-26 23:34:15,939:INFO: Dataset: zara2               Batch:  2/36	Loss 56.9500 (-191.4810)
2022-10-26 23:34:17,811:INFO: Dataset: zara2               Batch:  3/36	Loss 84.5437 (-104.8092)
2022-10-26 23:34:19,478:INFO: Dataset: zara2               Batch:  4/36	Loss 19.6154 (-69.2593)
2022-10-26 23:34:21,194:INFO: Dataset: zara2               Batch:  5/36	Loss 37.6459 (-49.0675)
2022-10-26 23:34:22,856:INFO: Dataset: zara2               Batch:  6/36	Loss 78.5225 (-30.7138)
2022-10-26 23:34:24,577:INFO: Dataset: zara2               Batch:  7/36	Loss 33.7866 (-21.2654)
2022-10-26 23:34:26,248:INFO: Dataset: zara2               Batch:  8/36	Loss 180.5515 (7.4609)
2022-10-26 23:34:28,411:INFO: Dataset: zara2               Batch:  9/36	Loss 239.5877 (33.1028)
2022-10-26 23:34:30,418:INFO: Dataset: zara2               Batch: 10/36	Loss 41.1720 (33.8501)
2022-10-26 23:34:32,556:INFO: Dataset: zara2               Batch: 11/36	Loss -7.2082 (30.3296)
2022-10-26 23:34:34,839:INFO: Dataset: zara2               Batch: 12/36	Loss 76.6276 (34.6489)
2022-10-26 23:34:36,933:INFO: Dataset: zara2               Batch: 13/36	Loss 28.1760 (34.0704)
2022-10-26 23:34:38,903:INFO: Dataset: zara2               Batch: 14/36	Loss 182.0968 (43.5137)
2022-10-26 23:34:40,737:INFO: Dataset: zara2               Batch: 15/36	Loss 82.2073 (45.9470)
2022-10-26 23:34:42,682:INFO: Dataset: zara2               Batch: 16/36	Loss 91.6998 (48.7795)
2022-10-26 23:34:44,569:INFO: Dataset: zara2               Batch: 17/36	Loss 154.5284 (55.1491)
2022-10-26 23:34:46,291:INFO: Dataset: zara2               Batch: 18/36	Loss 54.5195 (55.1152)
2022-10-26 23:34:48,006:INFO: Dataset: zara2               Batch: 19/36	Loss 179.9648 (60.9724)
2022-10-26 23:34:49,721:INFO: Dataset: zara2               Batch: 20/36	Loss 130.9807 (64.4625)
2022-10-26 23:34:51,425:INFO: Dataset: zara2               Batch: 21/36	Loss 64.3566 (64.4572)
2022-10-26 23:34:53,252:INFO: Dataset: zara2               Batch: 22/36	Loss 47.5042 (63.7339)
2022-10-26 23:34:54,891:INFO: Dataset: zara2               Batch: 23/36	Loss 123.9947 (66.2589)
2022-10-26 23:34:56,762:INFO: Dataset: zara2               Batch: 24/36	Loss 85.4503 (67.0261)
2022-10-26 23:34:58,483:INFO: Dataset: zara2               Batch: 25/36	Loss 112.4756 (68.6804)
2022-10-26 23:35:00,594:INFO: Dataset: zara2               Batch: 26/36	Loss 195.1557 (73.1222)
2022-10-26 23:35:02,303:INFO: Dataset: zara2               Batch: 27/36	Loss 48.0663 (72.0606)
2022-10-26 23:35:03,966:INFO: Dataset: zara2               Batch: 28/36	Loss -46.6651 (67.5701)
2022-10-26 23:35:05,609:INFO: Dataset: zara2               Batch: 29/36	Loss 399.3860 (78.4283)
2022-10-26 23:35:07,393:INFO: Dataset: zara2               Batch: 30/36	Loss 136.8225 (80.2897)
2022-10-26 23:35:09,071:INFO: Dataset: zara2               Batch: 31/36	Loss 184.4412 (84.4227)
2022-10-26 23:35:10,802:INFO: Dataset: zara2               Batch: 32/36	Loss 12.0372 (82.1955)
2022-10-26 23:35:12,618:INFO: Dataset: zara2               Batch: 33/36	Loss 84.7876 (82.2670)
2022-10-26 23:35:15,004:INFO: Dataset: zara2               Batch: 34/36	Loss -228.4767 (73.2919)
2022-10-26 23:35:17,135:INFO: Dataset: zara2               Batch: 35/36	Loss 31.9903 (72.1865)
2022-10-26 23:35:18,666:INFO: Dataset: zara2               Batch: 36/36	Loss 31.8211 (71.4658)
2022-10-26 23:35:19,468:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_7.pth.tar
2022-10-26 23:35:19,468:INFO: 
===> EPOCH: 8 (P2)
2022-10-26 23:35:19,469:INFO: - Computing loss (training)
2022-10-26 23:35:27,557:INFO: Dataset: hotel               Batch: 1/8	Loss -170.4572 (-170.4572)
2022-10-26 23:35:29,307:INFO: Dataset: hotel               Batch: 2/8	Loss -673.4294 (-420.7626)
2022-10-26 23:35:31,007:INFO: Dataset: hotel               Batch: 3/8	Loss 194.7270 (-213.6820)
2022-10-26 23:35:32,699:INFO: Dataset: hotel               Batch: 4/8	Loss 365.6167 (-71.9242)
2022-10-26 23:35:34,414:INFO: Dataset: hotel               Batch: 5/8	Loss 142.3836 (-31.4348)
2022-10-26 23:35:36,147:INFO: Dataset: hotel               Batch: 6/8	Loss 56.4074 (-15.8521)
2022-10-26 23:35:38,593:INFO: Dataset: hotel               Batch: 7/8	Loss 86.6134 (-1.4710)
2022-10-26 23:35:39,949:INFO: Dataset: hotel               Batch: 8/8	Loss 5.4918 (-1.3148)
2022-10-26 23:36:01,914:INFO: Dataset: univ                Batch:  1/29	Loss 54.8348 (54.8348)
2022-10-26 23:36:04,104:INFO: Dataset: univ                Batch:  2/29	Loss 103.3138 (78.0874)
2022-10-26 23:36:06,310:INFO: Dataset: univ                Batch:  3/29	Loss 29.4013 (63.9074)
2022-10-26 23:36:08,388:INFO: Dataset: univ                Batch:  4/29	Loss -32.6646 (39.9691)
2022-10-26 23:36:10,151:INFO: Dataset: univ                Batch:  5/29	Loss 54.0106 (42.9418)
2022-10-26 23:36:11,876:INFO: Dataset: univ                Batch:  6/29	Loss -2.5855 (34.6587)
2022-10-26 23:36:13,607:INFO: Dataset: univ                Batch:  7/29	Loss 74.5552 (40.1378)
2022-10-26 23:36:15,250:INFO: Dataset: univ                Batch:  8/29	Loss 175.0145 (55.5323)
2022-10-26 23:36:16,980:INFO: Dataset: univ                Batch:  9/29	Loss 203.2092 (71.4712)
2022-10-26 23:36:18,611:INFO: Dataset: univ                Batch: 10/29	Loss 53.6566 (69.7813)
2022-10-26 23:36:20,300:INFO: Dataset: univ                Batch: 11/29	Loss 89.9199 (71.5623)
2022-10-26 23:36:22,029:INFO: Dataset: univ                Batch: 12/29	Loss 381.4050 (100.9529)
2022-10-26 23:36:23,721:INFO: Dataset: univ                Batch: 13/29	Loss 50.9033 (97.3152)
2022-10-26 23:36:25,679:INFO: Dataset: univ                Batch: 14/29	Loss 475.0333 (125.9990)
2022-10-26 23:36:27,424:INFO: Dataset: univ                Batch: 15/29	Loss 77.8895 (123.0559)
2022-10-26 23:36:29,350:INFO: Dataset: univ                Batch: 16/29	Loss 41.6476 (118.4603)
2022-10-26 23:36:31,476:INFO: Dataset: univ                Batch: 17/29	Loss 140.4812 (119.9652)
2022-10-26 23:36:33,540:INFO: Dataset: univ                Batch: 18/29	Loss 26.5830 (114.1853)
2022-10-26 23:36:35,857:INFO: Dataset: univ                Batch: 19/29	Loss 45.1575 (110.8149)
2022-10-26 23:36:38,070:INFO: Dataset: univ                Batch: 20/29	Loss 56.8943 (108.3840)
2022-10-26 23:36:40,238:INFO: Dataset: univ                Batch: 21/29	Loss 64.3227 (106.1743)
2022-10-26 23:36:42,408:INFO: Dataset: univ                Batch: 22/29	Loss 83.5300 (105.1612)
2022-10-26 23:36:44,720:INFO: Dataset: univ                Batch: 23/29	Loss 149.8122 (106.9184)
2022-10-26 23:36:47,055:INFO: Dataset: univ                Batch: 24/29	Loss 110.9323 (107.0793)
2022-10-26 23:36:49,184:INFO: Dataset: univ                Batch: 25/29	Loss 44.5414 (104.5658)
2022-10-26 23:36:51,359:INFO: Dataset: univ                Batch: 26/29	Loss 139.0401 (105.7669)
2022-10-26 23:36:53,452:INFO: Dataset: univ                Batch: 27/29	Loss 221.2023 (110.4877)
2022-10-26 23:36:55,501:INFO: Dataset: univ                Batch: 28/29	Loss 176.6404 (112.8913)
2022-10-26 23:36:56,714:INFO: Dataset: univ                Batch: 29/29	Loss 13.1449 (111.4968)
2022-10-26 23:37:05,216:INFO: Dataset: zara1               Batch:  1/16	Loss 74.3726 (74.3726)
2022-10-26 23:37:06,956:INFO: Dataset: zara1               Batch:  2/16	Loss 89.9230 (82.0502)
2022-10-26 23:37:08,604:INFO: Dataset: zara1               Batch:  3/16	Loss 77.6683 (80.5297)
2022-10-26 23:37:10,271:INFO: Dataset: zara1               Batch:  4/16	Loss 74.2659 (79.0822)
2022-10-26 23:37:11,929:INFO: Dataset: zara1               Batch:  5/16	Loss 74.2711 (77.9119)
2022-10-26 23:37:13,700:INFO: Dataset: zara1               Batch:  6/16	Loss 113.0508 (83.1018)
2022-10-26 23:37:15,374:INFO: Dataset: zara1               Batch:  7/16	Loss 392.5135 (123.5549)
2022-10-26 23:37:17,039:INFO: Dataset: zara1               Batch:  8/16	Loss 137.5697 (125.4035)
2022-10-26 23:37:18,714:INFO: Dataset: zara1               Batch:  9/16	Loss -168.3498 (91.4067)
2022-10-26 23:37:20,521:INFO: Dataset: zara1               Batch: 10/16	Loss -63.2245 (76.7282)
2022-10-26 23:37:22,146:INFO: Dataset: zara1               Batch: 11/16	Loss 35.9517 (73.1652)
2022-10-26 23:37:23,787:INFO: Dataset: zara1               Batch: 12/16	Loss 220.7117 (85.1147)
2022-10-26 23:37:25,481:INFO: Dataset: zara1               Batch: 13/16	Loss 277.7087 (98.8627)
2022-10-26 23:37:27,315:INFO: Dataset: zara1               Batch: 14/16	Loss 36.3656 (94.4224)
2022-10-26 23:37:28,993:INFO: Dataset: zara1               Batch: 15/16	Loss 64.4936 (92.3908)
2022-10-26 23:37:30,722:INFO: Dataset: zara1               Batch: 16/16	Loss 197.4159 (97.2551)
2022-10-26 23:37:51,919:INFO: Dataset: zara2               Batch:  1/36	Loss 85.2726 (85.2726)
2022-10-26 23:37:54,055:INFO: Dataset: zara2               Batch:  2/36	Loss 62.4582 (74.5404)
2022-10-26 23:37:56,085:INFO: Dataset: zara2               Batch:  3/36	Loss 30.3816 (57.6680)
2022-10-26 23:37:58,250:INFO: Dataset: zara2               Batch:  4/36	Loss 336.4293 (122.8685)
2022-10-26 23:38:00,520:INFO: Dataset: zara2               Batch:  5/36	Loss 103.2114 (119.1423)
2022-10-26 23:38:02,786:INFO: Dataset: zara2               Batch:  6/36	Loss 77.5042 (112.4739)
2022-10-26 23:38:05,125:INFO: Dataset: zara2               Batch:  7/36	Loss 10.8228 (97.7983)
2022-10-26 23:38:07,467:INFO: Dataset: zara2               Batch:  8/36	Loss 49.1040 (91.9606)
2022-10-26 23:38:09,609:INFO: Dataset: zara2               Batch:  9/36	Loss 20.2376 (83.9152)
2022-10-26 23:38:11,837:INFO: Dataset: zara2               Batch: 10/36	Loss 128.2165 (87.7338)
2022-10-26 23:38:14,262:INFO: Dataset: zara2               Batch: 11/36	Loss 139.8731 (91.8457)
2022-10-26 23:38:16,523:INFO: Dataset: zara2               Batch: 12/36	Loss 405.5955 (117.0757)
2022-10-26 23:38:18,801:INFO: Dataset: zara2               Batch: 13/36	Loss 46.2170 (111.3851)
2022-10-26 23:38:21,063:INFO: Dataset: zara2               Batch: 14/36	Loss 45.6542 (106.2190)
2022-10-26 23:38:23,242:INFO: Dataset: zara2               Batch: 15/36	Loss -35.2556 (96.5229)
2022-10-26 23:38:25,551:INFO: Dataset: zara2               Batch: 16/36	Loss 40.4554 (93.2151)
2022-10-26 23:38:27,826:INFO: Dataset: zara2               Batch: 17/36	Loss 33.8453 (89.6566)
2022-10-26 23:38:30,092:INFO: Dataset: zara2               Batch: 18/36	Loss 256.0617 (99.1176)
2022-10-26 23:38:32,238:INFO: Dataset: zara2               Batch: 19/36	Loss 114.8918 (99.8924)
2022-10-26 23:38:34,602:INFO: Dataset: zara2               Batch: 20/36	Loss -348.9416 (79.8883)
2022-10-26 23:38:37,008:INFO: Dataset: zara2               Batch: 21/36	Loss -62.7785 (73.7239)
2022-10-26 23:38:39,201:INFO: Dataset: zara2               Batch: 22/36	Loss 145.5787 (76.8118)
2022-10-26 23:38:41,509:INFO: Dataset: zara2               Batch: 23/36	Loss 33.2706 (74.7384)
2022-10-26 23:38:43,693:INFO: Dataset: zara2               Batch: 24/36	Loss 45.9782 (73.6008)
2022-10-26 23:38:46,018:INFO: Dataset: zara2               Batch: 25/36	Loss 13.4400 (71.0650)
2022-10-26 23:38:48,320:INFO: Dataset: zara2               Batch: 26/36	Loss 25.6052 (69.3655)
2022-10-26 23:38:50,537:INFO: Dataset: zara2               Batch: 27/36	Loss 57.2414 (68.9906)
2022-10-26 23:38:52,818:INFO: Dataset: zara2               Batch: 28/36	Loss 58.0161 (68.5799)
2022-10-26 23:38:55,222:INFO: Dataset: zara2               Batch: 29/36	Loss 43.0764 (67.6952)
2022-10-26 23:38:57,454:INFO: Dataset: zara2               Batch: 30/36	Loss 564.9855 (85.4134)
2022-10-26 23:38:59,569:INFO: Dataset: zara2               Batch: 31/36	Loss 83.1330 (85.3434)
2022-10-26 23:39:01,586:INFO: Dataset: zara2               Batch: 32/36	Loss 35.4716 (83.5537)
2022-10-26 23:39:03,464:INFO: Dataset: zara2               Batch: 33/36	Loss 113.5556 (84.4788)
2022-10-26 23:39:05,218:INFO: Dataset: zara2               Batch: 34/36	Loss 37.1198 (82.9347)
2022-10-26 23:39:06,898:INFO: Dataset: zara2               Batch: 35/36	Loss 34.3510 (81.6549)
2022-10-26 23:39:08,355:INFO: Dataset: zara2               Batch: 36/36	Loss 489.2722 (90.2129)
2022-10-26 23:39:09,144:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_8.pth.tar
2022-10-26 23:39:09,144:INFO: 
===> EPOCH: 9 (P2)
2022-10-26 23:39:09,146:INFO: - Computing loss (training)
2022-10-26 23:39:17,336:INFO: Dataset: hotel               Batch: 1/8	Loss 143.6760 (143.6760)
2022-10-26 23:39:19,357:INFO: Dataset: hotel               Batch: 2/8	Loss 34.7559 (88.4560)
2022-10-26 23:39:21,246:INFO: Dataset: hotel               Batch: 3/8	Loss 46.6709 (75.1007)
2022-10-26 23:39:23,476:INFO: Dataset: hotel               Batch: 4/8	Loss -31.4308 (47.9653)
2022-10-26 23:39:25,546:INFO: Dataset: hotel               Batch: 5/8	Loss -20.9456 (34.7082)
2022-10-26 23:39:27,655:INFO: Dataset: hotel               Batch: 6/8	Loss 77.8908 (42.0758)
2022-10-26 23:39:29,780:INFO: Dataset: hotel               Batch: 7/8	Loss 59.8660 (44.6892)
2022-10-26 23:39:31,032:INFO: Dataset: hotel               Batch: 8/8	Loss 10.9583 (43.9772)
2022-10-26 23:39:52,830:INFO: Dataset: univ                Batch:  1/29	Loss 51.9208 (51.9208)
2022-10-26 23:39:54,973:INFO: Dataset: univ                Batch:  2/29	Loss 135.4889 (92.0463)
2022-10-26 23:39:57,086:INFO: Dataset: univ                Batch:  3/29	Loss 29.1566 (71.0116)
2022-10-26 23:39:59,399:INFO: Dataset: univ                Batch:  4/29	Loss 9.9634 (55.5505)
2022-10-26 23:40:01,544:INFO: Dataset: univ                Batch:  5/29	Loss 46.1861 (53.6373)
2022-10-26 23:40:03,888:INFO: Dataset: univ                Batch:  6/29	Loss 131.4250 (63.3565)
2022-10-26 23:40:05,933:INFO: Dataset: univ                Batch:  7/29	Loss 63.2393 (63.3387)
2022-10-26 23:40:08,255:INFO: Dataset: univ                Batch:  8/29	Loss 30.0088 (59.5755)
2022-10-26 23:40:10,283:INFO: Dataset: univ                Batch:  9/29	Loss 37.5899 (57.4073)
2022-10-26 23:40:12,634:INFO: Dataset: univ                Batch: 10/29	Loss 61.8238 (57.9093)
2022-10-26 23:40:14,970:INFO: Dataset: univ                Batch: 11/29	Loss 248.3692 (74.2006)
2022-10-26 23:40:17,407:INFO: Dataset: univ                Batch: 12/29	Loss 48.7380 (72.0816)
2022-10-26 23:40:19,672:INFO: Dataset: univ                Batch: 13/29	Loss 23.9304 (68.6729)
2022-10-26 23:40:21,913:INFO: Dataset: univ                Batch: 14/29	Loss 68.0819 (68.6378)
2022-10-26 23:40:24,219:INFO: Dataset: univ                Batch: 15/29	Loss 42.2456 (67.0219)
2022-10-26 23:40:26,657:INFO: Dataset: univ                Batch: 16/29	Loss 43.8773 (65.4720)
2022-10-26 23:40:28,966:INFO: Dataset: univ                Batch: 17/29	Loss 63.3797 (65.3518)
2022-10-26 23:40:31,189:INFO: Dataset: univ                Batch: 18/29	Loss 527.1558 (93.7538)
2022-10-26 23:40:33,396:INFO: Dataset: univ                Batch: 19/29	Loss 116.0791 (95.0548)
2022-10-26 23:40:35,792:INFO: Dataset: univ                Batch: 20/29	Loss 61.3391 (93.3166)
2022-10-26 23:40:38,217:INFO: Dataset: univ                Batch: 21/29	Loss 38.4092 (90.7289)
2022-10-26 23:40:40,503:INFO: Dataset: univ                Batch: 22/29	Loss 122.3451 (92.0719)
2022-10-26 23:40:42,748:INFO: Dataset: univ                Batch: 23/29	Loss -32.8833 (86.5708)
2022-10-26 23:40:44,943:INFO: Dataset: univ                Batch: 24/29	Loss 71.2557 (85.8821)
2022-10-26 23:40:47,315:INFO: Dataset: univ                Batch: 25/29	Loss 51.7337 (84.5646)
2022-10-26 23:40:49,652:INFO: Dataset: univ                Batch: 26/29	Loss 68.1876 (83.9005)
2022-10-26 23:40:51,781:INFO: Dataset: univ                Batch: 27/29	Loss 35.0619 (81.9529)
2022-10-26 23:40:53,975:INFO: Dataset: univ                Batch: 28/29	Loss -19.0396 (78.4444)
2022-10-26 23:40:55,539:INFO: Dataset: univ                Batch: 29/29	Loss 81.1546 (78.4854)
2022-10-26 23:41:05,142:INFO: Dataset: zara1               Batch:  1/16	Loss 28.2672 (28.2672)
2022-10-26 23:41:07,466:INFO: Dataset: zara1               Batch:  2/16	Loss 239.5769 (124.8659)
2022-10-26 23:41:09,774:INFO: Dataset: zara1               Batch:  3/16	Loss 136.3593 (128.2909)
2022-10-26 23:41:11,977:INFO: Dataset: zara1               Batch:  4/16	Loss 70.5477 (114.1676)
2022-10-26 23:41:14,313:INFO: Dataset: zara1               Batch:  5/16	Loss 54.0550 (103.0356)
2022-10-26 23:41:16,655:INFO: Dataset: zara1               Batch:  6/16	Loss 45.4188 (91.7574)
2022-10-26 23:41:18,991:INFO: Dataset: zara1               Batch:  7/16	Loss 68.8979 (87.7470)
2022-10-26 23:41:21,016:INFO: Dataset: zara1               Batch:  8/16	Loss 77.7021 (86.4747)
2022-10-26 23:41:23,219:INFO: Dataset: zara1               Batch:  9/16	Loss 336.1106 (113.1238)
2022-10-26 23:41:25,588:INFO: Dataset: zara1               Batch: 10/16	Loss 79.8525 (109.8158)
2022-10-26 23:41:27,945:INFO: Dataset: zara1               Batch: 11/16	Loss 72.8470 (106.1984)
2022-10-26 23:41:30,164:INFO: Dataset: zara1               Batch: 12/16	Loss 59.5714 (102.2443)
2022-10-26 23:41:32,335:INFO: Dataset: zara1               Batch: 13/16	Loss 55.1756 (98.5105)
2022-10-26 23:41:34,572:INFO: Dataset: zara1               Batch: 14/16	Loss 55.3176 (95.9235)
2022-10-26 23:41:37,057:INFO: Dataset: zara1               Batch: 15/16	Loss -2.6099 (89.2863)
2022-10-26 23:41:39,174:INFO: Dataset: zara1               Batch: 16/16	Loss -138.1764 (80.4272)
2022-10-26 23:42:02,474:INFO: Dataset: zara2               Batch:  1/36	Loss 54.9724 (54.9724)
2022-10-26 23:42:04,607:INFO: Dataset: zara2               Batch:  2/36	Loss 1.1709 (28.9656)
2022-10-26 23:42:06,978:INFO: Dataset: zara2               Batch:  3/36	Loss 1081.9645 (346.6425)
2022-10-26 23:42:09,292:INFO: Dataset: zara2               Batch:  4/36	Loss 17.4757 (248.2820)
2022-10-26 23:42:11,535:INFO: Dataset: zara2               Batch:  5/36	Loss 118.9201 (222.9018)
2022-10-26 23:42:13,893:INFO: Dataset: zara2               Batch:  6/36	Loss 127.3932 (207.7964)
2022-10-26 23:42:16,238:INFO: Dataset: zara2               Batch:  7/36	Loss 36.7259 (185.3341)
2022-10-26 23:42:18,584:INFO: Dataset: zara2               Batch:  8/36	Loss -60.8009 (152.0226)
2022-10-26 23:42:20,792:INFO: Dataset: zara2               Batch:  9/36	Loss 109.7548 (147.0087)
2022-10-26 23:42:23,074:INFO: Dataset: zara2               Batch: 10/36	Loss 30.2158 (135.6228)
2022-10-26 23:42:25,264:INFO: Dataset: zara2               Batch: 11/36	Loss -108.0490 (110.2893)
2022-10-26 23:42:27,646:INFO: Dataset: zara2               Batch: 12/36	Loss 48.0338 (104.5911)
2022-10-26 23:42:29,988:INFO: Dataset: zara2               Batch: 13/36	Loss 28.1366 (98.9380)
2022-10-26 23:42:32,142:INFO: Dataset: zara2               Batch: 14/36	Loss 46.5611 (94.6854)
2022-10-26 23:42:34,482:INFO: Dataset: zara2               Batch: 15/36	Loss 24.5490 (89.9952)
2022-10-26 23:42:36,819:INFO: Dataset: zara2               Batch: 16/36	Loss 26.1776 (86.3000)
2022-10-26 23:42:39,166:INFO: Dataset: zara2               Batch: 17/36	Loss 462.4153 (110.2801)
2022-10-26 23:42:41,390:INFO: Dataset: zara2               Batch: 18/36	Loss 58.3774 (107.7868)
2022-10-26 23:42:43,654:INFO: Dataset: zara2               Batch: 19/36	Loss -231.5281 (91.5341)
2022-10-26 23:42:45,985:INFO: Dataset: zara2               Batch: 20/36	Loss 38.0703 (88.8656)
2022-10-26 23:42:48,373:INFO: Dataset: zara2               Batch: 21/36	Loss 28.7978 (86.2511)
2022-10-26 23:42:50,663:INFO: Dataset: zara2               Batch: 22/36	Loss 49.3900 (84.5809)
2022-10-26 23:42:52,879:INFO: Dataset: zara2               Batch: 23/36	Loss 102.1643 (85.3474)
2022-10-26 23:42:55,008:INFO: Dataset: zara2               Batch: 24/36	Loss -70.6378 (79.0527)
2022-10-26 23:42:57,338:INFO: Dataset: zara2               Batch: 25/36	Loss 81.8560 (79.1538)
2022-10-26 23:42:59,599:INFO: Dataset: zara2               Batch: 26/36	Loss 18.8362 (76.9883)
2022-10-26 23:43:01,852:INFO: Dataset: zara2               Batch: 27/36	Loss 66.4670 (76.5856)
2022-10-26 23:43:04,203:INFO: Dataset: zara2               Batch: 28/36	Loss -6.6375 (73.4166)
2022-10-26 23:43:06,246:INFO: Dataset: zara2               Batch: 29/36	Loss 28.4302 (71.7575)
2022-10-26 23:43:08,379:INFO: Dataset: zara2               Batch: 30/36	Loss 38.2121 (70.6218)
2022-10-26 23:43:10,447:INFO: Dataset: zara2               Batch: 31/36	Loss 97.2408 (71.4296)
2022-10-26 23:43:12,470:INFO: Dataset: zara2               Batch: 32/36	Loss 17.2995 (69.8547)
2022-10-26 23:43:14,579:INFO: Dataset: zara2               Batch: 33/36	Loss -4.1807 (67.7615)
2022-10-26 23:43:16,587:INFO: Dataset: zara2               Batch: 34/36	Loss 33.6039 (66.6314)
2022-10-26 23:43:18,656:INFO: Dataset: zara2               Batch: 35/36	Loss 52.1309 (66.1813)
2022-10-26 23:43:20,487:INFO: Dataset: zara2               Batch: 36/36	Loss 120.3649 (67.2741)
2022-10-26 23:43:21,321:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_9.pth.tar
2022-10-26 23:43:21,321:INFO: 
===> EPOCH: 10 (P2)
2022-10-26 23:43:21,322:INFO: - Computing loss (training)
2022-10-26 23:43:29,413:INFO: Dataset: hotel               Batch: 1/8	Loss 90.2957 (90.2957)
2022-10-26 23:43:31,072:INFO: Dataset: hotel               Batch: 2/8	Loss 29.9582 (58.1716)
2022-10-26 23:43:32,714:INFO: Dataset: hotel               Batch: 3/8	Loss 30.5465 (49.1353)
2022-10-26 23:43:34,389:INFO: Dataset: hotel               Batch: 4/8	Loss 42.1716 (47.4687)
2022-10-26 23:43:36,032:INFO: Dataset: hotel               Batch: 5/8	Loss 18.9338 (41.8268)
2022-10-26 23:43:37,680:INFO: Dataset: hotel               Batch: 6/8	Loss 61.2764 (45.0375)
2022-10-26 23:43:39,364:INFO: Dataset: hotel               Batch: 7/8	Loss 46.7036 (45.2697)
2022-10-26 23:43:40,420:INFO: Dataset: hotel               Batch: 8/8	Loss 6.0885 (43.9257)
2022-10-26 23:44:02,564:INFO: Dataset: univ                Batch:  1/29	Loss 80.8207 (80.8207)
2022-10-26 23:44:04,383:INFO: Dataset: univ                Batch:  2/29	Loss 51.9110 (65.8517)
2022-10-26 23:44:06,104:INFO: Dataset: univ                Batch:  3/29	Loss 28.9151 (53.8810)
2022-10-26 23:44:07,956:INFO: Dataset: univ                Batch:  4/29	Loss 38.9751 (50.5644)
2022-10-26 23:44:09,879:INFO: Dataset: univ                Batch:  5/29	Loss 83.5481 (56.2709)
2022-10-26 23:44:11,668:INFO: Dataset: univ                Batch:  6/29	Loss -251.6417 (2.0487)
2022-10-26 23:44:13,581:INFO: Dataset: univ                Batch:  7/29	Loss 57.4136 (11.2045)
2022-10-26 23:44:15,477:INFO: Dataset: univ                Batch:  8/29	Loss 6.1920 (10.5160)
2022-10-26 23:44:17,159:INFO: Dataset: univ                Batch:  9/29	Loss 159.8335 (27.5035)
2022-10-26 23:44:18,869:INFO: Dataset: univ                Batch: 10/29	Loss 20.3207 (26.7044)
2022-10-26 23:44:20,665:INFO: Dataset: univ                Batch: 11/29	Loss 26.1269 (26.6594)
2022-10-26 23:44:22,478:INFO: Dataset: univ                Batch: 12/29	Loss 28.5403 (26.8491)
2022-10-26 23:44:24,152:INFO: Dataset: univ                Batch: 13/29	Loss 70.8004 (30.3514)
2022-10-26 23:44:26,247:INFO: Dataset: univ                Batch: 14/29	Loss 37.7996 (30.9575)
2022-10-26 23:44:28,280:INFO: Dataset: univ                Batch: 15/29	Loss 16.4337 (29.9277)
2022-10-26 23:44:30,206:INFO: Dataset: univ                Batch: 16/29	Loss 16.2955 (29.0456)
2022-10-26 23:44:31,917:INFO: Dataset: univ                Batch: 17/29	Loss 77.7331 (32.2432)
2022-10-26 23:44:33,735:INFO: Dataset: univ                Batch: 18/29	Loss 37.4550 (32.5778)

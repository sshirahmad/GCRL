2022-11-07 13:32:33,794:INFO: Initializing Training Set
2022-11-07 13:32:37,774:INFO: Initializing Validation Set
2022-11-07 13:32:38,377:INFO: Initializing Validation O Set
2022-11-07 13:32:41,496:INFO: 
===> EPOCH: 1 (P1)
2022-11-07 13:32:41,497:INFO: - Computing loss (training)
2022-11-07 13:32:47,612:INFO: Dataset: hotel               Batch: 1/8	Loss 4.3222 (4.3222)
2022-11-07 13:32:47,990:INFO: Dataset: hotel               Batch: 2/8	Loss 7.0425 (5.5884)
2022-11-07 13:32:48,351:INFO: Dataset: hotel               Batch: 3/8	Loss 5.9666 (5.7035)
2022-11-07 13:32:48,620:INFO: Dataset: hotel               Batch: 4/8	Loss 5.2478 (5.5913)
2022-11-07 13:32:48,917:INFO: Dataset: hotel               Batch: 5/8	Loss 5.7265 (5.6198)
2022-11-07 13:32:49,092:INFO: Dataset: hotel               Batch: 6/8	Loss 4.5344 (5.4375)
2022-11-07 13:32:49,145:INFO: Dataset: hotel               Batch: 7/8	Loss 5.5712 (5.4569)
2022-11-07 13:32:49,170:INFO: Dataset: hotel               Batch: 8/8	Loss 1.3663 (5.3490)
2022-11-07 13:33:14,060:INFO: Dataset: univ                Batch:  1/29	Loss 4.4717 (4.4717)
2022-11-07 13:33:14,116:INFO: Dataset: univ                Batch:  2/29	Loss 7.1798 (5.6512)
2022-11-07 13:33:14,183:INFO: Dataset: univ                Batch:  3/29	Loss 7.5697 (6.1831)
2022-11-07 13:33:14,241:INFO: Dataset: univ                Batch:  4/29	Loss 6.7175 (6.3147)
2022-11-07 13:33:14,303:INFO: Dataset: univ                Batch:  5/29	Loss 6.3775 (6.3273)
2022-11-07 13:33:14,370:INFO: Dataset: univ                Batch:  6/29	Loss 7.3263 (6.4878)
2022-11-07 13:33:14,428:INFO: Dataset: univ                Batch:  7/29	Loss 6.0875 (6.4276)
2022-11-07 13:33:14,484:INFO: Dataset: univ                Batch:  8/29	Loss 4.9400 (6.2117)
2022-11-07 13:33:14,544:INFO: Dataset: univ                Batch:  9/29	Loss 5.2667 (6.0963)
2022-11-07 13:33:14,609:INFO: Dataset: univ                Batch: 10/29	Loss 7.4766 (6.2161)
2022-11-07 13:33:14,671:INFO: Dataset: univ                Batch: 11/29	Loss 5.6572 (6.1655)
2022-11-07 13:33:14,724:INFO: Dataset: univ                Batch: 12/29	Loss 8.0640 (6.3230)
2022-11-07 13:33:14,775:INFO: Dataset: univ                Batch: 13/29	Loss 5.5824 (6.2555)
2022-11-07 13:33:14,835:INFO: Dataset: univ                Batch: 14/29	Loss 5.2440 (6.1766)
2022-11-07 13:33:14,893:INFO: Dataset: univ                Batch: 15/29	Loss 5.9817 (6.1641)
2022-11-07 13:33:14,947:INFO: Dataset: univ                Batch: 16/29	Loss 6.9850 (6.2113)
2022-11-07 13:33:15,021:INFO: Dataset: univ                Batch: 17/29	Loss 4.9665 (6.1301)
2022-11-07 13:33:15,080:INFO: Dataset: univ                Batch: 18/29	Loss 6.2869 (6.1391)
2022-11-07 13:33:15,138:INFO: Dataset: univ                Batch: 19/29	Loss 5.8225 (6.1205)
2022-11-07 13:33:15,192:INFO: Dataset: univ                Batch: 20/29	Loss 7.8471 (6.2004)
2022-11-07 13:33:15,251:INFO: Dataset: univ                Batch: 21/29	Loss 5.1500 (6.1450)
2022-11-07 13:33:15,309:INFO: Dataset: univ                Batch: 22/29	Loss 4.8186 (6.0781)
2022-11-07 13:33:15,362:INFO: Dataset: univ                Batch: 23/29	Loss 6.6834 (6.0991)
2022-11-07 13:33:15,408:INFO: Dataset: univ                Batch: 24/29	Loss 6.7609 (6.1256)
2022-11-07 13:33:15,455:INFO: Dataset: univ                Batch: 25/29	Loss 5.8203 (6.1142)
2022-11-07 13:33:15,507:INFO: Dataset: univ                Batch: 26/29	Loss 6.6428 (6.1321)
2022-11-07 13:33:15,558:INFO: Dataset: univ                Batch: 27/29	Loss 5.8676 (6.1229)
2022-11-07 13:33:15,607:INFO: Dataset: univ                Batch: 28/29	Loss 5.0743 (6.0860)
2022-11-07 13:33:15,634:INFO: Dataset: univ                Batch: 29/29	Loss 1.5140 (6.0249)
2022-11-07 13:33:23,574:INFO: Dataset: zara1               Batch:  1/16	Loss 9.7611 (9.7611)
2022-11-07 13:33:23,926:INFO: Dataset: zara1               Batch:  2/16	Loss 9.3590 (9.5648)
2022-11-07 13:33:24,166:INFO: Dataset: zara1               Batch:  3/16	Loss 8.3692 (9.1551)
2022-11-07 13:33:24,442:INFO: Dataset: zara1               Batch:  4/16	Loss 9.2355 (9.1737)
2022-11-07 13:33:24,724:INFO: Dataset: zara1               Batch:  5/16	Loss 10.2303 (9.4094)
2022-11-07 13:33:24,983:INFO: Dataset: zara1               Batch:  6/16	Loss 10.8549 (9.6640)
2022-11-07 13:33:25,053:INFO: Dataset: zara1               Batch:  7/16	Loss 8.0234 (9.3575)
2022-11-07 13:33:25,105:INFO: Dataset: zara1               Batch:  8/16	Loss 9.0271 (9.3155)
2022-11-07 13:33:25,164:INFO: Dataset: zara1               Batch:  9/16	Loss 7.8396 (9.1340)
2022-11-07 13:33:25,215:INFO: Dataset: zara1               Batch: 10/16	Loss 7.6335 (8.9607)
2022-11-07 13:33:25,273:INFO: Dataset: zara1               Batch: 11/16	Loss 8.8228 (8.9478)
2022-11-07 13:33:25,327:INFO: Dataset: zara1               Batch: 12/16	Loss 9.0623 (8.9576)
2022-11-07 13:33:25,376:INFO: Dataset: zara1               Batch: 13/16	Loss 8.4898 (8.9217)
2022-11-07 13:33:25,423:INFO: Dataset: zara1               Batch: 14/16	Loss 8.5169 (8.8858)
2022-11-07 13:33:25,487:INFO: Dataset: zara1               Batch: 15/16	Loss 8.5096 (8.8627)
2022-11-07 13:33:25,537:INFO: Dataset: zara1               Batch: 16/16	Loss 6.3355 (8.7457)

2022-11-25 20:20:26,927:INFO: Initializing Training Set
2022-11-25 20:20:28,504:INFO: Initializing Validation Set
2022-11-25 20:20:28,692:INFO: Initializing Validation O Set
2022-11-25 20:20:30,825:INFO: 
===> EPOCH: 251 (P3)
2022-11-25 20:20:30,826:INFO: - Computing loss (training)
2022-11-25 20:20:31,496:INFO: Dataset: hotel               Batch: 1/4	Loss 142.9342 (142.9342)
2022-11-25 20:20:31,932:INFO: Dataset: hotel               Batch: 2/4	Loss 146.5155 (144.6186)
2022-11-25 20:20:32,359:INFO: Dataset: hotel               Batch: 3/4	Loss 146.8817 (145.3718)
2022-11-25 20:20:32,743:INFO: Dataset: hotel               Batch: 4/4	Loss 90.3926 (136.1602)
2022-11-25 20:20:33,515:INFO: Dataset: univ                Batch:  1/15	Loss 136.4574 (136.4574)
2022-11-25 20:20:34,015:INFO: Dataset: univ                Batch:  2/15	Loss 135.3260 (135.8303)
2022-11-25 20:20:34,574:INFO: Dataset: univ                Batch:  3/15	Loss 135.5955 (135.7567)
2022-11-25 20:20:35,050:INFO: Dataset: univ                Batch:  4/15	Loss 136.3135 (135.8992)
2022-11-25 20:20:35,532:INFO: Dataset: univ                Batch:  5/15	Loss 138.3505 (136.3577)
2022-11-25 20:20:36,030:INFO: Dataset: univ                Batch:  6/15	Loss 135.0246 (136.1153)
2022-11-25 20:20:36,520:INFO: Dataset: univ                Batch:  7/15	Loss 135.7038 (136.0528)
2022-11-25 20:20:37,002:INFO: Dataset: univ                Batch:  8/15	Loss 133.6997 (135.7690)
2022-11-25 20:20:37,498:INFO: Dataset: univ                Batch:  9/15	Loss 134.7793 (135.6511)
2022-11-25 20:20:38,025:INFO: Dataset: univ                Batch: 10/15	Loss 133.7146 (135.4409)
2022-11-25 20:20:38,543:INFO: Dataset: univ                Batch: 11/15	Loss 133.5971 (135.2593)
2022-11-25 20:20:39,013:INFO: Dataset: univ                Batch: 12/15	Loss 136.4819 (135.3490)
2022-11-25 20:20:39,536:INFO: Dataset: univ                Batch: 13/15	Loss 134.3171 (135.2592)
2022-11-25 20:20:40,056:INFO: Dataset: univ                Batch: 14/15	Loss 133.2805 (135.1061)
2022-11-25 20:20:40,442:INFO: Dataset: univ                Batch: 15/15	Loss 25.5610 (133.5435)
2022-11-25 20:20:41,167:INFO: Dataset: zara1               Batch: 1/8	Loss 151.9444 (151.9444)
2022-11-25 20:20:41,633:INFO: Dataset: zara1               Batch: 2/8	Loss 152.4471 (152.2022)
2022-11-25 20:20:42,068:INFO: Dataset: zara1               Batch: 3/8	Loss 151.5229 (151.9858)
2022-11-25 20:20:42,500:INFO: Dataset: zara1               Batch: 4/8	Loss 151.0147 (151.7497)
2022-11-25 20:20:42,936:INFO: Dataset: zara1               Batch: 5/8	Loss 149.2668 (151.2990)
2022-11-25 20:20:43,372:INFO: Dataset: zara1               Batch: 6/8	Loss 151.8425 (151.3848)
2022-11-25 20:20:43,813:INFO: Dataset: zara1               Batch: 7/8	Loss 151.9866 (151.4794)
2022-11-25 20:20:44,234:INFO: Dataset: zara1               Batch: 8/8	Loss 130.8032 (149.2159)
2022-11-25 20:20:44,981:INFO: Dataset: zara2               Batch:  1/18	Loss 141.8064 (141.8064)
2022-11-25 20:20:45,430:INFO: Dataset: zara2               Batch:  2/18	Loss 142.0714 (141.9303)
2022-11-25 20:20:45,902:INFO: Dataset: zara2               Batch:  3/18	Loss 142.8160 (142.2363)
2022-11-25 20:20:46,355:INFO: Dataset: zara2               Batch:  4/18	Loss 142.1524 (142.2153)
2022-11-25 20:20:46,802:INFO: Dataset: zara2               Batch:  5/18	Loss 143.4027 (142.4727)
2022-11-25 20:20:47,268:INFO: Dataset: zara2               Batch:  6/18	Loss 141.7597 (142.3631)
2022-11-25 20:20:47,726:INFO: Dataset: zara2               Batch:  7/18	Loss 141.1997 (142.1667)
2022-11-25 20:20:48,231:INFO: Dataset: zara2               Batch:  8/18	Loss 143.0307 (142.2666)
2022-11-25 20:20:48,938:INFO: Dataset: zara2               Batch:  9/18	Loss 140.9723 (142.1110)
2022-11-25 20:20:49,410:INFO: Dataset: zara2               Batch: 10/18	Loss 140.7659 (141.9747)
2022-11-25 20:20:49,857:INFO: Dataset: zara2               Batch: 11/18	Loss 141.5444 (141.9376)
2022-11-25 20:20:50,304:INFO: Dataset: zara2               Batch: 12/18	Loss 141.5262 (141.9031)
2022-11-25 20:20:50,749:INFO: Dataset: zara2               Batch: 13/18	Loss 142.4722 (141.9447)
2022-11-25 20:20:51,198:INFO: Dataset: zara2               Batch: 14/18	Loss 140.8556 (141.8632)
2022-11-25 20:20:51,644:INFO: Dataset: zara2               Batch: 15/18	Loss 143.8547 (141.9800)
2022-11-25 20:20:52,090:INFO: Dataset: zara2               Batch: 16/18	Loss 141.2219 (141.9347)
2022-11-25 20:20:52,545:INFO: Dataset: zara2               Batch: 17/18	Loss 142.1791 (141.9490)
2022-11-25 20:20:52,987:INFO: Dataset: zara2               Batch: 18/18	Loss 121.1862 (140.8369)
2022-11-25 20:20:53,059:INFO: - Computing ADE (validation)
2022-11-25 20:20:53,416:INFO: 		 ADE on hotel                     dataset:	 1.778283953666687
2022-11-25 20:20:53,852:INFO: 		 ADE on univ                      dataset:	 2.2252843379974365
2022-11-25 20:20:54,216:INFO: 		 ADE on zara1                     dataset:	 2.2081878185272217
2022-11-25 20:20:54,756:INFO: 		 ADE on zara2                     dataset:	 2.0190043449401855
2022-11-25 20:20:54,756:INFO: Average validation:	ADE  2.1241	FDE  3.1768
2022-11-25 20:20:54,757:INFO: - Computing ADE (validation o)
2022-11-25 20:20:55,065:INFO: 		 ADE on eth                       dataset:	 2.9626545906066895
2022-11-25 20:20:55,066:INFO: Average validation o:	ADE  2.9627	FDE  4.7490
2022-11-25 20:20:55,075:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_251.pth.tar
2022-11-25 20:20:55,075:INFO: 
===> EPOCH: 252 (P3)
2022-11-25 20:20:55,075:INFO: - Computing loss (training)
2022-11-25 20:20:55,741:INFO: Dataset: hotel               Batch: 1/4	Loss 144.5069 (144.5069)
2022-11-25 20:20:56,186:INFO: Dataset: hotel               Batch: 2/4	Loss 143.3394 (143.9455)
2022-11-25 20:20:56,629:INFO: Dataset: hotel               Batch: 3/4	Loss 145.6057 (144.5111)
2022-11-25 20:20:57,018:INFO: Dataset: hotel               Batch: 4/4	Loss 88.0723 (135.2784)
2022-11-25 20:20:57,804:INFO: Dataset: univ                Batch:  1/15	Loss 134.7871 (134.7871)
2022-11-25 20:20:58,289:INFO: Dataset: univ                Batch:  2/15	Loss 133.3345 (134.0628)
2022-11-25 20:20:58,813:INFO: Dataset: univ                Batch:  3/15	Loss 136.4944 (134.8401)
2022-11-25 20:20:59,295:INFO: Dataset: univ                Batch:  4/15	Loss 134.3971 (134.7366)
2022-11-25 20:20:59,785:INFO: Dataset: univ                Batch:  5/15	Loss 133.7190 (134.5188)
2022-11-25 20:21:00,265:INFO: Dataset: univ                Batch:  6/15	Loss 134.5021 (134.5161)
2022-11-25 20:21:00,747:INFO: Dataset: univ                Batch:  7/15	Loss 134.8392 (134.5625)
2022-11-25 20:21:01,243:INFO: Dataset: univ                Batch:  8/15	Loss 131.7567 (134.1706)
2022-11-25 20:21:01,740:INFO: Dataset: univ                Batch:  9/15	Loss 133.4774 (134.0895)
2022-11-25 20:21:02,238:INFO: Dataset: univ                Batch: 10/15	Loss 133.2681 (134.0051)
2022-11-25 20:21:02,733:INFO: Dataset: univ                Batch: 11/15	Loss 132.7328 (133.8859)
2022-11-25 20:21:03,230:INFO: Dataset: univ                Batch: 12/15	Loss 133.4055 (133.8470)
2022-11-25 20:21:03,737:INFO: Dataset: univ                Batch: 13/15	Loss 132.5663 (133.7346)
2022-11-25 20:21:04,309:INFO: Dataset: univ                Batch: 14/15	Loss 133.8931 (133.7455)
2022-11-25 20:21:04,695:INFO: Dataset: univ                Batch: 15/15	Loss 24.7776 (132.4700)
2022-11-25 20:21:05,489:INFO: Dataset: zara1               Batch: 1/8	Loss 150.2470 (150.2470)
2022-11-25 20:21:05,942:INFO: Dataset: zara1               Batch: 2/8	Loss 153.6065 (151.9942)
2022-11-25 20:21:06,394:INFO: Dataset: zara1               Batch: 3/8	Loss 150.6673 (151.5822)
2022-11-25 20:21:06,834:INFO: Dataset: zara1               Batch: 4/8	Loss 153.1363 (151.9924)
2022-11-25 20:21:07,293:INFO: Dataset: zara1               Batch: 5/8	Loss 148.0121 (151.0729)
2022-11-25 20:21:07,739:INFO: Dataset: zara1               Batch: 6/8	Loss 150.8192 (151.0298)
2022-11-25 20:21:08,186:INFO: Dataset: zara1               Batch: 7/8	Loss 149.1161 (150.7674)
2022-11-25 20:21:08,624:INFO: Dataset: zara1               Batch: 8/8	Loss 129.2090 (148.4073)
2022-11-25 20:21:09,431:INFO: Dataset: zara2               Batch:  1/18	Loss 140.8291 (140.8291)
2022-11-25 20:21:09,900:INFO: Dataset: zara2               Batch:  2/18	Loss 139.8790 (140.2929)
2022-11-25 20:21:10,355:INFO: Dataset: zara2               Batch:  3/18	Loss 139.6545 (140.0881)
2022-11-25 20:21:10,811:INFO: Dataset: zara2               Batch:  4/18	Loss 139.4697 (139.9243)
2022-11-25 20:21:11,281:INFO: Dataset: zara2               Batch:  5/18	Loss 141.1397 (140.1535)
2022-11-25 20:21:11,749:INFO: Dataset: zara2               Batch:  6/18	Loss 139.3784 (140.0274)
2022-11-25 20:21:12,206:INFO: Dataset: zara2               Batch:  7/18	Loss 140.8120 (140.1394)
2022-11-25 20:21:12,716:INFO: Dataset: zara2               Batch:  8/18	Loss 141.3246 (140.2719)
2022-11-25 20:21:13,221:INFO: Dataset: zara2               Batch:  9/18	Loss 141.2085 (140.3625)
2022-11-25 20:21:13,726:INFO: Dataset: zara2               Batch: 10/18	Loss 138.2704 (140.1503)
2022-11-25 20:21:14,194:INFO: Dataset: zara2               Batch: 11/18	Loss 141.6327 (140.2821)
2022-11-25 20:21:14,668:INFO: Dataset: zara2               Batch: 12/18	Loss 138.3896 (140.1293)
2022-11-25 20:21:15,234:INFO: Dataset: zara2               Batch: 13/18	Loss 138.6387 (140.0085)
2022-11-25 20:21:15,945:INFO: Dataset: zara2               Batch: 14/18	Loss 141.1673 (140.0972)
2022-11-25 20:21:16,466:INFO: Dataset: zara2               Batch: 15/18	Loss 139.4611 (140.0546)
2022-11-25 20:21:16,943:INFO: Dataset: zara2               Batch: 16/18	Loss 139.7869 (140.0378)
2022-11-25 20:21:17,396:INFO: Dataset: zara2               Batch: 17/18	Loss 138.7558 (139.9653)
2022-11-25 20:21:17,863:INFO: Dataset: zara2               Batch: 18/18	Loss 120.1259 (138.9847)
2022-11-25 20:21:17,921:INFO: - Computing ADE (validation)
2022-11-25 20:21:18,287:INFO: 		 ADE on hotel                     dataset:	 1.7886759042739868
2022-11-25 20:21:18,716:INFO: 		 ADE on univ                      dataset:	 2.244744300842285
2022-11-25 20:21:19,077:INFO: 		 ADE on zara1                     dataset:	 2.234116792678833
2022-11-25 20:21:19,632:INFO: 		 ADE on zara2                     dataset:	 2.0084898471832275
2022-11-25 20:21:19,632:INFO: Average validation:	ADE  2.1325	FDE  3.1825
2022-11-25 20:21:19,633:INFO: - Computing ADE (validation o)
2022-11-25 20:21:19,933:INFO: 		 ADE on eth                       dataset:	 2.852149486541748
2022-11-25 20:21:19,934:INFO: Average validation o:	ADE  2.8521	FDE  4.6360
2022-11-25 20:21:19,944:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_252.pth.tar
2022-11-25 20:21:19,944:INFO: 
===> EPOCH: 253 (P3)
2022-11-25 20:21:19,944:INFO: - Computing loss (training)
2022-11-25 20:21:20,632:INFO: Dataset: hotel               Batch: 1/4	Loss 141.4973 (141.4973)
2022-11-25 20:21:21,127:INFO: Dataset: hotel               Batch: 2/4	Loss 142.9903 (142.2226)
2022-11-25 20:21:21,745:INFO: Dataset: hotel               Batch: 3/4	Loss 145.7970 (143.3758)
2022-11-25 20:21:22,181:INFO: Dataset: hotel               Batch: 4/4	Loss 84.9533 (132.9707)
2022-11-25 20:21:23,026:INFO: Dataset: univ                Batch:  1/15	Loss 130.9428 (130.9428)
2022-11-25 20:21:23,658:INFO: Dataset: univ                Batch:  2/15	Loss 132.0127 (131.4873)
2022-11-25 20:21:24,155:INFO: Dataset: univ                Batch:  3/15	Loss 132.4897 (131.7949)
2022-11-25 20:21:24,637:INFO: Dataset: univ                Batch:  4/15	Loss 133.5265 (132.1886)
2022-11-25 20:21:25,132:INFO: Dataset: univ                Batch:  5/15	Loss 132.0579 (132.1636)
2022-11-25 20:21:25,631:INFO: Dataset: univ                Batch:  6/15	Loss 133.6664 (132.4118)
2022-11-25 20:21:26,118:INFO: Dataset: univ                Batch:  7/15	Loss 132.9213 (132.4825)
2022-11-25 20:21:26,618:INFO: Dataset: univ                Batch:  8/15	Loss 131.5686 (132.3644)
2022-11-25 20:21:27,117:INFO: Dataset: univ                Batch:  9/15	Loss 134.4141 (132.5783)
2022-11-25 20:21:27,602:INFO: Dataset: univ                Batch: 10/15	Loss 134.3145 (132.7378)
2022-11-25 20:21:28,102:INFO: Dataset: univ                Batch: 11/15	Loss 132.9192 (132.7544)
2022-11-25 20:21:28,597:INFO: Dataset: univ                Batch: 12/15	Loss 130.8502 (132.5933)
2022-11-25 20:21:29,101:INFO: Dataset: univ                Batch: 13/15	Loss 132.2643 (132.5666)
2022-11-25 20:21:29,603:INFO: Dataset: univ                Batch: 14/15	Loss 132.2862 (132.5461)
2022-11-25 20:21:29,999:INFO: Dataset: univ                Batch: 15/15	Loss 24.5680 (131.0878)
2022-11-25 20:21:30,763:INFO: Dataset: zara1               Batch: 1/8	Loss 148.7790 (148.7790)
2022-11-25 20:21:31,225:INFO: Dataset: zara1               Batch: 2/8	Loss 150.2913 (149.5547)
2022-11-25 20:21:31,748:INFO: Dataset: zara1               Batch: 3/8	Loss 150.1734 (149.7446)
2022-11-25 20:21:32,194:INFO: Dataset: zara1               Batch: 4/8	Loss 148.5015 (149.4814)
2022-11-25 20:21:32,634:INFO: Dataset: zara1               Batch: 5/8	Loss 149.6064 (149.5059)
2022-11-25 20:21:33,074:INFO: Dataset: zara1               Batch: 6/8	Loss 146.8778 (149.0837)
2022-11-25 20:21:33,521:INFO: Dataset: zara1               Batch: 7/8	Loss 150.1210 (149.2134)
2022-11-25 20:21:33,960:INFO: Dataset: zara1               Batch: 8/8	Loss 127.3300 (146.6680)
2022-11-25 20:21:34,732:INFO: Dataset: zara2               Batch:  1/18	Loss 139.4981 (139.4981)
2022-11-25 20:21:35,203:INFO: Dataset: zara2               Batch:  2/18	Loss 137.4977 (138.4834)
2022-11-25 20:21:35,667:INFO: Dataset: zara2               Batch:  3/18	Loss 139.3793 (138.7625)
2022-11-25 20:21:36,129:INFO: Dataset: zara2               Batch:  4/18	Loss 137.8840 (138.5409)
2022-11-25 20:21:36,586:INFO: Dataset: zara2               Batch:  5/18	Loss 137.8825 (138.4098)
2022-11-25 20:21:37,053:INFO: Dataset: zara2               Batch:  6/18	Loss 137.2230 (138.2188)
2022-11-25 20:21:37,515:INFO: Dataset: zara2               Batch:  7/18	Loss 140.3432 (138.5344)
2022-11-25 20:21:37,969:INFO: Dataset: zara2               Batch:  8/18	Loss 137.4644 (138.3981)
2022-11-25 20:21:38,431:INFO: Dataset: zara2               Batch:  9/18	Loss 136.2083 (138.1241)
2022-11-25 20:21:38,892:INFO: Dataset: zara2               Batch: 10/18	Loss 137.3348 (138.0485)
2022-11-25 20:21:39,352:INFO: Dataset: zara2               Batch: 11/18	Loss 139.0465 (138.1369)
2022-11-25 20:21:39,823:INFO: Dataset: zara2               Batch: 12/18	Loss 138.3156 (138.1525)
2022-11-25 20:21:40,307:INFO: Dataset: zara2               Batch: 13/18	Loss 138.9676 (138.2181)
2022-11-25 20:21:40,787:INFO: Dataset: zara2               Batch: 14/18	Loss 139.0764 (138.2735)
2022-11-25 20:21:41,245:INFO: Dataset: zara2               Batch: 15/18	Loss 138.6584 (138.2990)
2022-11-25 20:21:41,713:INFO: Dataset: zara2               Batch: 16/18	Loss 137.2590 (138.2372)
2022-11-25 20:21:42,189:INFO: Dataset: zara2               Batch: 17/18	Loss 136.8113 (138.1539)
2022-11-25 20:21:42,669:INFO: Dataset: zara2               Batch: 18/18	Loss 118.3663 (137.1791)
2022-11-25 20:21:42,728:INFO: - Computing ADE (validation)
2022-11-25 20:21:43,089:INFO: 		 ADE on hotel                     dataset:	 1.7780051231384277
2022-11-25 20:21:43,533:INFO: 		 ADE on univ                      dataset:	 2.2358314990997314
2022-11-25 20:21:43,891:INFO: 		 ADE on zara1                     dataset:	 2.226811647415161
2022-11-25 20:21:44,445:INFO: 		 ADE on zara2                     dataset:	 2.0259695053100586
2022-11-25 20:21:44,446:INFO: Average validation:	ADE  2.1333	FDE  3.1521
2022-11-25 20:21:44,447:INFO: - Computing ADE (validation o)
2022-11-25 20:21:44,741:INFO: 		 ADE on eth                       dataset:	 2.921501398086548
2022-11-25 20:21:44,742:INFO: Average validation o:	ADE  2.9215	FDE  4.7134
2022-11-25 20:21:44,750:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_253.pth.tar
2022-11-25 20:21:44,750:INFO: 
===> EPOCH: 254 (P3)
2022-11-25 20:21:44,751:INFO: - Computing loss (training)
2022-11-25 20:21:45,429:INFO: Dataset: hotel               Batch: 1/4	Loss 141.1317 (141.1317)
2022-11-25 20:21:45,873:INFO: Dataset: hotel               Batch: 2/4	Loss 142.7788 (141.9177)
2022-11-25 20:21:46,313:INFO: Dataset: hotel               Batch: 3/4	Loss 142.9449 (142.2606)
2022-11-25 20:21:46,712:INFO: Dataset: hotel               Batch: 4/4	Loss 86.1120 (132.4828)
2022-11-25 20:21:47,489:INFO: Dataset: univ                Batch:  1/15	Loss 131.8326 (131.8326)
2022-11-25 20:21:47,998:INFO: Dataset: univ                Batch:  2/15	Loss 130.5465 (131.1328)
2022-11-25 20:21:48,495:INFO: Dataset: univ                Batch:  3/15	Loss 131.2448 (131.1730)
2022-11-25 20:21:48,987:INFO: Dataset: univ                Batch:  4/15	Loss 130.2337 (130.9360)
2022-11-25 20:21:49,471:INFO: Dataset: univ                Batch:  5/15	Loss 133.7094 (131.4379)
2022-11-25 20:21:49,953:INFO: Dataset: univ                Batch:  6/15	Loss 130.3918 (131.2663)
2022-11-25 20:21:50,450:INFO: Dataset: univ                Batch:  7/15	Loss 130.4480 (131.1391)
2022-11-25 20:21:50,952:INFO: Dataset: univ                Batch:  8/15	Loss 129.8344 (130.9584)
2022-11-25 20:21:51,442:INFO: Dataset: univ                Batch:  9/15	Loss 131.8164 (131.0519)
2022-11-25 20:21:51,940:INFO: Dataset: univ                Batch: 10/15	Loss 131.8751 (131.1354)
2022-11-25 20:21:52,443:INFO: Dataset: univ                Batch: 11/15	Loss 131.1409 (131.1359)
2022-11-25 20:21:52,938:INFO: Dataset: univ                Batch: 12/15	Loss 131.1742 (131.1393)
2022-11-25 20:21:53,443:INFO: Dataset: univ                Batch: 13/15	Loss 131.3334 (131.1549)
2022-11-25 20:21:53,952:INFO: Dataset: univ                Batch: 14/15	Loss 130.4811 (131.1033)

2022-10-10 17:11:52,209:INFO: Initializing Training Set
2022-10-10 17:11:56,348:INFO: Initializing Validation Set
2022-10-10 17:11:56,849:INFO: Initializing Validation O Set
2022-10-10 18:19:07,365:INFO: Initializing Training Set
2022-10-10 18:34:23,510:INFO: Initializing Validation Set
2022-10-10 18:52:50,933:INFO: Epoch: 1
2022-10-10 18:52:57,281:INFO: Training
2022-10-10 19:06:37,881:INFO: Initializing Training Set
2022-10-10 19:06:41,929:INFO: Initializing Validation Set
2022-10-10 19:06:49,326:INFO: Epoch: 1
2022-10-10 19:06:49,326:INFO: Training
2022-10-10 19:50:28,168:INFO: Initializing Training Set
2022-10-10 19:50:32,277:INFO: Initializing Validation Set
2022-10-10 19:50:36,039:INFO: Epoch: 1
2022-10-10 19:50:36,039:INFO: Training
2022-10-10 20:18:51,561:INFO: Initializing Training Set
2022-10-10 20:18:56,014:INFO: Initializing Validation Set
2022-10-10 20:18:59,881:INFO: Epoch: 1
2022-10-10 20:18:59,881:INFO: Training
2022-10-10 20:20:22,185:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6052 (26.1956)
2022-10-10 20:20:26,916:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 19.8264 (11.0754)
2022-10-10 20:20:31,686:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7485 (6.3209)
2022-10-10 20:20:36,169:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 7.4369 (11.6256)
2022-10-10 20:20:40,891:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8907 (7.2735)
2022-10-10 20:20:45,615:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6203 (4.4957)
2022-10-10 20:20:50,106:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.8407 (21.2507)
2022-10-10 20:20:50,107:INFO: Epoch: 2
2022-10-10 20:20:50,107:INFO: Training
2022-10-10 20:20:54,712:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.5462 (4.6577)
2022-10-10 20:20:59,477:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.2075 (10.5697)
2022-10-10 20:21:04,322:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7848 (5.6287)
2022-10-10 20:21:09,075:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6260 (8.8023)
2022-10-10 20:21:13,547:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9725 (4.4283)
2022-10-10 20:21:18,264:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6637 (4.9003)
2022-10-10 20:21:22,788:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 9.3605 (16.9451)
2022-10-10 20:21:22,789:INFO: Epoch: 3
2022-10-10 20:21:22,789:INFO: Training
2022-10-10 20:21:27,357:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 6.1286 (4.9918)
2022-10-10 20:21:31,987:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.3700 (9.4552)
2022-10-10 20:21:36,838:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.8075 (6.0976)
2022-10-10 20:21:41,559:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6568 (8.7449)
2022-10-10 20:21:46,281:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9789 (3.2743)
2022-10-10 20:21:51,170:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6980 (4.7719)
2022-10-10 20:21:55,371:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.6709 (14.6987)
2022-10-10 20:21:55,372:INFO: Epoch: 4
2022-10-10 20:21:55,372:INFO: Training
2022-10-10 20:21:59,951:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.6349 (4.7534)
2022-10-10 20:22:04,726:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.8596 (9.4733)
2022-10-10 20:22:09,621:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.8075 (5.7936)
2022-10-10 20:22:14,448:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6174 (8.7719)
2022-10-10 20:22:19,157:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9835 (3.1901)
2022-10-10 20:22:23,864:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.7059 (4.7627)
2022-10-10 20:22:27,998:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.4214 (13.9290)
2022-10-10 20:22:27,999:INFO: Epoch: 5
2022-10-10 20:22:28,000:INFO: Training
2022-10-10 20:22:32,445:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9809 (4.6624)
2022-10-10 20:22:37,824:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.7621 (9.4805)
2022-10-10 20:22:42,882:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7985 (5.6714)
2022-10-10 20:22:47,657:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6349 (8.7112)
2022-10-10 20:22:52,362:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9796 (3.1594)
2022-10-10 20:22:57,066:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.7029 (4.7893)
2022-10-10 20:23:01,594:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.1461 (13.4549)
2022-10-10 20:23:01,595:INFO: Epoch: 6
2022-10-10 20:23:01,595:INFO: Training
2022-10-10 20:23:05,971:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0438 (4.6814)
2022-10-10 20:23:10,833:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.3357 (9.5614)
2022-10-10 20:23:15,753:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.8039 (5.6442)
2022-10-10 20:23:20,474:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6638 (8.6886)
2022-10-10 20:23:25,004:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9746 (3.0941)
2022-10-10 20:23:29,802:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.7021 (4.7616)
2022-10-10 20:23:34,407:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.1375 (13.4579)
2022-10-10 20:23:34,408:INFO: Epoch: 7
2022-10-10 20:23:34,408:INFO: Training
2022-10-10 20:23:38,860:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.8642 (4.6428)
2022-10-10 20:23:43,553:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.3938 (9.5975)
2022-10-10 20:23:48,469:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7933 (5.6814)
2022-10-10 20:23:53,141:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6300 (8.7739)
2022-10-10 20:23:58,020:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9748 (2.9968)
2022-10-10 20:24:02,778:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6973 (4.7337)
2022-10-10 20:24:07,247:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.1296 (13.4603)
2022-10-10 20:24:07,249:INFO: Epoch: 8
2022-10-10 20:24:07,249:INFO: Training
2022-10-10 20:24:11,582:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9117 (4.5896)
2022-10-10 20:24:16,342:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.4071 (9.6759)
2022-10-10 20:24:21,143:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7920 (5.6217)
2022-10-10 20:24:25,920:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6380 (8.8295)
2022-10-10 20:24:30,524:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9631 (2.9786)
2022-10-10 20:24:35,293:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6891 (4.7423)
2022-10-10 20:24:39,831:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.1314 (13.2006)
2022-10-10 20:24:39,832:INFO: Epoch: 9
2022-10-10 20:24:39,832:INFO: Training
2022-10-10 20:24:44,264:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0008 (4.6494)
2022-10-10 20:24:49,003:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.2617 (9.6822)
2022-10-10 20:24:54,269:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7887 (5.6278)
2022-10-10 20:24:59,000:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6075 (8.8207)
2022-10-10 20:25:03,644:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9639 (2.9566)
2022-10-10 20:25:08,344:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6804 (4.6880)
2022-10-10 20:25:12,838:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.0985 (13.1572)
2022-10-10 20:25:12,839:INFO: Epoch: 10
2022-10-10 20:25:12,840:INFO: Training
2022-10-10 20:25:17,303:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9710 (4.6308)
2022-10-10 20:25:22,247:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.2326 (9.8250)
2022-10-10 20:25:27,172:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7764 (5.6758)
2022-10-10 20:25:31,847:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5919 (8.9134)
2022-10-10 20:25:36,515:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9526 (2.9233)
2022-10-10 20:25:41,180:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6592 (4.7191)
2022-10-10 20:25:45,712:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.1837 (12.8093)
2022-10-10 20:25:45,713:INFO: Epoch: 11
2022-10-10 20:25:45,713:INFO: Training
2022-10-10 20:25:50,046:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0292 (4.7146)
2022-10-10 20:25:54,793:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.9158 (9.9279)
2022-10-10 20:25:59,569:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7761 (5.6337)
2022-10-10 20:26:04,245:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5632 (8.8412)
2022-10-10 20:26:09,151:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9534 (2.9247)
2022-10-10 20:26:13,971:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6474 (4.6800)
2022-10-10 20:26:18,204:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.2793 (12.8777)
2022-10-10 20:26:18,206:INFO: Epoch: 12
2022-10-10 20:26:18,206:INFO: Training
2022-10-10 20:26:22,539:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9667 (4.7061)
2022-10-10 20:26:27,359:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.3113 (9.9597)
2022-10-10 20:26:32,481:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7615 (5.5439)
2022-10-10 20:26:37,392:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5699 (9.2272)
2022-10-10 20:26:42,274:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9307 (2.8868)
2022-10-10 20:26:46,870:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6595 (4.7680)
2022-10-10 20:26:51,159:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.3267 (12.9478)
2022-10-10 20:26:51,160:INFO: Epoch: 13
2022-10-10 20:26:51,160:INFO: Training
2022-10-10 20:26:55,698:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9255 (4.6678)
2022-10-10 20:27:00,604:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.0101 (9.8163)
2022-10-10 20:27:05,817:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7656 (5.4716)
2022-10-10 20:27:10,472:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5596 (8.8681)
2022-10-10 20:27:14,873:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9180 (3.0124)
2022-10-10 20:27:19,837:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6526 (4.7696)
2022-10-10 20:27:24,306:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.3118 (12.8850)
2022-10-10 20:27:24,307:INFO: Epoch: 14
2022-10-10 20:27:24,307:INFO: Training
2022-10-10 20:27:28,788:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0003 (4.8098)
2022-10-10 20:27:33,697:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.5794 (9.7879)
2022-10-10 20:27:38,742:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7642 (5.4809)
2022-10-10 20:27:43,404:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5929 (8.8242)
2022-10-10 20:27:48,037:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9236 (2.9176)
2022-10-10 20:27:52,512:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6590 (4.7885)
2022-10-10 20:27:56,814:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.4156 (12.8651)
2022-10-10 20:27:56,815:INFO: Epoch: 15
2022-10-10 20:27:56,815:INFO: Training
2022-10-10 20:28:01,421:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0713 (4.7977)
2022-10-10 20:28:06,368:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.3442 (9.8657)
2022-10-10 20:28:11,243:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7604 (5.4984)
2022-10-10 20:28:16,037:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5084 (8.5990)
2022-10-10 20:28:20,659:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9183 (2.8393)
2022-10-10 20:28:25,365:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6286 (4.6241)
2022-10-10 20:28:30,295:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.2542 (12.7436)
2022-10-10 20:28:30,297:INFO: Epoch: 16
2022-10-10 20:28:30,297:INFO: Training
2022-10-10 20:28:35,009:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.1321 (4.7819)
2022-10-10 20:28:39,862:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.5535 (9.9261)
2022-10-10 20:28:44,742:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7489 (5.5847)
2022-10-10 20:28:49,480:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.4316 (8.6355)
2022-10-10 20:28:54,122:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9317 (2.8010)
2022-10-10 20:28:58,710:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5768 (4.5153)
2022-10-10 20:29:03,193:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.2973 (12.6904)
2022-10-10 20:29:03,195:INFO: Epoch: 17
2022-10-10 20:29:03,195:INFO: Training
2022-10-10 20:29:07,531:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9143 (4.5733)
2022-10-10 20:29:12,273:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.7220 (9.4681)
2022-10-10 20:29:17,788:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7288 (5.1137)
2022-10-10 20:29:22,589:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.2141 (9.1392)
2022-10-10 20:29:27,234:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9133 (3.0258)
2022-10-10 20:29:31,766:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5145 (5.1110)
2022-10-10 20:29:35,962:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.7072 (12.8477)
2022-10-10 20:29:35,963:INFO: Epoch: 18
2022-10-10 20:29:35,964:INFO: Training
2022-10-10 20:29:40,633:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 6.0459 (5.3885)
2022-10-10 20:29:45,467:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.0516 (10.2862)
2022-10-10 20:29:50,626:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7491 (5.7610)
2022-10-10 20:29:55,314:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5694 (8.6204)
2022-10-10 20:30:00,024:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9193 (3.3226)
2022-10-10 20:30:04,998:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6331 (4.8006)
2022-10-10 20:30:09,270:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.0954 (12.6632)
2022-10-10 20:30:09,270:INFO: Epoch: 19
2022-10-10 20:30:09,270:INFO: Training
2022-10-10 20:30:13,894:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.1807 (4.7617)
2022-10-10 20:30:18,718:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.7249 (9.8854)
2022-10-10 20:30:23,513:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7549 (5.4650)
2022-10-10 20:30:28,269:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.4715 (8.5390)
2022-10-10 20:30:33,170:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9218 (2.8009)
2022-10-10 20:30:38,492:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5979 (4.5798)
2022-10-10 20:30:42,971:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.5453 (12.9515)
2022-10-10 20:30:42,973:INFO: Epoch: 20
2022-10-10 20:30:42,973:INFO: Training
2022-10-10 20:30:48,359:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.1842 (4.8360)
2022-10-10 20:30:53,518:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.3671 (9.9401)
2022-10-10 20:30:58,472:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7418 (5.4009)
2022-10-10 20:31:03,135:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.4415 (8.6507)
2022-10-10 20:31:07,888:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9014 (2.7442)
2022-10-10 20:31:12,506:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5937 (4.5150)
2022-10-10 20:31:16,891:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.2729 (12.9708)
2022-10-10 20:31:16,891:INFO: Epoch: 21
2022-10-10 20:31:16,893:INFO: Training
2022-10-10 20:31:21,287:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0864 (4.8248)
2022-10-10 20:31:25,664:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.5317 (9.4855)
2022-10-10 20:31:30,583:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7209 (5.3005)
2022-10-10 20:31:35,220:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.2263 (8.2703)
2022-10-10 20:31:39,772:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8932 (2.7275)
2022-10-10 20:31:44,284:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.4632 (4.3882)
2022-10-10 20:31:48,365:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.2327 (12.4666)
2022-10-10 20:31:48,366:INFO: Epoch: 22
2022-10-10 20:31:48,366:INFO: Training
2022-10-10 20:31:52,605:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9292 (4.7057)
2022-10-10 20:31:57,284:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.1934 (9.3847)
2022-10-10 20:32:02,174:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6927 (5.1282)
2022-10-10 20:32:06,906:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.5033 (9.3712)
2022-10-10 20:32:11,426:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8496 (3.3023)
2022-10-10 20:32:16,077:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.4425 (4.4161)
2022-10-10 20:32:20,145:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.7985 (12.0029)
2022-10-10 20:32:20,146:INFO: Epoch: 23
2022-10-10 20:32:20,146:INFO: Training
2022-10-10 20:32:24,595:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7971 (4.5184)
2022-10-10 20:32:29,133:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.1617 (9.1459)
2022-10-10 20:32:33,755:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6832 (5.1543)
2022-10-10 20:32:38,303:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 6.0801 (9.8645)
2022-10-10 20:32:42,900:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8141 (3.2843)
2022-10-10 20:32:47,282:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.4795 (4.2957)
2022-10-10 20:32:51,396:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.8972 (12.2368)
2022-10-10 20:32:51,397:INFO: Epoch: 24
2022-10-10 20:32:51,397:INFO: Training
2022-10-10 20:32:55,800:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.8633 (4.6044)
2022-10-10 20:33:00,436:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.2119 (9.2008)
2022-10-10 20:33:05,194:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6709 (5.2155)
2022-10-10 20:33:09,592:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.1696 (8.8684)
2022-10-10 20:33:14,118:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7874 (2.7512)
2022-10-10 20:33:18,684:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.3946 (4.2622)
2022-10-10 20:33:22,802:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.5798 (12.0333)
2022-10-10 20:33:22,805:INFO: Epoch: 25
2022-10-10 20:33:22,805:INFO: Training
2022-10-10 20:33:27,216:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.1921 (5.0042)
2022-10-10 20:33:31,907:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 15.4478 (10.1569)
2022-10-10 20:33:36,741:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6927 (5.7994)
2022-10-10 20:33:41,372:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6620 (9.7094)
2022-10-10 20:33:45,899:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8181 (3.1311)
2022-10-10 20:33:50,565:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5065 (4.4504)
2022-10-10 20:33:54,748:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.3002 (12.8199)
2022-10-10 20:33:54,749:INFO: Epoch: 26
2022-10-10 20:33:54,749:INFO: Training
2022-10-10 20:33:59,154:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.2106 (5.0215)
2022-10-10 20:34:03,832:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.2202 (9.6909)
2022-10-10 20:34:08,546:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6991 (5.3439)
2022-10-10 20:34:13,001:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.3861 (8.1391)
2022-10-10 20:34:17,555:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9271 (2.9263)
2022-10-10 20:34:22,117:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5363 (4.5487)
2022-10-10 20:34:26,190:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.1305 (12.3913)
2022-10-10 20:34:26,191:INFO: Epoch: 27
2022-10-10 20:34:26,191:INFO: Training
2022-10-10 20:34:30,627:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0038 (4.6701)
2022-10-10 20:34:35,197:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.3979 (9.5330)
2022-10-10 20:34:39,909:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6724 (5.2413)
2022-10-10 20:34:44,447:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.1924 (8.3851)
2022-10-10 20:34:48,790:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9019 (2.7511)
2022-10-10 20:34:53,673:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.5114 (4.3868)
2022-10-10 20:34:57,727:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.8810 (12.4951)
2022-10-10 20:34:57,728:INFO: Epoch: 28
2022-10-10 20:34:57,729:INFO: Training
2022-10-10 20:35:02,143:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9596 (4.5876)
2022-10-10 20:35:06,802:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.4246 (9.3750)
2022-10-10 20:35:11,432:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6651 (5.2413)
2022-10-10 20:35:16,008:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.1237 (8.1385)
2022-10-10 20:35:20,687:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8570 (2.6068)
2022-10-10 20:35:25,264:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.4507 (4.2845)
2022-10-10 20:35:29,612:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.0774 (12.2309)
2022-10-10 20:35:29,614:INFO: Epoch: 29
2022-10-10 20:35:29,615:INFO: Training
2022-10-10 20:35:34,057:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0762 (4.7983)
2022-10-10 20:35:38,702:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 9.8837 (8.6857)
2022-10-10 20:35:43,290:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6399 (4.8218)
2022-10-10 20:35:47,855:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.7998 (7.4238)
2022-10-10 20:35:52,371:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8311 (2.8113)
2022-10-10 20:35:56,773:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.2366 (4.2794)
2022-10-10 20:36:01,147:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.2741 (11.7320)
2022-10-10 20:36:01,148:INFO: Epoch: 30
2022-10-10 20:36:01,148:INFO: Training
2022-10-10 20:36:05,376:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.0793 (4.9097)
2022-10-10 20:36:10,022:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 16.0830 (11.1276)
2022-10-10 20:36:14,643:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6354 (5.4171)
2022-10-10 20:36:19,294:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.9376 (8.1097)
2022-10-10 20:36:23,851:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8279 (2.5440)
2022-10-10 20:36:28,490:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.3673 (4.1874)
2022-10-10 20:36:32,836:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.7581 (11.9571)
2022-10-10 20:36:32,837:INFO: Epoch: 31
2022-10-10 20:36:32,838:INFO: Training
2022-10-10 20:36:37,249:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.1746 (4.9359)
2022-10-10 20:36:41,955:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 10.9854 (8.6929)
2022-10-10 20:36:46,654:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6191 (4.7441)
2022-10-10 20:36:51,134:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.9881 (7.2667)
2022-10-10 20:36:55,501:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8120 (2.8588)
2022-10-10 20:37:00,938:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.1555 (4.1580)
2022-10-10 20:37:05,380:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.3187 (11.6892)
2022-10-10 20:37:05,381:INFO: Epoch: 32
2022-10-10 20:37:05,381:INFO: Training
2022-10-10 20:37:09,870:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.2553 (4.9767)
2022-10-10 20:37:14,475:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 15.3843 (11.0108)
2022-10-10 20:37:19,424:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6274 (5.2273)
2022-10-10 20:37:24,490:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.9057 (7.5564)
2022-10-10 20:37:29,040:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8538 (2.6927)
2022-10-10 20:37:33,676:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.2760 (4.1127)
2022-10-10 20:37:38,066:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.5450 (11.6160)
2022-10-10 20:37:38,067:INFO: Epoch: 33
2022-10-10 20:37:38,067:INFO: Training
2022-10-10 20:37:42,464:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.2242 (4.8869)
2022-10-10 20:37:46,896:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.7403 (8.5121)
2022-10-10 20:37:51,535:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6004 (4.5588)
2022-10-10 20:37:55,959:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.3706 (7.2026)
2022-10-10 20:38:00,578:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7411 (2.5016)
2022-10-10 20:38:05,197:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.0430 (4.0269)
2022-10-10 20:38:09,641:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.3149 (11.0228)
2022-10-10 20:38:09,642:INFO: Epoch: 34
2022-10-10 20:38:09,642:INFO: Training
2022-10-10 20:38:14,033:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.4207 (5.1023)
2022-10-10 20:38:18,660:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.6993 (8.7150)
2022-10-10 20:38:23,521:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6276 (4.6343)
2022-10-10 20:38:27,955:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.4654 (8.3367)
2022-10-10 20:38:32,683:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8426 (3.2164)
2022-10-10 20:38:37,326:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.0817 (4.9150)
2022-10-10 20:38:41,690:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.8112 (11.1454)
2022-10-10 20:38:41,691:INFO: Epoch: 35
2022-10-10 20:38:41,691:INFO: Training
2022-10-10 20:38:46,591:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.5638 (5.0843)
2022-10-10 20:38:51,204:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.0722 (9.6106)
2022-10-10 20:38:55,948:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5756 (5.0009)
2022-10-10 20:39:00,495:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.4290 (8.0269)
2022-10-10 20:39:05,079:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7369 (2.3518)
2022-10-10 20:39:09,864:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.0871 (3.7965)
2022-10-10 20:39:14,085:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.6955 (11.0870)
2022-10-10 20:39:14,087:INFO: Epoch: 36
2022-10-10 20:39:14,087:INFO: Training
2022-10-10 20:39:18,513:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.2273 (4.9182)
2022-10-10 20:39:23,105:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.4234 (7.8634)
2022-10-10 20:39:27,963:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5487 (4.1920)
2022-10-10 20:39:32,386:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.5488 (6.5569)
2022-10-10 20:39:36,749:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7470 (3.0763)
2022-10-10 20:39:41,422:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.9005 (4.1310)
2022-10-10 20:39:45,619:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.8880 (10.6024)
2022-10-10 20:39:45,620:INFO: Epoch: 37
2022-10-10 20:39:45,620:INFO: Training
2022-10-10 20:39:49,994:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.5283 (5.0556)
2022-10-10 20:39:54,445:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 7.5521 (7.8352)
2022-10-10 20:39:59,385:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5659 (4.1857)
2022-10-10 20:40:03,983:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.7403 (6.1415)
2022-10-10 20:40:08,676:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7614 (3.2606)
2022-10-10 20:40:13,469:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.9551 (3.5853)
2022-10-10 20:40:18,063:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.3251 (11.0890)
2022-10-10 20:40:18,064:INFO: Epoch: 38
2022-10-10 20:40:18,065:INFO: Training
2022-10-10 20:40:22,453:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.8546 (4.5501)
2022-10-10 20:40:27,317:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 9.3993 (6.6546)
2022-10-10 20:40:32,429:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5540 (3.9869)
2022-10-10 20:40:37,195:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 3.6516 (9.1911)
2022-10-10 20:40:41,857:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7543 (4.1600)
2022-10-10 20:40:46,777:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.8640 (4.1853)
2022-10-10 20:40:51,219:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.3260 (10.4456)
2022-10-10 20:40:51,219:INFO: Epoch: 39
2022-10-10 20:40:51,220:INFO: Training
2022-10-10 20:40:55,773:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.9921 (4.7263)
2022-10-10 20:41:00,499:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 6.0544 (7.3737)
2022-10-10 20:41:05,798:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5965 (4.1258)
2022-10-10 20:41:10,679:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.2218 (6.1329)
2022-10-10 20:41:15,588:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.6961 (3.1224)
2022-10-10 20:41:20,149:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.9089 (3.3325)
2022-10-10 20:41:24,389:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.2032 (10.2966)
2022-10-10 20:41:24,389:INFO: Epoch: 40
2022-10-10 20:41:24,390:INFO: Training
2022-10-10 20:41:28,875:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.4546 (4.8447)
2022-10-10 20:41:33,571:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.2666 (6.3167)
2022-10-10 20:41:38,310:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5818 (3.9452)
2022-10-10 20:41:42,835:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.4740 (12.6146)
2022-10-10 20:41:47,735:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9015 (2.9392)
2022-10-10 20:41:52,252:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.7818 (6.6546)
2022-10-10 20:41:57,138:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.6531 (10.0778)
2022-10-10 20:41:57,139:INFO: Epoch: 41
2022-10-10 20:41:57,140:INFO: Training
2022-10-10 20:42:01,630:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.1126 (4.6426)
2022-10-10 20:42:06,346:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 6.4710 (8.1009)
2022-10-10 20:42:11,324:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.6205 (4.8462)
2022-10-10 20:42:16,028:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.0103 (9.0586)
2022-10-10 20:42:20,610:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8884 (4.2991)
2022-10-10 20:42:25,336:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.0208 (4.2801)
2022-10-10 20:42:29,495:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.9800 (10.8973)
2022-10-10 20:42:29,497:INFO: Epoch: 42
2022-10-10 20:42:29,497:INFO: Training
2022-10-10 20:42:33,844:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.8695 (5.5568)
2022-10-10 20:42:38,361:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 10.6993 (8.3962)
2022-10-10 20:42:43,684:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5542 (5.3616)
2022-10-10 20:42:48,372:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.5634 (6.3320)
2022-10-10 20:42:52,951:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7478 (2.4821)
2022-10-10 20:42:57,352:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.8848 (3.7674)
2022-10-10 20:43:01,486:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.7265 (10.3072)
2022-10-10 20:43:01,488:INFO: Epoch: 43
2022-10-10 20:43:01,489:INFO: Training
2022-10-10 20:43:05,834:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.8849 (5.4085)
2022-10-10 20:43:10,563:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.4610 (7.9767)
2022-10-10 20:43:15,181:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.5349 (4.3025)
2022-10-10 20:43:19,784:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 3.9099 (8.1151)
2022-10-10 20:43:24,367:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.6875 (2.5592)
2022-10-10 20:43:29,087:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.7424 (3.9380)
2022-10-10 20:43:33,331:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 7.5118 (11.0579)
2022-10-10 20:43:33,332:INFO: Epoch: 44
2022-10-10 20:43:33,332:INFO: Training
2022-10-10 20:43:38,156:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.3855 (4.7547)
2022-10-10 20:43:42,950:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 10.6577 (8.3746)
2022-10-10 20:43:48,359:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.4225 (4.1654)
2022-10-10 20:43:53,562:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 4.1732 (5.9829)
2022-10-10 20:43:58,125:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.6163 (1.9451)
2022-10-10 20:44:02,602:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.6325 (3.1375)
2022-10-10 20:44:07,016:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 6.1024 (9.4068)
2022-10-10 20:44:07,017:INFO: Epoch: 45
2022-10-10 20:44:07,017:INFO: Training
2022-10-10 20:44:11,476:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.2632 (3.8935)
2022-10-10 20:44:16,162:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.6752 (5.7365)
2022-10-10 20:44:21,027:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.4005 (3.1480)
2022-10-10 20:44:25,507:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 3.0333 (5.0152)
2022-10-10 20:44:30,045:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.7217 (2.8938)
2022-10-10 20:44:34,822:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.5287 (3.1910)
2022-10-10 20:44:39,303:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 5.7413 (10.2909)
2022-10-10 20:44:39,305:INFO: Epoch: 46
2022-10-10 20:44:39,306:INFO: Training
2022-10-10 20:44:44,191:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.3123 (3.8702)
2022-10-10 20:44:49,412:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.3633 (5.2156)
2022-10-10 20:44:54,935:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.3691 (2.7461)
2022-10-10 20:44:59,715:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.6722 (4.4858)
2022-10-10 20:45:04,424:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.5800 (2.1981)
2022-10-10 20:45:09,306:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.4023 (2.6225)
2022-10-10 20:45:13,478:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 5.4761 (9.7320)
2022-10-10 20:45:13,479:INFO: Epoch: 47
2022-10-10 20:45:13,480:INFO: Training
2022-10-10 20:45:17,949:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.0597 (3.8297)
2022-10-10 20:45:22,676:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.3936 (6.3572)
2022-10-10 20:45:28,215:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.3568 (2.7797)
2022-10-10 20:45:32,711:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.6668 (4.1861)
2022-10-10 20:45:37,122:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.5365 (1.8369)
2022-10-10 20:45:41,665:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.2812 (2.4893)
2022-10-10 20:45:46,299:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 5.0054 (8.8839)
2022-10-10 20:45:46,300:INFO: Epoch: 48
2022-10-10 20:45:46,301:INFO: Training
2022-10-10 20:45:51,026:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.0283 (3.8191)
2022-10-10 20:45:55,696:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.9224 (5.2122)
2022-10-10 20:46:00,425:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.3421 (2.4858)
2022-10-10 20:46:05,108:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.5449 (4.1453)
2022-10-10 20:46:09,607:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4841 (1.8694)
2022-10-10 20:46:14,293:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.2384 (2.2994)
2022-10-10 20:46:18,768:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 4.2873 (8.9924)
2022-10-10 20:46:18,769:INFO: Epoch: 49
2022-10-10 20:46:18,769:INFO: Training
2022-10-10 20:46:23,247:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.0244 (3.8188)
2022-10-10 20:46:28,604:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.0096 (7.2870)
2022-10-10 20:46:33,590:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.3250 (2.8252)
2022-10-10 20:46:38,279:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.6188 (3.9865)
2022-10-10 20:46:42,896:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4985 (2.1675)
2022-10-10 20:46:47,433:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.1561 (2.5074)
2022-10-10 20:46:51,858:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 4.4340 (8.0803)
2022-10-10 20:46:51,859:INFO: Epoch: 50
2022-10-10 20:46:51,859:INFO: Training
2022-10-10 20:46:56,114:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.3827 (3.9277)
2022-10-10 20:47:00,875:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 7.9239 (5.0132)
2022-10-10 20:47:05,837:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2731 (2.3455)
2022-10-10 20:47:10,596:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.3896 (3.4934)
2022-10-10 20:47:15,238:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.5187 (1.8439)
2022-10-10 20:47:19,771:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.0786 (2.6676)
2022-10-10 20:47:23,878:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 4.1588 (9.7252)
2022-10-10 20:47:23,878:INFO: Epoch: 51
2022-10-10 20:47:23,879:INFO: Training
2022-10-10 20:47:28,299:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.2707 (3.8294)
2022-10-10 20:47:32,913:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 13.9296 (6.4532)
2022-10-10 20:47:37,835:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2565 (2.7040)
2022-10-10 20:47:42,419:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.3828 (3.5949)
2022-10-10 20:47:46,968:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4908 (1.6685)
2022-10-10 20:47:51,373:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 1.0149 (2.6546)
2022-10-10 20:47:55,543:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 4.0366 (8.2268)
2022-10-10 20:47:55,546:INFO: Epoch: 52
2022-10-10 20:47:55,547:INFO: Training
2022-10-10 20:48:00,001:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.3518 (3.8704)
2022-10-10 20:48:04,695:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.5761 (4.6660)
2022-10-10 20:48:09,844:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2404 (2.3375)
2022-10-10 20:48:14,438:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.2844 (3.2553)
2022-10-10 20:48:19,044:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4830 (1.6713)
2022-10-10 20:48:23,821:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.9707 (2.4742)
2022-10-10 20:48:28,150:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.7828 (8.7292)
2022-10-10 20:48:28,152:INFO: Epoch: 53
2022-10-10 20:48:28,152:INFO: Training
2022-10-10 20:48:32,705:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.1659 (3.8038)
2022-10-10 20:48:37,481:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 12.7497 (5.7476)
2022-10-10 20:48:42,511:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2353 (2.6169)
2022-10-10 20:48:47,261:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.3965 (3.6514)
2022-10-10 20:48:51,568:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4543 (1.5200)
2022-10-10 20:48:56,326:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.9050 (2.5165)
2022-10-10 20:49:00,851:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.7345 (7.4933)
2022-10-10 20:49:00,853:INFO: Epoch: 54
2022-10-10 20:49:00,854:INFO: Training
2022-10-10 20:49:05,476:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.4122 (3.9343)
2022-10-10 20:49:10,358:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 6.9115 (4.1270)
2022-10-10 20:49:15,460:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2250 (2.0796)
2022-10-10 20:49:20,010:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.0560 (2.8854)
2022-10-10 20:49:24,479:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4448 (1.5620)
2022-10-10 20:49:29,147:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.8640 (2.1227)
2022-10-10 20:49:33,740:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.3852 (8.5737)
2022-10-10 20:49:33,741:INFO: Epoch: 55
2022-10-10 20:49:33,742:INFO: Training
2022-10-10 20:49:38,162:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.1896 (3.8273)
2022-10-10 20:49:42,823:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.4311 (6.4267)
2022-10-10 20:49:47,886:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2224 (2.8729)
2022-10-10 20:49:52,836:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.3652 (3.2072)
2022-10-10 20:49:57,449:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4315 (1.5191)
2022-10-10 20:50:02,110:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.8584 (2.0885)
2022-10-10 20:50:06,520:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.5102 (7.3743)
2022-10-10 20:50:06,521:INFO: Epoch: 56
2022-10-10 20:50:06,522:INFO: Training
2022-10-10 20:50:10,965:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.5080 (3.9966)
2022-10-10 20:50:15,794:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.9613 (4.7102)
2022-10-10 20:50:20,975:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2093 (2.6666)
2022-10-10 20:50:25,594:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.4918 (3.8503)
2022-10-10 20:50:30,271:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4264 (1.3888)
2022-10-10 20:50:34,917:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7990 (2.0236)
2022-10-10 20:50:39,396:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.3557 (7.2587)
2022-10-10 20:50:39,397:INFO: Epoch: 57
2022-10-10 20:50:39,397:INFO: Training
2022-10-10 20:50:43,896:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.2261 (3.8699)
2022-10-10 20:50:48,421:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.8750 (4.6063)
2022-10-10 20:50:53,318:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2046 (2.6570)
2022-10-10 20:50:57,728:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.3877 (3.7919)
2022-10-10 20:51:02,359:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4127 (1.3605)
2022-10-10 20:51:07,021:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7770 (1.9215)
2022-10-10 20:51:11,434:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.2463 (7.3626)
2022-10-10 20:51:11,435:INFO: Epoch: 58
2022-10-10 20:51:11,435:INFO: Training
2022-10-10 20:51:16,112:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.3486 (3.8892)
2022-10-10 20:51:20,899:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 7.3293 (4.3818)
2022-10-10 20:51:26,198:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2061 (2.6340)
2022-10-10 20:51:31,029:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.1018 (3.4313)
2022-10-10 20:51:35,866:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4100 (1.3823)
2022-10-10 20:51:40,505:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7680 (1.8271)
2022-10-10 20:51:44,751:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.2144 (8.6266)
2022-10-10 20:51:44,752:INFO: Epoch: 59
2022-10-10 20:51:44,753:INFO: Training
2022-10-10 20:51:49,112:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.2746 (3.8948)
2022-10-10 20:51:53,991:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 9.0309 (5.4910)
2022-10-10 20:51:59,086:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2037 (3.0850)
2022-10-10 20:52:03,866:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 2.2675 (3.5736)
2022-10-10 20:52:08,461:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4206 (1.4824)
2022-10-10 20:52:13,261:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.8245 (1.8262)
2022-10-10 20:52:17,415:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.8004 (10.1468)
2022-10-10 20:52:17,417:INFO: Epoch: 60
2022-10-10 20:52:17,418:INFO: Training
2022-10-10 20:52:22,012:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.2989 (4.0281)
2022-10-10 20:52:26,946:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.6213 (4.6568)
2022-10-10 20:52:32,042:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2095 (3.0107)
2022-10-10 20:52:36,664:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.8752 (2.6004)
2022-10-10 20:52:41,301:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4410 (1.9728)
2022-10-10 20:52:46,026:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.8605 (1.8342)
2022-10-10 20:52:50,521:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.8893 (11.4329)
2022-10-10 20:52:50,523:INFO: Epoch: 61
2022-10-10 20:52:50,523:INFO: Training
2022-10-10 20:52:55,035:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.4360 (3.9619)
2022-10-10 20:52:59,861:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 8.1667 (5.0469)
2022-10-10 20:53:04,930:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2100 (2.6502)
2022-10-10 20:53:09,683:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.9728 (3.0417)
2022-10-10 20:53:14,392:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4262 (1.5014)
2022-10-10 20:53:18,924:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.8485 (1.8356)
2022-10-10 20:53:23,403:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 4.1338 (10.9026)
2022-10-10 20:53:23,404:INFO: Epoch: 62
2022-10-10 20:53:23,404:INFO: Training
2022-10-10 20:53:27,897:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.3250 (4.0801)
2022-10-10 20:53:32,758:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.7986 (4.1078)
2022-10-10 20:53:37,945:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2125 (2.4231)
2022-10-10 20:53:42,540:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.7581 (2.6256)
2022-10-10 20:53:47,158:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4377 (1.6866)
2022-10-10 20:53:51,934:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7762 (1.8750)
2022-10-10 20:53:56,418:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.4166 (9.2941)
2022-10-10 20:53:56,419:INFO: Epoch: 63
2022-10-10 20:53:56,419:INFO: Training
2022-10-10 20:54:01,008:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.4804 (4.0138)
2022-10-10 20:54:05,737:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.5432 (3.7631)
2022-10-10 20:54:10,755:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1993 (2.0352)
2022-10-10 20:54:15,702:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.8829 (2.6636)
2022-10-10 20:54:20,449:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4139 (1.3629)
2022-10-10 20:54:25,070:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7665 (1.7065)
2022-10-10 20:54:29,503:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.3665 (9.3953)
2022-10-10 20:54:29,504:INFO: Epoch: 64
2022-10-10 20:54:29,505:INFO: Training
2022-10-10 20:54:34,018:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.3467 (4.0665)
2022-10-10 20:54:38,822:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.6524 (4.2552)
2022-10-10 20:54:43,877:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1995 (2.5019)
2022-10-10 20:54:48,678:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.7399 (2.4054)
2022-10-10 20:54:53,775:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4185 (1.5947)
2022-10-10 20:54:58,710:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7732 (1.7493)
2022-10-10 20:55:03,186:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.6083 (10.4590)
2022-10-10 20:55:03,187:INFO: Epoch: 65
2022-10-10 20:55:03,188:INFO: Training
2022-10-10 20:55:07,737:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.4626 (4.0640)
2022-10-10 20:55:12,807:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.8967 (4.0530)
2022-10-10 20:55:18,165:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1976 (2.2936)
2022-10-10 20:55:23,212:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.7075 (2.4690)
2022-10-10 20:55:28,037:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4145 (1.4914)
2022-10-10 20:55:32,693:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7565 (1.7374)
2022-10-10 20:55:37,175:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.7842 (10.1416)
2022-10-10 20:55:37,176:INFO: Epoch: 66
2022-10-10 20:55:37,176:INFO: Training
2022-10-10 20:55:41,792:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.4590 (4.0999)
2022-10-10 20:55:46,703:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.4652 (3.9204)
2022-10-10 20:55:51,567:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.2009 (2.2685)
2022-10-10 20:55:56,107:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.6914 (2.3829)
2022-10-10 20:56:00,691:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4199 (1.5127)
2022-10-10 20:56:05,256:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7234 (1.7654)
2022-10-10 20:56:09,710:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.3517 (9.4106)
2022-10-10 20:56:09,711:INFO: Epoch: 67
2022-10-10 20:56:09,711:INFO: Training
2022-10-10 20:56:14,283:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.5930 (4.1202)
2022-10-10 20:56:18,946:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.5342 (3.5311)
2022-10-10 20:56:23,779:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1945 (1.9986)
2022-10-10 20:56:28,436:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.6320 (2.3385)
2022-10-10 20:56:32,997:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4093 (1.4038)
2022-10-10 20:56:37,882:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7141 (1.6703)
2022-10-10 20:56:42,439:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.2979 (8.8785)
2022-10-10 20:56:42,441:INFO: Epoch: 68
2022-10-10 20:56:42,442:INFO: Training
2022-10-10 20:56:47,262:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.5291 (4.1274)
2022-10-10 20:56:51,874:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.2571 (3.6870)
2022-10-10 20:56:56,818:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1931 (2.1020)
2022-10-10 20:57:01,322:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.6934 (2.3300)
2022-10-10 20:57:05,940:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4100 (1.4791)
2022-10-10 20:57:10,720:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.7024 (1.6757)
2022-10-10 20:57:15,168:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.5531 (9.2835)
2022-10-10 20:57:15,169:INFO: Epoch: 69
2022-10-10 20:57:15,169:INFO: Training
2022-10-10 20:57:19,715:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.5481 (4.1544)
2022-10-10 20:57:24,258:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.1102 (3.6115)
2022-10-10 20:57:29,187:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1963 (2.0025)
2022-10-10 20:57:33,653:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.5735 (2.2096)
2022-10-10 20:57:38,103:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4079 (1.4703)
2022-10-10 20:57:42,920:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6780 (1.6852)
2022-10-10 20:57:47,398:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.1810 (8.5250)
2022-10-10 20:57:47,400:INFO: Epoch: 70
2022-10-10 20:57:47,400:INFO: Training
2022-10-10 20:57:52,066:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6683 (4.1752)
2022-10-10 20:57:56,889:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.3258 (3.3520)
2022-10-10 20:58:01,699:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1913 (1.8425)
2022-10-10 20:58:06,641:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.5977 (2.2625)
2022-10-10 20:58:11,359:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4003 (1.3532)
2022-10-10 20:58:16,015:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6566 (1.5950)
2022-10-10 20:58:20,493:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.9010 (8.3812)
2022-10-10 20:58:20,495:INFO: Epoch: 71
2022-10-10 20:58:20,495:INFO: Training
2022-10-10 20:58:24,917:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6005 (4.2039)
2022-10-10 20:58:29,499:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.2117 (3.5238)
2022-10-10 20:58:34,253:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1881 (2.0525)
2022-10-10 20:58:39,038:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.6233 (2.1557)
2022-10-10 20:58:43,539:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4106 (1.4620)
2022-10-10 20:58:48,302:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6710 (1.6036)
2022-10-10 20:58:52,457:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 3.1728 (9.2903)
2022-10-10 20:58:52,460:INFO: Epoch: 72
2022-10-10 20:58:52,460:INFO: Training
2022-10-10 20:58:57,068:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6134 (4.2266)
2022-10-10 20:59:01,855:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.9325 (3.6762)
2022-10-10 20:59:06,695:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1921 (2.0422)
2022-10-10 20:59:11,435:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.5018 (2.0366)
2022-10-10 20:59:16,852:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.4052 (1.4492)
2022-10-10 20:59:21,548:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6423 (1.6257)
2022-10-10 20:59:26,009:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.9060 (8.4762)
2022-10-10 20:59:26,010:INFO: Epoch: 73
2022-10-10 20:59:26,011:INFO: Training
2022-10-10 20:59:30,553:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6974 (4.2256)
2022-10-10 20:59:35,411:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.6862 (3.2015)
2022-10-10 20:59:40,188:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1929 (1.7243)
2022-10-10 20:59:45,068:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.4973 (2.0802)
2022-10-10 20:59:49,599:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3993 (1.3622)
2022-10-10 20:59:54,391:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6205 (1.5804)
2022-10-10 20:59:58,898:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.7384 (7.8353)
2022-10-10 20:59:58,899:INFO: Epoch: 74
2022-10-10 20:59:58,899:INFO: Training
2022-10-10 21:00:03,533:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6662 (4.2426)
2022-10-10 21:00:08,323:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.3067 (3.2997)
2022-10-10 21:00:13,111:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1863 (1.8391)
2022-10-10 21:00:18,423:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.5873 (2.0203)
2022-10-10 21:00:23,441:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3922 (1.3384)
2022-10-10 21:00:28,280:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5986 (1.5625)
2022-10-10 21:00:33,517:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.5483 (7.8633)
2022-10-10 21:00:33,518:INFO: Epoch: 75
2022-10-10 21:00:33,519:INFO: Training
2022-10-10 21:00:38,155:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6910 (4.2577)
2022-10-10 21:00:43,165:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.4668 (3.5421)
2022-10-10 21:00:48,204:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1922 (1.9305)
2022-10-10 21:00:52,787:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.5119 (2.0919)
2022-10-10 21:00:57,261:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3972 (1.3905)
2022-10-10 21:01:02,150:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6096 (1.5490)
2022-10-10 21:01:06,676:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.8375 (8.3629)
2022-10-10 21:01:06,677:INFO: Epoch: 76
2022-10-10 21:01:06,678:INFO: Training
2022-10-10 21:01:11,250:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6290 (4.2828)
2022-10-10 21:01:16,123:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.7950 (3.3091)
2022-10-10 21:01:20,897:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1902 (1.7782)
2022-10-10 21:01:25,338:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.4246 (2.0106)
2022-10-10 21:01:29,923:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3921 (1.3385)
2022-10-10 21:01:34,846:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.6040 (1.5212)
2022-10-10 21:01:39,396:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.6743 (7.6602)
2022-10-10 21:01:39,397:INFO: Epoch: 77
2022-10-10 21:01:39,397:INFO: Training
2022-10-10 21:01:43,961:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6878 (4.2851)
2022-10-10 21:01:48,828:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.5573 (2.9966)
2022-10-10 21:01:53,774:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1860 (1.5874)
2022-10-10 21:01:58,493:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3721 (1.8373)
2022-10-10 21:02:03,218:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3885 (1.2965)
2022-10-10 21:02:07,901:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5854 (1.5025)
2022-10-10 21:02:12,078:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.4508 (7.5182)
2022-10-10 21:02:12,079:INFO: Epoch: 78
2022-10-10 21:02:12,080:INFO: Training
2022-10-10 21:02:16,583:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7119 (4.2785)
2022-10-10 21:02:21,391:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.9036 (3.3461)
2022-10-10 21:02:26,367:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1812 (1.7948)
2022-10-10 21:02:30,857:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.4341 (1.9138)
2022-10-10 21:02:35,688:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3867 (1.3041)
2022-10-10 21:02:40,501:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5676 (1.5173)
2022-10-10 21:02:44,737:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.4712 (7.5494)
2022-10-10 21:02:44,738:INFO: Epoch: 79
2022-10-10 21:02:44,738:INFO: Training
2022-10-10 21:02:49,394:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6907 (4.2953)
2022-10-10 21:02:54,316:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.9067 (3.2645)
2022-10-10 21:02:59,357:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1805 (1.6983)
2022-10-10 21:03:04,056:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.4124 (1.9833)
2022-10-10 21:03:08,593:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3852 (1.2807)
2022-10-10 21:03:13,187:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5767 (1.4944)
2022-10-10 21:03:17,730:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.7859 (7.9544)
2022-10-10 21:03:17,731:INFO: Epoch: 80
2022-10-10 21:03:17,732:INFO: Training
2022-10-10 21:03:22,304:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6520 (4.3379)
2022-10-10 21:03:27,062:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.3381 (3.2062)
2022-10-10 21:03:32,770:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1872 (1.7825)
2022-10-10 21:03:37,801:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3171 (1.8056)
2022-10-10 21:03:42,434:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3851 (1.3269)
2022-10-10 21:03:47,236:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5529 (1.5203)
2022-10-10 21:03:51,664:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.3647 (7.2114)
2022-10-10 21:03:51,665:INFO: Epoch: 81
2022-10-10 21:03:51,665:INFO: Training
2022-10-10 21:03:56,124:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7669 (4.3483)
2022-10-10 21:04:00,782:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 5.0343 (2.9579)
2022-10-10 21:04:05,761:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1806 (1.6007)
2022-10-10 21:04:10,465:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.4107 (1.9011)
2022-10-10 21:04:15,335:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3846 (1.2509)
2022-10-10 21:04:20,254:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5549 (1.4537)
2022-10-10 21:04:24,941:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.4850 (7.0346)
2022-10-10 21:04:24,943:INFO: Epoch: 82
2022-10-10 21:04:24,943:INFO: Training
2022-10-10 21:04:29,486:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6642 (4.3380)
2022-10-10 21:04:34,372:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.1301 (3.0829)
2022-10-10 21:04:39,374:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1851 (1.6561)
2022-10-10 21:04:44,035:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3394 (1.8026)
2022-10-10 21:04:48,517:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3767 (1.2819)
2022-10-10 21:04:53,416:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5391 (1.4517)
2022-10-10 21:04:57,989:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.3331 (6.5846)
2022-10-10 21:04:57,990:INFO: Epoch: 83
2022-10-10 21:04:57,990:INFO: Training
2022-10-10 21:05:02,531:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7774 (4.3290)
2022-10-10 21:05:07,352:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.9120 (2.7516)
2022-10-10 21:05:12,149:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1824 (1.4245)
2022-10-10 21:05:16,725:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3432 (1.7223)
2022-10-10 21:05:21,471:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3722 (1.2452)
2022-10-10 21:05:26,305:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5106 (1.4199)
2022-10-10 21:05:30,786:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 1.9900 (6.1720)
2022-10-10 21:05:30,788:INFO: Epoch: 84
2022-10-10 21:05:30,788:INFO: Training
2022-10-10 21:05:35,282:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7903 (4.3583)
2022-10-10 21:05:39,962:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.8924 (2.6370)
2022-10-10 21:05:44,824:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1810 (1.4077)
2022-10-10 21:05:49,354:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3616 (1.6429)
2022-10-10 21:05:53,821:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3684 (1.1685)
2022-10-10 21:05:58,512:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.5064 (1.3492)
2022-10-10 21:06:03,032:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 2.0248 (6.1221)
2022-10-10 21:06:03,033:INFO: Epoch: 85
2022-10-10 21:06:03,033:INFO: Training
2022-10-10 21:06:07,648:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6812 (4.3431)
2022-10-10 21:06:12,620:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.3155 (2.8484)
2022-10-10 21:06:17,628:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1801 (1.5352)
2022-10-10 21:06:22,371:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3409 (1.6483)
2022-10-10 21:06:27,104:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3697 (1.1949)
2022-10-10 21:06:31,754:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.4948 (1.3582)
2022-10-10 21:06:35,901:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 1.9502 (6.1245)
2022-10-10 21:06:35,902:INFO: Epoch: 86
2022-10-10 21:06:35,902:INFO: Training
2022-10-10 21:06:40,344:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7436 (4.3339)
2022-10-10 21:06:45,109:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.3624 (2.7160)
2022-10-10 21:06:50,094:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1737 (1.4295)
2022-10-10 21:06:54,892:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3017 (1.6761)
2022-10-10 21:06:59,563:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3657 (1.1766)
2022-10-10 21:07:04,234:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.4827 (1.3532)
2022-10-10 21:07:08,640:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 1.8516 (5.6827)
2022-10-10 21:07:08,642:INFO: Epoch: 87
2022-10-10 21:07:08,642:INFO: Training
2022-10-10 21:07:13,211:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7062 (4.3245)
2022-10-10 21:07:17,987:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 3.9851 (2.5938)
2022-10-10 21:07:23,059:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1754 (1.4385)
2022-10-10 21:07:27,627:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3383 (1.6633)
2022-10-10 21:07:32,171:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3603 (1.1484)
2022-10-10 21:07:37,022:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.4653 (1.3435)
2022-10-10 21:07:41,436:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 1.6931 (5.3163)
2022-10-10 21:07:41,436:INFO: Epoch: 88
2022-10-10 21:07:41,437:INFO: Training
2022-10-10 21:07:45,894:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.8392 (4.3647)
2022-10-10 21:07:50,687:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.3056 (2.6078)
2022-10-10 21:07:55,563:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1792 (1.3474)
2022-10-10 21:08:00,166:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.2751 (1.6197)
2022-10-10 21:08:04,623:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3652 (1.1914)
2022-10-10 21:08:09,346:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.4693 (1.3235)
2022-10-10 21:08:13,798:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 1.6646 (5.7996)
2022-10-10 21:08:13,798:INFO: Epoch: 89
2022-10-10 21:08:13,799:INFO: Training
2022-10-10 21:08:18,361:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.7115 (4.3446)
2022-10-10 21:08:23,143:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 4.6831 (2.8221)
2022-10-10 21:08:27,937:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.1735 (1.5269)
2022-10-10 21:08:32,420:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 1.3461 (1.7390)
2022-10-10 21:08:36,922:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.3566 (1.1414)
2022-10-10 21:08:41,405:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 0.4655 (1.3197)
2022-10-10 21:08:45,851:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 1.9266 (5.8526)
2022-10-10 21:08:45,851:INFO: Epoch: 90
2022-10-10 21:08:45,851:INFO: Training
2022-10-10 21:08:50,332:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6781 (4.3096)
2022-10-11 15:13:01,230:INFO: Initializing Training Set
2022-10-11 15:13:05,424:INFO: Initializing Validation Set
2022-10-11 15:13:11,749:INFO: Epoch: 1
2022-10-11 15:13:11,749:INFO: Training
2022-10-11 15:13:18,775:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 4.6052 (26.1956)
2022-10-11 15:13:22,908:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 19.8264 (11.0754)
2022-10-11 15:13:27,184:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7485 (6.3209)
2022-10-11 15:13:31,482:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 7.4369 (11.6256)
2022-10-11 15:13:35,581:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.8907 (7.2735)
2022-10-11 15:13:39,596:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6203 (4.4957)
2022-10-11 15:13:43,529:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 8.8407 (21.2507)
2022-10-11 15:13:43,530:INFO: Epoch: 2
2022-10-11 15:13:43,530:INFO: Training
2022-10-11 15:13:48,657:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 5.5462 (4.6577)
2022-10-11 15:13:52,891:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 14.2075 (10.5697)
2022-10-11 15:13:57,256:INFO: Dataset: crowds_zara02_train.txt  Batch: 12/12	Loss 0.7848 (5.6287)
2022-10-11 15:14:01,546:INFO: Dataset: crowds_zara03_train.txt  Batch: 7/7	Loss 5.6260 (8.8023)
2022-10-11 15:14:05,648:INFO: Dataset: students001_train.txt    Batch: 6/6	Loss 0.9725 (4.4283)
2022-10-11 15:14:09,700:INFO: Dataset: students003_train.txt    Batch: 7/7	Loss 2.6637 (4.9003)
2022-10-11 15:14:13,840:INFO: Dataset: uni_examples_train.txt   Batch: 3/3	Loss 9.3605 (16.9451)
2022-10-11 15:14:13,841:INFO: Epoch: 3
2022-10-11 15:14:13,841:INFO: Training
2022-10-11 15:14:17,812:INFO: Dataset: biwi_hotel_train.txt     Batch: 4/4	Loss 6.1286 (4.9918)
2022-10-11 15:14:21,922:INFO: Dataset: crowds_zara01_train.txt  Batch: 8/8	Loss 11.3700 (9.4552)

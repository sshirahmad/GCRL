2022-10-29 22:22:53,879:INFO: Initializing Training Set
2022-10-29 22:22:55,855:INFO: Initializing Validation Set
2022-10-29 22:22:56,105:INFO: Initializing Validation O Set
2022-10-29 22:22:58,011:INFO: => loaded checkpoint './models/eth/pretrain/P3/5.0/SSE_data_eth_irm[5.0]_epoch_400.pth.tar' (epoch 401)
2022-10-29 22:22:58,012:INFO: 
===> EPOCH: 401 (P4)
2022-10-29 22:22:58,012:INFO: - Computing loss (training)
2022-10-29 22:22:59,243:INFO: Dataset: hotel               Batch: 1/4	Loss 79.3206 (79.3206)
2022-10-29 22:23:00,244:INFO: Dataset: hotel               Batch: 2/4	Loss 80.3930 (79.8281)
2022-10-29 22:23:01,247:INFO: Dataset: hotel               Batch: 3/4	Loss 79.8985 (79.8519)
2022-10-29 22:23:01,981:INFO: Dataset: hotel               Batch: 4/4	Loss 47.9856 (74.1345)
2022-10-29 22:23:03,389:INFO: Dataset: univ                Batch:  1/15	Loss 85.5345 (85.5345)
2022-10-29 22:23:04,449:INFO: Dataset: univ                Batch:  2/15	Loss 97.8579 (91.4166)
2022-10-29 22:23:05,509:INFO: Dataset: univ                Batch:  3/15	Loss 95.8113 (92.8805)
2022-10-29 22:23:06,579:INFO: Dataset: univ                Batch:  4/15	Loss 141.8099 (105.9572)
2022-10-29 22:23:07,635:INFO: Dataset: univ                Batch:  5/15	Loss 86.1467 (102.2518)
2022-10-29 22:23:08,698:INFO: Dataset: univ                Batch:  6/15	Loss 91.8244 (100.4657)
2022-10-29 22:23:09,751:INFO: Dataset: univ                Batch:  7/15	Loss 108.9568 (101.5930)
2022-10-29 22:23:10,813:INFO: Dataset: univ                Batch:  8/15	Loss 87.5737 (99.8580)
2022-10-29 22:23:11,866:INFO: Dataset: univ                Batch:  9/15	Loss 121.4162 (102.0706)
2022-10-29 22:23:12,927:INFO: Dataset: univ                Batch: 10/15	Loss 111.7368 (103.0714)
2022-10-29 22:23:13,991:INFO: Dataset: univ                Batch: 11/15	Loss 93.1016 (102.1248)
2022-10-29 22:23:15,065:INFO: Dataset: univ                Batch: 12/15	Loss 114.0935 (103.2256)
2022-10-29 22:23:16,131:INFO: Dataset: univ                Batch: 13/15	Loss 87.3205 (101.9867)
2022-10-29 22:23:17,203:INFO: Dataset: univ                Batch: 14/15	Loss 256.1247 (113.8053)
2022-10-29 22:23:17,655:INFO: Dataset: univ                Batch: 15/15	Loss 15.1778 (112.4639)
2022-10-29 22:23:18,927:INFO: Dataset: zara1               Batch: 1/8	Loss 86.4346 (86.4346)
2022-10-29 22:23:19,927:INFO: Dataset: zara1               Batch: 2/8	Loss 88.0868 (87.1924)
2022-10-29 22:23:20,926:INFO: Dataset: zara1               Batch: 3/8	Loss 93.1228 (89.1692)
2022-10-29 22:23:21,927:INFO: Dataset: zara1               Batch: 4/8	Loss 456.7763 (174.9689)
2022-10-29 22:23:22,928:INFO: Dataset: zara1               Batch: 5/8	Loss 87.3135 (157.9781)
2022-10-29 22:23:23,925:INFO: Dataset: zara1               Batch: 6/8	Loss 101.1897 (148.4049)
2022-10-29 22:23:24,926:INFO: Dataset: zara1               Batch: 7/8	Loss 84.0118 (140.0863)
2022-10-29 22:23:25,833:INFO: Dataset: zara1               Batch: 8/8	Loss 74.3595 (133.2715)
2022-10-29 22:23:27,120:INFO: Dataset: zara2               Batch:  1/18	Loss 152.6642 (152.6642)
2022-10-29 22:23:28,126:INFO: Dataset: zara2               Batch:  2/18	Loss 111.5786 (132.5635)
2022-10-29 22:23:29,132:INFO: Dataset: zara2               Batch:  3/18	Loss 202.5050 (154.6647)
2022-10-29 22:23:30,137:INFO: Dataset: zara2               Batch:  4/18	Loss 407.9433 (219.1424)
2022-10-29 22:23:31,141:INFO: Dataset: zara2               Batch:  5/18	Loss 89.8886 (194.1304)
2022-10-29 22:23:32,147:INFO: Dataset: zara2               Batch:  6/18	Loss 107.6906 (179.4767)
2022-10-29 22:23:33,149:INFO: Dataset: zara2               Batch:  7/18	Loss 125.2601 (171.1946)
2022-10-29 22:23:34,154:INFO: Dataset: zara2               Batch:  8/18	Loss 99.3253 (162.3745)
2022-10-29 22:23:35,162:INFO: Dataset: zara2               Batch:  9/18	Loss 156.8737 (161.7844)
2022-10-29 22:23:36,169:INFO: Dataset: zara2               Batch: 10/18	Loss 95.8180 (155.3581)
2022-10-29 22:23:37,174:INFO: Dataset: zara2               Batch: 11/18	Loss 205.5505 (159.5801)
2022-10-29 22:23:38,180:INFO: Dataset: zara2               Batch: 12/18	Loss 99.9766 (154.5899)
2022-10-29 22:23:39,185:INFO: Dataset: zara2               Batch: 13/18	Loss 107.0397 (151.2277)
2022-10-29 22:23:40,191:INFO: Dataset: zara2               Batch: 14/18	Loss 88.1707 (146.5642)
2022-10-29 22:23:41,195:INFO: Dataset: zara2               Batch: 15/18	Loss 90.3240 (142.6810)
2022-10-29 22:23:42,198:INFO: Dataset: zara2               Batch: 16/18	Loss 85.4475 (139.2425)
2022-10-29 22:23:43,204:INFO: Dataset: zara2               Batch: 17/18	Loss 89.6989 (136.2718)
2022-10-29 22:23:44,116:INFO: Dataset: zara2               Batch: 18/18	Loss 298.5951 (144.6442)
2022-10-29 22:23:44,175:INFO: - Computing ADE (validation o)
2022-10-29 22:23:44,511:INFO: 		 ADE on eth                       dataset:	 2.9015285968780518
2022-10-29 22:23:44,511:INFO: Average validation o:	ADE  2.9015	FDE  4.9430
2022-10-29 22:23:44,512:INFO: - Computing ADE (validation)
2022-10-29 22:23:44,840:INFO: 		 ADE on hotel                     dataset:	 1.013829231262207
2022-10-29 22:23:45,253:INFO: 		 ADE on univ                      dataset:	 1.5185368061065674
2022-10-29 22:23:45,590:INFO: 		 ADE on zara1                     dataset:	 2.4246551990509033
2022-10-29 22:23:46,111:INFO: 		 ADE on zara2                     dataset:	 1.411049485206604
2022-10-29 22:23:46,111:INFO: Average validation:	ADE  1.5041	FDE  2.7563
2022-10-29 22:23:46,112:INFO: - Computing ADE (training)
2022-10-29 22:23:46,547:INFO: 		 ADE on hotel                     dataset:	 1.3625202178955078
2022-10-29 22:23:47,571:INFO: 		 ADE on univ                      dataset:	 1.3818446397781372
2022-10-29 22:23:48,223:INFO: 		 ADE on zara1                     dataset:	 2.406338691711426
2022-10-29 22:23:49,357:INFO: 		 ADE on zara2                     dataset:	 1.6887881755828857
2022-10-29 22:23:49,357:INFO: Average training:	ADE  1.5089	FDE  2.7722
2022-10-29 22:23:49,368:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_401.pth.tar
2022-10-29 22:23:49,368:INFO: 
===> EPOCH: 402 (P4)
2022-10-29 22:23:49,368:INFO: - Computing loss (training)
2022-10-29 22:23:50,589:INFO: Dataset: hotel               Batch: 1/4	Loss 79.7600 (79.7600)
2022-10-29 22:23:51,590:INFO: Dataset: hotel               Batch: 2/4	Loss 80.4356 (80.1027)
2022-10-29 22:23:52,593:INFO: Dataset: hotel               Batch: 3/4	Loss 79.9096 (80.0386)
2022-10-29 22:23:53,326:INFO: Dataset: hotel               Batch: 4/4	Loss 47.5230 (73.8615)
2022-10-29 22:23:54,686:INFO: Dataset: univ                Batch:  1/15	Loss 101.3297 (101.3297)
2022-10-29 22:23:55,759:INFO: Dataset: univ                Batch:  2/15	Loss 99.6208 (100.4620)
2022-10-29 22:23:56,824:INFO: Dataset: univ                Batch:  3/15	Loss 20.8065 (72.5413)
2022-10-29 22:23:57,884:INFO: Dataset: univ                Batch:  4/15	Loss 93.5053 (77.9491)
2022-10-29 22:23:58,952:INFO: Dataset: univ                Batch:  5/15	Loss 99.3518 (82.2699)
2022-10-29 22:24:00,022:INFO: Dataset: univ                Batch:  6/15	Loss 95.2461 (84.5448)
2022-10-29 22:24:01,095:INFO: Dataset: univ                Batch:  7/15	Loss 185.4440 (100.0655)
2022-10-29 22:24:02,171:INFO: Dataset: univ                Batch:  8/15	Loss 110.9299 (101.5912)
2022-10-29 22:24:03,240:INFO: Dataset: univ                Batch:  9/15	Loss 89.4501 (100.2790)
2022-10-29 22:24:04,312:INFO: Dataset: univ                Batch: 10/15	Loss 185.4380 (109.1738)
2022-10-29 22:24:05,465:INFO: Dataset: univ                Batch: 11/15	Loss 99.2391 (108.1966)
2022-10-29 22:24:06,538:INFO: Dataset: univ                Batch: 12/15	Loss 95.9030 (107.1385)
2022-10-29 22:24:07,612:INFO: Dataset: univ                Batch: 13/15	Loss 99.3733 (106.5201)
2022-10-29 22:24:08,685:INFO: Dataset: univ                Batch: 14/15	Loss 87.0230 (105.0743)
2022-10-29 22:24:09,137:INFO: Dataset: univ                Batch: 15/15	Loss 15.5801 (103.9377)
2022-10-29 22:24:10,441:INFO: Dataset: zara1               Batch: 1/8	Loss 227.0901 (227.0901)
2022-10-29 22:24:11,457:INFO: Dataset: zara1               Batch: 2/8	Loss 87.4335 (163.0933)
2022-10-29 22:24:12,477:INFO: Dataset: zara1               Batch: 3/8	Loss 86.3470 (135.2427)
2022-10-29 22:24:13,492:INFO: Dataset: zara1               Batch: 4/8	Loss 89.0753 (123.5363)
2022-10-29 22:24:14,502:INFO: Dataset: zara1               Batch: 5/8	Loss 144.9175 (127.7776)
2022-10-29 22:24:15,516:INFO: Dataset: zara1               Batch: 6/8	Loss 86.4839 (121.0600)
2022-10-29 22:24:16,530:INFO: Dataset: zara1               Batch: 7/8	Loss 89.4563 (116.4319)
2022-10-29 22:24:17,448:INFO: Dataset: zara1               Batch: 8/8	Loss 74.9502 (112.3711)
2022-10-29 22:24:18,761:INFO: Dataset: zara2               Batch:  1/18	Loss 112.8047 (112.8047)
2022-10-29 22:24:19,787:INFO: Dataset: zara2               Batch:  2/18	Loss 101.1643 (106.9845)
2022-10-29 22:24:20,813:INFO: Dataset: zara2               Batch:  3/18	Loss 107.1087 (107.0241)
2022-10-29 22:24:21,839:INFO: Dataset: zara2               Batch:  4/18	Loss 220.1122 (132.2123)
2022-10-29 22:24:22,862:INFO: Dataset: zara2               Batch:  5/18	Loss 171.5985 (140.1328)
2022-10-29 22:24:23,882:INFO: Dataset: zara2               Batch:  6/18	Loss 373.8228 (181.7921)
2022-10-29 22:24:24,895:INFO: Dataset: zara2               Batch:  7/18	Loss 94.3723 (168.0958)
2022-10-29 22:24:25,917:INFO: Dataset: zara2               Batch:  8/18	Loss 112.2449 (159.8186)
2022-10-29 22:24:26,934:INFO: Dataset: zara2               Batch:  9/18	Loss 86.7642 (151.4224)
2022-10-29 22:24:27,951:INFO: Dataset: zara2               Batch: 10/18	Loss 131.4677 (149.4012)
2022-10-29 22:24:28,966:INFO: Dataset: zara2               Batch: 11/18	Loss 104.4000 (145.9973)
2022-10-29 22:24:29,986:INFO: Dataset: zara2               Batch: 12/18	Loss 94.3719 (142.0736)
2022-10-29 22:24:31,007:INFO: Dataset: zara2               Batch: 13/18	Loss 104.3865 (139.4435)
2022-10-29 22:24:32,027:INFO: Dataset: zara2               Batch: 14/18	Loss 91.6779 (136.1573)
2022-10-29 22:24:33,043:INFO: Dataset: zara2               Batch: 15/18	Loss 90.0270 (132.7229)
2022-10-29 22:24:34,056:INFO: Dataset: zara2               Batch: 16/18	Loss 169.7969 (134.8005)
2022-10-29 22:24:35,073:INFO: Dataset: zara2               Batch: 17/18	Loss 169.2021 (137.0149)
2022-10-29 22:24:35,994:INFO: Dataset: zara2               Batch: 18/18	Loss 79.2399 (134.4361)
2022-10-29 22:24:36,052:INFO: - Computing ADE (validation o)
2022-10-29 22:24:36,381:INFO: 		 ADE on eth                       dataset:	 2.8476438522338867
2022-10-29 22:24:36,381:INFO: Average validation o:	ADE  2.8476	FDE  4.8287
2022-10-29 22:24:36,382:INFO: - Computing ADE (validation)
2022-10-29 22:24:36,705:INFO: 		 ADE on hotel                     dataset:	 0.9427573084831238
2022-10-29 22:24:37,133:INFO: 		 ADE on univ                      dataset:	 1.4687190055847168
2022-10-29 22:24:37,463:INFO: 		 ADE on zara1                     dataset:	 2.460387706756592
2022-10-29 22:24:37,975:INFO: 		 ADE on zara2                     dataset:	 1.3778951168060303
2022-10-29 22:24:37,975:INFO: Average validation:	ADE  1.4643	FDE  2.6732
2022-10-29 22:24:37,976:INFO: - Computing ADE (training)
2022-10-29 22:24:38,410:INFO: 		 ADE on hotel                     dataset:	 1.2928695678710938
2022-10-29 22:24:39,418:INFO: 		 ADE on univ                      dataset:	 1.3556654453277588
2022-10-29 22:24:40,091:INFO: 		 ADE on zara1                     dataset:	 2.3989548683166504
2022-10-29 22:24:41,239:INFO: 		 ADE on zara2                     dataset:	 1.6389998197555542
2022-10-29 22:24:41,240:INFO: Average training:	ADE  1.4781	FDE  2.7097
2022-10-29 22:24:41,251:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_402.pth.tar
2022-10-29 22:24:41,251:INFO: 
===> EPOCH: 403 (P4)
2022-10-29 22:24:41,251:INFO: - Computing loss (training)
2022-10-29 22:24:42,499:INFO: Dataset: hotel               Batch: 1/4	Loss 79.9470 (79.9470)
2022-10-29 22:24:43,519:INFO: Dataset: hotel               Batch: 2/4	Loss 81.0554 (80.5131)
2022-10-29 22:24:44,533:INFO: Dataset: hotel               Batch: 3/4	Loss 78.0252 (79.6642)
2022-10-29 22:24:45,273:INFO: Dataset: hotel               Batch: 4/4	Loss 48.2398 (74.6065)
2022-10-29 22:24:46,684:INFO: Dataset: univ                Batch:  1/15	Loss 83.9505 (83.9505)
2022-10-29 22:24:47,762:INFO: Dataset: univ                Batch:  2/15	Loss 92.4349 (87.6452)
2022-10-29 22:24:48,853:INFO: Dataset: univ                Batch:  3/15	Loss 100.2008 (91.7456)
2022-10-29 22:24:49,932:INFO: Dataset: univ                Batch:  4/15	Loss 84.7985 (90.1563)
2022-10-29 22:24:51,013:INFO: Dataset: univ                Batch:  5/15	Loss 89.3721 (90.0106)
2022-10-29 22:24:52,091:INFO: Dataset: univ                Batch:  6/15	Loss 86.6476 (89.4782)
2022-10-29 22:24:53,173:INFO: Dataset: univ                Batch:  7/15	Loss 83.6263 (88.6507)
2022-10-29 22:24:54,252:INFO: Dataset: univ                Batch:  8/15	Loss 84.3806 (88.1426)
2022-10-29 22:24:55,340:INFO: Dataset: univ                Batch:  9/15	Loss 140.4727 (94.3268)
2022-10-29 22:24:56,422:INFO: Dataset: univ                Batch: 10/15	Loss 90.5500 (93.9616)
2022-10-29 22:24:57,501:INFO: Dataset: univ                Batch: 11/15	Loss 88.9075 (93.5074)
2022-10-29 22:24:58,584:INFO: Dataset: univ                Batch: 12/15	Loss 88.1260 (93.0336)
2022-10-29 22:24:59,672:INFO: Dataset: univ                Batch: 13/15	Loss 88.8704 (92.6934)
2022-10-29 22:25:00,746:INFO: Dataset: univ                Batch: 14/15	Loss 122.7654 (94.5268)
2022-10-29 22:25:01,203:INFO: Dataset: univ                Batch: 15/15	Loss 17.8687 (93.4224)
2022-10-29 22:25:02,491:INFO: Dataset: zara1               Batch: 1/8	Loss 86.3841 (86.3841)
2022-10-29 22:25:03,504:INFO: Dataset: zara1               Batch: 2/8	Loss 85.8956 (86.1538)
2022-10-29 22:25:04,519:INFO: Dataset: zara1               Batch: 3/8	Loss 95.6356 (89.0413)
2022-10-29 22:25:05,536:INFO: Dataset: zara1               Batch: 4/8	Loss 85.2409 (88.0507)
2022-10-29 22:25:06,552:INFO: Dataset: zara1               Batch: 5/8	Loss 85.5247 (87.5763)
2022-10-29 22:25:07,567:INFO: Dataset: zara1               Batch: 6/8	Loss 84.8789 (87.1342)
2022-10-29 22:25:08,585:INFO: Dataset: zara1               Batch: 7/8	Loss 85.2939 (86.8825)
2022-10-29 22:25:09,528:INFO: Dataset: zara1               Batch: 8/8	Loss 77.7915 (85.8395)
2022-10-29 22:25:10,853:INFO: Dataset: zara2               Batch:  1/18	Loss 96.4836 (96.4836)
2022-10-29 22:25:11,893:INFO: Dataset: zara2               Batch:  2/18	Loss 110.8395 (103.6615)
2022-10-29 22:25:13,009:INFO: Dataset: zara2               Batch:  3/18	Loss 110.7577 (105.7502)
2022-10-29 22:25:14,040:INFO: Dataset: zara2               Batch:  4/18	Loss 504.9225 (201.3912)
2022-10-29 22:25:15,057:INFO: Dataset: zara2               Batch:  5/18	Loss 220.9270 (204.8509)
2022-10-29 22:25:16,085:INFO: Dataset: zara2               Batch:  6/18	Loss 186.3459 (201.9030)
2022-10-29 22:25:17,159:INFO: Dataset: zara2               Batch:  7/18	Loss 84.2753 (185.3929)
2022-10-29 22:25:18,210:INFO: Dataset: zara2               Batch:  8/18	Loss 84.4280 (173.0322)
2022-10-29 22:25:19,263:INFO: Dataset: zara2               Batch:  9/18	Loss 96.6821 (164.3210)
2022-10-29 22:25:20,336:INFO: Dataset: zara2               Batch: 10/18	Loss 112.2432 (159.3805)
2022-10-29 22:25:21,380:INFO: Dataset: zara2               Batch: 11/18	Loss 221.7450 (165.3563)
2022-10-29 22:25:22,460:INFO: Dataset: zara2               Batch: 12/18	Loss 86.5540 (158.6801)
2022-10-29 22:25:23,547:INFO: Dataset: zara2               Batch: 13/18	Loss 97.3343 (154.0707)
2022-10-29 22:25:24,573:INFO: Dataset: zara2               Batch: 14/18	Loss 107.8488 (150.6665)
2022-10-29 22:25:25,652:INFO: Dataset: zara2               Batch: 15/18	Loss 91.5770 (146.3742)
2022-10-29 22:25:26,686:INFO: Dataset: zara2               Batch: 16/18	Loss 135.9874 (145.7553)
2022-10-29 22:25:27,725:INFO: Dataset: zara2               Batch: 17/18	Loss 89.3853 (142.1507)
2022-10-29 22:25:28,674:INFO: Dataset: zara2               Batch: 18/18	Loss 72.5507 (138.7680)
2022-10-29 22:25:28,736:INFO: - Computing ADE (validation o)
2022-10-29 22:25:29,083:INFO: 		 ADE on eth                       dataset:	 2.8748581409454346
2022-10-29 22:25:29,083:INFO: Average validation o:	ADE  2.8749	FDE  4.8790
2022-10-29 22:25:29,084:INFO: - Computing ADE (validation)
2022-10-29 22:25:29,401:INFO: 		 ADE on hotel                     dataset:	 0.9463991522789001
2022-10-29 22:25:29,870:INFO: 		 ADE on univ                      dataset:	 1.4742369651794434
2022-10-29 22:25:30,249:INFO: 		 ADE on zara1                     dataset:	 2.4103586673736572
2022-10-29 22:25:30,805:INFO: 		 ADE on zara2                     dataset:	 1.3696255683898926
2022-10-29 22:25:30,805:INFO: Average validation:	ADE  1.4614	FDE  2.6777
2022-10-29 22:25:30,805:INFO: - Computing ADE (training)
2022-10-29 22:25:31,306:INFO: 		 ADE on hotel                     dataset:	 1.304936408996582
2022-10-29 22:25:32,397:INFO: 		 ADE on univ                      dataset:	 1.3553463220596313
2022-10-29 22:25:33,078:INFO: 		 ADE on zara1                     dataset:	 2.3878352642059326
2022-10-29 22:25:34,272:INFO: 		 ADE on zara2                     dataset:	 1.6433755159378052
2022-10-29 22:25:34,273:INFO: Average training:	ADE  1.4783	FDE  2.7163
2022-10-29 22:25:34,284:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_403.pth.tar
2022-10-29 22:25:34,284:INFO: 
===> EPOCH: 404 (P4)
2022-10-29 22:25:34,284:INFO: - Computing loss (training)
2022-10-29 22:25:35,802:INFO: Dataset: hotel               Batch: 1/4	Loss 80.1704 (80.1704)
2022-10-29 22:25:36,907:INFO: Dataset: hotel               Batch: 2/4	Loss 79.2935 (79.7228)
2022-10-29 22:25:37,951:INFO: Dataset: hotel               Batch: 3/4	Loss 79.3880 (79.6215)
2022-10-29 22:25:38,722:INFO: Dataset: hotel               Batch: 4/4	Loss 48.6319 (73.8978)
2022-10-29 22:25:40,133:INFO: Dataset: univ                Batch:  1/15	Loss 84.0585 (84.0585)
2022-10-29 22:25:41,235:INFO: Dataset: univ                Batch:  2/15	Loss 87.9667 (85.9865)
2022-10-29 22:25:42,320:INFO: Dataset: univ                Batch:  3/15	Loss 99.6401 (90.1260)
2022-10-29 22:25:43,400:INFO: Dataset: univ                Batch:  4/15	Loss 89.0375 (89.8561)
2022-10-29 22:25:44,491:INFO: Dataset: univ                Batch:  5/15	Loss 101.4317 (92.3118)
2022-10-29 22:25:45,576:INFO: Dataset: univ                Batch:  6/15	Loss 166.5514 (104.8880)
2022-10-29 22:25:46,671:INFO: Dataset: univ                Batch:  7/15	Loss 90.6461 (102.7235)
2022-10-29 22:25:47,747:INFO: Dataset: univ                Batch:  8/15	Loss 113.4095 (104.0051)
2022-10-29 22:25:48,846:INFO: Dataset: univ                Batch:  9/15	Loss 84.5853 (101.8267)
2022-10-29 22:25:49,979:INFO: Dataset: univ                Batch: 10/15	Loss 100.3709 (101.6763)
2022-10-29 22:25:51,129:INFO: Dataset: univ                Batch: 11/15	Loss 83.7581 (100.0870)
2022-10-29 22:25:52,235:INFO: Dataset: univ                Batch: 12/15	Loss 87.1896 (99.0904)
2022-10-29 22:25:53,308:INFO: Dataset: univ                Batch: 13/15	Loss 86.2513 (98.1486)
2022-10-29 22:25:54,390:INFO: Dataset: univ                Batch: 14/15	Loss 83.4274 (97.0474)
2022-10-29 22:25:54,842:INFO: Dataset: univ                Batch: 15/15	Loss 16.7353 (96.3623)
2022-10-29 22:25:56,240:INFO: Dataset: zara1               Batch: 1/8	Loss 85.0600 (85.0600)
2022-10-29 22:25:57,247:INFO: Dataset: zara1               Batch: 2/8	Loss 86.5475 (85.8176)
2022-10-29 22:25:58,259:INFO: Dataset: zara1               Batch: 3/8	Loss 83.5499 (85.0524)
2022-10-29 22:25:59,265:INFO: Dataset: zara1               Batch: 4/8	Loss 86.8235 (85.4630)
2022-10-29 22:26:00,275:INFO: Dataset: zara1               Batch: 5/8	Loss 85.9947 (85.5713)
2022-10-29 22:26:01,280:INFO: Dataset: zara1               Batch: 6/8	Loss 89.0168 (86.1726)
2022-10-29 22:26:02,292:INFO: Dataset: zara1               Batch: 7/8	Loss 88.6695 (86.5449)
2022-10-29 22:26:03,206:INFO: Dataset: zara1               Batch: 8/8	Loss 77.8485 (85.6158)
2022-10-29 22:26:04,521:INFO: Dataset: zara2               Batch:  1/18	Loss 103.5461 (103.5461)
2022-10-29 22:26:05,589:INFO: Dataset: zara2               Batch:  2/18	Loss 96.2515 (99.8407)
2022-10-29 22:26:06,616:INFO: Dataset: zara2               Batch:  3/18	Loss 87.6584 (95.7839)
2022-10-29 22:26:07,742:INFO: Dataset: zara2               Batch:  4/18	Loss 107.9356 (98.9240)
2022-10-29 22:26:08,813:INFO: Dataset: zara2               Batch:  5/18	Loss 106.3806 (100.3558)
2022-10-29 22:26:09,871:INFO: Dataset: zara2               Batch:  6/18	Loss 84.0653 (97.7118)
2022-10-29 22:26:10,938:INFO: Dataset: zara2               Batch:  7/18	Loss 144.0857 (104.5990)
2022-10-29 22:26:11,993:INFO: Dataset: zara2               Batch:  8/18	Loss 95.5481 (103.4088)
2022-10-29 22:26:13,060:INFO: Dataset: zara2               Batch:  9/18	Loss 102.9472 (103.3615)
2022-10-29 22:26:14,169:INFO: Dataset: zara2               Batch: 10/18	Loss 91.4547 (102.0986)
2022-10-29 22:26:15,276:INFO: Dataset: zara2               Batch: 11/18	Loss 156.8112 (107.1127)
2022-10-29 22:26:16,325:INFO: Dataset: zara2               Batch: 12/18	Loss 90.4567 (105.8247)
2022-10-29 22:26:17,351:INFO: Dataset: zara2               Batch: 13/18	Loss 85.4884 (104.3650)
2022-10-29 22:26:18,388:INFO: Dataset: zara2               Batch: 14/18	Loss 186.9720 (110.3450)
2022-10-29 22:26:19,483:INFO: Dataset: zara2               Batch: 15/18	Loss 90.0834 (109.1469)
2022-10-29 22:26:20,519:INFO: Dataset: zara2               Batch: 16/18	Loss 95.2249 (108.2806)
2022-10-29 22:26:21,565:INFO: Dataset: zara2               Batch: 17/18	Loss 106.1420 (108.1679)
2022-10-29 22:26:22,537:INFO: Dataset: zara2               Batch: 18/18	Loss 78.0606 (106.7693)
2022-10-29 22:26:22,600:INFO: - Computing ADE (validation o)
2022-10-29 22:26:22,939:INFO: 		 ADE on eth                       dataset:	 2.8827497959136963
2022-10-29 22:26:22,939:INFO: Average validation o:	ADE  2.8827	FDE  4.9050
2022-10-29 22:26:22,940:INFO: - Computing ADE (validation)
2022-10-29 22:26:23,276:INFO: 		 ADE on hotel                     dataset:	 0.9659910202026367
2022-10-29 22:26:23,765:INFO: 		 ADE on univ                      dataset:	 1.4689834117889404
2022-10-29 22:26:24,113:INFO: 		 ADE on zara1                     dataset:	 2.3854973316192627
2022-10-29 22:26:24,642:INFO: 		 ADE on zara2                     dataset:	 1.372086763381958
2022-10-29 22:26:24,642:INFO: Average validation:	ADE  1.4592	FDE  2.6777
2022-10-29 22:26:24,643:INFO: - Computing ADE (training)
2022-10-29 22:26:25,114:INFO: 		 ADE on hotel                     dataset:	 1.327499508857727
2022-10-29 22:26:26,168:INFO: 		 ADE on univ                      dataset:	 1.3513091802597046
2022-10-29 22:26:26,841:INFO: 		 ADE on zara1                     dataset:	 2.3691086769104004
2022-10-29 22:26:28,125:INFO: 		 ADE on zara2                     dataset:	 1.6482210159301758
2022-10-29 22:26:28,125:INFO: Average training:	ADE  1.4758	FDE  2.7171
2022-10-29 22:26:28,136:INFO:  --> Model Saved in ./models/eth/pretrain/P4/5.0/SSE_data_eth_irm[5.0]_epoch_404.pth.tar
2022-10-29 22:26:28,136:INFO: 
===> EPOCH: 405 (P4)
2022-10-29 22:26:28,137:INFO: - Computing loss (training)
2022-10-29 22:26:29,378:INFO: Dataset: hotel               Batch: 1/4	Loss 78.6508 (78.6508)
2022-10-29 22:26:30,403:INFO: Dataset: hotel               Batch: 2/4	Loss 80.3420 (79.5130)
2022-10-29 22:26:31,430:INFO: Dataset: hotel               Batch: 3/4	Loss 79.6920 (79.5744)
2022-10-29 22:26:32,181:INFO: Dataset: hotel               Batch: 4/4	Loss 48.6930 (73.9929)
2022-10-29 22:26:33,552:INFO: Dataset: univ                Batch:  1/15	Loss 92.2674 (92.2674)
2022-10-29 22:26:34,638:INFO: Dataset: univ                Batch:  2/15	Loss 100.0987 (96.1696)
2022-10-29 22:26:35,730:INFO: Dataset: univ                Batch:  3/15	Loss 83.5967 (92.0912)
2022-10-29 22:26:36,818:INFO: Dataset: univ                Batch:  4/15	Loss 82.5129 (89.9360)
2022-10-29 22:26:37,912:INFO: Dataset: univ                Batch:  5/15	Loss 85.9607 (89.1491)
2022-10-29 22:26:39,045:INFO: Dataset: univ                Batch:  6/15	Loss 84.6789 (88.4535)
2022-10-29 22:26:40,256:INFO: Dataset: univ                Batch:  7/15	Loss 95.3894 (89.3497)
2022-10-29 22:26:41,353:INFO: Dataset: univ                Batch:  8/15	Loss 89.2889 (89.3425)
2022-10-29 22:26:42,436:INFO: Dataset: univ                Batch:  9/15	Loss 87.4881 (89.1445)
2022-10-29 22:26:43,550:INFO: Dataset: univ                Batch: 10/15	Loss 124.8343 (93.3877)
2022-10-29 22:26:44,686:INFO: Dataset: univ                Batch: 11/15	Loss 111.5580 (94.9738)
2022-10-29 22:26:45,789:INFO: Dataset: univ                Batch: 12/15	Loss 94.9436 (94.9714)
2022-10-29 22:26:46,918:INFO: Dataset: univ                Batch: 13/15	Loss 111.3563 (96.2436)
2022-10-29 22:26:48,034:INFO: Dataset: univ                Batch: 14/15	Loss 94.5465 (96.1163)
2022-10-29 22:26:48,495:INFO: Dataset: univ                Batch: 15/15	Loss 15.4275 (94.9157)
2022-10-29 22:26:49,830:INFO: Dataset: zara1               Batch: 1/8	Loss 87.1048 (87.1048)
2022-10-29 22:26:50,886:INFO: Dataset: zara1               Batch: 2/8	Loss 86.9278 (87.0214)
2022-10-29 22:26:51,912:INFO: Dataset: zara1               Batch: 3/8	Loss 89.4226 (87.8293)
2022-10-29 22:26:52,949:INFO: Dataset: zara1               Batch: 4/8	Loss 85.8990 (87.3757)
2022-10-29 22:26:53,967:INFO: Dataset: zara1               Batch: 5/8	Loss 85.7514 (87.0516)
2022-10-29 22:26:54,975:INFO: Dataset: zara1               Batch: 6/8	Loss 84.3868 (86.6282)
2022-10-29 22:26:55,988:INFO: Dataset: zara1               Batch: 7/8	Loss 86.8948 (86.6611)
2022-10-29 22:26:56,903:INFO: Dataset: zara1               Batch: 8/8	Loss 73.7394 (85.0697)
2022-10-29 22:26:58,238:INFO: Dataset: zara2               Batch:  1/18	Loss 82.1562 (82.1562)
2022-10-29 22:26:59,270:INFO: Dataset: zara2               Batch:  2/18	Loss 88.0209 (85.0498)
2022-10-29 22:27:00,308:INFO: Dataset: zara2               Batch:  3/18	Loss 566.3779 (250.8972)
2022-10-29 22:27:01,348:INFO: Dataset: zara2               Batch:  4/18	Loss 108.3658 (217.2556)
2022-10-29 22:27:02,387:INFO: Dataset: zara2               Batch:  5/18	Loss 100.4142 (195.3391)
2022-10-29 22:27:03,422:INFO: Dataset: zara2               Batch:  6/18	Loss 84.6912 (176.9345)
2022-10-29 22:27:04,454:INFO: Dataset: zara2               Batch:  7/18	Loss 104.6506 (167.4844)
2022-10-29 22:27:05,489:INFO: Dataset: zara2               Batch:  8/18	Loss 102.8347 (159.9440)
2022-10-29 22:27:06,526:INFO: Dataset: zara2               Batch:  9/18	Loss 92.9290 (151.3243)
2022-10-29 22:27:07,585:INFO: Dataset: zara2               Batch: 10/18	Loss 93.0431 (145.0663)
2022-10-29 22:27:08,648:INFO: Dataset: zara2               Batch: 11/18	Loss 83.3485 (139.1268)
2022-10-29 22:27:09,712:INFO: Dataset: zara2               Batch: 12/18	Loss 118.2905 (137.3631)
2022-10-29 22:27:10,764:INFO: Dataset: zara2               Batch: 13/18	Loss 129.5538 (136.7585)
2022-10-29 22:27:11,812:INFO: Dataset: zara2               Batch: 14/18	Loss 93.2309 (133.3437)

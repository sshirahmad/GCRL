2022-11-07 14:47:50,471:INFO: Initializing Training Set
2022-11-07 14:47:55,617:INFO: Initializing Validation Set
2022-11-07 14:47:56,237:INFO: Initializing Validation O Set
2022-11-07 14:47:59,769:INFO: 
===> EPOCH: 1 (P1)
2022-11-07 14:47:59,770:INFO: - Computing loss (training)
2022-11-07 14:48:07,130:INFO: Dataset: hotel               Batch: 1/8	Loss 5.4863 (5.4863)
2022-11-07 14:48:07,479:INFO: Dataset: hotel               Batch: 2/8	Loss 7.2365 (6.3869)
2022-11-07 14:48:07,947:INFO: Dataset: hotel               Batch: 3/8	Loss 5.8261 (6.2012)
2022-11-07 14:48:08,188:INFO: Dataset: hotel               Batch: 4/8	Loss 4.6114 (5.7970)
2022-11-07 14:48:08,444:INFO: Dataset: hotel               Batch: 5/8	Loss 5.1174 (5.6510)
2022-11-07 14:48:08,697:INFO: Dataset: hotel               Batch: 6/8	Loss 5.1354 (5.5666)
2022-11-07 14:48:08,747:INFO: Dataset: hotel               Batch: 7/8	Loss 5.3843 (5.5399)
2022-11-07 14:48:08,769:INFO: Dataset: hotel               Batch: 8/8	Loss 1.7310 (5.4343)
2022-11-07 14:48:37,621:INFO: Dataset: univ                Batch:  1/29	Loss 7.5898 (7.5898)
2022-11-07 14:48:37,684:INFO: Dataset: univ                Batch:  2/29	Loss 5.0683 (6.1868)
2022-11-07 14:48:37,735:INFO: Dataset: univ                Batch:  3/29	Loss 5.5578 (5.9817)
2022-11-07 14:48:37,795:INFO: Dataset: univ                Batch:  4/29	Loss 7.5648 (6.3826)
2022-11-07 14:48:37,875:INFO: Dataset: univ                Batch:  5/29	Loss 6.6051 (6.4246)
2022-11-07 14:48:37,937:INFO: Dataset: univ                Batch:  6/29	Loss 7.8823 (6.6730)
2022-11-07 14:48:37,982:INFO: Dataset: univ                Batch:  7/29	Loss 6.7956 (6.6917)
2022-11-07 14:48:38,025:INFO: Dataset: univ                Batch:  8/29	Loss 5.3500 (6.5159)
2022-11-07 14:48:38,072:INFO: Dataset: univ                Batch:  9/29	Loss 6.6710 (6.5311)
2022-11-07 14:48:38,228:INFO: Dataset: univ                Batch: 10/29	Loss 7.4267 (6.6218)
2022-11-07 14:48:38,332:INFO: Dataset: univ                Batch: 11/29	Loss 6.6806 (6.6273)
2022-11-07 14:48:38,381:INFO: Dataset: univ                Batch: 12/29	Loss 5.8593 (6.5601)
2022-11-07 14:48:38,439:INFO: Dataset: univ                Batch: 13/29	Loss 5.5093 (6.4826)
2022-11-07 14:48:38,489:INFO: Dataset: univ                Batch: 14/29	Loss 5.2231 (6.3767)
2022-11-07 14:48:38,535:INFO: Dataset: univ                Batch: 15/29	Loss 6.1223 (6.3599)
2022-11-07 14:48:38,578:INFO: Dataset: univ                Batch: 16/29	Loss 5.8138 (6.3240)
2022-11-07 14:48:38,622:INFO: Dataset: univ                Batch: 17/29	Loss 5.2397 (6.2599)
2022-11-07 14:48:38,664:INFO: Dataset: univ                Batch: 18/29	Loss 6.2994 (6.2619)
2022-11-07 14:48:38,710:INFO: Dataset: univ                Batch: 19/29	Loss 7.3305 (6.3145)
2022-11-07 14:48:38,746:INFO: Dataset: univ                Batch: 20/29	Loss 6.1674 (6.3067)
2022-11-07 14:48:38,784:INFO: Dataset: univ                Batch: 21/29	Loss 6.7473 (6.3270)
2022-11-07 14:48:38,821:INFO: Dataset: univ                Batch: 22/29	Loss 4.4037 (6.2182)
2022-11-07 14:48:38,860:INFO: Dataset: univ                Batch: 23/29	Loss 5.4211 (6.1800)
2022-11-07 14:48:38,900:INFO: Dataset: univ                Batch: 24/29	Loss 4.6438 (6.1038)
2022-11-07 14:48:38,949:INFO: Dataset: univ                Batch: 25/29	Loss 6.3427 (6.1123)
2022-11-07 14:48:38,993:INFO: Dataset: univ                Batch: 26/29	Loss 3.8465 (6.0038)
2022-11-07 14:48:39,044:INFO: Dataset: univ                Batch: 27/29	Loss 7.1989 (6.0413)
2022-11-07 14:48:39,102:INFO: Dataset: univ                Batch: 28/29	Loss 4.5551 (5.9872)
2022-11-07 14:48:39,130:INFO: Dataset: univ                Batch: 29/29	Loss 2.6584 (5.9446)
2022-11-07 14:48:47,346:INFO: Dataset: zara1               Batch:  1/16	Loss 8.7811 (8.7811)
2022-11-07 14:48:47,760:INFO: Dataset: zara1               Batch:  2/16	Loss 8.5958 (8.6807)
2022-11-07 14:48:48,088:INFO: Dataset: zara1               Batch:  3/16	Loss 8.6136 (8.6619)
2022-11-07 14:48:48,376:INFO: Dataset: zara1               Batch:  4/16	Loss 8.3195 (8.5909)
2022-11-07 14:48:48,695:INFO: Dataset: zara1               Batch:  5/16	Loss 9.1040 (8.6843)
2022-11-07 14:48:48,911:INFO: Dataset: zara1               Batch:  6/16	Loss 8.5091 (8.6562)
2022-11-07 14:48:48,964:INFO: Dataset: zara1               Batch:  7/16	Loss 9.8781 (8.8356)
2022-11-07 14:48:49,015:INFO: Dataset: zara1               Batch:  8/16	Loss 8.8533 (8.8380)
2022-11-07 14:48:49,059:INFO: Dataset: zara1               Batch:  9/16	Loss 9.2711 (8.8882)
2022-11-07 14:48:49,104:INFO: Dataset: zara1               Batch: 10/16	Loss 8.5798 (8.8492)
2022-11-07 14:48:49,164:INFO: Dataset: zara1               Batch: 11/16	Loss 7.7463 (8.7372)
2022-11-07 14:48:49,207:INFO: Dataset: zara1               Batch: 12/16	Loss 8.3100 (8.7004)
2022-11-07 14:48:49,248:INFO: Dataset: zara1               Batch: 13/16	Loss 8.4621 (8.6827)
2022-11-07 14:48:49,288:INFO: Dataset: zara1               Batch: 14/16	Loss 7.3494 (8.5750)
2022-11-07 14:48:49,328:INFO: Dataset: zara1               Batch: 15/16	Loss 8.9869 (8.5999)
2022-11-07 14:48:49,364:INFO: Dataset: zara1               Batch: 16/16	Loss 5.8682 (8.4633)
2022-11-07 14:49:15,325:INFO: Dataset: zara2               Batch:  1/36	Loss 5.6076 (5.6076)
2022-11-07 14:49:15,381:INFO: Dataset: zara2               Batch:  2/36	Loss 5.1179 (5.3797)
2022-11-07 14:49:15,435:INFO: Dataset: zara2               Batch:  3/36	Loss 5.4627 (5.4053)
2022-11-07 14:49:15,484:INFO: Dataset: zara2               Batch:  4/36	Loss 5.8321 (5.5291)
2022-11-07 14:49:15,530:INFO: Dataset: zara2               Batch:  5/36	Loss 6.3214 (5.6961)
2022-11-07 14:49:15,613:INFO: Dataset: zara2               Batch:  6/36	Loss 5.4958 (5.6602)
2022-11-07 14:49:15,656:INFO: Dataset: zara2               Batch:  7/36	Loss 6.2899 (5.7414)
2022-11-07 14:49:15,704:INFO: Dataset: zara2               Batch:  8/36	Loss 6.5170 (5.8262)
2022-11-07 14:49:15,743:INFO: Dataset: zara2               Batch:  9/36	Loss 4.4823 (5.6729)
2022-11-07 14:49:15,797:INFO: Dataset: zara2               Batch: 10/36	Loss 5.4659 (5.6507)
2022-11-07 14:49:15,836:INFO: Dataset: zara2               Batch: 11/36	Loss 5.7766 (5.6654)
2022-11-07 14:49:15,875:INFO: Dataset: zara2               Batch: 12/36	Loss 5.4663 (5.6494)
2022-11-07 14:49:15,915:INFO: Dataset: zara2               Batch: 13/36	Loss 5.0882 (5.6109)
2022-11-07 14:49:15,959:INFO: Dataset: zara2               Batch: 14/36	Loss 5.1790 (5.5779)
2022-11-07 14:49:16,017:INFO: Dataset: zara2               Batch: 15/36	Loss 4.9330 (5.5327)
2022-11-07 14:49:16,065:INFO: Dataset: zara2               Batch: 16/36	Loss 4.7304 (5.4874)
2022-11-07 14:49:16,103:INFO: Dataset: zara2               Batch: 17/36	Loss 4.7478 (5.4435)
2022-11-07 14:49:16,147:INFO: Dataset: zara2               Batch: 18/36	Loss 4.5996 (5.3969)
2022-11-07 14:49:16,191:INFO: Dataset: zara2               Batch: 19/36	Loss 6.0789 (5.4334)
2022-11-07 14:49:16,245:INFO: Dataset: zara2               Batch: 20/36	Loss 5.0039 (5.4097)
2022-11-07 14:49:16,283:INFO: Dataset: zara2               Batch: 21/36	Loss 5.3399 (5.4064)
2022-11-07 14:49:16,323:INFO: Dataset: zara2               Batch: 22/36	Loss 5.6493 (5.4177)
2022-11-07 14:49:16,363:INFO: Dataset: zara2               Batch: 23/36	Loss 5.1816 (5.4072)
2022-11-07 14:49:16,404:INFO: Dataset: zara2               Batch: 24/36	Loss 4.2988 (5.3587)
2022-11-07 14:49:16,453:INFO: Dataset: zara2               Batch: 25/36	Loss 4.5334 (5.3262)
2022-11-07 14:49:16,494:INFO: Dataset: zara2               Batch: 26/36	Loss 4.8379 (5.3059)
2022-11-07 14:49:16,531:INFO: Dataset: zara2               Batch: 27/36	Loss 3.6865 (5.2460)
2022-11-07 14:49:16,575:INFO: Dataset: zara2               Batch: 28/36	Loss 4.3046 (5.2145)
2022-11-07 14:49:16,611:INFO: Dataset: zara2               Batch: 29/36	Loss 4.6679 (5.1949)
2022-11-07 14:49:16,667:INFO: Dataset: zara2               Batch: 30/36	Loss 4.3855 (5.1656)
2022-11-07 14:49:16,721:INFO: Dataset: zara2               Batch: 31/36	Loss 4.2064 (5.1335)
2022-11-07 14:49:16,755:INFO: Dataset: zara2               Batch: 32/36	Loss 4.4625 (5.1122)
2022-11-07 14:49:16,792:INFO: Dataset: zara2               Batch: 33/36	Loss 4.0736 (5.0805)
2022-11-07 14:49:16,829:INFO: Dataset: zara2               Batch: 34/36	Loss 3.7099 (5.0374)
2022-11-07 14:49:16,876:INFO: Dataset: zara2               Batch: 35/36	Loss 4.1095 (5.0106)
2022-11-07 14:49:16,907:INFO: Dataset: zara2               Batch: 36/36	Loss 2.4352 (4.9570)
2022-11-07 14:49:17,840:INFO:  --> Model Saved in ./models/eth/CRMF_risk_erm_0.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 300, 100)_shuffle_true_seed_72/pretrain/P1/CRMF_epoch_1.pth.tar
2022-11-07 14:49:17,840:INFO: 
===> EPOCH: 2 (P1)
2022-11-07 14:49:17,841:INFO: - Computing loss (training)
2022-11-07 14:49:24,090:INFO: Dataset: hotel               Batch: 1/8	Loss 6.4577 (6.4577)
2022-11-07 14:49:24,476:INFO: Dataset: hotel               Batch: 2/8	Loss 5.4794 (5.9614)
2022-11-07 14:49:24,861:INFO: Dataset: hotel               Batch: 3/8	Loss 5.2940 (5.7339)
2022-11-07 14:49:25,214:INFO: Dataset: hotel               Batch: 4/8	Loss 3.8683 (5.2904)
2022-11-07 14:49:25,474:INFO: Dataset: hotel               Batch: 5/8	Loss 5.5063 (5.3356)
2022-11-07 14:49:25,680:INFO: Dataset: hotel               Batch: 6/8	Loss 6.5276 (5.5466)
2022-11-07 14:49:25,745:INFO: Dataset: hotel               Batch: 7/8	Loss 5.1448 (5.4894)
2022-11-07 14:49:25,766:INFO: Dataset: hotel               Batch: 8/8	Loss 1.5732 (5.3500)
2022-11-07 14:49:51,504:INFO: Dataset: univ                Batch:  1/29	Loss 3.2346 (3.2346)
2022-11-07 14:49:51,591:INFO: Dataset: univ                Batch:  2/29	Loss 3.6513 (3.4442)
2022-11-07 14:49:51,670:INFO: Dataset: univ                Batch:  3/29	Loss 3.7382 (3.5453)
2022-11-07 14:49:51,745:INFO: Dataset: univ                Batch:  4/29	Loss 3.8404 (3.6196)
2022-11-07 14:49:51,813:INFO: Dataset: univ                Batch:  5/29	Loss 3.2210 (3.5350)
2022-11-07 14:49:51,880:INFO: Dataset: univ                Batch:  6/29	Loss 3.0114 (3.4436)
2022-11-07 14:49:51,940:INFO: Dataset: univ                Batch:  7/29	Loss 3.8514 (3.4965)
2022-11-07 14:49:52,003:INFO: Dataset: univ                Batch:  8/29	Loss 4.0016 (3.5527)
2022-11-07 14:49:52,056:INFO: Dataset: univ                Batch:  9/29	Loss 2.9126 (3.4757)
2022-11-07 14:49:52,112:INFO: Dataset: univ                Batch: 10/29	Loss 3.4476 (3.4729)
2022-11-07 14:49:52,162:INFO: Dataset: univ                Batch: 11/29	Loss 3.3976 (3.4663)
2022-11-07 14:49:52,222:INFO: Dataset: univ                Batch: 12/29	Loss 3.8712 (3.4999)
2022-11-07 14:49:52,289:INFO: Dataset: univ                Batch: 13/29	Loss 2.7550 (3.4386)
2022-11-07 14:49:52,341:INFO: Dataset: univ                Batch: 14/29	Loss 3.1273 (3.4152)
2022-11-07 14:49:52,406:INFO: Dataset: univ                Batch: 15/29	Loss 3.8742 (3.4418)
2022-11-07 14:49:52,458:INFO: Dataset: univ                Batch: 16/29	Loss 3.6468 (3.4544)
2022-11-07 14:49:52,508:INFO: Dataset: univ                Batch: 17/29	Loss 3.6776 (3.4673)
2022-11-07 14:49:52,560:INFO: Dataset: univ                Batch: 18/29	Loss 3.2112 (3.4517)
2022-11-07 14:49:52,618:INFO: Dataset: univ                Batch: 19/29	Loss 2.8279 (3.4135)
2022-11-07 14:49:52,671:INFO: Dataset: univ                Batch: 20/29	Loss 3.3509 (3.4105)
2022-11-07 14:49:52,727:INFO: Dataset: univ                Batch: 21/29	Loss 3.1051 (3.3962)
2022-11-07 14:49:52,785:INFO: Dataset: univ                Batch: 22/29	Loss 3.2747 (3.3908)
2022-11-07 14:49:52,833:INFO: Dataset: univ                Batch: 23/29	Loss 3.0522 (3.3767)
2022-11-07 14:49:52,887:INFO: Dataset: univ                Batch: 24/29	Loss 3.1618 (3.3676)
2022-11-07 14:49:52,940:INFO: Dataset: univ                Batch: 25/29	Loss 3.6324 (3.3768)
2022-11-07 14:49:53,001:INFO: Dataset: univ                Batch: 26/29	Loss 3.0823 (3.3652)
2022-11-07 14:49:53,052:INFO: Dataset: univ                Batch: 27/29	Loss 3.7198 (3.3768)
2022-11-07 14:49:53,103:INFO: Dataset: univ                Batch: 28/29	Loss 3.1878 (3.3704)
2022-11-07 14:49:53,133:INFO: Dataset: univ                Batch: 29/29	Loss 0.9370 (3.3348)
2022-11-07 14:50:01,482:INFO: Dataset: zara1               Batch:  1/16	Loss 4.5669 (4.5669)
2022-11-07 14:50:01,762:INFO: Dataset: zara1               Batch:  2/16	Loss 3.8706 (4.2330)
2022-11-07 14:50:02,218:INFO: Dataset: zara1               Batch:  3/16	Loss 4.2680 (4.2430)
2022-11-07 14:50:02,521:INFO: Dataset: zara1               Batch:  4/16	Loss 3.8522 (4.1357)
2022-11-07 14:50:02,841:INFO: Dataset: zara1               Batch:  5/16	Loss 3.6424 (4.0295)
2022-11-07 14:50:03,055:INFO: Dataset: zara1               Batch:  6/16	Loss 3.7717 (3.9795)
2022-11-07 14:50:03,115:INFO: Dataset: zara1               Batch:  7/16	Loss 3.9000 (3.9689)
2022-11-07 14:50:03,165:INFO: Dataset: zara1               Batch:  8/16	Loss 3.3195 (3.8857)
2022-11-07 14:50:03,209:INFO: Dataset: zara1               Batch:  9/16	Loss 3.4105 (3.8387)
2022-11-07 14:50:03,260:INFO: Dataset: zara1               Batch: 10/16	Loss 3.5137 (3.8020)
2022-11-07 14:50:03,315:INFO: Dataset: zara1               Batch: 11/16	Loss 3.6649 (3.7914)
2022-11-07 14:50:03,374:INFO: Dataset: zara1               Batch: 12/16	Loss 3.8443 (3.7954)
2022-11-07 14:50:03,424:INFO: Dataset: zara1               Batch: 13/16	Loss 3.7846 (3.7946)
2022-11-07 14:50:03,472:INFO: Dataset: zara1               Batch: 14/16	Loss 3.1870 (3.7523)
2022-11-07 14:50:03,549:INFO: Dataset: zara1               Batch: 15/16	Loss 3.4948 (3.7334)
2022-11-07 14:50:03,611:INFO: Dataset: zara1               Batch: 16/16	Loss 2.2363 (3.6648)
2022-11-07 14:50:30,260:INFO: Dataset: zara2               Batch:  1/36	Loss 2.6738 (2.6738)
2022-11-07 14:50:30,374:INFO: Dataset: zara2               Batch:  2/36	Loss 2.4856 (2.5764)
2022-11-07 14:50:30,470:INFO: Dataset: zara2               Batch:  3/36	Loss 2.8671 (2.6675)
2022-11-07 14:50:30,544:INFO: Dataset: zara2               Batch:  4/36	Loss 2.9374 (2.7341)
2022-11-07 14:50:30,608:INFO: Dataset: zara2               Batch:  5/36	Loss 2.7140 (2.7303)
2022-11-07 14:50:30,663:INFO: Dataset: zara2               Batch:  6/36	Loss 2.6855 (2.7242)
2022-11-07 14:50:30,720:INFO: Dataset: zara2               Batch:  7/36	Loss 2.2255 (2.6515)
2022-11-07 14:50:30,776:INFO: Dataset: zara2               Batch:  8/36	Loss 2.5376 (2.6364)
2022-11-07 14:50:30,834:INFO: Dataset: zara2               Batch:  9/36	Loss 2.4034 (2.6067)
2022-11-07 14:50:30,888:INFO: Dataset: zara2               Batch: 10/36	Loss 3.0985 (2.6545)
2022-11-07 14:50:30,946:INFO: Dataset: zara2               Batch: 11/36	Loss 3.0122 (2.6927)
2022-11-07 14:50:31,008:INFO: Dataset: zara2               Batch: 12/36	Loss 2.6909 (2.6925)
2022-11-07 14:50:31,067:INFO: Dataset: zara2               Batch: 13/36	Loss 2.4142 (2.6688)
2022-11-07 14:50:31,123:INFO: Dataset: zara2               Batch: 14/36	Loss 2.4122 (2.6505)
2022-11-07 14:50:31,176:INFO: Dataset: zara2               Batch: 15/36	Loss 2.2482 (2.6222)
2022-11-07 14:50:31,237:INFO: Dataset: zara2               Batch: 16/36	Loss 2.3174 (2.6035)
2022-11-07 14:50:31,295:INFO: Dataset: zara2               Batch: 17/36	Loss 2.1193 (2.5695)
2022-11-07 14:50:31,346:INFO: Dataset: zara2               Batch: 18/36	Loss 2.1403 (2.5455)
2022-11-07 14:50:31,398:INFO: Dataset: zara2               Batch: 19/36	Loss 2.2694 (2.5287)
2022-11-07 14:50:31,458:INFO: Dataset: zara2               Batch: 20/36	Loss 2.0848 (2.5048)
2022-11-07 14:50:31,524:INFO: Dataset: zara2               Batch: 21/36	Loss 2.0714 (2.4850)
2022-11-07 14:50:31,578:INFO: Dataset: zara2               Batch: 22/36	Loss 2.5268 (2.4868)
2022-11-07 14:50:31,637:INFO: Dataset: zara2               Batch: 23/36	Loss 1.9796 (2.4657)
2022-11-07 14:50:31,693:INFO: Dataset: zara2               Batch: 24/36	Loss 2.2917 (2.4589)
2022-11-07 14:50:31,753:INFO: Dataset: zara2               Batch: 25/36	Loss 2.2419 (2.4518)
2022-11-07 14:50:31,806:INFO: Dataset: zara2               Batch: 26/36	Loss 1.8299 (2.4293)
2022-11-07 14:50:31,853:INFO: Dataset: zara2               Batch: 27/36	Loss 1.9895 (2.4113)
2022-11-07 14:50:31,898:INFO: Dataset: zara2               Batch: 28/36	Loss 1.8981 (2.3931)
2022-11-07 14:50:31,950:INFO: Dataset: zara2               Batch: 29/36	Loss 1.7703 (2.3712)
2022-11-07 14:50:32,011:INFO: Dataset: zara2               Batch: 30/36	Loss 1.7138 (2.3500)
2022-11-07 14:50:32,072:INFO: Dataset: zara2               Batch: 31/36	Loss 2.4404 (2.3525)
2022-11-07 14:50:32,123:INFO: Dataset: zara2               Batch: 32/36	Loss 2.3533 (2.3525)
2022-11-07 14:50:32,176:INFO: Dataset: zara2               Batch: 33/36	Loss 2.0340 (2.3428)
2022-11-07 14:50:32,240:INFO: Dataset: zara2               Batch: 34/36	Loss 1.6015 (2.3157)
2022-11-07 14:50:32,291:INFO: Dataset: zara2               Batch: 35/36	Loss 1.9899 (2.3071)
2022-11-07 14:50:32,334:INFO: Dataset: zara2               Batch: 36/36	Loss 1.4625 (2.2905)
2022-11-07 14:50:33,402:INFO:  --> Model Saved in ./models/eth/CRMF_risk_erm_0.0_batch_hom_data_eth_ds_0_bk_20_ep_(2, 2, 2, 300, 100)_shuffle_true_seed_72/pretrain/P1/CRMF_epoch_2.pth.tar
2022-11-07 14:50:33,403:INFO: 
===> EPOCH: 3 (P2)
2022-11-07 14:50:33,403:INFO: - Computing loss (training)
2022-11-07 14:50:41,127:INFO: Dataset: hotel               Batch: 1/8	Loss 5.3096 (5.3096)
2022-11-07 14:50:41,566:INFO: Dataset: hotel               Batch: 2/8	Loss 5.1479 (5.2246)
2022-11-07 14:50:41,995:INFO: Dataset: hotel               Batch: 3/8	Loss 4.5154 (4.9682)
2022-11-07 14:50:42,327:INFO: Dataset: hotel               Batch: 4/8	Loss 7.1793 (5.5277)
2022-11-07 14:50:42,638:INFO: Dataset: hotel               Batch: 5/8	Loss 6.2752 (5.6775)
2022-11-07 14:50:42,945:INFO: Dataset: hotel               Batch: 6/8	Loss 5.3661 (5.6226)
2022-11-07 14:50:43,231:INFO: Dataset: hotel               Batch: 7/8	Loss 6.1622 (5.7035)
2022-11-07 14:50:43,336:INFO: Dataset: hotel               Batch: 8/8	Loss 1.3771 (5.5665)
2022-11-07 14:51:10,828:INFO: Dataset: univ                Batch:  1/29	Loss 6.0220 (6.0220)
2022-11-07 14:51:11,092:INFO: Dataset: univ                Batch:  2/29	Loss 5.8579 (5.9407)
2022-11-07 14:51:11,349:INFO: Dataset: univ                Batch:  3/29	Loss 5.1201 (5.6203)
2022-11-07 14:51:11,642:INFO: Dataset: univ                Batch:  4/29	Loss 5.5502 (5.6056)
2022-11-07 14:51:11,922:INFO: Dataset: univ                Batch:  5/29	Loss 5.0898 (5.4931)
2022-11-07 14:51:12,181:INFO: Dataset: univ                Batch:  6/29	Loss 4.8725 (5.3892)
2022-11-07 14:51:12,452:INFO: Dataset: univ                Batch:  7/29	Loss 5.2318 (5.3657)
2022-11-07 14:51:12,688:INFO: Dataset: univ                Batch:  8/29	Loss 6.4141 (5.4665)
2022-11-07 14:51:12,908:INFO: Dataset: univ                Batch:  9/29	Loss 6.0110 (5.5231)
2022-11-07 14:51:13,125:INFO: Dataset: univ                Batch: 10/29	Loss 5.4583 (5.5173)
2022-11-07 14:51:13,348:INFO: Dataset: univ                Batch: 11/29	Loss 4.4392 (5.4120)
2022-11-07 14:51:13,586:INFO: Dataset: univ                Batch: 12/29	Loss 4.8659 (5.3681)
2022-11-07 14:51:13,800:INFO: Dataset: univ                Batch: 13/29	Loss 5.5154 (5.3795)
2022-11-07 14:51:14,011:INFO: Dataset: univ                Batch: 14/29	Loss 5.4489 (5.3843)
2022-11-07 14:51:14,252:INFO: Dataset: univ                Batch: 15/29	Loss 5.6161 (5.3990)
2022-11-07 14:51:14,473:INFO: Dataset: univ                Batch: 16/29	Loss 4.4881 (5.3348)
2022-11-07 14:51:14,721:INFO: Dataset: univ                Batch: 17/29	Loss 4.1151 (5.2709)
2022-11-07 14:51:14,959:INFO: Dataset: univ                Batch: 18/29	Loss 3.5475 (5.1568)
2022-11-07 14:51:15,203:INFO: Dataset: univ                Batch: 19/29	Loss 5.8197 (5.1867)
2022-11-07 14:51:15,428:INFO: Dataset: univ                Batch: 20/29	Loss 4.0713 (5.1245)
2022-11-07 14:51:15,646:INFO: Dataset: univ                Batch: 21/29	Loss 4.3636 (5.0933)
2022-11-07 14:51:15,867:INFO: Dataset: univ                Batch: 22/29	Loss 3.4233 (5.0171)
2022-11-07 14:51:16,129:INFO: Dataset: univ                Batch: 23/29	Loss 3.6321 (4.9546)
2022-11-07 14:51:16,493:INFO: Dataset: univ                Batch: 24/29	Loss 3.8206 (4.9062)
2022-11-07 14:51:16,785:INFO: Dataset: univ                Batch: 25/29	Loss 3.7212 (4.8592)
2022-11-07 14:51:17,137:INFO: Dataset: univ                Batch: 26/29	Loss 4.3024 (4.8395)
2022-11-07 14:51:17,425:INFO: Dataset: univ                Batch: 27/29	Loss 3.6125 (4.7987)
2022-11-07 14:51:17,728:INFO: Dataset: univ                Batch: 28/29	Loss 3.1161 (4.7366)
2022-11-07 14:51:17,852:INFO: Dataset: univ                Batch: 29/29	Loss 0.9171 (4.6651)
2022-11-07 14:51:27,292:INFO: Dataset: zara1               Batch:  1/16	Loss 5.3098 (5.3098)
2022-11-07 14:51:28,458:INFO: Dataset: zara1               Batch:  2/16	Loss 5.0613 (5.1807)
2022-11-07 14:51:29,040:INFO: Dataset: zara1               Batch:  3/16	Loss 4.9564 (5.1106)
2022-11-07 14:51:29,408:INFO: Dataset: zara1               Batch:  4/16	Loss 4.5389 (4.9358)
2022-11-07 14:51:29,694:INFO: Dataset: zara1               Batch:  5/16	Loss 4.0365 (4.7489)
2022-11-07 14:51:29,983:INFO: Dataset: zara1               Batch:  6/16	Loss 4.8444 (4.7650)
2022-11-07 14:51:30,253:INFO: Dataset: zara1               Batch:  7/16	Loss 4.6809 (4.7546)
2022-11-07 14:51:30,525:INFO: Dataset: zara1               Batch:  8/16	Loss 4.6782 (4.7445)
2022-11-07 14:51:30,879:INFO: Dataset: zara1               Batch:  9/16	Loss 3.9069 (4.6445)
2022-11-07 14:51:31,139:INFO: Dataset: zara1               Batch: 10/16	Loss 4.4352 (4.6241)
2022-11-07 14:51:31,427:INFO: Dataset: zara1               Batch: 11/16	Loss 4.1359 (4.5776)
2022-11-07 14:51:31,703:INFO: Dataset: zara1               Batch: 12/16	Loss 3.8906 (4.5218)
2022-11-07 14:51:31,954:INFO: Dataset: zara1               Batch: 13/16	Loss 3.9452 (4.4808)
2022-11-07 14:51:32,178:INFO: Dataset: zara1               Batch: 14/16	Loss 3.9107 (4.4364)
2022-11-07 14:51:32,422:INFO: Dataset: zara1               Batch: 15/16	Loss 3.7051 (4.3937)
2022-11-07 14:51:32,609:INFO: Dataset: zara1               Batch: 16/16	Loss 2.7045 (4.3182)

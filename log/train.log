2022-11-13 19:21:02,500:INFO: Initializing Training Set
2022-11-13 19:21:07,178:INFO: Initializing Validation Set
2022-11-13 19:21:07,674:INFO: Initializing Validation O Set
2022-11-13 19:21:13,068:INFO: 
===> EPOCH: 3 (P2)
2022-11-13 19:21:13,069:INFO: - Computing loss (training)
2022-11-13 19:21:25,496:INFO: Dataset: hotel               Batch: 1/8	Loss 315.9842 (315.9842)
2022-11-13 19:21:28,332:INFO: Dataset: hotel               Batch: 2/8	Loss 208.7322 (264.2257)
2022-11-13 19:21:30,784:INFO: Dataset: hotel               Batch: 3/8	Loss 155.9647 (226.3869)
2022-11-13 19:21:33,153:INFO: Dataset: hotel               Batch: 4/8	Loss 123.5921 (201.8276)
2022-11-13 19:21:35,534:INFO: Dataset: hotel               Batch: 5/8	Loss 103.0202 (181.8331)
2022-11-13 19:21:38,201:INFO: Dataset: hotel               Batch: 6/8	Loss 87.1793 (164.5129)
2022-11-13 19:21:40,909:INFO: Dataset: hotel               Batch: 7/8	Loss 76.5887 (151.7283)
2022-11-13 19:21:44,424:INFO: Dataset: hotel               Batch: 8/8	Loss 14.8632 (146.4920)
2022-11-13 19:22:13,874:INFO: Dataset: univ                Batch:  1/29	Loss 137.6743 (137.6743)
2022-11-13 19:22:16,442:INFO: Dataset: univ                Batch:  2/29	Loss 117.0445 (128.3418)
2022-11-13 19:22:18,828:INFO: Dataset: univ                Batch:  3/29	Loss 94.6949 (118.3462)
2022-11-13 19:22:21,598:INFO: Dataset: univ                Batch:  4/29	Loss 73.4614 (106.9848)
2022-11-13 19:22:25,345:INFO: Dataset: univ                Batch:  5/29	Loss 61.4707 (97.4423)
2022-11-13 19:22:29,051:INFO: Dataset: univ                Batch:  6/29	Loss 52.7958 (90.2910)
2022-11-13 19:22:31,526:INFO: Dataset: univ                Batch:  7/29	Loss 49.9718 (84.5118)
2022-11-13 19:22:34,106:INFO: Dataset: univ                Batch:  8/29	Loss 45.2439 (79.5652)
2022-11-13 19:22:36,958:INFO: Dataset: univ                Batch:  9/29	Loss 41.7920 (74.9430)
2022-11-13 19:22:40,274:INFO: Dataset: univ                Batch: 10/29	Loss 38.6919 (71.3698)
2022-11-13 19:22:43,808:INFO: Dataset: univ                Batch: 11/29	Loss 36.5699 (68.3462)
2022-11-13 19:22:47,524:INFO: Dataset: univ                Batch: 12/29	Loss 33.0963 (65.2993)
2022-11-13 19:22:51,043:INFO: Dataset: univ                Batch: 13/29	Loss 31.2262 (62.5983)
2022-11-13 19:22:53,635:INFO: Dataset: univ                Batch: 14/29	Loss 28.6356 (60.1069)
2022-11-13 19:22:56,161:INFO: Dataset: univ                Batch: 15/29	Loss 26.8995 (57.8935)
2022-11-13 19:23:00,012:INFO: Dataset: univ                Batch: 16/29	Loss 24.4358 (55.5803)
2022-11-13 19:23:02,777:INFO: Dataset: univ                Batch: 17/29	Loss 24.6535 (54.0233)
2022-11-13 19:23:05,464:INFO: Dataset: univ                Batch: 18/29	Loss 22.5577 (52.3130)
2022-11-13 19:23:08,031:INFO: Dataset: univ                Batch: 19/29	Loss 21.1919 (50.5944)
2022-11-13 19:23:11,027:INFO: Dataset: univ                Batch: 20/29	Loss 20.5268 (49.1882)
2022-11-13 19:23:13,623:INFO: Dataset: univ                Batch: 21/29	Loss 19.0310 (47.7379)
2022-11-13 19:23:17,016:INFO: Dataset: univ                Batch: 22/29	Loss 18.1909 (46.4927)
2022-11-13 19:23:19,771:INFO: Dataset: univ                Batch: 23/29	Loss 16.6692 (44.9571)
2022-11-13 19:23:22,451:INFO: Dataset: univ                Batch: 24/29	Loss 15.7137 (43.5956)
2022-11-13 19:23:25,033:INFO: Dataset: univ                Batch: 25/29	Loss 16.0008 (42.5006)
2022-11-13 19:23:27,630:INFO: Dataset: univ                Batch: 26/29	Loss 15.4571 (41.3379)
2022-11-13 19:23:30,208:INFO: Dataset: univ                Batch: 27/29	Loss 14.0065 (40.3760)
2022-11-13 19:23:32,732:INFO: Dataset: univ                Batch: 28/29	Loss 13.8559 (39.4227)
2022-11-13 19:23:35,123:INFO: Dataset: univ                Batch: 29/29	Loss 5.0691 (38.8985)
2022-11-13 19:23:46,481:INFO: Dataset: zara1               Batch:  1/16	Loss 76.3191 (76.3191)
2022-11-13 19:23:48,945:INFO: Dataset: zara1               Batch:  2/16	Loss 62.3825 (68.8325)
2022-11-13 19:23:51,372:INFO: Dataset: zara1               Batch:  3/16	Loss 48.2530 (62.6468)
2022-11-13 19:23:53,800:INFO: Dataset: zara1               Batch:  4/16	Loss 35.8740 (56.7375)
2022-11-13 19:23:56,234:INFO: Dataset: zara1               Batch:  5/16	Loss 26.6918 (50.9028)
2022-11-13 19:23:58,683:INFO: Dataset: zara1               Batch:  6/16	Loss 21.3042 (44.9745)
2022-11-13 19:24:01,198:INFO: Dataset: zara1               Batch:  7/16	Loss 18.6126 (40.9263)
2022-11-13 19:24:03,663:INFO: Dataset: zara1               Batch:  8/16	Loss 18.3628 (37.9435)
2022-11-13 19:24:06,249:INFO: Dataset: zara1               Batch:  9/16	Loss 17.9953 (35.4175)
2022-11-13 19:24:08,715:INFO: Dataset: zara1               Batch: 10/16	Loss 19.4078 (33.9301)
2022-11-13 19:24:11,182:INFO: Dataset: zara1               Batch: 11/16	Loss 19.8542 (32.5957)
2022-11-13 19:24:13,828:INFO: Dataset: zara1               Batch: 12/16	Loss 19.4130 (31.1829)
2022-11-13 19:24:16,464:INFO: Dataset: zara1               Batch: 13/16	Loss 19.9915 (30.3683)
2022-11-13 19:24:19,183:INFO: Dataset: zara1               Batch: 14/16	Loss 19.0998 (29.5976)
2022-11-13 19:24:21,669:INFO: Dataset: zara1               Batch: 15/16	Loss 18.1788 (28.8784)
2022-11-13 19:24:24,051:INFO: Dataset: zara1               Batch: 16/16	Loss 12.4171 (28.0987)

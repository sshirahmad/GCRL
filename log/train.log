2022-10-28 14:17:50,805:INFO: Initializing Training Set
2022-10-28 14:17:54,382:INFO: Initializing Validation Set
2022-10-28 14:17:54,803:INFO: Initializing Validation O Set
2022-10-28 14:17:57,140:INFO: => loaded checkpoint './models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_243.pth.tar' (epoch 244)
2022-10-28 14:17:57,141:INFO: 
===> EPOCH: 244 (P2)
2022-10-28 14:17:57,141:INFO: - Computing loss (training)
2022-10-28 14:18:04,390:INFO: Dataset: hotel               Batch: 1/8	Loss 35.5455 (35.5455)
2022-10-28 14:18:06,095:INFO: Dataset: hotel               Batch: 2/8	Loss 35.6256 (35.5863)
2022-10-28 14:18:07,762:INFO: Dataset: hotel               Batch: 3/8	Loss 35.7277 (35.6303)
2022-10-28 14:18:09,454:INFO: Dataset: hotel               Batch: 4/8	Loss 35.6930 (35.6470)
2022-10-28 14:18:11,172:INFO: Dataset: hotel               Batch: 5/8	Loss 35.5790 (35.6316)
2022-10-28 14:18:12,906:INFO: Dataset: hotel               Batch: 6/8	Loss 35.6689 (35.6375)
2022-10-28 14:18:14,633:INFO: Dataset: hotel               Batch: 7/8	Loss 35.6217 (35.6354)
2022-10-28 14:18:15,698:INFO: Dataset: hotel               Batch: 8/8	Loss 7.8347 (34.8652)
2022-10-28 14:18:40,197:INFO: Dataset: univ                Batch:  1/29	Loss 35.4759 (35.4759)
2022-10-28 14:18:42,987:INFO: Dataset: univ                Batch:  2/29	Loss 35.3425 (35.4081)
2022-10-28 14:18:45,768:INFO: Dataset: univ                Batch:  3/29	Loss 35.4377 (35.4178)
2022-10-28 14:18:48,204:INFO: Dataset: univ                Batch:  4/29	Loss 35.3862 (35.4088)
2022-10-28 14:18:50,633:INFO: Dataset: univ                Batch:  5/29	Loss 35.3942 (35.4059)
2022-10-28 14:18:53,263:INFO: Dataset: univ                Batch:  6/29	Loss 35.4194 (35.4081)
2022-10-28 14:18:55,862:INFO: Dataset: univ                Batch:  7/29	Loss 35.3826 (35.4045)
2022-10-28 14:18:58,142:INFO: Dataset: univ                Batch:  8/29	Loss 35.3959 (35.4035)
2022-10-28 14:19:00,432:INFO: Dataset: univ                Batch:  9/29	Loss 35.3928 (35.4025)
2022-10-28 14:19:03,106:INFO: Dataset: univ                Batch: 10/29	Loss 35.4316 (35.4056)
2022-10-28 14:19:05,804:INFO: Dataset: univ                Batch: 11/29	Loss 35.3676 (35.4022)
2022-10-28 14:19:08,332:INFO: Dataset: univ                Batch: 12/29	Loss 35.4286 (35.4045)
2022-10-28 14:19:10,679:INFO: Dataset: univ                Batch: 13/29	Loss 35.3625 (35.4008)
2022-10-28 14:19:13,426:INFO: Dataset: univ                Batch: 14/29	Loss 35.3922 (35.4002)
2022-10-28 14:19:16,140:INFO: Dataset: univ                Batch: 15/29	Loss 35.4019 (35.4003)
2022-10-28 14:19:18,599:INFO: Dataset: univ                Batch: 16/29	Loss 35.4438 (35.4031)
2022-10-28 14:19:20,840:INFO: Dataset: univ                Batch: 17/29	Loss 35.3794 (35.4017)
2022-10-28 14:19:23,565:INFO: Dataset: univ                Batch: 18/29	Loss 35.3938 (35.4012)
2022-10-28 14:19:26,095:INFO: Dataset: univ                Batch: 19/29	Loss 35.4003 (35.4012)
2022-10-28 14:19:28,455:INFO: Dataset: univ                Batch: 20/29	Loss 35.3660 (35.3992)
2022-10-28 14:19:30,770:INFO: Dataset: univ                Batch: 21/29	Loss 35.4108 (35.3997)
2022-10-28 14:19:33,293:INFO: Dataset: univ                Batch: 22/29	Loss 35.4118 (35.4003)
2022-10-28 14:19:35,825:INFO: Dataset: univ                Batch: 23/29	Loss 35.4065 (35.4005)
2022-10-28 14:19:38,158:INFO: Dataset: univ                Batch: 24/29	Loss 35.4891 (35.4039)
2022-10-28 14:19:40,478:INFO: Dataset: univ                Batch: 25/29	Loss 35.4020 (35.4038)
2022-10-28 14:19:42,964:INFO: Dataset: univ                Batch: 26/29	Loss 35.3961 (35.4035)
2022-10-28 14:19:45,474:INFO: Dataset: univ                Batch: 27/29	Loss 35.3795 (35.4026)
2022-10-28 14:19:47,958:INFO: Dataset: univ                Batch: 28/29	Loss 35.3686 (35.4013)
2022-10-28 14:19:49,493:INFO: Dataset: univ                Batch: 29/29	Loss 13.3364 (35.0844)
2022-10-28 14:19:59,457:INFO: Dataset: zara1               Batch:  1/16	Loss 35.2318 (35.2318)
2022-10-28 14:20:01,550:INFO: Dataset: zara1               Batch:  2/16	Loss 35.2115 (35.2222)
2022-10-28 14:20:03,483:INFO: Dataset: zara1               Batch:  3/16	Loss 35.1928 (35.2117)
2022-10-28 14:20:05,171:INFO: Dataset: zara1               Batch:  4/16	Loss 35.1855 (35.2049)
2022-10-28 14:20:07,207:INFO: Dataset: zara1               Batch:  5/16	Loss 35.2563 (35.2150)
2022-10-28 14:20:09,430:INFO: Dataset: zara1               Batch:  6/16	Loss 35.1505 (35.2044)
2022-10-28 14:20:11,573:INFO: Dataset: zara1               Batch:  7/16	Loss 35.2176 (35.2061)
2022-10-28 14:20:13,703:INFO: Dataset: zara1               Batch:  8/16	Loss 35.1904 (35.2044)
2022-10-28 14:20:15,879:INFO: Dataset: zara1               Batch:  9/16	Loss 35.1563 (35.1995)
2022-10-28 14:20:18,107:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1640 (35.1958)
2022-10-28 14:20:20,269:INFO: Dataset: zara1               Batch: 11/16	Loss 35.1741 (35.1939)
2022-10-28 14:20:22,536:INFO: Dataset: zara1               Batch: 12/16	Loss 35.1911 (35.1937)
2022-10-28 14:20:24,717:INFO: Dataset: zara1               Batch: 13/16	Loss 35.2407 (35.1971)
2022-10-28 14:20:26,981:INFO: Dataset: zara1               Batch: 14/16	Loss 35.2711 (35.2023)
2022-10-28 14:20:29,187:INFO: Dataset: zara1               Batch: 15/16	Loss 35.1981 (35.2020)
2022-10-28 14:20:31,047:INFO: Dataset: zara1               Batch: 16/16	Loss 25.3417 (34.7142)
2022-10-28 14:20:52,554:INFO: Dataset: zara2               Batch:  1/36	Loss 35.1261 (35.1261)
2022-10-28 14:20:54,883:INFO: Dataset: zara2               Batch:  2/36	Loss 35.2439 (35.1786)
2022-10-28 14:20:57,174:INFO: Dataset: zara2               Batch:  3/36	Loss 35.2859 (35.2122)
2022-10-28 14:20:59,412:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1627 (35.2010)
2022-10-28 14:21:01,631:INFO: Dataset: zara2               Batch:  5/36	Loss 35.1335 (35.1874)
2022-10-28 14:21:03,817:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1289 (35.1774)
2022-10-28 14:21:06,106:INFO: Dataset: zara2               Batch:  7/36	Loss 35.2151 (35.1823)
2022-10-28 14:21:08,369:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1710 (35.1808)
2022-10-28 14:21:10,644:INFO: Dataset: zara2               Batch:  9/36	Loss 35.2475 (35.1871)
2022-10-28 14:21:12,934:INFO: Dataset: zara2               Batch: 10/36	Loss 35.2249 (35.1908)
2022-10-28 14:21:15,195:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1580 (35.1875)
2022-10-28 14:21:17,440:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1414 (35.1833)
2022-10-28 14:21:19,668:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1248 (35.1783)
2022-10-28 14:21:21,879:INFO: Dataset: zara2               Batch: 14/36	Loss 35.1530 (35.1763)
2022-10-28 14:21:24,087:INFO: Dataset: zara2               Batch: 15/36	Loss 35.0993 (35.1715)
2022-10-28 14:21:26,308:INFO: Dataset: zara2               Batch: 16/36	Loss 35.1720 (35.1715)
2022-10-28 14:21:28,517:INFO: Dataset: zara2               Batch: 17/36	Loss 35.1322 (35.1694)
2022-10-28 14:21:30,757:INFO: Dataset: zara2               Batch: 18/36	Loss 35.2162 (35.1722)
2022-10-28 14:21:33,070:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1257 (35.1694)
2022-10-28 14:21:35,306:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1664 (35.1692)
2022-10-28 14:21:37,564:INFO: Dataset: zara2               Batch: 21/36	Loss 35.2104 (35.1712)
2022-10-28 14:21:39,732:INFO: Dataset: zara2               Batch: 22/36	Loss 35.2105 (35.1732)
2022-10-28 14:21:41,896:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1156 (35.1706)
2022-10-28 14:21:44,346:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1840 (35.1712)
2022-10-28 14:21:46,713:INFO: Dataset: zara2               Batch: 25/36	Loss 35.2296 (35.1734)
2022-10-28 14:21:48,795:INFO: Dataset: zara2               Batch: 26/36	Loss 35.1301 (35.1717)
2022-10-28 14:21:50,956:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1938 (35.1725)
2022-10-28 14:21:53,097:INFO: Dataset: zara2               Batch: 28/36	Loss 35.1751 (35.1725)
2022-10-28 14:21:55,354:INFO: Dataset: zara2               Batch: 29/36	Loss 35.1694 (35.1724)
2022-10-28 14:21:57,551:INFO: Dataset: zara2               Batch: 30/36	Loss 35.1145 (35.1706)
2022-10-28 14:21:59,754:INFO: Dataset: zara2               Batch: 31/36	Loss 35.2995 (35.1742)
2022-10-28 14:22:01,904:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1604 (35.1738)
2022-10-28 14:22:04,155:INFO: Dataset: zara2               Batch: 33/36	Loss 35.2069 (35.1748)
2022-10-28 14:22:06,267:INFO: Dataset: zara2               Batch: 34/36	Loss 35.1911 (35.1753)
2022-10-28 14:22:08,426:INFO: Dataset: zara2               Batch: 35/36	Loss 35.1189 (35.1738)
2022-10-28 14:22:10,251:INFO: Dataset: zara2               Batch: 36/36	Loss 25.3036 (34.9519)
2022-10-28 14:22:11,035:INFO: - Computing ADE (validation o)
2022-10-28 14:22:18,091:INFO: 		 ADE on eth                       dataset:	 1.1558083295822144
2022-10-28 14:22:18,091:INFO: Average validation o:	ADE  1.1558	FDE  2.0291
2022-10-28 14:22:18,092:INFO: - Computing ADE (validation)
2022-10-28 14:22:25,021:INFO: 		 ADE on hotel                     dataset:	 1.0733124017715454
2022-10-28 14:22:32,118:INFO: 		 ADE on univ                      dataset:	 0.9421373605728149
2022-10-28 14:22:39,105:INFO: 		 ADE on zara1                     dataset:	 0.6386573314666748
2022-10-28 14:22:46,779:INFO: 		 ADE on zara2                     dataset:	 0.5558803677558899
2022-10-28 14:22:46,779:INFO: Average validation:	ADE  0.7900	FDE  1.4837
2022-10-28 14:22:46,780:INFO: - Computing ADE (training)
2022-10-28 14:22:55,114:INFO: 		 ADE on hotel                     dataset:	 1.3756043910980225
2022-10-28 14:23:21,219:INFO: 		 ADE on univ                      dataset:	 0.8939248919487
2022-10-28 14:23:30,533:INFO: 		 ADE on zara1                     dataset:	 0.6834134459495544
2022-10-28 14:23:52,508:INFO: 		 ADE on zara2                     dataset:	 0.5577682256698608
2022-10-28 14:23:52,509:INFO: Average training:	ADE  0.8245	FDE  1.5411
2022-10-28 14:23:52,546:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_244.pth.tar
2022-10-28 14:23:52,548:INFO: 
===> EPOCH: 245 (P2)
2022-10-28 14:23:52,549:INFO: - Computing loss (training)
2022-10-28 14:23:59,961:INFO: Dataset: hotel               Batch: 1/8	Loss 35.8283 (35.8283)
2022-10-28 14:24:02,169:INFO: Dataset: hotel               Batch: 2/8	Loss 35.6169 (35.7268)
2022-10-28 14:24:04,423:INFO: Dataset: hotel               Batch: 3/8	Loss 35.5299 (35.6578)
2022-10-28 14:24:06,644:INFO: Dataset: hotel               Batch: 4/8	Loss 35.6862 (35.6648)
2022-10-28 14:24:08,937:INFO: Dataset: hotel               Batch: 5/8	Loss 35.6133 (35.6542)
2022-10-28 14:24:11,110:INFO: Dataset: hotel               Batch: 6/8	Loss 35.6467 (35.6530)
2022-10-28 14:24:13,211:INFO: Dataset: hotel               Batch: 7/8	Loss 35.6000 (35.6454)
2022-10-28 14:24:14,478:INFO: Dataset: hotel               Batch: 8/8	Loss 7.7767 (34.6159)
2022-10-28 14:24:35,883:INFO: Dataset: univ                Batch:  1/29	Loss 35.4329 (35.4329)
2022-10-28 14:24:38,314:INFO: Dataset: univ                Batch:  2/29	Loss 35.3793 (35.4040)
2022-10-28 14:24:40,500:INFO: Dataset: univ                Batch:  3/29	Loss 35.4622 (35.4242)
2022-10-28 14:24:42,679:INFO: Dataset: univ                Batch:  4/29	Loss 35.4625 (35.4321)
2022-10-28 14:24:44,943:INFO: Dataset: univ                Batch:  5/29	Loss 35.3564 (35.4160)
2022-10-28 14:24:47,232:INFO: Dataset: univ                Batch:  6/29	Loss 35.3734 (35.4087)
2022-10-28 14:24:49,532:INFO: Dataset: univ                Batch:  7/29	Loss 35.4308 (35.4118)
2022-10-28 14:24:51,637:INFO: Dataset: univ                Batch:  8/29	Loss 35.4208 (35.4131)
2022-10-28 14:24:53,764:INFO: Dataset: univ                Batch:  9/29	Loss 35.4257 (35.4145)
2022-10-28 14:24:55,982:INFO: Dataset: univ                Batch: 10/29	Loss 35.3556 (35.4082)
2022-10-28 14:24:58,146:INFO: Dataset: univ                Batch: 11/29	Loss 35.3803 (35.4056)
2022-10-28 14:25:00,250:INFO: Dataset: univ                Batch: 12/29	Loss 35.3853 (35.4038)
2022-10-28 14:25:02,371:INFO: Dataset: univ                Batch: 13/29	Loss 35.4341 (35.4061)
2022-10-28 14:25:04,606:INFO: Dataset: univ                Batch: 14/29	Loss 35.3373 (35.4009)
2022-10-28 14:25:06,825:INFO: Dataset: univ                Batch: 15/29	Loss 35.4024 (35.4010)
2022-10-28 14:25:09,091:INFO: Dataset: univ                Batch: 16/29	Loss 35.4259 (35.4022)
2022-10-28 14:25:11,343:INFO: Dataset: univ                Batch: 17/29	Loss 35.3780 (35.4006)
2022-10-28 14:25:13,451:INFO: Dataset: univ                Batch: 18/29	Loss 35.3820 (35.3996)
2022-10-28 14:25:15,497:INFO: Dataset: univ                Batch: 19/29	Loss 35.3806 (35.3985)
2022-10-28 14:25:17,647:INFO: Dataset: univ                Batch: 20/29	Loss 35.3689 (35.3970)
2022-10-28 14:25:19,820:INFO: Dataset: univ                Batch: 21/29	Loss 35.4033 (35.3973)
2022-10-28 14:25:21,885:INFO: Dataset: univ                Batch: 22/29	Loss 35.4177 (35.3982)
2022-10-28 14:25:23,926:INFO: Dataset: univ                Batch: 23/29	Loss 35.4213 (35.3993)
2022-10-28 14:25:26,073:INFO: Dataset: univ                Batch: 24/29	Loss 35.4129 (35.3998)
2022-10-28 14:25:28,172:INFO: Dataset: univ                Batch: 25/29	Loss 35.3596 (35.3980)
2022-10-28 14:25:30,259:INFO: Dataset: univ                Batch: 26/29	Loss 35.3851 (35.3975)
2022-10-28 14:25:32,333:INFO: Dataset: univ                Batch: 27/29	Loss 35.4090 (35.3979)
2022-10-28 14:25:34,389:INFO: Dataset: univ                Batch: 28/29	Loss 35.4119 (35.3984)
2022-10-28 14:25:35,761:INFO: Dataset: univ                Batch: 29/29	Loss 13.2670 (35.1592)
2022-10-28 14:25:44,801:INFO: Dataset: zara1               Batch:  1/16	Loss 35.1687 (35.1687)
2022-10-28 14:25:47,011:INFO: Dataset: zara1               Batch:  2/16	Loss 35.1788 (35.1744)
2022-10-28 14:25:49,118:INFO: Dataset: zara1               Batch:  3/16	Loss 35.1789 (35.1759)
2022-10-28 14:25:51,228:INFO: Dataset: zara1               Batch:  4/16	Loss 35.2556 (35.1993)
2022-10-28 14:25:53,383:INFO: Dataset: zara1               Batch:  5/16	Loss 35.2000 (35.1995)
2022-10-28 14:25:55,570:INFO: Dataset: zara1               Batch:  6/16	Loss 35.1552 (35.1934)
2022-10-28 14:25:57,768:INFO: Dataset: zara1               Batch:  7/16	Loss 35.1920 (35.1932)
2022-10-28 14:25:59,983:INFO: Dataset: zara1               Batch:  8/16	Loss 35.1734 (35.1905)
2022-10-28 14:26:02,084:INFO: Dataset: zara1               Batch:  9/16	Loss 35.2598 (35.1979)
2022-10-28 14:26:04,279:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1985 (35.1979)
2022-10-28 14:26:06,348:INFO: Dataset: zara1               Batch: 11/16	Loss 35.2717 (35.2048)
2022-10-28 14:26:08,379:INFO: Dataset: zara1               Batch: 12/16	Loss 35.2435 (35.2078)
2022-10-28 14:26:10,557:INFO: Dataset: zara1               Batch: 13/16	Loss 35.1910 (35.2067)
2022-10-28 14:26:12,582:INFO: Dataset: zara1               Batch: 14/16	Loss 35.1828 (35.2050)
2022-10-28 14:26:14,764:INFO: Dataset: zara1               Batch: 15/16	Loss 35.1726 (35.2032)
2022-10-28 14:26:16,654:INFO: Dataset: zara1               Batch: 16/16	Loss 25.2908 (34.8119)
2022-10-28 14:26:37,793:INFO: Dataset: zara2               Batch:  1/36	Loss 35.1011 (35.1011)
2022-10-28 14:26:40,004:INFO: Dataset: zara2               Batch:  2/36	Loss 35.1788 (35.1378)
2022-10-28 14:26:42,074:INFO: Dataset: zara2               Batch:  3/36	Loss 35.2434 (35.1720)
2022-10-28 14:26:45,044:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1065 (35.1568)
2022-10-28 14:26:47,214:INFO: Dataset: zara2               Batch:  5/36	Loss 35.1777 (35.1614)
2022-10-28 14:26:49,091:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1631 (35.1616)
2022-10-28 14:26:51,163:INFO: Dataset: zara2               Batch:  7/36	Loss 35.1482 (35.1596)
2022-10-28 14:26:53,288:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1589 (35.1595)
2022-10-28 14:26:55,087:INFO: Dataset: zara2               Batch:  9/36	Loss 35.1968 (35.1633)
2022-10-28 14:26:56,927:INFO: Dataset: zara2               Batch: 10/36	Loss 35.1469 (35.1617)
2022-10-28 14:26:58,821:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1556 (35.1612)
2022-10-28 14:27:01,157:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1510 (35.1602)
2022-10-28 14:27:03,304:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1490 (35.1593)
2022-10-28 14:27:05,136:INFO: Dataset: zara2               Batch: 14/36	Loss 35.2273 (35.1651)
2022-10-28 14:27:06,887:INFO: Dataset: zara2               Batch: 15/36	Loss 35.1778 (35.1659)
2022-10-28 14:27:08,726:INFO: Dataset: zara2               Batch: 16/36	Loss 35.1726 (35.1662)
2022-10-28 14:27:10,610:INFO: Dataset: zara2               Batch: 17/36	Loss 35.1299 (35.1641)
2022-10-28 14:27:13,065:INFO: Dataset: zara2               Batch: 18/36	Loss 35.0786 (35.1596)
2022-10-28 14:27:14,806:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1134 (35.1575)
2022-10-28 14:27:16,635:INFO: Dataset: zara2               Batch: 20/36	Loss 35.2257 (35.1609)
2022-10-28 14:27:18,806:INFO: Dataset: zara2               Batch: 21/36	Loss 35.0920 (35.1574)
2022-10-28 14:27:20,918:INFO: Dataset: zara2               Batch: 22/36	Loss 35.1950 (35.1590)
2022-10-28 14:27:23,009:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1734 (35.1596)
2022-10-28 14:27:25,132:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1049 (35.1572)
2022-10-28 14:27:27,295:INFO: Dataset: zara2               Batch: 25/36	Loss 35.1632 (35.1575)
2022-10-28 14:27:29,517:INFO: Dataset: zara2               Batch: 26/36	Loss 35.1602 (35.1576)
2022-10-28 14:27:31,744:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1498 (35.1573)
2022-10-28 14:27:33,633:INFO: Dataset: zara2               Batch: 28/36	Loss 35.2020 (35.1590)
2022-10-28 14:27:35,784:INFO: Dataset: zara2               Batch: 29/36	Loss 35.1272 (35.1579)
2022-10-28 14:27:37,663:INFO: Dataset: zara2               Batch: 30/36	Loss 35.1654 (35.1582)
2022-10-28 14:27:39,687:INFO: Dataset: zara2               Batch: 31/36	Loss 35.1953 (35.1595)
2022-10-28 14:27:41,393:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1560 (35.1593)
2022-10-28 14:27:43,273:INFO: Dataset: zara2               Batch: 33/36	Loss 35.2278 (35.1613)
2022-10-28 14:27:45,080:INFO: Dataset: zara2               Batch: 34/36	Loss 35.2122 (35.1627)
2022-10-28 14:27:46,868:INFO: Dataset: zara2               Batch: 35/36	Loss 35.2178 (35.1642)
2022-10-28 14:27:48,516:INFO: Dataset: zara2               Batch: 36/36	Loss 25.3015 (34.9392)
2022-10-28 14:27:49,263:INFO: - Computing ADE (validation o)
2022-10-28 14:27:56,706:INFO: 		 ADE on eth                       dataset:	 1.1226056814193726
2022-10-28 14:27:56,706:INFO: Average validation o:	ADE  1.1226	FDE  1.9301
2022-10-28 14:27:56,707:INFO: - Computing ADE (validation)
2022-10-28 14:28:04,332:INFO: 		 ADE on hotel                     dataset:	 1.0422800779342651
2022-10-28 14:28:13,576:INFO: 		 ADE on univ                      dataset:	 0.9423829317092896
2022-10-28 14:28:21,076:INFO: 		 ADE on zara1                     dataset:	 0.6081036329269409
2022-10-28 14:28:29,451:INFO: 		 ADE on zara2                     dataset:	 0.5383630394935608
2022-10-28 14:28:29,451:INFO: Average validation:	ADE  0.7802	FDE  1.4835
2022-10-28 14:28:29,452:INFO: - Computing ADE (training)
2022-10-28 14:28:37,498:INFO: 		 ADE on hotel                     dataset:	 1.3795586824417114
2022-10-28 14:28:59,803:INFO: 		 ADE on univ                      dataset:	 0.8813053965568542
2022-10-28 14:29:08,895:INFO: 		 ADE on zara1                     dataset:	 0.6792932748794556
2022-10-28 14:29:31,752:INFO: 		 ADE on zara2                     dataset:	 0.5418422222137451
2022-10-28 14:29:31,752:INFO: Average training:	ADE  0.8122	FDE  1.5359
2022-10-28 14:29:31,776:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_245.pth.tar
2022-10-28 14:29:31,776:INFO: 
===> EPOCH: 246 (P2)
2022-10-28 14:29:31,776:INFO: - Computing loss (training)
2022-10-28 14:29:40,147:INFO: Dataset: hotel               Batch: 1/8	Loss 35.6335 (35.6335)
2022-10-28 14:29:41,933:INFO: Dataset: hotel               Batch: 2/8	Loss 35.5264 (35.5787)
2022-10-28 14:29:43,776:INFO: Dataset: hotel               Batch: 3/8	Loss 35.6040 (35.5866)
2022-10-28 14:29:45,400:INFO: Dataset: hotel               Batch: 4/8	Loss 35.8058 (35.6414)
2022-10-28 14:29:47,062:INFO: Dataset: hotel               Batch: 5/8	Loss 35.7800 (35.6681)
2022-10-28 14:29:48,720:INFO: Dataset: hotel               Batch: 6/8	Loss 35.6134 (35.6587)
2022-10-28 14:29:50,470:INFO: Dataset: hotel               Batch: 7/8	Loss 35.5655 (35.6455)
2022-10-28 14:29:51,619:INFO: Dataset: hotel               Batch: 8/8	Loss 7.7387 (34.9092)
2022-10-28 14:30:14,580:INFO: Dataset: univ                Batch:  1/29	Loss 35.3374 (35.3374)
2022-10-28 14:30:16,989:INFO: Dataset: univ                Batch:  2/29	Loss 35.4258 (35.3842)
2022-10-28 14:30:19,270:INFO: Dataset: univ                Batch:  3/29	Loss 35.4042 (35.3909)
2022-10-28 14:30:21,454:INFO: Dataset: univ                Batch:  4/29	Loss 35.3938 (35.3917)
2022-10-28 14:30:23,841:INFO: Dataset: univ                Batch:  5/29	Loss 35.3885 (35.3910)
2022-10-28 14:30:26,315:INFO: Dataset: univ                Batch:  6/29	Loss 35.3493 (35.3838)
2022-10-28 14:30:28,688:INFO: Dataset: univ                Batch:  7/29	Loss 35.4774 (35.3958)
2022-10-28 14:30:31,039:INFO: Dataset: univ                Batch:  8/29	Loss 35.4817 (35.4056)
2022-10-28 14:30:33,983:INFO: Dataset: univ                Batch:  9/29	Loss 35.3757 (35.4025)
2022-10-28 14:30:36,585:INFO: Dataset: univ                Batch: 10/29	Loss 35.3901 (35.4012)
2022-10-28 14:30:38,931:INFO: Dataset: univ                Batch: 11/29	Loss 35.4142 (35.4024)
2022-10-28 14:30:41,062:INFO: Dataset: univ                Batch: 12/29	Loss 35.3315 (35.3969)
2022-10-28 14:30:43,449:INFO: Dataset: univ                Batch: 13/29	Loss 35.4183 (35.3985)
2022-10-28 14:30:45,728:INFO: Dataset: univ                Batch: 14/29	Loss 35.3970 (35.3984)
2022-10-28 14:30:47,946:INFO: Dataset: univ                Batch: 15/29	Loss 35.4237 (35.4000)
2022-10-28 14:30:50,256:INFO: Dataset: univ                Batch: 16/29	Loss 35.3639 (35.3977)
2022-10-28 14:30:52,462:INFO: Dataset: univ                Batch: 17/29	Loss 35.4200 (35.3990)
2022-10-28 14:30:54,748:INFO: Dataset: univ                Batch: 18/29	Loss 35.3850 (35.3982)
2022-10-28 14:30:57,000:INFO: Dataset: univ                Batch: 19/29	Loss 35.3648 (35.3963)
2022-10-28 14:30:59,275:INFO: Dataset: univ                Batch: 20/29	Loss 35.3825 (35.3956)
2022-10-28 14:31:01,595:INFO: Dataset: univ                Batch: 21/29	Loss 35.3693 (35.3942)
2022-10-28 14:31:03,911:INFO: Dataset: univ                Batch: 22/29	Loss 35.4593 (35.3970)
2022-10-28 14:31:06,221:INFO: Dataset: univ                Batch: 23/29	Loss 35.3496 (35.3948)
2022-10-28 14:31:08,338:INFO: Dataset: univ                Batch: 24/29	Loss 35.4204 (35.3957)
2022-10-28 14:31:10,682:INFO: Dataset: univ                Batch: 25/29	Loss 35.3920 (35.3955)
2022-10-28 14:31:12,835:INFO: Dataset: univ                Batch: 26/29	Loss 35.3919 (35.3954)
2022-10-28 14:31:14,997:INFO: Dataset: univ                Batch: 27/29	Loss 35.3990 (35.3955)
2022-10-28 14:31:17,259:INFO: Dataset: univ                Batch: 28/29	Loss 35.3557 (35.3941)
2022-10-28 14:31:18,615:INFO: Dataset: univ                Batch: 29/29	Loss 13.2828 (35.0703)
2022-10-28 14:31:27,146:INFO: Dataset: zara1               Batch:  1/16	Loss 35.2103 (35.2103)
2022-10-28 14:31:29,249:INFO: Dataset: zara1               Batch:  2/16	Loss 35.1599 (35.1863)
2022-10-28 14:31:31,299:INFO: Dataset: zara1               Batch:  3/16	Loss 35.2073 (35.1923)
2022-10-28 14:31:33,530:INFO: Dataset: zara1               Batch:  4/16	Loss 35.1792 (35.1890)
2022-10-28 14:31:35,561:INFO: Dataset: zara1               Batch:  5/16	Loss 35.1634 (35.1835)
2022-10-28 14:31:37,773:INFO: Dataset: zara1               Batch:  6/16	Loss 35.2693 (35.1990)
2022-10-28 14:31:39,859:INFO: Dataset: zara1               Batch:  7/16	Loss 35.1264 (35.1895)
2022-10-28 14:31:42,110:INFO: Dataset: zara1               Batch:  8/16	Loss 35.2262 (35.1937)
2022-10-28 14:31:44,380:INFO: Dataset: zara1               Batch:  9/16	Loss 35.2398 (35.1986)
2022-10-28 14:31:46,586:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1603 (35.1949)
2022-10-28 14:31:48,761:INFO: Dataset: zara1               Batch: 11/16	Loss 35.2298 (35.1980)
2022-10-28 14:31:50,991:INFO: Dataset: zara1               Batch: 12/16	Loss 35.1601 (35.1948)
2022-10-28 14:31:53,084:INFO: Dataset: zara1               Batch: 13/16	Loss 35.1552 (35.1920)
2022-10-28 14:31:55,182:INFO: Dataset: zara1               Batch: 14/16	Loss 35.1296 (35.1871)
2022-10-28 14:31:57,298:INFO: Dataset: zara1               Batch: 15/16	Loss 35.1914 (35.1874)
2022-10-28 14:31:59,158:INFO: Dataset: zara1               Batch: 16/16	Loss 25.3196 (34.7044)
2022-10-28 14:32:20,997:INFO: Dataset: zara2               Batch:  1/36	Loss 35.1471 (35.1471)
2022-10-28 14:32:23,117:INFO: Dataset: zara2               Batch:  2/36	Loss 35.2293 (35.1847)
2022-10-28 14:32:25,320:INFO: Dataset: zara2               Batch:  3/36	Loss 35.2090 (35.1939)
2022-10-28 14:32:27,507:INFO: Dataset: zara2               Batch:  4/36	Loss 35.2522 (35.2085)
2022-10-28 14:32:29,664:INFO: Dataset: zara2               Batch:  5/36	Loss 35.1274 (35.1916)
2022-10-28 14:32:31,851:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1269 (35.1805)
2022-10-28 14:32:34,073:INFO: Dataset: zara2               Batch:  7/36	Loss 35.1826 (35.1808)
2022-10-28 14:32:36,287:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1813 (35.1809)
2022-10-28 14:32:38,526:INFO: Dataset: zara2               Batch:  9/36	Loss 35.0928 (35.1725)
2022-10-28 14:32:40,757:INFO: Dataset: zara2               Batch: 10/36	Loss 35.1821 (35.1736)
2022-10-28 14:32:42,972:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1002 (35.1661)
2022-10-28 14:32:45,136:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1930 (35.1683)
2022-10-28 14:32:47,352:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1860 (35.1697)
2022-10-28 14:32:49,534:INFO: Dataset: zara2               Batch: 14/36	Loss 35.1776 (35.1703)
2022-10-28 14:32:51,652:INFO: Dataset: zara2               Batch: 15/36	Loss 35.2526 (35.1752)
2022-10-28 14:32:53,885:INFO: Dataset: zara2               Batch: 16/36	Loss 35.2077 (35.1770)
2022-10-28 14:32:56,121:INFO: Dataset: zara2               Batch: 17/36	Loss 35.1188 (35.1733)
2022-10-28 14:32:58,366:INFO: Dataset: zara2               Batch: 18/36	Loss 35.1812 (35.1738)
2022-10-28 14:33:00,592:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1604 (35.1731)
2022-10-28 14:33:02,868:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1630 (35.1727)
2022-10-28 14:33:05,160:INFO: Dataset: zara2               Batch: 21/36	Loss 35.1458 (35.1714)
2022-10-28 14:33:07,333:INFO: Dataset: zara2               Batch: 22/36	Loss 35.1371 (35.1699)
2022-10-28 14:33:09,521:INFO: Dataset: zara2               Batch: 23/36	Loss 35.0833 (35.1660)
2022-10-28 14:33:11,792:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1763 (35.1664)
2022-10-28 14:33:13,956:INFO: Dataset: zara2               Batch: 25/36	Loss 35.1269 (35.1648)
2022-10-28 14:33:16,118:INFO: Dataset: zara2               Batch: 26/36	Loss 35.1097 (35.1626)
2022-10-28 14:33:18,343:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1275 (35.1611)
2022-10-28 14:33:20,529:INFO: Dataset: zara2               Batch: 28/36	Loss 35.1961 (35.1624)
2022-10-28 14:33:22,743:INFO: Dataset: zara2               Batch: 29/36	Loss 35.1512 (35.1620)
2022-10-28 14:33:24,914:INFO: Dataset: zara2               Batch: 30/36	Loss 35.1907 (35.1629)
2022-10-28 14:33:27,129:INFO: Dataset: zara2               Batch: 31/36	Loss 35.1744 (35.1633)
2022-10-28 14:33:29,326:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1857 (35.1640)
2022-10-28 14:33:31,514:INFO: Dataset: zara2               Batch: 33/36	Loss 35.1029 (35.1622)
2022-10-28 14:33:33,705:INFO: Dataset: zara2               Batch: 34/36	Loss 35.1605 (35.1621)
2022-10-28 14:33:35,935:INFO: Dataset: zara2               Batch: 35/36	Loss 35.0960 (35.1602)
2022-10-28 14:33:37,900:INFO: Dataset: zara2               Batch: 36/36	Loss 25.2478 (34.9537)
2022-10-28 14:33:38,661:INFO: - Computing ADE (validation o)
2022-10-28 14:33:45,471:INFO: 		 ADE on eth                       dataset:	 1.1194634437561035
2022-10-28 14:33:45,471:INFO: Average validation o:	ADE  1.1195	FDE  1.9816
2022-10-28 14:33:45,472:INFO: - Computing ADE (validation)
2022-10-28 14:33:52,302:INFO: 		 ADE on hotel                     dataset:	 1.0420113801956177
2022-10-28 14:33:59,317:INFO: 		 ADE on univ                      dataset:	 0.9242687225341797
2022-10-28 14:34:06,168:INFO: 		 ADE on zara1                     dataset:	 0.585770308971405
2022-10-28 14:34:13,807:INFO: 		 ADE on zara2                     dataset:	 0.5271510481834412
2022-10-28 14:34:13,808:INFO: Average validation:	ADE  0.7654	FDE  1.4531
2022-10-28 14:34:13,809:INFO: - Computing ADE (training)
2022-10-28 14:34:21,168:INFO: 		 ADE on hotel                     dataset:	 1.3595203161239624
2022-10-28 14:34:43,507:INFO: 		 ADE on univ                      dataset:	 0.867424726486206
2022-10-28 14:34:51,490:INFO: 		 ADE on zara1                     dataset:	 0.6536237597465515
2022-10-28 14:35:12,836:INFO: 		 ADE on zara2                     dataset:	 0.5260905623435974
2022-10-28 14:35:12,837:INFO: Average training:	ADE  0.7970	FDE  1.5037
2022-10-28 14:35:12,862:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_246.pth.tar
2022-10-28 14:35:12,864:INFO: 
===> EPOCH: 247 (P2)
2022-10-28 14:35:12,864:INFO: - Computing loss (training)
2022-10-28 14:35:20,113:INFO: Dataset: hotel               Batch: 1/8	Loss 35.5415 (35.5415)
2022-10-28 14:35:22,358:INFO: Dataset: hotel               Batch: 2/8	Loss 35.7497 (35.6446)
2022-10-28 14:35:24,678:INFO: Dataset: hotel               Batch: 3/8	Loss 35.6217 (35.6368)
2022-10-28 14:35:26,958:INFO: Dataset: hotel               Batch: 4/8	Loss 35.6418 (35.6381)
2022-10-28 14:35:29,280:INFO: Dataset: hotel               Batch: 5/8	Loss 35.5025 (35.6109)
2022-10-28 14:35:31,498:INFO: Dataset: hotel               Batch: 6/8	Loss 35.6338 (35.6146)
2022-10-28 14:35:33,750:INFO: Dataset: hotel               Batch: 7/8	Loss 35.8421 (35.6449)
2022-10-28 14:35:35,116:INFO: Dataset: hotel               Batch: 8/8	Loss 7.7206 (34.5766)
2022-10-28 14:35:56,016:INFO: Dataset: univ                Batch:  1/29	Loss 35.4033 (35.4033)
2022-10-28 14:35:58,273:INFO: Dataset: univ                Batch:  2/29	Loss 35.4465 (35.4270)
2022-10-28 14:36:00,435:INFO: Dataset: univ                Batch:  3/29	Loss 35.3987 (35.4175)
2022-10-28 14:36:02,607:INFO: Dataset: univ                Batch:  4/29	Loss 35.3846 (35.4087)
2022-10-28 14:36:04,895:INFO: Dataset: univ                Batch:  5/29	Loss 35.4405 (35.4151)
2022-10-28 14:36:07,244:INFO: Dataset: univ                Batch:  6/29	Loss 35.4023 (35.4130)
2022-10-28 14:36:09,437:INFO: Dataset: univ                Batch:  7/29	Loss 35.3986 (35.4112)
2022-10-28 14:36:11,520:INFO: Dataset: univ                Batch:  8/29	Loss 35.4224 (35.4126)
2022-10-28 14:36:13,697:INFO: Dataset: univ                Batch:  9/29	Loss 35.3818 (35.4095)
2022-10-28 14:36:15,890:INFO: Dataset: univ                Batch: 10/29	Loss 35.4616 (35.4146)
2022-10-28 14:36:18,473:INFO: Dataset: univ                Batch: 11/29	Loss 35.4129 (35.4145)
2022-10-28 14:36:21,144:INFO: Dataset: univ                Batch: 12/29	Loss 35.3778 (35.4115)
2022-10-28 14:36:23,121:INFO: Dataset: univ                Batch: 13/29	Loss 35.3882 (35.4098)
2022-10-28 14:36:25,331:INFO: Dataset: univ                Batch: 14/29	Loss 35.3750 (35.4073)
2022-10-28 14:36:26,986:INFO: Dataset: univ                Batch: 15/29	Loss 35.3830 (35.4055)
2022-10-28 14:36:28,714:INFO: Dataset: univ                Batch: 16/29	Loss 35.3699 (35.4030)
2022-10-28 14:36:31,093:INFO: Dataset: univ                Batch: 17/29	Loss 35.3695 (35.4008)
2022-10-28 14:36:33,434:INFO: Dataset: univ                Batch: 18/29	Loss 35.4214 (35.4020)
2022-10-28 14:36:35,599:INFO: Dataset: univ                Batch: 19/29	Loss 35.3936 (35.4015)
2022-10-28 14:36:37,779:INFO: Dataset: univ                Batch: 20/29	Loss 35.3743 (35.4002)
2022-10-28 14:36:39,983:INFO: Dataset: univ                Batch: 21/29	Loss 35.3962 (35.4001)
2022-10-28 14:36:42,111:INFO: Dataset: univ                Batch: 22/29	Loss 35.4096 (35.4005)
2022-10-28 14:36:44,338:INFO: Dataset: univ                Batch: 23/29	Loss 35.3959 (35.4003)
2022-10-28 14:36:46,579:INFO: Dataset: univ                Batch: 24/29	Loss 35.3695 (35.3988)
2022-10-28 14:36:48,737:INFO: Dataset: univ                Batch: 25/29	Loss 35.4097 (35.3992)
2022-10-28 14:36:50,929:INFO: Dataset: univ                Batch: 26/29	Loss 35.3283 (35.3965)
2022-10-28 14:36:53,191:INFO: Dataset: univ                Batch: 27/29	Loss 35.3253 (35.3936)
2022-10-28 14:36:55,457:INFO: Dataset: univ                Batch: 28/29	Loss 35.3310 (35.3912)
2022-10-28 14:36:56,947:INFO: Dataset: univ                Batch: 29/29	Loss 13.2444 (35.0228)
2022-10-28 14:37:04,894:INFO: Dataset: zara1               Batch:  1/16	Loss 35.1375 (35.1375)
2022-10-28 14:37:07,151:INFO: Dataset: zara1               Batch:  2/16	Loss 35.1811 (35.1589)
2022-10-28 14:37:09,349:INFO: Dataset: zara1               Batch:  3/16	Loss 35.2020 (35.1708)
2022-10-28 14:37:11,557:INFO: Dataset: zara1               Batch:  4/16	Loss 35.2375 (35.1879)
2022-10-28 14:37:13,667:INFO: Dataset: zara1               Batch:  5/16	Loss 35.1844 (35.1872)
2022-10-28 14:37:15,749:INFO: Dataset: zara1               Batch:  6/16	Loss 35.1465 (35.1809)
2022-10-28 14:37:17,824:INFO: Dataset: zara1               Batch:  7/16	Loss 35.2585 (35.1911)
2022-10-28 14:37:19,917:INFO: Dataset: zara1               Batch:  8/16	Loss 35.1804 (35.1896)
2022-10-28 14:37:22,075:INFO: Dataset: zara1               Batch:  9/16	Loss 35.1620 (35.1871)
2022-10-28 14:37:24,162:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1764 (35.1859)
2022-10-28 14:37:26,336:INFO: Dataset: zara1               Batch: 11/16	Loss 35.1439 (35.1824)
2022-10-28 14:37:28,448:INFO: Dataset: zara1               Batch: 12/16	Loss 35.1878 (35.1828)
2022-10-28 14:37:30,574:INFO: Dataset: zara1               Batch: 13/16	Loss 35.1273 (35.1783)
2022-10-28 14:37:32,837:INFO: Dataset: zara1               Batch: 14/16	Loss 35.1322 (35.1742)
2022-10-28 14:37:35,062:INFO: Dataset: zara1               Batch: 15/16	Loss 35.2725 (35.1803)
2022-10-28 14:37:36,839:INFO: Dataset: zara1               Batch: 16/16	Loss 25.3164 (34.6404)
2022-10-28 14:37:57,898:INFO: Dataset: zara2               Batch:  1/36	Loss 35.1599 (35.1599)
2022-10-28 14:38:00,093:INFO: Dataset: zara2               Batch:  2/36	Loss 35.1247 (35.1411)
2022-10-28 14:38:02,301:INFO: Dataset: zara2               Batch:  3/36	Loss 35.2162 (35.1642)
2022-10-28 14:38:04,699:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1288 (35.1554)
2022-10-28 14:38:06,759:INFO: Dataset: zara2               Batch:  5/36	Loss 35.1534 (35.1550)
2022-10-28 14:38:08,884:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1702 (35.1573)
2022-10-28 14:38:11,233:INFO: Dataset: zara2               Batch:  7/36	Loss 35.2263 (35.1665)
2022-10-28 14:38:13,245:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1765 (35.1679)
2022-10-28 14:38:15,389:INFO: Dataset: zara2               Batch:  9/36	Loss 35.1880 (35.1701)
2022-10-28 14:38:17,566:INFO: Dataset: zara2               Batch: 10/36	Loss 35.0802 (35.1614)
2022-10-28 14:38:19,625:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1484 (35.1603)
2022-10-28 14:38:21,985:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1323 (35.1580)
2022-10-28 14:38:24,021:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1333 (35.1561)
2022-10-28 14:38:25,699:INFO: Dataset: zara2               Batch: 14/36	Loss 35.1439 (35.1553)
2022-10-28 14:38:27,558:INFO: Dataset: zara2               Batch: 15/36	Loss 35.0896 (35.1510)
2022-10-28 14:38:29,209:INFO: Dataset: zara2               Batch: 16/36	Loss 35.1592 (35.1515)
2022-10-28 14:38:30,905:INFO: Dataset: zara2               Batch: 17/36	Loss 35.2353 (35.1574)
2022-10-28 14:38:32,650:INFO: Dataset: zara2               Batch: 18/36	Loss 35.1429 (35.1565)
2022-10-28 14:38:34,335:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1737 (35.1574)
2022-10-28 14:38:36,007:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1584 (35.1575)
2022-10-28 14:38:37,657:INFO: Dataset: zara2               Batch: 21/36	Loss 35.0546 (35.1525)
2022-10-28 14:38:39,345:INFO: Dataset: zara2               Batch: 22/36	Loss 35.1723 (35.1535)
2022-10-28 14:38:41,321:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1168 (35.1519)
2022-10-28 14:38:43,308:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1305 (35.1511)
2022-10-28 14:38:45,171:INFO: Dataset: zara2               Batch: 25/36	Loss 35.1337 (35.1503)
2022-10-28 14:38:47,171:INFO: Dataset: zara2               Batch: 26/36	Loss 35.2080 (35.1524)
2022-10-28 14:38:49,402:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1085 (35.1506)
2022-10-28 14:38:51,508:INFO: Dataset: zara2               Batch: 28/36	Loss 35.1268 (35.1497)
2022-10-28 14:38:53,688:INFO: Dataset: zara2               Batch: 29/36	Loss 35.2489 (35.1529)
2022-10-28 14:38:55,831:INFO: Dataset: zara2               Batch: 30/36	Loss 35.1501 (35.1528)
2022-10-28 14:38:58,042:INFO: Dataset: zara2               Batch: 31/36	Loss 35.1239 (35.1519)
2022-10-28 14:39:00,198:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1553 (35.1520)
2022-10-28 14:39:02,414:INFO: Dataset: zara2               Batch: 33/36	Loss 35.2087 (35.1537)
2022-10-28 14:39:04,985:INFO: Dataset: zara2               Batch: 34/36	Loss 35.2635 (35.1565)
2022-10-28 14:39:07,426:INFO: Dataset: zara2               Batch: 35/36	Loss 35.2102 (35.1580)
2022-10-28 14:39:09,149:INFO: Dataset: zara2               Batch: 36/36	Loss 25.2947 (34.9574)
2022-10-28 14:39:09,908:INFO: - Computing ADE (validation o)
2022-10-28 14:39:17,336:INFO: 		 ADE on eth                       dataset:	 1.1319280862808228
2022-10-28 14:39:17,337:INFO: Average validation o:	ADE  1.1319	FDE  1.9926
2022-10-28 14:39:17,337:INFO: - Computing ADE (validation)
2022-10-28 14:39:24,660:INFO: 		 ADE on hotel                     dataset:	 1.0404963493347168
2022-10-28 14:39:31,728:INFO: 		 ADE on univ                      dataset:	 0.9349912405014038
2022-10-28 14:39:38,692:INFO: 		 ADE on zara1                     dataset:	 0.6068496108055115
2022-10-28 14:39:46,055:INFO: 		 ADE on zara2                     dataset:	 0.5312790274620056
2022-10-28 14:39:46,055:INFO: Average validation:	ADE  0.7736	FDE  1.4722
2022-10-28 14:39:46,056:INFO: - Computing ADE (training)
2022-10-28 14:39:54,778:INFO: 		 ADE on hotel                     dataset:	 1.351050615310669
2022-10-28 14:40:17,574:INFO: 		 ADE on univ                      dataset:	 0.871222198009491
2022-10-28 14:40:26,225:INFO: 		 ADE on zara1                     dataset:	 0.6590079069137573
2022-10-28 14:40:48,859:INFO: 		 ADE on zara2                     dataset:	 0.5230040550231934
2022-10-28 14:40:48,859:INFO: Average training:	ADE  0.7992	FDE  1.5141
2022-10-28 14:40:48,885:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_247.pth.tar
2022-10-28 14:40:48,885:INFO: 
===> EPOCH: 248 (P2)
2022-10-28 14:40:48,885:INFO: - Computing loss (training)
2022-10-28 14:40:57,148:INFO: Dataset: hotel               Batch: 1/8	Loss 35.7180 (35.7180)
2022-10-28 14:40:58,876:INFO: Dataset: hotel               Batch: 2/8	Loss 35.6409 (35.6787)
2022-10-28 14:41:00,772:INFO: Dataset: hotel               Batch: 3/8	Loss 35.5986 (35.6503)
2022-10-28 14:41:02,763:INFO: Dataset: hotel               Batch: 4/8	Loss 35.5693 (35.6314)
2022-10-28 14:41:04,672:INFO: Dataset: hotel               Batch: 5/8	Loss 35.5213 (35.6076)
2022-10-28 14:41:06,556:INFO: Dataset: hotel               Batch: 6/8	Loss 35.5326 (35.5949)
2022-10-28 14:41:08,374:INFO: Dataset: hotel               Batch: 7/8	Loss 35.7402 (35.6157)
2022-10-28 14:41:09,490:INFO: Dataset: hotel               Batch: 8/8	Loss 7.9328 (34.9218)
2022-10-28 14:41:30,666:INFO: Dataset: univ                Batch:  1/29	Loss 35.3936 (35.3936)
2022-10-28 14:41:32,547:INFO: Dataset: univ                Batch:  2/29	Loss 35.4147 (35.4039)
2022-10-28 14:41:34,374:INFO: Dataset: univ                Batch:  3/29	Loss 35.3604 (35.3897)
2022-10-28 14:41:36,254:INFO: Dataset: univ                Batch:  4/29	Loss 35.3172 (35.3702)
2022-10-28 14:41:38,055:INFO: Dataset: univ                Batch:  5/29	Loss 35.3986 (35.3755)
2022-10-28 14:41:39,886:INFO: Dataset: univ                Batch:  6/29	Loss 35.3713 (35.3748)
2022-10-28 14:41:41,734:INFO: Dataset: univ                Batch:  7/29	Loss 35.3983 (35.3782)
2022-10-28 14:41:43,538:INFO: Dataset: univ                Batch:  8/29	Loss 35.3523 (35.3754)
2022-10-28 14:41:45,384:INFO: Dataset: univ                Batch:  9/29	Loss 35.4694 (35.3848)
2022-10-28 14:41:47,160:INFO: Dataset: univ                Batch: 10/29	Loss 35.3737 (35.3837)
2022-10-28 14:41:49,028:INFO: Dataset: univ                Batch: 11/29	Loss 35.4432 (35.3885)
2022-10-28 14:41:50,890:INFO: Dataset: univ                Batch: 12/29	Loss 35.4309 (35.3919)
2022-10-28 14:41:52,671:INFO: Dataset: univ                Batch: 13/29	Loss 35.4279 (35.3949)
2022-10-28 14:41:54,556:INFO: Dataset: univ                Batch: 14/29	Loss 35.4066 (35.3957)
2022-10-28 14:41:56,441:INFO: Dataset: univ                Batch: 15/29	Loss 35.3557 (35.3927)
2022-10-28 14:41:58,226:INFO: Dataset: univ                Batch: 16/29	Loss 35.3672 (35.3911)
2022-10-28 14:42:00,674:INFO: Dataset: univ                Batch: 17/29	Loss 35.3085 (35.3852)
2022-10-28 14:42:02,547:INFO: Dataset: univ                Batch: 18/29	Loss 35.3712 (35.3845)
2022-10-28 14:42:04,397:INFO: Dataset: univ                Batch: 19/29	Loss 35.3645 (35.3835)
2022-10-28 14:42:06,313:INFO: Dataset: univ                Batch: 20/29	Loss 35.4017 (35.3844)
2022-10-28 14:42:08,122:INFO: Dataset: univ                Batch: 21/29	Loss 35.4280 (35.3863)
2022-10-28 14:42:09,927:INFO: Dataset: univ                Batch: 22/29	Loss 35.4221 (35.3878)
2022-10-28 14:42:12,015:INFO: Dataset: univ                Batch: 23/29	Loss 35.4123 (35.3887)
2022-10-28 14:42:14,088:INFO: Dataset: univ                Batch: 24/29	Loss 35.3733 (35.3881)
2022-10-28 14:42:16,043:INFO: Dataset: univ                Batch: 25/29	Loss 35.3432 (35.3860)
2022-10-28 14:42:17,857:INFO: Dataset: univ                Batch: 26/29	Loss 35.4058 (35.3866)
2022-10-28 14:42:19,724:INFO: Dataset: univ                Batch: 27/29	Loss 35.3927 (35.3868)
2022-10-28 14:42:21,575:INFO: Dataset: univ                Batch: 28/29	Loss 35.3979 (35.3872)
2022-10-28 14:42:22,898:INFO: Dataset: univ                Batch: 29/29	Loss 13.2631 (35.0497)
2022-10-28 14:42:31,528:INFO: Dataset: zara1               Batch:  1/16	Loss 35.2095 (35.2095)
2022-10-28 14:42:33,450:INFO: Dataset: zara1               Batch:  2/16	Loss 35.1809 (35.1955)
2022-10-28 14:42:35,237:INFO: Dataset: zara1               Batch:  3/16	Loss 35.1619 (35.1840)
2022-10-28 14:42:37,088:INFO: Dataset: zara1               Batch:  4/16	Loss 35.1623 (35.1787)
2022-10-28 14:42:38,881:INFO: Dataset: zara1               Batch:  5/16	Loss 35.1681 (35.1764)
2022-10-28 14:42:40,767:INFO: Dataset: zara1               Batch:  6/16	Loss 35.1914 (35.1786)
2022-10-28 14:42:42,586:INFO: Dataset: zara1               Batch:  7/16	Loss 35.2306 (35.1860)
2022-10-28 14:42:44,406:INFO: Dataset: zara1               Batch:  8/16	Loss 35.1832 (35.1856)
2022-10-28 14:42:46,177:INFO: Dataset: zara1               Batch:  9/16	Loss 35.2375 (35.1916)
2022-10-28 14:42:48,008:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1608 (35.1882)
2022-10-28 14:42:49,783:INFO: Dataset: zara1               Batch: 11/16	Loss 35.1648 (35.1857)
2022-10-28 14:42:51,624:INFO: Dataset: zara1               Batch: 12/16	Loss 35.1308 (35.1809)
2022-10-28 14:42:53,373:INFO: Dataset: zara1               Batch: 13/16	Loss 35.2025 (35.1824)
2022-10-28 14:42:55,236:INFO: Dataset: zara1               Batch: 14/16	Loss 35.1736 (35.1818)
2022-10-28 14:42:57,054:INFO: Dataset: zara1               Batch: 15/16	Loss 35.1729 (35.1812)
2022-10-28 14:42:58,567:INFO: Dataset: zara1               Batch: 16/16	Loss 25.2947 (34.7285)
2022-10-28 14:43:19,777:INFO: Dataset: zara2               Batch:  1/36	Loss 35.2292 (35.2292)
2022-10-28 14:43:21,570:INFO: Dataset: zara2               Batch:  2/36	Loss 35.1435 (35.1836)
2022-10-28 14:43:23,324:INFO: Dataset: zara2               Batch:  3/36	Loss 35.1936 (35.1869)
2022-10-28 14:43:25,180:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1949 (35.1893)
2022-10-28 14:43:26,966:INFO: Dataset: zara2               Batch:  5/36	Loss 35.2048 (35.1925)
2022-10-28 14:43:28,787:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1041 (35.1766)
2022-10-28 14:43:30,529:INFO: Dataset: zara2               Batch:  7/36	Loss 35.0883 (35.1652)
2022-10-28 14:43:32,373:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1193 (35.1600)
2022-10-28 14:43:34,157:INFO: Dataset: zara2               Batch:  9/36	Loss 35.1918 (35.1632)
2022-10-28 14:43:36,056:INFO: Dataset: zara2               Batch: 10/36	Loss 35.0902 (35.1557)
2022-10-28 14:43:37,810:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1677 (35.1567)
2022-10-28 14:43:39,652:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1010 (35.1520)
2022-10-28 14:43:41,451:INFO: Dataset: zara2               Batch: 13/36	Loss 35.2192 (35.1574)
2022-10-28 14:43:43,277:INFO: Dataset: zara2               Batch: 14/36	Loss 35.2345 (35.1631)
2022-10-28 14:43:45,001:INFO: Dataset: zara2               Batch: 15/36	Loss 35.1319 (35.1608)
2022-10-28 14:43:46,805:INFO: Dataset: zara2               Batch: 16/36	Loss 35.1326 (35.1593)
2022-10-28 14:43:48,520:INFO: Dataset: zara2               Batch: 17/36	Loss 35.1455 (35.1584)
2022-10-28 14:43:50,316:INFO: Dataset: zara2               Batch: 18/36	Loss 35.2202 (35.1618)
2022-10-28 14:43:52,163:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1650 (35.1620)
2022-10-28 14:43:54,138:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1879 (35.1633)
2022-10-28 14:43:55,936:INFO: Dataset: zara2               Batch: 21/36	Loss 35.1728 (35.1638)
2022-10-28 14:43:57,836:INFO: Dataset: zara2               Batch: 22/36	Loss 35.1380 (35.1624)
2022-10-28 14:43:59,598:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1393 (35.1614)
2022-10-28 14:44:01,454:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1361 (35.1604)
2022-10-28 14:44:03,233:INFO: Dataset: zara2               Batch: 25/36	Loss 35.1239 (35.1587)
2022-10-28 14:44:05,036:INFO: Dataset: zara2               Batch: 26/36	Loss 35.0770 (35.1553)
2022-10-28 14:44:06,803:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1319 (35.1543)
2022-10-28 14:44:08,582:INFO: Dataset: zara2               Batch: 28/36	Loss 35.1725 (35.1549)
2022-10-28 14:44:10,315:INFO: Dataset: zara2               Batch: 29/36	Loss 35.2097 (35.1570)
2022-10-28 14:44:12,135:INFO: Dataset: zara2               Batch: 30/36	Loss 35.2264 (35.1591)
2022-10-28 14:44:13,898:INFO: Dataset: zara2               Batch: 31/36	Loss 35.1314 (35.1583)
2022-10-28 14:44:15,712:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1133 (35.1570)
2022-10-28 14:44:17,543:INFO: Dataset: zara2               Batch: 33/36	Loss 35.1300 (35.1562)
2022-10-28 14:44:19,344:INFO: Dataset: zara2               Batch: 34/36	Loss 35.1258 (35.1551)
2022-10-28 14:44:21,119:INFO: Dataset: zara2               Batch: 35/36	Loss 35.1196 (35.1542)
2022-10-28 14:44:22,709:INFO: Dataset: zara2               Batch: 36/36	Loss 25.3143 (34.9931)
2022-10-28 14:44:23,482:INFO: - Computing ADE (validation o)
2022-10-28 14:44:30,882:INFO: 		 ADE on eth                       dataset:	 1.0815409421920776
2022-10-28 14:44:30,882:INFO: Average validation o:	ADE  1.0815	FDE  1.9147
2022-10-28 14:44:30,883:INFO: - Computing ADE (validation)
2022-10-28 14:44:38,580:INFO: 		 ADE on hotel                     dataset:	 1.085653305053711
2022-10-28 14:44:46,713:INFO: 		 ADE on univ                      dataset:	 0.9599621891975403
2022-10-28 14:44:54,170:INFO: 		 ADE on zara1                     dataset:	 0.5934527516365051
2022-10-28 14:45:02,094:INFO: 		 ADE on zara2                     dataset:	 0.5423020124435425
2022-10-28 14:45:02,094:INFO: Average validation:	ADE  0.7923	FDE  1.5076
2022-10-28 14:45:02,095:INFO: - Computing ADE (training)
2022-10-28 14:45:09,781:INFO: 		 ADE on hotel                     dataset:	 1.3941106796264648
2022-10-28 14:45:31,370:INFO: 		 ADE on univ                      dataset:	 0.8883201479911804
2022-10-28 14:45:39,561:INFO: 		 ADE on zara1                     dataset:	 0.6623778343200684
2022-10-28 14:46:01,776:INFO: 		 ADE on zara2                     dataset:	 0.5457532405853271
2022-10-28 14:46:01,777:INFO: Average training:	ADE  0.8173	FDE  1.5502
2022-10-28 14:46:01,798:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_248.pth.tar
2022-10-28 14:46:01,799:INFO: 
===> EPOCH: 249 (P2)
2022-10-28 14:46:01,799:INFO: - Computing loss (training)
2022-10-28 14:46:09,457:INFO: Dataset: hotel               Batch: 1/8	Loss 35.6156 (35.6156)
2022-10-28 14:46:11,257:INFO: Dataset: hotel               Batch: 2/8	Loss 35.7326 (35.6738)
2022-10-28 14:46:13,053:INFO: Dataset: hotel               Batch: 3/8	Loss 35.5929 (35.6441)
2022-10-28 14:46:14,902:INFO: Dataset: hotel               Batch: 4/8	Loss 35.5623 (35.6211)
2022-10-28 14:46:16,708:INFO: Dataset: hotel               Batch: 5/8	Loss 35.6608 (35.6289)
2022-10-28 14:46:18,473:INFO: Dataset: hotel               Batch: 6/8	Loss 35.7759 (35.6527)
2022-10-28 14:46:20,335:INFO: Dataset: hotel               Batch: 7/8	Loss 35.5522 (35.6385)
2022-10-28 14:46:21,419:INFO: Dataset: hotel               Batch: 8/8	Loss 7.7831 (34.7565)
2022-10-28 14:46:43,428:INFO: Dataset: univ                Batch:  1/29	Loss 35.3798 (35.3798)
2022-10-28 14:46:45,264:INFO: Dataset: univ                Batch:  2/29	Loss 35.3047 (35.3429)
2022-10-28 14:46:47,053:INFO: Dataset: univ                Batch:  3/29	Loss 35.4333 (35.3734)
2022-10-28 14:46:48,899:INFO: Dataset: univ                Batch:  4/29	Loss 35.3845 (35.3764)
2022-10-28 14:46:50,780:INFO: Dataset: univ                Batch:  5/29	Loss 35.3953 (35.3803)
2022-10-28 14:46:52,565:INFO: Dataset: univ                Batch:  6/29	Loss 35.3863 (35.3812)
2022-10-28 14:46:54,400:INFO: Dataset: univ                Batch:  7/29	Loss 35.3880 (35.3823)
2022-10-28 14:46:56,230:INFO: Dataset: univ                Batch:  8/29	Loss 35.3683 (35.3802)
2022-10-28 14:46:58,046:INFO: Dataset: univ                Batch:  9/29	Loss 35.4273 (35.3854)
2022-10-28 14:46:59,859:INFO: Dataset: univ                Batch: 10/29	Loss 35.3247 (35.3789)
2022-10-28 14:47:01,765:INFO: Dataset: univ                Batch: 11/29	Loss 35.3677 (35.3779)
2022-10-28 14:47:03,580:INFO: Dataset: univ                Batch: 12/29	Loss 35.4391 (35.3827)
2022-10-28 14:47:05,438:INFO: Dataset: univ                Batch: 13/29	Loss 35.4349 (35.3868)
2022-10-28 14:47:07,302:INFO: Dataset: univ                Batch: 14/29	Loss 35.3894 (35.3870)
2022-10-28 14:47:09,097:INFO: Dataset: univ                Batch: 15/29	Loss 35.3745 (35.3861)
2022-10-28 14:47:10,950:INFO: Dataset: univ                Batch: 16/29	Loss 35.3734 (35.3852)
2022-10-28 14:47:12,846:INFO: Dataset: univ                Batch: 17/29	Loss 35.4142 (35.3870)
2022-10-28 14:47:14,713:INFO: Dataset: univ                Batch: 18/29	Loss 35.4227 (35.3888)
2022-10-28 14:47:16,537:INFO: Dataset: univ                Batch: 19/29	Loss 35.3780 (35.3882)
2022-10-28 14:47:18,388:INFO: Dataset: univ                Batch: 20/29	Loss 35.4384 (35.3908)
2022-10-28 14:47:20,167:INFO: Dataset: univ                Batch: 21/29	Loss 35.3847 (35.3905)
2022-10-28 14:47:22,003:INFO: Dataset: univ                Batch: 22/29	Loss 35.4483 (35.3924)
2022-10-28 14:47:23,830:INFO: Dataset: univ                Batch: 23/29	Loss 35.4050 (35.3930)
2022-10-28 14:47:25,668:INFO: Dataset: univ                Batch: 24/29	Loss 35.3446 (35.3910)
2022-10-28 14:47:27,586:INFO: Dataset: univ                Batch: 25/29	Loss 35.3243 (35.3882)
2022-10-28 14:47:29,427:INFO: Dataset: univ                Batch: 26/29	Loss 35.3164 (35.3852)
2022-10-28 14:47:31,301:INFO: Dataset: univ                Batch: 27/29	Loss 35.4029 (35.3858)
2022-10-28 14:47:33,197:INFO: Dataset: univ                Batch: 28/29	Loss 35.3796 (35.3856)
2022-10-28 14:47:34,396:INFO: Dataset: univ                Batch: 29/29	Loss 13.3194 (35.1493)
2022-10-28 14:47:42,930:INFO: Dataset: zara1               Batch:  1/16	Loss 35.1708 (35.1708)
2022-10-28 14:47:44,820:INFO: Dataset: zara1               Batch:  2/16	Loss 35.2610 (35.2110)
2022-10-28 14:47:46,624:INFO: Dataset: zara1               Batch:  3/16	Loss 35.1982 (35.2063)
2022-10-28 14:47:48,480:INFO: Dataset: zara1               Batch:  4/16	Loss 35.1888 (35.2015)
2022-10-28 14:47:50,313:INFO: Dataset: zara1               Batch:  5/16	Loss 35.1783 (35.1966)
2022-10-28 14:47:52,058:INFO: Dataset: zara1               Batch:  6/16	Loss 35.1723 (35.1922)
2022-10-28 14:47:53,880:INFO: Dataset: zara1               Batch:  7/16	Loss 35.1649 (35.1881)
2022-10-28 14:47:55,697:INFO: Dataset: zara1               Batch:  8/16	Loss 35.2318 (35.1933)
2022-10-28 14:47:57,531:INFO: Dataset: zara1               Batch:  9/16	Loss 35.1634 (35.1895)
2022-10-28 14:47:59,338:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1382 (35.1838)
2022-10-28 14:48:01,206:INFO: Dataset: zara1               Batch: 11/16	Loss 35.0932 (35.1757)
2022-10-28 14:48:03,024:INFO: Dataset: zara1               Batch: 12/16	Loss 35.1898 (35.1769)
2022-10-28 14:48:04,777:INFO: Dataset: zara1               Batch: 13/16	Loss 35.1352 (35.1737)
2022-10-28 14:48:06,617:INFO: Dataset: zara1               Batch: 14/16	Loss 35.2131 (35.1765)
2022-10-28 14:48:08,393:INFO: Dataset: zara1               Batch: 15/16	Loss 35.2457 (35.1808)
2022-10-28 14:48:10,022:INFO: Dataset: zara1               Batch: 16/16	Loss 25.2446 (34.7520)
2022-10-28 14:48:31,324:INFO: Dataset: zara2               Batch:  1/36	Loss 35.2477 (35.2477)
2022-10-28 14:48:33,137:INFO: Dataset: zara2               Batch:  2/36	Loss 35.1601 (35.2059)
2022-10-28 14:48:34,892:INFO: Dataset: zara2               Batch:  3/36	Loss 35.1677 (35.1937)
2022-10-28 14:48:36,732:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1504 (35.1842)
2022-10-28 14:48:38,515:INFO: Dataset: zara2               Batch:  5/36	Loss 35.1600 (35.1796)
2022-10-28 14:48:40,347:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1321 (35.1714)
2022-10-28 14:48:42,109:INFO: Dataset: zara2               Batch:  7/36	Loss 35.2288 (35.1794)
2022-10-28 14:48:44,027:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1236 (35.1717)
2022-10-28 14:48:45,808:INFO: Dataset: zara2               Batch:  9/36	Loss 35.1767 (35.1722)
2022-10-28 14:48:47,635:INFO: Dataset: zara2               Batch: 10/36	Loss 35.1028 (35.1651)
2022-10-28 14:48:49,416:INFO: Dataset: zara2               Batch: 11/36	Loss 35.0846 (35.1575)
2022-10-28 14:48:51,246:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1534 (35.1571)
2022-10-28 14:48:53,012:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1988 (35.1605)
2022-10-28 14:48:54,854:INFO: Dataset: zara2               Batch: 14/36	Loss 35.1486 (35.1597)
2022-10-28 14:48:56,653:INFO: Dataset: zara2               Batch: 15/36	Loss 35.2011 (35.1622)
2022-10-28 14:48:58,499:INFO: Dataset: zara2               Batch: 16/36	Loss 35.0927 (35.1580)
2022-10-28 14:49:00,288:INFO: Dataset: zara2               Batch: 17/36	Loss 35.1284 (35.1562)
2022-10-28 14:49:02,148:INFO: Dataset: zara2               Batch: 18/36	Loss 35.1333 (35.1553)
2022-10-28 14:49:03,990:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1631 (35.1557)
2022-10-28 14:49:05,749:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1245 (35.1543)
2022-10-28 14:49:07,589:INFO: Dataset: zara2               Batch: 21/36	Loss 35.1941 (35.1562)
2022-10-28 14:49:09,354:INFO: Dataset: zara2               Batch: 22/36	Loss 35.1584 (35.1563)
2022-10-28 14:49:11,193:INFO: Dataset: zara2               Batch: 23/36	Loss 35.0781 (35.1530)
2022-10-28 14:49:12,998:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1712 (35.1536)
2022-10-28 14:49:14,750:INFO: Dataset: zara2               Batch: 25/36	Loss 35.1848 (35.1550)
2022-10-28 14:49:16,616:INFO: Dataset: zara2               Batch: 26/36	Loss 35.0995 (35.1531)
2022-10-28 14:49:18,388:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1820 (35.1540)
2022-10-28 14:49:20,214:INFO: Dataset: zara2               Batch: 28/36	Loss 35.2416 (35.1569)
2022-10-28 14:49:22,016:INFO: Dataset: zara2               Batch: 29/36	Loss 35.1599 (35.1570)
2022-10-28 14:49:23,882:INFO: Dataset: zara2               Batch: 30/36	Loss 35.2034 (35.1586)
2022-10-28 14:49:25,658:INFO: Dataset: zara2               Batch: 31/36	Loss 35.1733 (35.1591)
2022-10-28 14:49:27,510:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1831 (35.1598)
2022-10-28 14:49:29,320:INFO: Dataset: zara2               Batch: 33/36	Loss 35.1288 (35.1588)
2022-10-28 14:49:31,167:INFO: Dataset: zara2               Batch: 34/36	Loss 35.1845 (35.1596)
2022-10-28 14:49:32,949:INFO: Dataset: zara2               Batch: 35/36	Loss 35.1198 (35.1585)
2022-10-28 14:49:34,637:INFO: Dataset: zara2               Batch: 36/36	Loss 25.2464 (34.9373)
2022-10-28 14:49:35,420:INFO: - Computing ADE (validation o)
2022-10-28 14:49:42,791:INFO: 		 ADE on eth                       dataset:	 1.140541434288025
2022-10-28 14:49:42,791:INFO: Average validation o:	ADE  1.1405	FDE  1.9840
2022-10-28 14:49:42,792:INFO: - Computing ADE (validation)
2022-10-28 14:49:50,136:INFO: 		 ADE on hotel                     dataset:	 1.0205000638961792
2022-10-28 14:49:57,729:INFO: 		 ADE on univ                      dataset:	 0.9266604781150818
2022-10-28 14:50:05,081:INFO: 		 ADE on zara1                     dataset:	 0.5859332084655762
2022-10-28 14:50:12,961:INFO: 		 ADE on zara2                     dataset:	 0.5028380155563354
2022-10-28 14:50:12,961:INFO: Average validation:	ADE  0.7565	FDE  1.4385
2022-10-28 14:50:12,962:INFO: - Computing ADE (training)
2022-10-28 14:50:20,629:INFO: 		 ADE on hotel                     dataset:	 1.3304694890975952
2022-10-28 14:50:44,693:INFO: 		 ADE on univ                      dataset:	 0.8645538687705994
2022-10-28 14:50:53,247:INFO: 		 ADE on zara1                     dataset:	 0.654840350151062
2022-10-28 14:51:14,794:INFO: 		 ADE on zara2                     dataset:	 0.5124518871307373
2022-10-28 14:51:14,794:INFO: Average training:	ADE  0.7916	FDE  1.4973
2022-10-28 14:51:14,819:INFO:  --> Model Saved in ./models/eth/pretrain/P2/5.0/SSE_data_eth_irm[5.0]_epoch_249.pth.tar
2022-10-28 14:51:14,819:INFO: 
===> EPOCH: 250 (P2)
2022-10-28 14:51:14,820:INFO: - Computing loss (training)
2022-10-28 14:51:22,168:INFO: Dataset: hotel               Batch: 1/8	Loss 35.6312 (35.6312)
2022-10-28 14:51:23,861:INFO: Dataset: hotel               Batch: 2/8	Loss 35.6224 (35.6269)
2022-10-28 14:51:25,538:INFO: Dataset: hotel               Batch: 3/8	Loss 35.4737 (35.5773)
2022-10-28 14:51:27,242:INFO: Dataset: hotel               Batch: 4/8	Loss 35.7100 (35.6078)
2022-10-28 14:51:28,902:INFO: Dataset: hotel               Batch: 5/8	Loss 35.4990 (35.5842)
2022-10-28 14:51:30,563:INFO: Dataset: hotel               Batch: 6/8	Loss 35.9164 (35.6331)
2022-10-28 14:51:32,237:INFO: Dataset: hotel               Batch: 7/8	Loss 35.5603 (35.6224)
2022-10-28 14:51:33,282:INFO: Dataset: hotel               Batch: 8/8	Loss 7.8246 (34.9623)
2022-10-28 14:51:55,627:INFO: Dataset: univ                Batch:  1/29	Loss 35.3357 (35.3357)
2022-10-28 14:51:57,457:INFO: Dataset: univ                Batch:  2/29	Loss 35.3659 (35.3516)
2022-10-28 14:51:59,343:INFO: Dataset: univ                Batch:  3/29	Loss 35.3720 (35.3582)
2022-10-28 14:52:01,396:INFO: Dataset: univ                Batch:  4/29	Loss 35.3937 (35.3657)
2022-10-28 14:52:03,519:INFO: Dataset: univ                Batch:  5/29	Loss 35.3434 (35.3611)
2022-10-28 14:52:05,427:INFO: Dataset: univ                Batch:  6/29	Loss 35.4432 (35.3732)
2022-10-28 14:52:07,326:INFO: Dataset: univ                Batch:  7/29	Loss 35.4041 (35.3776)
2022-10-28 14:52:09,144:INFO: Dataset: univ                Batch:  8/29	Loss 35.4148 (35.3823)
2022-10-28 14:52:11,234:INFO: Dataset: univ                Batch:  9/29	Loss 35.3293 (35.3764)
2022-10-28 14:52:13,296:INFO: Dataset: univ                Batch: 10/29	Loss 35.3841 (35.3772)
2022-10-28 14:52:15,238:INFO: Dataset: univ                Batch: 11/29	Loss 35.4501 (35.3844)
2022-10-28 14:52:17,049:INFO: Dataset: univ                Batch: 12/29	Loss 35.3573 (35.3823)
2022-10-28 14:52:18,870:INFO: Dataset: univ                Batch: 13/29	Loss 35.4134 (35.3848)
2022-10-28 14:52:20,911:INFO: Dataset: univ                Batch: 14/29	Loss 35.3153 (35.3799)
2022-10-28 14:52:23,018:INFO: Dataset: univ                Batch: 15/29	Loss 35.4104 (35.3817)
2022-10-28 14:52:25,041:INFO: Dataset: univ                Batch: 16/29	Loss 35.3313 (35.3781)
2022-10-28 14:52:26,874:INFO: Dataset: univ                Batch: 17/29	Loss 35.3777 (35.3781)
2022-10-28 14:52:28,699:INFO: Dataset: univ                Batch: 18/29	Loss 35.3936 (35.3790)
2022-10-28 14:52:30,745:INFO: Dataset: univ                Batch: 19/29	Loss 35.3898 (35.3796)
2022-10-28 14:52:32,781:INFO: Dataset: univ                Batch: 20/29	Loss 35.3672 (35.3790)
2022-10-28 14:52:34,862:INFO: Dataset: univ                Batch: 21/29	Loss 35.4440 (35.3816)
2022-10-28 14:52:36,758:INFO: Dataset: univ                Batch: 22/29	Loss 35.3772 (35.3813)
2022-10-28 14:52:38,539:INFO: Dataset: univ                Batch: 23/29	Loss 35.3880 (35.3817)
2022-10-28 14:52:40,514:INFO: Dataset: univ                Batch: 24/29	Loss 35.4235 (35.3834)
2022-10-28 14:52:42,616:INFO: Dataset: univ                Batch: 25/29	Loss 35.3611 (35.3825)
2022-10-28 14:52:44,698:INFO: Dataset: univ                Batch: 26/29	Loss 35.3975 (35.3831)
2022-10-28 14:52:46,562:INFO: Dataset: univ                Batch: 27/29	Loss 35.3924 (35.3834)
2022-10-28 14:52:48,422:INFO: Dataset: univ                Batch: 28/29	Loss 35.4720 (35.3862)
2022-10-28 14:52:49,642:INFO: Dataset: univ                Batch: 29/29	Loss 13.2438 (34.9917)
2022-10-28 14:53:00,192:INFO: Dataset: zara1               Batch:  1/16	Loss 35.1633 (35.1633)
2022-10-28 14:53:02,041:INFO: Dataset: zara1               Batch:  2/16	Loss 35.0988 (35.1315)
2022-10-28 14:53:04,058:INFO: Dataset: zara1               Batch:  3/16	Loss 35.2079 (35.1594)
2022-10-28 14:53:06,537:INFO: Dataset: zara1               Batch:  4/16	Loss 35.1351 (35.1533)
2022-10-28 14:53:08,178:INFO: Dataset: zara1               Batch:  5/16	Loss 35.1910 (35.1602)
2022-10-28 14:53:10,282:INFO: Dataset: zara1               Batch:  6/16	Loss 35.2035 (35.1679)
2022-10-28 14:53:12,491:INFO: Dataset: zara1               Batch:  7/16	Loss 35.1922 (35.1711)
2022-10-28 14:53:14,699:INFO: Dataset: zara1               Batch:  8/16	Loss 35.1063 (35.1617)
2022-10-28 14:53:16,831:INFO: Dataset: zara1               Batch:  9/16	Loss 35.2201 (35.1690)
2022-10-28 14:53:18,967:INFO: Dataset: zara1               Batch: 10/16	Loss 35.1126 (35.1641)
2022-10-28 14:53:21,024:INFO: Dataset: zara1               Batch: 11/16	Loss 35.1979 (35.1673)
2022-10-28 14:53:23,155:INFO: Dataset: zara1               Batch: 12/16	Loss 35.2199 (35.1706)
2022-10-28 14:53:25,311:INFO: Dataset: zara1               Batch: 13/16	Loss 35.1124 (35.1664)
2022-10-28 14:53:27,482:INFO: Dataset: zara1               Batch: 14/16	Loss 35.2491 (35.1733)
2022-10-28 14:53:29,730:INFO: Dataset: zara1               Batch: 15/16	Loss 35.1413 (35.1710)
2022-10-28 14:53:31,536:INFO: Dataset: zara1               Batch: 16/16	Loss 25.3083 (34.7298)
2022-10-28 14:53:53,478:INFO: Dataset: zara2               Batch:  1/36	Loss 35.1199 (35.1199)
2022-10-28 14:53:55,891:INFO: Dataset: zara2               Batch:  2/36	Loss 35.1804 (35.1490)
2022-10-28 14:53:58,208:INFO: Dataset: zara2               Batch:  3/36	Loss 35.1224 (35.1396)
2022-10-28 14:54:00,413:INFO: Dataset: zara2               Batch:  4/36	Loss 35.1395 (35.1396)
2022-10-28 14:54:02,583:INFO: Dataset: zara2               Batch:  5/36	Loss 35.2068 (35.1515)
2022-10-28 14:54:04,475:INFO: Dataset: zara2               Batch:  6/36	Loss 35.1688 (35.1544)
2022-10-28 14:54:06,574:INFO: Dataset: zara2               Batch:  7/36	Loss 35.1127 (35.1480)
2022-10-28 14:54:08,367:INFO: Dataset: zara2               Batch:  8/36	Loss 35.1236 (35.1448)
2022-10-28 14:54:10,113:INFO: Dataset: zara2               Batch:  9/36	Loss 35.1772 (35.1484)
2022-10-28 14:54:11,955:INFO: Dataset: zara2               Batch: 10/36	Loss 35.1766 (35.1514)
2022-10-28 14:54:13,824:INFO: Dataset: zara2               Batch: 11/36	Loss 35.1066 (35.1472)
2022-10-28 14:54:15,579:INFO: Dataset: zara2               Batch: 12/36	Loss 35.1426 (35.1468)
2022-10-28 14:54:17,260:INFO: Dataset: zara2               Batch: 13/36	Loss 35.1539 (35.1473)
2022-10-28 14:54:18,915:INFO: Dataset: zara2               Batch: 14/36	Loss 35.1038 (35.1440)
2022-10-28 14:54:20,831:INFO: Dataset: zara2               Batch: 15/36	Loss 35.1929 (35.1472)
2022-10-28 14:54:22,640:INFO: Dataset: zara2               Batch: 16/36	Loss 35.1113 (35.1449)
2022-10-28 14:54:24,290:INFO: Dataset: zara2               Batch: 17/36	Loss 35.0801 (35.1406)
2022-10-28 14:54:25,922:INFO: Dataset: zara2               Batch: 18/36	Loss 35.1472 (35.1410)
2022-10-28 14:54:27,673:INFO: Dataset: zara2               Batch: 19/36	Loss 35.1265 (35.1401)
2022-10-28 14:54:29,736:INFO: Dataset: zara2               Batch: 20/36	Loss 35.1422 (35.1402)
2022-10-28 14:54:31,562:INFO: Dataset: zara2               Batch: 21/36	Loss 35.1809 (35.1422)
2022-10-28 14:54:33,281:INFO: Dataset: zara2               Batch: 22/36	Loss 35.0901 (35.1398)
2022-10-28 14:54:35,055:INFO: Dataset: zara2               Batch: 23/36	Loss 35.1902 (35.1417)
2022-10-28 14:54:36,953:INFO: Dataset: zara2               Batch: 24/36	Loss 35.1181 (35.1407)
2022-10-28 14:54:38,781:INFO: Dataset: zara2               Batch: 25/36	Loss 35.0991 (35.1389)
2022-10-28 14:54:40,498:INFO: Dataset: zara2               Batch: 26/36	Loss 35.1907 (35.1408)
2022-10-28 14:54:42,211:INFO: Dataset: zara2               Batch: 27/36	Loss 35.1324 (35.1405)
2022-10-28 14:54:43,906:INFO: Dataset: zara2               Batch: 28/36	Loss 35.1545 (35.1409)
2022-10-28 14:54:46,510:INFO: Dataset: zara2               Batch: 29/36	Loss 35.1759 (35.1423)
2022-10-28 14:54:48,434:INFO: Dataset: zara2               Batch: 30/36	Loss 35.1081 (35.1413)
2022-10-28 14:54:50,200:INFO: Dataset: zara2               Batch: 31/36	Loss 35.3051 (35.1464)
2022-10-28 14:54:51,972:INFO: Dataset: zara2               Batch: 32/36	Loss 35.1481 (35.1465)
2022-10-28 14:54:53,649:INFO: Dataset: zara2               Batch: 33/36	Loss 35.2544 (35.1493)
2022-10-28 14:54:55,329:INFO: Dataset: zara2               Batch: 34/36	Loss 35.2130 (35.1513)
2022-10-28 14:54:57,000:INFO: Dataset: zara2               Batch: 35/36	Loss 35.0937 (35.1493)
2022-10-28 14:54:58,524:INFO: Dataset: zara2               Batch: 36/36	Loss 25.2743 (34.9224)
2022-10-28 14:54:59,697:INFO: - Computing ADE (validation o)
2022-10-28 14:55:07,787:INFO: 		 ADE on eth                       dataset:	 1.1327310800552368
2022-10-28 14:55:07,787:INFO: Average validation o:	ADE  1.1327	FDE  1.9598
2022-10-28 14:55:07,788:INFO: - Computing ADE (validation)
2022-10-28 14:55:15,864:INFO: 		 ADE on hotel                     dataset:	 1.0285751819610596
2022-10-28 14:55:23,358:INFO: 		 ADE on univ                      dataset:	 0.929729163646698

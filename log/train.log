2022-11-12 19:01:49,302:INFO: Initializing Training Set
2022-11-12 19:01:52,931:INFO: Initializing Validation Set
2022-11-12 19:01:53,343:INFO: Initializing Validation O Set
2022-11-12 19:01:56,554:INFO: 
===> EPOCH: 251 (P3)
2022-11-12 19:01:56,555:INFO: - Computing loss (training)
2022-11-12 19:02:05,186:INFO: Dataset: hotel               Batch: 1/8	Loss 44.1134 (44.1134)
2022-11-12 19:02:06,843:INFO: Dataset: hotel               Batch: 2/8	Loss 46.2701 (45.1355)
2022-11-12 19:02:08,527:INFO: Dataset: hotel               Batch: 3/8	Loss 42.5087 (44.3082)
2022-11-12 19:02:10,123:INFO: Dataset: hotel               Batch: 4/8	Loss 45.2991 (44.5547)
2022-11-12 19:02:11,714:INFO: Dataset: hotel               Batch: 5/8	Loss 44.3128 (44.5047)
2022-11-12 19:02:13,475:INFO: Dataset: hotel               Batch: 6/8	Loss 44.8864 (44.5676)
2022-11-12 19:02:15,937:INFO: Dataset: hotel               Batch: 7/8	Loss 42.0122 (44.1614)
2022-11-12 19:02:17,608:INFO: Dataset: hotel               Batch: 8/8	Loss 9.7103 (43.1615)
2022-11-12 19:02:46,108:INFO: Dataset: univ                Batch:  1/29	Loss 45.0305 (45.0305)
2022-11-12 19:02:48,964:INFO: Dataset: univ                Batch:  2/29	Loss 43.5380 (44.2277)
2022-11-12 19:02:50,333:INFO: Dataset: univ                Batch:  3/29	Loss 42.7041 (43.6533)
2022-11-12 19:02:51,740:INFO: Dataset: univ                Batch:  4/29	Loss 43.1406 (43.5242)
2022-11-12 19:02:53,041:INFO: Dataset: univ                Batch:  5/29	Loss 42.6504 (43.3297)
2022-11-12 19:02:54,427:INFO: Dataset: univ                Batch:  6/29	Loss 41.5321 (42.9862)
2022-11-12 19:02:55,714:INFO: Dataset: univ                Batch:  7/29	Loss 42.9099 (42.9748)
2022-11-12 19:02:57,361:INFO: Dataset: univ                Batch:  8/29	Loss 42.2924 (42.8795)
2022-11-12 19:02:58,963:INFO: Dataset: univ                Batch:  9/29	Loss 44.3959 (43.0269)
2022-11-12 19:03:00,251:INFO: Dataset: univ                Batch: 10/29	Loss 42.3826 (42.9650)
2022-11-12 19:03:01,487:INFO: Dataset: univ                Batch: 11/29	Loss 41.1955 (42.7879)
2022-11-12 19:03:02,966:INFO: Dataset: univ                Batch: 12/29	Loss 41.7710 (42.6993)
2022-11-12 19:03:04,375:INFO: Dataset: univ                Batch: 13/29	Loss 42.8336 (42.7088)
2022-11-12 19:03:05,959:INFO: Dataset: univ                Batch: 14/29	Loss 41.4276 (42.6142)
2022-11-12 19:03:07,282:INFO: Dataset: univ                Batch: 15/29	Loss 41.9736 (42.5713)
2022-11-12 19:03:09,273:INFO: Dataset: univ                Batch: 16/29	Loss 41.3120 (42.4974)
2022-11-12 19:03:11,121:INFO: Dataset: univ                Batch: 17/29	Loss 42.7774 (42.5129)
2022-11-12 19:03:12,495:INFO: Dataset: univ                Batch: 18/29	Loss 40.8235 (42.4129)
2022-11-12 19:03:13,980:INFO: Dataset: univ                Batch: 19/29	Loss 41.4551 (42.3629)
2022-11-12 19:03:16,006:INFO: Dataset: univ                Batch: 20/29	Loss 40.9240 (42.2942)
2022-11-12 19:03:17,483:INFO: Dataset: univ                Batch: 21/29	Loss 40.6784 (42.2137)
2022-11-12 19:03:19,084:INFO: Dataset: univ                Batch: 22/29	Loss 41.1978 (42.1678)
2022-11-12 19:03:20,347:INFO: Dataset: univ                Batch: 23/29	Loss 40.7960 (42.1058)
2022-11-12 19:03:21,693:INFO: Dataset: univ                Batch: 24/29	Loss 41.4828 (42.0840)
2022-11-12 19:03:23,073:INFO: Dataset: univ                Batch: 25/29	Loss 40.6163 (42.0243)

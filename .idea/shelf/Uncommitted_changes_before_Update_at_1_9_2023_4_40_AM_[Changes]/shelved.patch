Index: parser_file.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\r\nfrom utils import int_tuple\r\n\r\n\r\ndef get_parser():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\"--log_dir\", default=\"./log/\", help=\"Directory containing logging file\")\r\n    parser.add_argument(\"--model_dir\", default=\"./models/E19/\", help=\"Directory containing logging file\")\r\n    parser.add_argument(\"--tfdir\", default='./runs/E19/', type=str)\r\n    parser.add_argument(\"--dataset_name\", default=\"v4\", type=str)\r\n    parser.add_argument(\"--model_name\", default=\"mlp\", type=str)\r\n    parser.add_argument(\"--resume\", default=\"./models/E19//P4/CRMF_epoch_351.pth.tar\",\r\n                        type=str, metavar=\"PATH\", help=\"path to latest checkpoint (default: none)\")\r\n\r\n    # randomness\r\n    parser.add_argument(\"--num_samples\", type=int, default=10, help=\"Number of samples to calculate MC expectations\")\r\n    parser.add_argument(\"--seed\", type=int, default=1, help=\"Random seed\")\r\n\r\n    # computation\r\n    parser.add_argument(\"--gpu_num\", default=\"1\", type=str)\r\n    parser.add_argument(\"--loader_num_workers\", default=6, type=int)\r\n\r\n    # architecture (STGAT)\r\n    parser.add_argument(\"--traj_lstm_hidden_size\", default=32, type=int)\r\n    parser.add_argument(\"--hidden-units\", type=str, default=\"16\",\r\n                        help=\"Hidden units in each hidden layer, splitted with comma\")\r\n    parser.add_argument(\"--graph_lstm_hidden_size\", default=32, type=int)\r\n    parser.add_argument(\"--heads\", type=str, default=\"4,1\", help=\"Heads in each layer, splitted with comma\")\r\n    parser.add_argument(\"--graph_network_out_dims\", type=int, default=32,\r\n                        help=\"dims of every node after through GAT module\")\r\n    parser.add_argument(\"--dropout\", type=float, default=0, help=\"Dropout rate (1 - keep probability)\")\r\n    parser.add_argument(\"--alpha\", type=float, default=0.2, help=\"Alpha for the leaky_relu\")\r\n    parser.add_argument('--teachingratio', default=0.0, type=float,\r\n                        help=\"The probability of using ground truth future trajectories instead of model predictions during training\")\r\n\r\n    # dataset\r\n    parser.add_argument(\"--obs_len\", default=8, type=int)\r\n    parser.add_argument(\"--fut_len\", default=12, type=int)\r\n    parser.add_argument(\"--n_coordinates\", type=int, default=2, help=\"Number of coordinates\")\r\n    parser.add_argument(\"--filter_envs\", type=str, default=\"0.1-0.3-0.5\",\r\n                        help=\"Filter only certain environments (i.e 0.1-0.3-0.5)\")\r\n    parser.add_argument(\"--skip\", default=1, type=int)\r\n    parser.add_argument(\"--delim\", default=\"\\t\")\r\n    parser.add_argument(\"--finetune_ratio\", default=0.1, type=float, help=\"Number of batches to be used in finetuning\")\r\n    parser.add_argument(\"--batch_method\", default='het', type=str,\r\n                        help='Use Homogeneous (hom), Heterogeneous (het) or alternated homogeneous (alt) batches during training')\r\n    parser.add_argument(\"--contrastive\", default=False, type=bool, help='add contrastive loss')\r\n    parser.add_argument(\"--decoupled_loss\", default=True, type=bool, help='decouple ELBO from y')\r\n\r\n    parser.add_argument(\"--batch_size\", default='64', type=str)\r\n    parser.add_argument(\"--shuffle\", default=True, type=bool)\r\n    parser.add_argument('--reduce', default=0, type=int)\r\n    parser.add_argument('--reduceall', default=9000, type=int)\r\n\r\n\r\n    # architecture (VE)\r\n    parser.add_argument(\"--z_dim\", type=int, default=2, help=\"Dimension of z latent variable\")\r\n    parser.add_argument(\"--s_dim\", type=int, default=2, help=\"Dimension of s latent variable\")\r\n    parser.add_argument(\"--num_envs\", default=5, type=int, help=\"Number of environments in the dataset\")\r\n\r\n    # spurious feature\r\n    parser.add_argument(\"--add_confidence\", default=False, type=bool)\r\n    parser.add_argument(\"--domain_shifts\", default='0', type=str,\r\n                        help='domain_shifts per environment: hotel,univ,zara1,zara2,eth')\r\n\r\n    return parser\r\n\r\n\r\ndef get_evaluation_parser():\r\n    parser = get_parser()\r\n    parser.add_argument(\"--dset_type\", default=\"test\", type=str)\r\n    parser.add_argument(\"--best_k\", default=20, type=int)\r\n    parser.add_argument('--metrics', type=str, default='accuracy', choices=['accuracy', 'collision', 'qualitative'],\r\n                        help='evaluate metrics')\r\n\r\n    return parser\r\n\r\n\r\ndef get_training_parser():\r\n    parser = get_parser()\r\n\r\n    # dataset\r\n    parser.add_argument(\"--filter_envs_pretrain\", type=str, default=\"\",\r\n                        help=\"Say which env were used during pretraining (for contrastive loss) (i.e 0.1-0.3-0.5)\")\r\n\r\n    # training\r\n    parser.add_argument(\"--best_k\", default=20, type=int)\r\n    parser.add_argument(\"--start-epoch\", default=1, type=int, metavar=\"N\",\r\n                        help=\"manual epoch number (useful on restarts)\")\r\n    parser.add_argument(\"--use_gpu\", default=1, type=int)\r\n\r\n    # general training\r\n    parser.add_argument(\"--finetune\", default=\"\", type=str)\r\n    parser.add_argument(\"--num_epochs\", default='150-100-100-1-20-4000-100', type=lambda x: int_tuple(x, '-'))  # '150-100-150',\r\n\r\n    # learning rates\r\n    parser.add_argument(\"--lr_scheduler\", default=True, type=bool)  # '150-100-150',\r\n\r\n    parser.add_argument(\"--lrvar\", default=1e-2, type=float,\r\n                        help=\"initial learning rate for variant encoder optimizer\")\r\n    parser.add_argument('--lrinv', default=5e-3, type=float,\r\n                        help=\"initial learning rate for the invariant encoder optimizer\")\r\n    parser.add_argument('--lrfut', default=5e-3, type=float,\r\n                        help=\"initial learning rate for the future decoder optimizer\")\r\n    parser.add_argument('--lrpast', default=5e-3, type=float,\r\n                        help=\"initial learning rate for the past decoder optimizer\")\r\n    parser.add_argument('--lrmap', default=5e-3, type=float,\r\n                        help=\"initial learning rate for the regressor optimizer\")\r\n    parser.add_argument('--lrpar', default=1e-2, type=float,\r\n                        help=\"initial learning rate for the parameters optimizer\")\r\n\r\n    return parser\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/parser_file.py b/parser_file.py
--- a/parser_file.py	(revision e49d5ca3563b1639c7c7fb672f11981586cd98e0)
+++ b/parser_file.py	(date 1673257072027)
@@ -9,7 +9,7 @@
     parser.add_argument("--tfdir", default='./runs/E19/', type=str)
     parser.add_argument("--dataset_name", default="v4", type=str)
     parser.add_argument("--model_name", default="mlp", type=str)
-    parser.add_argument("--resume", default="./models/E19//P4/CRMF_epoch_351.pth.tar",
+    parser.add_argument("--resume", default="./models/E19//P6/CRMF_epoch_790.pth.tar",
                         type=str, metavar="PATH", help="path to latest checkpoint (default: none)")
 
     # randomness
@@ -50,7 +50,7 @@
     parser.add_argument("--batch_size", default='64', type=str)
     parser.add_argument("--shuffle", default=True, type=bool)
     parser.add_argument('--reduce', default=0, type=int)
-    parser.add_argument('--reduceall', default=9000, type=int)
+    parser.add_argument('--reduceall', default=10, type=int)
 
 
     # architecture (VE)
Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nfrom tqdm import tqdm\r\n\r\nfrom loader import data_loader\r\nfrom parser_file import get_training_parser\r\nfrom utils import *\r\nfrom models import CRMF\r\nfrom losses import erm_loss, irm_loss, SupConLoss\r\nimport math\r\nfrom torch.optim.lr_scheduler import OneCycleLR\r\nfrom torch.nn.utils import clip_grad\r\nfrom scipy.optimize import linear_sum_assignment as linear_assignment\r\n\r\n\r\ndef main(args):\r\n    # Set environment variables\r\n    set_seed_globally(args.seed)\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_num\r\n    model_name = set_name_experiment(args)\r\n    print('model name: ', model_name)\r\n    if not os.path.exists(args.tfdir + '/' + model_name):\r\n        os.makedirs(args.tfdir + '/' + model_name)\r\n\r\n    writer = SummaryWriter(log_dir=args.tfdir + '/' + model_name, flush_secs=10)\r\n\r\n    logging.info(\"Initializing Training Set\")\r\n    train_envs_path, train_envs_name = get_envs_path(args.dataset_name, \"train\", args.filter_envs)\r\n    train_loaders = [data_loader(args, train_env_path, train_env_name) for train_env_path, train_env_name in\r\n                     zip(train_envs_path, train_envs_name)]\r\n\r\n    logging.info(\"Initializing Validation Set\")\r\n    val_envs_path, val_envs_name = get_envs_path(args.dataset_name, \"val\", args.filter_envs)\r\n    val_loaders = [data_loader(args, val_env_path, val_env_name) for val_env_path, val_env_name in\r\n                   zip(val_envs_path, val_envs_name)]\r\n\r\n    logging.info(\"Initializing Validation O Set\")\r\n    valo_envs_path, valo_envs_name = get_envs_path(args.dataset_name, \"test\", '0.6')\r\n\r\n    valo_loaders = [data_loader(args, valo_env_path, valo_env_name, test=True) for valo_env_path, valo_env_name in\r\n                    zip(valo_envs_path, valo_envs_name)]\r\n    finetune_loaders = [data_loader(args, valo_env_path, valo_env_name, finetune=True) for valo_env_path, valo_env_name\r\n                        in zip(valo_envs_path, valo_envs_name)]\r\n\r\n    # training routine length\r\n    num_batches_train = min([len(train_loader) for train_loader in train_loaders])\r\n    num_batches_val = min([len(val_loader) for val_loader in val_loaders])\r\n    num_batches_valo = min([len(valo_loader) for valo_loader in valo_loaders])\r\n    num_batches_finetune = min([len(finetune_loader) for finetune_loader in finetune_loaders])\r\n\r\n    # get labels of envs and create dic linking env name and env label\r\n    if args.dataset_name in ('eth', 'hotel', 'univ', 'zara1', 'zara2'):\r\n        # assert (all_train_labels == all_valid_labels)\r\n        train_labels = {name: train_envs_name.index(name) for name in train_envs_name}\r\n        val_labels = {name: val_envs_name.index(name) for name in val_envs_name}\r\n        valo_labels = {name: valo_envs_name.index(name) for name in valo_envs_name}\r\n\r\n    elif 'synthetic' in args.dataset_name or args.dataset_name in ['synthetic', 'v2', 'v2full', 'v4']:\r\n        all_train_labels = sorted([float(d.split('_')[7]) for d in train_envs_name])\r\n        all_valid_labels = sorted([float(d.split('_')[7]) for d in val_envs_name])\r\n        all_valid_labelso = sorted([float(d.split('_')[7]) for d in valo_envs_name])\r\n        # assert (all_train_labels == all_valid_labels)\r\n        train_labels = {name: all_train_labels.index(float(name.split('_')[7])) for name in train_envs_name}\r\n        val_labels = {name: all_valid_labels.index(float(name.split('_')[7])) for name in val_envs_name}\r\n        valo_labels = {name: all_valid_labelso.index(float(name.split('_')[7])) for name in valo_envs_name}\r\n    else:\r\n        raise ValueError('Unrecognized dataset name \"%s\"' % args.dataset_name)\r\n\r\n    # bring different dataset all together for simplicity of the next functions\r\n    train_dataset = {'loaders': train_loaders, 'names': train_envs_name, 'labels': train_labels, 'num_batches': num_batches_train}\r\n    valid_dataset = {'loaders': val_loaders, 'names': val_envs_name, 'labels': val_labels, 'num_batches': num_batches_val}\r\n    valido_dataset = {'loaders': valo_loaders, 'names': valo_envs_name, 'labels': valo_labels, 'num_batches': num_batches_valo}\r\n    finetune_dataset = {'loaders': finetune_loaders, 'names': valo_envs_name, 'labels': valo_labels, 'num_batches': num_batches_finetune}\r\n\r\n    for dataset, ds_name in zip((train_dataset, valid_dataset, valido_dataset, finetune_dataset),\r\n                                ('Train', 'Validation', 'Validation O', 'Finetune')):\r\n        print(ds_name + ' dataset: ', dataset)\r\n\r\n    args.n_units = (\r\n            [args.traj_lstm_hidden_size]\r\n            + [int(x) for x in args.hidden_units.strip().split(\",\")]\r\n            + [args.graph_lstm_hidden_size]\r\n    )\r\n    args.n_heads = [int(x) for x in args.heads.strip().split(\",\")]\r\n\r\n    # create the model\r\n    model = CRMF(args).cuda()\r\n\r\n    # style related optimizer\r\n    optimizers = {\r\n        'par': torch.optim.Adam(\r\n            [\r\n                {\"params\": model.pi_priore, 'lr': args.lrpar},\r\n                {\"params\": model.mean_priors, 'lr': args.lrpar},\r\n                {\"params\": model.logvar_priors, 'lr': args.lrpar},\r\n                # {\"params\": model.coupling_layers_s.parameters(), 'lr': args.lrpar},\r\n            ]\r\n        ),\r\n        'var': torch.optim.Adam(\r\n            [\r\n                {\"params\": model.variant_encoder.parameters(), 'lr': args.lrvar},\r\n                {\"params\": model.x_to_s.parameters(), 'lr': args.lrvar},\r\n            ]\r\n        ),\r\n        'inv': torch.optim.Adam(\r\n            [\r\n                # {\"params\": model.coupling_layers_z.parameters(), 'lr': args.lrinv},\r\n                {\"params\": model.mean_priorz, 'lr': args.lrinv},\r\n                {\"params\": model.logvar_priorz, 'lr': args.lrinv},\r\n                {\"params\": model.x_to_z.parameters(), 'lr': args.lrinv},\r\n                {\"params\": model.invariant_encoder.parameters(), 'lr': args.lrinv},\r\n            ]\r\n        ),\r\n        'future_decoder': torch.optim.Adam(\r\n            model.future_decoder.parameters(),\r\n            lr=args.lrfut,\r\n        ),\r\n        'past_decoder': torch.optim.Adam(\r\n            model.past_decoder.parameters(),\r\n            lr=args.lrpast,\r\n        )\r\n    }\r\n\r\n    if args.contrastive:\r\n        optimizers[\"par\"].add_param_group({\"params\": model.cont_classifier.parameters(), 'lr': args.lrpar})\r\n\r\n    num_batches = 0\r\n    for train_loader in train_loaders:\r\n        num_batches += len(train_loader)\r\n\r\n    training_steps = np.array([sum(args.num_epochs[:i]) - sum(args.num_epochs[:i - 1]) for i in range(1, 8)])\r\n    total_steps = num_batches * training_steps\r\n\r\n    if args.lr_scheduler:\r\n        lr_schedulers = {\r\n            \"P1\": None,\r\n            \"P2\": None,\r\n            \"P3\": None,\r\n            \"P4\": None,\r\n            \"P5\": None,\r\n            \"P6\": {\r\n                'var': OneCycleLR(optimizers['var'], max_lr=1e-3, div_factor=25.0, total_steps=int(total_steps[5]),\r\n                                  pct_start=0.3),\r\n                'inv': OneCycleLR(optimizers['inv'], max_lr=1e-3, div_factor=25.0, total_steps=int(total_steps[5]),\r\n                                  pct_start=0.3),\r\n                'future_decoder': OneCycleLR(optimizers['future_decoder'], max_lr=1e-3, div_factor=25.0,\r\n                                             total_steps=int(total_steps[5]), pct_start=0.3),\r\n                'past_decoder': OneCycleLR(optimizers['past_decoder'], max_lr=1e-3, div_factor=25.0,\r\n                                           total_steps=int(total_steps[5]), pct_start=0.3),\r\n                'par': OneCycleLR(optimizers['par'], max_lr=1e-3, div_factor=25.0, total_steps=int(total_steps[5]),\r\n                                  pct_start=0.3),\r\n            },\r\n            \"P7\": None,\r\n        }\r\n    else:\r\n        lr_schedulers = {\r\n            \"P1\": None,\r\n            \"P2\": None,\r\n            \"P3\": None,\r\n            \"P4\": None,\r\n            \"P5\": None,\r\n            \"P6\": None,\r\n            \"P7\": None\r\n        }\r\n\r\n    # TRAINING HAPPENS IN 4 STEPS:\r\n    assert (len(args.num_epochs) == 7)\r\n    # 1. Train the invariant encoder along with the future decoder to learn z\r\n    # 2. Train everything except invariant encoder to learn the other variant latent variables\r\n    training_steps = {f'P{i}': [sum(args.num_epochs[:i - 1]), sum(args.num_epochs[:i])] for i in range(1, 8)}\r\n    print(training_steps)\r\n\r\n    if args.resume:\r\n        load_all_model(args, model, optimizers, lr_schedulers, training_steps, num_batches)\r\n        model.cuda()\r\n\r\n    def get_training_step(epoch):\r\n        for step, r in training_steps.items():\r\n            if r[0] < epoch <= r[1]:\r\n                return step\r\n\r\n    min_metric = 1e10\r\n    metric = min_metric\r\n    beta_scheduler = get_beta(training_steps[\"P3\"][0], 100, 1e-6)\r\n    for epoch in range(args.start_epoch, sum(args.num_epochs) + 1):\r\n\r\n        training_step = get_training_step(epoch)\r\n        if training_step in [\"P1\", \"P2\", \"P5\"]:\r\n            continue\r\n        logging.info(f\"\\n===> EPOCH: {epoch} ({training_step})\")\r\n\r\n        if training_step in ['P1', 'P2']:\r\n            freeze(True, (model.x_to_z, model.x_to_s, model.past_decoder, model.future_decoder, model.coupling_layers_z))\r\n            freeze(False, (model.variant_encoder, model.invariant_encoder))\r\n\r\n        # if training_step == 'P3':\r\n        #     freeze(True, (model.coupling_layers_z))\r\n        #     freeze(False, (model.variant_encoder, model.invariant_encoder, model.future_decoder, model.x_to_s, model.x_to_z, model.past_decoder))\r\n\r\n        if training_step == 'P4':\r\n            pass\r\n\r\n        elif training_step == 'P5':\r\n            freeze(True, (model.invariant_encoder, model.x_to_z, model.past_decoder, model.future_decoder))\r\n            freeze(False, (model.variant_encoder, model.x_to_s))\r\n\r\n        # elif training_step == \"P6\":\r\n        #     freeze(False, (model.invariant_encoder, model.variant_encoder, model.x_to_s, model.x_to_z, model.past_decoder, model.future_decoder, model.coupling_layers_z))\r\n\r\n        elif training_step == \"P7\":\r\n            freeze(True, (model.invariant_encoder, model.x_to_z, model.past_decoder, model.future_decoder, model.coupling_layers_z, model.coupling_layers_s))\r\n            freeze(False, (model.variant_encoder, model.x_to_s, model.future_decoder))\r\n\r\n        if training_step in [\"P1\", \"P2\", \"P3\", \"P5\", \"P6\"]:\r\n            train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer,\r\n                      beta_scheduler,\r\n                      lr_schedulers,\r\n                      stage='training')\r\n\r\n        elif training_step == \"P4\":\r\n            with torch.no_grad():\r\n                train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer,\r\n                          beta_scheduler,\r\n                          lr_schedulers,\r\n                          stage='training')\r\n\r\n        elif training_step == \"P7\":\r\n            train_all(args, model, optimizers, finetune_dataset, epoch, training_step, valo_envs_name, writer,\r\n                      beta_scheduler,\r\n                      lr_schedulers,\r\n                      stage='training')\r\n\r\n        with torch.no_grad():\r\n            if training_step == \"P6\":\r\n                validate_ade(args, model, train_dataset, epoch, training_step, writer, stage='training')\r\n                validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage='validation')\r\n                metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')\r\n\r\n            elif training_step == \"P7\":\r\n                metric = validate_ade(args, model, finetune_dataset, epoch, training_step, writer, stage='validation o')\r\n\r\n        if training_step in [\"P6\", \"P7\"]:\r\n            if metric < min_metric:\r\n                min_metric = metric\r\n                save_all_model(args, model, model_name, optimizers, metric, epoch, training_step)\r\n                print(f'\\n{\"_\" * 150}\\n')\r\n        else:\r\n            save_all_model(args, model, model_name, optimizers, metric, epoch, training_step)\r\n\r\n    writer.close()\r\n\r\n\r\ndef train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer, beta_scheduler,\r\n              lr_schedulers,\r\n              stage,\r\n              update=True):\r\n    \"\"\"\r\n    Train the entire model for an epoch\r\n\r\n    Args:\r\n        - model (CausalMotionModel): model to train\r\n        - optimizers: inv and style optimizers to use\r\n        - datasets: train dataset (and pretrain dataset if finetuning)\r\n        - stage (str): either 'validation' or 'training': says on which dataset we calculate the loss (and only backprop on 'training')\r\n    \"\"\"\r\n    model.train()\r\n    logging.info(f\"- Computing loss ({stage})\")\r\n\r\n    assert (stage in ['training', 'validation'])\r\n    if (args.contrastive and args.batch_method == \"hom\"):\r\n        raise ValueError(\"Contrastive loss must be implemented with heterogeneous batches!\")\r\n\r\n    contrastive_loss = SupConLoss()\r\n    s_vec = []\r\n    clusters = []\r\n    if args.batch_method == \"het\" or args.batch_method == \"alt\":\r\n        train_iter = [iter(train_loader) for train_loader in train_dataset['loaders']]\r\n        loss_meter = AverageMeter(\"Loss\", \":.4f\")\r\n        e1_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        e2_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        e3_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        e4_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        p_loss_meter = AverageMeter(\"Prediction Loss\", \":.4f\")\r\n        c_loss_meter = AverageMeter(\"Contrastive Loss\", \":.4f\")\r\n        progress = ProgressMeter(train_dataset['num_batches'], [loss_meter], prefix=\"\")\r\n        for batch_idx in range(train_dataset['num_batches']):\r\n            # compute loss (which depends on the training step)\r\n            batch_loss = []\r\n            env_embeddings, label_embeddings = [], []  # to store the low dim feat space for contrastive style loss, and their labels\r\n            ped_tot = torch.zeros(1).cuda()\r\n            # COMPUTE LOSS ON EACH OF THE ENVIRONMENTS\r\n            for train_idx, (env_iter, env_name) in enumerate(zip(train_iter, train_dataset['names'])):\r\n                try:\r\n                    batch = next(env_iter)\r\n                except StopIteration:\r\n                    raise RuntimeError()\r\n\r\n                batch = [tensor.cuda() for tensor in batch]\r\n\r\n                if args.dataset_name in ('eth', 'hotel', 'univ', 'zara1', 'zara2'):\r\n                    (\r\n                        obs_traj,\r\n                        fut_traj,\r\n                        obs_traj_rel,\r\n                        fut_traj_rel,\r\n                        seq_start_end,\r\n                    ) = batch\r\n                elif 'synthetic' in args.dataset_name or args.dataset_name in ['synthetic', 'v2', 'v2full', 'v4']:\r\n                    (\r\n                        obs_traj,\r\n                        _,\r\n                        obs_traj_rel,\r\n                        fut_traj_rel,\r\n                        seq_start_end,\r\n                        _,\r\n                        _\r\n                    ) = batch\r\n                else:\r\n                    raise ValueError('Unrecognized dataset name \"%s\"' % args.dataset_name)\r\n\r\n                # reset gradients\r\n                for opt in optimizers.values():\r\n                    opt.zero_grad()\r\n                ped_tot += fut_traj_rel.shape[1]\r\n\r\n                if training_step in [\"P1\", \"P2\"]:\r\n                    l2_loss_rel1 = []\r\n                    l2_loss_rel2 = []\r\n                    pred_past_rel1, pred_past_rel2 = model(batch, training_step)\r\n\r\n                    l2_loss_rel1.append(l2_loss(pred_past_rel1, obs_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel2.append(l2_loss(pred_past_rel2, obs_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel1 = torch.stack(l2_loss_rel1, dim=1)\r\n                    l2_loss_rel2 = torch.stack(l2_loss_rel2, dim=1)\r\n                    batch_loss.append(erm_loss(l2_loss_rel1, seq_start_end) + erm_loss(l2_loss_rel2, seq_start_end))\r\n\r\n                elif training_step == \"P3\":\r\n                    l2_loss_rel1 = []\r\n                    l2_loss_rel2 = []\r\n                    pred_past_rel, pred_fut_rel = model(batch, training_step)\r\n\r\n                    l2_loss_rel1.append(l2_loss(pred_past_rel, obs_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel2.append(l2_loss(pred_fut_rel, fut_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel1 = torch.stack(l2_loss_rel1, dim=1)\r\n                    l2_loss_rel2 = torch.stack(l2_loss_rel2, dim=1)\r\n                    batch_loss.append(erm_loss(l2_loss_rel1, seq_start_end) + erm_loss(l2_loss_rel2, seq_start_end))\r\n\r\n                elif training_step == \"P4\":\r\n                    s = model(batch, training_step)\r\n                    s_vec += [s]\r\n                    clusters += [torch.tensor(train_idx).repeat(s.shape[0])]\r\n                    continue\r\n\r\n                elif training_step == \"P5\":\r\n                    l2_loss_elbo = []\r\n                    E = model(batch, training_step)\r\n                    l2_loss_elbo.append(E)\r\n                    l2_loss_elbo = torch.stack(l2_loss_elbo, dim=1)\r\n                    elbo_loss = erm_loss(l2_loss_elbo, seq_start_end)\r\n                    batch_loss.append(- elbo_loss)\r\n\r\n                else:\r\n                    l2_loss_rel = []\r\n                    l2_loss_elbo1 = []\r\n                    l2_loss_elbo2 = []\r\n                    l2_loss_elbo3 = []\r\n\r\n                    log_py, E1, E2, E3, low_dim = model(batch, training_step)\r\n\r\n                    if args.decoupled_loss:\r\n                        for i in range(args.best_k):\r\n                            log_qy = - l2_loss(log_py[:, i, :, :], fut_traj_rel, mode=\"raw\")\r\n                            l2_loss_rel.append(log_qy)\r\n\r\n                        l2_loss_elbo1.append(E1)\r\n                        l2_loss_elbo2.append(E2)\r\n                        l2_loss_elbo3.append(E3)\r\n\r\n                    else:\r\n\r\n                        log_qy = torch.log(torch.exp(log_py).mean(0))\r\n                        l2_loss_rel.append(log_qy)\r\n                        l2_loss_elbo1.append(E1 / torch.exp(log_qy))\r\n                        l2_loss_elbo2.append(E2 / torch.exp(log_qy))\r\n                        l2_loss_elbo3.append(E3 / torch.exp(log_qy))\r\n\r\n                    l2_loss_rel = torch.stack(l2_loss_rel, dim=1)\r\n                    predict_loss = erm_loss(l2_loss_rel, seq_start_end)\r\n\r\n                    l2_loss_elbo1 = torch.stack(l2_loss_elbo1, dim=1)\r\n                    l2_loss_elbo2 = torch.stack(l2_loss_elbo2, dim=1)\r\n                    l2_loss_elbo3 = torch.stack(l2_loss_elbo3, dim=1)\r\n                    elbo_loss1 = erm_loss(l2_loss_elbo1, seq_start_end)\r\n                    elbo_loss2 = erm_loss(l2_loss_elbo2, seq_start_end)\r\n                    elbo_loss3 = erm_loss(l2_loss_elbo3, seq_start_end)\r\n\r\n                    # loss of style encoder only\r\n                    env_embeddings.append(low_dim)\r\n                    label_embeddings.append(torch.tensor(train_dataset['labels'][env_name]))\r\n\r\n                    batch_loss.append((- predict_loss) + (- elbo_loss1) + (- elbo_loss2) + (- elbo_loss3))\r\n\r\n                    e1_loss_meter.update(elbo_loss1.item(), fut_traj_rel.shape[1])\r\n                    e2_loss_meter.update(elbo_loss2.item(), fut_traj_rel.shape[1])\r\n                    e3_loss_meter.update(elbo_loss3.item(), fut_traj_rel.shape[1])\r\n                    p_loss_meter.update(predict_loss.item(), fut_traj_rel.shape[1])\r\n\r\n            if training_step != \"P4\":\r\n                loss = torch.zeros(()).cuda()\r\n                loss += torch.stack(batch_loss).sum()\r\n\r\n                if training_step == \"P6\" and args.contrastive:\r\n                    c_loss_list = []\r\n                    for i in range(args.num_samples):\r\n                        c_loss_list += [\r\n                            contrastive_loss(torch.stack(env_embeddings)[:, i, :, :], torch.stack(label_embeddings))]\r\n\r\n                    c_loss = torch.min(torch.stack(c_loss_list))\r\n                    loss += c_loss\r\n                    c_loss_meter.update(c_loss.item(), ped_tot.item())\r\n\r\n                # backpropagate if needed\r\n                if stage == 'training' and update:\r\n                    loss.backward()\r\n\r\n                    lr_scheduler_optims = lr_schedulers[training_step]\r\n                    # choose which optimizer to use depending on the training step\r\n                    if training_step in ['P1', 'P2', 'P3', 'P6']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['inv'].step()\r\n                        optimizers['inv'].step()\r\n\r\n                    if training_step in ['P3', 'P6', 'P7']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['future_decoder'].step()\r\n                        optimizers['future_decoder'].step()\r\n\r\n                    if training_step in ['P3', 'P6']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['past_decoder'].step()\r\n                        optimizers['past_decoder'].step()\r\n\r\n                    if training_step in ['P1', 'P2', 'P3', 'P5', 'P6', 'P7']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['var'].step()\r\n                        optimizers['var'].step()\r\n\r\n                    if training_step in ['P6', 'P7']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['par'].step()\r\n                        optimizers['par'].step()\r\n\r\n                loss_meter.update(loss.item(), ped_tot.item())\r\n                progress.display(batch_idx + 1)\r\n\r\n        # train GMM\r\n        if training_step == \"P4\":\r\n            S = torch.cat(s_vec, 0)\r\n            S = torch.nn.functional.normalize(S, dim=1).detach().cpu().numpy()\r\n            Y = torch.cat(clusters, 0).numpy()\r\n            pre = model.gmm.fit_predict(S)\r\n            print('Acc={:.4f}%'.format(cluster_acc(pre, Y)[0] * 100))\r\n\r\n        if training_step in [\"P1\", \"P2\"]:\r\n            writer.add_scalar(f\"STGAT_loss/{stage}\", loss_meter.avg, epoch)\r\n\r\n        elif training_step == \"P3\":\r\n            writer.add_scalar(f\"reconstruction_loss/{stage}\", loss_meter.avg, epoch)\r\n\r\n        elif training_step in [\"P5\"]:\r\n            writer.add_scalar(f\"variational_loss/{stage}\", loss_meter.avg, epoch)\r\n\r\n        elif training_step in [\"P6\"]:\r\n            writer.add_scalar(f\"variational_loss/{stage}\", loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"reconstruction_loss/{stage}\", e1_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"contrastive_loss/{stage}\", c_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"sreg/{stage}\", e2_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"zreg/{stage}\", e3_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"pred_loss/{stage}\", p_loss_meter.avg, epoch)\r\n\r\n    else:\r\n\r\n        # Homogenous batches\r\n        total_loss_meter = AverageMeter(\"Total Loss\", \":.4f\")\r\n        e1_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        e2_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        e3_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\r\n        p_loss_meter = AverageMeter(\"Prediction Loss\", \":.4f\")\r\n        for train_idx, train_loader in enumerate(train_dataset['loaders']):\r\n            loss_meter = AverageMeter(\"Loss\", \":.4f\")\r\n            progress = ProgressMeter(len(train_loader), [loss_meter],\r\n                                     prefix=\"Dataset: {:<20}\".format(train_envs_name[train_idx]))\r\n            for batch_idx, batch in enumerate(train_loader):\r\n                batch = [tensor.cuda() for tensor in batch]\r\n\r\n                if args.dataset_name in ('eth', 'hotel', 'univ', 'zara1', 'zara2'):\r\n                    (\r\n                        obs_traj,\r\n                        fut_traj,\r\n                        obs_traj_rel,\r\n                        fut_traj_rel,\r\n                        seq_start_end,\r\n                    ) = batch\r\n                elif 'synthetic' in args.dataset_name or args.dataset_name in ['synthetic', 'v2', 'v2full', 'v4']:\r\n                    (\r\n                        obs_traj,\r\n                        _,\r\n                        obs_traj_rel,\r\n                        fut_traj_rel,\r\n                        seq_start_end,\r\n                        _,\r\n                        _\r\n                    ) = batch\r\n                else:\r\n                    raise ValueError('Unrecognized dataset name \"%s\"' % args.dataset_name)\r\n\r\n                # reset gradients\r\n                for opt in optimizers.values():\r\n                    opt.zero_grad()\r\n\r\n                if training_step in [\"P1\", \"P2\"]:\r\n                    l2_loss_rel1 = []\r\n                    l2_loss_rel2 = []\r\n                    pred_past_rel1, pred_past_rel2 = model(batch, training_step)\r\n\r\n                    l2_loss_rel1.append(l2_loss(pred_past_rel1, obs_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel2.append(l2_loss(pred_past_rel2, obs_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel1 = torch.stack(l2_loss_rel1, dim=1)\r\n                    l2_loss_rel2 = torch.stack(l2_loss_rel2, dim=1)\r\n                    loss = erm_loss(l2_loss_rel1, seq_start_end) + erm_loss(l2_loss_rel2, seq_start_end)\r\n\r\n                elif training_step == \"P3\":\r\n                    l2_loss_rel1 = []\r\n                    l2_loss_rel2 = []\r\n                    pred_past_rel, pred_fut_rel = model(batch, training_step)\r\n\r\n                    l2_loss_rel1.append(l2_loss(pred_past_rel, obs_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel2.append(l2_loss(pred_fut_rel, fut_traj_rel, mode=\"raw\"))\r\n                    l2_loss_rel1 = torch.stack(l2_loss_rel1, dim=1)\r\n                    l2_loss_rel2 = torch.stack(l2_loss_rel2, dim=1)\r\n                    loss = erm_loss(l2_loss_rel1, seq_start_end) + erm_loss(l2_loss_rel2, seq_start_end)\r\n\r\n                elif training_step == \"P4\":\r\n                    s = model(batch, training_step)\r\n                    s_vec += [s]\r\n                    clusters += [torch.tensor(train_idx).repeat(s.shape[0])]\r\n                    continue\r\n\r\n                elif training_step == \"P5\":\r\n                    l2_loss_elbo = []\r\n                    E = model(batch, training_step)\r\n                    l2_loss_elbo.append(E)\r\n                    l2_loss_elbo = torch.stack(l2_loss_elbo, dim=1)\r\n                    elbo_loss = erm_loss(l2_loss_elbo, seq_start_end)\r\n                    loss = - elbo_loss\r\n\r\n                else:\r\n                    l2_loss_rel = []\r\n                    l2_loss_elbo1 = []\r\n                    l2_loss_elbo2 = []\r\n                    l2_loss_elbo3 = []\r\n\r\n                    log_py, E1, E2, E3, _ = model(batch, training_step)\r\n                    if args.decoupled_loss:\r\n                        for i in range(args.best_k):\r\n                            log_qy = - l2_loss(log_py[:, i, :, :], fut_traj_rel, mode=\"raw\")\r\n                            l2_loss_rel.append(log_qy)\r\n\r\n                        l2_loss_elbo1.append(E1)\r\n                        l2_loss_elbo2.append(E2)\r\n                        l2_loss_elbo3.append(E3)\r\n\r\n                    else:\r\n\r\n                        log_qy = torch.log(torch.exp(log_py).mean(0))\r\n                        l2_loss_rel.append(log_qy)\r\n                        l2_loss_elbo1.append(E1 / torch.exp(log_qy))\r\n                        l2_loss_elbo2.append(E2 / torch.exp(log_qy))\r\n                        l2_loss_elbo3.append(E3 / torch.exp(log_qy))\r\n\r\n                    l2_loss_rel = torch.stack(l2_loss_rel, dim=1)\r\n                    l2_loss_elbo1 = torch.stack(l2_loss_elbo1, dim=1)\r\n                    l2_loss_elbo2 = torch.stack(l2_loss_elbo2, dim=1)\r\n                    l2_loss_elbo3 = torch.stack(l2_loss_elbo3, dim=1)\r\n                    predict_loss = erm_loss(l2_loss_rel, seq_start_end)\r\n                    elbo_loss1 = erm_loss(l2_loss_elbo1, seq_start_end)\r\n                    elbo_loss2 = erm_loss(l2_loss_elbo2, seq_start_end)\r\n                    elbo_loss3 = erm_loss(l2_loss_elbo3, seq_start_end)\r\n\r\n                    loss = (- predict_loss) + (- elbo_loss1) + (- elbo_loss2) + (- elbo_loss3)\r\n\r\n                    e1_loss_meter.update(elbo_loss1.item(), obs_traj.shape[1])\r\n                    e2_loss_meter.update(elbo_loss2.item(), obs_traj.shape[1])\r\n                    e3_loss_meter.update(elbo_loss3.item(), obs_traj.shape[1])\r\n                    p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])\r\n\r\n                # backpropagate if needed\r\n                if stage == 'training' and update:\r\n                    loss.backward()\r\n\r\n                    lr_scheduler_optims = lr_schedulers[training_step]\r\n                    # choose which optimizer to use depending on the training step\r\n                    if training_step in ['P1', 'P2', 'P3', 'P6']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['inv'].step()\r\n                        optimizers['inv'].step()\r\n\r\n                    if training_step in ['P3', 'P6']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['future_decoder'].step()\r\n                        optimizers['future_decoder'].step()\r\n\r\n                    if training_step in ['P3', 'P6']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['past_decoder'].step()\r\n                        optimizers['past_decoder'].step()\r\n\r\n                    if training_step in ['P1', 'P2', 'P3', 'P5', 'P6', 'P7']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['var'].step()\r\n                        optimizers['var'].step()\r\n\r\n                    if training_step in ['P6', 'P7']:\r\n                        if lr_scheduler_optims is not None:\r\n                            lr_scheduler_optims['par'].step()\r\n                        optimizers['par'].step()\r\n\r\n                total_loss_meter.update(loss.item(), obs_traj.shape[1])\r\n                loss_meter.update(loss.item(), obs_traj.shape[1])\r\n                progress.display(batch_idx + 1)\r\n\r\n        # train GMM\r\n        if training_step == \"P4\":\r\n            S = torch.cat(s_vec, 0)\r\n            S = torch.nn.functional.normalize(S, dim=1).detach().cpu().numpy()\r\n            Y = torch.cat(clusters, 0).numpy()\r\n            pre = model.gmm.fit_predict(S)\r\n            print('Acc={:.4f}%'.format(cluster_acc(pre, Y)[0] * 100))\r\n\r\n        if training_step in [\"P1\", \"P2\"]:\r\n            writer.add_scalar(f\"STGAT_loss/{stage}\", total_loss_meter.avg, epoch)\r\n\r\n        elif training_step == \"P3\":\r\n            writer.add_scalar(f\"reconstruction_loss/{stage}\", total_loss_meter.avg, epoch)\r\n\r\n        elif training_step in [\"P5\"]:\r\n            writer.add_scalar(f\"variational_loss/{stage}\", total_loss_meter.avg, epoch)\r\n\r\n        elif training_step in [\"P6\"]:\r\n            writer.add_scalar(f\"variational_loss/{stage}\", total_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"reconstruction_loss/{stage}\", e1_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"sreg/{stage}\", e2_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"zreg/{stage}\", e3_loss_meter.avg, epoch)\r\n            writer.add_scalar(f\"pred_loss/{stage}\", p_loss_meter.avg, epoch)\r\n\r\n\r\ndef validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage, write=True):\r\n    \"\"\"\r\n    Evaluate the performances on the validation set\r\n\r\n    Args:\r\n        - stage (str): either 'validation' or 'training': says on which dataset the metrics are computed\r\n    \"\"\"\r\n    model.eval()\r\n\r\n    assert (stage in ['training', 'validation', 'validation o'])\r\n\r\n    logging.info(f\"- Computing ADE ({stage})\")\r\n    with torch.no_grad():\r\n        ade = 0\r\n        fde = 0\r\n        total_traj = 0\r\n        for val_idx, (loader, loader_name) in enumerate(zip(valid_dataset['loaders'], valid_dataset['names'])):\r\n            ade_outer, fde_outer = [], []\r\n            total_traj_i = 0\r\n            for batch_idx, batch in enumerate(loader):\r\n                batch = [tensor.cuda() for tensor in batch]\r\n                if args.dataset_name in ('eth', 'hotel', 'univ', 'zara1', 'zara2'):\r\n                    (\r\n                        obs_traj,\r\n                        fut_traj,\r\n                        obs_traj_rel,\r\n                        fut_traj_rel,\r\n                        seq_start_end,\r\n                    ) = batch\r\n                elif 'synthetic' in args.dataset_name or args.dataset_name in ['synthetic', 'v2', 'v2full', 'v4']:\r\n                    (\r\n                        obs_traj,\r\n                        fut_traj,\r\n                        obs_traj_rel,\r\n                        fut_traj_rel,\r\n                        seq_start_end,\r\n                        _,\r\n                        _\r\n                    ) = batch\r\n                else:\r\n                    raise ValueError('Unrecognized dataset name \"%s\"' % args.dataset_name)\r\n\r\n                ade_list, fde_list = [], []\r\n                total_traj_i += fut_traj.size(1)\r\n\r\n                for k in range(args.best_k):\r\n                    if stage == \"validation o\":\r\n                        pred_fut_traj_rel = model(batch, training_step)\r\n                    else:\r\n                        pred_fut_traj_rel = model(batch, training_step, env_idx=val_idx)\r\n\r\n                    # from relative path to absolute path\r\n                    pred_fut_traj = relative_to_abs(pred_fut_traj_rel, obs_traj[-1, :, :2])\r\n\r\n                    # compute ADE and FDE metrics\r\n                    ade_, fde_ = cal_ade_fde(fut_traj[:, :, :2], pred_fut_traj)\r\n\r\n                    ade_list.append(ade_)\r\n                    fde_list.append(fde_)\r\n\r\n                ade_sum_batch = evaluate_helper(ade_list, seq_start_end)\r\n                fde_sum_batch = evaluate_helper(fde_list, seq_start_end)\r\n                ade_outer.append(ade_sum_batch)\r\n                fde_outer.append(fde_sum_batch)\r\n\r\n            ade_sum = sum(ade_outer)\r\n            fde_sum = sum(fde_outer)\r\n            ade += ade_sum\r\n            fde += fde_sum\r\n            total_traj += total_traj_i\r\n\r\n            logging.info(f'\\t\\t ADE on {loader_name:<25} dataset:\\t {ade_sum / (total_traj_i * args.fut_len)}')\r\n\r\n        ade = ade / (total_traj * args.fut_len)\r\n        fde = fde / total_traj\r\n\r\n    logging.info(f\"Average {stage}:\\tADE  {ade:.4f}\\tFDE  {fde:.4f}\")\r\n    repoch = epoch\r\n    if write:\r\n        writer.add_scalar(f\"ade/{stage}\", ade, repoch)\r\n        writer.add_scalar(f\"fde/{stage}\", fde, repoch)\r\n\r\n    ## SAVE VISUALIZATIONS\r\n    # if epoch % 1 == 0 and stage == 'validation':\r\n    # if (stage == 'validation' and rp != None and epoch % 3 == 0) or force and write:\r\n\r\n    #     obs = [b[0] for b in rp]\r\n    #     fut = [b[1] for b in rp]\r\n    #     pred = [relative_to_abs(model(b, ts), b[0][-1, :, :2]) for b in rp]\r\n    #     res = [[obs[i], fut[i], pred[i]] for i in range(len(rp))]\r\n    #     fig, array = draw_image(res)\r\n    #     fig.savefig(f'images/visu/pred{epoch}.png')\r\n    #     writer.add_image(\"Some paths\", array, epoch)\r\n\r\n    return ade\r\n\r\n\r\ndef cal_ade_fde(fut_traj, pred_fut_traj):\r\n    \"\"\"\r\n    Compute the ADE and FDE\r\n    \"\"\"\r\n    ade = displacement_error(pred_fut_traj, fut_traj, mode=\"raw\")\r\n    fde = final_displacement_error(pred_fut_traj[-1], fut_traj[-1], mode=\"raw\")\r\n    return ade, fde\r\n\r\n\r\ndef cluster_acc(Y_pred, Y):\r\n    assert Y_pred.size == Y.size\r\n\r\n    D = max(Y_pred.max(), Y.max()) + 1\r\n    w = np.zeros((D, D), dtype=np.int64)\r\n    for i in range(Y_pred.size):\r\n        w[Y_pred[i], Y[i]] += 1\r\n    row_ind, col_ind = linear_assignment(w.max() - w)\r\n\r\n    return sum(w[row_ind, col_ind]) * 1.0 / Y_pred.size, w\r\n\r\n\r\ndef plot_grad_flow(named_parameters):\r\n    \"\"\"\r\n    Plots the gradients flowing through different layers in the net during training.\r\n    Can be used for checking for possible gradient vanishing / exploding problems.\r\n\r\n    Usage: Plug this function in Trainer class after loss.backwards() as\r\n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow\r\n    \"\"\"\r\n    ave_grads = []\r\n    max_grads = []\r\n    layers = []\r\n    for n, p in named_parameters:\r\n        if (p.requires_grad) and (\"bias\" not in n):\r\n            if p.grad is not None:\r\n                layers.append(n)\r\n                ave_grads.append(p.grad.abs().mean())\r\n                max_grads.append(p.grad.abs().max())\r\n\r\n    print(\"done\")\r\n    # plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\r\n    # plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\r\n    # plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"k\")\r\n    # plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\r\n    # plt.xlim(left=0, right=len(ave_grads))\r\n    # plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\r\n    # plt.xlabel(\"Layers\")\r\n    # plt.ylabel(\"average gradient\")\r\n    # plt.title(\"Gradient flow\")\r\n    # plt.grid(True)\r\n    # plt.legend([Line2D([0], [0], color=\"c\", lw=4),\r\n    #             Line2D([0], [0], color=\"b\", lw=4),\r\n    #             Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\r\n    # plt.show()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    print('Using GPU: ' + str(torch.cuda.is_available()))\r\n    input_args = get_training_parser().parse_args()\r\n    print('Arguments for training: ', input_args)\r\n    set_logger(os.path.join(input_args.log_dir, \"train.log\"))\r\n    main(input_args)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	(revision e49d5ca3563b1639c7c7fb672f11981586cd98e0)
+++ b/train.py	(date 1673256851734)
@@ -185,7 +185,7 @@
     for epoch in range(args.start_epoch, sum(args.num_epochs) + 1):
 
         training_step = get_training_step(epoch)
-        if training_step in ["P1", "P2", "P5"]:
+        if training_step in ["P1", "P2", "P5", "P6"]:
             continue
         logging.info(f"\n===> EPOCH: {epoch} ({training_step})")
 
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"8147b731-440c-4981-a21f-e4c8b1f0cd9a\" name=\"Changes\" comment=\"Experiments\">\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/misc.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/misc.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_11_17_2022_12_09_PM__Changes_.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_11_17_2022_12_09_PM__Changes_.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18,_4_44_p_m__[Changes]1/shelved.patch\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18__4_44_p_m___Changes_.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18__4_44_p_m___Changes_.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18__4_44_p_m___Changes_1.xml\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_11_9_2022_10_12_PM__Changes_.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_11_9_2022_10_12_PM__Changes_.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/Tensorboard.ipynb\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/Tensorboard.ipynb\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/log/train.log\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/log/train.log\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/models.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_1188.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_401.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_402.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_403.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_404.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_405.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_406.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_407.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_408.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P4/CRMF_epoch_409.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P5/CRMF_epoch_1201.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E1/P5/CRMF_epoch_1202.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E3_joint/P4/CRMF_epoch_1200.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E3_joint/P5/CRMF_epoch_1301.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E3_joint/P6/CRMF_epoch_1497.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E3_joint_64dimension/P4/CRMF_epoch_1200.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E3_joint_64dimension/P5/CRMF_epoch_1400.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E3_joint_64dimension/P6/CRMF_epoch_1547.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E4_q/P3/CRMF_epoch_850.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/models/E4_q/P4/CRMF_epoch_1050.pth.tar\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/parser_file.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/parser_file.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-19 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-27 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-33 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-39 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-08 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-13 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-17 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-25 TensorBoard.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E2_complex_beta_withoutz/events.out.tfevents.1668820224.zahra-ThinkPad-P15-Gen-2i.502147.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 800, 100, 50)_shuffle_true_seed_72/events.out.tfevents.1668906133.zahra-ThinkPad-P15-Gen-2i.775334.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 800, 100, 50)_shuffle_true_seed_72/events.out.tfevents.1668927818.zahra-ThinkPad-P15-Gen-2i.2542816.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/ade.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/adeo.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/elbo.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/hotel.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/loss.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/pred.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/univ.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/zara1.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint/zara2.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/New Text Document.txt\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/ade.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/adeo.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/e_loss.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/elbo.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/hotel.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/pred.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/univ.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/zara1.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E3_joint_64_dimension/zara2.png\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/E4_q/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 200, 100)_shuffle_true_seed_72/events.out.tfevents.1669430022.gra1184.184179.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/Ebeta_fixed_learning rate/events.out.tfevents.1668801034.zahra-ThinkPad-P15-Gen-2i.670101.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/Eonecycle_200/events.out.tfevents.1668747604.zahra-ThinkPad-P15-Gen-2i.3924938.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/Eonecycle_300/events.out.tfevents.1668791888.zahra-ThinkPad-P15-Gen-2i.5816.0\" beforeDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/runs/~$Notes.docx\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/runs/~$Notes.docx\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\r\n      <map>\r\n        <entry key=\"$PROJECT_DIR$\" value=\"DT\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"GithubProjectSettings\">\r\n    <option name=\"branchProtectionPatterns\">\r\n      <list>\r\n        <option value=\"master\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProjectId\" id=\"2HIAGvCj0b6BkQ5ayMuWDeDrSYY\" />\r\n  <component name=\"ProjectLevelVcsManager\" settingsEditedManually=\"true\" />\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;RunOnceActivity.OpenProjectViewOnStart&quot;: &quot;true&quot;,\r\n    &quot;RunOnceActivity.ShowReadmeOnStart&quot;: &quot;true&quot;,\r\n    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;C:/UWaterloo/PhD_thesis/motion_forecasting/codes/causalmotion-main/style/train.py&quot;,\r\n    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,\r\n    &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;\r\n  }\r\n}</component>\r\n  <component name=\"RunManager\" selected=\"Python.evaluate_model\">\r\n    <configuration name=\"distributions\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"CRMF\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/distributions.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"evaluate_model\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"CRMF\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/evaluate_model.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"invariance\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"CRMF\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/invariance.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"results\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"CRMF\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/results.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"train\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"CRMF\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/train.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <recent_temporary>\r\n      <list>\r\n        <item itemvalue=\"Python.evaluate_model\" />\r\n        <item itemvalue=\"Python.train\" />\r\n        <item itemvalue=\"Python.results\" />\r\n        <item itemvalue=\"Python.invariance\" />\r\n        <item itemvalue=\"Python.distributions\" />\r\n      </list>\r\n    </recent_temporary>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"application-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"8147b731-440c-4981-a21f-e4c8b1f0cd9a\" name=\"Changes\" comment=\"\" />\r\n      <created>1667962131599</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1667962131599</updated>\r\n      <workItem from=\"1667962133126\" duration=\"3158000\" />\r\n      <workItem from=\"1667965423442\" duration=\"14042000\" />\r\n      <workItem from=\"1668103527142\" duration=\"9238000\" />\r\n      <workItem from=\"1668113554483\" duration=\"9483000\" />\r\n      <workItem from=\"1668188593721\" duration=\"24064000\" />\r\n      <workItem from=\"1668280011795\" duration=\"20838000\" />\r\n      <workItem from=\"1668361022894\" duration=\"18561000\" />\r\n      <workItem from=\"1668385121155\" duration=\"9013000\" />\r\n      <workItem from=\"1668443724873\" duration=\"34005000\" />\r\n      <workItem from=\"1668544348636\" duration=\"11075000\" />\r\n      <workItem from=\"1668887765193\" duration=\"21673000\" />\r\n      <workItem from=\"1668966972913\" duration=\"2639000\" />\r\n      <workItem from=\"1669137246838\" duration=\"7027000\" />\r\n      <workItem from=\"1672294286229\" duration=\"7812000\" />\r\n      <workItem from=\"1672383569433\" duration=\"8892000\" />\r\n      <workItem from=\"1672473343807\" duration=\"4883000\" />\r\n    </task>\r\n    <task id=\"LOCAL-00001\" summary=\"Finetune completed\">\r\n      <created>1667968720588</created>\r\n      <option name=\"number\" value=\"00001\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00001\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1667968720588</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00002\" summary=\"Finetune completed\">\r\n      <created>1668103730039</created>\r\n      <option name=\"number\" value=\"00002\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00002\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668103730039</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00003\" summary=\"Finetune completed\">\r\n      <created>1668104402694</created>\r\n      <option name=\"number\" value=\"00003\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00003\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668104402694</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00004\" summary=\"Finetune completed\">\r\n      <created>1668104584281</created>\r\n      <option name=\"number\" value=\"00004\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00004\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668104584281</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00005\" summary=\"Finetune completed\">\r\n      <created>1668113946515</created>\r\n      <option name=\"number\" value=\"00005\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00005\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668113946515</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00006\" summary=\"Finetune completed\">\r\n      <created>1668115250936</created>\r\n      <option name=\"number\" value=\"00006\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00006\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668115250936</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00007\" summary=\"Finetune completed\">\r\n      <created>1668219013728</created>\r\n      <option name=\"number\" value=\"00007\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00007\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668219013728</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00008\" summary=\"Finetune completed\">\r\n      <created>1668220779257</created>\r\n      <option name=\"number\" value=\"00008\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00008\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668220779257</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00009\" summary=\"Finetune completed\">\r\n      <created>1668221775701</created>\r\n      <option name=\"number\" value=\"00009\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00009\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668221775701</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00010\" summary=\"Finetune completed\">\r\n      <created>1668224494801</created>\r\n      <option name=\"number\" value=\"00010\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00010\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668224494801</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00011\" summary=\"Finetune completed\">\r\n      <created>1668226094285</created>\r\n      <option name=\"number\" value=\"00011\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00011\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668226094285</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00012\" summary=\"Finetune completed\">\r\n      <created>1668227945871</created>\r\n      <option name=\"number\" value=\"00012\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00012\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668227945871</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00013\" summary=\"Finetune completed\">\r\n      <created>1668228170979</created>\r\n      <option name=\"number\" value=\"00013\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00013\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668228170979</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00014\" summary=\"Finetune completed\">\r\n      <created>1668280159093</created>\r\n      <option name=\"number\" value=\"00014\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00014\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668280159093</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00015\" summary=\"Added RNVP prior\">\r\n      <created>1668288288821</created>\r\n      <option name=\"number\" value=\"00015\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00015\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668288288821</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00016\" summary=\"Added RNVP prior\">\r\n      <created>1668297804098</created>\r\n      <option name=\"number\" value=\"00016\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00016\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668297804098</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00017\" summary=\"Added RNVP prior\">\r\n      <created>1668298020814</created>\r\n      <option name=\"number\" value=\"00017\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00017\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668298020814</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00018\" summary=\"Added RNVP prior\">\r\n      <created>1668300013429</created>\r\n      <option name=\"number\" value=\"00018\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00018\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668300013429</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00019\" summary=\"Added RNVP prior\">\r\n      <created>1668302208211</created>\r\n      <option name=\"number\" value=\"00019\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00019\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668302208212</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00020\" summary=\"Added RNVP prior\">\r\n      <created>1668365128077</created>\r\n      <option name=\"number\" value=\"00020\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00020\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668365128077</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00021\" summary=\"Added RNVP prior\">\r\n      <created>1668370461760</created>\r\n      <option name=\"number\" value=\"00021\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00021\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668370461760</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00022\" summary=\"Different training\">\r\n      <created>1668382815610</created>\r\n      <option name=\"number\" value=\"00022\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00022\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668382815610</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00023\" summary=\"Different training\">\r\n      <created>1668385484864</created>\r\n      <option name=\"number\" value=\"00023\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00023\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668385484864</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00024\" summary=\"Corrected theta convergence\">\r\n      <created>1668390475601</created>\r\n      <option name=\"number\" value=\"00024\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00024\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668390475601</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00025\" summary=\"Corrected theta convergence\">\r\n      <created>1668390596963</created>\r\n      <option name=\"number\" value=\"00025\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00025\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668390596963</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00026\" summary=\"Corrected theta convergence\">\r\n      <created>1668390811340</created>\r\n      <option name=\"number\" value=\"00026\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00026\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668390811340</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00027\" summary=\"Corrected theta convergence\">\r\n      <created>1668400370348</created>\r\n      <option name=\"number\" value=\"00027\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00027\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668400370348</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00028\" summary=\"Corrected theta convergence\">\r\n      <created>1668446498251</created>\r\n      <option name=\"number\" value=\"00028\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00028\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668446498251</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00029\" summary=\"Corrected theta convergence\">\r\n      <created>1668455384327</created>\r\n      <option name=\"number\" value=\"00029\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00029\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668455384327</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00030\" summary=\"Corrected theta convergence\">\r\n      <created>1668459008586</created>\r\n      <option name=\"number\" value=\"00030\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00030\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668459008586</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00031\" summary=\"Corrected theta convergence\">\r\n      <created>1668472315831</created>\r\n      <option name=\"number\" value=\"00031\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00031\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668472315831</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00032\" summary=\"Corrected theta convergence\">\r\n      <created>1668479484839</created>\r\n      <option name=\"number\" value=\"00032\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00032\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668479484839</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00033\" summary=\"Corrected theta convergence\">\r\n      <created>1668808110153</created>\r\n      <option name=\"number\" value=\"00033\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00033\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668808110153</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00034\" summary=\"Corrected theta convergence\">\r\n      <created>1668808830559</created>\r\n      <option name=\"number\" value=\"00034\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00034\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668808830559</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00035\" summary=\"Corrected theta convergence\">\r\n      <created>1668813059061</created>\r\n      <option name=\"number\" value=\"00035\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00035\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668813059061</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00036\" summary=\"added lr_schedulers\">\r\n      <created>1668813067764</created>\r\n      <option name=\"number\" value=\"00036\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00036\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668813067764</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00037\" summary=\"added lr_schedulers\">\r\n      <created>1668820179803</created>\r\n      <option name=\"number\" value=\"00037\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00037\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668820179803</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00038\" summary=\"joint training\">\r\n      <created>1668968494846</created>\r\n      <option name=\"number\" value=\"00038\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00038\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1668968494846</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00039\" summary=\"Updated model\">\r\n      <created>1669086543150</created>\r\n      <option name=\"number\" value=\"00039\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00039\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1669086543150</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00040\" summary=\"Deleted P3 and removed flow priors\">\r\n      <created>1669342712821</created>\r\n      <option name=\"number\" value=\"00040\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00040\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1669342712821</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00041\" summary=\"Added q model\">\r\n      <created>1669425714225</created>\r\n      <option name=\"number\" value=\"00041\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00041\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1669425714225</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00042\" summary=\"Added q model\">\r\n      <created>1669428342843</created>\r\n      <option name=\"number\" value=\"00042\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00042\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1669428342843</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00043\" summary=\"lr_scheduler in utils.py\">\r\n      <created>1669431333997</created>\r\n      <option name=\"number\" value=\"00043\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00043\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1669431333997</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00044\" summary=\"E_q4\">\r\n      <created>1669507555544</created>\r\n      <option name=\"number\" value=\"00044\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00044\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1669507555544</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00045\" summary=\"Added contrastive learning codes\">\r\n      <created>1672436612212</created>\r\n      <option name=\"number\" value=\"00045\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00045\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1672436612212</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00046\" summary=\"Experiments\">\r\n      <created>1672436746621</created>\r\n      <option name=\"number\" value=\"00046\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00046\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1672436746621</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00047\" summary=\"Experiments\">\r\n      <created>1672474634274</created>\r\n      <option name=\"number\" value=\"00047\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00047\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1672474634274</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00048\" summary=\"Experiments\">\r\n      <created>1672474643796</created>\r\n      <option name=\"number\" value=\"00048\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00048\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1672474643796</updated>\r\n    </task>\r\n    <option name=\"localTasksCounter\" value=\"49\" />\r\n    <servers />\r\n  </component>\r\n  <component name=\"TypeScriptGeneratedFilesManager\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State />\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <MESSAGE value=\"Finetune completed\" />\r\n    <MESSAGE value=\"Added RNVP prior\" />\r\n    <MESSAGE value=\"Different training\" />\r\n    <MESSAGE value=\"Corrected theta convergence\" />\r\n    <MESSAGE value=\"added lr_schedulers\" />\r\n    <MESSAGE value=\"joint training\" />\r\n    <MESSAGE value=\"Updated model\" />\r\n    <MESSAGE value=\"Deleted P3 and removed flow priors\" />\r\n    <MESSAGE value=\"Added q model\" />\r\n    <MESSAGE value=\"lr_scheduler in utils.py\" />\r\n    <MESSAGE value=\"E_q4\" />\r\n    <MESSAGE value=\"Added contrastive learning codes\" />\r\n    <MESSAGE value=\"Experiments\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"Experiments\" />\r\n  </component>\r\n  <component name=\"XDebuggerManager\">\r\n    <breakpoint-manager>\r\n      <breakpoints>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/test.py</url>\r\n          <line>5</line>\r\n          <option name=\"timeStamp\" value=\"29\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/results.py</url>\r\n          <line>18</line>\r\n          <option name=\"timeStamp\" value=\"42\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/invariance.py</url>\r\n          <line>93</line>\r\n          <option name=\"timeStamp\" value=\"43\" />\r\n        </line-breakpoint>\r\n      </breakpoints>\r\n    </breakpoint-manager>\r\n    <watches-manager>\r\n      <configuration name=\"PythonConfigurationType\">\r\n        <watch expression=\"optimizers['par']\" />\r\n        <watch expression=\"optimizers['par']\" />\r\n      </configuration>\r\n    </watches-manager>\r\n  </component>\r\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\r\n    <SUITE FILE_PATH=\"coverage/CRMF$train.coverage\" NAME=\"train Coverage Results\" MODIFIED=\"1672440543104\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/CRMF$results.coverage\" NAME=\"results Coverage Results\" MODIFIED=\"1668895584912\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/CRMF$evaluate_model.coverage\" NAME=\"evaluate_model Coverage Results\" MODIFIED=\"1672533621727\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/CRMF$a5.coverage\" NAME=\"a5 Coverage Results\" MODIFIED=\"1667872384207\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/CRMF$invariance.coverage\" NAME=\"invariance Coverage Results\" MODIFIED=\"1672012912771\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/CRMF$test.coverage\" NAME=\"test Coverage Results\" MODIFIED=\"1668451807866\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/CRMF$distributions.coverage\" NAME=\"distributions Coverage Results\" MODIFIED=\"1667958311916\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision e49d5ca3563b1639c7c7fb672f11981586cd98e0)
+++ b/.idea/workspace.xml	(date 1673257251126)
@@ -1,73 +1,13 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
   <component name="ChangeListManager">
-    <list default="true" id="8147b731-440c-4981-a21f-e4c8b1f0cd9a" name="Changes" comment="Experiments">
-      <change beforePath="$PROJECT_DIR$/.idea/misc.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_11_17_2022_12_09_PM__Changes_.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_11_17_2022_12_09_PM__Changes_.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18,_4_44_p_m__[Changes]1/shelved.patch" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18__4_44_p_m___Changes_.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18__4_44_p_m___Changes_.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Checkout_at_2022-11-18__4_44_p_m___Changes_1.xml" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_11_9_2022_10_12_PM__Changes_.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/shelf/Uncommitted_changes_before_Update_at_11_9_2022_10_12_PM__Changes_.xml" afterDir="false" />
+    <list default="true" id="8147b731-440c-4981-a21f-e4c8b1f0cd9a" name="Changes" comment="experiments cvpr">
+      <change afterPath="$PROJECT_DIR$/tt.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/Tensorboard.ipynb" beforeDir="false" afterPath="$PROJECT_DIR$/Tensorboard.ipynb" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/log/train.log" beforeDir="false" afterPath="$PROJECT_DIR$/log/train.log" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/models.py" beforeDir="false" afterPath="$PROJECT_DIR$/models.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_1188.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_401.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_402.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_403.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_404.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_405.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_406.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_407.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_408.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P4/CRMF_epoch_409.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P5/CRMF_epoch_1201.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E1/P5/CRMF_epoch_1202.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E3_joint/P4/CRMF_epoch_1200.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E3_joint/P5/CRMF_epoch_1301.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E3_joint/P6/CRMF_epoch_1497.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E3_joint_64dimension/P4/CRMF_epoch_1200.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E3_joint_64dimension/P5/CRMF_epoch_1400.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E3_joint_64dimension/P6/CRMF_epoch_1547.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E4_q/P3/CRMF_epoch_850.pth.tar" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/models/E4_q/P4/CRMF_epoch_1050.pth.tar" beforeDir="false" />
       <change beforePath="$PROJECT_DIR$/parser_file.py" beforeDir="false" afterPath="$PROJECT_DIR$/parser_file.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-19 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-27 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-33 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-07-39 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-08 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-13 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-17 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/Screenshot 2022-11-19 at 17-08-25 TensorBoard.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E2_complex_beta_withoutz/events.out.tfevents.1668820224.zahra-ThinkPad-P15-Gen-2i.502147.0" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 800, 100, 50)_shuffle_true_seed_72/events.out.tfevents.1668906133.zahra-ThinkPad-P15-Gen-2i.775334.0" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 150, 800, 100, 50)_shuffle_true_seed_72/events.out.tfevents.1668927818.zahra-ThinkPad-P15-Gen-2i.2542816.0" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/ade.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/adeo.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/elbo.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/hotel.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/loss.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/pred.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/univ.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/zara1.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint/zara2.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/New Text Document.txt" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/ade.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/adeo.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/e_loss.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/elbo.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/hotel.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/pred.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/univ.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/zara1.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E3_joint_64_dimension/zara2.png" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/E4_q/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 300, 200, 100)_shuffle_true_seed_72/events.out.tfevents.1669430022.gra1184.184179.0" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/Ebeta_fixed_learning rate/events.out.tfevents.1668801034.zahra-ThinkPad-P15-Gen-2i.670101.0" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/Eonecycle_200/events.out.tfevents.1668747604.zahra-ThinkPad-P15-Gen-2i.3924938.0" beforeDir="false" />
-      <change beforePath="$PROJECT_DIR$/runs/Eonecycle_300/events.out.tfevents.1668791888.zahra-ThinkPad-P15-Gen-2i.5816.0" beforeDir="false" />
       <change beforePath="$PROJECT_DIR$/runs/~$Notes.docx" beforeDir="false" afterPath="$PROJECT_DIR$/runs/~$Notes.docx" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/train.py" beforeDir="false" afterPath="$PROJECT_DIR$/train.py" afterDir="false" />
     </list>
     <option name="SHOW_DIALOG" value="false" />
     <option name="HIGHLIGHT_CONFLICTS" value="true" />
@@ -115,8 +55,8 @@
     &quot;settings.editor.selected.configurable&quot;: &quot;com.jetbrains.python.configuration.PyActiveSdkModuleConfigurable&quot;
   }
 }</component>
-  <component name="RunManager" selected="Python.evaluate_model">
-    <configuration name="distributions" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+  <component name="RunManager" selected="Python.train">
+    <configuration name="evaluate_model" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="CRMF" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
@@ -129,7 +69,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/distributions.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/evaluate_model.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -138,7 +78,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="evaluate_model" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="identifiability" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="CRMF" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
@@ -151,7 +91,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/evaluate_model.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/identifiability.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -182,7 +122,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="results" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="train" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="CRMF" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
@@ -195,7 +135,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/results.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/train.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -204,7 +144,7 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="train" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="tt" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="CRMF" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
@@ -217,7 +157,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/train.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/tt.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -228,11 +168,11 @@
     </configuration>
     <recent_temporary>
       <list>
-        <item itemvalue="Python.evaluate_model" />
         <item itemvalue="Python.train" />
-        <item itemvalue="Python.results" />
+        <item itemvalue="Python.identifiability" />
         <item itemvalue="Python.invariance" />
-        <item itemvalue="Python.distributions" />
+        <item itemvalue="Python.evaluate_model" />
+        <item itemvalue="Python.tt" />
       </list>
     </recent_temporary>
   </component>
@@ -260,48 +200,17 @@
       <workItem from="1672294286229" duration="7812000" />
       <workItem from="1672383569433" duration="8892000" />
       <workItem from="1672473343807" duration="4883000" />
-    </task>
-    <task id="LOCAL-00001" summary="Finetune completed">
-      <created>1667968720588</created>
-      <option name="number" value="00001" />
-      <option name="presentableId" value="LOCAL-00001" />
-      <option name="project" value="LOCAL" />
-      <updated>1667968720588</updated>
-    </task>
-    <task id="LOCAL-00002" summary="Finetune completed">
-      <created>1668103730039</created>
-      <option name="number" value="00002" />
-      <option name="presentableId" value="LOCAL-00002" />
-      <option name="project" value="LOCAL" />
-      <updated>1668103730039</updated>
-    </task>
-    <task id="LOCAL-00003" summary="Finetune completed">
-      <created>1668104402694</created>
-      <option name="number" value="00003" />
-      <option name="presentableId" value="LOCAL-00003" />
-      <option name="project" value="LOCAL" />
-      <updated>1668104402694</updated>
-    </task>
-    <task id="LOCAL-00004" summary="Finetune completed">
-      <created>1668104584281</created>
-      <option name="number" value="00004" />
-      <option name="presentableId" value="LOCAL-00004" />
-      <option name="project" value="LOCAL" />
-      <updated>1668104584281</updated>
-    </task>
-    <task id="LOCAL-00005" summary="Finetune completed">
-      <created>1668113946515</created>
-      <option name="number" value="00005" />
-      <option name="presentableId" value="LOCAL-00005" />
-      <option name="project" value="LOCAL" />
-      <updated>1668113946515</updated>
-    </task>
-    <task id="LOCAL-00006" summary="Finetune completed">
-      <created>1668115250936</created>
-      <option name="number" value="00006" />
-      <option name="presentableId" value="LOCAL-00006" />
-      <option name="project" value="LOCAL" />
-      <updated>1668115250936</updated>
+      <workItem from="1672564287482" duration="2009000" />
+      <workItem from="1672599652397" duration="4829000" />
+      <workItem from="1672708787974" duration="1467000" />
+      <workItem from="1672779888329" duration="2108000" />
+      <workItem from="1672870070877" duration="10688000" />
+      <workItem from="1672908835221" duration="3000" />
+      <workItem from="1672908984313" duration="16974000" />
+      <workItem from="1672977156771" duration="776000" />
+      <workItem from="1673037068778" duration="6530000" />
+      <workItem from="1673122675840" duration="14233000" />
+      <workItem from="1673212208883" duration="3130000" />
     </task>
     <task id="LOCAL-00007" summary="Finetune completed">
       <created>1668219013728</created>
@@ -597,7 +506,56 @@
       <option name="project" value="LOCAL" />
       <updated>1672474643796</updated>
     </task>
-    <option name="localTasksCounter" value="49" />
+    <task id="LOCAL-00049" summary="added variety loss">
+      <created>1672565246369</created>
+      <option name="number" value="00049" />
+      <option name="presentableId" value="LOCAL-00049" />
+      <option name="project" value="LOCAL" />
+      <updated>1672565246369</updated>
+    </task>
+    <task id="LOCAL-00050" summary="added variety loss">
+      <created>1672807899845</created>
+      <option name="number" value="00050" />
+      <option name="presentableId" value="LOCAL-00050" />
+      <option name="project" value="LOCAL" />
+      <updated>1672807899846</updated>
+    </task>
+    <task id="LOCAL-00051" summary="identifiability">
+      <created>1673139447735</created>
+      <option name="number" value="00051" />
+      <option name="presentableId" value="LOCAL-00051" />
+      <option name="project" value="LOCAL" />
+      <updated>1673139447735</updated>
+    </task>
+    <task id="LOCAL-00052" summary="finetune">
+      <created>1673212493433</created>
+      <option name="number" value="00052" />
+      <option name="presentableId" value="LOCAL-00052" />
+      <option name="project" value="LOCAL" />
+      <updated>1673212493433</updated>
+    </task>
+    <task id="LOCAL-00053" summary="finetune">
+      <created>1673212629350</created>
+      <option name="number" value="00053" />
+      <option name="presentableId" value="LOCAL-00053" />
+      <option name="project" value="LOCAL" />
+      <updated>1673212629350</updated>
+    </task>
+    <task id="LOCAL-00054" summary="experiments cvpr">
+      <created>1673212960077</created>
+      <option name="number" value="00054" />
+      <option name="presentableId" value="LOCAL-00054" />
+      <option name="project" value="LOCAL" />
+      <updated>1673212960077</updated>
+    </task>
+    <task id="LOCAL-00055" summary="experiments cvpr">
+      <created>1673215417181</created>
+      <option name="number" value="00055" />
+      <option name="presentableId" value="LOCAL-00055" />
+      <option name="project" value="LOCAL" />
+      <updated>1673215417181</updated>
+    </task>
+    <option name="localTasksCounter" value="56" />
     <servers />
   </component>
   <component name="TypeScriptGeneratedFilesManager">
@@ -628,7 +586,11 @@
     <MESSAGE value="E_q4" />
     <MESSAGE value="Added contrastive learning codes" />
     <MESSAGE value="Experiments" />
-    <option name="LAST_COMMIT_MESSAGE" value="Experiments" />
+    <MESSAGE value="added variety loss" />
+    <MESSAGE value="identifiability" />
+    <MESSAGE value="finetune" />
+    <MESSAGE value="experiments cvpr" />
+    <option name="LAST_COMMIT_MESSAGE" value="experiments cvpr" />
   </component>
   <component name="XDebuggerManager">
     <breakpoint-manager>
@@ -648,7 +610,29 @@
           <line>93</line>
           <option name="timeStamp" value="43" />
         </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/distributions.py</url>
+          <line>46</line>
+          <option name="timeStamp" value="47" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/distributions.py</url>
+          <line>41</line>
+          <option name="timeStamp" value="48" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/identifiability.py</url>
+          <line>107</line>
+          <option name="timeStamp" value="52" />
+        </line-breakpoint>
       </breakpoints>
+      <default-breakpoints>
+        <breakpoint type="python-exception">
+          <properties notifyOnTerminate="true" exception="BaseException">
+            <option name="notifyOnTerminate" value="true" />
+          </properties>
+        </breakpoint>
+      </default-breakpoints>
     </breakpoint-manager>
     <watches-manager>
       <configuration name="PythonConfigurationType">
@@ -658,12 +642,14 @@
     </watches-manager>
   </component>
   <component name="com.intellij.coverage.CoverageDataManagerImpl">
-    <SUITE FILE_PATH="coverage/CRMF$train.coverage" NAME="train Coverage Results" MODIFIED="1672440543104" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/CRMF$identifiability.coverage" NAME="identifiability Coverage Results" MODIFIED="1673140457483" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/CRMF$tt.coverage" NAME="tt Coverage Results" MODIFIED="1672977318357" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/CRMF$train.coverage" NAME="train Coverage Results" MODIFIED="1673257072033" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/CRMF$results.coverage" NAME="results Coverage Results" MODIFIED="1668895584912" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/CRMF$evaluate_model.coverage" NAME="evaluate_model Coverage Results" MODIFIED="1672533621727" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/CRMF$evaluate_model.coverage" NAME="evaluate_model Coverage Results" MODIFIED="1673139231705" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/CRMF$a5.coverage" NAME="a5 Coverage Results" MODIFIED="1667872384207" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/CRMF$invariance.coverage" NAME="invariance Coverage Results" MODIFIED="1672012912771" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/CRMF$invariance.coverage" NAME="invariance Coverage Results" MODIFIED="1673139935419" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/CRMF$test.coverage" NAME="test Coverage Results" MODIFIED="1668451807866" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/CRMF$distributions.coverage" NAME="distributions Coverage Results" MODIFIED="1667958311916" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/CRMF$distributions.coverage" NAME="distributions Coverage Results" MODIFIED="1672971638822" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
   </component>
 </project>
\ No newline at end of file
Index: tt.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tt.py b/tt.py
new file mode 100644
--- /dev/null	(date 1673256783931)
+++ b/tt.py	(date 1673256783931)
@@ -0,0 +1,34 @@
+import torch
+from torch.utils.tensorboard import SummaryWriter
+from tqdm import tqdm
+
+from loader import data_loader
+from parser_file import get_training_parser
+from utils import *
+from models import CRMF
+from scipy.optimize import least_squares
+
+
+def main(args):
+    seed = 2
+    set_seed_globally(seed)
+    print(torch.randn(2))
+
+    for i in range(10):
+        print(i)
+        print(torch.randn(2))
+
+    print(torch.randn(2))
+    print(torch.randn(2))
+    print(torch.randn(2))
+
+
+
+
+
+if __name__ == "__main__":
+    print('Using GPU: ' + str(torch.cuda.is_available()))
+    input_args = get_training_parser().parse_args()
+    print('Arguments for training: ', input_args)
+    set_logger(os.path.join(input_args.log_dir, "train.log"))
+    main(input_args)
Index: log/train.log
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>2023-01-01 04:26:59,718:INFO: Initializing Training Set\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/log/train.log b/log/train.log
--- a/log/train.log	(revision e49d5ca3563b1639c7c7fb672f11981586cd98e0)
+++ b/log/train.log	(date 1673257079234)
@@ -1,1 +1,3 @@
-2023-01-01 04:26:59,718:INFO: Initializing Training Set
+2023-01-09 04:37:59,059:INFO: Initializing Training Set
+2023-01-09 04:37:59,154:INFO: Initializing Validation Set
+2023-01-09 04:37:59,233:INFO: Initializing Validation O Set

Index: utils.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\nimport logging\nimport random\nfrom re import L\nimport torch\nimport numpy as np\nfrom torch.distributions import MultivariateNormal, Gamma, Poisson\n\nNUMBER_PERSONS = 2\nNUMBER_COUPLES = 2\n\nfrom datetime import datetime\n\n\nclass AverageMeter(object):\n    \"\"\"\n    Computes and stores the average and current value of a specific metric\n    \"\"\"\n\n    def __init__(self, name, fmt=\":.4f\"):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        logging.info(\"\\t\".join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = \"{:\" + str(num_digits) + \"d}\"\n        return \"Batch: \" + fmt + \"/\" + fmt.format(num_batches)\n\n\ndef set_logger(log_path):\n    \"\"\"\n    Set the logger to log info in terminal and file `log_path`.\n    \"\"\"\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n\n    if not logger.handlers:\n        # Logging to a file\n        fold = log_path.rsplit('/', 1)[0]\n        if not os.path.exists(fold):\n            os.makedirs(fold)\n        open(log_path, \"w+\")\n        file_handler = logging.FileHandler(log_path)\n        file_handler.setFormatter(\n            logging.Formatter(\"%(asctime)s:%(levelname)s: %(message)s\")\n        )\n        logger.addHandler(file_handler)\n\n        # Logging to console\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n        logger.addHandler(stream_handler)\n\n\ndef relative_to_abs(rel_traj, start_pos):\n    \"\"\"\n    Convert relative coordinates in 'natural' coordinates\n\n    Inputs:\n    - rel_traj: pytorch tensor of shape (seq_len, batch, 2)\n    - start_pos: pytorch tensor of shape (batch, 2)\n    Outputs:\n    - abs_traj: pytorch tensor of shape (seq_len, batch, 2)\n    \"\"\"\n    rel_traj = rel_traj.permute(1, 0, 2)  # --> (batch, seq_len, 2)\n    displacement = torch.cumsum(rel_traj, dim=1)\n    start_pos = torch.unsqueeze(start_pos, dim=1)\n    abs_traj = displacement + start_pos\n    return abs_traj.permute(1, 0, 2)\n\n\ndef get_dset_path(dset_name, dset_type):\n    _dir = os.path.dirname(__file__)\n    return os.path.join(_dir, \"datasets\", dset_name, dset_type)\n\n\ndef get_envs_path(dataset_name, dset_type, filter_envs):\n    dset_path = get_dset_path(dataset_name, dset_type)\n\n    # ETH-UCY Dataset\n    ETH_UCY = ['eth', 'hotel', 'univ', 'zara1', 'zara2']\n    if dataset_name in ETH_UCY:\n        files_name = os.listdir(dset_path)\n        if dset_type == 'test':\n            ETH_UCY = [dataset_name]\n        else:\n            ETH_UCY.remove(dataset_name)\n        envs_names = []\n        for i, env in enumerate(ETH_UCY):\n            envs_names.append([])\n            if env == 'eth':\n                for file_name in files_name:\n                    if 'biwi_eth' in file_name:\n                        envs_names[i].append(file_name)\n            elif env == 'hotel':\n                for file_name in files_name:\n                    if 'biwi_hotel' in file_name:\n                        envs_names[i].append(file_name)\n            elif env == 'univ':\n                for file_name in files_name:\n                    if ('students' in file_name) or ('uni_examples' in file_name):\n                        envs_names[i].append(file_name)\n            elif env == 'zara1':\n                for file_name in files_name:\n                    if 'crowds_zara01' in file_name:\n                        envs_names[i].append(file_name)\n            elif env == 'zara2':\n                for file_name in files_name:\n                    if ('crowds_zara02' in file_name) or ('crowds_zara03' in file_name):\n                        envs_names[i].append(file_name)\n        envs_paths = [[os.path.join(dset_path, env_name) for env_name in env_names] for env_names in envs_names]\n        return envs_paths, ETH_UCY\n\n    # Synthetic Dataset\n    elif dataset_name in ['synthetic', 'v2', 'v2full', 'v4'] or 'synthetic' in dataset_name:\n        envs_name = os.listdir(dset_path)\n        if filter_envs != '':\n            filter_envs = [i for i in filter_envs.split('-')]\n            envs_name_ = []\n            for env_name in envs_name:\n                for filter_env in filter_envs:\n                    if filter_env + '_radius' in env_name:\n                        envs_name_.append(env_name)\n            envs_name = envs_name_\n        envs_path = [os.path.join(dset_path, env_name) for env_name in envs_name]\n        return envs_path, envs_name\n\n    else:\n        logging.raiseExceptions(dataset_name + ' dataset doesn\\'t exists')\n\n\ndef int_tuple(s, delim=','):\n    return tuple(int(i) for i in s.strip().split(delim))\n\n\ndef l2_loss(pred_fut_traj, fut_traj, mode=\"average\"):\n    \"\"\"\n    Compute L2 loss\n\n    Input:\n    - pred_fut_traj: Tensor of shape (seq_len, batch, 2). Predicted trajectory.\n    - fut_traj: Tensor of shape (seq_len, batch, 2). Groud truth future trajectory.\n    - mode: Can be one of sum, average, raw\n    Output:\n    - loss: l2 loss depending on mode\n    \"\"\"\n    if len(fut_traj.size()) == 3:\n        loss = (fut_traj[:, :, :2].permute(1, 0, 2) - pred_fut_traj.permute(1, 0, 2)) ** 2\n    else:\n        loss = (fut_traj - pred_fut_traj) ** 2\n\n    if mode == \"sum\":\n        return torch.sum(loss)\n    elif mode == \"average\":\n        return torch.mean(loss)\n    elif mode == \"raw\":\n        return loss.sum(dim=2).sum(dim=1)\n\n\ndef displacement_error(pred_fut_traj, fut_traj, consider_ped=None, mode=\"sum\"):\n    \"\"\"\n    Compute ADE\n\n    Input:\n    - pred_fut_traj: Tensor of shape (seq_len, batch, 2). Predicted trajectory. [12, person_num, 2]\n    - fut_traj: Tensor of shape (seq_len, batch, 2). Groud truth future trajectory.\n    - consider_ped: Tensor of shape (batch)\n    - mode: Can be one of sum, raw\n    Output:\n    - loss: gives the Euclidean displacement error\n    \"\"\"\n\n    loss = (fut_traj.permute(1, 0, 2) - pred_fut_traj.permute(1, 0, 2)) ** 2\n    if consider_ped is not None:\n        loss = torch.sqrt(loss.sum(dim=2)).sum(dim=1) * consider_ped\n    else:\n        loss = torch.sqrt(loss.sum(dim=2)).sum(dim=1)\n    if mode == \"sum\":\n        return torch.sum(loss)\n    elif mode == \"mean\":\n        return torch.mean(loss)\n    elif mode == \"raw\":\n        return loss\n\n\ndef final_displacement_error(pred_fut_pos, fut_pos, consider_ped=None, mode=\"sum\"):\n    \"\"\"\n    Compute FDE\n\n    Input:\n    - pred_fut_pos: Tensor of shape (batch, 2). Predicted last pos.\n    - fut_pos: Tensor of shape (seq_len, batch, 2). Groud truth last pos.\n    - consider_ped: Tensor of shape (batch)\n    Output:\n    - loss: gives the eculidian displacement error\n    \"\"\"\n\n    loss = (fut_pos - pred_fut_pos) ** 2\n    if consider_ped is not None:\n        loss = torch.sqrt(loss.sum(dim=1)) * consider_ped\n    else:\n        loss = torch.sqrt(loss.sum(dim=1))\n    if mode == \"raw\":\n        return loss\n    else:\n        return torch.sum(loss)\n\n\ndef set_domain_shift(domain_shifts, env_name):\n    \"\"\"\n    Set the domain shift\n    \"\"\"\n    domain_shifts = [int(i) for i in domain_shifts.split('-')]\n    if len(domain_shifts) == 5:\n        if env_name == 'hotel' or 'env1' in env_name:\n            alpha_e = domain_shifts[0]\n        elif env_name == 'univ' or 'env2' in env_name:\n            alpha_e = domain_shifts[1]\n        elif env_name == 'zara1' or 'env3' in env_name:\n            alpha_e = domain_shifts[2]\n        elif env_name == 'zara2' or 'env4' in env_name:\n            alpha_e = domain_shifts[3]\n        elif env_name == 'eth' or 'env5' in env_name:\n            alpha_e = domain_shifts[4]\n        else:\n            raise ValueError('Unkown Environment!')\n    elif len(domain_shifts) == 1:\n        alpha_e = domain_shifts[0]\n    else:\n        raise ValueError('Express a domain_shift for each of the 5 enviroment or 1 for all.')\n    return alpha_e\n\n\ndef set_name_experiment(args, name='CRMF'):\n\n    if args.irm > 0:\n        name_risk = 'irm_' + str(args.irm)\n    elif args.vrex > 0:\n        name_risk = 'vrex_' + str(args.vrex)\n    else:\n        name_risk = 'erm_0.0'\n\n    return f'{name}_risk_{name_risk}_batch_{args.batch_method}_data_{args.dataset_name}_ds_{args.domain_shifts}_bk_{args.best_k}_ep_{args.num_epochs}_shuffle_{str(args.shuffle).lower()}_seed_{args.seed}'\n\n\ndef set_batch_size(batch_method, batch_sizes, env_name):\n    '''\n    Set the batch size\n    '''\n    # heterogenous batches\n    if batch_method == 'het' or batch_method == 'alt':\n        if batch_sizes == '':\n            # ETH-UCY Dataset\n            if env_name == 'hotel':\n                return 7\n            elif env_name == 'univ':\n                return 30\n            elif env_name == 'zara1':\n                return 16\n            elif env_name == 'zara2':\n                return 38\n            elif env_name == 'eth':\n                return 1\n            # Synthetic Dataset\n            else:\n                return 64\n        else:\n            batch_sizes = [int(i) for i in batch_sizes.split('-')]\n            if len(batch_sizes) == 5:\n                if env_name == 'hotel' or 'env1' in env_name:\n                    return batch_sizes[0]\n                elif env_name == 'univ' or 'env2' in env_name:\n                    return batch_sizes[1]\n                elif env_name == 'zara1' or 'env3' in env_name:\n                    return batch_sizes[2]\n                elif env_name == 'zara2' or 'env4' in env_name:\n                    return batch_sizes[3]\n                elif env_name == 'eth' or 'env5' in env_name:\n                    return batch_sizes[4]\n                else:\n                    raise ValueError('Unkown Environment!')\n            elif len(batch_sizes) == 1:\n                return batch_sizes[0]\n            else:\n                raise ValueError('Express a batch_size for each of the 5 enviroment or 1 for all.')\n\n    # homogeneous batches\n    elif batch_method == 'hom':\n        if batch_sizes == '':\n            return 64\n        else:\n            return int(batch_sizes)\n    else:\n        raise ValueError('Unkown batch method')\n\n\ndef interpolate_traj(traj, num_interp=4):\n    \"\"\"\n    Add linearly interpolated points of a trajectory\n    \"\"\"\n    sz = traj.shape\n    dense = np.zeros((sz[0], (sz[1] - 1) * (num_interp + 1) + 1, 2))\n    dense[:, :1, :] = traj[:, :1]\n\n    for i in range(num_interp + 1):\n        ratio = (i + 1) / (num_interp + 1)\n        dense[:, i + 1::num_interp + 1, :] = traj[:, 0:-1] * (1 - ratio) + traj[:, 1:] * ratio\n\n    return dense\n\n\ndef evaluate_helper(error, seq_start_end):\n    sum_ = 0\n    error = torch.stack(error, dim=1)\n    for (start, end) in seq_start_end:\n        start = start.item()\n        end = end.item()\n        _error = error[start:end]\n        _error = torch.sum(_error, dim=0)\n        _error = torch.min(_error)\n        sum_ += _error\n    return sum_\n\n\ndef cal_ade_fde(fut_traj, pred_fut_traj, mode='sum'):\n    \"\"\"\n    Compute the ADE and FDE\n    \"\"\"\n    ade = displacement_error(pred_fut_traj, fut_traj, mode=mode)\n    fde = final_displacement_error(pred_fut_traj[-1], fut_traj[-1], mode=mode)\n    return ade, fde\n\n\nbest_ade = 100\n\n\ndef save_checkpoint(state, ade, filename, is_best):\n    \"\"\"\n    Save the model\n    \"\"\"\n    global best_ade\n    best_ade = min(ade, best_ade)\n\n    if is_best:\n        torch.save(state, filename)\n        logging.info(\"Performances improved --> Model Saved\")\n\n\ndef set_seed_globally(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef get_method_name(args):\n    name_risk = 'erm_0.0'\n    if args.irm > 0:\n        name_risk += 'irm_' + str(args.irm)\n    elif args.vrex > 0:\n        name_risk += 'vrex_' + str(args.vrex)\n    else:\n        name_risk += 'erm_0.0'\n    return name_risk\n\n\ndef get_model_name(name='CRMF', epoch=None, time=False):\n    if time:\n        name = datetime.now().strftime(\"%m-%d_%H:%M_\") + name\n\n    if epoch:\n        name += f'_epoch_{epoch}'\n\n    return name\n\n\ndef set_name_method(method):\n    if 's_3' in method:\n        model = 'Baseline'\n    elif 's_6' in method:\n        model = 'Modular'\n\n    if 'i_0.0' in method:\n        risk = 'ERM'\n    else:\n        risk = f'IRM (λ={method[6:]})'\n    return f'{model} {risk}'\n\n\ndef set_name_env(env):\n    if env in [0.1, 0.3, 0.5]:\n        return 'IID'\n    elif env == 0.4:\n        return 'OoD-Inter'\n    elif env == 0.6:\n        return 'OoD-Extra'\n\n\ndef set_name_finetune(finetune):\n    if 'integ' in finetune:\n        return 'Update f only'\n    elif 'all' in finetune:\n        return 'Update Ψ,f,g'\n    elif 'refinement' in finetune:\n        return 'Update f + Refinement'\n\n\ndef save_all_model(args, model, model_name, optimizers, metric, epoch, training_step):\n    checkpoint = {\n        'epoch': epoch + 1,\n        'state_dicts': {\n            'variant_encoder': model.variant_encoder.state_dict(),\n            'invariant_encoder': model.invariant_encoder.state_dict(),\n            'coupling_layers_z': model.coupling_layers_z.state_dict(),\n            'coupling_layers_s': model.coupling_layers_s.state_dict(),\n            'coupling_layers_theta': model.coupling_layers_theta.state_dict(),\n            'x_to_s': model.x_to_s.state_dict(),\n            'future_decoder': model.future_decoder.state_dict(),\n            'past_decoder': model.past_decoder.state_dict(),\n            'mapping': model.mapping.state_dict(),\n            'theta': model.theta,\n        },\n        'optimizers': {\n            key: val.state_dict() for key, val in optimizers.items()\n        },\n        'metric': metric,\n    }\n\n    if args.model_dir:\n        filefolder = f'{args.model_dir}/{training_step}'\n    else:\n        if args.finetune:\n            phase = 'finetune'\n        else:\n            phase = 'pretrain'\n        # filefolder = f'./models/{args.dataset_name}/{phase}/{training_step}/{args.irm}/{real_style_integ}'\n        filefolder = f'./models/{args.dataset_name}/{model_name}/{phase}/{training_step}'\n\n        if args.finetune:\n            filefolder += f'/{args.finetune}/{args.original_seed}'\n\n    # Check whether the specified path exists or not\n    if not os.path.exists(filefolder): os.makedirs(filefolder)\n\n    filename = f'{filefolder}/{get_model_name(epoch=epoch)}.pth.tar'\n    torch.save(checkpoint, filename)\n    logging.info(f\" --> Model Saved in {filename}\")\n\n\ndef load_all_model(args, model, optimizers):\n    model_path = args.resume\n\n    if os.path.isfile(model_path):\n        checkpoint = torch.load(model_path, map_location='cpu')\n        args.start_epoch = checkpoint['epoch']\n\n        models_checkpoint = checkpoint['state_dicts']\n\n        # future decoder\n        model.future_decoder.load_state_dict(models_checkpoint['future_decoder'])\n        if optimizers != None:\n            optimizers['future_decoder'].load_state_dict(checkpoint['optimizers']['future_decoder'])\n            update_lr(optimizers['future_decoder'], args.lrfut)\n\n        # past decoder\n        model.past_decoder.load_state_dict(models_checkpoint['past_decoder'])\n        if optimizers != None:\n            optimizers['past_decoder'].load_state_dict(checkpoint['optimizers']['past_decoder'])\n            update_lr(optimizers['past_decoder'], args.lrpast)\n\n        # invariant encoder\n        model.invariant_encoder.load_state_dict(models_checkpoint['invariant_encoder'])\n        model.coupling_layers_z.load_state_dict(models_checkpoint['coupling_layers_z'])\n        if optimizers != None:\n            optimizers['inv'].load_state_dict(checkpoint['optimizers']['inv'])\n            update_lr(optimizers['inv'], args.lrinv)\n\n        # variant encoder\n        model.variant_encoder.load_state_dict(models_checkpoint['variant_encoder'])\n        if optimizers != None:\n            optimizers['var'].load_state_dict(checkpoint['optimizers']['var'])\n            update_lr(optimizers['var'], args.lrvar)\n\n        # Regressor\n        model.mapping.load_state_dict(models_checkpoint['mapping'])\n        if optimizers != None:\n            optimizers['map'].load_state_dict(checkpoint['optimizers']['map'])\n            update_lr(optimizers['map'], args.lrmap)\n\n        # variational models\n        model.coupling_layers_s.load_state_dict(models_checkpoint['coupling_layers_s'])\n        model.coupling_layers_theta.load_state_dict(models_checkpoint['coupling_layers_theta'])\n        model.x_to_s.load_state_dict(models_checkpoint['x_to_s'])\n        model.theta.data = models_checkpoint['theta'].data\n        if optimizers != None:\n            optimizers['variational'].load_state_dict(checkpoint['optimizers']['variational'])\n            update_lr(optimizers['variational'], args.lrvariation)\n\n        if optimizers != None:\n            optimizers['par'].load_state_dict(checkpoint['optimizers']['par'])\n            update_lr(optimizers['par'], args.lrpar)\n\n        logging.info(\"=> loaded checkpoint '{}' (epoch {})\".format(model_path, checkpoint[\"epoch\"]))\n\n    else:\n        logging.info('model {} not found'.format(model_path))\n\n\ndef get_fake_optim():\n    import torch.nn as nn\n    l = nn.Linear(1, 1)\n    return torch.optim.Adam(l.parameters())\n\n\ndef freeze(freez, models):\n    for model in models:\n        if model != None:\n            for p in model.parameters():\n                p.requires_grad = not freez\n\n\ndef update_lr(opt, lr):\n    for param_group in opt.param_groups:\n        param_group[\"lr\"] = lr\n\n\ndef from_abs_to_social(abs_coord):\n    res = []\n    for f in range(abs_coord.shape[0]):\n        sub = []\n        for k in range(abs_coord.shape[1] // NUMBER_PERSONS):\n            # for i in range(NUMBER_PERSONS):\n            #     for j in range(i+1, NUMBER_PERSONS):\n\n            #         sub.append(abs_coord[f, k*NUMBER_PERSONS+i]-abs_coord[f, k*NUMBER_PERSONS+j])\n\n            for i in range(NUMBER_PERSONS):\n                for j in range(NUMBER_PERSONS):  # each possible couple i,j\n                    if i == j: continue\n\n                    sub.append(abs_coord[f, k * NUMBER_PERSONS + i] - abs_coord[f, k * NUMBER_PERSONS + j])\n\n                    #     augm_data[idx, count, x, k] = data[idx, i, x, k] - data[idx, j, x, k]\n                    # count += 1\n\n        res.append(torch.stack(sub))\n    res = torch.stack(res)\n    return res\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/utils.py b/utils.py
--- a/utils.py	
+++ b/utils.py	
@@ -262,7 +262,6 @@
 
 
 def set_name_experiment(args, name='CRMF'):
-
     if args.irm > 0:
         name_risk = 'irm_' + str(args.irm)
     elif args.vrex > 0:
@@ -478,7 +477,7 @@
     logging.info(f" --> Model Saved in {filename}")
 
 
-def load_all_model(args, model, optimizers):
+def load_all_model(args, model, optimizers, lr_schedulers, num_batches):
     model_path = args.resume
 
     if os.path.isfile(model_path):
@@ -487,6 +486,11 @@
 
         models_checkpoint = checkpoint['state_dicts']
 
+        for k, lr_scheduler_optim in lr_schedulers.items():
+            if lr_scheduler_optim is not None:
+                for lr_scheduler in lr_scheduler_optim.item():
+                    lr_scheduler.last_epoch = (args.start_epoch - 1) * num_batches
+
         # future decoder
         model.future_decoder.load_state_dict(models_checkpoint['future_decoder'])
         if optimizers != None:
@@ -578,3 +582,18 @@
     res = torch.stack(res)
     return res
 
+
+class get_beta:
+    def __init__(self, start_epoch, interval, init_beta):
+        self.start_epoch = start_epoch
+        self.interval = interval
+        self.init_beta = init_beta
+
+    def beta_val(self, epoch):
+        if self.start_epoch < epoch <= self.start_epoch + self.interval:
+            beta = (1 - self.init_beta) / self.interval * (epoch - self.start_epoch - 1) + self.init_beta
+
+        else:
+            beta = 1.0
+
+        return beta

Index: Tensorboard.ipynb
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {\n    \"collapsed\": true\n   },\n   \"outputs\": [],\n   \"source\": [\n    \"%load_ext tensorboard\\n\",\n    \"tensorboard --logdir='./runs/Ec/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 50, 100, 100, 50)_shuffle_true_seed_72/'\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"outputs\": [],\n   \"source\": [],\n   \"metadata\": {\n    \"collapsed\": false\n   }\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"Python 3\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 2\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython2\",\n   \"version\": \"2.7.6\"\n  }\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 0\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Tensorboard.ipynb b/Tensorboard.ipynb
--- a/Tensorboard.ipynb	
+++ b/Tensorboard.ipynb	
@@ -4,12 +4,15 @@
    "cell_type": "code",
    "execution_count": null,
    "metadata": {
-    "collapsed": true
+    "collapsed": true,
+    "pycharm": {
+     "name": "#%%\n"
+    }
    },
    "outputs": [],
    "source": [
     "%load_ext tensorboard\n",
-    "tensorboard --logdir='./runs/Ec/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(150, 100, 50, 100, 100, 50)_shuffle_true_seed_72/'"
+    "tensorboard --logdir='./runs/E1/CRMF_risk_irm_5.0_batch_hom_data_eth_ds_0_bk_20_ep_(200, 400, 100, 100, 50)_shuffle_true_seed_72/'"
    ]
   },
   {
@@ -18,7 +21,10 @@
    "outputs": [],
    "source": [],
    "metadata": {
-    "collapsed": false
+    "collapsed": false,
+    "pycharm": {
+     "name": "#%%\n"
+    }
    }
   }
  ],
@@ -43,4 +49,4 @@
  },
  "nbformat": 4,
  "nbformat_minor": 0
-}
+}
\ No newline at end of file
Index: log/train.log
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/log/train.log b/log/train.log
--- a/log/train.log	
+++ b/log/train.log	
@@ -1,1401 +1,37158 @@
-2022-11-15 19:59:28,827:INFO: Initializing Training Set
-2022-11-15 19:59:32,815:INFO: Initializing Validation Set
-2022-11-15 19:59:33,267:INFO: Initializing Validation O Set
-2022-11-15 19:59:36,525:INFO: 
-===> EPOCH: 301 (P4)
-2022-11-15 19:59:36,526:INFO: - Computing loss (training)
-2022-11-15 19:59:44,562:INFO: Dataset: hotel               Batch:  1/15	Loss 26.1956 (26.1956)
-2022-11-15 19:59:45,536:INFO: Dataset: hotel               Batch:  2/15	Loss 26.0583 (26.1208)
-2022-11-15 19:59:46,410:INFO: Dataset: hotel               Batch:  3/15	Loss 26.0274 (26.0891)
-2022-11-15 19:59:47,187:INFO: Dataset: hotel               Batch:  4/15	Loss 25.4831 (25.9310)
-2022-11-15 19:59:48,811:INFO: Dataset: hotel               Batch:  5/15	Loss 25.5350 (25.8515)
-2022-11-15 19:59:50,045:INFO: Dataset: hotel               Batch:  6/15	Loss 25.8403 (25.8497)
-2022-11-15 19:59:50,825:INFO: Dataset: hotel               Batch:  7/15	Loss 24.9560 (25.7146)
-2022-11-15 19:59:51,572:INFO: Dataset: hotel               Batch:  8/15	Loss 23.9265 (25.4987)
-2022-11-15 19:59:52,347:INFO: Dataset: hotel               Batch:  9/15	Loss 25.7151 (25.5216)
-2022-11-15 19:59:53,131:INFO: Dataset: hotel               Batch: 10/15	Loss 25.9201 (25.5639)
-2022-11-15 19:59:53,889:INFO: Dataset: hotel               Batch: 11/15	Loss 25.3603 (25.5444)
-2022-11-15 19:59:54,651:INFO: Dataset: hotel               Batch: 12/15	Loss 25.7435 (25.5621)
-2022-11-15 19:59:55,390:INFO: Dataset: hotel               Batch: 13/15	Loss 24.4170 (25.4715)
-2022-11-15 19:59:56,135:INFO: Dataset: hotel               Batch: 14/15	Loss 26.2110 (25.5239)
-2022-11-15 19:59:56,832:INFO: Dataset: hotel               Batch: 15/15	Loss 11.7603 (25.1062)
-2022-11-15 20:00:21,319:INFO: Dataset: univ                Batch:  1/57	Loss 26.1616 (26.1616)
-2022-11-15 20:00:22,356:INFO: Dataset: univ                Batch:  2/57	Loss 25.9545 (26.0632)
-2022-11-15 20:00:23,158:INFO: Dataset: univ                Batch:  3/57	Loss 25.7481 (25.9587)
-2022-11-15 20:00:23,891:INFO: Dataset: univ                Batch:  4/57	Loss 26.8416 (26.1635)
-2022-11-15 20:00:24,654:INFO: Dataset: univ                Batch:  5/57	Loss 25.5636 (26.0385)
-2022-11-15 20:00:25,389:INFO: Dataset: univ                Batch:  6/57	Loss 26.3154 (26.0736)
-2022-11-15 20:00:26,150:INFO: Dataset: univ                Batch:  7/57	Loss 25.4761 (25.9763)
-2022-11-15 20:00:26,989:INFO: Dataset: univ                Batch:  8/57	Loss 25.8122 (25.9572)
-2022-11-15 20:00:27,844:INFO: Dataset: univ                Batch:  9/57	Loss 26.7712 (26.0479)
-2022-11-15 20:00:28,755:INFO: Dataset: univ                Batch: 10/57	Loss 26.1764 (26.0626)
-2022-11-15 20:00:29,825:INFO: Dataset: univ                Batch: 11/57	Loss 25.4134 (25.9946)
-2022-11-15 20:00:30,909:INFO: Dataset: univ                Batch: 12/57	Loss 26.8096 (26.0423)
-2022-11-15 20:00:31,836:INFO: Dataset: univ                Batch: 13/57	Loss 25.1474 (25.9666)
-2022-11-15 20:00:32,651:INFO: Dataset: univ                Batch: 14/57	Loss 25.8397 (25.9583)
-2022-11-15 20:00:33,486:INFO: Dataset: univ                Batch: 15/57	Loss 26.0834 (25.9656)
-2022-11-15 20:00:34,249:INFO: Dataset: univ                Batch: 16/57	Loss 26.3973 (25.9920)
-2022-11-15 20:00:35,017:INFO: Dataset: univ                Batch: 17/57	Loss 25.7305 (25.9759)
-2022-11-15 20:00:35,764:INFO: Dataset: univ                Batch: 18/57	Loss 26.4142 (25.9955)
-2022-11-15 20:00:36,500:INFO: Dataset: univ                Batch: 19/57	Loss 25.8679 (25.9891)
-2022-11-15 20:00:37,241:INFO: Dataset: univ                Batch: 20/57	Loss 25.5164 (25.9663)
-2022-11-15 20:00:37,990:INFO: Dataset: univ                Batch: 21/57	Loss 26.4182 (25.9834)
-2022-11-15 20:00:38,751:INFO: Dataset: univ                Batch: 22/57	Loss 25.2675 (25.9494)
-2022-11-15 20:00:39,546:INFO: Dataset: univ                Batch: 23/57	Loss 25.8403 (25.9445)
-2022-11-15 20:00:40,339:INFO: Dataset: univ                Batch: 24/57	Loss 25.7757 (25.9375)
-2022-11-15 20:00:41,145:INFO: Dataset: univ                Batch: 25/57	Loss 25.9329 (25.9373)
-2022-11-15 20:00:41,992:INFO: Dataset: univ                Batch: 26/57	Loss 25.3716 (25.9111)
-2022-11-15 20:00:42,837:INFO: Dataset: univ                Batch: 27/57	Loss 26.1231 (25.9177)
-2022-11-15 20:00:43,762:INFO: Dataset: univ                Batch: 28/57	Loss 24.8903 (25.8738)
-2022-11-15 20:00:44,651:INFO: Dataset: univ                Batch: 29/57	Loss 25.1626 (25.8461)
-2022-11-15 20:00:45,397:INFO: Dataset: univ                Batch: 30/57	Loss 25.7684 (25.8433)
-2022-11-15 20:00:46,166:INFO: Dataset: univ                Batch: 31/57	Loss 25.8448 (25.8433)
-2022-11-15 20:00:46,911:INFO: Dataset: univ                Batch: 32/57	Loss 25.4176 (25.8298)
-2022-11-15 20:00:47,668:INFO: Dataset: univ                Batch: 33/57	Loss 24.6485 (25.7876)
-2022-11-15 20:00:48,415:INFO: Dataset: univ                Batch: 34/57	Loss 24.8623 (25.7567)
-2022-11-15 20:00:49,178:INFO: Dataset: univ                Batch: 35/57	Loss 25.1349 (25.7372)
-2022-11-15 20:00:49,945:INFO: Dataset: univ                Batch: 36/57	Loss 25.8093 (25.7393)
-2022-11-15 20:00:50,699:INFO: Dataset: univ                Batch: 37/57	Loss 25.3542 (25.7264)
-2022-11-15 20:00:51,518:INFO: Dataset: univ                Batch: 38/57	Loss 26.1069 (25.7354)
-2022-11-15 20:00:52,293:INFO: Dataset: univ                Batch: 39/57	Loss 25.9199 (25.7408)
-2022-11-15 20:00:53,058:INFO: Dataset: univ                Batch: 40/57	Loss 25.4671 (25.7340)
-2022-11-15 20:00:53,820:INFO: Dataset: univ                Batch: 41/57	Loss 24.8181 (25.7108)
-2022-11-15 20:00:54,587:INFO: Dataset: univ                Batch: 42/57	Loss 25.1129 (25.6944)
-2022-11-15 20:00:55,341:INFO: Dataset: univ                Batch: 43/57	Loss 25.4073 (25.6881)
-2022-11-15 20:00:56,102:INFO: Dataset: univ                Batch: 44/57	Loss 25.7229 (25.6888)
-2022-11-15 20:00:56,849:INFO: Dataset: univ                Batch: 45/57	Loss 25.2618 (25.6785)
-2022-11-15 20:00:57,584:INFO: Dataset: univ                Batch: 46/57	Loss 24.9743 (25.6604)
-2022-11-15 20:00:58,329:INFO: Dataset: univ                Batch: 47/57	Loss 25.9132 (25.6653)
-2022-11-15 20:00:59,096:INFO: Dataset: univ                Batch: 48/57	Loss 24.8951 (25.6476)
-2022-11-15 20:00:59,882:INFO: Dataset: univ                Batch: 49/57	Loss 25.8238 (25.6509)
-2022-11-15 20:01:00,637:INFO: Dataset: univ                Batch: 50/57	Loss 25.7216 (25.6521)
-2022-11-15 20:01:01,401:INFO: Dataset: univ                Batch: 51/57	Loss 25.2900 (25.6450)
-2022-11-15 20:01:02,147:INFO: Dataset: univ                Batch: 52/57	Loss 25.7203 (25.6463)
-2022-11-15 20:01:02,887:INFO: Dataset: univ                Batch: 53/57	Loss 25.4776 (25.6430)
-2022-11-15 20:01:03,646:INFO: Dataset: univ                Batch: 54/57	Loss 25.1804 (25.6339)
-2022-11-15 20:01:04,419:INFO: Dataset: univ                Batch: 55/57	Loss 24.9640 (25.6224)
-2022-11-15 20:01:05,172:INFO: Dataset: univ                Batch: 56/57	Loss 26.3023 (25.6322)
-2022-11-15 20:01:05,903:INFO: Dataset: univ                Batch: 57/57	Loss 19.2065 (25.5451)
-2022-11-15 20:01:14,550:INFO: Dataset: zara1               Batch:  1/32	Loss 26.5086 (26.5086)
-2022-11-15 20:01:15,394:INFO: Dataset: zara1               Batch:  2/32	Loss 27.6195 (27.1669)
-2022-11-15 20:01:16,149:INFO: Dataset: zara1               Batch:  3/32	Loss 26.0034 (26.7340)
-2022-11-15 20:01:16,883:INFO: Dataset: zara1               Batch:  4/32	Loss 26.6388 (26.7122)
-2022-11-15 20:01:17,613:INFO: Dataset: zara1               Batch:  5/32	Loss 26.4974 (26.6581)
-2022-11-15 20:01:18,339:INFO: Dataset: zara1               Batch:  6/32	Loss 27.2818 (26.7712)
-2022-11-15 20:01:19,083:INFO: Dataset: zara1               Batch:  7/32	Loss 26.7079 (26.7645)
-2022-11-15 20:01:19,817:INFO: Dataset: zara1               Batch:  8/32	Loss 27.2911 (26.8272)
-2022-11-15 20:01:20,544:INFO: Dataset: zara1               Batch:  9/32	Loss 26.5357 (26.7957)
-2022-11-15 20:01:21,283:INFO: Dataset: zara1               Batch: 10/32	Loss 26.4361 (26.7612)
-2022-11-15 20:01:22,018:INFO: Dataset: zara1               Batch: 11/32	Loss 26.4639 (26.7288)
-2022-11-15 20:01:22,762:INFO: Dataset: zara1               Batch: 12/32	Loss 26.7558 (26.7314)
-2022-11-15 20:01:23,515:INFO: Dataset: zara1               Batch: 13/32	Loss 26.7947 (26.7369)
-2022-11-15 20:01:24,260:INFO: Dataset: zara1               Batch: 14/32	Loss 25.6688 (26.6421)
-2022-11-15 20:01:25,006:INFO: Dataset: zara1               Batch: 15/32	Loss 26.7521 (26.6509)
-2022-11-15 20:01:25,732:INFO: Dataset: zara1               Batch: 16/32	Loss 27.2969 (26.6900)
-2022-11-15 20:01:26,469:INFO: Dataset: zara1               Batch: 17/32	Loss 27.0647 (26.7084)
-2022-11-15 20:01:27,202:INFO: Dataset: zara1               Batch: 18/32	Loss 26.2659 (26.6873)
-2022-11-15 20:01:27,928:INFO: Dataset: zara1               Batch: 19/32	Loss 26.2130 (26.6674)
-2022-11-15 20:01:28,673:INFO: Dataset: zara1               Batch: 20/32	Loss 26.1875 (26.6378)
-2022-11-15 20:01:29,420:INFO: Dataset: zara1               Batch: 21/32	Loss 26.2663 (26.6203)
-2022-11-15 20:01:30,172:INFO: Dataset: zara1               Batch: 22/32	Loss 25.8622 (26.5923)
-2022-11-15 20:01:30,910:INFO: Dataset: zara1               Batch: 23/32	Loss 26.6353 (26.5947)
-2022-11-15 20:01:31,663:INFO: Dataset: zara1               Batch: 24/32	Loss 26.4258 (26.5877)
-2022-11-15 20:01:32,382:INFO: Dataset: zara1               Batch: 25/32	Loss 26.3292 (26.5797)
-2022-11-15 20:01:33,098:INFO: Dataset: zara1               Batch: 26/32	Loss 26.1378 (26.5596)
-2022-11-15 20:01:33,821:INFO: Dataset: zara1               Batch: 27/32	Loss 26.4600 (26.5555)
-2022-11-15 20:01:34,556:INFO: Dataset: zara1               Batch: 28/32	Loss 27.1296 (26.5704)
-2022-11-15 20:01:35,267:INFO: Dataset: zara1               Batch: 29/32	Loss 26.7691 (26.5766)
-2022-11-15 20:01:35,962:INFO: Dataset: zara1               Batch: 30/32	Loss 25.5998 (26.5479)
-2022-11-15 20:01:36,662:INFO: Dataset: zara1               Batch: 31/32	Loss 25.9373 (26.5273)
-2022-11-15 20:01:37,358:INFO: Dataset: zara1               Batch: 32/32	Loss 11.3621 (26.2479)
-2022-11-15 20:02:01,161:INFO: Dataset: zara2               Batch:  1/72	Loss 25.1755 (25.1755)
-2022-11-15 20:02:01,894:INFO: Dataset: zara2               Batch:  2/72	Loss 25.4151 (25.2644)
-2022-11-15 20:02:02,641:INFO: Dataset: zara2               Batch:  3/72	Loss 25.8828 (25.4820)
-2022-11-15 20:02:03,373:INFO: Dataset: zara2               Batch:  4/72	Loss 25.5751 (25.5059)
-2022-11-15 20:02:04,096:INFO: Dataset: zara2               Batch:  5/72	Loss 25.5689 (25.5173)
-2022-11-15 20:02:04,834:INFO: Dataset: zara2               Batch:  6/72	Loss 25.2531 (25.4789)
-2022-11-15 20:02:05,568:INFO: Dataset: zara2               Batch:  7/72	Loss 25.0016 (25.4171)
-2022-11-15 20:02:06,324:INFO: Dataset: zara2               Batch:  8/72	Loss 25.4359 (25.4190)
-2022-11-15 20:02:07,052:INFO: Dataset: zara2               Batch:  9/72	Loss 25.5777 (25.4344)
-2022-11-15 20:02:07,790:INFO: Dataset: zara2               Batch: 10/72	Loss 25.0206 (25.3884)
-2022-11-15 20:02:08,526:INFO: Dataset: zara2               Batch: 11/72	Loss 24.7748 (25.3362)
-2022-11-15 20:02:09,247:INFO: Dataset: zara2               Batch: 12/72	Loss 25.3451 (25.3369)
-2022-11-15 20:02:09,969:INFO: Dataset: zara2               Batch: 13/72	Loss 25.8175 (25.3775)
-2022-11-15 20:02:10,687:INFO: Dataset: zara2               Batch: 14/72	Loss 25.6788 (25.3950)
-2022-11-15 20:02:11,426:INFO: Dataset: zara2               Batch: 15/72	Loss 25.8824 (25.4254)
-2022-11-15 20:02:12,197:INFO: Dataset: zara2               Batch: 16/72	Loss 24.7106 (25.3739)
-2022-11-15 20:02:12,947:INFO: Dataset: zara2               Batch: 17/72	Loss 24.5636 (25.3265)
-2022-11-15 20:02:13,693:INFO: Dataset: zara2               Batch: 18/72	Loss 25.9554 (25.3521)
-2022-11-15 20:02:14,441:INFO: Dataset: zara2               Batch: 19/72	Loss 25.5212 (25.3591)
-2022-11-15 20:02:15,187:INFO: Dataset: zara2               Batch: 20/72	Loss 25.6152 (25.3723)
-2022-11-15 20:02:15,986:INFO: Dataset: zara2               Batch: 21/72	Loss 25.5282 (25.3800)
-2022-11-15 20:02:16,721:INFO: Dataset: zara2               Batch: 22/72	Loss 25.7471 (25.3964)
-2022-11-15 20:02:17,463:INFO: Dataset: zara2               Batch: 23/72	Loss 25.7791 (25.4144)
-2022-11-15 20:02:18,201:INFO: Dataset: zara2               Batch: 24/72	Loss 25.8818 (25.4359)
-2022-11-15 20:02:18,932:INFO: Dataset: zara2               Batch: 25/72	Loss 25.3948 (25.4340)
-2022-11-15 20:02:19,682:INFO: Dataset: zara2               Batch: 26/72	Loss 25.4276 (25.4338)
-2022-11-15 20:02:20,424:INFO: Dataset: zara2               Batch: 27/72	Loss 25.0947 (25.4209)
-2022-11-15 20:02:21,144:INFO: Dataset: zara2               Batch: 28/72	Loss 25.2917 (25.4162)
-2022-11-15 20:02:21,890:INFO: Dataset: zara2               Batch: 29/72	Loss 25.8078 (25.4313)
-2022-11-15 20:02:22,625:INFO: Dataset: zara2               Batch: 30/72	Loss 25.1598 (25.4223)
-2022-11-15 20:02:23,362:INFO: Dataset: zara2               Batch: 31/72	Loss 25.5674 (25.4261)
-2022-11-15 20:02:24,098:INFO: Dataset: zara2               Batch: 32/72	Loss 25.5970 (25.4309)
-2022-11-15 20:02:24,814:INFO: Dataset: zara2               Batch: 33/72	Loss 25.4186 (25.4306)
-2022-11-15 20:02:25,559:INFO: Dataset: zara2               Batch: 34/72	Loss 25.1736 (25.4237)
-2022-11-15 20:02:26,288:INFO: Dataset: zara2               Batch: 35/72	Loss 25.6894 (25.4300)
-2022-11-15 20:02:27,026:INFO: Dataset: zara2               Batch: 36/72	Loss 25.5905 (25.4345)
-2022-11-15 20:02:27,747:INFO: Dataset: zara2               Batch: 37/72	Loss 25.7232 (25.4411)
-2022-11-15 20:02:28,490:INFO: Dataset: zara2               Batch: 38/72	Loss 24.8463 (25.4273)
-2022-11-15 20:02:29,225:INFO: Dataset: zara2               Batch: 39/72	Loss 25.0836 (25.4173)
-2022-11-15 20:02:29,965:INFO: Dataset: zara2               Batch: 40/72	Loss 24.9000 (25.4058)
-2022-11-15 20:02:30,719:INFO: Dataset: zara2               Batch: 41/72	Loss 25.6968 (25.4140)
-2022-11-15 20:02:31,445:INFO: Dataset: zara2               Batch: 42/72	Loss 25.5515 (25.4173)
-2022-11-15 20:02:32,150:INFO: Dataset: zara2               Batch: 43/72	Loss 24.9817 (25.4065)
-2022-11-15 20:02:32,861:INFO: Dataset: zara2               Batch: 44/72	Loss 25.3127 (25.4044)
-2022-11-15 20:02:33,596:INFO: Dataset: zara2               Batch: 45/72	Loss 25.3570 (25.4032)
-2022-11-15 20:02:34,320:INFO: Dataset: zara2               Batch: 46/72	Loss 25.1354 (25.3972)
-2022-11-15 20:02:35,061:INFO: Dataset: zara2               Batch: 47/72	Loss 25.0969 (25.3897)
-2022-11-15 20:02:35,793:INFO: Dataset: zara2               Batch: 48/72	Loss 25.5553 (25.3929)
-2022-11-15 20:02:36,501:INFO: Dataset: zara2               Batch: 49/72	Loss 25.2993 (25.3911)
-2022-11-15 20:02:37,227:INFO: Dataset: zara2               Batch: 50/72	Loss 25.5454 (25.3943)
-2022-11-15 20:02:37,964:INFO: Dataset: zara2               Batch: 51/72	Loss 25.2435 (25.3915)
-2022-11-15 20:02:38,706:INFO: Dataset: zara2               Batch: 52/72	Loss 25.5814 (25.3949)
-2022-11-15 20:02:39,420:INFO: Dataset: zara2               Batch: 53/72	Loss 25.3393 (25.3938)
-2022-11-15 20:02:40,128:INFO: Dataset: zara2               Batch: 54/72	Loss 25.2887 (25.3918)
-2022-11-15 20:02:40,856:INFO: Dataset: zara2               Batch: 55/72	Loss 25.7814 (25.3989)
-2022-11-15 20:02:41,590:INFO: Dataset: zara2               Batch: 56/72	Loss 25.2090 (25.3950)
-2022-11-15 20:02:42,306:INFO: Dataset: zara2               Batch: 57/72	Loss 25.3486 (25.3943)
-2022-11-15 20:02:43,038:INFO: Dataset: zara2               Batch: 58/72	Loss 24.9527 (25.3855)
-2022-11-15 20:02:43,743:INFO: Dataset: zara2               Batch: 59/72	Loss 25.0846 (25.3804)
-2022-11-15 20:02:44,433:INFO: Dataset: zara2               Batch: 60/72	Loss 25.0993 (25.3752)
-2022-11-15 20:02:45,138:INFO: Dataset: zara2               Batch: 61/72	Loss 25.2075 (25.3721)
-2022-11-15 20:02:45,888:INFO: Dataset: zara2               Batch: 62/72	Loss 25.7840 (25.3781)
-2022-11-15 20:02:46,619:INFO: Dataset: zara2               Batch: 63/72	Loss 25.5402 (25.3799)
-2022-11-15 20:02:47,367:INFO: Dataset: zara2               Batch: 64/72	Loss 24.6389 (25.3680)
-2022-11-15 20:02:48,098:INFO: Dataset: zara2               Batch: 65/72	Loss 25.8926 (25.3753)
-2022-11-15 20:02:48,847:INFO: Dataset: zara2               Batch: 66/72	Loss 24.7270 (25.3642)
-2022-11-15 20:02:49,579:INFO: Dataset: zara2               Batch: 67/72	Loss 25.2637 (25.3631)
-2022-11-15 20:02:50,298:INFO: Dataset: zara2               Batch: 68/72	Loss 24.6318 (25.3532)
-2022-11-15 20:02:51,033:INFO: Dataset: zara2               Batch: 69/72	Loss 24.9744 (25.3475)
-2022-11-15 20:02:51,751:INFO: Dataset: zara2               Batch: 70/72	Loss 24.9263 (25.3419)
-2022-11-15 20:02:52,473:INFO: Dataset: zara2               Batch: 71/72	Loss 25.2149 (25.3403)
-2022-11-15 20:02:53,157:INFO: Dataset: zara2               Batch: 72/72	Loss 11.4512 (25.2737)
-2022-11-15 20:02:54,069:INFO: - Computing ADE (validation)
-2022-11-15 20:03:02,675:INFO: 		 ADE on hotel                     dataset:	 1.3141183853149414
-2022-11-15 20:03:11,687:INFO: 		 ADE on univ                      dataset:	 1.536051630973816
-2022-11-15 20:03:20,353:INFO: 		 ADE on zara1                     dataset:	 2.237816572189331
-2022-11-15 20:03:29,695:INFO: 		 ADE on zara2                     dataset:	 1.4872373342514038
-2022-11-15 20:03:29,696:INFO: Average validation:	ADE  1.5468	FDE  2.8095
-2022-11-15 20:03:29,701:INFO: - Computing ADE (training)
-2022-11-15 20:03:40,331:INFO: 		 ADE on hotel                     dataset:	 1.588235855102539
-2022-11-15 20:04:05,289:INFO: 		 ADE on univ                      dataset:	 1.4429010152816772
-2022-11-15 20:04:14,861:INFO: 		 ADE on zara1                     dataset:	 2.2202279567718506
-2022-11-15 20:04:40,037:INFO: 		 ADE on zara2                     dataset:	 1.688158392906189
-2022-11-15 20:04:40,037:INFO: Average training:	ADE  1.5459	FDE  2.8061
-2022-11-15 20:04:40,074:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_301.pth.tar
-2022-11-15 20:04:40,074:INFO: 
-===> EPOCH: 302 (P4)
-2022-11-15 20:04:40,076:INFO: - Computing loss (training)
-2022-11-15 20:04:47,619:INFO: Dataset: hotel               Batch:  1/15	Loss 25.4193 (25.4193)
-2022-11-15 20:04:48,473:INFO: Dataset: hotel               Batch:  2/15	Loss 24.9915 (25.2241)
-2022-11-15 20:04:49,230:INFO: Dataset: hotel               Batch:  3/15	Loss 26.1526 (25.4976)
-2022-11-15 20:04:49,970:INFO: Dataset: hotel               Batch:  4/15	Loss 25.3314 (25.4527)
-2022-11-15 20:04:50,707:INFO: Dataset: hotel               Batch:  5/15	Loss 25.3037 (25.4188)
-2022-11-15 20:04:51,434:INFO: Dataset: hotel               Batch:  6/15	Loss 25.0480 (25.3538)
-2022-11-15 20:04:52,156:INFO: Dataset: hotel               Batch:  7/15	Loss 25.0145 (25.3032)
-2022-11-15 20:04:52,957:INFO: Dataset: hotel               Batch:  8/15	Loss 25.3749 (25.3113)
-2022-11-15 20:04:53,700:INFO: Dataset: hotel               Batch:  9/15	Loss 24.9530 (25.2708)
-2022-11-15 20:04:54,420:INFO: Dataset: hotel               Batch: 10/15	Loss 25.3093 (25.2748)
-2022-11-15 20:04:55,157:INFO: Dataset: hotel               Batch: 11/15	Loss 24.6977 (25.2163)
-2022-11-15 20:04:55,878:INFO: Dataset: hotel               Batch: 12/15	Loss 25.0890 (25.2072)
-2022-11-15 20:04:56,604:INFO: Dataset: hotel               Batch: 13/15	Loss 25.1286 (25.2000)
-2022-11-15 20:04:57,327:INFO: Dataset: hotel               Batch: 14/15	Loss 24.7944 (25.1721)
-2022-11-15 20:04:58,044:INFO: Dataset: hotel               Batch: 15/15	Loss 11.3949 (24.8631)
-2022-11-15 20:05:21,435:INFO: Dataset: univ                Batch:  1/57	Loss 24.5498 (24.5498)
-2022-11-15 20:05:22,206:INFO: Dataset: univ                Batch:  2/57	Loss 24.5186 (24.5332)
-2022-11-15 20:05:22,922:INFO: Dataset: univ                Batch:  3/57	Loss 25.2107 (24.7411)
-2022-11-15 20:05:23,649:INFO: Dataset: univ                Batch:  4/57	Loss 25.1229 (24.8283)
-2022-11-15 20:05:24,377:INFO: Dataset: univ                Batch:  5/57	Loss 24.7605 (24.8130)
-2022-11-15 20:05:25,094:INFO: Dataset: univ                Batch:  6/57	Loss 25.1339 (24.8585)
-2022-11-15 20:05:25,810:INFO: Dataset: univ                Batch:  7/57	Loss 24.4859 (24.8074)
-2022-11-15 20:05:26,532:INFO: Dataset: univ                Batch:  8/57	Loss 25.0224 (24.8330)
-2022-11-15 20:05:27,264:INFO: Dataset: univ                Batch:  9/57	Loss 25.8269 (24.9281)
-2022-11-15 20:05:28,008:INFO: Dataset: univ                Batch: 10/57	Loss 24.8367 (24.9179)
-2022-11-15 20:05:28,736:INFO: Dataset: univ                Batch: 11/57	Loss 24.8520 (24.9118)
-2022-11-15 20:05:29,497:INFO: Dataset: univ                Batch: 12/57	Loss 24.9613 (24.9154)
-2022-11-15 20:05:30,241:INFO: Dataset: univ                Batch: 13/57	Loss 24.6284 (24.8896)
-2022-11-15 20:05:30,980:INFO: Dataset: univ                Batch: 14/57	Loss 25.2927 (24.9217)
-2022-11-15 20:05:31,702:INFO: Dataset: univ                Batch: 15/57	Loss 24.9947 (24.9267)
-2022-11-15 20:05:32,446:INFO: Dataset: univ                Batch: 16/57	Loss 25.6825 (24.9771)
-2022-11-15 20:05:33,244:INFO: Dataset: univ                Batch: 17/57	Loss 25.0761 (24.9826)
-2022-11-15 20:05:34,063:INFO: Dataset: univ                Batch: 18/57	Loss 25.7079 (25.0183)
-2022-11-15 20:05:34,979:INFO: Dataset: univ                Batch: 19/57	Loss 24.4411 (24.9854)
-2022-11-15 20:05:35,776:INFO: Dataset: univ                Batch: 20/57	Loss 25.2444 (24.9960)
-2022-11-15 20:05:36,579:INFO: Dataset: univ                Batch: 21/57	Loss 24.7193 (24.9821)
-2022-11-15 20:05:37,359:INFO: Dataset: univ                Batch: 22/57	Loss 24.4205 (24.9521)
-2022-11-15 20:05:38,131:INFO: Dataset: univ                Batch: 23/57	Loss 25.0619 (24.9567)
-2022-11-15 20:05:38,875:INFO: Dataset: univ                Batch: 24/57	Loss 24.5940 (24.9404)
-2022-11-15 20:05:39,610:INFO: Dataset: univ                Batch: 25/57	Loss 24.3324 (24.9131)
-2022-11-15 20:05:40,369:INFO: Dataset: univ                Batch: 26/57	Loss 24.6109 (24.9051)
-2022-11-15 20:05:41,110:INFO: Dataset: univ                Batch: 27/57	Loss 25.5510 (24.9283)
-2022-11-15 20:05:41,861:INFO: Dataset: univ                Batch: 28/57	Loss 24.6204 (24.9183)
-2022-11-15 20:05:42,606:INFO: Dataset: univ                Batch: 29/57	Loss 25.6377 (24.9428)
-2022-11-15 20:05:43,323:INFO: Dataset: univ                Batch: 30/57	Loss 25.0535 (24.9469)
-2022-11-15 20:05:44,074:INFO: Dataset: univ                Batch: 31/57	Loss 24.6508 (24.9373)
-2022-11-15 20:05:44,824:INFO: Dataset: univ                Batch: 32/57	Loss 24.2199 (24.9146)
-2022-11-15 20:05:45,568:INFO: Dataset: univ                Batch: 33/57	Loss 24.5660 (24.9032)
-2022-11-15 20:05:46,335:INFO: Dataset: univ                Batch: 34/57	Loss 24.6954 (24.8970)
-2022-11-15 20:05:47,171:INFO: Dataset: univ                Batch: 35/57	Loss 24.6152 (24.8874)
-2022-11-15 20:05:48,216:INFO: Dataset: univ                Batch: 36/57	Loss 25.7322 (24.9051)
-2022-11-15 20:05:49,030:INFO: Dataset: univ                Batch: 37/57	Loss 24.9815 (24.9071)
-2022-11-15 20:05:49,870:INFO: Dataset: univ                Batch: 38/57	Loss 24.4569 (24.8957)
-2022-11-15 20:05:50,674:INFO: Dataset: univ                Batch: 39/57	Loss 25.0528 (24.8994)
-2022-11-15 20:05:51,527:INFO: Dataset: univ                Batch: 40/57	Loss 24.3589 (24.8845)
-2022-11-15 20:05:52,361:INFO: Dataset: univ                Batch: 41/57	Loss 24.6678 (24.8784)
-2022-11-15 20:05:53,132:INFO: Dataset: univ                Batch: 42/57	Loss 24.8421 (24.8775)
-2022-11-15 20:05:53,897:INFO: Dataset: univ                Batch: 43/57	Loss 25.0658 (24.8815)
-2022-11-15 20:05:54,659:INFO: Dataset: univ                Batch: 44/57	Loss 24.2374 (24.8688)
-2022-11-15 20:05:55,448:INFO: Dataset: univ                Batch: 45/57	Loss 25.1280 (24.8744)
-2022-11-15 20:05:56,208:INFO: Dataset: univ                Batch: 46/57	Loss 24.4422 (24.8618)
-2022-11-15 20:05:56,951:INFO: Dataset: univ                Batch: 47/57	Loss 24.3231 (24.8504)
-2022-11-15 20:05:57,731:INFO: Dataset: univ                Batch: 48/57	Loss 24.6332 (24.8466)
-2022-11-15 20:05:58,518:INFO: Dataset: univ                Batch: 49/57	Loss 25.5124 (24.8599)
-2022-11-15 20:05:59,327:INFO: Dataset: univ                Batch: 50/57	Loss 24.3985 (24.8488)
-2022-11-15 20:06:00,172:INFO: Dataset: univ                Batch: 51/57	Loss 24.4137 (24.8399)
-2022-11-15 20:06:00,989:INFO: Dataset: univ                Batch: 52/57	Loss 24.4512 (24.8327)
-2022-11-15 20:06:01,855:INFO: Dataset: univ                Batch: 53/57	Loss 24.5873 (24.8274)
-2022-11-15 20:06:02,729:INFO: Dataset: univ                Batch: 54/57	Loss 24.3545 (24.8179)
-2022-11-15 20:06:03,582:INFO: Dataset: univ                Batch: 55/57	Loss 24.5922 (24.8129)
-2022-11-15 20:06:04,538:INFO: Dataset: univ                Batch: 56/57	Loss 24.5812 (24.8085)
-2022-11-15 20:06:05,421:INFO: Dataset: univ                Batch: 57/57	Loss 18.1497 (24.7098)
-2022-11-15 20:06:14,444:INFO: Dataset: zara1               Batch:  1/32	Loss 24.3476 (24.3476)
-2022-11-15 20:06:15,361:INFO: Dataset: zara1               Batch:  2/32	Loss 25.4856 (24.9066)
-2022-11-15 20:06:16,121:INFO: Dataset: zara1               Batch:  3/32	Loss 24.5083 (24.7921)
-2022-11-15 20:06:16,902:INFO: Dataset: zara1               Batch:  4/32	Loss 25.0997 (24.8838)
-2022-11-15 20:06:17,735:INFO: Dataset: zara1               Batch:  5/32	Loss 25.4957 (24.9732)
-2022-11-15 20:06:18,471:INFO: Dataset: zara1               Batch:  6/32	Loss 24.4632 (24.8914)
-2022-11-15 20:06:19,215:INFO: Dataset: zara1               Batch:  7/32	Loss 24.4525 (24.8077)
-2022-11-15 20:06:20,011:INFO: Dataset: zara1               Batch:  8/32	Loss 24.9587 (24.8277)
-2022-11-15 20:06:20,722:INFO: Dataset: zara1               Batch:  9/32	Loss 24.8688 (24.8337)
-2022-11-15 20:06:21,492:INFO: Dataset: zara1               Batch: 10/32	Loss 24.5495 (24.8053)
-2022-11-15 20:06:22,274:INFO: Dataset: zara1               Batch: 11/32	Loss 25.3436 (24.8434)
-2022-11-15 20:06:23,000:INFO: Dataset: zara1               Batch: 12/32	Loss 24.5302 (24.8172)
-2022-11-15 20:06:23,742:INFO: Dataset: zara1               Batch: 13/32	Loss 24.3879 (24.7846)
-2022-11-15 20:06:24,484:INFO: Dataset: zara1               Batch: 14/32	Loss 24.1765 (24.7423)
-2022-11-15 20:06:25,251:INFO: Dataset: zara1               Batch: 15/32	Loss 25.7001 (24.8108)
-2022-11-15 20:06:26,003:INFO: Dataset: zara1               Batch: 16/32	Loss 24.3747 (24.7799)
-2022-11-15 20:06:26,750:INFO: Dataset: zara1               Batch: 17/32	Loss 24.5883 (24.7686)
-2022-11-15 20:06:27,510:INFO: Dataset: zara1               Batch: 18/32	Loss 24.4768 (24.7499)
-2022-11-15 20:06:28,253:INFO: Dataset: zara1               Batch: 19/32	Loss 25.2287 (24.7759)
-2022-11-15 20:06:28,982:INFO: Dataset: zara1               Batch: 20/32	Loss 24.7725 (24.7757)
-2022-11-15 20:06:29,760:INFO: Dataset: zara1               Batch: 21/32	Loss 23.8004 (24.7167)
-2022-11-15 20:06:30,604:INFO: Dataset: zara1               Batch: 22/32	Loss 23.8584 (24.6670)
-2022-11-15 20:06:31,398:INFO: Dataset: zara1               Batch: 23/32	Loss 24.3326 (24.6508)
-2022-11-15 20:06:32,131:INFO: Dataset: zara1               Batch: 24/32	Loss 24.6453 (24.6506)
-2022-11-15 20:06:32,875:INFO: Dataset: zara1               Batch: 25/32	Loss 24.1308 (24.6237)
-2022-11-15 20:06:33,692:INFO: Dataset: zara1               Batch: 26/32	Loss 24.7427 (24.6278)
-2022-11-15 20:06:34,544:INFO: Dataset: zara1               Batch: 27/32	Loss 24.0950 (24.6099)
-2022-11-15 20:06:35,346:INFO: Dataset: zara1               Batch: 28/32	Loss 24.7635 (24.6162)
-2022-11-15 20:06:36,233:INFO: Dataset: zara1               Batch: 29/32	Loss 24.7936 (24.6216)
-2022-11-15 20:06:37,232:INFO: Dataset: zara1               Batch: 30/32	Loss 24.8732 (24.6291)
-2022-11-15 20:06:38,081:INFO: Dataset: zara1               Batch: 31/32	Loss 25.0630 (24.6444)
-2022-11-15 20:06:39,065:INFO: Dataset: zara1               Batch: 32/32	Loss 10.6689 (24.4752)
-2022-11-15 20:07:10,199:INFO: Dataset: zara2               Batch:  1/72	Loss 23.9744 (23.9744)
-2022-11-15 20:07:11,036:INFO: Dataset: zara2               Batch:  2/72	Loss 23.9454 (23.9620)
-2022-11-15 20:07:11,884:INFO: Dataset: zara2               Batch:  3/72	Loss 24.4790 (24.1583)
-2022-11-15 20:07:12,994:INFO: Dataset: zara2               Batch:  4/72	Loss 24.7863 (24.2915)
-2022-11-15 20:07:14,051:INFO: Dataset: zara2               Batch:  5/72	Loss 24.6354 (24.3681)
-2022-11-15 20:07:14,979:INFO: Dataset: zara2               Batch:  6/72	Loss 24.1533 (24.3268)
-2022-11-15 20:07:15,842:INFO: Dataset: zara2               Batch:  7/72	Loss 23.9038 (24.2587)
-2022-11-15 20:07:16,643:INFO: Dataset: zara2               Batch:  8/72	Loss 24.0133 (24.2317)
-2022-11-15 20:07:17,380:INFO: Dataset: zara2               Batch:  9/72	Loss 23.8760 (24.1896)
-2022-11-15 20:07:18,177:INFO: Dataset: zara2               Batch: 10/72	Loss 24.3952 (24.2082)
-2022-11-15 20:07:18,992:INFO: Dataset: zara2               Batch: 11/72	Loss 24.0815 (24.1973)
-2022-11-15 20:07:19,777:INFO: Dataset: zara2               Batch: 12/72	Loss 24.8857 (24.2608)
-2022-11-15 20:07:20,705:INFO: Dataset: zara2               Batch: 13/72	Loss 24.5751 (24.2842)
-2022-11-15 20:07:21,498:INFO: Dataset: zara2               Batch: 14/72	Loss 23.6917 (24.2357)
-2022-11-15 20:07:22,363:INFO: Dataset: zara2               Batch: 15/72	Loss 24.2926 (24.2395)
-2022-11-15 20:07:23,185:INFO: Dataset: zara2               Batch: 16/72	Loss 24.3888 (24.2500)
-2022-11-15 20:07:23,933:INFO: Dataset: zara2               Batch: 17/72	Loss 23.8328 (24.2270)
-2022-11-15 20:07:24,698:INFO: Dataset: zara2               Batch: 18/72	Loss 24.3211 (24.2322)
-2022-11-15 20:07:25,496:INFO: Dataset: zara2               Batch: 19/72	Loss 23.8987 (24.2151)
-2022-11-15 20:07:26,372:INFO: Dataset: zara2               Batch: 20/72	Loss 24.2201 (24.2154)
-2022-11-15 20:07:27,177:INFO: Dataset: zara2               Batch: 21/72	Loss 23.8963 (24.1986)
-2022-11-15 20:07:27,931:INFO: Dataset: zara2               Batch: 22/72	Loss 23.9564 (24.1853)
-2022-11-15 20:07:28,767:INFO: Dataset: zara2               Batch: 23/72	Loss 24.2100 (24.1864)
-2022-11-15 20:07:29,518:INFO: Dataset: zara2               Batch: 24/72	Loss 24.0797 (24.1813)
-2022-11-15 20:07:30,447:INFO: Dataset: zara2               Batch: 25/72	Loss 23.9143 (24.1707)
-2022-11-15 20:07:31,377:INFO: Dataset: zara2               Batch: 26/72	Loss 24.0044 (24.1653)
-2022-11-15 20:07:32,369:INFO: Dataset: zara2               Batch: 27/72	Loss 24.3740 (24.1728)
-2022-11-15 20:07:33,338:INFO: Dataset: zara2               Batch: 28/72	Loss 23.7205 (24.1577)
-2022-11-15 20:07:34,063:INFO: Dataset: zara2               Batch: 29/72	Loss 24.2574 (24.1611)
-2022-11-15 20:07:34,784:INFO: Dataset: zara2               Batch: 30/72	Loss 23.8530 (24.1527)
-2022-11-15 20:07:35,629:INFO: Dataset: zara2               Batch: 31/72	Loss 23.7554 (24.1383)
-2022-11-15 20:07:36,383:INFO: Dataset: zara2               Batch: 32/72	Loss 23.9795 (24.1328)
-2022-11-15 20:07:37,147:INFO: Dataset: zara2               Batch: 33/72	Loss 24.2561 (24.1370)
-2022-11-15 20:07:38,154:INFO: Dataset: zara2               Batch: 34/72	Loss 24.3484 (24.1437)
-2022-11-15 20:07:39,112:INFO: Dataset: zara2               Batch: 35/72	Loss 24.3162 (24.1490)
-2022-11-15 20:07:39,895:INFO: Dataset: zara2               Batch: 36/72	Loss 23.5787 (24.1345)
-2022-11-15 20:07:40,763:INFO: Dataset: zara2               Batch: 37/72	Loss 23.6358 (24.1211)
-2022-11-15 20:07:41,481:INFO: Dataset: zara2               Batch: 38/72	Loss 23.7431 (24.1114)
-2022-11-15 20:07:42,368:INFO: Dataset: zara2               Batch: 39/72	Loss 23.4462 (24.0955)
-2022-11-15 20:07:43,200:INFO: Dataset: zara2               Batch: 40/72	Loss 23.5566 (24.0830)
-2022-11-15 20:07:44,093:INFO: Dataset: zara2               Batch: 41/72	Loss 24.5046 (24.0925)
-2022-11-15 20:07:45,190:INFO: Dataset: zara2               Batch: 42/72	Loss 23.7096 (24.0820)
-2022-11-15 20:07:46,205:INFO: Dataset: zara2               Batch: 43/72	Loss 24.1773 (24.0841)
-2022-11-15 20:07:47,143:INFO: Dataset: zara2               Batch: 44/72	Loss 23.6524 (24.0759)
-2022-11-15 20:07:48,071:INFO: Dataset: zara2               Batch: 45/72	Loss 24.2774 (24.0801)
-2022-11-15 20:07:48,999:INFO: Dataset: zara2               Batch: 46/72	Loss 23.9965 (24.0785)
-2022-11-15 20:07:49,855:INFO: Dataset: zara2               Batch: 47/72	Loss 24.4469 (24.0854)
-2022-11-15 20:07:50,696:INFO: Dataset: zara2               Batch: 48/72	Loss 24.0177 (24.0841)
-2022-11-15 20:07:51,526:INFO: Dataset: zara2               Batch: 49/72	Loss 23.9818 (24.0822)
-2022-11-15 20:07:52,406:INFO: Dataset: zara2               Batch: 50/72	Loss 24.0527 (24.0816)
-2022-11-15 20:07:53,193:INFO: Dataset: zara2               Batch: 51/72	Loss 23.9049 (24.0791)
-2022-11-15 20:07:54,016:INFO: Dataset: zara2               Batch: 52/72	Loss 23.9308 (24.0763)
-2022-11-15 20:07:54,928:INFO: Dataset: zara2               Batch: 53/72	Loss 23.5418 (24.0626)
-2022-11-15 20:07:55,854:INFO: Dataset: zara2               Batch: 54/72	Loss 23.8810 (24.0593)
-2022-11-15 20:07:56,833:INFO: Dataset: zara2               Batch: 55/72	Loss 23.9341 (24.0575)
-2022-11-15 20:07:57,792:INFO: Dataset: zara2               Batch: 56/72	Loss 23.6097 (24.0491)
-2022-11-15 20:07:58,738:INFO: Dataset: zara2               Batch: 57/72	Loss 23.9305 (24.0467)
-2022-11-15 20:07:59,626:INFO: Dataset: zara2               Batch: 58/72	Loss 23.6865 (24.0395)
-2022-11-15 20:08:00,454:INFO: Dataset: zara2               Batch: 59/72	Loss 23.2851 (24.0281)
-2022-11-15 20:08:01,326:INFO: Dataset: zara2               Batch: 60/72	Loss 24.2079 (24.0311)
-2022-11-15 20:08:02,193:INFO: Dataset: zara2               Batch: 61/72	Loss 23.7519 (24.0265)
-2022-11-15 20:08:02,992:INFO: Dataset: zara2               Batch: 62/72	Loss 23.3267 (24.0126)
-2022-11-15 20:08:03,815:INFO: Dataset: zara2               Batch: 63/72	Loss 23.3946 (24.0024)
-2022-11-15 20:08:04,642:INFO: Dataset: zara2               Batch: 64/72	Loss 23.6885 (23.9968)
-2022-11-15 20:08:05,612:INFO: Dataset: zara2               Batch: 65/72	Loss 23.8164 (23.9937)
-2022-11-15 20:08:06,562:INFO: Dataset: zara2               Batch: 66/72	Loss 23.6746 (23.9895)
-2022-11-15 20:08:07,507:INFO: Dataset: zara2               Batch: 67/72	Loss 24.1921 (23.9918)
-2022-11-15 20:08:08,432:INFO: Dataset: zara2               Batch: 68/72	Loss 24.1549 (23.9938)
-2022-11-15 20:08:09,397:INFO: Dataset: zara2               Batch: 69/72	Loss 24.4487 (24.0002)
-2022-11-15 20:08:10,234:INFO: Dataset: zara2               Batch: 70/72	Loss 23.8227 (23.9978)
-2022-11-15 20:08:11,061:INFO: Dataset: zara2               Batch: 71/72	Loss 24.0327 (23.9982)
-2022-11-15 20:08:11,859:INFO: Dataset: zara2               Batch: 72/72	Loss 10.6761 (23.9343)
-2022-11-15 20:08:12,863:INFO: - Computing ADE (validation)
-2022-11-15 20:08:23,163:INFO: 		 ADE on hotel                     dataset:	 1.5965275764465332
-2022-11-15 20:08:33,760:INFO: 		 ADE on univ                      dataset:	 1.4956151247024536
-2022-11-15 20:08:44,024:INFO: 		 ADE on zara1                     dataset:	 1.251158595085144
-2022-11-15 20:08:54,922:INFO: 		 ADE on zara2                     dataset:	 1.3429155349731445
-2022-11-15 20:08:54,922:INFO: Average validation:	ADE  1.4309	FDE  2.7824
-2022-11-15 20:08:54,923:INFO: - Computing ADE (training)
-2022-11-15 20:09:05,575:INFO: 		 ADE on hotel                     dataset:	 1.8843997716903687
-2022-11-15 20:09:34,356:INFO: 		 ADE on univ                      dataset:	 1.4412652254104614
-2022-11-15 20:09:45,294:INFO: 		 ADE on zara1                     dataset:	 1.6447087526321411
-2022-11-15 20:10:15,323:INFO: 		 ADE on zara2                     dataset:	 1.4561996459960938
-2022-11-15 20:10:15,323:INFO: Average training:	ADE  1.4685	FDE  2.8399
-2022-11-15 20:10:15,360:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_302.pth.tar
-2022-11-15 20:10:15,360:INFO: 
-===> EPOCH: 303 (P4)
-2022-11-15 20:10:15,362:INFO: - Computing loss (training)
-2022-11-15 20:10:24,500:INFO: Dataset: hotel               Batch:  1/15	Loss 25.1982 (25.1982)
-2022-11-15 20:10:25,458:INFO: Dataset: hotel               Batch:  2/15	Loss 25.3467 (25.2724)
-2022-11-15 20:10:26,323:INFO: Dataset: hotel               Batch:  3/15	Loss 23.9155 (24.7456)
-2022-11-15 20:10:27,168:INFO: Dataset: hotel               Batch:  4/15	Loss 24.9367 (24.7930)
-2022-11-15 20:10:28,084:INFO: Dataset: hotel               Batch:  5/15	Loss 23.7591 (24.5761)
-2022-11-15 20:10:29,162:INFO: Dataset: hotel               Batch:  6/15	Loss 24.9159 (24.6317)
-2022-11-15 20:10:30,234:INFO: Dataset: hotel               Batch:  7/15	Loss 25.9828 (24.7857)
-2022-11-15 20:10:31,543:INFO: Dataset: hotel               Batch:  8/15	Loss 24.1146 (24.7061)
-2022-11-15 20:10:32,481:INFO: Dataset: hotel               Batch:  9/15	Loss 23.9871 (24.6271)
-2022-11-15 20:10:33,302:INFO: Dataset: hotel               Batch: 10/15	Loss 24.6195 (24.6264)
-2022-11-15 20:10:34,134:INFO: Dataset: hotel               Batch: 11/15	Loss 24.5223 (24.6175)
-2022-11-15 20:10:34,978:INFO: Dataset: hotel               Batch: 12/15	Loss 24.8096 (24.6318)
-2022-11-15 20:10:35,833:INFO: Dataset: hotel               Batch: 13/15	Loss 24.7028 (24.6367)
-2022-11-15 20:10:36,642:INFO: Dataset: hotel               Batch: 14/15	Loss 23.6415 (24.5731)
-2022-11-15 20:10:37,438:INFO: Dataset: hotel               Batch: 15/15	Loss 10.9001 (24.1763)
-2022-11-15 20:11:05,069:INFO: Dataset: univ                Batch:  1/57	Loss 24.1048 (24.1048)
-2022-11-15 20:11:05,953:INFO: Dataset: univ                Batch:  2/57	Loss 23.8884 (23.9988)
-2022-11-15 20:11:06,804:INFO: Dataset: univ                Batch:  3/57	Loss 23.7674 (23.9181)
-2022-11-15 20:11:07,709:INFO: Dataset: univ                Batch:  4/57	Loss 23.6268 (23.8429)
-2022-11-15 20:11:08,634:INFO: Dataset: univ                Batch:  5/57	Loss 24.1790 (23.9018)
-2022-11-15 20:11:09,626:INFO: Dataset: univ                Batch:  6/57	Loss 23.5161 (23.8295)
-2022-11-15 20:11:10,615:INFO: Dataset: univ                Batch:  7/57	Loss 23.6046 (23.7978)
-2022-11-15 20:11:11,560:INFO: Dataset: univ                Batch:  8/57	Loss 23.8882 (23.8079)
-2022-11-15 20:11:12,493:INFO: Dataset: univ                Batch:  9/57	Loss 23.9494 (23.8203)
-2022-11-15 20:11:13,391:INFO: Dataset: univ                Batch: 10/57	Loss 24.2709 (23.8631)
-2022-11-15 20:11:14,327:INFO: Dataset: univ                Batch: 11/57	Loss 23.5937 (23.8365)
-2022-11-15 20:11:15,210:INFO: Dataset: univ                Batch: 12/57	Loss 23.6874 (23.8234)
-2022-11-15 20:11:16,070:INFO: Dataset: univ                Batch: 13/57	Loss 23.6932 (23.8125)
-2022-11-15 20:11:16,928:INFO: Dataset: univ                Batch: 14/57	Loss 23.8972 (23.8199)
-2022-11-15 20:11:17,795:INFO: Dataset: univ                Batch: 15/57	Loss 23.5976 (23.8043)
-2022-11-15 20:11:18,726:INFO: Dataset: univ                Batch: 16/57	Loss 23.9177 (23.8097)
-2022-11-15 20:11:19,684:INFO: Dataset: univ                Batch: 17/57	Loss 23.6989 (23.8039)
-2022-11-15 20:11:20,662:INFO: Dataset: univ                Batch: 18/57	Loss 24.0690 (23.8193)
-2022-11-15 20:11:21,609:INFO: Dataset: univ                Batch: 19/57	Loss 24.0374 (23.8279)
-2022-11-15 20:11:22,543:INFO: Dataset: univ                Batch: 20/57	Loss 24.2608 (23.8501)
-2022-11-15 20:11:23,467:INFO: Dataset: univ                Batch: 21/57	Loss 23.9297 (23.8536)
-2022-11-15 20:11:24,304:INFO: Dataset: univ                Batch: 22/57	Loss 23.6872 (23.8449)
-2022-11-15 20:11:25,153:INFO: Dataset: univ                Batch: 23/57	Loss 23.7633 (23.8408)
-2022-11-15 20:11:26,020:INFO: Dataset: univ                Batch: 24/57	Loss 23.6556 (23.8337)
-2022-11-15 20:11:26,879:INFO: Dataset: univ                Batch: 25/57	Loss 23.8634 (23.8346)
-2022-11-15 20:11:27,758:INFO: Dataset: univ                Batch: 26/57	Loss 23.6693 (23.8279)
-2022-11-15 20:11:28,671:INFO: Dataset: univ                Batch: 27/57	Loss 23.5763 (23.8190)
-2022-11-15 20:11:29,652:INFO: Dataset: univ                Batch: 28/57	Loss 23.8036 (23.8184)
-2022-11-15 20:11:30,627:INFO: Dataset: univ                Batch: 29/57	Loss 23.8907 (23.8205)
-2022-11-15 20:11:31,604:INFO: Dataset: univ                Batch: 30/57	Loss 23.7668 (23.8186)
-2022-11-15 20:11:32,566:INFO: Dataset: univ                Batch: 31/57	Loss 23.6408 (23.8125)
-2022-11-15 20:11:33,541:INFO: Dataset: univ                Batch: 32/57	Loss 24.4293 (23.8274)
-2022-11-15 20:11:34,385:INFO: Dataset: univ                Batch: 33/57	Loss 23.8532 (23.8282)
-2022-11-15 20:11:35,260:INFO: Dataset: univ                Batch: 34/57	Loss 23.6285 (23.8218)
-2022-11-15 20:11:36,162:INFO: Dataset: univ                Batch: 35/57	Loss 23.3965 (23.8123)
-2022-11-15 20:11:37,083:INFO: Dataset: univ                Batch: 36/57	Loss 23.7804 (23.8112)
-2022-11-15 20:11:37,951:INFO: Dataset: univ                Batch: 37/57	Loss 23.9737 (23.8163)
-2022-11-15 20:11:38,830:INFO: Dataset: univ                Batch: 38/57	Loss 23.5691 (23.8088)
-2022-11-15 20:11:39,842:INFO: Dataset: univ                Batch: 39/57	Loss 23.9083 (23.8112)
-2022-11-15 20:11:40,851:INFO: Dataset: univ                Batch: 40/57	Loss 23.7205 (23.8086)
-2022-11-15 20:11:41,809:INFO: Dataset: univ                Batch: 41/57	Loss 23.9087 (23.8113)
-2022-11-15 20:11:42,798:INFO: Dataset: univ                Batch: 42/57	Loss 24.1141 (23.8175)
-2022-11-15 20:11:43,757:INFO: Dataset: univ                Batch: 43/57	Loss 23.7930 (23.8170)
-2022-11-15 20:11:44,577:INFO: Dataset: univ                Batch: 44/57	Loss 23.9791 (23.8206)
-2022-11-15 20:11:45,456:INFO: Dataset: univ                Batch: 45/57	Loss 23.7228 (23.8184)
-2022-11-15 20:11:46,338:INFO: Dataset: univ                Batch: 46/57	Loss 23.6518 (23.8145)
-2022-11-15 20:11:47,199:INFO: Dataset: univ                Batch: 47/57	Loss 24.1900 (23.8227)
-2022-11-15 20:11:48,095:INFO: Dataset: univ                Batch: 48/57	Loss 23.7558 (23.8209)
-2022-11-15 20:11:48,982:INFO: Dataset: univ                Batch: 49/57	Loss 23.6801 (23.8183)
-2022-11-15 20:11:49,950:INFO: Dataset: univ                Batch: 50/57	Loss 23.3674 (23.8100)
-2022-11-15 20:11:50,962:INFO: Dataset: univ                Batch: 51/57	Loss 23.9501 (23.8122)
-2022-11-15 20:11:51,963:INFO: Dataset: univ                Batch: 52/57	Loss 23.7994 (23.8120)
-2022-11-15 20:11:52,927:INFO: Dataset: univ                Batch: 53/57	Loss 23.4461 (23.8039)
-2022-11-15 20:11:53,872:INFO: Dataset: univ                Batch: 54/57	Loss 23.7354 (23.8026)
-2022-11-15 20:11:54,724:INFO: Dataset: univ                Batch: 55/57	Loss 23.4371 (23.7947)
-2022-11-15 20:11:55,606:INFO: Dataset: univ                Batch: 56/57	Loss 24.0799 (23.7998)
-2022-11-15 20:11:56,458:INFO: Dataset: univ                Batch: 57/57	Loss 17.5287 (23.7065)
-2022-11-15 20:12:06,658:INFO: Dataset: zara1               Batch:  1/32	Loss 23.2074 (23.2074)
-2022-11-15 20:12:07,599:INFO: Dataset: zara1               Batch:  2/32	Loss 23.1501 (23.1834)
-2022-11-15 20:12:08,452:INFO: Dataset: zara1               Batch:  3/32	Loss 23.8178 (23.3948)
-2022-11-15 20:12:09,383:INFO: Dataset: zara1               Batch:  4/32	Loss 23.8483 (23.5044)
-2022-11-15 20:12:10,336:INFO: Dataset: zara1               Batch:  5/32	Loss 23.3467 (23.4749)
-2022-11-15 20:12:11,331:INFO: Dataset: zara1               Batch:  6/32	Loss 23.9153 (23.5317)
-2022-11-15 20:12:12,335:INFO: Dataset: zara1               Batch:  7/32	Loss 24.2452 (23.6290)
-2022-11-15 20:12:13,372:INFO: Dataset: zara1               Batch:  8/32	Loss 23.6731 (23.6336)
-2022-11-15 20:12:14,268:INFO: Dataset: zara1               Batch:  9/32	Loss 24.2932 (23.7085)
-2022-11-15 20:12:15,075:INFO: Dataset: zara1               Batch: 10/32	Loss 23.4929 (23.6875)
-2022-11-15 20:12:15,932:INFO: Dataset: zara1               Batch: 11/32	Loss 23.1337 (23.6368)
-2022-11-15 20:12:16,774:INFO: Dataset: zara1               Batch: 12/32	Loss 23.3868 (23.6118)
-2022-11-15 20:12:17,633:INFO: Dataset: zara1               Batch: 13/32	Loss 23.6755 (23.6168)
-2022-11-15 20:12:18,523:INFO: Dataset: zara1               Batch: 14/32	Loss 23.4727 (23.6080)
-2022-11-15 20:12:19,452:INFO: Dataset: zara1               Batch: 15/32	Loss 23.0972 (23.5729)
-2022-11-15 20:12:20,410:INFO: Dataset: zara1               Batch: 16/32	Loss 24.1491 (23.6061)
-2022-11-15 20:12:21,387:INFO: Dataset: zara1               Batch: 17/32	Loss 23.8224 (23.6169)
-2022-11-15 20:12:22,387:INFO: Dataset: zara1               Batch: 18/32	Loss 23.2844 (23.5985)
-2022-11-15 20:12:23,425:INFO: Dataset: zara1               Batch: 19/32	Loss 23.3488 (23.5830)
-2022-11-15 20:12:24,289:INFO: Dataset: zara1               Batch: 20/32	Loss 23.9013 (23.5984)
-2022-11-15 20:12:25,422:INFO: Dataset: zara1               Batch: 21/32	Loss 24.4227 (23.6336)
-2022-11-15 20:12:26,205:INFO: Dataset: zara1               Batch: 22/32	Loss 23.8483 (23.6434)
-2022-11-15 20:12:27,146:INFO: Dataset: zara1               Batch: 23/32	Loss 23.3741 (23.6327)
-2022-11-15 20:12:27,980:INFO: Dataset: zara1               Batch: 24/32	Loss 23.6412 (23.6330)
-2022-11-15 20:12:28,894:INFO: Dataset: zara1               Batch: 25/32	Loss 23.8383 (23.6400)
-2022-11-15 20:12:29,678:INFO: Dataset: zara1               Batch: 26/32	Loss 23.5626 (23.6371)
-2022-11-15 20:12:30,498:INFO: Dataset: zara1               Batch: 27/32	Loss 24.0854 (23.6515)
-2022-11-15 20:12:31,358:INFO: Dataset: zara1               Batch: 28/32	Loss 23.6977 (23.6537)
-2022-11-15 20:12:32,234:INFO: Dataset: zara1               Batch: 29/32	Loss 23.9525 (23.6624)
-2022-11-15 20:12:33,552:INFO: Dataset: zara1               Batch: 30/32	Loss 23.6336 (23.6617)
-2022-11-15 20:12:34,518:INFO: Dataset: zara1               Batch: 31/32	Loss 23.3207 (23.6510)
-2022-11-15 20:12:35,381:INFO: Dataset: zara1               Batch: 32/32	Loss 10.6996 (23.3784)
-2022-11-15 20:13:04,607:INFO: Dataset: zara2               Batch:  1/72	Loss 23.9801 (23.9801)
-2022-11-15 20:13:05,545:INFO: Dataset: zara2               Batch:  2/72	Loss 23.2144 (23.6061)
-2022-11-15 20:13:06,483:INFO: Dataset: zara2               Batch:  3/72	Loss 23.3342 (23.5242)
-2022-11-15 20:13:07,417:INFO: Dataset: zara2               Batch:  4/72	Loss 23.0337 (23.3867)
-2022-11-15 20:13:08,332:INFO: Dataset: zara2               Batch:  5/72	Loss 23.3631 (23.3819)
-2022-11-15 20:13:09,268:INFO: Dataset: zara2               Batch:  6/72	Loss 24.4657 (23.5412)
-2022-11-15 20:13:10,083:INFO: Dataset: zara2               Batch:  7/72	Loss 23.1567 (23.4793)
-2022-11-15 20:13:10,955:INFO: Dataset: zara2               Batch:  8/72	Loss 23.2125 (23.4514)
-2022-11-15 20:13:11,795:INFO: Dataset: zara2               Batch:  9/72	Loss 23.3647 (23.4413)
-2022-11-15 20:13:12,664:INFO: Dataset: zara2               Batch: 10/72	Loss 23.4652 (23.4435)
-2022-11-15 20:13:13,532:INFO: Dataset: zara2               Batch: 11/72	Loss 23.4042 (23.4396)
-2022-11-15 20:13:14,416:INFO: Dataset: zara2               Batch: 12/72	Loss 23.8517 (23.4655)
-2022-11-15 20:13:15,343:INFO: Dataset: zara2               Batch: 13/72	Loss 23.9000 (23.5065)
-2022-11-15 20:13:16,281:INFO: Dataset: zara2               Batch: 14/72	Loss 23.9604 (23.5393)
-2022-11-15 20:13:17,206:INFO: Dataset: zara2               Batch: 15/72	Loss 23.5434 (23.5395)
-2022-11-15 20:13:18,178:INFO: Dataset: zara2               Batch: 16/72	Loss 22.9766 (23.5018)
-2022-11-15 20:13:19,120:INFO: Dataset: zara2               Batch: 17/72	Loss 23.0089 (23.4667)
-2022-11-15 20:13:19,948:INFO: Dataset: zara2               Batch: 18/72	Loss 23.4619 (23.4664)
-2022-11-15 20:13:20,765:INFO: Dataset: zara2               Batch: 19/72	Loss 23.3154 (23.4578)
-2022-11-15 20:13:21,586:INFO: Dataset: zara2               Batch: 20/72	Loss 23.3406 (23.4518)
-2022-11-15 20:13:22,456:INFO: Dataset: zara2               Batch: 21/72	Loss 23.7913 (23.4668)
-2022-11-15 20:13:23,298:INFO: Dataset: zara2               Batch: 22/72	Loss 24.1046 (23.4976)
-2022-11-15 20:13:24,116:INFO: Dataset: zara2               Batch: 23/72	Loss 23.5395 (23.4996)
-2022-11-15 20:13:25,166:INFO: Dataset: zara2               Batch: 24/72	Loss 23.5284 (23.5009)
-2022-11-15 20:13:26,140:INFO: Dataset: zara2               Batch: 25/72	Loss 23.4223 (23.4983)
-2022-11-15 20:13:27,074:INFO: Dataset: zara2               Batch: 26/72	Loss 23.5861 (23.5018)
-2022-11-15 20:13:28,009:INFO: Dataset: zara2               Batch: 27/72	Loss 23.7727 (23.5114)
-2022-11-15 20:13:28,930:INFO: Dataset: zara2               Batch: 28/72	Loss 23.6579 (23.5163)
-2022-11-15 20:13:29,840:INFO: Dataset: zara2               Batch: 29/72	Loss 23.6164 (23.5194)
-2022-11-15 20:13:30,689:INFO: Dataset: zara2               Batch: 30/72	Loss 23.6413 (23.5227)
-2022-11-15 20:13:31,548:INFO: Dataset: zara2               Batch: 31/72	Loss 23.5799 (23.5248)
-2022-11-15 20:13:32,395:INFO: Dataset: zara2               Batch: 32/72	Loss 23.6996 (23.5313)
-2022-11-15 20:13:33,250:INFO: Dataset: zara2               Batch: 33/72	Loss 23.2606 (23.5239)
-2022-11-15 20:13:34,086:INFO: Dataset: zara2               Batch: 34/72	Loss 23.9501 (23.5350)
-2022-11-15 20:13:35,001:INFO: Dataset: zara2               Batch: 35/72	Loss 23.9503 (23.5458)
-2022-11-15 20:13:36,079:INFO: Dataset: zara2               Batch: 36/72	Loss 22.8982 (23.5286)
-2022-11-15 20:13:37,063:INFO: Dataset: zara2               Batch: 37/72	Loss 23.6688 (23.5317)
-2022-11-15 20:13:38,032:INFO: Dataset: zara2               Batch: 38/72	Loss 23.6108 (23.5337)
-2022-11-15 20:13:38,977:INFO: Dataset: zara2               Batch: 39/72	Loss 23.3958 (23.5295)
-2022-11-15 20:13:39,890:INFO: Dataset: zara2               Batch: 40/72	Loss 23.7559 (23.5350)
-2022-11-15 20:13:40,732:INFO: Dataset: zara2               Batch: 41/72	Loss 23.3332 (23.5308)
-2022-11-15 20:13:41,605:INFO: Dataset: zara2               Batch: 42/72	Loss 23.5355 (23.5310)
-2022-11-15 20:13:42,500:INFO: Dataset: zara2               Batch: 43/72	Loss 23.0546 (23.5207)
-2022-11-15 20:13:43,382:INFO: Dataset: zara2               Batch: 44/72	Loss 23.5704 (23.5218)
-2022-11-15 20:13:44,205:INFO: Dataset: zara2               Batch: 45/72	Loss 23.4989 (23.5213)
-2022-11-15 20:13:45,098:INFO: Dataset: zara2               Batch: 46/72	Loss 23.5161 (23.5212)
-2022-11-15 20:13:46,031:INFO: Dataset: zara2               Batch: 47/72	Loss 23.1334 (23.5125)
-2022-11-15 20:13:46,985:INFO: Dataset: zara2               Batch: 48/72	Loss 23.3238 (23.5081)
-2022-11-15 20:13:47,940:INFO: Dataset: zara2               Batch: 49/72	Loss 23.4766 (23.5074)
-2022-11-15 20:13:48,864:INFO: Dataset: zara2               Batch: 50/72	Loss 23.5607 (23.5086)
-2022-11-15 20:13:49,809:INFO: Dataset: zara2               Batch: 51/72	Loss 23.7747 (23.5140)
-2022-11-15 20:13:50,622:INFO: Dataset: zara2               Batch: 52/72	Loss 23.5291 (23.5143)
-2022-11-15 20:13:51,466:INFO: Dataset: zara2               Batch: 53/72	Loss 23.2368 (23.5086)
-2022-11-15 20:13:52,297:INFO: Dataset: zara2               Batch: 54/72	Loss 23.3314 (23.5058)
-2022-11-15 20:13:53,148:INFO: Dataset: zara2               Batch: 55/72	Loss 23.0518 (23.4978)
-2022-11-15 20:13:53,985:INFO: Dataset: zara2               Batch: 56/72	Loss 23.5357 (23.4984)
-2022-11-15 20:13:54,824:INFO: Dataset: zara2               Batch: 57/72	Loss 23.0740 (23.4893)
-2022-11-15 20:13:55,818:INFO: Dataset: zara2               Batch: 58/72	Loss 23.3611 (23.4874)
-2022-11-15 20:13:56,746:INFO: Dataset: zara2               Batch: 59/72	Loss 23.4312 (23.4864)
-2022-11-15 20:13:57,689:INFO: Dataset: zara2               Batch: 60/72	Loss 23.2784 (23.4824)
-2022-11-15 20:13:58,648:INFO: Dataset: zara2               Batch: 61/72	Loss 23.5460 (23.4836)
-2022-11-15 20:13:59,560:INFO: Dataset: zara2               Batch: 62/72	Loss 23.2178 (23.4798)
-2022-11-15 20:14:00,446:INFO: Dataset: zara2               Batch: 63/72	Loss 23.2267 (23.4759)
-2022-11-15 20:14:01,266:INFO: Dataset: zara2               Batch: 64/72	Loss 23.2596 (23.4720)
-2022-11-15 20:14:02,096:INFO: Dataset: zara2               Batch: 65/72	Loss 23.5657 (23.4737)
-2022-11-15 20:14:02,930:INFO: Dataset: zara2               Batch: 66/72	Loss 22.7050 (23.4622)
-2022-11-15 20:14:03,865:INFO: Dataset: zara2               Batch: 67/72	Loss 23.1740 (23.4587)
-2022-11-15 20:14:04,763:INFO: Dataset: zara2               Batch: 68/72	Loss 23.8718 (23.4642)
-2022-11-15 20:14:05,695:INFO: Dataset: zara2               Batch: 69/72	Loss 23.1823 (23.4603)
-2022-11-15 20:14:06,648:INFO: Dataset: zara2               Batch: 70/72	Loss 22.7071 (23.4492)
-2022-11-15 20:14:07,612:INFO: Dataset: zara2               Batch: 71/72	Loss 23.3617 (23.4478)
-2022-11-15 20:14:08,517:INFO: Dataset: zara2               Batch: 72/72	Loss 10.2279 (23.3801)
-2022-11-15 20:14:09,591:INFO: - Computing ADE (validation)
-2022-11-15 20:14:20,253:INFO: 		 ADE on hotel                     dataset:	 2.146925449371338
-2022-11-15 20:14:31,016:INFO: 		 ADE on univ                      dataset:	 2.007312774658203
-2022-11-15 20:14:42,065:INFO: 		 ADE on zara1                     dataset:	 1.5389257669448853
-2022-11-15 20:14:54,139:INFO: 		 ADE on zara2                     dataset:	 1.871829867362976
-2022-11-15 20:14:54,139:INFO: Average validation:	ADE  1.9380	FDE  4.1701
-2022-11-15 20:14:54,141:INFO: - Computing ADE (training)
-2022-11-15 20:15:05,502:INFO: 		 ADE on hotel                     dataset:	 2.390205144882202
-2022-11-15 20:15:34,759:INFO: 		 ADE on univ                      dataset:	 1.93677818775177
-2022-11-15 20:15:46,403:INFO: 		 ADE on zara1                     dataset:	 1.9932146072387695
-2022-11-15 20:16:16,442:INFO: 		 ADE on zara2                     dataset:	 1.9488245248794556
-2022-11-15 20:16:16,442:INFO: Average training:	ADE  1.9543	FDE  4.1667
-2022-11-15 20:16:16,476:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_303.pth.tar
-2022-11-15 20:16:16,477:INFO: 
-===> EPOCH: 304 (P4)
-2022-11-15 20:16:16,478:INFO: - Computing loss (training)
-2022-11-15 20:16:25,799:INFO: Dataset: hotel               Batch:  1/15	Loss 24.6544 (24.6544)
-2022-11-15 20:16:26,746:INFO: Dataset: hotel               Batch:  2/15	Loss 25.0016 (24.8125)
-2022-11-15 20:16:27,652:INFO: Dataset: hotel               Batch:  3/15	Loss 24.1775 (24.6085)
-2022-11-15 20:16:28,619:INFO: Dataset: hotel               Batch:  4/15	Loss 24.2903 (24.5258)
-2022-11-15 20:16:29,542:INFO: Dataset: hotel               Batch:  5/15	Loss 25.1658 (24.6411)
-2022-11-15 20:16:30,607:INFO: Dataset: hotel               Batch:  6/15	Loss 23.7478 (24.5094)
-2022-11-15 20:16:31,666:INFO: Dataset: hotel               Batch:  7/15	Loss 25.1090 (24.5864)
-2022-11-15 20:16:32,612:INFO: Dataset: hotel               Batch:  8/15	Loss 24.6606 (24.5949)
-2022-11-15 20:16:33,427:INFO: Dataset: hotel               Batch:  9/15	Loss 24.4360 (24.5771)
-2022-11-15 20:16:34,294:INFO: Dataset: hotel               Batch: 10/15	Loss 23.8392 (24.4978)
-2022-11-15 20:16:35,146:INFO: Dataset: hotel               Batch: 11/15	Loss 24.6586 (24.5126)
-2022-11-15 20:16:36,047:INFO: Dataset: hotel               Batch: 12/15	Loss 24.7243 (24.5305)
-2022-11-15 20:16:36,889:INFO: Dataset: hotel               Batch: 13/15	Loss 24.1885 (24.5020)
-2022-11-15 20:16:37,842:INFO: Dataset: hotel               Batch: 14/15	Loss 23.8083 (24.4548)
-2022-11-15 20:16:38,641:INFO: Dataset: hotel               Batch: 15/15	Loss 10.6151 (24.0166)
-2022-11-15 20:17:06,343:INFO: Dataset: univ                Batch:  1/57	Loss 23.6571 (23.6571)
-2022-11-15 20:17:07,328:INFO: Dataset: univ                Batch:  2/57	Loss 23.9379 (23.7877)
-2022-11-15 20:17:08,140:INFO: Dataset: univ                Batch:  3/57	Loss 23.9189 (23.8287)
-2022-11-15 20:17:08,991:INFO: Dataset: univ                Batch:  4/57	Loss 24.4008 (23.9542)
-2022-11-15 20:17:09,862:INFO: Dataset: univ                Batch:  5/57	Loss 24.0233 (23.9684)
-2022-11-15 20:17:10,856:INFO: Dataset: univ                Batch:  6/57	Loss 23.3902 (23.8657)
-2022-11-15 20:17:11,955:INFO: Dataset: univ                Batch:  7/57	Loss 23.7609 (23.8521)
-2022-11-15 20:17:12,861:INFO: Dataset: univ                Batch:  8/57	Loss 23.0833 (23.7509)
-2022-11-15 20:17:14,009:INFO: Dataset: univ                Batch:  9/57	Loss 23.5552 (23.7302)
-2022-11-15 20:17:14,984:INFO: Dataset: univ                Batch: 10/57	Loss 23.5169 (23.7058)
-2022-11-15 20:17:16,040:INFO: Dataset: univ                Batch: 11/57	Loss 23.6102 (23.6970)
-2022-11-15 20:17:16,943:INFO: Dataset: univ                Batch: 12/57	Loss 23.6310 (23.6915)
-2022-11-15 20:17:17,903:INFO: Dataset: univ                Batch: 13/57	Loss 23.2172 (23.6542)
-2022-11-15 20:17:19,049:INFO: Dataset: univ                Batch: 14/57	Loss 23.6687 (23.6553)
-2022-11-15 20:17:20,172:INFO: Dataset: univ                Batch: 15/57	Loss 23.1590 (23.6194)
-2022-11-15 20:17:21,069:INFO: Dataset: univ                Batch: 16/57	Loss 23.7662 (23.6281)
-2022-11-15 20:17:21,975:INFO: Dataset: univ                Batch: 17/57	Loss 23.7161 (23.6343)
-2022-11-15 20:17:22,922:INFO: Dataset: univ                Batch: 18/57	Loss 23.5197 (23.6275)
-2022-11-15 20:17:23,940:INFO: Dataset: univ                Batch: 19/57	Loss 23.9607 (23.6427)
-2022-11-15 20:17:24,918:INFO: Dataset: univ                Batch: 20/57	Loss 24.2066 (23.6623)
-2022-11-15 20:17:25,937:INFO: Dataset: univ                Batch: 21/57	Loss 23.0141 (23.6261)
-2022-11-15 20:17:26,896:INFO: Dataset: univ                Batch: 22/57	Loss 23.2401 (23.6033)
-2022-11-15 20:17:27,940:INFO: Dataset: univ                Batch: 23/57	Loss 23.3423 (23.5899)
-2022-11-15 20:17:28,756:INFO: Dataset: univ                Batch: 24/57	Loss 23.2392 (23.5751)
-2022-11-15 20:17:29,635:INFO: Dataset: univ                Batch: 25/57	Loss 23.5895 (23.5758)
-2022-11-15 20:17:30,521:INFO: Dataset: univ                Batch: 26/57	Loss 23.2454 (23.5596)
-2022-11-15 20:17:31,419:INFO: Dataset: univ                Batch: 27/57	Loss 23.5443 (23.5591)
-2022-11-15 20:17:32,313:INFO: Dataset: univ                Batch: 28/57	Loss 23.3573 (23.5524)
-2022-11-15 20:17:33,345:INFO: Dataset: univ                Batch: 29/57	Loss 23.3351 (23.5446)
-2022-11-15 20:17:34,355:INFO: Dataset: univ                Batch: 30/57	Loss 23.5445 (23.5446)
-2022-11-15 20:17:35,329:INFO: Dataset: univ                Batch: 31/57	Loss 23.2751 (23.5349)
-2022-11-15 20:17:36,327:INFO: Dataset: univ                Batch: 32/57	Loss 23.4392 (23.5320)
-2022-11-15 20:17:37,317:INFO: Dataset: univ                Batch: 33/57	Loss 23.4176 (23.5295)
-2022-11-15 20:17:38,229:INFO: Dataset: univ                Batch: 34/57	Loss 23.4905 (23.5283)
-2022-11-15 20:17:39,104:INFO: Dataset: univ                Batch: 35/57	Loss 23.1306 (23.5161)
-2022-11-15 20:17:39,971:INFO: Dataset: univ                Batch: 36/57	Loss 23.2874 (23.5088)
-2022-11-15 20:17:40,834:INFO: Dataset: univ                Batch: 37/57	Loss 23.2518 (23.5024)
-2022-11-15 20:17:41,739:INFO: Dataset: univ                Batch: 38/57	Loss 23.0961 (23.4896)
-2022-11-15 20:17:42,626:INFO: Dataset: univ                Batch: 39/57	Loss 23.1021 (23.4805)
-2022-11-15 20:17:43,587:INFO: Dataset: univ                Batch: 40/57	Loss 23.5483 (23.4819)
-2022-11-15 20:17:44,548:INFO: Dataset: univ                Batch: 41/57	Loss 23.2837 (23.4771)
-2022-11-15 20:17:45,572:INFO: Dataset: univ                Batch: 42/57	Loss 23.9340 (23.4851)
-2022-11-15 20:17:46,552:INFO: Dataset: univ                Batch: 43/57	Loss 23.2661 (23.4797)
-2022-11-15 20:17:47,533:INFO: Dataset: univ                Batch: 44/57	Loss 23.5700 (23.4820)
-2022-11-15 20:17:48,442:INFO: Dataset: univ                Batch: 45/57	Loss 23.0002 (23.4728)
-2022-11-15 20:17:49,302:INFO: Dataset: univ                Batch: 46/57	Loss 23.5617 (23.4748)
-2022-11-15 20:17:50,238:INFO: Dataset: univ                Batch: 47/57	Loss 23.4902 (23.4752)
-2022-11-15 20:17:51,112:INFO: Dataset: univ                Batch: 48/57	Loss 23.1393 (23.4670)
-2022-11-15 20:17:51,985:INFO: Dataset: univ                Batch: 49/57	Loss 23.3127 (23.4638)
-2022-11-15 20:17:52,858:INFO: Dataset: univ                Batch: 50/57	Loss 23.2653 (23.4598)
-2022-11-15 20:17:53,848:INFO: Dataset: univ                Batch: 51/57	Loss 23.6932 (23.4630)
-2022-11-15 20:17:54,869:INFO: Dataset: univ                Batch: 52/57	Loss 23.0288 (23.4545)
-2022-11-15 20:17:55,949:INFO: Dataset: univ                Batch: 53/57	Loss 23.2417 (23.4501)
-2022-11-15 20:17:56,915:INFO: Dataset: univ                Batch: 54/57	Loss 23.4090 (23.4494)
-2022-11-15 20:17:57,896:INFO: Dataset: univ                Batch: 55/57	Loss 23.3789 (23.4480)
-2022-11-15 20:17:58,763:INFO: Dataset: univ                Batch: 56/57	Loss 23.0903 (23.4402)
-2022-11-15 20:17:59,620:INFO: Dataset: univ                Batch: 57/57	Loss 18.1856 (23.3757)
-2022-11-15 20:18:10,063:INFO: Dataset: zara1               Batch:  1/32	Loss 23.6900 (23.6900)
-2022-11-15 20:18:11,057:INFO: Dataset: zara1               Batch:  2/32	Loss 23.6972 (23.6939)
-2022-11-15 20:18:11,933:INFO: Dataset: zara1               Batch:  3/32	Loss 23.4966 (23.6310)
-2022-11-15 20:18:12,782:INFO: Dataset: zara1               Batch:  4/32	Loss 23.3463 (23.5818)
-2022-11-15 20:18:13,670:INFO: Dataset: zara1               Batch:  5/32	Loss 23.2633 (23.5114)
-2022-11-15 20:18:14,604:INFO: Dataset: zara1               Batch:  6/32	Loss 24.1434 (23.6119)
-2022-11-15 20:18:15,553:INFO: Dataset: zara1               Batch:  7/32	Loss 24.0470 (23.6742)
-2022-11-15 20:18:16,520:INFO: Dataset: zara1               Batch:  8/32	Loss 23.5446 (23.6605)
-2022-11-15 20:18:17,452:INFO: Dataset: zara1               Batch:  9/32	Loss 23.3572 (23.6241)
-2022-11-15 20:18:18,422:INFO: Dataset: zara1               Batch: 10/32	Loss 22.9927 (23.5725)
-2022-11-15 20:18:19,254:INFO: Dataset: zara1               Batch: 11/32	Loss 22.6613 (23.4821)
-2022-11-15 20:18:20,106:INFO: Dataset: zara1               Batch: 12/32	Loss 23.9233 (23.5123)
-2022-11-15 20:18:20,956:INFO: Dataset: zara1               Batch: 13/32	Loss 23.1905 (23.4870)
-2022-11-15 20:18:21,759:INFO: Dataset: zara1               Batch: 14/32	Loss 23.0180 (23.4538)
-2022-11-15 20:18:22,576:INFO: Dataset: zara1               Batch: 15/32	Loss 23.6987 (23.4720)
-2022-11-15 20:18:23,482:INFO: Dataset: zara1               Batch: 16/32	Loss 22.9266 (23.4359)
-2022-11-15 20:18:24,451:INFO: Dataset: zara1               Batch: 17/32	Loss 24.1676 (23.4753)
-2022-11-15 20:18:25,421:INFO: Dataset: zara1               Batch: 18/32	Loss 23.1812 (23.4595)
-2022-11-15 20:18:26,398:INFO: Dataset: zara1               Batch: 19/32	Loss 23.8484 (23.4764)
-2022-11-15 20:18:27,356:INFO: Dataset: zara1               Batch: 20/32	Loss 23.3786 (23.4710)
-2022-11-15 20:18:28,318:INFO: Dataset: zara1               Batch: 21/32	Loss 23.0895 (23.4490)
-2022-11-15 20:18:29,194:INFO: Dataset: zara1               Batch: 22/32	Loss 23.5929 (23.4552)
-2022-11-15 20:18:30,039:INFO: Dataset: zara1               Batch: 23/32	Loss 23.3967 (23.4530)
-2022-11-15 20:18:30,914:INFO: Dataset: zara1               Batch: 24/32	Loss 23.6294 (23.4612)
-2022-11-15 20:18:31,746:INFO: Dataset: zara1               Batch: 25/32	Loss 23.4853 (23.4623)
-2022-11-15 20:18:32,570:INFO: Dataset: zara1               Batch: 26/32	Loss 23.1969 (23.4514)
-2022-11-15 20:18:33,426:INFO: Dataset: zara1               Batch: 27/32	Loss 23.7668 (23.4615)
-2022-11-15 20:18:34,407:INFO: Dataset: zara1               Batch: 28/32	Loss 23.4317 (23.4605)
-2022-11-15 20:18:35,354:INFO: Dataset: zara1               Batch: 29/32	Loss 23.1828 (23.4506)
-2022-11-15 20:18:36,319:INFO: Dataset: zara1               Batch: 30/32	Loss 23.5177 (23.4530)
-2022-11-15 20:18:37,286:INFO: Dataset: zara1               Batch: 31/32	Loss 22.8812 (23.4358)
-2022-11-15 20:18:38,235:INFO: Dataset: zara1               Batch: 32/32	Loss 10.0947 (23.1901)
-2022-11-15 20:19:07,176:INFO: Dataset: zara2               Batch:  1/72	Loss 23.2048 (23.2048)
-2022-11-15 20:19:08,115:INFO: Dataset: zara2               Batch:  2/72	Loss 23.6615 (23.4376)
-2022-11-15 20:19:09,067:INFO: Dataset: zara2               Batch:  3/72	Loss 23.4658 (23.4482)
-2022-11-15 20:19:09,947:INFO: Dataset: zara2               Batch:  4/72	Loss 23.0827 (23.3725)
-2022-11-15 20:19:10,776:INFO: Dataset: zara2               Batch:  5/72	Loss 23.1541 (23.3295)
-2022-11-15 20:19:11,633:INFO: Dataset: zara2               Batch:  6/72	Loss 23.7668 (23.4084)
-2022-11-15 20:19:12,499:INFO: Dataset: zara2               Batch:  7/72	Loss 23.2962 (23.3902)
-2022-11-15 20:19:13,345:INFO: Dataset: zara2               Batch:  8/72	Loss 23.6002 (23.4150)
-2022-11-15 20:19:14,182:INFO: Dataset: zara2               Batch:  9/72	Loss 23.1813 (23.3852)
-2022-11-15 20:19:15,248:INFO: Dataset: zara2               Batch: 10/72	Loss 22.9093 (23.3310)
-2022-11-15 20:19:16,201:INFO: Dataset: zara2               Batch: 11/72	Loss 23.1902 (23.3201)
-2022-11-15 20:19:17,154:INFO: Dataset: zara2               Batch: 12/72	Loss 23.7244 (23.3522)
-2022-11-15 20:19:18,115:INFO: Dataset: zara2               Batch: 13/72	Loss 23.0470 (23.3279)
-2022-11-15 20:19:19,030:INFO: Dataset: zara2               Batch: 14/72	Loss 23.0603 (23.3065)
-2022-11-15 20:19:19,960:INFO: Dataset: zara2               Batch: 15/72	Loss 23.3150 (23.3070)
-2022-11-15 20:19:20,836:INFO: Dataset: zara2               Batch: 16/72	Loss 22.8905 (23.2859)
-2022-11-15 20:19:21,737:INFO: Dataset: zara2               Batch: 17/72	Loss 22.7834 (23.2571)
-2022-11-15 20:19:22,626:INFO: Dataset: zara2               Batch: 18/72	Loss 22.9960 (23.2444)
-2022-11-15 20:19:23,486:INFO: Dataset: zara2               Batch: 19/72	Loss 23.0587 (23.2344)
-2022-11-15 20:19:24,706:INFO: Dataset: zara2               Batch: 20/72	Loss 22.8872 (23.2162)
-2022-11-15 20:19:25,795:INFO: Dataset: zara2               Batch: 21/72	Loss 23.3820 (23.2227)
-2022-11-15 20:19:26,639:INFO: Dataset: zara2               Batch: 22/72	Loss 23.2016 (23.2219)
-2022-11-15 20:19:27,509:INFO: Dataset: zara2               Batch: 23/72	Loss 22.7607 (23.1982)
-2022-11-15 20:19:28,366:INFO: Dataset: zara2               Batch: 24/72	Loss 22.9421 (23.1880)
-2022-11-15 20:19:29,198:INFO: Dataset: zara2               Batch: 25/72	Loss 23.0537 (23.1810)
-2022-11-15 20:19:30,027:INFO: Dataset: zara2               Batch: 26/72	Loss 22.7733 (23.1646)
-2022-11-15 20:19:30,928:INFO: Dataset: zara2               Batch: 27/72	Loss 22.9621 (23.1569)
-2022-11-15 20:19:31,757:INFO: Dataset: zara2               Batch: 28/72	Loss 23.0183 (23.1521)
-2022-11-15 20:19:32,554:INFO: Dataset: zara2               Batch: 29/72	Loss 22.6934 (23.1339)
-2022-11-15 20:19:33,433:INFO: Dataset: zara2               Batch: 30/72	Loss 23.0698 (23.1322)
-2022-11-15 20:19:34,268:INFO: Dataset: zara2               Batch: 31/72	Loss 22.9379 (23.1242)
-2022-11-15 20:19:35,099:INFO: Dataset: zara2               Batch: 32/72	Loss 22.7216 (23.1075)
-2022-11-15 20:19:36,030:INFO: Dataset: zara2               Batch: 33/72	Loss 23.1868 (23.1102)
-2022-11-15 20:19:37,049:INFO: Dataset: zara2               Batch: 34/72	Loss 22.8516 (23.1038)
-2022-11-15 20:19:38,263:INFO: Dataset: zara2               Batch: 35/72	Loss 22.7106 (23.0937)
-2022-11-15 20:19:39,232:INFO: Dataset: zara2               Batch: 36/72	Loss 23.4548 (23.1041)
-2022-11-15 20:19:40,219:INFO: Dataset: zara2               Batch: 37/72	Loss 23.7314 (23.1192)
-2022-11-15 20:19:41,244:INFO: Dataset: zara2               Batch: 38/72	Loss 23.3654 (23.1249)
-2022-11-15 20:19:42,206:INFO: Dataset: zara2               Batch: 39/72	Loss 22.9049 (23.1185)
-2022-11-15 20:19:43,039:INFO: Dataset: zara2               Batch: 40/72	Loss 22.9298 (23.1136)
-2022-11-15 20:19:43,884:INFO: Dataset: zara2               Batch: 41/72	Loss 24.3858 (23.1386)
-2022-11-15 20:19:44,761:INFO: Dataset: zara2               Batch: 42/72	Loss 23.3080 (23.1421)
-2022-11-15 20:19:45,662:INFO: Dataset: zara2               Batch: 43/72	Loss 22.5846 (23.1274)
-2022-11-15 20:19:46,533:INFO: Dataset: zara2               Batch: 44/72	Loss 23.2882 (23.1311)
-2022-11-15 20:19:47,437:INFO: Dataset: zara2               Batch: 45/72	Loss 22.7379 (23.1211)
-2022-11-15 20:19:48,446:INFO: Dataset: zara2               Batch: 46/72	Loss 23.3650 (23.1276)
-2022-11-15 20:19:49,412:INFO: Dataset: zara2               Batch: 47/72	Loss 22.6161 (23.1153)
-2022-11-15 20:19:50,394:INFO: Dataset: zara2               Batch: 48/72	Loss 23.0493 (23.1141)
-2022-11-15 20:19:51,340:INFO: Dataset: zara2               Batch: 49/72	Loss 22.7799 (23.1068)
-2022-11-15 20:19:52,334:INFO: Dataset: zara2               Batch: 50/72	Loss 23.0204 (23.1052)
-2022-11-15 20:19:53,191:INFO: Dataset: zara2               Batch: 51/72	Loss 22.9779 (23.1024)
-2022-11-15 20:19:54,016:INFO: Dataset: zara2               Batch: 52/72	Loss 22.9727 (23.0994)
-2022-11-15 20:19:54,898:INFO: Dataset: zara2               Batch: 53/72	Loss 22.8882 (23.0956)
-2022-11-15 20:19:55,826:INFO: Dataset: zara2               Batch: 54/72	Loss 23.8215 (23.1101)
-2022-11-15 20:19:56,680:INFO: Dataset: zara2               Batch: 55/72	Loss 22.5589 (23.0987)
-2022-11-15 20:19:57,579:INFO: Dataset: zara2               Batch: 56/72	Loss 22.7547 (23.0915)
-2022-11-15 20:19:58,548:INFO: Dataset: zara2               Batch: 57/72	Loss 23.6544 (23.0999)
-2022-11-15 20:19:59,535:INFO: Dataset: zara2               Batch: 58/72	Loss 23.0789 (23.0996)
-2022-11-15 20:20:00,502:INFO: Dataset: zara2               Batch: 59/72	Loss 23.0400 (23.0985)
-2022-11-15 20:20:01,471:INFO: Dataset: zara2               Batch: 60/72	Loss 23.2531 (23.1013)
-2022-11-15 20:20:02,418:INFO: Dataset: zara2               Batch: 61/72	Loss 22.9269 (23.0984)
-2022-11-15 20:20:03,255:INFO: Dataset: zara2               Batch: 62/72	Loss 23.1839 (23.1000)
-2022-11-15 20:20:04,171:INFO: Dataset: zara2               Batch: 63/72	Loss 23.1333 (23.1004)
-2022-11-15 20:20:05,027:INFO: Dataset: zara2               Batch: 64/72	Loss 23.0461 (23.0996)
-2022-11-15 20:20:05,935:INFO: Dataset: zara2               Batch: 65/72	Loss 23.1449 (23.1003)
-2022-11-15 20:20:06,828:INFO: Dataset: zara2               Batch: 66/72	Loss 22.6375 (23.0927)
-2022-11-15 20:20:07,683:INFO: Dataset: zara2               Batch: 67/72	Loss 23.4004 (23.0970)
-2022-11-15 20:20:08,716:INFO: Dataset: zara2               Batch: 68/72	Loss 23.6346 (23.1063)
-2022-11-15 20:20:09,682:INFO: Dataset: zara2               Batch: 69/72	Loss 23.1896 (23.1078)
-2022-11-15 20:20:10,670:INFO: Dataset: zara2               Batch: 70/72	Loss 22.8105 (23.1036)
-2022-11-15 20:20:11,671:INFO: Dataset: zara2               Batch: 71/72	Loss 22.8286 (23.0995)
-2022-11-15 20:20:12,612:INFO: Dataset: zara2               Batch: 72/72	Loss 10.1441 (22.9924)
-2022-11-15 20:20:13,624:INFO: - Computing ADE (validation)
-2022-11-15 20:20:24,053:INFO: 		 ADE on hotel                     dataset:	 2.3086774349212646
-2022-11-15 20:20:35,218:INFO: 		 ADE on univ                      dataset:	 2.1006107330322266
-2022-11-15 20:20:45,663:INFO: 		 ADE on zara1                     dataset:	 1.6908961534500122
-2022-11-15 20:20:56,827:INFO: 		 ADE on zara2                     dataset:	 2.001817464828491
-2022-11-15 20:20:56,827:INFO: Average validation:	ADE  2.0519	FDE  4.4033
-2022-11-15 20:20:56,828:INFO: - Computing ADE (training)
-2022-11-15 20:21:07,709:INFO: 		 ADE on hotel                     dataset:	 2.4721243381500244
-2022-11-15 20:21:37,336:INFO: 		 ADE on univ                      dataset:	 2.0618627071380615
-2022-11-15 20:21:48,889:INFO: 		 ADE on zara1                     dataset:	 2.1023714542388916
-2022-11-15 20:22:18,750:INFO: 		 ADE on zara2                     dataset:	 2.058004379272461
-2022-11-15 20:22:18,751:INFO: Average training:	ADE  2.0741	FDE  4.4013
-2022-11-15 20:22:18,785:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_304.pth.tar
-2022-11-15 20:22:18,786:INFO: 
-===> EPOCH: 305 (P4)
-2022-11-15 20:22:18,787:INFO: - Computing loss (training)
-2022-11-15 20:22:28,074:INFO: Dataset: hotel               Batch:  1/15	Loss 25.4372 (25.4372)
-2022-11-15 20:22:29,006:INFO: Dataset: hotel               Batch:  2/15	Loss 23.5376 (24.4264)
-2022-11-15 20:22:29,863:INFO: Dataset: hotel               Batch:  3/15	Loss 24.4274 (24.4267)
-2022-11-15 20:22:30,842:INFO: Dataset: hotel               Batch:  4/15	Loss 24.7357 (24.5108)
-2022-11-15 20:22:31,788:INFO: Dataset: hotel               Batch:  5/15	Loss 23.9320 (24.4153)
-2022-11-15 20:22:32,735:INFO: Dataset: hotel               Batch:  6/15	Loss 23.2839 (24.2373)
-2022-11-15 20:22:33,690:INFO: Dataset: hotel               Batch:  7/15	Loss 23.7612 (24.1769)
-2022-11-15 20:22:34,740:INFO: Dataset: hotel               Batch:  8/15	Loss 22.8583 (24.0176)
-2022-11-15 20:22:35,566:INFO: Dataset: hotel               Batch:  9/15	Loss 24.7891 (24.1065)
-2022-11-15 20:22:36,394:INFO: Dataset: hotel               Batch: 10/15	Loss 23.9042 (24.0863)
-2022-11-15 20:22:37,227:INFO: Dataset: hotel               Batch: 11/15	Loss 24.5207 (24.1238)
-2022-11-15 20:22:38,097:INFO: Dataset: hotel               Batch: 12/15	Loss 24.1380 (24.1249)
-2022-11-15 20:22:38,940:INFO: Dataset: hotel               Batch: 13/15	Loss 23.4653 (24.0773)
-2022-11-15 20:22:39,794:INFO: Dataset: hotel               Batch: 14/15	Loss 24.4452 (24.1035)
-2022-11-15 20:22:40,739:INFO: Dataset: hotel               Batch: 15/15	Loss 10.4558 (23.6173)
-2022-11-15 20:23:08,259:INFO: Dataset: univ                Batch:  1/57	Loss 23.2261 (23.2261)
-2022-11-15 20:23:09,222:INFO: Dataset: univ                Batch:  2/57	Loss 23.5259 (23.3780)
-2022-11-15 20:23:10,141:INFO: Dataset: univ                Batch:  3/57	Loss 24.8396 (23.8363)
-2022-11-15 20:23:11,089:INFO: Dataset: univ                Batch:  4/57	Loss 23.5910 (23.7850)
-2022-11-15 20:23:12,058:INFO: Dataset: univ                Batch:  5/57	Loss 23.3431 (23.6882)
-2022-11-15 20:23:13,022:INFO: Dataset: univ                Batch:  6/57	Loss 23.6811 (23.6868)
-2022-11-15 20:23:14,007:INFO: Dataset: univ                Batch:  7/57	Loss 24.0427 (23.7340)
-2022-11-15 20:23:14,976:INFO: Dataset: univ                Batch:  8/57	Loss 23.3601 (23.6737)
-2022-11-15 20:23:15,924:INFO: Dataset: univ                Batch:  9/57	Loss 23.2102 (23.6134)
-2022-11-15 20:23:16,767:INFO: Dataset: univ                Batch: 10/57	Loss 23.4994 (23.6025)
-2022-11-15 20:23:17,731:INFO: Dataset: univ                Batch: 11/57	Loss 23.1904 (23.5600)
-2022-11-15 20:23:18,712:INFO: Dataset: univ                Batch: 12/57	Loss 23.2287 (23.5331)
-2022-11-15 20:23:19,622:INFO: Dataset: univ                Batch: 13/57	Loss 23.0346 (23.4877)
-2022-11-15 20:23:20,472:INFO: Dataset: univ                Batch: 14/57	Loss 22.9220 (23.4397)
-2022-11-15 20:23:21,456:INFO: Dataset: univ                Batch: 15/57	Loss 23.2200 (23.4236)
-2022-11-15 20:23:22,418:INFO: Dataset: univ                Batch: 16/57	Loss 23.0917 (23.4043)
-2022-11-15 20:23:23,389:INFO: Dataset: univ                Batch: 17/57	Loss 22.9262 (23.3743)
-2022-11-15 20:23:24,351:INFO: Dataset: univ                Batch: 18/57	Loss 22.9936 (23.3506)
-2022-11-15 20:23:25,326:INFO: Dataset: univ                Batch: 19/57	Loss 23.4419 (23.3555)
-2022-11-15 20:23:26,244:INFO: Dataset: univ                Batch: 20/57	Loss 23.1407 (23.3458)
-2022-11-15 20:23:27,106:INFO: Dataset: univ                Batch: 21/57	Loss 23.1268 (23.3340)
-2022-11-15 20:23:28,134:INFO: Dataset: univ                Batch: 22/57	Loss 22.7022 (23.3029)
-2022-11-15 20:23:28,999:INFO: Dataset: univ                Batch: 23/57	Loss 23.1695 (23.2968)
-2022-11-15 20:23:29,883:INFO: Dataset: univ                Batch: 24/57	Loss 23.2164 (23.2934)
-2022-11-15 20:23:30,768:INFO: Dataset: univ                Batch: 25/57	Loss 22.9615 (23.2813)
-2022-11-15 20:23:31,777:INFO: Dataset: univ                Batch: 26/57	Loss 23.4564 (23.2882)
-2022-11-15 20:23:32,733:INFO: Dataset: univ                Batch: 27/57	Loss 23.2511 (23.2868)
-2022-11-15 20:23:33,703:INFO: Dataset: univ                Batch: 28/57	Loss 23.2180 (23.2839)
-2022-11-15 20:23:34,665:INFO: Dataset: univ                Batch: 29/57	Loss 23.6676 (23.2973)
-2022-11-15 20:23:35,627:INFO: Dataset: univ                Batch: 30/57	Loss 23.1945 (23.2941)
-2022-11-15 20:23:36,524:INFO: Dataset: univ                Batch: 31/57	Loss 23.0499 (23.2875)
-2022-11-15 20:23:37,428:INFO: Dataset: univ                Batch: 32/57	Loss 23.8202 (23.3017)
-2022-11-15 20:23:38,322:INFO: Dataset: univ                Batch: 33/57	Loss 23.3700 (23.3037)
-2022-11-15 20:23:39,221:INFO: Dataset: univ                Batch: 34/57	Loss 23.3399 (23.3049)
-2022-11-15 20:23:40,133:INFO: Dataset: univ                Batch: 35/57	Loss 23.4604 (23.3087)
-2022-11-15 20:23:41,047:INFO: Dataset: univ                Batch: 36/57	Loss 23.6891 (23.3206)
-2022-11-15 20:23:42,039:INFO: Dataset: univ                Batch: 37/57	Loss 23.3148 (23.3204)
-2022-11-15 20:23:43,027:INFO: Dataset: univ                Batch: 38/57	Loss 23.9136 (23.3326)
-2022-11-15 20:23:44,003:INFO: Dataset: univ                Batch: 39/57	Loss 23.4600 (23.3359)
-2022-11-15 20:23:44,991:INFO: Dataset: univ                Batch: 40/57	Loss 23.6511 (23.3435)
-2022-11-15 20:23:45,992:INFO: Dataset: univ                Batch: 41/57	Loss 23.7262 (23.3527)
-2022-11-15 20:23:46,937:INFO: Dataset: univ                Batch: 42/57	Loss 24.0339 (23.3696)
-2022-11-15 20:23:47,826:INFO: Dataset: univ                Batch: 43/57	Loss 23.5560 (23.3742)
-2022-11-15 20:23:48,725:INFO: Dataset: univ                Batch: 44/57	Loss 23.3987 (23.3748)
-2022-11-15 20:23:49,648:INFO: Dataset: univ                Batch: 45/57	Loss 23.3845 (23.3751)
-2022-11-15 20:23:50,524:INFO: Dataset: univ                Batch: 46/57	Loss 23.2581 (23.3726)
-2022-11-15 20:23:51,459:INFO: Dataset: univ                Batch: 47/57	Loss 23.2128 (23.3682)
-2022-11-15 20:23:52,490:INFO: Dataset: univ                Batch: 48/57	Loss 23.4183 (23.3693)
-2022-11-15 20:23:53,454:INFO: Dataset: univ                Batch: 49/57	Loss 23.0999 (23.3637)
-2022-11-15 20:23:54,438:INFO: Dataset: univ                Batch: 50/57	Loss 23.5009 (23.3658)
-2022-11-15 20:23:55,421:INFO: Dataset: univ                Batch: 51/57	Loss 23.1655 (23.3628)
-2022-11-15 20:23:56,365:INFO: Dataset: univ                Batch: 52/57	Loss 23.1885 (23.3595)
-2022-11-15 20:23:57,212:INFO: Dataset: univ                Batch: 53/57	Loss 23.4831 (23.3620)
-2022-11-15 20:23:58,139:INFO: Dataset: univ                Batch: 54/57	Loss 23.3171 (23.3612)
-2022-11-15 20:23:59,041:INFO: Dataset: univ                Batch: 55/57	Loss 22.9439 (23.3528)
-2022-11-15 20:23:59,932:INFO: Dataset: univ                Batch: 56/57	Loss 23.1083 (23.3487)
-2022-11-15 20:24:00,808:INFO: Dataset: univ                Batch: 57/57	Loss 17.4809 (23.2820)
-2022-11-15 20:24:11,182:INFO: Dataset: zara1               Batch:  1/32	Loss 23.0012 (23.0012)
-2022-11-15 20:24:12,290:INFO: Dataset: zara1               Batch:  2/32	Loss 22.9289 (22.9666)
-2022-11-15 20:24:13,252:INFO: Dataset: zara1               Batch:  3/32	Loss 22.7311 (22.8833)
-2022-11-15 20:24:14,248:INFO: Dataset: zara1               Batch:  4/32	Loss 23.0670 (22.9225)
-2022-11-15 20:24:15,187:INFO: Dataset: zara1               Batch:  5/32	Loss 23.3550 (23.0131)
-2022-11-15 20:24:16,126:INFO: Dataset: zara1               Batch:  6/32	Loss 24.1601 (23.2199)
-2022-11-15 20:24:17,033:INFO: Dataset: zara1               Batch:  7/32	Loss 23.5367 (23.2670)
-2022-11-15 20:24:17,863:INFO: Dataset: zara1               Batch:  8/32	Loss 23.3388 (23.2754)
-2022-11-15 20:24:18,707:INFO: Dataset: zara1               Batch:  9/32	Loss 23.4778 (23.3012)
-2022-11-15 20:24:19,565:INFO: Dataset: zara1               Batch: 10/32	Loss 22.9523 (23.2711)
-2022-11-15 20:24:20,417:INFO: Dataset: zara1               Batch: 11/32	Loss 22.9030 (23.2367)
-2022-11-15 20:24:21,256:INFO: Dataset: zara1               Batch: 12/32	Loss 23.5210 (23.2603)
-2022-11-15 20:24:22,170:INFO: Dataset: zara1               Batch: 13/32	Loss 23.6336 (23.2920)
-2022-11-15 20:24:23,124:INFO: Dataset: zara1               Batch: 14/32	Loss 22.9543 (23.2659)
-2022-11-15 20:24:24,079:INFO: Dataset: zara1               Batch: 15/32	Loss 22.9544 (23.2485)
-2022-11-15 20:24:25,050:INFO: Dataset: zara1               Batch: 16/32	Loss 23.0967 (23.2406)
-2022-11-15 20:24:26,037:INFO: Dataset: zara1               Batch: 17/32	Loss 23.4883 (23.2600)
-2022-11-15 20:24:26,965:INFO: Dataset: zara1               Batch: 18/32	Loss 22.9344 (23.2375)
-2022-11-15 20:24:27,800:INFO: Dataset: zara1               Batch: 19/32	Loss 22.6757 (23.2024)
-2022-11-15 20:24:28,641:INFO: Dataset: zara1               Batch: 20/32	Loss 22.8995 (23.1905)
-2022-11-15 20:24:29,496:INFO: Dataset: zara1               Batch: 21/32	Loss 22.9017 (23.1763)
-2022-11-15 20:24:30,346:INFO: Dataset: zara1               Batch: 22/32	Loss 22.6855 (23.1565)
-2022-11-15 20:24:31,186:INFO: Dataset: zara1               Batch: 23/32	Loss 23.3104 (23.1631)
-2022-11-15 20:24:32,039:INFO: Dataset: zara1               Batch: 24/32	Loss 21.1787 (23.0658)
-2022-11-15 20:24:32,989:INFO: Dataset: zara1               Batch: 25/32	Loss 23.5238 (23.0846)
-2022-11-15 20:24:33,965:INFO: Dataset: zara1               Batch: 26/32	Loss 22.7162 (23.0707)
-2022-11-15 20:24:34,922:INFO: Dataset: zara1               Batch: 27/32	Loss 23.4933 (23.0875)
-2022-11-15 20:24:35,892:INFO: Dataset: zara1               Batch: 28/32	Loss 23.4161 (23.0996)
-2022-11-15 20:24:36,852:INFO: Dataset: zara1               Batch: 29/32	Loss 23.1772 (23.1015)
-2022-11-15 20:24:37,698:INFO: Dataset: zara1               Batch: 30/32	Loss 24.6526 (23.1484)
-2022-11-15 20:24:38,570:INFO: Dataset: zara1               Batch: 31/32	Loss 24.4484 (23.1852)
-2022-11-15 20:24:39,390:INFO: Dataset: zara1               Batch: 32/32	Loss 10.5661 (22.9926)
-2022-11-15 20:25:07,624:INFO: Dataset: zara2               Batch:  1/72	Loss 23.1079 (23.1079)
-2022-11-15 20:25:08,450:INFO: Dataset: zara2               Batch:  2/72	Loss 22.9050 (23.0193)
-2022-11-15 20:25:09,288:INFO: Dataset: zara2               Batch:  3/72	Loss 23.7992 (23.2719)
-2022-11-15 20:25:10,138:INFO: Dataset: zara2               Batch:  4/72	Loss 23.7525 (23.4086)
-2022-11-15 20:25:11,017:INFO: Dataset: zara2               Batch:  5/72	Loss 22.8071 (23.2908)
-2022-11-15 20:25:11,889:INFO: Dataset: zara2               Batch:  6/72	Loss 23.4349 (23.3174)
-2022-11-15 20:25:12,826:INFO: Dataset: zara2               Batch:  7/72	Loss 23.0530 (23.2766)
-2022-11-15 20:25:13,749:INFO: Dataset: zara2               Batch:  8/72	Loss 23.6235 (23.3187)
-2022-11-15 20:25:14,688:INFO: Dataset: zara2               Batch:  9/72	Loss 23.3848 (23.3259)
-2022-11-15 20:25:15,634:INFO: Dataset: zara2               Batch: 10/72	Loss 23.5798 (23.3480)
-2022-11-15 20:25:16,627:INFO: Dataset: zara2               Batch: 11/72	Loss 23.2770 (23.3407)
-2022-11-15 20:25:17,557:INFO: Dataset: zara2               Batch: 12/72	Loss 23.4074 (23.3460)
-2022-11-15 20:25:18,344:INFO: Dataset: zara2               Batch: 13/72	Loss 23.5224 (23.3567)
-2022-11-15 20:25:19,158:INFO: Dataset: zara2               Batch: 14/72	Loss 23.0876 (23.3387)
-2022-11-15 20:25:20,020:INFO: Dataset: zara2               Batch: 15/72	Loss 23.0829 (23.3218)
-2022-11-15 20:25:20,875:INFO: Dataset: zara2               Batch: 16/72	Loss 23.3948 (23.3267)
-2022-11-15 20:25:21,711:INFO: Dataset: zara2               Batch: 17/72	Loss 23.2854 (23.3238)
-2022-11-15 20:25:22,556:INFO: Dataset: zara2               Batch: 18/72	Loss 23.2017 (23.3171)
-2022-11-15 20:25:23,568:INFO: Dataset: zara2               Batch: 19/72	Loss 23.6312 (23.3292)
-2022-11-15 20:25:24,528:INFO: Dataset: zara2               Batch: 20/72	Loss 22.9715 (23.3121)
-2022-11-15 20:25:25,469:INFO: Dataset: zara2               Batch: 21/72	Loss 22.9986 (23.2986)
-2022-11-15 20:25:26,417:INFO: Dataset: zara2               Batch: 22/72	Loss 22.8373 (23.2774)
-2022-11-15 20:25:27,384:INFO: Dataset: zara2               Batch: 23/72	Loss 23.1328 (23.2710)
-2022-11-15 20:25:28,287:INFO: Dataset: zara2               Batch: 24/72	Loss 23.1677 (23.2668)
-2022-11-15 20:25:29,199:INFO: Dataset: zara2               Batch: 25/72	Loss 22.7808 (23.2494)
-2022-11-15 20:25:30,025:INFO: Dataset: zara2               Batch: 26/72	Loss 23.6861 (23.2647)
-2022-11-15 20:25:30,875:INFO: Dataset: zara2               Batch: 27/72	Loss 22.6195 (23.2417)
-2022-11-15 20:25:31,745:INFO: Dataset: zara2               Batch: 28/72	Loss 22.8116 (23.2273)
-2022-11-15 20:25:32,679:INFO: Dataset: zara2               Batch: 29/72	Loss 23.0334 (23.2195)
-2022-11-15 20:25:33,640:INFO: Dataset: zara2               Batch: 30/72	Loss 22.6385 (23.2005)
-2022-11-15 20:25:34,599:INFO: Dataset: zara2               Batch: 31/72	Loss 22.7151 (23.1819)
-2022-11-15 20:25:35,523:INFO: Dataset: zara2               Batch: 32/72	Loss 22.6805 (23.1655)
-2022-11-15 20:25:36,508:INFO: Dataset: zara2               Batch: 33/72	Loss 22.9328 (23.1591)
-2022-11-15 20:25:37,492:INFO: Dataset: zara2               Batch: 34/72	Loss 22.7811 (23.1503)
-2022-11-15 20:25:38,392:INFO: Dataset: zara2               Batch: 35/72	Loss 23.2006 (23.1522)
-2022-11-15 20:25:39,223:INFO: Dataset: zara2               Batch: 36/72	Loss 22.9019 (23.1447)
-2022-11-15 20:25:40,058:INFO: Dataset: zara2               Batch: 37/72	Loss 22.7477 (23.1353)
-2022-11-15 20:25:40,957:INFO: Dataset: zara2               Batch: 38/72	Loss 22.6654 (23.1221)
-2022-11-15 20:25:41,797:INFO: Dataset: zara2               Batch: 39/72	Loss 23.1182 (23.1220)
-2022-11-15 20:25:42,639:INFO: Dataset: zara2               Batch: 40/72	Loss 22.6432 (23.1112)
-2022-11-15 20:25:43,561:INFO: Dataset: zara2               Batch: 41/72	Loss 22.9267 (23.1065)
-2022-11-15 20:25:44,518:INFO: Dataset: zara2               Batch: 42/72	Loss 23.5781 (23.1173)
-2022-11-15 20:25:45,484:INFO: Dataset: zara2               Batch: 43/72	Loss 23.1023 (23.1170)
-2022-11-15 20:25:46,460:INFO: Dataset: zara2               Batch: 44/72	Loss 22.9390 (23.1128)
-2022-11-15 20:25:47,394:INFO: Dataset: zara2               Batch: 45/72	Loss 23.3261 (23.1170)
-2022-11-15 20:25:48,354:INFO: Dataset: zara2               Batch: 46/72	Loss 23.0247 (23.1150)
-2022-11-15 20:25:49,172:INFO: Dataset: zara2               Batch: 47/72	Loss 23.1020 (23.1147)
-2022-11-15 20:25:50,031:INFO: Dataset: zara2               Batch: 48/72	Loss 22.8695 (23.1097)
-2022-11-15 20:25:50,910:INFO: Dataset: zara2               Batch: 49/72	Loss 22.5889 (23.0973)
-2022-11-15 20:25:51,744:INFO: Dataset: zara2               Batch: 50/72	Loss 22.8131 (23.0915)
-2022-11-15 20:25:52,590:INFO: Dataset: zara2               Batch: 51/72	Loss 23.1856 (23.0935)
-2022-11-15 20:25:53,486:INFO: Dataset: zara2               Batch: 52/72	Loss 23.2083 (23.0957)
-2022-11-15 20:25:54,431:INFO: Dataset: zara2               Batch: 53/72	Loss 22.5902 (23.0843)
-2022-11-15 20:25:55,360:INFO: Dataset: zara2               Batch: 54/72	Loss 23.8930 (23.0959)
-2022-11-15 20:25:56,300:INFO: Dataset: zara2               Batch: 55/72	Loss 23.4853 (23.1037)
-2022-11-15 20:25:57,241:INFO: Dataset: zara2               Batch: 56/72	Loss 22.8288 (23.0975)
-2022-11-15 20:25:58,240:INFO: Dataset: zara2               Batch: 57/72	Loss 22.6958 (23.0901)
-2022-11-15 20:25:59,072:INFO: Dataset: zara2               Batch: 58/72	Loss 23.1329 (23.0910)
-2022-11-15 20:25:59,911:INFO: Dataset: zara2               Batch: 59/72	Loss 23.6651 (23.1007)
-2022-11-15 20:26:00,761:INFO: Dataset: zara2               Batch: 60/72	Loss 23.2251 (23.1030)
-2022-11-15 20:26:01,612:INFO: Dataset: zara2               Batch: 61/72	Loss 22.7518 (23.0976)
-2022-11-15 20:26:02,476:INFO: Dataset: zara2               Batch: 62/72	Loss 23.0323 (23.0966)
-2022-11-15 20:26:03,341:INFO: Dataset: zara2               Batch: 63/72	Loss 22.8968 (23.0935)
-2022-11-15 20:26:04,322:INFO: Dataset: zara2               Batch: 64/72	Loss 23.4906 (23.0995)
-2022-11-15 20:26:05,285:INFO: Dataset: zara2               Batch: 65/72	Loss 23.5006 (23.1056)
-2022-11-15 20:26:06,220:INFO: Dataset: zara2               Batch: 66/72	Loss 22.9244 (23.1026)
-2022-11-15 20:26:07,181:INFO: Dataset: zara2               Batch: 67/72	Loss 23.1690 (23.1035)
-2022-11-15 20:26:08,125:INFO: Dataset: zara2               Batch: 68/72	Loss 22.4506 (23.0940)
-2022-11-15 20:26:09,017:INFO: Dataset: zara2               Batch: 69/72	Loss 23.0867 (23.0939)
-2022-11-15 20:26:09,840:INFO: Dataset: zara2               Batch: 70/72	Loss 23.2050 (23.0954)
-2022-11-15 20:26:10,683:INFO: Dataset: zara2               Batch: 71/72	Loss 22.8775 (23.0928)
-2022-11-15 20:26:11,506:INFO: Dataset: zara2               Batch: 72/72	Loss 10.3696 (23.0171)
-2022-11-15 20:26:12,538:INFO: - Computing ADE (validation)
-2022-11-15 20:26:23,038:INFO: 		 ADE on hotel                     dataset:	 2.4634530544281006
-2022-11-15 20:26:33,748:INFO: 		 ADE on univ                      dataset:	 2.33486270904541
-2022-11-15 20:26:44,190:INFO: 		 ADE on zara1                     dataset:	 1.9857633113861084
-2022-11-15 20:26:55,738:INFO: 		 ADE on zara2                     dataset:	 2.2022528648376465
-2022-11-15 20:26:55,738:INFO: Average validation:	ADE  2.2730	FDE  4.8244
-2022-11-15 20:26:55,739:INFO: - Computing ADE (training)
-2022-11-15 20:27:06,909:INFO: 		 ADE on hotel                     dataset:	 2.6707348823547363
-2022-11-15 20:27:36,922:INFO: 		 ADE on univ                      dataset:	 2.270958423614502
-2022-11-15 20:27:48,403:INFO: 		 ADE on zara1                     dataset:	 2.3826777935028076
-2022-11-15 20:28:18,479:INFO: 		 ADE on zara2                     dataset:	 2.2834599018096924
-2022-11-15 20:28:18,480:INFO: Average training:	ADE  2.2908	FDE  4.8238
-2022-11-15 20:28:18,513:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_305.pth.tar
-2022-11-15 20:28:18,513:INFO: 
-===> EPOCH: 306 (P4)
-2022-11-15 20:28:18,515:INFO: - Computing loss (training)
-2022-11-15 20:28:27,678:INFO: Dataset: hotel               Batch:  1/15	Loss 24.6242 (24.6242)
-2022-11-15 20:28:28,636:INFO: Dataset: hotel               Batch:  2/15	Loss 24.4505 (24.5357)
-2022-11-15 20:28:29,493:INFO: Dataset: hotel               Batch:  3/15	Loss 23.7746 (24.2820)
-2022-11-15 20:28:30,334:INFO: Dataset: hotel               Batch:  4/15	Loss 24.0632 (24.2281)
-2022-11-15 20:28:31,194:INFO: Dataset: hotel               Batch:  5/15	Loss 24.5640 (24.2852)
-2022-11-15 20:28:32,065:INFO: Dataset: hotel               Batch:  6/15	Loss 23.5428 (24.1611)
-2022-11-15 20:28:32,912:INFO: Dataset: hotel               Batch:  7/15	Loss 24.4881 (24.2040)
-2022-11-15 20:28:33,823:INFO: Dataset: hotel               Batch:  8/15	Loss 23.6128 (24.1277)
-2022-11-15 20:28:34,765:INFO: Dataset: hotel               Batch:  9/15	Loss 23.3808 (24.0354)
-2022-11-15 20:28:35,741:INFO: Dataset: hotel               Batch: 10/15	Loss 23.0026 (23.9395)
-2022-11-15 20:28:36,682:INFO: Dataset: hotel               Batch: 11/15	Loss 23.3455 (23.8890)
-2022-11-15 20:28:37,722:INFO: Dataset: hotel               Batch: 12/15	Loss 23.0555 (23.8078)
-2022-11-15 20:28:38,700:INFO: Dataset: hotel               Batch: 13/15	Loss 24.8158 (23.8837)
-2022-11-15 20:28:39,581:INFO: Dataset: hotel               Batch: 14/15	Loss 23.4508 (23.8512)
-2022-11-15 20:28:40,565:INFO: Dataset: hotel               Batch: 15/15	Loss 10.0674 (23.3784)
-2022-11-15 20:29:08,678:INFO: Dataset: univ                Batch:  1/57	Loss 23.0970 (23.0970)
-2022-11-15 20:29:09,693:INFO: Dataset: univ                Batch:  2/57	Loss 23.1039 (23.1003)
-2022-11-15 20:29:10,684:INFO: Dataset: univ                Batch:  3/57	Loss 23.1177 (23.1063)
-2022-11-15 20:29:11,704:INFO: Dataset: univ                Batch:  4/57	Loss 23.0518 (23.0937)
-2022-11-15 20:29:12,827:INFO: Dataset: univ                Batch:  5/57	Loss 23.0402 (23.0824)
-2022-11-15 20:29:13,811:INFO: Dataset: univ                Batch:  6/57	Loss 23.0585 (23.0786)
-2022-11-15 20:29:14,800:INFO: Dataset: univ                Batch:  7/57	Loss 23.1463 (23.0863)
-2022-11-15 20:29:15,844:INFO: Dataset: univ                Batch:  8/57	Loss 22.9802 (23.0756)
-2022-11-15 20:29:17,118:INFO: Dataset: univ                Batch:  9/57	Loss 23.3037 (23.0998)
-2022-11-15 20:29:18,064:INFO: Dataset: univ                Batch: 10/57	Loss 23.8208 (23.1474)
-2022-11-15 20:29:19,023:INFO: Dataset: univ                Batch: 11/57	Loss 23.1820 (23.1506)
-2022-11-15 20:29:19,925:INFO: Dataset: univ                Batch: 12/57	Loss 22.6957 (23.1150)
-2022-11-15 20:29:20,829:INFO: Dataset: univ                Batch: 13/57	Loss 22.8271 (23.0962)
-2022-11-15 20:29:21,705:INFO: Dataset: univ                Batch: 14/57	Loss 22.7828 (23.0718)
-2022-11-15 20:29:22,564:INFO: Dataset: univ                Batch: 15/57	Loss 22.7881 (23.0499)
-2022-11-15 20:29:23,459:INFO: Dataset: univ                Batch: 16/57	Loss 22.7731 (23.0319)
-2022-11-15 20:29:24,419:INFO: Dataset: univ                Batch: 17/57	Loss 22.8620 (23.0220)
-2022-11-15 20:29:25,420:INFO: Dataset: univ                Batch: 18/57	Loss 22.6495 (23.0010)
-2022-11-15 20:29:26,379:INFO: Dataset: univ                Batch: 19/57	Loss 22.8551 (22.9925)
-2022-11-15 20:29:27,353:INFO: Dataset: univ                Batch: 20/57	Loss 23.1562 (23.0010)
-2022-11-15 20:29:28,328:INFO: Dataset: univ                Batch: 21/57	Loss 22.7803 (22.9922)
-2022-11-15 20:29:29,316:INFO: Dataset: univ                Batch: 22/57	Loss 22.9244 (22.9884)
-2022-11-15 20:29:30,153:INFO: Dataset: univ                Batch: 23/57	Loss 23.0797 (22.9918)
-2022-11-15 20:29:31,040:INFO: Dataset: univ                Batch: 24/57	Loss 23.0175 (22.9930)
-2022-11-15 20:29:31,919:INFO: Dataset: univ                Batch: 25/57	Loss 23.2548 (23.0041)
-2022-11-15 20:29:32,806:INFO: Dataset: univ                Batch: 26/57	Loss 22.8358 (22.9979)
-2022-11-15 20:29:33,722:INFO: Dataset: univ                Batch: 27/57	Loss 23.0747 (23.0009)
-2022-11-15 20:29:34,676:INFO: Dataset: univ                Batch: 28/57	Loss 22.7252 (22.9916)
-2022-11-15 20:29:35,689:INFO: Dataset: univ                Batch: 29/57	Loss 22.8745 (22.9882)
-2022-11-15 20:29:36,634:INFO: Dataset: univ                Batch: 30/57	Loss 22.9607 (22.9874)
-2022-11-15 20:29:37,673:INFO: Dataset: univ                Batch: 31/57	Loss 23.0192 (22.9881)
-2022-11-15 20:29:38,642:INFO: Dataset: univ                Batch: 32/57	Loss 22.9116 (22.9854)
-2022-11-15 20:29:39,619:INFO: Dataset: univ                Batch: 33/57	Loss 22.7912 (22.9782)
-2022-11-15 20:29:40,455:INFO: Dataset: univ                Batch: 34/57	Loss 23.6686 (22.9952)
-2022-11-15 20:29:41,343:INFO: Dataset: univ                Batch: 35/57	Loss 22.8571 (22.9912)
-2022-11-15 20:29:42,225:INFO: Dataset: univ                Batch: 36/57	Loss 23.0590 (22.9933)
-2022-11-15 20:29:43,091:INFO: Dataset: univ                Batch: 37/57	Loss 22.8233 (22.9889)
-2022-11-15 20:29:43,987:INFO: Dataset: univ                Batch: 38/57	Loss 23.2500 (22.9958)
-2022-11-15 20:29:44,907:INFO: Dataset: univ                Batch: 39/57	Loss 23.1876 (23.0006)
-2022-11-15 20:29:45,889:INFO: Dataset: univ                Batch: 40/57	Loss 22.8954 (22.9979)
-2022-11-15 20:29:46,916:INFO: Dataset: univ                Batch: 41/57	Loss 22.8395 (22.9935)
-2022-11-15 20:29:47,905:INFO: Dataset: univ                Batch: 42/57	Loss 22.9357 (22.9919)
-2022-11-15 20:29:48,892:INFO: Dataset: univ                Batch: 43/57	Loss 22.5915 (22.9829)
-2022-11-15 20:29:49,869:INFO: Dataset: univ                Batch: 44/57	Loss 22.8903 (22.9805)
-2022-11-15 20:29:50,736:INFO: Dataset: univ                Batch: 45/57	Loss 22.8498 (22.9771)
-2022-11-15 20:29:51,622:INFO: Dataset: univ                Batch: 46/57	Loss 23.1763 (22.9817)
-2022-11-15 20:29:52,469:INFO: Dataset: univ                Batch: 47/57	Loss 23.0233 (22.9827)
-2022-11-15 20:29:53,374:INFO: Dataset: univ                Batch: 48/57	Loss 22.9711 (22.9824)
-2022-11-15 20:29:54,246:INFO: Dataset: univ                Batch: 49/57	Loss 22.8074 (22.9785)
-2022-11-15 20:29:55,190:INFO: Dataset: univ                Batch: 50/57	Loss 23.0591 (22.9801)
-2022-11-15 20:29:56,156:INFO: Dataset: univ                Batch: 51/57	Loss 22.6976 (22.9740)
-2022-11-15 20:29:57,180:INFO: Dataset: univ                Batch: 52/57	Loss 22.8143 (22.9703)
-2022-11-15 20:29:58,173:INFO: Dataset: univ                Batch: 53/57	Loss 23.3837 (22.9774)
-2022-11-15 20:29:59,187:INFO: Dataset: univ                Batch: 54/57	Loss 23.0037 (22.9779)
-2022-11-15 20:30:00,119:INFO: Dataset: univ                Batch: 55/57	Loss 22.8854 (22.9760)
-2022-11-15 20:30:00,973:INFO: Dataset: univ                Batch: 56/57	Loss 23.0244 (22.9769)
-2022-11-15 20:30:01,832:INFO: Dataset: univ                Batch: 57/57	Loss 17.1308 (22.8949)
-2022-11-15 20:30:12,288:INFO: Dataset: zara1               Batch:  1/32	Loss 23.0770 (23.0770)
-2022-11-15 20:30:13,256:INFO: Dataset: zara1               Batch:  2/32	Loss 22.7762 (22.9059)
-2022-11-15 20:30:14,121:INFO: Dataset: zara1               Batch:  3/32	Loss 23.6965 (23.1506)
-2022-11-15 20:30:15,066:INFO: Dataset: zara1               Batch:  4/32	Loss 22.5125 (22.9847)
-2022-11-15 20:30:16,046:INFO: Dataset: zara1               Batch:  5/32	Loss 22.5949 (22.8990)
-2022-11-15 20:30:17,006:INFO: Dataset: zara1               Batch:  6/32	Loss 22.6164 (22.8411)
-2022-11-15 20:30:17,989:INFO: Dataset: zara1               Batch:  7/32	Loss 22.9326 (22.8511)
-2022-11-15 20:30:18,945:INFO: Dataset: zara1               Batch:  8/32	Loss 24.6138 (23.0982)
-2022-11-15 20:30:19,888:INFO: Dataset: zara1               Batch:  9/32	Loss 22.9468 (23.0808)
-2022-11-15 20:30:20,737:INFO: Dataset: zara1               Batch: 10/32	Loss 22.1481 (22.9989)
-2022-11-15 20:30:21,596:INFO: Dataset: zara1               Batch: 11/32	Loss 23.2494 (23.0223)
-2022-11-15 20:30:22,518:INFO: Dataset: zara1               Batch: 12/32	Loss 23.0926 (23.0292)
-2022-11-15 20:30:23,358:INFO: Dataset: zara1               Batch: 13/32	Loss 23.2567 (23.0492)
-2022-11-15 20:30:24,194:INFO: Dataset: zara1               Batch: 14/32	Loss 22.4383 (23.0017)
-2022-11-15 20:30:25,056:INFO: Dataset: zara1               Batch: 15/32	Loss 23.0059 (23.0019)
-2022-11-15 20:30:26,022:INFO: Dataset: zara1               Batch: 16/32	Loss 23.0481 (23.0050)
-2022-11-15 20:30:27,006:INFO: Dataset: zara1               Batch: 17/32	Loss 23.2630 (23.0203)
-2022-11-15 20:30:27,941:INFO: Dataset: zara1               Batch: 18/32	Loss 22.9929 (23.0188)
-2022-11-15 20:30:28,991:INFO: Dataset: zara1               Batch: 19/32	Loss 23.0319 (23.0194)
-2022-11-15 20:30:30,387:INFO: Dataset: zara1               Batch: 20/32	Loss 22.3985 (22.9855)
-2022-11-15 20:30:31,687:INFO: Dataset: zara1               Batch: 21/32	Loss 23.4066 (23.0029)
-2022-11-15 20:30:32,741:INFO: Dataset: zara1               Batch: 22/32	Loss 22.9608 (23.0012)
-2022-11-15 20:30:33,780:INFO: Dataset: zara1               Batch: 23/32	Loss 22.9878 (23.0007)
-2022-11-15 20:30:34,791:INFO: Dataset: zara1               Batch: 24/32	Loss 22.6408 (22.9845)
-2022-11-15 20:30:35,708:INFO: Dataset: zara1               Batch: 25/32	Loss 22.7310 (22.9745)
-2022-11-15 20:30:36,681:INFO: Dataset: zara1               Batch: 26/32	Loss 23.0493 (22.9768)
-2022-11-15 20:30:37,648:INFO: Dataset: zara1               Batch: 27/32	Loss 22.6094 (22.9638)
-2022-11-15 20:30:38,584:INFO: Dataset: zara1               Batch: 28/32	Loss 22.7255 (22.9558)
-2022-11-15 20:30:39,544:INFO: Dataset: zara1               Batch: 29/32	Loss 23.0154 (22.9580)
-2022-11-15 20:30:40,503:INFO: Dataset: zara1               Batch: 30/32	Loss 23.1870 (22.9659)
-2022-11-15 20:30:41,339:INFO: Dataset: zara1               Batch: 31/32	Loss 23.1685 (22.9737)
-2022-11-15 20:30:42,159:INFO: Dataset: zara1               Batch: 32/32	Loss 10.1181 (22.7504)
-2022-11-15 20:31:10,248:INFO: Dataset: zara2               Batch:  1/72	Loss 23.0628 (23.0628)
-2022-11-15 20:31:11,137:INFO: Dataset: zara2               Batch:  2/72	Loss 23.6209 (23.3404)
-2022-11-15 20:31:11,963:INFO: Dataset: zara2               Batch:  3/72	Loss 22.8065 (23.1695)
-2022-11-15 20:31:12,806:INFO: Dataset: zara2               Batch:  4/72	Loss 23.2555 (23.1915)
-2022-11-15 20:31:13,675:INFO: Dataset: zara2               Batch:  5/72	Loss 22.7462 (23.1067)
-2022-11-15 20:31:14,593:INFO: Dataset: zara2               Batch:  6/72	Loss 22.3787 (23.0074)
-2022-11-15 20:31:15,443:INFO: Dataset: zara2               Batch:  7/72	Loss 22.3053 (22.9063)
-2022-11-15 20:31:16,369:INFO: Dataset: zara2               Batch:  8/72	Loss 22.6427 (22.8767)
-2022-11-15 20:31:17,275:INFO: Dataset: zara2               Batch:  9/72	Loss 23.0256 (22.8891)
-2022-11-15 20:31:18,221:INFO: Dataset: zara2               Batch: 10/72	Loss 22.8662 (22.8867)
-2022-11-15 20:31:19,151:INFO: Dataset: zara2               Batch: 11/72	Loss 22.2690 (22.8281)
-2022-11-15 20:31:20,114:INFO: Dataset: zara2               Batch: 12/72	Loss 22.5800 (22.8057)
-2022-11-15 20:31:21,040:INFO: Dataset: zara2               Batch: 13/72	Loss 22.9831 (22.8175)
-2022-11-15 20:31:21,873:INFO: Dataset: zara2               Batch: 14/72	Loss 22.6407 (22.8054)
-2022-11-15 20:31:22,710:INFO: Dataset: zara2               Batch: 15/72	Loss 22.2952 (22.7727)
-2022-11-15 20:31:23,560:INFO: Dataset: zara2               Batch: 16/72	Loss 22.5809 (22.7621)
-2022-11-15 20:31:24,421:INFO: Dataset: zara2               Batch: 17/72	Loss 22.7983 (22.7646)
-2022-11-15 20:31:25,254:INFO: Dataset: zara2               Batch: 18/72	Loss 23.0240 (22.7797)
-2022-11-15 20:31:26,136:INFO: Dataset: zara2               Batch: 19/72	Loss 22.7653 (22.7789)
-2022-11-15 20:31:27,163:INFO: Dataset: zara2               Batch: 20/72	Loss 23.2001 (22.7951)
-2022-11-15 20:31:28,172:INFO: Dataset: zara2               Batch: 21/72	Loss 23.1435 (22.8102)
-2022-11-15 20:31:29,106:INFO: Dataset: zara2               Batch: 22/72	Loss 22.7560 (22.8082)
-2022-11-15 20:31:30,071:INFO: Dataset: zara2               Batch: 23/72	Loss 23.1770 (22.8197)
-2022-11-15 20:31:30,989:INFO: Dataset: zara2               Batch: 24/72	Loss 23.0062 (22.8278)
-2022-11-15 20:31:31,858:INFO: Dataset: zara2               Batch: 25/72	Loss 23.3700 (22.8472)
-2022-11-15 20:31:32,658:INFO: Dataset: zara2               Batch: 26/72	Loss 22.5632 (22.8371)
-2022-11-15 20:31:33,536:INFO: Dataset: zara2               Batch: 27/72	Loss 22.6390 (22.8283)
-2022-11-15 20:31:34,467:INFO: Dataset: zara2               Batch: 28/72	Loss 23.0536 (22.8377)
-2022-11-15 20:31:35,304:INFO: Dataset: zara2               Batch: 29/72	Loss 22.4001 (22.8251)
-2022-11-15 20:31:36,353:INFO: Dataset: zara2               Batch: 30/72	Loss 22.7405 (22.8221)
-2022-11-15 20:31:37,592:INFO: Dataset: zara2               Batch: 31/72	Loss 22.7670 (22.8201)
-2022-11-15 20:31:38,469:INFO: Dataset: zara2               Batch: 32/72	Loss 22.3867 (22.8077)
-2022-11-15 20:31:39,296:INFO: Dataset: zara2               Batch: 33/72	Loss 23.0008 (22.8145)
-2022-11-15 20:31:40,131:INFO: Dataset: zara2               Batch: 34/72	Loss 22.7031 (22.8118)
-2022-11-15 20:31:40,941:INFO: Dataset: zara2               Batch: 35/72	Loss 22.2952 (22.7954)
-2022-11-15 20:31:41,741:INFO: Dataset: zara2               Batch: 36/72	Loss 22.9303 (22.7989)
-2022-11-15 20:31:42,535:INFO: Dataset: zara2               Batch: 37/72	Loss 22.4421 (22.7900)
-2022-11-15 20:31:43,410:INFO: Dataset: zara2               Batch: 38/72	Loss 22.8713 (22.7922)
-2022-11-15 20:31:44,289:INFO: Dataset: zara2               Batch: 39/72	Loss 22.6511 (22.7887)
-2022-11-15 20:31:45,086:INFO: Dataset: zara2               Batch: 40/72	Loss 22.9570 (22.7926)
-2022-11-15 20:31:45,912:INFO: Dataset: zara2               Batch: 41/72	Loss 22.8445 (22.7940)
-2022-11-15 20:31:46,766:INFO: Dataset: zara2               Batch: 42/72	Loss 22.8959 (22.7974)
-2022-11-15 20:31:47,652:INFO: Dataset: zara2               Batch: 43/72	Loss 22.7961 (22.7974)
-2022-11-15 20:31:48,456:INFO: Dataset: zara2               Batch: 44/72	Loss 23.0976 (22.8042)
-2022-11-15 20:31:49,301:INFO: Dataset: zara2               Batch: 45/72	Loss 22.6762 (22.8011)
-2022-11-15 20:31:50,104:INFO: Dataset: zara2               Batch: 46/72	Loss 22.7007 (22.7994)
-2022-11-15 20:31:50,910:INFO: Dataset: zara2               Batch: 47/72	Loss 22.4178 (22.7908)
-2022-11-15 20:31:51,757:INFO: Dataset: zara2               Batch: 48/72	Loss 22.6859 (22.7881)
-2022-11-15 20:31:52,613:INFO: Dataset: zara2               Batch: 49/72	Loss 22.6966 (22.7861)
-2022-11-15 20:31:53,375:INFO: Dataset: zara2               Batch: 50/72	Loss 22.2903 (22.7767)
-2022-11-15 20:31:54,163:INFO: Dataset: zara2               Batch: 51/72	Loss 23.0981 (22.7819)
-2022-11-15 20:31:54,931:INFO: Dataset: zara2               Batch: 52/72	Loss 23.0092 (22.7867)
-2022-11-15 20:31:55,702:INFO: Dataset: zara2               Batch: 53/72	Loss 22.7080 (22.7854)
-2022-11-15 20:31:56,429:INFO: Dataset: zara2               Batch: 54/72	Loss 22.6749 (22.7834)
-2022-11-15 20:31:57,169:INFO: Dataset: zara2               Batch: 55/72	Loss 22.1405 (22.7716)
-2022-11-15 20:31:57,928:INFO: Dataset: zara2               Batch: 56/72	Loss 23.0635 (22.7763)
-2022-11-15 20:31:58,752:INFO: Dataset: zara2               Batch: 57/72	Loss 23.4529 (22.7866)
-2022-11-15 20:31:59,616:INFO: Dataset: zara2               Batch: 58/72	Loss 22.5774 (22.7827)
-2022-11-15 20:32:00,864:INFO: Dataset: zara2               Batch: 59/72	Loss 22.4781 (22.7773)
-2022-11-15 20:32:01,879:INFO: Dataset: zara2               Batch: 60/72	Loss 22.6141 (22.7742)
-2022-11-15 20:32:02,813:INFO: Dataset: zara2               Batch: 61/72	Loss 22.6741 (22.7728)
-2022-11-15 20:32:03,791:INFO: Dataset: zara2               Batch: 62/72	Loss 22.6614 (22.7708)
-2022-11-15 20:32:04,693:INFO: Dataset: zara2               Batch: 63/72	Loss 23.1688 (22.7761)
-2022-11-15 20:32:05,536:INFO: Dataset: zara2               Batch: 64/72	Loss 22.2573 (22.7673)
-2022-11-15 20:32:06,376:INFO: Dataset: zara2               Batch: 65/72	Loss 22.8639 (22.7687)
-2022-11-15 20:32:07,214:INFO: Dataset: zara2               Batch: 66/72	Loss 22.8267 (22.7698)
-2022-11-15 20:32:08,080:INFO: Dataset: zara2               Batch: 67/72	Loss 22.7378 (22.7692)
-2022-11-15 20:32:08,961:INFO: Dataset: zara2               Batch: 68/72	Loss 23.2713 (22.7777)
-2022-11-15 20:32:09,818:INFO: Dataset: zara2               Batch: 69/72	Loss 22.8489 (22.7787)
-2022-11-15 20:32:10,801:INFO: Dataset: zara2               Batch: 70/72	Loss 22.9413 (22.7807)
-2022-11-15 20:32:11,791:INFO: Dataset: zara2               Batch: 71/72	Loss 22.7721 (22.7806)
-2022-11-15 20:32:12,716:INFO: Dataset: zara2               Batch: 72/72	Loss 10.0531 (22.7091)
-2022-11-15 20:32:13,840:INFO: - Computing ADE (validation)
-2022-11-15 20:32:23,925:INFO: 		 ADE on hotel                     dataset:	 2.592046022415161
-2022-11-15 20:32:33,644:INFO: 		 ADE on univ                      dataset:	 2.316802501678467
-2022-11-15 20:32:42,726:INFO: 		 ADE on zara1                     dataset:	 2.0509746074676514
-2022-11-15 20:32:52,307:INFO: 		 ADE on zara2                     dataset:	 2.20626163482666
-2022-11-15 20:32:52,307:INFO: Average validation:	ADE  2.2759	FDE  4.7633
-2022-11-15 20:32:52,308:INFO: - Computing ADE (training)
-2022-11-15 20:33:01,691:INFO: 		 ADE on hotel                     dataset:	 2.7150280475616455
-2022-11-15 20:33:27,633:INFO: 		 ADE on univ                      dataset:	 2.283508777618408
-2022-11-15 20:33:37,508:INFO: 		 ADE on zara1                     dataset:	 2.332447052001953
-2022-11-15 20:34:03,501:INFO: 		 ADE on zara2                     dataset:	 2.3003170490264893
-2022-11-15 20:34:03,501:INFO: Average training:	ADE  2.3010	FDE  4.7710
-2022-11-15 20:34:03,531:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_306.pth.tar
-2022-11-15 20:34:03,531:INFO: 
-===> EPOCH: 307 (P4)
-2022-11-15 20:34:03,532:INFO: - Computing loss (training)
-2022-11-15 20:34:11,100:INFO: Dataset: hotel               Batch:  1/15	Loss 24.0817 (24.0817)
-2022-11-15 20:34:11,994:INFO: Dataset: hotel               Batch:  2/15	Loss 24.0129 (24.0470)
-2022-11-15 20:34:12,736:INFO: Dataset: hotel               Batch:  3/15	Loss 25.1005 (24.3581)
-2022-11-15 20:34:13,505:INFO: Dataset: hotel               Batch:  4/15	Loss 24.7304 (24.4488)
-2022-11-15 20:34:14,251:INFO: Dataset: hotel               Batch:  5/15	Loss 24.1009 (24.3676)
-2022-11-15 20:34:14,991:INFO: Dataset: hotel               Batch:  6/15	Loss 23.3259 (24.1867)
-2022-11-15 20:34:15,719:INFO: Dataset: hotel               Batch:  7/15	Loss 24.4863 (24.2352)
-2022-11-15 20:34:16,443:INFO: Dataset: hotel               Batch:  8/15	Loss 22.7317 (24.0472)
-2022-11-15 20:34:17,177:INFO: Dataset: hotel               Batch:  9/15	Loss 23.3266 (23.9699)
-2022-11-15 20:34:17,908:INFO: Dataset: hotel               Batch: 10/15	Loss 23.7583 (23.9493)
-2022-11-15 20:34:18,658:INFO: Dataset: hotel               Batch: 11/15	Loss 23.3453 (23.8912)
-2022-11-15 20:34:19,391:INFO: Dataset: hotel               Batch: 12/15	Loss 23.1152 (23.8287)
-2022-11-15 20:34:20,116:INFO: Dataset: hotel               Batch: 13/15	Loss 23.7339 (23.8218)
-2022-11-15 20:34:20,844:INFO: Dataset: hotel               Batch: 14/15	Loss 23.4685 (23.7972)
-2022-11-15 20:34:21,537:INFO: Dataset: hotel               Batch: 15/15	Loss 10.8276 (23.3866)
-2022-11-15 20:34:45,808:INFO: Dataset: univ                Batch:  1/57	Loss 22.9664 (22.9664)
-2022-11-15 20:34:46,552:INFO: Dataset: univ                Batch:  2/57	Loss 22.9746 (22.9700)
-2022-11-15 20:34:47,300:INFO: Dataset: univ                Batch:  3/57	Loss 22.9763 (22.9725)
-2022-11-15 20:34:48,032:INFO: Dataset: univ                Batch:  4/57	Loss 23.1114 (23.0052)
-2022-11-15 20:34:48,779:INFO: Dataset: univ                Batch:  5/57	Loss 22.9152 (22.9869)
-2022-11-15 20:34:49,552:INFO: Dataset: univ                Batch:  6/57	Loss 23.0506 (22.9959)
-2022-11-15 20:34:50,291:INFO: Dataset: univ                Batch:  7/57	Loss 22.5506 (22.9361)
-2022-11-15 20:34:51,058:INFO: Dataset: univ                Batch:  8/57	Loss 22.8038 (22.9188)
-2022-11-15 20:34:51,824:INFO: Dataset: univ                Batch:  9/57	Loss 22.8973 (22.9167)
-2022-11-15 20:34:52,560:INFO: Dataset: univ                Batch: 10/57	Loss 22.8197 (22.9062)
-2022-11-15 20:34:53,322:INFO: Dataset: univ                Batch: 11/57	Loss 23.3135 (22.9458)
-2022-11-15 20:34:54,153:INFO: Dataset: univ                Batch: 12/57	Loss 23.1978 (22.9652)
-2022-11-15 20:34:54,895:INFO: Dataset: univ                Batch: 13/57	Loss 22.7888 (22.9514)
-2022-11-15 20:34:55,641:INFO: Dataset: univ                Batch: 14/57	Loss 22.7517 (22.9364)
-2022-11-15 20:34:56,367:INFO: Dataset: univ                Batch: 15/57	Loss 23.5461 (22.9695)
-2022-11-15 20:34:57,124:INFO: Dataset: univ                Batch: 16/57	Loss 22.9819 (22.9701)
-2022-11-15 20:34:57,861:INFO: Dataset: univ                Batch: 17/57	Loss 22.8283 (22.9609)
-2022-11-15 20:34:58,626:INFO: Dataset: univ                Batch: 18/57	Loss 22.8536 (22.9541)
-2022-11-15 20:34:59,383:INFO: Dataset: univ                Batch: 19/57	Loss 23.0602 (22.9600)
-2022-11-15 20:35:00,165:INFO: Dataset: univ                Batch: 20/57	Loss 22.7782 (22.9506)
-2022-11-15 20:35:00,906:INFO: Dataset: univ                Batch: 21/57	Loss 22.9538 (22.9508)
-2022-11-15 20:35:01,642:INFO: Dataset: univ                Batch: 22/57	Loss 22.9961 (22.9532)
-2022-11-15 20:35:02,386:INFO: Dataset: univ                Batch: 23/57	Loss 22.9344 (22.9523)
-2022-11-15 20:35:03,120:INFO: Dataset: univ                Batch: 24/57	Loss 23.0809 (22.9581)
-2022-11-15 20:35:03,892:INFO: Dataset: univ                Batch: 25/57	Loss 23.3602 (22.9745)
-2022-11-15 20:35:04,640:INFO: Dataset: univ                Batch: 26/57	Loss 22.9829 (22.9747)
-2022-11-15 20:35:05,391:INFO: Dataset: univ                Batch: 27/57	Loss 22.8121 (22.9684)
-2022-11-15 20:35:06,141:INFO: Dataset: univ                Batch: 28/57	Loss 23.1698 (22.9755)
-2022-11-15 20:35:06,906:INFO: Dataset: univ                Batch: 29/57	Loss 22.9383 (22.9743)
-2022-11-15 20:35:07,672:INFO: Dataset: univ                Batch: 30/57	Loss 22.9065 (22.9722)
-2022-11-15 20:35:08,415:INFO: Dataset: univ                Batch: 31/57	Loss 23.1798 (22.9785)
-2022-11-15 20:35:09,155:INFO: Dataset: univ                Batch: 32/57	Loss 22.8046 (22.9733)
-2022-11-15 20:35:09,920:INFO: Dataset: univ                Batch: 33/57	Loss 23.3505 (22.9828)
-2022-11-15 20:35:10,672:INFO: Dataset: univ                Batch: 34/57	Loss 22.7250 (22.9740)
-2022-11-15 20:35:11,420:INFO: Dataset: univ                Batch: 35/57	Loss 22.7856 (22.9680)
-2022-11-15 20:35:12,176:INFO: Dataset: univ                Batch: 36/57	Loss 22.6438 (22.9573)
-2022-11-15 20:35:12,910:INFO: Dataset: univ                Batch: 37/57	Loss 23.1991 (22.9632)
-2022-11-15 20:35:13,691:INFO: Dataset: univ                Batch: 38/57	Loss 22.8494 (22.9609)
-2022-11-15 20:35:14,439:INFO: Dataset: univ                Batch: 39/57	Loss 23.0091 (22.9621)
-2022-11-15 20:35:15,186:INFO: Dataset: univ                Batch: 40/57	Loss 23.1350 (22.9657)
-2022-11-15 20:35:15,935:INFO: Dataset: univ                Batch: 41/57	Loss 22.9768 (22.9659)
-2022-11-15 20:35:16,704:INFO: Dataset: univ                Batch: 42/57	Loss 22.7645 (22.9604)
-2022-11-15 20:35:17,460:INFO: Dataset: univ                Batch: 43/57	Loss 22.8134 (22.9573)
-2022-11-15 20:35:18,217:INFO: Dataset: univ                Batch: 44/57	Loss 22.8202 (22.9538)
-2022-11-15 20:35:18,970:INFO: Dataset: univ                Batch: 45/57	Loss 23.0568 (22.9564)
-2022-11-15 20:35:19,709:INFO: Dataset: univ                Batch: 46/57	Loss 23.1568 (22.9603)
-2022-11-15 20:35:20,585:INFO: Dataset: univ                Batch: 47/57	Loss 22.9363 (22.9598)
-2022-11-15 20:35:21,491:INFO: Dataset: univ                Batch: 48/57	Loss 22.5344 (22.9497)
-2022-11-15 20:35:22,303:INFO: Dataset: univ                Batch: 49/57	Loss 22.7096 (22.9445)
-2022-11-15 20:35:23,029:INFO: Dataset: univ                Batch: 50/57	Loss 23.0071 (22.9460)
-2022-11-15 20:35:23,791:INFO: Dataset: univ                Batch: 51/57	Loss 23.0152 (22.9473)
-2022-11-15 20:35:24,548:INFO: Dataset: univ                Batch: 52/57	Loss 23.6639 (22.9616)
-2022-11-15 20:35:25,306:INFO: Dataset: univ                Batch: 53/57	Loss 22.6326 (22.9550)
-2022-11-15 20:35:26,062:INFO: Dataset: univ                Batch: 54/57	Loss 22.5870 (22.9470)
-2022-11-15 20:35:26,833:INFO: Dataset: univ                Batch: 55/57	Loss 22.6849 (22.9407)
-2022-11-15 20:35:27,592:INFO: Dataset: univ                Batch: 56/57	Loss 22.9685 (22.9412)
-2022-11-15 20:35:28,333:INFO: Dataset: univ                Batch: 57/57	Loss 17.1827 (22.8763)
-2022-11-15 20:35:37,176:INFO: Dataset: zara1               Batch:  1/32	Loss 22.9466 (22.9466)
-2022-11-15 20:35:37,981:INFO: Dataset: zara1               Batch:  2/32	Loss 22.3882 (22.6918)
-2022-11-15 20:35:38,726:INFO: Dataset: zara1               Batch:  3/32	Loss 22.4920 (22.6138)
-2022-11-15 20:35:39,472:INFO: Dataset: zara1               Batch:  4/32	Loss 22.8497 (22.6814)
-2022-11-15 20:35:40,201:INFO: Dataset: zara1               Batch:  5/32	Loss 23.0097 (22.7469)
-2022-11-15 20:35:40,971:INFO: Dataset: zara1               Batch:  6/32	Loss 23.0614 (22.8021)
-2022-11-15 20:35:41,707:INFO: Dataset: zara1               Batch:  7/32	Loss 22.8950 (22.8159)
-2022-11-15 20:35:42,480:INFO: Dataset: zara1               Batch:  8/32	Loss 22.7654 (22.8083)
-2022-11-15 20:35:43,280:INFO: Dataset: zara1               Batch:  9/32	Loss 23.0040 (22.8322)
-2022-11-15 20:35:44,013:INFO: Dataset: zara1               Batch: 10/32	Loss 22.9301 (22.8410)
-2022-11-15 20:35:44,750:INFO: Dataset: zara1               Batch: 11/32	Loss 22.3205 (22.7945)
-2022-11-15 20:35:45,501:INFO: Dataset: zara1               Batch: 12/32	Loss 22.8979 (22.8044)
-2022-11-15 20:35:46,234:INFO: Dataset: zara1               Batch: 13/32	Loss 22.5112 (22.7841)
-2022-11-15 20:35:46,962:INFO: Dataset: zara1               Batch: 14/32	Loss 22.5644 (22.7709)
-2022-11-15 20:35:47,696:INFO: Dataset: zara1               Batch: 15/32	Loss 23.2235 (22.7980)
-2022-11-15 20:35:48,442:INFO: Dataset: zara1               Batch: 16/32	Loss 22.8469 (22.8004)
-2022-11-15 20:35:49,177:INFO: Dataset: zara1               Batch: 17/32	Loss 22.5987 (22.7873)
-2022-11-15 20:35:49,899:INFO: Dataset: zara1               Batch: 18/32	Loss 22.6700 (22.7803)
-2022-11-15 20:35:50,653:INFO: Dataset: zara1               Batch: 19/32	Loss 22.4987 (22.7667)
-2022-11-15 20:35:51,392:INFO: Dataset: zara1               Batch: 20/32	Loss 22.4990 (22.7503)
-2022-11-15 20:35:52,128:INFO: Dataset: zara1               Batch: 21/32	Loss 22.6309 (22.7456)
-2022-11-15 20:35:52,866:INFO: Dataset: zara1               Batch: 22/32	Loss 22.1550 (22.7238)
-2022-11-15 20:35:53,585:INFO: Dataset: zara1               Batch: 23/32	Loss 23.4889 (22.7510)
-2022-11-15 20:35:54,305:INFO: Dataset: zara1               Batch: 24/32	Loss 22.8618 (22.7553)
-2022-11-15 20:35:55,031:INFO: Dataset: zara1               Batch: 25/32	Loss 22.9274 (22.7613)
-2022-11-15 20:35:55,779:INFO: Dataset: zara1               Batch: 26/32	Loss 22.9985 (22.7727)
-2022-11-15 20:35:56,507:INFO: Dataset: zara1               Batch: 27/32	Loss 23.0325 (22.7819)
-2022-11-15 20:35:57,223:INFO: Dataset: zara1               Batch: 28/32	Loss 23.3306 (22.7970)
-2022-11-15 20:35:57,963:INFO: Dataset: zara1               Batch: 29/32	Loss 23.3790 (22.8205)
-2022-11-15 20:35:58,711:INFO: Dataset: zara1               Batch: 30/32	Loss 22.5896 (22.8141)
-2022-11-15 20:35:59,445:INFO: Dataset: zara1               Batch: 31/32	Loss 22.7914 (22.8130)
-2022-11-15 20:36:00,134:INFO: Dataset: zara1               Batch: 32/32	Loss 9.6053 (22.5975)
-2022-11-15 20:36:27,671:INFO: Dataset: zara2               Batch:  1/72	Loss 22.9507 (22.9507)
-2022-11-15 20:36:28,715:INFO: Dataset: zara2               Batch:  2/72	Loss 22.3835 (22.6119)
-2022-11-15 20:36:29,635:INFO: Dataset: zara2               Batch:  3/72	Loss 22.7182 (22.6455)
-2022-11-15 20:36:30,499:INFO: Dataset: zara2               Batch:  4/72	Loss 22.3458 (22.5643)
-2022-11-15 20:36:31,360:INFO: Dataset: zara2               Batch:  5/72	Loss 22.6280 (22.5791)
-2022-11-15 20:36:32,231:INFO: Dataset: zara2               Batch:  6/72	Loss 22.6744 (22.6010)
-2022-11-15 20:36:33,154:INFO: Dataset: zara2               Batch:  7/72	Loss 23.3301 (22.7050)
-2022-11-15 20:36:34,087:INFO: Dataset: zara2               Batch:  8/72	Loss 22.8781 (22.7263)
-2022-11-15 20:36:35,007:INFO: Dataset: zara2               Batch:  9/72	Loss 22.5735 (22.7094)
-2022-11-15 20:36:35,965:INFO: Dataset: zara2               Batch: 10/72	Loss 22.8390 (22.7215)
-2022-11-15 20:36:36,850:INFO: Dataset: zara2               Batch: 11/72	Loss 22.7911 (22.7281)
-2022-11-15 20:36:37,732:INFO: Dataset: zara2               Batch: 12/72	Loss 22.1543 (22.6792)
-2022-11-15 20:36:38,617:INFO: Dataset: zara2               Batch: 13/72	Loss 22.6759 (22.6789)
-2022-11-15 20:36:39,456:INFO: Dataset: zara2               Batch: 14/72	Loss 22.7125 (22.6816)
-2022-11-15 20:36:40,344:INFO: Dataset: zara2               Batch: 15/72	Loss 22.2200 (22.6558)
-2022-11-15 20:36:41,180:INFO: Dataset: zara2               Batch: 16/72	Loss 23.0250 (22.6738)
-2022-11-15 20:36:42,111:INFO: Dataset: zara2               Batch: 17/72	Loss 23.3866 (22.7169)
-2022-11-15 20:36:42,961:INFO: Dataset: zara2               Batch: 18/72	Loss 22.5416 (22.7054)
-2022-11-15 20:36:43,978:INFO: Dataset: zara2               Batch: 19/72	Loss 22.8521 (22.7125)
-2022-11-15 20:36:44,969:INFO: Dataset: zara2               Batch: 20/72	Loss 24.0977 (22.7632)
-2022-11-15 20:36:45,937:INFO: Dataset: zara2               Batch: 21/72	Loss 22.4874 (22.7502)
-2022-11-15 20:36:47,107:INFO: Dataset: zara2               Batch: 22/72	Loss 22.4957 (22.7386)
-2022-11-15 20:36:48,163:INFO: Dataset: zara2               Batch: 23/72	Loss 22.9949 (22.7497)
-2022-11-15 20:36:49,242:INFO: Dataset: zara2               Batch: 24/72	Loss 22.3437 (22.7331)
-2022-11-15 20:36:50,285:INFO: Dataset: zara2               Batch: 25/72	Loss 22.9239 (22.7410)
-2022-11-15 20:36:51,326:INFO: Dataset: zara2               Batch: 26/72	Loss 22.5138 (22.7327)
-2022-11-15 20:36:52,229:INFO: Dataset: zara2               Batch: 27/72	Loss 23.2103 (22.7525)
-2022-11-15 20:36:53,130:INFO: Dataset: zara2               Batch: 28/72	Loss 23.0373 (22.7635)
-2022-11-15 20:36:54,189:INFO: Dataset: zara2               Batch: 29/72	Loss 23.2189 (22.7800)
-2022-11-15 20:36:55,221:INFO: Dataset: zara2               Batch: 30/72	Loss 22.0964 (22.7587)
-2022-11-15 20:36:56,110:INFO: Dataset: zara2               Batch: 31/72	Loss 22.8377 (22.7616)
-2022-11-15 20:36:56,996:INFO: Dataset: zara2               Batch: 32/72	Loss 22.9405 (22.7680)
-2022-11-15 20:36:57,728:INFO: Dataset: zara2               Batch: 33/72	Loss 22.3433 (22.7556)
-2022-11-15 20:36:58,484:INFO: Dataset: zara2               Batch: 34/72	Loss 22.5905 (22.7512)
-2022-11-15 20:36:59,214:INFO: Dataset: zara2               Batch: 35/72	Loss 23.2102 (22.7665)
-2022-11-15 20:36:59,967:INFO: Dataset: zara2               Batch: 36/72	Loss 22.5510 (22.7621)
-2022-11-15 20:37:00,721:INFO: Dataset: zara2               Batch: 37/72	Loss 22.5602 (22.7556)
-2022-11-15 20:37:01,461:INFO: Dataset: zara2               Batch: 38/72	Loss 22.5374 (22.7495)
-2022-11-15 20:37:02,187:INFO: Dataset: zara2               Batch: 39/72	Loss 22.7154 (22.7487)
-2022-11-15 20:37:02,910:INFO: Dataset: zara2               Batch: 40/72	Loss 22.6657 (22.7467)
-2022-11-15 20:37:03,646:INFO: Dataset: zara2               Batch: 41/72	Loss 19.5693 (22.6537)
-2022-11-15 20:37:04,380:INFO: Dataset: zara2               Batch: 42/72	Loss 25.5563 (22.7169)
-2022-11-15 20:37:05,106:INFO: Dataset: zara2               Batch: 43/72	Loss 22.8851 (22.7206)
-2022-11-15 20:37:05,830:INFO: Dataset: zara2               Batch: 44/72	Loss 23.5558 (22.7394)
-2022-11-15 20:37:06,581:INFO: Dataset: zara2               Batch: 45/72	Loss 22.7725 (22.7402)
-2022-11-15 20:37:07,295:INFO: Dataset: zara2               Batch: 46/72	Loss 23.3185 (22.7514)
-2022-11-15 20:37:08,003:INFO: Dataset: zara2               Batch: 47/72	Loss 23.4176 (22.7691)
-2022-11-15 20:37:08,718:INFO: Dataset: zara2               Batch: 48/72	Loss 23.0413 (22.7744)
-2022-11-15 20:37:09,450:INFO: Dataset: zara2               Batch: 49/72	Loss 22.6864 (22.7727)
-2022-11-15 20:37:10,161:INFO: Dataset: zara2               Batch: 50/72	Loss 22.9370 (22.7767)
-2022-11-15 20:37:10,914:INFO: Dataset: zara2               Batch: 51/72	Loss 23.2780 (22.7860)
-2022-11-15 20:37:11,634:INFO: Dataset: zara2               Batch: 52/72	Loss 23.4926 (22.8005)
-2022-11-15 20:37:12,362:INFO: Dataset: zara2               Batch: 53/72	Loss 23.5381 (22.8167)
-2022-11-15 20:37:13,121:INFO: Dataset: zara2               Batch: 54/72	Loss 23.4042 (22.8278)
-2022-11-15 20:37:13,871:INFO: Dataset: zara2               Batch: 55/72	Loss 23.0732 (22.8335)
-2022-11-15 20:37:14,591:INFO: Dataset: zara2               Batch: 56/72	Loss 23.1053 (22.8381)
-2022-11-15 20:37:15,318:INFO: Dataset: zara2               Batch: 57/72	Loss 23.8326 (22.8555)
-2022-11-15 20:37:16,017:INFO: Dataset: zara2               Batch: 58/72	Loss 23.0717 (22.8589)
-2022-11-15 20:37:16,719:INFO: Dataset: zara2               Batch: 59/72	Loss 22.7048 (22.8567)
-2022-11-15 20:37:17,455:INFO: Dataset: zara2               Batch: 60/72	Loss 23.4854 (22.8694)
-2022-11-15 20:37:18,199:INFO: Dataset: zara2               Batch: 61/72	Loss 23.5379 (22.8798)
-2022-11-15 20:37:18,956:INFO: Dataset: zara2               Batch: 62/72	Loss 23.1780 (22.8853)
-2022-11-15 20:37:19,823:INFO: Dataset: zara2               Batch: 63/72	Loss 23.6896 (22.8995)
-2022-11-15 20:37:20,683:INFO: Dataset: zara2               Batch: 64/72	Loss 23.4372 (22.9087)
-2022-11-15 20:37:21,453:INFO: Dataset: zara2               Batch: 65/72	Loss 23.5268 (22.9184)
-2022-11-15 20:37:22,189:INFO: Dataset: zara2               Batch: 66/72	Loss 24.0786 (22.9323)
-2022-11-15 20:37:22,924:INFO: Dataset: zara2               Batch: 67/72	Loss 23.6094 (22.9418)
-2022-11-15 20:37:23,659:INFO: Dataset: zara2               Batch: 68/72	Loss 23.1794 (22.9455)
-2022-11-15 20:37:24,390:INFO: Dataset: zara2               Batch: 69/72	Loss 23.7154 (22.9590)
-2022-11-15 20:37:25,114:INFO: Dataset: zara2               Batch: 70/72	Loss 23.2332 (22.9632)
-2022-11-15 20:37:25,873:INFO: Dataset: zara2               Batch: 71/72	Loss 25.7331 (22.9991)
-2022-11-15 20:37:26,549:INFO: Dataset: zara2               Batch: 72/72	Loss 10.0543 (22.9092)
-2022-11-15 20:37:27,487:INFO: - Computing ADE (validation)
-2022-11-15 20:37:36,333:INFO: 		 ADE on hotel                     dataset:	 2.5292820930480957
-2022-11-15 20:37:45,527:INFO: 		 ADE on univ                      dataset:	 2.333799123764038
-2022-11-15 20:37:54,410:INFO: 		 ADE on zara1                     dataset:	 1.7381418943405151
-2022-11-15 20:38:03,851:INFO: 		 ADE on zara2                     dataset:	 2.28761887550354
-2022-11-15 20:38:03,851:INFO: Average validation:	ADE  2.2929	FDE  4.4523
-2022-11-15 20:38:03,852:INFO: - Computing ADE (training)
-2022-11-15 20:38:13,105:INFO: 		 ADE on hotel                     dataset:	 2.67812180519104
-2022-11-15 20:38:38,397:INFO: 		 ADE on univ                      dataset:	 2.299947500228882
-2022-11-15 20:38:48,401:INFO: 		 ADE on zara1                     dataset:	 2.4589223861694336
-2022-11-15 20:39:18,518:INFO: 		 ADE on zara2                     dataset:	 2.364274501800537
-2022-11-15 20:39:18,519:INFO: Average training:	ADE  2.3328	FDE  4.5050
-2022-11-15 20:39:18,552:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_307.pth.tar
-2022-11-15 20:39:18,552:INFO: 
-===> EPOCH: 308 (P4)
-2022-11-15 20:39:18,553:INFO: - Computing loss (training)
-2022-11-15 20:39:27,484:INFO: Dataset: hotel               Batch:  1/15	Loss 24.4497 (24.4497)
-2022-11-15 20:39:29,547:INFO: Dataset: hotel               Batch:  2/15	Loss 23.6315 (24.0446)
-2022-11-15 20:39:30,845:INFO: Dataset: hotel               Batch:  3/15	Loss 24.7877 (24.2772)
-2022-11-15 20:39:32,269:INFO: Dataset: hotel               Batch:  4/15	Loss 24.2083 (24.2563)
-2022-11-15 20:39:33,411:INFO: Dataset: hotel               Batch:  5/15	Loss 23.4198 (24.0833)
-2022-11-15 20:39:34,538:INFO: Dataset: hotel               Batch:  6/15	Loss 23.9567 (24.0636)
-2022-11-15 20:39:35,569:INFO: Dataset: hotel               Batch:  7/15	Loss 24.4510 (24.1120)
-2022-11-15 20:39:37,145:INFO: Dataset: hotel               Batch:  8/15	Loss 23.5926 (24.0368)
-2022-11-15 20:39:38,560:INFO: Dataset: hotel               Batch:  9/15	Loss 24.3482 (24.0751)
-2022-11-15 20:39:39,424:INFO: Dataset: hotel               Batch: 10/15	Loss 24.5991 (24.1254)
-2022-11-15 20:39:40,422:INFO: Dataset: hotel               Batch: 11/15	Loss 23.1520 (24.0371)
-2022-11-15 20:39:41,317:INFO: Dataset: hotel               Batch: 12/15	Loss 23.7131 (24.0092)
-2022-11-15 20:39:42,101:INFO: Dataset: hotel               Batch: 13/15	Loss 24.3456 (24.0336)
-2022-11-15 20:39:42,905:INFO: Dataset: hotel               Batch: 14/15	Loss 24.6512 (24.0746)
-2022-11-15 20:39:43,665:INFO: Dataset: hotel               Batch: 15/15	Loss 10.8215 (23.7249)
-2022-11-15 20:40:11,021:INFO: Dataset: univ                Batch:  1/57	Loss 23.3293 (23.3293)
-2022-11-15 20:40:11,787:INFO: Dataset: univ                Batch:  2/57	Loss 23.9413 (23.5493)
-2022-11-15 20:40:12,644:INFO: Dataset: univ                Batch:  3/57	Loss 23.6214 (23.5706)
-2022-11-15 20:40:13,559:INFO: Dataset: univ                Batch:  4/57	Loss 23.4839 (23.5452)
-2022-11-15 20:40:14,422:INFO: Dataset: univ                Batch:  5/57	Loss 23.6684 (23.5699)
-2022-11-15 20:40:15,431:INFO: Dataset: univ                Batch:  6/57	Loss 23.9863 (23.6392)
-2022-11-15 20:40:16,410:INFO: Dataset: univ                Batch:  7/57	Loss 23.3551 (23.6035)
-2022-11-15 20:40:17,418:INFO: Dataset: univ                Batch:  8/57	Loss 23.3565 (23.5668)
-2022-11-15 20:40:18,426:INFO: Dataset: univ                Batch:  9/57	Loss 23.7528 (23.5893)
-2022-11-15 20:40:19,363:INFO: Dataset: univ                Batch: 10/57	Loss 23.5397 (23.5849)
-2022-11-15 20:40:20,333:INFO: Dataset: univ                Batch: 11/57	Loss 23.4277 (23.5700)
-2022-11-15 20:40:21,307:INFO: Dataset: univ                Batch: 12/57	Loss 24.1210 (23.6067)
-2022-11-15 20:40:22,252:INFO: Dataset: univ                Batch: 13/57	Loss 24.1808 (23.6456)
-2022-11-15 20:40:23,270:INFO: Dataset: univ                Batch: 14/57	Loss 23.2649 (23.6211)
-2022-11-15 20:40:24,237:INFO: Dataset: univ                Batch: 15/57	Loss 24.1031 (23.6491)
-2022-11-15 20:40:25,103:INFO: Dataset: univ                Batch: 16/57	Loss 23.3770 (23.6304)
-2022-11-15 20:40:25,965:INFO: Dataset: univ                Batch: 17/57	Loss 23.2337 (23.6034)
-2022-11-15 20:40:26,864:INFO: Dataset: univ                Batch: 18/57	Loss 23.4233 (23.5933)
-2022-11-15 20:40:27,771:INFO: Dataset: univ                Batch: 19/57	Loss 23.1914 (23.5714)
-2022-11-15 20:40:28,716:INFO: Dataset: univ                Batch: 20/57	Loss 23.4408 (23.5660)
-2022-11-15 20:40:29,855:INFO: Dataset: univ                Batch: 21/57	Loss 23.9134 (23.5804)
-2022-11-15 20:40:31,018:INFO: Dataset: univ                Batch: 22/57	Loss 24.0205 (23.5977)
-2022-11-15 20:40:32,019:INFO: Dataset: univ                Batch: 23/57	Loss 23.7921 (23.6059)
-2022-11-15 20:40:32,987:INFO: Dataset: univ                Batch: 24/57	Loss 23.0082 (23.5836)
-2022-11-15 20:40:33,981:INFO: Dataset: univ                Batch: 25/57	Loss 23.3896 (23.5742)
-2022-11-15 20:40:34,853:INFO: Dataset: univ                Batch: 26/57	Loss 23.4576 (23.5697)
-2022-11-15 20:40:35,706:INFO: Dataset: univ                Batch: 27/57	Loss 23.4083 (23.5638)
-2022-11-15 20:40:36,591:INFO: Dataset: univ                Batch: 28/57	Loss 23.1081 (23.5453)
-2022-11-15 20:40:37,491:INFO: Dataset: univ                Batch: 29/57	Loss 23.2917 (23.5359)
-2022-11-15 20:40:38,381:INFO: Dataset: univ                Batch: 30/57	Loss 23.3654 (23.5292)
-2022-11-15 20:40:39,267:INFO: Dataset: univ                Batch: 31/57	Loss 23.2597 (23.5191)
-2022-11-15 20:40:40,252:INFO: Dataset: univ                Batch: 32/57	Loss 23.0279 (23.5050)
-2022-11-15 20:40:41,248:INFO: Dataset: univ                Batch: 33/57	Loss 23.1136 (23.4946)
-2022-11-15 20:40:42,311:INFO: Dataset: univ                Batch: 34/57	Loss 23.2625 (23.4876)
-2022-11-15 20:40:43,453:INFO: Dataset: univ                Batch: 35/57	Loss 23.7181 (23.4928)
-2022-11-15 20:40:44,290:INFO: Dataset: univ                Batch: 36/57	Loss 23.0834 (23.4815)
+2022-11-18 14:50:34,470:INFO: Initializing Training Set
+2022-11-18 14:50:36,025:INFO: Initializing Validation Set
+2022-11-18 14:50:36,199:INFO: Initializing Validation O Set
+2022-11-18 14:50:38,284:INFO: 
+===> EPOCH: 201 (P2)
+2022-11-18 14:50:38,285:INFO: - Computing loss (training)
+2022-11-18 14:50:38,648:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0981 (0.0981)
+2022-11-18 14:50:38,807:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0983 (0.0982)
+2022-11-18 14:50:38,960:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0893 (0.0951)
+2022-11-18 14:50:39,070:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0994 (0.0958)
+2022-11-18 14:50:39,530:INFO: Dataset: univ                Batch:  1/15	Loss 0.0851 (0.0851)
+2022-11-18 14:50:39,684:INFO: Dataset: univ                Batch:  2/15	Loss 0.0869 (0.0860)
+2022-11-18 14:50:39,859:INFO: Dataset: univ                Batch:  3/15	Loss 0.0850 (0.0856)
+2022-11-18 14:50:40,019:INFO: Dataset: univ                Batch:  4/15	Loss 0.0846 (0.0853)
+2022-11-18 14:50:40,175:INFO: Dataset: univ                Batch:  5/15	Loss 0.0856 (0.0854)
+2022-11-18 14:50:40,327:INFO: Dataset: univ                Batch:  6/15	Loss 0.0840 (0.0852)
+2022-11-18 14:50:40,480:INFO: Dataset: univ                Batch:  7/15	Loss 0.0867 (0.0854)
+2022-11-18 14:50:40,628:INFO: Dataset: univ                Batch:  8/15	Loss 0.0828 (0.0851)
+2022-11-18 14:50:40,779:INFO: Dataset: univ                Batch:  9/15	Loss 0.0853 (0.0851)
+2022-11-18 14:50:40,927:INFO: Dataset: univ                Batch: 10/15	Loss 0.0875 (0.0853)
+2022-11-18 14:50:41,077:INFO: Dataset: univ                Batch: 11/15	Loss 0.0841 (0.0852)
+2022-11-18 14:50:41,227:INFO: Dataset: univ                Batch: 12/15	Loss 0.0838 (0.0851)
+2022-11-18 14:50:41,376:INFO: Dataset: univ                Batch: 13/15	Loss 0.0845 (0.0850)
+2022-11-18 14:50:41,526:INFO: Dataset: univ                Batch: 14/15	Loss 0.0836 (0.0849)
+2022-11-18 14:50:41,606:INFO: Dataset: univ                Batch: 15/15	Loss 0.0866 (0.0850)
+2022-11-18 14:50:41,981:INFO: Dataset: zara1               Batch: 1/8	Loss 0.1226 (0.1226)
+2022-11-18 14:50:42,132:INFO: Dataset: zara1               Batch: 2/8	Loss 0.1228 (0.1227)
+2022-11-18 14:50:42,279:INFO: Dataset: zara1               Batch: 3/8	Loss 0.1271 (0.1240)
+2022-11-18 14:50:42,428:INFO: Dataset: zara1               Batch: 4/8	Loss 0.1209 (0.1233)
+2022-11-18 14:50:42,579:INFO: Dataset: zara1               Batch: 5/8	Loss 0.1258 (0.1238)
+2022-11-18 14:50:42,725:INFO: Dataset: zara1               Batch: 6/8	Loss 0.1264 (0.1242)
+2022-11-18 14:50:42,873:INFO: Dataset: zara1               Batch: 7/8	Loss 0.1287 (0.1248)
+2022-11-18 14:50:43,018:INFO: Dataset: zara1               Batch: 8/8	Loss 0.1270 (0.1251)
+2022-11-18 14:50:43,443:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1089 (0.1089)
+2022-11-18 14:50:43,611:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1041 (0.1065)
+2022-11-18 14:50:43,766:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0995 (0.1041)
+2022-11-18 14:50:43,919:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0992 (0.1029)
+2022-11-18 14:50:44,074:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1034 (0.1030)
+2022-11-18 14:50:44,232:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1032 (0.1030)
+2022-11-18 14:50:44,387:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1008 (0.1027)
+2022-11-18 14:50:44,548:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1042 (0.1029)
+2022-11-18 14:50:44,706:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1014 (0.1027)
+2022-11-18 14:50:44,854:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1020 (0.1026)
+2022-11-18 14:50:45,012:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1046 (0.1028)
+2022-11-18 14:50:45,163:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1091 (0.1033)
+2022-11-18 14:50:45,316:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1112 (0.1040)
+2022-11-18 14:50:45,469:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1015 (0.1038)
+2022-11-18 14:50:45,623:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1106 (0.1042)
+2022-11-18 14:50:45,775:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1014 (0.1040)
+2022-11-18 14:50:45,933:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1018 (0.1039)
+2022-11-18 14:50:46,070:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0964 (0.1035)
+2022-11-18 14:50:46,115:INFO: - Computing loss (validation)
+2022-11-18 14:50:46,368:INFO: Dataset: hotel               Batch: 1/2	Loss 0.0827 (0.0827)
+2022-11-18 14:50:46,400:INFO: Dataset: hotel               Batch: 2/2	Loss 0.1019 (0.0840)
+2022-11-18 14:50:46,705:INFO: Dataset: univ                Batch: 1/3	Loss 0.0843 (0.0843)
+2022-11-18 14:50:46,775:INFO: Dataset: univ                Batch: 2/3	Loss 0.0900 (0.0871)
+2022-11-18 14:50:46,843:INFO: Dataset: univ                Batch: 3/3	Loss 0.0848 (0.0863)
+2022-11-18 14:50:47,142:INFO: Dataset: zara1               Batch: 1/2	Loss 0.1319 (0.1319)
+2022-11-18 14:50:47,186:INFO: Dataset: zara1               Batch: 2/2	Loss 0.1344 (0.1325)
+2022-11-18 14:50:47,491:INFO: Dataset: zara2               Batch: 1/5	Loss 0.0930 (0.0930)
+2022-11-18 14:50:47,561:INFO: Dataset: zara2               Batch: 2/5	Loss 0.0927 (0.0928)
+2022-11-18 14:50:47,631:INFO: Dataset: zara2               Batch: 3/5	Loss 0.0885 (0.0914)
+2022-11-18 14:50:47,701:INFO: Dataset: zara2               Batch: 4/5	Loss 0.0961 (0.0925)
+2022-11-18 14:50:47,770:INFO: Dataset: zara2               Batch: 5/5	Loss 0.0920 (0.0924)
+2022-11-18 14:50:47,822:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_201.pth.tar
+2022-11-18 14:50:47,822:INFO: 
+===> EPOCH: 202 (P2)
+2022-11-18 14:50:47,823:INFO: - Computing loss (training)
+2022-11-18 14:50:48,160:INFO: Dataset: hotel               Batch: 1/4	Loss 0.1354 (0.1354)
+2022-11-18 14:50:48,398:INFO: Dataset: hotel               Batch: 2/4	Loss 0.1396 (0.1373)
+2022-11-18 14:50:48,568:INFO: Dataset: hotel               Batch: 3/4	Loss 0.1494 (0.1414)
+2022-11-18 14:50:48,683:INFO: Dataset: hotel               Batch: 4/4	Loss 0.1447 (0.1419)
+2022-11-18 14:50:49,091:INFO: Dataset: univ                Batch:  1/15	Loss 0.1337 (0.1337)
+2022-11-18 14:50:49,262:INFO: Dataset: univ                Batch:  2/15	Loss 0.1334 (0.1336)
+2022-11-18 14:50:49,426:INFO: Dataset: univ                Batch:  3/15	Loss 0.1317 (0.1329)
+2022-11-18 14:50:49,590:INFO: Dataset: univ                Batch:  4/15	Loss 0.1313 (0.1325)
+2022-11-18 14:50:49,745:INFO: Dataset: univ                Batch:  5/15	Loss 0.1321 (0.1324)
+2022-11-18 14:50:49,898:INFO: Dataset: univ                Batch:  6/15	Loss 0.1313 (0.1322)
+2022-11-18 14:50:50,052:INFO: Dataset: univ                Batch:  7/15	Loss 0.1305 (0.1320)
+2022-11-18 14:50:50,202:INFO: Dataset: univ                Batch:  8/15	Loss 0.1302 (0.1317)
+2022-11-18 14:50:50,355:INFO: Dataset: univ                Batch:  9/15	Loss 0.1313 (0.1317)
+2022-11-18 14:50:50,505:INFO: Dataset: univ                Batch: 10/15	Loss 0.1317 (0.1317)
+2022-11-18 14:50:50,658:INFO: Dataset: univ                Batch: 11/15	Loss 0.1307 (0.1316)
+2022-11-18 14:50:50,806:INFO: Dataset: univ                Batch: 12/15	Loss 0.1335 (0.1317)
+2022-11-18 14:50:50,961:INFO: Dataset: univ                Batch: 13/15	Loss 0.1313 (0.1317)
+2022-11-18 14:50:51,111:INFO: Dataset: univ                Batch: 14/15	Loss 0.1311 (0.1317)
+2022-11-18 14:50:51,191:INFO: Dataset: univ                Batch: 15/15	Loss 0.1350 (0.1317)
+2022-11-18 14:50:51,589:INFO: Dataset: zara1               Batch: 1/8	Loss 0.1745 (0.1745)
+2022-11-18 14:50:51,735:INFO: Dataset: zara1               Batch: 2/8	Loss 0.1735 (0.1740)
+2022-11-18 14:50:51,884:INFO: Dataset: zara1               Batch: 3/8	Loss 0.1798 (0.1759)
+2022-11-18 14:50:52,029:INFO: Dataset: zara1               Batch: 4/8	Loss 0.1791 (0.1767)
+2022-11-18 14:50:52,175:INFO: Dataset: zara1               Batch: 5/8	Loss 0.1699 (0.1753)
+2022-11-18 14:50:52,322:INFO: Dataset: zara1               Batch: 6/8	Loss 0.1746 (0.1752)
+2022-11-18 14:50:52,467:INFO: Dataset: zara1               Batch: 7/8	Loss 0.1744 (0.1750)
+2022-11-18 14:50:52,600:INFO: Dataset: zara1               Batch: 8/8	Loss 0.1717 (0.1747)
+2022-11-18 14:50:52,977:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1558 (0.1558)
+2022-11-18 14:50:53,127:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1527 (0.1542)
+2022-11-18 14:50:53,287:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1545 (0.1544)
+2022-11-18 14:50:53,436:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1550 (0.1545)
+2022-11-18 14:50:53,587:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1501 (0.1536)
+2022-11-18 14:50:53,739:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1450 (0.1519)
+2022-11-18 14:50:53,888:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1509 (0.1517)
+2022-11-18 14:50:54,048:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1523 (0.1518)
+2022-11-18 14:50:54,235:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1539 (0.1521)
+2022-11-18 14:50:54,401:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1479 (0.1516)
+2022-11-18 14:50:54,569:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1451 (0.1510)
+2022-11-18 14:50:54,740:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1530 (0.1511)
+2022-11-18 14:50:54,903:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1436 (0.1506)
+2022-11-18 14:50:55,069:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1485 (0.1504)
+2022-11-18 14:50:55,233:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1476 (0.1502)
+2022-11-18 14:50:55,404:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1563 (0.1506)
+2022-11-18 14:50:55,557:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1519 (0.1507)
+2022-11-18 14:50:55,708:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1412 (0.1503)
+2022-11-18 14:50:55,756:INFO: - Computing loss (validation)
+2022-11-18 14:50:56,031:INFO: Dataset: hotel               Batch: 1/2	Loss 0.1289 (0.1289)
+2022-11-18 14:50:56,067:INFO: Dataset: hotel               Batch: 2/2	Loss 0.1313 (0.1291)
+2022-11-18 14:50:56,408:INFO: Dataset: univ                Batch: 1/3	Loss 0.1318 (0.1318)
+2022-11-18 14:50:56,491:INFO: Dataset: univ                Batch: 2/3	Loss 0.1316 (0.1317)
+2022-11-18 14:50:56,567:INFO: Dataset: univ                Batch: 3/3	Loss 0.1291 (0.1308)
+2022-11-18 14:50:56,893:INFO: Dataset: zara1               Batch: 1/2	Loss 0.1781 (0.1781)
+2022-11-18 14:50:56,938:INFO: Dataset: zara1               Batch: 2/2	Loss 0.1791 (0.1783)
+2022-11-18 14:50:57,253:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1390 (0.1390)
+2022-11-18 14:50:57,326:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1391 (0.1390)
+2022-11-18 14:50:57,398:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1363 (0.1381)
+2022-11-18 14:50:57,469:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1342 (0.1371)
+2022-11-18 14:50:57,540:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1364 (0.1370)
+2022-11-18 14:50:57,592:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_202.pth.tar
+2022-11-18 14:50:57,592:INFO: 
+===> EPOCH: 203 (P2)
+2022-11-18 14:50:57,593:INFO: - Computing loss (training)
+2022-11-18 14:50:57,925:INFO: Dataset: hotel               Batch: 1/4	Loss 0.1832 (0.1832)
+2022-11-18 14:50:58,071:INFO: Dataset: hotel               Batch: 2/4	Loss 0.1861 (0.1846)
+2022-11-18 14:50:58,213:INFO: Dataset: hotel               Batch: 3/4	Loss 0.1899 (0.1862)
+2022-11-18 14:50:58,338:INFO: Dataset: hotel               Batch: 4/4	Loss 0.1929 (0.1873)
+2022-11-18 14:50:58,743:INFO: Dataset: univ                Batch:  1/15	Loss 0.1723 (0.1723)
+2022-11-18 14:50:58,896:INFO: Dataset: univ                Batch:  2/15	Loss 0.1767 (0.1744)
+2022-11-18 14:50:59,045:INFO: Dataset: univ                Batch:  3/15	Loss 0.1740 (0.1743)
+2022-11-18 14:50:59,207:INFO: Dataset: univ                Batch:  4/15	Loss 0.1736 (0.1741)
+2022-11-18 14:50:59,422:INFO: Dataset: univ                Batch:  5/15	Loss 0.1740 (0.1741)
+2022-11-18 14:50:59,592:INFO: Dataset: univ                Batch:  6/15	Loss 0.1723 (0.1738)
+2022-11-18 14:50:59,766:INFO: Dataset: univ                Batch:  7/15	Loss 0.1699 (0.1732)
+2022-11-18 14:50:59,921:INFO: Dataset: univ                Batch:  8/15	Loss 0.1757 (0.1735)
+2022-11-18 14:51:00,095:INFO: Dataset: univ                Batch:  9/15	Loss 0.1726 (0.1734)
+2022-11-18 14:51:00,251:INFO: Dataset: univ                Batch: 10/15	Loss 0.1719 (0.1733)
+2022-11-18 14:51:00,406:INFO: Dataset: univ                Batch: 11/15	Loss 0.1760 (0.1735)
+2022-11-18 14:51:00,558:INFO: Dataset: univ                Batch: 12/15	Loss 0.1745 (0.1736)
+2022-11-18 14:51:00,720:INFO: Dataset: univ                Batch: 13/15	Loss 0.1709 (0.1734)
+2022-11-18 14:51:00,883:INFO: Dataset: univ                Batch: 14/15	Loss 0.1756 (0.1735)
+2022-11-18 14:51:00,978:INFO: Dataset: univ                Batch: 15/15	Loss 0.1717 (0.1735)
+2022-11-18 14:51:01,402:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2185 (0.2185)
+2022-11-18 14:51:01,612:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2173 (0.2179)
+2022-11-18 14:51:01,764:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2180 (0.2179)
+2022-11-18 14:51:01,951:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2171 (0.2177)
+2022-11-18 14:51:02,103:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2230 (0.2187)
+2022-11-18 14:51:02,250:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2148 (0.2180)
+2022-11-18 14:51:02,396:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2210 (0.2185)
+2022-11-18 14:51:02,530:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2135 (0.2179)
+2022-11-18 14:51:02,921:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1879 (0.1879)
+2022-11-18 14:51:03,072:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1895 (0.1888)
+2022-11-18 14:51:03,219:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2011 (0.1927)
+2022-11-18 14:51:03,371:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1900 (0.1920)
+2022-11-18 14:51:03,519:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2057 (0.1947)
+2022-11-18 14:51:03,668:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1867 (0.1934)
+2022-11-18 14:51:03,815:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1932 (0.1934)
+2022-11-18 14:51:03,960:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1893 (0.1928)
+2022-11-18 14:51:04,115:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1907 (0.1926)
+2022-11-18 14:51:04,282:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1868 (0.1920)
+2022-11-18 14:51:04,433:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1939 (0.1921)
+2022-11-18 14:51:04,586:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1842 (0.1915)
+2022-11-18 14:51:04,741:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1953 (0.1918)
+2022-11-18 14:51:04,890:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1892 (0.1916)
+2022-11-18 14:51:05,042:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1959 (0.1919)
+2022-11-18 14:51:05,194:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1850 (0.1915)
+2022-11-18 14:51:05,376:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1846 (0.1911)
+2022-11-18 14:51:05,530:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1824 (0.1907)
+2022-11-18 14:51:05,577:INFO: - Computing loss (validation)
+2022-11-18 14:51:05,840:INFO: Dataset: hotel               Batch: 1/2	Loss 0.1695 (0.1695)
+2022-11-18 14:51:05,874:INFO: Dataset: hotel               Batch: 2/2	Loss 0.1747 (0.1700)
+2022-11-18 14:51:06,189:INFO: Dataset: univ                Batch: 1/3	Loss 0.1722 (0.1722)
+2022-11-18 14:51:06,268:INFO: Dataset: univ                Batch: 2/3	Loss 0.1707 (0.1714)
+2022-11-18 14:51:06,341:INFO: Dataset: univ                Batch: 3/3	Loss 0.1731 (0.1719)
+2022-11-18 14:51:06,649:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2169 (0.2169)
+2022-11-18 14:51:06,695:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2262 (0.2191)
+2022-11-18 14:51:07,009:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1824 (0.1824)
+2022-11-18 14:51:07,084:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1792 (0.1807)
+2022-11-18 14:51:07,159:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1865 (0.1826)
+2022-11-18 14:51:07,242:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1769 (0.1810)
+2022-11-18 14:51:07,324:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1715 (0.1791)
+2022-11-18 14:51:07,383:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_203.pth.tar
+2022-11-18 14:51:07,383:INFO: 
+===> EPOCH: 204 (P2)
+2022-11-18 14:51:07,384:INFO: - Computing loss (training)
+2022-11-18 14:51:07,740:INFO: Dataset: hotel               Batch: 1/4	Loss 0.2321 (0.2321)
+2022-11-18 14:51:07,891:INFO: Dataset: hotel               Batch: 2/4	Loss 0.2143 (0.2227)
+2022-11-18 14:51:08,041:INFO: Dataset: hotel               Batch: 3/4	Loss 0.2218 (0.2224)
+2022-11-18 14:51:08,157:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2228 (0.2225)
+2022-11-18 14:51:08,565:INFO: Dataset: univ                Batch:  1/15	Loss 0.2097 (0.2097)
+2022-11-18 14:51:08,720:INFO: Dataset: univ                Batch:  2/15	Loss 0.2155 (0.2122)
+2022-11-18 14:51:08,880:INFO: Dataset: univ                Batch:  3/15	Loss 0.2114 (0.2120)
+2022-11-18 14:51:09,037:INFO: Dataset: univ                Batch:  4/15	Loss 0.2130 (0.2122)
+2022-11-18 14:51:09,193:INFO: Dataset: univ                Batch:  5/15	Loss 0.2063 (0.2110)
+2022-11-18 14:51:09,349:INFO: Dataset: univ                Batch:  6/15	Loss 0.2075 (0.2104)
+2022-11-18 14:51:09,507:INFO: Dataset: univ                Batch:  7/15	Loss 0.2068 (0.2098)
+2022-11-18 14:51:09,670:INFO: Dataset: univ                Batch:  8/15	Loss 0.2075 (0.2095)
+2022-11-18 14:51:09,829:INFO: Dataset: univ                Batch:  9/15	Loss 0.2129 (0.2099)
+2022-11-18 14:51:09,981:INFO: Dataset: univ                Batch: 10/15	Loss 0.2091 (0.2098)
+2022-11-18 14:51:10,138:INFO: Dataset: univ                Batch: 11/15	Loss 0.2120 (0.2100)
+2022-11-18 14:51:10,299:INFO: Dataset: univ                Batch: 12/15	Loss 0.2088 (0.2099)
+2022-11-18 14:51:10,461:INFO: Dataset: univ                Batch: 13/15	Loss 0.2076 (0.2097)
+2022-11-18 14:51:10,630:INFO: Dataset: univ                Batch: 14/15	Loss 0.2114 (0.2098)
+2022-11-18 14:51:10,722:INFO: Dataset: univ                Batch: 15/15	Loss 0.1972 (0.2097)
+2022-11-18 14:51:11,117:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2617 (0.2617)
+2022-11-18 14:51:11,272:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2549 (0.2581)
+2022-11-18 14:51:11,460:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2599 (0.2587)
+2022-11-18 14:51:11,628:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2551 (0.2578)
+2022-11-18 14:51:11,807:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2490 (0.2560)
+2022-11-18 14:51:11,989:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2474 (0.2544)
+2022-11-18 14:51:12,166:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2533 (0.2542)
+2022-11-18 14:51:12,325:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2607 (0.2549)
+2022-11-18 14:51:12,750:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2352 (0.2352)
+2022-11-18 14:51:12,907:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2296 (0.2324)
+2022-11-18 14:51:13,060:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2309 (0.2320)
+2022-11-18 14:51:13,223:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2264 (0.2306)
+2022-11-18 14:51:13,393:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2243 (0.2294)
+2022-11-18 14:51:13,569:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2334 (0.2300)
+2022-11-18 14:51:13,740:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2361 (0.2308)
+2022-11-18 14:51:13,925:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2244 (0.2299)
+2022-11-18 14:51:14,084:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2297 (0.2299)
+2022-11-18 14:51:14,251:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2208 (0.2290)
+2022-11-18 14:51:14,420:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2242 (0.2286)
+2022-11-18 14:51:14,578:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2178 (0.2277)
+2022-11-18 14:51:14,782:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2237 (0.2274)
+2022-11-18 14:51:14,971:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2191 (0.2268)
+2022-11-18 14:51:15,145:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2247 (0.2267)
+2022-11-18 14:51:15,314:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2281 (0.2268)
+2022-11-18 14:51:15,489:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2220 (0.2265)
+2022-11-18 14:51:15,651:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2319 (0.2267)
+2022-11-18 14:51:15,712:INFO: - Computing loss (validation)
+2022-11-18 14:51:15,981:INFO: Dataset: hotel               Batch: 1/2	Loss 0.2081 (0.2081)
+2022-11-18 14:51:16,017:INFO: Dataset: hotel               Batch: 2/2	Loss 0.1950 (0.2070)
+2022-11-18 14:51:16,339:INFO: Dataset: univ                Batch: 1/3	Loss 0.2036 (0.2036)
+2022-11-18 14:51:16,420:INFO: Dataset: univ                Batch: 2/3	Loss 0.2087 (0.2060)
+2022-11-18 14:51:16,497:INFO: Dataset: univ                Batch: 3/3	Loss 0.2032 (0.2051)
+2022-11-18 14:51:16,843:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2542 (0.2542)
+2022-11-18 14:51:16,889:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2700 (0.2576)
+2022-11-18 14:51:17,210:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2151 (0.2151)
+2022-11-18 14:51:17,284:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2132 (0.2141)
+2022-11-18 14:51:17,368:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2142 (0.2141)
+2022-11-18 14:51:17,450:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2102 (0.2132)
+2022-11-18 14:51:17,529:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2160 (0.2137)
+2022-11-18 14:51:17,586:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_204.pth.tar
+2022-11-18 14:51:17,586:INFO: 
+===> EPOCH: 205 (P2)
+2022-11-18 14:51:17,586:INFO: - Computing loss (training)
+2022-11-18 14:51:17,917:INFO: Dataset: hotel               Batch: 1/4	Loss 0.2604 (0.2604)
+2022-11-18 14:51:18,065:INFO: Dataset: hotel               Batch: 2/4	Loss 0.2498 (0.2551)
+2022-11-18 14:51:18,216:INFO: Dataset: hotel               Batch: 3/4	Loss 0.2615 (0.2573)
+2022-11-18 14:51:18,342:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2622 (0.2581)
+2022-11-18 14:51:18,778:INFO: Dataset: univ                Batch:  1/15	Loss 0.2416 (0.2416)
+2022-11-18 14:51:18,944:INFO: Dataset: univ                Batch:  2/15	Loss 0.2439 (0.2427)
+2022-11-18 14:51:19,100:INFO: Dataset: univ                Batch:  3/15	Loss 0.2425 (0.2426)
+2022-11-18 14:51:19,258:INFO: Dataset: univ                Batch:  4/15	Loss 0.2414 (0.2423)
+2022-11-18 14:51:19,427:INFO: Dataset: univ                Batch:  5/15	Loss 0.2388 (0.2416)
+2022-11-18 14:51:19,599:INFO: Dataset: univ                Batch:  6/15	Loss 0.2388 (0.2411)
+2022-11-18 14:51:19,771:INFO: Dataset: univ                Batch:  7/15	Loss 0.2430 (0.2414)
+2022-11-18 14:51:19,940:INFO: Dataset: univ                Batch:  8/15	Loss 0.2394 (0.2411)
+2022-11-18 14:51:20,109:INFO: Dataset: univ                Batch:  9/15	Loss 0.2414 (0.2411)
+2022-11-18 14:51:20,278:INFO: Dataset: univ                Batch: 10/15	Loss 0.2347 (0.2405)
+2022-11-18 14:51:20,447:INFO: Dataset: univ                Batch: 11/15	Loss 0.2436 (0.2408)
+2022-11-18 14:51:20,621:INFO: Dataset: univ                Batch: 12/15	Loss 0.2400 (0.2407)
+2022-11-18 14:51:20,799:INFO: Dataset: univ                Batch: 13/15	Loss 0.2407 (0.2407)
+2022-11-18 14:51:20,969:INFO: Dataset: univ                Batch: 14/15	Loss 0.2437 (0.2409)
+2022-11-18 14:51:21,055:INFO: Dataset: univ                Batch: 15/15	Loss 0.2341 (0.2408)
+2022-11-18 14:51:21,483:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2950 (0.2950)
+2022-11-18 14:51:21,642:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2885 (0.2916)
+2022-11-18 14:51:21,802:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2815 (0.2883)
+2022-11-18 14:51:21,966:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2812 (0.2865)
+2022-11-18 14:51:22,136:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2924 (0.2876)
+2022-11-18 14:51:22,328:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2970 (0.2891)
+2022-11-18 14:51:22,509:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2932 (0.2896)
+2022-11-18 14:51:22,690:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2809 (0.2887)
+2022-11-18 14:51:23,134:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2722 (0.2722)
+2022-11-18 14:51:23,313:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2607 (0.2663)
+2022-11-18 14:51:23,487:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2673 (0.2666)
+2022-11-18 14:51:23,661:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2687 (0.2671)
+2022-11-18 14:51:23,835:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2656 (0.2668)
+2022-11-18 14:51:23,998:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2634 (0.2662)
+2022-11-18 14:51:24,161:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2639 (0.2659)
+2022-11-18 14:51:24,327:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2585 (0.2650)
+2022-11-18 14:51:24,497:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2692 (0.2654)
+2022-11-18 14:51:24,665:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2591 (0.2648)
+2022-11-18 14:51:24,920:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2756 (0.2658)
+2022-11-18 14:51:25,085:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2568 (0.2649)
+2022-11-18 14:51:25,252:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2559 (0.2643)
+2022-11-18 14:51:25,422:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2508 (0.2634)
+2022-11-18 14:51:25,592:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2641 (0.2634)
+2022-11-18 14:51:25,752:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2482 (0.2624)
+2022-11-18 14:51:25,914:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2498 (0.2616)
+2022-11-18 14:51:26,049:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2488 (0.2610)
+2022-11-18 14:51:26,096:INFO: - Computing loss (validation)
+2022-11-18 14:51:26,363:INFO: Dataset: hotel               Batch: 1/2	Loss 0.2311 (0.2311)
+2022-11-18 14:51:26,398:INFO: Dataset: hotel               Batch: 2/2	Loss 0.2303 (0.2310)
+2022-11-18 14:51:26,720:INFO: Dataset: univ                Batch: 1/3	Loss 0.2356 (0.2356)
+2022-11-18 14:51:26,796:INFO: Dataset: univ                Batch: 2/3	Loss 0.2372 (0.2364)
+2022-11-18 14:51:26,866:INFO: Dataset: univ                Batch: 3/3	Loss 0.2345 (0.2358)
+2022-11-18 14:51:27,180:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2855 (0.2855)
+2022-11-18 14:51:27,226:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2830 (0.2849)
+2022-11-18 14:51:27,549:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2401 (0.2401)
+2022-11-18 14:51:27,629:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2513 (0.2457)
+2022-11-18 14:51:27,708:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2470 (0.2461)
+2022-11-18 14:51:27,786:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2469 (0.2463)
+2022-11-18 14:51:27,859:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2435 (0.2457)
+2022-11-18 14:51:27,911:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_205.pth.tar
+2022-11-18 14:51:27,912:INFO: 
+===> EPOCH: 206 (P2)
+2022-11-18 14:51:27,912:INFO: - Computing loss (training)
+2022-11-18 14:51:28,265:INFO: Dataset: hotel               Batch: 1/4	Loss 0.2867 (0.2867)
+2022-11-18 14:51:28,420:INFO: Dataset: hotel               Batch: 2/4	Loss 0.2932 (0.2901)
+2022-11-18 14:51:28,581:INFO: Dataset: hotel               Batch: 3/4	Loss 0.2838 (0.2879)
+2022-11-18 14:51:28,706:INFO: Dataset: hotel               Batch: 4/4	Loss 0.2978 (0.2897)
+2022-11-18 14:51:29,120:INFO: Dataset: univ                Batch:  1/15	Loss 0.2710 (0.2710)
+2022-11-18 14:51:29,294:INFO: Dataset: univ                Batch:  2/15	Loss 0.2697 (0.2704)
+2022-11-18 14:51:29,475:INFO: Dataset: univ                Batch:  3/15	Loss 0.2787 (0.2731)
+2022-11-18 14:51:29,680:INFO: Dataset: univ                Batch:  4/15	Loss 0.2739 (0.2733)
+2022-11-18 14:51:29,858:INFO: Dataset: univ                Batch:  5/15	Loss 0.2741 (0.2734)
+2022-11-18 14:51:30,042:INFO: Dataset: univ                Batch:  6/15	Loss 0.2679 (0.2725)
+2022-11-18 14:51:30,218:INFO: Dataset: univ                Batch:  7/15	Loss 0.2683 (0.2719)
+2022-11-18 14:51:30,396:INFO: Dataset: univ                Batch:  8/15	Loss 0.2673 (0.2713)
+2022-11-18 14:51:30,579:INFO: Dataset: univ                Batch:  9/15	Loss 0.2655 (0.2706)
+2022-11-18 14:51:30,791:INFO: Dataset: univ                Batch: 10/15	Loss 0.2687 (0.2704)
+2022-11-18 14:51:30,990:INFO: Dataset: univ                Batch: 11/15	Loss 0.2659 (0.2700)
+2022-11-18 14:51:31,201:INFO: Dataset: univ                Batch: 12/15	Loss 0.2656 (0.2696)
+2022-11-18 14:51:31,379:INFO: Dataset: univ                Batch: 13/15	Loss 0.2681 (0.2695)
+2022-11-18 14:51:31,562:INFO: Dataset: univ                Batch: 14/15	Loss 0.2695 (0.2695)
+2022-11-18 14:51:31,669:INFO: Dataset: univ                Batch: 15/15	Loss 0.2605 (0.2693)
+2022-11-18 14:51:32,107:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3224 (0.3224)
+2022-11-18 14:51:32,275:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3237 (0.3230)
+2022-11-18 14:51:32,444:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3018 (0.3155)
+2022-11-18 14:51:32,599:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3291 (0.3187)
+2022-11-18 14:51:32,762:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3189 (0.3187)
+2022-11-18 14:51:32,923:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3112 (0.3175)
+2022-11-18 14:51:33,089:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3166 (0.3174)
+2022-11-18 14:51:33,226:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3217 (0.3178)
+2022-11-18 14:51:33,680:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3017 (0.3017)
+2022-11-18 14:51:33,869:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2909 (0.2961)
+2022-11-18 14:51:34,055:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2897 (0.2938)
+2022-11-18 14:51:34,245:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2824 (0.2909)
+2022-11-18 14:51:34,419:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2927 (0.2913)
+2022-11-18 14:51:34,600:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2896 (0.2910)
+2022-11-18 14:51:34,784:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2934 (0.2914)
+2022-11-18 14:51:34,954:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2919 (0.2914)
+2022-11-18 14:51:35,119:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2828 (0.2905)
+2022-11-18 14:51:35,291:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2791 (0.2893)
+2022-11-18 14:51:35,460:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2919 (0.2895)
+2022-11-18 14:51:35,631:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2871 (0.2893)
+2022-11-18 14:51:35,806:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2854 (0.2890)
+2022-11-18 14:51:36,005:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2845 (0.2887)
+2022-11-18 14:51:36,179:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2888 (0.2887)
+2022-11-18 14:51:36,345:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2930 (0.2890)
+2022-11-18 14:51:36,515:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2890 (0.2890)
+2022-11-18 14:51:36,671:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2840 (0.2888)
+2022-11-18 14:51:36,717:INFO: - Computing loss (validation)
+2022-11-18 14:51:36,983:INFO: Dataset: hotel               Batch: 1/2	Loss 0.2684 (0.2684)
+2022-11-18 14:51:37,017:INFO: Dataset: hotel               Batch: 2/2	Loss 0.3136 (0.2718)
+2022-11-18 14:51:37,329:INFO: Dataset: univ                Batch: 1/3	Loss 0.2640 (0.2640)
+2022-11-18 14:51:37,409:INFO: Dataset: univ                Batch: 2/3	Loss 0.2698 (0.2666)
+2022-11-18 14:51:37,486:INFO: Dataset: univ                Batch: 3/3	Loss 0.2580 (0.2637)
+2022-11-18 14:51:37,793:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3185 (0.3185)
+2022-11-18 14:51:37,836:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3147 (0.3176)
+2022-11-18 14:51:38,141:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2732 (0.2732)
+2022-11-18 14:51:38,217:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2815 (0.2776)
+2022-11-18 14:51:38,292:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2686 (0.2747)
+2022-11-18 14:51:38,367:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2787 (0.2758)
+2022-11-18 14:51:38,440:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2766 (0.2760)
+2022-11-18 14:51:38,491:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_206.pth.tar
+2022-11-18 14:51:38,491:INFO: 
+===> EPOCH: 207 (P2)
+2022-11-18 14:51:38,491:INFO: - Computing loss (training)
+2022-11-18 14:51:38,832:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3106 (0.3106)
+2022-11-18 14:51:38,982:INFO: Dataset: hotel               Batch: 2/4	Loss 0.3156 (0.3130)
+2022-11-18 14:51:39,126:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3133 (0.3131)
+2022-11-18 14:51:39,239:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3206 (0.3144)
+2022-11-18 14:51:39,638:INFO: Dataset: univ                Batch:  1/15	Loss 0.2932 (0.2932)
+2022-11-18 14:51:39,800:INFO: Dataset: univ                Batch:  2/15	Loss 0.2963 (0.2947)
+2022-11-18 14:51:39,954:INFO: Dataset: univ                Batch:  3/15	Loss 0.2908 (0.2935)
+2022-11-18 14:51:40,103:INFO: Dataset: univ                Batch:  4/15	Loss 0.2945 (0.2937)
+2022-11-18 14:51:40,253:INFO: Dataset: univ                Batch:  5/15	Loss 0.2948 (0.2939)
+2022-11-18 14:51:40,405:INFO: Dataset: univ                Batch:  6/15	Loss 0.3003 (0.2949)
+2022-11-18 14:51:40,558:INFO: Dataset: univ                Batch:  7/15	Loss 0.2908 (0.2943)
+2022-11-18 14:51:40,724:INFO: Dataset: univ                Batch:  8/15	Loss 0.2946 (0.2943)
+2022-11-18 14:51:40,883:INFO: Dataset: univ                Batch:  9/15	Loss 0.2963 (0.2945)
+2022-11-18 14:51:41,039:INFO: Dataset: univ                Batch: 10/15	Loss 0.2921 (0.2943)
+2022-11-18 14:51:41,207:INFO: Dataset: univ                Batch: 11/15	Loss 0.2903 (0.2940)
+2022-11-18 14:51:41,374:INFO: Dataset: univ                Batch: 12/15	Loss 0.2935 (0.2939)
+2022-11-18 14:51:41,532:INFO: Dataset: univ                Batch: 13/15	Loss 0.2891 (0.2936)
+2022-11-18 14:51:41,683:INFO: Dataset: univ                Batch: 14/15	Loss 0.2956 (0.2937)
+2022-11-18 14:51:41,764:INFO: Dataset: univ                Batch: 15/15	Loss 0.2832 (0.2936)
+2022-11-18 14:51:42,153:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3473 (0.3473)
+2022-11-18 14:51:42,304:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3449 (0.3461)
+2022-11-18 14:51:42,450:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3422 (0.3449)
+2022-11-18 14:51:42,592:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3555 (0.3475)
+2022-11-18 14:51:42,738:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3306 (0.3442)
+2022-11-18 14:51:42,887:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3346 (0.3427)
+2022-11-18 14:51:43,034:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3468 (0.3434)
+2022-11-18 14:51:43,166:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3458 (0.3436)
+2022-11-18 14:51:43,568:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3231 (0.3231)
+2022-11-18 14:51:43,721:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3227 (0.3229)
+2022-11-18 14:51:43,874:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3091 (0.3184)
+2022-11-18 14:51:44,023:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3117 (0.3167)
+2022-11-18 14:51:44,190:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3141 (0.3162)
+2022-11-18 14:51:44,346:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3097 (0.3153)
+2022-11-18 14:51:44,493:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3063 (0.3139)
+2022-11-18 14:51:44,639:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3118 (0.3137)
+2022-11-18 14:51:44,787:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3170 (0.3140)
+2022-11-18 14:51:44,945:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3157 (0.3142)
+2022-11-18 14:51:45,126:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3181 (0.3145)
+2022-11-18 14:51:45,307:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3155 (0.3146)
+2022-11-18 14:51:45,486:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3017 (0.3136)
+2022-11-18 14:51:45,639:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3075 (0.3131)
+2022-11-18 14:51:45,920:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3007 (0.3122)
+2022-11-18 14:51:46,108:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3120 (0.3122)
+2022-11-18 14:51:46,269:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3130 (0.3122)
+2022-11-18 14:51:46,409:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3019 (0.3118)
+2022-11-18 14:51:46,454:INFO: - Computing loss (validation)
+2022-11-18 14:51:46,711:INFO: Dataset: hotel               Batch: 1/2	Loss 0.2927 (0.2927)
+2022-11-18 14:51:46,745:INFO: Dataset: hotel               Batch: 2/2	Loss 0.2807 (0.2916)
+2022-11-18 14:51:47,056:INFO: Dataset: univ                Batch: 1/3	Loss 0.2833 (0.2833)
+2022-11-18 14:51:47,132:INFO: Dataset: univ                Batch: 2/3	Loss 0.2896 (0.2860)
+2022-11-18 14:51:47,203:INFO: Dataset: univ                Batch: 3/3	Loss 0.2878 (0.2866)
+2022-11-18 14:51:47,509:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3432 (0.3432)
+2022-11-18 14:51:47,553:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3642 (0.3485)
+2022-11-18 14:51:47,856:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2954 (0.2954)
+2022-11-18 14:51:47,932:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2978 (0.2967)
+2022-11-18 14:51:48,006:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2990 (0.2975)
+2022-11-18 14:51:48,081:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2996 (0.2980)
+2022-11-18 14:51:48,155:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3067 (0.2999)
+2022-11-18 14:51:48,206:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_207.pth.tar
+2022-11-18 14:51:48,206:INFO: 
+===> EPOCH: 208 (P2)
+2022-11-18 14:51:48,206:INFO: - Computing loss (training)
+2022-11-18 14:51:48,543:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3319 (0.3319)
+2022-11-18 14:51:48,690:INFO: Dataset: hotel               Batch: 2/4	Loss 0.3439 (0.3382)
+2022-11-18 14:51:48,844:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3430 (0.3399)
+2022-11-18 14:51:48,967:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3239 (0.3370)
+2022-11-18 14:51:49,371:INFO: Dataset: univ                Batch:  1/15	Loss 0.3161 (0.3161)
+2022-11-18 14:51:49,525:INFO: Dataset: univ                Batch:  2/15	Loss 0.3199 (0.3180)
+2022-11-18 14:51:49,689:INFO: Dataset: univ                Batch:  3/15	Loss 0.3189 (0.3183)
+2022-11-18 14:51:49,846:INFO: Dataset: univ                Batch:  4/15	Loss 0.3151 (0.3175)
+2022-11-18 14:51:50,003:INFO: Dataset: univ                Batch:  5/15	Loss 0.3208 (0.3182)
+2022-11-18 14:51:50,154:INFO: Dataset: univ                Batch:  6/15	Loss 0.3199 (0.3184)
+2022-11-18 14:51:50,311:INFO: Dataset: univ                Batch:  7/15	Loss 0.3171 (0.3182)
+2022-11-18 14:51:50,467:INFO: Dataset: univ                Batch:  8/15	Loss 0.3200 (0.3184)
+2022-11-18 14:51:50,624:INFO: Dataset: univ                Batch:  9/15	Loss 0.3141 (0.3179)
+2022-11-18 14:51:50,775:INFO: Dataset: univ                Batch: 10/15	Loss 0.3170 (0.3178)
+2022-11-18 14:51:50,943:INFO: Dataset: univ                Batch: 11/15	Loss 0.3174 (0.3178)
+2022-11-18 14:51:51,103:INFO: Dataset: univ                Batch: 12/15	Loss 0.3152 (0.3176)
+2022-11-18 14:51:51,266:INFO: Dataset: univ                Batch: 13/15	Loss 0.3176 (0.3176)
+2022-11-18 14:51:51,427:INFO: Dataset: univ                Batch: 14/15	Loss 0.3149 (0.3174)
+2022-11-18 14:51:51,513:INFO: Dataset: univ                Batch: 15/15	Loss 0.3141 (0.3174)
+2022-11-18 14:51:51,905:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3716 (0.3716)
+2022-11-18 14:51:52,051:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3691 (0.3704)
+2022-11-18 14:51:52,203:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3649 (0.3684)
+2022-11-18 14:51:52,351:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3618 (0.3667)
+2022-11-18 14:51:52,499:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3694 (0.3673)
+2022-11-18 14:51:52,645:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3672 (0.3673)
+2022-11-18 14:51:52,792:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3593 (0.3663)
+2022-11-18 14:51:52,930:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3656 (0.3662)
+2022-11-18 14:51:53,326:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3351 (0.3351)
+2022-11-18 14:51:53,478:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3348 (0.3350)
+2022-11-18 14:51:53,624:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3382 (0.3362)
+2022-11-18 14:51:53,774:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3324 (0.3353)
+2022-11-18 14:51:53,928:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3422 (0.3366)
+2022-11-18 14:51:54,074:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3422 (0.3375)
+2022-11-18 14:51:54,220:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3267 (0.3360)
+2022-11-18 14:51:54,364:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3427 (0.3369)
+2022-11-18 14:51:54,511:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3397 (0.3372)
+2022-11-18 14:51:54,658:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3390 (0.3374)
+2022-11-18 14:51:54,806:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3278 (0.3365)
+2022-11-18 14:51:54,950:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3375 (0.3366)
+2022-11-18 14:51:55,098:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3256 (0.3358)
+2022-11-18 14:51:55,248:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3223 (0.3349)
+2022-11-18 14:51:55,400:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3323 (0.3347)
+2022-11-18 14:51:55,551:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3361 (0.3348)
+2022-11-18 14:51:55,703:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3306 (0.3346)
+2022-11-18 14:51:55,842:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3231 (0.3340)
+2022-11-18 14:51:55,887:INFO: - Computing loss (validation)
+2022-11-18 14:51:56,150:INFO: Dataset: hotel               Batch: 1/2	Loss 0.3193 (0.3193)
+2022-11-18 14:51:56,184:INFO: Dataset: hotel               Batch: 2/2	Loss 0.3535 (0.3213)
+2022-11-18 14:51:56,492:INFO: Dataset: univ                Batch: 1/3	Loss 0.3122 (0.3122)
+2022-11-18 14:51:56,567:INFO: Dataset: univ                Batch: 2/3	Loss 0.3036 (0.3076)
+2022-11-18 14:51:56,636:INFO: Dataset: univ                Batch: 3/3	Loss 0.3046 (0.3067)
+2022-11-18 14:51:56,932:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3578 (0.3578)
+2022-11-18 14:51:56,977:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3732 (0.3615)
+2022-11-18 14:51:57,281:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3110 (0.3110)
+2022-11-18 14:51:57,356:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3249 (0.3184)
+2022-11-18 14:51:57,429:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3258 (0.3210)
+2022-11-18 14:51:57,501:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3198 (0.3207)
+2022-11-18 14:51:57,572:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3148 (0.3196)
+2022-11-18 14:51:57,628:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_208.pth.tar
+2022-11-18 14:51:57,628:INFO: 
+===> EPOCH: 209 (P2)
+2022-11-18 14:51:57,628:INFO: - Computing loss (training)
+2022-11-18 14:51:57,977:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3608 (0.3608)
+2022-11-18 14:51:58,128:INFO: Dataset: hotel               Batch: 2/4	Loss 0.3568 (0.3587)
+2022-11-18 14:51:58,277:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3449 (0.3540)
+2022-11-18 14:51:58,396:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3610 (0.3552)
+2022-11-18 14:51:58,809:INFO: Dataset: univ                Batch:  1/15	Loss 0.3411 (0.3411)
+2022-11-18 14:51:58,966:INFO: Dataset: univ                Batch:  2/15	Loss 0.3399 (0.3405)
+2022-11-18 14:51:59,154:INFO: Dataset: univ                Batch:  3/15	Loss 0.3406 (0.3405)
+2022-11-18 14:51:59,312:INFO: Dataset: univ                Batch:  4/15	Loss 0.3369 (0.3396)
+2022-11-18 14:51:59,474:INFO: Dataset: univ                Batch:  5/15	Loss 0.3377 (0.3393)
+2022-11-18 14:51:59,630:INFO: Dataset: univ                Batch:  6/15	Loss 0.3349 (0.3385)
+2022-11-18 14:51:59,782:INFO: Dataset: univ                Batch:  7/15	Loss 0.3383 (0.3385)
+2022-11-18 14:51:59,934:INFO: Dataset: univ                Batch:  8/15	Loss 0.3464 (0.3395)
+2022-11-18 14:52:00,087:INFO: Dataset: univ                Batch:  9/15	Loss 0.3414 (0.3397)
+2022-11-18 14:52:00,241:INFO: Dataset: univ                Batch: 10/15	Loss 0.3369 (0.3394)
+2022-11-18 14:52:00,405:INFO: Dataset: univ                Batch: 11/15	Loss 0.3374 (0.3392)
+2022-11-18 14:52:00,563:INFO: Dataset: univ                Batch: 12/15	Loss 0.3349 (0.3389)
+2022-11-18 14:52:00,717:INFO: Dataset: univ                Batch: 13/15	Loss 0.3343 (0.3385)
+2022-11-18 14:52:00,874:INFO: Dataset: univ                Batch: 14/15	Loss 0.3346 (0.3382)
+2022-11-18 14:52:00,956:INFO: Dataset: univ                Batch: 15/15	Loss 0.3405 (0.3383)
+2022-11-18 14:52:01,348:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3784 (0.3784)
+2022-11-18 14:52:01,497:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3726 (0.3755)
+2022-11-18 14:52:01,644:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4032 (0.3846)
+2022-11-18 14:52:01,792:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3916 (0.3862)
+2022-11-18 14:52:01,942:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3728 (0.3834)
+2022-11-18 14:52:02,091:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3866 (0.3839)
+2022-11-18 14:52:02,240:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3843 (0.3840)
+2022-11-18 14:52:02,379:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3737 (0.3829)
+2022-11-18 14:52:02,761:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3530 (0.3530)
+2022-11-18 14:52:02,914:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3488 (0.3509)
+2022-11-18 14:52:03,066:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3668 (0.3555)
+2022-11-18 14:52:03,215:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3588 (0.3562)
+2022-11-18 14:52:03,367:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3576 (0.3565)
+2022-11-18 14:52:03,519:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3546 (0.3562)
+2022-11-18 14:52:03,669:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3471 (0.3547)
+2022-11-18 14:52:03,818:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3480 (0.3538)
+2022-11-18 14:52:03,975:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3611 (0.3546)
+2022-11-18 14:52:04,122:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3616 (0.3553)
+2022-11-18 14:52:04,276:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3485 (0.3547)
+2022-11-18 14:52:04,427:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3592 (0.3550)
+2022-11-18 14:52:04,576:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3455 (0.3542)
+2022-11-18 14:52:04,724:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3502 (0.3539)
+2022-11-18 14:52:04,875:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3393 (0.3529)
+2022-11-18 14:52:05,025:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3537 (0.3530)
+2022-11-18 14:52:05,175:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3605 (0.3535)
+2022-11-18 14:52:05,311:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3578 (0.3536)
+2022-11-18 14:52:05,355:INFO: - Computing loss (validation)
+2022-11-18 14:52:05,620:INFO: Dataset: hotel               Batch: 1/2	Loss 0.3360 (0.3360)
+2022-11-18 14:52:05,654:INFO: Dataset: hotel               Batch: 2/2	Loss 0.3433 (0.3365)
+2022-11-18 14:52:05,958:INFO: Dataset: univ                Batch: 1/3	Loss 0.3282 (0.3282)
+2022-11-18 14:52:06,037:INFO: Dataset: univ                Batch: 2/3	Loss 0.3270 (0.3276)
+2022-11-18 14:52:06,109:INFO: Dataset: univ                Batch: 3/3	Loss 0.3333 (0.3294)
+2022-11-18 14:52:06,411:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3944 (0.3944)
+2022-11-18 14:52:06,457:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3786 (0.3907)
+2022-11-18 14:52:06,773:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3466 (0.3466)
+2022-11-18 14:52:06,846:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3338 (0.3399)
+2022-11-18 14:52:06,918:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3454 (0.3419)
+2022-11-18 14:52:06,990:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3448 (0.3426)
+2022-11-18 14:52:07,062:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3438 (0.3429)
+2022-11-18 14:52:07,113:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_209.pth.tar
+2022-11-18 14:52:07,113:INFO: 
+===> EPOCH: 210 (P2)
+2022-11-18 14:52:07,114:INFO: - Computing loss (training)
+2022-11-18 14:52:07,447:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3768 (0.3768)
+2022-11-18 14:52:07,595:INFO: Dataset: hotel               Batch: 2/4	Loss 0.3873 (0.3820)
+2022-11-18 14:52:07,744:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3638 (0.3760)
+2022-11-18 14:52:07,858:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3980 (0.3796)
+2022-11-18 14:52:08,249:INFO: Dataset: univ                Batch:  1/15	Loss 0.3526 (0.3526)
+2022-11-18 14:52:08,402:INFO: Dataset: univ                Batch:  2/15	Loss 0.3602 (0.3561)
+2022-11-18 14:52:08,557:INFO: Dataset: univ                Batch:  3/15	Loss 0.3599 (0.3575)
+2022-11-18 14:52:08,707:INFO: Dataset: univ                Batch:  4/15	Loss 0.3563 (0.3572)
+2022-11-18 14:52:08,865:INFO: Dataset: univ                Batch:  5/15	Loss 0.3555 (0.3568)
+2022-11-18 14:52:09,017:INFO: Dataset: univ                Batch:  6/15	Loss 0.3596 (0.3573)
+2022-11-18 14:52:09,168:INFO: Dataset: univ                Batch:  7/15	Loss 0.3553 (0.3570)
+2022-11-18 14:52:09,318:INFO: Dataset: univ                Batch:  8/15	Loss 0.3537 (0.3566)
+2022-11-18 14:52:09,472:INFO: Dataset: univ                Batch:  9/15	Loss 0.3532 (0.3562)
+2022-11-18 14:52:09,624:INFO: Dataset: univ                Batch: 10/15	Loss 0.3520 (0.3558)
+2022-11-18 14:52:09,779:INFO: Dataset: univ                Batch: 11/15	Loss 0.3502 (0.3553)
+2022-11-18 14:52:09,934:INFO: Dataset: univ                Batch: 12/15	Loss 0.3490 (0.3548)
+2022-11-18 14:52:10,089:INFO: Dataset: univ                Batch: 13/15	Loss 0.3561 (0.3549)
+2022-11-18 14:52:10,240:INFO: Dataset: univ                Batch: 14/15	Loss 0.3510 (0.3546)
+2022-11-18 14:52:10,320:INFO: Dataset: univ                Batch: 15/15	Loss 0.3542 (0.3546)
+2022-11-18 14:52:10,703:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3954 (0.3954)
+2022-11-18 14:52:10,849:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4102 (0.4027)
+2022-11-18 14:52:10,996:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4068 (0.4039)
+2022-11-18 14:52:11,150:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4081 (0.4050)
+2022-11-18 14:52:11,308:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4189 (0.4077)
+2022-11-18 14:52:11,466:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4013 (0.4067)
+2022-11-18 14:52:11,628:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4019 (0.4060)
+2022-11-18 14:52:11,769:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4075 (0.4061)
+2022-11-18 14:52:12,164:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3669 (0.3669)
+2022-11-18 14:52:12,316:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3805 (0.3734)
+2022-11-18 14:52:12,465:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3791 (0.3754)
+2022-11-18 14:52:12,620:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3787 (0.3762)
+2022-11-18 14:52:12,773:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3743 (0.3758)
+2022-11-18 14:52:12,927:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3766 (0.3760)
+2022-11-18 14:52:13,078:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3602 (0.3735)
+2022-11-18 14:52:13,228:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3755 (0.3738)
+2022-11-18 14:52:13,378:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3820 (0.3746)
+2022-11-18 14:52:13,525:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3678 (0.3739)
+2022-11-18 14:52:13,692:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3638 (0.3731)
+2022-11-18 14:52:13,867:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3675 (0.3726)
+2022-11-18 14:52:14,024:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3766 (0.3729)
+2022-11-18 14:52:14,180:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3657 (0.3725)
+2022-11-18 14:52:14,445:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3767 (0.3728)
+2022-11-18 14:52:14,680:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3831 (0.3734)
+2022-11-18 14:52:14,870:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3546 (0.3725)
+2022-11-18 14:52:15,015:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3688 (0.3723)
+2022-11-18 14:52:15,063:INFO: - Computing loss (validation)
+2022-11-18 14:52:15,333:INFO: Dataset: hotel               Batch: 1/2	Loss 0.3624 (0.3624)
+2022-11-18 14:52:15,368:INFO: Dataset: hotel               Batch: 2/2	Loss 0.3574 (0.3620)
+2022-11-18 14:52:15,685:INFO: Dataset: univ                Batch: 1/3	Loss 0.3517 (0.3517)
+2022-11-18 14:52:15,760:INFO: Dataset: univ                Batch: 2/3	Loss 0.3482 (0.3499)
+2022-11-18 14:52:15,832:INFO: Dataset: univ                Batch: 3/3	Loss 0.3511 (0.3503)
+2022-11-18 14:52:16,127:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4073 (0.4073)
+2022-11-18 14:52:16,170:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4118 (0.4083)
+2022-11-18 14:52:16,466:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3581 (0.3581)
+2022-11-18 14:52:16,543:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3552 (0.3566)
+2022-11-18 14:52:16,619:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3518 (0.3550)
+2022-11-18 14:52:16,696:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3519 (0.3543)
+2022-11-18 14:52:16,771:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3689 (0.3570)
+2022-11-18 14:52:16,826:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_210.pth.tar
+2022-11-18 14:52:16,826:INFO: 
+===> EPOCH: 211 (P2)
+2022-11-18 14:52:16,826:INFO: - Computing loss (training)
+2022-11-18 14:52:17,180:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3871 (0.3871)
+2022-11-18 14:52:17,340:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4056 (0.3964)
+2022-11-18 14:52:17,500:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3845 (0.3922)
+2022-11-18 14:52:17,625:INFO: Dataset: hotel               Batch: 4/4	Loss 0.3974 (0.3930)
+2022-11-18 14:52:18,012:INFO: Dataset: univ                Batch:  1/15	Loss 0.3658 (0.3658)
+2022-11-18 14:52:18,175:INFO: Dataset: univ                Batch:  2/15	Loss 0.3740 (0.3697)
+2022-11-18 14:52:18,349:INFO: Dataset: univ                Batch:  3/15	Loss 0.3756 (0.3716)
+2022-11-18 14:52:18,511:INFO: Dataset: univ                Batch:  4/15	Loss 0.3780 (0.3732)
+2022-11-18 14:52:18,672:INFO: Dataset: univ                Batch:  5/15	Loss 0.3773 (0.3740)
+2022-11-18 14:52:18,839:INFO: Dataset: univ                Batch:  6/15	Loss 0.3695 (0.3732)
+2022-11-18 14:52:19,001:INFO: Dataset: univ                Batch:  7/15	Loss 0.3732 (0.3732)
+2022-11-18 14:52:19,163:INFO: Dataset: univ                Batch:  8/15	Loss 0.3731 (0.3732)
+2022-11-18 14:52:19,324:INFO: Dataset: univ                Batch:  9/15	Loss 0.3724 (0.3731)
+2022-11-18 14:52:19,485:INFO: Dataset: univ                Batch: 10/15	Loss 0.3738 (0.3732)
+2022-11-18 14:52:19,650:INFO: Dataset: univ                Batch: 11/15	Loss 0.3739 (0.3732)
+2022-11-18 14:52:19,814:INFO: Dataset: univ                Batch: 12/15	Loss 0.3712 (0.3731)
+2022-11-18 14:52:19,977:INFO: Dataset: univ                Batch: 13/15	Loss 0.3673 (0.3726)
+2022-11-18 14:52:20,140:INFO: Dataset: univ                Batch: 14/15	Loss 0.3720 (0.3726)
+2022-11-18 14:52:20,231:INFO: Dataset: univ                Batch: 15/15	Loss 0.3870 (0.3728)
+2022-11-18 14:52:20,626:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4243 (0.4243)
+2022-11-18 14:52:20,778:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4101 (0.4178)
+2022-11-18 14:52:20,936:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4304 (0.4219)
+2022-11-18 14:52:21,094:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4190 (0.4212)
+2022-11-18 14:52:21,252:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4122 (0.4192)
+2022-11-18 14:52:21,411:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4267 (0.4204)
+2022-11-18 14:52:21,568:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4319 (0.4219)
+2022-11-18 14:52:21,709:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4167 (0.4213)
+2022-11-18 14:52:22,111:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3903 (0.3903)
+2022-11-18 14:52:22,278:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3842 (0.3872)
+2022-11-18 14:52:22,444:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3980 (0.3908)
+2022-11-18 14:52:22,604:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4021 (0.3936)
+2022-11-18 14:52:22,753:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3863 (0.3922)
+2022-11-18 14:52:22,914:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4016 (0.3936)
+2022-11-18 14:52:23,062:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3819 (0.3919)
+2022-11-18 14:52:23,212:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3840 (0.3910)
+2022-11-18 14:52:23,359:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3791 (0.3897)
+2022-11-18 14:52:23,509:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3921 (0.3899)
+2022-11-18 14:52:23,678:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3883 (0.3898)
+2022-11-18 14:52:23,847:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3882 (0.3897)
+2022-11-18 14:52:24,000:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3862 (0.3894)
+2022-11-18 14:52:24,148:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3831 (0.3890)
+2022-11-18 14:52:24,297:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3772 (0.3882)
+2022-11-18 14:52:24,445:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3809 (0.3877)
+2022-11-18 14:52:24,612:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3778 (0.3871)
+2022-11-18 14:52:24,756:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3883 (0.3872)
+2022-11-18 14:52:24,802:INFO: - Computing loss (validation)
+2022-11-18 14:52:25,078:INFO: Dataset: hotel               Batch: 1/2	Loss 0.3632 (0.3632)
+2022-11-18 14:52:25,113:INFO: Dataset: hotel               Batch: 2/2	Loss 0.3874 (0.3655)
+2022-11-18 14:52:25,427:INFO: Dataset: univ                Batch: 1/3	Loss 0.3630 (0.3630)
+2022-11-18 14:52:25,509:INFO: Dataset: univ                Batch: 2/3	Loss 0.3604 (0.3617)
+2022-11-18 14:52:25,587:INFO: Dataset: univ                Batch: 3/3	Loss 0.3636 (0.3623)
+2022-11-18 14:52:25,901:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4223 (0.4223)
+2022-11-18 14:52:25,947:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4151 (0.4204)
+2022-11-18 14:52:26,247:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3740 (0.3740)
+2022-11-18 14:52:26,328:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3721 (0.3730)
+2022-11-18 14:52:26,407:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3885 (0.3783)
+2022-11-18 14:52:26,481:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3869 (0.3804)
+2022-11-18 14:52:26,554:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3690 (0.3782)
+2022-11-18 14:52:26,609:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_211.pth.tar
+2022-11-18 14:52:26,609:INFO: 
+===> EPOCH: 212 (P2)
+2022-11-18 14:52:26,610:INFO: - Computing loss (training)
+2022-11-18 14:52:26,946:INFO: Dataset: hotel               Batch: 1/4	Loss 0.3886 (0.3886)
+2022-11-18 14:52:27,094:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4090 (0.3987)
+2022-11-18 14:52:27,245:INFO: Dataset: hotel               Batch: 3/4	Loss 0.3910 (0.3962)
+2022-11-18 14:52:27,369:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4188 (0.4001)
+2022-11-18 14:52:27,799:INFO: Dataset: univ                Batch:  1/15	Loss 0.3807 (0.3807)
+2022-11-18 14:52:27,962:INFO: Dataset: univ                Batch:  2/15	Loss 0.3874 (0.3840)
+2022-11-18 14:52:28,131:INFO: Dataset: univ                Batch:  3/15	Loss 0.3814 (0.3831)
+2022-11-18 14:52:28,289:INFO: Dataset: univ                Batch:  4/15	Loss 0.3884 (0.3845)
+2022-11-18 14:52:28,451:INFO: Dataset: univ                Batch:  5/15	Loss 0.3865 (0.3849)
+2022-11-18 14:52:28,603:INFO: Dataset: univ                Batch:  6/15	Loss 0.3861 (0.3851)
+2022-11-18 14:52:28,757:INFO: Dataset: univ                Batch:  7/15	Loss 0.3831 (0.3848)
+2022-11-18 14:52:28,912:INFO: Dataset: univ                Batch:  8/15	Loss 0.3906 (0.3856)
+2022-11-18 14:52:29,065:INFO: Dataset: univ                Batch:  9/15	Loss 0.3866 (0.3857)
+2022-11-18 14:52:29,219:INFO: Dataset: univ                Batch: 10/15	Loss 0.3887 (0.3860)
+2022-11-18 14:52:29,378:INFO: Dataset: univ                Batch: 11/15	Loss 0.3810 (0.3856)
+2022-11-18 14:52:29,533:INFO: Dataset: univ                Batch: 12/15	Loss 0.3847 (0.3855)
+2022-11-18 14:52:29,686:INFO: Dataset: univ                Batch: 13/15	Loss 0.3787 (0.3850)
+2022-11-18 14:52:29,845:INFO: Dataset: univ                Batch: 14/15	Loss 0.3863 (0.3851)
+2022-11-18 14:52:29,929:INFO: Dataset: univ                Batch: 15/15	Loss 0.3972 (0.3853)
+2022-11-18 14:52:30,336:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4528 (0.4528)
+2022-11-18 14:52:30,508:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4384 (0.4455)
+2022-11-18 14:52:30,671:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4522 (0.4478)
+2022-11-18 14:52:30,826:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4486 (0.4480)
+2022-11-18 14:52:30,977:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4444 (0.4473)
+2022-11-18 14:52:31,128:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4331 (0.4450)
+2022-11-18 14:52:31,279:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4296 (0.4430)
+2022-11-18 14:52:31,415:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4297 (0.4417)
+2022-11-18 14:52:31,811:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4050 (0.4050)
+2022-11-18 14:52:31,958:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4100 (0.4075)
+2022-11-18 14:52:32,109:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3950 (0.4032)
+2022-11-18 14:52:32,263:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3975 (0.4018)
+2022-11-18 14:52:32,418:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4073 (0.4029)
+2022-11-18 14:52:32,575:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4122 (0.4043)
+2022-11-18 14:52:32,730:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4107 (0.4053)
+2022-11-18 14:52:32,882:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4047 (0.4052)
+2022-11-18 14:52:33,034:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4016 (0.4048)
+2022-11-18 14:52:33,184:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3973 (0.4040)
+2022-11-18 14:52:33,342:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4009 (0.4038)
+2022-11-18 14:52:33,491:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3981 (0.4033)
+2022-11-18 14:52:33,648:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4157 (0.4042)
+2022-11-18 14:52:33,804:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4036 (0.4042)
+2022-11-18 14:52:33,955:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4008 (0.4040)
+2022-11-18 14:52:34,107:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4088 (0.4043)
+2022-11-18 14:52:34,258:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4019 (0.4042)
+2022-11-18 14:52:34,395:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4080 (0.4043)
+2022-11-18 14:52:34,439:INFO: - Computing loss (validation)
+2022-11-18 14:52:34,720:INFO: Dataset: hotel               Batch: 1/2	Loss 0.4011 (0.4011)
+2022-11-18 14:52:34,759:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4198 (0.4027)
+2022-11-18 14:52:35,059:INFO: Dataset: univ                Batch: 1/3	Loss 0.3773 (0.3773)
+2022-11-18 14:52:35,132:INFO: Dataset: univ                Batch: 2/3	Loss 0.3856 (0.3810)
+2022-11-18 14:52:35,204:INFO: Dataset: univ                Batch: 3/3	Loss 0.3804 (0.3808)
+2022-11-18 14:52:35,517:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4401 (0.4401)
+2022-11-18 14:52:35,563:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4009 (0.4309)
+2022-11-18 14:52:35,863:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3974 (0.3974)
+2022-11-18 14:52:35,941:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3901 (0.3938)
+2022-11-18 14:52:36,015:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3826 (0.3900)
+2022-11-18 14:52:36,088:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3992 (0.3923)
+2022-11-18 14:52:36,161:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3933 (0.3925)
+2022-11-18 14:52:36,218:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_212.pth.tar
+2022-11-18 14:52:36,218:INFO: 
+===> EPOCH: 213 (P2)
+2022-11-18 14:52:36,218:INFO: - Computing loss (training)
+2022-11-18 14:52:36,612:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4351 (0.4351)
+2022-11-18 14:52:36,787:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4370 (0.4361)
+2022-11-18 14:52:36,955:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4330 (0.4350)
+2022-11-18 14:52:37,093:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4349 (0.4350)
+2022-11-18 14:52:37,545:INFO: Dataset: univ                Batch:  1/15	Loss 0.4001 (0.4001)
+2022-11-18 14:52:37,724:INFO: Dataset: univ                Batch:  2/15	Loss 0.4033 (0.4017)
+2022-11-18 14:52:37,884:INFO: Dataset: univ                Batch:  3/15	Loss 0.4059 (0.4030)
+2022-11-18 14:52:38,042:INFO: Dataset: univ                Batch:  4/15	Loss 0.4063 (0.4037)
+2022-11-18 14:52:38,196:INFO: Dataset: univ                Batch:  5/15	Loss 0.4057 (0.4041)
+2022-11-18 14:52:38,363:INFO: Dataset: univ                Batch:  6/15	Loss 0.4045 (0.4042)
+2022-11-18 14:52:38,557:INFO: Dataset: univ                Batch:  7/15	Loss 0.4045 (0.4042)
+2022-11-18 14:52:38,735:INFO: Dataset: univ                Batch:  8/15	Loss 0.3997 (0.4037)
+2022-11-18 14:52:38,934:INFO: Dataset: univ                Batch:  9/15	Loss 0.3905 (0.4023)
+2022-11-18 14:52:39,153:INFO: Dataset: univ                Batch: 10/15	Loss 0.4028 (0.4023)
+2022-11-18 14:52:39,359:INFO: Dataset: univ                Batch: 11/15	Loss 0.4042 (0.4025)
+2022-11-18 14:52:39,532:INFO: Dataset: univ                Batch: 12/15	Loss 0.3983 (0.4022)
+2022-11-18 14:52:39,697:INFO: Dataset: univ                Batch: 13/15	Loss 0.3962 (0.4017)
+2022-11-18 14:52:39,855:INFO: Dataset: univ                Batch: 14/15	Loss 0.4050 (0.4020)
+2022-11-18 14:52:39,943:INFO: Dataset: univ                Batch: 15/15	Loss 0.3987 (0.4019)
+2022-11-18 14:52:40,359:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4518 (0.4518)
+2022-11-18 14:52:40,508:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4314 (0.4418)
+2022-11-18 14:52:40,657:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4383 (0.4406)
+2022-11-18 14:52:40,805:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4658 (0.4462)
+2022-11-18 14:52:40,951:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4548 (0.4481)
+2022-11-18 14:52:41,095:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4545 (0.4492)
+2022-11-18 14:52:41,239:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4431 (0.4483)
+2022-11-18 14:52:41,370:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4610 (0.4496)
+2022-11-18 14:52:41,752:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4213 (0.4213)
+2022-11-18 14:52:41,997:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4101 (0.4157)
+2022-11-18 14:52:42,161:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4315 (0.4213)
+2022-11-18 14:52:42,323:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4193 (0.4208)
+2022-11-18 14:52:42,471:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4287 (0.4224)
+2022-11-18 14:52:42,619:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4168 (0.4216)
+2022-11-18 14:52:42,763:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4244 (0.4220)
+2022-11-18 14:52:42,906:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4077 (0.4201)
+2022-11-18 14:52:43,049:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4176 (0.4198)
+2022-11-18 14:52:43,192:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4120 (0.4191)
+2022-11-18 14:52:43,335:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4246 (0.4196)
+2022-11-18 14:52:43,480:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4087 (0.4187)
+2022-11-18 14:52:43,623:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4015 (0.4174)
+2022-11-18 14:52:43,770:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4195 (0.4175)
+2022-11-18 14:52:43,915:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4176 (0.4175)
+2022-11-18 14:52:44,059:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4053 (0.4167)
+2022-11-18 14:52:44,209:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4242 (0.4172)
+2022-11-18 14:52:44,352:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4116 (0.4169)
+2022-11-18 14:52:44,403:INFO: - Computing loss (validation)
+2022-11-18 14:52:44,670:INFO: Dataset: hotel               Batch: 1/2	Loss 0.3982 (0.3982)
+2022-11-18 14:52:44,704:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4544 (0.4024)
+2022-11-18 14:52:45,003:INFO: Dataset: univ                Batch: 1/3	Loss 0.3948 (0.3948)
+2022-11-18 14:52:45,080:INFO: Dataset: univ                Batch: 2/3	Loss 0.3988 (0.3968)
+2022-11-18 14:52:45,152:INFO: Dataset: univ                Batch: 3/3	Loss 0.4075 (0.4000)
+2022-11-18 14:52:45,466:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4358 (0.4358)
+2022-11-18 14:52:45,511:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4626 (0.4420)
+2022-11-18 14:52:45,828:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4078 (0.4078)
+2022-11-18 14:52:45,903:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3928 (0.4003)
+2022-11-18 14:52:45,979:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4260 (0.4088)
+2022-11-18 14:52:46,058:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4036 (0.4075)
+2022-11-18 14:52:46,145:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4138 (0.4088)
+2022-11-18 14:52:46,205:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_213.pth.tar
+2022-11-18 14:52:46,206:INFO: 
+===> EPOCH: 214 (P2)
+2022-11-18 14:52:46,206:INFO: - Computing loss (training)
+2022-11-18 14:52:46,576:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4178 (0.4178)
+2022-11-18 14:52:46,725:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4565 (0.4368)
+2022-11-18 14:52:46,879:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4474 (0.4403)
+2022-11-18 14:52:46,998:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4481 (0.4415)
+2022-11-18 14:52:47,410:INFO: Dataset: univ                Batch:  1/15	Loss 0.4200 (0.4200)
+2022-11-18 14:52:47,568:INFO: Dataset: univ                Batch:  2/15	Loss 0.4261 (0.4230)
+2022-11-18 14:52:47,739:INFO: Dataset: univ                Batch:  3/15	Loss 0.4289 (0.4251)
+2022-11-18 14:52:47,902:INFO: Dataset: univ                Batch:  4/15	Loss 0.4208 (0.4241)
+2022-11-18 14:52:48,064:INFO: Dataset: univ                Batch:  5/15	Loss 0.4186 (0.4230)
+2022-11-18 14:52:48,218:INFO: Dataset: univ                Batch:  6/15	Loss 0.4093 (0.4206)
+2022-11-18 14:52:48,371:INFO: Dataset: univ                Batch:  7/15	Loss 0.4131 (0.4195)
+2022-11-18 14:52:48,527:INFO: Dataset: univ                Batch:  8/15	Loss 0.4130 (0.4187)
+2022-11-18 14:52:48,681:INFO: Dataset: univ                Batch:  9/15	Loss 0.4107 (0.4178)
+2022-11-18 14:52:48,837:INFO: Dataset: univ                Batch: 10/15	Loss 0.4157 (0.4176)
+2022-11-18 14:52:48,995:INFO: Dataset: univ                Batch: 11/15	Loss 0.4108 (0.4169)
+2022-11-18 14:52:49,148:INFO: Dataset: univ                Batch: 12/15	Loss 0.4132 (0.4166)
+2022-11-18 14:52:49,302:INFO: Dataset: univ                Batch: 13/15	Loss 0.4048 (0.4157)
+2022-11-18 14:52:49,458:INFO: Dataset: univ                Batch: 14/15	Loss 0.4084 (0.4152)
+2022-11-18 14:52:49,539:INFO: Dataset: univ                Batch: 15/15	Loss 0.3954 (0.4149)
+2022-11-18 14:52:49,921:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4576 (0.4576)
+2022-11-18 14:52:50,069:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4787 (0.4680)
+2022-11-18 14:52:50,221:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4629 (0.4662)
+2022-11-18 14:52:50,367:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4563 (0.4638)
+2022-11-18 14:52:50,526:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4455 (0.4598)
+2022-11-18 14:52:50,690:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4555 (0.4591)
+2022-11-18 14:52:50,843:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4608 (0.4594)
+2022-11-18 14:52:50,980:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4665 (0.4601)
+2022-11-18 14:52:51,361:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4567 (0.4567)
+2022-11-18 14:52:51,532:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4347 (0.4463)
+2022-11-18 14:52:51,694:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4391 (0.4438)
+2022-11-18 14:52:51,853:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4433 (0.4437)
+2022-11-18 14:52:52,008:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4495 (0.4449)
+2022-11-18 14:52:52,166:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4349 (0.4430)
+2022-11-18 14:52:52,317:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4465 (0.4435)
+2022-11-18 14:52:52,476:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4345 (0.4423)
+2022-11-18 14:52:52,628:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4425 (0.4423)
+2022-11-18 14:52:52,779:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4349 (0.4415)
+2022-11-18 14:52:52,932:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3995 (0.4374)
+2022-11-18 14:52:53,087:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4341 (0.4371)
+2022-11-18 14:52:53,237:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4247 (0.4362)
+2022-11-18 14:52:53,390:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4202 (0.4351)
+2022-11-18 14:52:53,544:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4404 (0.4354)
+2022-11-18 14:52:53,727:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4086 (0.4337)
+2022-11-18 14:52:53,883:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4208 (0.4329)
+2022-11-18 14:52:54,025:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4216 (0.4323)
+2022-11-18 14:52:54,071:INFO: - Computing loss (validation)
+2022-11-18 14:52:54,336:INFO: Dataset: hotel               Batch: 1/2	Loss 0.3993 (0.3993)
+2022-11-18 14:52:54,372:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4427 (0.4024)
+2022-11-18 14:52:54,682:INFO: Dataset: univ                Batch: 1/3	Loss 0.4111 (0.4111)
+2022-11-18 14:52:54,758:INFO: Dataset: univ                Batch: 2/3	Loss 0.4059 (0.4083)
+2022-11-18 14:52:54,828:INFO: Dataset: univ                Batch: 3/3	Loss 0.4149 (0.4101)
+2022-11-18 14:52:55,126:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4537 (0.4537)
+2022-11-18 14:52:55,172:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4586 (0.4547)
+2022-11-18 14:52:55,474:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4186 (0.4186)
+2022-11-18 14:52:55,547:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4247 (0.4217)
+2022-11-18 14:52:55,622:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4096 (0.4176)
+2022-11-18 14:52:55,696:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4135 (0.4165)
+2022-11-18 14:52:55,770:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4141 (0.4161)
+2022-11-18 14:52:55,823:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_214.pth.tar
+2022-11-18 14:52:55,824:INFO: 
+===> EPOCH: 215 (P2)
+2022-11-18 14:52:55,824:INFO: - Computing loss (training)
+2022-11-18 14:52:56,160:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4580 (0.4580)
+2022-11-18 14:52:56,305:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4697 (0.4636)
+2022-11-18 14:52:56,450:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4770 (0.4678)
+2022-11-18 14:52:56,564:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4876 (0.4714)
+2022-11-18 14:52:57,046:INFO: Dataset: univ                Batch:  1/15	Loss 0.4319 (0.4319)
+2022-11-18 14:52:57,198:INFO: Dataset: univ                Batch:  2/15	Loss 0.4305 (0.4313)
+2022-11-18 14:52:57,353:INFO: Dataset: univ                Batch:  3/15	Loss 0.4262 (0.4296)
+2022-11-18 14:52:57,506:INFO: Dataset: univ                Batch:  4/15	Loss 0.4295 (0.4296)
+2022-11-18 14:52:57,659:INFO: Dataset: univ                Batch:  5/15	Loss 0.4272 (0.4291)
+2022-11-18 14:52:57,815:INFO: Dataset: univ                Batch:  6/15	Loss 0.4201 (0.4275)
+2022-11-18 14:52:57,969:INFO: Dataset: univ                Batch:  7/15	Loss 0.4255 (0.4272)
+2022-11-18 14:52:58,120:INFO: Dataset: univ                Batch:  8/15	Loss 0.4279 (0.4273)
+2022-11-18 14:52:58,273:INFO: Dataset: univ                Batch:  9/15	Loss 0.4300 (0.4276)
+2022-11-18 14:52:58,423:INFO: Dataset: univ                Batch: 10/15	Loss 0.4395 (0.4288)
+2022-11-18 14:52:58,578:INFO: Dataset: univ                Batch: 11/15	Loss 0.4286 (0.4288)
+2022-11-18 14:52:58,733:INFO: Dataset: univ                Batch: 12/15	Loss 0.4281 (0.4287)
+2022-11-18 14:52:58,890:INFO: Dataset: univ                Batch: 13/15	Loss 0.4242 (0.4284)
+2022-11-18 14:52:59,043:INFO: Dataset: univ                Batch: 14/15	Loss 0.4260 (0.4282)
+2022-11-18 14:52:59,125:INFO: Dataset: univ                Batch: 15/15	Loss 0.4167 (0.4281)
+2022-11-18 14:52:59,506:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4683 (0.4683)
+2022-11-18 14:52:59,650:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4835 (0.4763)
+2022-11-18 14:52:59,800:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4759 (0.4761)
+2022-11-18 14:52:59,946:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4875 (0.4791)
+2022-11-18 14:53:00,092:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4691 (0.4775)
+2022-11-18 14:53:00,240:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4636 (0.4755)
+2022-11-18 14:53:00,386:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4954 (0.4782)
+2022-11-18 14:53:00,521:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4838 (0.4789)
+2022-11-18 14:53:00,907:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4340 (0.4340)
+2022-11-18 14:53:01,056:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4472 (0.4403)
+2022-11-18 14:53:01,204:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4622 (0.4478)
+2022-11-18 14:53:01,354:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4702 (0.4532)
+2022-11-18 14:53:01,500:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4398 (0.4504)
+2022-11-18 14:53:01,649:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4361 (0.4480)
+2022-11-18 14:53:01,795:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4567 (0.4492)
+2022-11-18 14:53:01,941:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4749 (0.4523)
+2022-11-18 14:53:02,087:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4460 (0.4516)
+2022-11-18 14:53:02,233:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4403 (0.4505)
+2022-11-18 14:53:02,379:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4676 (0.4521)
+2022-11-18 14:53:02,527:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4405 (0.4512)
+2022-11-18 14:53:02,672:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4558 (0.4516)
+2022-11-18 14:53:02,823:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4525 (0.4516)
+2022-11-18 14:53:02,972:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4305 (0.4503)
+2022-11-18 14:53:03,120:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4377 (0.4496)
+2022-11-18 14:53:03,268:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4335 (0.4488)
+2022-11-18 14:53:03,403:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4447 (0.4486)
+2022-11-18 14:53:03,448:INFO: - Computing loss (validation)
+2022-11-18 14:53:03,719:INFO: Dataset: hotel               Batch: 1/2	Loss 0.4460 (0.4460)
+2022-11-18 14:53:03,752:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4662 (0.4472)
+2022-11-18 14:53:04,058:INFO: Dataset: univ                Batch: 1/3	Loss 0.4253 (0.4253)
+2022-11-18 14:53:04,133:INFO: Dataset: univ                Batch: 2/3	Loss 0.4203 (0.4227)
+2022-11-18 14:53:04,205:INFO: Dataset: univ                Batch: 3/3	Loss 0.4210 (0.4221)
+2022-11-18 14:53:04,495:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4663 (0.4663)
+2022-11-18 14:53:04,540:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4795 (0.4693)
+2022-11-18 14:53:04,838:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4383 (0.4383)
+2022-11-18 14:53:04,911:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4402 (0.4392)
+2022-11-18 14:53:04,983:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4220 (0.4332)
+2022-11-18 14:53:05,056:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4320 (0.4329)
+2022-11-18 14:53:05,128:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4389 (0.4340)
+2022-11-18 14:53:05,182:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_215.pth.tar
+2022-11-18 14:53:05,182:INFO: 
+===> EPOCH: 216 (P2)
+2022-11-18 14:53:05,182:INFO: - Computing loss (training)
+2022-11-18 14:53:05,525:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4625 (0.4625)
+2022-11-18 14:53:05,673:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4728 (0.4678)
+2022-11-18 14:53:05,822:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4931 (0.4755)
+2022-11-18 14:53:05,937:INFO: Dataset: hotel               Batch: 4/4	Loss 0.4730 (0.4751)
+2022-11-18 14:53:06,329:INFO: Dataset: univ                Batch:  1/15	Loss 0.4477 (0.4477)
+2022-11-18 14:53:06,483:INFO: Dataset: univ                Batch:  2/15	Loss 0.4392 (0.4432)
+2022-11-18 14:53:06,637:INFO: Dataset: univ                Batch:  3/15	Loss 0.4468 (0.4444)
+2022-11-18 14:53:06,793:INFO: Dataset: univ                Batch:  4/15	Loss 0.4464 (0.4449)
+2022-11-18 14:53:06,947:INFO: Dataset: univ                Batch:  5/15	Loss 0.4482 (0.4455)
+2022-11-18 14:53:07,103:INFO: Dataset: univ                Batch:  6/15	Loss 0.4411 (0.4447)
+2022-11-18 14:53:07,258:INFO: Dataset: univ                Batch:  7/15	Loss 0.4488 (0.4453)
+2022-11-18 14:53:07,410:INFO: Dataset: univ                Batch:  8/15	Loss 0.4471 (0.4455)
+2022-11-18 14:53:07,566:INFO: Dataset: univ                Batch:  9/15	Loss 0.4495 (0.4460)
+2022-11-18 14:53:07,718:INFO: Dataset: univ                Batch: 10/15	Loss 0.4347 (0.4448)
+2022-11-18 14:53:07,873:INFO: Dataset: univ                Batch: 11/15	Loss 0.4456 (0.4449)
+2022-11-18 14:53:08,028:INFO: Dataset: univ                Batch: 12/15	Loss 0.4359 (0.4440)
+2022-11-18 14:53:08,183:INFO: Dataset: univ                Batch: 13/15	Loss 0.4434 (0.4440)
+2022-11-18 14:53:08,337:INFO: Dataset: univ                Batch: 14/15	Loss 0.4386 (0.4436)
+2022-11-18 14:53:08,418:INFO: Dataset: univ                Batch: 15/15	Loss 0.4388 (0.4436)
+2022-11-18 14:53:08,822:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4784 (0.4784)
+2022-11-18 14:53:08,967:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4723 (0.4753)
+2022-11-18 14:53:09,116:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4700 (0.4736)
+2022-11-18 14:53:09,261:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5185 (0.4848)
+2022-11-18 14:53:09,406:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5032 (0.4887)
+2022-11-18 14:53:09,553:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4582 (0.4837)
+2022-11-18 14:53:09,700:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4778 (0.4828)
+2022-11-18 14:53:09,833:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4848 (0.4831)
+2022-11-18 14:53:10,212:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4629 (0.4629)
+2022-11-18 14:53:10,358:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4572 (0.4600)
+2022-11-18 14:53:10,508:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4604 (0.4601)
+2022-11-18 14:53:10,654:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4615 (0.4605)
+2022-11-18 14:53:10,804:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4561 (0.4596)
+2022-11-18 14:53:10,953:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4629 (0.4601)
+2022-11-18 14:53:11,098:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4579 (0.4598)
+2022-11-18 14:53:11,244:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4519 (0.4588)
+2022-11-18 14:53:11,390:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4575 (0.4586)
+2022-11-18 14:53:11,535:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4619 (0.4590)
+2022-11-18 14:53:11,682:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4445 (0.4577)
+2022-11-18 14:53:11,828:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4638 (0.4583)
+2022-11-18 14:53:11,973:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4685 (0.4591)
+2022-11-18 14:53:12,119:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4599 (0.4592)
+2022-11-18 14:53:12,267:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4615 (0.4593)
+2022-11-18 14:53:12,411:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4700 (0.4600)
+2022-11-18 14:53:12,558:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4601 (0.4600)
+2022-11-18 14:53:12,693:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4579 (0.4599)
+2022-11-18 14:53:12,739:INFO: - Computing loss (validation)
+2022-11-18 14:53:12,988:INFO: Dataset: hotel               Batch: 1/2	Loss 0.4593 (0.4593)
+2022-11-18 14:53:13,021:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4459 (0.4583)
+2022-11-18 14:53:13,341:INFO: Dataset: univ                Batch: 1/3	Loss 0.4317 (0.4317)
+2022-11-18 14:53:13,418:INFO: Dataset: univ                Batch: 2/3	Loss 0.4496 (0.4404)
+2022-11-18 14:53:13,492:INFO: Dataset: univ                Batch: 3/3	Loss 0.4449 (0.4419)
+2022-11-18 14:53:13,793:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5056 (0.5056)
+2022-11-18 14:53:13,837:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5043 (0.5053)
+2022-11-18 14:53:14,135:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4474 (0.4474)
+2022-11-18 14:53:14,210:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4468 (0.4471)
+2022-11-18 14:53:14,285:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4539 (0.4493)
+2022-11-18 14:53:14,359:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4644 (0.4530)
+2022-11-18 14:53:14,433:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4541 (0.4532)
+2022-11-18 14:53:14,485:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_216.pth.tar
+2022-11-18 14:53:14,485:INFO: 
+===> EPOCH: 217 (P2)
+2022-11-18 14:53:14,486:INFO: - Computing loss (training)
+2022-11-18 14:53:14,822:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4969 (0.4969)
+2022-11-18 14:53:14,967:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4810 (0.4891)
+2022-11-18 14:53:15,112:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4868 (0.4883)
+2022-11-18 14:53:15,227:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5056 (0.4913)
+2022-11-18 14:53:15,640:INFO: Dataset: univ                Batch:  1/15	Loss 0.4576 (0.4576)
+2022-11-18 14:53:15,795:INFO: Dataset: univ                Batch:  2/15	Loss 0.4581 (0.4579)
+2022-11-18 14:53:15,952:INFO: Dataset: univ                Batch:  3/15	Loss 0.4531 (0.4563)
+2022-11-18 14:53:16,104:INFO: Dataset: univ                Batch:  4/15	Loss 0.4591 (0.4570)
+2022-11-18 14:53:16,264:INFO: Dataset: univ                Batch:  5/15	Loss 0.4501 (0.4557)
+2022-11-18 14:53:16,417:INFO: Dataset: univ                Batch:  6/15	Loss 0.4631 (0.4569)
+2022-11-18 14:53:16,572:INFO: Dataset: univ                Batch:  7/15	Loss 0.4528 (0.4563)
+2022-11-18 14:53:16,724:INFO: Dataset: univ                Batch:  8/15	Loss 0.4603 (0.4568)
+2022-11-18 14:53:16,879:INFO: Dataset: univ                Batch:  9/15	Loss 0.4514 (0.4562)
+2022-11-18 14:53:17,032:INFO: Dataset: univ                Batch: 10/15	Loss 0.4470 (0.4552)
+2022-11-18 14:53:17,186:INFO: Dataset: univ                Batch: 11/15	Loss 0.4582 (0.4555)
+2022-11-18 14:53:17,341:INFO: Dataset: univ                Batch: 12/15	Loss 0.4577 (0.4557)
+2022-11-18 14:53:17,495:INFO: Dataset: univ                Batch: 13/15	Loss 0.4606 (0.4560)
+2022-11-18 14:53:17,649:INFO: Dataset: univ                Batch: 14/15	Loss 0.4516 (0.4557)
+2022-11-18 14:53:17,734:INFO: Dataset: univ                Batch: 15/15	Loss 0.4344 (0.4554)
+2022-11-18 14:53:18,116:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4993 (0.4993)
+2022-11-18 14:53:18,265:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4685 (0.4849)
+2022-11-18 14:53:18,411:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4941 (0.4880)
+2022-11-18 14:53:18,560:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4906 (0.4886)
+2022-11-18 14:53:18,706:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5066 (0.4919)
+2022-11-18 14:53:18,855:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4858 (0.4908)
+2022-11-18 14:53:19,002:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4932 (0.4912)
+2022-11-18 14:53:19,136:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4999 (0.4922)
+2022-11-18 14:53:19,541:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4841 (0.4841)
+2022-11-18 14:53:19,690:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4553 (0.4700)
+2022-11-18 14:53:19,837:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4948 (0.4782)
+2022-11-18 14:53:19,982:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4740 (0.4772)
+2022-11-18 14:53:20,128:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4642 (0.4743)
+2022-11-18 14:53:20,279:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4662 (0.4730)
+2022-11-18 14:53:20,425:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4875 (0.4751)
+2022-11-18 14:53:20,570:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4560 (0.4728)
+2022-11-18 14:53:20,715:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4884 (0.4745)
+2022-11-18 14:53:20,860:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4616 (0.4733)
+2022-11-18 14:53:21,008:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4636 (0.4723)
+2022-11-18 14:53:21,153:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4630 (0.4715)
+2022-11-18 14:53:21,299:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4604 (0.4707)
+2022-11-18 14:53:21,445:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4669 (0.4704)
+2022-11-18 14:53:21,592:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4857 (0.4715)
+2022-11-18 14:53:21,738:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4696 (0.4713)
+2022-11-18 14:53:21,885:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4812 (0.4719)
+2022-11-18 14:53:22,020:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4578 (0.4711)
+2022-11-18 14:53:22,065:INFO: - Computing loss (validation)
+2022-11-18 14:53:22,332:INFO: Dataset: hotel               Batch: 1/2	Loss 0.4681 (0.4681)
+2022-11-18 14:53:22,366:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4625 (0.4676)
+2022-11-18 14:53:22,672:INFO: Dataset: univ                Batch: 1/3	Loss 0.4459 (0.4459)
+2022-11-18 14:53:22,747:INFO: Dataset: univ                Batch: 2/3	Loss 0.4562 (0.4511)
+2022-11-18 14:53:22,818:INFO: Dataset: univ                Batch: 3/3	Loss 0.4583 (0.4533)
+2022-11-18 14:53:23,116:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5113 (0.5113)
+2022-11-18 14:53:23,161:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5383 (0.5175)
+2022-11-18 14:53:23,463:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4602 (0.4602)
+2022-11-18 14:53:23,534:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4578 (0.4590)
+2022-11-18 14:53:23,609:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4508 (0.4560)
+2022-11-18 14:53:23,680:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4541 (0.4555)
+2022-11-18 14:53:23,751:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4747 (0.4595)
+2022-11-18 14:53:23,803:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_217.pth.tar
+2022-11-18 14:53:23,804:INFO: 
+===> EPOCH: 218 (P2)
+2022-11-18 14:53:23,804:INFO: - Computing loss (training)
+2022-11-18 14:53:24,145:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4925 (0.4925)
+2022-11-18 14:53:24,295:INFO: Dataset: hotel               Batch: 2/4	Loss 0.4958 (0.4940)
+2022-11-18 14:53:24,442:INFO: Dataset: hotel               Batch: 3/4	Loss 0.4951 (0.4944)
+2022-11-18 14:53:24,558:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5138 (0.4973)
+2022-11-18 14:53:24,964:INFO: Dataset: univ                Batch:  1/15	Loss 0.4630 (0.4630)
+2022-11-18 14:53:25,119:INFO: Dataset: univ                Batch:  2/15	Loss 0.4750 (0.4686)
+2022-11-18 14:53:25,276:INFO: Dataset: univ                Batch:  3/15	Loss 0.4710 (0.4695)
+2022-11-18 14:53:25,430:INFO: Dataset: univ                Batch:  4/15	Loss 0.4727 (0.4702)
+2022-11-18 14:53:25,585:INFO: Dataset: univ                Batch:  5/15	Loss 0.4657 (0.4693)
+2022-11-18 14:53:25,742:INFO: Dataset: univ                Batch:  6/15	Loss 0.4712 (0.4696)
+2022-11-18 14:53:25,897:INFO: Dataset: univ                Batch:  7/15	Loss 0.4668 (0.4692)
+2022-11-18 14:53:26,049:INFO: Dataset: univ                Batch:  8/15	Loss 0.4709 (0.4694)
+2022-11-18 14:53:26,201:INFO: Dataset: univ                Batch:  9/15	Loss 0.4658 (0.4690)
+2022-11-18 14:53:26,355:INFO: Dataset: univ                Batch: 10/15	Loss 0.4587 (0.4680)
+2022-11-18 14:53:26,509:INFO: Dataset: univ                Batch: 11/15	Loss 0.4636 (0.4676)
+2022-11-18 14:53:26,662:INFO: Dataset: univ                Batch: 12/15	Loss 0.4657 (0.4675)
+2022-11-18 14:53:26,817:INFO: Dataset: univ                Batch: 13/15	Loss 0.4643 (0.4672)
+2022-11-18 14:53:26,969:INFO: Dataset: univ                Batch: 14/15	Loss 0.4589 (0.4666)
+2022-11-18 14:53:27,051:INFO: Dataset: univ                Batch: 15/15	Loss 0.4639 (0.4666)
+2022-11-18 14:53:27,442:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4982 (0.4982)
+2022-11-18 14:53:27,599:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5214 (0.5096)
+2022-11-18 14:53:27,747:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5036 (0.5077)
+2022-11-18 14:53:27,892:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5251 (0.5122)
+2022-11-18 14:53:28,039:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5157 (0.5129)
+2022-11-18 14:53:28,187:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5180 (0.5137)
+2022-11-18 14:53:28,334:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5074 (0.5128)
+2022-11-18 14:53:28,469:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5044 (0.5119)
+2022-11-18 14:53:28,856:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4628 (0.4628)
+2022-11-18 14:53:29,004:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5062 (0.4860)
+2022-11-18 14:53:29,151:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4941 (0.4887)
+2022-11-18 14:53:29,302:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4927 (0.4897)
+2022-11-18 14:53:29,449:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4700 (0.4857)
+2022-11-18 14:53:29,599:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4808 (0.4849)
+2022-11-18 14:53:29,745:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4771 (0.4838)
+2022-11-18 14:53:29,890:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4784 (0.4832)
+2022-11-18 14:53:30,036:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4821 (0.4830)
+2022-11-18 14:53:30,181:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4882 (0.4836)
+2022-11-18 14:53:30,329:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4723 (0.4825)
+2022-11-18 14:53:30,475:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4813 (0.4824)
+2022-11-18 14:53:30,623:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4957 (0.4834)
+2022-11-18 14:53:30,771:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5152 (0.4856)
+2022-11-18 14:53:30,919:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4776 (0.4851)
+2022-11-18 14:53:31,067:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4824 (0.4849)
+2022-11-18 14:53:31,216:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5030 (0.4859)
+2022-11-18 14:53:31,352:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4801 (0.4856)
+2022-11-18 14:53:31,396:INFO: - Computing loss (validation)
+2022-11-18 14:53:31,657:INFO: Dataset: hotel               Batch: 1/2	Loss 0.5021 (0.5021)
+2022-11-18 14:53:31,690:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4533 (0.4983)
+2022-11-18 14:53:31,992:INFO: Dataset: univ                Batch: 1/3	Loss 0.4790 (0.4790)
+2022-11-18 14:53:32,070:INFO: Dataset: univ                Batch: 2/3	Loss 0.4670 (0.4726)
+2022-11-18 14:53:32,141:INFO: Dataset: univ                Batch: 3/3	Loss 0.4533 (0.4664)
+2022-11-18 14:53:32,450:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5162 (0.5162)
+2022-11-18 14:53:32,495:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4980 (0.5117)
+2022-11-18 14:53:32,809:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4723 (0.4723)
+2022-11-18 14:53:32,882:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4627 (0.4677)
+2022-11-18 14:53:32,955:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4682 (0.4678)
+2022-11-18 14:53:33,029:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4799 (0.4709)
+2022-11-18 14:53:33,101:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4730 (0.4714)
+2022-11-18 14:53:33,152:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_218.pth.tar
+2022-11-18 14:53:33,152:INFO: 
+===> EPOCH: 219 (P2)
+2022-11-18 14:53:33,152:INFO: - Computing loss (training)
+2022-11-18 14:53:33,490:INFO: Dataset: hotel               Batch: 1/4	Loss 0.4963 (0.4963)
+2022-11-18 14:53:33,639:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5191 (0.5079)
+2022-11-18 14:53:33,790:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5166 (0.5108)
+2022-11-18 14:53:33,903:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5506 (0.5177)
+2022-11-18 14:53:34,297:INFO: Dataset: univ                Batch:  1/15	Loss 0.4828 (0.4828)
+2022-11-18 14:53:34,449:INFO: Dataset: univ                Batch:  2/15	Loss 0.4926 (0.4877)
+2022-11-18 14:53:34,607:INFO: Dataset: univ                Batch:  3/15	Loss 0.4838 (0.4864)
+2022-11-18 14:53:34,759:INFO: Dataset: univ                Batch:  4/15	Loss 0.4792 (0.4847)
+2022-11-18 14:53:34,913:INFO: Dataset: univ                Batch:  5/15	Loss 0.4852 (0.4848)
+2022-11-18 14:53:35,068:INFO: Dataset: univ                Batch:  6/15	Loss 0.4810 (0.4842)
+2022-11-18 14:53:35,221:INFO: Dataset: univ                Batch:  7/15	Loss 0.4832 (0.4841)
+2022-11-18 14:53:35,373:INFO: Dataset: univ                Batch:  8/15	Loss 0.4864 (0.4844)
+2022-11-18 14:53:35,525:INFO: Dataset: univ                Batch:  9/15	Loss 0.4798 (0.4839)
+2022-11-18 14:53:35,678:INFO: Dataset: univ                Batch: 10/15	Loss 0.4898 (0.4846)
+2022-11-18 14:53:35,831:INFO: Dataset: univ                Batch: 11/15	Loss 0.4789 (0.4841)
+2022-11-18 14:53:35,984:INFO: Dataset: univ                Batch: 12/15	Loss 0.4823 (0.4839)
+2022-11-18 14:53:36,137:INFO: Dataset: univ                Batch: 13/15	Loss 0.4815 (0.4837)
+2022-11-18 14:53:36,289:INFO: Dataset: univ                Batch: 14/15	Loss 0.4898 (0.4841)
+2022-11-18 14:53:36,371:INFO: Dataset: univ                Batch: 15/15	Loss 0.5016 (0.4844)
+2022-11-18 14:53:36,775:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5166 (0.5166)
+2022-11-18 14:53:36,927:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4921 (0.5041)
+2022-11-18 14:53:37,075:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5233 (0.5109)
+2022-11-18 14:53:37,223:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5544 (0.5229)
+2022-11-18 14:53:37,371:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5172 (0.5218)
+2022-11-18 14:53:37,521:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5326 (0.5237)
+2022-11-18 14:53:37,669:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5118 (0.5221)
+2022-11-18 14:53:37,806:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5421 (0.5241)
+2022-11-18 14:53:38,189:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4870 (0.4870)
+2022-11-18 14:53:38,339:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5011 (0.4948)
+2022-11-18 14:53:38,499:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4911 (0.4937)
+2022-11-18 14:53:38,645:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5089 (0.4974)
+2022-11-18 14:53:38,794:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4847 (0.4950)
+2022-11-18 14:53:38,943:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5001 (0.4959)
+2022-11-18 14:53:39,090:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4749 (0.4935)
+2022-11-18 14:53:39,233:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4816 (0.4920)
+2022-11-18 14:53:39,379:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5121 (0.4941)
+2022-11-18 14:53:39,525:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4995 (0.4947)
+2022-11-18 14:53:39,670:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4998 (0.4952)
+2022-11-18 14:53:39,816:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4950 (0.4952)
+2022-11-18 14:53:39,961:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4991 (0.4955)
+2022-11-18 14:53:40,107:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4762 (0.4940)
+2022-11-18 14:53:40,253:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4847 (0.4934)
+2022-11-18 14:53:40,399:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4902 (0.4932)
+2022-11-18 14:53:40,546:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4946 (0.4932)
+2022-11-18 14:53:40,680:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4881 (0.4930)
+2022-11-18 14:53:40,724:INFO: - Computing loss (validation)
+2022-11-18 14:53:40,989:INFO: Dataset: hotel               Batch: 1/2	Loss 0.4943 (0.4943)
+2022-11-18 14:53:41,025:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5676 (0.4990)
+2022-11-18 14:53:41,342:INFO: Dataset: univ                Batch: 1/3	Loss 0.4821 (0.4821)
+2022-11-18 14:53:41,420:INFO: Dataset: univ                Batch: 2/3	Loss 0.4736 (0.4777)
+2022-11-18 14:53:41,490:INFO: Dataset: univ                Batch: 3/3	Loss 0.4765 (0.4773)
+2022-11-18 14:53:41,788:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5287 (0.5287)
+2022-11-18 14:53:41,833:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4956 (0.5203)
+2022-11-18 14:53:42,129:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4830 (0.4830)
+2022-11-18 14:53:42,207:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4793 (0.4812)
+2022-11-18 14:53:42,283:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4943 (0.4858)
+2022-11-18 14:53:42,360:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4963 (0.4885)
+2022-11-18 14:53:42,435:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4802 (0.4870)
+2022-11-18 14:53:42,487:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_219.pth.tar
+2022-11-18 14:53:42,487:INFO: 
+===> EPOCH: 220 (P2)
+2022-11-18 14:53:42,487:INFO: - Computing loss (training)
+2022-11-18 14:53:42,826:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5073 (0.5073)
+2022-11-18 14:53:42,974:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5181 (0.5127)
+2022-11-18 14:53:43,123:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5316 (0.5189)
+2022-11-18 14:53:43,237:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5252 (0.5199)
+2022-11-18 14:53:43,691:INFO: Dataset: univ                Batch:  1/15	Loss 0.4946 (0.4946)
+2022-11-18 14:53:43,843:INFO: Dataset: univ                Batch:  2/15	Loss 0.4907 (0.4928)
+2022-11-18 14:53:43,996:INFO: Dataset: univ                Batch:  3/15	Loss 0.5037 (0.4967)
+2022-11-18 14:53:44,148:INFO: Dataset: univ                Batch:  4/15	Loss 0.5018 (0.4980)
+2022-11-18 14:53:44,300:INFO: Dataset: univ                Batch:  5/15	Loss 0.4949 (0.4974)
+2022-11-18 14:53:44,457:INFO: Dataset: univ                Batch:  6/15	Loss 0.5003 (0.4980)
+2022-11-18 14:53:44,611:INFO: Dataset: univ                Batch:  7/15	Loss 0.4837 (0.4956)
+2022-11-18 14:53:44,760:INFO: Dataset: univ                Batch:  8/15	Loss 0.5044 (0.4966)
+2022-11-18 14:53:44,914:INFO: Dataset: univ                Batch:  9/15	Loss 0.4889 (0.4958)
+2022-11-18 14:53:45,067:INFO: Dataset: univ                Batch: 10/15	Loss 0.4931 (0.4955)
+2022-11-18 14:53:45,219:INFO: Dataset: univ                Batch: 11/15	Loss 0.4966 (0.4956)
+2022-11-18 14:53:45,372:INFO: Dataset: univ                Batch: 12/15	Loss 0.4943 (0.4955)
+2022-11-18 14:53:45,527:INFO: Dataset: univ                Batch: 13/15	Loss 0.4950 (0.4954)
+2022-11-18 14:53:45,680:INFO: Dataset: univ                Batch: 14/15	Loss 0.5043 (0.4961)
+2022-11-18 14:53:45,762:INFO: Dataset: univ                Batch: 15/15	Loss 0.4865 (0.4959)
+2022-11-18 14:53:46,148:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5215 (0.5215)
+2022-11-18 14:53:46,293:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5373 (0.5298)
+2022-11-18 14:53:46,443:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5424 (0.5342)
+2022-11-18 14:53:46,588:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5521 (0.5388)
+2022-11-18 14:53:46,733:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5288 (0.5367)
+2022-11-18 14:53:46,879:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5136 (0.5331)
+2022-11-18 14:53:47,025:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5584 (0.5366)
+2022-11-18 14:53:47,159:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5413 (0.5371)
+2022-11-18 14:53:47,563:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5428 (0.5428)
+2022-11-18 14:53:47,713:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5036 (0.5239)
+2022-11-18 14:53:47,864:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5302 (0.5258)
+2022-11-18 14:53:48,013:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5142 (0.5227)
+2022-11-18 14:53:48,164:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5212 (0.5224)
+2022-11-18 14:53:48,316:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4993 (0.5185)
+2022-11-18 14:53:48,465:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5027 (0.5164)
+2022-11-18 14:53:48,614:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5135 (0.5161)
+2022-11-18 14:53:48,765:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4975 (0.5139)
+2022-11-18 14:53:48,915:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4962 (0.5121)
+2022-11-18 14:53:49,065:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5317 (0.5141)
+2022-11-18 14:53:49,214:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5070 (0.5135)
+2022-11-18 14:53:49,363:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5036 (0.5127)
+2022-11-18 14:53:49,513:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5082 (0.5123)
+2022-11-18 14:53:49,664:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5089 (0.5121)
+2022-11-18 14:53:49,815:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4919 (0.5109)
+2022-11-18 14:53:49,966:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5194 (0.5113)
+2022-11-18 14:53:50,105:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4964 (0.5105)
+2022-11-18 14:53:50,155:INFO: - Computing loss (validation)
+2022-11-18 14:53:50,421:INFO: Dataset: hotel               Batch: 1/2	Loss 0.5191 (0.5191)
+2022-11-18 14:53:50,456:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4727 (0.5159)
+2022-11-18 14:53:50,768:INFO: Dataset: univ                Batch: 1/3	Loss 0.5003 (0.5003)
+2022-11-18 14:53:50,846:INFO: Dataset: univ                Batch: 2/3	Loss 0.4855 (0.4935)
+2022-11-18 14:53:50,922:INFO: Dataset: univ                Batch: 3/3	Loss 0.4857 (0.4910)
+2022-11-18 14:53:51,215:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5599 (0.5599)
+2022-11-18 14:53:51,258:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5244 (0.5506)
+2022-11-18 14:53:51,575:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4898 (0.4898)
+2022-11-18 14:53:51,648:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4818 (0.4858)
+2022-11-18 14:53:51,720:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4951 (0.4891)
+2022-11-18 14:53:51,793:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4907 (0.4895)
+2022-11-18 14:53:51,867:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4855 (0.4888)
+2022-11-18 14:53:51,921:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_220.pth.tar
+2022-11-18 14:53:51,921:INFO: 
+===> EPOCH: 221 (P2)
+2022-11-18 14:53:51,922:INFO: - Computing loss (training)
+2022-11-18 14:53:52,263:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5573 (0.5573)
+2022-11-18 14:53:52,412:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5347 (0.5465)
+2022-11-18 14:53:52,564:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5166 (0.5368)
+2022-11-18 14:53:52,679:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5621 (0.5414)
+2022-11-18 14:53:53,080:INFO: Dataset: univ                Batch:  1/15	Loss 0.5193 (0.5193)
+2022-11-18 14:53:53,236:INFO: Dataset: univ                Batch:  2/15	Loss 0.5110 (0.5153)
+2022-11-18 14:53:53,392:INFO: Dataset: univ                Batch:  3/15	Loss 0.5064 (0.5124)
+2022-11-18 14:53:53,549:INFO: Dataset: univ                Batch:  4/15	Loss 0.5098 (0.5117)
+2022-11-18 14:53:53,707:INFO: Dataset: univ                Batch:  5/15	Loss 0.5182 (0.5129)
+2022-11-18 14:53:53,866:INFO: Dataset: univ                Batch:  6/15	Loss 0.5137 (0.5130)
+2022-11-18 14:53:54,022:INFO: Dataset: univ                Batch:  7/15	Loss 0.5092 (0.5125)
+2022-11-18 14:53:54,175:INFO: Dataset: univ                Batch:  8/15	Loss 0.5105 (0.5122)
+2022-11-18 14:53:54,330:INFO: Dataset: univ                Batch:  9/15	Loss 0.5094 (0.5119)
+2022-11-18 14:53:54,484:INFO: Dataset: univ                Batch: 10/15	Loss 0.5170 (0.5124)
+2022-11-18 14:53:54,639:INFO: Dataset: univ                Batch: 11/15	Loss 0.5038 (0.5116)
+2022-11-18 14:53:54,793:INFO: Dataset: univ                Batch: 12/15	Loss 0.4971 (0.5103)
+2022-11-18 14:53:54,948:INFO: Dataset: univ                Batch: 13/15	Loss 0.5092 (0.5103)
+2022-11-18 14:53:55,104:INFO: Dataset: univ                Batch: 14/15	Loss 0.5135 (0.5105)
+2022-11-18 14:53:55,187:INFO: Dataset: univ                Batch: 15/15	Loss 0.5067 (0.5104)
+2022-11-18 14:53:55,564:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5400 (0.5400)
+2022-11-18 14:53:55,710:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5675 (0.5542)
+2022-11-18 14:53:55,857:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5500 (0.5529)
+2022-11-18 14:53:56,005:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5673 (0.5568)
+2022-11-18 14:53:56,153:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5448 (0.5542)
+2022-11-18 14:53:56,302:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5627 (0.5556)
+2022-11-18 14:53:56,448:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5832 (0.5595)
+2022-11-18 14:53:56,583:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5389 (0.5571)
+2022-11-18 14:53:56,964:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5289 (0.5289)
+2022-11-18 14:53:57,114:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5285 (0.5287)
+2022-11-18 14:53:57,343:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5113 (0.5219)
+2022-11-18 14:53:57,491:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5330 (0.5247)
+2022-11-18 14:53:57,645:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5303 (0.5257)
+2022-11-18 14:53:57,795:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5066 (0.5226)
+2022-11-18 14:53:57,943:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5210 (0.5224)
+2022-11-18 14:53:58,091:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5191 (0.5220)
+2022-11-18 14:53:58,239:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5394 (0.5240)
+2022-11-18 14:53:58,386:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5414 (0.5259)
+2022-11-18 14:53:58,537:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5165 (0.5250)
+2022-11-18 14:53:58,685:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5310 (0.5255)
+2022-11-18 14:53:58,836:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5013 (0.5237)
+2022-11-18 14:53:58,986:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5333 (0.5243)
+2022-11-18 14:53:59,136:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5235 (0.5243)
+2022-11-18 14:53:59,284:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5133 (0.5236)
+2022-11-18 14:53:59,435:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5013 (0.5223)
+2022-11-18 14:53:59,573:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5211 (0.5223)
+2022-11-18 14:53:59,628:INFO: - Computing loss (validation)
+2022-11-18 14:53:59,900:INFO: Dataset: hotel               Batch: 1/2	Loss 0.5297 (0.5297)
+2022-11-18 14:53:59,935:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5376 (0.5303)
+2022-11-18 14:54:00,234:INFO: Dataset: univ                Batch: 1/3	Loss 0.4975 (0.4975)
+2022-11-18 14:54:00,310:INFO: Dataset: univ                Batch: 2/3	Loss 0.5067 (0.5022)
+2022-11-18 14:54:00,381:INFO: Dataset: univ                Batch: 3/3	Loss 0.4949 (0.4997)
+2022-11-18 14:54:00,677:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5595 (0.5595)
+2022-11-18 14:54:00,721:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5587 (0.5593)
+2022-11-18 14:54:01,071:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5162 (0.5162)
+2022-11-18 14:54:01,145:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5044 (0.5105)
+2022-11-18 14:54:01,219:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5130 (0.5114)
+2022-11-18 14:54:01,293:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5067 (0.5102)
+2022-11-18 14:54:01,368:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5172 (0.5117)
+2022-11-18 14:54:01,420:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_221.pth.tar
+2022-11-18 14:54:01,420:INFO: 
+===> EPOCH: 222 (P2)
+2022-11-18 14:54:01,421:INFO: - Computing loss (training)
+2022-11-18 14:54:01,761:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5401 (0.5401)
+2022-11-18 14:54:01,912:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5612 (0.5510)
+2022-11-18 14:54:02,064:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5475 (0.5499)
+2022-11-18 14:54:02,181:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5209 (0.5448)
+2022-11-18 14:54:02,577:INFO: Dataset: univ                Batch:  1/15	Loss 0.5330 (0.5330)
+2022-11-18 14:54:02,739:INFO: Dataset: univ                Batch:  2/15	Loss 0.5175 (0.5254)
+2022-11-18 14:54:02,895:INFO: Dataset: univ                Batch:  3/15	Loss 0.5231 (0.5247)
+2022-11-18 14:54:03,047:INFO: Dataset: univ                Batch:  4/15	Loss 0.5167 (0.5226)
+2022-11-18 14:54:03,206:INFO: Dataset: univ                Batch:  5/15	Loss 0.5281 (0.5237)
+2022-11-18 14:54:03,359:INFO: Dataset: univ                Batch:  6/15	Loss 0.5289 (0.5247)
+2022-11-18 14:54:03,513:INFO: Dataset: univ                Batch:  7/15	Loss 0.5287 (0.5253)
+2022-11-18 14:54:03,666:INFO: Dataset: univ                Batch:  8/15	Loss 0.5248 (0.5252)
+2022-11-18 14:54:03,819:INFO: Dataset: univ                Batch:  9/15	Loss 0.5319 (0.5259)
+2022-11-18 14:54:03,972:INFO: Dataset: univ                Batch: 10/15	Loss 0.5171 (0.5250)
+2022-11-18 14:54:04,129:INFO: Dataset: univ                Batch: 11/15	Loss 0.5095 (0.5234)
+2022-11-18 14:54:04,281:INFO: Dataset: univ                Batch: 12/15	Loss 0.5145 (0.5227)
+2022-11-18 14:54:04,435:INFO: Dataset: univ                Batch: 13/15	Loss 0.5202 (0.5225)
+2022-11-18 14:54:04,590:INFO: Dataset: univ                Batch: 14/15	Loss 0.5234 (0.5226)
+2022-11-18 14:54:04,672:INFO: Dataset: univ                Batch: 15/15	Loss 0.5031 (0.5223)
+2022-11-18 14:54:05,062:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5334 (0.5334)
+2022-11-18 14:54:05,209:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5541 (0.5440)
+2022-11-18 14:54:05,361:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5540 (0.5475)
+2022-11-18 14:54:05,509:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5619 (0.5509)
+2022-11-18 14:54:05,657:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5625 (0.5532)
+2022-11-18 14:54:05,806:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5627 (0.5548)
+2022-11-18 14:54:05,955:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5794 (0.5582)
+2022-11-18 14:54:06,091:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5518 (0.5575)
+2022-11-18 14:54:06,480:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5463 (0.5463)
+2022-11-18 14:54:06,628:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5407 (0.5436)
+2022-11-18 14:54:06,783:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5255 (0.5377)
+2022-11-18 14:54:06,933:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5443 (0.5393)
+2022-11-18 14:54:07,086:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5315 (0.5374)
+2022-11-18 14:54:07,236:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5524 (0.5399)
+2022-11-18 14:54:07,383:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5380 (0.5396)
+2022-11-18 14:54:07,530:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5553 (0.5416)
+2022-11-18 14:54:07,678:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5455 (0.5420)
+2022-11-18 14:54:07,825:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5335 (0.5412)
+2022-11-18 14:54:07,973:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5391 (0.5410)
+2022-11-18 14:54:08,121:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5194 (0.5391)
+2022-11-18 14:54:08,269:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5371 (0.5390)
+2022-11-18 14:54:08,417:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5567 (0.5404)
+2022-11-18 14:54:08,567:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5235 (0.5393)
+2022-11-18 14:54:08,714:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5421 (0.5395)
+2022-11-18 14:54:08,864:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5569 (0.5405)
+2022-11-18 14:54:09,000:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5392 (0.5404)
+2022-11-18 14:54:09,044:INFO: - Computing loss (validation)
+2022-11-18 14:54:09,309:INFO: Dataset: hotel               Batch: 1/2	Loss 0.5488 (0.5488)
+2022-11-18 14:54:09,343:INFO: Dataset: hotel               Batch: 2/2	Loss 0.4777 (0.5435)
+2022-11-18 14:54:09,651:INFO: Dataset: univ                Batch: 1/3	Loss 0.5113 (0.5113)
+2022-11-18 14:54:09,727:INFO: Dataset: univ                Batch: 2/3	Loss 0.5315 (0.5215)
+2022-11-18 14:54:09,797:INFO: Dataset: univ                Batch: 3/3	Loss 0.5166 (0.5200)
+2022-11-18 14:54:10,105:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5434 (0.5434)
+2022-11-18 14:54:10,150:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5490 (0.5447)
+2022-11-18 14:54:10,445:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5221 (0.5221)
+2022-11-18 14:54:10,519:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5462 (0.5343)
+2022-11-18 14:54:10,594:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5229 (0.5303)
+2022-11-18 14:54:10,666:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5234 (0.5287)
+2022-11-18 14:54:10,739:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5324 (0.5294)
+2022-11-18 14:54:10,792:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_222.pth.tar
+2022-11-18 14:54:10,792:INFO: 
+===> EPOCH: 223 (P2)
+2022-11-18 14:54:10,793:INFO: - Computing loss (training)
+2022-11-18 14:54:11,126:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6148 (0.6148)
+2022-11-18 14:54:11,275:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5944 (0.6043)
+2022-11-18 14:54:11,423:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5757 (0.5953)
+2022-11-18 14:54:11,538:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5363 (0.5851)
+2022-11-18 14:54:11,939:INFO: Dataset: univ                Batch:  1/15	Loss 0.5442 (0.5442)
+2022-11-18 14:54:12,098:INFO: Dataset: univ                Batch:  2/15	Loss 0.5312 (0.5375)
+2022-11-18 14:54:12,257:INFO: Dataset: univ                Batch:  3/15	Loss 0.5340 (0.5363)
+2022-11-18 14:54:12,411:INFO: Dataset: univ                Batch:  4/15	Loss 0.5292 (0.5343)
+2022-11-18 14:54:12,570:INFO: Dataset: univ                Batch:  5/15	Loss 0.5247 (0.5323)
+2022-11-18 14:54:12,726:INFO: Dataset: univ                Batch:  6/15	Loss 0.5400 (0.5336)
+2022-11-18 14:54:12,882:INFO: Dataset: univ                Batch:  7/15	Loss 0.5367 (0.5340)
+2022-11-18 14:54:13,035:INFO: Dataset: univ                Batch:  8/15	Loss 0.5460 (0.5356)
+2022-11-18 14:54:13,190:INFO: Dataset: univ                Batch:  9/15	Loss 0.5340 (0.5354)
+2022-11-18 14:54:13,343:INFO: Dataset: univ                Batch: 10/15	Loss 0.5295 (0.5348)
+2022-11-18 14:54:13,498:INFO: Dataset: univ                Batch: 11/15	Loss 0.5437 (0.5356)
+2022-11-18 14:54:13,653:INFO: Dataset: univ                Batch: 12/15	Loss 0.5377 (0.5358)
+2022-11-18 14:54:13,808:INFO: Dataset: univ                Batch: 13/15	Loss 0.5396 (0.5361)
+2022-11-18 14:54:13,963:INFO: Dataset: univ                Batch: 14/15	Loss 0.5305 (0.5357)
+2022-11-18 14:54:14,046:INFO: Dataset: univ                Batch: 15/15	Loss 0.5304 (0.5356)
+2022-11-18 14:54:14,429:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6009 (0.6009)
+2022-11-18 14:54:14,577:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5805 (0.5904)
+2022-11-18 14:54:14,724:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5827 (0.5880)
+2022-11-18 14:54:14,871:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5854 (0.5874)
+2022-11-18 14:54:15,022:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5475 (0.5792)
+2022-11-18 14:54:15,170:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5914 (0.5813)
+2022-11-18 14:54:15,317:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5739 (0.5801)
+2022-11-18 14:54:15,452:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5828 (0.5804)
+2022-11-18 14:54:15,832:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5391 (0.5391)
+2022-11-18 14:54:15,992:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5607 (0.5500)
+2022-11-18 14:54:16,144:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5720 (0.5574)
+2022-11-18 14:54:16,289:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5745 (0.5614)
+2022-11-18 14:54:16,437:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5552 (0.5601)
+2022-11-18 14:54:16,587:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5959 (0.5661)
+2022-11-18 14:54:16,734:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5675 (0.5663)
+2022-11-18 14:54:16,880:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5620 (0.5658)
+2022-11-18 14:54:17,027:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5556 (0.5646)
+2022-11-18 14:54:17,174:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5181 (0.5600)
+2022-11-18 14:54:17,321:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5639 (0.5604)
+2022-11-18 14:54:17,469:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5451 (0.5590)
+2022-11-18 14:54:17,616:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5356 (0.5571)
+2022-11-18 14:54:17,765:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5652 (0.5577)
+2022-11-18 14:54:17,914:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5501 (0.5572)
+2022-11-18 14:54:18,061:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5349 (0.5557)
+2022-11-18 14:54:18,209:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5662 (0.5563)
+2022-11-18 14:54:18,345:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5461 (0.5558)
+2022-11-18 14:54:18,390:INFO: - Computing loss (validation)
+2022-11-18 14:54:18,654:INFO: Dataset: hotel               Batch: 1/2	Loss 0.5604 (0.5604)
+2022-11-18 14:54:18,687:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6776 (0.5696)
+2022-11-18 14:54:18,993:INFO: Dataset: univ                Batch: 1/3	Loss 0.5460 (0.5460)
+2022-11-18 14:54:19,068:INFO: Dataset: univ                Batch: 2/3	Loss 0.5334 (0.5400)
+2022-11-18 14:54:19,139:INFO: Dataset: univ                Batch: 3/3	Loss 0.5356 (0.5387)
+2022-11-18 14:54:19,442:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5621 (0.5621)
+2022-11-18 14:54:19,486:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5798 (0.5663)
+2022-11-18 14:54:19,788:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5258 (0.5258)
+2022-11-18 14:54:19,861:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5436 (0.5344)
+2022-11-18 14:54:19,935:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5562 (0.5420)
+2022-11-18 14:54:20,008:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5571 (0.5459)
+2022-11-18 14:54:20,080:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5383 (0.5444)
+2022-11-18 14:54:20,134:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_223.pth.tar
+2022-11-18 14:54:20,135:INFO: 
+===> EPOCH: 224 (P2)
+2022-11-18 14:54:20,135:INFO: - Computing loss (training)
+2022-11-18 14:54:20,475:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5935 (0.5935)
+2022-11-18 14:54:20,622:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5952 (0.5943)
+2022-11-18 14:54:20,769:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5794 (0.5893)
+2022-11-18 14:54:20,884:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5766 (0.5873)
+2022-11-18 14:54:21,287:INFO: Dataset: univ                Batch:  1/15	Loss 0.5591 (0.5591)
+2022-11-18 14:54:21,441:INFO: Dataset: univ                Batch:  2/15	Loss 0.5370 (0.5474)
+2022-11-18 14:54:21,596:INFO: Dataset: univ                Batch:  3/15	Loss 0.5425 (0.5458)
+2022-11-18 14:54:21,748:INFO: Dataset: univ                Batch:  4/15	Loss 0.5549 (0.5482)
+2022-11-18 14:54:21,903:INFO: Dataset: univ                Batch:  5/15	Loss 0.5550 (0.5496)
+2022-11-18 14:54:22,061:INFO: Dataset: univ                Batch:  6/15	Loss 0.5494 (0.5495)
+2022-11-18 14:54:22,214:INFO: Dataset: univ                Batch:  7/15	Loss 0.5489 (0.5494)
+2022-11-18 14:54:22,365:INFO: Dataset: univ                Batch:  8/15	Loss 0.5497 (0.5495)
+2022-11-18 14:54:22,518:INFO: Dataset: univ                Batch:  9/15	Loss 0.5514 (0.5497)
+2022-11-18 14:54:22,671:INFO: Dataset: univ                Batch: 10/15	Loss 0.5505 (0.5498)
+2022-11-18 14:54:22,826:INFO: Dataset: univ                Batch: 11/15	Loss 0.5414 (0.5490)
+2022-11-18 14:54:22,980:INFO: Dataset: univ                Batch: 12/15	Loss 0.5413 (0.5483)
+2022-11-18 14:54:23,134:INFO: Dataset: univ                Batch: 13/15	Loss 0.5480 (0.5483)
+2022-11-18 14:54:23,286:INFO: Dataset: univ                Batch: 14/15	Loss 0.5629 (0.5493)
+2022-11-18 14:54:23,368:INFO: Dataset: univ                Batch: 15/15	Loss 0.5536 (0.5494)
+2022-11-18 14:54:23,767:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5959 (0.5959)
+2022-11-18 14:54:23,916:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5997 (0.5978)
+2022-11-18 14:54:24,065:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5907 (0.5955)
+2022-11-18 14:54:24,217:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5852 (0.5931)
+2022-11-18 14:54:24,365:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5840 (0.5912)
+2022-11-18 14:54:24,515:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5990 (0.5925)
+2022-11-18 14:54:24,663:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5970 (0.5932)
+2022-11-18 14:54:24,799:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5540 (0.5888)
+2022-11-18 14:54:25,202:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5709 (0.5709)
+2022-11-18 14:54:25,349:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5661 (0.5686)
+2022-11-18 14:54:25,501:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5635 (0.5669)
+2022-11-18 14:54:25,649:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5666 (0.5668)
+2022-11-18 14:54:25,798:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5656 (0.5665)
+2022-11-18 14:54:25,955:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5768 (0.5683)
+2022-11-18 14:54:26,103:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5708 (0.5687)
+2022-11-18 14:54:26,248:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5450 (0.5654)
+2022-11-18 14:54:26,395:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5533 (0.5641)
+2022-11-18 14:54:26,542:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5606 (0.5637)
+2022-11-18 14:54:26,690:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5590 (0.5633)
+2022-11-18 14:54:26,838:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5602 (0.5630)
+2022-11-18 14:54:26,985:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5579 (0.5626)
+2022-11-18 14:54:27,134:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5636 (0.5627)
+2022-11-18 14:54:27,282:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5613 (0.5626)
+2022-11-18 14:54:27,430:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5922 (0.5646)
+2022-11-18 14:54:27,578:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5628 (0.5645)
+2022-11-18 14:54:27,715:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5784 (0.5652)
+2022-11-18 14:54:27,759:INFO: - Computing loss (validation)
+2022-11-18 14:54:28,033:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6045 (0.6045)
+2022-11-18 14:54:28,068:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5719 (0.6017)
+2022-11-18 14:54:28,386:INFO: Dataset: univ                Batch: 1/3	Loss 0.5415 (0.5415)
+2022-11-18 14:54:28,465:INFO: Dataset: univ                Batch: 2/3	Loss 0.5599 (0.5507)
+2022-11-18 14:54:28,538:INFO: Dataset: univ                Batch: 3/3	Loss 0.5431 (0.5483)
+2022-11-18 14:54:28,839:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5949 (0.5949)
+2022-11-18 14:54:28,883:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5832 (0.5918)
+2022-11-18 14:54:29,200:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5683 (0.5683)
+2022-11-18 14:54:29,276:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5438 (0.5556)
+2022-11-18 14:54:29,350:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5707 (0.5605)
+2022-11-18 14:54:29,426:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5281 (0.5522)
+2022-11-18 14:54:29,499:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5582 (0.5534)
+2022-11-18 14:54:29,552:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_224.pth.tar
+2022-11-18 14:54:29,552:INFO: 
+===> EPOCH: 225 (P2)
+2022-11-18 14:54:29,552:INFO: - Computing loss (training)
+2022-11-18 14:54:29,886:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5730 (0.5730)
+2022-11-18 14:54:30,035:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6046 (0.5890)
+2022-11-18 14:54:30,184:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6224 (0.6001)
+2022-11-18 14:54:30,307:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6429 (0.6073)
+2022-11-18 14:54:30,706:INFO: Dataset: univ                Batch:  1/15	Loss 0.5687 (0.5687)
+2022-11-18 14:54:30,866:INFO: Dataset: univ                Batch:  2/15	Loss 0.5759 (0.5721)
+2022-11-18 14:54:31,021:INFO: Dataset: univ                Batch:  3/15	Loss 0.5765 (0.5735)
+2022-11-18 14:54:31,173:INFO: Dataset: univ                Batch:  4/15	Loss 0.5677 (0.5722)
+2022-11-18 14:54:31,330:INFO: Dataset: univ                Batch:  5/15	Loss 0.5693 (0.5717)
+2022-11-18 14:54:31,490:INFO: Dataset: univ                Batch:  6/15	Loss 0.5571 (0.5690)
+2022-11-18 14:54:31,646:INFO: Dataset: univ                Batch:  7/15	Loss 0.5701 (0.5692)
+2022-11-18 14:54:31,811:INFO: Dataset: univ                Batch:  8/15	Loss 0.5603 (0.5680)
+2022-11-18 14:54:31,971:INFO: Dataset: univ                Batch:  9/15	Loss 0.5695 (0.5681)
+2022-11-18 14:54:32,129:INFO: Dataset: univ                Batch: 10/15	Loss 0.5706 (0.5684)
+2022-11-18 14:54:32,294:INFO: Dataset: univ                Batch: 11/15	Loss 0.5629 (0.5678)
+2022-11-18 14:54:32,460:INFO: Dataset: univ                Batch: 12/15	Loss 0.5658 (0.5677)
+2022-11-18 14:54:32,621:INFO: Dataset: univ                Batch: 13/15	Loss 0.5670 (0.5676)
+2022-11-18 14:54:32,781:INFO: Dataset: univ                Batch: 14/15	Loss 0.5632 (0.5673)
+2022-11-18 14:54:32,866:INFO: Dataset: univ                Batch: 15/15	Loss 0.5984 (0.5678)
+2022-11-18 14:54:33,344:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6155 (0.6155)
+2022-11-18 14:54:33,510:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5979 (0.6069)
+2022-11-18 14:54:33,662:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5994 (0.6042)
+2022-11-18 14:54:33,817:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5912 (0.6008)
+2022-11-18 14:54:33,974:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5937 (0.5994)
+2022-11-18 14:54:34,127:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6275 (0.6045)
+2022-11-18 14:54:34,278:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6176 (0.6062)
+2022-11-18 14:54:34,417:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5948 (0.6050)
+2022-11-18 14:54:34,826:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5652 (0.5652)
+2022-11-18 14:54:34,976:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5811 (0.5727)
+2022-11-18 14:54:35,128:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5701 (0.5717)
+2022-11-18 14:54:35,280:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5915 (0.5761)
+2022-11-18 14:54:35,429:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5745 (0.5758)
+2022-11-18 14:54:35,583:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5612 (0.5732)
+2022-11-18 14:54:35,738:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5674 (0.5724)
+2022-11-18 14:54:35,887:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5566 (0.5706)
+2022-11-18 14:54:36,039:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5758 (0.5711)
+2022-11-18 14:54:36,191:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5787 (0.5718)
+2022-11-18 14:54:36,345:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5947 (0.5740)
+2022-11-18 14:54:36,496:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5701 (0.5737)
+2022-11-18 14:54:36,648:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5764 (0.5739)
+2022-11-18 14:54:36,802:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5880 (0.5750)
+2022-11-18 14:54:36,965:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5810 (0.5754)
+2022-11-18 14:54:37,123:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5811 (0.5757)
+2022-11-18 14:54:37,278:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5815 (0.5760)
+2022-11-18 14:54:37,416:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5461 (0.5745)
+2022-11-18 14:54:37,479:INFO: - Computing loss (validation)
+2022-11-18 14:54:37,774:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6111 (0.6111)
+2022-11-18 14:54:37,812:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5505 (0.6068)
+2022-11-18 14:54:38,126:INFO: Dataset: univ                Batch: 1/3	Loss 0.5530 (0.5530)
+2022-11-18 14:54:38,211:INFO: Dataset: univ                Batch: 2/3	Loss 0.5582 (0.5557)
+2022-11-18 14:54:38,289:INFO: Dataset: univ                Batch: 3/3	Loss 0.5602 (0.5572)
+2022-11-18 14:54:38,600:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5939 (0.5939)
+2022-11-18 14:54:38,648:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5586 (0.5854)
+2022-11-18 14:54:38,965:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5702 (0.5702)
+2022-11-18 14:54:39,043:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5460 (0.5578)
+2022-11-18 14:54:39,119:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6084 (0.5741)
+2022-11-18 14:54:39,199:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5651 (0.5719)
+2022-11-18 14:54:39,275:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5549 (0.5685)
+2022-11-18 14:54:39,329:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_225.pth.tar
+2022-11-18 14:54:39,329:INFO: 
+===> EPOCH: 226 (P2)
+2022-11-18 14:54:39,330:INFO: - Computing loss (training)
+2022-11-18 14:54:39,727:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5964 (0.5964)
+2022-11-18 14:54:39,903:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6178 (0.6065)
+2022-11-18 14:54:40,091:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6244 (0.6127)
+2022-11-18 14:54:40,231:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6236 (0.6145)
+2022-11-18 14:54:40,634:INFO: Dataset: univ                Batch:  1/15	Loss 0.5863 (0.5863)
+2022-11-18 14:54:40,799:INFO: Dataset: univ                Batch:  2/15	Loss 0.5857 (0.5860)
+2022-11-18 14:54:40,961:INFO: Dataset: univ                Batch:  3/15	Loss 0.5797 (0.5839)
+2022-11-18 14:54:41,136:INFO: Dataset: univ                Batch:  4/15	Loss 0.5879 (0.5850)
+2022-11-18 14:54:41,321:INFO: Dataset: univ                Batch:  5/15	Loss 0.5829 (0.5846)
+2022-11-18 14:54:41,497:INFO: Dataset: univ                Batch:  6/15	Loss 0.5912 (0.5857)
+2022-11-18 14:54:41,657:INFO: Dataset: univ                Batch:  7/15	Loss 0.5831 (0.5854)
+2022-11-18 14:54:41,809:INFO: Dataset: univ                Batch:  8/15	Loss 0.5787 (0.5846)
+2022-11-18 14:54:41,962:INFO: Dataset: univ                Batch:  9/15	Loss 0.5825 (0.5843)
+2022-11-18 14:54:42,116:INFO: Dataset: univ                Batch: 10/15	Loss 0.5873 (0.5846)
+2022-11-18 14:54:42,271:INFO: Dataset: univ                Batch: 11/15	Loss 0.5776 (0.5839)
+2022-11-18 14:54:42,431:INFO: Dataset: univ                Batch: 12/15	Loss 0.5771 (0.5835)
+2022-11-18 14:54:42,618:INFO: Dataset: univ                Batch: 13/15	Loss 0.5793 (0.5831)
+2022-11-18 14:54:42,803:INFO: Dataset: univ                Batch: 14/15	Loss 0.5888 (0.5836)
+2022-11-18 14:54:42,906:INFO: Dataset: univ                Batch: 15/15	Loss 0.5826 (0.5836)
+2022-11-18 14:54:43,327:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6275 (0.6275)
+2022-11-18 14:54:43,486:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5875 (0.6062)
+2022-11-18 14:54:43,640:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6150 (0.6088)
+2022-11-18 14:54:43,810:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6461 (0.6174)
+2022-11-18 14:54:43,968:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6178 (0.6174)
+2022-11-18 14:54:44,119:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6422 (0.6215)
+2022-11-18 14:54:44,269:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5964 (0.6179)
+2022-11-18 14:54:44,405:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6282 (0.6191)
+2022-11-18 14:54:44,838:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6011 (0.6011)
+2022-11-18 14:54:45,008:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5820 (0.5915)
+2022-11-18 14:54:45,176:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6089 (0.5974)
+2022-11-18 14:54:45,331:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6013 (0.5984)
+2022-11-18 14:54:45,494:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5749 (0.5934)
+2022-11-18 14:54:45,655:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5939 (0.5935)
+2022-11-18 14:54:45,814:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6095 (0.5959)
+2022-11-18 14:54:45,968:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5816 (0.5941)
+2022-11-18 14:54:46,140:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5826 (0.5928)
+2022-11-18 14:54:46,307:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5866 (0.5922)
+2022-11-18 14:54:46,482:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6147 (0.5943)
+2022-11-18 14:54:46,643:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5942 (0.5943)
+2022-11-18 14:54:46,805:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5899 (0.5939)
+2022-11-18 14:54:46,966:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5970 (0.5941)
+2022-11-18 14:54:47,125:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6127 (0.5955)
+2022-11-18 14:54:47,278:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5948 (0.5954)
+2022-11-18 14:54:47,458:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5913 (0.5952)
+2022-11-18 14:54:47,635:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5640 (0.5937)
+2022-11-18 14:54:47,688:INFO: - Computing loss (validation)
+2022-11-18 14:54:47,968:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6084 (0.6084)
+2022-11-18 14:54:48,008:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7311 (0.6176)
+2022-11-18 14:54:48,338:INFO: Dataset: univ                Batch: 1/3	Loss 0.5771 (0.5771)
+2022-11-18 14:54:48,419:INFO: Dataset: univ                Batch: 2/3	Loss 0.5866 (0.5817)
+2022-11-18 14:54:48,493:INFO: Dataset: univ                Batch: 3/3	Loss 0.5972 (0.5856)
+2022-11-18 14:54:48,846:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6124 (0.6124)
+2022-11-18 14:54:48,896:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6537 (0.6217)
+2022-11-18 14:54:49,210:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5674 (0.5674)
+2022-11-18 14:54:49,288:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5845 (0.5760)
+2022-11-18 14:54:49,369:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5705 (0.5742)
+2022-11-18 14:54:49,452:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5734 (0.5740)
+2022-11-18 14:54:49,531:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5743 (0.5741)
+2022-11-18 14:54:49,584:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_226.pth.tar
+2022-11-18 14:54:49,585:INFO: 
+===> EPOCH: 227 (P2)
+2022-11-18 14:54:49,585:INFO: - Computing loss (training)
+2022-11-18 14:54:49,931:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6618 (0.6618)
+2022-11-18 14:54:50,085:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6614 (0.6616)
+2022-11-18 14:54:50,233:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6512 (0.6580)
+2022-11-18 14:54:50,347:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5844 (0.6458)
+2022-11-18 14:54:50,769:INFO: Dataset: univ                Batch:  1/15	Loss 0.5989 (0.5989)
+2022-11-18 14:54:50,944:INFO: Dataset: univ                Batch:  2/15	Loss 0.5952 (0.5969)
+2022-11-18 14:54:51,111:INFO: Dataset: univ                Batch:  3/15	Loss 0.5878 (0.5940)
+2022-11-18 14:54:51,277:INFO: Dataset: univ                Batch:  4/15	Loss 0.5959 (0.5945)
+2022-11-18 14:54:51,445:INFO: Dataset: univ                Batch:  5/15	Loss 0.5883 (0.5932)
+2022-11-18 14:54:51,628:INFO: Dataset: univ                Batch:  6/15	Loss 0.5844 (0.5918)
+2022-11-18 14:54:51,806:INFO: Dataset: univ                Batch:  7/15	Loss 0.6072 (0.5939)
+2022-11-18 14:54:51,987:INFO: Dataset: univ                Batch:  8/15	Loss 0.5884 (0.5933)
+2022-11-18 14:54:52,239:INFO: Dataset: univ                Batch:  9/15	Loss 0.5995 (0.5939)
+2022-11-18 14:54:52,401:INFO: Dataset: univ                Batch: 10/15	Loss 0.5983 (0.5944)
+2022-11-18 14:54:52,586:INFO: Dataset: univ                Batch: 11/15	Loss 0.5921 (0.5941)
+2022-11-18 14:54:52,745:INFO: Dataset: univ                Batch: 12/15	Loss 0.5983 (0.5945)
+2022-11-18 14:54:52,900:INFO: Dataset: univ                Batch: 13/15	Loss 0.5923 (0.5943)
+2022-11-18 14:54:53,076:INFO: Dataset: univ                Batch: 14/15	Loss 0.5966 (0.5945)
+2022-11-18 14:54:53,161:INFO: Dataset: univ                Batch: 15/15	Loss 0.6156 (0.5948)
+2022-11-18 14:54:53,542:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6108 (0.6108)
+2022-11-18 14:54:53,690:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6485 (0.6293)
+2022-11-18 14:54:53,845:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6234 (0.6274)
+2022-11-18 14:54:53,998:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6474 (0.6324)
+2022-11-18 14:54:54,158:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6260 (0.6312)
+2022-11-18 14:54:54,323:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6233 (0.6300)
+2022-11-18 14:54:54,486:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6266 (0.6295)
+2022-11-18 14:54:54,630:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6470 (0.6315)
+2022-11-18 14:54:55,028:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6085 (0.6085)
+2022-11-18 14:54:55,182:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6034 (0.6059)
+2022-11-18 14:54:55,332:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6051 (0.6057)
+2022-11-18 14:54:55,479:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6042 (0.6053)
+2022-11-18 14:54:55,628:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5866 (0.6013)
+2022-11-18 14:54:55,791:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6138 (0.6034)
+2022-11-18 14:54:55,975:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6175 (0.6053)
+2022-11-18 14:54:56,132:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6176 (0.6067)
+2022-11-18 14:54:56,286:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6023 (0.6062)
+2022-11-18 14:54:56,441:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6202 (0.6075)
+2022-11-18 14:54:56,591:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6119 (0.6079)
+2022-11-18 14:54:56,737:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6042 (0.6076)
+2022-11-18 14:54:56,893:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6103 (0.6078)
+2022-11-18 14:54:57,050:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6245 (0.6091)
+2022-11-18 14:54:57,202:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5965 (0.6083)
+2022-11-18 14:54:57,350:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5997 (0.6078)
+2022-11-18 14:54:57,504:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6449 (0.6100)
+2022-11-18 14:54:57,649:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6064 (0.6098)
+2022-11-18 14:54:57,695:INFO: - Computing loss (validation)
+2022-11-18 14:54:57,971:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6309 (0.6309)
+2022-11-18 14:54:58,006:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6378 (0.6313)
+2022-11-18 14:54:58,316:INFO: Dataset: univ                Batch: 1/3	Loss 0.5982 (0.5982)
+2022-11-18 14:54:58,400:INFO: Dataset: univ                Batch: 2/3	Loss 0.6013 (0.5999)
+2022-11-18 14:54:58,478:INFO: Dataset: univ                Batch: 3/3	Loss 0.5994 (0.5997)
+2022-11-18 14:54:58,784:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6376 (0.6376)
+2022-11-18 14:54:58,831:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6062 (0.6291)
+2022-11-18 14:54:59,141:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6084 (0.6084)
+2022-11-18 14:54:59,223:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6014 (0.6049)
+2022-11-18 14:54:59,303:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5914 (0.6004)
+2022-11-18 14:54:59,391:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6025 (0.6009)
+2022-11-18 14:54:59,472:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5955 (0.5997)
+2022-11-18 14:54:59,526:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_227.pth.tar
+2022-11-18 14:54:59,526:INFO: 
+===> EPOCH: 228 (P2)
+2022-11-18 14:54:59,527:INFO: - Computing loss (training)
+2022-11-18 14:54:59,890:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6295 (0.6295)
+2022-11-18 14:55:00,061:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6458 (0.6378)
+2022-11-18 14:55:00,248:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6733 (0.6501)
+2022-11-18 14:55:00,441:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6172 (0.6447)
+2022-11-18 14:55:00,945:INFO: Dataset: univ                Batch:  1/15	Loss 0.6159 (0.6159)
+2022-11-18 14:55:01,149:INFO: Dataset: univ                Batch:  2/15	Loss 0.6211 (0.6186)
+2022-11-18 14:55:01,342:INFO: Dataset: univ                Batch:  3/15	Loss 0.6021 (0.6133)
+2022-11-18 14:55:01,511:INFO: Dataset: univ                Batch:  4/15	Loss 0.6075 (0.6118)
+2022-11-18 14:55:01,671:INFO: Dataset: univ                Batch:  5/15	Loss 0.6030 (0.6101)
+2022-11-18 14:55:01,834:INFO: Dataset: univ                Batch:  6/15	Loss 0.6142 (0.6108)
+2022-11-18 14:55:02,001:INFO: Dataset: univ                Batch:  7/15	Loss 0.6097 (0.6107)
+2022-11-18 14:55:02,181:INFO: Dataset: univ                Batch:  8/15	Loss 0.5983 (0.6093)
+2022-11-18 14:55:02,353:INFO: Dataset: univ                Batch:  9/15	Loss 0.6057 (0.6088)
+2022-11-18 14:55:02,516:INFO: Dataset: univ                Batch: 10/15	Loss 0.6183 (0.6097)
+2022-11-18 14:55:02,676:INFO: Dataset: univ                Batch: 11/15	Loss 0.6158 (0.6102)
+2022-11-18 14:55:02,840:INFO: Dataset: univ                Batch: 12/15	Loss 0.6166 (0.6107)
+2022-11-18 14:55:03,002:INFO: Dataset: univ                Batch: 13/15	Loss 0.6090 (0.6106)
+2022-11-18 14:55:03,163:INFO: Dataset: univ                Batch: 14/15	Loss 0.6133 (0.6108)
+2022-11-18 14:55:03,244:INFO: Dataset: univ                Batch: 15/15	Loss 0.6143 (0.6108)
+2022-11-18 14:55:03,622:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6195 (0.6195)
+2022-11-18 14:55:03,770:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6768 (0.6450)
+2022-11-18 14:55:03,917:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6174 (0.6374)
+2022-11-18 14:55:04,065:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6379 (0.6375)
+2022-11-18 14:55:04,246:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6288 (0.6358)
+2022-11-18 14:55:04,435:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5963 (0.6288)
+2022-11-18 14:55:04,611:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6565 (0.6326)
+2022-11-18 14:55:04,767:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6511 (0.6348)
+2022-11-18 14:55:05,203:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6320 (0.6320)
+2022-11-18 14:55:05,401:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6126 (0.6219)
+2022-11-18 14:55:05,554:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6416 (0.6290)
+2022-11-18 14:55:05,711:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6289 (0.6290)
+2022-11-18 14:55:05,871:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6298 (0.6291)
+2022-11-18 14:55:06,025:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5963 (0.6236)
+2022-11-18 14:55:06,191:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6137 (0.6221)
+2022-11-18 14:55:06,346:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6142 (0.6212)
+2022-11-18 14:55:06,497:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6101 (0.6201)
+2022-11-18 14:55:06,648:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6126 (0.6194)
+2022-11-18 14:55:06,796:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6634 (0.6233)
+2022-11-18 14:55:06,953:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6253 (0.6234)
+2022-11-18 14:55:07,124:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6196 (0.6232)
+2022-11-18 14:55:07,283:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6221 (0.6231)
+2022-11-18 14:55:07,452:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6275 (0.6234)
+2022-11-18 14:55:07,640:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6371 (0.6243)
+2022-11-18 14:55:07,794:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6012 (0.6229)
+2022-11-18 14:55:07,935:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6373 (0.6236)
+2022-11-18 14:55:07,982:INFO: - Computing loss (validation)
+2022-11-18 14:55:08,264:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6193 (0.6193)
+2022-11-18 14:55:08,301:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7210 (0.6245)
+2022-11-18 14:55:08,633:INFO: Dataset: univ                Batch: 1/3	Loss 0.6247 (0.6247)
+2022-11-18 14:55:08,716:INFO: Dataset: univ                Batch: 2/3	Loss 0.6218 (0.6231)
+2022-11-18 14:55:08,794:INFO: Dataset: univ                Batch: 3/3	Loss 0.6271 (0.6244)
+2022-11-18 14:55:09,104:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6564 (0.6564)
+2022-11-18 14:55:09,151:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6637 (0.6581)
+2022-11-18 14:55:09,469:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6278 (0.6278)
+2022-11-18 14:55:09,545:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6175 (0.6227)
+2022-11-18 14:55:09,622:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6117 (0.6191)
+2022-11-18 14:55:09,699:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6165 (0.6185)
+2022-11-18 14:55:09,775:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6159 (0.6180)
+2022-11-18 14:55:09,831:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_228.pth.tar
+2022-11-18 14:55:09,831:INFO: 
+===> EPOCH: 229 (P2)
+2022-11-18 14:55:09,831:INFO: - Computing loss (training)
+2022-11-18 14:55:10,167:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6815 (0.6815)
+2022-11-18 14:55:10,316:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6540 (0.6671)
+2022-11-18 14:55:10,466:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7071 (0.6807)
+2022-11-18 14:55:10,581:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6784 (0.6803)
+2022-11-18 14:55:10,984:INFO: Dataset: univ                Batch:  1/15	Loss 0.6282 (0.6282)
+2022-11-18 14:55:11,145:INFO: Dataset: univ                Batch:  2/15	Loss 0.6237 (0.6260)
+2022-11-18 14:55:11,310:INFO: Dataset: univ                Batch:  3/15	Loss 0.6123 (0.6215)
+2022-11-18 14:55:11,465:INFO: Dataset: univ                Batch:  4/15	Loss 0.6204 (0.6212)
+2022-11-18 14:55:11,622:INFO: Dataset: univ                Batch:  5/15	Loss 0.6261 (0.6221)
+2022-11-18 14:55:11,857:INFO: Dataset: univ                Batch:  6/15	Loss 0.6317 (0.6237)
+2022-11-18 14:55:12,012:INFO: Dataset: univ                Batch:  7/15	Loss 0.6378 (0.6256)
+2022-11-18 14:55:12,176:INFO: Dataset: univ                Batch:  8/15	Loss 0.6333 (0.6266)
+2022-11-18 14:55:12,334:INFO: Dataset: univ                Batch:  9/15	Loss 0.6239 (0.6263)
+2022-11-18 14:55:12,498:INFO: Dataset: univ                Batch: 10/15	Loss 0.6236 (0.6260)
+2022-11-18 14:55:12,663:INFO: Dataset: univ                Batch: 11/15	Loss 0.6384 (0.6272)
+2022-11-18 14:55:12,828:INFO: Dataset: univ                Batch: 12/15	Loss 0.6348 (0.6278)
+2022-11-18 14:55:12,986:INFO: Dataset: univ                Batch: 13/15	Loss 0.6198 (0.6272)
+2022-11-18 14:55:13,143:INFO: Dataset: univ                Batch: 14/15	Loss 0.6376 (0.6280)
+2022-11-18 14:55:13,223:INFO: Dataset: univ                Batch: 15/15	Loss 0.6414 (0.6282)
+2022-11-18 14:55:13,612:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6560 (0.6560)
+2022-11-18 14:55:13,759:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6422 (0.6491)
+2022-11-18 14:55:13,908:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6496 (0.6493)
+2022-11-18 14:55:14,053:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6541 (0.6505)
+2022-11-18 14:55:14,202:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6651 (0.6531)
+2022-11-18 14:55:14,348:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6598 (0.6542)
+2022-11-18 14:55:14,494:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6430 (0.6527)
+2022-11-18 14:55:14,627:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6692 (0.6545)
+2022-11-18 14:55:15,020:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6446 (0.6446)
+2022-11-18 14:55:15,166:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6243 (0.6343)
+2022-11-18 14:55:15,313:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6218 (0.6301)
+2022-11-18 14:55:15,462:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6035 (0.6233)
+2022-11-18 14:55:15,612:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6531 (0.6298)
+2022-11-18 14:55:15,761:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6581 (0.6347)
+2022-11-18 14:55:15,908:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6557 (0.6378)
+2022-11-18 14:55:16,052:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6353 (0.6375)
+2022-11-18 14:55:16,199:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6269 (0.6364)
+2022-11-18 14:55:16,344:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6431 (0.6371)
+2022-11-18 14:55:16,491:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6424 (0.6375)
+2022-11-18 14:55:16,637:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6198 (0.6360)
+2022-11-18 14:55:16,787:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6587 (0.6377)
+2022-11-18 14:55:16,934:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6179 (0.6364)
+2022-11-18 14:55:17,082:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6555 (0.6377)
+2022-11-18 14:55:17,229:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6339 (0.6375)
+2022-11-18 14:55:17,379:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6301 (0.6370)
+2022-11-18 14:55:17,519:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6665 (0.6382)
+2022-11-18 14:55:17,566:INFO: - Computing loss (validation)
+2022-11-18 14:55:17,831:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6549 (0.6549)
+2022-11-18 14:55:17,866:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6336 (0.6536)
+2022-11-18 14:55:18,176:INFO: Dataset: univ                Batch: 1/3	Loss 0.6211 (0.6211)
+2022-11-18 14:55:18,257:INFO: Dataset: univ                Batch: 2/3	Loss 0.6138 (0.6173)
+2022-11-18 14:55:18,332:INFO: Dataset: univ                Batch: 3/3	Loss 0.6365 (0.6236)
+2022-11-18 14:55:18,627:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6460 (0.6460)
+2022-11-18 14:55:18,670:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6985 (0.6591)
+2022-11-18 14:55:18,965:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6218 (0.6218)
+2022-11-18 14:55:19,040:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6375 (0.6298)
+2022-11-18 14:55:19,113:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6282 (0.6293)
+2022-11-18 14:55:19,187:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6343 (0.6305)
+2022-11-18 14:55:19,259:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6243 (0.6292)
+2022-11-18 14:55:19,311:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_229.pth.tar
+2022-11-18 14:55:19,311:INFO: 
+===> EPOCH: 230 (P2)
+2022-11-18 14:55:19,311:INFO: - Computing loss (training)
+2022-11-18 14:55:19,652:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6739 (0.6739)
+2022-11-18 14:55:19,802:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6753 (0.6746)
+2022-11-18 14:55:19,952:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6636 (0.6708)
+2022-11-18 14:55:20,066:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6832 (0.6729)
+2022-11-18 14:55:20,519:INFO: Dataset: univ                Batch:  1/15	Loss 0.6537 (0.6537)
+2022-11-18 14:55:20,672:INFO: Dataset: univ                Batch:  2/15	Loss 0.6426 (0.6479)
+2022-11-18 14:55:20,827:INFO: Dataset: univ                Batch:  3/15	Loss 0.6427 (0.6462)
+2022-11-18 14:55:20,977:INFO: Dataset: univ                Batch:  4/15	Loss 0.6412 (0.6450)
+2022-11-18 14:55:21,131:INFO: Dataset: univ                Batch:  5/15	Loss 0.6491 (0.6459)
+2022-11-18 14:55:21,287:INFO: Dataset: univ                Batch:  6/15	Loss 0.6432 (0.6454)
+2022-11-18 14:55:21,440:INFO: Dataset: univ                Batch:  7/15	Loss 0.6413 (0.6449)
+2022-11-18 14:55:21,592:INFO: Dataset: univ                Batch:  8/15	Loss 0.6510 (0.6457)
+2022-11-18 14:55:21,747:INFO: Dataset: univ                Batch:  9/15	Loss 0.6587 (0.6472)
+2022-11-18 14:55:21,898:INFO: Dataset: univ                Batch: 10/15	Loss 0.6372 (0.6462)
+2022-11-18 14:55:22,052:INFO: Dataset: univ                Batch: 11/15	Loss 0.6459 (0.6462)
+2022-11-18 14:55:22,206:INFO: Dataset: univ                Batch: 12/15	Loss 0.6354 (0.6452)
+2022-11-18 14:55:22,360:INFO: Dataset: univ                Batch: 13/15	Loss 0.6552 (0.6460)
+2022-11-18 14:55:22,514:INFO: Dataset: univ                Batch: 14/15	Loss 0.6538 (0.6466)
+2022-11-18 14:55:22,595:INFO: Dataset: univ                Batch: 15/15	Loss 0.6803 (0.6471)
+2022-11-18 14:55:22,978:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6947 (0.6947)
+2022-11-18 14:55:23,125:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7021 (0.6985)
+2022-11-18 14:55:23,274:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6688 (0.6896)
+2022-11-18 14:55:23,424:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6774 (0.6866)
+2022-11-18 14:55:23,572:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6916 (0.6876)
+2022-11-18 14:55:23,720:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6899 (0.6880)
+2022-11-18 14:55:23,869:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6646 (0.6846)
+2022-11-18 14:55:24,003:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6608 (0.6819)
+2022-11-18 14:55:24,392:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6536 (0.6536)
+2022-11-18 14:55:24,541:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6526 (0.6531)
+2022-11-18 14:55:24,692:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6523 (0.6528)
+2022-11-18 14:55:24,842:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6328 (0.6476)
+2022-11-18 14:55:24,996:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6372 (0.6455)
+2022-11-18 14:55:25,146:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6857 (0.6523)
+2022-11-18 14:55:25,296:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6600 (0.6533)
+2022-11-18 14:55:25,443:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6732 (0.6561)
+2022-11-18 14:55:25,592:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6819 (0.6588)
+2022-11-18 14:55:25,739:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6682 (0.6597)
+2022-11-18 14:55:25,889:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6305 (0.6571)
+2022-11-18 14:55:26,037:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6449 (0.6562)
+2022-11-18 14:55:26,186:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6560 (0.6562)
+2022-11-18 14:55:26,335:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6409 (0.6552)
+2022-11-18 14:55:26,485:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6413 (0.6543)
+2022-11-18 14:55:26,634:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6265 (0.6525)
+2022-11-18 14:55:26,784:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6750 (0.6539)
+2022-11-18 14:55:26,922:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6376 (0.6530)
+2022-11-18 14:55:26,967:INFO: - Computing loss (validation)
+2022-11-18 14:55:27,233:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6633 (0.6633)
+2022-11-18 14:55:27,268:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6695 (0.6638)
+2022-11-18 14:55:27,566:INFO: Dataset: univ                Batch: 1/3	Loss 0.6406 (0.6406)
+2022-11-18 14:55:27,643:INFO: Dataset: univ                Batch: 2/3	Loss 0.6470 (0.6437)
+2022-11-18 14:55:27,712:INFO: Dataset: univ                Batch: 3/3	Loss 0.6493 (0.6453)
+2022-11-18 14:55:28,010:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6844 (0.6844)
+2022-11-18 14:55:28,055:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6605 (0.6787)
+2022-11-18 14:55:28,367:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6781 (0.6781)
+2022-11-18 14:55:28,449:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6555 (0.6669)
+2022-11-18 14:55:28,525:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6677 (0.6671)
+2022-11-18 14:55:28,601:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6631 (0.6662)
+2022-11-18 14:55:28,676:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6730 (0.6675)
+2022-11-18 14:55:28,729:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_230.pth.tar
+2022-11-18 14:55:28,729:INFO: 
+===> EPOCH: 231 (P2)
+2022-11-18 14:55:28,730:INFO: - Computing loss (training)
+2022-11-18 14:55:29,069:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7269 (0.7269)
+2022-11-18 14:55:29,220:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6461 (0.6852)
+2022-11-18 14:55:29,368:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6965 (0.6889)
+2022-11-18 14:55:29,483:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7258 (0.6946)
+2022-11-18 14:55:29,870:INFO: Dataset: univ                Batch:  1/15	Loss 0.6607 (0.6607)
+2022-11-18 14:55:30,022:INFO: Dataset: univ                Batch:  2/15	Loss 0.6546 (0.6578)
+2022-11-18 14:55:30,175:INFO: Dataset: univ                Batch:  3/15	Loss 0.6559 (0.6572)
+2022-11-18 14:55:30,329:INFO: Dataset: univ                Batch:  4/15	Loss 0.6411 (0.6531)
+2022-11-18 14:55:30,483:INFO: Dataset: univ                Batch:  5/15	Loss 0.6580 (0.6541)
+2022-11-18 14:55:30,633:INFO: Dataset: univ                Batch:  6/15	Loss 0.6495 (0.6533)
+2022-11-18 14:55:30,784:INFO: Dataset: univ                Batch:  7/15	Loss 0.6612 (0.6544)
+2022-11-18 14:55:30,933:INFO: Dataset: univ                Batch:  8/15	Loss 0.6588 (0.6550)
+2022-11-18 14:55:31,084:INFO: Dataset: univ                Batch:  9/15	Loss 0.6515 (0.6546)
+2022-11-18 14:55:31,234:INFO: Dataset: univ                Batch: 10/15	Loss 0.6596 (0.6551)
+2022-11-18 14:55:31,389:INFO: Dataset: univ                Batch: 11/15	Loss 0.6590 (0.6555)
+2022-11-18 14:55:31,538:INFO: Dataset: univ                Batch: 12/15	Loss 0.6699 (0.6568)
+2022-11-18 14:55:31,690:INFO: Dataset: univ                Batch: 13/15	Loss 0.6591 (0.6570)
+2022-11-18 14:55:31,842:INFO: Dataset: univ                Batch: 14/15	Loss 0.6612 (0.6573)
+2022-11-18 14:55:31,922:INFO: Dataset: univ                Batch: 15/15	Loss 0.6844 (0.6575)
+2022-11-18 14:55:32,313:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7223 (0.7223)
+2022-11-18 14:55:32,459:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6647 (0.6951)
+2022-11-18 14:55:32,607:INFO: Dataset: zara1               Batch: 3/8	Loss 0.6940 (0.6947)
+2022-11-18 14:55:32,753:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6656 (0.6875)
+2022-11-18 14:55:32,901:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7239 (0.6938)
+2022-11-18 14:55:33,048:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7152 (0.6973)
+2022-11-18 14:55:33,195:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7305 (0.7019)
+2022-11-18 14:55:33,330:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6776 (0.6990)
+2022-11-18 14:55:33,709:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6740 (0.6740)
+2022-11-18 14:55:33,857:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6677 (0.6707)
+2022-11-18 14:55:34,007:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6550 (0.6655)
+2022-11-18 14:55:34,154:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7036 (0.6750)
+2022-11-18 14:55:34,302:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6758 (0.6752)
+2022-11-18 14:55:34,449:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6843 (0.6767)
+2022-11-18 14:55:34,595:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6639 (0.6747)
+2022-11-18 14:55:34,738:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6961 (0.6774)
+2022-11-18 14:55:34,885:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6844 (0.6781)
+2022-11-18 14:55:35,028:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6753 (0.6778)
+2022-11-18 14:55:35,177:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6853 (0.6785)
+2022-11-18 14:55:35,322:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6613 (0.6771)
+2022-11-18 14:55:35,471:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6697 (0.6764)
+2022-11-18 14:55:35,618:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6572 (0.6752)
+2022-11-18 14:55:35,766:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6679 (0.6747)
+2022-11-18 14:55:35,911:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7046 (0.6767)
+2022-11-18 14:55:36,058:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6314 (0.6739)
+2022-11-18 14:55:36,193:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6548 (0.6730)
+2022-11-18 14:55:36,237:INFO: - Computing loss (validation)
+2022-11-18 14:55:36,518:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7353 (0.7353)
+2022-11-18 14:55:36,552:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6719 (0.7316)
+2022-11-18 14:55:36,856:INFO: Dataset: univ                Batch: 1/3	Loss 0.6634 (0.6634)
+2022-11-18 14:55:36,934:INFO: Dataset: univ                Batch: 2/3	Loss 0.6762 (0.6692)
+2022-11-18 14:55:37,009:INFO: Dataset: univ                Batch: 3/3	Loss 0.6521 (0.6637)
+2022-11-18 14:55:37,307:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6717 (0.6717)
+2022-11-18 14:55:37,353:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6999 (0.6782)
+2022-11-18 14:55:37,659:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6875 (0.6875)
+2022-11-18 14:55:37,737:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6588 (0.6731)
+2022-11-18 14:55:37,815:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6583 (0.6680)
+2022-11-18 14:55:37,895:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6557 (0.6651)
+2022-11-18 14:55:37,972:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6838 (0.6690)
+2022-11-18 14:55:38,022:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_231.pth.tar
+2022-11-18 14:55:38,022:INFO: 
+===> EPOCH: 232 (P2)
+2022-11-18 14:55:38,023:INFO: - Computing loss (training)
+2022-11-18 14:55:38,352:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7180 (0.7180)
+2022-11-18 14:55:38,497:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6912 (0.7050)
+2022-11-18 14:55:38,643:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7229 (0.7112)
+2022-11-18 14:55:38,756:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7219 (0.7131)
+2022-11-18 14:55:39,193:INFO: Dataset: univ                Batch:  1/15	Loss 0.6938 (0.6938)
+2022-11-18 14:55:39,346:INFO: Dataset: univ                Batch:  2/15	Loss 0.6884 (0.6910)
+2022-11-18 14:55:39,498:INFO: Dataset: univ                Batch:  3/15	Loss 0.6840 (0.6887)
+2022-11-18 14:55:39,646:INFO: Dataset: univ                Batch:  4/15	Loss 0.6764 (0.6858)
+2022-11-18 14:55:39,799:INFO: Dataset: univ                Batch:  5/15	Loss 0.6807 (0.6847)
+2022-11-18 14:55:39,951:INFO: Dataset: univ                Batch:  6/15	Loss 0.6743 (0.6830)
+2022-11-18 14:55:40,103:INFO: Dataset: univ                Batch:  7/15	Loss 0.6999 (0.6855)
+2022-11-18 14:55:40,252:INFO: Dataset: univ                Batch:  8/15	Loss 0.6931 (0.6865)
+2022-11-18 14:55:40,404:INFO: Dataset: univ                Batch:  9/15	Loss 0.6877 (0.6866)
+2022-11-18 14:55:40,553:INFO: Dataset: univ                Batch: 10/15	Loss 0.6794 (0.6859)
+2022-11-18 14:55:40,705:INFO: Dataset: univ                Batch: 11/15	Loss 0.6761 (0.6850)
+2022-11-18 14:55:40,857:INFO: Dataset: univ                Batch: 12/15	Loss 0.6671 (0.6835)
+2022-11-18 14:55:41,010:INFO: Dataset: univ                Batch: 13/15	Loss 0.6862 (0.6837)
+2022-11-18 14:55:41,161:INFO: Dataset: univ                Batch: 14/15	Loss 0.6799 (0.6835)
+2022-11-18 14:55:41,243:INFO: Dataset: univ                Batch: 15/15	Loss 0.6767 (0.6834)
+2022-11-18 14:55:41,623:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6886 (0.6886)
+2022-11-18 14:55:41,768:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6739 (0.6812)
+2022-11-18 14:55:41,913:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7202 (0.6945)
+2022-11-18 14:55:42,055:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6981 (0.6954)
+2022-11-18 14:55:42,201:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6985 (0.6961)
+2022-11-18 14:55:42,348:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7457 (0.7044)
+2022-11-18 14:55:42,493:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6982 (0.7035)
+2022-11-18 14:55:42,626:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7022 (0.7033)
+2022-11-18 14:55:43,014:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6583 (0.6583)
+2022-11-18 14:55:43,162:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6943 (0.6757)
+2022-11-18 14:55:43,311:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7053 (0.6854)
+2022-11-18 14:55:43,458:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6987 (0.6888)
+2022-11-18 14:55:43,606:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6736 (0.6860)
+2022-11-18 14:55:43,758:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6832 (0.6855)
+2022-11-18 14:55:43,905:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6916 (0.6864)
+2022-11-18 14:55:44,049:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7039 (0.6887)
+2022-11-18 14:55:44,196:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7016 (0.6901)
+2022-11-18 14:55:44,341:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6784 (0.6888)
+2022-11-18 14:55:44,487:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7145 (0.6911)
+2022-11-18 14:55:44,633:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7050 (0.6922)
+2022-11-18 14:55:44,780:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6792 (0.6912)
+2022-11-18 14:55:44,926:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6779 (0.6901)
+2022-11-18 14:55:45,073:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6944 (0.6904)
+2022-11-18 14:55:45,219:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7015 (0.6911)
+2022-11-18 14:55:45,366:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6905 (0.6911)
+2022-11-18 14:55:45,501:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6910 (0.6911)
+2022-11-18 14:55:45,546:INFO: - Computing loss (validation)
+2022-11-18 14:55:45,819:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6860 (0.6860)
+2022-11-18 14:55:45,853:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7132 (0.6879)
+2022-11-18 14:55:46,165:INFO: Dataset: univ                Batch: 1/3	Loss 0.6942 (0.6942)
+2022-11-18 14:55:46,244:INFO: Dataset: univ                Batch: 2/3	Loss 0.6550 (0.6749)
+2022-11-18 14:55:46,319:INFO: Dataset: univ                Batch: 3/3	Loss 0.6729 (0.6743)
+2022-11-18 14:55:46,627:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6737 (0.6737)
+2022-11-18 14:55:46,671:INFO: Dataset: zara1               Batch: 2/2	Loss 0.7058 (0.6815)
+2022-11-18 14:55:46,963:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6592 (0.6592)
+2022-11-18 14:55:47,034:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6605 (0.6599)
+2022-11-18 14:55:47,108:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6992 (0.6724)
+2022-11-18 14:55:47,180:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6907 (0.6768)
+2022-11-18 14:55:47,254:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6775 (0.6770)
+2022-11-18 14:55:47,307:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_232.pth.tar
+2022-11-18 14:55:47,308:INFO: 
+===> EPOCH: 233 (P2)
+2022-11-18 14:55:47,308:INFO: - Computing loss (training)
+2022-11-18 14:55:47,653:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7228 (0.7228)
+2022-11-18 14:55:47,822:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7292 (0.7260)
+2022-11-18 14:55:47,976:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7623 (0.7386)
+2022-11-18 14:55:48,094:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7746 (0.7447)
+2022-11-18 14:55:48,487:INFO: Dataset: univ                Batch:  1/15	Loss 0.6916 (0.6916)
+2022-11-18 14:55:48,645:INFO: Dataset: univ                Batch:  2/15	Loss 0.7046 (0.6982)
+2022-11-18 14:55:48,806:INFO: Dataset: univ                Batch:  3/15	Loss 0.6920 (0.6959)
+2022-11-18 14:55:48,962:INFO: Dataset: univ                Batch:  4/15	Loss 0.6981 (0.6965)
+2022-11-18 14:55:49,117:INFO: Dataset: univ                Batch:  5/15	Loss 0.6961 (0.6964)
+2022-11-18 14:55:49,275:INFO: Dataset: univ                Batch:  6/15	Loss 0.6994 (0.6969)
+2022-11-18 14:55:49,430:INFO: Dataset: univ                Batch:  7/15	Loss 0.6886 (0.6958)
+2022-11-18 14:55:49,582:INFO: Dataset: univ                Batch:  8/15	Loss 0.6814 (0.6939)
+2022-11-18 14:55:49,736:INFO: Dataset: univ                Batch:  9/15	Loss 0.6925 (0.6938)
+2022-11-18 14:55:49,890:INFO: Dataset: univ                Batch: 10/15	Loss 0.6820 (0.6927)
+2022-11-18 14:55:50,047:INFO: Dataset: univ                Batch: 11/15	Loss 0.7032 (0.6936)
+2022-11-18 14:55:50,199:INFO: Dataset: univ                Batch: 12/15	Loss 0.6793 (0.6924)
+2022-11-18 14:55:50,353:INFO: Dataset: univ                Batch: 13/15	Loss 0.7000 (0.6930)
+2022-11-18 14:55:50,512:INFO: Dataset: univ                Batch: 14/15	Loss 0.6992 (0.6935)
+2022-11-18 14:55:50,595:INFO: Dataset: univ                Batch: 15/15	Loss 0.7434 (0.6940)
+2022-11-18 14:55:50,989:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7225 (0.7225)
+2022-11-18 14:55:51,135:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7734 (0.7454)
+2022-11-18 14:55:51,285:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7152 (0.7360)
+2022-11-18 14:55:51,430:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7106 (0.7295)
+2022-11-18 14:55:51,579:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7331 (0.7302)
+2022-11-18 14:55:51,724:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7440 (0.7325)
+2022-11-18 14:55:51,876:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7301 (0.7322)
+2022-11-18 14:55:52,011:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7090 (0.7295)
+2022-11-18 14:55:52,440:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6878 (0.6878)
+2022-11-18 14:55:52,585:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7053 (0.6966)
+2022-11-18 14:55:52,732:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7019 (0.6984)
+2022-11-18 14:55:52,877:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7246 (0.7056)
+2022-11-18 14:55:53,023:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6814 (0.7006)
+2022-11-18 14:55:53,170:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6890 (0.6986)
+2022-11-18 14:55:53,316:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6881 (0.6970)
+2022-11-18 14:55:53,461:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6816 (0.6950)
+2022-11-18 14:55:53,607:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7083 (0.6964)
+2022-11-18 14:55:53,750:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7538 (0.7017)
+2022-11-18 14:55:53,896:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7183 (0.7033)
+2022-11-18 14:55:54,041:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6746 (0.7008)
+2022-11-18 14:55:54,188:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7221 (0.7024)
+2022-11-18 14:55:54,335:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7065 (0.7027)
+2022-11-18 14:55:54,481:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6982 (0.7024)
+2022-11-18 14:55:54,626:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7139 (0.7030)
+2022-11-18 14:55:54,773:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7204 (0.7040)
+2022-11-18 14:55:54,908:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7076 (0.7042)
+2022-11-18 14:55:54,954:INFO: - Computing loss (validation)
+2022-11-18 14:55:55,220:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7739 (0.7739)
+2022-11-18 14:55:55,254:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7085 (0.7701)
+2022-11-18 14:55:55,561:INFO: Dataset: univ                Batch: 1/3	Loss 0.6783 (0.6783)
+2022-11-18 14:55:55,638:INFO: Dataset: univ                Batch: 2/3	Loss 0.6771 (0.6777)
+2022-11-18 14:55:55,712:INFO: Dataset: univ                Batch: 3/3	Loss 0.7024 (0.6859)
+2022-11-18 14:55:56,020:INFO: Dataset: zara1               Batch: 1/2	Loss 0.7381 (0.7381)
+2022-11-18 14:55:56,066:INFO: Dataset: zara1               Batch: 2/2	Loss 0.7850 (0.7499)
+2022-11-18 14:55:56,360:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7049 (0.7049)
+2022-11-18 14:55:56,432:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7119 (0.7083)
+2022-11-18 14:55:56,506:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7156 (0.7107)
+2022-11-18 14:55:56,579:INFO: Dataset: zara2               Batch: 4/5	Loss 0.7085 (0.7102)
+2022-11-18 14:55:56,651:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6895 (0.7063)
+2022-11-18 14:55:56,701:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_233.pth.tar
+2022-11-18 14:55:56,702:INFO: 
+===> EPOCH: 234 (P2)
+2022-11-18 14:55:56,702:INFO: - Computing loss (training)
+2022-11-18 14:55:57,041:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8022 (0.8022)
+2022-11-18 14:55:57,189:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7740 (0.7874)
+2022-11-18 14:55:57,335:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7456 (0.7724)
+2022-11-18 14:55:57,448:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7914 (0.7754)
+2022-11-18 14:55:57,834:INFO: Dataset: univ                Batch:  1/15	Loss 0.7137 (0.7137)
+2022-11-18 14:55:57,986:INFO: Dataset: univ                Batch:  2/15	Loss 0.6996 (0.7068)
+2022-11-18 14:55:58,141:INFO: Dataset: univ                Batch:  3/15	Loss 0.7215 (0.7116)
+2022-11-18 14:55:58,295:INFO: Dataset: univ                Batch:  4/15	Loss 0.7059 (0.7100)
+2022-11-18 14:55:58,447:INFO: Dataset: univ                Batch:  5/15	Loss 0.7146 (0.7109)
+2022-11-18 14:55:58,607:INFO: Dataset: univ                Batch:  6/15	Loss 0.7082 (0.7105)
+2022-11-18 14:55:58,759:INFO: Dataset: univ                Batch:  7/15	Loss 0.7063 (0.7099)
+2022-11-18 14:55:58,912:INFO: Dataset: univ                Batch:  8/15	Loss 0.7279 (0.7121)
+2022-11-18 14:55:59,063:INFO: Dataset: univ                Batch:  9/15	Loss 0.7129 (0.7122)
+2022-11-18 14:55:59,213:INFO: Dataset: univ                Batch: 10/15	Loss 0.6845 (0.7095)
+2022-11-18 14:55:59,364:INFO: Dataset: univ                Batch: 11/15	Loss 0.7060 (0.7091)
+2022-11-18 14:55:59,516:INFO: Dataset: univ                Batch: 12/15	Loss 0.7025 (0.7086)
+2022-11-18 14:55:59,667:INFO: Dataset: univ                Batch: 13/15	Loss 0.7110 (0.7088)
+2022-11-18 14:55:59,820:INFO: Dataset: univ                Batch: 14/15	Loss 0.7090 (0.7088)
+2022-11-18 14:55:59,900:INFO: Dataset: univ                Batch: 15/15	Loss 0.7285 (0.7090)
+2022-11-18 14:56:00,284:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7727 (0.7727)
+2022-11-18 14:56:00,427:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7613 (0.7671)
+2022-11-18 14:56:00,577:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7932 (0.7762)
+2022-11-18 14:56:00,719:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7372 (0.7672)
+2022-11-18 14:56:00,866:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7475 (0.7631)
+2022-11-18 14:56:01,012:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7254 (0.7570)
+2022-11-18 14:56:01,158:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7688 (0.7586)
+2022-11-18 14:56:01,291:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7756 (0.7605)
+2022-11-18 14:56:01,680:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7165 (0.7165)
+2022-11-18 14:56:01,829:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7381 (0.7278)
+2022-11-18 14:56:01,978:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7107 (0.7223)
+2022-11-18 14:56:02,127:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7449 (0.7274)
+2022-11-18 14:56:02,280:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7242 (0.7268)
+2022-11-18 14:56:02,431:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7309 (0.7275)
+2022-11-18 14:56:02,580:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7267 (0.7274)
+2022-11-18 14:56:02,726:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7234 (0.7269)
+2022-11-18 14:56:02,875:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7060 (0.7245)
+2022-11-18 14:56:03,021:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7220 (0.7243)
+2022-11-18 14:56:03,169:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7203 (0.7239)
+2022-11-18 14:56:03,318:INFO: Dataset: zara2               Batch: 12/18	Loss 0.6851 (0.7209)
+2022-11-18 14:56:03,467:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7082 (0.7201)
+2022-11-18 14:56:03,615:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7609 (0.7230)
+2022-11-18 14:56:03,764:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7277 (0.7233)
+2022-11-18 14:56:03,913:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6964 (0.7217)
+2022-11-18 14:56:04,063:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7493 (0.7231)
+2022-11-18 14:56:04,199:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7071 (0.7224)
+2022-11-18 14:56:04,246:INFO: - Computing loss (validation)
+2022-11-18 14:56:04,505:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7805 (0.7805)
+2022-11-18 14:56:04,538:INFO: Dataset: hotel               Batch: 2/2	Loss 0.9116 (0.7867)
+2022-11-18 14:56:04,840:INFO: Dataset: univ                Batch: 1/3	Loss 0.7269 (0.7269)
+2022-11-18 14:56:04,920:INFO: Dataset: univ                Batch: 2/3	Loss 0.7020 (0.7140)
+2022-11-18 14:56:04,992:INFO: Dataset: univ                Batch: 3/3	Loss 0.7056 (0.7115)
+2022-11-18 14:56:05,292:INFO: Dataset: zara1               Batch: 1/2	Loss 0.7363 (0.7363)
+2022-11-18 14:56:05,337:INFO: Dataset: zara1               Batch: 2/2	Loss 0.7938 (0.7505)
+2022-11-18 14:56:05,662:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6893 (0.6893)
+2022-11-18 14:56:05,741:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7413 (0.7158)
+2022-11-18 14:56:05,819:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7276 (0.7196)
+2022-11-18 14:56:05,898:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6997 (0.7144)
+2022-11-18 14:56:05,975:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6990 (0.7114)
+2022-11-18 14:56:06,027:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_234.pth.tar
+2022-11-18 14:56:06,027:INFO: 
+===> EPOCH: 235 (P2)
+2022-11-18 14:56:06,028:INFO: - Computing loss (training)
+2022-11-18 14:56:06,364:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8005 (0.8005)
+2022-11-18 14:56:06,510:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8102 (0.8052)
+2022-11-18 14:56:06,654:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7741 (0.7940)
+2022-11-18 14:56:06,765:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7308 (0.7828)
+2022-11-18 14:56:07,157:INFO: Dataset: univ                Batch:  1/15	Loss 0.7210 (0.7210)
+2022-11-18 14:56:07,315:INFO: Dataset: univ                Batch:  2/15	Loss 0.7404 (0.7306)
+2022-11-18 14:56:07,473:INFO: Dataset: univ                Batch:  3/15	Loss 0.7383 (0.7328)
+2022-11-18 14:56:07,628:INFO: Dataset: univ                Batch:  4/15	Loss 0.7331 (0.7329)
+2022-11-18 14:56:07,787:INFO: Dataset: univ                Batch:  5/15	Loss 0.7305 (0.7323)
+2022-11-18 14:56:07,940:INFO: Dataset: univ                Batch:  6/15	Loss 0.7274 (0.7316)
+2022-11-18 14:56:08,093:INFO: Dataset: univ                Batch:  7/15	Loss 0.7181 (0.7297)
+2022-11-18 14:56:08,244:INFO: Dataset: univ                Batch:  8/15	Loss 0.7307 (0.7298)
+2022-11-18 14:56:08,397:INFO: Dataset: univ                Batch:  9/15	Loss 0.7235 (0.7291)
+2022-11-18 14:56:08,548:INFO: Dataset: univ                Batch: 10/15	Loss 0.7330 (0.7295)
+2022-11-18 14:56:08,704:INFO: Dataset: univ                Batch: 11/15	Loss 0.7163 (0.7283)
+2022-11-18 14:56:08,859:INFO: Dataset: univ                Batch: 12/15	Loss 0.7170 (0.7275)
+2022-11-18 14:56:09,013:INFO: Dataset: univ                Batch: 13/15	Loss 0.7306 (0.7277)
+2022-11-18 14:56:09,167:INFO: Dataset: univ                Batch: 14/15	Loss 0.7396 (0.7286)
+2022-11-18 14:56:09,248:INFO: Dataset: univ                Batch: 15/15	Loss 0.7464 (0.7289)
+2022-11-18 14:56:09,645:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7875 (0.7875)
+2022-11-18 14:56:09,792:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7713 (0.7799)
+2022-11-18 14:56:09,939:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7586 (0.7730)
+2022-11-18 14:56:10,083:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7439 (0.7664)
+2022-11-18 14:56:10,233:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7500 (0.7629)
+2022-11-18 14:56:10,393:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7442 (0.7594)
+2022-11-18 14:56:10,541:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7190 (0.7536)
+2022-11-18 14:56:10,673:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7355 (0.7515)
+2022-11-18 14:56:11,049:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7549 (0.7549)
+2022-11-18 14:56:11,194:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7467 (0.7508)
+2022-11-18 14:56:11,343:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7325 (0.7454)
+2022-11-18 14:56:11,490:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7558 (0.7484)
+2022-11-18 14:56:11,637:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7245 (0.7436)
+2022-11-18 14:56:11,784:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7653 (0.7472)
+2022-11-18 14:56:11,930:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7712 (0.7504)
+2022-11-18 14:56:12,073:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7206 (0.7468)
+2022-11-18 14:56:12,219:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7381 (0.7459)
+2022-11-18 14:56:12,364:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7719 (0.7482)
+2022-11-18 14:56:12,510:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7307 (0.7465)
+2022-11-18 14:56:12,655:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7104 (0.7434)
+2022-11-18 14:56:12,801:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7552 (0.7443)
+2022-11-18 14:56:12,947:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7563 (0.7451)
+2022-11-18 14:56:13,093:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7607 (0.7460)
+2022-11-18 14:56:13,238:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7497 (0.7463)
+2022-11-18 14:56:13,385:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7055 (0.7440)
+2022-11-18 14:56:13,518:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7157 (0.7426)
+2022-11-18 14:56:13,564:INFO: - Computing loss (validation)
+2022-11-18 14:56:13,841:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7732 (0.7732)
+2022-11-18 14:56:13,877:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7722 (0.7731)
+2022-11-18 14:56:14,184:INFO: Dataset: univ                Batch: 1/3	Loss 0.7128 (0.7128)
+2022-11-18 14:56:14,261:INFO: Dataset: univ                Batch: 2/3	Loss 0.7126 (0.7127)
+2022-11-18 14:56:14,332:INFO: Dataset: univ                Batch: 3/3	Loss 0.7263 (0.7166)
+2022-11-18 14:56:14,629:INFO: Dataset: zara1               Batch: 1/2	Loss 0.7540 (0.7540)
+2022-11-18 14:56:14,675:INFO: Dataset: zara1               Batch: 2/2	Loss 0.7313 (0.7483)
+2022-11-18 14:56:14,990:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7520 (0.7520)
+2022-11-18 14:56:15,069:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7479 (0.7499)
+2022-11-18 14:56:15,147:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7204 (0.7401)
+2022-11-18 14:56:15,226:INFO: Dataset: zara2               Batch: 4/5	Loss 0.7256 (0.7364)
+2022-11-18 14:56:15,303:INFO: Dataset: zara2               Batch: 5/5	Loss 0.7350 (0.7361)
+2022-11-18 14:56:15,354:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_235.pth.tar
+2022-11-18 14:56:15,354:INFO: 
+===> EPOCH: 236 (P2)
+2022-11-18 14:56:15,355:INFO: - Computing loss (training)
+2022-11-18 14:56:15,694:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7648 (0.7648)
+2022-11-18 14:56:15,851:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8165 (0.7909)
+2022-11-18 14:56:16,001:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7741 (0.7851)
+2022-11-18 14:56:16,114:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8065 (0.7890)
+2022-11-18 14:56:16,503:INFO: Dataset: univ                Batch:  1/15	Loss 0.7409 (0.7409)
+2022-11-18 14:56:16,659:INFO: Dataset: univ                Batch:  2/15	Loss 0.7538 (0.7470)
+2022-11-18 14:56:16,811:INFO: Dataset: univ                Batch:  3/15	Loss 0.7512 (0.7484)
+2022-11-18 14:56:16,962:INFO: Dataset: univ                Batch:  4/15	Loss 0.7533 (0.7497)
+2022-11-18 14:56:17,113:INFO: Dataset: univ                Batch:  5/15	Loss 0.7637 (0.7522)
+2022-11-18 14:56:17,269:INFO: Dataset: univ                Batch:  6/15	Loss 0.7454 (0.7510)
+2022-11-18 14:56:17,420:INFO: Dataset: univ                Batch:  7/15	Loss 0.7515 (0.7511)
+2022-11-18 14:56:17,570:INFO: Dataset: univ                Batch:  8/15	Loss 0.7576 (0.7519)
+2022-11-18 14:56:17,722:INFO: Dataset: univ                Batch:  9/15	Loss 0.7608 (0.7530)
+2022-11-18 14:56:17,872:INFO: Dataset: univ                Batch: 10/15	Loss 0.7425 (0.7519)
+2022-11-18 14:56:18,022:INFO: Dataset: univ                Batch: 11/15	Loss 0.7600 (0.7526)
+2022-11-18 14:56:18,173:INFO: Dataset: univ                Batch: 12/15	Loss 0.7489 (0.7523)
+2022-11-18 14:56:18,326:INFO: Dataset: univ                Batch: 13/15	Loss 0.7397 (0.7513)
+2022-11-18 14:56:18,477:INFO: Dataset: univ                Batch: 14/15	Loss 0.7482 (0.7511)
+2022-11-18 14:56:18,557:INFO: Dataset: univ                Batch: 15/15	Loss 0.7849 (0.7515)
+2022-11-18 14:56:18,933:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7425 (0.7425)
+2022-11-18 14:56:19,082:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7502 (0.7463)
+2022-11-18 14:56:19,230:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7759 (0.7558)
+2022-11-18 14:56:19,379:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7816 (0.7628)
+2022-11-18 14:56:19,526:INFO: Dataset: zara1               Batch: 5/8	Loss 0.7514 (0.7607)
+2022-11-18 14:56:19,672:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7656 (0.7615)
+2022-11-18 14:56:19,819:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7708 (0.7629)
+2022-11-18 14:56:19,953:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7870 (0.7653)
+2022-11-18 14:56:20,338:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7485 (0.7485)
+2022-11-18 14:56:20,490:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7789 (0.7630)
+2022-11-18 14:56:20,642:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7101 (0.7457)
+2022-11-18 14:56:20,787:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7618 (0.7502)
+2022-11-18 14:56:20,936:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7026 (0.7400)
+2022-11-18 14:56:21,083:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7428 (0.7405)
+2022-11-18 14:56:21,231:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7557 (0.7426)
+2022-11-18 14:56:21,376:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7959 (0.7498)
+2022-11-18 14:56:21,522:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7494 (0.7497)
+2022-11-18 14:56:21,666:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7378 (0.7485)
+2022-11-18 14:56:21,814:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7362 (0.7474)
+2022-11-18 14:56:21,960:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7494 (0.7475)
+2022-11-18 14:56:22,106:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7175 (0.7452)
+2022-11-18 14:56:22,253:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7755 (0.7475)
+2022-11-18 14:56:22,401:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7474 (0.7475)
+2022-11-18 14:56:22,546:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7590 (0.7482)
+2022-11-18 14:56:22,694:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7491 (0.7483)
+2022-11-18 14:56:22,831:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7427 (0.7480)
+2022-11-18 14:56:22,877:INFO: - Computing loss (validation)
+2022-11-18 14:56:23,133:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7815 (0.7815)
+2022-11-18 14:56:23,166:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7613 (0.7804)
+2022-11-18 14:56:23,471:INFO: Dataset: univ                Batch: 1/3	Loss 0.7431 (0.7431)
+2022-11-18 14:56:23,548:INFO: Dataset: univ                Batch: 2/3	Loss 0.7564 (0.7503)
+2022-11-18 14:56:23,620:INFO: Dataset: univ                Batch: 3/3	Loss 0.7364 (0.7457)
+2022-11-18 14:56:23,919:INFO: Dataset: zara1               Batch: 1/2	Loss 0.7507 (0.7507)
+2022-11-18 14:56:23,963:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6720 (0.7322)
+2022-11-18 14:56:24,262:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7536 (0.7536)
+2022-11-18 14:56:24,337:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7361 (0.7456)
+2022-11-18 14:56:24,411:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7583 (0.7498)
+2022-11-18 14:56:24,484:INFO: Dataset: zara2               Batch: 4/5	Loss 0.7506 (0.7500)
+2022-11-18 14:56:24,557:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6967 (0.7396)
+2022-11-18 14:56:24,607:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_236.pth.tar
+2022-11-18 14:56:24,607:INFO: 
+===> EPOCH: 237 (P2)
+2022-11-18 14:56:24,608:INFO: - Computing loss (training)
+2022-11-18 14:56:24,942:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7754 (0.7754)
+2022-11-18 14:56:25,089:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8275 (0.8012)
+2022-11-18 14:56:25,237:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7903 (0.7977)
+2022-11-18 14:56:25,349:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8376 (0.8048)
+2022-11-18 14:56:25,793:INFO: Dataset: univ                Batch:  1/15	Loss 0.7776 (0.7776)
+2022-11-18 14:56:25,949:INFO: Dataset: univ                Batch:  2/15	Loss 0.7491 (0.7635)
+2022-11-18 14:56:26,103:INFO: Dataset: univ                Batch:  3/15	Loss 0.7648 (0.7639)
+2022-11-18 14:56:26,255:INFO: Dataset: univ                Batch:  4/15	Loss 0.7686 (0.7651)
+2022-11-18 14:56:26,409:INFO: Dataset: univ                Batch:  5/15	Loss 0.7671 (0.7654)
+2022-11-18 14:56:26,565:INFO: Dataset: univ                Batch:  6/15	Loss 0.7582 (0.7642)
+2022-11-18 14:56:26,720:INFO: Dataset: univ                Batch:  7/15	Loss 0.7727 (0.7655)
+2022-11-18 14:56:26,951:INFO: Dataset: univ                Batch:  8/15	Loss 0.7569 (0.7644)
+2022-11-18 14:56:27,105:INFO: Dataset: univ                Batch:  9/15	Loss 0.7691 (0.7650)
+2022-11-18 14:56:27,260:INFO: Dataset: univ                Batch: 10/15	Loss 0.7651 (0.7650)
+2022-11-18 14:56:27,415:INFO: Dataset: univ                Batch: 11/15	Loss 0.7616 (0.7647)
+2022-11-18 14:56:27,572:INFO: Dataset: univ                Batch: 12/15	Loss 0.7485 (0.7633)
+2022-11-18 14:56:27,726:INFO: Dataset: univ                Batch: 13/15	Loss 0.7558 (0.7628)
+2022-11-18 14:56:27,882:INFO: Dataset: univ                Batch: 14/15	Loss 0.7647 (0.7629)
+2022-11-18 14:56:27,963:INFO: Dataset: univ                Batch: 15/15	Loss 0.7764 (0.7631)
+2022-11-18 14:56:28,345:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8274 (0.8274)
+2022-11-18 14:56:28,492:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8288 (0.8281)
+2022-11-18 14:56:28,637:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7984 (0.8175)
+2022-11-18 14:56:28,779:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7886 (0.8101)
+2022-11-18 14:56:28,924:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8077 (0.8096)
+2022-11-18 14:56:29,070:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7765 (0.8030)
+2022-11-18 14:56:29,215:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7633 (0.7978)
+2022-11-18 14:56:29,347:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8008 (0.7981)
+2022-11-18 14:56:29,739:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7492 (0.7492)
+2022-11-18 14:56:29,885:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7727 (0.7610)
+2022-11-18 14:56:30,034:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7534 (0.7586)
+2022-11-18 14:56:30,182:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8003 (0.7686)
+2022-11-18 14:56:30,328:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8103 (0.7770)
+2022-11-18 14:56:30,475:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7918 (0.7794)
+2022-11-18 14:56:30,621:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7911 (0.7809)
+2022-11-18 14:56:30,764:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7735 (0.7800)
+2022-11-18 14:56:30,909:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7640 (0.7781)
+2022-11-18 14:56:31,052:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7880 (0.7791)
+2022-11-18 14:56:31,200:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7585 (0.7772)
+2022-11-18 14:56:31,347:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7873 (0.7781)
+2022-11-18 14:56:31,492:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7722 (0.7777)
+2022-11-18 14:56:31,636:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7924 (0.7786)
+2022-11-18 14:56:31,782:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7618 (0.7774)
+2022-11-18 14:56:31,928:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8050 (0.7790)
+2022-11-18 14:56:32,074:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7542 (0.7775)
+2022-11-18 14:56:32,208:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7716 (0.7772)
+2022-11-18 14:56:32,253:INFO: - Computing loss (validation)
+2022-11-18 14:56:32,515:INFO: Dataset: hotel               Batch: 1/2	Loss 0.8063 (0.8063)
+2022-11-18 14:56:32,549:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8147 (0.8068)
+2022-11-18 14:56:32,854:INFO: Dataset: univ                Batch: 1/3	Loss 0.7637 (0.7637)
+2022-11-18 14:56:32,936:INFO: Dataset: univ                Batch: 2/3	Loss 0.7608 (0.7623)
+2022-11-18 14:56:33,009:INFO: Dataset: univ                Batch: 3/3	Loss 0.7416 (0.7573)
+2022-11-18 14:56:33,326:INFO: Dataset: zara1               Batch: 1/2	Loss 0.7697 (0.7697)
+2022-11-18 14:56:33,372:INFO: Dataset: zara1               Batch: 2/2	Loss 0.7644 (0.7684)
+2022-11-18 14:56:33,684:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7870 (0.7870)
+2022-11-18 14:56:33,757:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7714 (0.7788)
+2022-11-18 14:56:33,830:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7231 (0.7591)
+2022-11-18 14:56:33,903:INFO: Dataset: zara2               Batch: 4/5	Loss 0.7585 (0.7589)
+2022-11-18 14:56:33,976:INFO: Dataset: zara2               Batch: 5/5	Loss 0.7343 (0.7544)
+2022-11-18 14:56:34,033:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_237.pth.tar
+2022-11-18 14:56:34,033:INFO: 
+===> EPOCH: 238 (P2)
+2022-11-18 14:56:34,033:INFO: - Computing loss (training)
+2022-11-18 14:56:34,360:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8071 (0.8071)
+2022-11-18 14:56:34,509:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8749 (0.8420)
+2022-11-18 14:56:34,656:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8219 (0.8353)
+2022-11-18 14:56:34,770:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8530 (0.8385)
+2022-11-18 14:56:35,160:INFO: Dataset: univ                Batch:  1/15	Loss 0.7798 (0.7798)
+2022-11-18 14:56:35,318:INFO: Dataset: univ                Batch:  2/15	Loss 0.7912 (0.7855)
+2022-11-18 14:56:35,474:INFO: Dataset: univ                Batch:  3/15	Loss 0.7895 (0.7867)
+2022-11-18 14:56:35,631:INFO: Dataset: univ                Batch:  4/15	Loss 0.7800 (0.7850)
+2022-11-18 14:56:35,783:INFO: Dataset: univ                Batch:  5/15	Loss 0.7829 (0.7845)
+2022-11-18 14:56:35,939:INFO: Dataset: univ                Batch:  6/15	Loss 0.7818 (0.7840)
+2022-11-18 14:56:36,092:INFO: Dataset: univ                Batch:  7/15	Loss 0.7933 (0.7854)
+2022-11-18 14:56:36,242:INFO: Dataset: univ                Batch:  8/15	Loss 0.7848 (0.7853)
+2022-11-18 14:56:36,394:INFO: Dataset: univ                Batch:  9/15	Loss 0.7731 (0.7839)
+2022-11-18 14:56:36,545:INFO: Dataset: univ                Batch: 10/15	Loss 0.7706 (0.7825)
+2022-11-18 14:56:36,697:INFO: Dataset: univ                Batch: 11/15	Loss 0.7634 (0.7808)
+2022-11-18 14:56:36,850:INFO: Dataset: univ                Batch: 12/15	Loss 0.7919 (0.7816)
+2022-11-18 14:56:37,017:INFO: Dataset: univ                Batch: 13/15	Loss 0.7809 (0.7816)
+2022-11-18 14:56:37,171:INFO: Dataset: univ                Batch: 14/15	Loss 0.7570 (0.7798)
+2022-11-18 14:56:37,253:INFO: Dataset: univ                Batch: 15/15	Loss 0.8203 (0.7804)
+2022-11-18 14:56:37,650:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8547 (0.8547)
+2022-11-18 14:56:37,799:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8049 (0.8311)
+2022-11-18 14:56:37,949:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7914 (0.8176)
+2022-11-18 14:56:38,095:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8076 (0.8148)
+2022-11-18 14:56:38,244:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8283 (0.8174)
+2022-11-18 14:56:38,396:INFO: Dataset: zara1               Batch: 6/8	Loss 0.7872 (0.8127)
+2022-11-18 14:56:38,574:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8107 (0.8124)
+2022-11-18 14:56:38,732:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8079 (0.8119)
+2022-11-18 14:56:39,207:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7912 (0.7912)
+2022-11-18 14:56:39,396:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7685 (0.7802)
+2022-11-18 14:56:39,558:INFO: Dataset: zara2               Batch:  3/18	Loss 0.7962 (0.7856)
+2022-11-18 14:56:39,708:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8280 (0.7964)
+2022-11-18 14:56:39,863:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7941 (0.7960)
+2022-11-18 14:56:40,023:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8257 (0.8011)
+2022-11-18 14:56:40,194:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8024 (0.8013)
+2022-11-18 14:56:40,364:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8348 (0.8051)
+2022-11-18 14:56:40,516:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7623 (0.8008)
+2022-11-18 14:56:40,662:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7797 (0.7986)
+2022-11-18 14:56:40,826:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7993 (0.7987)
+2022-11-18 14:56:40,991:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8247 (0.8009)
+2022-11-18 14:56:41,160:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8025 (0.8010)
+2022-11-18 14:56:41,309:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7893 (0.8000)
+2022-11-18 14:56:41,457:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7852 (0.7990)
+2022-11-18 14:56:41,603:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7992 (0.7990)
+2022-11-18 14:56:41,751:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7543 (0.7966)
+2022-11-18 14:56:41,887:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7797 (0.7957)
+2022-11-18 14:56:41,930:INFO: - Computing loss (validation)
+2022-11-18 14:56:42,195:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7936 (0.7936)
+2022-11-18 14:56:42,230:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8892 (0.8024)
+2022-11-18 14:56:42,548:INFO: Dataset: univ                Batch: 1/3	Loss 0.7545 (0.7545)
+2022-11-18 14:56:42,624:INFO: Dataset: univ                Batch: 2/3	Loss 0.7846 (0.7699)
+2022-11-18 14:56:42,695:INFO: Dataset: univ                Batch: 3/3	Loss 0.8046 (0.7810)
+2022-11-18 14:56:42,997:INFO: Dataset: zara1               Batch: 1/2	Loss 0.8174 (0.8174)
+2022-11-18 14:56:43,043:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8375 (0.8226)
+2022-11-18 14:56:43,344:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7883 (0.7883)
+2022-11-18 14:56:43,421:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7877 (0.7880)
+2022-11-18 14:56:43,497:INFO: Dataset: zara2               Batch: 3/5	Loss 0.8265 (0.8007)
+2022-11-18 14:56:43,571:INFO: Dataset: zara2               Batch: 4/5	Loss 0.7741 (0.7944)
+2022-11-18 14:56:43,646:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8092 (0.7971)
+2022-11-18 14:56:43,696:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_238.pth.tar
+2022-11-18 14:56:43,696:INFO: 
+===> EPOCH: 239 (P2)
+2022-11-18 14:56:43,697:INFO: - Computing loss (training)
+2022-11-18 14:56:44,039:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8636 (0.8636)
+2022-11-18 14:56:44,190:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8414 (0.8529)
+2022-11-18 14:56:44,349:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8278 (0.8446)
+2022-11-18 14:56:44,464:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8765 (0.8499)
+2022-11-18 14:56:44,854:INFO: Dataset: univ                Batch:  1/15	Loss 0.7796 (0.7796)
+2022-11-18 14:56:45,007:INFO: Dataset: univ                Batch:  2/15	Loss 0.8124 (0.7947)
+2022-11-18 14:56:45,164:INFO: Dataset: univ                Batch:  3/15	Loss 0.7925 (0.7940)
+2022-11-18 14:56:45,313:INFO: Dataset: univ                Batch:  4/15	Loss 0.7930 (0.7937)
+2022-11-18 14:56:45,463:INFO: Dataset: univ                Batch:  5/15	Loss 0.8062 (0.7960)
+2022-11-18 14:56:45,618:INFO: Dataset: univ                Batch:  6/15	Loss 0.7902 (0.7949)
+2022-11-18 14:56:45,770:INFO: Dataset: univ                Batch:  7/15	Loss 0.7902 (0.7942)
+2022-11-18 14:56:45,922:INFO: Dataset: univ                Batch:  8/15	Loss 0.7935 (0.7941)
+2022-11-18 14:56:46,073:INFO: Dataset: univ                Batch:  9/15	Loss 0.8042 (0.7952)
+2022-11-18 14:56:46,227:INFO: Dataset: univ                Batch: 10/15	Loss 0.8065 (0.7964)
+2022-11-18 14:56:46,394:INFO: Dataset: univ                Batch: 11/15	Loss 0.7926 (0.7960)
+2022-11-18 14:56:46,545:INFO: Dataset: univ                Batch: 12/15	Loss 0.7951 (0.7959)
+2022-11-18 14:56:46,703:INFO: Dataset: univ                Batch: 13/15	Loss 0.8004 (0.7963)
+2022-11-18 14:56:46,870:INFO: Dataset: univ                Batch: 14/15	Loss 0.8022 (0.7967)
+2022-11-18 14:56:46,954:INFO: Dataset: univ                Batch: 15/15	Loss 0.7708 (0.7964)
+2022-11-18 14:56:47,338:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8126 (0.8126)
+2022-11-18 14:56:47,489:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8617 (0.8362)
+2022-11-18 14:56:47,640:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7710 (0.8139)
+2022-11-18 14:56:47,791:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8274 (0.8172)
+2022-11-18 14:56:47,947:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8323 (0.8206)
+2022-11-18 14:56:48,104:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8430 (0.8243)
+2022-11-18 14:56:48,260:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8035 (0.8213)
+2022-11-18 14:56:48,397:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8420 (0.8237)
+2022-11-18 14:56:48,848:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8340 (0.8340)
+2022-11-18 14:56:49,026:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7666 (0.8004)
+2022-11-18 14:56:49,215:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8081 (0.8028)
+2022-11-18 14:56:49,402:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8205 (0.8069)
+2022-11-18 14:56:49,559:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8250 (0.8104)
+2022-11-18 14:56:49,712:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8312 (0.8139)
+2022-11-18 14:56:49,861:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7845 (0.8100)
+2022-11-18 14:56:50,009:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8179 (0.8110)
+2022-11-18 14:56:50,174:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8195 (0.8120)
+2022-11-18 14:56:50,343:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8209 (0.8129)
+2022-11-18 14:56:50,522:INFO: Dataset: zara2               Batch: 11/18	Loss 0.7932 (0.8112)
+2022-11-18 14:56:50,707:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7833 (0.8087)
+2022-11-18 14:56:50,877:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8125 (0.8090)
+2022-11-18 14:56:51,048:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8333 (0.8108)
+2022-11-18 14:56:51,213:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8032 (0.8102)
+2022-11-18 14:56:51,370:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8219 (0.8110)
+2022-11-18 14:56:51,529:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7864 (0.8096)
+2022-11-18 14:56:51,667:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8038 (0.8093)
+2022-11-18 14:56:51,718:INFO: - Computing loss (validation)
+2022-11-18 14:56:51,977:INFO: Dataset: hotel               Batch: 1/2	Loss 0.8670 (0.8670)
+2022-11-18 14:56:52,011:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7502 (0.8619)
+2022-11-18 14:56:52,324:INFO: Dataset: univ                Batch: 1/3	Loss 0.8159 (0.8159)
+2022-11-18 14:56:52,401:INFO: Dataset: univ                Batch: 2/3	Loss 0.8013 (0.8085)
+2022-11-18 14:56:52,476:INFO: Dataset: univ                Batch: 3/3	Loss 0.7913 (0.8029)
+2022-11-18 14:56:52,782:INFO: Dataset: zara1               Batch: 1/2	Loss 0.8423 (0.8423)
+2022-11-18 14:56:52,828:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8033 (0.8338)
+2022-11-18 14:56:53,143:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7915 (0.7915)
+2022-11-18 14:56:53,228:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8236 (0.8081)
+2022-11-18 14:56:53,321:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7795 (0.7989)
+2022-11-18 14:56:53,402:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8158 (0.8028)
+2022-11-18 14:56:53,477:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8007 (0.8024)
+2022-11-18 14:56:53,530:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_239.pth.tar
+2022-11-18 14:56:53,531:INFO: 
+===> EPOCH: 240 (P2)
+2022-11-18 14:56:53,531:INFO: - Computing loss (training)
+2022-11-18 14:56:53,886:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8368 (0.8368)
+2022-11-18 14:56:54,049:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8394 (0.8380)
+2022-11-18 14:56:54,201:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8409 (0.8390)
+2022-11-18 14:56:54,317:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8648 (0.8435)
+2022-11-18 14:56:54,761:INFO: Dataset: univ                Batch:  1/15	Loss 0.8193 (0.8193)
+2022-11-18 14:56:54,939:INFO: Dataset: univ                Batch:  2/15	Loss 0.8313 (0.8255)
+2022-11-18 14:56:55,116:INFO: Dataset: univ                Batch:  3/15	Loss 0.7995 (0.8165)
+2022-11-18 14:56:55,293:INFO: Dataset: univ                Batch:  4/15	Loss 0.8237 (0.8183)
+2022-11-18 14:56:55,460:INFO: Dataset: univ                Batch:  5/15	Loss 0.8169 (0.8180)
+2022-11-18 14:56:55,625:INFO: Dataset: univ                Batch:  6/15	Loss 0.8042 (0.8155)
+2022-11-18 14:56:55,786:INFO: Dataset: univ                Batch:  7/15	Loss 0.8341 (0.8181)
+2022-11-18 14:56:55,950:INFO: Dataset: univ                Batch:  8/15	Loss 0.8219 (0.8186)
+2022-11-18 14:56:56,109:INFO: Dataset: univ                Batch:  9/15	Loss 0.7922 (0.8158)
+2022-11-18 14:56:56,270:INFO: Dataset: univ                Batch: 10/15	Loss 0.8218 (0.8163)
+2022-11-18 14:56:56,461:INFO: Dataset: univ                Batch: 11/15	Loss 0.8122 (0.8159)
+2022-11-18 14:56:56,645:INFO: Dataset: univ                Batch: 12/15	Loss 0.8107 (0.8155)
+2022-11-18 14:56:56,816:INFO: Dataset: univ                Batch: 13/15	Loss 0.8236 (0.8161)
+2022-11-18 14:56:57,012:INFO: Dataset: univ                Batch: 14/15	Loss 0.8278 (0.8169)
+2022-11-18 14:56:57,107:INFO: Dataset: univ                Batch: 15/15	Loss 0.7938 (0.8166)
+2022-11-18 14:56:57,503:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8719 (0.8719)
+2022-11-18 14:56:57,655:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8433 (0.8568)
+2022-11-18 14:56:57,813:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8054 (0.8393)
+2022-11-18 14:56:57,969:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8559 (0.8432)
+2022-11-18 14:56:58,142:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8238 (0.8393)
+2022-11-18 14:56:58,336:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8655 (0.8439)
+2022-11-18 14:56:58,504:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8395 (0.8433)
+2022-11-18 14:56:58,662:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8491 (0.8439)
+2022-11-18 14:56:59,127:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8001 (0.8001)
+2022-11-18 14:56:59,284:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8356 (0.8175)
+2022-11-18 14:56:59,471:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8142 (0.8164)
+2022-11-18 14:56:59,650:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8242 (0.8182)
+2022-11-18 14:56:59,808:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8175 (0.8181)
+2022-11-18 14:56:59,975:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8128 (0.8172)
+2022-11-18 14:57:00,143:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8070 (0.8159)
+2022-11-18 14:57:00,308:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8510 (0.8204)
+2022-11-18 14:57:00,471:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8409 (0.8228)
+2022-11-18 14:57:00,638:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8684 (0.8271)
+2022-11-18 14:57:00,813:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8137 (0.8260)
+2022-11-18 14:57:00,981:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8433 (0.8274)
+2022-11-18 14:57:01,134:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8472 (0.8290)
+2022-11-18 14:57:01,303:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8618 (0.8311)
+2022-11-18 14:57:01,466:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8364 (0.8315)
+2022-11-18 14:57:01,622:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8075 (0.8301)
+2022-11-18 14:57:01,788:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7854 (0.8277)
+2022-11-18 14:57:01,940:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8392 (0.8283)
+2022-11-18 14:57:01,991:INFO: - Computing loss (validation)
+2022-11-18 14:57:02,261:INFO: Dataset: hotel               Batch: 1/2	Loss 0.8708 (0.8708)
+2022-11-18 14:57:02,294:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7501 (0.8613)
+2022-11-18 14:57:02,610:INFO: Dataset: univ                Batch: 1/3	Loss 0.8286 (0.8286)
+2022-11-18 14:57:02,685:INFO: Dataset: univ                Batch: 2/3	Loss 0.7845 (0.8073)
+2022-11-18 14:57:02,758:INFO: Dataset: univ                Batch: 3/3	Loss 0.8211 (0.8120)
+2022-11-18 14:57:03,056:INFO: Dataset: zara1               Batch: 1/2	Loss 0.8050 (0.8050)
+2022-11-18 14:57:03,104:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8696 (0.8225)
+2022-11-18 14:57:03,416:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7896 (0.7896)
+2022-11-18 14:57:03,491:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8278 (0.8084)
+2022-11-18 14:57:03,564:INFO: Dataset: zara2               Batch: 3/5	Loss 0.8274 (0.8147)
+2022-11-18 14:57:03,640:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8434 (0.8221)
+2022-11-18 14:57:03,719:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8551 (0.8287)
+2022-11-18 14:57:03,770:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_240.pth.tar
+2022-11-18 14:57:03,770:INFO: 
+===> EPOCH: 241 (P2)
+2022-11-18 14:57:03,771:INFO: - Computing loss (training)
+2022-11-18 14:57:04,115:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9324 (0.9324)
+2022-11-18 14:57:04,278:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8690 (0.9014)
+2022-11-18 14:57:04,433:INFO: Dataset: hotel               Batch: 3/4	Loss 0.8345 (0.8782)
+2022-11-18 14:57:04,553:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8344 (0.8700)
+2022-11-18 14:57:04,986:INFO: Dataset: univ                Batch:  1/15	Loss 0.8393 (0.8393)
+2022-11-18 14:57:05,157:INFO: Dataset: univ                Batch:  2/15	Loss 0.8400 (0.8396)
+2022-11-18 14:57:05,353:INFO: Dataset: univ                Batch:  3/15	Loss 0.8144 (0.8313)
+2022-11-18 14:57:05,545:INFO: Dataset: univ                Batch:  4/15	Loss 0.8339 (0.8319)
+2022-11-18 14:57:05,749:INFO: Dataset: univ                Batch:  5/15	Loss 0.8313 (0.8318)
+2022-11-18 14:57:05,962:INFO: Dataset: univ                Batch:  6/15	Loss 0.8320 (0.8318)
+2022-11-18 14:57:06,131:INFO: Dataset: univ                Batch:  7/15	Loss 0.8482 (0.8341)
+2022-11-18 14:57:06,291:INFO: Dataset: univ                Batch:  8/15	Loss 0.8441 (0.8353)
+2022-11-18 14:57:06,448:INFO: Dataset: univ                Batch:  9/15	Loss 0.8371 (0.8355)
+2022-11-18 14:57:06,604:INFO: Dataset: univ                Batch: 10/15	Loss 0.8424 (0.8362)
+2022-11-18 14:57:06,778:INFO: Dataset: univ                Batch: 11/15	Loss 0.8357 (0.8362)
+2022-11-18 14:57:06,980:INFO: Dataset: univ                Batch: 12/15	Loss 0.8372 (0.8363)
+2022-11-18 14:57:07,163:INFO: Dataset: univ                Batch: 13/15	Loss 0.8275 (0.8356)
+2022-11-18 14:57:07,337:INFO: Dataset: univ                Batch: 14/15	Loss 0.8386 (0.8358)
+2022-11-18 14:57:07,425:INFO: Dataset: univ                Batch: 15/15	Loss 0.8443 (0.8359)
+2022-11-18 14:57:07,820:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8212 (0.8212)
+2022-11-18 14:57:07,993:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8609 (0.8394)
+2022-11-18 14:57:08,192:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8680 (0.8483)
+2022-11-18 14:57:08,366:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8649 (0.8524)
+2022-11-18 14:57:08,529:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8624 (0.8544)
+2022-11-18 14:57:08,692:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8804 (0.8587)
+2022-11-18 14:57:08,851:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8930 (0.8632)
+2022-11-18 14:57:09,010:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8001 (0.8549)
+2022-11-18 14:57:09,420:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8187 (0.8187)
+2022-11-18 14:57:09,573:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8048 (0.8118)
+2022-11-18 14:57:09,726:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8596 (0.8269)
+2022-11-18 14:57:09,880:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7958 (0.8190)
+2022-11-18 14:57:10,043:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8462 (0.8247)
+2022-11-18 14:57:10,204:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8503 (0.8291)
+2022-11-18 14:57:10,369:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8569 (0.8333)
+2022-11-18 14:57:10,522:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8354 (0.8336)
+2022-11-18 14:57:10,672:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8411 (0.8343)
+2022-11-18 14:57:10,822:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8270 (0.8336)
+2022-11-18 14:57:10,976:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8263 (0.8329)
+2022-11-18 14:57:11,121:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8402 (0.8335)
+2022-11-18 14:57:11,288:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8668 (0.8361)
+2022-11-18 14:57:11,463:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8369 (0.8362)
+2022-11-18 14:57:11,631:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8874 (0.8395)
+2022-11-18 14:57:11,795:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8342 (0.8391)
+2022-11-18 14:57:11,956:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8228 (0.8381)
+2022-11-18 14:57:12,103:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8696 (0.8395)
+2022-11-18 14:57:12,151:INFO: - Computing loss (validation)
+2022-11-18 14:57:12,415:INFO: Dataset: hotel               Batch: 1/2	Loss 0.8824 (0.8824)
+2022-11-18 14:57:12,451:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7971 (0.8751)
+2022-11-18 14:57:12,756:INFO: Dataset: univ                Batch: 1/3	Loss 0.8268 (0.8268)
+2022-11-18 14:57:12,833:INFO: Dataset: univ                Batch: 2/3	Loss 0.8314 (0.8293)
+2022-11-18 14:57:12,905:INFO: Dataset: univ                Batch: 3/3	Loss 0.8509 (0.8364)
+2022-11-18 14:57:13,201:INFO: Dataset: zara1               Batch: 1/2	Loss 0.8916 (0.8916)
+2022-11-18 14:57:13,246:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8087 (0.8740)
+2022-11-18 14:57:13,568:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8487 (0.8487)
+2022-11-18 14:57:13,658:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8196 (0.8354)
+2022-11-18 14:57:13,743:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7990 (0.8232)
+2022-11-18 14:57:13,829:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8459 (0.8288)
+2022-11-18 14:57:13,909:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8531 (0.8332)
+2022-11-18 14:57:13,962:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_241.pth.tar
+2022-11-18 14:57:13,962:INFO: 
+===> EPOCH: 242 (P2)
+2022-11-18 14:57:13,963:INFO: - Computing loss (training)
+2022-11-18 14:57:14,316:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9328 (0.9328)
+2022-11-18 14:57:14,468:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8838 (0.9081)
+2022-11-18 14:57:14,618:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9233 (0.9135)
+2022-11-18 14:57:14,737:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8939 (0.9102)
+2022-11-18 14:57:15,231:INFO: Dataset: univ                Batch:  1/15	Loss 0.8507 (0.8507)
+2022-11-18 14:57:15,388:INFO: Dataset: univ                Batch:  2/15	Loss 0.8405 (0.8456)
+2022-11-18 14:57:15,558:INFO: Dataset: univ                Batch:  3/15	Loss 0.8571 (0.8492)
+2022-11-18 14:57:15,759:INFO: Dataset: univ                Batch:  4/15	Loss 0.8407 (0.8471)
+2022-11-18 14:57:15,930:INFO: Dataset: univ                Batch:  5/15	Loss 0.8485 (0.8474)
+2022-11-18 14:57:16,104:INFO: Dataset: univ                Batch:  6/15	Loss 0.8490 (0.8476)
+2022-11-18 14:57:16,272:INFO: Dataset: univ                Batch:  7/15	Loss 0.8445 (0.8472)
+2022-11-18 14:57:16,440:INFO: Dataset: univ                Batch:  8/15	Loss 0.8517 (0.8478)
+2022-11-18 14:57:16,608:INFO: Dataset: univ                Batch:  9/15	Loss 0.8530 (0.8485)
+2022-11-18 14:57:16,773:INFO: Dataset: univ                Batch: 10/15	Loss 0.8484 (0.8485)
+2022-11-18 14:57:16,936:INFO: Dataset: univ                Batch: 11/15	Loss 0.8450 (0.8482)
+2022-11-18 14:57:17,105:INFO: Dataset: univ                Batch: 12/15	Loss 0.8371 (0.8473)
+2022-11-18 14:57:17,271:INFO: Dataset: univ                Batch: 13/15	Loss 0.8464 (0.8472)
+2022-11-18 14:57:17,438:INFO: Dataset: univ                Batch: 14/15	Loss 0.8661 (0.8486)
+2022-11-18 14:57:17,527:INFO: Dataset: univ                Batch: 15/15	Loss 0.8341 (0.8484)
+2022-11-18 14:57:17,942:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8742 (0.8742)
+2022-11-18 14:57:18,104:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9174 (0.8969)
+2022-11-18 14:57:18,263:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8354 (0.8766)
+2022-11-18 14:57:18,418:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8891 (0.8796)
+2022-11-18 14:57:18,578:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8805 (0.8798)
+2022-11-18 14:57:18,737:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8751 (0.8790)
+2022-11-18 14:57:18,907:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9162 (0.8842)
+2022-11-18 14:57:19,054:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8675 (0.8821)
+2022-11-18 14:57:19,462:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8271 (0.8271)
+2022-11-18 14:57:19,620:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8644 (0.8461)
+2022-11-18 14:57:19,779:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8442 (0.8455)
+2022-11-18 14:57:19,942:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8757 (0.8530)
+2022-11-18 14:57:20,100:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8483 (0.8521)
+2022-11-18 14:57:20,261:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8241 (0.8475)
+2022-11-18 14:57:20,417:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8557 (0.8488)
+2022-11-18 14:57:20,575:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8750 (0.8519)
+2022-11-18 14:57:20,738:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8515 (0.8519)
+2022-11-18 14:57:20,888:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8675 (0.8535)
+2022-11-18 14:57:21,039:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8507 (0.8532)
+2022-11-18 14:57:21,188:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8443 (0.8525)
+2022-11-18 14:57:21,336:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8693 (0.8538)
+2022-11-18 14:57:21,484:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8755 (0.8551)
+2022-11-18 14:57:21,634:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8919 (0.8577)
+2022-11-18 14:57:21,806:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8678 (0.8583)
+2022-11-18 14:57:21,974:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8444 (0.8575)
+2022-11-18 14:57:22,124:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8385 (0.8566)
+2022-11-18 14:57:22,167:INFO: - Computing loss (validation)
+2022-11-18 14:57:22,427:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9351 (0.9351)
+2022-11-18 14:57:22,462:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0109 (0.9411)
+2022-11-18 14:57:22,772:INFO: Dataset: univ                Batch: 1/3	Loss 0.8520 (0.8520)
+2022-11-18 14:57:22,852:INFO: Dataset: univ                Batch: 2/3	Loss 0.8586 (0.8553)
+2022-11-18 14:57:22,924:INFO: Dataset: univ                Batch: 3/3	Loss 0.8556 (0.8554)
+2022-11-18 14:57:23,249:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9134 (0.9134)
+2022-11-18 14:57:23,296:INFO: Dataset: zara1               Batch: 2/2	Loss 0.9283 (0.9175)
+2022-11-18 14:57:23,634:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8637 (0.8637)
+2022-11-18 14:57:23,732:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8173 (0.8401)
+2022-11-18 14:57:23,814:INFO: Dataset: zara2               Batch: 3/5	Loss 0.8221 (0.8337)
+2022-11-18 14:57:23,894:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8729 (0.8433)
+2022-11-18 14:57:23,976:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8018 (0.8352)
+2022-11-18 14:57:24,045:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_242.pth.tar
+2022-11-18 14:57:24,045:INFO: 
+===> EPOCH: 243 (P2)
+2022-11-18 14:57:24,046:INFO: - Computing loss (training)
+2022-11-18 14:57:24,408:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9589 (0.9589)
+2022-11-18 14:57:24,587:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9667 (0.9628)
+2022-11-18 14:57:24,774:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9137 (0.9463)
+2022-11-18 14:57:24,923:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9119 (0.9401)
+2022-11-18 14:57:25,375:INFO: Dataset: univ                Batch:  1/15	Loss 0.8659 (0.8659)
+2022-11-18 14:57:25,541:INFO: Dataset: univ                Batch:  2/15	Loss 0.8684 (0.8672)
+2022-11-18 14:57:25,711:INFO: Dataset: univ                Batch:  3/15	Loss 0.8719 (0.8688)
+2022-11-18 14:57:25,872:INFO: Dataset: univ                Batch:  4/15	Loss 0.8925 (0.8749)
+2022-11-18 14:57:26,036:INFO: Dataset: univ                Batch:  5/15	Loss 0.8578 (0.8715)
+2022-11-18 14:57:26,195:INFO: Dataset: univ                Batch:  6/15	Loss 0.8639 (0.8703)
+2022-11-18 14:57:26,355:INFO: Dataset: univ                Batch:  7/15	Loss 0.8619 (0.8691)
+2022-11-18 14:57:26,515:INFO: Dataset: univ                Batch:  8/15	Loss 0.8887 (0.8715)
+2022-11-18 14:57:26,674:INFO: Dataset: univ                Batch:  9/15	Loss 0.8665 (0.8710)
+2022-11-18 14:57:26,833:INFO: Dataset: univ                Batch: 10/15	Loss 0.8724 (0.8712)
+2022-11-18 14:57:26,989:INFO: Dataset: univ                Batch: 11/15	Loss 0.8562 (0.8699)
+2022-11-18 14:57:27,146:INFO: Dataset: univ                Batch: 12/15	Loss 0.8840 (0.8711)
+2022-11-18 14:57:27,304:INFO: Dataset: univ                Batch: 13/15	Loss 0.8849 (0.8721)
+2022-11-18 14:57:27,460:INFO: Dataset: univ                Batch: 14/15	Loss 0.8672 (0.8718)
+2022-11-18 14:57:27,544:INFO: Dataset: univ                Batch: 15/15	Loss 0.8586 (0.8716)
+2022-11-18 14:57:27,958:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8712 (0.8712)
+2022-11-18 14:57:28,122:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9279 (0.9015)
+2022-11-18 14:57:28,289:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9344 (0.9122)
+2022-11-18 14:57:28,435:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8937 (0.9080)
+2022-11-18 14:57:28,587:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8579 (0.8975)
+2022-11-18 14:57:28,750:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9500 (0.9053)
+2022-11-18 14:57:28,933:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9466 (0.9109)
+2022-11-18 14:57:29,069:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9236 (0.9123)
+2022-11-18 14:57:29,505:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8743 (0.8743)
+2022-11-18 14:57:29,673:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8580 (0.8658)
+2022-11-18 14:57:29,840:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8953 (0.8756)
+2022-11-18 14:57:30,000:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8771 (0.8760)
+2022-11-18 14:57:30,149:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9287 (0.8875)
+2022-11-18 14:57:30,317:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9056 (0.8906)
+2022-11-18 14:57:30,477:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8758 (0.8885)
+2022-11-18 14:57:30,629:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9139 (0.8917)
+2022-11-18 14:57:30,791:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8883 (0.8913)
+2022-11-18 14:57:30,955:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8786 (0.8901)
+2022-11-18 14:57:31,115:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8988 (0.8909)
+2022-11-18 14:57:31,263:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8568 (0.8878)
+2022-11-18 14:57:31,410:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9229 (0.8904)
+2022-11-18 14:57:31,558:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8794 (0.8896)
+2022-11-18 14:57:31,710:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8648 (0.8879)
+2022-11-18 14:57:31,858:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8571 (0.8862)
+2022-11-18 14:57:32,018:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8647 (0.8850)
+2022-11-18 14:57:32,164:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8505 (0.8834)
+2022-11-18 14:57:32,213:INFO: - Computing loss (validation)
+2022-11-18 14:57:32,484:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9038 (0.9038)
+2022-11-18 14:57:32,518:INFO: Dataset: hotel               Batch: 2/2	Loss 0.9329 (0.9059)
+2022-11-18 14:57:32,830:INFO: Dataset: univ                Batch: 1/3	Loss 0.8713 (0.8713)
+2022-11-18 14:57:32,909:INFO: Dataset: univ                Batch: 2/3	Loss 0.8578 (0.8642)
+2022-11-18 14:57:32,985:INFO: Dataset: univ                Batch: 3/3	Loss 0.8595 (0.8625)
+2022-11-18 14:57:33,299:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9533 (0.9533)
+2022-11-18 14:57:33,346:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8853 (0.9352)
+2022-11-18 14:57:33,690:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8414 (0.8414)
+2022-11-18 14:57:33,776:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8564 (0.8489)
+2022-11-18 14:57:33,864:INFO: Dataset: zara2               Batch: 3/5	Loss 0.8961 (0.8634)
+2022-11-18 14:57:33,945:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8721 (0.8656)
+2022-11-18 14:57:34,023:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8811 (0.8689)
+2022-11-18 14:57:34,076:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_243.pth.tar
+2022-11-18 14:57:34,077:INFO: 
+===> EPOCH: 244 (P2)
+2022-11-18 14:57:34,077:INFO: - Computing loss (training)
+2022-11-18 14:57:34,421:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9210 (0.9210)
+2022-11-18 14:57:34,584:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9101 (0.9155)
+2022-11-18 14:57:34,748:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9900 (0.9396)
+2022-11-18 14:57:34,880:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9442 (0.9404)
+2022-11-18 14:57:35,290:INFO: Dataset: univ                Batch:  1/15	Loss 0.9054 (0.9054)
+2022-11-18 14:57:35,454:INFO: Dataset: univ                Batch:  2/15	Loss 0.8935 (0.8988)
+2022-11-18 14:57:35,613:INFO: Dataset: univ                Batch:  3/15	Loss 0.8735 (0.8910)
+2022-11-18 14:57:35,773:INFO: Dataset: univ                Batch:  4/15	Loss 0.8951 (0.8920)
+2022-11-18 14:57:35,940:INFO: Dataset: univ                Batch:  5/15	Loss 0.8941 (0.8924)
+2022-11-18 14:57:36,111:INFO: Dataset: univ                Batch:  6/15	Loss 0.8836 (0.8909)
+2022-11-18 14:57:36,282:INFO: Dataset: univ                Batch:  7/15	Loss 0.8720 (0.8881)
+2022-11-18 14:57:36,447:INFO: Dataset: univ                Batch:  8/15	Loss 0.8974 (0.8893)
+2022-11-18 14:57:36,606:INFO: Dataset: univ                Batch:  9/15	Loss 0.8913 (0.8895)
+2022-11-18 14:57:36,763:INFO: Dataset: univ                Batch: 10/15	Loss 0.9053 (0.8911)
+2022-11-18 14:57:36,920:INFO: Dataset: univ                Batch: 11/15	Loss 0.8841 (0.8904)
+2022-11-18 14:57:37,075:INFO: Dataset: univ                Batch: 12/15	Loss 0.8925 (0.8906)
+2022-11-18 14:57:37,231:INFO: Dataset: univ                Batch: 13/15	Loss 0.8778 (0.8896)
+2022-11-18 14:57:37,385:INFO: Dataset: univ                Batch: 14/15	Loss 0.8813 (0.8890)
+2022-11-18 14:57:37,468:INFO: Dataset: univ                Batch: 15/15	Loss 0.8992 (0.8892)
+2022-11-18 14:57:37,859:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9006 (0.9006)
+2022-11-18 14:57:38,015:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8992 (0.8999)
+2022-11-18 14:57:38,169:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8894 (0.8964)
+2022-11-18 14:57:38,320:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9195 (0.9026)
+2022-11-18 14:57:38,473:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8925 (0.9004)
+2022-11-18 14:57:38,621:INFO: Dataset: zara1               Batch: 6/8	Loss 0.8942 (0.8993)
+2022-11-18 14:57:38,772:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9542 (0.9069)
+2022-11-18 14:57:38,910:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9064 (0.9068)
+2022-11-18 14:57:39,335:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9066 (0.9066)
+2022-11-18 14:57:39,494:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9291 (0.9175)
+2022-11-18 14:57:39,646:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9200 (0.9184)
+2022-11-18 14:57:39,816:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9035 (0.9144)
+2022-11-18 14:57:40,008:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8777 (0.9066)
+2022-11-18 14:57:40,185:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9404 (0.9116)
+2022-11-18 14:57:40,351:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8929 (0.9089)
+2022-11-18 14:57:40,512:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8851 (0.9057)
+2022-11-18 14:57:40,676:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9179 (0.9072)
+2022-11-18 14:57:40,825:INFO: Dataset: zara2               Batch: 10/18	Loss 0.8991 (0.9063)
+2022-11-18 14:57:40,985:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8592 (0.9020)
+2022-11-18 14:57:41,142:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8437 (0.8970)
+2022-11-18 14:57:41,296:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8524 (0.8937)
+2022-11-18 14:57:41,452:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8884 (0.8933)
+2022-11-18 14:57:41,602:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8836 (0.8927)
+2022-11-18 14:57:41,750:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8918 (0.8926)
+2022-11-18 14:57:41,906:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8811 (0.8920)
+2022-11-18 14:57:42,052:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9164 (0.8932)
+2022-11-18 14:57:42,098:INFO: - Computing loss (validation)
+2022-11-18 14:57:42,376:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9129 (0.9129)
+2022-11-18 14:57:42,416:INFO: Dataset: hotel               Batch: 2/2	Loss 0.9034 (0.9121)
+2022-11-18 14:57:42,746:INFO: Dataset: univ                Batch: 1/3	Loss 0.8957 (0.8957)
+2022-11-18 14:57:42,824:INFO: Dataset: univ                Batch: 2/3	Loss 0.8880 (0.8918)
+2022-11-18 14:57:42,898:INFO: Dataset: univ                Batch: 3/3	Loss 0.8713 (0.8843)
+2022-11-18 14:57:43,228:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9101 (0.9101)
+2022-11-18 14:57:43,279:INFO: Dataset: zara1               Batch: 2/2	Loss 0.9004 (0.9074)
+2022-11-18 14:57:43,622:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8808 (0.8808)
+2022-11-18 14:57:43,703:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8874 (0.8840)
+2022-11-18 14:57:43,783:INFO: Dataset: zara2               Batch: 3/5	Loss 0.8691 (0.8789)
+2022-11-18 14:57:43,866:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8817 (0.8797)
+2022-11-18 14:57:43,944:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8609 (0.8760)
+2022-11-18 14:57:43,996:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_244.pth.tar
+2022-11-18 14:57:43,996:INFO: 
+===> EPOCH: 245 (P2)
+2022-11-18 14:57:43,997:INFO: - Computing loss (training)
+2022-11-18 14:57:44,350:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0123 (1.0123)
+2022-11-18 14:57:44,504:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9864 (0.9998)
+2022-11-18 14:57:44,656:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9113 (0.9698)
+2022-11-18 14:57:44,776:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9705 (0.9699)
+2022-11-18 14:57:45,169:INFO: Dataset: univ                Batch:  1/15	Loss 0.9132 (0.9132)
+2022-11-18 14:57:45,324:INFO: Dataset: univ                Batch:  2/15	Loss 0.9000 (0.9069)
+2022-11-18 14:57:45,482:INFO: Dataset: univ                Batch:  3/15	Loss 0.8840 (0.8993)
+2022-11-18 14:57:45,637:INFO: Dataset: univ                Batch:  4/15	Loss 0.8948 (0.8981)
+2022-11-18 14:57:45,867:INFO: Dataset: univ                Batch:  5/15	Loss 0.8917 (0.8968)
+2022-11-18 14:57:46,027:INFO: Dataset: univ                Batch:  6/15	Loss 0.8831 (0.8945)
+2022-11-18 14:57:46,188:INFO: Dataset: univ                Batch:  7/15	Loss 0.8977 (0.8950)
+2022-11-18 14:57:46,347:INFO: Dataset: univ                Batch:  8/15	Loss 0.9104 (0.8969)
+2022-11-18 14:57:46,508:INFO: Dataset: univ                Batch:  9/15	Loss 0.9021 (0.8975)
+2022-11-18 14:57:46,660:INFO: Dataset: univ                Batch: 10/15	Loss 0.9153 (0.8992)
+2022-11-18 14:57:46,816:INFO: Dataset: univ                Batch: 11/15	Loss 0.8914 (0.8985)
+2022-11-18 14:57:46,976:INFO: Dataset: univ                Batch: 12/15	Loss 0.8982 (0.8985)
+2022-11-18 14:57:47,131:INFO: Dataset: univ                Batch: 13/15	Loss 0.9182 (0.9001)
+2022-11-18 14:57:47,286:INFO: Dataset: univ                Batch: 14/15	Loss 0.8923 (0.8996)
+2022-11-18 14:57:47,374:INFO: Dataset: univ                Batch: 15/15	Loss 0.9454 (0.9001)
+2022-11-18 14:57:47,756:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9277 (0.9277)
+2022-11-18 14:57:47,908:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9409 (0.9340)
+2022-11-18 14:57:48,060:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9601 (0.9425)
+2022-11-18 14:57:48,239:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9149 (0.9362)
+2022-11-18 14:57:48,424:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9649 (0.9415)
+2022-11-18 14:57:48,612:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9312 (0.9399)
+2022-11-18 14:57:48,773:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9373 (0.9395)
+2022-11-18 14:57:48,914:INFO: Dataset: zara1               Batch: 8/8	Loss 0.8994 (0.9353)
+2022-11-18 14:57:49,306:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8959 (0.8959)
+2022-11-18 14:57:49,461:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9268 (0.9115)
+2022-11-18 14:57:49,620:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8946 (0.9057)
+2022-11-18 14:57:49,773:INFO: Dataset: zara2               Batch:  4/18	Loss 0.8604 (0.8941)
+2022-11-18 14:57:49,932:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9213 (0.8999)
+2022-11-18 14:57:50,091:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9158 (0.9024)
+2022-11-18 14:57:50,246:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9308 (0.9069)
+2022-11-18 14:57:50,400:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9096 (0.9072)
+2022-11-18 14:57:50,556:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9217 (0.9090)
+2022-11-18 14:57:50,715:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9137 (0.9095)
+2022-11-18 14:57:50,869:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9077 (0.9093)
+2022-11-18 14:57:51,022:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9093 (0.9093)
+2022-11-18 14:57:51,174:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9152 (0.9098)
+2022-11-18 14:57:51,324:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8953 (0.9087)
+2022-11-18 14:57:51,476:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9413 (0.9109)
+2022-11-18 14:57:51,636:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9231 (0.9118)
+2022-11-18 14:57:51,790:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9411 (0.9135)
+2022-11-18 14:57:51,928:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9201 (0.9138)
+2022-11-18 14:57:51,979:INFO: - Computing loss (validation)
+2022-11-18 14:57:52,253:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9931 (0.9931)
+2022-11-18 14:57:52,288:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8710 (0.9839)
+2022-11-18 14:57:52,618:INFO: Dataset: univ                Batch: 1/3	Loss 0.8792 (0.8792)
+2022-11-18 14:57:52,696:INFO: Dataset: univ                Batch: 2/3	Loss 0.9193 (0.8987)
+2022-11-18 14:57:52,771:INFO: Dataset: univ                Batch: 3/3	Loss 0.8854 (0.8949)
+2022-11-18 14:57:53,079:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9314 (0.9314)
+2022-11-18 14:57:53,125:INFO: Dataset: zara1               Batch: 2/2	Loss 0.9060 (0.9246)
+2022-11-18 14:57:53,475:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8899 (0.8899)
+2022-11-18 14:57:53,554:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9180 (0.9043)
+2022-11-18 14:57:53,632:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9196 (0.9096)
+2022-11-18 14:57:53,709:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9166 (0.9113)
+2022-11-18 14:57:53,788:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9037 (0.9098)
+2022-11-18 14:57:53,854:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_245.pth.tar
+2022-11-18 14:57:53,854:INFO: 
+===> EPOCH: 246 (P2)
+2022-11-18 14:57:53,855:INFO: - Computing loss (training)
+2022-11-18 14:57:54,242:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9786 (0.9786)
+2022-11-18 14:57:54,401:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9546 (0.9670)
+2022-11-18 14:57:54,564:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9705 (0.9681)
+2022-11-18 14:57:54,689:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9451 (0.9641)
+2022-11-18 14:57:55,135:INFO: Dataset: univ                Batch:  1/15	Loss 0.9272 (0.9272)
+2022-11-18 14:57:55,311:INFO: Dataset: univ                Batch:  2/15	Loss 0.9257 (0.9264)
+2022-11-18 14:57:55,485:INFO: Dataset: univ                Batch:  3/15	Loss 0.9268 (0.9265)
+2022-11-18 14:57:55,650:INFO: Dataset: univ                Batch:  4/15	Loss 0.9394 (0.9296)
+2022-11-18 14:57:55,818:INFO: Dataset: univ                Batch:  5/15	Loss 0.9260 (0.9290)
+2022-11-18 14:57:55,987:INFO: Dataset: univ                Batch:  6/15	Loss 0.9245 (0.9282)
+2022-11-18 14:57:56,153:INFO: Dataset: univ                Batch:  7/15	Loss 0.9302 (0.9285)
+2022-11-18 14:57:56,316:INFO: Dataset: univ                Batch:  8/15	Loss 0.9314 (0.9289)
+2022-11-18 14:57:56,484:INFO: Dataset: univ                Batch:  9/15	Loss 0.9078 (0.9263)
+2022-11-18 14:57:56,655:INFO: Dataset: univ                Batch: 10/15	Loss 0.9335 (0.9270)
+2022-11-18 14:57:56,825:INFO: Dataset: univ                Batch: 11/15	Loss 0.9457 (0.9288)
+2022-11-18 14:57:56,998:INFO: Dataset: univ                Batch: 12/15	Loss 0.9173 (0.9278)
+2022-11-18 14:57:57,190:INFO: Dataset: univ                Batch: 13/15	Loss 0.9272 (0.9277)
+2022-11-18 14:57:57,369:INFO: Dataset: univ                Batch: 14/15	Loss 0.9420 (0.9287)
+2022-11-18 14:57:57,465:INFO: Dataset: univ                Batch: 15/15	Loss 0.9268 (0.9287)
+2022-11-18 14:57:57,910:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9582 (0.9582)
+2022-11-18 14:57:58,067:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8838 (0.9186)
+2022-11-18 14:57:58,229:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9712 (0.9362)
+2022-11-18 14:57:58,412:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9825 (0.9485)
+2022-11-18 14:57:58,594:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9172 (0.9418)
+2022-11-18 14:57:58,784:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9689 (0.9465)
+2022-11-18 14:57:58,957:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9201 (0.9427)
+2022-11-18 14:57:59,113:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9575 (0.9444)
+2022-11-18 14:57:59,548:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9260 (0.9260)
+2022-11-18 14:57:59,713:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8969 (0.9100)
+2022-11-18 14:57:59,907:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8987 (0.9062)
+2022-11-18 14:58:00,095:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9191 (0.9093)
+2022-11-18 14:58:00,271:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9254 (0.9128)
+2022-11-18 14:58:00,454:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9660 (0.9215)
+2022-11-18 14:58:00,625:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9450 (0.9247)
+2022-11-18 14:58:00,795:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9382 (0.9265)
+2022-11-18 14:58:00,974:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9489 (0.9292)
+2022-11-18 14:58:01,151:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9135 (0.9276)
+2022-11-18 14:58:01,331:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9703 (0.9320)
+2022-11-18 14:58:01,511:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9029 (0.9299)
+2022-11-18 14:58:01,689:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9253 (0.9295)
+2022-11-18 14:58:01,863:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8817 (0.9260)
+2022-11-18 14:58:02,051:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9374 (0.9267)
+2022-11-18 14:58:02,244:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9492 (0.9282)
+2022-11-18 14:58:02,422:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8895 (0.9261)
+2022-11-18 14:58:02,585:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9638 (0.9279)
+2022-11-18 14:58:02,641:INFO: - Computing loss (validation)
+2022-11-18 14:58:02,970:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9493 (0.9493)
+2022-11-18 14:58:03,014:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0273 (0.9548)
+2022-11-18 14:58:03,363:INFO: Dataset: univ                Batch: 1/3	Loss 0.9435 (0.9435)
+2022-11-18 14:58:03,450:INFO: Dataset: univ                Batch: 2/3	Loss 0.9276 (0.9369)
+2022-11-18 14:58:03,533:INFO: Dataset: univ                Batch: 3/3	Loss 0.9103 (0.9289)
+2022-11-18 14:58:03,860:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9704 (0.9704)
+2022-11-18 14:58:03,908:INFO: Dataset: zara1               Batch: 2/2	Loss 0.9666 (0.9696)
+2022-11-18 14:58:04,323:INFO: Dataset: zara2               Batch: 1/5	Loss 0.9442 (0.9442)
+2022-11-18 14:58:04,429:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9129 (0.9287)
+2022-11-18 14:58:04,525:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9212 (0.9263)
+2022-11-18 14:58:04,623:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9034 (0.9206)
+2022-11-18 14:58:04,715:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9119 (0.9189)
+2022-11-18 14:58:04,787:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_246.pth.tar
+2022-11-18 14:58:04,787:INFO: 
+===> EPOCH: 247 (P2)
+2022-11-18 14:58:04,788:INFO: - Computing loss (training)
+2022-11-18 14:58:05,200:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9830 (0.9830)
+2022-11-18 14:58:05,377:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9864 (0.9847)
+2022-11-18 14:58:05,544:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9753 (0.9815)
+2022-11-18 14:58:05,678:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9910 (0.9832)
+2022-11-18 14:58:06,135:INFO: Dataset: univ                Batch:  1/15	Loss 0.9368 (0.9368)
+2022-11-18 14:58:06,330:INFO: Dataset: univ                Batch:  2/15	Loss 0.9294 (0.9330)
+2022-11-18 14:58:06,522:INFO: Dataset: univ                Batch:  3/15	Loss 0.9550 (0.9404)
+2022-11-18 14:58:06,703:INFO: Dataset: univ                Batch:  4/15	Loss 0.9208 (0.9354)
+2022-11-18 14:58:06,859:INFO: Dataset: univ                Batch:  5/15	Loss 0.9501 (0.9382)
+2022-11-18 14:58:07,016:INFO: Dataset: univ                Batch:  6/15	Loss 0.9475 (0.9396)
+2022-11-18 14:58:07,173:INFO: Dataset: univ                Batch:  7/15	Loss 0.9423 (0.9400)
+2022-11-18 14:58:07,362:INFO: Dataset: univ                Batch:  8/15	Loss 0.9318 (0.9391)
+2022-11-18 14:58:07,553:INFO: Dataset: univ                Batch:  9/15	Loss 0.9670 (0.9421)
+2022-11-18 14:58:07,737:INFO: Dataset: univ                Batch: 10/15	Loss 0.9432 (0.9422)
+2022-11-18 14:58:07,912:INFO: Dataset: univ                Batch: 11/15	Loss 0.9532 (0.9432)
+2022-11-18 14:58:08,084:INFO: Dataset: univ                Batch: 12/15	Loss 0.9271 (0.9418)
+2022-11-18 14:58:08,254:INFO: Dataset: univ                Batch: 13/15	Loss 0.9185 (0.9400)
+2022-11-18 14:58:08,431:INFO: Dataset: univ                Batch: 14/15	Loss 0.9423 (0.9402)
+2022-11-18 14:58:08,523:INFO: Dataset: univ                Batch: 15/15	Loss 0.9514 (0.9403)
+2022-11-18 14:58:08,941:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9619 (0.9619)
+2022-11-18 14:58:09,104:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9422 (0.9519)
+2022-11-18 14:58:09,283:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9264 (0.9445)
+2022-11-18 14:58:09,446:INFO: Dataset: zara1               Batch: 4/8	Loss 0.9322 (0.9416)
+2022-11-18 14:58:09,620:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9436 (0.9419)
+2022-11-18 14:58:09,784:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0116 (0.9543)
+2022-11-18 14:58:09,955:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0143 (0.9622)
+2022-11-18 14:58:10,105:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9523 (0.9611)
+2022-11-18 14:58:10,545:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9876 (0.9876)
+2022-11-18 14:58:10,720:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9556 (0.9714)
+2022-11-18 14:58:10,887:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0067 (0.9831)
+2022-11-18 14:58:11,053:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9515 (0.9751)
+2022-11-18 14:58:11,213:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9220 (0.9646)
+2022-11-18 14:58:11,385:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9724 (0.9659)
+2022-11-18 14:58:11,547:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9245 (0.9600)
+2022-11-18 14:58:11,714:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9190 (0.9551)
+2022-11-18 14:58:11,873:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9124 (0.9503)
+2022-11-18 14:58:12,041:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9892 (0.9537)
+2022-11-18 14:58:12,196:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9687 (0.9551)
+2022-11-18 14:58:12,351:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9416 (0.9540)
+2022-11-18 14:58:12,503:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9124 (0.9509)
+2022-11-18 14:58:12,658:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9725 (0.9525)
+2022-11-18 14:58:12,811:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9907 (0.9548)
+2022-11-18 14:58:12,961:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9562 (0.9549)
+2022-11-18 14:58:13,120:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9116 (0.9523)
+2022-11-18 14:58:13,284:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9846 (0.9540)
+2022-11-18 14:58:13,347:INFO: - Computing loss (validation)
+2022-11-18 14:58:13,689:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9486 (0.9486)
+2022-11-18 14:58:13,730:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0635 (0.9573)
+2022-11-18 14:58:14,181:INFO: Dataset: univ                Batch: 1/3	Loss 0.9528 (0.9528)
+2022-11-18 14:58:14,284:INFO: Dataset: univ                Batch: 2/3	Loss 0.9439 (0.9480)
+2022-11-18 14:58:14,377:INFO: Dataset: univ                Batch: 3/3	Loss 0.9581 (0.9511)
+2022-11-18 14:58:14,749:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9852 (0.9852)
+2022-11-18 14:58:14,797:INFO: Dataset: zara1               Batch: 2/2	Loss 0.9249 (0.9707)
+2022-11-18 14:58:15,165:INFO: Dataset: zara2               Batch: 1/5	Loss 0.9181 (0.9181)
+2022-11-18 14:58:15,251:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9438 (0.9317)
+2022-11-18 14:58:15,336:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9285 (0.9306)
+2022-11-18 14:58:15,420:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9666 (0.9397)
+2022-11-18 14:58:15,517:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9673 (0.9456)
+2022-11-18 14:58:15,584:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_247.pth.tar
+2022-11-18 14:58:15,584:INFO: 
+===> EPOCH: 248 (P2)
+2022-11-18 14:58:15,585:INFO: - Computing loss (training)
+2022-11-18 14:58:15,957:INFO: Dataset: hotel               Batch: 1/4	Loss 0.9986 (0.9986)
+2022-11-18 14:58:16,106:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0725 (1.0358)
+2022-11-18 14:58:16,255:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0626 (1.0447)
+2022-11-18 14:58:16,384:INFO: Dataset: hotel               Batch: 4/4	Loss 1.0058 (1.0381)
+2022-11-18 14:58:16,849:INFO: Dataset: univ                Batch:  1/15	Loss 0.9661 (0.9661)
+2022-11-18 14:58:17,011:INFO: Dataset: univ                Batch:  2/15	Loss 0.9651 (0.9656)
+2022-11-18 14:58:17,190:INFO: Dataset: univ                Batch:  3/15	Loss 0.9662 (0.9658)
+2022-11-18 14:58:17,369:INFO: Dataset: univ                Batch:  4/15	Loss 0.9372 (0.9589)
+2022-11-18 14:58:17,547:INFO: Dataset: univ                Batch:  5/15	Loss 0.9550 (0.9581)
+2022-11-18 14:58:17,725:INFO: Dataset: univ                Batch:  6/15	Loss 0.9542 (0.9573)
+2022-11-18 14:58:17,896:INFO: Dataset: univ                Batch:  7/15	Loss 0.9421 (0.9552)
+2022-11-18 14:58:18,075:INFO: Dataset: univ                Batch:  8/15	Loss 0.9479 (0.9543)
+2022-11-18 14:58:18,237:INFO: Dataset: univ                Batch:  9/15	Loss 0.9542 (0.9543)
+2022-11-18 14:58:18,409:INFO: Dataset: univ                Batch: 10/15	Loss 0.9788 (0.9567)
+2022-11-18 14:58:18,593:INFO: Dataset: univ                Batch: 11/15	Loss 0.9548 (0.9565)
+2022-11-18 14:58:18,764:INFO: Dataset: univ                Batch: 12/15	Loss 0.9704 (0.9576)
+2022-11-18 14:58:18,925:INFO: Dataset: univ                Batch: 13/15	Loss 0.9710 (0.9587)
+2022-11-18 14:58:19,081:INFO: Dataset: univ                Batch: 14/15	Loss 0.9772 (0.9601)
+2022-11-18 14:58:19,163:INFO: Dataset: univ                Batch: 15/15	Loss 0.9406 (0.9598)
+2022-11-18 14:58:19,569:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0205 (1.0205)
+2022-11-18 14:58:19,718:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9926 (1.0075)
+2022-11-18 14:58:19,874:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9960 (1.0039)
+2022-11-18 14:58:20,028:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0195 (1.0079)
+2022-11-18 14:58:20,178:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9749 (1.0018)
+2022-11-18 14:58:20,325:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9892 (0.9996)
+2022-11-18 14:58:20,476:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9777 (0.9963)
+2022-11-18 14:58:20,614:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0239 (0.9997)
+2022-11-18 14:58:21,005:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9768 (0.9768)
+2022-11-18 14:58:21,158:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9475 (0.9619)
+2022-11-18 14:58:21,310:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9995 (0.9744)
+2022-11-18 14:58:21,460:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9656 (0.9721)
+2022-11-18 14:58:21,610:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9815 (0.9739)
+2022-11-18 14:58:21,761:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9703 (0.9733)
+2022-11-18 14:58:21,909:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9251 (0.9665)
+2022-11-18 14:58:22,055:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9750 (0.9676)
+2022-11-18 14:58:22,207:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9428 (0.9650)
+2022-11-18 14:58:22,377:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0310 (0.9712)
+2022-11-18 14:58:22,546:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9620 (0.9704)
+2022-11-18 14:58:22,701:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0070 (0.9733)
+2022-11-18 14:58:22,853:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9045 (0.9680)
+2022-11-18 14:58:23,002:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0244 (0.9722)
+2022-11-18 14:58:23,163:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9433 (0.9704)
+2022-11-18 14:58:23,331:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9975 (0.9721)
+2022-11-18 14:58:23,496:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9692 (0.9720)
+2022-11-18 14:58:23,656:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0296 (0.9747)
+2022-11-18 14:58:23,712:INFO: - Computing loss (validation)
+2022-11-18 14:58:23,990:INFO: Dataset: hotel               Batch: 1/2	Loss 1.0001 (1.0001)
+2022-11-18 14:58:24,028:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8367 (0.9861)
+2022-11-18 14:58:24,366:INFO: Dataset: univ                Batch: 1/3	Loss 0.9632 (0.9632)
+2022-11-18 14:58:24,443:INFO: Dataset: univ                Batch: 2/3	Loss 0.9441 (0.9544)
+2022-11-18 14:58:24,521:INFO: Dataset: univ                Batch: 3/3	Loss 0.9551 (0.9547)
+2022-11-18 14:58:24,862:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9453 (0.9453)
+2022-11-18 14:58:24,907:INFO: Dataset: zara1               Batch: 2/2	Loss 1.0142 (0.9634)
+2022-11-18 14:58:25,244:INFO: Dataset: zara2               Batch: 1/5	Loss 0.9182 (0.9182)
+2022-11-18 14:58:25,323:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9288 (0.9236)
+2022-11-18 14:58:25,400:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9738 (0.9405)
+2022-11-18 14:58:25,478:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9502 (0.9431)
+2022-11-18 14:58:25,554:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9718 (0.9491)
+2022-11-18 14:58:25,612:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_248.pth.tar
+2022-11-18 14:58:25,613:INFO: 
+===> EPOCH: 249 (P2)
+2022-11-18 14:58:25,613:INFO: - Computing loss (training)
+2022-11-18 14:58:26,006:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0902 (1.0902)
+2022-11-18 14:58:26,166:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0116 (1.0507)
+2022-11-18 14:58:26,338:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9836 (1.0274)
+2022-11-18 14:58:26,462:INFO: Dataset: hotel               Batch: 4/4	Loss 1.0706 (1.0347)
+2022-11-18 14:58:26,847:INFO: Dataset: univ                Batch:  1/15	Loss 0.9760 (0.9760)
+2022-11-18 14:58:27,019:INFO: Dataset: univ                Batch:  2/15	Loss 1.0037 (0.9899)
+2022-11-18 14:58:27,199:INFO: Dataset: univ                Batch:  3/15	Loss 0.9610 (0.9806)
+2022-11-18 14:58:27,372:INFO: Dataset: univ                Batch:  4/15	Loss 0.9757 (0.9793)
+2022-11-18 14:58:27,530:INFO: Dataset: univ                Batch:  5/15	Loss 0.9711 (0.9775)
+2022-11-18 14:58:27,686:INFO: Dataset: univ                Batch:  6/15	Loss 0.9765 (0.9774)
+2022-11-18 14:58:27,845:INFO: Dataset: univ                Batch:  7/15	Loss 0.9810 (0.9779)
+2022-11-18 14:58:28,016:INFO: Dataset: univ                Batch:  8/15	Loss 0.9664 (0.9765)
+2022-11-18 14:58:28,176:INFO: Dataset: univ                Batch:  9/15	Loss 0.9823 (0.9771)
+2022-11-18 14:58:28,329:INFO: Dataset: univ                Batch: 10/15	Loss 0.9741 (0.9768)
+2022-11-18 14:58:28,483:INFO: Dataset: univ                Batch: 11/15	Loss 0.9879 (0.9779)
+2022-11-18 14:58:28,639:INFO: Dataset: univ                Batch: 12/15	Loss 0.9641 (0.9768)
+2022-11-18 14:58:28,796:INFO: Dataset: univ                Batch: 13/15	Loss 0.9706 (0.9763)
+2022-11-18 14:58:28,952:INFO: Dataset: univ                Batch: 14/15	Loss 0.9815 (0.9767)
+2022-11-18 14:58:29,035:INFO: Dataset: univ                Batch: 15/15	Loss 0.9918 (0.9769)
+2022-11-18 14:58:29,456:INFO: Dataset: zara1               Batch: 1/8	Loss 0.9852 (0.9852)
+2022-11-18 14:58:29,606:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9955 (0.9905)
+2022-11-18 14:58:29,755:INFO: Dataset: zara1               Batch: 3/8	Loss 0.9937 (0.9916)
+2022-11-18 14:58:29,906:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0494 (1.0055)
+2022-11-18 14:58:30,074:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9623 (0.9970)
+2022-11-18 14:58:30,240:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0083 (0.9988)
+2022-11-18 14:58:30,410:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0184 (1.0015)
+2022-11-18 14:58:30,552:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0295 (1.0049)
+2022-11-18 14:58:30,956:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0011 (1.0011)
+2022-11-18 14:58:31,105:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0104 (1.0060)
+2022-11-18 14:58:31,264:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9627 (0.9916)
+2022-11-18 14:58:31,423:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9901 (0.9912)
+2022-11-18 14:58:31,581:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9856 (0.9902)
+2022-11-18 14:58:31,735:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9791 (0.9884)
+2022-11-18 14:58:31,898:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9962 (0.9895)
+2022-11-18 14:58:32,061:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0138 (0.9926)
+2022-11-18 14:58:32,234:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9420 (0.9872)
+2022-11-18 14:58:32,413:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9819 (0.9866)
+2022-11-18 14:58:32,590:INFO: Dataset: zara2               Batch: 11/18	Loss 0.9279 (0.9812)
+2022-11-18 14:58:32,779:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9697 (0.9802)
+2022-11-18 14:58:32,985:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0044 (0.9821)
+2022-11-18 14:58:33,144:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9711 (0.9813)
+2022-11-18 14:58:33,299:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0045 (0.9827)
+2022-11-18 14:58:33,450:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0260 (0.9855)
+2022-11-18 14:58:33,605:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9960 (0.9861)
+2022-11-18 14:58:33,758:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0000 (0.9867)
+2022-11-18 14:58:33,810:INFO: - Computing loss (validation)
+2022-11-18 14:58:34,091:INFO: Dataset: hotel               Batch: 1/2	Loss 1.0209 (1.0209)
+2022-11-18 14:58:34,127:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8852 (1.0098)
+2022-11-18 14:58:34,442:INFO: Dataset: univ                Batch: 1/3	Loss 0.9805 (0.9805)
+2022-11-18 14:58:34,520:INFO: Dataset: univ                Batch: 2/3	Loss 0.9622 (0.9709)
+2022-11-18 14:58:34,592:INFO: Dataset: univ                Batch: 3/3	Loss 1.0065 (0.9822)
+2022-11-18 14:58:34,959:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0061 (1.0061)
+2022-11-18 14:58:35,008:INFO: Dataset: zara1               Batch: 2/2	Loss 1.0646 (1.0221)
+2022-11-18 14:58:35,323:INFO: Dataset: zara2               Batch: 1/5	Loss 0.9794 (0.9794)
+2022-11-18 14:58:35,400:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9437 (0.9613)
+2022-11-18 14:58:35,476:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9885 (0.9699)
+2022-11-18 14:58:35,551:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9759 (0.9714)
+2022-11-18 14:58:35,625:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9794 (0.9729)
+2022-11-18 14:58:35,680:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_249.pth.tar
+2022-11-18 14:58:35,681:INFO: 
+===> EPOCH: 250 (P2)
+2022-11-18 14:58:35,681:INFO: - Computing loss (training)
+2022-11-18 14:58:36,040:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0430 (1.0430)
+2022-11-18 14:58:36,195:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0642 (1.0538)
+2022-11-18 14:58:36,357:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1033 (1.0710)
+2022-11-18 14:58:36,472:INFO: Dataset: hotel               Batch: 4/4	Loss 1.0153 (1.0607)
+2022-11-18 14:58:36,877:INFO: Dataset: univ                Batch:  1/15	Loss 1.0132 (1.0132)
+2022-11-18 14:58:37,072:INFO: Dataset: univ                Batch:  2/15	Loss 1.0041 (1.0090)
+2022-11-18 14:58:37,273:INFO: Dataset: univ                Batch:  3/15	Loss 0.9853 (1.0006)
+2022-11-18 14:58:37,440:INFO: Dataset: univ                Batch:  4/15	Loss 1.0186 (1.0046)
+2022-11-18 14:58:37,603:INFO: Dataset: univ                Batch:  5/15	Loss 1.0009 (1.0038)
+2022-11-18 14:58:37,774:INFO: Dataset: univ                Batch:  6/15	Loss 1.0092 (1.0047)
+2022-11-18 14:58:37,956:INFO: Dataset: univ                Batch:  7/15	Loss 0.9877 (1.0022)
+2022-11-18 14:58:38,122:INFO: Dataset: univ                Batch:  8/15	Loss 0.9897 (1.0006)
+2022-11-18 14:58:38,286:INFO: Dataset: univ                Batch:  9/15	Loss 0.9758 (0.9977)
+2022-11-18 14:58:38,465:INFO: Dataset: univ                Batch: 10/15	Loss 0.9794 (0.9960)
+2022-11-18 14:58:38,677:INFO: Dataset: univ                Batch: 11/15	Loss 1.0012 (0.9965)
+2022-11-18 14:58:38,860:INFO: Dataset: univ                Batch: 12/15	Loss 0.9949 (0.9964)
+2022-11-18 14:58:39,028:INFO: Dataset: univ                Batch: 13/15	Loss 1.0023 (0.9968)
+2022-11-18 14:58:39,224:INFO: Dataset: univ                Batch: 14/15	Loss 1.0223 (0.9984)
+2022-11-18 14:58:39,311:INFO: Dataset: univ                Batch: 15/15	Loss 0.9989 (0.9984)
+2022-11-18 14:58:39,716:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0941 (1.0941)
+2022-11-18 14:58:39,869:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9956 (1.0447)
+2022-11-18 14:58:40,028:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0270 (1.0385)
+2022-11-18 14:58:40,187:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0103 (1.0315)
+2022-11-18 14:58:40,347:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0520 (1.0356)
+2022-11-18 14:58:40,509:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9870 (1.0268)
+2022-11-18 14:58:40,667:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0456 (1.0295)
+2022-11-18 14:58:40,812:INFO: Dataset: zara1               Batch: 8/8	Loss 0.9931 (1.0250)
+2022-11-18 14:58:41,213:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0239 (1.0239)
+2022-11-18 14:58:41,363:INFO: Dataset: zara2               Batch:  2/18	Loss 0.9445 (0.9836)
+2022-11-18 14:58:41,515:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9595 (0.9761)
+2022-11-18 14:58:41,664:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0270 (0.9890)
+2022-11-18 14:58:41,814:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0063 (0.9925)
+2022-11-18 14:58:41,968:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0307 (0.9986)
+2022-11-18 14:58:42,118:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0217 (1.0020)
+2022-11-18 14:58:42,268:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0071 (1.0026)
+2022-11-18 14:58:42,419:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0223 (1.0047)
+2022-11-18 14:58:42,568:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0135 (1.0055)
+2022-11-18 14:58:42,719:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0499 (1.0094)
+2022-11-18 14:58:42,869:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9766 (1.0066)
+2022-11-18 14:58:43,020:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0060 (1.0066)
+2022-11-18 14:58:43,172:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0755 (1.0120)
+2022-11-18 14:58:43,325:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9955 (1.0108)
+2022-11-18 14:58:43,474:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0127 (1.0110)
+2022-11-18 14:58:43,626:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0486 (1.0134)
+2022-11-18 14:58:43,764:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0310 (1.0143)
+2022-11-18 14:58:43,811:INFO: - Computing loss (validation)
+2022-11-18 14:58:44,083:INFO: Dataset: hotel               Batch: 1/2	Loss 1.0589 (1.0589)
+2022-11-18 14:58:44,120:INFO: Dataset: hotel               Batch: 2/2	Loss 1.1272 (1.0640)
+2022-11-18 14:58:44,430:INFO: Dataset: univ                Batch: 1/3	Loss 0.9897 (0.9897)
+2022-11-18 14:58:44,509:INFO: Dataset: univ                Batch: 2/3	Loss 0.9760 (0.9831)
+2022-11-18 14:58:44,582:INFO: Dataset: univ                Batch: 3/3	Loss 0.9833 (0.9831)
+2022-11-18 14:58:44,896:INFO: Dataset: zara1               Batch: 1/2	Loss 0.9707 (0.9707)
+2022-11-18 14:58:44,943:INFO: Dataset: zara1               Batch: 2/2	Loss 1.0557 (0.9912)
+2022-11-18 14:58:45,247:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0518 (1.0518)
+2022-11-18 14:58:45,320:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9928 (1.0218)
+2022-11-18 14:58:45,394:INFO: Dataset: zara2               Batch: 3/5	Loss 1.0395 (1.0277)
+2022-11-18 14:58:45,468:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9851 (1.0175)
+2022-11-18 14:58:45,543:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9665 (1.0080)
+2022-11-18 14:58:45,594:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_250.pth.tar
+2022-11-18 14:58:45,594:INFO: 
+===> EPOCH: 251 (P2)
+2022-11-18 14:58:45,595:INFO: - Computing loss (training)
+2022-11-18 14:58:45,938:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1069 (1.1069)
+2022-11-18 14:58:46,093:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0748 (1.0906)
+2022-11-18 14:58:46,244:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0774 (1.0860)
+2022-11-18 14:58:46,359:INFO: Dataset: hotel               Batch: 4/4	Loss 1.0741 (1.0840)
+2022-11-18 14:58:46,820:INFO: Dataset: univ                Batch:  1/15	Loss 1.0378 (1.0378)
+2022-11-18 14:58:46,982:INFO: Dataset: univ                Batch:  2/15	Loss 1.0185 (1.0284)
+2022-11-18 14:58:47,140:INFO: Dataset: univ                Batch:  3/15	Loss 0.9918 (1.0156)
+2022-11-18 14:58:47,297:INFO: Dataset: univ                Batch:  4/15	Loss 1.0043 (1.0130)
+2022-11-18 14:58:47,457:INFO: Dataset: univ                Batch:  5/15	Loss 1.0088 (1.0121)
+2022-11-18 14:58:47,622:INFO: Dataset: univ                Batch:  6/15	Loss 1.0315 (1.0151)
+2022-11-18 14:58:47,785:INFO: Dataset: univ                Batch:  7/15	Loss 1.0458 (1.0193)
+2022-11-18 14:58:47,945:INFO: Dataset: univ                Batch:  8/15	Loss 1.0267 (1.0202)
+2022-11-18 14:58:48,106:INFO: Dataset: univ                Batch:  9/15	Loss 1.0139 (1.0195)
+2022-11-18 14:58:48,265:INFO: Dataset: univ                Batch: 10/15	Loss 1.0360 (1.0211)
+2022-11-18 14:58:48,431:INFO: Dataset: univ                Batch: 11/15	Loss 1.0266 (1.0216)
+2022-11-18 14:58:48,591:INFO: Dataset: univ                Batch: 12/15	Loss 1.0004 (1.0197)
+2022-11-18 14:58:48,752:INFO: Dataset: univ                Batch: 13/15	Loss 0.9905 (1.0175)
+2022-11-18 14:58:48,914:INFO: Dataset: univ                Batch: 14/15	Loss 1.0126 (1.0172)
+2022-11-18 14:58:48,998:INFO: Dataset: univ                Batch: 15/15	Loss 0.9855 (1.0167)
+2022-11-18 14:58:49,389:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0675 (1.0675)
+2022-11-18 14:58:49,536:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0731 (1.0702)
+2022-11-18 14:58:49,691:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0776 (1.0727)
+2022-11-18 14:58:49,841:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0222 (1.0604)
+2022-11-18 14:58:49,998:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9651 (1.0427)
+2022-11-18 14:58:50,149:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0873 (1.0504)
+2022-11-18 14:58:50,300:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0277 (1.0470)
+2022-11-18 14:58:50,437:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0254 (1.0450)
+2022-11-18 14:58:50,831:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0517 (1.0517)
+2022-11-18 14:58:50,986:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0059 (1.0290)
+2022-11-18 14:58:51,139:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9762 (1.0107)
+2022-11-18 14:58:51,289:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9789 (1.0027)
+2022-11-18 14:58:51,440:INFO: Dataset: zara2               Batch:  5/18	Loss 0.9826 (0.9989)
+2022-11-18 14:58:51,592:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0090 (1.0007)
+2022-11-18 14:58:51,741:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0053 (1.0014)
+2022-11-18 14:58:51,888:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0494 (1.0078)
+2022-11-18 14:58:52,036:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0579 (1.0133)
+2022-11-18 14:58:52,183:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0593 (1.0175)
+2022-11-18 14:58:52,334:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0241 (1.0181)
+2022-11-18 14:58:52,481:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0413 (1.0200)
+2022-11-18 14:58:52,629:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0148 (1.0197)
+2022-11-18 14:58:52,777:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0246 (1.0200)
+2022-11-18 14:58:52,942:INFO: Dataset: zara2               Batch: 15/18	Loss 0.9854 (1.0179)
+2022-11-18 14:58:53,091:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0312 (1.0188)
+2022-11-18 14:58:53,239:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0162 (1.0186)
+2022-11-18 14:58:53,378:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0818 (1.0219)
+2022-11-18 14:58:53,425:INFO: - Computing loss (validation)
+2022-11-18 14:58:53,715:INFO: Dataset: hotel               Batch: 1/2	Loss 1.0547 (1.0547)
+2022-11-18 14:58:53,750:INFO: Dataset: hotel               Batch: 2/2	Loss 0.9981 (1.0494)
+2022-11-18 14:58:54,087:INFO: Dataset: univ                Batch: 1/3	Loss 1.0110 (1.0110)
+2022-11-18 14:58:54,170:INFO: Dataset: univ                Batch: 2/3	Loss 1.0116 (1.0113)
+2022-11-18 14:58:54,257:INFO: Dataset: univ                Batch: 3/3	Loss 1.0249 (1.0151)
+2022-11-18 14:58:54,565:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0421 (1.0421)
+2022-11-18 14:58:54,611:INFO: Dataset: zara1               Batch: 2/2	Loss 1.0222 (1.0371)
+2022-11-18 14:58:54,909:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0493 (1.0493)
+2022-11-18 14:58:54,982:INFO: Dataset: zara2               Batch: 2/5	Loss 1.0147 (1.0320)
+2022-11-18 14:58:55,054:INFO: Dataset: zara2               Batch: 3/5	Loss 1.0357 (1.0332)
+2022-11-18 14:58:55,131:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9847 (1.0214)
+2022-11-18 14:58:55,204:INFO: Dataset: zara2               Batch: 5/5	Loss 1.0410 (1.0253)
+2022-11-18 14:58:55,258:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_251.pth.tar
+2022-11-18 14:58:55,258:INFO: 
+===> EPOCH: 252 (P2)
+2022-11-18 14:58:55,259:INFO: - Computing loss (training)
+2022-11-18 14:58:55,669:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1057 (1.1057)
+2022-11-18 14:58:55,876:INFO: Dataset: hotel               Batch: 2/4	Loss 1.0695 (1.0878)
+2022-11-18 14:58:56,074:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0655 (1.0805)
+2022-11-18 14:58:56,200:INFO: Dataset: hotel               Batch: 4/4	Loss 1.0411 (1.0744)
+2022-11-18 14:58:56,605:INFO: Dataset: univ                Batch:  1/15	Loss 0.9860 (0.9860)
+2022-11-18 14:58:56,769:INFO: Dataset: univ                Batch:  2/15	Loss 1.0093 (0.9968)
+2022-11-18 14:58:56,946:INFO: Dataset: univ                Batch:  3/15	Loss 1.0497 (1.0145)
+2022-11-18 14:58:57,119:INFO: Dataset: univ                Batch:  4/15	Loss 1.0468 (1.0231)
+2022-11-18 14:58:57,283:INFO: Dataset: univ                Batch:  5/15	Loss 1.0378 (1.0260)
+2022-11-18 14:58:57,438:INFO: Dataset: univ                Batch:  6/15	Loss 1.0353 (1.0275)
+2022-11-18 14:58:57,593:INFO: Dataset: univ                Batch:  7/15	Loss 1.0487 (1.0302)
+2022-11-18 14:58:57,751:INFO: Dataset: univ                Batch:  8/15	Loss 1.0307 (1.0303)
+2022-11-18 14:58:57,924:INFO: Dataset: univ                Batch:  9/15	Loss 1.0539 (1.0328)
+2022-11-18 14:58:58,089:INFO: Dataset: univ                Batch: 10/15	Loss 0.9944 (1.0287)
+2022-11-18 14:58:58,258:INFO: Dataset: univ                Batch: 11/15	Loss 1.0216 (1.0281)
+2022-11-18 14:58:58,416:INFO: Dataset: univ                Batch: 12/15	Loss 1.0137 (1.0267)
+2022-11-18 14:58:58,570:INFO: Dataset: univ                Batch: 13/15	Loss 1.0438 (1.0279)
+2022-11-18 14:58:58,725:INFO: Dataset: univ                Batch: 14/15	Loss 1.0294 (1.0280)
+2022-11-18 14:58:58,814:INFO: Dataset: univ                Batch: 15/15	Loss 1.0380 (1.0281)
+2022-11-18 14:58:59,228:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0223 (1.0223)
+2022-11-18 14:58:59,381:INFO: Dataset: zara1               Batch: 2/8	Loss 0.9819 (1.0020)
+2022-11-18 14:58:59,534:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0281 (1.0110)
+2022-11-18 14:58:59,687:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0849 (1.0286)
+2022-11-18 14:58:59,858:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0041 (1.0242)
+2022-11-18 14:59:00,039:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1834 (1.0477)
+2022-11-18 14:59:00,234:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0224 (1.0438)
+2022-11-18 14:59:00,394:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1119 (1.0508)
+2022-11-18 14:59:00,843:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9909 (0.9909)
+2022-11-18 14:59:01,001:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0608 (1.0253)
+2022-11-18 14:59:01,155:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0739 (1.0419)
+2022-11-18 14:59:01,312:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0015 (1.0314)
+2022-11-18 14:59:01,472:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0447 (1.0341)
+2022-11-18 14:59:01,627:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0807 (1.0412)
+2022-11-18 14:59:01,780:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0707 (1.0453)
+2022-11-18 14:59:01,950:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0042 (1.0401)
+2022-11-18 14:59:02,105:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0196 (1.0378)
+2022-11-18 14:59:02,271:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9779 (1.0320)
+2022-11-18 14:59:02,534:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0116 (1.0303)
+2022-11-18 14:59:02,689:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0363 (1.0308)
+2022-11-18 14:59:02,850:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9876 (1.0274)
+2022-11-18 14:59:03,008:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0248 (1.0273)
+2022-11-18 14:59:03,240:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0505 (1.0286)
+2022-11-18 14:59:03,396:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0327 (1.0289)
+2022-11-18 14:59:03,552:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0340 (1.0292)
+2022-11-18 14:59:03,692:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0394 (1.0297)
+2022-11-18 14:59:03,745:INFO: - Computing loss (validation)
+2022-11-18 14:59:04,057:INFO: Dataset: hotel               Batch: 1/2	Loss 1.0292 (1.0292)
+2022-11-18 14:59:04,102:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2854 (1.0510)
+2022-11-18 14:59:04,422:INFO: Dataset: univ                Batch: 1/3	Loss 1.0272 (1.0272)
+2022-11-18 14:59:04,500:INFO: Dataset: univ                Batch: 2/3	Loss 1.0254 (1.0262)
+2022-11-18 14:59:04,571:INFO: Dataset: univ                Batch: 3/3	Loss 1.0333 (1.0285)
+2022-11-18 14:59:04,878:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0658 (1.0658)
+2022-11-18 14:59:04,926:INFO: Dataset: zara1               Batch: 2/2	Loss 1.1177 (1.0805)
+2022-11-18 14:59:05,237:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0057 (1.0057)
+2022-11-18 14:59:05,316:INFO: Dataset: zara2               Batch: 2/5	Loss 1.0210 (1.0131)
+2022-11-18 14:59:05,393:INFO: Dataset: zara2               Batch: 3/5	Loss 1.0342 (1.0200)
+2022-11-18 14:59:05,476:INFO: Dataset: zara2               Batch: 4/5	Loss 1.0123 (1.0181)
+2022-11-18 14:59:05,549:INFO: Dataset: zara2               Batch: 5/5	Loss 1.0366 (1.0219)
+2022-11-18 14:59:05,607:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_252.pth.tar
+2022-11-18 14:59:05,607:INFO: 
+===> EPOCH: 253 (P2)
+2022-11-18 14:59:05,608:INFO: - Computing loss (training)
+2022-11-18 14:59:05,941:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0884 (1.0884)
+2022-11-18 14:59:06,089:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1454 (1.1176)
+2022-11-18 14:59:06,240:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1034 (1.1129)
+2022-11-18 14:59:06,353:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1515 (1.1192)
+2022-11-18 14:59:06,743:INFO: Dataset: univ                Batch:  1/15	Loss 1.0611 (1.0611)
+2022-11-18 14:59:06,900:INFO: Dataset: univ                Batch:  2/15	Loss 1.0204 (1.0399)
+2022-11-18 14:59:07,060:INFO: Dataset: univ                Batch:  3/15	Loss 1.0538 (1.0443)
+2022-11-18 14:59:07,218:INFO: Dataset: univ                Batch:  4/15	Loss 1.0373 (1.0425)
+2022-11-18 14:59:07,374:INFO: Dataset: univ                Batch:  5/15	Loss 1.0382 (1.0417)
+2022-11-18 14:59:07,534:INFO: Dataset: univ                Batch:  6/15	Loss 1.0498 (1.0430)
+2022-11-18 14:59:07,693:INFO: Dataset: univ                Batch:  7/15	Loss 1.0514 (1.0441)
+2022-11-18 14:59:07,853:INFO: Dataset: univ                Batch:  8/15	Loss 1.0689 (1.0476)
+2022-11-18 14:59:08,013:INFO: Dataset: univ                Batch:  9/15	Loss 1.0536 (1.0482)
+2022-11-18 14:59:08,167:INFO: Dataset: univ                Batch: 10/15	Loss 1.0410 (1.0476)
+2022-11-18 14:59:08,335:INFO: Dataset: univ                Batch: 11/15	Loss 1.0364 (1.0465)
+2022-11-18 14:59:08,495:INFO: Dataset: univ                Batch: 12/15	Loss 1.0339 (1.0455)
+2022-11-18 14:59:08,653:INFO: Dataset: univ                Batch: 13/15	Loss 1.0512 (1.0460)
+2022-11-18 14:59:08,815:INFO: Dataset: univ                Batch: 14/15	Loss 1.0322 (1.0450)
+2022-11-18 14:59:08,903:INFO: Dataset: univ                Batch: 15/15	Loss 1.0095 (1.0445)
+2022-11-18 14:59:09,292:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0243 (1.0243)
+2022-11-18 14:59:09,438:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0698 (1.0483)
+2022-11-18 14:59:09,592:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0810 (1.0587)
+2022-11-18 14:59:09,751:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0855 (1.0657)
+2022-11-18 14:59:09,923:INFO: Dataset: zara1               Batch: 5/8	Loss 1.0799 (1.0687)
+2022-11-18 14:59:10,131:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0482 (1.0650)
+2022-11-18 14:59:10,339:INFO: Dataset: zara1               Batch: 7/8	Loss 1.0735 (1.0661)
+2022-11-18 14:59:10,476:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0837 (1.0680)
+2022-11-18 14:59:10,929:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0665 (1.0665)
+2022-11-18 14:59:11,081:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0601 (1.0630)
+2022-11-18 14:59:11,247:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0257 (1.0494)
+2022-11-18 14:59:11,401:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0507 (1.0497)
+2022-11-18 14:59:11,552:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0898 (1.0581)
+2022-11-18 14:59:11,715:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1066 (1.0656)
+2022-11-18 14:59:11,866:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0526 (1.0636)
+2022-11-18 14:59:12,013:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0140 (1.0566)
+2022-11-18 14:59:12,161:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0749 (1.0586)
+2022-11-18 14:59:12,315:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0544 (1.0582)
+2022-11-18 14:59:12,464:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0412 (1.0566)
+2022-11-18 14:59:12,614:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0745 (1.0581)
+2022-11-18 14:59:12,793:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0223 (1.0552)
+2022-11-18 14:59:12,958:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0088 (1.0519)
+2022-11-18 14:59:13,121:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0317 (1.0506)
+2022-11-18 14:59:13,285:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0577 (1.0510)
+2022-11-18 14:59:13,449:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0319 (1.0499)
+2022-11-18 14:59:13,592:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0697 (1.0508)
+2022-11-18 14:59:13,640:INFO: - Computing loss (validation)
+2022-11-18 14:59:13,903:INFO: Dataset: hotel               Batch: 1/2	Loss 1.1323 (1.1323)
+2022-11-18 14:59:13,938:INFO: Dataset: hotel               Batch: 2/2	Loss 0.9281 (1.1197)
+2022-11-18 14:59:14,245:INFO: Dataset: univ                Batch: 1/3	Loss 1.0596 (1.0596)
+2022-11-18 14:59:14,321:INFO: Dataset: univ                Batch: 2/3	Loss 1.0415 (1.0500)
+2022-11-18 14:59:14,394:INFO: Dataset: univ                Batch: 3/3	Loss 1.0800 (1.0594)
+2022-11-18 14:59:14,740:INFO: Dataset: zara1               Batch: 1/2	Loss 1.1396 (1.1396)
+2022-11-18 14:59:14,793:INFO: Dataset: zara1               Batch: 2/2	Loss 1.0915 (1.1275)
+2022-11-18 14:59:15,160:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0617 (1.0617)
+2022-11-18 14:59:15,239:INFO: Dataset: zara2               Batch: 2/5	Loss 1.0123 (1.0372)
+2022-11-18 14:59:15,315:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1133 (1.0622)
+2022-11-18 14:59:15,392:INFO: Dataset: zara2               Batch: 4/5	Loss 1.0595 (1.0615)
+2022-11-18 14:59:15,467:INFO: Dataset: zara2               Batch: 5/5	Loss 1.0820 (1.0653)
+2022-11-18 14:59:15,523:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_253.pth.tar
+2022-11-18 14:59:15,523:INFO: 
+===> EPOCH: 254 (P2)
+2022-11-18 14:59:15,524:INFO: - Computing loss (training)
+2022-11-18 14:59:15,878:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0850 (1.0850)
+2022-11-18 14:59:16,034:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1818 (1.1350)
+2022-11-18 14:59:16,188:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0843 (1.1173)
+2022-11-18 14:59:16,303:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2410 (1.1390)
+2022-11-18 14:59:16,717:INFO: Dataset: univ                Batch:  1/15	Loss 1.0688 (1.0688)
+2022-11-18 14:59:16,878:INFO: Dataset: univ                Batch:  2/15	Loss 1.0616 (1.0655)
+2022-11-18 14:59:17,042:INFO: Dataset: univ                Batch:  3/15	Loss 1.0639 (1.0650)
+2022-11-18 14:59:17,202:INFO: Dataset: univ                Batch:  4/15	Loss 1.0401 (1.0586)
+2022-11-18 14:59:17,364:INFO: Dataset: univ                Batch:  5/15	Loss 1.0814 (1.0630)
+2022-11-18 14:59:17,524:INFO: Dataset: univ                Batch:  6/15	Loss 1.0498 (1.0608)
+2022-11-18 14:59:17,681:INFO: Dataset: univ                Batch:  7/15	Loss 1.0755 (1.0625)
+2022-11-18 14:59:17,835:INFO: Dataset: univ                Batch:  8/15	Loss 1.0733 (1.0639)
+2022-11-18 14:59:17,991:INFO: Dataset: univ                Batch:  9/15	Loss 1.0752 (1.0653)
+2022-11-18 14:59:18,145:INFO: Dataset: univ                Batch: 10/15	Loss 1.0821 (1.0670)
+2022-11-18 14:59:18,315:INFO: Dataset: univ                Batch: 11/15	Loss 1.0549 (1.0659)
+2022-11-18 14:59:18,486:INFO: Dataset: univ                Batch: 12/15	Loss 1.0800 (1.0671)
+2022-11-18 14:59:18,658:INFO: Dataset: univ                Batch: 13/15	Loss 1.0816 (1.0682)
+2022-11-18 14:59:18,831:INFO: Dataset: univ                Batch: 14/15	Loss 1.0849 (1.0693)
+2022-11-18 14:59:18,923:INFO: Dataset: univ                Batch: 15/15	Loss 1.0778 (1.0694)
+2022-11-18 14:59:19,326:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0320 (1.0320)
+2022-11-18 14:59:19,476:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1005 (1.0668)
+2022-11-18 14:59:19,625:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0952 (1.0766)
+2022-11-18 14:59:19,774:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0386 (1.0676)
+2022-11-18 14:59:19,923:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1503 (1.0814)
+2022-11-18 14:59:20,079:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0981 (1.0842)
+2022-11-18 14:59:20,241:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1048 (1.0875)
+2022-11-18 14:59:20,391:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0674 (1.0850)
+2022-11-18 14:59:20,832:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0991 (1.0991)
+2022-11-18 14:59:20,981:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0867 (1.0928)
+2022-11-18 14:59:21,135:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0641 (1.0841)
+2022-11-18 14:59:21,290:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0314 (1.0716)
+2022-11-18 14:59:21,442:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0525 (1.0681)
+2022-11-18 14:59:21,599:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0864 (1.0709)
+2022-11-18 14:59:21,766:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0539 (1.0687)
+2022-11-18 14:59:21,921:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1025 (1.0730)
+2022-11-18 14:59:22,087:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0727 (1.0730)
+2022-11-18 14:59:22,240:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0447 (1.0700)
+2022-11-18 14:59:22,391:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0959 (1.0722)
+2022-11-18 14:59:22,546:INFO: Dataset: zara2               Batch: 12/18	Loss 1.0953 (1.0741)
+2022-11-18 14:59:22,703:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1042 (1.0761)
+2022-11-18 14:59:22,857:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0641 (1.0753)
+2022-11-18 14:59:23,010:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0505 (1.0737)
+2022-11-18 14:59:23,159:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1016 (1.0754)
+2022-11-18 14:59:23,311:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0948 (1.0766)
+2022-11-18 14:59:23,449:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0322 (1.0746)
+2022-11-18 14:59:23,494:INFO: - Computing loss (validation)
+2022-11-18 14:59:23,753:INFO: Dataset: hotel               Batch: 1/2	Loss 1.1628 (1.1628)
+2022-11-18 14:59:23,787:INFO: Dataset: hotel               Batch: 2/2	Loss 1.1183 (1.1593)
+2022-11-18 14:59:24,098:INFO: Dataset: univ                Batch: 1/3	Loss 1.0721 (1.0721)
+2022-11-18 14:59:24,177:INFO: Dataset: univ                Batch: 2/3	Loss 1.0810 (1.0767)
+2022-11-18 14:59:24,253:INFO: Dataset: univ                Batch: 3/3	Loss 1.0739 (1.0758)
+2022-11-18 14:59:24,567:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0359 (1.0359)
+2022-11-18 14:59:24,611:INFO: Dataset: zara1               Batch: 2/2	Loss 1.1090 (1.0538)
+2022-11-18 14:59:24,918:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0322 (1.0322)
+2022-11-18 14:59:24,997:INFO: Dataset: zara2               Batch: 2/5	Loss 1.0688 (1.0518)
+2022-11-18 14:59:25,074:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1170 (1.0754)
+2022-11-18 14:59:25,153:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1001 (1.0818)
+2022-11-18 14:59:25,230:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1314 (1.0916)
+2022-11-18 14:59:25,283:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_254.pth.tar
+2022-11-18 14:59:25,283:INFO: 
+===> EPOCH: 255 (P2)
+2022-11-18 14:59:25,284:INFO: - Computing loss (training)
+2022-11-18 14:59:25,634:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1664 (1.1664)
+2022-11-18 14:59:25,786:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1510 (1.1590)
+2022-11-18 14:59:25,935:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1937 (1.1707)
+2022-11-18 14:59:26,049:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1831 (1.1728)
+2022-11-18 14:59:26,456:INFO: Dataset: univ                Batch:  1/15	Loss 1.0877 (1.0877)
+2022-11-18 14:59:26,627:INFO: Dataset: univ                Batch:  2/15	Loss 1.0897 (1.0887)
+2022-11-18 14:59:26,799:INFO: Dataset: univ                Batch:  3/15	Loss 1.0638 (1.0807)
+2022-11-18 14:59:26,964:INFO: Dataset: univ                Batch:  4/15	Loss 1.0767 (1.0797)
+2022-11-18 14:59:27,131:INFO: Dataset: univ                Batch:  5/15	Loss 1.0904 (1.0819)
+2022-11-18 14:59:27,293:INFO: Dataset: univ                Batch:  6/15	Loss 1.1000 (1.0849)
+2022-11-18 14:59:27,449:INFO: Dataset: univ                Batch:  7/15	Loss 1.0603 (1.0810)
+2022-11-18 14:59:27,604:INFO: Dataset: univ                Batch:  8/15	Loss 1.0852 (1.0815)
+2022-11-18 14:59:27,762:INFO: Dataset: univ                Batch:  9/15	Loss 1.0709 (1.0803)
+2022-11-18 14:59:27,923:INFO: Dataset: univ                Batch: 10/15	Loss 1.0742 (1.0797)
+2022-11-18 14:59:28,085:INFO: Dataset: univ                Batch: 11/15	Loss 1.0900 (1.0808)
+2022-11-18 14:59:28,242:INFO: Dataset: univ                Batch: 12/15	Loss 1.0913 (1.0817)
+2022-11-18 14:59:28,408:INFO: Dataset: univ                Batch: 13/15	Loss 1.0775 (1.0813)
+2022-11-18 14:59:28,578:INFO: Dataset: univ                Batch: 14/15	Loss 1.0987 (1.0826)
+2022-11-18 14:59:28,663:INFO: Dataset: univ                Batch: 15/15	Loss 1.1341 (1.0833)
+2022-11-18 14:59:29,095:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1734 (1.1734)
+2022-11-18 14:59:29,287:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1126 (1.1399)
+2022-11-18 14:59:29,455:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1090 (1.1296)
+2022-11-18 14:59:29,604:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1396 (1.1321)
+2022-11-18 14:59:29,751:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1587 (1.1375)
+2022-11-18 14:59:29,904:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1067 (1.1324)
+2022-11-18 14:59:30,059:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1449 (1.1342)
+2022-11-18 14:59:30,194:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0965 (1.1298)
+2022-11-18 14:59:30,581:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1338 (1.1338)
+2022-11-18 14:59:30,735:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0786 (1.1073)
+2022-11-18 14:59:30,892:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0623 (1.0934)
+2022-11-18 14:59:31,038:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0662 (1.0860)
+2022-11-18 14:59:31,187:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1594 (1.1009)
+2022-11-18 14:59:31,338:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1025 (1.1011)
+2022-11-18 14:59:31,486:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0434 (1.0933)
+2022-11-18 14:59:31,634:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0839 (1.0921)
+2022-11-18 14:59:31,803:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0629 (1.0890)
+2022-11-18 14:59:31,963:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0111 (1.0816)
+2022-11-18 14:59:32,118:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0774 (1.0812)
+2022-11-18 14:59:32,280:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1104 (1.0836)
+2022-11-18 14:59:32,433:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0691 (1.0824)
+2022-11-18 14:59:32,593:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1000 (1.0837)
+2022-11-18 14:59:32,755:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1026 (1.0848)
+2022-11-18 14:59:32,908:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1139 (1.0868)
+2022-11-18 14:59:33,059:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0838 (1.0867)
+2022-11-18 14:59:33,206:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1403 (1.0890)
+2022-11-18 14:59:33,257:INFO: - Computing loss (validation)
+2022-11-18 14:59:33,524:INFO: Dataset: hotel               Batch: 1/2	Loss 1.1089 (1.1089)
+2022-11-18 14:59:33,558:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0800 (1.1068)
+2022-11-18 14:59:33,865:INFO: Dataset: univ                Batch: 1/3	Loss 1.0879 (1.0879)
+2022-11-18 14:59:33,941:INFO: Dataset: univ                Batch: 2/3	Loss 1.1218 (1.1075)
+2022-11-18 14:59:34,013:INFO: Dataset: univ                Batch: 3/3	Loss 1.0672 (1.0935)
+2022-11-18 14:59:34,317:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0803 (1.0803)
+2022-11-18 14:59:34,363:INFO: Dataset: zara1               Batch: 2/2	Loss 1.1983 (1.1095)
+2022-11-18 14:59:34,684:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0647 (1.0647)
+2022-11-18 14:59:34,757:INFO: Dataset: zara2               Batch: 2/5	Loss 1.0502 (1.0573)
+2022-11-18 14:59:34,830:INFO: Dataset: zara2               Batch: 3/5	Loss 1.0655 (1.0600)
+2022-11-18 14:59:34,903:INFO: Dataset: zara2               Batch: 4/5	Loss 1.0936 (1.0687)
+2022-11-18 14:59:34,979:INFO: Dataset: zara2               Batch: 5/5	Loss 1.0568 (1.0662)
+2022-11-18 14:59:35,031:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_255.pth.tar
+2022-11-18 14:59:35,031:INFO: 
+===> EPOCH: 256 (P2)
+2022-11-18 14:59:35,032:INFO: - Computing loss (training)
+2022-11-18 14:59:35,376:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1422 (1.1422)
+2022-11-18 14:59:35,530:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1900 (1.1663)
+2022-11-18 14:59:35,681:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1141 (1.1477)
+2022-11-18 14:59:35,797:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1802 (1.1535)
+2022-11-18 14:59:36,192:INFO: Dataset: univ                Batch:  1/15	Loss 1.0888 (1.0888)
+2022-11-18 14:59:36,358:INFO: Dataset: univ                Batch:  2/15	Loss 1.1021 (1.0959)
+2022-11-18 14:59:36,520:INFO: Dataset: univ                Batch:  3/15	Loss 1.0904 (1.0941)
+2022-11-18 14:59:36,683:INFO: Dataset: univ                Batch:  4/15	Loss 1.0771 (1.0901)
+2022-11-18 14:59:36,842:INFO: Dataset: univ                Batch:  5/15	Loss 1.1083 (1.0938)
+2022-11-18 14:59:36,997:INFO: Dataset: univ                Batch:  6/15	Loss 1.1141 (1.0974)
+2022-11-18 14:59:37,152:INFO: Dataset: univ                Batch:  7/15	Loss 1.1111 (1.0994)
+2022-11-18 14:59:37,306:INFO: Dataset: univ                Batch:  8/15	Loss 1.1174 (1.1016)
+2022-11-18 14:59:37,461:INFO: Dataset: univ                Batch:  9/15	Loss 1.1122 (1.1028)
+2022-11-18 14:59:37,613:INFO: Dataset: univ                Batch: 10/15	Loss 1.0987 (1.1024)
+2022-11-18 14:59:37,774:INFO: Dataset: univ                Batch: 11/15	Loss 1.1078 (1.1029)
+2022-11-18 14:59:37,928:INFO: Dataset: univ                Batch: 12/15	Loss 1.1215 (1.1044)
+2022-11-18 14:59:38,084:INFO: Dataset: univ                Batch: 13/15	Loss 1.0902 (1.1034)
+2022-11-18 14:59:38,239:INFO: Dataset: univ                Batch: 14/15	Loss 1.0792 (1.1017)
+2022-11-18 14:59:38,322:INFO: Dataset: univ                Batch: 15/15	Loss 1.1192 (1.1019)
+2022-11-18 14:59:38,721:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0777 (1.0777)
+2022-11-18 14:59:38,876:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1762 (1.1285)
+2022-11-18 14:59:39,027:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0867 (1.1131)
+2022-11-18 14:59:39,185:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1442 (1.1201)
+2022-11-18 14:59:39,344:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1068 (1.1172)
+2022-11-18 14:59:39,507:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0072 (1.0991)
+2022-11-18 14:59:39,669:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1360 (1.1045)
+2022-11-18 14:59:39,810:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1109 (1.1052)
+2022-11-18 14:59:40,264:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1327 (1.1327)
+2022-11-18 14:59:40,419:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0337 (1.0872)
+2022-11-18 14:59:40,574:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0621 (1.0790)
+2022-11-18 14:59:40,735:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0644 (1.0757)
+2022-11-18 14:59:40,896:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0968 (1.0798)
+2022-11-18 14:59:41,057:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0320 (1.0713)
+2022-11-18 14:59:41,221:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0206 (1.0634)
+2022-11-18 14:59:41,409:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1608 (1.0756)
+2022-11-18 14:59:41,592:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0670 (1.0747)
+2022-11-18 14:59:41,764:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1442 (1.0822)
+2022-11-18 14:59:41,936:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1234 (1.0856)
+2022-11-18 14:59:42,110:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1358 (1.0899)
+2022-11-18 14:59:42,271:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1384 (1.0933)
+2022-11-18 14:59:42,437:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1201 (1.0954)
+2022-11-18 14:59:42,596:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1028 (1.0959)
+2022-11-18 14:59:42,764:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1389 (1.0983)
+2022-11-18 14:59:42,920:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0999 (1.0984)
+2022-11-18 14:59:43,058:INFO: Dataset: zara2               Batch: 18/18	Loss 1.0295 (1.0952)
+2022-11-18 14:59:43,102:INFO: - Computing loss (validation)
+2022-11-18 14:59:43,368:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2528 (1.2528)
+2022-11-18 14:59:43,404:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2251 (1.2510)
+2022-11-18 14:59:43,727:INFO: Dataset: univ                Batch: 1/3	Loss 1.0823 (1.0823)
+2022-11-18 14:59:43,808:INFO: Dataset: univ                Batch: 2/3	Loss 1.1301 (1.1042)
+2022-11-18 14:59:43,888:INFO: Dataset: univ                Batch: 3/3	Loss 1.0947 (1.1010)
+2022-11-18 14:59:44,202:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0701 (1.0701)
+2022-11-18 14:59:44,246:INFO: Dataset: zara1               Batch: 2/2	Loss 1.1334 (1.0874)
+2022-11-18 14:59:44,555:INFO: Dataset: zara2               Batch: 1/5	Loss 1.1350 (1.1350)
+2022-11-18 14:59:44,632:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1130 (1.1241)
+2022-11-18 14:59:44,708:INFO: Dataset: zara2               Batch: 3/5	Loss 1.0678 (1.1062)
+2022-11-18 14:59:44,783:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1369 (1.1137)
+2022-11-18 14:59:44,858:INFO: Dataset: zara2               Batch: 5/5	Loss 1.0763 (1.1065)
+2022-11-18 14:59:44,912:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_256.pth.tar
+2022-11-18 14:59:44,912:INFO: 
+===> EPOCH: 257 (P2)
+2022-11-18 14:59:44,912:INFO: - Computing loss (training)
+2022-11-18 14:59:45,265:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1139 (1.1139)
+2022-11-18 14:59:45,423:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2154 (1.1641)
+2022-11-18 14:59:45,583:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1602 (1.1628)
+2022-11-18 14:59:45,703:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2164 (1.1722)
+2022-11-18 14:59:46,111:INFO: Dataset: univ                Batch:  1/15	Loss 1.1302 (1.1302)
+2022-11-18 14:59:46,276:INFO: Dataset: univ                Batch:  2/15	Loss 1.0941 (1.1117)
+2022-11-18 14:59:46,450:INFO: Dataset: univ                Batch:  3/15	Loss 1.1204 (1.1145)
+2022-11-18 14:59:46,607:INFO: Dataset: univ                Batch:  4/15	Loss 1.1011 (1.1111)
+2022-11-18 14:59:46,766:INFO: Dataset: univ                Batch:  5/15	Loss 1.1222 (1.1133)
+2022-11-18 14:59:46,924:INFO: Dataset: univ                Batch:  6/15	Loss 1.1035 (1.1118)
+2022-11-18 14:59:47,082:INFO: Dataset: univ                Batch:  7/15	Loss 1.0976 (1.1096)
+2022-11-18 14:59:47,243:INFO: Dataset: univ                Batch:  8/15	Loss 1.1216 (1.1112)
+2022-11-18 14:59:47,415:INFO: Dataset: univ                Batch:  9/15	Loss 1.1203 (1.1121)
+2022-11-18 14:59:47,590:INFO: Dataset: univ                Batch: 10/15	Loss 1.1012 (1.1112)
+2022-11-18 14:59:47,770:INFO: Dataset: univ                Batch: 11/15	Loss 1.1386 (1.1139)
+2022-11-18 14:59:47,951:INFO: Dataset: univ                Batch: 12/15	Loss 1.1141 (1.1139)
+2022-11-18 14:59:48,122:INFO: Dataset: univ                Batch: 13/15	Loss 1.1009 (1.1129)
+2022-11-18 14:59:48,298:INFO: Dataset: univ                Batch: 14/15	Loss 1.1162 (1.1131)
+2022-11-18 14:59:48,394:INFO: Dataset: univ                Batch: 15/15	Loss 1.1693 (1.1138)
+2022-11-18 14:59:48,850:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1292 (1.1292)
+2022-11-18 14:59:49,043:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1292 (1.1292)
+2022-11-18 14:59:49,216:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1460 (1.1350)
+2022-11-18 14:59:49,379:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1401 (1.1363)
+2022-11-18 14:59:49,528:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1669 (1.1427)
+2022-11-18 14:59:49,680:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1733 (1.1476)
+2022-11-18 14:59:49,828:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1549 (1.1487)
+2022-11-18 14:59:49,973:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1772 (1.1521)
+2022-11-18 14:59:50,450:INFO: Dataset: zara2               Batch:  1/18	Loss 1.0918 (1.0918)
+2022-11-18 14:59:50,618:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1617 (1.1287)
+2022-11-18 14:59:50,771:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1684 (1.1413)
+2022-11-18 14:59:50,934:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1788 (1.1511)
+2022-11-18 14:59:51,152:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1706 (1.1548)
+2022-11-18 14:59:51,391:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1320 (1.1511)
+2022-11-18 14:59:51,628:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1146 (1.1456)
+2022-11-18 14:59:51,781:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1284 (1.1436)
+2022-11-18 14:59:51,929:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0956 (1.1377)
+2022-11-18 14:59:52,075:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1661 (1.1405)
+2022-11-18 14:59:52,224:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1529 (1.1418)
+2022-11-18 14:59:52,377:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1454 (1.1421)
+2022-11-18 14:59:52,524:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1780 (1.1449)
+2022-11-18 14:59:52,670:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1111 (1.1422)
+2022-11-18 14:59:52,819:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1019 (1.1397)
+2022-11-18 14:59:52,964:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0848 (1.1357)
+2022-11-18 14:59:53,132:INFO: Dataset: zara2               Batch: 17/18	Loss 1.1308 (1.1354)
+2022-11-18 14:59:53,270:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1307 (1.1352)
+2022-11-18 14:59:53,315:INFO: - Computing loss (validation)
+2022-11-18 14:59:53,589:INFO: Dataset: hotel               Batch: 1/2	Loss 1.1656 (1.1656)
+2022-11-18 14:59:53,635:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0632 (1.1572)
+2022-11-18 14:59:53,956:INFO: Dataset: univ                Batch: 1/3	Loss 1.1219 (1.1219)
+2022-11-18 14:59:54,039:INFO: Dataset: univ                Batch: 2/3	Loss 1.1225 (1.1222)
+2022-11-18 14:59:54,114:INFO: Dataset: univ                Batch: 3/3	Loss 1.0985 (1.1147)
+2022-11-18 14:59:54,445:INFO: Dataset: zara1               Batch: 1/2	Loss 1.1013 (1.1013)
+2022-11-18 14:59:54,494:INFO: Dataset: zara1               Batch: 2/2	Loss 1.0978 (1.1005)
+2022-11-18 14:59:54,837:INFO: Dataset: zara2               Batch: 1/5	Loss 1.1409 (1.1409)
+2022-11-18 14:59:54,920:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1690 (1.1553)
+2022-11-18 14:59:55,005:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1602 (1.1569)
+2022-11-18 14:59:55,091:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1397 (1.1525)
+2022-11-18 14:59:55,174:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1003 (1.1420)
+2022-11-18 14:59:55,238:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_257.pth.tar
+2022-11-18 14:59:55,238:INFO: 
+===> EPOCH: 258 (P2)
+2022-11-18 14:59:55,239:INFO: - Computing loss (training)
+2022-11-18 14:59:55,578:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1941 (1.1941)
+2022-11-18 14:59:55,727:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1351 (1.1655)
+2022-11-18 14:59:55,876:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1939 (1.1751)
+2022-11-18 14:59:55,991:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2026 (1.1802)
+2022-11-18 14:59:56,411:INFO: Dataset: univ                Batch:  1/15	Loss 1.1557 (1.1557)
+2022-11-18 14:59:56,589:INFO: Dataset: univ                Batch:  2/15	Loss 1.1301 (1.1431)
+2022-11-18 14:59:56,753:INFO: Dataset: univ                Batch:  3/15	Loss 1.1460 (1.1439)
+2022-11-18 14:59:56,910:INFO: Dataset: univ                Batch:  4/15	Loss 1.1541 (1.1463)
+2022-11-18 14:59:57,068:INFO: Dataset: univ                Batch:  5/15	Loss 1.1267 (1.1425)
+2022-11-18 14:59:57,222:INFO: Dataset: univ                Batch:  6/15	Loss 1.1400 (1.1421)
+2022-11-18 14:59:57,379:INFO: Dataset: univ                Batch:  7/15	Loss 1.1577 (1.1441)
+2022-11-18 14:59:57,531:INFO: Dataset: univ                Batch:  8/15	Loss 1.1395 (1.1435)
+2022-11-18 14:59:57,685:INFO: Dataset: univ                Batch:  9/15	Loss 1.1320 (1.1424)
+2022-11-18 14:59:57,839:INFO: Dataset: univ                Batch: 10/15	Loss 1.1485 (1.1430)
+2022-11-18 14:59:57,996:INFO: Dataset: univ                Batch: 11/15	Loss 1.1296 (1.1417)
+2022-11-18 14:59:58,149:INFO: Dataset: univ                Batch: 12/15	Loss 1.1230 (1.1401)
+2022-11-18 14:59:58,304:INFO: Dataset: univ                Batch: 13/15	Loss 1.1136 (1.1380)
+2022-11-18 14:59:58,456:INFO: Dataset: univ                Batch: 14/15	Loss 1.1618 (1.1396)
+2022-11-18 14:59:58,537:INFO: Dataset: univ                Batch: 15/15	Loss 1.1288 (1.1396)
+2022-11-18 14:59:58,935:INFO: Dataset: zara1               Batch: 1/8	Loss 1.1198 (1.1198)
+2022-11-18 14:59:59,082:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1851 (1.1500)
+2022-11-18 14:59:59,233:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1221 (1.1408)
+2022-11-18 14:59:59,379:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2095 (1.1578)
+2022-11-18 14:59:59,532:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1747 (1.1611)
+2022-11-18 14:59:59,680:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2324 (1.1728)
+2022-11-18 14:59:59,828:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2032 (1.1769)
+2022-11-18 14:59:59,963:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1859 (1.1778)
+2022-11-18 15:00:00,345:INFO: Dataset: zara2               Batch:  1/18	Loss 1.1597 (1.1597)
+2022-11-18 15:00:00,498:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1161 (1.1389)
+2022-11-18 15:00:00,652:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1733 (1.1496)
+2022-11-18 15:00:00,801:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1380 (1.1466)
+2022-11-18 15:00:00,949:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1651 (1.1500)
+2022-11-18 15:00:01,099:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1642 (1.1526)
+2022-11-18 15:00:01,247:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2066 (1.1612)
+2022-11-18 15:00:01,393:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1644 (1.1616)
+2022-11-18 15:00:01,539:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1350 (1.1589)
+2022-11-18 15:00:01,683:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1500 (1.1579)
+2022-11-18 15:00:01,831:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0949 (1.1524)
+2022-11-18 15:00:01,978:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1421 (1.1515)
+2022-11-18 15:00:02,124:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1234 (1.1494)
+2022-11-18 15:00:02,272:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1467 (1.1492)
+2022-11-18 15:00:02,421:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2036 (1.1525)
+2022-11-18 15:00:02,567:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1614 (1.1531)
+2022-11-18 15:00:02,715:INFO: Dataset: zara2               Batch: 17/18	Loss 1.1867 (1.1549)
+2022-11-18 15:00:02,852:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1841 (1.1563)
+2022-11-18 15:00:02,899:INFO: - Computing loss (validation)
+2022-11-18 15:00:03,172:INFO: Dataset: hotel               Batch: 1/2	Loss 1.1859 (1.1859)
+2022-11-18 15:00:03,207:INFO: Dataset: hotel               Batch: 2/2	Loss 1.1778 (1.1853)
+2022-11-18 15:00:03,507:INFO: Dataset: univ                Batch: 1/3	Loss 1.1529 (1.1529)
+2022-11-18 15:00:03,589:INFO: Dataset: univ                Batch: 2/3	Loss 1.1052 (1.1277)
+2022-11-18 15:00:03,666:INFO: Dataset: univ                Batch: 3/3	Loss 1.1576 (1.1375)
+2022-11-18 15:00:03,981:INFO: Dataset: zara1               Batch: 1/2	Loss 1.1859 (1.1859)
+2022-11-18 15:00:04,028:INFO: Dataset: zara1               Batch: 2/2	Loss 1.1903 (1.1870)
+2022-11-18 15:00:04,340:INFO: Dataset: zara2               Batch: 1/5	Loss 1.1312 (1.1312)
+2022-11-18 15:00:04,415:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1409 (1.1362)
+2022-11-18 15:00:04,490:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1014 (1.1251)
+2022-11-18 15:00:04,564:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1473 (1.1308)
+2022-11-18 15:00:04,638:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1724 (1.1389)
+2022-11-18 15:00:04,691:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_258.pth.tar
+2022-11-18 15:00:04,691:INFO: 
+===> EPOCH: 259 (P2)
+2022-11-18 15:00:04,691:INFO: - Computing loss (training)
+2022-11-18 15:00:05,030:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2021 (1.2021)
+2022-11-18 15:00:05,179:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2240 (1.2128)
+2022-11-18 15:00:05,325:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2842 (1.2368)
+2022-11-18 15:00:05,437:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1962 (1.2299)
+2022-11-18 15:00:05,843:INFO: Dataset: univ                Batch:  1/15	Loss 1.1373 (1.1373)
+2022-11-18 15:00:05,997:INFO: Dataset: univ                Batch:  2/15	Loss 1.1448 (1.1408)
+2022-11-18 15:00:06,156:INFO: Dataset: univ                Batch:  3/15	Loss 1.1401 (1.1406)
+2022-11-18 15:00:06,308:INFO: Dataset: univ                Batch:  4/15	Loss 1.1481 (1.1423)
+2022-11-18 15:00:06,461:INFO: Dataset: univ                Batch:  5/15	Loss 1.1364 (1.1410)
+2022-11-18 15:00:06,614:INFO: Dataset: univ                Batch:  6/15	Loss 1.1667 (1.1453)
+2022-11-18 15:00:06,767:INFO: Dataset: univ                Batch:  7/15	Loss 1.1680 (1.1483)
+2022-11-18 15:00:06,918:INFO: Dataset: univ                Batch:  8/15	Loss 1.1581 (1.1495)
+2022-11-18 15:00:07,071:INFO: Dataset: univ                Batch:  9/15	Loss 1.1601 (1.1508)
+2022-11-18 15:00:07,220:INFO: Dataset: univ                Batch: 10/15	Loss 1.1651 (1.1521)
+2022-11-18 15:00:07,376:INFO: Dataset: univ                Batch: 11/15	Loss 1.1643 (1.1533)
+2022-11-18 15:00:07,526:INFO: Dataset: univ                Batch: 12/15	Loss 1.1403 (1.1523)
+2022-11-18 15:00:07,678:INFO: Dataset: univ                Batch: 13/15	Loss 1.1177 (1.1495)
+2022-11-18 15:00:07,831:INFO: Dataset: univ                Batch: 14/15	Loss 1.1599 (1.1502)
+2022-11-18 15:00:07,911:INFO: Dataset: univ                Batch: 15/15	Loss 1.1718 (1.1505)
+2022-11-18 15:00:08,303:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0993 (1.0993)
+2022-11-18 15:00:08,456:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1855 (1.1467)
+2022-11-18 15:00:08,604:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1801 (1.1581)
+2022-11-18 15:00:08,755:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2054 (1.1717)
+2022-11-18 15:00:08,906:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1604 (1.1695)
+2022-11-18 15:00:09,052:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2638 (1.1843)
+2022-11-18 15:00:09,201:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1261 (1.1758)
+2022-11-18 15:00:09,336:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2026 (1.1786)
+2022-11-18 15:00:09,725:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2029 (1.2029)
+2022-11-18 15:00:09,874:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1968 (1.2001)
+2022-11-18 15:00:10,025:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1889 (1.1961)
+2022-11-18 15:00:10,174:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1344 (1.1814)
+2022-11-18 15:00:10,327:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1501 (1.1746)
+2022-11-18 15:00:10,476:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1788 (1.1753)
+2022-11-18 15:00:10,624:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1664 (1.1739)
+2022-11-18 15:00:10,772:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1379 (1.1692)
+2022-11-18 15:00:10,921:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1661 (1.1689)
+2022-11-18 15:00:11,069:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2077 (1.1734)
+2022-11-18 15:00:11,220:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1500 (1.1712)
+2022-11-18 15:00:11,367:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1755 (1.1716)
+2022-11-18 15:00:11,515:INFO: Dataset: zara2               Batch: 13/18	Loss 1.0998 (1.1659)
+2022-11-18 15:00:11,662:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1840 (1.1671)
+2022-11-18 15:00:11,811:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1776 (1.1678)
+2022-11-18 15:00:11,959:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2101 (1.1702)
+2022-11-18 15:00:12,108:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2093 (1.1728)
+2022-11-18 15:00:12,245:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1362 (1.1711)
+2022-11-18 15:00:12,290:INFO: - Computing loss (validation)
+2022-11-18 15:00:12,563:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2038 (1.2038)
+2022-11-18 15:00:12,597:INFO: Dataset: hotel               Batch: 2/2	Loss 1.3268 (1.2122)
+2022-11-18 15:00:12,909:INFO: Dataset: univ                Batch: 1/3	Loss 1.2059 (1.2059)
+2022-11-18 15:00:12,990:INFO: Dataset: univ                Batch: 2/3	Loss 1.1541 (1.1800)
+2022-11-18 15:00:13,067:INFO: Dataset: univ                Batch: 3/3	Loss 1.1849 (1.1817)
+2022-11-18 15:00:13,366:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2275 (1.2275)
+2022-11-18 15:00:13,410:INFO: Dataset: zara1               Batch: 2/2	Loss 1.3605 (1.2622)
+2022-11-18 15:00:13,706:INFO: Dataset: zara2               Batch: 1/5	Loss 1.1687 (1.1687)
+2022-11-18 15:00:13,780:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1322 (1.1502)
+2022-11-18 15:00:13,854:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1646 (1.1547)
+2022-11-18 15:00:13,927:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1676 (1.1583)
+2022-11-18 15:00:14,001:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1410 (1.1547)
+2022-11-18 15:00:14,052:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_259.pth.tar
+2022-11-18 15:00:14,052:INFO: 
+===> EPOCH: 260 (P2)
+2022-11-18 15:00:14,053:INFO: - Computing loss (training)
+2022-11-18 15:00:14,385:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2774 (1.2774)
+2022-11-18 15:00:14,532:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2198 (1.2476)
+2022-11-18 15:00:14,679:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2549 (1.2500)
+2022-11-18 15:00:14,793:INFO: Dataset: hotel               Batch: 4/4	Loss 1.1620 (1.2346)
+2022-11-18 15:00:15,189:INFO: Dataset: univ                Batch:  1/15	Loss 1.1852 (1.1852)
+2022-11-18 15:00:15,344:INFO: Dataset: univ                Batch:  2/15	Loss 1.1950 (1.1898)
+2022-11-18 15:00:15,502:INFO: Dataset: univ                Batch:  3/15	Loss 1.2076 (1.1954)
+2022-11-18 15:00:15,661:INFO: Dataset: univ                Batch:  4/15	Loss 1.1574 (1.1862)
+2022-11-18 15:00:15,820:INFO: Dataset: univ                Batch:  5/15	Loss 1.1786 (1.1845)
+2022-11-18 15:00:15,978:INFO: Dataset: univ                Batch:  6/15	Loss 1.1739 (1.1826)
+2022-11-18 15:00:16,133:INFO: Dataset: univ                Batch:  7/15	Loss 1.1819 (1.1825)
+2022-11-18 15:00:16,286:INFO: Dataset: univ                Batch:  8/15	Loss 1.2118 (1.1863)
+2022-11-18 15:00:16,440:INFO: Dataset: univ                Batch:  9/15	Loss 1.1679 (1.1842)
+2022-11-18 15:00:16,593:INFO: Dataset: univ                Batch: 10/15	Loss 1.1656 (1.1823)
+2022-11-18 15:00:16,748:INFO: Dataset: univ                Batch: 11/15	Loss 1.1782 (1.1819)
+2022-11-18 15:00:16,979:INFO: Dataset: univ                Batch: 12/15	Loss 1.1509 (1.1791)
+2022-11-18 15:00:17,138:INFO: Dataset: univ                Batch: 13/15	Loss 1.1944 (1.1803)
+2022-11-18 15:00:17,297:INFO: Dataset: univ                Batch: 14/15	Loss 1.1848 (1.1806)
+2022-11-18 15:00:17,380:INFO: Dataset: univ                Batch: 15/15	Loss 1.0759 (1.1791)
+2022-11-18 15:00:17,773:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2339 (1.2339)
+2022-11-18 15:00:17,922:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2607 (1.2472)
+2022-11-18 15:00:18,074:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2067 (1.2338)
+2022-11-18 15:00:18,225:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2755 (1.2446)
+2022-11-18 15:00:18,378:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1963 (1.2348)
+2022-11-18 15:00:18,531:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2154 (1.2317)
+2022-11-18 15:00:18,684:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1725 (1.2225)
+2022-11-18 15:00:18,827:INFO: Dataset: zara1               Batch: 8/8	Loss 1.1229 (1.2124)
+2022-11-18 15:00:19,220:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2154 (1.2154)
+2022-11-18 15:00:19,372:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1531 (1.1836)
+2022-11-18 15:00:19,525:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2261 (1.1988)
+2022-11-18 15:00:19,671:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2206 (1.2037)
+2022-11-18 15:00:19,822:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1578 (1.1939)
+2022-11-18 15:00:19,973:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1586 (1.1872)
+2022-11-18 15:00:20,122:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2456 (1.1949)
+2022-11-18 15:00:20,269:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1869 (1.1939)
+2022-11-18 15:00:20,420:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1984 (1.1944)
+2022-11-18 15:00:20,568:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2197 (1.1970)
+2022-11-18 15:00:20,728:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1815 (1.1956)
+2022-11-18 15:00:20,877:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1977 (1.1957)
+2022-11-18 15:00:21,032:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2353 (1.1987)
+2022-11-18 15:00:21,183:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1417 (1.1946)
+2022-11-18 15:00:21,348:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1862 (1.1940)
+2022-11-18 15:00:21,499:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2102 (1.1950)
+2022-11-18 15:00:21,652:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2089 (1.1958)
+2022-11-18 15:00:21,789:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1456 (1.1932)
+2022-11-18 15:00:21,834:INFO: - Computing loss (validation)
+2022-11-18 15:00:22,093:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2227 (1.2227)
+2022-11-18 15:00:22,127:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0756 (1.2112)
+2022-11-18 15:00:22,463:INFO: Dataset: univ                Batch: 1/3	Loss 1.1877 (1.1877)
+2022-11-18 15:00:22,549:INFO: Dataset: univ                Batch: 2/3	Loss 1.1987 (1.1933)
+2022-11-18 15:00:22,624:INFO: Dataset: univ                Batch: 3/3	Loss 1.1974 (1.1944)
+2022-11-18 15:00:22,951:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2323 (1.2323)
+2022-11-18 15:00:22,998:INFO: Dataset: zara1               Batch: 2/2	Loss 1.1896 (1.2221)
+2022-11-18 15:00:23,319:INFO: Dataset: zara2               Batch: 1/5	Loss 1.1890 (1.1890)
+2022-11-18 15:00:23,397:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1869 (1.1879)
+2022-11-18 15:00:23,472:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1715 (1.1824)
+2022-11-18 15:00:23,546:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1314 (1.1700)
+2022-11-18 15:00:23,619:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1977 (1.1754)
+2022-11-18 15:00:23,671:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_260.pth.tar
+2022-11-18 15:00:23,671:INFO: 
+===> EPOCH: 261 (P2)
+2022-11-18 15:00:23,671:INFO: - Computing loss (training)
+2022-11-18 15:00:24,010:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3032 (1.3032)
+2022-11-18 15:00:24,159:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1794 (1.2432)
+2022-11-18 15:00:24,306:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2298 (1.2390)
+2022-11-18 15:00:24,418:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2483 (1.2405)
+2022-11-18 15:00:24,858:INFO: Dataset: univ                Batch:  1/15	Loss 1.1949 (1.1949)
+2022-11-18 15:00:25,036:INFO: Dataset: univ                Batch:  2/15	Loss 1.1875 (1.1913)
+2022-11-18 15:00:25,199:INFO: Dataset: univ                Batch:  3/15	Loss 1.1793 (1.1873)
+2022-11-18 15:00:25,364:INFO: Dataset: univ                Batch:  4/15	Loss 1.1982 (1.1897)
+2022-11-18 15:00:25,525:INFO: Dataset: univ                Batch:  5/15	Loss 1.1978 (1.1911)
+2022-11-18 15:00:25,683:INFO: Dataset: univ                Batch:  6/15	Loss 1.1591 (1.1859)
+2022-11-18 15:00:25,839:INFO: Dataset: univ                Batch:  7/15	Loss 1.2122 (1.1897)
+2022-11-18 15:00:25,992:INFO: Dataset: univ                Batch:  8/15	Loss 1.1909 (1.1899)
+2022-11-18 15:00:26,148:INFO: Dataset: univ                Batch:  9/15	Loss 1.1934 (1.1903)
+2022-11-18 15:00:26,303:INFO: Dataset: univ                Batch: 10/15	Loss 1.1868 (1.1899)
+2022-11-18 15:00:26,460:INFO: Dataset: univ                Batch: 11/15	Loss 1.2108 (1.1917)
+2022-11-18 15:00:26,619:INFO: Dataset: univ                Batch: 12/15	Loss 1.1851 (1.1911)
+2022-11-18 15:00:26,778:INFO: Dataset: univ                Batch: 13/15	Loss 1.1748 (1.1899)
+2022-11-18 15:00:26,934:INFO: Dataset: univ                Batch: 14/15	Loss 1.1590 (1.1877)
+2022-11-18 15:00:27,019:INFO: Dataset: univ                Batch: 15/15	Loss 1.1858 (1.1877)
+2022-11-18 15:00:27,406:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2155 (1.2155)
+2022-11-18 15:00:27,557:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2476 (1.2323)
+2022-11-18 15:00:27,712:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1692 (1.2118)
+2022-11-18 15:00:27,883:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2604 (1.2228)
+2022-11-18 15:00:28,075:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2160 (1.2215)
+2022-11-18 15:00:28,241:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1326 (1.2087)
+2022-11-18 15:00:28,412:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2456 (1.2137)
+2022-11-18 15:00:28,555:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3188 (1.2238)
+2022-11-18 15:00:29,032:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2030 (1.2030)
+2022-11-18 15:00:29,182:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1745 (1.1880)
+2022-11-18 15:00:29,335:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1990 (1.1918)
+2022-11-18 15:00:29,500:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1920 (1.1918)
+2022-11-18 15:00:29,673:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2076 (1.1950)
+2022-11-18 15:00:29,843:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2233 (1.2001)
+2022-11-18 15:00:30,022:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1778 (1.1970)
+2022-11-18 15:00:30,187:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1799 (1.1948)
+2022-11-18 15:00:30,350:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1998 (1.1953)
+2022-11-18 15:00:30,520:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2310 (1.1993)
+2022-11-18 15:00:30,690:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1757 (1.1972)
+2022-11-18 15:00:30,840:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1869 (1.1963)
+2022-11-18 15:00:30,993:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1917 (1.1959)
+2022-11-18 15:00:31,143:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1309 (1.1912)
+2022-11-18 15:00:31,298:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1935 (1.1913)
+2022-11-18 15:00:31,463:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2213 (1.1932)
+2022-11-18 15:00:31,631:INFO: Dataset: zara2               Batch: 17/18	Loss 1.1878 (1.1928)
+2022-11-18 15:00:31,772:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1962 (1.1930)
+2022-11-18 15:00:31,820:INFO: - Computing loss (validation)
+2022-11-18 15:00:32,124:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2175 (1.2175)
+2022-11-18 15:00:32,160:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5984 (1.2357)
+2022-11-18 15:00:32,488:INFO: Dataset: univ                Batch: 1/3	Loss 1.1635 (1.1635)
+2022-11-18 15:00:32,565:INFO: Dataset: univ                Batch: 2/3	Loss 1.1760 (1.1693)
+2022-11-18 15:00:32,635:INFO: Dataset: univ                Batch: 3/3	Loss 1.2178 (1.1812)
+2022-11-18 15:00:32,930:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2667 (1.2667)
+2022-11-18 15:00:32,975:INFO: Dataset: zara1               Batch: 2/2	Loss 1.2534 (1.2634)
+2022-11-18 15:00:33,305:INFO: Dataset: zara2               Batch: 1/5	Loss 1.2171 (1.2171)
+2022-11-18 15:00:33,383:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1674 (1.1934)
+2022-11-18 15:00:33,462:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2247 (1.2036)
+2022-11-18 15:00:33,542:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1857 (1.1992)
+2022-11-18 15:00:33,619:INFO: Dataset: zara2               Batch: 5/5	Loss 1.2236 (1.2041)
+2022-11-18 15:00:33,679:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_261.pth.tar
+2022-11-18 15:00:33,680:INFO: 
+===> EPOCH: 262 (P2)
+2022-11-18 15:00:33,680:INFO: - Computing loss (training)
+2022-11-18 15:00:34,073:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3350 (1.3350)
+2022-11-18 15:00:34,228:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2237 (1.2794)
+2022-11-18 15:00:34,378:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2441 (1.2669)
+2022-11-18 15:00:34,493:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2458 (1.2633)
+2022-11-18 15:00:34,893:INFO: Dataset: univ                Batch:  1/15	Loss 1.1958 (1.1958)
+2022-11-18 15:00:35,051:INFO: Dataset: univ                Batch:  2/15	Loss 1.2256 (1.2108)
+2022-11-18 15:00:35,208:INFO: Dataset: univ                Batch:  3/15	Loss 1.2441 (1.2215)
+2022-11-18 15:00:35,368:INFO: Dataset: univ                Batch:  4/15	Loss 1.2181 (1.2206)
+2022-11-18 15:00:35,523:INFO: Dataset: univ                Batch:  5/15	Loss 1.1785 (1.2121)
+2022-11-18 15:00:35,677:INFO: Dataset: univ                Batch:  6/15	Loss 1.2408 (1.2163)
+2022-11-18 15:00:35,831:INFO: Dataset: univ                Batch:  7/15	Loss 1.2120 (1.2157)
+2022-11-18 15:00:36,001:INFO: Dataset: univ                Batch:  8/15	Loss 1.2176 (1.2159)
+2022-11-18 15:00:36,173:INFO: Dataset: univ                Batch:  9/15	Loss 1.2260 (1.2170)
+2022-11-18 15:00:36,339:INFO: Dataset: univ                Batch: 10/15	Loss 1.1985 (1.2152)
+2022-11-18 15:00:36,495:INFO: Dataset: univ                Batch: 11/15	Loss 1.2104 (1.2147)
+2022-11-18 15:00:36,647:INFO: Dataset: univ                Batch: 12/15	Loss 1.2264 (1.2157)
+2022-11-18 15:00:36,801:INFO: Dataset: univ                Batch: 13/15	Loss 1.2099 (1.2153)
+2022-11-18 15:00:36,955:INFO: Dataset: univ                Batch: 14/15	Loss 1.2249 (1.2160)
+2022-11-18 15:00:37,035:INFO: Dataset: univ                Batch: 15/15	Loss 1.2191 (1.2161)
+2022-11-18 15:00:37,423:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2294 (1.2294)
+2022-11-18 15:00:37,571:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2245 (1.2270)
+2022-11-18 15:00:37,719:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2707 (1.2409)
+2022-11-18 15:00:37,869:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2825 (1.2512)
+2022-11-18 15:00:38,024:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1984 (1.2407)
+2022-11-18 15:00:38,190:INFO: Dataset: zara1               Batch: 6/8	Loss 1.1891 (1.2318)
+2022-11-18 15:00:38,344:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2737 (1.2382)
+2022-11-18 15:00:38,482:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2664 (1.2420)
+2022-11-18 15:00:38,888:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2255 (1.2255)
+2022-11-18 15:00:39,040:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1607 (1.1914)
+2022-11-18 15:00:39,218:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2380 (1.2061)
+2022-11-18 15:00:39,391:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2322 (1.2118)
+2022-11-18 15:00:39,555:INFO: Dataset: zara2               Batch:  5/18	Loss 1.1642 (1.2029)
+2022-11-18 15:00:39,708:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2110 (1.2041)
+2022-11-18 15:00:39,861:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2338 (1.2084)
+2022-11-18 15:00:40,045:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2655 (1.2153)
+2022-11-18 15:00:40,237:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2076 (1.2145)
+2022-11-18 15:00:40,437:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2328 (1.2162)
+2022-11-18 15:00:40,638:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2037 (1.2150)
+2022-11-18 15:00:40,811:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1406 (1.2089)
+2022-11-18 15:00:40,970:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1936 (1.2077)
+2022-11-18 15:00:41,122:INFO: Dataset: zara2               Batch: 14/18	Loss 1.2194 (1.2084)
+2022-11-18 15:00:41,277:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2705 (1.2128)
+2022-11-18 15:00:41,448:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1731 (1.2104)
+2022-11-18 15:00:41,628:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2209 (1.2110)
+2022-11-18 15:00:41,790:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2695 (1.2137)
+2022-11-18 15:00:41,854:INFO: - Computing loss (validation)
+2022-11-18 15:00:42,162:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2598 (1.2598)
+2022-11-18 15:00:42,200:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0619 (1.2463)
+2022-11-18 15:00:42,534:INFO: Dataset: univ                Batch: 1/3	Loss 1.2074 (1.2074)
+2022-11-18 15:00:42,614:INFO: Dataset: univ                Batch: 2/3	Loss 1.1979 (1.2024)
+2022-11-18 15:00:42,687:INFO: Dataset: univ                Batch: 3/3	Loss 1.2089 (1.2042)
+2022-11-18 15:00:42,993:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2231 (1.2231)
+2022-11-18 15:00:43,039:INFO: Dataset: zara1               Batch: 2/2	Loss 1.3364 (1.2515)
+2022-11-18 15:00:43,345:INFO: Dataset: zara2               Batch: 1/5	Loss 1.2160 (1.2160)
+2022-11-18 15:00:43,421:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1907 (1.2035)
+2022-11-18 15:00:43,495:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2310 (1.2126)
+2022-11-18 15:00:43,568:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1845 (1.2055)
+2022-11-18 15:00:43,644:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1930 (1.2031)
+2022-11-18 15:00:43,697:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_262.pth.tar
+2022-11-18 15:00:43,697:INFO: 
+===> EPOCH: 263 (P2)
+2022-11-18 15:00:43,698:INFO: - Computing loss (training)
+2022-11-18 15:00:44,049:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3121 (1.3121)
+2022-11-18 15:00:44,203:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2530 (1.2826)
+2022-11-18 15:00:44,371:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3290 (1.2980)
+2022-11-18 15:00:44,515:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2996 (1.2983)
+2022-11-18 15:00:44,952:INFO: Dataset: univ                Batch:  1/15	Loss 1.2612 (1.2612)
+2022-11-18 15:00:45,115:INFO: Dataset: univ                Batch:  2/15	Loss 1.2198 (1.2398)
+2022-11-18 15:00:45,276:INFO: Dataset: univ                Batch:  3/15	Loss 1.2478 (1.2425)
+2022-11-18 15:00:45,440:INFO: Dataset: univ                Batch:  4/15	Loss 1.2428 (1.2425)
+2022-11-18 15:00:45,603:INFO: Dataset: univ                Batch:  5/15	Loss 1.2021 (1.2343)
+2022-11-18 15:00:45,771:INFO: Dataset: univ                Batch:  6/15	Loss 1.2411 (1.2354)
+2022-11-18 15:00:45,951:INFO: Dataset: univ                Batch:  7/15	Loss 1.2279 (1.2342)
+2022-11-18 15:00:46,120:INFO: Dataset: univ                Batch:  8/15	Loss 1.2566 (1.2369)
+2022-11-18 15:00:46,278:INFO: Dataset: univ                Batch:  9/15	Loss 1.2164 (1.2348)
+2022-11-18 15:00:46,436:INFO: Dataset: univ                Batch: 10/15	Loss 1.2202 (1.2333)
+2022-11-18 15:00:46,592:INFO: Dataset: univ                Batch: 11/15	Loss 1.2228 (1.2323)
+2022-11-18 15:00:46,746:INFO: Dataset: univ                Batch: 12/15	Loss 1.2286 (1.2320)
+2022-11-18 15:00:46,904:INFO: Dataset: univ                Batch: 13/15	Loss 1.2265 (1.2316)
+2022-11-18 15:00:47,068:INFO: Dataset: univ                Batch: 14/15	Loss 1.2582 (1.2336)
+2022-11-18 15:00:47,154:INFO: Dataset: univ                Batch: 15/15	Loss 1.2220 (1.2334)
+2022-11-18 15:00:47,564:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2625 (1.2625)
+2022-11-18 15:00:47,723:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3034 (1.2834)
+2022-11-18 15:00:47,903:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3028 (1.2893)
+2022-11-18 15:00:48,104:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2220 (1.2707)
+2022-11-18 15:00:48,287:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2696 (1.2705)
+2022-11-18 15:00:48,465:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3108 (1.2768)
+2022-11-18 15:00:48,640:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2409 (1.2722)
+2022-11-18 15:00:48,793:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2473 (1.2692)
+2022-11-18 15:00:49,184:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2369 (1.2369)
+2022-11-18 15:00:49,356:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2517 (1.2441)
+2022-11-18 15:00:49,512:INFO: Dataset: zara2               Batch:  3/18	Loss 1.1949 (1.2270)
+2022-11-18 15:00:49,690:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1639 (1.2127)
+2022-11-18 15:00:49,873:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2816 (1.2246)
+2022-11-18 15:00:50,040:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1572 (1.2136)
+2022-11-18 15:00:50,203:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2231 (1.2151)
+2022-11-18 15:00:50,359:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2327 (1.2173)
+2022-11-18 15:00:50,509:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2370 (1.2193)
+2022-11-18 15:00:50,660:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2299 (1.2203)
+2022-11-18 15:00:50,815:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1718 (1.2159)
+2022-11-18 15:00:50,973:INFO: Dataset: zara2               Batch: 12/18	Loss 1.2428 (1.2183)
+2022-11-18 15:00:51,138:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2095 (1.2176)
+2022-11-18 15:00:51,305:INFO: Dataset: zara2               Batch: 14/18	Loss 1.2137 (1.2173)
+2022-11-18 15:00:51,467:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2598 (1.2202)
+2022-11-18 15:00:51,617:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2025 (1.2190)
+2022-11-18 15:00:51,770:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2361 (1.2199)
+2022-11-18 15:00:51,915:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2140 (1.2197)
+2022-11-18 15:00:51,959:INFO: - Computing loss (validation)
+2022-11-18 15:00:52,223:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2038 (1.2038)
+2022-11-18 15:00:52,257:INFO: Dataset: hotel               Batch: 2/2	Loss 1.3162 (1.2104)
+2022-11-18 15:00:52,576:INFO: Dataset: univ                Batch: 1/3	Loss 1.2644 (1.2644)
+2022-11-18 15:00:52,659:INFO: Dataset: univ                Batch: 2/3	Loss 1.2209 (1.2413)
+2022-11-18 15:00:52,738:INFO: Dataset: univ                Batch: 3/3	Loss 1.2719 (1.2510)
+2022-11-18 15:00:53,067:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2785 (1.2785)
+2022-11-18 15:00:53,117:INFO: Dataset: zara1               Batch: 2/2	Loss 1.2963 (1.2824)
+2022-11-18 15:00:53,465:INFO: Dataset: zara2               Batch: 1/5	Loss 1.2208 (1.2208)
+2022-11-18 15:00:53,558:INFO: Dataset: zara2               Batch: 2/5	Loss 1.2480 (1.2342)
+2022-11-18 15:00:53,635:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1776 (1.2143)
+2022-11-18 15:00:53,711:INFO: Dataset: zara2               Batch: 4/5	Loss 1.2141 (1.2142)
+2022-11-18 15:00:53,788:INFO: Dataset: zara2               Batch: 5/5	Loss 1.2297 (1.2173)
+2022-11-18 15:00:53,842:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_263.pth.tar
+2022-11-18 15:00:53,843:INFO: 
+===> EPOCH: 264 (P2)
+2022-11-18 15:00:53,843:INFO: - Computing loss (training)
+2022-11-18 15:00:54,167:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3642 (1.3642)
+2022-11-18 15:00:54,315:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2996 (1.3309)
+2022-11-18 15:00:54,465:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2723 (1.3102)
+2022-11-18 15:00:54,584:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3524 (1.3179)
+2022-11-18 15:00:55,017:INFO: Dataset: univ                Batch:  1/15	Loss 1.2756 (1.2756)
+2022-11-18 15:00:55,173:INFO: Dataset: univ                Batch:  2/15	Loss 1.2783 (1.2769)
+2022-11-18 15:00:55,331:INFO: Dataset: univ                Batch:  3/15	Loss 1.2505 (1.2679)
+2022-11-18 15:00:55,497:INFO: Dataset: univ                Batch:  4/15	Loss 1.2383 (1.2604)
+2022-11-18 15:00:55,657:INFO: Dataset: univ                Batch:  5/15	Loss 1.2395 (1.2564)
+2022-11-18 15:00:55,824:INFO: Dataset: univ                Batch:  6/15	Loss 1.2520 (1.2556)
+2022-11-18 15:00:55,985:INFO: Dataset: univ                Batch:  7/15	Loss 1.2430 (1.2538)
+2022-11-18 15:00:56,140:INFO: Dataset: univ                Batch:  8/15	Loss 1.2745 (1.2565)
+2022-11-18 15:00:56,303:INFO: Dataset: univ                Batch:  9/15	Loss 1.2696 (1.2579)
+2022-11-18 15:00:56,476:INFO: Dataset: univ                Batch: 10/15	Loss 1.2701 (1.2592)
+2022-11-18 15:00:56,648:INFO: Dataset: univ                Batch: 11/15	Loss 1.2377 (1.2574)
+2022-11-18 15:00:56,813:INFO: Dataset: univ                Batch: 12/15	Loss 1.2565 (1.2573)
+2022-11-18 15:00:56,971:INFO: Dataset: univ                Batch: 13/15	Loss 1.2568 (1.2573)
+2022-11-18 15:00:57,157:INFO: Dataset: univ                Batch: 14/15	Loss 1.2582 (1.2573)
+2022-11-18 15:00:57,244:INFO: Dataset: univ                Batch: 15/15	Loss 1.3024 (1.2580)
+2022-11-18 15:00:57,643:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2411 (1.2411)
+2022-11-18 15:00:57,790:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2326 (1.2365)
+2022-11-18 15:00:57,946:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2920 (1.2546)
+2022-11-18 15:00:58,131:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2944 (1.2649)
+2022-11-18 15:00:58,300:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2412 (1.2603)
+2022-11-18 15:00:58,457:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3079 (1.2683)
+2022-11-18 15:00:58,610:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2495 (1.2657)
+2022-11-18 15:00:58,750:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3208 (1.2717)
+2022-11-18 15:00:59,229:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2266 (1.2266)
+2022-11-18 15:00:59,381:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2745 (1.2494)
+2022-11-18 15:00:59,532:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2245 (1.2418)
+2022-11-18 15:00:59,686:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2682 (1.2484)
+2022-11-18 15:00:59,842:INFO: Dataset: zara2               Batch:  5/18	Loss 1.3137 (1.2615)
+2022-11-18 15:01:00,018:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2715 (1.2631)
+2022-11-18 15:01:00,185:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3016 (1.2687)
+2022-11-18 15:01:00,337:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2399 (1.2649)
+2022-11-18 15:01:00,490:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2724 (1.2657)
+2022-11-18 15:01:00,643:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3327 (1.2717)
+2022-11-18 15:01:00,801:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2913 (1.2733)
+2022-11-18 15:01:00,969:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3400 (1.2787)
+2022-11-18 15:01:01,128:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2724 (1.2782)
+2022-11-18 15:01:01,289:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3235 (1.2814)
+2022-11-18 15:01:01,450:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1866 (1.2749)
+2022-11-18 15:01:01,621:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2067 (1.2704)
+2022-11-18 15:01:01,787:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2790 (1.2710)
+2022-11-18 15:01:01,941:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2912 (1.2719)
+2022-11-18 15:01:01,995:INFO: - Computing loss (validation)
+2022-11-18 15:01:02,280:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3132 (1.3132)
+2022-11-18 15:01:02,314:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2856 (1.3111)
+2022-11-18 15:01:02,634:INFO: Dataset: univ                Batch: 1/3	Loss 1.2528 (1.2528)
+2022-11-18 15:01:02,714:INFO: Dataset: univ                Batch: 2/3	Loss 1.2720 (1.2629)
+2022-11-18 15:01:02,789:INFO: Dataset: univ                Batch: 3/3	Loss 1.2520 (1.2595)
+2022-11-18 15:01:03,101:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2875 (1.2875)
+2022-11-18 15:01:03,149:INFO: Dataset: zara1               Batch: 2/2	Loss 1.3345 (1.2993)
+2022-11-18 15:01:03,488:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3296 (1.3296)
+2022-11-18 15:01:03,562:INFO: Dataset: zara2               Batch: 2/5	Loss 1.2477 (1.2893)
+2022-11-18 15:01:03,636:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2569 (1.2785)
+2022-11-18 15:01:03,711:INFO: Dataset: zara2               Batch: 4/5	Loss 1.2989 (1.2834)
+2022-11-18 15:01:03,787:INFO: Dataset: zara2               Batch: 5/5	Loss 1.2256 (1.2720)
+2022-11-18 15:01:03,842:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_264.pth.tar
+2022-11-18 15:01:03,842:INFO: 
+===> EPOCH: 265 (P2)
+2022-11-18 15:01:03,843:INFO: - Computing loss (training)
+2022-11-18 15:01:04,197:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3167 (1.3167)
+2022-11-18 15:01:04,352:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3654 (1.3406)
+2022-11-18 15:01:04,510:INFO: Dataset: hotel               Batch: 3/4	Loss 1.2983 (1.3269)
+2022-11-18 15:01:04,636:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3938 (1.3384)
+2022-11-18 15:01:05,074:INFO: Dataset: univ                Batch:  1/15	Loss 1.2447 (1.2447)
+2022-11-18 15:01:05,240:INFO: Dataset: univ                Batch:  2/15	Loss 1.2789 (1.2629)
+2022-11-18 15:01:05,413:INFO: Dataset: univ                Batch:  3/15	Loss 1.2518 (1.2591)
+2022-11-18 15:01:05,584:INFO: Dataset: univ                Batch:  4/15	Loss 1.2812 (1.2648)
+2022-11-18 15:01:05,766:INFO: Dataset: univ                Batch:  5/15	Loss 1.2591 (1.2636)
+2022-11-18 15:01:05,942:INFO: Dataset: univ                Batch:  6/15	Loss 1.2914 (1.2676)
+2022-11-18 15:01:06,102:INFO: Dataset: univ                Batch:  7/15	Loss 1.2642 (1.2671)
+2022-11-18 15:01:06,267:INFO: Dataset: univ                Batch:  8/15	Loss 1.2707 (1.2676)
+2022-11-18 15:01:06,429:INFO: Dataset: univ                Batch:  9/15	Loss 1.2528 (1.2659)
+2022-11-18 15:01:06,593:INFO: Dataset: univ                Batch: 10/15	Loss 1.2892 (1.2680)
+2022-11-18 15:01:06,757:INFO: Dataset: univ                Batch: 11/15	Loss 1.2485 (1.2662)
+2022-11-18 15:01:06,926:INFO: Dataset: univ                Batch: 12/15	Loss 1.2427 (1.2643)
+2022-11-18 15:01:07,087:INFO: Dataset: univ                Batch: 13/15	Loss 1.2632 (1.2643)
+2022-11-18 15:01:07,244:INFO: Dataset: univ                Batch: 14/15	Loss 1.2597 (1.2639)
+2022-11-18 15:01:07,327:INFO: Dataset: univ                Batch: 15/15	Loss 1.2191 (1.2634)
+2022-11-18 15:01:07,740:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3461 (1.3461)
+2022-11-18 15:01:07,905:INFO: Dataset: zara1               Batch: 2/8	Loss 1.2769 (1.3106)
+2022-11-18 15:01:08,074:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3076 (1.3096)
+2022-11-18 15:01:08,230:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2711 (1.2999)
+2022-11-18 15:01:08,422:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2736 (1.2940)
+2022-11-18 15:01:08,618:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3030 (1.2955)
+2022-11-18 15:01:08,800:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3355 (1.3009)
+2022-11-18 15:01:08,974:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3860 (1.3098)
+2022-11-18 15:01:09,401:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2635 (1.2635)
+2022-11-18 15:01:09,563:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2235 (1.2433)
+2022-11-18 15:01:09,730:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2506 (1.2459)
+2022-11-18 15:01:09,890:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2861 (1.2565)
+2022-11-18 15:01:10,046:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2314 (1.2515)
+2022-11-18 15:01:10,206:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2615 (1.2533)
+2022-11-18 15:01:10,359:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3227 (1.2626)
+2022-11-18 15:01:10,514:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3370 (1.2714)
+2022-11-18 15:01:10,667:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3755 (1.2835)
+2022-11-18 15:01:10,816:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2897 (1.2841)
+2022-11-18 15:01:10,965:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2818 (1.2839)
+2022-11-18 15:01:11,114:INFO: Dataset: zara2               Batch: 12/18	Loss 1.2918 (1.2846)
+2022-11-18 15:01:11,268:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2210 (1.2796)
+2022-11-18 15:01:11,424:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3093 (1.2816)
+2022-11-18 15:01:11,577:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2327 (1.2783)
+2022-11-18 15:01:11,728:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2735 (1.2780)
+2022-11-18 15:01:11,881:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2846 (1.2784)
+2022-11-18 15:01:12,023:INFO: Dataset: zara2               Batch: 18/18	Loss 1.2859 (1.2787)
+2022-11-18 15:01:12,071:INFO: - Computing loss (validation)
+2022-11-18 15:01:12,365:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3370 (1.3370)
+2022-11-18 15:01:12,419:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2391 (1.3320)
+2022-11-18 15:01:12,781:INFO: Dataset: univ                Batch: 1/3	Loss 1.2428 (1.2428)
+2022-11-18 15:01:12,862:INFO: Dataset: univ                Batch: 2/3	Loss 1.2549 (1.2487)
+2022-11-18 15:01:12,938:INFO: Dataset: univ                Batch: 3/3	Loss 1.3052 (1.2691)
+2022-11-18 15:01:13,244:INFO: Dataset: zara1               Batch: 1/2	Loss 1.3025 (1.3025)
+2022-11-18 15:01:13,291:INFO: Dataset: zara1               Batch: 2/2	Loss 1.2326 (1.2868)
+2022-11-18 15:01:13,592:INFO: Dataset: zara2               Batch: 1/5	Loss 1.2489 (1.2489)
+2022-11-18 15:01:13,667:INFO: Dataset: zara2               Batch: 2/5	Loss 1.2338 (1.2407)
+2022-11-18 15:01:13,744:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2836 (1.2554)
+2022-11-18 15:01:13,818:INFO: Dataset: zara2               Batch: 4/5	Loss 1.2966 (1.2659)
+2022-11-18 15:01:13,892:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3079 (1.2742)
+2022-11-18 15:01:13,946:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_265.pth.tar
+2022-11-18 15:01:13,946:INFO: 
+===> EPOCH: 266 (P2)
+2022-11-18 15:01:13,947:INFO: - Computing loss (training)
+2022-11-18 15:01:14,294:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3864 (1.3864)
+2022-11-18 15:01:14,450:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3113 (1.3476)
+2022-11-18 15:01:14,605:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3109 (1.3357)
+2022-11-18 15:01:14,726:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4248 (1.3497)
+2022-11-18 15:01:15,158:INFO: Dataset: univ                Batch:  1/15	Loss 1.2542 (1.2542)
+2022-11-18 15:01:15,340:INFO: Dataset: univ                Batch:  2/15	Loss 1.2845 (1.2691)
+2022-11-18 15:01:15,521:INFO: Dataset: univ                Batch:  3/15	Loss 1.2811 (1.2731)
+2022-11-18 15:01:15,700:INFO: Dataset: univ                Batch:  4/15	Loss 1.3104 (1.2818)
+2022-11-18 15:01:15,873:INFO: Dataset: univ                Batch:  5/15	Loss 1.2571 (1.2766)
+2022-11-18 15:01:16,039:INFO: Dataset: univ                Batch:  6/15	Loss 1.2817 (1.2774)
+2022-11-18 15:01:16,202:INFO: Dataset: univ                Batch:  7/15	Loss 1.2974 (1.2802)
+2022-11-18 15:01:16,362:INFO: Dataset: univ                Batch:  8/15	Loss 1.2794 (1.2801)
+2022-11-18 15:01:16,522:INFO: Dataset: univ                Batch:  9/15	Loss 1.2580 (1.2775)
+2022-11-18 15:01:16,681:INFO: Dataset: univ                Batch: 10/15	Loss 1.3142 (1.2810)
+2022-11-18 15:01:16,862:INFO: Dataset: univ                Batch: 11/15	Loss 1.3272 (1.2852)
+2022-11-18 15:01:17,032:INFO: Dataset: univ                Batch: 12/15	Loss 1.2831 (1.2850)
+2022-11-18 15:01:17,204:INFO: Dataset: univ                Batch: 13/15	Loss 1.3164 (1.2875)
+2022-11-18 15:01:17,377:INFO: Dataset: univ                Batch: 14/15	Loss 1.3301 (1.2904)
+2022-11-18 15:01:17,474:INFO: Dataset: univ                Batch: 15/15	Loss 1.2692 (1.2901)
+2022-11-18 15:01:17,868:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3919 (1.3919)
+2022-11-18 15:01:18,032:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3186 (1.3535)
+2022-11-18 15:01:18,198:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3235 (1.3436)
+2022-11-18 15:01:18,360:INFO: Dataset: zara1               Batch: 4/8	Loss 1.2574 (1.3227)
+2022-11-18 15:01:18,535:INFO: Dataset: zara1               Batch: 5/8	Loss 1.2330 (1.3044)
+2022-11-18 15:01:18,712:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3391 (1.3101)
+2022-11-18 15:01:18,878:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2956 (1.3079)
+2022-11-18 15:01:19,028:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3375 (1.3108)
+2022-11-18 15:01:19,427:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2933 (1.2933)
+2022-11-18 15:01:19,583:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2656 (1.2793)
+2022-11-18 15:01:19,750:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3050 (1.2881)
+2022-11-18 15:01:19,907:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3188 (1.2960)
+2022-11-18 15:01:20,061:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2980 (1.2964)
+2022-11-18 15:01:20,221:INFO: Dataset: zara2               Batch:  6/18	Loss 1.3388 (1.3024)
+2022-11-18 15:01:20,383:INFO: Dataset: zara2               Batch:  7/18	Loss 1.2354 (1.2928)
+2022-11-18 15:01:20,539:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3846 (1.3039)
+2022-11-18 15:01:20,693:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3376 (1.3077)
+2022-11-18 15:01:20,852:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3604 (1.3128)
+2022-11-18 15:01:21,009:INFO: Dataset: zara2               Batch: 11/18	Loss 1.2580 (1.3080)
+2022-11-18 15:01:21,163:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3255 (1.3094)
+2022-11-18 15:01:21,315:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3053 (1.3091)
+2022-11-18 15:01:21,468:INFO: Dataset: zara2               Batch: 14/18	Loss 1.2543 (1.3050)
+2022-11-18 15:01:21,623:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3381 (1.3072)
+2022-11-18 15:01:21,786:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3289 (1.3084)
+2022-11-18 15:01:21,952:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3262 (1.3095)
+2022-11-18 15:01:22,100:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3359 (1.3107)
+2022-11-18 15:01:22,145:INFO: - Computing loss (validation)
+2022-11-18 15:01:22,420:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3369 (1.3369)
+2022-11-18 15:01:22,455:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4979 (1.3440)
+2022-11-18 15:01:22,769:INFO: Dataset: univ                Batch: 1/3	Loss 1.2955 (1.2955)
+2022-11-18 15:01:22,847:INFO: Dataset: univ                Batch: 2/3	Loss 1.2706 (1.2833)
+2022-11-18 15:01:22,921:INFO: Dataset: univ                Batch: 3/3	Loss 1.2767 (1.2811)
+2022-11-18 15:01:23,232:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2878 (1.2878)
+2022-11-18 15:01:23,279:INFO: Dataset: zara1               Batch: 2/2	Loss 1.3124 (1.2933)
+2022-11-18 15:01:23,591:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3301 (1.3301)
+2022-11-18 15:01:23,673:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3066 (1.3173)
+2022-11-18 15:01:23,754:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2718 (1.3020)
+2022-11-18 15:01:23,838:INFO: Dataset: zara2               Batch: 4/5	Loss 1.2765 (1.2960)
+2022-11-18 15:01:23,916:INFO: Dataset: zara2               Batch: 5/5	Loss 1.2541 (1.2878)
+2022-11-18 15:01:23,973:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_266.pth.tar
+2022-11-18 15:01:23,973:INFO: 
+===> EPOCH: 267 (P2)
+2022-11-18 15:01:23,974:INFO: - Computing loss (training)
+2022-11-18 15:01:24,328:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3058 (1.3058)
+2022-11-18 15:01:24,488:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2484 (1.2770)
+2022-11-18 15:01:24,646:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3889 (1.3139)
+2022-11-18 15:01:24,778:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3116 (1.3135)
+2022-11-18 15:01:25,223:INFO: Dataset: univ                Batch:  1/15	Loss 1.3009 (1.3009)
+2022-11-18 15:01:25,384:INFO: Dataset: univ                Batch:  2/15	Loss 1.3042 (1.3024)
+2022-11-18 15:01:25,546:INFO: Dataset: univ                Batch:  3/15	Loss 1.3048 (1.3032)
+2022-11-18 15:01:25,706:INFO: Dataset: univ                Batch:  4/15	Loss 1.3078 (1.3043)
+2022-11-18 15:01:25,867:INFO: Dataset: univ                Batch:  5/15	Loss 1.3120 (1.3060)
+2022-11-18 15:01:26,028:INFO: Dataset: univ                Batch:  6/15	Loss 1.3279 (1.3094)
+2022-11-18 15:01:26,190:INFO: Dataset: univ                Batch:  7/15	Loss 1.3024 (1.3083)
+2022-11-18 15:01:26,348:INFO: Dataset: univ                Batch:  8/15	Loss 1.3089 (1.3084)
+2022-11-18 15:01:26,507:INFO: Dataset: univ                Batch:  9/15	Loss 1.2931 (1.3068)
+2022-11-18 15:01:26,665:INFO: Dataset: univ                Batch: 10/15	Loss 1.3075 (1.3068)
+2022-11-18 15:01:26,827:INFO: Dataset: univ                Batch: 11/15	Loss 1.2685 (1.3035)
+2022-11-18 15:01:26,989:INFO: Dataset: univ                Batch: 12/15	Loss 1.3168 (1.3047)
+2022-11-18 15:01:27,151:INFO: Dataset: univ                Batch: 13/15	Loss 1.2838 (1.3032)
+2022-11-18 15:01:27,313:INFO: Dataset: univ                Batch: 14/15	Loss 1.3122 (1.3038)
+2022-11-18 15:01:27,400:INFO: Dataset: univ                Batch: 15/15	Loss 1.2946 (1.3037)
+2022-11-18 15:01:27,797:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2695 (1.2695)
+2022-11-18 15:01:27,957:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3012 (1.2852)
+2022-11-18 15:01:28,124:INFO: Dataset: zara1               Batch: 3/8	Loss 1.2737 (1.2813)
+2022-11-18 15:01:28,283:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3787 (1.3032)
+2022-11-18 15:01:28,446:INFO: Dataset: zara1               Batch: 5/8	Loss 1.3550 (1.3138)
+2022-11-18 15:01:28,603:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3957 (1.3275)
+2022-11-18 15:01:28,761:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3040 (1.3242)
+2022-11-18 15:01:28,904:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3469 (1.3268)
+2022-11-18 15:01:29,299:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2647 (1.2647)
+2022-11-18 15:01:29,456:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3479 (1.3063)
+2022-11-18 15:01:29,610:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3200 (1.3108)
+2022-11-18 15:01:29,768:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2703 (1.3005)
+2022-11-18 15:01:29,924:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2881 (1.2982)
+2022-11-18 15:01:30,079:INFO: Dataset: zara2               Batch:  6/18	Loss 1.3716 (1.3103)
+2022-11-18 15:01:30,233:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3301 (1.3129)
+2022-11-18 15:01:30,385:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2993 (1.3114)
+2022-11-18 15:01:30,538:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3863 (1.3191)
+2022-11-18 15:01:30,691:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3265 (1.3199)
+2022-11-18 15:01:30,846:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3119 (1.3192)
+2022-11-18 15:01:30,999:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3395 (1.3208)
+2022-11-18 15:01:31,153:INFO: Dataset: zara2               Batch: 13/18	Loss 1.2812 (1.3178)
+2022-11-18 15:01:31,307:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3728 (1.3219)
+2022-11-18 15:01:31,461:INFO: Dataset: zara2               Batch: 15/18	Loss 1.2613 (1.3179)
+2022-11-18 15:01:31,620:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3269 (1.3184)
+2022-11-18 15:01:31,786:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2653 (1.3153)
+2022-11-18 15:01:31,933:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3942 (1.3196)
+2022-11-18 15:01:31,979:INFO: - Computing loss (validation)
+2022-11-18 15:01:32,242:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3621 (1.3621)
+2022-11-18 15:01:32,277:INFO: Dataset: hotel               Batch: 2/2	Loss 1.3183 (1.3586)
+2022-11-18 15:01:32,589:INFO: Dataset: univ                Batch: 1/3	Loss 1.3374 (1.3374)
+2022-11-18 15:01:32,674:INFO: Dataset: univ                Batch: 2/3	Loss 1.3229 (1.3291)
+2022-11-18 15:01:32,753:INFO: Dataset: univ                Batch: 3/3	Loss 1.3020 (1.3205)
+2022-11-18 15:01:33,080:INFO: Dataset: zara1               Batch: 1/2	Loss 1.2552 (1.2552)
+2022-11-18 15:01:33,126:INFO: Dataset: zara1               Batch: 2/2	Loss 1.3415 (1.2766)
+2022-11-18 15:01:33,442:INFO: Dataset: zara2               Batch: 1/5	Loss 1.2901 (1.2901)
+2022-11-18 15:01:33,524:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3276 (1.3086)
+2022-11-18 15:01:33,603:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2764 (1.2974)
+2022-11-18 15:01:33,682:INFO: Dataset: zara2               Batch: 4/5	Loss 1.3521 (1.3105)
+2022-11-18 15:01:33,763:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3933 (1.3256)
+2022-11-18 15:01:33,819:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_267.pth.tar
+2022-11-18 15:01:33,819:INFO: 
+===> EPOCH: 268 (P2)
+2022-11-18 15:01:33,819:INFO: - Computing loss (training)
+2022-11-18 15:01:34,182:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3388 (1.3388)
+2022-11-18 15:01:34,338:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3796 (1.3598)
+2022-11-18 15:01:34,492:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3348 (1.3508)
+2022-11-18 15:01:34,612:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3770 (1.3553)
+2022-11-18 15:01:35,013:INFO: Dataset: univ                Batch:  1/15	Loss 1.2969 (1.2969)
+2022-11-18 15:01:35,179:INFO: Dataset: univ                Batch:  2/15	Loss 1.3185 (1.3077)
+2022-11-18 15:01:35,343:INFO: Dataset: univ                Batch:  3/15	Loss 1.3131 (1.3095)
+2022-11-18 15:01:35,502:INFO: Dataset: univ                Batch:  4/15	Loss 1.3159 (1.3111)
+2022-11-18 15:01:35,671:INFO: Dataset: univ                Batch:  5/15	Loss 1.3167 (1.3122)
+2022-11-18 15:01:35,835:INFO: Dataset: univ                Batch:  6/15	Loss 1.3307 (1.3153)
+2022-11-18 15:01:35,994:INFO: Dataset: univ                Batch:  7/15	Loss 1.3085 (1.3143)
+2022-11-18 15:01:36,152:INFO: Dataset: univ                Batch:  8/15	Loss 1.3269 (1.3159)
+2022-11-18 15:01:36,311:INFO: Dataset: univ                Batch:  9/15	Loss 1.3231 (1.3166)
+2022-11-18 15:01:36,469:INFO: Dataset: univ                Batch: 10/15	Loss 1.3346 (1.3184)
+2022-11-18 15:01:36,633:INFO: Dataset: univ                Batch: 11/15	Loss 1.3411 (1.3207)
+2022-11-18 15:01:36,803:INFO: Dataset: univ                Batch: 12/15	Loss 1.2965 (1.3187)
+2022-11-18 15:01:36,965:INFO: Dataset: univ                Batch: 13/15	Loss 1.3025 (1.3174)
+2022-11-18 15:01:37,129:INFO: Dataset: univ                Batch: 14/15	Loss 1.3188 (1.3175)
+2022-11-18 15:01:37,216:INFO: Dataset: univ                Batch: 15/15	Loss 1.2978 (1.3172)
+2022-11-18 15:01:37,606:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3579 (1.3579)
+2022-11-18 15:01:37,764:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3518 (1.3550)
+2022-11-18 15:01:37,925:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3512 (1.3538)
+2022-11-18 15:01:38,084:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3647 (1.3564)
+2022-11-18 15:01:38,241:INFO: Dataset: zara1               Batch: 5/8	Loss 1.3318 (1.3520)
+2022-11-18 15:01:38,397:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3507 (1.3518)
+2022-11-18 15:01:38,552:INFO: Dataset: zara1               Batch: 7/8	Loss 1.2490 (1.3376)
+2022-11-18 15:01:38,692:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3598 (1.3402)
+2022-11-18 15:01:39,090:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3764 (1.3764)
+2022-11-18 15:01:39,247:INFO: Dataset: zara2               Batch:  2/18	Loss 1.2631 (1.3241)
+2022-11-18 15:01:39,401:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3117 (1.3198)
+2022-11-18 15:01:39,556:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3611 (1.3303)
+2022-11-18 15:01:39,711:INFO: Dataset: zara2               Batch:  5/18	Loss 1.3247 (1.3293)
+2022-11-18 15:01:39,877:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2560 (1.3172)
+2022-11-18 15:01:40,036:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3295 (1.3190)
+2022-11-18 15:01:40,194:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3557 (1.3231)
+2022-11-18 15:01:40,355:INFO: Dataset: zara2               Batch:  9/18	Loss 1.2389 (1.3132)
+2022-11-18 15:01:40,517:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3710 (1.3196)
+2022-11-18 15:01:40,680:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3095 (1.3187)
+2022-11-18 15:01:40,838:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3628 (1.3224)
+2022-11-18 15:01:40,995:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3958 (1.3280)
+2022-11-18 15:01:41,149:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3320 (1.3283)
+2022-11-18 15:01:41,307:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3336 (1.3287)
+2022-11-18 15:01:41,461:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3549 (1.3302)
+2022-11-18 15:01:41,616:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2456 (1.3250)
+2022-11-18 15:01:41,762:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3027 (1.3238)
+2022-11-18 15:01:41,811:INFO: - Computing loss (validation)
+2022-11-18 15:01:42,068:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3203 (1.3203)
+2022-11-18 15:01:42,103:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4901 (1.3331)
+2022-11-18 15:01:42,406:INFO: Dataset: univ                Batch: 1/3	Loss 1.3133 (1.3133)
+2022-11-18 15:01:42,485:INFO: Dataset: univ                Batch: 2/3	Loss 1.3248 (1.3186)
+2022-11-18 15:01:42,563:INFO: Dataset: univ                Batch: 3/3	Loss 1.3182 (1.3185)
+2022-11-18 15:01:42,876:INFO: Dataset: zara1               Batch: 1/2	Loss 1.3798 (1.3798)
+2022-11-18 15:01:42,923:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4307 (1.3921)
+2022-11-18 15:01:43,312:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3281 (1.3281)
+2022-11-18 15:01:43,386:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3041 (1.3165)
+2022-11-18 15:01:43,463:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3275 (1.3199)
+2022-11-18 15:01:43,538:INFO: Dataset: zara2               Batch: 4/5	Loss 1.2963 (1.3137)
+2022-11-18 15:01:43,613:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3628 (1.3233)
+2022-11-18 15:01:43,665:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_268.pth.tar
+2022-11-18 15:01:43,665:INFO: 
+===> EPOCH: 269 (P2)
+2022-11-18 15:01:43,666:INFO: - Computing loss (training)
+2022-11-18 15:01:44,007:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3586 (1.3586)
+2022-11-18 15:01:44,161:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3318 (1.3455)
+2022-11-18 15:01:44,316:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3079 (1.3323)
+2022-11-18 15:01:44,434:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3269 (1.3313)
+2022-11-18 15:01:44,835:INFO: Dataset: univ                Batch:  1/15	Loss 1.3466 (1.3466)
+2022-11-18 15:01:44,997:INFO: Dataset: univ                Batch:  2/15	Loss 1.3627 (1.3548)
+2022-11-18 15:01:45,160:INFO: Dataset: univ                Batch:  3/15	Loss 1.3529 (1.3541)
+2022-11-18 15:01:45,320:INFO: Dataset: univ                Batch:  4/15	Loss 1.3462 (1.3522)
+2022-11-18 15:01:45,484:INFO: Dataset: univ                Batch:  5/15	Loss 1.3666 (1.3549)
+2022-11-18 15:01:45,644:INFO: Dataset: univ                Batch:  6/15	Loss 1.3249 (1.3501)
+2022-11-18 15:01:45,805:INFO: Dataset: univ                Batch:  7/15	Loss 1.3502 (1.3501)
+2022-11-18 15:01:45,964:INFO: Dataset: univ                Batch:  8/15	Loss 1.3618 (1.3515)
+2022-11-18 15:01:46,122:INFO: Dataset: univ                Batch:  9/15	Loss 1.3372 (1.3499)
+2022-11-18 15:01:46,279:INFO: Dataset: univ                Batch: 10/15	Loss 1.3199 (1.3471)
+2022-11-18 15:01:46,441:INFO: Dataset: univ                Batch: 11/15	Loss 1.3735 (1.3497)
+2022-11-18 15:01:46,601:INFO: Dataset: univ                Batch: 12/15	Loss 1.3205 (1.3473)
+2022-11-18 15:01:46,767:INFO: Dataset: univ                Batch: 13/15	Loss 1.3119 (1.3446)
+2022-11-18 15:01:46,929:INFO: Dataset: univ                Batch: 14/15	Loss 1.3366 (1.3440)
+2022-11-18 15:01:47,014:INFO: Dataset: univ                Batch: 15/15	Loss 1.3222 (1.3437)
+2022-11-18 15:01:47,418:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4110 (1.4110)
+2022-11-18 15:01:47,570:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3673 (1.3893)
+2022-11-18 15:01:47,725:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3427 (1.3731)
+2022-11-18 15:01:47,879:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3938 (1.3783)
+2022-11-18 15:01:48,032:INFO: Dataset: zara1               Batch: 5/8	Loss 1.3870 (1.3803)
+2022-11-18 15:01:48,186:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3365 (1.3735)
+2022-11-18 15:01:48,346:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3740 (1.3736)
+2022-11-18 15:01:48,496:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3978 (1.3761)
+2022-11-18 15:01:48,892:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4023 (1.4023)
+2022-11-18 15:01:49,047:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3058 (1.3518)
+2022-11-18 15:01:49,205:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4187 (1.3722)
+2022-11-18 15:01:49,357:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3802 (1.3743)
+2022-11-18 15:01:49,512:INFO: Dataset: zara2               Batch:  5/18	Loss 1.3336 (1.3659)
+2022-11-18 15:01:49,666:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4556 (1.3784)
+2022-11-18 15:01:49,823:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4044 (1.3821)
+2022-11-18 15:01:49,976:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3271 (1.3755)
+2022-11-18 15:01:50,129:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3182 (1.3695)
+2022-11-18 15:01:50,279:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3149 (1.3638)
+2022-11-18 15:01:50,434:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3382 (1.3616)
+2022-11-18 15:01:50,584:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3582 (1.3613)
+2022-11-18 15:01:50,739:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3230 (1.3587)
+2022-11-18 15:01:50,892:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3391 (1.3574)
+2022-11-18 15:01:51,046:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3792 (1.3588)
+2022-11-18 15:01:51,199:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3770 (1.3601)
+2022-11-18 15:01:51,353:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3447 (1.3593)
+2022-11-18 15:01:51,494:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3001 (1.3564)
+2022-11-18 15:01:51,540:INFO: - Computing loss (validation)
+2022-11-18 15:01:51,809:INFO: Dataset: hotel               Batch: 1/2	Loss 1.4844 (1.4844)
+2022-11-18 15:01:51,844:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2972 (1.4697)
+2022-11-18 15:01:52,149:INFO: Dataset: univ                Batch: 1/3	Loss 1.3413 (1.3413)
+2022-11-18 15:01:52,229:INFO: Dataset: univ                Batch: 2/3	Loss 1.3641 (1.3541)
+2022-11-18 15:01:52,303:INFO: Dataset: univ                Batch: 3/3	Loss 1.3204 (1.3436)
+2022-11-18 15:01:52,625:INFO: Dataset: zara1               Batch: 1/2	Loss 1.3352 (1.3352)
+2022-11-18 15:01:52,671:INFO: Dataset: zara1               Batch: 2/2	Loss 1.2815 (1.3215)
+2022-11-18 15:01:52,978:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3414 (1.3414)
+2022-11-18 15:01:53,055:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3137 (1.3266)
+2022-11-18 15:01:53,131:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3367 (1.3299)
+2022-11-18 15:01:53,208:INFO: Dataset: zara2               Batch: 4/5	Loss 1.3361 (1.3316)
+2022-11-18 15:01:53,281:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3326 (1.3318)
+2022-11-18 15:01:53,334:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_269.pth.tar
+2022-11-18 15:01:53,334:INFO: 
+===> EPOCH: 270 (P2)
+2022-11-18 15:01:53,335:INFO: - Computing loss (training)
+2022-11-18 15:01:53,679:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3446 (1.3446)
+2022-11-18 15:01:53,839:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4381 (1.3917)
+2022-11-18 15:01:53,998:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5154 (1.4331)
+2022-11-18 15:01:54,116:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4732 (1.4399)
+2022-11-18 15:01:54,514:INFO: Dataset: univ                Batch:  1/15	Loss 1.3520 (1.3520)
+2022-11-18 15:01:54,672:INFO: Dataset: univ                Batch:  2/15	Loss 1.3465 (1.3492)
+2022-11-18 15:01:54,844:INFO: Dataset: univ                Batch:  3/15	Loss 1.3365 (1.3450)
+2022-11-18 15:01:55,039:INFO: Dataset: univ                Batch:  4/15	Loss 1.3726 (1.3522)
+2022-11-18 15:01:55,230:INFO: Dataset: univ                Batch:  5/15	Loss 1.3203 (1.3458)
+2022-11-18 15:01:55,423:INFO: Dataset: univ                Batch:  6/15	Loss 1.3457 (1.3458)
+2022-11-18 15:01:55,611:INFO: Dataset: univ                Batch:  7/15	Loss 1.3455 (1.3458)
+2022-11-18 15:01:55,797:INFO: Dataset: univ                Batch:  8/15	Loss 1.3782 (1.3499)
+2022-11-18 15:01:55,982:INFO: Dataset: univ                Batch:  9/15	Loss 1.3704 (1.3521)
+2022-11-18 15:01:56,168:INFO: Dataset: univ                Batch: 10/15	Loss 1.3326 (1.3501)
+2022-11-18 15:01:56,359:INFO: Dataset: univ                Batch: 11/15	Loss 1.3732 (1.3524)
+2022-11-18 15:01:56,547:INFO: Dataset: univ                Batch: 12/15	Loss 1.3404 (1.3513)
+2022-11-18 15:01:56,741:INFO: Dataset: univ                Batch: 13/15	Loss 1.3625 (1.3522)
+2022-11-18 15:01:56,916:INFO: Dataset: univ                Batch: 14/15	Loss 1.3602 (1.3529)
+2022-11-18 15:01:57,008:INFO: Dataset: univ                Batch: 15/15	Loss 1.3613 (1.3530)
+2022-11-18 15:01:57,417:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4050 (1.4050)
+2022-11-18 15:01:57,577:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4924 (1.4503)
+2022-11-18 15:01:57,742:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3603 (1.4212)
+2022-11-18 15:01:57,896:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4616 (1.4319)
+2022-11-18 15:01:58,049:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4527 (1.4355)
+2022-11-18 15:01:58,204:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3936 (1.4291)
+2022-11-18 15:01:58,357:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3542 (1.4183)
+2022-11-18 15:01:58,497:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4055 (1.4169)
+2022-11-18 15:01:58,970:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3791 (1.3791)
+2022-11-18 15:01:59,149:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4354 (1.4082)
+2022-11-18 15:01:59,329:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3001 (1.3733)
+2022-11-18 15:01:59,509:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3661 (1.3715)
+2022-11-18 15:01:59,689:INFO: Dataset: zara2               Batch:  5/18	Loss 1.3743 (1.3720)
+2022-11-18 15:01:59,876:INFO: Dataset: zara2               Batch:  6/18	Loss 1.3134 (1.3614)
+2022-11-18 15:02:00,057:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4142 (1.3691)
+2022-11-18 15:02:00,238:INFO: Dataset: zara2               Batch:  8/18	Loss 1.4362 (1.3766)
+2022-11-18 15:02:00,425:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3003 (1.3682)
+2022-11-18 15:02:00,598:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3410 (1.3654)
+2022-11-18 15:02:00,772:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3947 (1.3681)
+2022-11-18 15:02:00,945:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4008 (1.3710)
+2022-11-18 15:02:01,120:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3625 (1.3704)
+2022-11-18 15:02:01,289:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3139 (1.3663)
+2022-11-18 15:02:01,445:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3525 (1.3655)
+2022-11-18 15:02:01,599:INFO: Dataset: zara2               Batch: 16/18	Loss 1.3407 (1.3639)
+2022-11-18 15:02:01,751:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2603 (1.3579)
+2022-11-18 15:02:01,890:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3665 (1.3584)
+2022-11-18 15:02:01,935:INFO: - Computing loss (validation)
+2022-11-18 15:02:02,202:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3874 (1.3874)
+2022-11-18 15:02:02,236:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2357 (1.3771)
+2022-11-18 15:02:02,547:INFO: Dataset: univ                Batch: 1/3	Loss 1.3793 (1.3793)
+2022-11-18 15:02:02,623:INFO: Dataset: univ                Batch: 2/3	Loss 1.3617 (1.3719)
+2022-11-18 15:02:02,695:INFO: Dataset: univ                Batch: 3/3	Loss 1.3632 (1.3692)
+2022-11-18 15:02:02,995:INFO: Dataset: zara1               Batch: 1/2	Loss 1.3994 (1.3994)
+2022-11-18 15:02:03,041:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4439 (1.4091)
+2022-11-18 15:02:03,370:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3956 (1.3956)
+2022-11-18 15:02:03,462:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3854 (1.3905)
+2022-11-18 15:02:03,563:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3743 (1.3848)
+2022-11-18 15:02:03,657:INFO: Dataset: zara2               Batch: 4/5	Loss 1.3136 (1.3675)
+2022-11-18 15:02:03,741:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3542 (1.3651)
+2022-11-18 15:02:03,800:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_270.pth.tar
+2022-11-18 15:02:03,800:INFO: 
+===> EPOCH: 271 (P2)
+2022-11-18 15:02:03,801:INFO: - Computing loss (training)
+2022-11-18 15:02:04,171:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3384 (1.3384)
+2022-11-18 15:02:04,348:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3743 (1.3556)
+2022-11-18 15:02:04,525:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4414 (1.3852)
+2022-11-18 15:02:04,654:INFO: Dataset: hotel               Batch: 4/4	Loss 1.3375 (1.3778)
+2022-11-18 15:02:05,108:INFO: Dataset: univ                Batch:  1/15	Loss 1.3774 (1.3774)
+2022-11-18 15:02:05,278:INFO: Dataset: univ                Batch:  2/15	Loss 1.3703 (1.3738)
+2022-11-18 15:02:05,444:INFO: Dataset: univ                Batch:  3/15	Loss 1.3738 (1.3738)
+2022-11-18 15:02:05,608:INFO: Dataset: univ                Batch:  4/15	Loss 1.3812 (1.3757)
+2022-11-18 15:02:05,773:INFO: Dataset: univ                Batch:  5/15	Loss 1.3391 (1.3688)
+2022-11-18 15:02:05,935:INFO: Dataset: univ                Batch:  6/15	Loss 1.3926 (1.3726)
+2022-11-18 15:02:06,095:INFO: Dataset: univ                Batch:  7/15	Loss 1.3614 (1.3710)
+2022-11-18 15:02:06,253:INFO: Dataset: univ                Batch:  8/15	Loss 1.4083 (1.3756)
+2022-11-18 15:02:06,412:INFO: Dataset: univ                Batch:  9/15	Loss 1.3942 (1.3775)
+2022-11-18 15:02:06,580:INFO: Dataset: univ                Batch: 10/15	Loss 1.4277 (1.3825)
+2022-11-18 15:02:06,753:INFO: Dataset: univ                Batch: 11/15	Loss 1.3825 (1.3825)
+2022-11-18 15:02:06,920:INFO: Dataset: univ                Batch: 12/15	Loss 1.3813 (1.3824)
+2022-11-18 15:02:07,086:INFO: Dataset: univ                Batch: 13/15	Loss 1.3571 (1.3804)
+2022-11-18 15:02:07,252:INFO: Dataset: univ                Batch: 14/15	Loss 1.3841 (1.3807)
+2022-11-18 15:02:07,340:INFO: Dataset: univ                Batch: 15/15	Loss 1.3440 (1.3802)
+2022-11-18 15:02:07,755:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3483 (1.3483)
+2022-11-18 15:02:07,914:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3805 (1.3652)
+2022-11-18 15:02:08,075:INFO: Dataset: zara1               Batch: 3/8	Loss 1.3302 (1.3533)
+2022-11-18 15:02:08,238:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3907 (1.3633)
+2022-11-18 15:02:08,393:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4351 (1.3777)
+2022-11-18 15:02:08,550:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4308 (1.3867)
+2022-11-18 15:02:08,703:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4292 (1.3933)
+2022-11-18 15:02:08,845:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4023 (1.3943)
+2022-11-18 15:02:09,232:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3907 (1.3907)
+2022-11-18 15:02:09,382:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3425 (1.3658)
+2022-11-18 15:02:09,536:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3976 (1.3763)
+2022-11-18 15:02:09,684:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3317 (1.3650)
+2022-11-18 15:02:09,834:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4034 (1.3728)
+2022-11-18 15:02:09,986:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4088 (1.3781)
+2022-11-18 15:02:10,137:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3240 (1.3703)
+2022-11-18 15:02:10,286:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3553 (1.3684)
+2022-11-18 15:02:10,438:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3820 (1.3699)
+2022-11-18 15:02:10,586:INFO: Dataset: zara2               Batch: 10/18	Loss 1.2875 (1.3617)
+2022-11-18 15:02:10,740:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3482 (1.3606)
+2022-11-18 15:02:10,890:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4271 (1.3664)
+2022-11-18 15:02:11,040:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3731 (1.3669)
+2022-11-18 15:02:11,190:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3968 (1.3690)
+2022-11-18 15:02:11,342:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3637 (1.3686)
+2022-11-18 15:02:11,492:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4007 (1.3705)
+2022-11-18 15:02:11,643:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3029 (1.3669)
+2022-11-18 15:02:11,781:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4497 (1.3713)
+2022-11-18 15:02:11,827:INFO: - Computing loss (validation)
+2022-11-18 15:02:12,091:INFO: Dataset: hotel               Batch: 1/2	Loss 1.4456 (1.4456)
+2022-11-18 15:02:12,125:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4479 (1.4457)
+2022-11-18 15:02:12,455:INFO: Dataset: univ                Batch: 1/3	Loss 1.4119 (1.4119)
+2022-11-18 15:02:12,539:INFO: Dataset: univ                Batch: 2/3	Loss 1.3352 (1.3766)
+2022-11-18 15:02:12,616:INFO: Dataset: univ                Batch: 3/3	Loss 1.3612 (1.3716)
+2022-11-18 15:02:12,930:INFO: Dataset: zara1               Batch: 1/2	Loss 1.3595 (1.3595)
+2022-11-18 15:02:12,975:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4144 (1.3724)
+2022-11-18 15:02:13,283:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3987 (1.3987)
+2022-11-18 15:02:13,358:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3797 (1.3896)
+2022-11-18 15:02:13,434:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3850 (1.3881)
+2022-11-18 15:02:13,511:INFO: Dataset: zara2               Batch: 4/5	Loss 1.3808 (1.3862)
+2022-11-18 15:02:13,586:INFO: Dataset: zara2               Batch: 5/5	Loss 1.4384 (1.3965)
+2022-11-18 15:02:13,646:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_271.pth.tar
+2022-11-18 15:02:13,647:INFO: 
+===> EPOCH: 272 (P2)
+2022-11-18 15:02:13,647:INFO: - Computing loss (training)
+2022-11-18 15:02:13,985:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5117 (1.5117)
+2022-11-18 15:02:14,141:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5443 (1.5286)
+2022-11-18 15:02:14,297:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5908 (1.5495)
+2022-11-18 15:02:14,421:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4039 (1.5232)
+2022-11-18 15:02:14,841:INFO: Dataset: univ                Batch:  1/15	Loss 1.4075 (1.4075)
+2022-11-18 15:02:15,005:INFO: Dataset: univ                Batch:  2/15	Loss 1.3932 (1.4007)
+2022-11-18 15:02:15,168:INFO: Dataset: univ                Batch:  3/15	Loss 1.3931 (1.3982)
+2022-11-18 15:02:15,329:INFO: Dataset: univ                Batch:  4/15	Loss 1.4075 (1.4008)
+2022-11-18 15:02:15,494:INFO: Dataset: univ                Batch:  5/15	Loss 1.3922 (1.3991)
+2022-11-18 15:02:15,659:INFO: Dataset: univ                Batch:  6/15	Loss 1.3971 (1.3987)
+2022-11-18 15:02:15,819:INFO: Dataset: univ                Batch:  7/15	Loss 1.3830 (1.3961)
+2022-11-18 15:02:15,976:INFO: Dataset: univ                Batch:  8/15	Loss 1.4168 (1.3990)
+2022-11-18 15:02:16,133:INFO: Dataset: univ                Batch:  9/15	Loss 1.4093 (1.4001)
+2022-11-18 15:02:16,289:INFO: Dataset: univ                Batch: 10/15	Loss 1.3854 (1.3983)
+2022-11-18 15:02:16,447:INFO: Dataset: univ                Batch: 11/15	Loss 1.3673 (1.3952)
+2022-11-18 15:02:16,603:INFO: Dataset: univ                Batch: 12/15	Loss 1.4011 (1.3957)
+2022-11-18 15:02:16,762:INFO: Dataset: univ                Batch: 13/15	Loss 1.4150 (1.3972)
+2022-11-18 15:02:16,926:INFO: Dataset: univ                Batch: 14/15	Loss 1.3848 (1.3962)
+2022-11-18 15:02:17,012:INFO: Dataset: univ                Batch: 15/15	Loss 1.4157 (1.3965)
+2022-11-18 15:02:17,411:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4642 (1.4642)
+2022-11-18 15:02:17,561:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4384 (1.4509)
+2022-11-18 15:02:17,710:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4236 (1.4409)
+2022-11-18 15:02:17,862:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4741 (1.4483)
+2022-11-18 15:02:18,016:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4378 (1.4462)
+2022-11-18 15:02:18,167:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4745 (1.4507)
+2022-11-18 15:02:18,315:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4521 (1.4509)
+2022-11-18 15:02:18,451:INFO: Dataset: zara1               Batch: 8/8	Loss 1.3947 (1.4450)
+2022-11-18 15:02:18,864:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3385 (1.3385)
+2022-11-18 15:02:19,023:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4964 (1.4204)
+2022-11-18 15:02:19,177:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4257 (1.4222)
+2022-11-18 15:02:19,330:INFO: Dataset: zara2               Batch:  4/18	Loss 1.3420 (1.4027)
+2022-11-18 15:02:19,486:INFO: Dataset: zara2               Batch:  5/18	Loss 1.3748 (1.3967)
+2022-11-18 15:02:19,639:INFO: Dataset: zara2               Batch:  6/18	Loss 1.3809 (1.3942)
+2022-11-18 15:02:19,790:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3404 (1.3868)
+2022-11-18 15:02:19,941:INFO: Dataset: zara2               Batch:  8/18	Loss 1.4662 (1.3973)
+2022-11-18 15:02:20,092:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4200 (1.3997)
+2022-11-18 15:02:20,241:INFO: Dataset: zara2               Batch: 10/18	Loss 1.4015 (1.3999)
+2022-11-18 15:02:20,394:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3556 (1.3959)
+2022-11-18 15:02:20,545:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3956 (1.3959)
+2022-11-18 15:02:20,697:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3214 (1.3901)
+2022-11-18 15:02:20,848:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3865 (1.3899)
+2022-11-18 15:02:21,001:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4742 (1.3960)
+2022-11-18 15:02:21,151:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4095 (1.3969)
+2022-11-18 15:02:21,306:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3194 (1.3926)
+2022-11-18 15:02:21,446:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4790 (1.3965)
+2022-11-18 15:02:21,491:INFO: - Computing loss (validation)
+2022-11-18 15:02:21,760:INFO: Dataset: hotel               Batch: 1/2	Loss 1.4309 (1.4309)
+2022-11-18 15:02:21,795:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2836 (1.4198)
+2022-11-18 15:02:22,095:INFO: Dataset: univ                Batch: 1/3	Loss 1.3872 (1.3872)
+2022-11-18 15:02:22,171:INFO: Dataset: univ                Batch: 2/3	Loss 1.3835 (1.3854)
+2022-11-18 15:02:22,242:INFO: Dataset: univ                Batch: 3/3	Loss 1.3874 (1.3860)
+2022-11-18 15:02:22,545:INFO: Dataset: zara1               Batch: 1/2	Loss 1.4018 (1.4018)
+2022-11-18 15:02:22,591:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4127 (1.4045)
+2022-11-18 15:02:22,892:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3919 (1.3919)
+2022-11-18 15:02:22,973:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3636 (1.3775)
+2022-11-18 15:02:23,054:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3814 (1.3788)
+2022-11-18 15:02:23,133:INFO: Dataset: zara2               Batch: 4/5	Loss 1.4224 (1.3898)
+2022-11-18 15:02:23,211:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3518 (1.3819)
+2022-11-18 15:02:23,265:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_272.pth.tar
+2022-11-18 15:02:23,265:INFO: 
+===> EPOCH: 273 (P2)
+2022-11-18 15:02:23,266:INFO: - Computing loss (training)
+2022-11-18 15:02:23,608:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4466 (1.4466)
+2022-11-18 15:02:23,760:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4635 (1.4551)
+2022-11-18 15:02:23,912:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4697 (1.4598)
+2022-11-18 15:02:24,029:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5150 (1.4689)
+2022-11-18 15:02:24,419:INFO: Dataset: univ                Batch:  1/15	Loss 1.4341 (1.4341)
+2022-11-18 15:02:24,576:INFO: Dataset: univ                Batch:  2/15	Loss 1.3796 (1.4083)
+2022-11-18 15:02:24,741:INFO: Dataset: univ                Batch:  3/15	Loss 1.4491 (1.4221)
+2022-11-18 15:02:24,896:INFO: Dataset: univ                Batch:  4/15	Loss 1.4119 (1.4197)
+2022-11-18 15:02:25,052:INFO: Dataset: univ                Batch:  5/15	Loss 1.4115 (1.4179)
+2022-11-18 15:02:25,210:INFO: Dataset: univ                Batch:  6/15	Loss 1.3990 (1.4147)
+2022-11-18 15:02:25,366:INFO: Dataset: univ                Batch:  7/15	Loss 1.4232 (1.4160)
+2022-11-18 15:02:25,522:INFO: Dataset: univ                Batch:  8/15	Loss 1.4182 (1.4163)
+2022-11-18 15:02:25,678:INFO: Dataset: univ                Batch:  9/15	Loss 1.4020 (1.4148)
+2022-11-18 15:02:25,832:INFO: Dataset: univ                Batch: 10/15	Loss 1.4288 (1.4164)
+2022-11-18 15:02:25,990:INFO: Dataset: univ                Batch: 11/15	Loss 1.4567 (1.4196)
+2022-11-18 15:02:26,147:INFO: Dataset: univ                Batch: 12/15	Loss 1.4118 (1.4189)
+2022-11-18 15:02:26,303:INFO: Dataset: univ                Batch: 13/15	Loss 1.4266 (1.4194)
+2022-11-18 15:02:26,459:INFO: Dataset: univ                Batch: 14/15	Loss 1.4097 (1.4187)
+2022-11-18 15:02:26,543:INFO: Dataset: univ                Batch: 15/15	Loss 1.3813 (1.4182)
+2022-11-18 15:02:26,931:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4538 (1.4538)
+2022-11-18 15:02:27,082:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4919 (1.4707)
+2022-11-18 15:02:27,232:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4918 (1.4782)
+2022-11-18 15:02:27,379:INFO: Dataset: zara1               Batch: 4/8	Loss 1.3637 (1.4510)
+2022-11-18 15:02:27,528:INFO: Dataset: zara1               Batch: 5/8	Loss 1.5172 (1.4631)
+2022-11-18 15:02:27,679:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3370 (1.4419)
+2022-11-18 15:02:27,828:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4381 (1.4413)
+2022-11-18 15:02:27,964:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4339 (1.4406)
+2022-11-18 15:02:28,355:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3737 (1.3737)
+2022-11-18 15:02:28,506:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4616 (1.4171)
+2022-11-18 15:02:28,659:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4763 (1.4357)
+2022-11-18 15:02:28,814:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4589 (1.4418)
+2022-11-18 15:02:28,971:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5038 (1.4542)
+2022-11-18 15:02:29,122:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4792 (1.4583)
+2022-11-18 15:02:29,272:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4375 (1.4555)
+2022-11-18 15:02:29,421:INFO: Dataset: zara2               Batch:  8/18	Loss 1.4314 (1.4525)
+2022-11-18 15:02:29,572:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4120 (1.4482)
+2022-11-18 15:02:29,719:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3821 (1.4413)
+2022-11-18 15:02:29,871:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4423 (1.4414)
+2022-11-18 15:02:30,020:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4059 (1.4385)
+2022-11-18 15:02:30,171:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4323 (1.4380)
+2022-11-18 15:02:30,323:INFO: Dataset: zara2               Batch: 14/18	Loss 1.4959 (1.4417)
+2022-11-18 15:02:30,477:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4640 (1.4432)
+2022-11-18 15:02:30,628:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4169 (1.4416)
+2022-11-18 15:02:30,780:INFO: Dataset: zara2               Batch: 17/18	Loss 1.3767 (1.4378)
+2022-11-18 15:02:30,918:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4347 (1.4376)
+2022-11-18 15:02:30,964:INFO: - Computing loss (validation)
+2022-11-18 15:02:31,225:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5478 (1.5478)
+2022-11-18 15:02:31,260:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5477 (1.5478)
+2022-11-18 15:02:31,570:INFO: Dataset: univ                Batch: 1/3	Loss 1.4080 (1.4080)
+2022-11-18 15:02:31,649:INFO: Dataset: univ                Batch: 2/3	Loss 1.4018 (1.4047)
+2022-11-18 15:02:31,722:INFO: Dataset: univ                Batch: 3/3	Loss 1.4206 (1.4093)
+2022-11-18 15:02:32,026:INFO: Dataset: zara1               Batch: 1/2	Loss 1.3765 (1.3765)
+2022-11-18 15:02:32,073:INFO: Dataset: zara1               Batch: 2/2	Loss 1.3966 (1.3818)
+2022-11-18 15:02:32,381:INFO: Dataset: zara2               Batch: 1/5	Loss 1.4533 (1.4533)
+2022-11-18 15:02:32,455:INFO: Dataset: zara2               Batch: 2/5	Loss 1.4132 (1.4324)
+2022-11-18 15:02:32,531:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3512 (1.4066)
+2022-11-18 15:02:32,605:INFO: Dataset: zara2               Batch: 4/5	Loss 1.3673 (1.3970)
+2022-11-18 15:02:32,679:INFO: Dataset: zara2               Batch: 5/5	Loss 1.4155 (1.4008)
+2022-11-18 15:02:32,731:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_273.pth.tar
+2022-11-18 15:02:32,731:INFO: 
+===> EPOCH: 274 (P2)
+2022-11-18 15:02:32,732:INFO: - Computing loss (training)
+2022-11-18 15:02:33,069:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5709 (1.5709)
+2022-11-18 15:02:33,221:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5206 (1.5460)
+2022-11-18 15:02:33,374:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4301 (1.5059)
+2022-11-18 15:02:33,491:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6899 (1.5397)
+2022-11-18 15:02:33,948:INFO: Dataset: univ                Batch:  1/15	Loss 1.4331 (1.4331)
+2022-11-18 15:02:34,106:INFO: Dataset: univ                Batch:  2/15	Loss 1.4249 (1.4290)
+2022-11-18 15:02:34,262:INFO: Dataset: univ                Batch:  3/15	Loss 1.4104 (1.4229)
+2022-11-18 15:02:34,416:INFO: Dataset: univ                Batch:  4/15	Loss 1.4569 (1.4319)
+2022-11-18 15:02:34,571:INFO: Dataset: univ                Batch:  5/15	Loss 1.4314 (1.4318)
+2022-11-18 15:02:34,731:INFO: Dataset: univ                Batch:  6/15	Loss 1.4098 (1.4279)
+2022-11-18 15:02:34,890:INFO: Dataset: univ                Batch:  7/15	Loss 1.4432 (1.4303)
+2022-11-18 15:02:35,049:INFO: Dataset: univ                Batch:  8/15	Loss 1.4447 (1.4321)
+2022-11-18 15:02:35,208:INFO: Dataset: univ                Batch:  9/15	Loss 1.4189 (1.4306)
+2022-11-18 15:02:35,367:INFO: Dataset: univ                Batch: 10/15	Loss 1.4245 (1.4300)
+2022-11-18 15:02:35,527:INFO: Dataset: univ                Batch: 11/15	Loss 1.4432 (1.4311)
+2022-11-18 15:02:35,687:INFO: Dataset: univ                Batch: 12/15	Loss 1.4439 (1.4323)
+2022-11-18 15:02:35,846:INFO: Dataset: univ                Batch: 13/15	Loss 1.4243 (1.4318)
+2022-11-18 15:02:36,007:INFO: Dataset: univ                Batch: 14/15	Loss 1.4172 (1.4307)
+2022-11-18 15:02:36,092:INFO: Dataset: univ                Batch: 15/15	Loss 1.4606 (1.4311)
+2022-11-18 15:02:36,484:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4017 (1.4017)
+2022-11-18 15:02:36,632:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3638 (1.3827)
+2022-11-18 15:02:36,785:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4361 (1.4022)
+2022-11-18 15:02:36,933:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4274 (1.4080)
+2022-11-18 15:02:37,088:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4778 (1.4225)
+2022-11-18 15:02:37,237:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4086 (1.4200)
+2022-11-18 15:02:37,389:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4766 (1.4279)
+2022-11-18 15:02:37,527:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5382 (1.4400)
+2022-11-18 15:02:37,932:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4951 (1.4951)
+2022-11-18 15:02:38,088:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4392 (1.4669)
+2022-11-18 15:02:38,241:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4452 (1.4602)
+2022-11-18 15:02:38,395:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4649 (1.4615)
+2022-11-18 15:02:38,546:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4809 (1.4656)
+2022-11-18 15:02:38,701:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4577 (1.4643)
+2022-11-18 15:02:38,855:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4229 (1.4581)
+2022-11-18 15:02:39,005:INFO: Dataset: zara2               Batch:  8/18	Loss 1.4134 (1.4523)
+2022-11-18 15:02:39,157:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4376 (1.4506)
+2022-11-18 15:02:39,305:INFO: Dataset: zara2               Batch: 10/18	Loss 1.4077 (1.4462)
+2022-11-18 15:02:39,459:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4648 (1.4476)
+2022-11-18 15:02:39,608:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4350 (1.4466)
+2022-11-18 15:02:39,758:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4729 (1.4487)
+2022-11-18 15:02:39,909:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3339 (1.4409)
+2022-11-18 15:02:40,061:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5201 (1.4465)
+2022-11-18 15:02:40,211:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4514 (1.4468)
+2022-11-18 15:02:40,363:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4098 (1.4447)
+2022-11-18 15:02:40,503:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4334 (1.4442)
+2022-11-18 15:02:40,548:INFO: - Computing loss (validation)
+2022-11-18 15:02:40,829:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5313 (1.5313)
+2022-11-18 15:02:40,864:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6445 (1.5410)
+2022-11-18 15:02:41,171:INFO: Dataset: univ                Batch: 1/3	Loss 1.4006 (1.4006)
+2022-11-18 15:02:41,252:INFO: Dataset: univ                Batch: 2/3	Loss 1.3911 (1.3956)
+2022-11-18 15:02:41,324:INFO: Dataset: univ                Batch: 3/3	Loss 1.4468 (1.4120)
+2022-11-18 15:02:41,629:INFO: Dataset: zara1               Batch: 1/2	Loss 1.4514 (1.4514)
+2022-11-18 15:02:41,675:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4903 (1.4608)
+2022-11-18 15:02:41,983:INFO: Dataset: zara2               Batch: 1/5	Loss 1.4219 (1.4219)
+2022-11-18 15:02:42,061:INFO: Dataset: zara2               Batch: 2/5	Loss 1.4285 (1.4253)
+2022-11-18 15:02:42,138:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3630 (1.4047)
+2022-11-18 15:02:42,218:INFO: Dataset: zara2               Batch: 4/5	Loss 1.4057 (1.4050)
+2022-11-18 15:02:42,294:INFO: Dataset: zara2               Batch: 5/5	Loss 1.4353 (1.4115)
+2022-11-18 15:02:42,352:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_274.pth.tar
+2022-11-18 15:02:42,352:INFO: 
+===> EPOCH: 275 (P2)
+2022-11-18 15:02:42,353:INFO: - Computing loss (training)
+2022-11-18 15:02:42,698:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6118 (1.6118)
+2022-11-18 15:02:42,851:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5302 (1.5711)
+2022-11-18 15:02:43,007:INFO: Dataset: hotel               Batch: 3/4	Loss 1.4928 (1.5456)
+2022-11-18 15:02:43,124:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4620 (1.5318)
+2022-11-18 15:02:43,519:INFO: Dataset: univ                Batch:  1/15	Loss 1.4554 (1.4554)
+2022-11-18 15:02:43,677:INFO: Dataset: univ                Batch:  2/15	Loss 1.4604 (1.4579)
+2022-11-18 15:02:43,836:INFO: Dataset: univ                Batch:  3/15	Loss 1.4576 (1.4578)
+2022-11-18 15:02:43,997:INFO: Dataset: univ                Batch:  4/15	Loss 1.4319 (1.4515)
+2022-11-18 15:02:44,156:INFO: Dataset: univ                Batch:  5/15	Loss 1.4317 (1.4476)
+2022-11-18 15:02:44,311:INFO: Dataset: univ                Batch:  6/15	Loss 1.4470 (1.4475)
+2022-11-18 15:02:44,467:INFO: Dataset: univ                Batch:  7/15	Loss 1.4737 (1.4509)
+2022-11-18 15:02:44,621:INFO: Dataset: univ                Batch:  8/15	Loss 1.4766 (1.4537)
+2022-11-18 15:02:44,778:INFO: Dataset: univ                Batch:  9/15	Loss 1.4660 (1.4552)
+2022-11-18 15:02:44,932:INFO: Dataset: univ                Batch: 10/15	Loss 1.4780 (1.4574)
+2022-11-18 15:02:45,089:INFO: Dataset: univ                Batch: 11/15	Loss 1.4521 (1.4569)
+2022-11-18 15:02:45,246:INFO: Dataset: univ                Batch: 12/15	Loss 1.4549 (1.4567)
+2022-11-18 15:02:45,405:INFO: Dataset: univ                Batch: 13/15	Loss 1.4376 (1.4550)
+2022-11-18 15:02:45,562:INFO: Dataset: univ                Batch: 14/15	Loss 1.4352 (1.4534)
+2022-11-18 15:02:45,646:INFO: Dataset: univ                Batch: 15/15	Loss 1.4112 (1.4529)
+2022-11-18 15:02:46,042:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4456 (1.4456)
+2022-11-18 15:02:46,194:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5133 (1.4780)
+2022-11-18 15:02:46,347:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4716 (1.4758)
+2022-11-18 15:02:46,498:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5447 (1.4942)
+2022-11-18 15:02:46,651:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4509 (1.4855)
+2022-11-18 15:02:46,804:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5698 (1.4993)
+2022-11-18 15:02:46,956:INFO: Dataset: zara1               Batch: 7/8	Loss 1.3873 (1.4832)
+2022-11-18 15:02:47,098:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4541 (1.4799)
+2022-11-18 15:02:47,476:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4723 (1.4723)
+2022-11-18 15:02:47,627:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5256 (1.5003)
+2022-11-18 15:02:47,779:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4479 (1.4849)
+2022-11-18 15:02:47,929:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4956 (1.4873)
+2022-11-18 15:02:48,080:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4458 (1.4795)
+2022-11-18 15:02:48,244:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5022 (1.4830)
+2022-11-18 15:02:48,395:INFO: Dataset: zara2               Batch:  7/18	Loss 1.5099 (1.4864)
+2022-11-18 15:02:48,549:INFO: Dataset: zara2               Batch:  8/18	Loss 1.4292 (1.4790)
+2022-11-18 15:02:48,702:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5163 (1.4830)
+2022-11-18 15:02:48,854:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5192 (1.4863)
+2022-11-18 15:02:49,020:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5337 (1.4909)
+2022-11-18 15:02:49,178:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4632 (1.4885)
+2022-11-18 15:02:49,329:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3924 (1.4810)
+2022-11-18 15:02:49,480:INFO: Dataset: zara2               Batch: 14/18	Loss 1.4563 (1.4792)
+2022-11-18 15:02:49,631:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4850 (1.4796)
+2022-11-18 15:02:49,779:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4274 (1.4763)
+2022-11-18 15:02:49,934:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4315 (1.4734)
+2022-11-18 15:02:50,072:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3762 (1.4686)
+2022-11-18 15:02:50,116:INFO: - Computing loss (validation)
+2022-11-18 15:02:50,385:INFO: Dataset: hotel               Batch: 1/2	Loss 1.4998 (1.4998)
+2022-11-18 15:02:50,419:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5714 (1.5044)
+2022-11-18 15:02:50,733:INFO: Dataset: univ                Batch: 1/3	Loss 1.4737 (1.4737)
+2022-11-18 15:02:50,813:INFO: Dataset: univ                Batch: 2/3	Loss 1.4436 (1.4592)
+2022-11-18 15:02:50,889:INFO: Dataset: univ                Batch: 3/3	Loss 1.4774 (1.4651)
+2022-11-18 15:02:51,197:INFO: Dataset: zara1               Batch: 1/2	Loss 1.4636 (1.4636)
+2022-11-18 15:02:51,244:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4723 (1.4659)
+2022-11-18 15:02:51,553:INFO: Dataset: zara2               Batch: 1/5	Loss 1.5228 (1.5228)
+2022-11-18 15:02:51,626:INFO: Dataset: zara2               Batch: 2/5	Loss 1.4456 (1.4846)
+2022-11-18 15:02:51,699:INFO: Dataset: zara2               Batch: 3/5	Loss 1.4825 (1.4839)
+2022-11-18 15:02:51,774:INFO: Dataset: zara2               Batch: 4/5	Loss 1.4168 (1.4665)
+2022-11-18 15:02:51,847:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3793 (1.4490)
+2022-11-18 15:02:51,900:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_275.pth.tar
+2022-11-18 15:02:51,900:INFO: 
+===> EPOCH: 276 (P2)
+2022-11-18 15:02:51,901:INFO: - Computing loss (training)
+2022-11-18 15:02:52,247:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5371 (1.5371)
+2022-11-18 15:02:52,399:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6295 (1.5846)
+2022-11-18 15:02:52,552:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5346 (1.5681)
+2022-11-18 15:02:52,669:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5907 (1.5717)
+2022-11-18 15:02:53,068:INFO: Dataset: univ                Batch:  1/15	Loss 1.4683 (1.4683)
+2022-11-18 15:02:53,225:INFO: Dataset: univ                Batch:  2/15	Loss 1.4731 (1.4707)
+2022-11-18 15:02:53,388:INFO: Dataset: univ                Batch:  3/15	Loss 1.4857 (1.4757)
+2022-11-18 15:02:53,543:INFO: Dataset: univ                Batch:  4/15	Loss 1.4775 (1.4761)
+2022-11-18 15:02:53,700:INFO: Dataset: univ                Batch:  5/15	Loss 1.4512 (1.4708)
+2022-11-18 15:02:53,860:INFO: Dataset: univ                Batch:  6/15	Loss 1.4789 (1.4723)
+2022-11-18 15:02:54,018:INFO: Dataset: univ                Batch:  7/15	Loss 1.4564 (1.4699)
+2022-11-18 15:02:54,173:INFO: Dataset: univ                Batch:  8/15	Loss 1.4666 (1.4695)
+2022-11-18 15:02:54,329:INFO: Dataset: univ                Batch:  9/15	Loss 1.4575 (1.4682)
+2022-11-18 15:02:54,484:INFO: Dataset: univ                Batch: 10/15	Loss 1.4774 (1.4690)
+2022-11-18 15:02:54,640:INFO: Dataset: univ                Batch: 11/15	Loss 1.4787 (1.4700)
+2022-11-18 15:02:54,795:INFO: Dataset: univ                Batch: 12/15	Loss 1.4910 (1.4717)
+2022-11-18 15:02:54,952:INFO: Dataset: univ                Batch: 13/15	Loss 1.5137 (1.4751)
+2022-11-18 15:02:55,108:INFO: Dataset: univ                Batch: 14/15	Loss 1.4777 (1.4753)
+2022-11-18 15:02:55,192:INFO: Dataset: univ                Batch: 15/15	Loss 1.4841 (1.4754)
+2022-11-18 15:02:55,600:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6226 (1.6226)
+2022-11-18 15:02:55,746:INFO: Dataset: zara1               Batch: 2/8	Loss 1.4957 (1.5621)
+2022-11-18 15:02:55,901:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5325 (1.5515)
+2022-11-18 15:02:56,065:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4656 (1.5297)
+2022-11-18 15:02:56,220:INFO: Dataset: zara1               Batch: 5/8	Loss 1.5195 (1.5275)
+2022-11-18 15:02:56,378:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5186 (1.5260)
+2022-11-18 15:02:56,529:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4515 (1.5153)
+2022-11-18 15:02:56,665:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4318 (1.5073)
+2022-11-18 15:02:57,061:INFO: Dataset: zara2               Batch:  1/18	Loss 1.4963 (1.4963)
+2022-11-18 15:02:57,217:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4712 (1.4842)
+2022-11-18 15:02:57,374:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4367 (1.4680)
+2022-11-18 15:02:57,609:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4570 (1.4651)
+2022-11-18 15:02:57,766:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4894 (1.4699)
+2022-11-18 15:02:57,937:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4426 (1.4655)
+2022-11-18 15:02:58,097:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4538 (1.4637)
+2022-11-18 15:02:58,257:INFO: Dataset: zara2               Batch:  8/18	Loss 1.4037 (1.4566)
+2022-11-18 15:02:58,412:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4261 (1.4534)
+2022-11-18 15:02:58,566:INFO: Dataset: zara2               Batch: 10/18	Loss 1.4625 (1.4543)
+2022-11-18 15:02:58,722:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4794 (1.4566)
+2022-11-18 15:02:58,882:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4380 (1.4550)
+2022-11-18 15:02:59,069:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5227 (1.4602)
+2022-11-18 15:02:59,248:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3743 (1.4537)
+2022-11-18 15:02:59,450:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4049 (1.4501)
+2022-11-18 15:02:59,684:INFO: Dataset: zara2               Batch: 16/18	Loss 1.4988 (1.4530)
+2022-11-18 15:02:59,853:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4440 (1.4525)
+2022-11-18 15:02:59,999:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4254 (1.4512)
+2022-11-18 15:03:00,045:INFO: - Computing loss (validation)
+2022-11-18 15:03:00,315:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5367 (1.5367)
+2022-11-18 15:03:00,350:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2054 (1.5152)
+2022-11-18 15:03:00,691:INFO: Dataset: univ                Batch: 1/3	Loss 1.4633 (1.4633)
+2022-11-18 15:03:00,771:INFO: Dataset: univ                Batch: 2/3	Loss 1.4814 (1.4728)
+2022-11-18 15:03:00,849:INFO: Dataset: univ                Batch: 3/3	Loss 1.4954 (1.4794)
+2022-11-18 15:03:01,157:INFO: Dataset: zara1               Batch: 1/2	Loss 1.4469 (1.4469)
+2022-11-18 15:03:01,203:INFO: Dataset: zara1               Batch: 2/2	Loss 1.5644 (1.4794)
+2022-11-18 15:03:01,527:INFO: Dataset: zara2               Batch: 1/5	Loss 1.4624 (1.4624)
+2022-11-18 15:03:01,609:INFO: Dataset: zara2               Batch: 2/5	Loss 1.4711 (1.4669)
+2022-11-18 15:03:01,687:INFO: Dataset: zara2               Batch: 3/5	Loss 1.5166 (1.4842)
+2022-11-18 15:03:01,767:INFO: Dataset: zara2               Batch: 4/5	Loss 1.4757 (1.4822)
+2022-11-18 15:03:01,844:INFO: Dataset: zara2               Batch: 5/5	Loss 1.4980 (1.4851)
+2022-11-18 15:03:01,897:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_276.pth.tar
+2022-11-18 15:03:01,897:INFO: 
+===> EPOCH: 277 (P2)
+2022-11-18 15:03:01,897:INFO: - Computing loss (training)
+2022-11-18 15:03:02,253:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4984 (1.4984)
+2022-11-18 15:03:02,412:INFO: Dataset: hotel               Batch: 2/4	Loss 1.4905 (1.4942)
+2022-11-18 15:03:02,569:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5691 (1.5207)
+2022-11-18 15:03:02,688:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5197 (1.5206)
+2022-11-18 15:03:03,089:INFO: Dataset: univ                Batch:  1/15	Loss 1.5085 (1.5085)
+2022-11-18 15:03:03,248:INFO: Dataset: univ                Batch:  2/15	Loss 1.4707 (1.4894)
+2022-11-18 15:03:03,408:INFO: Dataset: univ                Batch:  3/15	Loss 1.4807 (1.4863)
+2022-11-18 15:03:03,565:INFO: Dataset: univ                Batch:  4/15	Loss 1.4675 (1.4815)
+2022-11-18 15:03:03,721:INFO: Dataset: univ                Batch:  5/15	Loss 1.4820 (1.4816)
+2022-11-18 15:03:03,878:INFO: Dataset: univ                Batch:  6/15	Loss 1.4747 (1.4804)
+2022-11-18 15:03:04,034:INFO: Dataset: univ                Batch:  7/15	Loss 1.5061 (1.4841)
+2022-11-18 15:03:04,187:INFO: Dataset: univ                Batch:  8/15	Loss 1.5191 (1.4884)
+2022-11-18 15:03:04,343:INFO: Dataset: univ                Batch:  9/15	Loss 1.4877 (1.4883)
+2022-11-18 15:03:04,497:INFO: Dataset: univ                Batch: 10/15	Loss 1.5007 (1.4895)
+2022-11-18 15:03:04,654:INFO: Dataset: univ                Batch: 11/15	Loss 1.4688 (1.4879)
+2022-11-18 15:03:04,815:INFO: Dataset: univ                Batch: 12/15	Loss 1.4598 (1.4854)
+2022-11-18 15:03:04,971:INFO: Dataset: univ                Batch: 13/15	Loss 1.4727 (1.4844)
+2022-11-18 15:03:05,126:INFO: Dataset: univ                Batch: 14/15	Loss 1.4681 (1.4832)
+2022-11-18 15:03:05,208:INFO: Dataset: univ                Batch: 15/15	Loss 1.3988 (1.4818)
+2022-11-18 15:03:05,610:INFO: Dataset: zara1               Batch: 1/8	Loss 1.3927 (1.3927)
+2022-11-18 15:03:05,764:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5247 (1.4543)
+2022-11-18 15:03:05,918:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5113 (1.4739)
+2022-11-18 15:03:06,074:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5106 (1.4825)
+2022-11-18 15:03:06,230:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6252 (1.5126)
+2022-11-18 15:03:06,384:INFO: Dataset: zara1               Batch: 6/8	Loss 1.3974 (1.4945)
+2022-11-18 15:03:06,538:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5230 (1.4990)
+2022-11-18 15:03:06,678:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4808 (1.4970)
+2022-11-18 15:03:07,081:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5087 (1.5087)
+2022-11-18 15:03:07,252:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4901 (1.4994)
+2022-11-18 15:03:07,410:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5308 (1.5101)
+2022-11-18 15:03:07,566:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4886 (1.5050)
+2022-11-18 15:03:07,725:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4449 (1.4932)
+2022-11-18 15:03:07,882:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5356 (1.5000)
+2022-11-18 15:03:08,038:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4784 (1.4967)
+2022-11-18 15:03:08,192:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5301 (1.5007)
+2022-11-18 15:03:08,348:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5011 (1.5008)
+2022-11-18 15:03:08,506:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5624 (1.5066)
+2022-11-18 15:03:08,664:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5149 (1.5075)
+2022-11-18 15:03:08,823:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4449 (1.5020)
+2022-11-18 15:03:08,980:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4857 (1.5007)
+2022-11-18 15:03:09,137:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3923 (1.4928)
+2022-11-18 15:03:09,294:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4936 (1.4928)
+2022-11-18 15:03:09,449:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5142 (1.4941)
+2022-11-18 15:03:09,607:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4360 (1.4910)
+2022-11-18 15:03:09,751:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4537 (1.4892)
+2022-11-18 15:03:09,798:INFO: - Computing loss (validation)
+2022-11-18 15:03:10,064:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5649 (1.5649)
+2022-11-18 15:03:10,101:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5530 (1.5643)
+2022-11-18 15:03:10,408:INFO: Dataset: univ                Batch: 1/3	Loss 1.4846 (1.4846)
+2022-11-18 15:03:10,485:INFO: Dataset: univ                Batch: 2/3	Loss 1.4936 (1.4884)
+2022-11-18 15:03:10,559:INFO: Dataset: univ                Batch: 3/3	Loss 1.5012 (1.4923)
+2022-11-18 15:03:10,867:INFO: Dataset: zara1               Batch: 1/2	Loss 1.5202 (1.5202)
+2022-11-18 15:03:10,914:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8206 (1.5946)
+2022-11-18 15:03:11,233:INFO: Dataset: zara2               Batch: 1/5	Loss 1.5513 (1.5513)
+2022-11-18 15:03:11,306:INFO: Dataset: zara2               Batch: 2/5	Loss 1.4619 (1.5061)
+2022-11-18 15:03:11,380:INFO: Dataset: zara2               Batch: 3/5	Loss 1.4744 (1.4947)
+2022-11-18 15:03:11,454:INFO: Dataset: zara2               Batch: 4/5	Loss 1.5581 (1.5105)
+2022-11-18 15:03:11,527:INFO: Dataset: zara2               Batch: 5/5	Loss 1.4513 (1.4985)
+2022-11-18 15:03:11,580:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_277.pth.tar
+2022-11-18 15:03:11,580:INFO: 
+===> EPOCH: 278 (P2)
+2022-11-18 15:03:11,580:INFO: - Computing loss (training)
+2022-11-18 15:03:11,914:INFO: Dataset: hotel               Batch: 1/4	Loss 1.4939 (1.4939)
+2022-11-18 15:03:12,066:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5816 (1.5389)
+2022-11-18 15:03:12,215:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5878 (1.5553)
+2022-11-18 15:03:12,329:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5857 (1.5603)
+2022-11-18 15:03:12,736:INFO: Dataset: univ                Batch:  1/15	Loss 1.5079 (1.5079)
+2022-11-18 15:03:12,898:INFO: Dataset: univ                Batch:  2/15	Loss 1.4626 (1.4845)
+2022-11-18 15:03:13,062:INFO: Dataset: univ                Batch:  3/15	Loss 1.4944 (1.4878)
+2022-11-18 15:03:13,222:INFO: Dataset: univ                Batch:  4/15	Loss 1.5111 (1.4940)
+2022-11-18 15:03:13,385:INFO: Dataset: univ                Batch:  5/15	Loss 1.5227 (1.5002)
+2022-11-18 15:03:13,550:INFO: Dataset: univ                Batch:  6/15	Loss 1.5211 (1.5039)
+2022-11-18 15:03:13,713:INFO: Dataset: univ                Batch:  7/15	Loss 1.5048 (1.5040)
+2022-11-18 15:03:13,878:INFO: Dataset: univ                Batch:  8/15	Loss 1.5222 (1.5061)
+2022-11-18 15:03:14,048:INFO: Dataset: univ                Batch:  9/15	Loss 1.5128 (1.5068)
+2022-11-18 15:03:14,211:INFO: Dataset: univ                Batch: 10/15	Loss 1.4996 (1.5061)
+2022-11-18 15:03:14,389:INFO: Dataset: univ                Batch: 11/15	Loss 1.4943 (1.5050)
+2022-11-18 15:03:14,591:INFO: Dataset: univ                Batch: 12/15	Loss 1.5398 (1.5078)
+2022-11-18 15:03:14,795:INFO: Dataset: univ                Batch: 13/15	Loss 1.4905 (1.5063)
+2022-11-18 15:03:15,009:INFO: Dataset: univ                Batch: 14/15	Loss 1.5055 (1.5063)
+2022-11-18 15:03:15,134:INFO: Dataset: univ                Batch: 15/15	Loss 1.3950 (1.5049)
+2022-11-18 15:03:15,561:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5619 (1.5619)
+2022-11-18 15:03:15,726:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5765 (1.5685)
+2022-11-18 15:03:15,916:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5827 (1.5730)
+2022-11-18 15:03:16,106:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5858 (1.5765)
+2022-11-18 15:03:16,288:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4933 (1.5594)
+2022-11-18 15:03:16,487:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4988 (1.5487)
+2022-11-18 15:03:16,644:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5149 (1.5438)
+2022-11-18 15:03:16,789:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4929 (1.5374)
+2022-11-18 15:03:17,200:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5811 (1.5811)
+2022-11-18 15:03:17,348:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5270 (1.5529)
+2022-11-18 15:03:17,522:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4875 (1.5314)
+2022-11-18 15:03:17,686:INFO: Dataset: zara2               Batch:  4/18	Loss 1.5255 (1.5301)
+2022-11-18 15:03:17,839:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4887 (1.5218)
+2022-11-18 15:03:17,992:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5294 (1.5231)
+2022-11-18 15:03:18,142:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4874 (1.5182)
+2022-11-18 15:03:18,291:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5148 (1.5178)
+2022-11-18 15:03:18,440:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4767 (1.5130)
+2022-11-18 15:03:18,591:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3853 (1.5007)
+2022-11-18 15:03:18,765:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5176 (1.5023)
+2022-11-18 15:03:18,934:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4720 (1.4998)
+2022-11-18 15:03:19,091:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4701 (1.4976)
+2022-11-18 15:03:19,246:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5630 (1.5025)
+2022-11-18 15:03:19,408:INFO: Dataset: zara2               Batch: 15/18	Loss 1.4735 (1.5007)
+2022-11-18 15:03:19,568:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5632 (1.5046)
+2022-11-18 15:03:19,730:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5642 (1.5080)
+2022-11-18 15:03:19,870:INFO: Dataset: zara2               Batch: 18/18	Loss 1.5450 (1.5100)
+2022-11-18 15:03:19,915:INFO: - Computing loss (validation)
+2022-11-18 15:03:20,194:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5112 (1.5112)
+2022-11-18 15:03:20,228:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6570 (1.5211)
+2022-11-18 15:03:20,533:INFO: Dataset: univ                Batch: 1/3	Loss 1.4731 (1.4731)
+2022-11-18 15:03:20,610:INFO: Dataset: univ                Batch: 2/3	Loss 1.5059 (1.4909)
+2022-11-18 15:03:20,681:INFO: Dataset: univ                Batch: 3/3	Loss 1.5095 (1.4969)
+2022-11-18 15:03:20,974:INFO: Dataset: zara1               Batch: 1/2	Loss 1.5495 (1.5495)
+2022-11-18 15:03:21,020:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6460 (1.5728)
+2022-11-18 15:03:21,322:INFO: Dataset: zara2               Batch: 1/5	Loss 1.4975 (1.4975)
+2022-11-18 15:03:21,396:INFO: Dataset: zara2               Batch: 2/5	Loss 1.5060 (1.5020)
+2022-11-18 15:03:21,470:INFO: Dataset: zara2               Batch: 3/5	Loss 1.5090 (1.5041)
+2022-11-18 15:03:21,545:INFO: Dataset: zara2               Batch: 4/5	Loss 1.5068 (1.5048)
+2022-11-18 15:03:21,619:INFO: Dataset: zara2               Batch: 5/5	Loss 1.4646 (1.4968)
+2022-11-18 15:03:21,671:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_278.pth.tar
+2022-11-18 15:03:21,671:INFO: 
+===> EPOCH: 279 (P2)
+2022-11-18 15:03:21,672:INFO: - Computing loss (training)
+2022-11-18 15:03:22,011:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6727 (1.6727)
+2022-11-18 15:03:22,160:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6046 (1.6380)
+2022-11-18 15:03:22,311:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5341 (1.6037)
+2022-11-18 15:03:22,431:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5739 (1.5982)
+2022-11-18 15:03:22,862:INFO: Dataset: univ                Batch:  1/15	Loss 1.5384 (1.5384)
+2022-11-18 15:03:23,028:INFO: Dataset: univ                Batch:  2/15	Loss 1.5313 (1.5350)
+2022-11-18 15:03:23,198:INFO: Dataset: univ                Batch:  3/15	Loss 1.5203 (1.5299)
+2022-11-18 15:03:23,359:INFO: Dataset: univ                Batch:  4/15	Loss 1.5245 (1.5285)
+2022-11-18 15:03:23,521:INFO: Dataset: univ                Batch:  5/15	Loss 1.5434 (1.5313)
+2022-11-18 15:03:23,685:INFO: Dataset: univ                Batch:  6/15	Loss 1.5058 (1.5274)
+2022-11-18 15:03:23,847:INFO: Dataset: univ                Batch:  7/15	Loss 1.5538 (1.5309)
+2022-11-18 15:03:24,008:INFO: Dataset: univ                Batch:  8/15	Loss 1.5117 (1.5286)
+2022-11-18 15:03:24,171:INFO: Dataset: univ                Batch:  9/15	Loss 1.5475 (1.5309)
+2022-11-18 15:03:24,332:INFO: Dataset: univ                Batch: 10/15	Loss 1.5007 (1.5275)
+2022-11-18 15:03:24,494:INFO: Dataset: univ                Batch: 11/15	Loss 1.5538 (1.5300)
+2022-11-18 15:03:24,656:INFO: Dataset: univ                Batch: 12/15	Loss 1.5170 (1.5289)
+2022-11-18 15:03:24,819:INFO: Dataset: univ                Batch: 13/15	Loss 1.4916 (1.5262)
+2022-11-18 15:03:24,980:INFO: Dataset: univ                Batch: 14/15	Loss 1.5017 (1.5246)
+2022-11-18 15:03:25,067:INFO: Dataset: univ                Batch: 15/15	Loss 1.4961 (1.5241)
+2022-11-18 15:03:25,469:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5571 (1.5571)
+2022-11-18 15:03:25,624:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5542 (1.5556)
+2022-11-18 15:03:25,780:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4501 (1.5210)
+2022-11-18 15:03:25,932:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5484 (1.5276)
+2022-11-18 15:03:26,086:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6082 (1.5442)
+2022-11-18 15:03:26,241:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5216 (1.5411)
+2022-11-18 15:03:26,395:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5154 (1.5373)
+2022-11-18 15:03:26,536:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5924 (1.5433)
+2022-11-18 15:03:26,918:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5246 (1.5246)
+2022-11-18 15:03:27,067:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5433 (1.5344)
+2022-11-18 15:03:27,220:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5073 (1.5247)
+2022-11-18 15:03:27,367:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4327 (1.5001)
+2022-11-18 15:03:27,522:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5337 (1.5070)
+2022-11-18 15:03:27,677:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4535 (1.4989)
+2022-11-18 15:03:27,830:INFO: Dataset: zara2               Batch:  7/18	Loss 1.5293 (1.5027)
+2022-11-18 15:03:27,981:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5234 (1.5052)
+2022-11-18 15:03:28,134:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5010 (1.5047)
+2022-11-18 15:03:28,287:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5277 (1.5070)
+2022-11-18 15:03:28,442:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5698 (1.5125)
+2022-11-18 15:03:28,593:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4892 (1.5106)
+2022-11-18 15:03:28,746:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5470 (1.5133)
+2022-11-18 15:03:28,901:INFO: Dataset: zara2               Batch: 14/18	Loss 1.4911 (1.5118)
+2022-11-18 15:03:29,057:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5702 (1.5157)
+2022-11-18 15:03:29,213:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5085 (1.5152)
+2022-11-18 15:03:29,368:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5666 (1.5182)
+2022-11-18 15:03:29,509:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7009 (1.5260)
+2022-11-18 15:03:29,556:INFO: - Computing loss (validation)
+2022-11-18 15:03:29,823:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5926 (1.5926)
+2022-11-18 15:03:29,857:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4267 (1.5824)
+2022-11-18 15:03:30,164:INFO: Dataset: univ                Batch: 1/3	Loss 1.5522 (1.5522)
+2022-11-18 15:03:30,243:INFO: Dataset: univ                Batch: 2/3	Loss 1.5082 (1.5275)
+2022-11-18 15:03:30,317:INFO: Dataset: univ                Batch: 3/3	Loss 1.5224 (1.5256)
+2022-11-18 15:03:30,626:INFO: Dataset: zara1               Batch: 1/2	Loss 1.4858 (1.4858)
+2022-11-18 15:03:30,670:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6437 (1.5243)
+2022-11-18 15:03:30,981:INFO: Dataset: zara2               Batch: 1/5	Loss 1.5102 (1.5102)
+2022-11-18 15:03:31,054:INFO: Dataset: zara2               Batch: 2/5	Loss 1.5322 (1.5208)
+2022-11-18 15:03:31,129:INFO: Dataset: zara2               Batch: 3/5	Loss 1.4797 (1.5067)
+2022-11-18 15:03:31,203:INFO: Dataset: zara2               Batch: 4/5	Loss 1.5129 (1.5082)
+2022-11-18 15:03:31,278:INFO: Dataset: zara2               Batch: 5/5	Loss 1.5244 (1.5115)
+2022-11-18 15:03:31,331:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_279.pth.tar
+2022-11-18 15:03:31,331:INFO: 
+===> EPOCH: 280 (P2)
+2022-11-18 15:03:31,332:INFO: - Computing loss (training)
+2022-11-18 15:03:31,674:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5379 (1.5379)
+2022-11-18 15:03:31,827:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6871 (1.6128)
+2022-11-18 15:03:31,976:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6379 (1.6213)
+2022-11-18 15:03:32,091:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6296 (1.6227)
+2022-11-18 15:03:32,479:INFO: Dataset: univ                Batch:  1/15	Loss 1.5574 (1.5574)
+2022-11-18 15:03:32,641:INFO: Dataset: univ                Batch:  2/15	Loss 1.5227 (1.5390)
+2022-11-18 15:03:32,801:INFO: Dataset: univ                Batch:  3/15	Loss 1.5458 (1.5415)
+2022-11-18 15:03:32,957:INFO: Dataset: univ                Batch:  4/15	Loss 1.5404 (1.5412)
+2022-11-18 15:03:33,116:INFO: Dataset: univ                Batch:  5/15	Loss 1.5168 (1.5363)
+2022-11-18 15:03:33,274:INFO: Dataset: univ                Batch:  6/15	Loss 1.5344 (1.5359)
+2022-11-18 15:03:33,429:INFO: Dataset: univ                Batch:  7/15	Loss 1.5612 (1.5397)
+2022-11-18 15:03:33,582:INFO: Dataset: univ                Batch:  8/15	Loss 1.5195 (1.5373)
+2022-11-18 15:03:33,737:INFO: Dataset: univ                Batch:  9/15	Loss 1.5404 (1.5376)
+2022-11-18 15:03:33,891:INFO: Dataset: univ                Batch: 10/15	Loss 1.5636 (1.5402)
+2022-11-18 15:03:34,047:INFO: Dataset: univ                Batch: 11/15	Loss 1.5138 (1.5377)
+2022-11-18 15:03:34,204:INFO: Dataset: univ                Batch: 12/15	Loss 1.5583 (1.5396)
+2022-11-18 15:03:34,360:INFO: Dataset: univ                Batch: 13/15	Loss 1.5583 (1.5411)
+2022-11-18 15:03:34,513:INFO: Dataset: univ                Batch: 14/15	Loss 1.5526 (1.5419)
+2022-11-18 15:03:34,596:INFO: Dataset: univ                Batch: 15/15	Loss 1.5878 (1.5424)
+2022-11-18 15:03:34,986:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5941 (1.5941)
+2022-11-18 15:03:35,134:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5227 (1.5561)
+2022-11-18 15:03:35,287:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5270 (1.5465)
+2022-11-18 15:03:35,435:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6050 (1.5617)
+2022-11-18 15:03:35,586:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6425 (1.5780)
+2022-11-18 15:03:35,735:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6158 (1.5848)
+2022-11-18 15:03:35,884:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6293 (1.5910)
+2022-11-18 15:03:36,020:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6027 (1.5924)
+2022-11-18 15:03:36,399:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5010 (1.5010)
+2022-11-18 15:03:36,547:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5423 (1.5216)
+2022-11-18 15:03:36,703:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5813 (1.5413)
+2022-11-18 15:03:36,851:INFO: Dataset: zara2               Batch:  4/18	Loss 1.4405 (1.5181)
+2022-11-18 15:03:37,001:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4864 (1.5112)
+2022-11-18 15:03:37,154:INFO: Dataset: zara2               Batch:  6/18	Loss 1.5293 (1.5142)
+2022-11-18 15:03:37,303:INFO: Dataset: zara2               Batch:  7/18	Loss 1.5511 (1.5198)
+2022-11-18 15:03:37,452:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5571 (1.5241)
+2022-11-18 15:03:37,601:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5257 (1.5242)
+2022-11-18 15:03:37,749:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5256 (1.5244)
+2022-11-18 15:03:37,897:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5582 (1.5278)
+2022-11-18 15:03:38,046:INFO: Dataset: zara2               Batch: 12/18	Loss 1.5348 (1.5284)
+2022-11-18 15:03:38,196:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5447 (1.5296)
+2022-11-18 15:03:38,345:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5723 (1.5327)
+2022-11-18 15:03:38,494:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5374 (1.5330)
+2022-11-18 15:03:38,643:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5589 (1.5347)
+2022-11-18 15:03:38,797:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5686 (1.5365)
+2022-11-18 15:03:38,942:INFO: Dataset: zara2               Batch: 18/18	Loss 1.5659 (1.5378)
+2022-11-18 15:03:38,986:INFO: - Computing loss (validation)
+2022-11-18 15:03:39,248:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5919 (1.5919)
+2022-11-18 15:03:39,282:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8121 (1.6077)
+2022-11-18 15:03:39,584:INFO: Dataset: univ                Batch: 1/3	Loss 1.5195 (1.5195)
+2022-11-18 15:03:39,659:INFO: Dataset: univ                Batch: 2/3	Loss 1.5319 (1.5251)
+2022-11-18 15:03:39,731:INFO: Dataset: univ                Batch: 3/3	Loss 1.5338 (1.5277)
+2022-11-18 15:03:40,033:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6963 (1.6963)
+2022-11-18 15:03:40,079:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6631 (1.6882)
+2022-11-18 15:03:40,389:INFO: Dataset: zara2               Batch: 1/5	Loss 1.5106 (1.5106)
+2022-11-18 15:03:40,464:INFO: Dataset: zara2               Batch: 2/5	Loss 1.5490 (1.5300)
+2022-11-18 15:03:40,538:INFO: Dataset: zara2               Batch: 3/5	Loss 1.5619 (1.5410)
+2022-11-18 15:03:40,614:INFO: Dataset: zara2               Batch: 4/5	Loss 1.4994 (1.5310)
+2022-11-18 15:03:40,688:INFO: Dataset: zara2               Batch: 5/5	Loss 1.5698 (1.5387)
+2022-11-18 15:03:40,740:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_280.pth.tar
+2022-11-18 15:03:40,741:INFO: 
+===> EPOCH: 281 (P2)
+2022-11-18 15:03:40,741:INFO: - Computing loss (training)
+2022-11-18 15:03:41,084:INFO: Dataset: hotel               Batch: 1/4	Loss 1.5899 (1.5899)
+2022-11-18 15:03:41,241:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5615 (1.5764)
+2022-11-18 15:03:41,396:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5863 (1.5796)
+2022-11-18 15:03:41,515:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6421 (1.5904)
+2022-11-18 15:03:41,920:INFO: Dataset: univ                Batch:  1/15	Loss 1.5602 (1.5602)
+2022-11-18 15:03:42,078:INFO: Dataset: univ                Batch:  2/15	Loss 1.5288 (1.5455)
+2022-11-18 15:03:42,237:INFO: Dataset: univ                Batch:  3/15	Loss 1.5689 (1.5530)
+2022-11-18 15:03:42,397:INFO: Dataset: univ                Batch:  4/15	Loss 1.5428 (1.5505)
+2022-11-18 15:03:42,559:INFO: Dataset: univ                Batch:  5/15	Loss 1.5640 (1.5534)
+2022-11-18 15:03:42,720:INFO: Dataset: univ                Batch:  6/15	Loss 1.5404 (1.5509)
+2022-11-18 15:03:42,879:INFO: Dataset: univ                Batch:  7/15	Loss 1.5650 (1.5529)
+2022-11-18 15:03:43,036:INFO: Dataset: univ                Batch:  8/15	Loss 1.5854 (1.5566)
+2022-11-18 15:03:43,196:INFO: Dataset: univ                Batch:  9/15	Loss 1.5715 (1.5582)
+2022-11-18 15:03:43,354:INFO: Dataset: univ                Batch: 10/15	Loss 1.5647 (1.5589)
+2022-11-18 15:03:43,516:INFO: Dataset: univ                Batch: 11/15	Loss 1.5773 (1.5606)
+2022-11-18 15:03:43,673:INFO: Dataset: univ                Batch: 12/15	Loss 1.5452 (1.5593)
+2022-11-18 15:03:43,833:INFO: Dataset: univ                Batch: 13/15	Loss 1.5238 (1.5564)
+2022-11-18 15:03:43,993:INFO: Dataset: univ                Batch: 14/15	Loss 1.5660 (1.5571)
+2022-11-18 15:03:44,079:INFO: Dataset: univ                Batch: 15/15	Loss 1.4506 (1.5556)
+2022-11-18 15:03:44,475:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5977 (1.5977)
+2022-11-18 15:03:44,626:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5085 (1.5519)
+2022-11-18 15:03:44,781:INFO: Dataset: zara1               Batch: 3/8	Loss 1.4309 (1.5126)
+2022-11-18 15:03:44,932:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5568 (1.5235)
+2022-11-18 15:03:45,088:INFO: Dataset: zara1               Batch: 5/8	Loss 1.5440 (1.5274)
+2022-11-18 15:03:45,242:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4777 (1.5186)
+2022-11-18 15:03:45,395:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5273 (1.5200)
+2022-11-18 15:03:45,534:INFO: Dataset: zara1               Batch: 8/8	Loss 1.5109 (1.5191)
+2022-11-18 15:03:45,931:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5722 (1.5722)
+2022-11-18 15:03:46,090:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5438 (1.5574)
+2022-11-18 15:03:46,245:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5692 (1.5616)
+2022-11-18 15:03:46,398:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6264 (1.5776)
+2022-11-18 15:03:46,552:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6416 (1.5912)
+2022-11-18 15:03:46,710:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4942 (1.5755)
+2022-11-18 15:03:46,864:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4763 (1.5619)
+2022-11-18 15:03:47,016:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6410 (1.5715)
+2022-11-18 15:03:47,169:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5958 (1.5743)
+2022-11-18 15:03:47,321:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6621 (1.5838)
+2022-11-18 15:03:47,475:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4975 (1.5765)
+2022-11-18 15:03:47,632:INFO: Dataset: zara2               Batch: 12/18	Loss 1.5281 (1.5728)
+2022-11-18 15:03:47,785:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5361 (1.5705)
+2022-11-18 15:03:47,938:INFO: Dataset: zara2               Batch: 14/18	Loss 1.4853 (1.5650)
+2022-11-18 15:03:48,093:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5177 (1.5619)
+2022-11-18 15:03:48,247:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5571 (1.5616)
+2022-11-18 15:03:48,401:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5170 (1.5589)
+2022-11-18 15:03:48,541:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6109 (1.5615)
+2022-11-18 15:03:48,591:INFO: - Computing loss (validation)
+2022-11-18 15:03:48,852:INFO: Dataset: hotel               Batch: 1/2	Loss 1.6513 (1.6513)
+2022-11-18 15:03:48,885:INFO: Dataset: hotel               Batch: 2/2	Loss 1.3588 (1.6294)
+2022-11-18 15:03:49,188:INFO: Dataset: univ                Batch: 1/3	Loss 1.5694 (1.5694)
+2022-11-18 15:03:49,267:INFO: Dataset: univ                Batch: 2/3	Loss 1.5354 (1.5511)
+2022-11-18 15:03:49,341:INFO: Dataset: univ                Batch: 3/3	Loss 1.5696 (1.5567)
+2022-11-18 15:03:49,651:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6402 (1.6402)
+2022-11-18 15:03:49,699:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7156 (1.6574)
+2022-11-18 15:03:50,003:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6204 (1.6204)
+2022-11-18 15:03:50,077:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6373 (1.6289)
+2022-11-18 15:03:50,151:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6232 (1.6271)
+2022-11-18 15:03:50,224:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6356 (1.6291)
+2022-11-18 15:03:50,297:INFO: Dataset: zara2               Batch: 5/5	Loss 1.5844 (1.6206)
+2022-11-18 15:03:50,349:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_281.pth.tar
+2022-11-18 15:03:50,349:INFO: 
+===> EPOCH: 282 (P2)
+2022-11-18 15:03:50,350:INFO: - Computing loss (training)
+2022-11-18 15:03:50,696:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6424 (1.6424)
+2022-11-18 15:03:50,854:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6252 (1.6333)
+2022-11-18 15:03:51,010:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7190 (1.6609)
+2022-11-18 15:03:51,130:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6981 (1.6676)
+2022-11-18 15:03:51,570:INFO: Dataset: univ                Batch:  1/15	Loss 1.5404 (1.5404)
+2022-11-18 15:03:51,733:INFO: Dataset: univ                Batch:  2/15	Loss 1.5569 (1.5493)
+2022-11-18 15:03:51,895:INFO: Dataset: univ                Batch:  3/15	Loss 1.5693 (1.5563)
+2022-11-18 15:03:52,055:INFO: Dataset: univ                Batch:  4/15	Loss 1.5366 (1.5508)
+2022-11-18 15:03:52,215:INFO: Dataset: univ                Batch:  5/15	Loss 1.5602 (1.5528)
+2022-11-18 15:03:52,379:INFO: Dataset: univ                Batch:  6/15	Loss 1.5519 (1.5526)
+2022-11-18 15:03:52,538:INFO: Dataset: univ                Batch:  7/15	Loss 1.5565 (1.5532)
+2022-11-18 15:03:52,696:INFO: Dataset: univ                Batch:  8/15	Loss 1.5561 (1.5535)
+2022-11-18 15:03:52,856:INFO: Dataset: univ                Batch:  9/15	Loss 1.5591 (1.5542)
+2022-11-18 15:03:53,016:INFO: Dataset: univ                Batch: 10/15	Loss 1.5691 (1.5557)
+2022-11-18 15:03:53,176:INFO: Dataset: univ                Batch: 11/15	Loss 1.5798 (1.5581)
+2022-11-18 15:03:53,335:INFO: Dataset: univ                Batch: 12/15	Loss 1.5721 (1.5592)
+2022-11-18 15:03:53,495:INFO: Dataset: univ                Batch: 13/15	Loss 1.5811 (1.5608)
+2022-11-18 15:03:53,656:INFO: Dataset: univ                Batch: 14/15	Loss 1.5413 (1.5592)
+2022-11-18 15:03:53,740:INFO: Dataset: univ                Batch: 15/15	Loss 1.4350 (1.5579)
+2022-11-18 15:03:54,141:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6519 (1.6519)
+2022-11-18 15:03:54,296:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6784 (1.6646)
+2022-11-18 15:03:54,451:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6396 (1.6567)
+2022-11-18 15:03:54,608:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5730 (1.6366)
+2022-11-18 15:03:54,764:INFO: Dataset: zara1               Batch: 5/8	Loss 1.5821 (1.6268)
+2022-11-18 15:03:54,919:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6004 (1.6223)
+2022-11-18 15:03:55,074:INFO: Dataset: zara1               Batch: 7/8	Loss 1.5267 (1.6101)
+2022-11-18 15:03:55,216:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6937 (1.6196)
+2022-11-18 15:03:55,619:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6348 (1.6348)
+2022-11-18 15:03:55,775:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6245 (1.6290)
+2022-11-18 15:03:55,931:INFO: Dataset: zara2               Batch:  3/18	Loss 1.5565 (1.6037)
+2022-11-18 15:03:56,089:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6021 (1.6034)
+2022-11-18 15:03:56,247:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5647 (1.5960)
+2022-11-18 15:03:56,402:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4965 (1.5800)
+2022-11-18 15:03:56,557:INFO: Dataset: zara2               Batch:  7/18	Loss 1.6053 (1.5844)
+2022-11-18 15:03:56,709:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6467 (1.5923)
+2022-11-18 15:03:56,864:INFO: Dataset: zara2               Batch:  9/18	Loss 1.6092 (1.5940)
+2022-11-18 15:03:57,016:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6310 (1.5976)
+2022-11-18 15:03:57,170:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5375 (1.5919)
+2022-11-18 15:03:57,325:INFO: Dataset: zara2               Batch: 12/18	Loss 1.5780 (1.5908)
+2022-11-18 15:03:57,480:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6311 (1.5937)
+2022-11-18 15:03:57,633:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5746 (1.5922)
+2022-11-18 15:03:57,789:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6531 (1.5962)
+2022-11-18 15:03:57,943:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6380 (1.5987)
+2022-11-18 15:03:58,098:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6458 (1.6013)
+2022-11-18 15:03:58,242:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4106 (1.5928)
+2022-11-18 15:03:58,286:INFO: - Computing loss (validation)
+2022-11-18 15:03:58,550:INFO: Dataset: hotel               Batch: 1/2	Loss 1.6014 (1.6014)
+2022-11-18 15:03:58,584:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6262 (1.6033)
+2022-11-18 15:03:58,893:INFO: Dataset: univ                Batch: 1/3	Loss 1.5742 (1.5742)
+2022-11-18 15:03:58,969:INFO: Dataset: univ                Batch: 2/3	Loss 1.5750 (1.5746)
+2022-11-18 15:03:59,042:INFO: Dataset: univ                Batch: 3/3	Loss 1.5440 (1.5640)
+2022-11-18 15:03:59,335:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6480 (1.6480)
+2022-11-18 15:03:59,380:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6833 (1.6558)
+2022-11-18 15:03:59,685:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6383 (1.6383)
+2022-11-18 15:03:59,765:INFO: Dataset: zara2               Batch: 2/5	Loss 1.5775 (1.6071)
+2022-11-18 15:03:59,844:INFO: Dataset: zara2               Batch: 3/5	Loss 1.5733 (1.5951)
+2022-11-18 15:03:59,924:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6138 (1.5995)
+2022-11-18 15:03:59,999:INFO: Dataset: zara2               Batch: 5/5	Loss 1.5886 (1.5974)
+2022-11-18 15:04:00,052:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_282.pth.tar
+2022-11-18 15:04:00,052:INFO: 
+===> EPOCH: 283 (P2)
+2022-11-18 15:04:00,053:INFO: - Computing loss (training)
+2022-11-18 15:04:00,403:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6139 (1.6139)
+2022-11-18 15:04:00,558:INFO: Dataset: hotel               Batch: 2/4	Loss 1.6081 (1.6109)
+2022-11-18 15:04:00,713:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6254 (1.6158)
+2022-11-18 15:04:00,833:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7518 (1.6403)
+2022-11-18 15:04:01,242:INFO: Dataset: univ                Batch:  1/15	Loss 1.5860 (1.5860)
+2022-11-18 15:04:01,404:INFO: Dataset: univ                Batch:  2/15	Loss 1.5820 (1.5840)
+2022-11-18 15:04:01,568:INFO: Dataset: univ                Batch:  3/15	Loss 1.5876 (1.5852)
+2022-11-18 15:04:01,727:INFO: Dataset: univ                Batch:  4/15	Loss 1.6258 (1.5955)
+2022-11-18 15:04:01,891:INFO: Dataset: univ                Batch:  5/15	Loss 1.6058 (1.5977)
+2022-11-18 15:04:02,053:INFO: Dataset: univ                Batch:  6/15	Loss 1.6093 (1.5996)
+2022-11-18 15:04:02,213:INFO: Dataset: univ                Batch:  7/15	Loss 1.5419 (1.5914)
+2022-11-18 15:04:02,371:INFO: Dataset: univ                Batch:  8/15	Loss 1.6112 (1.5940)
+2022-11-18 15:04:02,531:INFO: Dataset: univ                Batch:  9/15	Loss 1.6017 (1.5949)
+2022-11-18 15:04:02,693:INFO: Dataset: univ                Batch: 10/15	Loss 1.5226 (1.5878)
+2022-11-18 15:04:02,856:INFO: Dataset: univ                Batch: 11/15	Loss 1.5542 (1.5848)
+2022-11-18 15:04:03,016:INFO: Dataset: univ                Batch: 12/15	Loss 1.5846 (1.5848)
+2022-11-18 15:04:03,176:INFO: Dataset: univ                Batch: 13/15	Loss 1.5457 (1.5817)
+2022-11-18 15:04:03,337:INFO: Dataset: univ                Batch: 14/15	Loss 1.5813 (1.5817)
+2022-11-18 15:04:03,422:INFO: Dataset: univ                Batch: 15/15	Loss 1.5364 (1.5811)
+2022-11-18 15:04:03,815:INFO: Dataset: zara1               Batch: 1/8	Loss 1.5955 (1.5955)
+2022-11-18 15:04:03,961:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6768 (1.6329)
+2022-11-18 15:04:04,113:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6257 (1.6304)
+2022-11-18 15:04:04,261:INFO: Dataset: zara1               Batch: 4/8	Loss 1.5725 (1.6167)
+2022-11-18 15:04:04,410:INFO: Dataset: zara1               Batch: 5/8	Loss 1.5461 (1.6015)
+2022-11-18 15:04:04,559:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6413 (1.6083)
+2022-11-18 15:04:04,708:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6745 (1.6179)
+2022-11-18 15:04:04,843:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6604 (1.6223)
+2022-11-18 15:04:05,259:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6282 (1.6282)
+2022-11-18 15:04:05,409:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5467 (1.5886)
+2022-11-18 15:04:05,561:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6186 (1.5990)
+2022-11-18 15:04:05,716:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7023 (1.6234)
+2022-11-18 15:04:05,869:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5541 (1.6100)
+2022-11-18 15:04:06,022:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6362 (1.6146)
+2022-11-18 15:04:06,173:INFO: Dataset: zara2               Batch:  7/18	Loss 1.5851 (1.6104)
+2022-11-18 15:04:06,322:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6600 (1.6169)
+2022-11-18 15:04:06,472:INFO: Dataset: zara2               Batch:  9/18	Loss 1.5769 (1.6121)
+2022-11-18 15:04:06,620:INFO: Dataset: zara2               Batch: 10/18	Loss 1.5802 (1.6091)
+2022-11-18 15:04:06,771:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5667 (1.6053)
+2022-11-18 15:04:06,919:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6064 (1.6054)
+2022-11-18 15:04:07,070:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6357 (1.6077)
+2022-11-18 15:04:07,248:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5588 (1.6041)
+2022-11-18 15:04:07,424:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6189 (1.6051)
+2022-11-18 15:04:07,574:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6489 (1.6081)
+2022-11-18 15:04:07,725:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5492 (1.6043)
+2022-11-18 15:04:07,864:INFO: Dataset: zara2               Batch: 18/18	Loss 1.5780 (1.6031)
+2022-11-18 15:04:07,911:INFO: - Computing loss (validation)
+2022-11-18 15:04:08,176:INFO: Dataset: hotel               Batch: 1/2	Loss 1.6693 (1.6693)
+2022-11-18 15:04:08,210:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0480 (1.6977)
+2022-11-18 15:04:08,508:INFO: Dataset: univ                Batch: 1/3	Loss 1.6355 (1.6355)
+2022-11-18 15:04:08,584:INFO: Dataset: univ                Batch: 2/3	Loss 1.5708 (1.6037)
+2022-11-18 15:04:08,656:INFO: Dataset: univ                Batch: 3/3	Loss 1.5717 (1.5938)
+2022-11-18 15:04:08,962:INFO: Dataset: zara1               Batch: 1/2	Loss 1.5699 (1.5699)
+2022-11-18 15:04:09,009:INFO: Dataset: zara1               Batch: 2/2	Loss 1.5228 (1.5575)
+2022-11-18 15:04:09,315:INFO: Dataset: zara2               Batch: 1/5	Loss 1.5000 (1.5000)
+2022-11-18 15:04:09,389:INFO: Dataset: zara2               Batch: 2/5	Loss 1.5238 (1.5115)
+2022-11-18 15:04:09,466:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6007 (1.5408)
+2022-11-18 15:04:09,541:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6189 (1.5589)
+2022-11-18 15:04:09,616:INFO: Dataset: zara2               Batch: 5/5	Loss 1.6058 (1.5681)
+2022-11-18 15:04:09,670:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_283.pth.tar
+2022-11-18 15:04:09,671:INFO: 
+===> EPOCH: 284 (P2)
+2022-11-18 15:04:09,671:INFO: - Computing loss (training)
+2022-11-18 15:04:10,019:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6657 (1.6657)
+2022-11-18 15:04:10,172:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7564 (1.7076)
+2022-11-18 15:04:10,323:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6176 (1.6775)
+2022-11-18 15:04:10,438:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7137 (1.6836)
+2022-11-18 15:04:10,857:INFO: Dataset: univ                Batch:  1/15	Loss 1.6329 (1.6329)
+2022-11-18 15:04:11,016:INFO: Dataset: univ                Batch:  2/15	Loss 1.6284 (1.6307)
+2022-11-18 15:04:11,174:INFO: Dataset: univ                Batch:  3/15	Loss 1.6167 (1.6258)
+2022-11-18 15:04:11,338:INFO: Dataset: univ                Batch:  4/15	Loss 1.5852 (1.6170)
+2022-11-18 15:04:11,499:INFO: Dataset: univ                Batch:  5/15	Loss 1.5964 (1.6127)
+2022-11-18 15:04:11,655:INFO: Dataset: univ                Batch:  6/15	Loss 1.5934 (1.6093)
+2022-11-18 15:04:11,812:INFO: Dataset: univ                Batch:  7/15	Loss 1.6012 (1.6081)
+2022-11-18 15:04:11,968:INFO: Dataset: univ                Batch:  8/15	Loss 1.6050 (1.6077)
+2022-11-18 15:04:12,125:INFO: Dataset: univ                Batch:  9/15	Loss 1.6161 (1.6087)
+2022-11-18 15:04:12,281:INFO: Dataset: univ                Batch: 10/15	Loss 1.6064 (1.6084)
+2022-11-18 15:04:12,440:INFO: Dataset: univ                Batch: 11/15	Loss 1.6150 (1.6090)
+2022-11-18 15:04:12,600:INFO: Dataset: univ                Batch: 12/15	Loss 1.5838 (1.6069)
+2022-11-18 15:04:12,758:INFO: Dataset: univ                Batch: 13/15	Loss 1.6238 (1.6080)
+2022-11-18 15:04:12,916:INFO: Dataset: univ                Batch: 14/15	Loss 1.6019 (1.6076)
+2022-11-18 15:04:13,001:INFO: Dataset: univ                Batch: 15/15	Loss 1.6357 (1.6080)
+2022-11-18 15:04:13,404:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7121 (1.7121)
+2022-11-18 15:04:13,555:INFO: Dataset: zara1               Batch: 2/8	Loss 1.5929 (1.6532)
+2022-11-18 15:04:13,705:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6081 (1.6375)
+2022-11-18 15:04:13,853:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6986 (1.6531)
+2022-11-18 15:04:14,004:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6327 (1.6486)
+2022-11-18 15:04:14,154:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6919 (1.6560)
+2022-11-18 15:04:14,384:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6627 (1.6568)
+2022-11-18 15:04:14,520:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6548 (1.6566)
+2022-11-18 15:04:14,982:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6662 (1.6662)
+2022-11-18 15:04:15,132:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6553 (1.6609)
+2022-11-18 15:04:15,281:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6710 (1.6642)
+2022-11-18 15:04:15,437:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6424 (1.6589)
+2022-11-18 15:04:15,584:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6326 (1.6536)
+2022-11-18 15:04:15,734:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7308 (1.6651)
+2022-11-18 15:04:15,886:INFO: Dataset: zara2               Batch:  7/18	Loss 1.6243 (1.6595)
+2022-11-18 15:04:16,035:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6094 (1.6532)
+2022-11-18 15:04:16,182:INFO: Dataset: zara2               Batch:  9/18	Loss 1.6215 (1.6494)
+2022-11-18 15:04:16,328:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6296 (1.6476)
+2022-11-18 15:04:16,478:INFO: Dataset: zara2               Batch: 11/18	Loss 1.5680 (1.6405)
+2022-11-18 15:04:16,626:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6192 (1.6388)
+2022-11-18 15:04:16,778:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6313 (1.6382)
+2022-11-18 15:04:16,925:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5360 (1.6311)
+2022-11-18 15:04:17,074:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6352 (1.6313)
+2022-11-18 15:04:17,221:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7115 (1.6373)
+2022-11-18 15:04:17,372:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6274 (1.6367)
+2022-11-18 15:04:17,510:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6952 (1.6395)
+2022-11-18 15:04:17,554:INFO: - Computing loss (validation)
+2022-11-18 15:04:17,823:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5987 (1.5987)
+2022-11-18 15:04:17,858:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8557 (1.6144)
+2022-11-18 15:04:18,169:INFO: Dataset: univ                Batch: 1/3	Loss 1.6155 (1.6155)
+2022-11-18 15:04:18,246:INFO: Dataset: univ                Batch: 2/3	Loss 1.5689 (1.5910)
+2022-11-18 15:04:18,319:INFO: Dataset: univ                Batch: 3/3	Loss 1.5922 (1.5914)
+2022-11-18 15:04:18,650:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6059 (1.6059)
+2022-11-18 15:04:18,698:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7006 (1.6306)
+2022-11-18 15:04:19,003:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6345 (1.6345)
+2022-11-18 15:04:19,077:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6132 (1.6238)
+2022-11-18 15:04:19,151:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7303 (1.6583)
+2022-11-18 15:04:19,226:INFO: Dataset: zara2               Batch: 4/5	Loss 1.5748 (1.6391)
+2022-11-18 15:04:19,299:INFO: Dataset: zara2               Batch: 5/5	Loss 1.6546 (1.6420)
+2022-11-18 15:04:19,355:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_284.pth.tar
+2022-11-18 15:04:19,355:INFO: 
+===> EPOCH: 285 (P2)
+2022-11-18 15:04:19,356:INFO: - Computing loss (training)
+2022-11-18 15:04:19,695:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7373 (1.7373)
+2022-11-18 15:04:19,863:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5569 (1.6502)
+2022-11-18 15:04:20,042:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7182 (1.6745)
+2022-11-18 15:04:20,179:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8056 (1.6946)
+2022-11-18 15:04:20,594:INFO: Dataset: univ                Batch:  1/15	Loss 1.6399 (1.6399)
+2022-11-18 15:04:20,753:INFO: Dataset: univ                Batch:  2/15	Loss 1.6312 (1.6353)
+2022-11-18 15:04:20,912:INFO: Dataset: univ                Batch:  3/15	Loss 1.6669 (1.6457)
+2022-11-18 15:04:21,070:INFO: Dataset: univ                Batch:  4/15	Loss 1.5938 (1.6322)
+2022-11-18 15:04:21,228:INFO: Dataset: univ                Batch:  5/15	Loss 1.6334 (1.6324)
+2022-11-18 15:04:21,386:INFO: Dataset: univ                Batch:  6/15	Loss 1.6295 (1.6319)
+2022-11-18 15:04:21,540:INFO: Dataset: univ                Batch:  7/15	Loss 1.6201 (1.6302)
+2022-11-18 15:04:21,695:INFO: Dataset: univ                Batch:  8/15	Loss 1.6202 (1.6289)
+2022-11-18 15:04:21,856:INFO: Dataset: univ                Batch:  9/15	Loss 1.6214 (1.6281)
+2022-11-18 15:04:22,010:INFO: Dataset: univ                Batch: 10/15	Loss 1.6443 (1.6296)
+2022-11-18 15:04:22,166:INFO: Dataset: univ                Batch: 11/15	Loss 1.6595 (1.6324)
+2022-11-18 15:04:22,320:INFO: Dataset: univ                Batch: 12/15	Loss 1.6371 (1.6328)
+2022-11-18 15:04:22,476:INFO: Dataset: univ                Batch: 13/15	Loss 1.6817 (1.6368)
+2022-11-18 15:04:22,629:INFO: Dataset: univ                Batch: 14/15	Loss 1.5948 (1.6336)
+2022-11-18 15:04:22,711:INFO: Dataset: univ                Batch: 15/15	Loss 1.6823 (1.6342)
+2022-11-18 15:04:23,109:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7180 (1.7180)
+2022-11-18 15:04:23,262:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6455 (1.6825)
+2022-11-18 15:04:23,414:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5969 (1.6548)
+2022-11-18 15:04:23,566:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6347 (1.6496)
+2022-11-18 15:04:23,718:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6642 (1.6526)
+2022-11-18 15:04:23,870:INFO: Dataset: zara1               Batch: 6/8	Loss 1.5876 (1.6420)
+2022-11-18 15:04:24,021:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6111 (1.6369)
+2022-11-18 15:04:24,159:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7299 (1.6471)
+2022-11-18 15:04:24,555:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7144 (1.7144)
+2022-11-18 15:04:24,707:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6549 (1.6830)
+2022-11-18 15:04:24,857:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6263 (1.6620)
+2022-11-18 15:04:25,009:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6527 (1.6596)
+2022-11-18 15:04:25,158:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6492 (1.6573)
+2022-11-18 15:04:25,309:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7153 (1.6674)
+2022-11-18 15:04:25,459:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7077 (1.6721)
+2022-11-18 15:04:25,605:INFO: Dataset: zara2               Batch:  8/18	Loss 1.5843 (1.6606)
+2022-11-18 15:04:25,753:INFO: Dataset: zara2               Batch:  9/18	Loss 1.6722 (1.6620)
+2022-11-18 15:04:25,900:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6775 (1.6637)
+2022-11-18 15:04:26,052:INFO: Dataset: zara2               Batch: 11/18	Loss 1.6578 (1.6632)
+2022-11-18 15:04:26,199:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6910 (1.6654)
+2022-11-18 15:04:26,348:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6150 (1.6615)
+2022-11-18 15:04:26,496:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6267 (1.6588)
+2022-11-18 15:04:26,645:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5555 (1.6518)
+2022-11-18 15:04:26,794:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6774 (1.6534)
+2022-11-18 15:04:26,944:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6695 (1.6543)
+2022-11-18 15:04:27,082:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6404 (1.6536)
+2022-11-18 15:04:27,130:INFO: - Computing loss (validation)
+2022-11-18 15:04:27,390:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7125 (1.7125)
+2022-11-18 15:04:27,425:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5642 (1.7009)
+2022-11-18 15:04:27,719:INFO: Dataset: univ                Batch: 1/3	Loss 1.6372 (1.6372)
+2022-11-18 15:04:27,798:INFO: Dataset: univ                Batch: 2/3	Loss 1.6202 (1.6283)
+2022-11-18 15:04:27,870:INFO: Dataset: univ                Batch: 3/3	Loss 1.6184 (1.6253)
+2022-11-18 15:04:28,174:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7172 (1.7172)
+2022-11-18 15:04:28,218:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7486 (1.7251)
+2022-11-18 15:04:28,528:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6516 (1.6516)
+2022-11-18 15:04:28,602:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6881 (1.6704)
+2022-11-18 15:04:28,677:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6092 (1.6489)
+2022-11-18 15:04:28,751:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6330 (1.6451)
+2022-11-18 15:04:28,825:INFO: Dataset: zara2               Batch: 5/5	Loss 1.5662 (1.6297)
+2022-11-18 15:04:28,877:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_285.pth.tar
+2022-11-18 15:04:28,877:INFO: 
+===> EPOCH: 286 (P2)
+2022-11-18 15:04:28,877:INFO: - Computing loss (training)
+2022-11-18 15:04:29,231:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6797 (1.6797)
+2022-11-18 15:04:29,387:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7266 (1.7031)
+2022-11-18 15:04:29,544:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7224 (1.7097)
+2022-11-18 15:04:29,665:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7174 (1.7111)
+2022-11-18 15:04:30,072:INFO: Dataset: univ                Batch:  1/15	Loss 1.6575 (1.6575)
+2022-11-18 15:04:30,236:INFO: Dataset: univ                Batch:  2/15	Loss 1.6369 (1.6471)
+2022-11-18 15:04:30,403:INFO: Dataset: univ                Batch:  3/15	Loss 1.6275 (1.6401)
+2022-11-18 15:04:30,568:INFO: Dataset: univ                Batch:  4/15	Loss 1.6757 (1.6487)
+2022-11-18 15:04:30,729:INFO: Dataset: univ                Batch:  5/15	Loss 1.6384 (1.6466)
+2022-11-18 15:04:30,892:INFO: Dataset: univ                Batch:  6/15	Loss 1.6695 (1.6502)
+2022-11-18 15:04:31,054:INFO: Dataset: univ                Batch:  7/15	Loss 1.6496 (1.6501)
+2022-11-18 15:04:31,215:INFO: Dataset: univ                Batch:  8/15	Loss 1.5900 (1.6425)
+2022-11-18 15:04:31,378:INFO: Dataset: univ                Batch:  9/15	Loss 1.6813 (1.6470)
+2022-11-18 15:04:31,537:INFO: Dataset: univ                Batch: 10/15	Loss 1.6568 (1.6481)
+2022-11-18 15:04:31,697:INFO: Dataset: univ                Batch: 11/15	Loss 1.6318 (1.6468)
+2022-11-18 15:04:31,859:INFO: Dataset: univ                Batch: 12/15	Loss 1.6424 (1.6464)
+2022-11-18 15:04:32,021:INFO: Dataset: univ                Batch: 13/15	Loss 1.6340 (1.6454)
+2022-11-18 15:04:32,181:INFO: Dataset: univ                Batch: 14/15	Loss 1.6553 (1.6461)
+2022-11-18 15:04:32,268:INFO: Dataset: univ                Batch: 15/15	Loss 1.6236 (1.6457)
+2022-11-18 15:04:32,669:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6179 (1.6179)
+2022-11-18 15:04:32,824:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7573 (1.6884)
+2022-11-18 15:04:32,979:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5526 (1.6397)
+2022-11-18 15:04:33,132:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6778 (1.6486)
+2022-11-18 15:04:33,288:INFO: Dataset: zara1               Batch: 5/8	Loss 1.5578 (1.6304)
+2022-11-18 15:04:33,442:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6724 (1.6375)
+2022-11-18 15:04:33,595:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7216 (1.6499)
+2022-11-18 15:04:33,733:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7836 (1.6623)
+2022-11-18 15:04:34,132:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7122 (1.7122)
+2022-11-18 15:04:34,288:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7068 (1.7097)
+2022-11-18 15:04:34,440:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6469 (1.6892)
+2022-11-18 15:04:34,593:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6426 (1.6775)
+2022-11-18 15:04:34,744:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6885 (1.6800)
+2022-11-18 15:04:34,897:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6729 (1.6788)
+2022-11-18 15:04:35,047:INFO: Dataset: zara2               Batch:  7/18	Loss 1.5857 (1.6650)
+2022-11-18 15:04:35,194:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6806 (1.6671)
+2022-11-18 15:04:35,345:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7162 (1.6724)
+2022-11-18 15:04:35,493:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6718 (1.6723)
+2022-11-18 15:04:35,643:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7090 (1.6757)
+2022-11-18 15:04:35,792:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6983 (1.6776)
+2022-11-18 15:04:35,942:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7306 (1.6818)
+2022-11-18 15:04:36,092:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6557 (1.6799)
+2022-11-18 15:04:36,244:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6595 (1.6787)
+2022-11-18 15:04:36,393:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7429 (1.6830)
+2022-11-18 15:04:36,544:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6350 (1.6801)
+2022-11-18 15:04:36,682:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6359 (1.6780)
+2022-11-18 15:04:36,727:INFO: - Computing loss (validation)
+2022-11-18 15:04:36,999:INFO: Dataset: hotel               Batch: 1/2	Loss 1.6321 (1.6321)
+2022-11-18 15:04:37,034:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5753 (1.6277)
+2022-11-18 15:04:37,342:INFO: Dataset: univ                Batch: 1/3	Loss 1.6564 (1.6564)
+2022-11-18 15:04:37,419:INFO: Dataset: univ                Batch: 2/3	Loss 1.6548 (1.6556)
+2022-11-18 15:04:37,491:INFO: Dataset: univ                Batch: 3/3	Loss 1.6655 (1.6588)
+2022-11-18 15:04:37,778:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7676 (1.7676)
+2022-11-18 15:04:37,823:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6401 (1.7364)
+2022-11-18 15:04:38,128:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6990 (1.6990)
+2022-11-18 15:04:38,204:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6887 (1.6940)
+2022-11-18 15:04:38,282:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6353 (1.6757)
+2022-11-18 15:04:38,360:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6411 (1.6669)
+2022-11-18 15:04:38,436:INFO: Dataset: zara2               Batch: 5/5	Loss 1.6014 (1.6542)
+2022-11-18 15:04:38,494:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_286.pth.tar
+2022-11-18 15:04:38,494:INFO: 
+===> EPOCH: 287 (P2)
+2022-11-18 15:04:38,495:INFO: - Computing loss (training)
+2022-11-18 15:04:38,835:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7653 (1.7653)
+2022-11-18 15:04:38,989:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7208 (1.7432)
+2022-11-18 15:04:39,142:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6904 (1.7253)
+2022-11-18 15:04:39,258:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7785 (1.7346)
+2022-11-18 15:04:39,652:INFO: Dataset: univ                Batch:  1/15	Loss 1.6902 (1.6902)
+2022-11-18 15:04:39,808:INFO: Dataset: univ                Batch:  2/15	Loss 1.6571 (1.6728)
+2022-11-18 15:04:39,964:INFO: Dataset: univ                Batch:  3/15	Loss 1.6475 (1.6649)
+2022-11-18 15:04:40,122:INFO: Dataset: univ                Batch:  4/15	Loss 1.6696 (1.6660)
+2022-11-18 15:04:40,280:INFO: Dataset: univ                Batch:  5/15	Loss 1.6800 (1.6688)
+2022-11-18 15:04:40,438:INFO: Dataset: univ                Batch:  6/15	Loss 1.6761 (1.6701)
+2022-11-18 15:04:40,592:INFO: Dataset: univ                Batch:  7/15	Loss 1.6738 (1.6706)
+2022-11-18 15:04:40,746:INFO: Dataset: univ                Batch:  8/15	Loss 1.6636 (1.6697)
+2022-11-18 15:04:40,901:INFO: Dataset: univ                Batch:  9/15	Loss 1.6274 (1.6650)
+2022-11-18 15:04:41,055:INFO: Dataset: univ                Batch: 10/15	Loss 1.6417 (1.6626)
+2022-11-18 15:04:41,214:INFO: Dataset: univ                Batch: 11/15	Loss 1.6662 (1.6629)
+2022-11-18 15:04:41,369:INFO: Dataset: univ                Batch: 12/15	Loss 1.6479 (1.6616)
+2022-11-18 15:04:41,523:INFO: Dataset: univ                Batch: 13/15	Loss 1.6684 (1.6620)
+2022-11-18 15:04:41,679:INFO: Dataset: univ                Batch: 14/15	Loss 1.6679 (1.6625)
+2022-11-18 15:04:41,762:INFO: Dataset: univ                Batch: 15/15	Loss 1.6823 (1.6628)
+2022-11-18 15:04:42,155:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7353 (1.7353)
+2022-11-18 15:04:42,305:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7107 (1.7227)
+2022-11-18 15:04:42,458:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7060 (1.7166)
+2022-11-18 15:04:42,605:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6934 (1.7103)
+2022-11-18 15:04:42,753:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6762 (1.7032)
+2022-11-18 15:04:42,904:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7393 (1.7093)
+2022-11-18 15:04:43,053:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6384 (1.6989)
+2022-11-18 15:04:43,188:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8115 (1.7130)
+2022-11-18 15:04:43,575:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7324 (1.7324)
+2022-11-18 15:04:43,723:INFO: Dataset: zara2               Batch:  2/18	Loss 1.5963 (1.6622)
+2022-11-18 15:04:43,876:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6608 (1.6617)
+2022-11-18 15:04:44,023:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6779 (1.6659)
+2022-11-18 15:04:44,174:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6037 (1.6538)
+2022-11-18 15:04:44,323:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6503 (1.6532)
+2022-11-18 15:04:44,471:INFO: Dataset: zara2               Batch:  7/18	Loss 1.6713 (1.6556)
+2022-11-18 15:04:44,616:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7067 (1.6619)
+2022-11-18 15:04:44,764:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7092 (1.6675)
+2022-11-18 15:04:44,910:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6831 (1.6689)
+2022-11-18 15:04:45,060:INFO: Dataset: zara2               Batch: 11/18	Loss 1.6895 (1.6706)
+2022-11-18 15:04:45,207:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6208 (1.6669)
+2022-11-18 15:04:45,355:INFO: Dataset: zara2               Batch: 13/18	Loss 1.6329 (1.6643)
+2022-11-18 15:04:45,503:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7283 (1.6689)
+2022-11-18 15:04:45,652:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7654 (1.6748)
+2022-11-18 15:04:45,798:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7565 (1.6802)
+2022-11-18 15:04:45,947:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6369 (1.6777)
+2022-11-18 15:04:46,083:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6498 (1.6763)
+2022-11-18 15:04:46,128:INFO: - Computing loss (validation)
+2022-11-18 15:04:46,393:INFO: Dataset: hotel               Batch: 1/2	Loss 1.6408 (1.6408)
+2022-11-18 15:04:46,426:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0233 (1.6669)
+2022-11-18 15:04:46,744:INFO: Dataset: univ                Batch: 1/3	Loss 1.6559 (1.6559)
+2022-11-18 15:04:46,828:INFO: Dataset: univ                Batch: 2/3	Loss 1.6723 (1.6646)
+2022-11-18 15:04:46,904:INFO: Dataset: univ                Batch: 3/3	Loss 1.7064 (1.6764)
+2022-11-18 15:04:47,194:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6266 (1.6266)
+2022-11-18 15:04:47,239:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6381 (1.6294)
+2022-11-18 15:04:47,566:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6482 (1.6482)
+2022-11-18 15:04:47,646:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6677 (1.6581)
+2022-11-18 15:04:47,725:INFO: Dataset: zara2               Batch: 3/5	Loss 1.5972 (1.6363)
+2022-11-18 15:04:47,804:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7096 (1.6551)
+2022-11-18 15:04:47,880:INFO: Dataset: zara2               Batch: 5/5	Loss 1.6715 (1.6582)
+2022-11-18 15:04:47,932:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_287.pth.tar
+2022-11-18 15:04:47,933:INFO: 
+===> EPOCH: 288 (P2)
+2022-11-18 15:04:47,933:INFO: - Computing loss (training)
+2022-11-18 15:04:48,264:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6777 (1.6777)
+2022-11-18 15:04:48,415:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7137 (1.6963)
+2022-11-18 15:04:48,565:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6834 (1.6922)
+2022-11-18 15:04:48,678:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6745 (1.6889)
+2022-11-18 15:04:49,061:INFO: Dataset: univ                Batch:  1/15	Loss 1.6788 (1.6788)
+2022-11-18 15:04:49,219:INFO: Dataset: univ                Batch:  2/15	Loss 1.7239 (1.7003)
+2022-11-18 15:04:49,378:INFO: Dataset: univ                Batch:  3/15	Loss 1.6860 (1.6951)
+2022-11-18 15:04:49,537:INFO: Dataset: univ                Batch:  4/15	Loss 1.7090 (1.6988)
+2022-11-18 15:04:49,694:INFO: Dataset: univ                Batch:  5/15	Loss 1.7242 (1.7040)
+2022-11-18 15:04:49,851:INFO: Dataset: univ                Batch:  6/15	Loss 1.6507 (1.6955)
+2022-11-18 15:04:50,008:INFO: Dataset: univ                Batch:  7/15	Loss 1.6730 (1.6922)
+2022-11-18 15:04:50,162:INFO: Dataset: univ                Batch:  8/15	Loss 1.6497 (1.6869)
+2022-11-18 15:04:50,318:INFO: Dataset: univ                Batch:  9/15	Loss 1.6633 (1.6844)
+2022-11-18 15:04:50,472:INFO: Dataset: univ                Batch: 10/15	Loss 1.6783 (1.6838)
+2022-11-18 15:04:50,629:INFO: Dataset: univ                Batch: 11/15	Loss 1.6928 (1.6847)
+2022-11-18 15:04:50,783:INFO: Dataset: univ                Batch: 12/15	Loss 1.6785 (1.6842)
+2022-11-18 15:04:50,940:INFO: Dataset: univ                Batch: 13/15	Loss 1.7031 (1.6858)
+2022-11-18 15:04:51,095:INFO: Dataset: univ                Batch: 14/15	Loss 1.6815 (1.6855)
+2022-11-18 15:04:51,178:INFO: Dataset: univ                Batch: 15/15	Loss 1.6723 (1.6853)
+2022-11-18 15:04:51,577:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7787 (1.7787)
+2022-11-18 15:04:51,726:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7351 (1.7573)
+2022-11-18 15:04:51,874:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6737 (1.7308)
+2022-11-18 15:04:52,021:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6964 (1.7212)
+2022-11-18 15:04:52,171:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7199 (1.7209)
+2022-11-18 15:04:52,318:INFO: Dataset: zara1               Batch: 6/8	Loss 1.6367 (1.7074)
+2022-11-18 15:04:52,467:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8342 (1.7257)
+2022-11-18 15:04:52,601:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7749 (1.7315)
+2022-11-18 15:04:52,990:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7158 (1.7158)
+2022-11-18 15:04:53,143:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6865 (1.7007)
+2022-11-18 15:04:53,300:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6781 (1.6933)
+2022-11-18 15:04:53,451:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6901 (1.6925)
+2022-11-18 15:04:53,603:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6693 (1.6880)
+2022-11-18 15:04:53,756:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6803 (1.6868)
+2022-11-18 15:04:53,907:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7629 (1.6970)
+2022-11-18 15:04:54,058:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7088 (1.6986)
+2022-11-18 15:04:54,209:INFO: Dataset: zara2               Batch:  9/18	Loss 1.6489 (1.6934)
+2022-11-18 15:04:54,360:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6799 (1.6920)
+2022-11-18 15:04:54,510:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7090 (1.6936)
+2022-11-18 15:04:54,661:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7218 (1.6955)
+2022-11-18 15:04:54,812:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7297 (1.6984)
+2022-11-18 15:04:54,963:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6488 (1.6950)
+2022-11-18 15:04:55,116:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6905 (1.6947)
+2022-11-18 15:04:55,266:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7107 (1.6957)
+2022-11-18 15:04:55,419:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7371 (1.6980)
+2022-11-18 15:04:55,563:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6038 (1.6935)
+2022-11-18 15:04:55,616:INFO: - Computing loss (validation)
+2022-11-18 15:04:55,911:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7569 (1.7569)
+2022-11-18 15:04:55,948:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9298 (1.7687)
+2022-11-18 15:04:56,266:INFO: Dataset: univ                Batch: 1/3	Loss 1.7249 (1.7249)
+2022-11-18 15:04:56,350:INFO: Dataset: univ                Batch: 2/3	Loss 1.6337 (1.6819)
+2022-11-18 15:04:56,427:INFO: Dataset: univ                Batch: 3/3	Loss 1.6598 (1.6757)
+2022-11-18 15:04:56,738:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7714 (1.7714)
+2022-11-18 15:04:56,786:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8183 (1.7823)
+2022-11-18 15:04:57,153:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6950 (1.6950)
+2022-11-18 15:04:57,238:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6921 (1.6936)
+2022-11-18 15:04:57,321:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6974 (1.6949)
+2022-11-18 15:04:57,408:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6959 (1.6951)
+2022-11-18 15:04:57,496:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7017 (1.6964)
+2022-11-18 15:04:57,569:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_288.pth.tar
+2022-11-18 15:04:57,569:INFO: 
+===> EPOCH: 289 (P2)
+2022-11-18 15:04:57,570:INFO: - Computing loss (training)
+2022-11-18 15:04:57,964:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7927 (1.7927)
+2022-11-18 15:04:58,117:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8751 (1.8347)
+2022-11-18 15:04:58,271:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8516 (1.8402)
+2022-11-18 15:04:58,392:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8304 (1.8383)
+2022-11-18 15:04:58,856:INFO: Dataset: univ                Batch:  1/15	Loss 1.7108 (1.7108)
+2022-11-18 15:04:59,048:INFO: Dataset: univ                Batch:  2/15	Loss 1.6928 (1.7013)
+2022-11-18 15:04:59,237:INFO: Dataset: univ                Batch:  3/15	Loss 1.6799 (1.6940)
+2022-11-18 15:04:59,407:INFO: Dataset: univ                Batch:  4/15	Loss 1.7223 (1.7012)
+2022-11-18 15:04:59,579:INFO: Dataset: univ                Batch:  5/15	Loss 1.7552 (1.7121)
+2022-11-18 15:04:59,751:INFO: Dataset: univ                Batch:  6/15	Loss 1.6954 (1.7095)
+2022-11-18 15:04:59,918:INFO: Dataset: univ                Batch:  7/15	Loss 1.7371 (1.7138)
+2022-11-18 15:05:00,081:INFO: Dataset: univ                Batch:  8/15	Loss 1.6944 (1.7118)
+2022-11-18 15:05:00,243:INFO: Dataset: univ                Batch:  9/15	Loss 1.6819 (1.7085)
+2022-11-18 15:05:00,442:INFO: Dataset: univ                Batch: 10/15	Loss 1.6970 (1.7075)
+2022-11-18 15:05:00,640:INFO: Dataset: univ                Batch: 11/15	Loss 1.7078 (1.7075)
+2022-11-18 15:05:00,814:INFO: Dataset: univ                Batch: 12/15	Loss 1.6640 (1.7042)
+2022-11-18 15:05:00,995:INFO: Dataset: univ                Batch: 13/15	Loss 1.6722 (1.7021)
+2022-11-18 15:05:01,170:INFO: Dataset: univ                Batch: 14/15	Loss 1.7026 (1.7021)
+2022-11-18 15:05:01,269:INFO: Dataset: univ                Batch: 15/15	Loss 1.6027 (1.7009)
+2022-11-18 15:05:01,675:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6365 (1.6365)
+2022-11-18 15:05:01,832:INFO: Dataset: zara1               Batch: 2/8	Loss 1.6756 (1.6536)
+2022-11-18 15:05:02,034:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7082 (1.6716)
+2022-11-18 15:05:02,199:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6258 (1.6601)
+2022-11-18 15:05:02,372:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7490 (1.6768)
+2022-11-18 15:05:02,535:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7909 (1.6993)
+2022-11-18 15:05:02,688:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7259 (1.7032)
+2022-11-18 15:05:02,826:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6742 (1.7001)
+2022-11-18 15:05:03,209:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7818 (1.7818)
+2022-11-18 15:05:03,363:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6488 (1.7179)
+2022-11-18 15:05:03,532:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7028 (1.7126)
+2022-11-18 15:05:03,702:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7797 (1.7290)
+2022-11-18 15:05:03,871:INFO: Dataset: zara2               Batch:  5/18	Loss 1.5998 (1.7014)
+2022-11-18 15:05:04,024:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8154 (1.7178)
+2022-11-18 15:05:04,176:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7216 (1.7183)
+2022-11-18 15:05:04,325:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6885 (1.7145)
+2022-11-18 15:05:04,476:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7467 (1.7181)
+2022-11-18 15:05:04,624:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6908 (1.7154)
+2022-11-18 15:05:04,777:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7760 (1.7201)
+2022-11-18 15:05:04,927:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7585 (1.7234)
+2022-11-18 15:05:05,081:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7220 (1.7233)
+2022-11-18 15:05:05,233:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7299 (1.7237)
+2022-11-18 15:05:05,387:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7393 (1.7247)
+2022-11-18 15:05:05,539:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6287 (1.7185)
+2022-11-18 15:05:05,691:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7585 (1.7209)
+2022-11-18 15:05:05,830:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6644 (1.7182)
+2022-11-18 15:05:05,875:INFO: - Computing loss (validation)
+2022-11-18 15:05:06,143:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7372 (1.7372)
+2022-11-18 15:05:06,177:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5697 (1.7269)
+2022-11-18 15:05:06,485:INFO: Dataset: univ                Batch: 1/3	Loss 1.6785 (1.6785)
+2022-11-18 15:05:06,563:INFO: Dataset: univ                Batch: 2/3	Loss 1.6767 (1.6776)
+2022-11-18 15:05:06,634:INFO: Dataset: univ                Batch: 3/3	Loss 1.6630 (1.6733)
+2022-11-18 15:05:06,926:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8250 (1.8250)
+2022-11-18 15:05:06,970:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6713 (1.7830)
+2022-11-18 15:05:07,269:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7322 (1.7322)
+2022-11-18 15:05:07,344:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7133 (1.7224)
+2022-11-18 15:05:07,421:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6813 (1.7082)
+2022-11-18 15:05:07,497:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7189 (1.7108)
+2022-11-18 15:05:07,571:INFO: Dataset: zara2               Batch: 5/5	Loss 1.6560 (1.7002)
+2022-11-18 15:05:07,622:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_289.pth.tar
+2022-11-18 15:05:07,622:INFO: 
+===> EPOCH: 290 (P2)
+2022-11-18 15:05:07,623:INFO: - Computing loss (training)
+2022-11-18 15:05:07,966:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7744 (1.7744)
+2022-11-18 15:05:08,119:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7878 (1.7814)
+2022-11-18 15:05:08,271:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8054 (1.7891)
+2022-11-18 15:05:08,388:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7460 (1.7818)
+2022-11-18 15:05:08,794:INFO: Dataset: univ                Batch:  1/15	Loss 1.7148 (1.7148)
+2022-11-18 15:05:08,955:INFO: Dataset: univ                Batch:  2/15	Loss 1.7009 (1.7085)
+2022-11-18 15:05:09,120:INFO: Dataset: univ                Batch:  3/15	Loss 1.7147 (1.7105)
+2022-11-18 15:05:09,281:INFO: Dataset: univ                Batch:  4/15	Loss 1.7217 (1.7134)
+2022-11-18 15:05:09,439:INFO: Dataset: univ                Batch:  5/15	Loss 1.7135 (1.7134)
+2022-11-18 15:05:09,598:INFO: Dataset: univ                Batch:  6/15	Loss 1.7453 (1.7184)
+2022-11-18 15:05:09,755:INFO: Dataset: univ                Batch:  7/15	Loss 1.7090 (1.7170)
+2022-11-18 15:05:09,912:INFO: Dataset: univ                Batch:  8/15	Loss 1.7552 (1.7221)
+2022-11-18 15:05:10,069:INFO: Dataset: univ                Batch:  9/15	Loss 1.7143 (1.7212)
+2022-11-18 15:05:10,224:INFO: Dataset: univ                Batch: 10/15	Loss 1.7476 (1.7240)
+2022-11-18 15:05:10,382:INFO: Dataset: univ                Batch: 11/15	Loss 1.6983 (1.7217)
+2022-11-18 15:05:10,540:INFO: Dataset: univ                Batch: 12/15	Loss 1.7590 (1.7250)
+2022-11-18 15:05:10,696:INFO: Dataset: univ                Batch: 13/15	Loss 1.7289 (1.7253)
+2022-11-18 15:05:10,855:INFO: Dataset: univ                Batch: 14/15	Loss 1.7254 (1.7253)
+2022-11-18 15:05:10,939:INFO: Dataset: univ                Batch: 15/15	Loss 1.6876 (1.7250)
+2022-11-18 15:05:11,345:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7471 (1.7471)
+2022-11-18 15:05:11,510:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7389 (1.7428)
+2022-11-18 15:05:11,668:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8002 (1.7620)
+2022-11-18 15:05:11,822:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7579 (1.7610)
+2022-11-18 15:05:11,978:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6898 (1.7453)
+2022-11-18 15:05:12,133:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7673 (1.7495)
+2022-11-18 15:05:12,288:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8020 (1.7567)
+2022-11-18 15:05:12,430:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6987 (1.7503)
+2022-11-18 15:05:12,879:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6962 (1.6962)
+2022-11-18 15:05:13,030:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7264 (1.7118)
+2022-11-18 15:05:13,181:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8457 (1.7581)
+2022-11-18 15:05:13,327:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6570 (1.7316)
+2022-11-18 15:05:13,476:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7612 (1.7377)
+2022-11-18 15:05:13,626:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6309 (1.7196)
+2022-11-18 15:05:13,773:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7339 (1.7217)
+2022-11-18 15:05:13,926:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7063 (1.7199)
+2022-11-18 15:05:14,079:INFO: Dataset: zara2               Batch:  9/18	Loss 1.6998 (1.7175)
+2022-11-18 15:05:14,226:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6686 (1.7130)
+2022-11-18 15:05:14,376:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7545 (1.7168)
+2022-11-18 15:05:14,523:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7376 (1.7186)
+2022-11-18 15:05:14,673:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7651 (1.7217)
+2022-11-18 15:05:14,821:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8204 (1.7290)
+2022-11-18 15:05:14,971:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7455 (1.7301)
+2022-11-18 15:05:15,119:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7168 (1.7293)
+2022-11-18 15:05:15,269:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7882 (1.7329)
+2022-11-18 15:05:15,407:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8176 (1.7372)
+2022-11-18 15:05:15,454:INFO: - Computing loss (validation)
+2022-11-18 15:05:15,720:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7833 (1.7833)
+2022-11-18 15:05:15,754:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7431 (1.7808)
+2022-11-18 15:05:16,058:INFO: Dataset: univ                Batch: 1/3	Loss 1.7023 (1.7023)
+2022-11-18 15:05:16,134:INFO: Dataset: univ                Batch: 2/3	Loss 1.7030 (1.7026)
+2022-11-18 15:05:16,206:INFO: Dataset: univ                Batch: 3/3	Loss 1.7078 (1.7043)
+2022-11-18 15:05:16,509:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8870 (1.8870)
+2022-11-18 15:05:16,556:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7582 (1.8539)
+2022-11-18 15:05:16,858:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7127 (1.7127)
+2022-11-18 15:05:16,936:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6951 (1.7040)
+2022-11-18 15:05:17,015:INFO: Dataset: zara2               Batch: 3/5	Loss 1.6622 (1.6908)
+2022-11-18 15:05:17,092:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6692 (1.6854)
+2022-11-18 15:05:17,169:INFO: Dataset: zara2               Batch: 5/5	Loss 1.6854 (1.6854)
+2022-11-18 15:05:17,223:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_290.pth.tar
+2022-11-18 15:05:17,223:INFO: 
+===> EPOCH: 291 (P2)
+2022-11-18 15:05:17,223:INFO: - Computing loss (training)
+2022-11-18 15:05:17,577:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8178 (1.8178)
+2022-11-18 15:05:17,732:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7580 (1.7878)
+2022-11-18 15:05:17,887:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7774 (1.7844)
+2022-11-18 15:05:18,007:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7263 (1.7749)
+2022-11-18 15:05:18,406:INFO: Dataset: univ                Batch:  1/15	Loss 1.7640 (1.7640)
+2022-11-18 15:05:18,564:INFO: Dataset: univ                Batch:  2/15	Loss 1.7172 (1.7393)
+2022-11-18 15:05:18,718:INFO: Dataset: univ                Batch:  3/15	Loss 1.7628 (1.7469)
+2022-11-18 15:05:18,880:INFO: Dataset: univ                Batch:  4/15	Loss 1.7216 (1.7400)
+2022-11-18 15:05:19,037:INFO: Dataset: univ                Batch:  5/15	Loss 1.7307 (1.7380)
+2022-11-18 15:05:19,195:INFO: Dataset: univ                Batch:  6/15	Loss 1.7717 (1.7438)
+2022-11-18 15:05:19,351:INFO: Dataset: univ                Batch:  7/15	Loss 1.7058 (1.7386)
+2022-11-18 15:05:19,505:INFO: Dataset: univ                Batch:  8/15	Loss 1.7269 (1.7370)
+2022-11-18 15:05:19,663:INFO: Dataset: univ                Batch:  9/15	Loss 1.7367 (1.7369)
+2022-11-18 15:05:19,818:INFO: Dataset: univ                Batch: 10/15	Loss 1.7533 (1.7387)
+2022-11-18 15:05:19,975:INFO: Dataset: univ                Batch: 11/15	Loss 1.7541 (1.7400)
+2022-11-18 15:05:20,131:INFO: Dataset: univ                Batch: 12/15	Loss 1.7222 (1.7386)
+2022-11-18 15:05:20,287:INFO: Dataset: univ                Batch: 13/15	Loss 1.7373 (1.7385)
+2022-11-18 15:05:20,443:INFO: Dataset: univ                Batch: 14/15	Loss 1.7641 (1.7402)
+2022-11-18 15:05:20,526:INFO: Dataset: univ                Batch: 15/15	Loss 1.7930 (1.7409)
+2022-11-18 15:05:20,914:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6454 (1.6454)
+2022-11-18 15:05:21,066:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7966 (1.7224)
+2022-11-18 15:05:21,218:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7447 (1.7296)
+2022-11-18 15:05:21,368:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7440 (1.7334)
+2022-11-18 15:05:21,517:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6998 (1.7268)
+2022-11-18 15:05:21,665:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8777 (1.7542)
+2022-11-18 15:05:21,814:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9201 (1.7772)
+2022-11-18 15:05:21,950:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7650 (1.7758)
+2022-11-18 15:05:22,341:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7812 (1.7812)
+2022-11-18 15:05:22,494:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8158 (1.7979)
+2022-11-18 15:05:22,652:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6784 (1.7560)
+2022-11-18 15:05:22,802:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8382 (1.7775)
+2022-11-18 15:05:22,953:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6691 (1.7546)
+2022-11-18 15:05:23,105:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7560 (1.7549)
+2022-11-18 15:05:23,256:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8008 (1.7615)
+2022-11-18 15:05:23,405:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8197 (1.7688)
+2022-11-18 15:05:23,555:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8167 (1.7737)
+2022-11-18 15:05:23,704:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6915 (1.7654)
+2022-11-18 15:05:23,857:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8064 (1.7689)
+2022-11-18 15:05:24,007:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7208 (1.7654)
+2022-11-18 15:05:24,158:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7304 (1.7627)
+2022-11-18 15:05:24,307:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7172 (1.7592)
+2022-11-18 15:05:24,459:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7794 (1.7605)
+2022-11-18 15:05:24,609:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6691 (1.7544)
+2022-11-18 15:05:24,760:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7549 (1.7545)
+2022-11-18 15:05:24,900:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7656 (1.7550)
+2022-11-18 15:05:24,946:INFO: - Computing loss (validation)
+2022-11-18 15:05:25,206:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7229 (1.7229)
+2022-11-18 15:05:25,240:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7560 (1.7257)
+2022-11-18 15:05:25,556:INFO: Dataset: univ                Batch: 1/3	Loss 1.7123 (1.7123)
+2022-11-18 15:05:25,635:INFO: Dataset: univ                Batch: 2/3	Loss 1.7446 (1.7274)
+2022-11-18 15:05:25,710:INFO: Dataset: univ                Batch: 3/3	Loss 1.7571 (1.7368)
+2022-11-18 15:05:26,013:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7157 (1.7157)
+2022-11-18 15:05:26,060:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7208 (1.7170)
+2022-11-18 15:05:26,375:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7661 (1.7661)
+2022-11-18 15:05:26,454:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7966 (1.7812)
+2022-11-18 15:05:26,534:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7470 (1.7710)
+2022-11-18 15:05:26,612:INFO: Dataset: zara2               Batch: 4/5	Loss 1.6625 (1.7441)
+2022-11-18 15:05:26,689:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7511 (1.7454)
+2022-11-18 15:05:26,746:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_291.pth.tar
+2022-11-18 15:05:26,746:INFO: 
+===> EPOCH: 292 (P2)
+2022-11-18 15:05:26,747:INFO: - Computing loss (training)
+2022-11-18 15:05:27,094:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8892 (1.8892)
+2022-11-18 15:05:27,245:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7941 (1.8423)
+2022-11-18 15:05:27,395:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6958 (1.7933)
+2022-11-18 15:05:27,512:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8828 (1.8080)
+2022-11-18 15:05:27,911:INFO: Dataset: univ                Batch:  1/15	Loss 1.7322 (1.7322)
+2022-11-18 15:05:28,069:INFO: Dataset: univ                Batch:  2/15	Loss 1.7680 (1.7503)
+2022-11-18 15:05:28,231:INFO: Dataset: univ                Batch:  3/15	Loss 1.7409 (1.7476)
+2022-11-18 15:05:28,389:INFO: Dataset: univ                Batch:  4/15	Loss 1.7522 (1.7488)
+2022-11-18 15:05:28,546:INFO: Dataset: univ                Batch:  5/15	Loss 1.7526 (1.7495)
+2022-11-18 15:05:28,705:INFO: Dataset: univ                Batch:  6/15	Loss 1.7535 (1.7503)
+2022-11-18 15:05:28,864:INFO: Dataset: univ                Batch:  7/15	Loss 1.7555 (1.7510)
+2022-11-18 15:05:29,019:INFO: Dataset: univ                Batch:  8/15	Loss 1.7743 (1.7538)
+2022-11-18 15:05:29,179:INFO: Dataset: univ                Batch:  9/15	Loss 1.7575 (1.7543)
+2022-11-18 15:05:29,335:INFO: Dataset: univ                Batch: 10/15	Loss 1.7864 (1.7576)
+2022-11-18 15:05:29,495:INFO: Dataset: univ                Batch: 11/15	Loss 1.8098 (1.7631)
+2022-11-18 15:05:29,650:INFO: Dataset: univ                Batch: 12/15	Loss 1.7619 (1.7630)
+2022-11-18 15:05:29,807:INFO: Dataset: univ                Batch: 13/15	Loss 1.7479 (1.7618)
+2022-11-18 15:05:29,965:INFO: Dataset: univ                Batch: 14/15	Loss 1.7519 (1.7610)
+2022-11-18 15:05:30,049:INFO: Dataset: univ                Batch: 15/15	Loss 1.7788 (1.7613)
+2022-11-18 15:05:30,436:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7952 (1.7952)
+2022-11-18 15:05:30,584:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7094 (1.7522)
+2022-11-18 15:05:30,733:INFO: Dataset: zara1               Batch: 3/8	Loss 1.6466 (1.7202)
+2022-11-18 15:05:30,881:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8893 (1.7621)
+2022-11-18 15:05:31,032:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8717 (1.7870)
+2022-11-18 15:05:31,182:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7746 (1.7848)
+2022-11-18 15:05:31,333:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8390 (1.7924)
+2022-11-18 15:05:31,468:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6703 (1.7809)
+2022-11-18 15:05:31,854:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7718 (1.7718)
+2022-11-18 15:05:32,007:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8078 (1.7899)
+2022-11-18 15:05:32,160:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7618 (1.7802)
+2022-11-18 15:05:32,312:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6600 (1.7489)
+2022-11-18 15:05:32,470:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6403 (1.7281)
+2022-11-18 15:05:32,623:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8232 (1.7454)
+2022-11-18 15:05:32,774:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8190 (1.7555)
+2022-11-18 15:05:33,005:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8337 (1.7654)
+2022-11-18 15:05:33,156:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7909 (1.7682)
+2022-11-18 15:05:33,306:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8579 (1.7771)
+2022-11-18 15:05:33,457:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7171 (1.7713)
+2022-11-18 15:05:33,608:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6782 (1.7631)
+2022-11-18 15:05:33,757:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7417 (1.7613)
+2022-11-18 15:05:33,912:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7962 (1.7638)
+2022-11-18 15:05:34,063:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6654 (1.7580)
+2022-11-18 15:05:34,216:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7156 (1.7554)
+2022-11-18 15:05:34,367:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7403 (1.7544)
+2022-11-18 15:05:34,509:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7430 (1.7539)
+2022-11-18 15:05:34,559:INFO: - Computing loss (validation)
+2022-11-18 15:05:34,827:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7412 (1.7412)
+2022-11-18 15:05:34,864:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7345 (1.7408)
+2022-11-18 15:05:35,180:INFO: Dataset: univ                Batch: 1/3	Loss 1.7637 (1.7637)
+2022-11-18 15:05:35,256:INFO: Dataset: univ                Batch: 2/3	Loss 1.7657 (1.7646)
+2022-11-18 15:05:35,327:INFO: Dataset: univ                Batch: 3/3	Loss 1.7537 (1.7613)
+2022-11-18 15:05:35,628:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6500 (1.6500)
+2022-11-18 15:05:35,676:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7257 (1.6671)
+2022-11-18 15:05:35,996:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6567 (1.6567)
+2022-11-18 15:05:36,080:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8228 (1.7444)
+2022-11-18 15:05:36,168:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7897 (1.7592)
+2022-11-18 15:05:36,253:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7671 (1.7612)
+2022-11-18 15:05:36,331:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8361 (1.7766)
+2022-11-18 15:05:36,383:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_292.pth.tar
+2022-11-18 15:05:36,384:INFO: 
+===> EPOCH: 293 (P2)
+2022-11-18 15:05:36,384:INFO: - Computing loss (training)
+2022-11-18 15:05:36,725:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9852 (1.9852)
+2022-11-18 15:05:36,876:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7957 (1.8931)
+2022-11-18 15:05:37,027:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8808 (1.8890)
+2022-11-18 15:05:37,142:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8314 (1.8795)
+2022-11-18 15:05:37,536:INFO: Dataset: univ                Batch:  1/15	Loss 1.7723 (1.7723)
+2022-11-18 15:05:37,696:INFO: Dataset: univ                Batch:  2/15	Loss 1.8180 (1.7953)
+2022-11-18 15:05:37,854:INFO: Dataset: univ                Batch:  3/15	Loss 1.8012 (1.7974)
+2022-11-18 15:05:38,015:INFO: Dataset: univ                Batch:  4/15	Loss 1.7570 (1.7888)
+2022-11-18 15:05:38,172:INFO: Dataset: univ                Batch:  5/15	Loss 1.7745 (1.7858)
+2022-11-18 15:05:38,329:INFO: Dataset: univ                Batch:  6/15	Loss 1.7995 (1.7879)
+2022-11-18 15:05:38,486:INFO: Dataset: univ                Batch:  7/15	Loss 1.7852 (1.7875)
+2022-11-18 15:05:38,642:INFO: Dataset: univ                Batch:  8/15	Loss 1.8042 (1.7898)
+2022-11-18 15:05:38,801:INFO: Dataset: univ                Batch:  9/15	Loss 1.8106 (1.7921)
+2022-11-18 15:05:38,958:INFO: Dataset: univ                Batch: 10/15	Loss 1.7697 (1.7896)
+2022-11-18 15:05:39,119:INFO: Dataset: univ                Batch: 11/15	Loss 1.7310 (1.7840)
+2022-11-18 15:05:39,272:INFO: Dataset: univ                Batch: 12/15	Loss 1.7896 (1.7844)
+2022-11-18 15:05:39,430:INFO: Dataset: univ                Batch: 13/15	Loss 1.7933 (1.7851)
+2022-11-18 15:05:39,586:INFO: Dataset: univ                Batch: 14/15	Loss 1.7812 (1.7848)
+2022-11-18 15:05:39,670:INFO: Dataset: univ                Batch: 15/15	Loss 1.7591 (1.7844)
+2022-11-18 15:05:40,065:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8070 (1.8070)
+2022-11-18 15:05:40,215:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8219 (1.8152)
+2022-11-18 15:05:40,372:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5704 (1.7431)
+2022-11-18 15:05:40,524:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7559 (1.7459)
+2022-11-18 15:05:40,675:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7866 (1.7537)
+2022-11-18 15:05:40,825:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7743 (1.7575)
+2022-11-18 15:05:40,973:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8301 (1.7672)
+2022-11-18 15:05:41,108:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9099 (1.7833)
+2022-11-18 15:05:41,496:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7763 (1.7763)
+2022-11-18 15:05:41,647:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8213 (1.7988)
+2022-11-18 15:05:41,799:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7820 (1.7934)
+2022-11-18 15:05:41,952:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7903 (1.7927)
+2022-11-18 15:05:42,104:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7435 (1.7826)
+2022-11-18 15:05:42,254:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7962 (1.7851)
+2022-11-18 15:05:42,406:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8035 (1.7878)
+2022-11-18 15:05:42,555:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6908 (1.7759)
+2022-11-18 15:05:42,706:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8133 (1.7795)
+2022-11-18 15:05:42,854:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9142 (1.7951)
+2022-11-18 15:05:43,005:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7800 (1.7936)
+2022-11-18 15:05:43,155:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8082 (1.7948)
+2022-11-18 15:05:43,305:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7801 (1.7937)
+2022-11-18 15:05:43,456:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7851 (1.7931)
+2022-11-18 15:05:43,608:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7888 (1.7928)
+2022-11-18 15:05:43,759:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7730 (1.7917)
+2022-11-18 15:05:43,913:INFO: Dataset: zara2               Batch: 17/18	Loss 1.6856 (1.7855)
+2022-11-18 15:05:44,051:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6981 (1.7811)
+2022-11-18 15:05:44,098:INFO: - Computing loss (validation)
+2022-11-18 15:05:44,360:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8163 (1.8163)
+2022-11-18 15:05:44,395:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6200 (1.8022)
+2022-11-18 15:05:44,703:INFO: Dataset: univ                Batch: 1/3	Loss 1.7803 (1.7803)
+2022-11-18 15:05:44,782:INFO: Dataset: univ                Batch: 2/3	Loss 1.8360 (1.8056)
+2022-11-18 15:05:44,860:INFO: Dataset: univ                Batch: 3/3	Loss 1.7902 (1.8003)
+2022-11-18 15:05:45,162:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7844 (1.7844)
+2022-11-18 15:05:45,207:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9125 (1.8165)
+2022-11-18 15:05:45,520:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7023 (1.7023)
+2022-11-18 15:05:45,597:INFO: Dataset: zara2               Batch: 2/5	Loss 1.6646 (1.6837)
+2022-11-18 15:05:45,670:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7713 (1.7138)
+2022-11-18 15:05:45,745:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7397 (1.7205)
+2022-11-18 15:05:45,818:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7283 (1.7221)
+2022-11-18 15:05:45,872:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_293.pth.tar
+2022-11-18 15:05:45,872:INFO: 
+===> EPOCH: 294 (P2)
+2022-11-18 15:05:45,872:INFO: - Computing loss (training)
+2022-11-18 15:05:46,199:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8230 (1.8230)
+2022-11-18 15:05:46,352:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7840 (1.8045)
+2022-11-18 15:05:46,503:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8425 (1.8175)
+2022-11-18 15:05:46,619:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7439 (1.8052)
+2022-11-18 15:05:47,025:INFO: Dataset: univ                Batch:  1/15	Loss 1.8189 (1.8189)
+2022-11-18 15:05:47,186:INFO: Dataset: univ                Batch:  2/15	Loss 1.8019 (1.8105)
+2022-11-18 15:05:47,346:INFO: Dataset: univ                Batch:  3/15	Loss 1.7626 (1.7924)
+2022-11-18 15:05:47,511:INFO: Dataset: univ                Batch:  4/15	Loss 1.7820 (1.7899)
+2022-11-18 15:05:47,672:INFO: Dataset: univ                Batch:  5/15	Loss 1.7770 (1.7871)
+2022-11-18 15:05:47,831:INFO: Dataset: univ                Batch:  6/15	Loss 1.8002 (1.7894)
+2022-11-18 15:05:47,988:INFO: Dataset: univ                Batch:  7/15	Loss 1.7935 (1.7900)
+2022-11-18 15:05:48,144:INFO: Dataset: univ                Batch:  8/15	Loss 1.8025 (1.7917)
+2022-11-18 15:05:48,304:INFO: Dataset: univ                Batch:  9/15	Loss 1.7593 (1.7875)
+2022-11-18 15:05:48,460:INFO: Dataset: univ                Batch: 10/15	Loss 1.8066 (1.7895)
+2022-11-18 15:05:48,618:INFO: Dataset: univ                Batch: 11/15	Loss 1.7803 (1.7887)
+2022-11-18 15:05:48,776:INFO: Dataset: univ                Batch: 12/15	Loss 1.7982 (1.7895)
+2022-11-18 15:05:48,936:INFO: Dataset: univ                Batch: 13/15	Loss 1.7812 (1.7889)
+2022-11-18 15:05:49,094:INFO: Dataset: univ                Batch: 14/15	Loss 1.7513 (1.7862)
+2022-11-18 15:05:49,178:INFO: Dataset: univ                Batch: 15/15	Loss 1.7991 (1.7864)
+2022-11-18 15:05:49,577:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8515 (1.8515)
+2022-11-18 15:05:49,725:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8237 (1.8377)
+2022-11-18 15:05:49,876:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8372 (1.8375)
+2022-11-18 15:05:50,025:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8253 (1.8346)
+2022-11-18 15:05:50,176:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7858 (1.8253)
+2022-11-18 15:05:50,326:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9619 (1.8476)
+2022-11-18 15:05:50,477:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8102 (1.8422)
+2022-11-18 15:05:50,614:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8594 (1.8442)
+2022-11-18 15:05:51,011:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8609 (1.8609)
+2022-11-18 15:05:51,165:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9246 (1.8915)
+2022-11-18 15:05:51,320:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8972 (1.8932)
+2022-11-18 15:05:51,474:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8511 (1.8829)
+2022-11-18 15:05:51,625:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7669 (1.8589)
+2022-11-18 15:05:51,776:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7722 (1.8443)
+2022-11-18 15:05:51,928:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8281 (1.8420)
+2022-11-18 15:05:52,078:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7818 (1.8345)
+2022-11-18 15:05:52,230:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7441 (1.8246)
+2022-11-18 15:05:52,379:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7903 (1.8212)
+2022-11-18 15:05:52,532:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8829 (1.8273)
+2022-11-18 15:05:52,683:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7347 (1.8192)
+2022-11-18 15:05:52,834:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7169 (1.8117)
+2022-11-18 15:05:52,985:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8080 (1.8114)
+2022-11-18 15:05:53,138:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8162 (1.8117)
+2022-11-18 15:05:53,289:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7234 (1.8067)
+2022-11-18 15:05:53,442:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7493 (1.8036)
+2022-11-18 15:05:53,580:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7088 (1.7994)
+2022-11-18 15:05:53,626:INFO: - Computing loss (validation)
+2022-11-18 15:05:53,893:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8745 (1.8745)
+2022-11-18 15:05:53,928:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0107 (1.8838)
+2022-11-18 15:05:54,238:INFO: Dataset: univ                Batch: 1/3	Loss 1.8201 (1.8201)
+2022-11-18 15:05:54,320:INFO: Dataset: univ                Batch: 2/3	Loss 1.8240 (1.8220)
+2022-11-18 15:05:54,396:INFO: Dataset: univ                Batch: 3/3	Loss 1.7830 (1.8110)
+2022-11-18 15:05:54,700:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8205 (1.8205)
+2022-11-18 15:05:54,749:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8382 (1.8245)
+2022-11-18 15:05:55,069:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7970 (1.7970)
+2022-11-18 15:05:55,150:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7941 (1.7956)
+2022-11-18 15:05:55,228:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8040 (1.7985)
+2022-11-18 15:05:55,307:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8402 (1.8091)
+2022-11-18 15:05:55,385:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8784 (1.8227)
+2022-11-18 15:05:55,438:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_294.pth.tar
+2022-11-18 15:05:55,438:INFO: 
+===> EPOCH: 295 (P2)
+2022-11-18 15:05:55,439:INFO: - Computing loss (training)
+2022-11-18 15:05:55,785:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9490 (1.9490)
+2022-11-18 15:05:55,937:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8193 (1.8860)
+2022-11-18 15:05:56,087:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8553 (1.8759)
+2022-11-18 15:05:56,202:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7809 (1.8590)
+2022-11-18 15:05:56,595:INFO: Dataset: univ                Batch:  1/15	Loss 1.8182 (1.8182)
+2022-11-18 15:05:56,753:INFO: Dataset: univ                Batch:  2/15	Loss 1.7829 (1.8016)
+2022-11-18 15:05:56,915:INFO: Dataset: univ                Batch:  3/15	Loss 1.8179 (1.8071)
+2022-11-18 15:05:57,077:INFO: Dataset: univ                Batch:  4/15	Loss 1.8350 (1.8144)
+2022-11-18 15:05:57,235:INFO: Dataset: univ                Batch:  5/15	Loss 1.8106 (1.8136)
+2022-11-18 15:05:57,394:INFO: Dataset: univ                Batch:  6/15	Loss 1.7813 (1.8084)
+2022-11-18 15:05:57,553:INFO: Dataset: univ                Batch:  7/15	Loss 1.8245 (1.8107)
+2022-11-18 15:05:57,709:INFO: Dataset: univ                Batch:  8/15	Loss 1.8423 (1.8147)
+2022-11-18 15:05:57,867:INFO: Dataset: univ                Batch:  9/15	Loss 1.8738 (1.8212)
+2022-11-18 15:05:58,024:INFO: Dataset: univ                Batch: 10/15	Loss 1.7742 (1.8168)
+2022-11-18 15:05:58,183:INFO: Dataset: univ                Batch: 11/15	Loss 1.7936 (1.8145)
+2022-11-18 15:05:58,337:INFO: Dataset: univ                Batch: 12/15	Loss 1.8486 (1.8175)
+2022-11-18 15:05:58,493:INFO: Dataset: univ                Batch: 13/15	Loss 1.8413 (1.8194)
+2022-11-18 15:05:58,649:INFO: Dataset: univ                Batch: 14/15	Loss 1.7819 (1.8166)
+2022-11-18 15:05:58,734:INFO: Dataset: univ                Batch: 15/15	Loss 1.8890 (1.8175)
+2022-11-18 15:05:59,150:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7508 (1.7508)
+2022-11-18 15:05:59,311:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7693 (1.7597)
+2022-11-18 15:05:59,470:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9520 (1.8275)
+2022-11-18 15:05:59,626:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8068 (1.8221)
+2022-11-18 15:05:59,785:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8995 (1.8376)
+2022-11-18 15:05:59,941:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7833 (1.8288)
+2022-11-18 15:06:00,097:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8285 (1.8288)
+2022-11-18 15:06:00,236:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9546 (1.8438)
+2022-11-18 15:06:00,624:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8530 (1.8530)
+2022-11-18 15:06:00,773:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8384 (1.8458)
+2022-11-18 15:06:00,929:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7869 (1.8282)
+2022-11-18 15:06:01,082:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8535 (1.8344)
+2022-11-18 15:06:01,232:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7745 (1.8226)
+2022-11-18 15:06:01,384:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8248 (1.8230)
+2022-11-18 15:06:01,533:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8216 (1.8228)
+2022-11-18 15:06:01,679:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9124 (1.8336)
+2022-11-18 15:06:01,830:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8572 (1.8362)
+2022-11-18 15:06:01,979:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8306 (1.8356)
+2022-11-18 15:06:02,128:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8255 (1.8346)
+2022-11-18 15:06:02,276:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8948 (1.8395)
+2022-11-18 15:06:02,429:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7907 (1.8360)
+2022-11-18 15:06:02,580:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8339 (1.8359)
+2022-11-18 15:06:02,729:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8255 (1.8351)
+2022-11-18 15:06:02,877:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8597 (1.8366)
+2022-11-18 15:06:03,028:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8805 (1.8394)
+2022-11-18 15:06:03,165:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8357 (1.8392)
+2022-11-18 15:06:03,210:INFO: - Computing loss (validation)
+2022-11-18 15:06:03,481:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8820 (1.8820)
+2022-11-18 15:06:03,516:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5049 (1.8589)
+2022-11-18 15:06:03,838:INFO: Dataset: univ                Batch: 1/3	Loss 1.7795 (1.7795)
+2022-11-18 15:06:03,921:INFO: Dataset: univ                Batch: 2/3	Loss 1.8073 (1.7919)
+2022-11-18 15:06:04,004:INFO: Dataset: univ                Batch: 3/3	Loss 1.8044 (1.7961)
+2022-11-18 15:06:04,317:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7307 (1.7307)
+2022-11-18 15:06:04,366:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9922 (1.7937)
+2022-11-18 15:06:04,709:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8169 (1.8169)
+2022-11-18 15:06:04,810:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7979 (1.8076)
+2022-11-18 15:06:04,894:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8345 (1.8163)
+2022-11-18 15:06:04,975:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7226 (1.7941)
+2022-11-18 15:06:05,052:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8287 (1.8010)
+2022-11-18 15:06:05,107:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_295.pth.tar
+2022-11-18 15:06:05,107:INFO: 
+===> EPOCH: 296 (P2)
+2022-11-18 15:06:05,107:INFO: - Computing loss (training)
+2022-11-18 15:06:05,451:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8971 (1.8971)
+2022-11-18 15:06:05,601:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8454 (1.8718)
+2022-11-18 15:06:05,750:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8846 (1.8758)
+2022-11-18 15:06:05,872:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9146 (1.8821)
+2022-11-18 15:06:06,327:INFO: Dataset: univ                Batch:  1/15	Loss 1.8244 (1.8244)
+2022-11-18 15:06:06,484:INFO: Dataset: univ                Batch:  2/15	Loss 1.8507 (1.8370)
+2022-11-18 15:06:06,645:INFO: Dataset: univ                Batch:  3/15	Loss 1.8171 (1.8305)
+2022-11-18 15:06:06,798:INFO: Dataset: univ                Batch:  4/15	Loss 1.8630 (1.8385)
+2022-11-18 15:06:06,954:INFO: Dataset: univ                Batch:  5/15	Loss 1.8250 (1.8360)
+2022-11-18 15:06:07,112:INFO: Dataset: univ                Batch:  6/15	Loss 1.8108 (1.8315)
+2022-11-18 15:06:07,267:INFO: Dataset: univ                Batch:  7/15	Loss 1.8278 (1.8309)
+2022-11-18 15:06:07,420:INFO: Dataset: univ                Batch:  8/15	Loss 1.8216 (1.8298)
+2022-11-18 15:06:07,576:INFO: Dataset: univ                Batch:  9/15	Loss 1.7918 (1.8252)
+2022-11-18 15:06:07,729:INFO: Dataset: univ                Batch: 10/15	Loss 1.8325 (1.8259)
+2022-11-18 15:06:07,888:INFO: Dataset: univ                Batch: 11/15	Loss 1.8057 (1.8240)
+2022-11-18 15:06:08,044:INFO: Dataset: univ                Batch: 12/15	Loss 1.8163 (1.8233)
+2022-11-18 15:06:08,199:INFO: Dataset: univ                Batch: 13/15	Loss 1.8252 (1.8235)
+2022-11-18 15:06:08,352:INFO: Dataset: univ                Batch: 14/15	Loss 1.8507 (1.8254)
+2022-11-18 15:06:08,435:INFO: Dataset: univ                Batch: 15/15	Loss 1.8296 (1.8254)
+2022-11-18 15:06:08,821:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9405 (1.9405)
+2022-11-18 15:06:08,979:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8942 (1.9175)
+2022-11-18 15:06:09,134:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8251 (1.8840)
+2022-11-18 15:06:09,286:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0285 (1.9184)
+2022-11-18 15:06:09,440:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7746 (1.8873)
+2022-11-18 15:06:09,595:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8434 (1.8799)
+2022-11-18 15:06:09,748:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8743 (1.8790)
+2022-11-18 15:06:09,888:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7178 (1.8611)
+2022-11-18 15:06:10,282:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8699 (1.8699)
+2022-11-18 15:06:10,441:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7446 (1.8093)
+2022-11-18 15:06:10,606:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9050 (1.8364)
+2022-11-18 15:06:10,767:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8037 (1.8280)
+2022-11-18 15:06:10,925:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8879 (1.8405)
+2022-11-18 15:06:11,085:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8464 (1.8414)
+2022-11-18 15:06:11,243:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7000 (1.8206)
+2022-11-18 15:06:11,399:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7602 (1.8125)
+2022-11-18 15:06:11,556:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9140 (1.8238)
+2022-11-18 15:06:11,711:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9192 (1.8340)
+2022-11-18 15:06:11,869:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8187 (1.8325)
+2022-11-18 15:06:12,028:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7768 (1.8275)
+2022-11-18 15:06:12,185:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8097 (1.8260)
+2022-11-18 15:06:12,341:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8849 (1.8304)
+2022-11-18 15:06:12,499:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8849 (1.8342)
+2022-11-18 15:06:12,657:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7961 (1.8315)
+2022-11-18 15:06:12,814:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8703 (1.8339)
+2022-11-18 15:06:12,960:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7978 (1.8321)
+2022-11-18 15:06:13,005:INFO: - Computing loss (validation)
+2022-11-18 15:06:13,270:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8911 (1.8911)
+2022-11-18 15:06:13,304:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8793 (1.8904)
+2022-11-18 15:06:13,603:INFO: Dataset: univ                Batch: 1/3	Loss 1.8232 (1.8232)
+2022-11-18 15:06:13,682:INFO: Dataset: univ                Batch: 2/3	Loss 1.8362 (1.8303)
+2022-11-18 15:06:13,755:INFO: Dataset: univ                Batch: 3/3	Loss 1.8148 (1.8253)
+2022-11-18 15:06:14,048:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8431 (1.8431)
+2022-11-18 15:06:14,092:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7949 (1.8296)
+2022-11-18 15:06:14,394:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8061 (1.8061)
+2022-11-18 15:06:14,469:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7826 (1.7937)
+2022-11-18 15:06:14,544:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8495 (1.8125)
+2022-11-18 15:06:14,623:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8237 (1.8153)
+2022-11-18 15:06:14,699:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8498 (1.8223)
+2022-11-18 15:06:14,754:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_296.pth.tar
+2022-11-18 15:06:14,754:INFO: 
+===> EPOCH: 297 (P2)
+2022-11-18 15:06:14,754:INFO: - Computing loss (training)
+2022-11-18 15:06:15,111:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9677 (1.9677)
+2022-11-18 15:06:15,263:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8645 (1.9155)
+2022-11-18 15:06:15,417:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8250 (1.8854)
+2022-11-18 15:06:15,535:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8931 (1.8867)
+2022-11-18 15:06:15,938:INFO: Dataset: univ                Batch:  1/15	Loss 1.8904 (1.8904)
+2022-11-18 15:06:16,103:INFO: Dataset: univ                Batch:  2/15	Loss 1.8417 (1.8647)
+2022-11-18 15:06:16,264:INFO: Dataset: univ                Batch:  3/15	Loss 1.8244 (1.8517)
+2022-11-18 15:06:16,429:INFO: Dataset: univ                Batch:  4/15	Loss 1.8794 (1.8589)
+2022-11-18 15:06:16,590:INFO: Dataset: univ                Batch:  5/15	Loss 1.8629 (1.8596)
+2022-11-18 15:06:16,750:INFO: Dataset: univ                Batch:  6/15	Loss 1.8085 (1.8519)
+2022-11-18 15:06:16,912:INFO: Dataset: univ                Batch:  7/15	Loss 1.8202 (1.8472)
+2022-11-18 15:06:17,071:INFO: Dataset: univ                Batch:  8/15	Loss 1.8857 (1.8522)
+2022-11-18 15:06:17,231:INFO: Dataset: univ                Batch:  9/15	Loss 1.8569 (1.8527)
+2022-11-18 15:06:17,389:INFO: Dataset: univ                Batch: 10/15	Loss 1.8600 (1.8534)
+2022-11-18 15:06:17,551:INFO: Dataset: univ                Batch: 11/15	Loss 1.8477 (1.8529)
+2022-11-18 15:06:17,710:INFO: Dataset: univ                Batch: 12/15	Loss 1.8738 (1.8547)
+2022-11-18 15:06:17,870:INFO: Dataset: univ                Batch: 13/15	Loss 1.8350 (1.8532)
+2022-11-18 15:06:18,031:INFO: Dataset: univ                Batch: 14/15	Loss 1.8211 (1.8509)
+2022-11-18 15:06:18,117:INFO: Dataset: univ                Batch: 15/15	Loss 1.8359 (1.8508)
+2022-11-18 15:06:18,517:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8277 (1.8277)
+2022-11-18 15:06:18,669:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7727 (1.7990)
+2022-11-18 15:06:18,822:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8131 (1.8037)
+2022-11-18 15:06:18,977:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8748 (1.8210)
+2022-11-18 15:06:19,132:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7832 (1.8139)
+2022-11-18 15:06:19,284:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8417 (1.8190)
+2022-11-18 15:06:19,438:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8625 (1.8253)
+2022-11-18 15:06:19,577:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8992 (1.8330)
+2022-11-18 15:06:19,967:INFO: Dataset: zara2               Batch:  1/18	Loss 2.0362 (2.0362)
+2022-11-18 15:06:20,119:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9244 (1.9791)
+2022-11-18 15:06:20,270:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9481 (1.9691)
+2022-11-18 15:06:20,431:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7928 (1.9264)
+2022-11-18 15:06:20,582:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7826 (1.8971)
+2022-11-18 15:06:20,734:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8162 (1.8826)
+2022-11-18 15:06:20,894:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9674 (1.8953)
+2022-11-18 15:06:21,044:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8532 (1.8903)
+2022-11-18 15:06:21,193:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8976 (1.8912)
+2022-11-18 15:06:21,341:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7804 (1.8800)
+2022-11-18 15:06:21,492:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8283 (1.8752)
+2022-11-18 15:06:21,639:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8168 (1.8701)
+2022-11-18 15:06:21,790:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9127 (1.8731)
+2022-11-18 15:06:21,952:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7816 (1.8660)
+2022-11-18 15:06:22,102:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8495 (1.8649)
+2022-11-18 15:06:22,251:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8892 (1.8663)
+2022-11-18 15:06:22,401:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8782 (1.8670)
+2022-11-18 15:06:22,539:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7960 (1.8640)
+2022-11-18 15:06:22,589:INFO: - Computing loss (validation)
+2022-11-18 15:06:22,861:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9338 (1.9338)
+2022-11-18 15:06:22,894:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7563 (1.9156)
+2022-11-18 15:06:23,208:INFO: Dataset: univ                Batch: 1/3	Loss 1.8550 (1.8550)
+2022-11-18 15:06:23,284:INFO: Dataset: univ                Batch: 2/3	Loss 1.8678 (1.8616)
+2022-11-18 15:06:23,356:INFO: Dataset: univ                Batch: 3/3	Loss 1.8872 (1.8700)
+2022-11-18 15:06:23,653:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8667 (1.8667)
+2022-11-18 15:06:23,697:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7163 (1.8323)
+2022-11-18 15:06:24,000:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9019 (1.9019)
+2022-11-18 15:06:24,075:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9300 (1.9155)
+2022-11-18 15:06:24,150:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9037 (1.9115)
+2022-11-18 15:06:24,223:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8204 (1.8882)
+2022-11-18 15:06:24,297:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8066 (1.8722)
+2022-11-18 15:06:24,352:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_297.pth.tar
+2022-11-18 15:06:24,352:INFO: 
+===> EPOCH: 298 (P2)
+2022-11-18 15:06:24,352:INFO: - Computing loss (training)
+2022-11-18 15:06:24,704:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9537 (1.9537)
+2022-11-18 15:06:24,853:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0153 (1.9831)
+2022-11-18 15:06:25,004:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8879 (1.9510)
+2022-11-18 15:06:25,118:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9962 (1.9591)
+2022-11-18 15:06:25,525:INFO: Dataset: univ                Batch:  1/15	Loss 1.8643 (1.8643)
+2022-11-18 15:06:25,689:INFO: Dataset: univ                Batch:  2/15	Loss 1.8613 (1.8627)
+2022-11-18 15:06:25,879:INFO: Dataset: univ                Batch:  3/15	Loss 1.8778 (1.8676)
+2022-11-18 15:06:26,049:INFO: Dataset: univ                Batch:  4/15	Loss 1.8115 (1.8541)
+2022-11-18 15:06:26,227:INFO: Dataset: univ                Batch:  5/15	Loss 1.8620 (1.8558)
+2022-11-18 15:06:26,398:INFO: Dataset: univ                Batch:  6/15	Loss 1.8297 (1.8514)
+2022-11-18 15:06:26,563:INFO: Dataset: univ                Batch:  7/15	Loss 1.8966 (1.8582)
+2022-11-18 15:06:26,728:INFO: Dataset: univ                Batch:  8/15	Loss 1.8995 (1.8634)
+2022-11-18 15:06:26,906:INFO: Dataset: univ                Batch:  9/15	Loss 1.8233 (1.8589)
+2022-11-18 15:06:27,104:INFO: Dataset: univ                Batch: 10/15	Loss 1.8697 (1.8599)
+2022-11-18 15:06:27,301:INFO: Dataset: univ                Batch: 11/15	Loss 1.8874 (1.8622)
+2022-11-18 15:06:27,513:INFO: Dataset: univ                Batch: 12/15	Loss 1.8631 (1.8623)
+2022-11-18 15:06:27,698:INFO: Dataset: univ                Batch: 13/15	Loss 1.8868 (1.8642)
+2022-11-18 15:06:27,864:INFO: Dataset: univ                Batch: 14/15	Loss 1.9236 (1.8687)
+2022-11-18 15:06:27,948:INFO: Dataset: univ                Batch: 15/15	Loss 1.9277 (1.8693)
+2022-11-18 15:06:28,357:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8679 (1.8679)
+2022-11-18 15:06:28,538:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9383 (1.9035)
+2022-11-18 15:06:28,723:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8489 (1.8838)
+2022-11-18 15:06:28,886:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9130 (1.8909)
+2022-11-18 15:06:29,050:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8999 (1.8927)
+2022-11-18 15:06:29,211:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8786 (1.8905)
+2022-11-18 15:06:29,369:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8784 (1.8889)
+2022-11-18 15:06:29,515:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9168 (1.8919)
+2022-11-18 15:06:29,909:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8934 (1.8934)
+2022-11-18 15:06:30,065:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8237 (1.8585)
+2022-11-18 15:06:30,218:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8316 (1.8500)
+2022-11-18 15:06:30,373:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7986 (1.8369)
+2022-11-18 15:06:30,529:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9354 (1.8564)
+2022-11-18 15:06:30,685:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8876 (1.8618)
+2022-11-18 15:06:30,843:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9098 (1.8682)
+2022-11-18 15:06:30,999:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8551 (1.8665)
+2022-11-18 15:06:31,161:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9402 (1.8750)
+2022-11-18 15:06:31,317:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8411 (1.8719)
+2022-11-18 15:06:31,475:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9575 (1.8792)
+2022-11-18 15:06:31,631:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9910 (1.8875)
+2022-11-18 15:06:31,785:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8554 (1.8851)
+2022-11-18 15:06:31,941:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8820 (1.8848)
+2022-11-18 15:06:32,095:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8706 (1.8840)
+2022-11-18 15:06:32,247:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8692 (1.8831)
+2022-11-18 15:06:32,402:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9352 (1.8858)
+2022-11-18 15:06:32,543:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9302 (1.8879)
+2022-11-18 15:06:32,588:INFO: - Computing loss (validation)
+2022-11-18 15:06:32,856:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8563 (1.8563)
+2022-11-18 15:06:32,892:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9575 (1.8643)
+2022-11-18 15:06:33,241:INFO: Dataset: univ                Batch: 1/3	Loss 1.8106 (1.8106)
+2022-11-18 15:06:33,332:INFO: Dataset: univ                Batch: 2/3	Loss 1.8242 (1.8182)
+2022-11-18 15:06:33,411:INFO: Dataset: univ                Batch: 3/3	Loss 1.8340 (1.8234)
+2022-11-18 15:06:33,723:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7765 (1.7765)
+2022-11-18 15:06:33,770:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7640 (1.7733)
+2022-11-18 15:06:34,079:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8957 (1.8957)
+2022-11-18 15:06:34,154:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9286 (1.9130)
+2022-11-18 15:06:34,228:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8457 (1.8913)
+2022-11-18 15:06:34,303:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9353 (1.9020)
+2022-11-18 15:06:34,376:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9141 (1.9044)
+2022-11-18 15:06:34,427:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_298.pth.tar
+2022-11-18 15:06:34,428:INFO: 
+===> EPOCH: 299 (P2)
+2022-11-18 15:06:34,428:INFO: - Computing loss (training)
+2022-11-18 15:06:34,768:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8886 (1.8886)
+2022-11-18 15:06:34,922:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0071 (1.9488)
+2022-11-18 15:06:35,084:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0394 (1.9789)
+2022-11-18 15:06:35,209:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8983 (1.9656)
+2022-11-18 15:06:35,637:INFO: Dataset: univ                Batch:  1/15	Loss 1.8807 (1.8807)
+2022-11-18 15:06:35,807:INFO: Dataset: univ                Batch:  2/15	Loss 1.8804 (1.8805)
+2022-11-18 15:06:35,967:INFO: Dataset: univ                Batch:  3/15	Loss 1.8725 (1.8779)
+2022-11-18 15:06:36,128:INFO: Dataset: univ                Batch:  4/15	Loss 1.8964 (1.8819)
+2022-11-18 15:06:36,314:INFO: Dataset: univ                Batch:  5/15	Loss 1.8380 (1.8734)
+2022-11-18 15:06:36,489:INFO: Dataset: univ                Batch:  6/15	Loss 1.8518 (1.8691)
+2022-11-18 15:06:36,649:INFO: Dataset: univ                Batch:  7/15	Loss 1.8630 (1.8683)
+2022-11-18 15:06:36,806:INFO: Dataset: univ                Batch:  8/15	Loss 1.9003 (1.8725)
+2022-11-18 15:06:36,964:INFO: Dataset: univ                Batch:  9/15	Loss 1.9000 (1.8756)
+2022-11-18 15:06:37,121:INFO: Dataset: univ                Batch: 10/15	Loss 1.8983 (1.8777)
+2022-11-18 15:06:37,279:INFO: Dataset: univ                Batch: 11/15	Loss 1.9162 (1.8808)
+2022-11-18 15:06:37,436:INFO: Dataset: univ                Batch: 12/15	Loss 1.8606 (1.8790)
+2022-11-18 15:06:37,594:INFO: Dataset: univ                Batch: 13/15	Loss 1.8957 (1.8805)
+2022-11-18 15:06:37,753:INFO: Dataset: univ                Batch: 14/15	Loss 1.8366 (1.8771)
+2022-11-18 15:06:37,836:INFO: Dataset: univ                Batch: 15/15	Loss 1.8713 (1.8770)
+2022-11-18 15:06:38,222:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9079 (1.9079)
+2022-11-18 15:06:38,378:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8838 (1.8958)
+2022-11-18 15:06:38,542:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8750 (1.8885)
+2022-11-18 15:06:38,707:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9264 (1.8972)
+2022-11-18 15:06:38,877:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9583 (1.9091)
+2022-11-18 15:06:39,046:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9571 (1.9165)
+2022-11-18 15:06:39,206:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7794 (1.8973)
+2022-11-18 15:06:39,361:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8382 (1.8916)
+2022-11-18 15:06:39,811:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8546 (1.8546)
+2022-11-18 15:06:39,975:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8813 (1.8686)
+2022-11-18 15:06:40,157:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8497 (1.8622)
+2022-11-18 15:06:40,310:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7814 (1.8418)
+2022-11-18 15:06:40,467:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8415 (1.8418)
+2022-11-18 15:06:40,636:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8998 (1.8514)
+2022-11-18 15:06:40,816:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8826 (1.8564)
+2022-11-18 15:06:40,974:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8736 (1.8587)
+2022-11-18 15:06:41,132:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8788 (1.8609)
+2022-11-18 15:06:41,293:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8680 (1.8616)
+2022-11-18 15:06:41,460:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9056 (1.8655)
+2022-11-18 15:06:41,614:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9570 (1.8732)
+2022-11-18 15:06:41,768:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9350 (1.8781)
+2022-11-18 15:06:41,923:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9472 (1.8829)
+2022-11-18 15:06:42,077:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8853 (1.8831)
+2022-11-18 15:06:42,230:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8834 (1.8831)
+2022-11-18 15:06:42,395:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9738 (1.8879)
+2022-11-18 15:06:42,539:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9672 (1.8915)
+2022-11-18 15:06:42,583:INFO: - Computing loss (validation)
+2022-11-18 15:06:42,851:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9562 (1.9562)
+2022-11-18 15:06:42,887:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0052 (1.9604)
+2022-11-18 15:06:43,218:INFO: Dataset: univ                Batch: 1/3	Loss 1.8797 (1.8797)
+2022-11-18 15:06:43,299:INFO: Dataset: univ                Batch: 2/3	Loss 1.9165 (1.8968)
+2022-11-18 15:06:43,376:INFO: Dataset: univ                Batch: 3/3	Loss 1.8828 (1.8924)
+2022-11-18 15:06:43,690:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9685 (1.9685)
+2022-11-18 15:06:43,742:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6901 (1.8942)
+2022-11-18 15:06:44,082:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8471 (1.8471)
+2022-11-18 15:06:44,166:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9747 (1.9060)
+2022-11-18 15:06:44,256:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8309 (1.8812)
+2022-11-18 15:06:44,351:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9638 (1.9016)
+2022-11-18 15:06:44,436:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9587 (1.9127)
+2022-11-18 15:06:44,494:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_299.pth.tar
+2022-11-18 15:06:44,494:INFO: 
+===> EPOCH: 300 (P2)
+2022-11-18 15:06:44,494:INFO: - Computing loss (training)
+2022-11-18 15:06:44,871:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0010 (2.0010)
+2022-11-18 15:06:45,032:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0472 (2.0252)
+2022-11-18 15:06:45,192:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0037 (2.0178)
+2022-11-18 15:06:45,315:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8877 (1.9947)
+2022-11-18 15:06:45,709:INFO: Dataset: univ                Batch:  1/15	Loss 1.9368 (1.9368)
+2022-11-18 15:06:45,872:INFO: Dataset: univ                Batch:  2/15	Loss 1.8674 (1.9032)
+2022-11-18 15:06:46,045:INFO: Dataset: univ                Batch:  3/15	Loss 1.9307 (1.9130)
+2022-11-18 15:06:46,207:INFO: Dataset: univ                Batch:  4/15	Loss 1.8213 (1.8891)
+2022-11-18 15:06:46,367:INFO: Dataset: univ                Batch:  5/15	Loss 1.8860 (1.8885)
+2022-11-18 15:06:46,538:INFO: Dataset: univ                Batch:  6/15	Loss 1.8928 (1.8892)
+2022-11-18 15:06:46,703:INFO: Dataset: univ                Batch:  7/15	Loss 1.9399 (1.8965)
+2022-11-18 15:06:46,868:INFO: Dataset: univ                Batch:  8/15	Loss 1.8964 (1.8965)
+2022-11-18 15:06:47,037:INFO: Dataset: univ                Batch:  9/15	Loss 1.9185 (1.8990)
+2022-11-18 15:06:47,200:INFO: Dataset: univ                Batch: 10/15	Loss 1.8843 (1.8975)
+2022-11-18 15:06:47,369:INFO: Dataset: univ                Batch: 11/15	Loss 1.8601 (1.8938)
+2022-11-18 15:06:47,532:INFO: Dataset: univ                Batch: 12/15	Loss 1.9219 (1.8960)
+2022-11-18 15:06:47,693:INFO: Dataset: univ                Batch: 13/15	Loss 1.9050 (1.8967)
+2022-11-18 15:06:47,910:INFO: Dataset: univ                Batch: 14/15	Loss 1.9010 (1.8970)
+2022-11-18 15:06:48,014:INFO: Dataset: univ                Batch: 15/15	Loss 1.8912 (1.8969)
+2022-11-18 15:06:48,520:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9265 (1.9265)
+2022-11-18 15:06:48,702:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9118 (1.9194)
+2022-11-18 15:06:48,875:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9559 (1.9318)
+2022-11-18 15:06:49,049:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0063 (1.9506)
+2022-11-18 15:06:49,214:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9316 (1.9468)
+2022-11-18 15:06:49,389:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9098 (1.9408)
+2022-11-18 15:06:49,571:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8083 (1.9223)
+2022-11-18 15:06:49,722:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9138 (1.9213)
+2022-11-18 15:06:50,119:INFO: Dataset: zara2               Batch:  1/18	Loss 2.0146 (2.0146)
+2022-11-18 15:06:50,273:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8743 (1.9477)
+2022-11-18 15:06:50,427:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8568 (1.9189)
+2022-11-18 15:06:50,580:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9872 (1.9364)
+2022-11-18 15:06:50,757:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8773 (1.9243)
+2022-11-18 15:06:51,027:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9072 (1.9214)
+2022-11-18 15:06:51,181:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8702 (1.9137)
+2022-11-18 15:06:51,336:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8868 (1.9107)
+2022-11-18 15:06:51,488:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9199 (1.9117)
+2022-11-18 15:06:51,641:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9095 (1.9114)
+2022-11-18 15:06:51,791:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8813 (1.9090)
+2022-11-18 15:06:51,945:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9282 (1.9107)
+2022-11-18 15:06:52,097:INFO: Dataset: zara2               Batch: 13/18	Loss 2.0037 (1.9179)
+2022-11-18 15:06:52,254:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8063 (1.9104)
+2022-11-18 15:06:52,407:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9285 (1.9116)
+2022-11-18 15:06:52,562:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8824 (1.9100)
+2022-11-18 15:06:52,714:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9141 (1.9102)
+2022-11-18 15:06:52,859:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9642 (1.9127)
+2022-11-18 15:06:52,906:INFO: - Computing loss (validation)
+2022-11-18 15:06:53,172:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9475 (1.9475)
+2022-11-18 15:06:53,207:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0676 (1.9561)
+2022-11-18 15:06:53,519:INFO: Dataset: univ                Batch: 1/3	Loss 1.9248 (1.9248)
+2022-11-18 15:06:53,602:INFO: Dataset: univ                Batch: 2/3	Loss 1.8792 (1.9012)
+2022-11-18 15:06:53,674:INFO: Dataset: univ                Batch: 3/3	Loss 1.8917 (1.8983)
+2022-11-18 15:06:53,968:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8607 (1.8607)
+2022-11-18 15:06:54,015:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9619 (1.8861)
+2022-11-18 15:06:54,328:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8926 (1.8926)
+2022-11-18 15:06:54,402:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9143 (1.9031)
+2022-11-18 15:06:54,478:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8858 (1.8971)
+2022-11-18 15:06:54,554:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8854 (1.8940)
+2022-11-18 15:06:54,630:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8360 (1.8828)
+2022-11-18 15:06:54,684:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_300.pth.tar
+2022-11-18 15:06:54,684:INFO: 
+===> EPOCH: 301 (P2)
+2022-11-18 15:06:54,684:INFO: - Computing loss (training)
+2022-11-18 15:06:55,032:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1048 (2.1048)
+2022-11-18 15:06:55,187:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0851 (2.0950)
+2022-11-18 15:06:55,344:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1538 (2.1138)
+2022-11-18 15:06:55,462:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1854 (2.1257)
+2022-11-18 15:06:55,868:INFO: Dataset: univ                Batch:  1/15	Loss 1.8707 (1.8707)
+2022-11-18 15:06:56,034:INFO: Dataset: univ                Batch:  2/15	Loss 1.9005 (1.8859)
+2022-11-18 15:06:56,192:INFO: Dataset: univ                Batch:  3/15	Loss 1.8873 (1.8864)
+2022-11-18 15:06:56,347:INFO: Dataset: univ                Batch:  4/15	Loss 1.9623 (1.9043)
+2022-11-18 15:06:56,507:INFO: Dataset: univ                Batch:  5/15	Loss 1.9101 (1.9056)
+2022-11-18 15:06:56,667:INFO: Dataset: univ                Batch:  6/15	Loss 1.9166 (1.9074)
+2022-11-18 15:06:56,825:INFO: Dataset: univ                Batch:  7/15	Loss 1.9395 (1.9122)
+2022-11-18 15:06:56,981:INFO: Dataset: univ                Batch:  8/15	Loss 1.9111 (1.9121)
+2022-11-18 15:06:57,139:INFO: Dataset: univ                Batch:  9/15	Loss 1.9334 (1.9145)
+2022-11-18 15:06:57,295:INFO: Dataset: univ                Batch: 10/15	Loss 1.9335 (1.9163)
+2022-11-18 15:06:57,454:INFO: Dataset: univ                Batch: 11/15	Loss 1.9249 (1.9171)
+2022-11-18 15:06:57,612:INFO: Dataset: univ                Batch: 12/15	Loss 1.9651 (1.9212)
+2022-11-18 15:06:57,770:INFO: Dataset: univ                Batch: 13/15	Loss 1.8950 (1.9191)
+2022-11-18 15:06:57,936:INFO: Dataset: univ                Batch: 14/15	Loss 1.9605 (1.9220)
+2022-11-18 15:06:58,020:INFO: Dataset: univ                Batch: 15/15	Loss 1.9589 (1.9226)
+2022-11-18 15:06:58,421:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9841 (1.9841)
+2022-11-18 15:06:58,575:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8192 (1.8996)
+2022-11-18 15:06:58,729:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8958 (1.8982)
+2022-11-18 15:06:58,887:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0565 (1.9402)
+2022-11-18 15:06:59,042:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0458 (1.9587)
+2022-11-18 15:06:59,196:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9564 (1.9584)
+2022-11-18 15:06:59,353:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9686 (1.9598)
+2022-11-18 15:06:59,496:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9207 (1.9557)
+2022-11-18 15:06:59,900:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9195 (1.9195)
+2022-11-18 15:07:00,053:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9676 (1.9438)
+2022-11-18 15:07:00,209:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9002 (1.9281)
+2022-11-18 15:07:00,359:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9015 (1.9211)
+2022-11-18 15:07:00,513:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8643 (1.9105)
+2022-11-18 15:07:00,671:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8514 (1.9002)
+2022-11-18 15:07:00,827:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8820 (1.8976)
+2022-11-18 15:07:00,983:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9575 (1.9053)
+2022-11-18 15:07:01,142:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8546 (1.8997)
+2022-11-18 15:07:01,294:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9363 (1.9035)
+2022-11-18 15:07:01,447:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9837 (1.9112)
+2022-11-18 15:07:01,596:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9245 (1.9123)
+2022-11-18 15:07:01,746:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9664 (1.9169)
+2022-11-18 15:07:01,897:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9727 (1.9206)
+2022-11-18 15:07:02,049:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8177 (1.9131)
+2022-11-18 15:07:02,205:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9755 (1.9166)
+2022-11-18 15:07:02,358:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9010 (1.9157)
+2022-11-18 15:07:02,498:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9504 (1.9175)
+2022-11-18 15:07:02,544:INFO: - Computing loss (validation)
+2022-11-18 15:07:02,812:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0496 (2.0496)
+2022-11-18 15:07:02,848:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7228 (2.0273)
+2022-11-18 15:07:03,151:INFO: Dataset: univ                Batch: 1/3	Loss 1.9322 (1.9322)
+2022-11-18 15:07:03,229:INFO: Dataset: univ                Batch: 2/3	Loss 1.9454 (1.9388)
+2022-11-18 15:07:03,303:INFO: Dataset: univ                Batch: 3/3	Loss 1.9186 (1.9318)
+2022-11-18 15:07:03,624:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0527 (2.0527)
+2022-11-18 15:07:03,672:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9083 (2.0160)
+2022-11-18 15:07:03,985:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9996 (1.9996)
+2022-11-18 15:07:04,069:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9303 (1.9664)
+2022-11-18 15:07:04,148:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9446 (1.9591)
+2022-11-18 15:07:04,226:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9820 (1.9650)
+2022-11-18 15:07:04,303:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8816 (1.9481)
+2022-11-18 15:07:04,356:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_301.pth.tar
+2022-11-18 15:07:04,356:INFO: 
+===> EPOCH: 302 (P2)
+2022-11-18 15:07:04,357:INFO: - Computing loss (training)
+2022-11-18 15:07:04,703:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0278 (2.0278)
+2022-11-18 15:07:04,857:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0361 (2.0318)
+2022-11-18 15:07:05,010:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0197 (2.0277)
+2022-11-18 15:07:05,129:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9741 (2.0184)
+2022-11-18 15:07:05,553:INFO: Dataset: univ                Batch:  1/15	Loss 1.9291 (1.9291)
+2022-11-18 15:07:05,726:INFO: Dataset: univ                Batch:  2/15	Loss 1.9240 (1.9265)
+2022-11-18 15:07:05,898:INFO: Dataset: univ                Batch:  3/15	Loss 1.9221 (1.9250)
+2022-11-18 15:07:06,067:INFO: Dataset: univ                Batch:  4/15	Loss 1.8872 (1.9156)
+2022-11-18 15:07:06,234:INFO: Dataset: univ                Batch:  5/15	Loss 1.9242 (1.9172)
+2022-11-18 15:07:06,404:INFO: Dataset: univ                Batch:  6/15	Loss 1.9708 (1.9259)
+2022-11-18 15:07:06,570:INFO: Dataset: univ                Batch:  7/15	Loss 1.8897 (1.9209)
+2022-11-18 15:07:06,736:INFO: Dataset: univ                Batch:  8/15	Loss 1.9398 (1.9232)
+2022-11-18 15:07:06,904:INFO: Dataset: univ                Batch:  9/15	Loss 1.8909 (1.9192)
+2022-11-18 15:07:07,069:INFO: Dataset: univ                Batch: 10/15	Loss 1.9383 (1.9210)
+2022-11-18 15:07:07,236:INFO: Dataset: univ                Batch: 11/15	Loss 1.9547 (1.9239)
+2022-11-18 15:07:07,404:INFO: Dataset: univ                Batch: 12/15	Loss 1.8935 (1.9216)
+2022-11-18 15:07:07,572:INFO: Dataset: univ                Batch: 13/15	Loss 1.9358 (1.9227)
+2022-11-18 15:07:07,739:INFO: Dataset: univ                Batch: 14/15	Loss 1.9097 (1.9217)
+2022-11-18 15:07:07,828:INFO: Dataset: univ                Batch: 15/15	Loss 1.9845 (1.9226)
+2022-11-18 15:07:08,215:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0557 (2.0557)
+2022-11-18 15:07:08,367:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8992 (1.9805)
+2022-11-18 15:07:08,518:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9183 (1.9597)
+2022-11-18 15:07:08,668:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9550 (1.9585)
+2022-11-18 15:07:08,822:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8671 (1.9406)
+2022-11-18 15:07:08,974:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0149 (1.9522)
+2022-11-18 15:07:09,124:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9129 (1.9465)
+2022-11-18 15:07:09,260:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9699 (1.9492)
+2022-11-18 15:07:09,647:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9312 (1.9312)
+2022-11-18 15:07:09,798:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9635 (1.9476)
+2022-11-18 15:07:09,953:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9643 (1.9537)
+2022-11-18 15:07:10,108:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0747 (1.9851)
+2022-11-18 15:07:10,260:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9614 (1.9798)
+2022-11-18 15:07:10,411:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9976 (1.9830)
+2022-11-18 15:07:10,562:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9860 (1.9835)
+2022-11-18 15:07:10,710:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9126 (1.9742)
+2022-11-18 15:07:10,862:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9183 (1.9681)
+2022-11-18 15:07:11,011:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8695 (1.9571)
+2022-11-18 15:07:11,164:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0328 (1.9633)
+2022-11-18 15:07:11,315:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0580 (1.9711)
+2022-11-18 15:07:11,466:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9097 (1.9659)
+2022-11-18 15:07:11,616:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0710 (1.9736)
+2022-11-18 15:07:11,768:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8856 (1.9679)
+2022-11-18 15:07:11,918:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8775 (1.9622)
+2022-11-18 15:07:12,071:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9879 (1.9637)
+2022-11-18 15:07:12,209:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9990 (1.9653)
+2022-11-18 15:07:12,269:INFO: - Computing loss (validation)
+2022-11-18 15:07:12,535:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9543 (1.9543)
+2022-11-18 15:07:12,570:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9033 (1.9509)
+2022-11-18 15:07:12,874:INFO: Dataset: univ                Batch: 1/3	Loss 1.9668 (1.9668)
+2022-11-18 15:07:12,957:INFO: Dataset: univ                Batch: 2/3	Loss 1.9640 (1.9653)
+2022-11-18 15:07:13,034:INFO: Dataset: univ                Batch: 3/3	Loss 1.9484 (1.9595)
+2022-11-18 15:07:13,338:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0060 (2.0060)
+2022-11-18 15:07:13,384:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8935 (1.9770)
+2022-11-18 15:07:13,706:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9021 (1.9021)
+2022-11-18 15:07:13,786:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9668 (1.9335)
+2022-11-18 15:07:13,864:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8804 (1.9164)
+2022-11-18 15:07:13,942:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9635 (1.9281)
+2022-11-18 15:07:14,019:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9143 (1.9253)
+2022-11-18 15:07:14,078:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_302.pth.tar
+2022-11-18 15:07:14,078:INFO: 
+===> EPOCH: 303 (P2)
+2022-11-18 15:07:14,078:INFO: - Computing loss (training)
+2022-11-18 15:07:14,425:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9522 (1.9522)
+2022-11-18 15:07:14,581:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9609 (1.9566)
+2022-11-18 15:07:14,736:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0175 (1.9777)
+2022-11-18 15:07:14,855:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8889 (1.9636)
+2022-11-18 15:07:15,297:INFO: Dataset: univ                Batch:  1/15	Loss 1.9176 (1.9176)
+2022-11-18 15:07:15,455:INFO: Dataset: univ                Batch:  2/15	Loss 1.9480 (1.9332)
+2022-11-18 15:07:15,614:INFO: Dataset: univ                Batch:  3/15	Loss 1.9087 (1.9248)
+2022-11-18 15:07:15,772:INFO: Dataset: univ                Batch:  4/15	Loss 1.9181 (1.9231)
+2022-11-18 15:07:15,930:INFO: Dataset: univ                Batch:  5/15	Loss 1.9332 (1.9251)
+2022-11-18 15:07:16,091:INFO: Dataset: univ                Batch:  6/15	Loss 1.9189 (1.9242)
+2022-11-18 15:07:16,249:INFO: Dataset: univ                Batch:  7/15	Loss 1.9715 (1.9314)
+2022-11-18 15:07:16,406:INFO: Dataset: univ                Batch:  8/15	Loss 1.9241 (1.9304)
+2022-11-18 15:07:16,563:INFO: Dataset: univ                Batch:  9/15	Loss 1.9149 (1.9289)
+2022-11-18 15:07:16,719:INFO: Dataset: univ                Batch: 10/15	Loss 1.9186 (1.9278)
+2022-11-18 15:07:16,877:INFO: Dataset: univ                Batch: 11/15	Loss 1.9461 (1.9294)
+2022-11-18 15:07:17,037:INFO: Dataset: univ                Batch: 12/15	Loss 1.9324 (1.9297)
+2022-11-18 15:07:17,195:INFO: Dataset: univ                Batch: 13/15	Loss 1.9438 (1.9308)
+2022-11-18 15:07:17,353:INFO: Dataset: univ                Batch: 14/15	Loss 1.9475 (1.9321)
+2022-11-18 15:07:17,437:INFO: Dataset: univ                Batch: 15/15	Loss 1.8919 (1.9316)
+2022-11-18 15:07:17,835:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8958 (1.8958)
+2022-11-18 15:07:17,997:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8896 (1.8926)
+2022-11-18 15:07:18,157:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9228 (1.9029)
+2022-11-18 15:07:18,315:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9570 (1.9155)
+2022-11-18 15:07:18,474:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8464 (1.9015)
+2022-11-18 15:07:18,634:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9720 (1.9125)
+2022-11-18 15:07:18,795:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0087 (1.9250)
+2022-11-18 15:07:18,943:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9716 (1.9300)
+2022-11-18 15:07:19,365:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9470 (1.9470)
+2022-11-18 15:07:19,516:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9270 (1.9376)
+2022-11-18 15:07:19,672:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9580 (1.9440)
+2022-11-18 15:07:19,822:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8962 (1.9323)
+2022-11-18 15:07:19,976:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9120 (1.9282)
+2022-11-18 15:07:20,131:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9986 (1.9392)
+2022-11-18 15:07:20,281:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9288 (1.9375)
+2022-11-18 15:07:20,430:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0189 (1.9471)
+2022-11-18 15:07:20,581:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9186 (1.9443)
+2022-11-18 15:07:20,730:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9556 (1.9454)
+2022-11-18 15:07:20,881:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8930 (1.9413)
+2022-11-18 15:07:21,032:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9516 (1.9421)
+2022-11-18 15:07:21,183:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9274 (1.9410)
+2022-11-18 15:07:21,334:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8097 (1.9306)
+2022-11-18 15:07:21,487:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0167 (1.9369)
+2022-11-18 15:07:21,637:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9081 (1.9349)
+2022-11-18 15:07:21,789:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9656 (1.9363)
+2022-11-18 15:07:21,927:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9035 (1.9346)
+2022-11-18 15:07:21,973:INFO: - Computing loss (validation)
+2022-11-18 15:07:22,243:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9860 (1.9860)
+2022-11-18 15:07:22,278:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8101 (1.9752)
+2022-11-18 15:07:22,611:INFO: Dataset: univ                Batch: 1/3	Loss 1.9221 (1.9221)
+2022-11-18 15:07:22,688:INFO: Dataset: univ                Batch: 2/3	Loss 1.9242 (1.9231)
+2022-11-18 15:07:22,761:INFO: Dataset: univ                Batch: 3/3	Loss 1.8853 (1.9105)
+2022-11-18 15:07:23,064:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8778 (1.8778)
+2022-11-18 15:07:23,112:INFO: Dataset: zara1               Batch: 2/2	Loss 2.1479 (1.9508)
+2022-11-18 15:07:23,424:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9459 (1.9459)
+2022-11-18 15:07:23,498:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9544 (1.9502)
+2022-11-18 15:07:23,571:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9922 (1.9645)
+2022-11-18 15:07:23,645:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8990 (1.9483)
+2022-11-18 15:07:23,719:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9753 (1.9536)
+2022-11-18 15:07:23,772:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_303.pth.tar
+2022-11-18 15:07:23,772:INFO: 
+===> EPOCH: 304 (P2)
+2022-11-18 15:07:23,772:INFO: - Computing loss (training)
+2022-11-18 15:07:24,122:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0665 (2.0665)
+2022-11-18 15:07:24,283:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0004 (2.0340)
+2022-11-18 15:07:24,441:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8730 (1.9799)
+2022-11-18 15:07:24,561:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0445 (1.9905)
+2022-11-18 15:07:24,966:INFO: Dataset: univ                Batch:  1/15	Loss 1.8946 (1.8946)
+2022-11-18 15:07:25,130:INFO: Dataset: univ                Batch:  2/15	Loss 1.9330 (1.9150)
+2022-11-18 15:07:25,290:INFO: Dataset: univ                Batch:  3/15	Loss 1.8615 (1.8968)
+2022-11-18 15:07:25,449:INFO: Dataset: univ                Batch:  4/15	Loss 1.8932 (1.8960)
+2022-11-18 15:07:25,608:INFO: Dataset: univ                Batch:  5/15	Loss 1.9090 (1.8989)
+2022-11-18 15:07:25,767:INFO: Dataset: univ                Batch:  6/15	Loss 1.9187 (1.9021)
+2022-11-18 15:07:25,926:INFO: Dataset: univ                Batch:  7/15	Loss 1.9225 (1.9048)
+2022-11-18 15:07:26,084:INFO: Dataset: univ                Batch:  8/15	Loss 1.9452 (1.9100)
+2022-11-18 15:07:26,243:INFO: Dataset: univ                Batch:  9/15	Loss 1.9140 (1.9104)
+2022-11-18 15:07:26,399:INFO: Dataset: univ                Batch: 10/15	Loss 1.9082 (1.9102)
+2022-11-18 15:07:26,560:INFO: Dataset: univ                Batch: 11/15	Loss 1.9291 (1.9120)
+2022-11-18 15:07:26,719:INFO: Dataset: univ                Batch: 12/15	Loss 1.9453 (1.9145)
+2022-11-18 15:07:26,885:INFO: Dataset: univ                Batch: 13/15	Loss 1.9104 (1.9142)
+2022-11-18 15:07:27,046:INFO: Dataset: univ                Batch: 14/15	Loss 1.8693 (1.9111)
+2022-11-18 15:07:27,132:INFO: Dataset: univ                Batch: 15/15	Loss 2.0139 (1.9125)
+2022-11-18 15:07:27,527:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9230 (1.9230)
+2022-11-18 15:07:27,687:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9208 (1.9220)
+2022-11-18 15:07:27,846:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9576 (1.9342)
+2022-11-18 15:07:28,007:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9615 (1.9396)
+2022-11-18 15:07:28,167:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9326 (1.9381)
+2022-11-18 15:07:28,326:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9102 (1.9333)
+2022-11-18 15:07:28,486:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9950 (1.9424)
+2022-11-18 15:07:28,631:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8453 (1.9329)
+2022-11-18 15:07:29,020:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8882 (1.8882)
+2022-11-18 15:07:29,179:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8996 (1.8939)
+2022-11-18 15:07:29,336:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8763 (1.8883)
+2022-11-18 15:07:29,486:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9343 (1.8998)
+2022-11-18 15:07:29,637:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9288 (1.9057)
+2022-11-18 15:07:29,790:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9778 (1.9190)
+2022-11-18 15:07:29,940:INFO: Dataset: zara2               Batch:  7/18	Loss 2.0024 (1.9308)
+2022-11-18 15:07:30,092:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8555 (1.9216)
+2022-11-18 15:07:30,242:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9626 (1.9263)
+2022-11-18 15:07:30,394:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8989 (1.9235)
+2022-11-18 15:07:30,544:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8720 (1.9185)
+2022-11-18 15:07:30,698:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0035 (1.9249)
+2022-11-18 15:07:30,850:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9263 (1.9251)
+2022-11-18 15:07:30,999:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8930 (1.9226)
+2022-11-18 15:07:31,154:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9518 (1.9246)
+2022-11-18 15:07:31,307:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9192 (1.9243)
+2022-11-18 15:07:31,458:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9416 (1.9253)
+2022-11-18 15:07:31,597:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8749 (1.9230)
+2022-11-18 15:07:31,643:INFO: - Computing loss (validation)
+2022-11-18 15:07:31,904:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0407 (2.0407)
+2022-11-18 15:07:31,939:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7894 (2.0210)
+2022-11-18 15:07:32,245:INFO: Dataset: univ                Batch: 1/3	Loss 1.9408 (1.9408)
+2022-11-18 15:07:32,322:INFO: Dataset: univ                Batch: 2/3	Loss 1.9466 (1.9438)
+2022-11-18 15:07:32,393:INFO: Dataset: univ                Batch: 3/3	Loss 1.9528 (1.9466)
+2022-11-18 15:07:32,694:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0375 (2.0375)
+2022-11-18 15:07:32,738:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8542 (1.9927)
+2022-11-18 15:07:33,054:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8818 (1.8818)
+2022-11-18 15:07:33,131:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9190 (1.8999)
+2022-11-18 15:07:33,204:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8749 (1.8912)
+2022-11-18 15:07:33,279:INFO: Dataset: zara2               Batch: 4/5	Loss 2.0171 (1.9223)
+2022-11-18 15:07:33,353:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9432 (1.9262)
+2022-11-18 15:07:33,408:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_304.pth.tar
+2022-11-18 15:07:33,408:INFO: 
+===> EPOCH: 305 (P2)
+2022-11-18 15:07:33,409:INFO: - Computing loss (training)
+2022-11-18 15:07:33,756:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9818 (1.9818)
+2022-11-18 15:07:33,908:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9366 (1.9574)
+2022-11-18 15:07:34,064:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9883 (1.9677)
+2022-11-18 15:07:34,181:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8970 (1.9566)
+2022-11-18 15:07:34,591:INFO: Dataset: univ                Batch:  1/15	Loss 1.9225 (1.9225)
+2022-11-18 15:07:34,750:INFO: Dataset: univ                Batch:  2/15	Loss 1.8742 (1.8998)
+2022-11-18 15:07:34,908:INFO: Dataset: univ                Batch:  3/15	Loss 1.9112 (1.9035)
+2022-11-18 15:07:35,070:INFO: Dataset: univ                Batch:  4/15	Loss 1.9656 (1.9186)
+2022-11-18 15:07:35,241:INFO: Dataset: univ                Batch:  5/15	Loss 1.9469 (1.9237)
+2022-11-18 15:07:35,399:INFO: Dataset: univ                Batch:  6/15	Loss 1.9185 (1.9228)
+2022-11-18 15:07:35,557:INFO: Dataset: univ                Batch:  7/15	Loss 1.8561 (1.9128)
+2022-11-18 15:07:35,711:INFO: Dataset: univ                Batch:  8/15	Loss 1.9322 (1.9151)
+2022-11-18 15:07:35,867:INFO: Dataset: univ                Batch:  9/15	Loss 1.9558 (1.9196)
+2022-11-18 15:07:36,023:INFO: Dataset: univ                Batch: 10/15	Loss 1.9267 (1.9202)
+2022-11-18 15:07:36,182:INFO: Dataset: univ                Batch: 11/15	Loss 1.9406 (1.9223)
+2022-11-18 15:07:36,336:INFO: Dataset: univ                Batch: 12/15	Loss 1.8869 (1.9191)
+2022-11-18 15:07:36,493:INFO: Dataset: univ                Batch: 13/15	Loss 1.9171 (1.9189)
+2022-11-18 15:07:36,648:INFO: Dataset: univ                Batch: 14/15	Loss 1.9118 (1.9185)
+2022-11-18 15:07:36,731:INFO: Dataset: univ                Batch: 15/15	Loss 1.9374 (1.9187)
+2022-11-18 15:07:37,131:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9849 (1.9849)
+2022-11-18 15:07:37,286:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9538 (1.9683)
+2022-11-18 15:07:37,446:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8738 (1.9381)
+2022-11-18 15:07:37,602:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8725 (1.9218)
+2022-11-18 15:07:37,758:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9702 (1.9315)
+2022-11-18 15:07:37,914:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9639 (1.9370)
+2022-11-18 15:07:38,071:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9299 (1.9361)
+2022-11-18 15:07:38,214:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8520 (1.9258)
+2022-11-18 15:07:38,610:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9961 (1.9961)
+2022-11-18 15:07:38,772:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9223 (1.9613)
+2022-11-18 15:07:38,941:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9206 (1.9481)
+2022-11-18 15:07:39,100:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0135 (1.9647)
+2022-11-18 15:07:39,266:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9184 (1.9557)
+2022-11-18 15:07:39,426:INFO: Dataset: zara2               Batch:  6/18	Loss 2.0232 (1.9682)
+2022-11-18 15:07:39,585:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9538 (1.9661)
+2022-11-18 15:07:39,742:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9203 (1.9604)
+2022-11-18 15:07:39,901:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7959 (1.9433)
+2022-11-18 15:07:40,059:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9041 (1.9393)
+2022-11-18 15:07:40,220:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9386 (1.9392)
+2022-11-18 15:07:40,378:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8763 (1.9340)
+2022-11-18 15:07:40,538:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8723 (1.9292)
+2022-11-18 15:07:40,697:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9693 (1.9322)
+2022-11-18 15:07:40,864:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9314 (1.9321)
+2022-11-18 15:07:41,032:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8568 (1.9274)
+2022-11-18 15:07:41,194:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9101 (1.9263)
+2022-11-18 15:07:41,343:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9285 (1.9264)
+2022-11-18 15:07:41,388:INFO: - Computing loss (validation)
+2022-11-18 15:07:41,662:INFO: Dataset: hotel               Batch: 1/2	Loss 2.1033 (2.1033)
+2022-11-18 15:07:41,699:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9868 (2.0966)
+2022-11-18 15:07:42,004:INFO: Dataset: univ                Batch: 1/3	Loss 1.8905 (1.8905)
+2022-11-18 15:07:42,088:INFO: Dataset: univ                Batch: 2/3	Loss 1.9006 (1.8956)
+2022-11-18 15:07:42,163:INFO: Dataset: univ                Batch: 3/3	Loss 1.9257 (1.9042)
+2022-11-18 15:07:42,456:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8635 (1.8635)
+2022-11-18 15:07:42,501:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9946 (1.8947)
+2022-11-18 15:07:42,809:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8892 (1.8892)
+2022-11-18 15:07:42,886:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9002 (1.8948)
+2022-11-18 15:07:42,960:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9576 (1.9149)
+2022-11-18 15:07:43,036:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8562 (1.9011)
+2022-11-18 15:07:43,111:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8964 (1.9001)
+2022-11-18 15:07:43,164:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_305.pth.tar
+2022-11-18 15:07:43,164:INFO: 
+===> EPOCH: 306 (P2)
+2022-11-18 15:07:43,164:INFO: - Computing loss (training)
+2022-11-18 15:07:43,507:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8768 (1.8768)
+2022-11-18 15:07:43,659:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9732 (1.9221)
+2022-11-18 15:07:43,810:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8685 (1.9042)
+2022-11-18 15:07:43,925:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9965 (1.9183)
+2022-11-18 15:07:44,334:INFO: Dataset: univ                Batch:  1/15	Loss 1.9131 (1.9131)
+2022-11-18 15:07:44,490:INFO: Dataset: univ                Batch:  2/15	Loss 1.8704 (1.8910)
+2022-11-18 15:07:44,654:INFO: Dataset: univ                Batch:  3/15	Loss 1.9509 (1.9114)
+2022-11-18 15:07:44,809:INFO: Dataset: univ                Batch:  4/15	Loss 1.9101 (1.9111)
+2022-11-18 15:07:44,965:INFO: Dataset: univ                Batch:  5/15	Loss 1.9436 (1.9182)
+2022-11-18 15:07:45,125:INFO: Dataset: univ                Batch:  6/15	Loss 1.9240 (1.9192)
+2022-11-18 15:07:45,280:INFO: Dataset: univ                Batch:  7/15	Loss 1.9299 (1.9206)
+2022-11-18 15:07:45,434:INFO: Dataset: univ                Batch:  8/15	Loss 1.9368 (1.9226)
+2022-11-18 15:07:45,590:INFO: Dataset: univ                Batch:  9/15	Loss 1.9671 (1.9273)
+2022-11-18 15:07:45,745:INFO: Dataset: univ                Batch: 10/15	Loss 1.9252 (1.9271)
+2022-11-18 15:07:45,901:INFO: Dataset: univ                Batch: 11/15	Loss 1.8857 (1.9230)
+2022-11-18 15:07:46,058:INFO: Dataset: univ                Batch: 12/15	Loss 1.9388 (1.9244)
+2022-11-18 15:07:46,214:INFO: Dataset: univ                Batch: 13/15	Loss 1.9233 (1.9243)
+2022-11-18 15:07:46,369:INFO: Dataset: univ                Batch: 14/15	Loss 1.8774 (1.9209)
+2022-11-18 15:07:46,452:INFO: Dataset: univ                Batch: 15/15	Loss 1.9156 (1.9208)
+2022-11-18 15:07:46,847:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9417 (1.9417)
+2022-11-18 15:07:46,998:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9201 (1.9298)
+2022-11-18 15:07:47,148:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9380 (1.9324)
+2022-11-18 15:07:47,296:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8720 (1.9169)
+2022-11-18 15:07:47,446:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7382 (1.8821)
+2022-11-18 15:07:47,598:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9003 (1.8854)
+2022-11-18 15:07:47,749:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9656 (1.8968)
+2022-11-18 15:07:47,885:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8644 (1.8928)
+2022-11-18 15:07:48,277:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8576 (1.8576)
+2022-11-18 15:07:48,432:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0072 (1.9352)
+2022-11-18 15:07:48,587:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9110 (1.9271)
+2022-11-18 15:07:48,743:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8941 (1.9187)
+2022-11-18 15:07:48,901:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9082 (1.9167)
+2022-11-18 15:07:49,062:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7829 (1.8921)
+2022-11-18 15:07:49,218:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8821 (1.8908)
+2022-11-18 15:07:49,370:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9244 (1.8949)
+2022-11-18 15:07:49,526:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9908 (1.9053)
+2022-11-18 15:07:49,681:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9540 (1.9101)
+2022-11-18 15:07:49,835:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9907 (1.9163)
+2022-11-18 15:07:49,988:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9248 (1.9171)
+2022-11-18 15:07:50,145:INFO: Dataset: zara2               Batch: 13/18	Loss 2.0554 (1.9272)
+2022-11-18 15:07:50,299:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9248 (1.9270)
+2022-11-18 15:07:50,453:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9009 (1.9252)
+2022-11-18 15:07:50,608:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9250 (1.9252)
+2022-11-18 15:07:50,764:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9610 (1.9272)
+2022-11-18 15:07:50,914:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8713 (1.9245)
+2022-11-18 15:07:50,959:INFO: - Computing loss (validation)
+2022-11-18 15:07:51,232:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9330 (1.9330)
+2022-11-18 15:07:51,269:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0804 (1.9411)
+2022-11-18 15:07:51,570:INFO: Dataset: univ                Batch: 1/3	Loss 1.8995 (1.8995)
+2022-11-18 15:07:51,649:INFO: Dataset: univ                Batch: 2/3	Loss 1.8522 (1.8746)
+2022-11-18 15:07:51,721:INFO: Dataset: univ                Batch: 3/3	Loss 1.9442 (1.8947)
+2022-11-18 15:07:52,018:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9880 (1.9880)
+2022-11-18 15:07:52,064:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8428 (1.9544)
+2022-11-18 15:07:52,384:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9994 (1.9994)
+2022-11-18 15:07:52,468:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9156 (1.9574)
+2022-11-18 15:07:52,555:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9961 (1.9698)
+2022-11-18 15:07:52,638:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9250 (1.9580)
+2022-11-18 15:07:52,717:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9280 (1.9520)
+2022-11-18 15:07:52,771:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_306.pth.tar
+2022-11-18 15:07:52,771:INFO: 
+===> EPOCH: 307 (P2)
+2022-11-18 15:07:52,771:INFO: - Computing loss (training)
+2022-11-18 15:07:53,117:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0030 (2.0030)
+2022-11-18 15:07:53,269:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9781 (1.9907)
+2022-11-18 15:07:53,423:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1062 (2.0305)
+2022-11-18 15:07:53,540:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9885 (2.0227)
+2022-11-18 15:07:53,945:INFO: Dataset: univ                Batch:  1/15	Loss 1.8891 (1.8891)
+2022-11-18 15:07:54,106:INFO: Dataset: univ                Batch:  2/15	Loss 1.9194 (1.9036)
+2022-11-18 15:07:54,267:INFO: Dataset: univ                Batch:  3/15	Loss 1.8955 (1.9008)
+2022-11-18 15:07:54,427:INFO: Dataset: univ                Batch:  4/15	Loss 2.0160 (1.9287)
+2022-11-18 15:07:54,588:INFO: Dataset: univ                Batch:  5/15	Loss 1.9219 (1.9273)
+2022-11-18 15:07:54,746:INFO: Dataset: univ                Batch:  6/15	Loss 1.9283 (1.9275)
+2022-11-18 15:07:54,902:INFO: Dataset: univ                Batch:  7/15	Loss 1.9107 (1.9253)
+2022-11-18 15:07:55,061:INFO: Dataset: univ                Batch:  8/15	Loss 1.9611 (1.9300)
+2022-11-18 15:07:55,219:INFO: Dataset: univ                Batch:  9/15	Loss 1.9216 (1.9291)
+2022-11-18 15:07:55,376:INFO: Dataset: univ                Batch: 10/15	Loss 1.9104 (1.9273)
+2022-11-18 15:07:55,534:INFO: Dataset: univ                Batch: 11/15	Loss 1.8777 (1.9227)
+2022-11-18 15:07:55,693:INFO: Dataset: univ                Batch: 12/15	Loss 1.9358 (1.9238)
+2022-11-18 15:07:55,850:INFO: Dataset: univ                Batch: 13/15	Loss 1.9081 (1.9227)
+2022-11-18 15:07:56,009:INFO: Dataset: univ                Batch: 14/15	Loss 1.9488 (1.9248)
+2022-11-18 15:07:56,094:INFO: Dataset: univ                Batch: 15/15	Loss 1.8002 (1.9233)
+2022-11-18 15:07:56,487:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0397 (2.0397)
+2022-11-18 15:07:56,640:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9619 (1.9974)
+2022-11-18 15:07:56,791:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9359 (1.9776)
+2022-11-18 15:07:56,940:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8924 (1.9563)
+2022-11-18 15:07:57,096:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9913 (1.9626)
+2022-11-18 15:07:57,247:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9942 (1.9683)
+2022-11-18 15:07:57,399:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9265 (1.9621)
+2022-11-18 15:07:57,538:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9515 (1.9610)
+2022-11-18 15:07:57,936:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9388 (1.9388)
+2022-11-18 15:07:58,089:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9369 (1.9378)
+2022-11-18 15:07:58,243:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9155 (1.9304)
+2022-11-18 15:07:58,393:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8753 (1.9155)
+2022-11-18 15:07:58,549:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8874 (1.9095)
+2022-11-18 15:07:58,704:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9121 (1.9099)
+2022-11-18 15:07:58,861:INFO: Dataset: zara2               Batch:  7/18	Loss 2.0003 (1.9224)
+2022-11-18 15:07:59,012:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8729 (1.9160)
+2022-11-18 15:07:59,166:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9083 (1.9152)
+2022-11-18 15:07:59,316:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9625 (1.9194)
+2022-11-18 15:07:59,469:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9959 (1.9259)
+2022-11-18 15:07:59,622:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9460 (1.9277)
+2022-11-18 15:07:59,779:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9848 (1.9323)
+2022-11-18 15:07:59,930:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0091 (1.9374)
+2022-11-18 15:08:00,087:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0038 (1.9416)
+2022-11-18 15:08:00,238:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9341 (1.9411)
+2022-11-18 15:08:00,393:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8704 (1.9372)
+2022-11-18 15:08:00,534:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8651 (1.9339)
+2022-11-18 15:08:00,578:INFO: - Computing loss (validation)
+2022-11-18 15:08:00,847:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0640 (2.0640)
+2022-11-18 15:08:00,883:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9601 (2.0548)
+2022-11-18 15:08:01,194:INFO: Dataset: univ                Batch: 1/3	Loss 1.8986 (1.8986)
+2022-11-18 15:08:01,273:INFO: Dataset: univ                Batch: 2/3	Loss 1.9426 (1.9199)
+2022-11-18 15:08:01,344:INFO: Dataset: univ                Batch: 3/3	Loss 1.9010 (1.9144)
+2022-11-18 15:08:01,647:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8600 (1.8600)
+2022-11-18 15:08:01,692:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8637 (1.8610)
+2022-11-18 15:08:01,997:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8809 (1.8809)
+2022-11-18 15:08:02,072:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9446 (1.9120)
+2022-11-18 15:08:02,145:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9069 (1.9104)
+2022-11-18 15:08:02,220:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9931 (1.9319)
+2022-11-18 15:08:02,292:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9047 (1.9265)
+2022-11-18 15:08:02,344:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_307.pth.tar
+2022-11-18 15:08:02,344:INFO: 
+===> EPOCH: 308 (P2)
+2022-11-18 15:08:02,345:INFO: - Computing loss (training)
+2022-11-18 15:08:02,681:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0615 (2.0615)
+2022-11-18 15:08:02,833:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0847 (2.0726)
+2022-11-18 15:08:02,983:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0875 (2.0775)
+2022-11-18 15:08:03,101:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0156 (2.0662)
+2022-11-18 15:08:03,495:INFO: Dataset: univ                Batch:  1/15	Loss 1.9149 (1.9149)
+2022-11-18 15:08:03,652:INFO: Dataset: univ                Batch:  2/15	Loss 1.8765 (1.8953)
+2022-11-18 15:08:03,812:INFO: Dataset: univ                Batch:  3/15	Loss 1.9272 (1.9059)
+2022-11-18 15:08:03,969:INFO: Dataset: univ                Batch:  4/15	Loss 1.9337 (1.9133)
+2022-11-18 15:08:04,130:INFO: Dataset: univ                Batch:  5/15	Loss 1.9162 (1.9139)
+2022-11-18 15:08:04,289:INFO: Dataset: univ                Batch:  6/15	Loss 1.8993 (1.9115)
+2022-11-18 15:08:04,446:INFO: Dataset: univ                Batch:  7/15	Loss 1.8700 (1.9049)
+2022-11-18 15:08:04,602:INFO: Dataset: univ                Batch:  8/15	Loss 1.9234 (1.9072)
+2022-11-18 15:08:04,759:INFO: Dataset: univ                Batch:  9/15	Loss 1.8967 (1.9060)
+2022-11-18 15:08:04,914:INFO: Dataset: univ                Batch: 10/15	Loss 1.9482 (1.9102)
+2022-11-18 15:08:05,072:INFO: Dataset: univ                Batch: 11/15	Loss 1.8751 (1.9070)
+2022-11-18 15:08:05,229:INFO: Dataset: univ                Batch: 12/15	Loss 1.9221 (1.9083)
+2022-11-18 15:08:05,386:INFO: Dataset: univ                Batch: 13/15	Loss 1.9048 (1.9080)
+2022-11-18 15:08:05,541:INFO: Dataset: univ                Batch: 14/15	Loss 1.9427 (1.9102)
+2022-11-18 15:08:05,624:INFO: Dataset: univ                Batch: 15/15	Loss 1.8614 (1.9096)
+2022-11-18 15:08:06,015:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9198 (1.9198)
+2022-11-18 15:08:06,168:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8801 (1.9000)
+2022-11-18 15:08:06,324:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9018 (1.9007)
+2022-11-18 15:08:06,476:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8864 (1.8970)
+2022-11-18 15:08:06,632:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9558 (1.9075)
+2022-11-18 15:08:06,785:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9354 (1.9122)
+2022-11-18 15:08:06,938:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8683 (1.9057)
+2022-11-18 15:08:07,079:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9618 (1.9115)
+2022-11-18 15:08:07,487:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8893 (1.8893)
+2022-11-18 15:08:07,643:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9728 (1.9278)
+2022-11-18 15:08:07,796:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8853 (1.9143)
+2022-11-18 15:08:07,946:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9901 (1.9321)
+2022-11-18 15:08:08,102:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0312 (1.9510)
+2022-11-18 15:08:08,253:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8310 (1.9306)
+2022-11-18 15:08:08,403:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9639 (1.9350)
+2022-11-18 15:08:08,552:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9026 (1.9307)
+2022-11-18 15:08:08,703:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8783 (1.9251)
+2022-11-18 15:08:08,856:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8483 (1.9172)
+2022-11-18 15:08:09,008:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8970 (1.9156)
+2022-11-18 15:08:09,159:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8711 (1.9116)
+2022-11-18 15:08:09,310:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9718 (1.9159)
+2022-11-18 15:08:09,538:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9857 (1.9206)
+2022-11-18 15:08:09,689:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9052 (1.9197)
+2022-11-18 15:08:09,841:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8621 (1.9160)
+2022-11-18 15:08:09,992:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0148 (1.9217)
+2022-11-18 15:08:10,132:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8601 (1.9186)
+2022-11-18 15:08:10,178:INFO: - Computing loss (validation)
+2022-11-18 15:08:10,438:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9538 (1.9538)
+2022-11-18 15:08:10,471:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8297 (1.9432)
+2022-11-18 15:08:10,775:INFO: Dataset: univ                Batch: 1/3	Loss 1.8936 (1.8936)
+2022-11-18 15:08:10,851:INFO: Dataset: univ                Batch: 2/3	Loss 1.9371 (1.9157)
+2022-11-18 15:08:10,925:INFO: Dataset: univ                Batch: 3/3	Loss 1.9171 (1.9162)
+2022-11-18 15:08:11,230:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0418 (2.0418)
+2022-11-18 15:08:11,279:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8508 (1.9902)
+2022-11-18 15:08:11,578:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9478 (1.9478)
+2022-11-18 15:08:11,651:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9144 (1.9313)
+2022-11-18 15:08:11,727:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9285 (1.9303)
+2022-11-18 15:08:11,801:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9686 (1.9388)
+2022-11-18 15:08:11,874:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9367 (1.9384)
+2022-11-18 15:08:11,925:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_308.pth.tar
+2022-11-18 15:08:11,925:INFO: 
+===> EPOCH: 309 (P2)
+2022-11-18 15:08:11,926:INFO: - Computing loss (training)
+2022-11-18 15:08:12,275:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9313 (1.9313)
+2022-11-18 15:08:12,432:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8247 (1.8793)
+2022-11-18 15:08:12,592:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9413 (1.8998)
+2022-11-18 15:08:12,714:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0174 (1.9201)
+2022-11-18 15:08:13,115:INFO: Dataset: univ                Batch:  1/15	Loss 1.9286 (1.9286)
+2022-11-18 15:08:13,271:INFO: Dataset: univ                Batch:  2/15	Loss 1.9120 (1.9203)
+2022-11-18 15:08:13,429:INFO: Dataset: univ                Batch:  3/15	Loss 1.8826 (1.9086)
+2022-11-18 15:08:13,586:INFO: Dataset: univ                Batch:  4/15	Loss 1.8969 (1.9055)
+2022-11-18 15:08:13,745:INFO: Dataset: univ                Batch:  5/15	Loss 1.9150 (1.9075)
+2022-11-18 15:08:13,905:INFO: Dataset: univ                Batch:  6/15	Loss 1.9203 (1.9096)
+2022-11-18 15:08:14,063:INFO: Dataset: univ                Batch:  7/15	Loss 1.9306 (1.9129)
+2022-11-18 15:08:14,218:INFO: Dataset: univ                Batch:  8/15	Loss 1.9517 (1.9181)
+2022-11-18 15:08:14,375:INFO: Dataset: univ                Batch:  9/15	Loss 1.9282 (1.9192)
+2022-11-18 15:08:14,530:INFO: Dataset: univ                Batch: 10/15	Loss 1.8679 (1.9140)
+2022-11-18 15:08:14,687:INFO: Dataset: univ                Batch: 11/15	Loss 1.9150 (1.9141)
+2022-11-18 15:08:14,843:INFO: Dataset: univ                Batch: 12/15	Loss 1.9145 (1.9141)
+2022-11-18 15:08:14,999:INFO: Dataset: univ                Batch: 13/15	Loss 1.9350 (1.9157)
+2022-11-18 15:08:15,157:INFO: Dataset: univ                Batch: 14/15	Loss 1.8828 (1.9135)
+2022-11-18 15:08:15,240:INFO: Dataset: univ                Batch: 15/15	Loss 1.9135 (1.9135)
+2022-11-18 15:08:15,658:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9441 (1.9441)
+2022-11-18 15:08:15,806:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9029 (1.9229)
+2022-11-18 15:08:15,964:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0277 (1.9580)
+2022-11-18 15:08:16,114:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9651 (1.9599)
+2022-11-18 15:08:16,267:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9205 (1.9521)
+2022-11-18 15:08:16,415:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0194 (1.9631)
+2022-11-18 15:08:16,565:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9760 (1.9649)
+2022-11-18 15:08:16,700:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0125 (1.9704)
+2022-11-18 15:08:17,078:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9397 (1.9397)
+2022-11-18 15:08:17,235:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0413 (1.9897)
+2022-11-18 15:08:17,386:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9675 (1.9823)
+2022-11-18 15:08:17,536:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0369 (1.9959)
+2022-11-18 15:08:17,687:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8352 (1.9665)
+2022-11-18 15:08:17,838:INFO: Dataset: zara2               Batch:  6/18	Loss 2.0015 (1.9722)
+2022-11-18 15:08:17,988:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9630 (1.9708)
+2022-11-18 15:08:18,139:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9459 (1.9678)
+2022-11-18 15:08:18,289:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9378 (1.9643)
+2022-11-18 15:08:18,437:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9587 (1.9638)
+2022-11-18 15:08:18,591:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9344 (1.9611)
+2022-11-18 15:08:18,740:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9918 (1.9637)
+2022-11-18 15:08:18,893:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9483 (1.9626)
+2022-11-18 15:08:19,047:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9253 (1.9601)
+2022-11-18 15:08:19,200:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8447 (1.9529)
+2022-11-18 15:08:19,350:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7699 (1.9407)
+2022-11-18 15:08:19,501:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9919 (1.9440)
+2022-11-18 15:08:19,640:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8944 (1.9414)
+2022-11-18 15:08:19,686:INFO: - Computing loss (validation)
+2022-11-18 15:08:19,938:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9182 (1.9182)
+2022-11-18 15:08:19,973:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9346 (1.9194)
+2022-11-18 15:08:20,292:INFO: Dataset: univ                Batch: 1/3	Loss 1.9163 (1.9163)
+2022-11-18 15:08:20,372:INFO: Dataset: univ                Batch: 2/3	Loss 1.8623 (1.8882)
+2022-11-18 15:08:20,447:INFO: Dataset: univ                Batch: 3/3	Loss 1.8317 (1.8720)
+2022-11-18 15:08:20,749:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0370 (2.0370)
+2022-11-18 15:08:20,794:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9309 (2.0141)
+2022-11-18 15:08:21,095:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9457 (1.9457)
+2022-11-18 15:08:21,171:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9342 (1.9397)
+2022-11-18 15:08:21,244:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9433 (1.9410)
+2022-11-18 15:08:21,317:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8739 (1.9238)
+2022-11-18 15:08:21,391:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8515 (1.9089)
+2022-11-18 15:08:21,444:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_309.pth.tar
+2022-11-18 15:08:21,445:INFO: 
+===> EPOCH: 310 (P2)
+2022-11-18 15:08:21,445:INFO: - Computing loss (training)
+2022-11-18 15:08:21,791:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9070 (1.9070)
+2022-11-18 15:08:21,951:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0297 (1.9682)
+2022-11-18 15:08:22,111:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9934 (1.9763)
+2022-11-18 15:08:22,235:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0134 (1.9823)
+2022-11-18 15:08:22,644:INFO: Dataset: univ                Batch:  1/15	Loss 1.9074 (1.9074)
+2022-11-18 15:08:22,816:INFO: Dataset: univ                Batch:  2/15	Loss 1.8754 (1.8913)
+2022-11-18 15:08:22,984:INFO: Dataset: univ                Batch:  3/15	Loss 1.9211 (1.9010)
+2022-11-18 15:08:23,150:INFO: Dataset: univ                Batch:  4/15	Loss 1.8905 (1.8981)
+2022-11-18 15:08:23,315:INFO: Dataset: univ                Batch:  5/15	Loss 1.9570 (1.9093)
+2022-11-18 15:08:23,481:INFO: Dataset: univ                Batch:  6/15	Loss 1.9570 (1.9169)
+2022-11-18 15:08:23,647:INFO: Dataset: univ                Batch:  7/15	Loss 1.9557 (1.9229)
+2022-11-18 15:08:23,810:INFO: Dataset: univ                Batch:  8/15	Loss 1.9156 (1.9220)
+2022-11-18 15:08:23,975:INFO: Dataset: univ                Batch:  9/15	Loss 1.8452 (1.9128)
+2022-11-18 15:08:24,139:INFO: Dataset: univ                Batch: 10/15	Loss 1.9462 (1.9161)
+2022-11-18 15:08:24,307:INFO: Dataset: univ                Batch: 11/15	Loss 1.9232 (1.9168)
+2022-11-18 15:08:24,471:INFO: Dataset: univ                Batch: 12/15	Loss 1.8920 (1.9148)
+2022-11-18 15:08:24,635:INFO: Dataset: univ                Batch: 13/15	Loss 1.9590 (1.9181)
+2022-11-18 15:08:24,801:INFO: Dataset: univ                Batch: 14/15	Loss 1.9051 (1.9171)
+2022-11-18 15:08:24,890:INFO: Dataset: univ                Batch: 15/15	Loss 1.8721 (1.9165)
+2022-11-18 15:08:25,281:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9706 (1.9706)
+2022-11-18 15:08:25,438:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8709 (1.9224)
+2022-11-18 15:08:25,601:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8482 (1.8951)
+2022-11-18 15:08:25,760:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0124 (1.9243)
+2022-11-18 15:08:25,919:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0282 (1.9453)
+2022-11-18 15:08:26,078:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8982 (1.9374)
+2022-11-18 15:08:26,237:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0611 (1.9528)
+2022-11-18 15:08:26,380:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8887 (1.9455)
+2022-11-18 15:08:26,765:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8741 (1.8741)
+2022-11-18 15:08:26,919:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9721 (1.9252)
+2022-11-18 15:08:27,076:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9172 (1.9227)
+2022-11-18 15:08:27,232:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8815 (1.9127)
+2022-11-18 15:08:27,386:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8916 (1.9085)
+2022-11-18 15:08:27,540:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9932 (1.9219)
+2022-11-18 15:08:27,695:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9070 (1.9197)
+2022-11-18 15:08:27,846:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9242 (1.9203)
+2022-11-18 15:08:27,999:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8713 (1.9150)
+2022-11-18 15:08:28,150:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9944 (1.9227)
+2022-11-18 15:08:28,303:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7946 (1.9120)
+2022-11-18 15:08:28,454:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9782 (1.9182)
+2022-11-18 15:08:28,607:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9736 (1.9230)
+2022-11-18 15:08:28,759:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9044 (1.9217)
+2022-11-18 15:08:28,916:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9116 (1.9210)
+2022-11-18 15:08:29,068:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9017 (1.9196)
+2022-11-18 15:08:29,224:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8659 (1.9165)
+2022-11-18 15:08:29,367:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9653 (1.9189)
+2022-11-18 15:08:29,413:INFO: - Computing loss (validation)
+2022-11-18 15:08:29,685:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9063 (1.9063)
+2022-11-18 15:08:29,720:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7699 (1.8970)
+2022-11-18 15:08:30,032:INFO: Dataset: univ                Batch: 1/3	Loss 1.9132 (1.9132)
+2022-11-18 15:08:30,112:INFO: Dataset: univ                Batch: 2/3	Loss 1.8590 (1.8888)
+2022-11-18 15:08:30,188:INFO: Dataset: univ                Batch: 3/3	Loss 1.9613 (1.9114)
+2022-11-18 15:08:30,491:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8751 (1.8751)
+2022-11-18 15:08:30,536:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0238 (1.9157)
+2022-11-18 15:08:30,833:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8420 (1.8420)
+2022-11-18 15:08:30,912:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8618 (1.8520)
+2022-11-18 15:08:30,992:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9238 (1.8766)
+2022-11-18 15:08:31,072:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8736 (1.8759)
+2022-11-18 15:08:31,150:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8410 (1.8692)
+2022-11-18 15:08:31,204:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_310.pth.tar
+2022-11-18 15:08:31,204:INFO: 
+===> EPOCH: 311 (P2)
+2022-11-18 15:08:31,204:INFO: - Computing loss (training)
+2022-11-18 15:08:31,541:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8312 (1.8312)
+2022-11-18 15:08:31,693:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9701 (1.9011)
+2022-11-18 15:08:31,846:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0150 (1.9370)
+2022-11-18 15:08:31,964:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0252 (1.9509)
+2022-11-18 15:08:32,376:INFO: Dataset: univ                Batch:  1/15	Loss 1.9594 (1.9594)
+2022-11-18 15:08:32,534:INFO: Dataset: univ                Batch:  2/15	Loss 1.9090 (1.9355)
+2022-11-18 15:08:32,691:INFO: Dataset: univ                Batch:  3/15	Loss 1.8821 (1.9179)
+2022-11-18 15:08:32,848:INFO: Dataset: univ                Batch:  4/15	Loss 1.8884 (1.9101)
+2022-11-18 15:08:33,008:INFO: Dataset: univ                Batch:  5/15	Loss 1.8986 (1.9078)
+2022-11-18 15:08:33,166:INFO: Dataset: univ                Batch:  6/15	Loss 1.9537 (1.9147)
+2022-11-18 15:08:33,323:INFO: Dataset: univ                Batch:  7/15	Loss 1.9337 (1.9174)
+2022-11-18 15:08:33,476:INFO: Dataset: univ                Batch:  8/15	Loss 1.9093 (1.9165)
+2022-11-18 15:08:33,632:INFO: Dataset: univ                Batch:  9/15	Loss 1.9016 (1.9147)
+2022-11-18 15:08:33,786:INFO: Dataset: univ                Batch: 10/15	Loss 1.9011 (1.9133)
+2022-11-18 15:08:33,944:INFO: Dataset: univ                Batch: 11/15	Loss 1.9192 (1.9139)
+2022-11-18 15:08:34,104:INFO: Dataset: univ                Batch: 12/15	Loss 1.9155 (1.9140)
+2022-11-18 15:08:34,261:INFO: Dataset: univ                Batch: 13/15	Loss 1.9247 (1.9148)
+2022-11-18 15:08:34,416:INFO: Dataset: univ                Batch: 14/15	Loss 1.9136 (1.9147)
+2022-11-18 15:08:34,499:INFO: Dataset: univ                Batch: 15/15	Loss 1.8055 (1.9135)
+2022-11-18 15:08:34,887:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9797 (1.9797)
+2022-11-18 15:08:35,032:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9373 (1.9591)
+2022-11-18 15:08:35,181:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0672 (1.9940)
+2022-11-18 15:08:35,326:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9083 (1.9737)
+2022-11-18 15:08:35,477:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9857 (1.9758)
+2022-11-18 15:08:35,625:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8856 (1.9600)
+2022-11-18 15:08:35,772:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9431 (1.9578)
+2022-11-18 15:08:35,905:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8516 (1.9473)
+2022-11-18 15:08:36,307:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9041 (1.9041)
+2022-11-18 15:08:36,457:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9553 (1.9295)
+2022-11-18 15:08:36,611:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9210 (1.9269)
+2022-11-18 15:08:36,765:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8761 (1.9145)
+2022-11-18 15:08:36,916:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8929 (1.9104)
+2022-11-18 15:08:37,070:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9584 (1.9181)
+2022-11-18 15:08:37,221:INFO: Dataset: zara2               Batch:  7/18	Loss 2.0508 (1.9341)
+2022-11-18 15:08:37,369:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8774 (1.9270)
+2022-11-18 15:08:37,520:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8287 (1.9172)
+2022-11-18 15:08:37,669:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9220 (1.9177)
+2022-11-18 15:08:37,819:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9031 (1.9165)
+2022-11-18 15:08:37,968:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9266 (1.9173)
+2022-11-18 15:08:38,119:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9160 (1.9172)
+2022-11-18 15:08:38,269:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9303 (1.9183)
+2022-11-18 15:08:38,420:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8767 (1.9156)
+2022-11-18 15:08:38,570:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8899 (1.9139)
+2022-11-18 15:08:38,721:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8569 (1.9102)
+2022-11-18 15:08:38,863:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9130 (1.9103)
+2022-11-18 15:08:38,908:INFO: - Computing loss (validation)
+2022-11-18 15:08:39,176:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9958 (1.9958)
+2022-11-18 15:08:39,211:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8643 (1.9859)
+2022-11-18 15:08:39,518:INFO: Dataset: univ                Batch: 1/3	Loss 1.9537 (1.9537)
+2022-11-18 15:08:39,599:INFO: Dataset: univ                Batch: 2/3	Loss 1.8671 (1.9076)
+2022-11-18 15:08:39,674:INFO: Dataset: univ                Batch: 3/3	Loss 1.9311 (1.9146)
+2022-11-18 15:08:39,979:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9952 (1.9952)
+2022-11-18 15:08:40,026:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8760 (1.9634)
+2022-11-18 15:08:40,333:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8178 (1.8178)
+2022-11-18 15:08:40,410:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9138 (1.8666)
+2022-11-18 15:08:40,482:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8711 (1.8681)
+2022-11-18 15:08:40,556:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9263 (1.8840)
+2022-11-18 15:08:40,628:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9531 (1.8971)
+2022-11-18 15:08:40,684:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_311.pth.tar
+2022-11-18 15:08:40,684:INFO: 
+===> EPOCH: 312 (P2)
+2022-11-18 15:08:40,685:INFO: - Computing loss (training)
+2022-11-18 15:08:41,026:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0102 (2.0102)
+2022-11-18 15:08:41,177:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9259 (1.9681)
+2022-11-18 15:08:41,328:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9406 (1.9584)
+2022-11-18 15:08:41,443:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0432 (1.9730)
+2022-11-18 15:08:41,843:INFO: Dataset: univ                Batch:  1/15	Loss 1.9214 (1.9214)
+2022-11-18 15:08:42,009:INFO: Dataset: univ                Batch:  2/15	Loss 1.8759 (1.8989)
+2022-11-18 15:08:42,173:INFO: Dataset: univ                Batch:  3/15	Loss 1.9133 (1.9039)
+2022-11-18 15:08:42,334:INFO: Dataset: univ                Batch:  4/15	Loss 1.8771 (1.8971)
+2022-11-18 15:08:42,496:INFO: Dataset: univ                Batch:  5/15	Loss 1.9441 (1.9059)
+2022-11-18 15:08:42,658:INFO: Dataset: univ                Batch:  6/15	Loss 1.9318 (1.9104)
+2022-11-18 15:08:42,818:INFO: Dataset: univ                Batch:  7/15	Loss 1.8698 (1.9045)
+2022-11-18 15:08:42,975:INFO: Dataset: univ                Batch:  8/15	Loss 1.9162 (1.9060)
+2022-11-18 15:08:43,135:INFO: Dataset: univ                Batch:  9/15	Loss 1.9069 (1.9061)
+2022-11-18 15:08:43,292:INFO: Dataset: univ                Batch: 10/15	Loss 1.9328 (1.9088)
+2022-11-18 15:08:43,452:INFO: Dataset: univ                Batch: 11/15	Loss 1.8902 (1.9071)
+2022-11-18 15:08:43,611:INFO: Dataset: univ                Batch: 12/15	Loss 1.9414 (1.9098)
+2022-11-18 15:08:43,772:INFO: Dataset: univ                Batch: 13/15	Loss 1.9678 (1.9144)
+2022-11-18 15:08:43,931:INFO: Dataset: univ                Batch: 14/15	Loss 1.8848 (1.9122)
+2022-11-18 15:08:44,015:INFO: Dataset: univ                Batch: 15/15	Loss 1.9669 (1.9129)
+2022-11-18 15:08:44,395:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8755 (1.8755)
+2022-11-18 15:08:44,547:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8823 (1.8788)
+2022-11-18 15:08:44,698:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9770 (1.9113)
+2022-11-18 15:08:44,848:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9507 (1.9217)
+2022-11-18 15:08:45,000:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9004 (1.9173)
+2022-11-18 15:08:45,151:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9711 (1.9263)
+2022-11-18 15:08:45,302:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9364 (1.9277)
+2022-11-18 15:08:45,438:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9236 (1.9272)
+2022-11-18 15:08:45,815:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9963 (1.9963)
+2022-11-18 15:08:45,966:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0086 (2.0027)
+2022-11-18 15:08:46,124:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9053 (1.9659)
+2022-11-18 15:08:46,275:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9191 (1.9543)
+2022-11-18 15:08:46,427:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9185 (1.9473)
+2022-11-18 15:08:46,579:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9533 (1.9482)
+2022-11-18 15:08:46,730:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9038 (1.9416)
+2022-11-18 15:08:46,878:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0075 (1.9505)
+2022-11-18 15:08:47,027:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9146 (1.9460)
+2022-11-18 15:08:47,177:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8993 (1.9410)
+2022-11-18 15:08:47,329:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8439 (1.9320)
+2022-11-18 15:08:47,477:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9695 (1.9355)
+2022-11-18 15:08:47,628:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9135 (1.9338)
+2022-11-18 15:08:47,778:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8639 (1.9291)
+2022-11-18 15:08:47,931:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8614 (1.9248)
+2022-11-18 15:08:48,082:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8846 (1.9223)
+2022-11-18 15:08:48,233:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0226 (1.9282)
+2022-11-18 15:08:48,371:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9756 (1.9304)
+2022-11-18 15:08:48,415:INFO: - Computing loss (validation)
+2022-11-18 15:08:48,674:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8452 (1.8452)
+2022-11-18 15:08:48,707:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0705 (1.8575)
+2022-11-18 15:08:49,018:INFO: Dataset: univ                Batch: 1/3	Loss 1.9204 (1.9204)
+2022-11-18 15:08:49,095:INFO: Dataset: univ                Batch: 2/3	Loss 1.9190 (1.9197)
+2022-11-18 15:08:49,167:INFO: Dataset: univ                Batch: 3/3	Loss 1.9134 (1.9177)
+2022-11-18 15:08:49,474:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8888 (1.8888)
+2022-11-18 15:08:49,519:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8980 (1.8911)
+2022-11-18 15:08:49,841:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9648 (1.9648)
+2022-11-18 15:08:49,921:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9529 (1.9589)
+2022-11-18 15:08:50,000:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8688 (1.9278)
+2022-11-18 15:08:50,078:INFO: Dataset: zara2               Batch: 4/5	Loss 2.0155 (1.9492)
+2022-11-18 15:08:50,156:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9161 (1.9432)
+2022-11-18 15:08:50,207:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_312.pth.tar
+2022-11-18 15:08:50,207:INFO: 
+===> EPOCH: 313 (P2)
+2022-11-18 15:08:50,208:INFO: - Computing loss (training)
+2022-11-18 15:08:50,543:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0468 (2.0468)
+2022-11-18 15:08:50,696:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0461 (2.0465)
+2022-11-18 15:08:50,847:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0001 (2.0305)
+2022-11-18 15:08:50,961:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7861 (1.9895)
+2022-11-18 15:08:51,383:INFO: Dataset: univ                Batch:  1/15	Loss 1.9683 (1.9683)
+2022-11-18 15:08:51,547:INFO: Dataset: univ                Batch:  2/15	Loss 1.8915 (1.9290)
+2022-11-18 15:08:51,717:INFO: Dataset: univ                Batch:  3/15	Loss 1.9418 (1.9335)
+2022-11-18 15:08:51,878:INFO: Dataset: univ                Batch:  4/15	Loss 1.9466 (1.9367)
+2022-11-18 15:08:52,040:INFO: Dataset: univ                Batch:  5/15	Loss 1.8549 (1.9209)
+2022-11-18 15:08:52,205:INFO: Dataset: univ                Batch:  6/15	Loss 1.9453 (1.9251)
+2022-11-18 15:08:52,380:INFO: Dataset: univ                Batch:  7/15	Loss 1.9451 (1.9278)
+2022-11-18 15:08:52,563:INFO: Dataset: univ                Batch:  8/15	Loss 1.9145 (1.9260)
+2022-11-18 15:08:52,727:INFO: Dataset: univ                Batch:  9/15	Loss 1.9948 (1.9325)
+2022-11-18 15:08:52,889:INFO: Dataset: univ                Batch: 10/15	Loss 1.8960 (1.9287)
+2022-11-18 15:08:53,056:INFO: Dataset: univ                Batch: 11/15	Loss 1.9041 (1.9263)
+2022-11-18 15:08:53,218:INFO: Dataset: univ                Batch: 12/15	Loss 1.9211 (1.9260)
+2022-11-18 15:08:53,382:INFO: Dataset: univ                Batch: 13/15	Loss 1.9334 (1.9265)
+2022-11-18 15:08:53,555:INFO: Dataset: univ                Batch: 14/15	Loss 1.9125 (1.9255)
+2022-11-18 15:08:53,654:INFO: Dataset: univ                Batch: 15/15	Loss 1.9313 (1.9256)
+2022-11-18 15:08:54,081:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9040 (1.9040)
+2022-11-18 15:08:54,241:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0267 (1.9628)
+2022-11-18 15:08:54,403:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9558 (1.9604)
+2022-11-18 15:08:54,561:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9013 (1.9477)
+2022-11-18 15:08:54,721:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9788 (1.9538)
+2022-11-18 15:08:54,881:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8577 (1.9375)
+2022-11-18 15:08:55,040:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0090 (1.9474)
+2022-11-18 15:08:55,186:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9298 (1.9452)
+2022-11-18 15:08:55,584:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9148 (1.9148)
+2022-11-18 15:08:55,740:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9117 (1.9133)
+2022-11-18 15:08:55,894:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0063 (1.9444)
+2022-11-18 15:08:56,049:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9438 (1.9442)
+2022-11-18 15:08:56,205:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9097 (1.9373)
+2022-11-18 15:08:56,362:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9518 (1.9395)
+2022-11-18 15:08:56,517:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9332 (1.9387)
+2022-11-18 15:08:56,670:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8415 (1.9262)
+2022-11-18 15:08:56,825:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8614 (1.9188)
+2022-11-18 15:08:56,976:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9226 (1.9192)
+2022-11-18 15:08:57,136:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9354 (1.9207)
+2022-11-18 15:08:57,289:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9151 (1.9203)
+2022-11-18 15:08:57,443:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9085 (1.9193)
+2022-11-18 15:08:57,598:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0264 (1.9266)
+2022-11-18 15:08:57,755:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0926 (1.9373)
+2022-11-18 15:08:57,907:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9726 (1.9397)
+2022-11-18 15:08:58,063:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9315 (1.9392)
+2022-11-18 15:08:58,206:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8896 (1.9367)
+2022-11-18 15:08:58,250:INFO: - Computing loss (validation)
+2022-11-18 15:08:58,520:INFO: Dataset: hotel               Batch: 1/2	Loss 2.1234 (2.1234)
+2022-11-18 15:08:58,554:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0514 (2.1175)
+2022-11-18 15:08:58,865:INFO: Dataset: univ                Batch: 1/3	Loss 1.8808 (1.8808)
+2022-11-18 15:08:58,942:INFO: Dataset: univ                Batch: 2/3	Loss 1.8916 (1.8863)
+2022-11-18 15:08:59,013:INFO: Dataset: univ                Batch: 3/3	Loss 1.9067 (1.8925)
+2022-11-18 15:08:59,315:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9114 (1.9114)
+2022-11-18 15:08:59,359:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9931 (1.9316)
+2022-11-18 15:08:59,663:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8829 (1.8829)
+2022-11-18 15:08:59,739:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9124 (1.8975)
+2022-11-18 15:08:59,815:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9130 (1.9027)
+2022-11-18 15:08:59,890:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9303 (1.9093)
+2022-11-18 15:08:59,966:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9430 (1.9164)
+2022-11-18 15:09:00,020:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_313.pth.tar
+2022-11-18 15:09:00,020:INFO: 
+===> EPOCH: 314 (P2)
+2022-11-18 15:09:00,021:INFO: - Computing loss (training)
+2022-11-18 15:09:00,376:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8753 (1.8753)
+2022-11-18 15:09:00,528:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9235 (1.8995)
+2022-11-18 15:09:00,679:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9459 (1.9146)
+2022-11-18 15:09:00,795:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8611 (1.9047)
+2022-11-18 15:09:01,207:INFO: Dataset: univ                Batch:  1/15	Loss 1.9340 (1.9340)
+2022-11-18 15:09:01,368:INFO: Dataset: univ                Batch:  2/15	Loss 1.9054 (1.9202)
+2022-11-18 15:09:01,529:INFO: Dataset: univ                Batch:  3/15	Loss 1.9250 (1.9218)
+2022-11-18 15:09:01,686:INFO: Dataset: univ                Batch:  4/15	Loss 1.8945 (1.9152)
+2022-11-18 15:09:01,850:INFO: Dataset: univ                Batch:  5/15	Loss 1.9630 (1.9241)
+2022-11-18 15:09:02,008:INFO: Dataset: univ                Batch:  6/15	Loss 1.8539 (1.9138)
+2022-11-18 15:09:02,168:INFO: Dataset: univ                Batch:  7/15	Loss 1.8975 (1.9115)
+2022-11-18 15:09:02,328:INFO: Dataset: univ                Batch:  8/15	Loss 1.9042 (1.9106)
+2022-11-18 15:09:02,485:INFO: Dataset: univ                Batch:  9/15	Loss 1.8664 (1.9055)
+2022-11-18 15:09:02,643:INFO: Dataset: univ                Batch: 10/15	Loss 1.9552 (1.9110)
+2022-11-18 15:09:02,804:INFO: Dataset: univ                Batch: 11/15	Loss 1.9030 (1.9101)
+2022-11-18 15:09:02,960:INFO: Dataset: univ                Batch: 12/15	Loss 1.9276 (1.9118)
+2022-11-18 15:09:03,118:INFO: Dataset: univ                Batch: 13/15	Loss 1.9262 (1.9129)
+2022-11-18 15:09:03,276:INFO: Dataset: univ                Batch: 14/15	Loss 1.9186 (1.9133)
+2022-11-18 15:09:03,358:INFO: Dataset: univ                Batch: 15/15	Loss 1.9246 (1.9135)
+2022-11-18 15:09:03,748:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8218 (1.8218)
+2022-11-18 15:09:03,898:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8222 (1.8220)
+2022-11-18 15:09:04,056:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9536 (1.8648)
+2022-11-18 15:09:04,210:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9185 (1.8776)
+2022-11-18 15:09:04,362:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9235 (1.8868)
+2022-11-18 15:09:04,513:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9594 (1.8986)
+2022-11-18 15:09:04,664:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8551 (1.8923)
+2022-11-18 15:09:04,802:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7553 (1.8768)
+2022-11-18 15:09:05,192:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9407 (1.9407)
+2022-11-18 15:09:05,346:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9223 (1.9319)
+2022-11-18 15:09:05,512:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9695 (1.9435)
+2022-11-18 15:09:05,662:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9866 (1.9550)
+2022-11-18 15:09:05,815:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7744 (1.9188)
+2022-11-18 15:09:05,966:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8406 (1.9054)
+2022-11-18 15:09:06,118:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9248 (1.9080)
+2022-11-18 15:09:06,266:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9209 (1.9095)
+2022-11-18 15:09:06,416:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0030 (1.9197)
+2022-11-18 15:09:06,563:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9351 (1.9213)
+2022-11-18 15:09:06,714:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9319 (1.9223)
+2022-11-18 15:09:06,864:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8099 (1.9123)
+2022-11-18 15:09:07,016:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9104 (1.9122)
+2022-11-18 15:09:07,169:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9400 (1.9141)
+2022-11-18 15:09:07,322:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9134 (1.9140)
+2022-11-18 15:09:07,474:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9433 (1.9159)
+2022-11-18 15:09:07,629:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9370 (1.9172)
+2022-11-18 15:09:07,769:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8899 (1.9160)
+2022-11-18 15:09:07,815:INFO: - Computing loss (validation)
+2022-11-18 15:09:08,069:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0432 (2.0432)
+2022-11-18 15:09:08,103:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8752 (2.0289)
+2022-11-18 15:09:08,421:INFO: Dataset: univ                Batch: 1/3	Loss 1.8659 (1.8659)
+2022-11-18 15:09:08,502:INFO: Dataset: univ                Batch: 2/3	Loss 1.9209 (1.8942)
+2022-11-18 15:09:08,577:INFO: Dataset: univ                Batch: 3/3	Loss 1.8640 (1.8849)
+2022-11-18 15:09:08,904:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9239 (1.9239)
+2022-11-18 15:09:08,952:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8536 (1.9081)
+2022-11-18 15:09:09,254:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9085 (1.9085)
+2022-11-18 15:09:09,330:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8700 (1.8889)
+2022-11-18 15:09:09,404:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9142 (1.8979)
+2022-11-18 15:09:09,478:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8904 (1.8961)
+2022-11-18 15:09:09,551:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8774 (1.8925)
+2022-11-18 15:09:09,603:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_314.pth.tar
+2022-11-18 15:09:09,603:INFO: 
+===> EPOCH: 315 (P2)
+2022-11-18 15:09:09,603:INFO: - Computing loss (training)
+2022-11-18 15:09:09,947:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8430 (1.8430)
+2022-11-18 15:09:10,099:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9090 (1.8768)
+2022-11-18 15:09:10,250:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9192 (1.8910)
+2022-11-18 15:09:10,365:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9858 (1.9060)
+2022-11-18 15:09:10,786:INFO: Dataset: univ                Batch:  1/15	Loss 1.9612 (1.9612)
+2022-11-18 15:09:10,949:INFO: Dataset: univ                Batch:  2/15	Loss 1.9130 (1.9365)
+2022-11-18 15:09:11,115:INFO: Dataset: univ                Batch:  3/15	Loss 1.9301 (1.9345)
+2022-11-18 15:09:11,281:INFO: Dataset: univ                Batch:  4/15	Loss 1.9430 (1.9367)
+2022-11-18 15:09:11,445:INFO: Dataset: univ                Batch:  5/15	Loss 1.9406 (1.9374)
+2022-11-18 15:09:11,609:INFO: Dataset: univ                Batch:  6/15	Loss 1.9313 (1.9364)
+2022-11-18 15:09:11,771:INFO: Dataset: univ                Batch:  7/15	Loss 1.9368 (1.9364)
+2022-11-18 15:09:11,931:INFO: Dataset: univ                Batch:  8/15	Loss 1.8649 (1.9281)
+2022-11-18 15:09:12,096:INFO: Dataset: univ                Batch:  9/15	Loss 1.9520 (1.9305)
+2022-11-18 15:09:12,259:INFO: Dataset: univ                Batch: 10/15	Loss 1.9178 (1.9290)
+2022-11-18 15:09:12,420:INFO: Dataset: univ                Batch: 11/15	Loss 1.9399 (1.9300)
+2022-11-18 15:09:12,584:INFO: Dataset: univ                Batch: 12/15	Loss 1.9472 (1.9315)
+2022-11-18 15:09:12,747:INFO: Dataset: univ                Batch: 13/15	Loss 1.9480 (1.9328)
+2022-11-18 15:09:12,908:INFO: Dataset: univ                Batch: 14/15	Loss 1.8925 (1.9298)
+2022-11-18 15:09:12,995:INFO: Dataset: univ                Batch: 15/15	Loss 1.8644 (1.9288)
+2022-11-18 15:09:13,397:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8416 (1.8416)
+2022-11-18 15:09:13,548:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8495 (1.8461)
+2022-11-18 15:09:13,700:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0727 (1.9244)
+2022-11-18 15:09:13,852:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8862 (1.9159)
+2022-11-18 15:09:14,005:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0458 (1.9414)
+2022-11-18 15:09:14,156:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0003 (1.9510)
+2022-11-18 15:09:14,307:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8611 (1.9384)
+2022-11-18 15:09:14,444:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0210 (1.9474)
+2022-11-18 15:09:14,877:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9759 (1.9759)
+2022-11-18 15:09:15,028:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9054 (1.9426)
+2022-11-18 15:09:15,181:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9198 (1.9350)
+2022-11-18 15:09:15,331:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9443 (1.9374)
+2022-11-18 15:09:15,482:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8684 (1.9250)
+2022-11-18 15:09:15,637:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9548 (1.9301)
+2022-11-18 15:09:15,791:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9204 (1.9288)
+2022-11-18 15:09:15,943:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9820 (1.9351)
+2022-11-18 15:09:16,098:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9641 (1.9380)
+2022-11-18 15:09:16,251:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9000 (1.9345)
+2022-11-18 15:09:16,405:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0114 (1.9414)
+2022-11-18 15:09:16,557:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9316 (1.9406)
+2022-11-18 15:09:16,711:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9094 (1.9380)
+2022-11-18 15:09:16,864:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0005 (1.9420)
+2022-11-18 15:09:17,019:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9426 (1.9421)
+2022-11-18 15:09:17,174:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8354 (1.9361)
+2022-11-18 15:09:17,329:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8434 (1.9303)
+2022-11-18 15:09:17,469:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8507 (1.9271)
+2022-11-18 15:09:17,516:INFO: - Computing loss (validation)
+2022-11-18 15:09:17,793:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9870 (1.9870)
+2022-11-18 15:09:17,829:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1061 (1.9972)
+2022-11-18 15:09:18,149:INFO: Dataset: univ                Batch: 1/3	Loss 1.9106 (1.9106)
+2022-11-18 15:09:18,232:INFO: Dataset: univ                Batch: 2/3	Loss 1.8575 (1.8840)
+2022-11-18 15:09:18,310:INFO: Dataset: univ                Batch: 3/3	Loss 1.9169 (1.8951)
+2022-11-18 15:09:18,613:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9961 (1.9961)
+2022-11-18 15:09:18,658:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7936 (1.9434)
+2022-11-18 15:09:18,958:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8826 (1.8826)
+2022-11-18 15:09:19,034:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9905 (1.9375)
+2022-11-18 15:09:19,111:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9361 (1.9370)
+2022-11-18 15:09:19,187:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9568 (1.9421)
+2022-11-18 15:09:19,263:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9301 (1.9398)
+2022-11-18 15:09:19,315:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_315.pth.tar
+2022-11-18 15:09:19,315:INFO: 
+===> EPOCH: 316 (P2)
+2022-11-18 15:09:19,316:INFO: - Computing loss (training)
+2022-11-18 15:09:19,650:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0426 (2.0426)
+2022-11-18 15:09:19,800:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0397 (2.0412)
+2022-11-18 15:09:19,950:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9163 (1.9993)
+2022-11-18 15:09:20,066:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9367 (1.9889)
+2022-11-18 15:09:20,494:INFO: Dataset: univ                Batch:  1/15	Loss 1.9063 (1.9063)
+2022-11-18 15:09:20,658:INFO: Dataset: univ                Batch:  2/15	Loss 1.9311 (1.9186)
+2022-11-18 15:09:20,817:INFO: Dataset: univ                Batch:  3/15	Loss 1.9231 (1.9201)
+2022-11-18 15:09:20,973:INFO: Dataset: univ                Batch:  4/15	Loss 1.9069 (1.9170)
+2022-11-18 15:09:21,132:INFO: Dataset: univ                Batch:  5/15	Loss 1.9234 (1.9183)
+2022-11-18 15:09:21,294:INFO: Dataset: univ                Batch:  6/15	Loss 1.9409 (1.9221)
+2022-11-18 15:09:21,452:INFO: Dataset: univ                Batch:  7/15	Loss 1.9592 (1.9269)
+2022-11-18 15:09:21,605:INFO: Dataset: univ                Batch:  8/15	Loss 1.9104 (1.9249)
+2022-11-18 15:09:21,762:INFO: Dataset: univ                Batch:  9/15	Loss 1.8985 (1.9221)
+2022-11-18 15:09:21,917:INFO: Dataset: univ                Batch: 10/15	Loss 1.8971 (1.9197)
+2022-11-18 15:09:22,073:INFO: Dataset: univ                Batch: 11/15	Loss 1.9087 (1.9187)
+2022-11-18 15:09:22,230:INFO: Dataset: univ                Batch: 12/15	Loss 1.9015 (1.9173)
+2022-11-18 15:09:22,385:INFO: Dataset: univ                Batch: 13/15	Loss 1.9424 (1.9192)
+2022-11-18 15:09:22,541:INFO: Dataset: univ                Batch: 14/15	Loss 1.9389 (1.9206)
+2022-11-18 15:09:22,625:INFO: Dataset: univ                Batch: 15/15	Loss 1.8362 (1.9196)
+2022-11-18 15:09:23,017:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8262 (1.8262)
+2022-11-18 15:09:23,170:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9956 (1.9028)
+2022-11-18 15:09:23,320:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9828 (1.9283)
+2022-11-18 15:09:23,470:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9240 (1.9272)
+2022-11-18 15:09:23,623:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9099 (1.9240)
+2022-11-18 15:09:23,772:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9379 (1.9264)
+2022-11-18 15:09:23,922:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7580 (1.9027)
+2022-11-18 15:09:24,059:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9180 (1.9045)
+2022-11-18 15:09:24,453:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9728 (1.9728)
+2022-11-18 15:09:24,604:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7925 (1.8852)
+2022-11-18 15:09:24,759:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9914 (1.9191)
+2022-11-18 15:09:24,913:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9213 (1.9196)
+2022-11-18 15:09:25,065:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9404 (1.9240)
+2022-11-18 15:09:25,216:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9983 (1.9375)
+2022-11-18 15:09:25,366:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8474 (1.9243)
+2022-11-18 15:09:25,515:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9574 (1.9284)
+2022-11-18 15:09:25,666:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9983 (1.9367)
+2022-11-18 15:09:25,814:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9710 (1.9400)
+2022-11-18 15:09:25,965:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9797 (1.9439)
+2022-11-18 15:09:26,114:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9323 (1.9429)
+2022-11-18 15:09:26,264:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8652 (1.9370)
+2022-11-18 15:09:26,413:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9164 (1.9355)
+2022-11-18 15:09:26,565:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9839 (1.9386)
+2022-11-18 15:09:26,796:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9019 (1.9363)
+2022-11-18 15:09:26,946:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9831 (1.9390)
+2022-11-18 15:09:27,087:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8238 (1.9338)
+2022-11-18 15:09:27,134:INFO: - Computing loss (validation)
+2022-11-18 15:09:27,391:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0674 (2.0674)
+2022-11-18 15:09:27,426:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8994 (2.0565)
+2022-11-18 15:09:27,742:INFO: Dataset: univ                Batch: 1/3	Loss 1.8996 (1.8996)
+2022-11-18 15:09:27,819:INFO: Dataset: univ                Batch: 2/3	Loss 1.8535 (1.8810)
+2022-11-18 15:09:27,893:INFO: Dataset: univ                Batch: 3/3	Loss 1.9064 (1.8885)
+2022-11-18 15:09:28,198:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8922 (1.8922)
+2022-11-18 15:09:28,243:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0754 (1.9340)
+2022-11-18 15:09:28,559:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9429 (1.9429)
+2022-11-18 15:09:28,637:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8898 (1.9175)
+2022-11-18 15:09:28,716:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9244 (1.9199)
+2022-11-18 15:09:28,794:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8221 (1.8958)
+2022-11-18 15:09:28,873:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8898 (1.8946)
+2022-11-18 15:09:28,925:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_316.pth.tar
+2022-11-18 15:09:28,925:INFO: 
+===> EPOCH: 317 (P2)
+2022-11-18 15:09:28,926:INFO: - Computing loss (training)
+2022-11-18 15:09:29,266:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0378 (2.0378)
+2022-11-18 15:09:29,416:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9654 (2.0017)
+2022-11-18 15:09:29,567:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9397 (1.9810)
+2022-11-18 15:09:29,684:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9591 (1.9775)
+2022-11-18 15:09:30,102:INFO: Dataset: univ                Batch:  1/15	Loss 1.8560 (1.8560)
+2022-11-18 15:09:30,262:INFO: Dataset: univ                Batch:  2/15	Loss 1.8876 (1.8709)
+2022-11-18 15:09:30,421:INFO: Dataset: univ                Batch:  3/15	Loss 1.9344 (1.8909)
+2022-11-18 15:09:30,576:INFO: Dataset: univ                Batch:  4/15	Loss 1.9258 (1.8989)
+2022-11-18 15:09:30,733:INFO: Dataset: univ                Batch:  5/15	Loss 1.8953 (1.8981)
+2022-11-18 15:09:30,891:INFO: Dataset: univ                Batch:  6/15	Loss 1.8882 (1.8964)
+2022-11-18 15:09:31,046:INFO: Dataset: univ                Batch:  7/15	Loss 1.9365 (1.9017)
+2022-11-18 15:09:31,202:INFO: Dataset: univ                Batch:  8/15	Loss 1.9218 (1.9044)
+2022-11-18 15:09:31,362:INFO: Dataset: univ                Batch:  9/15	Loss 1.9036 (1.9043)
+2022-11-18 15:09:31,517:INFO: Dataset: univ                Batch: 10/15	Loss 1.9407 (1.9082)
+2022-11-18 15:09:31,674:INFO: Dataset: univ                Batch: 11/15	Loss 1.8956 (1.9071)
+2022-11-18 15:09:31,830:INFO: Dataset: univ                Batch: 12/15	Loss 1.9168 (1.9079)
+2022-11-18 15:09:31,986:INFO: Dataset: univ                Batch: 13/15	Loss 1.8649 (1.9045)
+2022-11-18 15:09:32,143:INFO: Dataset: univ                Batch: 14/15	Loss 1.9152 (1.9051)
+2022-11-18 15:09:32,227:INFO: Dataset: univ                Batch: 15/15	Loss 1.8588 (1.9047)
+2022-11-18 15:09:32,611:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9211 (1.9211)
+2022-11-18 15:09:32,770:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9213 (1.9212)
+2022-11-18 15:09:32,931:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8877 (1.9109)
+2022-11-18 15:09:33,090:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0859 (1.9556)
+2022-11-18 15:09:33,248:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8863 (1.9437)
+2022-11-18 15:09:33,405:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8805 (1.9328)
+2022-11-18 15:09:33,563:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9000 (1.9288)
+2022-11-18 15:09:33,708:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9871 (1.9359)
+2022-11-18 15:09:34,109:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8946 (1.8946)
+2022-11-18 15:09:34,269:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9512 (1.9220)
+2022-11-18 15:09:34,428:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8887 (1.9103)
+2022-11-18 15:09:34,587:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9439 (1.9189)
+2022-11-18 15:09:34,748:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8835 (1.9115)
+2022-11-18 15:09:34,905:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9887 (1.9258)
+2022-11-18 15:09:35,062:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9259 (1.9258)
+2022-11-18 15:09:35,217:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8926 (1.9215)
+2022-11-18 15:09:35,373:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9469 (1.9246)
+2022-11-18 15:09:35,527:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9463 (1.9265)
+2022-11-18 15:09:35,684:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9754 (1.9309)
+2022-11-18 15:09:35,841:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9321 (1.9310)
+2022-11-18 15:09:35,997:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9029 (1.9288)
+2022-11-18 15:09:36,154:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8477 (1.9231)
+2022-11-18 15:09:36,313:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9791 (1.9267)
+2022-11-18 15:09:36,469:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8324 (1.9208)
+2022-11-18 15:09:36,627:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9464 (1.9224)
+2022-11-18 15:09:36,771:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9962 (1.9257)
+2022-11-18 15:09:36,816:INFO: - Computing loss (validation)
+2022-11-18 15:09:37,083:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8938 (1.8938)
+2022-11-18 15:09:37,118:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0474 (1.9027)
+2022-11-18 15:09:37,418:INFO: Dataset: univ                Batch: 1/3	Loss 1.8365 (1.8365)
+2022-11-18 15:09:37,495:INFO: Dataset: univ                Batch: 2/3	Loss 1.8658 (1.8500)
+2022-11-18 15:09:37,568:INFO: Dataset: univ                Batch: 3/3	Loss 1.9337 (1.8765)
+2022-11-18 15:09:37,880:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9546 (1.9546)
+2022-11-18 15:09:37,925:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8504 (1.9302)
+2022-11-18 15:09:38,241:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9418 (1.9418)
+2022-11-18 15:09:38,315:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9502 (1.9459)
+2022-11-18 15:09:38,389:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9479 (1.9466)
+2022-11-18 15:09:38,461:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9319 (1.9429)
+2022-11-18 15:09:38,535:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9752 (1.9492)
+2022-11-18 15:09:38,586:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_317.pth.tar
+2022-11-18 15:09:38,586:INFO: 
+===> EPOCH: 318 (P2)
+2022-11-18 15:09:38,587:INFO: - Computing loss (training)
+2022-11-18 15:09:38,926:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9321 (1.9321)
+2022-11-18 15:09:39,077:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9571 (1.9441)
+2022-11-18 15:09:39,229:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8872 (1.9242)
+2022-11-18 15:09:39,343:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9586 (1.9295)
+2022-11-18 15:09:39,748:INFO: Dataset: univ                Batch:  1/15	Loss 1.8796 (1.8796)
+2022-11-18 15:09:39,908:INFO: Dataset: univ                Batch:  2/15	Loss 1.9339 (1.9077)
+2022-11-18 15:09:40,070:INFO: Dataset: univ                Batch:  3/15	Loss 1.8916 (1.9024)
+2022-11-18 15:09:40,234:INFO: Dataset: univ                Batch:  4/15	Loss 1.9310 (1.9095)
+2022-11-18 15:09:40,394:INFO: Dataset: univ                Batch:  5/15	Loss 1.8852 (1.9045)
+2022-11-18 15:09:40,562:INFO: Dataset: univ                Batch:  6/15	Loss 1.9121 (1.9058)
+2022-11-18 15:09:40,721:INFO: Dataset: univ                Batch:  7/15	Loss 1.9229 (1.9083)
+2022-11-18 15:09:40,878:INFO: Dataset: univ                Batch:  8/15	Loss 1.8865 (1.9052)
+2022-11-18 15:09:41,038:INFO: Dataset: univ                Batch:  9/15	Loss 1.9098 (1.9057)
+2022-11-18 15:09:41,197:INFO: Dataset: univ                Batch: 10/15	Loss 1.9063 (1.9058)
+2022-11-18 15:09:41,360:INFO: Dataset: univ                Batch: 11/15	Loss 1.9007 (1.9054)
+2022-11-18 15:09:41,516:INFO: Dataset: univ                Batch: 12/15	Loss 1.8822 (1.9035)
+2022-11-18 15:09:41,677:INFO: Dataset: univ                Batch: 13/15	Loss 1.9328 (1.9058)
+2022-11-18 15:09:41,836:INFO: Dataset: univ                Batch: 14/15	Loss 1.8896 (1.9046)
+2022-11-18 15:09:41,920:INFO: Dataset: univ                Batch: 15/15	Loss 1.8968 (1.9045)
+2022-11-18 15:09:42,309:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0405 (2.0405)
+2022-11-18 15:09:42,458:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9693 (2.0048)
+2022-11-18 15:09:42,608:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9811 (1.9970)
+2022-11-18 15:09:42,761:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8916 (1.9705)
+2022-11-18 15:09:42,913:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9012 (1.9558)
+2022-11-18 15:09:43,063:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8962 (1.9462)
+2022-11-18 15:09:43,213:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8251 (1.9289)
+2022-11-18 15:09:43,348:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8340 (1.9194)
+2022-11-18 15:09:43,745:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9102 (1.9102)
+2022-11-18 15:09:43,897:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8698 (1.8919)
+2022-11-18 15:09:44,048:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8707 (1.8847)
+2022-11-18 15:09:44,200:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8958 (1.8876)
+2022-11-18 15:09:44,356:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8331 (1.8768)
+2022-11-18 15:09:44,508:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8784 (1.8771)
+2022-11-18 15:09:44,658:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9996 (1.8957)
+2022-11-18 15:09:44,806:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8745 (1.8930)
+2022-11-18 15:09:44,956:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8690 (1.8901)
+2022-11-18 15:09:45,105:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9235 (1.8932)
+2022-11-18 15:09:45,254:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9492 (1.8980)
+2022-11-18 15:09:45,404:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0036 (1.9066)
+2022-11-18 15:09:45,554:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9886 (1.9131)
+2022-11-18 15:09:45,704:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8479 (1.9082)
+2022-11-18 15:09:45,856:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0021 (1.9149)
+2022-11-18 15:09:46,006:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9341 (1.9161)
+2022-11-18 15:09:46,158:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9619 (1.9188)
+2022-11-18 15:09:46,297:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9797 (1.9220)
+2022-11-18 15:09:46,344:INFO: - Computing loss (validation)
+2022-11-18 15:09:46,608:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9420 (1.9420)
+2022-11-18 15:09:46,644:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0640 (1.9516)
+2022-11-18 15:09:46,948:INFO: Dataset: univ                Batch: 1/3	Loss 1.9088 (1.9088)
+2022-11-18 15:09:47,024:INFO: Dataset: univ                Batch: 2/3	Loss 1.9316 (1.9200)
+2022-11-18 15:09:47,096:INFO: Dataset: univ                Batch: 3/3	Loss 1.9325 (1.9237)
+2022-11-18 15:09:47,391:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8461 (1.8461)
+2022-11-18 15:09:47,437:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9717 (1.8780)
+2022-11-18 15:09:47,735:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8800 (1.8800)
+2022-11-18 15:09:47,815:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8627 (1.8709)
+2022-11-18 15:09:47,890:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8986 (1.8801)
+2022-11-18 15:09:47,965:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8342 (1.8694)
+2022-11-18 15:09:48,039:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8979 (1.8747)
+2022-11-18 15:09:48,090:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_318.pth.tar
+2022-11-18 15:09:48,091:INFO: 
+===> EPOCH: 319 (P2)
+2022-11-18 15:09:48,091:INFO: - Computing loss (training)
+2022-11-18 15:09:48,433:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0672 (2.0672)
+2022-11-18 15:09:48,586:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9723 (2.0207)
+2022-11-18 15:09:48,737:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9345 (1.9924)
+2022-11-18 15:09:48,857:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9496 (1.9854)
+2022-11-18 15:09:49,271:INFO: Dataset: univ                Batch:  1/15	Loss 1.8710 (1.8710)
+2022-11-18 15:09:49,436:INFO: Dataset: univ                Batch:  2/15	Loss 1.8743 (1.8727)
+2022-11-18 15:09:49,604:INFO: Dataset: univ                Batch:  3/15	Loss 1.8711 (1.8721)
+2022-11-18 15:09:49,765:INFO: Dataset: univ                Batch:  4/15	Loss 1.9044 (1.8800)
+2022-11-18 15:09:49,931:INFO: Dataset: univ                Batch:  5/15	Loss 1.8940 (1.8829)
+2022-11-18 15:09:50,094:INFO: Dataset: univ                Batch:  6/15	Loss 1.8853 (1.8833)
+2022-11-18 15:09:50,257:INFO: Dataset: univ                Batch:  7/15	Loss 1.8472 (1.8777)
+2022-11-18 15:09:50,418:INFO: Dataset: univ                Batch:  8/15	Loss 1.8272 (1.8713)
+2022-11-18 15:09:50,580:INFO: Dataset: univ                Batch:  9/15	Loss 1.9424 (1.8788)
+2022-11-18 15:09:50,740:INFO: Dataset: univ                Batch: 10/15	Loss 1.8677 (1.8778)
+2022-11-18 15:09:50,903:INFO: Dataset: univ                Batch: 11/15	Loss 1.9288 (1.8823)
+2022-11-18 15:09:51,066:INFO: Dataset: univ                Batch: 12/15	Loss 1.8465 (1.8794)
+2022-11-18 15:09:51,229:INFO: Dataset: univ                Batch: 13/15	Loss 1.9276 (1.8832)
+2022-11-18 15:09:51,392:INFO: Dataset: univ                Batch: 14/15	Loss 1.8927 (1.8839)
+2022-11-18 15:09:51,478:INFO: Dataset: univ                Batch: 15/15	Loss 1.8348 (1.8831)
+2022-11-18 15:09:51,864:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9270 (1.9270)
+2022-11-18 15:09:52,016:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9615 (1.9438)
+2022-11-18 15:09:52,169:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8475 (1.9126)
+2022-11-18 15:09:52,317:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8351 (1.8930)
+2022-11-18 15:09:52,468:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0034 (1.9147)
+2022-11-18 15:09:52,618:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8852 (1.9095)
+2022-11-18 15:09:52,769:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9700 (1.9179)
+2022-11-18 15:09:52,905:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9016 (1.9159)
+2022-11-18 15:09:53,312:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9898 (1.9898)
+2022-11-18 15:09:53,464:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9317 (1.9602)
+2022-11-18 15:09:53,617:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9843 (1.9677)
+2022-11-18 15:09:53,768:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9760 (1.9698)
+2022-11-18 15:09:53,925:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8877 (1.9553)
+2022-11-18 15:09:54,078:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9417 (1.9532)
+2022-11-18 15:09:54,230:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8675 (1.9406)
+2022-11-18 15:09:54,379:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7959 (1.9222)
+2022-11-18 15:09:54,531:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8691 (1.9168)
+2022-11-18 15:09:54,681:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8503 (1.9099)
+2022-11-18 15:09:54,835:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9330 (1.9120)
+2022-11-18 15:09:54,985:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0291 (1.9218)
+2022-11-18 15:09:55,136:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8969 (1.9198)
+2022-11-18 15:09:55,288:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8624 (1.9154)
+2022-11-18 15:09:55,441:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9753 (1.9193)
+2022-11-18 15:09:55,592:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8955 (1.9179)
+2022-11-18 15:09:55,745:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9372 (1.9189)
+2022-11-18 15:09:55,885:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9755 (1.9218)
+2022-11-18 15:09:55,931:INFO: - Computing loss (validation)
+2022-11-18 15:09:56,194:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9687 (1.9687)
+2022-11-18 15:09:56,228:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8043 (1.9580)
+2022-11-18 15:09:56,538:INFO: Dataset: univ                Batch: 1/3	Loss 1.9404 (1.9404)
+2022-11-18 15:09:56,619:INFO: Dataset: univ                Batch: 2/3	Loss 1.9162 (1.9273)
+2022-11-18 15:09:56,695:INFO: Dataset: univ                Batch: 3/3	Loss 1.9319 (1.9289)
+2022-11-18 15:09:56,999:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8833 (1.8833)
+2022-11-18 15:09:57,044:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8499 (1.8759)
+2022-11-18 15:09:57,347:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8130 (1.8130)
+2022-11-18 15:09:57,421:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9186 (1.8642)
+2022-11-18 15:09:57,494:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8239 (1.8506)
+2022-11-18 15:09:57,568:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8880 (1.8602)
+2022-11-18 15:09:57,641:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8604 (1.8602)
+2022-11-18 15:09:57,692:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_319.pth.tar
+2022-11-18 15:09:57,692:INFO: 
+===> EPOCH: 320 (P2)
+2022-11-18 15:09:57,693:INFO: - Computing loss (training)
+2022-11-18 15:09:58,028:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0332 (2.0332)
+2022-11-18 15:09:58,185:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9129 (1.9746)
+2022-11-18 15:09:58,336:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9955 (1.9812)
+2022-11-18 15:09:58,452:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0512 (1.9932)
+2022-11-18 15:09:58,860:INFO: Dataset: univ                Batch:  1/15	Loss 1.9496 (1.9496)
+2022-11-18 15:09:59,019:INFO: Dataset: univ                Batch:  2/15	Loss 1.9193 (1.9348)
+2022-11-18 15:09:59,177:INFO: Dataset: univ                Batch:  3/15	Loss 1.9465 (1.9386)
+2022-11-18 15:09:59,333:INFO: Dataset: univ                Batch:  4/15	Loss 1.9551 (1.9427)
+2022-11-18 15:09:59,494:INFO: Dataset: univ                Batch:  5/15	Loss 1.9006 (1.9349)
+2022-11-18 15:09:59,651:INFO: Dataset: univ                Batch:  6/15	Loss 1.8749 (1.9245)
+2022-11-18 15:09:59,807:INFO: Dataset: univ                Batch:  7/15	Loss 1.9001 (1.9207)
+2022-11-18 15:09:59,963:INFO: Dataset: univ                Batch:  8/15	Loss 1.9740 (1.9278)
+2022-11-18 15:10:00,120:INFO: Dataset: univ                Batch:  9/15	Loss 1.9249 (1.9274)
+2022-11-18 15:10:00,275:INFO: Dataset: univ                Batch: 10/15	Loss 1.9329 (1.9280)
+2022-11-18 15:10:00,437:INFO: Dataset: univ                Batch: 11/15	Loss 1.9094 (1.9262)
+2022-11-18 15:10:00,597:INFO: Dataset: univ                Batch: 12/15	Loss 1.9335 (1.9268)
+2022-11-18 15:10:00,753:INFO: Dataset: univ                Batch: 13/15	Loss 1.9326 (1.9273)
+2022-11-18 15:10:00,910:INFO: Dataset: univ                Batch: 14/15	Loss 1.9060 (1.9257)
+2022-11-18 15:10:00,993:INFO: Dataset: univ                Batch: 15/15	Loss 1.8799 (1.9250)
+2022-11-18 15:10:01,395:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9928 (1.9928)
+2022-11-18 15:10:01,555:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9332 (1.9611)
+2022-11-18 15:10:01,714:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8822 (1.9342)
+2022-11-18 15:10:01,872:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9328 (1.9339)
+2022-11-18 15:10:02,035:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0098 (1.9506)
+2022-11-18 15:10:02,193:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0553 (1.9668)
+2022-11-18 15:10:02,351:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9227 (1.9603)
+2022-11-18 15:10:02,496:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9917 (1.9640)
+2022-11-18 15:10:02,897:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8915 (1.8915)
+2022-11-18 15:10:03,058:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9624 (1.9266)
+2022-11-18 15:10:03,221:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9646 (1.9395)
+2022-11-18 15:10:03,382:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8755 (1.9231)
+2022-11-18 15:10:03,543:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0196 (1.9438)
+2022-11-18 15:10:03,704:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9700 (1.9480)
+2022-11-18 15:10:03,863:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9680 (1.9509)
+2022-11-18 15:10:04,022:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9519 (1.9510)
+2022-11-18 15:10:04,183:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9260 (1.9482)
+2022-11-18 15:10:04,340:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8875 (1.9413)
+2022-11-18 15:10:04,499:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8738 (1.9357)
+2022-11-18 15:10:04,657:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9039 (1.9331)
+2022-11-18 15:10:04,816:INFO: Dataset: zara2               Batch: 13/18	Loss 2.0181 (1.9392)
+2022-11-18 15:10:04,975:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0002 (1.9437)
+2022-11-18 15:10:05,136:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8505 (1.9380)
+2022-11-18 15:10:05,295:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9088 (1.9363)
+2022-11-18 15:10:05,456:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8917 (1.9338)
+2022-11-18 15:10:05,603:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8106 (1.9277)
+2022-11-18 15:10:05,649:INFO: - Computing loss (validation)
+2022-11-18 15:10:05,920:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8034 (1.8034)
+2022-11-18 15:10:05,953:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2164 (1.8401)
+2022-11-18 15:10:06,261:INFO: Dataset: univ                Batch: 1/3	Loss 1.9118 (1.9118)
+2022-11-18 15:10:06,337:INFO: Dataset: univ                Batch: 2/3	Loss 1.8425 (1.8790)
+2022-11-18 15:10:06,411:INFO: Dataset: univ                Batch: 3/3	Loss 1.9146 (1.8918)
+2022-11-18 15:10:06,704:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9121 (1.9121)
+2022-11-18 15:10:06,748:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9637 (1.9237)
+2022-11-18 15:10:07,057:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9373 (1.9373)
+2022-11-18 15:10:07,131:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8943 (1.9159)
+2022-11-18 15:10:07,208:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8851 (1.9050)
+2022-11-18 15:10:07,283:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8577 (1.8935)
+2022-11-18 15:10:07,357:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9159 (1.8978)
+2022-11-18 15:10:07,409:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_320.pth.tar
+2022-11-18 15:10:07,409:INFO: 
+===> EPOCH: 321 (P2)
+2022-11-18 15:10:07,410:INFO: - Computing loss (training)
+2022-11-18 15:10:07,750:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9206 (1.9206)
+2022-11-18 15:10:07,905:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0627 (1.9887)
+2022-11-18 15:10:08,057:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9207 (1.9670)
+2022-11-18 15:10:08,174:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0163 (1.9749)
+2022-11-18 15:10:08,604:INFO: Dataset: univ                Batch:  1/15	Loss 1.9110 (1.9110)
+2022-11-18 15:10:08,769:INFO: Dataset: univ                Batch:  2/15	Loss 1.8667 (1.8892)
+2022-11-18 15:10:08,937:INFO: Dataset: univ                Batch:  3/15	Loss 1.9012 (1.8935)
+2022-11-18 15:10:09,100:INFO: Dataset: univ                Batch:  4/15	Loss 1.9518 (1.9088)
+2022-11-18 15:10:09,267:INFO: Dataset: univ                Batch:  5/15	Loss 1.8900 (1.9049)
+2022-11-18 15:10:09,432:INFO: Dataset: univ                Batch:  6/15	Loss 1.8844 (1.9014)
+2022-11-18 15:10:09,594:INFO: Dataset: univ                Batch:  7/15	Loss 1.8758 (1.8977)
+2022-11-18 15:10:09,755:INFO: Dataset: univ                Batch:  8/15	Loss 1.8665 (1.8933)
+2022-11-18 15:10:09,918:INFO: Dataset: univ                Batch:  9/15	Loss 1.9003 (1.8941)
+2022-11-18 15:10:10,078:INFO: Dataset: univ                Batch: 10/15	Loss 1.9345 (1.8980)
+2022-11-18 15:10:10,242:INFO: Dataset: univ                Batch: 11/15	Loss 1.8648 (1.8951)
+2022-11-18 15:10:10,406:INFO: Dataset: univ                Batch: 12/15	Loss 1.8757 (1.8935)
+2022-11-18 15:10:10,568:INFO: Dataset: univ                Batch: 13/15	Loss 1.9103 (1.8948)
+2022-11-18 15:10:10,730:INFO: Dataset: univ                Batch: 14/15	Loss 1.9018 (1.8953)
+2022-11-18 15:10:10,816:INFO: Dataset: univ                Batch: 15/15	Loss 1.9930 (1.8964)
+2022-11-18 15:10:11,222:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9535 (1.9535)
+2022-11-18 15:10:11,373:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9320 (1.9433)
+2022-11-18 15:10:11,527:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9237 (1.9370)
+2022-11-18 15:10:11,676:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8875 (1.9253)
+2022-11-18 15:10:11,829:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8960 (1.9202)
+2022-11-18 15:10:11,982:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9750 (1.9293)
+2022-11-18 15:10:12,133:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8906 (1.9235)
+2022-11-18 15:10:12,270:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9779 (1.9298)
+2022-11-18 15:10:12,667:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9203 (1.9203)
+2022-11-18 15:10:12,821:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8516 (1.8872)
+2022-11-18 15:10:12,977:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9125 (1.8957)
+2022-11-18 15:10:13,129:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8575 (1.8863)
+2022-11-18 15:10:13,282:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8990 (1.8888)
+2022-11-18 15:10:13,435:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9796 (1.9043)
+2022-11-18 15:10:13,584:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9808 (1.9145)
+2022-11-18 15:10:13,732:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0380 (1.9303)
+2022-11-18 15:10:13,883:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9868 (1.9364)
+2022-11-18 15:10:14,031:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8860 (1.9316)
+2022-11-18 15:10:14,182:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9994 (1.9380)
+2022-11-18 15:10:14,331:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8288 (1.9295)
+2022-11-18 15:10:14,481:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9578 (1.9317)
+2022-11-18 15:10:14,630:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8380 (1.9239)
+2022-11-18 15:10:14,781:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8651 (1.9195)
+2022-11-18 15:10:14,931:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8816 (1.9174)
+2022-11-18 15:10:15,083:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7739 (1.9086)
+2022-11-18 15:10:15,221:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9286 (1.9095)
+2022-11-18 15:10:15,266:INFO: - Computing loss (validation)
+2022-11-18 15:10:15,531:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9557 (1.9557)
+2022-11-18 15:10:15,565:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9842 (1.9573)
+2022-11-18 15:10:15,865:INFO: Dataset: univ                Batch: 1/3	Loss 1.9934 (1.9934)
+2022-11-18 15:10:15,939:INFO: Dataset: univ                Batch: 2/3	Loss 1.9257 (1.9645)
+2022-11-18 15:10:16,010:INFO: Dataset: univ                Batch: 3/3	Loss 1.8775 (1.9408)
+2022-11-18 15:10:16,314:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9623 (1.9623)
+2022-11-18 15:10:16,362:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8431 (1.9351)
+2022-11-18 15:10:16,668:INFO: Dataset: zara2               Batch: 1/5	Loss 2.0098 (2.0098)
+2022-11-18 15:10:16,742:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9551 (1.9828)
+2022-11-18 15:10:16,818:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9612 (1.9762)
+2022-11-18 15:10:16,891:INFO: Dataset: zara2               Batch: 4/5	Loss 2.0336 (1.9904)
+2022-11-18 15:10:16,965:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8740 (1.9677)
+2022-11-18 15:10:17,016:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_321.pth.tar
+2022-11-18 15:10:17,016:INFO: 
+===> EPOCH: 322 (P2)
+2022-11-18 15:10:17,017:INFO: - Computing loss (training)
+2022-11-18 15:10:17,364:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9499 (1.9499)
+2022-11-18 15:10:17,516:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9498 (1.9499)
+2022-11-18 15:10:17,669:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0489 (1.9822)
+2022-11-18 15:10:17,784:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9784 (1.9815)
+2022-11-18 15:10:18,184:INFO: Dataset: univ                Batch:  1/15	Loss 1.9368 (1.9368)
+2022-11-18 15:10:18,343:INFO: Dataset: univ                Batch:  2/15	Loss 1.8690 (1.9015)
+2022-11-18 15:10:18,502:INFO: Dataset: univ                Batch:  3/15	Loss 1.9103 (1.9046)
+2022-11-18 15:10:18,658:INFO: Dataset: univ                Batch:  4/15	Loss 1.9246 (1.9090)
+2022-11-18 15:10:18,823:INFO: Dataset: univ                Batch:  5/15	Loss 1.8492 (1.8974)
+2022-11-18 15:10:18,982:INFO: Dataset: univ                Batch:  6/15	Loss 1.8977 (1.8974)
+2022-11-18 15:10:19,142:INFO: Dataset: univ                Batch:  7/15	Loss 1.8988 (1.8976)
+2022-11-18 15:10:19,297:INFO: Dataset: univ                Batch:  8/15	Loss 1.8913 (1.8969)
+2022-11-18 15:10:19,455:INFO: Dataset: univ                Batch:  9/15	Loss 1.9208 (1.8995)
+2022-11-18 15:10:19,609:INFO: Dataset: univ                Batch: 10/15	Loss 1.8786 (1.8974)
+2022-11-18 15:10:19,766:INFO: Dataset: univ                Batch: 11/15	Loss 1.8641 (1.8945)
+2022-11-18 15:10:19,923:INFO: Dataset: univ                Batch: 12/15	Loss 1.8762 (1.8930)
+2022-11-18 15:10:20,080:INFO: Dataset: univ                Batch: 13/15	Loss 1.8791 (1.8921)
+2022-11-18 15:10:20,236:INFO: Dataset: univ                Batch: 14/15	Loss 1.9452 (1.8959)
+2022-11-18 15:10:20,320:INFO: Dataset: univ                Batch: 15/15	Loss 1.8397 (1.8952)
+2022-11-18 15:10:20,725:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9558 (1.9558)
+2022-11-18 15:10:20,875:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9546 (1.9552)
+2022-11-18 15:10:21,028:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9570 (1.9558)
+2022-11-18 15:10:21,179:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9740 (1.9606)
+2022-11-18 15:10:21,334:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9234 (1.9529)
+2022-11-18 15:10:21,487:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0042 (1.9618)
+2022-11-18 15:10:21,639:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9058 (1.9535)
+2022-11-18 15:10:21,777:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8216 (1.9400)
+2022-11-18 15:10:22,177:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9847 (1.9847)
+2022-11-18 15:10:22,333:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9007 (1.9442)
+2022-11-18 15:10:22,487:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9505 (1.9462)
+2022-11-18 15:10:22,641:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8400 (1.9200)
+2022-11-18 15:10:22,800:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8811 (1.9119)
+2022-11-18 15:10:22,954:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9951 (1.9253)
+2022-11-18 15:10:23,108:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9276 (1.9256)
+2022-11-18 15:10:23,260:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8795 (1.9198)
+2022-11-18 15:10:23,414:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8936 (1.9168)
+2022-11-18 15:10:23,565:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8672 (1.9120)
+2022-11-18 15:10:23,720:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9228 (1.9130)
+2022-11-18 15:10:23,871:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0077 (1.9210)
+2022-11-18 15:10:24,026:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9351 (1.9220)
+2022-11-18 15:10:24,179:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9358 (1.9230)
+2022-11-18 15:10:24,330:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9241 (1.9230)
+2022-11-18 15:10:24,479:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8407 (1.9180)
+2022-11-18 15:10:24,630:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8485 (1.9141)
+2022-11-18 15:10:24,768:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8458 (1.9105)
+2022-11-18 15:10:24,816:INFO: - Computing loss (validation)
+2022-11-18 15:10:25,070:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8928 (1.8928)
+2022-11-18 15:10:25,104:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7947 (1.8864)
+2022-11-18 15:10:25,410:INFO: Dataset: univ                Batch: 1/3	Loss 1.9640 (1.9640)
+2022-11-18 15:10:25,485:INFO: Dataset: univ                Batch: 2/3	Loss 1.8631 (1.9172)
+2022-11-18 15:10:25,559:INFO: Dataset: univ                Batch: 3/3	Loss 1.8911 (1.9086)
+2022-11-18 15:10:25,858:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9975 (1.9975)
+2022-11-18 15:10:25,904:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8817 (1.9681)
+2022-11-18 15:10:26,223:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8635 (1.8635)
+2022-11-18 15:10:26,304:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9093 (1.8872)
+2022-11-18 15:10:26,381:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8338 (1.8687)
+2022-11-18 15:10:26,460:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9799 (1.8945)
+2022-11-18 15:10:26,537:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8751 (1.8907)
+2022-11-18 15:10:26,591:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_322.pth.tar
+2022-11-18 15:10:26,592:INFO: 
+===> EPOCH: 323 (P2)
+2022-11-18 15:10:26,592:INFO: - Computing loss (training)
+2022-11-18 15:10:26,924:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9903 (1.9903)
+2022-11-18 15:10:27,076:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9596 (1.9745)
+2022-11-18 15:10:27,225:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0395 (1.9963)
+2022-11-18 15:10:27,340:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0241 (2.0010)
+2022-11-18 15:10:27,753:INFO: Dataset: univ                Batch:  1/15	Loss 1.9124 (1.9124)
+2022-11-18 15:10:27,922:INFO: Dataset: univ                Batch:  2/15	Loss 1.8510 (1.8830)
+2022-11-18 15:10:28,093:INFO: Dataset: univ                Batch:  3/15	Loss 1.9451 (1.9035)
+2022-11-18 15:10:28,257:INFO: Dataset: univ                Batch:  4/15	Loss 1.9097 (1.9049)
+2022-11-18 15:10:28,426:INFO: Dataset: univ                Batch:  5/15	Loss 1.9003 (1.9040)
+2022-11-18 15:10:28,592:INFO: Dataset: univ                Batch:  6/15	Loss 1.8926 (1.9020)
+2022-11-18 15:10:28,758:INFO: Dataset: univ                Batch:  7/15	Loss 1.8976 (1.9014)
+2022-11-18 15:10:28,925:INFO: Dataset: univ                Batch:  8/15	Loss 1.9053 (1.9018)
+2022-11-18 15:10:29,085:INFO: Dataset: univ                Batch:  9/15	Loss 1.8655 (1.8978)
+2022-11-18 15:10:29,241:INFO: Dataset: univ                Batch: 10/15	Loss 1.9217 (1.9001)
+2022-11-18 15:10:29,400:INFO: Dataset: univ                Batch: 11/15	Loss 1.9009 (1.9001)
+2022-11-18 15:10:29,556:INFO: Dataset: univ                Batch: 12/15	Loss 1.9655 (1.9053)
+2022-11-18 15:10:29,713:INFO: Dataset: univ                Batch: 13/15	Loss 1.9169 (1.9061)
+2022-11-18 15:10:29,871:INFO: Dataset: univ                Batch: 14/15	Loss 1.9255 (1.9075)
+2022-11-18 15:10:29,955:INFO: Dataset: univ                Batch: 15/15	Loss 1.9254 (1.9078)
+2022-11-18 15:10:30,334:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0150 (2.0150)
+2022-11-18 15:10:30,495:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8204 (1.9167)
+2022-11-18 15:10:30,643:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0113 (1.9491)
+2022-11-18 15:10:30,790:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0792 (1.9837)
+2022-11-18 15:10:30,938:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8811 (1.9628)
+2022-11-18 15:10:31,087:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9086 (1.9544)
+2022-11-18 15:10:31,236:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9647 (1.9557)
+2022-11-18 15:10:31,371:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0569 (1.9664)
+2022-11-18 15:10:31,757:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9361 (1.9361)
+2022-11-18 15:10:31,909:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8741 (1.9054)
+2022-11-18 15:10:32,062:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8960 (1.9025)
+2022-11-18 15:10:32,212:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8794 (1.8968)
+2022-11-18 15:10:32,364:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8894 (1.8954)
+2022-11-18 15:10:32,513:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9220 (1.8994)
+2022-11-18 15:10:32,662:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9887 (1.9126)
+2022-11-18 15:10:32,808:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9328 (1.9148)
+2022-11-18 15:10:32,957:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8662 (1.9088)
+2022-11-18 15:10:33,104:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9973 (1.9176)
+2022-11-18 15:10:33,254:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9536 (1.9206)
+2022-11-18 15:10:33,400:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9152 (1.9201)
+2022-11-18 15:10:33,549:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8458 (1.9144)
+2022-11-18 15:10:33,697:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8711 (1.9111)
+2022-11-18 15:10:33,846:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9133 (1.9113)
+2022-11-18 15:10:33,995:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9528 (1.9136)
+2022-11-18 15:10:34,149:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9715 (1.9170)
+2022-11-18 15:10:34,285:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9016 (1.9161)
+2022-11-18 15:10:34,334:INFO: - Computing loss (validation)
+2022-11-18 15:10:34,603:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9190 (1.9190)
+2022-11-18 15:10:34,639:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5898 (1.9010)
+2022-11-18 15:10:34,967:INFO: Dataset: univ                Batch: 1/3	Loss 1.9394 (1.9394)
+2022-11-18 15:10:35,048:INFO: Dataset: univ                Batch: 2/3	Loss 1.8943 (1.9166)
+2022-11-18 15:10:35,123:INFO: Dataset: univ                Batch: 3/3	Loss 1.8721 (1.9037)
+2022-11-18 15:10:35,422:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0070 (2.0070)
+2022-11-18 15:10:35,467:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7990 (1.9589)
+2022-11-18 15:10:35,767:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8975 (1.8975)
+2022-11-18 15:10:35,843:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9082 (1.9029)
+2022-11-18 15:10:35,916:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9360 (1.9138)
+2022-11-18 15:10:35,989:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9558 (1.9247)
+2022-11-18 15:10:36,063:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8792 (1.9158)
+2022-11-18 15:10:36,115:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_323.pth.tar
+2022-11-18 15:10:36,115:INFO: 
+===> EPOCH: 324 (P2)
+2022-11-18 15:10:36,116:INFO: - Computing loss (training)
+2022-11-18 15:10:36,457:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9308 (1.9308)
+2022-11-18 15:10:36,609:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8362 (1.8863)
+2022-11-18 15:10:36,759:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9863 (1.9192)
+2022-11-18 15:10:36,875:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8764 (1.9121)
+2022-11-18 15:10:37,276:INFO: Dataset: univ                Batch:  1/15	Loss 1.9199 (1.9199)
+2022-11-18 15:10:37,434:INFO: Dataset: univ                Batch:  2/15	Loss 1.9061 (1.9129)
+2022-11-18 15:10:37,595:INFO: Dataset: univ                Batch:  3/15	Loss 1.8825 (1.9033)
+2022-11-18 15:10:37,751:INFO: Dataset: univ                Batch:  4/15	Loss 1.8975 (1.9018)
+2022-11-18 15:10:37,912:INFO: Dataset: univ                Batch:  5/15	Loss 1.9039 (1.9022)
+2022-11-18 15:10:38,071:INFO: Dataset: univ                Batch:  6/15	Loss 1.9178 (1.9048)
+2022-11-18 15:10:38,230:INFO: Dataset: univ                Batch:  7/15	Loss 1.8836 (1.9019)
+2022-11-18 15:10:38,385:INFO: Dataset: univ                Batch:  8/15	Loss 1.8983 (1.9015)
+2022-11-18 15:10:38,543:INFO: Dataset: univ                Batch:  9/15	Loss 1.9215 (1.9037)
+2022-11-18 15:10:38,698:INFO: Dataset: univ                Batch: 10/15	Loss 1.9150 (1.9049)
+2022-11-18 15:10:38,862:INFO: Dataset: univ                Batch: 11/15	Loss 1.9538 (1.9093)
+2022-11-18 15:10:39,020:INFO: Dataset: univ                Batch: 12/15	Loss 1.9079 (1.9091)
+2022-11-18 15:10:39,178:INFO: Dataset: univ                Batch: 13/15	Loss 1.9409 (1.9114)
+2022-11-18 15:10:39,334:INFO: Dataset: univ                Batch: 14/15	Loss 1.9470 (1.9137)
+2022-11-18 15:10:39,418:INFO: Dataset: univ                Batch: 15/15	Loss 1.9355 (1.9139)
+2022-11-18 15:10:39,810:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9844 (1.9844)
+2022-11-18 15:10:39,957:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9281 (1.9598)
+2022-11-18 15:10:40,106:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0162 (1.9771)
+2022-11-18 15:10:40,251:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8389 (1.9429)
+2022-11-18 15:10:40,399:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9031 (1.9353)
+2022-11-18 15:10:40,556:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9385 (1.9359)
+2022-11-18 15:10:40,705:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8770 (1.9282)
+2022-11-18 15:10:40,839:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0055 (1.9361)
+2022-11-18 15:10:41,298:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8943 (1.8943)
+2022-11-18 15:10:41,449:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9737 (1.9305)
+2022-11-18 15:10:41,598:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9221 (1.9278)
+2022-11-18 15:10:41,746:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9082 (1.9224)
+2022-11-18 15:10:41,894:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8724 (1.9122)
+2022-11-18 15:10:42,126:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8204 (1.8972)
+2022-11-18 15:10:42,274:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9169 (1.9000)
+2022-11-18 15:10:42,424:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9491 (1.9056)
+2022-11-18 15:10:42,571:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8953 (1.9045)
+2022-11-18 15:10:42,720:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9404 (1.9080)
+2022-11-18 15:10:42,870:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0130 (1.9176)
+2022-11-18 15:10:43,018:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8885 (1.9151)
+2022-11-18 15:10:43,168:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9565 (1.9178)
+2022-11-18 15:10:43,317:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0740 (1.9289)
+2022-11-18 15:10:43,466:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0145 (1.9344)
+2022-11-18 15:10:43,616:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8474 (1.9286)
+2022-11-18 15:10:43,764:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8584 (1.9246)
+2022-11-18 15:10:43,902:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8710 (1.9219)
+2022-11-18 15:10:43,947:INFO: - Computing loss (validation)
+2022-11-18 15:10:44,228:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9871 (1.9871)
+2022-11-18 15:10:44,264:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4554 (1.9544)
+2022-11-18 15:10:44,585:INFO: Dataset: univ                Batch: 1/3	Loss 1.9621 (1.9621)
+2022-11-18 15:10:44,665:INFO: Dataset: univ                Batch: 2/3	Loss 1.8967 (1.9288)
+2022-11-18 15:10:44,737:INFO: Dataset: univ                Batch: 3/3	Loss 1.9713 (1.9413)
+2022-11-18 15:10:45,039:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9725 (1.9725)
+2022-11-18 15:10:45,085:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7682 (1.9252)
+2022-11-18 15:10:45,392:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8783 (1.8783)
+2022-11-18 15:10:45,470:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9569 (1.9184)
+2022-11-18 15:10:45,547:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9422 (1.9265)
+2022-11-18 15:10:45,625:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9054 (1.9211)
+2022-11-18 15:10:45,701:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8718 (1.9119)
+2022-11-18 15:10:45,756:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_324.pth.tar
+2022-11-18 15:10:45,756:INFO: 
+===> EPOCH: 325 (P2)
+2022-11-18 15:10:45,757:INFO: - Computing loss (training)
+2022-11-18 15:10:46,100:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9655 (1.9655)
+2022-11-18 15:10:46,255:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9293 (1.9461)
+2022-11-18 15:10:46,405:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9058 (1.9325)
+2022-11-18 15:10:46,520:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1123 (1.9617)
+2022-11-18 15:10:46,968:INFO: Dataset: univ                Batch:  1/15	Loss 1.8783 (1.8783)
+2022-11-18 15:10:47,126:INFO: Dataset: univ                Batch:  2/15	Loss 1.8816 (1.8800)
+2022-11-18 15:10:47,286:INFO: Dataset: univ                Batch:  3/15	Loss 1.9264 (1.8960)
+2022-11-18 15:10:47,444:INFO: Dataset: univ                Batch:  4/15	Loss 1.9346 (1.9053)
+2022-11-18 15:10:47,606:INFO: Dataset: univ                Batch:  5/15	Loss 1.8832 (1.9003)
+2022-11-18 15:10:47,767:INFO: Dataset: univ                Batch:  6/15	Loss 1.8842 (1.8974)
+2022-11-18 15:10:47,924:INFO: Dataset: univ                Batch:  7/15	Loss 1.8904 (1.8963)
+2022-11-18 15:10:48,082:INFO: Dataset: univ                Batch:  8/15	Loss 1.9487 (1.9031)
+2022-11-18 15:10:48,239:INFO: Dataset: univ                Batch:  9/15	Loss 1.8659 (1.8992)
+2022-11-18 15:10:48,394:INFO: Dataset: univ                Batch: 10/15	Loss 1.8875 (1.8981)
+2022-11-18 15:10:48,552:INFO: Dataset: univ                Batch: 11/15	Loss 1.8991 (1.8982)
+2022-11-18 15:10:48,709:INFO: Dataset: univ                Batch: 12/15	Loss 1.9021 (1.8985)
+2022-11-18 15:10:48,873:INFO: Dataset: univ                Batch: 13/15	Loss 1.8807 (1.8970)
+2022-11-18 15:10:49,035:INFO: Dataset: univ                Batch: 14/15	Loss 1.9062 (1.8976)
+2022-11-18 15:10:49,120:INFO: Dataset: univ                Batch: 15/15	Loss 1.9567 (1.8980)
+2022-11-18 15:10:49,514:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9484 (1.9484)
+2022-11-18 15:10:49,663:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8348 (1.8933)
+2022-11-18 15:10:49,817:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0189 (1.9391)
+2022-11-18 15:10:49,966:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9510 (1.9420)
+2022-11-18 15:10:50,116:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8737 (1.9295)
+2022-11-18 15:10:50,266:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9496 (1.9328)
+2022-11-18 15:10:50,416:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8238 (1.9172)
+2022-11-18 15:10:50,554:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9176 (1.9172)
+2022-11-18 15:10:50,946:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8543 (1.8543)
+2022-11-18 15:10:51,097:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9741 (1.9128)
+2022-11-18 15:10:51,254:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9438 (1.9235)
+2022-11-18 15:10:51,405:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9339 (1.9259)
+2022-11-18 15:10:51,561:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9669 (1.9338)
+2022-11-18 15:10:51,714:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9848 (1.9416)
+2022-11-18 15:10:51,867:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9122 (1.9373)
+2022-11-18 15:10:52,018:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8849 (1.9311)
+2022-11-18 15:10:52,171:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9148 (1.9294)
+2022-11-18 15:10:52,321:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9835 (1.9344)
+2022-11-18 15:10:52,474:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9009 (1.9317)
+2022-11-18 15:10:52,626:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8543 (1.9245)
+2022-11-18 15:10:52,782:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8372 (1.9182)
+2022-11-18 15:10:52,935:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8550 (1.9136)
+2022-11-18 15:10:53,090:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8783 (1.9111)
+2022-11-18 15:10:53,241:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8724 (1.9087)
+2022-11-18 15:10:53,396:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8721 (1.9066)
+2022-11-18 15:10:53,536:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8537 (1.9042)
+2022-11-18 15:10:53,581:INFO: - Computing loss (validation)
+2022-11-18 15:10:53,849:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9081 (1.9081)
+2022-11-18 15:10:53,884:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8360 (1.9032)
+2022-11-18 15:10:54,190:INFO: Dataset: univ                Batch: 1/3	Loss 1.8689 (1.8689)
+2022-11-18 15:10:54,269:INFO: Dataset: univ                Batch: 2/3	Loss 1.9229 (1.8934)
+2022-11-18 15:10:54,344:INFO: Dataset: univ                Batch: 3/3	Loss 1.9170 (1.9007)
+2022-11-18 15:10:54,644:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8661 (1.8661)
+2022-11-18 15:10:54,689:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7543 (1.8381)
+2022-11-18 15:10:55,006:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9473 (1.9473)
+2022-11-18 15:10:55,084:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9240 (1.9352)
+2022-11-18 15:10:55,161:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9292 (1.9331)
+2022-11-18 15:10:55,238:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9226 (1.9304)
+2022-11-18 15:10:55,315:INFO: Dataset: zara2               Batch: 5/5	Loss 2.0227 (1.9494)
+2022-11-18 15:10:55,386:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_325.pth.tar
+2022-11-18 15:10:55,386:INFO: 
+===> EPOCH: 326 (P2)
+2022-11-18 15:10:55,387:INFO: - Computing loss (training)
+2022-11-18 15:10:55,724:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0254 (2.0254)
+2022-11-18 15:10:55,875:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9682 (1.9981)
+2022-11-18 15:10:56,025:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9530 (1.9830)
+2022-11-18 15:10:56,140:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8700 (1.9659)
+2022-11-18 15:10:56,558:INFO: Dataset: univ                Batch:  1/15	Loss 1.9320 (1.9320)
+2022-11-18 15:10:56,715:INFO: Dataset: univ                Batch:  2/15	Loss 1.8883 (1.9102)
+2022-11-18 15:10:56,877:INFO: Dataset: univ                Batch:  3/15	Loss 1.9022 (1.9075)
+2022-11-18 15:10:57,032:INFO: Dataset: univ                Batch:  4/15	Loss 1.9006 (1.9059)
+2022-11-18 15:10:57,195:INFO: Dataset: univ                Batch:  5/15	Loss 1.8745 (1.8998)
+2022-11-18 15:10:57,354:INFO: Dataset: univ                Batch:  6/15	Loss 1.8778 (1.8958)
+2022-11-18 15:10:57,512:INFO: Dataset: univ                Batch:  7/15	Loss 1.8839 (1.8940)
+2022-11-18 15:10:57,667:INFO: Dataset: univ                Batch:  8/15	Loss 1.8707 (1.8911)
+2022-11-18 15:10:57,825:INFO: Dataset: univ                Batch:  9/15	Loss 1.8894 (1.8909)
+2022-11-18 15:10:57,980:INFO: Dataset: univ                Batch: 10/15	Loss 1.8781 (1.8897)
+2022-11-18 15:10:58,138:INFO: Dataset: univ                Batch: 11/15	Loss 1.8519 (1.8867)
+2022-11-18 15:10:58,295:INFO: Dataset: univ                Batch: 12/15	Loss 1.8637 (1.8848)
+2022-11-18 15:10:58,454:INFO: Dataset: univ                Batch: 13/15	Loss 1.9244 (1.8880)
+2022-11-18 15:10:58,610:INFO: Dataset: univ                Batch: 14/15	Loss 1.8895 (1.8881)
+2022-11-18 15:10:58,694:INFO: Dataset: univ                Batch: 15/15	Loss 1.8859 (1.8881)
+2022-11-18 15:10:59,110:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9810 (1.9810)
+2022-11-18 15:10:59,264:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0214 (2.0024)
+2022-11-18 15:10:59,417:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9463 (1.9833)
+2022-11-18 15:10:59,566:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9107 (1.9643)
+2022-11-18 15:10:59,718:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9273 (1.9566)
+2022-11-18 15:10:59,869:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8495 (1.9372)
+2022-11-18 15:11:00,022:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9620 (1.9405)
+2022-11-18 15:11:00,168:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9841 (1.9446)
+2022-11-18 15:11:00,558:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9553 (1.9553)
+2022-11-18 15:11:00,705:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8655 (1.9092)
+2022-11-18 15:11:00,857:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8547 (1.8905)
+2022-11-18 15:11:01,008:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8510 (1.8808)
+2022-11-18 15:11:01,158:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8667 (1.8780)
+2022-11-18 15:11:01,311:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8991 (1.8815)
+2022-11-18 15:11:01,460:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8960 (1.8836)
+2022-11-18 15:11:01,606:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9000 (1.8859)
+2022-11-18 15:11:01,753:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0291 (1.9000)
+2022-11-18 15:11:01,898:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8748 (1.8973)
+2022-11-18 15:11:02,046:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9183 (1.8993)
+2022-11-18 15:11:02,197:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8713 (1.8970)
+2022-11-18 15:11:02,346:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9331 (1.8999)
+2022-11-18 15:11:02,493:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9481 (1.9035)
+2022-11-18 15:11:02,642:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9273 (1.9052)
+2022-11-18 15:11:02,790:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8988 (1.9048)
+2022-11-18 15:11:02,939:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8297 (1.9005)
+2022-11-18 15:11:03,076:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9074 (1.9009)
+2022-11-18 15:11:03,123:INFO: - Computing loss (validation)
+2022-11-18 15:11:03,384:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9548 (1.9548)
+2022-11-18 15:11:03,418:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1805 (1.9725)
+2022-11-18 15:11:03,727:INFO: Dataset: univ                Batch: 1/3	Loss 1.9418 (1.9418)
+2022-11-18 15:11:03,809:INFO: Dataset: univ                Batch: 2/3	Loss 1.9145 (1.9273)
+2022-11-18 15:11:03,885:INFO: Dataset: univ                Batch: 3/3	Loss 1.8983 (1.9189)
+2022-11-18 15:11:04,177:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9039 (1.9039)
+2022-11-18 15:11:04,221:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7211 (1.8527)
+2022-11-18 15:11:04,564:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9291 (1.9291)
+2022-11-18 15:11:04,643:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8998 (1.9140)
+2022-11-18 15:11:04,719:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8741 (1.9006)
+2022-11-18 15:11:04,797:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8061 (1.8766)
+2022-11-18 15:11:04,873:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8905 (1.8796)
+2022-11-18 15:11:04,933:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_326.pth.tar
+2022-11-18 15:11:04,933:INFO: 
+===> EPOCH: 327 (P2)
+2022-11-18 15:11:04,934:INFO: - Computing loss (training)
+2022-11-18 15:11:05,273:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1114 (2.1114)
+2022-11-18 15:11:05,433:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9316 (2.0191)
+2022-11-18 15:11:05,583:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9809 (2.0058)
+2022-11-18 15:11:05,696:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0058 (2.0058)
+2022-11-18 15:11:06,098:INFO: Dataset: univ                Batch:  1/15	Loss 1.8950 (1.8950)
+2022-11-18 15:11:06,259:INFO: Dataset: univ                Batch:  2/15	Loss 1.8858 (1.8904)
+2022-11-18 15:11:06,419:INFO: Dataset: univ                Batch:  3/15	Loss 1.8855 (1.8886)
+2022-11-18 15:11:06,576:INFO: Dataset: univ                Batch:  4/15	Loss 1.9128 (1.8952)
+2022-11-18 15:11:06,732:INFO: Dataset: univ                Batch:  5/15	Loss 1.9393 (1.9045)
+2022-11-18 15:11:06,890:INFO: Dataset: univ                Batch:  6/15	Loss 1.9180 (1.9065)
+2022-11-18 15:11:07,046:INFO: Dataset: univ                Batch:  7/15	Loss 1.8997 (1.9056)
+2022-11-18 15:11:07,202:INFO: Dataset: univ                Batch:  8/15	Loss 1.8594 (1.8992)
+2022-11-18 15:11:07,358:INFO: Dataset: univ                Batch:  9/15	Loss 1.9068 (1.9001)
+2022-11-18 15:11:07,513:INFO: Dataset: univ                Batch: 10/15	Loss 1.8456 (1.8946)
+2022-11-18 15:11:07,670:INFO: Dataset: univ                Batch: 11/15	Loss 1.9064 (1.8957)
+2022-11-18 15:11:07,826:INFO: Dataset: univ                Batch: 12/15	Loss 1.9227 (1.8977)
+2022-11-18 15:11:07,983:INFO: Dataset: univ                Batch: 13/15	Loss 1.8626 (1.8952)
+2022-11-18 15:11:08,141:INFO: Dataset: univ                Batch: 14/15	Loss 1.9027 (1.8957)
+2022-11-18 15:11:08,223:INFO: Dataset: univ                Batch: 15/15	Loss 1.9115 (1.8960)
+2022-11-18 15:11:08,623:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9509 (1.9509)
+2022-11-18 15:11:08,772:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0304 (1.9889)
+2022-11-18 15:11:08,927:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9719 (1.9830)
+2022-11-18 15:11:09,077:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8688 (1.9559)
+2022-11-18 15:11:09,228:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9234 (1.9491)
+2022-11-18 15:11:09,379:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9485 (1.9490)
+2022-11-18 15:11:09,529:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8645 (1.9358)
+2022-11-18 15:11:09,666:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0042 (1.9428)
+2022-11-18 15:11:10,049:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9702 (1.9702)
+2022-11-18 15:11:10,198:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9345 (1.9524)
+2022-11-18 15:11:10,351:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9667 (1.9571)
+2022-11-18 15:11:10,505:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8739 (1.9380)
+2022-11-18 15:11:10,659:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8537 (1.9211)
+2022-11-18 15:11:10,812:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9413 (1.9247)
+2022-11-18 15:11:10,963:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8377 (1.9135)
+2022-11-18 15:11:11,113:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8057 (1.9001)
+2022-11-18 15:11:11,265:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8791 (1.8979)
+2022-11-18 15:11:11,413:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9374 (1.9020)
+2022-11-18 15:11:11,564:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8338 (1.8959)
+2022-11-18 15:11:11,713:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9138 (1.8975)
+2022-11-18 15:11:11,864:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9053 (1.8980)
+2022-11-18 15:11:12,012:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9692 (1.9032)
+2022-11-18 15:11:12,162:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9628 (1.9073)
+2022-11-18 15:11:12,311:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8631 (1.9046)
+2022-11-18 15:11:12,463:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9054 (1.9047)
+2022-11-18 15:11:12,601:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8901 (1.9040)
+2022-11-18 15:11:12,646:INFO: - Computing loss (validation)
+2022-11-18 15:11:12,912:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9256 (1.9256)
+2022-11-18 15:11:12,946:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0052 (1.9324)
+2022-11-18 15:11:13,254:INFO: Dataset: univ                Batch: 1/3	Loss 1.9419 (1.9419)
+2022-11-18 15:11:13,328:INFO: Dataset: univ                Batch: 2/3	Loss 1.9211 (1.9324)
+2022-11-18 15:11:13,401:INFO: Dataset: univ                Batch: 3/3	Loss 1.8767 (1.9136)
+2022-11-18 15:11:13,701:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9602 (1.9602)
+2022-11-18 15:11:13,747:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9791 (1.9648)
+2022-11-18 15:11:14,048:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8954 (1.8954)
+2022-11-18 15:11:14,121:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8856 (1.8904)
+2022-11-18 15:11:14,195:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9095 (1.8968)
+2022-11-18 15:11:14,270:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8074 (1.8760)
+2022-11-18 15:11:14,343:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8711 (1.8750)
+2022-11-18 15:11:14,397:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_327.pth.tar
+2022-11-18 15:11:14,397:INFO: 
+===> EPOCH: 328 (P2)
+2022-11-18 15:11:14,398:INFO: - Computing loss (training)
+2022-11-18 15:11:14,734:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9653 (1.9653)
+2022-11-18 15:11:14,905:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9709 (1.9681)
+2022-11-18 15:11:15,057:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8919 (1.9435)
+2022-11-18 15:11:15,174:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9186 (1.9389)
+2022-11-18 15:11:15,614:INFO: Dataset: univ                Batch:  1/15	Loss 1.9311 (1.9311)
+2022-11-18 15:11:15,772:INFO: Dataset: univ                Batch:  2/15	Loss 1.8820 (1.9065)
+2022-11-18 15:11:15,930:INFO: Dataset: univ                Batch:  3/15	Loss 1.8841 (1.8995)
+2022-11-18 15:11:16,087:INFO: Dataset: univ                Batch:  4/15	Loss 1.8978 (1.8991)
+2022-11-18 15:11:16,245:INFO: Dataset: univ                Batch:  5/15	Loss 1.8977 (1.8988)
+2022-11-18 15:11:16,405:INFO: Dataset: univ                Batch:  6/15	Loss 1.9027 (1.8995)
+2022-11-18 15:11:16,562:INFO: Dataset: univ                Batch:  7/15	Loss 1.8944 (1.8988)
+2022-11-18 15:11:16,718:INFO: Dataset: univ                Batch:  8/15	Loss 1.8938 (1.8982)
+2022-11-18 15:11:16,875:INFO: Dataset: univ                Batch:  9/15	Loss 1.9060 (1.8990)
+2022-11-18 15:11:17,032:INFO: Dataset: univ                Batch: 10/15	Loss 1.9323 (1.9020)
+2022-11-18 15:11:17,190:INFO: Dataset: univ                Batch: 11/15	Loss 1.9036 (1.9021)
+2022-11-18 15:11:17,347:INFO: Dataset: univ                Batch: 12/15	Loss 1.8737 (1.8997)
+2022-11-18 15:11:17,505:INFO: Dataset: univ                Batch: 13/15	Loss 1.9004 (1.8998)
+2022-11-18 15:11:17,663:INFO: Dataset: univ                Batch: 14/15	Loss 1.9094 (1.9005)
+2022-11-18 15:11:17,746:INFO: Dataset: univ                Batch: 15/15	Loss 2.0203 (1.9023)
+2022-11-18 15:11:18,131:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9819 (1.9819)
+2022-11-18 15:11:18,281:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8884 (1.9346)
+2022-11-18 15:11:18,430:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9211 (1.9300)
+2022-11-18 15:11:18,579:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9200 (1.9279)
+2022-11-18 15:11:18,731:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9945 (1.9414)
+2022-11-18 15:11:18,888:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9271 (1.9389)
+2022-11-18 15:11:19,040:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9031 (1.9338)
+2022-11-18 15:11:19,178:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8726 (1.9268)
+2022-11-18 15:11:19,568:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9823 (1.9823)
+2022-11-18 15:11:19,719:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7986 (1.9018)
+2022-11-18 15:11:19,875:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0133 (1.9399)
+2022-11-18 15:11:20,029:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9851 (1.9510)
+2022-11-18 15:11:20,183:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9288 (1.9465)
+2022-11-18 15:11:20,339:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8299 (1.9280)
+2022-11-18 15:11:20,492:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9077 (1.9248)
+2022-11-18 15:11:20,642:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0230 (1.9372)
+2022-11-18 15:11:20,794:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9743 (1.9419)
+2022-11-18 15:11:20,945:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9313 (1.9408)
+2022-11-18 15:11:21,097:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8747 (1.9350)
+2022-11-18 15:11:21,248:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9024 (1.9319)
+2022-11-18 15:11:21,424:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9754 (1.9353)
+2022-11-18 15:11:21,575:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9467 (1.9362)
+2022-11-18 15:11:21,727:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0243 (1.9422)
+2022-11-18 15:11:21,876:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8196 (1.9341)
+2022-11-18 15:11:22,029:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9833 (1.9370)
+2022-11-18 15:11:22,168:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0220 (1.9407)
+2022-11-18 15:11:22,214:INFO: - Computing loss (validation)
+2022-11-18 15:11:22,479:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9554 (1.9554)
+2022-11-18 15:11:22,513:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0378 (1.9622)
+2022-11-18 15:11:22,825:INFO: Dataset: univ                Batch: 1/3	Loss 1.9816 (1.9816)
+2022-11-18 15:11:22,907:INFO: Dataset: univ                Batch: 2/3	Loss 1.8817 (1.9357)
+2022-11-18 15:11:22,985:INFO: Dataset: univ                Batch: 3/3	Loss 1.9907 (1.9546)
+2022-11-18 15:11:23,288:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0708 (2.0708)
+2022-11-18 15:11:23,334:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8700 (2.0230)
+2022-11-18 15:11:23,649:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9332 (1.9332)
+2022-11-18 15:11:23,723:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9561 (1.9451)
+2022-11-18 15:11:23,796:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9040 (1.9314)
+2022-11-18 15:11:23,869:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9396 (1.9334)
+2022-11-18 15:11:23,942:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9264 (1.9320)
+2022-11-18 15:11:23,995:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_328.pth.tar
+2022-11-18 15:11:23,995:INFO: 
+===> EPOCH: 329 (P2)
+2022-11-18 15:11:23,996:INFO: - Computing loss (training)
+2022-11-18 15:11:24,338:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9458 (1.9458)
+2022-11-18 15:11:24,489:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0153 (1.9818)
+2022-11-18 15:11:24,641:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1234 (2.0286)
+2022-11-18 15:11:24,758:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0114 (2.0259)
+2022-11-18 15:11:25,145:INFO: Dataset: univ                Batch:  1/15	Loss 1.8942 (1.8942)
+2022-11-18 15:11:25,303:INFO: Dataset: univ                Batch:  2/15	Loss 1.9306 (1.9120)
+2022-11-18 15:11:25,460:INFO: Dataset: univ                Batch:  3/15	Loss 1.8911 (1.9052)
+2022-11-18 15:11:25,616:INFO: Dataset: univ                Batch:  4/15	Loss 1.8970 (1.9033)
+2022-11-18 15:11:25,772:INFO: Dataset: univ                Batch:  5/15	Loss 1.9083 (1.9043)
+2022-11-18 15:11:25,929:INFO: Dataset: univ                Batch:  6/15	Loss 1.9738 (1.9160)
+2022-11-18 15:11:26,083:INFO: Dataset: univ                Batch:  7/15	Loss 1.9026 (1.9142)
+2022-11-18 15:11:26,237:INFO: Dataset: univ                Batch:  8/15	Loss 1.9546 (1.9190)
+2022-11-18 15:11:26,392:INFO: Dataset: univ                Batch:  9/15	Loss 1.8857 (1.9151)
+2022-11-18 15:11:26,544:INFO: Dataset: univ                Batch: 10/15	Loss 1.9179 (1.9154)
+2022-11-18 15:11:26,701:INFO: Dataset: univ                Batch: 11/15	Loss 1.8949 (1.9136)
+2022-11-18 15:11:26,854:INFO: Dataset: univ                Batch: 12/15	Loss 1.9005 (1.9126)
+2022-11-18 15:11:27,009:INFO: Dataset: univ                Batch: 13/15	Loss 1.8838 (1.9105)
+2022-11-18 15:11:27,162:INFO: Dataset: univ                Batch: 14/15	Loss 1.9234 (1.9114)
+2022-11-18 15:11:27,244:INFO: Dataset: univ                Batch: 15/15	Loss 1.8026 (1.9100)
+2022-11-18 15:11:27,626:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0338 (2.0338)
+2022-11-18 15:11:27,774:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9281 (1.9891)
+2022-11-18 15:11:27,927:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9291 (1.9689)
+2022-11-18 15:11:28,076:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9894 (1.9742)
+2022-11-18 15:11:28,225:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9697 (1.9733)
+2022-11-18 15:11:28,373:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9233 (1.9645)
+2022-11-18 15:11:28,523:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8258 (1.9426)
+2022-11-18 15:11:28,658:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9045 (1.9383)
+2022-11-18 15:11:29,063:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9093 (1.9093)
+2022-11-18 15:11:29,213:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9689 (1.9382)
+2022-11-18 15:11:29,364:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8986 (1.9248)
+2022-11-18 15:11:29,515:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9216 (1.9240)
+2022-11-18 15:11:29,666:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9347 (1.9259)
+2022-11-18 15:11:29,815:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9043 (1.9222)
+2022-11-18 15:11:29,963:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9285 (1.9231)
+2022-11-18 15:11:30,110:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8435 (1.9121)
+2022-11-18 15:11:30,258:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9742 (1.9196)
+2022-11-18 15:11:30,404:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9403 (1.9216)
+2022-11-18 15:11:30,554:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9184 (1.9213)
+2022-11-18 15:11:30,702:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8685 (1.9172)
+2022-11-18 15:11:30,850:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9009 (1.9159)
+2022-11-18 15:11:30,998:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9309 (1.9170)
+2022-11-18 15:11:31,148:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9870 (1.9217)
+2022-11-18 15:11:31,297:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8885 (1.9196)
+2022-11-18 15:11:31,445:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8957 (1.9182)
+2022-11-18 15:11:31,582:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0460 (1.9249)
+2022-11-18 15:11:31,627:INFO: - Computing loss (validation)
+2022-11-18 15:11:31,892:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8053 (1.8053)
+2022-11-18 15:11:31,925:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9189 (1.8150)
+2022-11-18 15:11:32,225:INFO: Dataset: univ                Batch: 1/3	Loss 1.9384 (1.9384)
+2022-11-18 15:11:32,302:INFO: Dataset: univ                Batch: 2/3	Loss 1.9050 (1.9219)
+2022-11-18 15:11:32,374:INFO: Dataset: univ                Batch: 3/3	Loss 1.8623 (1.9024)
+2022-11-18 15:11:32,669:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9296 (1.9296)
+2022-11-18 15:11:32,713:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9030 (1.9233)
+2022-11-18 15:11:33,015:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9354 (1.9354)
+2022-11-18 15:11:33,090:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8806 (1.9086)
+2022-11-18 15:11:33,167:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9184 (1.9117)
+2022-11-18 15:11:33,241:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9186 (1.9134)
+2022-11-18 15:11:33,316:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9906 (1.9285)
+2022-11-18 15:11:33,368:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_329.pth.tar
+2022-11-18 15:11:33,368:INFO: 
+===> EPOCH: 330 (P2)
+2022-11-18 15:11:33,369:INFO: - Computing loss (training)
+2022-11-18 15:11:33,712:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9670 (1.9670)
+2022-11-18 15:11:33,859:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8708 (1.9189)
+2022-11-18 15:11:34,007:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9730 (1.9372)
+2022-11-18 15:11:34,126:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0632 (1.9588)
+2022-11-18 15:11:34,531:INFO: Dataset: univ                Batch:  1/15	Loss 1.9156 (1.9156)
+2022-11-18 15:11:34,689:INFO: Dataset: univ                Batch:  2/15	Loss 1.8924 (1.9036)
+2022-11-18 15:11:34,849:INFO: Dataset: univ                Batch:  3/15	Loss 1.9362 (1.9138)
+2022-11-18 15:11:35,004:INFO: Dataset: univ                Batch:  4/15	Loss 1.9258 (1.9165)
+2022-11-18 15:11:35,165:INFO: Dataset: univ                Batch:  5/15	Loss 1.9059 (1.9144)
+2022-11-18 15:11:35,326:INFO: Dataset: univ                Batch:  6/15	Loss 1.8796 (1.9080)
+2022-11-18 15:11:35,482:INFO: Dataset: univ                Batch:  7/15	Loss 1.8911 (1.9058)
+2022-11-18 15:11:35,637:INFO: Dataset: univ                Batch:  8/15	Loss 1.9042 (1.9056)
+2022-11-18 15:11:35,795:INFO: Dataset: univ                Batch:  9/15	Loss 1.8702 (1.9015)
+2022-11-18 15:11:35,949:INFO: Dataset: univ                Batch: 10/15	Loss 1.8970 (1.9010)
+2022-11-18 15:11:36,110:INFO: Dataset: univ                Batch: 11/15	Loss 1.9294 (1.9038)
+2022-11-18 15:11:36,265:INFO: Dataset: univ                Batch: 12/15	Loss 1.8826 (1.9021)
+2022-11-18 15:11:36,421:INFO: Dataset: univ                Batch: 13/15	Loss 1.8702 (1.8997)
+2022-11-18 15:11:36,577:INFO: Dataset: univ                Batch: 14/15	Loss 1.8903 (1.8990)
+2022-11-18 15:11:36,660:INFO: Dataset: univ                Batch: 15/15	Loss 1.9820 (1.9001)
+2022-11-18 15:11:37,051:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0365 (2.0365)
+2022-11-18 15:11:37,204:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0008 (2.0179)
+2022-11-18 15:11:37,354:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9128 (1.9832)
+2022-11-18 15:11:37,505:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8272 (1.9429)
+2022-11-18 15:11:37,655:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0100 (1.9554)
+2022-11-18 15:11:37,804:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0496 (1.9714)
+2022-11-18 15:11:37,953:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8677 (1.9575)
+2022-11-18 15:11:38,090:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9064 (1.9522)
+2022-11-18 15:11:38,482:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8990 (1.8990)
+2022-11-18 15:11:38,639:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9242 (1.9114)
+2022-11-18 15:11:38,791:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8613 (1.8954)
+2022-11-18 15:11:38,943:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8876 (1.8933)
+2022-11-18 15:11:39,096:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9848 (1.9115)
+2022-11-18 15:11:39,248:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9005 (1.9097)
+2022-11-18 15:11:39,399:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9347 (1.9135)
+2022-11-18 15:11:39,547:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9169 (1.9140)
+2022-11-18 15:11:39,698:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8805 (1.9103)
+2022-11-18 15:11:39,846:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9136 (1.9106)
+2022-11-18 15:11:39,997:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9373 (1.9129)
+2022-11-18 15:11:40,148:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9452 (1.9157)
+2022-11-18 15:11:40,299:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9259 (1.9164)
+2022-11-18 15:11:40,449:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8991 (1.9152)
+2022-11-18 15:11:40,601:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8998 (1.9142)
+2022-11-18 15:11:40,751:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8287 (1.9087)
+2022-11-18 15:11:40,903:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0295 (1.9158)
+2022-11-18 15:11:41,042:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8959 (1.9150)
+2022-11-18 15:11:41,087:INFO: - Computing loss (validation)
+2022-11-18 15:11:41,350:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8558 (1.8558)
+2022-11-18 15:11:41,386:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1402 (1.8761)
+2022-11-18 15:11:41,689:INFO: Dataset: univ                Batch: 1/3	Loss 1.8837 (1.8837)
+2022-11-18 15:11:41,765:INFO: Dataset: univ                Batch: 2/3	Loss 1.9047 (1.8943)
+2022-11-18 15:11:41,838:INFO: Dataset: univ                Batch: 3/3	Loss 1.9304 (1.9077)
+2022-11-18 15:11:42,141:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9708 (1.9708)
+2022-11-18 15:11:42,185:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8767 (1.9475)
+2022-11-18 15:11:42,506:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8819 (1.8819)
+2022-11-18 15:11:42,580:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9606 (1.9219)
+2022-11-18 15:11:42,653:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9621 (1.9349)
+2022-11-18 15:11:42,727:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8627 (1.9165)
+2022-11-18 15:11:42,802:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9179 (1.9168)
+2022-11-18 15:11:42,857:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_330.pth.tar
+2022-11-18 15:11:42,857:INFO: 
+===> EPOCH: 331 (P2)
+2022-11-18 15:11:42,858:INFO: - Computing loss (training)
+2022-11-18 15:11:43,199:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0578 (2.0578)
+2022-11-18 15:11:43,349:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9402 (2.0003)
+2022-11-18 15:11:43,499:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9118 (1.9705)
+2022-11-18 15:11:43,614:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0183 (1.9787)
+2022-11-18 15:11:44,010:INFO: Dataset: univ                Batch:  1/15	Loss 1.9268 (1.9268)
+2022-11-18 15:11:44,165:INFO: Dataset: univ                Batch:  2/15	Loss 1.9099 (1.9189)
+2022-11-18 15:11:44,327:INFO: Dataset: univ                Batch:  3/15	Loss 1.8767 (1.9053)
+2022-11-18 15:11:44,479:INFO: Dataset: univ                Batch:  4/15	Loss 1.8913 (1.9020)
+2022-11-18 15:11:44,635:INFO: Dataset: univ                Batch:  5/15	Loss 1.9046 (1.9025)
+2022-11-18 15:11:44,791:INFO: Dataset: univ                Batch:  6/15	Loss 1.9229 (1.9060)
+2022-11-18 15:11:44,946:INFO: Dataset: univ                Batch:  7/15	Loss 1.8991 (1.9051)
+2022-11-18 15:11:45,099:INFO: Dataset: univ                Batch:  8/15	Loss 1.9248 (1.9076)
+2022-11-18 15:11:45,255:INFO: Dataset: univ                Batch:  9/15	Loss 1.8664 (1.9033)
+2022-11-18 15:11:45,409:INFO: Dataset: univ                Batch: 10/15	Loss 1.9006 (1.9031)
+2022-11-18 15:11:45,565:INFO: Dataset: univ                Batch: 11/15	Loss 1.8881 (1.9017)
+2022-11-18 15:11:45,719:INFO: Dataset: univ                Batch: 12/15	Loss 1.9100 (1.9024)
+2022-11-18 15:11:45,876:INFO: Dataset: univ                Batch: 13/15	Loss 1.9215 (1.9038)
+2022-11-18 15:11:46,031:INFO: Dataset: univ                Batch: 14/15	Loss 1.9668 (1.9086)
+2022-11-18 15:11:46,115:INFO: Dataset: univ                Batch: 15/15	Loss 1.8693 (1.9081)
+2022-11-18 15:11:46,500:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8704 (1.8704)
+2022-11-18 15:11:46,649:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8142 (1.8413)
+2022-11-18 15:11:46,800:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9351 (1.8768)
+2022-11-18 15:11:46,946:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8903 (1.8801)
+2022-11-18 15:11:47,095:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8895 (1.8820)
+2022-11-18 15:11:47,242:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8894 (1.8833)
+2022-11-18 15:11:47,390:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9360 (1.8915)
+2022-11-18 15:11:47,526:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0075 (1.9050)
+2022-11-18 15:11:47,961:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8753 (1.8753)
+2022-11-18 15:11:48,112:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8525 (1.8631)
+2022-11-18 15:11:48,263:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9331 (1.8869)
+2022-11-18 15:11:48,413:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9479 (1.9016)
+2022-11-18 15:11:48,564:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8852 (1.8984)
+2022-11-18 15:11:48,720:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9158 (1.9011)
+2022-11-18 15:11:48,876:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9366 (1.9062)
+2022-11-18 15:11:49,029:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9105 (1.9067)
+2022-11-18 15:11:49,181:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8967 (1.9055)
+2022-11-18 15:11:49,330:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9024 (1.9052)
+2022-11-18 15:11:49,481:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9692 (1.9109)
+2022-11-18 15:11:49,632:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9024 (1.9101)
+2022-11-18 15:11:49,783:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9117 (1.9103)
+2022-11-18 15:11:49,933:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0160 (1.9177)
+2022-11-18 15:11:50,085:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9341 (1.9188)
+2022-11-18 15:11:50,236:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9454 (1.9203)
+2022-11-18 15:11:50,388:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9335 (1.9212)
+2022-11-18 15:11:50,527:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9827 (1.9241)
+2022-11-18 15:11:50,571:INFO: - Computing loss (validation)
+2022-11-18 15:11:50,835:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9783 (1.9783)
+2022-11-18 15:11:50,870:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6455 (1.9555)
+2022-11-18 15:11:51,186:INFO: Dataset: univ                Batch: 1/3	Loss 1.8836 (1.8836)
+2022-11-18 15:11:51,265:INFO: Dataset: univ                Batch: 2/3	Loss 1.9376 (1.9077)
+2022-11-18 15:11:51,342:INFO: Dataset: univ                Batch: 3/3	Loss 1.8634 (1.8934)
+2022-11-18 15:11:51,641:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9549 (1.9549)
+2022-11-18 15:11:51,688:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8567 (1.9294)
+2022-11-18 15:11:52,000:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9060 (1.9060)
+2022-11-18 15:11:52,080:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9609 (1.9320)
+2022-11-18 15:11:52,157:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9581 (1.9405)
+2022-11-18 15:11:52,235:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9373 (1.9396)
+2022-11-18 15:11:52,312:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9319 (1.9382)
+2022-11-18 15:11:52,370:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_331.pth.tar
+2022-11-18 15:11:52,370:INFO: 
+===> EPOCH: 332 (P2)
+2022-11-18 15:11:52,370:INFO: - Computing loss (training)
+2022-11-18 15:11:52,726:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8722 (1.8722)
+2022-11-18 15:11:52,889:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8648 (1.8686)
+2022-11-18 15:11:53,059:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9550 (1.8988)
+2022-11-18 15:11:53,179:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0175 (1.9190)
+2022-11-18 15:11:53,586:INFO: Dataset: univ                Batch:  1/15	Loss 1.9531 (1.9531)
+2022-11-18 15:11:53,744:INFO: Dataset: univ                Batch:  2/15	Loss 1.9487 (1.9509)
+2022-11-18 15:11:53,909:INFO: Dataset: univ                Batch:  3/15	Loss 1.8482 (1.9165)
+2022-11-18 15:11:54,086:INFO: Dataset: univ                Batch:  4/15	Loss 1.8837 (1.9082)
+2022-11-18 15:11:54,273:INFO: Dataset: univ                Batch:  5/15	Loss 1.8987 (1.9061)
+2022-11-18 15:11:54,460:INFO: Dataset: univ                Batch:  6/15	Loss 1.9164 (1.9078)
+2022-11-18 15:11:54,649:INFO: Dataset: univ                Batch:  7/15	Loss 1.9072 (1.9077)
+2022-11-18 15:11:54,872:INFO: Dataset: univ                Batch:  8/15	Loss 1.8942 (1.9062)
+2022-11-18 15:11:55,069:INFO: Dataset: univ                Batch:  9/15	Loss 1.9096 (1.9065)
+2022-11-18 15:11:55,243:INFO: Dataset: univ                Batch: 10/15	Loss 1.8851 (1.9043)
+2022-11-18 15:11:55,407:INFO: Dataset: univ                Batch: 11/15	Loss 1.9427 (1.9081)
+2022-11-18 15:11:55,573:INFO: Dataset: univ                Batch: 12/15	Loss 1.8760 (1.9053)
+2022-11-18 15:11:55,746:INFO: Dataset: univ                Batch: 13/15	Loss 1.9093 (1.9056)
+2022-11-18 15:11:55,933:INFO: Dataset: univ                Batch: 14/15	Loss 1.8845 (1.9040)
+2022-11-18 15:11:56,035:INFO: Dataset: univ                Batch: 15/15	Loss 1.9717 (1.9050)
+2022-11-18 15:11:56,469:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9869 (1.9869)
+2022-11-18 15:11:56,616:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9522 (1.9697)
+2022-11-18 15:11:56,766:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9401 (1.9597)
+2022-11-18 15:11:56,991:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8618 (1.9352)
+2022-11-18 15:11:57,138:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8755 (1.9239)
+2022-11-18 15:11:57,288:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8877 (1.9174)
+2022-11-18 15:11:57,434:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8530 (1.9089)
+2022-11-18 15:11:57,571:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0274 (1.9221)
+2022-11-18 15:11:57,974:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9831 (1.9831)
+2022-11-18 15:11:58,154:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9204 (1.9498)
+2022-11-18 15:11:58,322:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9598 (1.9532)
+2022-11-18 15:11:58,498:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9405 (1.9498)
+2022-11-18 15:11:58,662:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9584 (1.9514)
+2022-11-18 15:11:58,821:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8824 (1.9417)
+2022-11-18 15:11:58,979:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8743 (1.9327)
+2022-11-18 15:11:59,139:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9731 (1.9373)
+2022-11-18 15:11:59,321:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9356 (1.9371)
+2022-11-18 15:11:59,472:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9335 (1.9367)
+2022-11-18 15:11:59,645:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9627 (1.9392)
+2022-11-18 15:11:59,827:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9316 (1.9386)
+2022-11-18 15:11:59,995:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7603 (1.9258)
+2022-11-18 15:12:00,159:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0287 (1.9327)
+2022-11-18 15:12:00,320:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8723 (1.9285)
+2022-11-18 15:12:00,491:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9673 (1.9308)
+2022-11-18 15:12:00,654:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9143 (1.9298)
+2022-11-18 15:12:00,797:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9557 (1.9310)
+2022-11-18 15:12:00,847:INFO: - Computing loss (validation)
+2022-11-18 15:12:01,178:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0434 (2.0434)
+2022-11-18 15:12:01,225:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5076 (2.0141)
+2022-11-18 15:12:01,557:INFO: Dataset: univ                Batch: 1/3	Loss 1.8618 (1.8618)
+2022-11-18 15:12:01,639:INFO: Dataset: univ                Batch: 2/3	Loss 1.8741 (1.8673)
+2022-11-18 15:12:01,715:INFO: Dataset: univ                Batch: 3/3	Loss 1.9426 (1.8903)
+2022-11-18 15:12:02,021:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7826 (1.7826)
+2022-11-18 15:12:02,069:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8710 (1.8053)
+2022-11-18 15:12:02,388:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8528 (1.8528)
+2022-11-18 15:12:02,474:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9958 (1.9235)
+2022-11-18 15:12:02,559:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9619 (1.9362)
+2022-11-18 15:12:02,647:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9545 (1.9407)
+2022-11-18 15:12:02,726:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8574 (1.9221)
+2022-11-18 15:12:02,783:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_332.pth.tar
+2022-11-18 15:12:02,783:INFO: 
+===> EPOCH: 333 (P2)
+2022-11-18 15:12:02,783:INFO: - Computing loss (training)
+2022-11-18 15:12:03,117:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8591 (1.8591)
+2022-11-18 15:12:03,279:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9450 (1.9025)
+2022-11-18 15:12:03,443:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9777 (1.9272)
+2022-11-18 15:12:03,569:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9867 (1.9364)
+2022-11-18 15:12:04,017:INFO: Dataset: univ                Batch:  1/15	Loss 1.8816 (1.8816)
+2022-11-18 15:12:04,179:INFO: Dataset: univ                Batch:  2/15	Loss 1.8995 (1.8906)
+2022-11-18 15:12:04,347:INFO: Dataset: univ                Batch:  3/15	Loss 1.9107 (1.8970)
+2022-11-18 15:12:04,518:INFO: Dataset: univ                Batch:  4/15	Loss 1.9036 (1.8986)
+2022-11-18 15:12:04,693:INFO: Dataset: univ                Batch:  5/15	Loss 1.8770 (1.8944)
+2022-11-18 15:12:04,857:INFO: Dataset: univ                Batch:  6/15	Loss 1.8959 (1.8947)
+2022-11-18 15:12:05,031:INFO: Dataset: univ                Batch:  7/15	Loss 1.9199 (1.8984)
+2022-11-18 15:12:05,198:INFO: Dataset: univ                Batch:  8/15	Loss 1.9462 (1.9048)
+2022-11-18 15:12:05,367:INFO: Dataset: univ                Batch:  9/15	Loss 1.9016 (1.9044)
+2022-11-18 15:12:05,551:INFO: Dataset: univ                Batch: 10/15	Loss 1.9404 (1.9077)
+2022-11-18 15:12:05,713:INFO: Dataset: univ                Batch: 11/15	Loss 1.8932 (1.9065)
+2022-11-18 15:12:05,870:INFO: Dataset: univ                Batch: 12/15	Loss 1.9051 (1.9064)
+2022-11-18 15:12:06,031:INFO: Dataset: univ                Batch: 13/15	Loss 1.8861 (1.9048)
+2022-11-18 15:12:06,190:INFO: Dataset: univ                Batch: 14/15	Loss 1.8715 (1.9026)
+2022-11-18 15:12:06,272:INFO: Dataset: univ                Batch: 15/15	Loss 2.0160 (1.9042)
+2022-11-18 15:12:06,691:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9927 (1.9927)
+2022-11-18 15:12:06,866:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8893 (1.9393)
+2022-11-18 15:12:07,040:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8459 (1.9085)
+2022-11-18 15:12:07,191:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9237 (1.9128)
+2022-11-18 15:12:07,343:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9903 (1.9298)
+2022-11-18 15:12:07,498:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8924 (1.9234)
+2022-11-18 15:12:07,653:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0451 (1.9409)
+2022-11-18 15:12:07,792:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0077 (1.9482)
+2022-11-18 15:12:08,184:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8640 (1.8640)
+2022-11-18 15:12:08,338:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9369 (1.8991)
+2022-11-18 15:12:08,487:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0537 (1.9553)
+2022-11-18 15:12:08,635:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8575 (1.9314)
+2022-11-18 15:12:08,785:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8544 (1.9160)
+2022-11-18 15:12:08,938:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9685 (1.9246)
+2022-11-18 15:12:09,089:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8511 (1.9140)
+2022-11-18 15:12:09,238:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8170 (1.9017)
+2022-11-18 15:12:09,387:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9484 (1.9068)
+2022-11-18 15:12:09,534:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9199 (1.9080)
+2022-11-18 15:12:09,681:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9493 (1.9120)
+2022-11-18 15:12:09,829:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9204 (1.9127)
+2022-11-18 15:12:09,977:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9250 (1.9137)
+2022-11-18 15:12:10,126:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9125 (1.9136)
+2022-11-18 15:12:10,276:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9688 (1.9175)
+2022-11-18 15:12:10,425:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9285 (1.9182)
+2022-11-18 15:12:10,575:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8821 (1.9160)
+2022-11-18 15:12:10,713:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9934 (1.9198)
+2022-11-18 15:12:10,759:INFO: - Computing loss (validation)
+2022-11-18 15:12:11,034:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8445 (1.8445)
+2022-11-18 15:12:11,068:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9218 (1.8493)
+2022-11-18 15:12:11,380:INFO: Dataset: univ                Batch: 1/3	Loss 1.9306 (1.9306)
+2022-11-18 15:12:11,458:INFO: Dataset: univ                Batch: 2/3	Loss 1.8868 (1.9097)
+2022-11-18 15:12:11,535:INFO: Dataset: univ                Batch: 3/3	Loss 1.8793 (1.8996)
+2022-11-18 15:12:11,836:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0518 (2.0518)
+2022-11-18 15:12:11,882:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0098 (2.0410)
+2022-11-18 15:12:12,193:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8378 (1.8378)
+2022-11-18 15:12:12,273:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9224 (1.8818)
+2022-11-18 15:12:12,349:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9030 (1.8887)
+2022-11-18 15:12:12,428:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9309 (1.8984)
+2022-11-18 15:12:12,504:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9146 (1.9017)
+2022-11-18 15:12:12,561:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_333.pth.tar
+2022-11-18 15:12:12,562:INFO: 
+===> EPOCH: 334 (P2)
+2022-11-18 15:12:12,562:INFO: - Computing loss (training)
+2022-11-18 15:12:12,912:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0293 (2.0293)
+2022-11-18 15:12:13,067:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0244 (2.0268)
+2022-11-18 15:12:13,218:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9342 (1.9952)
+2022-11-18 15:12:13,333:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1571 (2.0227)
+2022-11-18 15:12:13,728:INFO: Dataset: univ                Batch:  1/15	Loss 1.9122 (1.9122)
+2022-11-18 15:12:13,887:INFO: Dataset: univ                Batch:  2/15	Loss 1.9463 (1.9293)
+2022-11-18 15:12:14,043:INFO: Dataset: univ                Batch:  3/15	Loss 1.9095 (1.9221)
+2022-11-18 15:12:14,198:INFO: Dataset: univ                Batch:  4/15	Loss 1.9106 (1.9190)
+2022-11-18 15:12:14,357:INFO: Dataset: univ                Batch:  5/15	Loss 1.8749 (1.9102)
+2022-11-18 15:12:14,512:INFO: Dataset: univ                Batch:  6/15	Loss 1.8958 (1.9078)
+2022-11-18 15:12:14,668:INFO: Dataset: univ                Batch:  7/15	Loss 1.9186 (1.9094)
+2022-11-18 15:12:14,821:INFO: Dataset: univ                Batch:  8/15	Loss 1.9610 (1.9157)
+2022-11-18 15:12:14,975:INFO: Dataset: univ                Batch:  9/15	Loss 1.8977 (1.9138)
+2022-11-18 15:12:15,130:INFO: Dataset: univ                Batch: 10/15	Loss 1.8724 (1.9100)
+2022-11-18 15:12:15,287:INFO: Dataset: univ                Batch: 11/15	Loss 1.8863 (1.9076)
+2022-11-18 15:12:15,440:INFO: Dataset: univ                Batch: 12/15	Loss 1.8594 (1.9037)
+2022-11-18 15:12:15,595:INFO: Dataset: univ                Batch: 13/15	Loss 1.9091 (1.9040)
+2022-11-18 15:12:15,751:INFO: Dataset: univ                Batch: 14/15	Loss 1.9126 (1.9046)
+2022-11-18 15:12:15,834:INFO: Dataset: univ                Batch: 15/15	Loss 1.9250 (1.9049)
+2022-11-18 15:12:16,227:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9540 (1.9540)
+2022-11-18 15:12:16,376:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8785 (1.9190)
+2022-11-18 15:12:16,528:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8686 (1.9034)
+2022-11-18 15:12:16,678:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9368 (1.9122)
+2022-11-18 15:12:16,829:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9661 (1.9234)
+2022-11-18 15:12:16,981:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8941 (1.9185)
+2022-11-18 15:12:17,133:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8956 (1.9149)
+2022-11-18 15:12:17,272:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8930 (1.9126)
+2022-11-18 15:12:17,667:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8492 (1.8492)
+2022-11-18 15:12:17,823:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9503 (1.8977)
+2022-11-18 15:12:17,975:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9882 (1.9300)
+2022-11-18 15:12:18,129:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8639 (1.9145)
+2022-11-18 15:12:18,287:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8533 (1.9024)
+2022-11-18 15:12:18,440:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8764 (1.8985)
+2022-11-18 15:12:18,592:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8774 (1.8953)
+2022-11-18 15:12:18,742:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9215 (1.8988)
+2022-11-18 15:12:18,897:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8575 (1.8943)
+2022-11-18 15:12:19,048:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9162 (1.8965)
+2022-11-18 15:12:19,201:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0670 (1.9116)
+2022-11-18 15:12:19,353:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9343 (1.9135)
+2022-11-18 15:12:19,505:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9193 (1.9139)
+2022-11-18 15:12:19,659:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8348 (1.9081)
+2022-11-18 15:12:19,812:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9503 (1.9108)
+2022-11-18 15:12:19,963:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8635 (1.9076)
+2022-11-18 15:12:20,116:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8804 (1.9060)
+2022-11-18 15:12:20,257:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9064 (1.9060)
+2022-11-18 15:12:20,303:INFO: - Computing loss (validation)
+2022-11-18 15:12:20,586:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9007 (1.9007)
+2022-11-18 15:12:20,622:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1326 (1.9173)
+2022-11-18 15:12:20,936:INFO: Dataset: univ                Batch: 1/3	Loss 1.9042 (1.9042)
+2022-11-18 15:12:21,013:INFO: Dataset: univ                Batch: 2/3	Loss 1.8786 (1.8926)
+2022-11-18 15:12:21,085:INFO: Dataset: univ                Batch: 3/3	Loss 1.8852 (1.8903)
+2022-11-18 15:12:21,393:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8286 (1.8286)
+2022-11-18 15:12:21,440:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9156 (1.8493)
+2022-11-18 15:12:21,745:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9686 (1.9686)
+2022-11-18 15:12:21,819:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8668 (1.9186)
+2022-11-18 15:12:21,895:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8755 (1.9039)
+2022-11-18 15:12:21,969:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9567 (1.9169)
+2022-11-18 15:12:22,042:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8561 (1.9049)
+2022-11-18 15:12:22,095:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_334.pth.tar
+2022-11-18 15:12:22,095:INFO: 
+===> EPOCH: 335 (P2)
+2022-11-18 15:12:22,095:INFO: - Computing loss (training)
+2022-11-18 15:12:22,436:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9801 (1.9801)
+2022-11-18 15:12:22,585:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9641 (1.9726)
+2022-11-18 15:12:22,736:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8151 (1.9189)
+2022-11-18 15:12:22,850:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0220 (1.9355)
+2022-11-18 15:12:23,236:INFO: Dataset: univ                Batch:  1/15	Loss 1.8398 (1.8398)
+2022-11-18 15:12:23,394:INFO: Dataset: univ                Batch:  2/15	Loss 1.9563 (1.9010)
+2022-11-18 15:12:23,552:INFO: Dataset: univ                Batch:  3/15	Loss 1.8803 (1.8940)
+2022-11-18 15:12:23,710:INFO: Dataset: univ                Batch:  4/15	Loss 1.9046 (1.8967)
+2022-11-18 15:12:23,868:INFO: Dataset: univ                Batch:  5/15	Loss 1.9136 (1.9001)
+2022-11-18 15:12:24,026:INFO: Dataset: univ                Batch:  6/15	Loss 1.9566 (1.9091)
+2022-11-18 15:12:24,182:INFO: Dataset: univ                Batch:  7/15	Loss 1.9378 (1.9131)
+2022-11-18 15:12:24,336:INFO: Dataset: univ                Batch:  8/15	Loss 1.9064 (1.9122)
+2022-11-18 15:12:24,493:INFO: Dataset: univ                Batch:  9/15	Loss 1.9055 (1.9115)
+2022-11-18 15:12:24,648:INFO: Dataset: univ                Batch: 10/15	Loss 1.9045 (1.9108)
+2022-11-18 15:12:24,804:INFO: Dataset: univ                Batch: 11/15	Loss 1.8776 (1.9080)
+2022-11-18 15:12:24,960:INFO: Dataset: univ                Batch: 12/15	Loss 1.9017 (1.9075)
+2022-11-18 15:12:25,119:INFO: Dataset: univ                Batch: 13/15	Loss 1.8931 (1.9063)
+2022-11-18 15:12:25,276:INFO: Dataset: univ                Batch: 14/15	Loss 1.9262 (1.9077)
+2022-11-18 15:12:25,362:INFO: Dataset: univ                Batch: 15/15	Loss 2.0317 (1.9095)
+2022-11-18 15:12:25,744:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9806 (1.9806)
+2022-11-18 15:12:25,892:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9748 (1.9777)
+2022-11-18 15:12:26,045:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8419 (1.9270)
+2022-11-18 15:12:26,194:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9705 (1.9390)
+2022-11-18 15:12:26,341:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8752 (1.9274)
+2022-11-18 15:12:26,488:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8905 (1.9213)
+2022-11-18 15:12:26,637:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9612 (1.9268)
+2022-11-18 15:12:26,772:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9736 (1.9318)
+2022-11-18 15:12:27,164:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8848 (1.8848)
+2022-11-18 15:12:27,315:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9940 (1.9387)
+2022-11-18 15:12:27,470:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9124 (1.9298)
+2022-11-18 15:12:27,619:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9370 (1.9316)
+2022-11-18 15:12:27,770:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9765 (1.9399)
+2022-11-18 15:12:27,921:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9206 (1.9368)
+2022-11-18 15:12:28,070:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9372 (1.9368)
+2022-11-18 15:12:28,219:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8797 (1.9295)
+2022-11-18 15:12:28,368:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8933 (1.9255)
+2022-11-18 15:12:28,516:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8453 (1.9177)
+2022-11-18 15:12:28,670:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9065 (1.9166)
+2022-11-18 15:12:28,821:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8130 (1.9091)
+2022-11-18 15:12:28,972:INFO: Dataset: zara2               Batch: 13/18	Loss 2.0957 (1.9235)
+2022-11-18 15:12:29,121:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9242 (1.9236)
+2022-11-18 15:12:29,272:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9864 (1.9277)
+2022-11-18 15:12:29,421:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9121 (1.9267)
+2022-11-18 15:12:29,572:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8505 (1.9223)
+2022-11-18 15:12:29,710:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9116 (1.9218)
+2022-11-18 15:12:29,755:INFO: - Computing loss (validation)
+2022-11-18 15:12:30,021:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9700 (1.9700)
+2022-11-18 15:12:30,056:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7723 (1.9572)
+2022-11-18 15:12:30,362:INFO: Dataset: univ                Batch: 1/3	Loss 1.9167 (1.9167)
+2022-11-18 15:12:30,441:INFO: Dataset: univ                Batch: 2/3	Loss 1.9567 (1.9392)
+2022-11-18 15:12:30,516:INFO: Dataset: univ                Batch: 3/3	Loss 1.8894 (1.9215)
+2022-11-18 15:12:30,819:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0563 (2.0563)
+2022-11-18 15:12:30,865:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0540 (2.0557)
+2022-11-18 15:12:31,179:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8670 (1.8670)
+2022-11-18 15:12:31,255:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9329 (1.8993)
+2022-11-18 15:12:31,333:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8756 (1.8913)
+2022-11-18 15:12:31,408:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9295 (1.9001)
+2022-11-18 15:12:31,483:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9640 (1.9123)
+2022-11-18 15:12:31,535:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_335.pth.tar
+2022-11-18 15:12:31,535:INFO: 
+===> EPOCH: 336 (P2)
+2022-11-18 15:12:31,536:INFO: - Computing loss (training)
+2022-11-18 15:12:31,866:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9234 (1.9234)
+2022-11-18 15:12:32,015:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8392 (1.8813)
+2022-11-18 15:12:32,165:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9726 (1.9112)
+2022-11-18 15:12:32,280:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9542 (1.9179)
+2022-11-18 15:12:32,696:INFO: Dataset: univ                Batch:  1/15	Loss 1.9325 (1.9325)
+2022-11-18 15:12:32,858:INFO: Dataset: univ                Batch:  2/15	Loss 1.8883 (1.9111)
+2022-11-18 15:12:33,018:INFO: Dataset: univ                Batch:  3/15	Loss 1.8724 (1.8975)
+2022-11-18 15:12:33,175:INFO: Dataset: univ                Batch:  4/15	Loss 1.9116 (1.9011)
+2022-11-18 15:12:33,335:INFO: Dataset: univ                Batch:  5/15	Loss 1.9292 (1.9060)
+2022-11-18 15:12:33,494:INFO: Dataset: univ                Batch:  6/15	Loss 1.8763 (1.9010)
+2022-11-18 15:12:33,648:INFO: Dataset: univ                Batch:  7/15	Loss 1.8774 (1.8980)
+2022-11-18 15:12:33,802:INFO: Dataset: univ                Batch:  8/15	Loss 1.8846 (1.8964)
+2022-11-18 15:12:33,957:INFO: Dataset: univ                Batch:  9/15	Loss 1.9090 (1.8979)
+2022-11-18 15:12:34,111:INFO: Dataset: univ                Batch: 10/15	Loss 1.9384 (1.9019)
+2022-11-18 15:12:34,268:INFO: Dataset: univ                Batch: 11/15	Loss 1.8931 (1.9011)
+2022-11-18 15:12:34,422:INFO: Dataset: univ                Batch: 12/15	Loss 1.8717 (1.8985)
+2022-11-18 15:12:34,577:INFO: Dataset: univ                Batch: 13/15	Loss 1.9179 (1.8998)
+2022-11-18 15:12:34,733:INFO: Dataset: univ                Batch: 14/15	Loss 1.9023 (1.9000)
+2022-11-18 15:12:34,815:INFO: Dataset: univ                Batch: 15/15	Loss 2.0183 (1.9015)
+2022-11-18 15:12:35,195:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8544 (1.8544)
+2022-11-18 15:12:35,343:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9428 (1.8970)
+2022-11-18 15:12:35,493:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0303 (1.9395)
+2022-11-18 15:12:35,641:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9933 (1.9521)
+2022-11-18 15:12:35,792:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7898 (1.9192)
+2022-11-18 15:12:35,941:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9801 (1.9287)
+2022-11-18 15:12:36,091:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9716 (1.9353)
+2022-11-18 15:12:36,229:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0050 (1.9434)
+2022-11-18 15:12:36,620:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9705 (1.9705)
+2022-11-18 15:12:36,775:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9718 (1.9712)
+2022-11-18 15:12:36,933:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7955 (1.9207)
+2022-11-18 15:12:37,084:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9286 (1.9228)
+2022-11-18 15:12:37,240:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9633 (1.9304)
+2022-11-18 15:12:37,393:INFO: Dataset: zara2               Batch:  6/18	Loss 2.0107 (1.9453)
+2022-11-18 15:12:37,546:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9002 (1.9382)
+2022-11-18 15:12:37,697:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7990 (1.9193)
+2022-11-18 15:12:37,849:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8364 (1.9102)
+2022-11-18 15:12:37,998:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9075 (1.9099)
+2022-11-18 15:12:38,153:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9939 (1.9177)
+2022-11-18 15:12:38,303:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7825 (1.9062)
+2022-11-18 15:12:38,455:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9066 (1.9063)
+2022-11-18 15:12:38,606:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8729 (1.9041)
+2022-11-18 15:12:38,759:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9622 (1.9082)
+2022-11-18 15:12:38,914:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9488 (1.9107)
+2022-11-18 15:12:39,068:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9222 (1.9113)
+2022-11-18 15:12:39,208:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0029 (1.9155)
+2022-11-18 15:12:39,257:INFO: - Computing loss (validation)
+2022-11-18 15:12:39,532:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0396 (2.0396)
+2022-11-18 15:12:39,567:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0477 (2.0402)
+2022-11-18 15:12:39,878:INFO: Dataset: univ                Batch: 1/3	Loss 1.8782 (1.8782)
+2022-11-18 15:12:39,952:INFO: Dataset: univ                Batch: 2/3	Loss 1.9130 (1.8931)
+2022-11-18 15:12:40,023:INFO: Dataset: univ                Batch: 3/3	Loss 1.8201 (1.8703)
+2022-11-18 15:12:40,318:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9019 (1.9019)
+2022-11-18 15:12:40,363:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8985 (1.9011)
+2022-11-18 15:12:40,680:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9081 (1.9081)
+2022-11-18 15:12:40,756:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8825 (1.8949)
+2022-11-18 15:12:40,829:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8325 (1.8751)
+2022-11-18 15:12:40,903:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9997 (1.9058)
+2022-11-18 15:12:40,977:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9111 (1.9068)
+2022-11-18 15:12:41,029:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_336.pth.tar
+2022-11-18 15:12:41,029:INFO: 
+===> EPOCH: 337 (P2)
+2022-11-18 15:12:41,030:INFO: - Computing loss (training)
+2022-11-18 15:12:41,365:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8659 (1.8659)
+2022-11-18 15:12:41,519:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9225 (1.8966)
+2022-11-18 15:12:41,673:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8867 (1.8933)
+2022-11-18 15:12:41,789:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9729 (1.9074)
+2022-11-18 15:12:42,229:INFO: Dataset: univ                Batch:  1/15	Loss 1.8683 (1.8683)
+2022-11-18 15:12:42,386:INFO: Dataset: univ                Batch:  2/15	Loss 1.8798 (1.8743)
+2022-11-18 15:12:42,545:INFO: Dataset: univ                Batch:  3/15	Loss 1.8840 (1.8777)
+2022-11-18 15:12:42,701:INFO: Dataset: univ                Batch:  4/15	Loss 1.9191 (1.8865)
+2022-11-18 15:12:42,859:INFO: Dataset: univ                Batch:  5/15	Loss 1.8895 (1.8871)
+2022-11-18 15:12:43,017:INFO: Dataset: univ                Batch:  6/15	Loss 1.8778 (1.8854)
+2022-11-18 15:12:43,176:INFO: Dataset: univ                Batch:  7/15	Loss 1.9242 (1.8913)
+2022-11-18 15:12:43,330:INFO: Dataset: univ                Batch:  8/15	Loss 1.8866 (1.8908)
+2022-11-18 15:12:43,487:INFO: Dataset: univ                Batch:  9/15	Loss 1.8946 (1.8912)
+2022-11-18 15:12:43,642:INFO: Dataset: univ                Batch: 10/15	Loss 1.9042 (1.8925)
+2022-11-18 15:12:43,799:INFO: Dataset: univ                Batch: 11/15	Loss 1.8563 (1.8889)
+2022-11-18 15:12:43,955:INFO: Dataset: univ                Batch: 12/15	Loss 1.8782 (1.8880)
+2022-11-18 15:12:44,113:INFO: Dataset: univ                Batch: 13/15	Loss 1.9867 (1.8958)
+2022-11-18 15:12:44,269:INFO: Dataset: univ                Batch: 14/15	Loss 1.9078 (1.8967)
+2022-11-18 15:12:44,353:INFO: Dataset: univ                Batch: 15/15	Loss 1.9117 (1.8969)
+2022-11-18 15:12:44,742:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0707 (2.0707)
+2022-11-18 15:12:44,891:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9496 (2.0119)
+2022-11-18 15:12:45,042:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9461 (1.9880)
+2022-11-18 15:12:45,196:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9366 (1.9746)
+2022-11-18 15:12:45,345:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9921 (1.9782)
+2022-11-18 15:12:45,493:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0380 (1.9885)
+2022-11-18 15:12:45,640:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8967 (1.9756)
+2022-11-18 15:12:45,776:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0666 (1.9853)
+2022-11-18 15:12:46,160:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9966 (1.9966)
+2022-11-18 15:12:46,312:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8386 (1.9190)
+2022-11-18 15:12:46,462:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9582 (1.9317)
+2022-11-18 15:12:46,612:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9455 (1.9353)
+2022-11-18 15:12:46,763:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9561 (1.9395)
+2022-11-18 15:12:46,913:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9534 (1.9419)
+2022-11-18 15:12:47,061:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8910 (1.9347)
+2022-11-18 15:12:47,210:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9305 (1.9342)
+2022-11-18 15:12:47,358:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9219 (1.9328)
+2022-11-18 15:12:47,505:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9871 (1.9382)
+2022-11-18 15:12:47,658:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8502 (1.9307)
+2022-11-18 15:12:47,807:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9258 (1.9303)
+2022-11-18 15:12:47,955:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8915 (1.9273)
+2022-11-18 15:12:48,104:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9926 (1.9319)
+2022-11-18 15:12:48,254:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9220 (1.9312)
+2022-11-18 15:12:48,402:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9440 (1.9319)
+2022-11-18 15:12:48,551:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9705 (1.9343)
+2022-11-18 15:12:48,688:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8427 (1.9296)
+2022-11-18 15:12:48,733:INFO: - Computing loss (validation)
+2022-11-18 15:12:48,996:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9535 (1.9535)
+2022-11-18 15:12:49,031:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8152 (1.9441)
+2022-11-18 15:12:49,338:INFO: Dataset: univ                Batch: 1/3	Loss 1.9043 (1.9043)
+2022-11-18 15:12:49,418:INFO: Dataset: univ                Batch: 2/3	Loss 1.8913 (1.8987)
+2022-11-18 15:12:49,498:INFO: Dataset: univ                Batch: 3/3	Loss 1.9123 (1.9037)
+2022-11-18 15:12:49,801:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0142 (2.0142)
+2022-11-18 15:12:49,848:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8503 (1.9741)
+2022-11-18 15:12:50,165:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9202 (1.9202)
+2022-11-18 15:12:50,240:INFO: Dataset: zara2               Batch: 2/5	Loss 2.0101 (1.9672)
+2022-11-18 15:12:50,316:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8373 (1.9248)
+2022-11-18 15:12:50,391:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8790 (1.9131)
+2022-11-18 15:12:50,466:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8826 (1.9069)
+2022-11-18 15:12:50,523:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_337.pth.tar
+2022-11-18 15:12:50,523:INFO: 
+===> EPOCH: 338 (P2)
+2022-11-18 15:12:50,523:INFO: - Computing loss (training)
+2022-11-18 15:12:50,865:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8816 (1.8816)
+2022-11-18 15:12:51,014:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0240 (1.9535)
+2022-11-18 15:12:51,163:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9144 (1.9401)
+2022-11-18 15:12:51,277:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9840 (1.9477)
+2022-11-18 15:12:51,674:INFO: Dataset: univ                Batch:  1/15	Loss 1.9282 (1.9282)
+2022-11-18 15:12:51,830:INFO: Dataset: univ                Batch:  2/15	Loss 1.8975 (1.9121)
+2022-11-18 15:12:51,988:INFO: Dataset: univ                Batch:  3/15	Loss 1.9314 (1.9182)
+2022-11-18 15:12:52,146:INFO: Dataset: univ                Batch:  4/15	Loss 1.8895 (1.9109)
+2022-11-18 15:12:52,304:INFO: Dataset: univ                Batch:  5/15	Loss 1.9745 (1.9233)
+2022-11-18 15:12:52,460:INFO: Dataset: univ                Batch:  6/15	Loss 1.9394 (1.9260)
+2022-11-18 15:12:52,616:INFO: Dataset: univ                Batch:  7/15	Loss 1.9554 (1.9305)
+2022-11-18 15:12:52,771:INFO: Dataset: univ                Batch:  8/15	Loss 1.8604 (1.9215)
+2022-11-18 15:12:52,926:INFO: Dataset: univ                Batch:  9/15	Loss 1.8938 (1.9181)
+2022-11-18 15:12:53,079:INFO: Dataset: univ                Batch: 10/15	Loss 1.9159 (1.9179)
+2022-11-18 15:12:53,235:INFO: Dataset: univ                Batch: 11/15	Loss 1.9020 (1.9165)
+2022-11-18 15:12:53,390:INFO: Dataset: univ                Batch: 12/15	Loss 1.9028 (1.9153)
+2022-11-18 15:12:53,545:INFO: Dataset: univ                Batch: 13/15	Loss 1.9073 (1.9146)
+2022-11-18 15:12:53,702:INFO: Dataset: univ                Batch: 14/15	Loss 1.8816 (1.9121)
+2022-11-18 15:12:53,787:INFO: Dataset: univ                Batch: 15/15	Loss 1.7824 (1.9103)
+2022-11-18 15:12:54,176:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9102 (1.9102)
+2022-11-18 15:12:54,325:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0235 (1.9670)
+2022-11-18 15:12:54,478:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9113 (1.9481)
+2022-11-18 15:12:54,625:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9252 (1.9425)
+2022-11-18 15:12:54,776:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9141 (1.9371)
+2022-11-18 15:12:54,924:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9545 (1.9401)
+2022-11-18 15:12:55,073:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0016 (1.9483)
+2022-11-18 15:12:55,209:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9128 (1.9449)
+2022-11-18 15:12:55,618:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9447 (1.9447)
+2022-11-18 15:12:55,769:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8237 (1.8869)
+2022-11-18 15:12:55,927:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8959 (1.8898)
+2022-11-18 15:12:56,076:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9491 (1.9052)
+2022-11-18 15:12:56,228:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8794 (1.9000)
+2022-11-18 15:12:56,381:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9735 (1.9123)
+2022-11-18 15:12:56,532:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8655 (1.9057)
+2022-11-18 15:12:56,681:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8442 (1.8974)
+2022-11-18 15:12:56,834:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8187 (1.8894)
+2022-11-18 15:12:56,983:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8539 (1.8859)
+2022-11-18 15:12:57,134:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8630 (1.8841)
+2022-11-18 15:12:57,285:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9354 (1.8889)
+2022-11-18 15:12:57,436:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8447 (1.8855)
+2022-11-18 15:12:57,586:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9438 (1.8896)
+2022-11-18 15:12:57,739:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9591 (1.8937)
+2022-11-18 15:12:57,889:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9211 (1.8955)
+2022-11-18 15:12:58,041:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8973 (1.8956)
+2022-11-18 15:12:58,180:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8767 (1.8947)
+2022-11-18 15:12:58,225:INFO: - Computing loss (validation)
+2022-11-18 15:12:58,495:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9470 (1.9470)
+2022-11-18 15:12:58,531:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0102 (1.9511)
+2022-11-18 15:12:58,834:INFO: Dataset: univ                Batch: 1/3	Loss 1.8516 (1.8516)
+2022-11-18 15:12:58,912:INFO: Dataset: univ                Batch: 2/3	Loss 1.8858 (1.8688)
+2022-11-18 15:12:58,985:INFO: Dataset: univ                Batch: 3/3	Loss 1.8965 (1.8780)
+2022-11-18 15:12:59,284:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8774 (1.8774)
+2022-11-18 15:12:59,329:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8322 (1.8668)
+2022-11-18 15:12:59,631:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9168 (1.9168)
+2022-11-18 15:12:59,709:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8965 (1.9071)
+2022-11-18 15:12:59,787:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9469 (1.9198)
+2022-11-18 15:12:59,866:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8641 (1.9059)
+2022-11-18 15:12:59,941:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9456 (1.9138)
+2022-11-18 15:12:59,994:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_338.pth.tar
+2022-11-18 15:12:59,994:INFO: 
+===> EPOCH: 339 (P2)
+2022-11-18 15:12:59,995:INFO: - Computing loss (training)
+2022-11-18 15:13:00,338:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8359 (1.8359)
+2022-11-18 15:13:00,491:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8968 (1.8670)
+2022-11-18 15:13:00,645:INFO: Dataset: hotel               Batch: 3/4	Loss 2.1015 (1.9428)
+2022-11-18 15:13:00,762:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8127 (1.9205)
+2022-11-18 15:13:01,157:INFO: Dataset: univ                Batch:  1/15	Loss 1.8808 (1.8808)
+2022-11-18 15:13:01,316:INFO: Dataset: univ                Batch:  2/15	Loss 1.8936 (1.8870)
+2022-11-18 15:13:01,476:INFO: Dataset: univ                Batch:  3/15	Loss 1.9477 (1.9067)
+2022-11-18 15:13:01,629:INFO: Dataset: univ                Batch:  4/15	Loss 1.8627 (1.8963)
+2022-11-18 15:13:01,783:INFO: Dataset: univ                Batch:  5/15	Loss 1.8822 (1.8933)
+2022-11-18 15:13:01,939:INFO: Dataset: univ                Batch:  6/15	Loss 1.8676 (1.8887)
+2022-11-18 15:13:02,094:INFO: Dataset: univ                Batch:  7/15	Loss 1.9249 (1.8941)
+2022-11-18 15:13:02,250:INFO: Dataset: univ                Batch:  8/15	Loss 1.9051 (1.8956)
+2022-11-18 15:13:02,404:INFO: Dataset: univ                Batch:  9/15	Loss 1.9075 (1.8971)
+2022-11-18 15:13:02,558:INFO: Dataset: univ                Batch: 10/15	Loss 1.9354 (1.9011)
+2022-11-18 15:13:02,718:INFO: Dataset: univ                Batch: 11/15	Loss 1.8435 (1.8963)
+2022-11-18 15:13:02,877:INFO: Dataset: univ                Batch: 12/15	Loss 1.9517 (1.9013)
+2022-11-18 15:13:03,032:INFO: Dataset: univ                Batch: 13/15	Loss 1.9173 (1.9027)
+2022-11-18 15:13:03,188:INFO: Dataset: univ                Batch: 14/15	Loss 1.8778 (1.9008)
+2022-11-18 15:13:03,271:INFO: Dataset: univ                Batch: 15/15	Loss 1.9109 (1.9009)
+2022-11-18 15:13:03,667:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9292 (1.9292)
+2022-11-18 15:13:03,817:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9000 (1.9174)
+2022-11-18 15:13:03,970:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7497 (1.8642)
+2022-11-18 15:13:04,122:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9170 (1.8770)
+2022-11-18 15:13:04,275:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8890 (1.8797)
+2022-11-18 15:13:04,426:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9029 (1.8835)
+2022-11-18 15:13:04,576:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9371 (1.8906)
+2022-11-18 15:13:04,714:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9841 (1.9012)
+2022-11-18 15:13:05,129:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8759 (1.8759)
+2022-11-18 15:13:05,281:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9420 (1.9086)
+2022-11-18 15:13:05,443:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9655 (1.9271)
+2022-11-18 15:13:05,599:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9390 (1.9301)
+2022-11-18 15:13:05,752:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8991 (1.9242)
+2022-11-18 15:13:05,905:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7673 (1.8993)
+2022-11-18 15:13:06,056:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8204 (1.8882)
+2022-11-18 15:13:06,208:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9394 (1.8941)
+2022-11-18 15:13:06,359:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8963 (1.8944)
+2022-11-18 15:13:06,508:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8417 (1.8889)
+2022-11-18 15:13:06,661:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9882 (1.8978)
+2022-11-18 15:13:06,811:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8937 (1.8974)
+2022-11-18 15:13:06,962:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8834 (1.8964)
+2022-11-18 15:13:07,117:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9825 (1.9023)
+2022-11-18 15:13:07,293:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9191 (1.9035)
+2022-11-18 15:13:07,445:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9989 (1.9093)
+2022-11-18 15:13:07,598:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8376 (1.9052)
+2022-11-18 15:13:07,739:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0532 (1.9123)
+2022-11-18 15:13:07,784:INFO: - Computing loss (validation)
+2022-11-18 15:13:08,052:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9320 (1.9320)
+2022-11-18 15:13:08,086:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2002 (1.9494)
+2022-11-18 15:13:08,422:INFO: Dataset: univ                Batch: 1/3	Loss 1.8664 (1.8664)
+2022-11-18 15:13:08,509:INFO: Dataset: univ                Batch: 2/3	Loss 1.8962 (1.8815)
+2022-11-18 15:13:08,591:INFO: Dataset: univ                Batch: 3/3	Loss 1.9375 (1.8992)
+2022-11-18 15:13:08,913:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9355 (1.9355)
+2022-11-18 15:13:08,960:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8173 (1.9070)
+2022-11-18 15:13:09,271:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9329 (1.9329)
+2022-11-18 15:13:09,349:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9680 (1.9501)
+2022-11-18 15:13:09,424:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9555 (1.9520)
+2022-11-18 15:13:09,501:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9209 (1.9440)
+2022-11-18 15:13:09,575:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8737 (1.9310)
+2022-11-18 15:13:09,627:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_339.pth.tar
+2022-11-18 15:13:09,627:INFO: 
+===> EPOCH: 340 (P2)
+2022-11-18 15:13:09,628:INFO: - Computing loss (training)
+2022-11-18 15:13:09,974:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0255 (2.0255)
+2022-11-18 15:13:10,209:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8666 (1.9434)
+2022-11-18 15:13:10,358:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9455 (1.9441)
+2022-11-18 15:13:10,476:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1627 (1.9819)
+2022-11-18 15:13:10,873:INFO: Dataset: univ                Batch:  1/15	Loss 1.8872 (1.8872)
+2022-11-18 15:13:11,031:INFO: Dataset: univ                Batch:  2/15	Loss 1.8798 (1.8835)
+2022-11-18 15:13:11,191:INFO: Dataset: univ                Batch:  3/15	Loss 1.9156 (1.8940)
+2022-11-18 15:13:11,351:INFO: Dataset: univ                Batch:  4/15	Loss 1.9087 (1.8976)
+2022-11-18 15:13:11,506:INFO: Dataset: univ                Batch:  5/15	Loss 1.8739 (1.8928)
+2022-11-18 15:13:11,662:INFO: Dataset: univ                Batch:  6/15	Loss 1.9127 (1.8961)
+2022-11-18 15:13:11,818:INFO: Dataset: univ                Batch:  7/15	Loss 1.8993 (1.8965)
+2022-11-18 15:13:11,971:INFO: Dataset: univ                Batch:  8/15	Loss 1.9077 (1.8980)
+2022-11-18 15:13:12,126:INFO: Dataset: univ                Batch:  9/15	Loss 1.8948 (1.8976)
+2022-11-18 15:13:12,281:INFO: Dataset: univ                Batch: 10/15	Loss 1.9279 (1.9004)
+2022-11-18 15:13:12,439:INFO: Dataset: univ                Batch: 11/15	Loss 1.9572 (1.9058)
+2022-11-18 15:13:12,594:INFO: Dataset: univ                Batch: 12/15	Loss 1.8890 (1.9043)
+2022-11-18 15:13:12,751:INFO: Dataset: univ                Batch: 13/15	Loss 1.8665 (1.9012)
+2022-11-18 15:13:12,906:INFO: Dataset: univ                Batch: 14/15	Loss 1.9325 (1.9034)
+2022-11-18 15:13:12,991:INFO: Dataset: univ                Batch: 15/15	Loss 1.8370 (1.9025)
+2022-11-18 15:13:13,376:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9096 (1.9096)
+2022-11-18 15:13:13,523:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8605 (1.8856)
+2022-11-18 15:13:13,671:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9108 (1.8947)
+2022-11-18 15:13:13,824:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9180 (1.9005)
+2022-11-18 15:13:13,972:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9845 (1.9164)
+2022-11-18 15:13:14,123:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8746 (1.9086)
+2022-11-18 15:13:14,272:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8800 (1.9042)
+2022-11-18 15:13:14,406:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7787 (1.8922)
+2022-11-18 15:13:14,785:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8960 (1.8960)
+2022-11-18 15:13:14,936:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9558 (1.9263)
+2022-11-18 15:13:15,090:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9019 (1.9182)
+2022-11-18 15:13:15,243:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8552 (1.9038)
+2022-11-18 15:13:15,395:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7482 (1.8753)
+2022-11-18 15:13:15,546:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8928 (1.8780)
+2022-11-18 15:13:15,697:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9971 (1.8961)
+2022-11-18 15:13:15,846:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9325 (1.9005)
+2022-11-18 15:13:15,995:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9266 (1.9036)
+2022-11-18 15:13:16,146:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8991 (1.9032)
+2022-11-18 15:13:16,298:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8842 (1.9014)
+2022-11-18 15:13:16,449:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9452 (1.9050)
+2022-11-18 15:13:16,599:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9553 (1.9090)
+2022-11-18 15:13:16,752:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8580 (1.9056)
+2022-11-18 15:13:16,905:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8566 (1.9027)
+2022-11-18 15:13:17,053:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8760 (1.9011)
+2022-11-18 15:13:17,204:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8138 (1.8959)
+2022-11-18 15:13:17,342:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9001 (1.8961)
+2022-11-18 15:13:17,386:INFO: - Computing loss (validation)
+2022-11-18 15:13:17,659:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9610 (1.9610)
+2022-11-18 15:13:17,694:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7711 (1.9512)
+2022-11-18 15:13:17,988:INFO: Dataset: univ                Batch: 1/3	Loss 1.8821 (1.8821)
+2022-11-18 15:13:18,065:INFO: Dataset: univ                Batch: 2/3	Loss 1.8902 (1.8862)
+2022-11-18 15:13:18,139:INFO: Dataset: univ                Batch: 3/3	Loss 1.9256 (1.8997)
+2022-11-18 15:13:18,440:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8792 (1.8792)
+2022-11-18 15:13:18,485:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8902 (1.8819)
+2022-11-18 15:13:18,790:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9957 (1.9957)
+2022-11-18 15:13:18,871:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8796 (1.9369)
+2022-11-18 15:13:18,947:INFO: Dataset: zara2               Batch: 3/5	Loss 2.0299 (1.9663)
+2022-11-18 15:13:19,022:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8091 (1.9245)
+2022-11-18 15:13:19,097:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8899 (1.9182)
+2022-11-18 15:13:19,151:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_340.pth.tar
+2022-11-18 15:13:19,151:INFO: 
+===> EPOCH: 341 (P2)
+2022-11-18 15:13:19,151:INFO: - Computing loss (training)
+2022-11-18 15:13:19,496:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0004 (2.0004)
+2022-11-18 15:13:19,647:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9017 (1.9540)
+2022-11-18 15:13:19,800:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0311 (1.9793)
+2022-11-18 15:13:19,917:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7628 (1.9430)
+2022-11-18 15:13:20,320:INFO: Dataset: univ                Batch:  1/15	Loss 1.9191 (1.9191)
+2022-11-18 15:13:20,480:INFO: Dataset: univ                Batch:  2/15	Loss 1.9471 (1.9334)
+2022-11-18 15:13:20,641:INFO: Dataset: univ                Batch:  3/15	Loss 1.8962 (1.9207)
+2022-11-18 15:13:20,797:INFO: Dataset: univ                Batch:  4/15	Loss 1.9082 (1.9177)
+2022-11-18 15:13:20,958:INFO: Dataset: univ                Batch:  5/15	Loss 1.8923 (1.9128)
+2022-11-18 15:13:21,117:INFO: Dataset: univ                Batch:  6/15	Loss 1.8971 (1.9102)
+2022-11-18 15:13:21,274:INFO: Dataset: univ                Batch:  7/15	Loss 1.9029 (1.9092)
+2022-11-18 15:13:21,430:INFO: Dataset: univ                Batch:  8/15	Loss 1.9303 (1.9119)
+2022-11-18 15:13:21,588:INFO: Dataset: univ                Batch:  9/15	Loss 1.9254 (1.9134)
+2022-11-18 15:13:21,744:INFO: Dataset: univ                Batch: 10/15	Loss 1.9756 (1.9201)
+2022-11-18 15:13:21,900:INFO: Dataset: univ                Batch: 11/15	Loss 1.8996 (1.9183)
+2022-11-18 15:13:22,058:INFO: Dataset: univ                Batch: 12/15	Loss 1.9015 (1.9168)
+2022-11-18 15:13:22,216:INFO: Dataset: univ                Batch: 13/15	Loss 1.8774 (1.9137)
+2022-11-18 15:13:22,373:INFO: Dataset: univ                Batch: 14/15	Loss 1.9003 (1.9129)
+2022-11-18 15:13:22,456:INFO: Dataset: univ                Batch: 15/15	Loss 1.9394 (1.9132)
+2022-11-18 15:13:22,858:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9690 (1.9690)
+2022-11-18 15:13:23,007:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7666 (1.8737)
+2022-11-18 15:13:23,159:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9155 (1.8878)
+2022-11-18 15:13:23,310:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9229 (1.8954)
+2022-11-18 15:13:23,463:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9405 (1.9043)
+2022-11-18 15:13:23,614:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8674 (1.8984)
+2022-11-18 15:13:23,765:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9500 (1.9059)
+2022-11-18 15:13:23,902:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9019 (1.9054)
+2022-11-18 15:13:24,289:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9394 (1.9394)
+2022-11-18 15:13:24,447:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8571 (1.8979)
+2022-11-18 15:13:24,599:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9648 (1.9201)
+2022-11-18 15:13:24,750:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8939 (1.9143)
+2022-11-18 15:13:24,902:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8896 (1.9090)
+2022-11-18 15:13:25,054:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9554 (1.9170)
+2022-11-18 15:13:25,207:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8442 (1.9065)
+2022-11-18 15:13:25,357:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8534 (1.9002)
+2022-11-18 15:13:25,508:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8346 (1.8927)
+2022-11-18 15:13:25,657:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8675 (1.8903)
+2022-11-18 15:13:25,808:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9451 (1.8945)
+2022-11-18 15:13:25,959:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0218 (1.9040)
+2022-11-18 15:13:26,111:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9439 (1.9070)
+2022-11-18 15:13:26,262:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9659 (1.9110)
+2022-11-18 15:13:26,415:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0442 (1.9187)
+2022-11-18 15:13:26,566:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8690 (1.9154)
+2022-11-18 15:13:26,719:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8633 (1.9124)
+2022-11-18 15:13:26,858:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9379 (1.9136)
+2022-11-18 15:13:26,903:INFO: - Computing loss (validation)
+2022-11-18 15:13:27,167:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9805 (1.9805)
+2022-11-18 15:13:27,203:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9386 (1.9775)
+2022-11-18 15:13:27,510:INFO: Dataset: univ                Batch: 1/3	Loss 1.8457 (1.8457)
+2022-11-18 15:13:27,588:INFO: Dataset: univ                Batch: 2/3	Loss 1.9343 (1.8921)
+2022-11-18 15:13:27,658:INFO: Dataset: univ                Batch: 3/3	Loss 1.8833 (1.8895)
+2022-11-18 15:13:27,961:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9885 (1.9885)
+2022-11-18 15:13:28,007:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7244 (1.9248)
+2022-11-18 15:13:28,323:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9567 (1.9567)
+2022-11-18 15:13:28,398:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9041 (1.9308)
+2022-11-18 15:13:28,475:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8535 (1.9065)
+2022-11-18 15:13:28,550:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8398 (1.8895)
+2022-11-18 15:13:28,625:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7973 (1.8702)
+2022-11-18 15:13:28,691:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_341.pth.tar
+2022-11-18 15:13:28,692:INFO: 
+===> EPOCH: 342 (P2)
+2022-11-18 15:13:28,692:INFO: - Computing loss (training)
+2022-11-18 15:13:29,038:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9954 (1.9954)
+2022-11-18 15:13:29,186:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8222 (1.9078)
+2022-11-18 15:13:29,335:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9428 (1.9196)
+2022-11-18 15:13:29,448:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1954 (1.9621)
+2022-11-18 15:13:29,841:INFO: Dataset: univ                Batch:  1/15	Loss 1.9607 (1.9607)
+2022-11-18 15:13:29,999:INFO: Dataset: univ                Batch:  2/15	Loss 1.9773 (1.9693)
+2022-11-18 15:13:30,159:INFO: Dataset: univ                Batch:  3/15	Loss 1.8766 (1.9382)
+2022-11-18 15:13:30,318:INFO: Dataset: univ                Batch:  4/15	Loss 1.8986 (1.9282)
+2022-11-18 15:13:30,476:INFO: Dataset: univ                Batch:  5/15	Loss 1.9049 (1.9233)
+2022-11-18 15:13:30,631:INFO: Dataset: univ                Batch:  6/15	Loss 1.8906 (1.9180)
+2022-11-18 15:13:30,786:INFO: Dataset: univ                Batch:  7/15	Loss 1.9023 (1.9162)
+2022-11-18 15:13:30,939:INFO: Dataset: univ                Batch:  8/15	Loss 1.9455 (1.9196)
+2022-11-18 15:13:31,095:INFO: Dataset: univ                Batch:  9/15	Loss 1.9525 (1.9230)
+2022-11-18 15:13:31,251:INFO: Dataset: univ                Batch: 10/15	Loss 1.9051 (1.9209)
+2022-11-18 15:13:31,409:INFO: Dataset: univ                Batch: 11/15	Loss 1.8881 (1.9178)
+2022-11-18 15:13:31,564:INFO: Dataset: univ                Batch: 12/15	Loss 1.8899 (1.9154)
+2022-11-18 15:13:31,719:INFO: Dataset: univ                Batch: 13/15	Loss 1.8690 (1.9119)
+2022-11-18 15:13:31,875:INFO: Dataset: univ                Batch: 14/15	Loss 1.8797 (1.9096)
+2022-11-18 15:13:31,957:INFO: Dataset: univ                Batch: 15/15	Loss 1.8559 (1.9089)
+2022-11-18 15:13:32,342:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9667 (1.9667)
+2022-11-18 15:13:32,494:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8608 (1.9093)
+2022-11-18 15:13:32,647:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8261 (1.8778)
+2022-11-18 15:13:32,795:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8423 (1.8699)
+2022-11-18 15:13:32,944:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9978 (1.8966)
+2022-11-18 15:13:33,094:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9183 (1.8997)
+2022-11-18 15:13:33,244:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8835 (1.8973)
+2022-11-18 15:13:33,379:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8788 (1.8951)
+2022-11-18 15:13:33,769:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8986 (1.8986)
+2022-11-18 15:13:33,919:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8937 (1.8962)
+2022-11-18 15:13:34,071:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8717 (1.8885)
+2022-11-18 15:13:34,223:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9608 (1.9078)
+2022-11-18 15:13:34,379:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9467 (1.9154)
+2022-11-18 15:13:34,532:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9351 (1.9188)
+2022-11-18 15:13:34,682:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8510 (1.9094)
+2022-11-18 15:13:34,832:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8603 (1.9039)
+2022-11-18 15:13:34,982:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8423 (1.8970)
+2022-11-18 15:13:35,132:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9454 (1.9019)
+2022-11-18 15:13:35,286:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9591 (1.9072)
+2022-11-18 15:13:35,434:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9456 (1.9103)
+2022-11-18 15:13:35,584:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8576 (1.9064)
+2022-11-18 15:13:35,736:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9933 (1.9121)
+2022-11-18 15:13:35,888:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9061 (1.9116)
+2022-11-18 15:13:36,037:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7931 (1.9033)
+2022-11-18 15:13:36,190:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8953 (1.9028)
+2022-11-18 15:13:36,329:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9202 (1.9036)
+2022-11-18 15:13:36,376:INFO: - Computing loss (validation)
+2022-11-18 15:13:36,639:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9823 (1.9823)
+2022-11-18 15:13:36,672:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8347 (1.9708)
+2022-11-18 15:13:36,979:INFO: Dataset: univ                Batch: 1/3	Loss 1.8579 (1.8579)
+2022-11-18 15:13:37,056:INFO: Dataset: univ                Batch: 2/3	Loss 1.9002 (1.8802)
+2022-11-18 15:13:37,129:INFO: Dataset: univ                Batch: 3/3	Loss 1.9352 (1.8970)
+2022-11-18 15:13:37,434:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9141 (1.9141)
+2022-11-18 15:13:37,479:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8594 (1.9007)
+2022-11-18 15:13:37,796:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9194 (1.9194)
+2022-11-18 15:13:37,875:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8701 (1.8956)
+2022-11-18 15:13:37,954:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9138 (1.9016)
+2022-11-18 15:13:38,032:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8765 (1.8946)
+2022-11-18 15:13:38,109:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8848 (1.8928)
+2022-11-18 15:13:38,168:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_342.pth.tar
+2022-11-18 15:13:38,168:INFO: 
+===> EPOCH: 343 (P2)
+2022-11-18 15:13:38,169:INFO: - Computing loss (training)
+2022-11-18 15:13:38,508:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9460 (1.9460)
+2022-11-18 15:13:38,659:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9858 (1.9667)
+2022-11-18 15:13:38,811:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9141 (1.9496)
+2022-11-18 15:13:38,932:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9176 (1.9443)
+2022-11-18 15:13:39,330:INFO: Dataset: univ                Batch:  1/15	Loss 1.8837 (1.8837)
+2022-11-18 15:13:39,485:INFO: Dataset: univ                Batch:  2/15	Loss 1.9250 (1.9037)
+2022-11-18 15:13:39,646:INFO: Dataset: univ                Batch:  3/15	Loss 1.8912 (1.8992)
+2022-11-18 15:13:39,799:INFO: Dataset: univ                Batch:  4/15	Loss 1.8790 (1.8946)
+2022-11-18 15:13:39,954:INFO: Dataset: univ                Batch:  5/15	Loss 1.8365 (1.8827)
+2022-11-18 15:13:40,110:INFO: Dataset: univ                Batch:  6/15	Loss 1.8502 (1.8773)
+2022-11-18 15:13:40,266:INFO: Dataset: univ                Batch:  7/15	Loss 1.9470 (1.8869)
+2022-11-18 15:13:40,419:INFO: Dataset: univ                Batch:  8/15	Loss 1.9163 (1.8905)
+2022-11-18 15:13:40,573:INFO: Dataset: univ                Batch:  9/15	Loss 1.8516 (1.8864)
+2022-11-18 15:13:40,727:INFO: Dataset: univ                Batch: 10/15	Loss 1.8973 (1.8875)
+2022-11-18 15:13:40,885:INFO: Dataset: univ                Batch: 11/15	Loss 1.8897 (1.8877)
+2022-11-18 15:13:41,037:INFO: Dataset: univ                Batch: 12/15	Loss 1.9374 (1.8917)
+2022-11-18 15:13:41,192:INFO: Dataset: univ                Batch: 13/15	Loss 1.9132 (1.8933)
+2022-11-18 15:13:41,348:INFO: Dataset: univ                Batch: 14/15	Loss 1.8949 (1.8934)
+2022-11-18 15:13:41,430:INFO: Dataset: univ                Batch: 15/15	Loss 1.8273 (1.8925)
+2022-11-18 15:13:41,813:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9660 (1.9660)
+2022-11-18 15:13:41,961:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9441 (1.9553)
+2022-11-18 15:13:42,112:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9340 (1.9487)
+2022-11-18 15:13:42,259:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8095 (1.9128)
+2022-11-18 15:13:42,406:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0696 (1.9415)
+2022-11-18 15:13:42,554:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9779 (1.9483)
+2022-11-18 15:13:42,702:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9140 (1.9430)
+2022-11-18 15:13:42,837:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9421 (1.9429)
+2022-11-18 15:13:43,223:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8318 (1.8318)
+2022-11-18 15:13:43,373:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9039 (1.8650)
+2022-11-18 15:13:43,524:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0052 (1.9092)
+2022-11-18 15:13:43,676:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8941 (1.9053)
+2022-11-18 15:13:43,829:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8942 (1.9032)
+2022-11-18 15:13:43,980:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8314 (1.8906)
+2022-11-18 15:13:44,131:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8309 (1.8830)
+2022-11-18 15:13:44,280:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8563 (1.8799)
+2022-11-18 15:13:44,429:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9983 (1.8926)
+2022-11-18 15:13:44,577:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8995 (1.8934)
+2022-11-18 15:13:44,728:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0681 (1.9095)
+2022-11-18 15:13:44,877:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9061 (1.9092)
+2022-11-18 15:13:45,026:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9106 (1.9093)
+2022-11-18 15:13:45,176:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0630 (1.9204)
+2022-11-18 15:13:45,327:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8971 (1.9191)
+2022-11-18 15:13:45,476:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9711 (1.9226)
+2022-11-18 15:13:45,627:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8843 (1.9205)
+2022-11-18 15:13:45,765:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9504 (1.9220)
+2022-11-18 15:13:45,811:INFO: - Computing loss (validation)
+2022-11-18 15:13:46,065:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9859 (1.9859)
+2022-11-18 15:13:46,098:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7532 (1.9645)
+2022-11-18 15:13:46,405:INFO: Dataset: univ                Batch: 1/3	Loss 1.8909 (1.8909)
+2022-11-18 15:13:46,483:INFO: Dataset: univ                Batch: 2/3	Loss 1.9251 (1.9073)
+2022-11-18 15:13:46,556:INFO: Dataset: univ                Batch: 3/3	Loss 1.8617 (1.8949)
+2022-11-18 15:13:46,858:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0149 (2.0149)
+2022-11-18 15:13:46,903:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8140 (1.9639)
+2022-11-18 15:13:47,206:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8457 (1.8457)
+2022-11-18 15:13:47,282:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9050 (1.8770)
+2022-11-18 15:13:47,357:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9098 (1.8877)
+2022-11-18 15:13:47,434:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9419 (1.9011)
+2022-11-18 15:13:47,507:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8037 (1.8820)
+2022-11-18 15:13:47,570:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_343.pth.tar
+2022-11-18 15:13:47,570:INFO: 
+===> EPOCH: 344 (P2)
+2022-11-18 15:13:47,570:INFO: - Computing loss (training)
+2022-11-18 15:13:47,914:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8890 (1.8890)
+2022-11-18 15:13:48,065:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7573 (1.8236)
+2022-11-18 15:13:48,218:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9484 (1.8689)
+2022-11-18 15:13:48,333:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9286 (1.8788)
+2022-11-18 15:13:48,738:INFO: Dataset: univ                Batch:  1/15	Loss 1.9170 (1.9170)
+2022-11-18 15:13:48,903:INFO: Dataset: univ                Batch:  2/15	Loss 1.8627 (1.8875)
+2022-11-18 15:13:49,066:INFO: Dataset: univ                Batch:  3/15	Loss 1.8950 (1.8900)
+2022-11-18 15:13:49,223:INFO: Dataset: univ                Batch:  4/15	Loss 1.9246 (1.8981)
+2022-11-18 15:13:49,381:INFO: Dataset: univ                Batch:  5/15	Loss 1.9802 (1.9131)
+2022-11-18 15:13:49,540:INFO: Dataset: univ                Batch:  6/15	Loss 1.9277 (1.9156)
+2022-11-18 15:13:49,697:INFO: Dataset: univ                Batch:  7/15	Loss 1.9406 (1.9190)
+2022-11-18 15:13:49,854:INFO: Dataset: univ                Batch:  8/15	Loss 1.9023 (1.9169)
+2022-11-18 15:13:50,011:INFO: Dataset: univ                Batch:  9/15	Loss 1.9641 (1.9219)
+2022-11-18 15:13:50,168:INFO: Dataset: univ                Batch: 10/15	Loss 1.9102 (1.9207)
+2022-11-18 15:13:50,325:INFO: Dataset: univ                Batch: 11/15	Loss 1.8904 (1.9182)
+2022-11-18 15:13:50,483:INFO: Dataset: univ                Batch: 12/15	Loss 1.8975 (1.9164)
+2022-11-18 15:13:50,640:INFO: Dataset: univ                Batch: 13/15	Loss 1.8848 (1.9141)
+2022-11-18 15:13:50,798:INFO: Dataset: univ                Batch: 14/15	Loss 1.9095 (1.9138)
+2022-11-18 15:13:50,882:INFO: Dataset: univ                Batch: 15/15	Loss 2.0191 (1.9151)
+2022-11-18 15:13:51,283:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8905 (1.8905)
+2022-11-18 15:13:51,433:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8784 (1.8846)
+2022-11-18 15:13:51,583:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8978 (1.8887)
+2022-11-18 15:13:51,735:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9562 (1.9052)
+2022-11-18 15:13:51,885:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8573 (1.8950)
+2022-11-18 15:13:52,033:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9824 (1.9092)
+2022-11-18 15:13:52,182:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9736 (1.9180)
+2022-11-18 15:13:52,319:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8987 (1.9159)
+2022-11-18 15:13:52,706:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9788 (1.9788)
+2022-11-18 15:13:52,860:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9362 (1.9573)
+2022-11-18 15:13:53,011:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9483 (1.9542)
+2022-11-18 15:13:53,163:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9849 (1.9612)
+2022-11-18 15:13:53,313:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8554 (1.9397)
+2022-11-18 15:13:53,466:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8293 (1.9188)
+2022-11-18 15:13:53,616:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9694 (1.9257)
+2022-11-18 15:13:53,765:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9255 (1.9256)
+2022-11-18 15:13:53,915:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8564 (1.9182)
+2022-11-18 15:13:54,063:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8515 (1.9113)
+2022-11-18 15:13:54,216:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9352 (1.9136)
+2022-11-18 15:13:54,364:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8515 (1.9082)
+2022-11-18 15:13:54,514:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8690 (1.9051)
+2022-11-18 15:13:54,663:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8522 (1.9012)
+2022-11-18 15:13:54,813:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8840 (1.9000)
+2022-11-18 15:13:54,963:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0002 (1.9068)
+2022-11-18 15:13:55,114:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9147 (1.9072)
+2022-11-18 15:13:55,253:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9603 (1.9095)
+2022-11-18 15:13:55,308:INFO: - Computing loss (validation)
+2022-11-18 15:13:55,573:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9585 (1.9585)
+2022-11-18 15:13:55,607:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6158 (1.9340)
+2022-11-18 15:13:55,910:INFO: Dataset: univ                Batch: 1/3	Loss 1.8934 (1.8934)
+2022-11-18 15:13:55,987:INFO: Dataset: univ                Batch: 2/3	Loss 1.8905 (1.8919)
+2022-11-18 15:13:56,061:INFO: Dataset: univ                Batch: 3/3	Loss 1.8535 (1.8781)
+2022-11-18 15:13:56,364:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8820 (1.8820)
+2022-11-18 15:13:56,409:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9209 (1.8920)
+2022-11-18 15:13:56,723:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9589 (1.9589)
+2022-11-18 15:13:56,799:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8618 (1.9115)
+2022-11-18 15:13:56,875:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8999 (1.9077)
+2022-11-18 15:13:56,951:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9238 (1.9116)
+2022-11-18 15:13:57,025:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8999 (1.9095)
+2022-11-18 15:13:57,078:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_344.pth.tar
+2022-11-18 15:13:57,078:INFO: 
+===> EPOCH: 345 (P2)
+2022-11-18 15:13:57,079:INFO: - Computing loss (training)
+2022-11-18 15:13:57,423:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8589 (1.8589)
+2022-11-18 15:13:57,574:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9813 (1.9195)
+2022-11-18 15:13:57,724:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9794 (1.9392)
+2022-11-18 15:13:57,841:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9096 (1.9342)
+2022-11-18 15:13:58,240:INFO: Dataset: univ                Batch:  1/15	Loss 1.8917 (1.8917)
+2022-11-18 15:13:58,398:INFO: Dataset: univ                Batch:  2/15	Loss 1.9298 (1.9099)
+2022-11-18 15:13:58,560:INFO: Dataset: univ                Batch:  3/15	Loss 1.8676 (1.8959)
+2022-11-18 15:13:58,716:INFO: Dataset: univ                Batch:  4/15	Loss 1.8697 (1.8896)
+2022-11-18 15:13:58,880:INFO: Dataset: univ                Batch:  5/15	Loss 1.8571 (1.8840)
+2022-11-18 15:13:59,039:INFO: Dataset: univ                Batch:  6/15	Loss 1.8860 (1.8843)
+2022-11-18 15:13:59,196:INFO: Dataset: univ                Batch:  7/15	Loss 1.9172 (1.8886)
+2022-11-18 15:13:59,353:INFO: Dataset: univ                Batch:  8/15	Loss 1.8846 (1.8881)
+2022-11-18 15:13:59,510:INFO: Dataset: univ                Batch:  9/15	Loss 1.9283 (1.8925)
+2022-11-18 15:13:59,665:INFO: Dataset: univ                Batch: 10/15	Loss 1.9459 (1.8976)
+2022-11-18 15:13:59,822:INFO: Dataset: univ                Batch: 11/15	Loss 1.9020 (1.8979)
+2022-11-18 15:13:59,979:INFO: Dataset: univ                Batch: 12/15	Loss 1.9392 (1.9012)
+2022-11-18 15:14:00,138:INFO: Dataset: univ                Batch: 13/15	Loss 1.8697 (1.8988)
+2022-11-18 15:14:00,297:INFO: Dataset: univ                Batch: 14/15	Loss 1.9434 (1.9018)
+2022-11-18 15:14:00,381:INFO: Dataset: univ                Batch: 15/15	Loss 1.9118 (1.9019)
+2022-11-18 15:14:00,767:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9576 (1.9576)
+2022-11-18 15:14:00,918:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9549 (1.9562)
+2022-11-18 15:14:01,069:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8592 (1.9234)
+2022-11-18 15:14:01,218:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9418 (1.9279)
+2022-11-18 15:14:01,373:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9857 (1.9409)
+2022-11-18 15:14:01,524:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9447 (1.9415)
+2022-11-18 15:14:01,674:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7917 (1.9202)
+2022-11-18 15:14:01,812:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9381 (1.9223)
+2022-11-18 15:14:02,209:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8719 (1.8719)
+2022-11-18 15:14:02,362:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9209 (1.8963)
+2022-11-18 15:14:02,519:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8891 (1.8939)
+2022-11-18 15:14:02,673:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9359 (1.9046)
+2022-11-18 15:14:02,825:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8430 (1.8922)
+2022-11-18 15:14:02,977:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9511 (1.9014)
+2022-11-18 15:14:03,129:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8015 (1.8864)
+2022-11-18 15:14:03,278:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8050 (1.8759)
+2022-11-18 15:14:03,429:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8381 (1.8721)
+2022-11-18 15:14:03,578:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9274 (1.8774)
+2022-11-18 15:14:03,730:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9136 (1.8808)
+2022-11-18 15:14:03,881:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9024 (1.8828)
+2022-11-18 15:14:04,032:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9437 (1.8872)
+2022-11-18 15:14:04,182:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8349 (1.8835)
+2022-11-18 15:14:04,334:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7732 (1.8764)
+2022-11-18 15:14:04,485:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7844 (1.8710)
+2022-11-18 15:14:04,637:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8408 (1.8693)
+2022-11-18 15:14:04,776:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9192 (1.8715)
+2022-11-18 15:14:04,821:INFO: - Computing loss (validation)
+2022-11-18 15:14:05,085:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9969 (1.9969)
+2022-11-18 15:14:05,119:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2671 (2.0135)
+2022-11-18 15:14:05,441:INFO: Dataset: univ                Batch: 1/3	Loss 1.9361 (1.9361)
+2022-11-18 15:14:05,523:INFO: Dataset: univ                Batch: 2/3	Loss 1.9325 (1.9343)
+2022-11-18 15:14:05,598:INFO: Dataset: univ                Batch: 3/3	Loss 1.9068 (1.9265)
+2022-11-18 15:14:05,894:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8742 (1.8742)
+2022-11-18 15:14:05,938:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9099 (1.8835)
+2022-11-18 15:14:06,240:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9033 (1.9033)
+2022-11-18 15:14:06,318:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8673 (1.8865)
+2022-11-18 15:14:06,397:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9536 (1.9101)
+2022-11-18 15:14:06,476:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9685 (1.9239)
+2022-11-18 15:14:06,554:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9640 (1.9321)
+2022-11-18 15:14:06,605:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_345.pth.tar
+2022-11-18 15:14:06,605:INFO: 
+===> EPOCH: 346 (P2)
+2022-11-18 15:14:06,605:INFO: - Computing loss (training)
+2022-11-18 15:14:06,952:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8514 (1.8514)
+2022-11-18 15:14:07,103:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7960 (1.8232)
+2022-11-18 15:14:07,254:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9498 (1.8669)
+2022-11-18 15:14:07,372:INFO: Dataset: hotel               Batch: 4/4	Loss 2.2282 (1.9312)
+2022-11-18 15:14:07,762:INFO: Dataset: univ                Batch:  1/15	Loss 1.9004 (1.9004)
+2022-11-18 15:14:07,918:INFO: Dataset: univ                Batch:  2/15	Loss 1.8790 (1.8893)
+2022-11-18 15:14:08,076:INFO: Dataset: univ                Batch:  3/15	Loss 1.9294 (1.9009)
+2022-11-18 15:14:08,231:INFO: Dataset: univ                Batch:  4/15	Loss 1.9209 (1.9061)
+2022-11-18 15:14:08,386:INFO: Dataset: univ                Batch:  5/15	Loss 1.8804 (1.9013)
+2022-11-18 15:14:08,541:INFO: Dataset: univ                Batch:  6/15	Loss 1.9776 (1.9127)
+2022-11-18 15:14:08,697:INFO: Dataset: univ                Batch:  7/15	Loss 1.8380 (1.9013)
+2022-11-18 15:14:08,855:INFO: Dataset: univ                Batch:  8/15	Loss 1.9483 (1.9074)
+2022-11-18 15:14:09,010:INFO: Dataset: univ                Batch:  9/15	Loss 1.8629 (1.9024)
+2022-11-18 15:14:09,164:INFO: Dataset: univ                Batch: 10/15	Loss 1.8975 (1.9018)
+2022-11-18 15:14:09,324:INFO: Dataset: univ                Batch: 11/15	Loss 1.9039 (1.9020)
+2022-11-18 15:14:09,488:INFO: Dataset: univ                Batch: 12/15	Loss 1.8728 (1.8992)
+2022-11-18 15:14:09,643:INFO: Dataset: univ                Batch: 13/15	Loss 1.8500 (1.8955)
+2022-11-18 15:14:09,798:INFO: Dataset: univ                Batch: 14/15	Loss 1.8723 (1.8939)
+2022-11-18 15:14:09,880:INFO: Dataset: univ                Batch: 15/15	Loss 1.8080 (1.8928)
+2022-11-18 15:14:10,269:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0278 (2.0278)
+2022-11-18 15:14:10,417:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8921 (1.9619)
+2022-11-18 15:14:10,564:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8524 (1.9263)
+2022-11-18 15:14:10,712:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9535 (1.9331)
+2022-11-18 15:14:10,861:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9933 (1.9449)
+2022-11-18 15:14:11,008:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8702 (1.9325)
+2022-11-18 15:14:11,157:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9976 (1.9424)
+2022-11-18 15:14:11,293:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9025 (1.9386)
+2022-11-18 15:14:11,685:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9341 (1.9341)
+2022-11-18 15:14:11,841:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9142 (1.9242)
+2022-11-18 15:14:11,998:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8733 (1.9083)
+2022-11-18 15:14:12,149:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9189 (1.9109)
+2022-11-18 15:14:12,302:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8868 (1.9061)
+2022-11-18 15:14:12,454:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8953 (1.9045)
+2022-11-18 15:14:12,606:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9869 (1.9158)
+2022-11-18 15:14:12,758:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9393 (1.9187)
+2022-11-18 15:14:12,911:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0017 (1.9277)
+2022-11-18 15:14:13,060:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9540 (1.9302)
+2022-11-18 15:14:13,214:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8473 (1.9236)
+2022-11-18 15:14:13,366:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9499 (1.9257)
+2022-11-18 15:14:13,517:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8629 (1.9212)
+2022-11-18 15:14:13,678:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8859 (1.9188)
+2022-11-18 15:14:13,834:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9273 (1.9194)
+2022-11-18 15:14:13,985:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9534 (1.9214)
+2022-11-18 15:14:14,138:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8802 (1.9191)
+2022-11-18 15:14:14,279:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8639 (1.9165)
+2022-11-18 15:14:14,326:INFO: - Computing loss (validation)
+2022-11-18 15:14:14,603:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9920 (1.9920)
+2022-11-18 15:14:14,638:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8434 (1.9808)
+2022-11-18 15:14:14,942:INFO: Dataset: univ                Batch: 1/3	Loss 1.8579 (1.8579)
+2022-11-18 15:14:15,017:INFO: Dataset: univ                Batch: 2/3	Loss 1.8503 (1.8545)
+2022-11-18 15:14:15,092:INFO: Dataset: univ                Batch: 3/3	Loss 1.9013 (1.8696)
+2022-11-18 15:14:15,396:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9022 (1.9022)
+2022-11-18 15:14:15,440:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7374 (1.8609)
+2022-11-18 15:14:15,755:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8508 (1.8508)
+2022-11-18 15:14:15,830:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9115 (1.8800)
+2022-11-18 15:14:15,906:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9079 (1.8891)
+2022-11-18 15:14:15,983:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8311 (1.8739)
+2022-11-18 15:14:16,058:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9055 (1.8804)
+2022-11-18 15:14:16,111:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_346.pth.tar
+2022-11-18 15:14:16,111:INFO: 
+===> EPOCH: 347 (P2)
+2022-11-18 15:14:16,111:INFO: - Computing loss (training)
+2022-11-18 15:14:16,460:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0022 (2.0022)
+2022-11-18 15:14:16,611:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9539 (1.9790)
+2022-11-18 15:14:16,762:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9091 (1.9555)
+2022-11-18 15:14:16,877:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7990 (1.9295)
+2022-11-18 15:14:17,287:INFO: Dataset: univ                Batch:  1/15	Loss 1.8824 (1.8824)
+2022-11-18 15:14:17,448:INFO: Dataset: univ                Batch:  2/15	Loss 1.9161 (1.8985)
+2022-11-18 15:14:17,609:INFO: Dataset: univ                Batch:  3/15	Loss 1.8991 (1.8987)
+2022-11-18 15:14:17,773:INFO: Dataset: univ                Batch:  4/15	Loss 1.9335 (1.9074)
+2022-11-18 15:14:17,930:INFO: Dataset: univ                Batch:  5/15	Loss 1.9085 (1.9076)
+2022-11-18 15:14:18,090:INFO: Dataset: univ                Batch:  6/15	Loss 1.8833 (1.9032)
+2022-11-18 15:14:18,247:INFO: Dataset: univ                Batch:  7/15	Loss 1.9027 (1.9032)
+2022-11-18 15:14:18,403:INFO: Dataset: univ                Batch:  8/15	Loss 1.8975 (1.9024)
+2022-11-18 15:14:18,561:INFO: Dataset: univ                Batch:  9/15	Loss 1.8841 (1.9004)
+2022-11-18 15:14:18,716:INFO: Dataset: univ                Batch: 10/15	Loss 1.8827 (1.8989)
+2022-11-18 15:14:18,882:INFO: Dataset: univ                Batch: 11/15	Loss 1.9147 (1.9004)
+2022-11-18 15:14:19,038:INFO: Dataset: univ                Batch: 12/15	Loss 1.8830 (1.8989)
+2022-11-18 15:14:19,275:INFO: Dataset: univ                Batch: 13/15	Loss 1.9009 (1.8990)
+2022-11-18 15:14:19,432:INFO: Dataset: univ                Batch: 14/15	Loss 1.9294 (1.9013)
+2022-11-18 15:14:19,516:INFO: Dataset: univ                Batch: 15/15	Loss 1.8713 (1.9008)
+2022-11-18 15:14:19,909:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0138 (2.0138)
+2022-11-18 15:14:20,060:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9657 (1.9883)
+2022-11-18 15:14:20,211:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0302 (2.0011)
+2022-11-18 15:14:20,362:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9561 (1.9912)
+2022-11-18 15:14:20,521:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8712 (1.9696)
+2022-11-18 15:14:20,671:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9239 (1.9616)
+2022-11-18 15:14:20,822:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9451 (1.9591)
+2022-11-18 15:14:20,959:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9886 (1.9622)
+2022-11-18 15:14:21,396:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9581 (1.9581)
+2022-11-18 15:14:21,543:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8889 (1.9268)
+2022-11-18 15:14:21,693:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9234 (1.9257)
+2022-11-18 15:14:21,841:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9253 (1.9256)
+2022-11-18 15:14:21,990:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9271 (1.9259)
+2022-11-18 15:14:22,140:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8755 (1.9176)
+2022-11-18 15:14:22,289:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8754 (1.9121)
+2022-11-18 15:14:22,435:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9425 (1.9161)
+2022-11-18 15:14:22,584:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9271 (1.9173)
+2022-11-18 15:14:22,730:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8689 (1.9121)
+2022-11-18 15:14:22,880:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8615 (1.9076)
+2022-11-18 15:14:23,027:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9352 (1.9099)
+2022-11-18 15:14:23,176:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9895 (1.9159)
+2022-11-18 15:14:23,325:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8228 (1.9096)
+2022-11-18 15:14:23,474:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9700 (1.9134)
+2022-11-18 15:14:23,621:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9173 (1.9137)
+2022-11-18 15:14:23,770:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8011 (1.9076)
+2022-11-18 15:14:23,906:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7739 (1.9004)
+2022-11-18 15:14:23,954:INFO: - Computing loss (validation)
+2022-11-18 15:14:24,216:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9912 (1.9912)
+2022-11-18 15:14:24,251:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3182 (2.0112)
+2022-11-18 15:14:24,566:INFO: Dataset: univ                Batch: 1/3	Loss 1.9274 (1.9274)
+2022-11-18 15:14:24,642:INFO: Dataset: univ                Batch: 2/3	Loss 1.8880 (1.9076)
+2022-11-18 15:14:24,714:INFO: Dataset: univ                Batch: 3/3	Loss 1.8894 (1.9018)
+2022-11-18 15:14:25,017:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0686 (2.0686)
+2022-11-18 15:14:25,065:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0198 (2.0559)
+2022-11-18 15:14:25,376:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8836 (1.8836)
+2022-11-18 15:14:25,452:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8900 (1.8867)
+2022-11-18 15:14:25,528:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9083 (1.8938)
+2022-11-18 15:14:25,605:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9243 (1.9018)
+2022-11-18 15:14:25,680:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9390 (1.9085)
+2022-11-18 15:14:25,733:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_347.pth.tar
+2022-11-18 15:14:25,733:INFO: 
+===> EPOCH: 348 (P2)
+2022-11-18 15:14:25,734:INFO: - Computing loss (training)
+2022-11-18 15:14:26,073:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9570 (1.9570)
+2022-11-18 15:14:26,226:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8809 (1.9195)
+2022-11-18 15:14:26,378:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9880 (1.9425)
+2022-11-18 15:14:26,494:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9942 (1.9517)
+2022-11-18 15:14:26,890:INFO: Dataset: univ                Batch:  1/15	Loss 1.9377 (1.9377)
+2022-11-18 15:14:27,047:INFO: Dataset: univ                Batch:  2/15	Loss 1.8719 (1.9053)
+2022-11-18 15:14:27,203:INFO: Dataset: univ                Batch:  3/15	Loss 1.9228 (1.9113)
+2022-11-18 15:14:27,361:INFO: Dataset: univ                Batch:  4/15	Loss 1.9298 (1.9156)
+2022-11-18 15:14:27,518:INFO: Dataset: univ                Batch:  5/15	Loss 1.8977 (1.9122)
+2022-11-18 15:14:27,674:INFO: Dataset: univ                Batch:  6/15	Loss 1.8450 (1.9014)
+2022-11-18 15:14:27,830:INFO: Dataset: univ                Batch:  7/15	Loss 1.8806 (1.8983)
+2022-11-18 15:14:27,983:INFO: Dataset: univ                Batch:  8/15	Loss 1.9407 (1.9039)
+2022-11-18 15:14:28,139:INFO: Dataset: univ                Batch:  9/15	Loss 1.8877 (1.9020)
+2022-11-18 15:14:28,292:INFO: Dataset: univ                Batch: 10/15	Loss 1.8944 (1.9012)
+2022-11-18 15:14:28,446:INFO: Dataset: univ                Batch: 11/15	Loss 1.9608 (1.9065)
+2022-11-18 15:14:28,600:INFO: Dataset: univ                Batch: 12/15	Loss 1.8767 (1.9038)
+2022-11-18 15:14:28,754:INFO: Dataset: univ                Batch: 13/15	Loss 1.9080 (1.9041)
+2022-11-18 15:14:28,911:INFO: Dataset: univ                Batch: 14/15	Loss 1.8687 (1.9016)
+2022-11-18 15:14:28,993:INFO: Dataset: univ                Batch: 15/15	Loss 1.9069 (1.9016)
+2022-11-18 15:14:29,378:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9602 (1.9602)
+2022-11-18 15:14:29,528:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9609 (1.9605)
+2022-11-18 15:14:29,678:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9429 (1.9545)
+2022-11-18 15:14:29,826:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9053 (1.9420)
+2022-11-18 15:14:29,978:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9543 (1.9444)
+2022-11-18 15:14:30,127:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9142 (1.9395)
+2022-11-18 15:14:30,277:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8862 (1.9313)
+2022-11-18 15:14:30,413:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9927 (1.9374)
+2022-11-18 15:14:30,806:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8829 (1.8829)
+2022-11-18 15:14:30,962:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9319 (1.9065)
+2022-11-18 15:14:31,114:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8945 (1.9025)
+2022-11-18 15:14:31,263:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8770 (1.8959)
+2022-11-18 15:14:31,416:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8286 (1.8826)
+2022-11-18 15:14:31,566:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8163 (1.8706)
+2022-11-18 15:14:31,715:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9855 (1.8881)
+2022-11-18 15:14:31,863:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9413 (1.8948)
+2022-11-18 15:14:32,012:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9547 (1.9021)
+2022-11-18 15:14:32,159:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9780 (1.9103)
+2022-11-18 15:14:32,310:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8310 (1.9033)
+2022-11-18 15:14:32,457:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8638 (1.9001)
+2022-11-18 15:14:32,606:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8645 (1.8975)
+2022-11-18 15:14:32,759:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9968 (1.9040)
+2022-11-18 15:14:32,910:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9556 (1.9074)
+2022-11-18 15:14:33,059:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0186 (1.9135)
+2022-11-18 15:14:33,210:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8768 (1.9111)
+2022-11-18 15:14:33,348:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8608 (1.9088)
+2022-11-18 15:14:33,393:INFO: - Computing loss (validation)
+2022-11-18 15:14:33,653:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0702 (2.0702)
+2022-11-18 15:14:33,688:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0770 (2.0707)
+2022-11-18 15:14:33,995:INFO: Dataset: univ                Batch: 1/3	Loss 1.9227 (1.9227)
+2022-11-18 15:14:34,072:INFO: Dataset: univ                Batch: 2/3	Loss 1.9248 (1.9237)
+2022-11-18 15:14:34,149:INFO: Dataset: univ                Batch: 3/3	Loss 1.8428 (1.8964)
+2022-11-18 15:14:34,453:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9250 (1.9250)
+2022-11-18 15:14:34,499:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8389 (1.9020)
+2022-11-18 15:14:34,812:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9066 (1.9066)
+2022-11-18 15:14:34,888:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9411 (1.9245)
+2022-11-18 15:14:34,966:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9087 (1.9194)
+2022-11-18 15:14:35,042:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9386 (1.9239)
+2022-11-18 15:14:35,119:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9964 (1.9381)
+2022-11-18 15:14:35,174:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_348.pth.tar
+2022-11-18 15:14:35,174:INFO: 
+===> EPOCH: 349 (P2)
+2022-11-18 15:14:35,175:INFO: - Computing loss (training)
+2022-11-18 15:14:35,508:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8660 (1.8660)
+2022-11-18 15:14:35,659:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8159 (1.8408)
+2022-11-18 15:14:35,809:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9830 (1.8921)
+2022-11-18 15:14:35,923:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8578 (1.8864)
+2022-11-18 15:14:36,332:INFO: Dataset: univ                Batch:  1/15	Loss 1.8779 (1.8779)
+2022-11-18 15:14:36,489:INFO: Dataset: univ                Batch:  2/15	Loss 1.8621 (1.8703)
+2022-11-18 15:14:36,647:INFO: Dataset: univ                Batch:  3/15	Loss 1.8666 (1.8692)
+2022-11-18 15:14:36,802:INFO: Dataset: univ                Batch:  4/15	Loss 1.8822 (1.8724)
+2022-11-18 15:14:36,966:INFO: Dataset: univ                Batch:  5/15	Loss 1.8886 (1.8758)
+2022-11-18 15:14:37,123:INFO: Dataset: univ                Batch:  6/15	Loss 1.9010 (1.8801)
+2022-11-18 15:14:37,280:INFO: Dataset: univ                Batch:  7/15	Loss 1.8986 (1.8830)
+2022-11-18 15:14:37,435:INFO: Dataset: univ                Batch:  8/15	Loss 1.8818 (1.8828)
+2022-11-18 15:14:37,592:INFO: Dataset: univ                Batch:  9/15	Loss 1.9264 (1.8878)
+2022-11-18 15:14:37,747:INFO: Dataset: univ                Batch: 10/15	Loss 1.8771 (1.8867)
+2022-11-18 15:14:37,904:INFO: Dataset: univ                Batch: 11/15	Loss 1.9248 (1.8901)
+2022-11-18 15:14:38,060:INFO: Dataset: univ                Batch: 12/15	Loss 1.9182 (1.8925)
+2022-11-18 15:14:38,216:INFO: Dataset: univ                Batch: 13/15	Loss 1.8496 (1.8888)
+2022-11-18 15:14:38,374:INFO: Dataset: univ                Batch: 14/15	Loss 1.8609 (1.8867)
+2022-11-18 15:14:38,457:INFO: Dataset: univ                Batch: 15/15	Loss 1.8284 (1.8861)
+2022-11-18 15:14:38,854:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8871 (1.8871)
+2022-11-18 15:14:39,003:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8804 (1.8838)
+2022-11-18 15:14:39,157:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9348 (1.9001)
+2022-11-18 15:14:39,305:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8992 (1.8999)
+2022-11-18 15:14:39,456:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7918 (1.8782)
+2022-11-18 15:14:39,605:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8896 (1.8801)
+2022-11-18 15:14:39,755:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8154 (1.8708)
+2022-11-18 15:14:39,891:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0078 (1.8845)
+2022-11-18 15:14:40,274:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9564 (1.9564)
+2022-11-18 15:14:40,436:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9370 (1.9469)
+2022-11-18 15:14:40,585:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8814 (1.9232)
+2022-11-18 15:14:40,734:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8519 (1.9044)
+2022-11-18 15:14:40,883:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8296 (1.8889)
+2022-11-18 15:14:41,033:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9007 (1.8909)
+2022-11-18 15:14:41,181:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9540 (1.9002)
+2022-11-18 15:14:41,330:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8994 (1.9001)
+2022-11-18 15:14:41,479:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9345 (1.9034)
+2022-11-18 15:14:41,624:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8122 (1.8940)
+2022-11-18 15:14:41,774:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8075 (1.8855)
+2022-11-18 15:14:41,922:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9130 (1.8878)
+2022-11-18 15:14:42,070:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8741 (1.8867)
+2022-11-18 15:14:42,218:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9258 (1.8896)
+2022-11-18 15:14:42,367:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9868 (1.8957)
+2022-11-18 15:14:42,516:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8955 (1.8957)
+2022-11-18 15:14:42,666:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0023 (1.9021)
+2022-11-18 15:14:42,803:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8996 (1.9019)
+2022-11-18 15:14:42,850:INFO: - Computing loss (validation)
+2022-11-18 15:14:43,122:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0656 (2.0656)
+2022-11-18 15:14:43,158:INFO: Dataset: hotel               Batch: 2/2	Loss 2.5747 (2.0969)
+2022-11-18 15:14:43,471:INFO: Dataset: univ                Batch: 1/3	Loss 1.9370 (1.9370)
+2022-11-18 15:14:43,554:INFO: Dataset: univ                Batch: 2/3	Loss 1.8329 (1.8807)
+2022-11-18 15:14:43,632:INFO: Dataset: univ                Batch: 3/3	Loss 1.9127 (1.8923)
+2022-11-18 15:14:43,932:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0426 (2.0426)
+2022-11-18 15:14:43,979:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9976 (2.0321)
+2022-11-18 15:14:44,284:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8247 (1.8247)
+2022-11-18 15:14:44,359:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9215 (1.8721)
+2022-11-18 15:14:44,434:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8893 (1.8781)
+2022-11-18 15:14:44,509:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8357 (1.8674)
+2022-11-18 15:14:44,585:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8477 (1.8635)
+2022-11-18 15:14:44,639:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_349.pth.tar
+2022-11-18 15:14:44,639:INFO: 
+===> EPOCH: 350 (P2)
+2022-11-18 15:14:44,640:INFO: - Computing loss (training)
+2022-11-18 15:14:44,971:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0378 (2.0378)
+2022-11-18 15:14:45,123:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8878 (1.9593)
+2022-11-18 15:14:45,273:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9063 (1.9415)
+2022-11-18 15:14:45,389:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9429 (1.9417)
+2022-11-18 15:14:45,832:INFO: Dataset: univ                Batch:  1/15	Loss 1.8920 (1.8920)
+2022-11-18 15:14:45,994:INFO: Dataset: univ                Batch:  2/15	Loss 1.8953 (1.8937)
+2022-11-18 15:14:46,152:INFO: Dataset: univ                Batch:  3/15	Loss 1.9277 (1.9050)
+2022-11-18 15:14:46,309:INFO: Dataset: univ                Batch:  4/15	Loss 1.8761 (1.8967)
+2022-11-18 15:14:46,466:INFO: Dataset: univ                Batch:  5/15	Loss 1.9139 (1.9001)
+2022-11-18 15:14:46,625:INFO: Dataset: univ                Batch:  6/15	Loss 1.9272 (1.9048)
+2022-11-18 15:14:46,782:INFO: Dataset: univ                Batch:  7/15	Loss 1.9518 (1.9118)
+2022-11-18 15:14:46,938:INFO: Dataset: univ                Batch:  8/15	Loss 1.9119 (1.9118)
+2022-11-18 15:14:47,097:INFO: Dataset: univ                Batch:  9/15	Loss 1.9435 (1.9153)
+2022-11-18 15:14:47,252:INFO: Dataset: univ                Batch: 10/15	Loss 1.8871 (1.9122)
+2022-11-18 15:14:47,409:INFO: Dataset: univ                Batch: 11/15	Loss 1.8935 (1.9107)
+2022-11-18 15:14:47,566:INFO: Dataset: univ                Batch: 12/15	Loss 1.8994 (1.9097)
+2022-11-18 15:14:47,725:INFO: Dataset: univ                Batch: 13/15	Loss 1.8761 (1.9070)
+2022-11-18 15:14:47,884:INFO: Dataset: univ                Batch: 14/15	Loss 1.9001 (1.9065)
+2022-11-18 15:14:47,967:INFO: Dataset: univ                Batch: 15/15	Loss 1.8686 (1.9059)
+2022-11-18 15:14:48,365:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8101 (1.8101)
+2022-11-18 15:14:48,514:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0581 (1.9413)
+2022-11-18 15:14:48,664:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8908 (1.9248)
+2022-11-18 15:14:48,816:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9476 (1.9305)
+2022-11-18 15:14:48,972:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9958 (1.9450)
+2022-11-18 15:14:49,122:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9728 (1.9495)
+2022-11-18 15:14:49,272:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8333 (1.9347)
+2022-11-18 15:14:49,408:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9984 (1.9420)
+2022-11-18 15:14:49,796:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9397 (1.9397)
+2022-11-18 15:14:49,945:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9015 (1.9226)
+2022-11-18 15:14:50,099:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8870 (1.9105)
+2022-11-18 15:14:50,250:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9560 (1.9223)
+2022-11-18 15:14:50,400:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9177 (1.9214)
+2022-11-18 15:14:50,552:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8652 (1.9122)
+2022-11-18 15:14:50,702:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8812 (1.9074)
+2022-11-18 15:14:50,851:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9313 (1.9108)
+2022-11-18 15:14:51,001:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9033 (1.9099)
+2022-11-18 15:14:51,150:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8425 (1.9037)
+2022-11-18 15:14:51,302:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9159 (1.9048)
+2022-11-18 15:14:51,454:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8899 (1.9036)
+2022-11-18 15:14:51,604:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8809 (1.9018)
+2022-11-18 15:14:51,753:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8808 (1.9005)
+2022-11-18 15:14:51,905:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9999 (1.9067)
+2022-11-18 15:14:52,055:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7757 (1.8986)
+2022-11-18 15:14:52,207:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9692 (1.9026)
+2022-11-18 15:14:52,346:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9732 (1.9061)
+2022-11-18 15:14:52,391:INFO: - Computing loss (validation)
+2022-11-18 15:14:52,652:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9198 (1.9198)
+2022-11-18 15:14:52,686:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0473 (1.9294)
+2022-11-18 15:14:52,989:INFO: Dataset: univ                Batch: 1/3	Loss 1.8901 (1.8901)
+2022-11-18 15:14:53,068:INFO: Dataset: univ                Batch: 2/3	Loss 1.8835 (1.8866)
+2022-11-18 15:14:53,140:INFO: Dataset: univ                Batch: 3/3	Loss 1.8715 (1.8821)
+2022-11-18 15:14:53,450:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8647 (1.8647)
+2022-11-18 15:14:53,497:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0302 (1.9030)
+2022-11-18 15:14:53,804:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8683 (1.8683)
+2022-11-18 15:14:53,883:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9810 (1.9261)
+2022-11-18 15:14:53,962:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8479 (1.9015)
+2022-11-18 15:14:54,041:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8570 (1.8902)
+2022-11-18 15:14:54,118:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9912 (1.9108)
+2022-11-18 15:14:54,170:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_350.pth.tar
+2022-11-18 15:14:54,170:INFO: 
+===> EPOCH: 351 (P2)
+2022-11-18 15:14:54,171:INFO: - Computing loss (training)
+2022-11-18 15:14:54,518:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9312 (1.9312)
+2022-11-18 15:14:54,667:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0402 (1.9849)
+2022-11-18 15:14:54,816:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9064 (1.9596)
+2022-11-18 15:14:54,931:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9875 (1.9649)
+2022-11-18 15:14:55,384:INFO: Dataset: univ                Batch:  1/15	Loss 1.8706 (1.8706)
+2022-11-18 15:14:55,539:INFO: Dataset: univ                Batch:  2/15	Loss 1.9587 (1.9152)
+2022-11-18 15:14:55,698:INFO: Dataset: univ                Batch:  3/15	Loss 1.8687 (1.8994)
+2022-11-18 15:14:55,854:INFO: Dataset: univ                Batch:  4/15	Loss 1.8954 (1.8984)
+2022-11-18 15:14:56,010:INFO: Dataset: univ                Batch:  5/15	Loss 1.8630 (1.8913)
+2022-11-18 15:14:56,166:INFO: Dataset: univ                Batch:  6/15	Loss 1.9106 (1.8944)
+2022-11-18 15:14:56,323:INFO: Dataset: univ                Batch:  7/15	Loss 1.8691 (1.8908)
+2022-11-18 15:14:56,476:INFO: Dataset: univ                Batch:  8/15	Loss 1.8918 (1.8909)
+2022-11-18 15:14:56,631:INFO: Dataset: univ                Batch:  9/15	Loss 1.8894 (1.8907)
+2022-11-18 15:14:56,785:INFO: Dataset: univ                Batch: 10/15	Loss 1.9229 (1.8941)
+2022-11-18 15:14:56,942:INFO: Dataset: univ                Batch: 11/15	Loss 1.8856 (1.8934)
+2022-11-18 15:14:57,097:INFO: Dataset: univ                Batch: 12/15	Loss 1.9415 (1.8973)
+2022-11-18 15:14:57,255:INFO: Dataset: univ                Batch: 13/15	Loss 1.9108 (1.8984)
+2022-11-18 15:14:57,411:INFO: Dataset: univ                Batch: 14/15	Loss 1.9339 (1.9009)
+2022-11-18 15:14:57,493:INFO: Dataset: univ                Batch: 15/15	Loss 1.9021 (1.9009)
+2022-11-18 15:14:57,890:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8645 (1.8645)
+2022-11-18 15:14:58,038:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9316 (1.8969)
+2022-11-18 15:14:58,189:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9194 (1.9042)
+2022-11-18 15:14:58,337:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9022 (1.9037)
+2022-11-18 15:14:58,485:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8641 (1.8956)
+2022-11-18 15:14:58,635:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8384 (1.8872)
+2022-11-18 15:14:58,784:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7717 (1.8714)
+2022-11-18 15:14:58,924:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9308 (1.8776)
+2022-11-18 15:14:59,408:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8875 (1.8875)
+2022-11-18 15:14:59,555:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8909 (1.8891)
+2022-11-18 15:14:59,703:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8850 (1.8878)
+2022-11-18 15:14:59,851:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8887 (1.8880)
+2022-11-18 15:14:59,999:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8915 (1.8887)
+2022-11-18 15:15:00,152:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9040 (1.8913)
+2022-11-18 15:15:00,299:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8271 (1.8821)
+2022-11-18 15:15:00,448:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9265 (1.8876)
+2022-11-18 15:15:00,596:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9756 (1.8971)
+2022-11-18 15:15:00,742:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8833 (1.8957)
+2022-11-18 15:15:00,891:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0366 (1.9073)
+2022-11-18 15:15:01,039:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7830 (1.8975)
+2022-11-18 15:15:01,189:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9633 (1.9027)
+2022-11-18 15:15:01,339:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8378 (1.8975)
+2022-11-18 15:15:01,488:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9424 (1.9003)
+2022-11-18 15:15:01,636:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8965 (1.9000)
+2022-11-18 15:15:01,785:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8553 (1.8977)
+2022-11-18 15:15:01,922:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9529 (1.9005)
+2022-11-18 15:15:01,966:INFO: - Computing loss (validation)
+2022-11-18 15:15:02,226:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8252 (1.8252)
+2022-11-18 15:15:02,260:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6342 (1.8089)
+2022-11-18 15:15:02,565:INFO: Dataset: univ                Batch: 1/3	Loss 1.8851 (1.8851)
+2022-11-18 15:15:02,645:INFO: Dataset: univ                Batch: 2/3	Loss 1.9372 (1.9116)
+2022-11-18 15:15:02,719:INFO: Dataset: univ                Batch: 3/3	Loss 1.9141 (1.9123)
+2022-11-18 15:15:03,018:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9041 (1.9041)
+2022-11-18 15:15:03,064:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9132 (1.9062)
+2022-11-18 15:15:03,373:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7930 (1.7930)
+2022-11-18 15:15:03,447:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9084 (1.8504)
+2022-11-18 15:15:03,522:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8330 (1.8445)
+2022-11-18 15:15:03,595:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8707 (1.8507)
+2022-11-18 15:15:03,669:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8416 (1.8489)
+2022-11-18 15:15:03,721:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_351.pth.tar
+2022-11-18 15:15:03,721:INFO: 
+===> EPOCH: 352 (P2)
+2022-11-18 15:15:03,721:INFO: - Computing loss (training)
+2022-11-18 15:15:04,063:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0618 (2.0618)
+2022-11-18 15:15:04,215:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8772 (1.9704)
+2022-11-18 15:15:04,367:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9908 (1.9773)
+2022-11-18 15:15:04,483:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9306 (1.9697)
+2022-11-18 15:15:04,883:INFO: Dataset: univ                Batch:  1/15	Loss 1.9151 (1.9151)
+2022-11-18 15:15:05,039:INFO: Dataset: univ                Batch:  2/15	Loss 1.9252 (1.9202)
+2022-11-18 15:15:05,197:INFO: Dataset: univ                Batch:  3/15	Loss 1.9009 (1.9135)
+2022-11-18 15:15:05,357:INFO: Dataset: univ                Batch:  4/15	Loss 1.8744 (1.9043)
+2022-11-18 15:15:05,512:INFO: Dataset: univ                Batch:  5/15	Loss 1.9198 (1.9074)
+2022-11-18 15:15:05,669:INFO: Dataset: univ                Batch:  6/15	Loss 1.9052 (1.9070)
+2022-11-18 15:15:05,825:INFO: Dataset: univ                Batch:  7/15	Loss 1.8670 (1.9011)
+2022-11-18 15:15:05,979:INFO: Dataset: univ                Batch:  8/15	Loss 1.8951 (1.9003)
+2022-11-18 15:15:06,135:INFO: Dataset: univ                Batch:  9/15	Loss 1.8803 (1.8979)
+2022-11-18 15:15:06,289:INFO: Dataset: univ                Batch: 10/15	Loss 1.9013 (1.8983)
+2022-11-18 15:15:06,444:INFO: Dataset: univ                Batch: 11/15	Loss 1.8862 (1.8972)
+2022-11-18 15:15:06,599:INFO: Dataset: univ                Batch: 12/15	Loss 1.9016 (1.8975)
+2022-11-18 15:15:06,754:INFO: Dataset: univ                Batch: 13/15	Loss 1.8784 (1.8960)
+2022-11-18 15:15:06,911:INFO: Dataset: univ                Batch: 14/15	Loss 1.8788 (1.8948)
+2022-11-18 15:15:06,994:INFO: Dataset: univ                Batch: 15/15	Loss 1.8332 (1.8937)
+2022-11-18 15:15:07,376:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8924 (1.8924)
+2022-11-18 15:15:07,530:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9817 (1.9359)
+2022-11-18 15:15:07,681:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9887 (1.9574)
+2022-11-18 15:15:07,832:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9336 (1.9511)
+2022-11-18 15:15:07,986:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9767 (1.9563)
+2022-11-18 15:15:08,137:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8977 (1.9472)
+2022-11-18 15:15:08,288:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8365 (1.9321)
+2022-11-18 15:15:08,426:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9726 (1.9359)
+2022-11-18 15:15:08,815:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9245 (1.9245)
+2022-11-18 15:15:08,968:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9862 (1.9563)
+2022-11-18 15:15:09,120:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8604 (1.9253)
+2022-11-18 15:15:09,266:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8751 (1.9121)
+2022-11-18 15:15:09,417:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9109 (1.9119)
+2022-11-18 15:15:09,566:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9789 (1.9232)
+2022-11-18 15:15:09,713:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8912 (1.9185)
+2022-11-18 15:15:09,861:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9256 (1.9193)
+2022-11-18 15:15:10,008:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9729 (1.9255)
+2022-11-18 15:15:10,156:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8480 (1.9180)
+2022-11-18 15:15:10,304:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9061 (1.9169)
+2022-11-18 15:15:10,454:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9909 (1.9225)
+2022-11-18 15:15:10,602:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9671 (1.9256)
+2022-11-18 15:15:10,752:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9004 (1.9240)
+2022-11-18 15:15:10,902:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8874 (1.9216)
+2022-11-18 15:15:11,050:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9265 (1.9219)
+2022-11-18 15:15:11,200:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8887 (1.9199)
+2022-11-18 15:15:11,338:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8288 (1.9154)
+2022-11-18 15:15:11,383:INFO: - Computing loss (validation)
+2022-11-18 15:15:11,643:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0368 (2.0368)
+2022-11-18 15:15:11,677:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6260 (2.0060)
+2022-11-18 15:15:11,988:INFO: Dataset: univ                Batch: 1/3	Loss 1.9025 (1.9025)
+2022-11-18 15:15:12,064:INFO: Dataset: univ                Batch: 2/3	Loss 1.8879 (1.8962)
+2022-11-18 15:15:12,138:INFO: Dataset: univ                Batch: 3/3	Loss 1.8699 (1.8888)
+2022-11-18 15:15:12,442:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9321 (1.9321)
+2022-11-18 15:15:12,488:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7934 (1.8991)
+2022-11-18 15:15:12,791:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8435 (1.8435)
+2022-11-18 15:15:12,867:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8346 (1.8390)
+2022-11-18 15:15:12,944:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9172 (1.8655)
+2022-11-18 15:15:13,019:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8845 (1.8702)
+2022-11-18 15:15:13,094:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9368 (1.8836)
+2022-11-18 15:15:13,146:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_352.pth.tar
+2022-11-18 15:15:13,147:INFO: 
+===> EPOCH: 353 (P2)
+2022-11-18 15:15:13,147:INFO: - Computing loss (training)
+2022-11-18 15:15:13,481:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7883 (1.7883)
+2022-11-18 15:15:13,629:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9282 (1.8598)
+2022-11-18 15:15:13,781:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0401 (1.9213)
+2022-11-18 15:15:13,922:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8336 (1.9055)
+2022-11-18 15:15:14,322:INFO: Dataset: univ                Batch:  1/15	Loss 1.8589 (1.8589)
+2022-11-18 15:15:14,480:INFO: Dataset: univ                Batch:  2/15	Loss 1.9105 (1.8837)
+2022-11-18 15:15:14,639:INFO: Dataset: univ                Batch:  3/15	Loss 1.8767 (1.8814)
+2022-11-18 15:15:14,795:INFO: Dataset: univ                Batch:  4/15	Loss 1.8798 (1.8810)
+2022-11-18 15:15:14,950:INFO: Dataset: univ                Batch:  5/15	Loss 1.8692 (1.8786)
+2022-11-18 15:15:15,106:INFO: Dataset: univ                Batch:  6/15	Loss 1.8736 (1.8778)
+2022-11-18 15:15:15,261:INFO: Dataset: univ                Batch:  7/15	Loss 1.8983 (1.8807)
+2022-11-18 15:15:15,414:INFO: Dataset: univ                Batch:  8/15	Loss 1.8744 (1.8799)
+2022-11-18 15:15:15,569:INFO: Dataset: univ                Batch:  9/15	Loss 1.9042 (1.8825)
+2022-11-18 15:15:15,723:INFO: Dataset: univ                Batch: 10/15	Loss 1.9585 (1.8891)
+2022-11-18 15:15:15,880:INFO: Dataset: univ                Batch: 11/15	Loss 1.9126 (1.8913)
+2022-11-18 15:15:16,035:INFO: Dataset: univ                Batch: 12/15	Loss 1.8759 (1.8900)
+2022-11-18 15:15:16,189:INFO: Dataset: univ                Batch: 13/15	Loss 1.8535 (1.8871)
+2022-11-18 15:15:16,344:INFO: Dataset: univ                Batch: 14/15	Loss 1.9125 (1.8890)
+2022-11-18 15:15:16,427:INFO: Dataset: univ                Batch: 15/15	Loss 1.8884 (1.8890)
+2022-11-18 15:15:16,821:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9492 (1.9492)
+2022-11-18 15:15:16,970:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9597 (1.9547)
+2022-11-18 15:15:17,120:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9146 (1.9409)
+2022-11-18 15:15:17,268:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8482 (1.9202)
+2022-11-18 15:15:17,417:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7836 (1.8947)
+2022-11-18 15:15:17,564:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9903 (1.9116)
+2022-11-18 15:15:17,716:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8762 (1.9066)
+2022-11-18 15:15:17,851:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8735 (1.9028)
+2022-11-18 15:15:18,258:INFO: Dataset: zara2               Batch:  1/18	Loss 2.0482 (2.0482)
+2022-11-18 15:15:18,412:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9236 (1.9818)
+2022-11-18 15:15:18,560:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8787 (1.9461)
+2022-11-18 15:15:18,707:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8507 (1.9223)
+2022-11-18 15:15:18,860:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8296 (1.9040)
+2022-11-18 15:15:19,012:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9196 (1.9065)
+2022-11-18 15:15:19,161:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9305 (1.9100)
+2022-11-18 15:15:19,307:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8302 (1.9003)
+2022-11-18 15:15:19,455:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9388 (1.9050)
+2022-11-18 15:15:19,600:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9276 (1.9077)
+2022-11-18 15:15:19,748:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8556 (1.9026)
+2022-11-18 15:15:19,896:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8694 (1.9000)
+2022-11-18 15:15:20,044:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9236 (1.9018)
+2022-11-18 15:15:20,193:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9849 (1.9074)
+2022-11-18 15:15:20,343:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8613 (1.9042)
+2022-11-18 15:15:20,491:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9679 (1.9080)
+2022-11-18 15:15:20,641:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8272 (1.9031)
+2022-11-18 15:15:20,777:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8679 (1.9013)
+2022-11-18 15:15:20,827:INFO: - Computing loss (validation)
+2022-11-18 15:15:21,104:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9141 (1.9141)
+2022-11-18 15:15:21,137:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6292 (1.8976)
+2022-11-18 15:15:21,439:INFO: Dataset: univ                Batch: 1/3	Loss 1.8743 (1.8743)
+2022-11-18 15:15:21,517:INFO: Dataset: univ                Batch: 2/3	Loss 1.8964 (1.8870)
+2022-11-18 15:15:21,590:INFO: Dataset: univ                Batch: 3/3	Loss 1.8770 (1.8837)
+2022-11-18 15:15:21,898:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9604 (1.9604)
+2022-11-18 15:15:21,943:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9598 (1.9603)
+2022-11-18 15:15:22,265:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9063 (1.9063)
+2022-11-18 15:15:22,346:INFO: Dataset: zara2               Batch: 2/5	Loss 2.0162 (1.9629)
+2022-11-18 15:15:22,423:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8553 (1.9289)
+2022-11-18 15:15:22,501:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8361 (1.9072)
+2022-11-18 15:15:22,579:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8577 (1.8973)
+2022-11-18 15:15:22,632:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_353.pth.tar
+2022-11-18 15:15:22,632:INFO: 
+===> EPOCH: 354 (P2)
+2022-11-18 15:15:22,632:INFO: - Computing loss (training)
+2022-11-18 15:15:22,980:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8543 (1.8543)
+2022-11-18 15:15:23,133:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9013 (1.8782)
+2022-11-18 15:15:23,282:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0089 (1.9211)
+2022-11-18 15:15:23,399:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8635 (1.9116)
+2022-11-18 15:15:23,784:INFO: Dataset: univ                Batch:  1/15	Loss 1.8888 (1.8888)
+2022-11-18 15:15:23,944:INFO: Dataset: univ                Batch:  2/15	Loss 1.8957 (1.8924)
+2022-11-18 15:15:24,098:INFO: Dataset: univ                Batch:  3/15	Loss 1.9201 (1.9013)
+2022-11-18 15:15:24,254:INFO: Dataset: univ                Batch:  4/15	Loss 1.8714 (1.8937)
+2022-11-18 15:15:24,412:INFO: Dataset: univ                Batch:  5/15	Loss 1.8535 (1.8848)
+2022-11-18 15:15:24,567:INFO: Dataset: univ                Batch:  6/15	Loss 1.8722 (1.8829)
+2022-11-18 15:15:24,721:INFO: Dataset: univ                Batch:  7/15	Loss 1.8502 (1.8779)
+2022-11-18 15:15:24,874:INFO: Dataset: univ                Batch:  8/15	Loss 1.8479 (1.8741)
+2022-11-18 15:15:25,028:INFO: Dataset: univ                Batch:  9/15	Loss 1.9117 (1.8783)
+2022-11-18 15:15:25,181:INFO: Dataset: univ                Batch: 10/15	Loss 1.8586 (1.8765)
+2022-11-18 15:15:25,336:INFO: Dataset: univ                Batch: 11/15	Loss 1.9056 (1.8792)
+2022-11-18 15:15:25,491:INFO: Dataset: univ                Batch: 12/15	Loss 1.8619 (1.8777)
+2022-11-18 15:15:25,645:INFO: Dataset: univ                Batch: 13/15	Loss 1.8642 (1.8767)
+2022-11-18 15:15:25,798:INFO: Dataset: univ                Batch: 14/15	Loss 1.8825 (1.8771)
+2022-11-18 15:15:25,880:INFO: Dataset: univ                Batch: 15/15	Loss 1.9612 (1.8786)
+2022-11-18 15:15:26,264:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9901 (1.9901)
+2022-11-18 15:15:26,412:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8974 (1.9456)
+2022-11-18 15:15:26,561:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9369 (1.9429)
+2022-11-18 15:15:26,708:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0040 (1.9589)
+2022-11-18 15:15:26,857:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9837 (1.9640)
+2022-11-18 15:15:27,007:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8913 (1.9519)
+2022-11-18 15:15:27,156:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9453 (1.9509)
+2022-11-18 15:15:27,291:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9392 (1.9498)
+2022-11-18 15:15:27,681:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9230 (1.9230)
+2022-11-18 15:15:27,833:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8482 (1.8838)
+2022-11-18 15:15:27,988:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8871 (1.8850)
+2022-11-18 15:15:28,138:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8400 (1.8747)
+2022-11-18 15:15:28,289:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9184 (1.8843)
+2022-11-18 15:15:28,444:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9371 (1.8927)
+2022-11-18 15:15:28,595:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9270 (1.8973)
+2022-11-18 15:15:28,743:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9010 (1.8978)
+2022-11-18 15:15:28,896:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8879 (1.8968)
+2022-11-18 15:15:29,045:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9218 (1.8990)
+2022-11-18 15:15:29,197:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9401 (1.9034)
+2022-11-18 15:15:29,350:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8595 (1.9000)
+2022-11-18 15:15:29,501:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9484 (1.9038)
+2022-11-18 15:15:29,651:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8723 (1.9014)
+2022-11-18 15:15:29,803:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8571 (1.8986)
+2022-11-18 15:15:29,953:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8511 (1.8954)
+2022-11-18 15:15:30,105:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8230 (1.8908)
+2022-11-18 15:15:30,244:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9198 (1.8923)
+2022-11-18 15:15:30,288:INFO: - Computing loss (validation)
+2022-11-18 15:15:30,554:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0277 (2.0277)
+2022-11-18 15:15:30,591:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8095 (2.0084)
+2022-11-18 15:15:30,895:INFO: Dataset: univ                Batch: 1/3	Loss 1.8608 (1.8608)
+2022-11-18 15:15:30,972:INFO: Dataset: univ                Batch: 2/3	Loss 1.9180 (1.8927)
+2022-11-18 15:15:31,044:INFO: Dataset: univ                Batch: 3/3	Loss 1.9110 (1.8973)
+2022-11-18 15:15:31,360:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8700 (1.8700)
+2022-11-18 15:15:31,407:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7732 (1.8476)
+2022-11-18 15:15:31,717:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8165 (1.8165)
+2022-11-18 15:15:31,795:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8635 (1.8405)
+2022-11-18 15:15:31,869:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8883 (1.8567)
+2022-11-18 15:15:31,945:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8541 (1.8561)
+2022-11-18 15:15:32,021:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8286 (1.8507)
+2022-11-18 15:15:32,073:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_354.pth.tar
+2022-11-18 15:15:32,073:INFO: 
+===> EPOCH: 355 (P2)
+2022-11-18 15:15:32,074:INFO: - Computing loss (training)
+2022-11-18 15:15:32,421:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9235 (1.9235)
+2022-11-18 15:15:32,573:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8808 (1.9020)
+2022-11-18 15:15:32,728:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9521 (1.9184)
+2022-11-18 15:15:32,847:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9168 (1.9181)
+2022-11-18 15:15:33,244:INFO: Dataset: univ                Batch:  1/15	Loss 1.8336 (1.8336)
+2022-11-18 15:15:33,403:INFO: Dataset: univ                Batch:  2/15	Loss 1.8956 (1.8655)
+2022-11-18 15:15:33,558:INFO: Dataset: univ                Batch:  3/15	Loss 1.9139 (1.8824)
+2022-11-18 15:15:33,794:INFO: Dataset: univ                Batch:  4/15	Loss 1.9156 (1.8909)
+2022-11-18 15:15:33,949:INFO: Dataset: univ                Batch:  5/15	Loss 1.8803 (1.8887)
+2022-11-18 15:15:34,106:INFO: Dataset: univ                Batch:  6/15	Loss 1.8779 (1.8869)
+2022-11-18 15:15:34,259:INFO: Dataset: univ                Batch:  7/15	Loss 1.9302 (1.8930)
+2022-11-18 15:15:34,413:INFO: Dataset: univ                Batch:  8/15	Loss 1.8984 (1.8937)
+2022-11-18 15:15:34,565:INFO: Dataset: univ                Batch:  9/15	Loss 1.8434 (1.8886)
+2022-11-18 15:15:34,719:INFO: Dataset: univ                Batch: 10/15	Loss 1.9134 (1.8910)
+2022-11-18 15:15:34,871:INFO: Dataset: univ                Batch: 11/15	Loss 1.9376 (1.8950)
+2022-11-18 15:15:35,028:INFO: Dataset: univ                Batch: 12/15	Loss 1.8453 (1.8902)
+2022-11-18 15:15:35,181:INFO: Dataset: univ                Batch: 13/15	Loss 1.9244 (1.8927)
+2022-11-18 15:15:35,337:INFO: Dataset: univ                Batch: 14/15	Loss 1.9165 (1.8945)
+2022-11-18 15:15:35,419:INFO: Dataset: univ                Batch: 15/15	Loss 1.8236 (1.8938)
+2022-11-18 15:15:35,804:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8808 (1.8808)
+2022-11-18 15:15:35,958:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9619 (1.9219)
+2022-11-18 15:15:36,109:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8814 (1.9086)
+2022-11-18 15:15:36,256:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0187 (1.9354)
+2022-11-18 15:15:36,406:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8991 (1.9277)
+2022-11-18 15:15:36,556:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8997 (1.9232)
+2022-11-18 15:15:36,705:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9981 (1.9346)
+2022-11-18 15:15:36,841:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9510 (1.9364)
+2022-11-18 15:15:37,281:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9378 (1.9378)
+2022-11-18 15:15:37,432:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9269 (1.9322)
+2022-11-18 15:15:37,582:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8877 (1.9188)
+2022-11-18 15:15:37,729:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9311 (1.9218)
+2022-11-18 15:15:37,877:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8254 (1.9046)
+2022-11-18 15:15:38,026:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9332 (1.9093)
+2022-11-18 15:15:38,175:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9558 (1.9149)
+2022-11-18 15:15:38,320:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9158 (1.9150)
+2022-11-18 15:15:38,468:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9217 (1.9157)
+2022-11-18 15:15:38,613:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9468 (1.9186)
+2022-11-18 15:15:38,766:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0161 (1.9267)
+2022-11-18 15:15:38,915:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9306 (1.9270)
+2022-11-18 15:15:39,065:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9042 (1.9253)
+2022-11-18 15:15:39,213:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9233 (1.9252)
+2022-11-18 15:15:39,363:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8358 (1.9187)
+2022-11-18 15:15:39,510:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8402 (1.9142)
+2022-11-18 15:15:39,659:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9209 (1.9145)
+2022-11-18 15:15:39,794:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9529 (1.9163)
+2022-11-18 15:15:39,839:INFO: - Computing loss (validation)
+2022-11-18 15:15:40,113:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8769 (1.8769)
+2022-11-18 15:15:40,148:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2920 (1.9067)
+2022-11-18 15:15:40,450:INFO: Dataset: univ                Batch: 1/3	Loss 1.8748 (1.8748)
+2022-11-18 15:15:40,527:INFO: Dataset: univ                Batch: 2/3	Loss 1.9162 (1.8973)
+2022-11-18 15:15:40,598:INFO: Dataset: univ                Batch: 3/3	Loss 1.9455 (1.9113)
+2022-11-18 15:15:40,917:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0298 (2.0298)
+2022-11-18 15:15:40,964:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9838 (2.0187)
+2022-11-18 15:15:41,276:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8858 (1.8858)
+2022-11-18 15:15:41,356:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8835 (1.8846)
+2022-11-18 15:15:41,433:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8655 (1.8783)
+2022-11-18 15:15:41,510:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8549 (1.8726)
+2022-11-18 15:15:41,586:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8775 (1.8735)
+2022-11-18 15:15:41,639:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_355.pth.tar
+2022-11-18 15:15:41,640:INFO: 
+===> EPOCH: 356 (P2)
+2022-11-18 15:15:41,640:INFO: - Computing loss (training)
+2022-11-18 15:15:41,984:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0454 (2.0454)
+2022-11-18 15:15:42,137:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9583 (2.0016)
+2022-11-18 15:15:42,290:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9775 (1.9941)
+2022-11-18 15:15:42,406:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9781 (1.9916)
+2022-11-18 15:15:42,801:INFO: Dataset: univ                Batch:  1/15	Loss 1.9010 (1.9010)
+2022-11-18 15:15:42,958:INFO: Dataset: univ                Batch:  2/15	Loss 1.9224 (1.9119)
+2022-11-18 15:15:43,121:INFO: Dataset: univ                Batch:  3/15	Loss 1.8964 (1.9065)
+2022-11-18 15:15:43,279:INFO: Dataset: univ                Batch:  4/15	Loss 1.8831 (1.9004)
+2022-11-18 15:15:43,435:INFO: Dataset: univ                Batch:  5/15	Loss 1.9129 (1.9028)
+2022-11-18 15:15:43,594:INFO: Dataset: univ                Batch:  6/15	Loss 1.8533 (1.8952)
+2022-11-18 15:15:43,751:INFO: Dataset: univ                Batch:  7/15	Loss 1.9414 (1.9017)
+2022-11-18 15:15:43,906:INFO: Dataset: univ                Batch:  8/15	Loss 1.9353 (1.9062)
+2022-11-18 15:15:44,063:INFO: Dataset: univ                Batch:  9/15	Loss 1.8871 (1.9042)
+2022-11-18 15:15:44,218:INFO: Dataset: univ                Batch: 10/15	Loss 1.8969 (1.9034)
+2022-11-18 15:15:44,374:INFO: Dataset: univ                Batch: 11/15	Loss 1.8866 (1.9020)
+2022-11-18 15:15:44,532:INFO: Dataset: univ                Batch: 12/15	Loss 1.8989 (1.9018)
+2022-11-18 15:15:44,690:INFO: Dataset: univ                Batch: 13/15	Loss 1.9230 (1.9037)
+2022-11-18 15:15:44,848:INFO: Dataset: univ                Batch: 14/15	Loss 1.9028 (1.9036)
+2022-11-18 15:15:44,933:INFO: Dataset: univ                Batch: 15/15	Loss 1.9273 (1.9039)
+2022-11-18 15:15:45,334:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9408 (1.9408)
+2022-11-18 15:15:45,486:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9080 (1.9244)
+2022-11-18 15:15:45,639:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9833 (1.9433)
+2022-11-18 15:15:45,789:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9703 (1.9502)
+2022-11-18 15:15:45,941:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8734 (1.9346)
+2022-11-18 15:15:46,094:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9971 (1.9445)
+2022-11-18 15:15:46,247:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8582 (1.9323)
+2022-11-18 15:15:46,385:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8348 (1.9217)
+2022-11-18 15:15:46,846:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9343 (1.9343)
+2022-11-18 15:15:46,994:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9495 (1.9418)
+2022-11-18 15:15:47,144:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8448 (1.9094)
+2022-11-18 15:15:47,290:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0177 (1.9375)
+2022-11-18 15:15:47,438:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8767 (1.9247)
+2022-11-18 15:15:47,588:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8135 (1.9075)
+2022-11-18 15:15:47,738:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9983 (1.9208)
+2022-11-18 15:15:47,886:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9163 (1.9203)
+2022-11-18 15:15:48,034:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8070 (1.9077)
+2022-11-18 15:15:48,180:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8662 (1.9035)
+2022-11-18 15:15:48,329:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8986 (1.9031)
+2022-11-18 15:15:48,477:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0177 (1.9124)
+2022-11-18 15:15:48,625:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8373 (1.9071)
+2022-11-18 15:15:48,772:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9031 (1.9068)
+2022-11-18 15:15:48,923:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8452 (1.9026)
+2022-11-18 15:15:49,070:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8555 (1.8995)
+2022-11-18 15:15:49,220:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9847 (1.9045)
+2022-11-18 15:15:49,357:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8159 (1.9000)
+2022-11-18 15:15:49,402:INFO: - Computing loss (validation)
+2022-11-18 15:15:49,661:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9852 (1.9852)
+2022-11-18 15:15:49,695:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9283 (1.9815)
+2022-11-18 15:15:49,999:INFO: Dataset: univ                Batch: 1/3	Loss 1.9147 (1.9147)
+2022-11-18 15:15:50,077:INFO: Dataset: univ                Batch: 2/3	Loss 1.8654 (1.8886)
+2022-11-18 15:15:50,149:INFO: Dataset: univ                Batch: 3/3	Loss 1.8543 (1.8782)
+2022-11-18 15:15:50,459:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8182 (1.8182)
+2022-11-18 15:15:50,506:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0045 (1.8577)
+2022-11-18 15:15:50,808:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8789 (1.8789)
+2022-11-18 15:15:50,885:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8600 (1.8693)
+2022-11-18 15:15:50,959:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9567 (1.9001)
+2022-11-18 15:15:51,033:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9292 (1.9073)
+2022-11-18 15:15:51,107:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9311 (1.9119)
+2022-11-18 15:15:51,159:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_356.pth.tar
+2022-11-18 15:15:51,159:INFO: 
+===> EPOCH: 357 (P2)
+2022-11-18 15:15:51,160:INFO: - Computing loss (training)
+2022-11-18 15:15:51,503:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9673 (1.9673)
+2022-11-18 15:15:51,654:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9433 (1.9550)
+2022-11-18 15:15:51,803:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8639 (1.9266)
+2022-11-18 15:15:51,919:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7551 (1.8978)
+2022-11-18 15:15:52,326:INFO: Dataset: univ                Batch:  1/15	Loss 1.8911 (1.8911)
+2022-11-18 15:15:52,481:INFO: Dataset: univ                Batch:  2/15	Loss 1.9451 (1.9177)
+2022-11-18 15:15:52,637:INFO: Dataset: univ                Batch:  3/15	Loss 1.9051 (1.9135)
+2022-11-18 15:15:52,792:INFO: Dataset: univ                Batch:  4/15	Loss 1.8761 (1.9045)
+2022-11-18 15:15:52,946:INFO: Dataset: univ                Batch:  5/15	Loss 1.8886 (1.9016)
+2022-11-18 15:15:53,104:INFO: Dataset: univ                Batch:  6/15	Loss 1.8625 (1.8945)
+2022-11-18 15:15:53,258:INFO: Dataset: univ                Batch:  7/15	Loss 1.9207 (1.8980)
+2022-11-18 15:15:53,411:INFO: Dataset: univ                Batch:  8/15	Loss 1.8943 (1.8976)
+2022-11-18 15:15:53,565:INFO: Dataset: univ                Batch:  9/15	Loss 1.9117 (1.8992)
+2022-11-18 15:15:53,717:INFO: Dataset: univ                Batch: 10/15	Loss 1.8817 (1.8975)
+2022-11-18 15:15:53,873:INFO: Dataset: univ                Batch: 11/15	Loss 1.8675 (1.8948)
+2022-11-18 15:15:54,026:INFO: Dataset: univ                Batch: 12/15	Loss 1.8854 (1.8940)
+2022-11-18 15:15:54,181:INFO: Dataset: univ                Batch: 13/15	Loss 1.8985 (1.8944)
+2022-11-18 15:15:54,335:INFO: Dataset: univ                Batch: 14/15	Loss 1.8940 (1.8943)
+2022-11-18 15:15:54,418:INFO: Dataset: univ                Batch: 15/15	Loss 1.9522 (1.8950)
+2022-11-18 15:15:54,813:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9147 (1.9147)
+2022-11-18 15:15:54,964:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9606 (1.9371)
+2022-11-18 15:15:55,114:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8602 (1.9134)
+2022-11-18 15:15:55,265:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9731 (1.9273)
+2022-11-18 15:15:55,416:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9146 (1.9247)
+2022-11-18 15:15:55,565:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8089 (1.9055)
+2022-11-18 15:15:55,715:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8536 (1.8981)
+2022-11-18 15:15:55,850:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0091 (1.9106)
+2022-11-18 15:15:56,231:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9560 (1.9560)
+2022-11-18 15:15:56,382:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8949 (1.9256)
+2022-11-18 15:15:56,530:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8209 (1.8909)
+2022-11-18 15:15:56,681:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8127 (1.8733)
+2022-11-18 15:15:56,830:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8081 (1.8596)
+2022-11-18 15:15:56,980:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8697 (1.8613)
+2022-11-18 15:15:57,128:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9633 (1.8754)
+2022-11-18 15:15:57,275:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8822 (1.8762)
+2022-11-18 15:15:57,423:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0075 (1.8920)
+2022-11-18 15:15:57,570:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9885 (1.9013)
+2022-11-18 15:15:57,718:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0048 (1.9100)
+2022-11-18 15:15:57,867:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9524 (1.9132)
+2022-11-18 15:15:58,014:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9620 (1.9173)
+2022-11-18 15:15:58,164:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9085 (1.9167)
+2022-11-18 15:15:58,314:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8456 (1.9123)
+2022-11-18 15:15:58,463:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8485 (1.9085)
+2022-11-18 15:15:58,613:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9066 (1.9084)
+2022-11-18 15:15:58,749:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8189 (1.9042)
+2022-11-18 15:15:58,794:INFO: - Computing loss (validation)
+2022-11-18 15:15:59,054:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9327 (1.9327)
+2022-11-18 15:15:59,090:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6929 (1.9172)
+2022-11-18 15:15:59,400:INFO: Dataset: univ                Batch: 1/3	Loss 1.8788 (1.8788)
+2022-11-18 15:15:59,479:INFO: Dataset: univ                Batch: 2/3	Loss 1.8798 (1.8793)
+2022-11-18 15:15:59,551:INFO: Dataset: univ                Batch: 3/3	Loss 1.9025 (1.8861)
+2022-11-18 15:15:59,853:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8826 (1.8826)
+2022-11-18 15:15:59,900:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9400 (1.8973)
+2022-11-18 15:16:00,211:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9272 (1.9272)
+2022-11-18 15:16:00,288:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9178 (1.9227)
+2022-11-18 15:16:00,366:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8740 (1.9058)
+2022-11-18 15:16:00,445:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8856 (1.9009)
+2022-11-18 15:16:00,521:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9498 (1.9107)
+2022-11-18 15:16:00,574:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_357.pth.tar
+2022-11-18 15:16:00,574:INFO: 
+===> EPOCH: 358 (P2)
+2022-11-18 15:16:00,575:INFO: - Computing loss (training)
+2022-11-18 15:16:00,922:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9083 (1.9083)
+2022-11-18 15:16:01,076:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9373 (1.9232)
+2022-11-18 15:16:01,229:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9694 (1.9393)
+2022-11-18 15:16:01,346:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9918 (1.9480)
+2022-11-18 15:16:01,756:INFO: Dataset: univ                Batch:  1/15	Loss 1.8857 (1.8857)
+2022-11-18 15:16:01,917:INFO: Dataset: univ                Batch:  2/15	Loss 1.9129 (1.8991)
+2022-11-18 15:16:02,077:INFO: Dataset: univ                Batch:  3/15	Loss 1.8701 (1.8891)
+2022-11-18 15:16:02,240:INFO: Dataset: univ                Batch:  4/15	Loss 1.9133 (1.8953)
+2022-11-18 15:16:02,399:INFO: Dataset: univ                Batch:  5/15	Loss 1.9127 (1.8990)
+2022-11-18 15:16:02,557:INFO: Dataset: univ                Batch:  6/15	Loss 1.9042 (1.8999)
+2022-11-18 15:16:02,720:INFO: Dataset: univ                Batch:  7/15	Loss 1.9057 (1.9007)
+2022-11-18 15:16:02,878:INFO: Dataset: univ                Batch:  8/15	Loss 1.9711 (1.9098)
+2022-11-18 15:16:03,036:INFO: Dataset: univ                Batch:  9/15	Loss 1.8611 (1.9040)
+2022-11-18 15:16:03,193:INFO: Dataset: univ                Batch: 10/15	Loss 1.8740 (1.9007)
+2022-11-18 15:16:03,350:INFO: Dataset: univ                Batch: 11/15	Loss 1.8989 (1.9005)
+2022-11-18 15:16:03,508:INFO: Dataset: univ                Batch: 12/15	Loss 1.9210 (1.9022)
+2022-11-18 15:16:03,665:INFO: Dataset: univ                Batch: 13/15	Loss 1.8852 (1.9010)
+2022-11-18 15:16:03,821:INFO: Dataset: univ                Batch: 14/15	Loss 1.8889 (1.9001)
+2022-11-18 15:16:03,905:INFO: Dataset: univ                Batch: 15/15	Loss 1.8741 (1.8997)
+2022-11-18 15:16:04,295:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9924 (1.9924)
+2022-11-18 15:16:04,442:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9471 (1.9687)
+2022-11-18 15:16:04,589:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8678 (1.9356)
+2022-11-18 15:16:04,736:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8425 (1.9128)
+2022-11-18 15:16:04,885:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8030 (1.8921)
+2022-11-18 15:16:05,033:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9695 (1.9057)
+2022-11-18 15:16:05,181:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9429 (1.9113)
+2022-11-18 15:16:05,315:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9471 (1.9152)
+2022-11-18 15:16:05,705:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8077 (1.8077)
+2022-11-18 15:16:05,855:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7860 (1.7973)
+2022-11-18 15:16:06,009:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9936 (1.8638)
+2022-11-18 15:16:06,159:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9018 (1.8726)
+2022-11-18 15:16:06,316:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9513 (1.8882)
+2022-11-18 15:16:06,467:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9177 (1.8933)
+2022-11-18 15:16:06,618:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8133 (1.8821)
+2022-11-18 15:16:06,767:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8676 (1.8804)
+2022-11-18 15:16:06,918:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9369 (1.8865)
+2022-11-18 15:16:07,067:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8388 (1.8820)
+2022-11-18 15:16:07,220:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8955 (1.8831)
+2022-11-18 15:16:07,369:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8765 (1.8826)
+2022-11-18 15:16:07,520:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7965 (1.8760)
+2022-11-18 15:16:07,670:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8958 (1.8775)
+2022-11-18 15:16:07,823:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0204 (1.8871)
+2022-11-18 15:16:07,975:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8874 (1.8871)
+2022-11-18 15:16:08,127:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9181 (1.8888)
+2022-11-18 15:16:08,266:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0103 (1.8956)
+2022-11-18 15:16:08,310:INFO: - Computing loss (validation)
+2022-11-18 15:16:08,574:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9460 (1.9460)
+2022-11-18 15:16:08,609:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7755 (1.9350)
+2022-11-18 15:16:08,916:INFO: Dataset: univ                Batch: 1/3	Loss 1.8652 (1.8652)
+2022-11-18 15:16:08,996:INFO: Dataset: univ                Batch: 2/3	Loss 1.8646 (1.8649)
+2022-11-18 15:16:09,068:INFO: Dataset: univ                Batch: 3/3	Loss 1.8618 (1.8639)
+2022-11-18 15:16:09,366:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8571 (1.8571)
+2022-11-18 15:16:09,410:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8348 (1.8521)
+2022-11-18 15:16:09,721:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9199 (1.9199)
+2022-11-18 15:16:09,798:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8337 (1.8763)
+2022-11-18 15:16:09,878:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9147 (1.8898)
+2022-11-18 15:16:09,955:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9018 (1.8928)
+2022-11-18 15:16:10,033:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9069 (1.8956)
+2022-11-18 15:16:10,084:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_358.pth.tar
+2022-11-18 15:16:10,084:INFO: 
+===> EPOCH: 359 (P2)
+2022-11-18 15:16:10,084:INFO: - Computing loss (training)
+2022-11-18 15:16:10,420:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9440 (1.9440)
+2022-11-18 15:16:10,574:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9048 (1.9241)
+2022-11-18 15:16:10,725:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0058 (1.9508)
+2022-11-18 15:16:10,844:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9150 (1.9445)
+2022-11-18 15:16:11,282:INFO: Dataset: univ                Batch:  1/15	Loss 1.8282 (1.8282)
+2022-11-18 15:16:11,441:INFO: Dataset: univ                Batch:  2/15	Loss 1.8671 (1.8481)
+2022-11-18 15:16:11,598:INFO: Dataset: univ                Batch:  3/15	Loss 1.9014 (1.8649)
+2022-11-18 15:16:11,751:INFO: Dataset: univ                Batch:  4/15	Loss 1.9594 (1.8890)
+2022-11-18 15:16:11,906:INFO: Dataset: univ                Batch:  5/15	Loss 1.8949 (1.8902)
+2022-11-18 15:16:12,063:INFO: Dataset: univ                Batch:  6/15	Loss 1.9032 (1.8925)
+2022-11-18 15:16:12,218:INFO: Dataset: univ                Batch:  7/15	Loss 1.9870 (1.9041)
+2022-11-18 15:16:12,371:INFO: Dataset: univ                Batch:  8/15	Loss 1.9087 (1.9046)
+2022-11-18 15:16:12,525:INFO: Dataset: univ                Batch:  9/15	Loss 1.8681 (1.9009)
+2022-11-18 15:16:12,678:INFO: Dataset: univ                Batch: 10/15	Loss 1.8907 (1.8998)
+2022-11-18 15:16:12,833:INFO: Dataset: univ                Batch: 11/15	Loss 1.8949 (1.8994)
+2022-11-18 15:16:12,989:INFO: Dataset: univ                Batch: 12/15	Loss 1.8887 (1.8985)
+2022-11-18 15:16:13,145:INFO: Dataset: univ                Batch: 13/15	Loss 1.8668 (1.8962)
+2022-11-18 15:16:13,299:INFO: Dataset: univ                Batch: 14/15	Loss 1.9178 (1.8977)
+2022-11-18 15:16:13,382:INFO: Dataset: univ                Batch: 15/15	Loss 1.9443 (1.8984)
+2022-11-18 15:16:13,771:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8325 (1.8325)
+2022-11-18 15:16:13,920:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8900 (1.8595)
+2022-11-18 15:16:14,070:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9183 (1.8791)
+2022-11-18 15:16:14,222:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0024 (1.9096)
+2022-11-18 15:16:14,372:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9190 (1.9117)
+2022-11-18 15:16:14,523:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9909 (1.9241)
+2022-11-18 15:16:14,672:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9433 (1.9269)
+2022-11-18 15:16:14,809:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9060 (1.9245)
+2022-11-18 15:16:15,205:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8041 (1.8041)
+2022-11-18 15:16:15,358:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9461 (1.8765)
+2022-11-18 15:16:15,507:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8588 (1.8709)
+2022-11-18 15:16:15,654:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8686 (1.8704)
+2022-11-18 15:16:15,803:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9036 (1.8766)
+2022-11-18 15:16:15,961:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9904 (1.8952)
+2022-11-18 15:16:16,109:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8231 (1.8851)
+2022-11-18 15:16:16,256:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9009 (1.8871)
+2022-11-18 15:16:16,403:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8568 (1.8835)
+2022-11-18 15:16:16,550:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9068 (1.8857)
+2022-11-18 15:16:16,700:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8570 (1.8831)
+2022-11-18 15:16:16,845:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8407 (1.8795)
+2022-11-18 15:16:16,993:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9308 (1.8837)
+2022-11-18 15:16:17,141:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8703 (1.8828)
+2022-11-18 15:16:17,289:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8477 (1.8807)
+2022-11-18 15:16:17,438:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9610 (1.8853)
+2022-11-18 15:16:17,587:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8430 (1.8828)
+2022-11-18 15:16:17,726:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8865 (1.8830)
+2022-11-18 15:16:17,771:INFO: - Computing loss (validation)
+2022-11-18 15:16:18,038:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9328 (1.9328)
+2022-11-18 15:16:18,072:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0451 (1.9439)
+2022-11-18 15:16:18,366:INFO: Dataset: univ                Batch: 1/3	Loss 1.8716 (1.8716)
+2022-11-18 15:16:18,447:INFO: Dataset: univ                Batch: 2/3	Loss 1.8344 (1.8512)
+2022-11-18 15:16:18,522:INFO: Dataset: univ                Batch: 3/3	Loss 1.8597 (1.8539)
+2022-11-18 15:16:18,815:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8927 (1.8927)
+2022-11-18 15:16:18,862:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0780 (1.9409)
+2022-11-18 15:16:19,171:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9009 (1.9009)
+2022-11-18 15:16:19,244:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9378 (1.9200)
+2022-11-18 15:16:19,318:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8830 (1.9080)
+2022-11-18 15:16:19,391:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9582 (1.9202)
+2022-11-18 15:16:19,464:INFO: Dataset: zara2               Batch: 5/5	Loss 2.0055 (1.9355)
+2022-11-18 15:16:19,517:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_359.pth.tar
+2022-11-18 15:16:19,517:INFO: 
+===> EPOCH: 360 (P2)
+2022-11-18 15:16:19,518:INFO: - Computing loss (training)
+2022-11-18 15:16:19,866:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9160 (1.9160)
+2022-11-18 15:16:20,018:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9627 (1.9392)
+2022-11-18 15:16:20,170:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0000 (1.9603)
+2022-11-18 15:16:20,285:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8672 (1.9446)
+2022-11-18 15:16:20,690:INFO: Dataset: univ                Batch:  1/15	Loss 1.9229 (1.9229)
+2022-11-18 15:16:20,849:INFO: Dataset: univ                Batch:  2/15	Loss 1.9039 (1.9135)
+2022-11-18 15:16:21,010:INFO: Dataset: univ                Batch:  3/15	Loss 1.9103 (1.9124)
+2022-11-18 15:16:21,168:INFO: Dataset: univ                Batch:  4/15	Loss 1.8945 (1.9080)
+2022-11-18 15:16:21,326:INFO: Dataset: univ                Batch:  5/15	Loss 1.8722 (1.9013)
+2022-11-18 15:16:21,487:INFO: Dataset: univ                Batch:  6/15	Loss 1.9091 (1.9025)
+2022-11-18 15:16:21,644:INFO: Dataset: univ                Batch:  7/15	Loss 1.8520 (1.8959)
+2022-11-18 15:16:21,799:INFO: Dataset: univ                Batch:  8/15	Loss 1.9179 (1.8985)
+2022-11-18 15:16:21,957:INFO: Dataset: univ                Batch:  9/15	Loss 1.8983 (1.8985)
+2022-11-18 15:16:22,114:INFO: Dataset: univ                Batch: 10/15	Loss 1.8749 (1.8963)
+2022-11-18 15:16:22,276:INFO: Dataset: univ                Batch: 11/15	Loss 1.8816 (1.8949)
+2022-11-18 15:16:22,433:INFO: Dataset: univ                Batch: 12/15	Loss 1.9152 (1.8965)
+2022-11-18 15:16:22,628:INFO: Dataset: univ                Batch: 13/15	Loss 1.8917 (1.8962)
+2022-11-18 15:16:22,794:INFO: Dataset: univ                Batch: 14/15	Loss 1.8523 (1.8930)
+2022-11-18 15:16:22,879:INFO: Dataset: univ                Batch: 15/15	Loss 1.7972 (1.8918)
+2022-11-18 15:16:23,266:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8998 (1.8998)
+2022-11-18 15:16:23,416:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8356 (1.8663)
+2022-11-18 15:16:23,567:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8729 (1.8685)
+2022-11-18 15:16:23,719:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8803 (1.8713)
+2022-11-18 15:16:23,870:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8376 (1.8644)
+2022-11-18 15:16:24,020:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7713 (1.8501)
+2022-11-18 15:16:24,171:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8865 (1.8558)
+2022-11-18 15:16:24,308:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9050 (1.8617)
+2022-11-18 15:16:24,702:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9220 (1.9220)
+2022-11-18 15:16:24,852:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8481 (1.8829)
+2022-11-18 15:16:25,004:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7911 (1.8538)
+2022-11-18 15:16:25,159:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8758 (1.8590)
+2022-11-18 15:16:25,310:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9390 (1.8756)
+2022-11-18 15:16:25,470:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9318 (1.8850)
+2022-11-18 15:16:25,620:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9556 (1.8957)
+2022-11-18 15:16:25,768:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8222 (1.8871)
+2022-11-18 15:16:25,920:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0550 (1.9051)
+2022-11-18 15:16:26,069:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8567 (1.9003)
+2022-11-18 15:16:26,222:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9511 (1.9051)
+2022-11-18 15:16:26,371:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0135 (1.9142)
+2022-11-18 15:16:26,521:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9474 (1.9165)
+2022-11-18 15:16:26,670:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8940 (1.9148)
+2022-11-18 15:16:26,821:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8775 (1.9124)
+2022-11-18 15:16:26,972:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9107 (1.9123)
+2022-11-18 15:16:27,124:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9082 (1.9120)
+2022-11-18 15:16:27,261:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7989 (1.9065)
+2022-11-18 15:16:27,312:INFO: - Computing loss (validation)
+2022-11-18 15:16:27,584:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9431 (1.9431)
+2022-11-18 15:16:27,619:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0461 (1.9502)
+2022-11-18 15:16:27,929:INFO: Dataset: univ                Batch: 1/3	Loss 1.8326 (1.8326)
+2022-11-18 15:16:28,011:INFO: Dataset: univ                Batch: 2/3	Loss 1.9150 (1.8750)
+2022-11-18 15:16:28,087:INFO: Dataset: univ                Batch: 3/3	Loss 1.9009 (1.8832)
+2022-11-18 15:16:28,375:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9281 (1.9281)
+2022-11-18 15:16:28,422:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7857 (1.8929)
+2022-11-18 15:16:28,736:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9675 (1.9675)
+2022-11-18 15:16:28,820:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8821 (1.9255)
+2022-11-18 15:16:28,899:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9154 (1.9221)
+2022-11-18 15:16:28,978:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8972 (1.9162)
+2022-11-18 15:16:29,055:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7861 (1.8902)
+2022-11-18 15:16:29,107:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_360.pth.tar
+2022-11-18 15:16:29,107:INFO: 
+===> EPOCH: 361 (P2)
+2022-11-18 15:16:29,108:INFO: - Computing loss (training)
+2022-11-18 15:16:29,445:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0412 (2.0412)
+2022-11-18 15:16:29,594:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8479 (1.9463)
+2022-11-18 15:16:29,742:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8867 (1.9245)
+2022-11-18 15:16:29,858:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8621 (1.9136)
+2022-11-18 15:16:30,247:INFO: Dataset: univ                Batch:  1/15	Loss 1.8509 (1.8509)
+2022-11-18 15:16:30,408:INFO: Dataset: univ                Batch:  2/15	Loss 1.8440 (1.8474)
+2022-11-18 15:16:30,568:INFO: Dataset: univ                Batch:  3/15	Loss 1.9349 (1.8772)
+2022-11-18 15:16:30,727:INFO: Dataset: univ                Batch:  4/15	Loss 1.8610 (1.8731)
+2022-11-18 15:16:30,887:INFO: Dataset: univ                Batch:  5/15	Loss 1.8893 (1.8765)
+2022-11-18 15:16:31,045:INFO: Dataset: univ                Batch:  6/15	Loss 1.8590 (1.8734)
+2022-11-18 15:16:31,202:INFO: Dataset: univ                Batch:  7/15	Loss 1.9120 (1.8783)
+2022-11-18 15:16:31,359:INFO: Dataset: univ                Batch:  8/15	Loss 1.8873 (1.8794)
+2022-11-18 15:16:31,517:INFO: Dataset: univ                Batch:  9/15	Loss 1.8760 (1.8790)
+2022-11-18 15:16:31,671:INFO: Dataset: univ                Batch: 10/15	Loss 1.8510 (1.8762)
+2022-11-18 15:16:31,832:INFO: Dataset: univ                Batch: 11/15	Loss 1.8669 (1.8753)
+2022-11-18 15:16:31,987:INFO: Dataset: univ                Batch: 12/15	Loss 1.8848 (1.8760)
+2022-11-18 15:16:32,146:INFO: Dataset: univ                Batch: 13/15	Loss 1.8980 (1.8779)
+2022-11-18 15:16:32,302:INFO: Dataset: univ                Batch: 14/15	Loss 1.8742 (1.8776)
+2022-11-18 15:16:32,386:INFO: Dataset: univ                Batch: 15/15	Loss 1.9128 (1.8781)
+2022-11-18 15:16:32,794:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9230 (1.9230)
+2022-11-18 15:16:32,944:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9304 (1.9264)
+2022-11-18 15:16:33,093:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9332 (1.9287)
+2022-11-18 15:16:33,241:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8825 (1.9172)
+2022-11-18 15:16:33,393:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8893 (1.9119)
+2022-11-18 15:16:33,542:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8425 (1.9012)
+2022-11-18 15:16:33,692:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8229 (1.8907)
+2022-11-18 15:16:33,827:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9452 (1.8966)
+2022-11-18 15:16:34,258:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9088 (1.9088)
+2022-11-18 15:16:34,408:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9204 (1.9145)
+2022-11-18 15:16:34,559:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8922 (1.9067)
+2022-11-18 15:16:34,707:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8619 (1.8955)
+2022-11-18 15:16:34,859:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8319 (1.8821)
+2022-11-18 15:16:35,012:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9139 (1.8870)
+2022-11-18 15:16:35,164:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8305 (1.8796)
+2022-11-18 15:16:35,313:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8626 (1.8776)
+2022-11-18 15:16:35,464:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7829 (1.8671)
+2022-11-18 15:16:35,613:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8067 (1.8606)
+2022-11-18 15:16:35,763:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8075 (1.8560)
+2022-11-18 15:16:35,914:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9267 (1.8621)
+2022-11-18 15:16:36,065:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9584 (1.8696)
+2022-11-18 15:16:36,216:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8259 (1.8663)
+2022-11-18 15:16:36,367:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8455 (1.8648)
+2022-11-18 15:16:36,518:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9056 (1.8673)
+2022-11-18 15:16:36,670:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9260 (1.8708)
+2022-11-18 15:16:36,809:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9379 (1.8743)
+2022-11-18 15:16:36,855:INFO: - Computing loss (validation)
+2022-11-18 15:16:37,119:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9315 (1.9315)
+2022-11-18 15:16:37,154:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3574 (1.9620)
+2022-11-18 15:16:37,459:INFO: Dataset: univ                Batch: 1/3	Loss 1.8910 (1.8910)
+2022-11-18 15:16:37,539:INFO: Dataset: univ                Batch: 2/3	Loss 1.9495 (1.9203)
+2022-11-18 15:16:37,615:INFO: Dataset: univ                Batch: 3/3	Loss 1.8610 (1.9034)
+2022-11-18 15:16:37,918:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0049 (2.0049)
+2022-11-18 15:16:37,963:INFO: Dataset: zara1               Batch: 2/2	Loss 2.1041 (2.0269)
+2022-11-18 15:16:38,265:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8123 (1.8123)
+2022-11-18 15:16:38,342:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8083 (1.8102)
+2022-11-18 15:16:38,418:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8338 (1.8183)
+2022-11-18 15:16:38,494:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8155 (1.8177)
+2022-11-18 15:16:38,569:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8313 (1.8201)
+2022-11-18 15:16:38,621:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_361.pth.tar
+2022-11-18 15:16:38,622:INFO: 
+===> EPOCH: 362 (P2)
+2022-11-18 15:16:38,622:INFO: - Computing loss (training)
+2022-11-18 15:16:38,972:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8685 (1.8685)
+2022-11-18 15:16:39,122:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0026 (1.9374)
+2022-11-18 15:16:39,271:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8047 (1.8915)
+2022-11-18 15:16:39,385:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8533 (1.8851)
+2022-11-18 15:16:39,780:INFO: Dataset: univ                Batch:  1/15	Loss 1.8986 (1.8986)
+2022-11-18 15:16:39,942:INFO: Dataset: univ                Batch:  2/15	Loss 1.9061 (1.9024)
+2022-11-18 15:16:40,103:INFO: Dataset: univ                Batch:  3/15	Loss 1.9354 (1.9141)
+2022-11-18 15:16:40,264:INFO: Dataset: univ                Batch:  4/15	Loss 1.9673 (1.9273)
+2022-11-18 15:16:40,426:INFO: Dataset: univ                Batch:  5/15	Loss 1.8723 (1.9151)
+2022-11-18 15:16:40,582:INFO: Dataset: univ                Batch:  6/15	Loss 1.8583 (1.9055)
+2022-11-18 15:16:40,739:INFO: Dataset: univ                Batch:  7/15	Loss 1.8807 (1.9020)
+2022-11-18 15:16:40,894:INFO: Dataset: univ                Batch:  8/15	Loss 1.8935 (1.9010)
+2022-11-18 15:16:41,053:INFO: Dataset: univ                Batch:  9/15	Loss 1.8732 (1.8976)
+2022-11-18 15:16:41,210:INFO: Dataset: univ                Batch: 10/15	Loss 1.8614 (1.8937)
+2022-11-18 15:16:41,370:INFO: Dataset: univ                Batch: 11/15	Loss 1.9247 (1.8963)
+2022-11-18 15:16:41,526:INFO: Dataset: univ                Batch: 12/15	Loss 1.8896 (1.8958)
+2022-11-18 15:16:41,683:INFO: Dataset: univ                Batch: 13/15	Loss 1.9086 (1.8968)
+2022-11-18 15:16:41,840:INFO: Dataset: univ                Batch: 14/15	Loss 1.8927 (1.8965)
+2022-11-18 15:16:41,923:INFO: Dataset: univ                Batch: 15/15	Loss 1.8176 (1.8955)
+2022-11-18 15:16:42,314:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9756 (1.9756)
+2022-11-18 15:16:42,464:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9904 (1.9824)
+2022-11-18 15:16:42,614:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9290 (1.9641)
+2022-11-18 15:16:42,762:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9747 (1.9669)
+2022-11-18 15:16:42,913:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9113 (1.9554)
+2022-11-18 15:16:43,065:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9416 (1.9529)
+2022-11-18 15:16:43,217:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9500 (1.9525)
+2022-11-18 15:16:43,353:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7721 (1.9328)
+2022-11-18 15:16:43,754:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9310 (1.9310)
+2022-11-18 15:16:43,904:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9203 (1.9256)
+2022-11-18 15:16:44,058:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8706 (1.9066)
+2022-11-18 15:16:44,210:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8917 (1.9029)
+2022-11-18 15:16:44,361:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8116 (1.8831)
+2022-11-18 15:16:44,512:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8989 (1.8859)
+2022-11-18 15:16:44,661:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8819 (1.8854)
+2022-11-18 15:16:44,808:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9407 (1.8925)
+2022-11-18 15:16:44,958:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9098 (1.8943)
+2022-11-18 15:16:45,106:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8243 (1.8873)
+2022-11-18 15:16:45,256:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8442 (1.8833)
+2022-11-18 15:16:45,403:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8764 (1.8827)
+2022-11-18 15:16:45,551:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7997 (1.8763)
+2022-11-18 15:16:45,699:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8653 (1.8755)
+2022-11-18 15:16:45,848:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9410 (1.8798)
+2022-11-18 15:16:45,997:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8916 (1.8805)
+2022-11-18 15:16:46,147:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0506 (1.8897)
+2022-11-18 15:16:46,283:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9801 (1.8939)
+2022-11-18 15:16:46,330:INFO: - Computing loss (validation)
+2022-11-18 15:16:46,596:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0919 (2.0919)
+2022-11-18 15:16:46,631:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9625 (2.0827)
+2022-11-18 15:16:46,943:INFO: Dataset: univ                Batch: 1/3	Loss 1.9003 (1.9003)
+2022-11-18 15:16:47,026:INFO: Dataset: univ                Batch: 2/3	Loss 1.8393 (1.8707)
+2022-11-18 15:16:47,102:INFO: Dataset: univ                Batch: 3/3	Loss 1.9019 (1.8794)
+2022-11-18 15:16:47,402:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9860 (1.9860)
+2022-11-18 15:16:47,449:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8758 (1.9580)
+2022-11-18 15:16:47,773:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8915 (1.8915)
+2022-11-18 15:16:47,852:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9174 (1.9043)
+2022-11-18 15:16:47,929:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9699 (1.9248)
+2022-11-18 15:16:48,006:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9167 (1.9228)
+2022-11-18 15:16:48,082:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9505 (1.9282)
+2022-11-18 15:16:48,138:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_362.pth.tar
+2022-11-18 15:16:48,138:INFO: 
+===> EPOCH: 363 (P2)
+2022-11-18 15:16:48,139:INFO: - Computing loss (training)
+2022-11-18 15:16:48,492:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8851 (1.8851)
+2022-11-18 15:16:48,641:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8984 (1.8915)
+2022-11-18 15:16:48,792:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0369 (1.9403)
+2022-11-18 15:16:48,910:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1017 (1.9671)
+2022-11-18 15:16:49,338:INFO: Dataset: univ                Batch:  1/15	Loss 1.8438 (1.8438)
+2022-11-18 15:16:49,503:INFO: Dataset: univ                Batch:  2/15	Loss 1.8924 (1.8671)
+2022-11-18 15:16:49,658:INFO: Dataset: univ                Batch:  3/15	Loss 1.8847 (1.8725)
+2022-11-18 15:16:49,814:INFO: Dataset: univ                Batch:  4/15	Loss 1.8403 (1.8642)
+2022-11-18 15:16:50,050:INFO: Dataset: univ                Batch:  5/15	Loss 1.8485 (1.8612)
+2022-11-18 15:16:50,209:INFO: Dataset: univ                Batch:  6/15	Loss 1.9007 (1.8674)
+2022-11-18 15:16:50,371:INFO: Dataset: univ                Batch:  7/15	Loss 1.8715 (1.8681)
+2022-11-18 15:16:50,527:INFO: Dataset: univ                Batch:  8/15	Loss 1.8505 (1.8658)
+2022-11-18 15:16:50,682:INFO: Dataset: univ                Batch:  9/15	Loss 1.8786 (1.8673)
+2022-11-18 15:16:50,852:INFO: Dataset: univ                Batch: 10/15	Loss 1.8949 (1.8696)
+2022-11-18 15:16:51,016:INFO: Dataset: univ                Batch: 11/15	Loss 1.8719 (1.8698)
+2022-11-18 15:16:51,173:INFO: Dataset: univ                Batch: 12/15	Loss 1.8721 (1.8700)
+2022-11-18 15:16:51,330:INFO: Dataset: univ                Batch: 13/15	Loss 1.8989 (1.8720)
+2022-11-18 15:16:51,489:INFO: Dataset: univ                Batch: 14/15	Loss 1.9019 (1.8741)
+2022-11-18 15:16:51,575:INFO: Dataset: univ                Batch: 15/15	Loss 1.9036 (1.8744)
+2022-11-18 15:16:51,985:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9025 (1.9025)
+2022-11-18 15:16:52,146:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8824 (1.8923)
+2022-11-18 15:16:52,306:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8793 (1.8878)
+2022-11-18 15:16:52,463:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8548 (1.8798)
+2022-11-18 15:16:52,621:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9688 (1.8965)
+2022-11-18 15:16:52,778:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8533 (1.8898)
+2022-11-18 15:16:52,952:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8728 (1.8873)
+2022-11-18 15:16:53,144:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8502 (1.8832)
+2022-11-18 15:16:53,623:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9723 (1.9723)
+2022-11-18 15:16:53,829:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8536 (1.9124)
+2022-11-18 15:16:53,990:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9852 (1.9368)
+2022-11-18 15:16:54,146:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9059 (1.9295)
+2022-11-18 15:16:54,297:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9741 (1.9382)
+2022-11-18 15:16:54,450:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9646 (1.9430)
+2022-11-18 15:16:54,611:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8305 (1.9271)
+2022-11-18 15:16:54,779:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9466 (1.9296)
+2022-11-18 15:16:54,951:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9668 (1.9336)
+2022-11-18 15:16:55,104:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9826 (1.9384)
+2022-11-18 15:16:55,255:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8608 (1.9317)
+2022-11-18 15:16:55,403:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8188 (1.9220)
+2022-11-18 15:16:55,550:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9042 (1.9207)
+2022-11-18 15:16:55,702:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8498 (1.9160)
+2022-11-18 15:16:55,850:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9846 (1.9202)
+2022-11-18 15:16:56,000:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8793 (1.9177)
+2022-11-18 15:16:56,149:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8911 (1.9161)
+2022-11-18 15:16:56,287:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8382 (1.9125)
+2022-11-18 15:16:56,333:INFO: - Computing loss (validation)
+2022-11-18 15:16:56,604:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8710 (1.8710)
+2022-11-18 15:16:56,639:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3443 (1.9082)
+2022-11-18 15:16:56,955:INFO: Dataset: univ                Batch: 1/3	Loss 1.9003 (1.9003)
+2022-11-18 15:16:57,040:INFO: Dataset: univ                Batch: 2/3	Loss 1.8981 (1.8990)
+2022-11-18 15:16:57,115:INFO: Dataset: univ                Batch: 3/3	Loss 1.8784 (1.8926)
+2022-11-18 15:16:57,418:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8443 (1.8443)
+2022-11-18 15:16:57,465:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0725 (1.9008)
+2022-11-18 15:16:57,792:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8601 (1.8601)
+2022-11-18 15:16:57,870:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8683 (1.8642)
+2022-11-18 15:16:57,948:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9680 (1.9000)
+2022-11-18 15:16:58,027:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8726 (1.8934)
+2022-11-18 15:16:58,105:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9383 (1.9025)
+2022-11-18 15:16:58,158:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_363.pth.tar
+2022-11-18 15:16:58,158:INFO: 
+===> EPOCH: 364 (P2)
+2022-11-18 15:16:58,159:INFO: - Computing loss (training)
+2022-11-18 15:16:58,501:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9530 (1.9530)
+2022-11-18 15:16:58,653:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9602 (1.9565)
+2022-11-18 15:16:58,811:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9370 (1.9498)
+2022-11-18 15:16:58,935:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9073 (1.9428)
+2022-11-18 15:16:59,327:INFO: Dataset: univ                Batch:  1/15	Loss 1.9066 (1.9066)
+2022-11-18 15:16:59,485:INFO: Dataset: univ                Batch:  2/15	Loss 1.8761 (1.8910)
+2022-11-18 15:16:59,647:INFO: Dataset: univ                Batch:  3/15	Loss 1.8798 (1.8872)
+2022-11-18 15:16:59,801:INFO: Dataset: univ                Batch:  4/15	Loss 1.8868 (1.8871)
+2022-11-18 15:16:59,958:INFO: Dataset: univ                Batch:  5/15	Loss 1.8771 (1.8851)
+2022-11-18 15:17:00,121:INFO: Dataset: univ                Batch:  6/15	Loss 1.8571 (1.8802)
+2022-11-18 15:17:00,278:INFO: Dataset: univ                Batch:  7/15	Loss 1.8745 (1.8794)
+2022-11-18 15:17:00,434:INFO: Dataset: univ                Batch:  8/15	Loss 1.8881 (1.8805)
+2022-11-18 15:17:00,592:INFO: Dataset: univ                Batch:  9/15	Loss 1.8547 (1.8775)
+2022-11-18 15:17:00,746:INFO: Dataset: univ                Batch: 10/15	Loss 1.8516 (1.8751)
+2022-11-18 15:17:00,903:INFO: Dataset: univ                Batch: 11/15	Loss 1.8801 (1.8756)
+2022-11-18 15:17:01,063:INFO: Dataset: univ                Batch: 12/15	Loss 1.9254 (1.8798)
+2022-11-18 15:17:01,223:INFO: Dataset: univ                Batch: 13/15	Loss 1.9482 (1.8847)
+2022-11-18 15:17:01,380:INFO: Dataset: univ                Batch: 14/15	Loss 1.8940 (1.8853)
+2022-11-18 15:17:01,465:INFO: Dataset: univ                Batch: 15/15	Loss 1.8189 (1.8845)
+2022-11-18 15:17:01,854:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9946 (1.9946)
+2022-11-18 15:17:02,005:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0485 (2.0189)
+2022-11-18 15:17:02,160:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8208 (1.9518)
+2022-11-18 15:17:02,311:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9448 (1.9500)
+2022-11-18 15:17:02,463:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8522 (1.9312)
+2022-11-18 15:17:02,612:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0155 (1.9443)
+2022-11-18 15:17:02,762:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9288 (1.9421)
+2022-11-18 15:17:02,899:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9091 (1.9386)
+2022-11-18 15:17:03,305:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8844 (1.8844)
+2022-11-18 15:17:03,456:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8143 (1.8491)
+2022-11-18 15:17:03,607:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8291 (1.8429)
+2022-11-18 15:17:03,756:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9415 (1.8701)
+2022-11-18 15:17:03,908:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8981 (1.8754)
+2022-11-18 15:17:04,061:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8664 (1.8740)
+2022-11-18 15:17:04,211:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9321 (1.8828)
+2022-11-18 15:17:04,359:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8957 (1.8843)
+2022-11-18 15:17:04,509:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9103 (1.8872)
+2022-11-18 15:17:04,656:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8949 (1.8878)
+2022-11-18 15:17:04,805:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9065 (1.8896)
+2022-11-18 15:17:04,955:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9249 (1.8927)
+2022-11-18 15:17:05,103:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8969 (1.8930)
+2022-11-18 15:17:05,259:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8950 (1.8931)
+2022-11-18 15:17:05,411:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8308 (1.8890)
+2022-11-18 15:17:05,561:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9842 (1.8948)
+2022-11-18 15:17:05,711:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8542 (1.8922)
+2022-11-18 15:17:05,849:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8957 (1.8924)
+2022-11-18 15:17:05,893:INFO: - Computing loss (validation)
+2022-11-18 15:17:06,163:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0293 (2.0293)
+2022-11-18 15:17:06,199:INFO: Dataset: hotel               Batch: 2/2	Loss 2.4519 (2.0538)
+2022-11-18 15:17:06,511:INFO: Dataset: univ                Batch: 1/3	Loss 1.9064 (1.9064)
+2022-11-18 15:17:06,591:INFO: Dataset: univ                Batch: 2/3	Loss 1.9017 (1.9041)
+2022-11-18 15:17:06,667:INFO: Dataset: univ                Batch: 3/3	Loss 1.9062 (1.9048)
+2022-11-18 15:17:06,966:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8411 (1.8411)
+2022-11-18 15:17:07,012:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7763 (1.8246)
+2022-11-18 15:17:07,303:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8042 (1.8042)
+2022-11-18 15:17:07,378:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8826 (1.8442)
+2022-11-18 15:17:07,452:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9290 (1.8721)
+2022-11-18 15:17:07,526:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9515 (1.8912)
+2022-11-18 15:17:07,600:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8335 (1.8795)
+2022-11-18 15:17:07,653:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_364.pth.tar
+2022-11-18 15:17:07,653:INFO: 
+===> EPOCH: 365 (P2)
+2022-11-18 15:17:07,653:INFO: - Computing loss (training)
+2022-11-18 15:17:07,993:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8871 (1.8871)
+2022-11-18 15:17:08,144:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9312 (1.9098)
+2022-11-18 15:17:08,295:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9335 (1.9171)
+2022-11-18 15:17:08,408:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0386 (1.9362)
+2022-11-18 15:17:08,809:INFO: Dataset: univ                Batch:  1/15	Loss 1.9054 (1.9054)
+2022-11-18 15:17:08,976:INFO: Dataset: univ                Batch:  2/15	Loss 1.9409 (1.9228)
+2022-11-18 15:17:09,136:INFO: Dataset: univ                Batch:  3/15	Loss 1.8664 (1.9032)
+2022-11-18 15:17:09,291:INFO: Dataset: univ                Batch:  4/15	Loss 1.8441 (1.8872)
+2022-11-18 15:17:09,450:INFO: Dataset: univ                Batch:  5/15	Loss 1.8859 (1.8869)
+2022-11-18 15:17:09,609:INFO: Dataset: univ                Batch:  6/15	Loss 1.8980 (1.8888)
+2022-11-18 15:17:09,768:INFO: Dataset: univ                Batch:  7/15	Loss 1.9079 (1.8916)
+2022-11-18 15:17:09,922:INFO: Dataset: univ                Batch:  8/15	Loss 1.9722 (1.9006)
+2022-11-18 15:17:10,081:INFO: Dataset: univ                Batch:  9/15	Loss 1.9041 (1.9010)
+2022-11-18 15:17:10,236:INFO: Dataset: univ                Batch: 10/15	Loss 1.9187 (1.9029)
+2022-11-18 15:17:10,393:INFO: Dataset: univ                Batch: 11/15	Loss 1.8681 (1.8997)
+2022-11-18 15:17:10,551:INFO: Dataset: univ                Batch: 12/15	Loss 1.9146 (1.9009)
+2022-11-18 15:17:10,707:INFO: Dataset: univ                Batch: 13/15	Loss 1.9223 (1.9024)
+2022-11-18 15:17:10,866:INFO: Dataset: univ                Batch: 14/15	Loss 1.8361 (1.8975)
+2022-11-18 15:17:10,948:INFO: Dataset: univ                Batch: 15/15	Loss 1.8058 (1.8960)
+2022-11-18 15:17:11,337:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8879 (1.8879)
+2022-11-18 15:17:11,489:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9290 (1.9067)
+2022-11-18 15:17:11,640:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8783 (1.8980)
+2022-11-18 15:17:11,788:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9833 (1.9179)
+2022-11-18 15:17:11,938:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8508 (1.9046)
+2022-11-18 15:17:12,091:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9053 (1.9047)
+2022-11-18 15:17:12,242:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8090 (1.8909)
+2022-11-18 15:17:12,378:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9205 (1.8943)
+2022-11-18 15:17:12,768:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8163 (1.8163)
+2022-11-18 15:17:12,918:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9661 (1.8919)
+2022-11-18 15:17:13,068:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8834 (1.8891)
+2022-11-18 15:17:13,219:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8554 (1.8802)
+2022-11-18 15:17:13,371:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8473 (1.8734)
+2022-11-18 15:17:13,520:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8483 (1.8689)
+2022-11-18 15:17:13,669:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8428 (1.8652)
+2022-11-18 15:17:13,816:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0075 (1.8837)
+2022-11-18 15:17:13,965:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8616 (1.8814)
+2022-11-18 15:17:14,111:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9162 (1.8852)
+2022-11-18 15:17:14,259:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8732 (1.8839)
+2022-11-18 15:17:14,408:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9251 (1.8874)
+2022-11-18 15:17:14,560:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8056 (1.8811)
+2022-11-18 15:17:14,710:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9152 (1.8834)
+2022-11-18 15:17:14,862:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8585 (1.8817)
+2022-11-18 15:17:15,013:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9362 (1.8858)
+2022-11-18 15:17:15,166:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9248 (1.8881)
+2022-11-18 15:17:15,306:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8911 (1.8882)
+2022-11-18 15:17:15,350:INFO: - Computing loss (validation)
+2022-11-18 15:17:15,607:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9625 (1.9625)
+2022-11-18 15:17:15,641:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7252 (1.9414)
+2022-11-18 15:17:15,958:INFO: Dataset: univ                Batch: 1/3	Loss 1.9072 (1.9072)
+2022-11-18 15:17:16,040:INFO: Dataset: univ                Batch: 2/3	Loss 1.9241 (1.9152)
+2022-11-18 15:17:16,115:INFO: Dataset: univ                Batch: 3/3	Loss 1.8809 (1.9042)
+2022-11-18 15:17:16,407:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8234 (1.8234)
+2022-11-18 15:17:16,452:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0502 (1.8825)
+2022-11-18 15:17:16,752:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9284 (1.9284)
+2022-11-18 15:17:16,828:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8753 (1.9019)
+2022-11-18 15:17:16,902:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8473 (1.8838)
+2022-11-18 15:17:16,976:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8851 (1.8841)
+2022-11-18 15:17:17,050:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9462 (1.8960)
+2022-11-18 15:17:17,105:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_365.pth.tar
+2022-11-18 15:17:17,106:INFO: 
+===> EPOCH: 366 (P2)
+2022-11-18 15:17:17,106:INFO: - Computing loss (training)
+2022-11-18 15:17:17,441:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9478 (1.9478)
+2022-11-18 15:17:17,592:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9219 (1.9347)
+2022-11-18 15:17:17,745:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9637 (1.9444)
+2022-11-18 15:17:17,861:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1721 (1.9795)
+2022-11-18 15:17:18,264:INFO: Dataset: univ                Batch:  1/15	Loss 1.9160 (1.9160)
+2022-11-18 15:17:18,427:INFO: Dataset: univ                Batch:  2/15	Loss 1.8649 (1.8906)
+2022-11-18 15:17:18,587:INFO: Dataset: univ                Batch:  3/15	Loss 1.8434 (1.8752)
+2022-11-18 15:17:18,748:INFO: Dataset: univ                Batch:  4/15	Loss 1.8612 (1.8713)
+2022-11-18 15:17:18,914:INFO: Dataset: univ                Batch:  5/15	Loss 1.8710 (1.8712)
+2022-11-18 15:17:19,076:INFO: Dataset: univ                Batch:  6/15	Loss 1.9320 (1.8812)
+2022-11-18 15:17:19,234:INFO: Dataset: univ                Batch:  7/15	Loss 1.8610 (1.8783)
+2022-11-18 15:17:19,391:INFO: Dataset: univ                Batch:  8/15	Loss 1.8186 (1.8710)
+2022-11-18 15:17:19,552:INFO: Dataset: univ                Batch:  9/15	Loss 1.9072 (1.8752)
+2022-11-18 15:17:19,708:INFO: Dataset: univ                Batch: 10/15	Loss 1.8759 (1.8753)
+2022-11-18 15:17:19,868:INFO: Dataset: univ                Batch: 11/15	Loss 1.9087 (1.8784)
+2022-11-18 15:17:20,023:INFO: Dataset: univ                Batch: 12/15	Loss 1.8836 (1.8789)
+2022-11-18 15:17:20,180:INFO: Dataset: univ                Batch: 13/15	Loss 1.8847 (1.8793)
+2022-11-18 15:17:20,337:INFO: Dataset: univ                Batch: 14/15	Loss 1.9113 (1.8817)
+2022-11-18 15:17:20,420:INFO: Dataset: univ                Batch: 15/15	Loss 1.9134 (1.8821)
+2022-11-18 15:17:20,807:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8949 (1.8949)
+2022-11-18 15:17:20,959:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9070 (1.9009)
+2022-11-18 15:17:21,109:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0165 (1.9403)
+2022-11-18 15:17:21,256:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0376 (1.9680)
+2022-11-18 15:17:21,406:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9239 (1.9586)
+2022-11-18 15:17:21,556:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9820 (1.9626)
+2022-11-18 15:17:21,705:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0042 (1.9691)
+2022-11-18 15:17:21,841:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8199 (1.9540)
+2022-11-18 15:17:22,227:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9721 (1.9721)
+2022-11-18 15:17:22,380:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9984 (1.9845)
+2022-11-18 15:17:22,531:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9147 (1.9638)
+2022-11-18 15:17:22,683:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8899 (1.9462)
+2022-11-18 15:17:22,836:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9412 (1.9452)
+2022-11-18 15:17:22,998:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8002 (1.9202)
+2022-11-18 15:17:23,150:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7801 (1.9029)
+2022-11-18 15:17:23,298:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9122 (1.9040)
+2022-11-18 15:17:23,447:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9056 (1.9042)
+2022-11-18 15:17:23,595:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8645 (1.9005)
+2022-11-18 15:17:23,750:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9373 (1.9038)
+2022-11-18 15:17:23,898:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8835 (1.9022)
+2022-11-18 15:17:24,047:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9450 (1.9057)
+2022-11-18 15:17:24,197:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9890 (1.9116)
+2022-11-18 15:17:24,348:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8996 (1.9109)
+2022-11-18 15:17:24,498:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8771 (1.9088)
+2022-11-18 15:17:24,649:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8867 (1.9076)
+2022-11-18 15:17:24,785:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8030 (1.9029)
+2022-11-18 15:17:24,831:INFO: - Computing loss (validation)
+2022-11-18 15:17:25,097:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0732 (2.0732)
+2022-11-18 15:17:25,133:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9815 (2.0654)
+2022-11-18 15:17:25,436:INFO: Dataset: univ                Batch: 1/3	Loss 1.8920 (1.8920)
+2022-11-18 15:17:25,513:INFO: Dataset: univ                Batch: 2/3	Loss 1.8950 (1.8934)
+2022-11-18 15:17:25,584:INFO: Dataset: univ                Batch: 3/3	Loss 1.8920 (1.8930)
+2022-11-18 15:17:25,881:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9463 (1.9463)
+2022-11-18 15:17:25,925:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8518 (1.9248)
+2022-11-18 15:17:26,226:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8247 (1.8247)
+2022-11-18 15:17:26,301:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8851 (1.8534)
+2022-11-18 15:17:26,376:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9002 (1.8696)
+2022-11-18 15:17:26,449:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8818 (1.8727)
+2022-11-18 15:17:26,523:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9448 (1.8869)
+2022-11-18 15:17:26,576:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_366.pth.tar
+2022-11-18 15:17:26,576:INFO: 
+===> EPOCH: 367 (P2)
+2022-11-18 15:17:26,577:INFO: - Computing loss (training)
+2022-11-18 15:17:26,921:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9135 (1.9135)
+2022-11-18 15:17:27,077:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9729 (1.9426)
+2022-11-18 15:17:27,247:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8892 (1.9248)
+2022-11-18 15:17:27,367:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8568 (1.9132)
+2022-11-18 15:17:27,779:INFO: Dataset: univ                Batch:  1/15	Loss 1.8572 (1.8572)
+2022-11-18 15:17:27,939:INFO: Dataset: univ                Batch:  2/15	Loss 1.8911 (1.8734)
+2022-11-18 15:17:28,102:INFO: Dataset: univ                Batch:  3/15	Loss 1.8603 (1.8691)
+2022-11-18 15:17:28,262:INFO: Dataset: univ                Batch:  4/15	Loss 1.9185 (1.8829)
+2022-11-18 15:17:28,424:INFO: Dataset: univ                Batch:  5/15	Loss 1.9205 (1.8900)
+2022-11-18 15:17:28,583:INFO: Dataset: univ                Batch:  6/15	Loss 1.8999 (1.8917)
+2022-11-18 15:17:28,740:INFO: Dataset: univ                Batch:  7/15	Loss 1.8574 (1.8872)
+2022-11-18 15:17:28,901:INFO: Dataset: univ                Batch:  8/15	Loss 1.8852 (1.8869)
+2022-11-18 15:17:29,058:INFO: Dataset: univ                Batch:  9/15	Loss 1.8899 (1.8873)
+2022-11-18 15:17:29,215:INFO: Dataset: univ                Batch: 10/15	Loss 1.8634 (1.8847)
+2022-11-18 15:17:29,375:INFO: Dataset: univ                Batch: 11/15	Loss 1.8934 (1.8855)
+2022-11-18 15:17:29,531:INFO: Dataset: univ                Batch: 12/15	Loss 1.9065 (1.8871)
+2022-11-18 15:17:29,688:INFO: Dataset: univ                Batch: 13/15	Loss 1.9464 (1.8918)
+2022-11-18 15:17:29,844:INFO: Dataset: univ                Batch: 14/15	Loss 1.9707 (1.8975)
+2022-11-18 15:17:29,926:INFO: Dataset: univ                Batch: 15/15	Loss 1.8496 (1.8969)
+2022-11-18 15:17:30,322:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9724 (1.9724)
+2022-11-18 15:17:30,471:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9966 (1.9841)
+2022-11-18 15:17:30,620:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8450 (1.9400)
+2022-11-18 15:17:30,768:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8602 (1.9207)
+2022-11-18 15:17:30,917:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9383 (1.9240)
+2022-11-18 15:17:31,069:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9043 (1.9204)
+2022-11-18 15:17:31,221:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9114 (1.9191)
+2022-11-18 15:17:31,360:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8869 (1.9154)
+2022-11-18 15:17:31,753:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7747 (1.7747)
+2022-11-18 15:17:31,905:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9268 (1.8508)
+2022-11-18 15:17:32,061:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8489 (1.8502)
+2022-11-18 15:17:32,212:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9394 (1.8710)
+2022-11-18 15:17:32,364:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8394 (1.8642)
+2022-11-18 15:17:32,518:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8179 (1.8573)
+2022-11-18 15:17:32,667:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8912 (1.8625)
+2022-11-18 15:17:32,816:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8601 (1.8622)
+2022-11-18 15:17:32,966:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9304 (1.8691)
+2022-11-18 15:17:33,115:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9174 (1.8741)
+2022-11-18 15:17:33,267:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9221 (1.8786)
+2022-11-18 15:17:33,416:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8589 (1.8770)
+2022-11-18 15:17:33,567:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8125 (1.8720)
+2022-11-18 15:17:33,716:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8441 (1.8700)
+2022-11-18 15:17:33,867:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8554 (1.8690)
+2022-11-18 15:17:34,018:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9038 (1.8713)
+2022-11-18 15:17:34,171:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9578 (1.8755)
+2022-11-18 15:17:34,311:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8859 (1.8761)
+2022-11-18 15:17:34,355:INFO: - Computing loss (validation)
+2022-11-18 15:17:34,627:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8551 (1.8551)
+2022-11-18 15:17:34,661:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8388 (1.8543)
+2022-11-18 15:17:34,967:INFO: Dataset: univ                Batch: 1/3	Loss 1.8849 (1.8849)
+2022-11-18 15:17:35,045:INFO: Dataset: univ                Batch: 2/3	Loss 1.8814 (1.8829)
+2022-11-18 15:17:35,118:INFO: Dataset: univ                Batch: 3/3	Loss 1.9313 (1.8977)
+2022-11-18 15:17:35,417:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0002 (2.0002)
+2022-11-18 15:17:35,464:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9611 (1.9911)
+2022-11-18 15:17:35,767:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8654 (1.8654)
+2022-11-18 15:17:35,845:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9459 (1.9035)
+2022-11-18 15:17:35,923:INFO: Dataset: zara2               Batch: 3/5	Loss 2.0144 (1.9416)
+2022-11-18 15:17:36,001:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8234 (1.9105)
+2022-11-18 15:17:36,078:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9205 (1.9126)
+2022-11-18 15:17:36,132:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_367.pth.tar
+2022-11-18 15:17:36,132:INFO: 
+===> EPOCH: 368 (P2)
+2022-11-18 15:17:36,133:INFO: - Computing loss (training)
+2022-11-18 15:17:36,485:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8885 (1.8885)
+2022-11-18 15:17:36,640:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9245 (1.9070)
+2022-11-18 15:17:36,791:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0077 (1.9406)
+2022-11-18 15:17:36,908:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9205 (1.9374)
+2022-11-18 15:17:37,298:INFO: Dataset: univ                Batch:  1/15	Loss 1.9032 (1.9032)
+2022-11-18 15:17:37,460:INFO: Dataset: univ                Batch:  2/15	Loss 1.9218 (1.9129)
+2022-11-18 15:17:37,619:INFO: Dataset: univ                Batch:  3/15	Loss 1.9229 (1.9164)
+2022-11-18 15:17:37,773:INFO: Dataset: univ                Batch:  4/15	Loss 1.8929 (1.9109)
+2022-11-18 15:17:37,930:INFO: Dataset: univ                Batch:  5/15	Loss 1.8957 (1.9078)
+2022-11-18 15:17:38,090:INFO: Dataset: univ                Batch:  6/15	Loss 1.8734 (1.9020)
+2022-11-18 15:17:38,246:INFO: Dataset: univ                Batch:  7/15	Loss 1.8639 (1.8967)
+2022-11-18 15:17:38,401:INFO: Dataset: univ                Batch:  8/15	Loss 1.8928 (1.8962)
+2022-11-18 15:17:38,556:INFO: Dataset: univ                Batch:  9/15	Loss 1.8556 (1.8921)
+2022-11-18 15:17:38,716:INFO: Dataset: univ                Batch: 10/15	Loss 1.8892 (1.8919)
+2022-11-18 15:17:38,880:INFO: Dataset: univ                Batch: 11/15	Loss 1.8987 (1.8925)
+2022-11-18 15:17:39,038:INFO: Dataset: univ                Batch: 12/15	Loss 1.9207 (1.8948)
+2022-11-18 15:17:39,195:INFO: Dataset: univ                Batch: 13/15	Loss 1.8916 (1.8945)
+2022-11-18 15:17:39,351:INFO: Dataset: univ                Batch: 14/15	Loss 1.9065 (1.8954)
+2022-11-18 15:17:39,434:INFO: Dataset: univ                Batch: 15/15	Loss 1.9064 (1.8955)
+2022-11-18 15:17:39,819:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8710 (1.8710)
+2022-11-18 15:17:39,969:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9371 (1.9036)
+2022-11-18 15:17:40,137:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0000 (1.9351)
+2022-11-18 15:17:40,300:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8938 (1.9249)
+2022-11-18 15:17:40,458:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9066 (1.9210)
+2022-11-18 15:17:40,623:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8519 (1.9083)
+2022-11-18 15:17:40,783:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8674 (1.9029)
+2022-11-18 15:17:40,924:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9309 (1.9056)
+2022-11-18 15:17:41,324:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9192 (1.9192)
+2022-11-18 15:17:41,533:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9095 (1.9145)
+2022-11-18 15:17:41,722:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9098 (1.9129)
+2022-11-18 15:17:41,929:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9198 (1.9145)
+2022-11-18 15:17:42,129:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9412 (1.9200)
+2022-11-18 15:17:42,295:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8750 (1.9127)
+2022-11-18 15:17:42,446:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9450 (1.9174)
+2022-11-18 15:17:42,597:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8854 (1.9136)
+2022-11-18 15:17:42,749:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9756 (1.9200)
+2022-11-18 15:17:42,925:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8936 (1.9174)
+2022-11-18 15:17:43,096:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8971 (1.9157)
+2022-11-18 15:17:43,259:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7862 (1.9043)
+2022-11-18 15:17:43,407:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8721 (1.9017)
+2022-11-18 15:17:43,559:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9498 (1.9048)
+2022-11-18 15:17:43,709:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8525 (1.9013)
+2022-11-18 15:17:43,862:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9842 (1.9062)
+2022-11-18 15:17:44,020:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8928 (1.9054)
+2022-11-18 15:17:44,191:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9011 (1.9052)
+2022-11-18 15:17:44,247:INFO: - Computing loss (validation)
+2022-11-18 15:17:44,563:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9859 (1.9859)
+2022-11-18 15:17:44,601:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8509 (1.9758)
+2022-11-18 15:17:44,909:INFO: Dataset: univ                Batch: 1/3	Loss 1.8860 (1.8860)
+2022-11-18 15:17:44,991:INFO: Dataset: univ                Batch: 2/3	Loss 1.8554 (1.8697)
+2022-11-18 15:17:45,069:INFO: Dataset: univ                Batch: 3/3	Loss 1.9238 (1.8878)
+2022-11-18 15:17:45,378:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9286 (1.9286)
+2022-11-18 15:17:45,424:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8405 (1.9028)
+2022-11-18 15:17:45,731:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8547 (1.8547)
+2022-11-18 15:17:45,817:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9625 (1.9101)
+2022-11-18 15:17:45,902:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9300 (1.9167)
+2022-11-18 15:17:45,979:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8119 (1.8915)
+2022-11-18 15:17:46,060:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8601 (1.8854)
+2022-11-18 15:17:46,134:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_368.pth.tar
+2022-11-18 15:17:46,134:INFO: 
+===> EPOCH: 369 (P2)
+2022-11-18 15:17:46,135:INFO: - Computing loss (training)
+2022-11-18 15:17:46,491:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9734 (1.9734)
+2022-11-18 15:17:46,662:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9156 (1.9436)
+2022-11-18 15:17:46,831:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9486 (1.9452)
+2022-11-18 15:17:46,952:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1159 (1.9752)
+2022-11-18 15:17:47,385:INFO: Dataset: univ                Batch:  1/15	Loss 1.8784 (1.8784)
+2022-11-18 15:17:47,572:INFO: Dataset: univ                Batch:  2/15	Loss 1.8937 (1.8864)
+2022-11-18 15:17:47,749:INFO: Dataset: univ                Batch:  3/15	Loss 1.8754 (1.8828)
+2022-11-18 15:17:47,922:INFO: Dataset: univ                Batch:  4/15	Loss 1.9215 (1.8925)
+2022-11-18 15:17:48,111:INFO: Dataset: univ                Batch:  5/15	Loss 1.9044 (1.8951)
+2022-11-18 15:17:48,279:INFO: Dataset: univ                Batch:  6/15	Loss 1.8942 (1.8950)
+2022-11-18 15:17:48,444:INFO: Dataset: univ                Batch:  7/15	Loss 1.9003 (1.8958)
+2022-11-18 15:17:48,620:INFO: Dataset: univ                Batch:  8/15	Loss 1.8839 (1.8943)
+2022-11-18 15:17:48,812:INFO: Dataset: univ                Batch:  9/15	Loss 1.9188 (1.8973)
+2022-11-18 15:17:48,995:INFO: Dataset: univ                Batch: 10/15	Loss 1.8443 (1.8922)
+2022-11-18 15:17:49,160:INFO: Dataset: univ                Batch: 11/15	Loss 1.8841 (1.8914)
+2022-11-18 15:17:49,323:INFO: Dataset: univ                Batch: 12/15	Loss 1.8738 (1.8899)
+2022-11-18 15:17:49,501:INFO: Dataset: univ                Batch: 13/15	Loss 1.8800 (1.8892)
+2022-11-18 15:17:49,691:INFO: Dataset: univ                Batch: 14/15	Loss 1.8789 (1.8885)
+2022-11-18 15:17:49,794:INFO: Dataset: univ                Batch: 15/15	Loss 1.9344 (1.8893)
+2022-11-18 15:17:50,252:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8943 (1.8943)
+2022-11-18 15:17:50,441:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9796 (1.9359)
+2022-11-18 15:17:50,646:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8664 (1.9114)
+2022-11-18 15:17:50,823:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9205 (1.9137)
+2022-11-18 15:17:50,981:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9176 (1.9146)
+2022-11-18 15:17:51,138:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9263 (1.9165)
+2022-11-18 15:17:51,295:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8800 (1.9109)
+2022-11-18 15:17:51,439:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7778 (1.8975)
+2022-11-18 15:17:51,869:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7961 (1.7961)
+2022-11-18 15:17:52,033:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8791 (1.8364)
+2022-11-18 15:17:52,187:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9401 (1.8672)
+2022-11-18 15:17:52,343:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8474 (1.8620)
+2022-11-18 15:17:52,502:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9065 (1.8711)
+2022-11-18 15:17:52,660:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9335 (1.8823)
+2022-11-18 15:17:52,814:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9279 (1.8889)
+2022-11-18 15:17:52,966:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9983 (1.9019)
+2022-11-18 15:17:53,129:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9077 (1.9026)
+2022-11-18 15:17:53,294:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9098 (1.9032)
+2022-11-18 15:17:53,456:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9350 (1.9061)
+2022-11-18 15:17:53,628:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8145 (1.8982)
+2022-11-18 15:17:53,813:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9031 (1.8985)
+2022-11-18 15:17:53,991:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9381 (1.9012)
+2022-11-18 15:17:54,155:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8785 (1.8997)
+2022-11-18 15:17:54,316:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9572 (1.9033)
+2022-11-18 15:17:54,481:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8528 (1.9004)
+2022-11-18 15:17:54,629:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8810 (1.8994)
+2022-11-18 15:17:54,680:INFO: - Computing loss (validation)
+2022-11-18 15:17:54,950:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9911 (1.9911)
+2022-11-18 15:17:54,984:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7624 (1.9786)
+2022-11-18 15:17:55,290:INFO: Dataset: univ                Batch: 1/3	Loss 1.9015 (1.9015)
+2022-11-18 15:17:55,372:INFO: Dataset: univ                Batch: 2/3	Loss 1.8832 (1.8923)
+2022-11-18 15:17:55,451:INFO: Dataset: univ                Batch: 3/3	Loss 1.9132 (1.8992)
+2022-11-18 15:17:55,754:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0283 (2.0283)
+2022-11-18 15:17:55,798:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8037 (1.9821)
+2022-11-18 15:17:56,113:INFO: Dataset: zara2               Batch: 1/5	Loss 2.0013 (2.0013)
+2022-11-18 15:17:56,194:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9107 (1.9543)
+2022-11-18 15:17:56,273:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9118 (1.9406)
+2022-11-18 15:17:56,351:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9724 (1.9479)
+2022-11-18 15:17:56,428:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8512 (1.9287)
+2022-11-18 15:17:56,487:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_369.pth.tar
+2022-11-18 15:17:56,487:INFO: 
+===> EPOCH: 370 (P2)
+2022-11-18 15:17:56,487:INFO: - Computing loss (training)
+2022-11-18 15:17:56,838:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0017 (2.0017)
+2022-11-18 15:17:56,994:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8694 (1.9399)
+2022-11-18 15:17:57,148:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9398 (1.9398)
+2022-11-18 15:17:57,264:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7916 (1.9158)
+2022-11-18 15:17:57,671:INFO: Dataset: univ                Batch:  1/15	Loss 1.8465 (1.8465)
+2022-11-18 15:17:57,845:INFO: Dataset: univ                Batch:  2/15	Loss 1.8828 (1.8637)
+2022-11-18 15:17:58,017:INFO: Dataset: univ                Batch:  3/15	Loss 1.8706 (1.8661)
+2022-11-18 15:17:58,184:INFO: Dataset: univ                Batch:  4/15	Loss 1.8899 (1.8721)
+2022-11-18 15:17:58,354:INFO: Dataset: univ                Batch:  5/15	Loss 1.8808 (1.8739)
+2022-11-18 15:17:58,526:INFO: Dataset: univ                Batch:  6/15	Loss 1.8690 (1.8730)
+2022-11-18 15:17:58,692:INFO: Dataset: univ                Batch:  7/15	Loss 1.9049 (1.8773)
+2022-11-18 15:17:58,860:INFO: Dataset: univ                Batch:  8/15	Loss 1.8709 (1.8765)
+2022-11-18 15:17:59,026:INFO: Dataset: univ                Batch:  9/15	Loss 1.8943 (1.8784)
+2022-11-18 15:17:59,191:INFO: Dataset: univ                Batch: 10/15	Loss 1.8740 (1.8780)
+2022-11-18 15:17:59,359:INFO: Dataset: univ                Batch: 11/15	Loss 1.8765 (1.8779)
+2022-11-18 15:17:59,522:INFO: Dataset: univ                Batch: 12/15	Loss 1.8899 (1.8787)
+2022-11-18 15:17:59,689:INFO: Dataset: univ                Batch: 13/15	Loss 1.9115 (1.8810)
+2022-11-18 15:17:59,855:INFO: Dataset: univ                Batch: 14/15	Loss 1.8735 (1.8804)
+2022-11-18 15:17:59,944:INFO: Dataset: univ                Batch: 15/15	Loss 1.8880 (1.8805)
+2022-11-18 15:18:00,326:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8348 (1.8348)
+2022-11-18 15:18:00,487:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8459 (1.8400)
+2022-11-18 15:18:00,640:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9305 (1.8725)
+2022-11-18 15:18:00,796:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9258 (1.8860)
+2022-11-18 15:18:00,953:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8590 (1.8810)
+2022-11-18 15:18:01,112:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0556 (1.9078)
+2022-11-18 15:18:01,301:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9412 (1.9128)
+2022-11-18 15:18:01,472:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0670 (1.9298)
+2022-11-18 15:18:01,962:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9196 (1.9196)
+2022-11-18 15:18:02,120:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8973 (1.9088)
+2022-11-18 15:18:02,279:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8922 (1.9034)
+2022-11-18 15:18:02,436:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8527 (1.8894)
+2022-11-18 15:18:02,595:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9103 (1.8932)
+2022-11-18 15:18:02,754:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9765 (1.9077)
+2022-11-18 15:18:02,913:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8792 (1.9039)
+2022-11-18 15:18:03,068:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9872 (1.9138)
+2022-11-18 15:18:03,226:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9253 (1.9151)
+2022-11-18 15:18:03,381:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8997 (1.9135)
+2022-11-18 15:18:03,540:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8797 (1.9101)
+2022-11-18 15:18:03,696:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9136 (1.9104)
+2022-11-18 15:18:03,856:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8960 (1.9094)
+2022-11-18 15:18:04,011:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8328 (1.9035)
+2022-11-18 15:18:04,171:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9430 (1.9060)
+2022-11-18 15:18:04,330:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0275 (1.9135)
+2022-11-18 15:18:04,488:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8256 (1.9088)
+2022-11-18 15:18:04,634:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8377 (1.9054)
+2022-11-18 15:18:04,679:INFO: - Computing loss (validation)
+2022-11-18 15:18:04,948:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8516 (1.8516)
+2022-11-18 15:18:04,985:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3225 (1.8789)
+2022-11-18 15:18:05,302:INFO: Dataset: univ                Batch: 1/3	Loss 1.8805 (1.8805)
+2022-11-18 15:18:05,386:INFO: Dataset: univ                Batch: 2/3	Loss 1.8880 (1.8842)
+2022-11-18 15:18:05,467:INFO: Dataset: univ                Batch: 3/3	Loss 1.8823 (1.8835)
+2022-11-18 15:18:05,773:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9763 (1.9763)
+2022-11-18 15:18:05,818:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0235 (1.9895)
+2022-11-18 15:18:06,124:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8630 (1.8630)
+2022-11-18 15:18:06,202:INFO: Dataset: zara2               Batch: 2/5	Loss 2.0222 (1.9365)
+2022-11-18 15:18:06,280:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9115 (1.9278)
+2022-11-18 15:18:06,359:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8880 (1.9174)
+2022-11-18 15:18:06,436:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9196 (1.9179)
+2022-11-18 15:18:06,489:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_370.pth.tar
+2022-11-18 15:18:06,489:INFO: 
+===> EPOCH: 371 (P2)
+2022-11-18 15:18:06,490:INFO: - Computing loss (training)
+2022-11-18 15:18:06,832:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9298 (1.9298)
+2022-11-18 15:18:06,984:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9306 (1.9301)
+2022-11-18 15:18:07,135:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7452 (1.8681)
+2022-11-18 15:18:07,251:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8940 (1.8726)
+2022-11-18 15:18:07,654:INFO: Dataset: univ                Batch:  1/15	Loss 1.8946 (1.8946)
+2022-11-18 15:18:07,819:INFO: Dataset: univ                Batch:  2/15	Loss 1.8670 (1.8813)
+2022-11-18 15:18:07,983:INFO: Dataset: univ                Batch:  3/15	Loss 1.9146 (1.8917)
+2022-11-18 15:18:08,149:INFO: Dataset: univ                Batch:  4/15	Loss 1.8738 (1.8875)
+2022-11-18 15:18:08,314:INFO: Dataset: univ                Batch:  5/15	Loss 1.9122 (1.8923)
+2022-11-18 15:18:08,479:INFO: Dataset: univ                Batch:  6/15	Loss 1.8923 (1.8923)
+2022-11-18 15:18:08,643:INFO: Dataset: univ                Batch:  7/15	Loss 1.9225 (1.8966)
+2022-11-18 15:18:08,805:INFO: Dataset: univ                Batch:  8/15	Loss 1.9306 (1.9005)
+2022-11-18 15:18:08,969:INFO: Dataset: univ                Batch:  9/15	Loss 1.8702 (1.8975)
+2022-11-18 15:18:09,132:INFO: Dataset: univ                Batch: 10/15	Loss 1.8859 (1.8963)
+2022-11-18 15:18:09,295:INFO: Dataset: univ                Batch: 11/15	Loss 1.8514 (1.8925)
+2022-11-18 15:18:09,458:INFO: Dataset: univ                Batch: 12/15	Loss 1.8890 (1.8922)
+2022-11-18 15:18:09,620:INFO: Dataset: univ                Batch: 13/15	Loss 1.9036 (1.8931)
+2022-11-18 15:18:09,783:INFO: Dataset: univ                Batch: 14/15	Loss 1.9000 (1.8935)
+2022-11-18 15:18:09,870:INFO: Dataset: univ                Batch: 15/15	Loss 1.9612 (1.8947)
+2022-11-18 15:18:10,265:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9344 (1.9344)
+2022-11-18 15:18:10,418:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8647 (1.9006)
+2022-11-18 15:18:10,568:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9491 (1.9149)
+2022-11-18 15:18:10,716:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9174 (1.9155)
+2022-11-18 15:18:10,866:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8878 (1.9093)
+2022-11-18 15:18:11,016:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8617 (1.9016)
+2022-11-18 15:18:11,245:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8521 (1.8947)
+2022-11-18 15:18:11,382:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8656 (1.8914)
+2022-11-18 15:18:11,794:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8556 (1.8556)
+2022-11-18 15:18:11,948:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9590 (1.9078)
+2022-11-18 15:18:12,103:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8994 (1.9050)
+2022-11-18 15:18:12,255:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8859 (1.8998)
+2022-11-18 15:18:12,410:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8501 (1.8894)
+2022-11-18 15:18:12,562:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7743 (1.8700)
+2022-11-18 15:18:12,711:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9157 (1.8769)
+2022-11-18 15:18:12,862:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8138 (1.8691)
+2022-11-18 15:18:13,012:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9034 (1.8731)
+2022-11-18 15:18:13,163:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8618 (1.8721)
+2022-11-18 15:18:13,314:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9135 (1.8756)
+2022-11-18 15:18:13,465:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8910 (1.8768)
+2022-11-18 15:18:13,614:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9165 (1.8797)
+2022-11-18 15:18:13,766:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8701 (1.8790)
+2022-11-18 15:18:13,917:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9986 (1.8865)
+2022-11-18 15:18:14,066:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9352 (1.8898)
+2022-11-18 15:18:14,219:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0246 (1.8980)
+2022-11-18 15:18:14,358:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8857 (1.8974)
+2022-11-18 15:18:14,408:INFO: - Computing loss (validation)
+2022-11-18 15:18:14,667:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9419 (1.9419)
+2022-11-18 15:18:14,700:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0213 (1.9465)
+2022-11-18 15:18:15,018:INFO: Dataset: univ                Batch: 1/3	Loss 1.9824 (1.9824)
+2022-11-18 15:18:15,099:INFO: Dataset: univ                Batch: 2/3	Loss 1.8736 (1.9260)
+2022-11-18 15:18:15,174:INFO: Dataset: univ                Batch: 3/3	Loss 1.9292 (1.9269)
+2022-11-18 15:18:15,490:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8193 (1.8193)
+2022-11-18 15:18:15,538:INFO: Dataset: zara1               Batch: 2/2	Loss 2.2086 (1.9157)
+2022-11-18 15:18:15,842:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9073 (1.9073)
+2022-11-18 15:18:15,917:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8218 (1.8648)
+2022-11-18 15:18:15,994:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9490 (1.8933)
+2022-11-18 15:18:16,070:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8925 (1.8931)
+2022-11-18 15:18:16,145:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9335 (1.9011)
+2022-11-18 15:18:16,200:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_371.pth.tar
+2022-11-18 15:18:16,200:INFO: 
+===> EPOCH: 372 (P2)
+2022-11-18 15:18:16,201:INFO: - Computing loss (training)
+2022-11-18 15:18:16,559:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9468 (1.9468)
+2022-11-18 15:18:16,715:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9795 (1.9629)
+2022-11-18 15:18:16,866:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8604 (1.9288)
+2022-11-18 15:18:16,982:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9524 (1.9328)
+2022-11-18 15:18:17,395:INFO: Dataset: univ                Batch:  1/15	Loss 1.8964 (1.8964)
+2022-11-18 15:18:17,555:INFO: Dataset: univ                Batch:  2/15	Loss 1.8648 (1.8802)
+2022-11-18 15:18:17,718:INFO: Dataset: univ                Batch:  3/15	Loss 1.8846 (1.8816)
+2022-11-18 15:18:17,873:INFO: Dataset: univ                Batch:  4/15	Loss 1.8701 (1.8789)
+2022-11-18 15:18:18,031:INFO: Dataset: univ                Batch:  5/15	Loss 1.8911 (1.8814)
+2022-11-18 15:18:18,190:INFO: Dataset: univ                Batch:  6/15	Loss 1.8809 (1.8813)
+2022-11-18 15:18:18,347:INFO: Dataset: univ                Batch:  7/15	Loss 1.8788 (1.8810)
+2022-11-18 15:18:18,501:INFO: Dataset: univ                Batch:  8/15	Loss 1.9109 (1.8850)
+2022-11-18 15:18:18,660:INFO: Dataset: univ                Batch:  9/15	Loss 1.8639 (1.8824)
+2022-11-18 15:18:18,814:INFO: Dataset: univ                Batch: 10/15	Loss 1.8729 (1.8815)
+2022-11-18 15:18:18,976:INFO: Dataset: univ                Batch: 11/15	Loss 1.8772 (1.8811)
+2022-11-18 15:18:19,131:INFO: Dataset: univ                Batch: 12/15	Loss 1.9154 (1.8839)
+2022-11-18 15:18:19,288:INFO: Dataset: univ                Batch: 13/15	Loss 1.8563 (1.8818)
+2022-11-18 15:18:19,444:INFO: Dataset: univ                Batch: 14/15	Loss 1.9248 (1.8846)
+2022-11-18 15:18:19,527:INFO: Dataset: univ                Batch: 15/15	Loss 1.8485 (1.8842)
+2022-11-18 15:18:19,920:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7978 (1.7978)
+2022-11-18 15:18:20,071:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9083 (1.8509)
+2022-11-18 15:18:20,225:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9790 (1.8965)
+2022-11-18 15:18:20,379:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8466 (1.8836)
+2022-11-18 15:18:20,529:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8865 (1.8842)
+2022-11-18 15:18:20,681:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9742 (1.8968)
+2022-11-18 15:18:20,832:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8933 (1.8963)
+2022-11-18 15:18:20,969:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9217 (1.8996)
+2022-11-18 15:18:21,397:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9197 (1.9197)
+2022-11-18 15:18:21,547:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8586 (1.8907)
+2022-11-18 15:18:21,703:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9888 (1.9228)
+2022-11-18 15:18:21,854:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8978 (1.9166)
+2022-11-18 15:18:22,005:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8596 (1.9062)
+2022-11-18 15:18:22,159:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8012 (1.8876)
+2022-11-18 15:18:22,310:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8208 (1.8784)
+2022-11-18 15:18:22,459:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9400 (1.8858)
+2022-11-18 15:18:22,611:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8916 (1.8865)
+2022-11-18 15:18:22,761:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8889 (1.8867)
+2022-11-18 15:18:22,912:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8778 (1.8860)
+2022-11-18 15:18:23,062:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8657 (1.8844)
+2022-11-18 15:18:23,215:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9163 (1.8868)
+2022-11-18 15:18:23,365:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8902 (1.8871)
+2022-11-18 15:18:23,517:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8271 (1.8834)
+2022-11-18 15:18:23,668:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9716 (1.8890)
+2022-11-18 15:18:23,820:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9334 (1.8915)
+2022-11-18 15:18:23,959:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9160 (1.8927)
+2022-11-18 15:18:24,004:INFO: - Computing loss (validation)
+2022-11-18 15:18:24,279:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9565 (1.9565)
+2022-11-18 15:18:24,314:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2142 (1.9758)
+2022-11-18 15:18:24,625:INFO: Dataset: univ                Batch: 1/3	Loss 1.9287 (1.9287)
+2022-11-18 15:18:24,702:INFO: Dataset: univ                Batch: 2/3	Loss 1.8546 (1.8940)
+2022-11-18 15:18:24,776:INFO: Dataset: univ                Batch: 3/3	Loss 1.9193 (1.9022)
+2022-11-18 15:18:25,120:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9362 (1.9362)
+2022-11-18 15:18:25,169:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9893 (1.9490)
+2022-11-18 15:18:25,507:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9133 (1.9133)
+2022-11-18 15:18:25,588:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8242 (1.8691)
+2022-11-18 15:18:25,669:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8707 (1.8696)
+2022-11-18 15:18:25,751:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8327 (1.8611)
+2022-11-18 15:18:25,830:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8981 (1.8684)
+2022-11-18 15:18:25,885:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_372.pth.tar
+2022-11-18 15:18:25,885:INFO: 
+===> EPOCH: 373 (P2)
+2022-11-18 15:18:25,886:INFO: - Computing loss (training)
+2022-11-18 15:18:26,248:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8650 (1.8650)
+2022-11-18 15:18:26,411:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9139 (1.8880)
+2022-11-18 15:18:26,572:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7935 (1.8545)
+2022-11-18 15:18:26,698:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0604 (1.8887)
+2022-11-18 15:18:27,103:INFO: Dataset: univ                Batch:  1/15	Loss 1.8603 (1.8603)
+2022-11-18 15:18:27,262:INFO: Dataset: univ                Batch:  2/15	Loss 1.9184 (1.8883)
+2022-11-18 15:18:27,434:INFO: Dataset: univ                Batch:  3/15	Loss 1.9044 (1.8940)
+2022-11-18 15:18:27,600:INFO: Dataset: univ                Batch:  4/15	Loss 1.9111 (1.8982)
+2022-11-18 15:18:27,760:INFO: Dataset: univ                Batch:  5/15	Loss 1.8812 (1.8950)
+2022-11-18 15:18:27,917:INFO: Dataset: univ                Batch:  6/15	Loss 1.9006 (1.8960)
+2022-11-18 15:18:28,075:INFO: Dataset: univ                Batch:  7/15	Loss 1.8737 (1.8927)
+2022-11-18 15:18:28,232:INFO: Dataset: univ                Batch:  8/15	Loss 1.8910 (1.8924)
+2022-11-18 15:18:28,388:INFO: Dataset: univ                Batch:  9/15	Loss 1.8531 (1.8879)
+2022-11-18 15:18:28,543:INFO: Dataset: univ                Batch: 10/15	Loss 1.8971 (1.8889)
+2022-11-18 15:18:28,703:INFO: Dataset: univ                Batch: 11/15	Loss 1.8964 (1.8895)
+2022-11-18 15:18:28,862:INFO: Dataset: univ                Batch: 12/15	Loss 1.8840 (1.8890)
+2022-11-18 15:18:29,022:INFO: Dataset: univ                Batch: 13/15	Loss 1.9010 (1.8900)
+2022-11-18 15:18:29,181:INFO: Dataset: univ                Batch: 14/15	Loss 1.8446 (1.8866)
+2022-11-18 15:18:29,265:INFO: Dataset: univ                Batch: 15/15	Loss 1.9862 (1.8878)
+2022-11-18 15:18:29,669:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8884 (1.8884)
+2022-11-18 15:18:29,819:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9147 (1.9008)
+2022-11-18 15:18:29,971:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9530 (1.9181)
+2022-11-18 15:18:30,121:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8343 (1.8990)
+2022-11-18 15:18:30,275:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8778 (1.8944)
+2022-11-18 15:18:30,426:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8738 (1.8913)
+2022-11-18 15:18:30,578:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9022 (1.8929)
+2022-11-18 15:18:30,715:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9058 (1.8944)
+2022-11-18 15:18:31,107:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8170 (1.8170)
+2022-11-18 15:18:31,266:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9408 (1.8787)
+2022-11-18 15:18:31,422:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9072 (1.8884)
+2022-11-18 15:18:31,575:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9364 (1.9002)
+2022-11-18 15:18:31,734:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9357 (1.9084)
+2022-11-18 15:18:31,897:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8962 (1.9063)
+2022-11-18 15:18:32,048:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8410 (1.8961)
+2022-11-18 15:18:32,199:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8445 (1.8897)
+2022-11-18 15:18:32,351:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7948 (1.8781)
+2022-11-18 15:18:32,501:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8740 (1.8778)
+2022-11-18 15:18:32,655:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8832 (1.8782)
+2022-11-18 15:18:32,807:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9542 (1.8841)
+2022-11-18 15:18:32,960:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9271 (1.8873)
+2022-11-18 15:18:33,111:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8114 (1.8814)
+2022-11-18 15:18:33,266:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9009 (1.8826)
+2022-11-18 15:18:33,417:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8921 (1.8832)
+2022-11-18 15:18:33,571:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8274 (1.8801)
+2022-11-18 15:18:33,710:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8220 (1.8774)
+2022-11-18 15:18:33,758:INFO: - Computing loss (validation)
+2022-11-18 15:18:34,028:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8204 (1.8204)
+2022-11-18 15:18:34,063:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3430 (1.8632)
+2022-11-18 15:18:34,372:INFO: Dataset: univ                Batch: 1/3	Loss 1.9209 (1.9209)
+2022-11-18 15:18:34,455:INFO: Dataset: univ                Batch: 2/3	Loss 1.9051 (1.9132)
+2022-11-18 15:18:34,531:INFO: Dataset: univ                Batch: 3/3	Loss 1.8587 (1.8963)
+2022-11-18 15:18:34,840:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8105 (1.8105)
+2022-11-18 15:18:34,887:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9032 (1.8350)
+2022-11-18 15:18:35,197:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7885 (1.7885)
+2022-11-18 15:18:35,272:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8988 (1.8428)
+2022-11-18 15:18:35,347:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9186 (1.8701)
+2022-11-18 15:18:35,420:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8946 (1.8759)
+2022-11-18 15:18:35,493:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8451 (1.8702)
+2022-11-18 15:18:35,551:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_373.pth.tar
+2022-11-18 15:18:35,551:INFO: 
+===> EPOCH: 374 (P2)
+2022-11-18 15:18:35,551:INFO: - Computing loss (training)
+2022-11-18 15:18:35,902:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8792 (1.8792)
+2022-11-18 15:18:36,055:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9090 (1.8939)
+2022-11-18 15:18:36,208:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9169 (1.9020)
+2022-11-18 15:18:36,324:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7572 (1.8768)
+2022-11-18 15:18:36,725:INFO: Dataset: univ                Batch:  1/15	Loss 1.9021 (1.9021)
+2022-11-18 15:18:36,889:INFO: Dataset: univ                Batch:  2/15	Loss 1.9212 (1.9110)
+2022-11-18 15:18:37,050:INFO: Dataset: univ                Batch:  3/15	Loss 1.8999 (1.9076)
+2022-11-18 15:18:37,207:INFO: Dataset: univ                Batch:  4/15	Loss 1.8822 (1.9014)
+2022-11-18 15:18:37,368:INFO: Dataset: univ                Batch:  5/15	Loss 1.9042 (1.9020)
+2022-11-18 15:18:37,524:INFO: Dataset: univ                Batch:  6/15	Loss 1.8898 (1.9002)
+2022-11-18 15:18:37,682:INFO: Dataset: univ                Batch:  7/15	Loss 1.8661 (1.8953)
+2022-11-18 15:18:37,837:INFO: Dataset: univ                Batch:  8/15	Loss 1.8861 (1.8942)
+2022-11-18 15:18:37,994:INFO: Dataset: univ                Batch:  9/15	Loss 1.9394 (1.8988)
+2022-11-18 15:18:38,150:INFO: Dataset: univ                Batch: 10/15	Loss 1.9245 (1.9013)
+2022-11-18 15:18:38,307:INFO: Dataset: univ                Batch: 11/15	Loss 1.8743 (1.8991)
+2022-11-18 15:18:38,464:INFO: Dataset: univ                Batch: 12/15	Loss 1.8606 (1.8959)
+2022-11-18 15:18:38,619:INFO: Dataset: univ                Batch: 13/15	Loss 1.8524 (1.8929)
+2022-11-18 15:18:38,777:INFO: Dataset: univ                Batch: 14/15	Loss 1.8835 (1.8922)
+2022-11-18 15:18:38,864:INFO: Dataset: univ                Batch: 15/15	Loss 1.8525 (1.8916)
+2022-11-18 15:18:39,268:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8565 (1.8565)
+2022-11-18 15:18:39,425:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7433 (1.7969)
+2022-11-18 15:18:39,583:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9925 (1.8560)
+2022-11-18 15:18:39,739:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9332 (1.8777)
+2022-11-18 15:18:39,898:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8860 (1.8794)
+2022-11-18 15:18:40,055:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9462 (1.8907)
+2022-11-18 15:18:40,213:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0374 (1.9117)
+2022-11-18 15:18:40,355:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9342 (1.9141)
+2022-11-18 15:18:40,751:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8890 (1.8890)
+2022-11-18 15:18:40,902:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9004 (1.8946)
+2022-11-18 15:18:41,054:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8836 (1.8911)
+2022-11-18 15:18:41,208:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8811 (1.8888)
+2022-11-18 15:18:41,364:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8840 (1.8878)
+2022-11-18 15:18:41,514:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8867 (1.8876)
+2022-11-18 15:18:41,665:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8258 (1.8789)
+2022-11-18 15:18:41,815:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9597 (1.8893)
+2022-11-18 15:18:41,966:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8949 (1.8900)
+2022-11-18 15:18:42,113:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7878 (1.8782)
+2022-11-18 15:18:42,266:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9819 (1.8878)
+2022-11-18 15:18:42,414:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8026 (1.8808)
+2022-11-18 15:18:42,564:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9487 (1.8858)
+2022-11-18 15:18:42,715:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8547 (1.8836)
+2022-11-18 15:18:42,867:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9118 (1.8855)
+2022-11-18 15:18:43,017:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8715 (1.8845)
+2022-11-18 15:18:43,169:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9395 (1.8879)
+2022-11-18 15:18:43,307:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9492 (1.8907)
+2022-11-18 15:18:43,353:INFO: - Computing loss (validation)
+2022-11-18 15:18:43,615:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0475 (2.0475)
+2022-11-18 15:18:43,649:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0698 (2.0495)
+2022-11-18 15:18:43,966:INFO: Dataset: univ                Batch: 1/3	Loss 1.8606 (1.8606)
+2022-11-18 15:18:44,046:INFO: Dataset: univ                Batch: 2/3	Loss 1.9548 (1.9029)
+2022-11-18 15:18:44,121:INFO: Dataset: univ                Batch: 3/3	Loss 1.8605 (1.8898)
+2022-11-18 15:18:44,434:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8332 (1.8332)
+2022-11-18 15:18:44,482:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9720 (1.8694)
+2022-11-18 15:18:44,788:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8953 (1.8953)
+2022-11-18 15:18:44,863:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9058 (1.9006)
+2022-11-18 15:18:44,939:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9429 (1.9133)
+2022-11-18 15:18:45,015:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8922 (1.9077)
+2022-11-18 15:18:45,089:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8884 (1.9038)
+2022-11-18 15:18:45,143:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_374.pth.tar
+2022-11-18 15:18:45,143:INFO: 
+===> EPOCH: 375 (P2)
+2022-11-18 15:18:45,143:INFO: - Computing loss (training)
+2022-11-18 15:18:45,488:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8462 (1.8462)
+2022-11-18 15:18:45,642:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9559 (1.8974)
+2022-11-18 15:18:45,794:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9866 (1.9258)
+2022-11-18 15:18:45,911:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9395 (1.9280)
+2022-11-18 15:18:46,326:INFO: Dataset: univ                Batch:  1/15	Loss 1.9373 (1.9373)
+2022-11-18 15:18:46,497:INFO: Dataset: univ                Batch:  2/15	Loss 1.8738 (1.9061)
+2022-11-18 15:18:46,666:INFO: Dataset: univ                Batch:  3/15	Loss 1.9083 (1.9067)
+2022-11-18 15:18:46,837:INFO: Dataset: univ                Batch:  4/15	Loss 1.8463 (1.8917)
+2022-11-18 15:18:47,006:INFO: Dataset: univ                Batch:  5/15	Loss 1.9355 (1.8999)
+2022-11-18 15:18:47,176:INFO: Dataset: univ                Batch:  6/15	Loss 1.8962 (1.8993)
+2022-11-18 15:18:47,343:INFO: Dataset: univ                Batch:  7/15	Loss 1.9221 (1.9029)
+2022-11-18 15:18:47,509:INFO: Dataset: univ                Batch:  8/15	Loss 1.8342 (1.8943)
+2022-11-18 15:18:47,676:INFO: Dataset: univ                Batch:  9/15	Loss 1.8364 (1.8885)
+2022-11-18 15:18:47,843:INFO: Dataset: univ                Batch: 10/15	Loss 1.8896 (1.8886)
+2022-11-18 15:18:48,011:INFO: Dataset: univ                Batch: 11/15	Loss 1.8903 (1.8888)
+2022-11-18 15:18:48,177:INFO: Dataset: univ                Batch: 12/15	Loss 1.9507 (1.8939)
+2022-11-18 15:18:48,344:INFO: Dataset: univ                Batch: 13/15	Loss 1.8957 (1.8940)
+2022-11-18 15:18:48,512:INFO: Dataset: univ                Batch: 14/15	Loss 1.8559 (1.8912)
+2022-11-18 15:18:48,600:INFO: Dataset: univ                Batch: 15/15	Loss 1.9238 (1.8917)
+2022-11-18 15:18:48,998:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0095 (2.0095)
+2022-11-18 15:18:49,150:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8929 (1.9540)
+2022-11-18 15:18:49,301:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8893 (1.9319)
+2022-11-18 15:18:49,449:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7847 (1.8970)
+2022-11-18 15:18:49,599:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9383 (1.9056)
+2022-11-18 15:18:49,751:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9407 (1.9117)
+2022-11-18 15:18:49,901:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9373 (1.9150)
+2022-11-18 15:18:50,037:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7524 (1.8963)
+2022-11-18 15:18:50,444:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8422 (1.8422)
+2022-11-18 15:18:50,605:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8086 (1.8253)
+2022-11-18 15:18:50,770:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9154 (1.8539)
+2022-11-18 15:18:50,929:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8757 (1.8591)
+2022-11-18 15:18:51,089:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0051 (1.8887)
+2022-11-18 15:18:51,252:INFO: Dataset: zara2               Batch:  6/18	Loss 2.0354 (1.9125)
+2022-11-18 15:18:51,413:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8710 (1.9066)
+2022-11-18 15:18:51,570:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9260 (1.9092)
+2022-11-18 15:18:51,734:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8777 (1.9058)
+2022-11-18 15:18:51,893:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9334 (1.9085)
+2022-11-18 15:18:52,052:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0753 (1.9236)
+2022-11-18 15:18:52,213:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8433 (1.9165)
+2022-11-18 15:18:52,373:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9510 (1.9190)
+2022-11-18 15:18:52,534:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8947 (1.9170)
+2022-11-18 15:18:52,697:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8655 (1.9137)
+2022-11-18 15:18:52,857:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8925 (1.9125)
+2022-11-18 15:18:53,019:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9792 (1.9161)
+2022-11-18 15:18:53,166:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8645 (1.9136)
+2022-11-18 15:18:53,216:INFO: - Computing loss (validation)
+2022-11-18 15:18:53,492:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9925 (1.9925)
+2022-11-18 15:18:53,528:INFO: Dataset: hotel               Batch: 2/2	Loss 1.3609 (1.9472)
+2022-11-18 15:18:53,837:INFO: Dataset: univ                Batch: 1/3	Loss 1.9166 (1.9166)
+2022-11-18 15:18:53,921:INFO: Dataset: univ                Batch: 2/3	Loss 1.9058 (1.9107)
+2022-11-18 15:18:53,996:INFO: Dataset: univ                Batch: 3/3	Loss 1.8622 (1.8972)
+2022-11-18 15:18:54,308:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9646 (1.9646)
+2022-11-18 15:18:54,353:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8397 (1.9341)
+2022-11-18 15:18:54,657:INFO: Dataset: zara2               Batch: 1/5	Loss 2.0046 (2.0046)
+2022-11-18 15:18:54,738:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9360 (1.9698)
+2022-11-18 15:18:54,816:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8711 (1.9375)
+2022-11-18 15:18:54,892:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9088 (1.9301)
+2022-11-18 15:18:54,966:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8905 (1.9225)
+2022-11-18 15:18:55,021:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_375.pth.tar
+2022-11-18 15:18:55,021:INFO: 
+===> EPOCH: 376 (P2)
+2022-11-18 15:18:55,022:INFO: - Computing loss (training)
+2022-11-18 15:18:55,361:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8714 (1.8714)
+2022-11-18 15:18:55,513:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8097 (1.8408)
+2022-11-18 15:18:55,665:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9262 (1.8689)
+2022-11-18 15:18:55,794:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8730 (1.8696)
+2022-11-18 15:18:56,219:INFO: Dataset: univ                Batch:  1/15	Loss 1.8549 (1.8549)
+2022-11-18 15:18:56,377:INFO: Dataset: univ                Batch:  2/15	Loss 1.9406 (1.8947)
+2022-11-18 15:18:56,539:INFO: Dataset: univ                Batch:  3/15	Loss 1.9054 (1.8982)
+2022-11-18 15:18:56,694:INFO: Dataset: univ                Batch:  4/15	Loss 1.9445 (1.9088)
+2022-11-18 15:18:56,853:INFO: Dataset: univ                Batch:  5/15	Loss 1.8900 (1.9054)
+2022-11-18 15:18:57,012:INFO: Dataset: univ                Batch:  6/15	Loss 1.9408 (1.9118)
+2022-11-18 15:18:57,169:INFO: Dataset: univ                Batch:  7/15	Loss 1.8702 (1.9057)
+2022-11-18 15:18:57,324:INFO: Dataset: univ                Batch:  8/15	Loss 1.9151 (1.9069)
+2022-11-18 15:18:57,481:INFO: Dataset: univ                Batch:  9/15	Loss 1.9305 (1.9097)
+2022-11-18 15:18:57,635:INFO: Dataset: univ                Batch: 10/15	Loss 1.9207 (1.9108)
+2022-11-18 15:18:57,792:INFO: Dataset: univ                Batch: 11/15	Loss 1.8556 (1.9062)
+2022-11-18 15:18:57,948:INFO: Dataset: univ                Batch: 12/15	Loss 1.8319 (1.8997)
+2022-11-18 15:18:58,103:INFO: Dataset: univ                Batch: 13/15	Loss 1.8818 (1.8984)
+2022-11-18 15:18:58,258:INFO: Dataset: univ                Batch: 14/15	Loss 1.8949 (1.8982)
+2022-11-18 15:18:58,341:INFO: Dataset: univ                Batch: 15/15	Loss 1.8881 (1.8980)
+2022-11-18 15:18:58,733:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8910 (1.8910)
+2022-11-18 15:18:58,886:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9690 (1.9313)
+2022-11-18 15:18:59,037:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8246 (1.8941)
+2022-11-18 15:18:59,185:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8449 (1.8823)
+2022-11-18 15:18:59,334:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8157 (1.8684)
+2022-11-18 15:18:59,484:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9126 (1.8753)
+2022-11-18 15:18:59,633:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8328 (1.8699)
+2022-11-18 15:18:59,770:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8240 (1.8647)
+2022-11-18 15:19:00,162:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9303 (1.9303)
+2022-11-18 15:19:00,316:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9083 (1.9194)
+2022-11-18 15:19:00,466:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9442 (1.9274)
+2022-11-18 15:19:00,614:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9349 (1.9292)
+2022-11-18 15:19:00,767:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9696 (1.9376)
+2022-11-18 15:19:00,919:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7950 (1.9150)
+2022-11-18 15:19:01,067:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9844 (1.9257)
+2022-11-18 15:19:01,218:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7984 (1.9099)
+2022-11-18 15:19:01,367:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8569 (1.9032)
+2022-11-18 15:19:01,515:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7824 (1.8906)
+2022-11-18 15:19:01,664:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9106 (1.8923)
+2022-11-18 15:19:01,813:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8953 (1.8926)
+2022-11-18 15:19:01,962:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8448 (1.8888)
+2022-11-18 15:19:02,111:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8777 (1.8880)
+2022-11-18 15:19:02,264:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9049 (1.8892)
+2022-11-18 15:19:02,413:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8670 (1.8877)
+2022-11-18 15:19:02,563:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8824 (1.8873)
+2022-11-18 15:19:02,700:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8763 (1.8868)
+2022-11-18 15:19:02,747:INFO: - Computing loss (validation)
+2022-11-18 15:19:03,005:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8247 (1.8247)
+2022-11-18 15:19:03,039:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0550 (1.8388)
+2022-11-18 15:19:03,347:INFO: Dataset: univ                Batch: 1/3	Loss 1.8575 (1.8575)
+2022-11-18 15:19:03,431:INFO: Dataset: univ                Batch: 2/3	Loss 1.8919 (1.8762)
+2022-11-18 15:19:03,507:INFO: Dataset: univ                Batch: 3/3	Loss 1.8160 (1.8571)
+2022-11-18 15:19:03,806:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9458 (1.9458)
+2022-11-18 15:19:03,853:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0070 (1.9611)
+2022-11-18 15:19:04,169:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8516 (1.8516)
+2022-11-18 15:19:04,244:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8126 (1.8332)
+2022-11-18 15:19:04,317:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7695 (1.8138)
+2022-11-18 15:19:04,390:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9345 (1.8446)
+2022-11-18 15:19:04,463:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9148 (1.8576)
+2022-11-18 15:19:04,514:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_376.pth.tar
+2022-11-18 15:19:04,515:INFO: 
+===> EPOCH: 377 (P2)
+2022-11-18 15:19:04,515:INFO: - Computing loss (training)
+2022-11-18 15:19:04,857:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0102 (2.0102)
+2022-11-18 15:19:05,008:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8677 (1.9419)
+2022-11-18 15:19:05,159:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9362 (1.9401)
+2022-11-18 15:19:05,277:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1300 (1.9701)
+2022-11-18 15:19:05,735:INFO: Dataset: univ                Batch:  1/15	Loss 1.8847 (1.8847)
+2022-11-18 15:19:05,889:INFO: Dataset: univ                Batch:  2/15	Loss 1.9025 (1.8924)
+2022-11-18 15:19:06,045:INFO: Dataset: univ                Batch:  3/15	Loss 1.8840 (1.8894)
+2022-11-18 15:19:06,200:INFO: Dataset: univ                Batch:  4/15	Loss 1.9011 (1.8922)
+2022-11-18 15:19:06,356:INFO: Dataset: univ                Batch:  5/15	Loss 1.9005 (1.8939)
+2022-11-18 15:19:06,514:INFO: Dataset: univ                Batch:  6/15	Loss 1.9091 (1.8964)
+2022-11-18 15:19:06,669:INFO: Dataset: univ                Batch:  7/15	Loss 1.8490 (1.8903)
+2022-11-18 15:19:06,828:INFO: Dataset: univ                Batch:  8/15	Loss 1.8449 (1.8852)
+2022-11-18 15:19:06,986:INFO: Dataset: univ                Batch:  9/15	Loss 1.8987 (1.8867)
+2022-11-18 15:19:07,141:INFO: Dataset: univ                Batch: 10/15	Loss 1.8828 (1.8863)
+2022-11-18 15:19:07,296:INFO: Dataset: univ                Batch: 11/15	Loss 1.9092 (1.8883)
+2022-11-18 15:19:07,452:INFO: Dataset: univ                Batch: 12/15	Loss 1.8541 (1.8853)
+2022-11-18 15:19:07,609:INFO: Dataset: univ                Batch: 13/15	Loss 1.8664 (1.8837)
+2022-11-18 15:19:07,765:INFO: Dataset: univ                Batch: 14/15	Loss 1.8642 (1.8824)
+2022-11-18 15:19:07,849:INFO: Dataset: univ                Batch: 15/15	Loss 1.9777 (1.8838)
+2022-11-18 15:19:08,232:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9032 (1.9032)
+2022-11-18 15:19:08,384:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8031 (1.8486)
+2022-11-18 15:19:08,535:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8135 (1.8380)
+2022-11-18 15:19:08,684:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8544 (1.8420)
+2022-11-18 15:19:08,836:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9010 (1.8537)
+2022-11-18 15:19:08,988:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0778 (1.8947)
+2022-11-18 15:19:09,140:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8739 (1.8919)
+2022-11-18 15:19:09,277:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8591 (1.8883)
+2022-11-18 15:19:09,668:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8559 (1.8559)
+2022-11-18 15:19:09,817:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9915 (1.9236)
+2022-11-18 15:19:09,969:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9033 (1.9176)
+2022-11-18 15:19:10,121:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8635 (1.9045)
+2022-11-18 15:19:10,273:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9467 (1.9134)
+2022-11-18 15:19:10,424:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8698 (1.9068)
+2022-11-18 15:19:10,574:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8555 (1.8998)
+2022-11-18 15:19:10,722:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8943 (1.8992)
+2022-11-18 15:19:10,872:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9306 (1.9028)
+2022-11-18 15:19:11,019:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9767 (1.9101)
+2022-11-18 15:19:11,171:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9362 (1.9127)
+2022-11-18 15:19:11,319:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8330 (1.9056)
+2022-11-18 15:19:11,469:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9642 (1.9108)
+2022-11-18 15:19:11,618:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9259 (1.9120)
+2022-11-18 15:19:11,768:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7877 (1.9036)
+2022-11-18 15:19:11,917:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8925 (1.9030)
+2022-11-18 15:19:12,067:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9740 (1.9072)
+2022-11-18 15:19:12,206:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9694 (1.9101)
+2022-11-18 15:19:12,259:INFO: - Computing loss (validation)
+2022-11-18 15:19:12,522:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9459 (1.9459)
+2022-11-18 15:19:12,557:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9890 (1.9491)
+2022-11-18 15:19:12,863:INFO: Dataset: univ                Batch: 1/3	Loss 1.8534 (1.8534)
+2022-11-18 15:19:12,940:INFO: Dataset: univ                Batch: 2/3	Loss 1.8902 (1.8733)
+2022-11-18 15:19:13,013:INFO: Dataset: univ                Batch: 3/3	Loss 1.8715 (1.8727)
+2022-11-18 15:19:13,313:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8589 (1.8589)
+2022-11-18 15:19:13,357:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8326 (1.8532)
+2022-11-18 15:19:13,657:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8253 (1.8253)
+2022-11-18 15:19:13,731:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9528 (1.8891)
+2022-11-18 15:19:13,806:INFO: Dataset: zara2               Batch: 3/5	Loss 2.0052 (1.9278)
+2022-11-18 15:19:13,881:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9307 (1.9285)
+2022-11-18 15:19:13,955:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8595 (1.9146)
+2022-11-18 15:19:14,008:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_377.pth.tar
+2022-11-18 15:19:14,008:INFO: 
+===> EPOCH: 378 (P2)
+2022-11-18 15:19:14,009:INFO: - Computing loss (training)
+2022-11-18 15:19:14,351:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9654 (1.9654)
+2022-11-18 15:19:14,504:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8891 (1.9293)
+2022-11-18 15:19:14,656:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0177 (1.9586)
+2022-11-18 15:19:14,775:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0515 (1.9756)
+2022-11-18 15:19:15,175:INFO: Dataset: univ                Batch:  1/15	Loss 1.8170 (1.8170)
+2022-11-18 15:19:15,332:INFO: Dataset: univ                Batch:  2/15	Loss 1.8861 (1.8525)
+2022-11-18 15:19:15,494:INFO: Dataset: univ                Batch:  3/15	Loss 1.9012 (1.8684)
+2022-11-18 15:19:15,648:INFO: Dataset: univ                Batch:  4/15	Loss 1.8937 (1.8753)
+2022-11-18 15:19:15,806:INFO: Dataset: univ                Batch:  5/15	Loss 1.8587 (1.8718)
+2022-11-18 15:19:15,964:INFO: Dataset: univ                Batch:  6/15	Loss 1.8657 (1.8708)
+2022-11-18 15:19:16,119:INFO: Dataset: univ                Batch:  7/15	Loss 1.9669 (1.8834)
+2022-11-18 15:19:16,276:INFO: Dataset: univ                Batch:  8/15	Loss 1.8786 (1.8828)
+2022-11-18 15:19:16,432:INFO: Dataset: univ                Batch:  9/15	Loss 1.9073 (1.8856)
+2022-11-18 15:19:16,585:INFO: Dataset: univ                Batch: 10/15	Loss 1.8907 (1.8861)
+2022-11-18 15:19:16,741:INFO: Dataset: univ                Batch: 11/15	Loss 1.8256 (1.8807)
+2022-11-18 15:19:16,896:INFO: Dataset: univ                Batch: 12/15	Loss 1.8972 (1.8820)
+2022-11-18 15:19:17,052:INFO: Dataset: univ                Batch: 13/15	Loss 1.9349 (1.8864)
+2022-11-18 15:19:17,210:INFO: Dataset: univ                Batch: 14/15	Loss 1.9001 (1.8873)
+2022-11-18 15:19:17,292:INFO: Dataset: univ                Batch: 15/15	Loss 1.9599 (1.8885)
+2022-11-18 15:19:17,677:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8910 (1.8910)
+2022-11-18 15:19:17,829:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9518 (1.9230)
+2022-11-18 15:19:17,979:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8955 (1.9142)
+2022-11-18 15:19:18,126:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8959 (1.9095)
+2022-11-18 15:19:18,275:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8010 (1.8860)
+2022-11-18 15:19:18,423:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8965 (1.8879)
+2022-11-18 15:19:18,570:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9927 (1.9026)
+2022-11-18 15:19:18,704:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8968 (1.9019)
+2022-11-18 15:19:19,097:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8903 (1.8903)
+2022-11-18 15:19:19,248:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8623 (1.8761)
+2022-11-18 15:19:19,399:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8213 (1.8585)
+2022-11-18 15:19:19,552:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9431 (1.8813)
+2022-11-18 15:19:19,703:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9465 (1.8949)
+2022-11-18 15:19:19,856:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8862 (1.8932)
+2022-11-18 15:19:20,006:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8837 (1.8919)
+2022-11-18 15:19:20,154:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8404 (1.8850)
+2022-11-18 15:19:20,304:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8637 (1.8828)
+2022-11-18 15:19:20,452:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9499 (1.8897)
+2022-11-18 15:19:20,605:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8605 (1.8873)
+2022-11-18 15:19:20,754:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8561 (1.8846)
+2022-11-18 15:19:20,904:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8780 (1.8841)
+2022-11-18 15:19:21,054:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8836 (1.8840)
+2022-11-18 15:19:21,206:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9848 (1.8908)
+2022-11-18 15:19:21,358:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9395 (1.8939)
+2022-11-18 15:19:21,510:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8907 (1.8937)
+2022-11-18 15:19:21,648:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8826 (1.8931)
+2022-11-18 15:19:21,694:INFO: - Computing loss (validation)
+2022-11-18 15:19:21,960:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9512 (1.9512)
+2022-11-18 15:19:21,994:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0076 (1.9560)
+2022-11-18 15:19:22,303:INFO: Dataset: univ                Batch: 1/3	Loss 1.8225 (1.8225)
+2022-11-18 15:19:22,378:INFO: Dataset: univ                Batch: 2/3	Loss 1.8550 (1.8388)
+2022-11-18 15:19:22,452:INFO: Dataset: univ                Batch: 3/3	Loss 1.8193 (1.8317)
+2022-11-18 15:19:22,759:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7927 (1.7927)
+2022-11-18 15:19:22,807:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7303 (1.7775)
+2022-11-18 15:19:23,124:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8854 (1.8854)
+2022-11-18 15:19:23,200:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9533 (1.9165)
+2022-11-18 15:19:23,273:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9400 (1.9247)
+2022-11-18 15:19:23,346:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9131 (1.9220)
+2022-11-18 15:19:23,420:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8721 (1.9119)
+2022-11-18 15:19:23,476:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_378.pth.tar
+2022-11-18 15:19:23,476:INFO: 
+===> EPOCH: 379 (P2)
+2022-11-18 15:19:23,476:INFO: - Computing loss (training)
+2022-11-18 15:19:23,824:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9754 (1.9754)
+2022-11-18 15:19:23,979:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8729 (1.9246)
+2022-11-18 15:19:24,133:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8647 (1.9047)
+2022-11-18 15:19:24,255:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8981 (1.9035)
+2022-11-18 15:19:24,748:INFO: Dataset: univ                Batch:  1/15	Loss 1.8952 (1.8952)
+2022-11-18 15:19:24,913:INFO: Dataset: univ                Batch:  2/15	Loss 1.9028 (1.8989)
+2022-11-18 15:19:25,075:INFO: Dataset: univ                Batch:  3/15	Loss 1.8993 (1.8991)
+2022-11-18 15:19:25,234:INFO: Dataset: univ                Batch:  4/15	Loss 1.8867 (1.8959)
+2022-11-18 15:19:25,394:INFO: Dataset: univ                Batch:  5/15	Loss 1.8794 (1.8925)
+2022-11-18 15:19:25,553:INFO: Dataset: univ                Batch:  6/15	Loss 1.9272 (1.8980)
+2022-11-18 15:19:25,711:INFO: Dataset: univ                Batch:  7/15	Loss 1.8536 (1.8920)
+2022-11-18 15:19:25,867:INFO: Dataset: univ                Batch:  8/15	Loss 1.8638 (1.8886)
+2022-11-18 15:19:26,024:INFO: Dataset: univ                Batch:  9/15	Loss 1.8700 (1.8865)
+2022-11-18 15:19:26,179:INFO: Dataset: univ                Batch: 10/15	Loss 1.8591 (1.8838)
+2022-11-18 15:19:26,337:INFO: Dataset: univ                Batch: 11/15	Loss 1.8829 (1.8837)
+2022-11-18 15:19:26,495:INFO: Dataset: univ                Batch: 12/15	Loss 1.8560 (1.8814)
+2022-11-18 15:19:26,652:INFO: Dataset: univ                Batch: 13/15	Loss 1.8853 (1.8817)
+2022-11-18 15:19:26,810:INFO: Dataset: univ                Batch: 14/15	Loss 1.8883 (1.8821)
+2022-11-18 15:19:26,894:INFO: Dataset: univ                Batch: 15/15	Loss 1.9571 (1.8830)
+2022-11-18 15:19:27,288:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0172 (2.0172)
+2022-11-18 15:19:27,439:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9169 (1.9602)
+2022-11-18 15:19:27,590:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9073 (1.9416)
+2022-11-18 15:19:27,741:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9195 (1.9360)
+2022-11-18 15:19:27,896:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8675 (1.9217)
+2022-11-18 15:19:28,046:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8902 (1.9162)
+2022-11-18 15:19:28,196:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8798 (1.9114)
+2022-11-18 15:19:28,335:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8729 (1.9073)
+2022-11-18 15:19:28,717:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9513 (1.9513)
+2022-11-18 15:19:28,876:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8903 (1.9212)
+2022-11-18 15:19:29,030:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9464 (1.9290)
+2022-11-18 15:19:29,185:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8851 (1.9185)
+2022-11-18 15:19:29,340:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9994 (1.9367)
+2022-11-18 15:19:29,494:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9224 (1.9343)
+2022-11-18 15:19:29,645:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9012 (1.9297)
+2022-11-18 15:19:29,793:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8505 (1.9190)
+2022-11-18 15:19:29,943:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8860 (1.9154)
+2022-11-18 15:19:30,090:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8993 (1.9138)
+2022-11-18 15:19:30,242:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9843 (1.9201)
+2022-11-18 15:19:30,390:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8991 (1.9182)
+2022-11-18 15:19:30,538:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9618 (1.9218)
+2022-11-18 15:19:30,688:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8347 (1.9156)
+2022-11-18 15:19:30,839:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8935 (1.9140)
+2022-11-18 15:19:30,989:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9024 (1.9133)
+2022-11-18 15:19:31,140:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9268 (1.9141)
+2022-11-18 15:19:31,280:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8994 (1.9134)
+2022-11-18 15:19:31,325:INFO: - Computing loss (validation)
+2022-11-18 15:19:31,585:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9364 (1.9364)
+2022-11-18 15:19:31,619:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9886 (1.9412)
+2022-11-18 15:19:31,939:INFO: Dataset: univ                Batch: 1/3	Loss 1.8891 (1.8891)
+2022-11-18 15:19:32,019:INFO: Dataset: univ                Batch: 2/3	Loss 1.9138 (1.9008)
+2022-11-18 15:19:32,093:INFO: Dataset: univ                Batch: 3/3	Loss 1.8566 (1.8876)
+2022-11-18 15:19:32,394:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9423 (1.9423)
+2022-11-18 15:19:32,441:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7577 (1.8972)
+2022-11-18 15:19:32,740:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9149 (1.9149)
+2022-11-18 15:19:32,818:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9199 (1.9173)
+2022-11-18 15:19:32,891:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9006 (1.9118)
+2022-11-18 15:19:32,965:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9110 (1.9116)
+2022-11-18 15:19:33,040:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8950 (1.9082)
+2022-11-18 15:19:33,091:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_379.pth.tar
+2022-11-18 15:19:33,091:INFO: 
+===> EPOCH: 380 (P2)
+2022-11-18 15:19:33,092:INFO: - Computing loss (training)
+2022-11-18 15:19:33,428:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9457 (1.9457)
+2022-11-18 15:19:33,576:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9403 (1.9430)
+2022-11-18 15:19:33,725:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9203 (1.9354)
+2022-11-18 15:19:33,841:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8399 (1.9204)
+2022-11-18 15:19:34,245:INFO: Dataset: univ                Batch:  1/15	Loss 1.8811 (1.8811)
+2022-11-18 15:19:34,404:INFO: Dataset: univ                Batch:  2/15	Loss 1.8726 (1.8766)
+2022-11-18 15:19:34,564:INFO: Dataset: univ                Batch:  3/15	Loss 1.8900 (1.8813)
+2022-11-18 15:19:34,723:INFO: Dataset: univ                Batch:  4/15	Loss 1.8992 (1.8860)
+2022-11-18 15:19:34,881:INFO: Dataset: univ                Batch:  5/15	Loss 1.8729 (1.8833)
+2022-11-18 15:19:35,040:INFO: Dataset: univ                Batch:  6/15	Loss 1.8838 (1.8834)
+2022-11-18 15:19:35,198:INFO: Dataset: univ                Batch:  7/15	Loss 1.9196 (1.8886)
+2022-11-18 15:19:35,354:INFO: Dataset: univ                Batch:  8/15	Loss 1.9269 (1.8932)
+2022-11-18 15:19:35,510:INFO: Dataset: univ                Batch:  9/15	Loss 1.9542 (1.8995)
+2022-11-18 15:19:35,665:INFO: Dataset: univ                Batch: 10/15	Loss 1.8763 (1.8972)
+2022-11-18 15:19:35,822:INFO: Dataset: univ                Batch: 11/15	Loss 1.8578 (1.8936)
+2022-11-18 15:19:35,979:INFO: Dataset: univ                Batch: 12/15	Loss 1.8598 (1.8907)
+2022-11-18 15:19:36,142:INFO: Dataset: univ                Batch: 13/15	Loss 1.9328 (1.8938)
+2022-11-18 15:19:36,305:INFO: Dataset: univ                Batch: 14/15	Loss 1.8742 (1.8924)
+2022-11-18 15:19:36,389:INFO: Dataset: univ                Batch: 15/15	Loss 1.8462 (1.8916)
+2022-11-18 15:19:36,788:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8732 (1.8732)
+2022-11-18 15:19:36,937:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9321 (1.9053)
+2022-11-18 15:19:37,088:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8366 (1.8831)
+2022-11-18 15:19:37,240:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7506 (1.8501)
+2022-11-18 15:19:37,390:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9497 (1.8721)
+2022-11-18 15:19:37,541:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0437 (1.8991)
+2022-11-18 15:19:37,690:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8422 (1.8904)
+2022-11-18 15:19:37,828:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7197 (1.8716)
+2022-11-18 15:19:38,232:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9935 (1.9935)
+2022-11-18 15:19:38,382:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9223 (1.9548)
+2022-11-18 15:19:38,534:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8747 (1.9326)
+2022-11-18 15:19:38,685:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9258 (1.9310)
+2022-11-18 15:19:38,842:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9901 (1.9430)
+2022-11-18 15:19:38,997:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9528 (1.9445)
+2022-11-18 15:19:39,151:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8783 (1.9361)
+2022-11-18 15:19:39,302:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9187 (1.9340)
+2022-11-18 15:19:39,453:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7764 (1.9154)
+2022-11-18 15:19:39,602:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8698 (1.9104)
+2022-11-18 15:19:39,753:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8523 (1.9047)
+2022-11-18 15:19:39,904:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8967 (1.9041)
+2022-11-18 15:19:40,055:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9744 (1.9094)
+2022-11-18 15:19:40,207:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9566 (1.9127)
+2022-11-18 15:19:40,359:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9040 (1.9121)
+2022-11-18 15:19:40,510:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9261 (1.9131)
+2022-11-18 15:19:40,662:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9421 (1.9149)
+2022-11-18 15:19:40,802:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8565 (1.9120)
+2022-11-18 15:19:40,848:INFO: - Computing loss (validation)
+2022-11-18 15:19:41,119:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0987 (2.0987)
+2022-11-18 15:19:41,154:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7744 (2.0777)
+2022-11-18 15:19:41,476:INFO: Dataset: univ                Batch: 1/3	Loss 1.8932 (1.8932)
+2022-11-18 15:19:41,561:INFO: Dataset: univ                Batch: 2/3	Loss 1.8678 (1.8807)
+2022-11-18 15:19:41,637:INFO: Dataset: univ                Batch: 3/3	Loss 1.8495 (1.8710)
+2022-11-18 15:19:41,949:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0331 (2.0331)
+2022-11-18 15:19:41,997:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8291 (1.9813)
+2022-11-18 15:19:42,298:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8034 (1.8034)
+2022-11-18 15:19:42,373:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8661 (1.8362)
+2022-11-18 15:19:42,447:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8055 (1.8262)
+2022-11-18 15:19:42,522:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8852 (1.8413)
+2022-11-18 15:19:42,595:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8821 (1.8492)
+2022-11-18 15:19:42,663:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_380.pth.tar
+2022-11-18 15:19:42,663:INFO: 
+===> EPOCH: 381 (P2)
+2022-11-18 15:19:42,664:INFO: - Computing loss (training)
+2022-11-18 15:19:42,992:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9137 (1.9137)
+2022-11-18 15:19:43,144:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0550 (1.9869)
+2022-11-18 15:19:43,298:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8833 (1.9523)
+2022-11-18 15:19:43,414:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8601 (1.9363)
+2022-11-18 15:19:43,813:INFO: Dataset: univ                Batch:  1/15	Loss 1.8602 (1.8602)
+2022-11-18 15:19:43,971:INFO: Dataset: univ                Batch:  2/15	Loss 1.9075 (1.8839)
+2022-11-18 15:19:44,127:INFO: Dataset: univ                Batch:  3/15	Loss 1.8934 (1.8871)
+2022-11-18 15:19:44,284:INFO: Dataset: univ                Batch:  4/15	Loss 1.9195 (1.8954)
+2022-11-18 15:19:44,439:INFO: Dataset: univ                Batch:  5/15	Loss 1.8707 (1.8905)
+2022-11-18 15:19:44,596:INFO: Dataset: univ                Batch:  6/15	Loss 1.9420 (1.8991)
+2022-11-18 15:19:44,751:INFO: Dataset: univ                Batch:  7/15	Loss 1.8467 (1.8924)
+2022-11-18 15:19:44,903:INFO: Dataset: univ                Batch:  8/15	Loss 1.9445 (1.8987)
+2022-11-18 15:19:45,059:INFO: Dataset: univ                Batch:  9/15	Loss 1.8435 (1.8927)
+2022-11-18 15:19:45,212:INFO: Dataset: univ                Batch: 10/15	Loss 1.9041 (1.8938)
+2022-11-18 15:19:45,367:INFO: Dataset: univ                Batch: 11/15	Loss 1.8859 (1.8931)
+2022-11-18 15:19:45,521:INFO: Dataset: univ                Batch: 12/15	Loss 1.9229 (1.8957)
+2022-11-18 15:19:45,677:INFO: Dataset: univ                Batch: 13/15	Loss 1.8554 (1.8922)
+2022-11-18 15:19:45,832:INFO: Dataset: univ                Batch: 14/15	Loss 1.8734 (1.8908)
+2022-11-18 15:19:45,914:INFO: Dataset: univ                Batch: 15/15	Loss 1.8933 (1.8908)
+2022-11-18 15:19:46,301:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8366 (1.8366)
+2022-11-18 15:19:46,449:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8728 (1.8545)
+2022-11-18 15:19:46,602:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9877 (1.9005)
+2022-11-18 15:19:46,753:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9024 (1.9010)
+2022-11-18 15:19:46,904:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9579 (1.9124)
+2022-11-18 15:19:47,053:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9007 (1.9104)
+2022-11-18 15:19:47,204:INFO: Dataset: zara1               Batch: 7/8	Loss 2.1607 (1.9429)
+2022-11-18 15:19:47,340:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9325 (1.9418)
+2022-11-18 15:19:47,731:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8783 (1.8783)
+2022-11-18 15:19:47,882:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8165 (1.8458)
+2022-11-18 15:19:48,037:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9127 (1.8667)
+2022-11-18 15:19:48,187:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8584 (1.8646)
+2022-11-18 15:19:48,338:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9942 (1.8895)
+2022-11-18 15:19:48,489:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9529 (1.8990)
+2022-11-18 15:19:48,639:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9877 (1.9115)
+2022-11-18 15:19:48,788:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8845 (1.9081)
+2022-11-18 15:19:48,939:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8675 (1.9031)
+2022-11-18 15:19:49,087:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9059 (1.9034)
+2022-11-18 15:19:49,238:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7638 (1.8901)
+2022-11-18 15:19:49,388:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8807 (1.8893)
+2022-11-18 15:19:49,538:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9481 (1.8936)
+2022-11-18 15:19:49,688:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8651 (1.8915)
+2022-11-18 15:19:49,839:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8047 (1.8855)
+2022-11-18 15:19:49,988:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8329 (1.8824)
+2022-11-18 15:19:50,140:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0049 (1.8891)
+2022-11-18 15:19:50,279:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9494 (1.8922)
+2022-11-18 15:19:50,333:INFO: - Computing loss (validation)
+2022-11-18 15:19:50,601:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8873 (1.8873)
+2022-11-18 15:19:50,635:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9360 (1.8920)
+2022-11-18 15:19:50,935:INFO: Dataset: univ                Batch: 1/3	Loss 1.8269 (1.8269)
+2022-11-18 15:19:51,013:INFO: Dataset: univ                Batch: 2/3	Loss 1.8525 (1.8407)
+2022-11-18 15:19:51,086:INFO: Dataset: univ                Batch: 3/3	Loss 1.8970 (1.8599)
+2022-11-18 15:19:51,397:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8957 (1.8957)
+2022-11-18 15:19:51,443:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8136 (1.8767)
+2022-11-18 15:19:51,749:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9076 (1.9076)
+2022-11-18 15:19:51,826:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9353 (1.9218)
+2022-11-18 15:19:51,905:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8468 (1.8949)
+2022-11-18 15:19:51,982:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8097 (1.8720)
+2022-11-18 15:19:52,058:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9550 (1.8886)
+2022-11-18 15:19:52,110:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_381.pth.tar
+2022-11-18 15:19:52,111:INFO: 
+===> EPOCH: 382 (P2)
+2022-11-18 15:19:52,111:INFO: - Computing loss (training)
+2022-11-18 15:19:52,457:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9836 (1.9836)
+2022-11-18 15:19:52,607:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8797 (1.9311)
+2022-11-18 15:19:52,761:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9844 (1.9488)
+2022-11-18 15:19:52,878:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0193 (1.9619)
+2022-11-18 15:19:53,298:INFO: Dataset: univ                Batch:  1/15	Loss 1.9147 (1.9147)
+2022-11-18 15:19:53,458:INFO: Dataset: univ                Batch:  2/15	Loss 1.9151 (1.9149)
+2022-11-18 15:19:53,618:INFO: Dataset: univ                Batch:  3/15	Loss 1.8880 (1.9061)
+2022-11-18 15:19:53,780:INFO: Dataset: univ                Batch:  4/15	Loss 1.9143 (1.9080)
+2022-11-18 15:19:53,937:INFO: Dataset: univ                Batch:  5/15	Loss 1.8641 (1.9000)
+2022-11-18 15:19:54,095:INFO: Dataset: univ                Batch:  6/15	Loss 1.8655 (1.8938)
+2022-11-18 15:19:54,254:INFO: Dataset: univ                Batch:  7/15	Loss 1.8351 (1.8847)
+2022-11-18 15:19:54,409:INFO: Dataset: univ                Batch:  8/15	Loss 1.9035 (1.8871)
+2022-11-18 15:19:54,566:INFO: Dataset: univ                Batch:  9/15	Loss 1.9121 (1.8898)
+2022-11-18 15:19:54,721:INFO: Dataset: univ                Batch: 10/15	Loss 1.9304 (1.8940)
+2022-11-18 15:19:54,882:INFO: Dataset: univ                Batch: 11/15	Loss 1.8871 (1.8934)
+2022-11-18 15:19:55,038:INFO: Dataset: univ                Batch: 12/15	Loss 1.8746 (1.8917)
+2022-11-18 15:19:55,195:INFO: Dataset: univ                Batch: 13/15	Loss 1.9039 (1.8926)
+2022-11-18 15:19:55,355:INFO: Dataset: univ                Batch: 14/15	Loss 1.8516 (1.8895)
+2022-11-18 15:19:55,439:INFO: Dataset: univ                Batch: 15/15	Loss 1.8677 (1.8893)
+2022-11-18 15:19:55,830:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9412 (1.9412)
+2022-11-18 15:19:55,978:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8991 (1.9209)
+2022-11-18 15:19:56,129:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8201 (1.8874)
+2022-11-18 15:19:56,302:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9584 (1.9056)
+2022-11-18 15:19:56,473:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8705 (1.8987)
+2022-11-18 15:19:56,624:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8459 (1.8893)
+2022-11-18 15:19:56,775:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8951 (1.8901)
+2022-11-18 15:19:56,913:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8355 (1.8837)
+2022-11-18 15:19:57,305:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8639 (1.8639)
+2022-11-18 15:19:57,461:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8674 (1.8657)
+2022-11-18 15:19:57,634:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0088 (1.9133)
+2022-11-18 15:19:57,808:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8893 (1.9080)
+2022-11-18 15:19:57,965:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8787 (1.9019)
+2022-11-18 15:19:58,115:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9243 (1.9056)
+2022-11-18 15:19:58,267:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8643 (1.8997)
+2022-11-18 15:19:58,416:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0189 (1.9155)
+2022-11-18 15:19:58,566:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8476 (1.9080)
+2022-11-18 15:19:58,714:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9913 (1.9169)
+2022-11-18 15:19:58,871:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9226 (1.9174)
+2022-11-18 15:19:59,019:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9760 (1.9226)
+2022-11-18 15:19:59,170:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8311 (1.9156)
+2022-11-18 15:19:59,321:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8078 (1.9075)
+2022-11-18 15:19:59,472:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9461 (1.9101)
+2022-11-18 15:19:59,620:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9377 (1.9117)
+2022-11-18 15:19:59,772:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8024 (1.9051)
+2022-11-18 15:19:59,911:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8310 (1.9018)
+2022-11-18 15:19:59,958:INFO: - Computing loss (validation)
+2022-11-18 15:20:00,225:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8783 (1.8783)
+2022-11-18 15:20:00,260:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0547 (1.8885)
+2022-11-18 15:20:00,578:INFO: Dataset: univ                Batch: 1/3	Loss 1.8914 (1.8914)
+2022-11-18 15:20:00,655:INFO: Dataset: univ                Batch: 2/3	Loss 1.8717 (1.8814)
+2022-11-18 15:20:00,729:INFO: Dataset: univ                Batch: 3/3	Loss 1.9080 (1.8902)
+2022-11-18 15:20:01,025:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8634 (1.8634)
+2022-11-18 15:20:01,071:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8918 (1.8707)
+2022-11-18 15:20:01,379:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8113 (1.8113)
+2022-11-18 15:20:01,456:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8729 (1.8401)
+2022-11-18 15:20:01,534:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8630 (1.8477)
+2022-11-18 15:20:01,609:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9587 (1.8764)
+2022-11-18 15:20:01,683:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8734 (1.8758)
+2022-11-18 15:20:01,737:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_382.pth.tar
+2022-11-18 15:20:01,737:INFO: 
+===> EPOCH: 383 (P2)
+2022-11-18 15:20:01,737:INFO: - Computing loss (training)
+2022-11-18 15:20:02,075:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8825 (1.8825)
+2022-11-18 15:20:02,226:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8319 (1.8576)
+2022-11-18 15:20:02,377:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8566 (1.8573)
+2022-11-18 15:20:02,492:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9690 (1.8763)
+2022-11-18 15:20:02,892:INFO: Dataset: univ                Batch:  1/15	Loss 1.8860 (1.8860)
+2022-11-18 15:20:03,055:INFO: Dataset: univ                Batch:  2/15	Loss 1.9072 (1.8965)
+2022-11-18 15:20:03,212:INFO: Dataset: univ                Batch:  3/15	Loss 1.8464 (1.8806)
+2022-11-18 15:20:03,366:INFO: Dataset: univ                Batch:  4/15	Loss 1.8642 (1.8767)
+2022-11-18 15:20:03,524:INFO: Dataset: univ                Batch:  5/15	Loss 1.9436 (1.8915)
+2022-11-18 15:20:03,678:INFO: Dataset: univ                Batch:  6/15	Loss 1.8887 (1.8911)
+2022-11-18 15:20:03,834:INFO: Dataset: univ                Batch:  7/15	Loss 1.9197 (1.8949)
+2022-11-18 15:20:03,990:INFO: Dataset: univ                Batch:  8/15	Loss 1.8652 (1.8908)
+2022-11-18 15:20:04,146:INFO: Dataset: univ                Batch:  9/15	Loss 1.8332 (1.8840)
+2022-11-18 15:20:04,302:INFO: Dataset: univ                Batch: 10/15	Loss 1.9061 (1.8864)
+2022-11-18 15:20:04,460:INFO: Dataset: univ                Batch: 11/15	Loss 1.8669 (1.8847)
+2022-11-18 15:20:04,614:INFO: Dataset: univ                Batch: 12/15	Loss 1.9104 (1.8869)
+2022-11-18 15:20:04,773:INFO: Dataset: univ                Batch: 13/15	Loss 1.8749 (1.8858)
+2022-11-18 15:20:04,929:INFO: Dataset: univ                Batch: 14/15	Loss 1.8866 (1.8859)
+2022-11-18 15:20:05,013:INFO: Dataset: univ                Batch: 15/15	Loss 1.7947 (1.8849)
+2022-11-18 15:20:05,395:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9487 (1.9487)
+2022-11-18 15:20:05,544:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9345 (1.9417)
+2022-11-18 15:20:05,693:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0941 (1.9908)
+2022-11-18 15:20:05,845:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0278 (1.9998)
+2022-11-18 15:20:05,994:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9900 (1.9980)
+2022-11-18 15:20:06,143:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8497 (1.9724)
+2022-11-18 15:20:06,294:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9722 (1.9724)
+2022-11-18 15:20:06,429:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9450 (1.9692)
+2022-11-18 15:20:06,850:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8183 (1.8183)
+2022-11-18 15:20:07,003:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9523 (1.8853)
+2022-11-18 15:20:07,156:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8606 (1.8765)
+2022-11-18 15:20:07,308:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8231 (1.8625)
+2022-11-18 15:20:07,461:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8859 (1.8670)
+2022-11-18 15:20:07,615:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9134 (1.8754)
+2022-11-18 15:20:07,767:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8357 (1.8701)
+2022-11-18 15:20:07,919:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9186 (1.8759)
+2022-11-18 15:20:08,070:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9367 (1.8821)
+2022-11-18 15:20:08,221:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9756 (1.8918)
+2022-11-18 15:20:08,373:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9115 (1.8934)
+2022-11-18 15:20:08,524:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8667 (1.8912)
+2022-11-18 15:20:08,676:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9060 (1.8924)
+2022-11-18 15:20:08,829:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9130 (1.8940)
+2022-11-18 15:20:08,986:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0027 (1.9008)
+2022-11-18 15:20:09,138:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9204 (1.9020)
+2022-11-18 15:20:09,293:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8323 (1.8980)
+2022-11-18 15:20:09,433:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8767 (1.8971)
+2022-11-18 15:20:09,478:INFO: - Computing loss (validation)
+2022-11-18 15:20:09,732:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8374 (1.8374)
+2022-11-18 15:20:09,766:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2308 (1.8670)
+2022-11-18 15:20:10,072:INFO: Dataset: univ                Batch: 1/3	Loss 1.8789 (1.8789)
+2022-11-18 15:20:10,156:INFO: Dataset: univ                Batch: 2/3	Loss 1.9074 (1.8926)
+2022-11-18 15:20:10,232:INFO: Dataset: univ                Batch: 3/3	Loss 1.8954 (1.8935)
+2022-11-18 15:20:10,537:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8945 (1.8945)
+2022-11-18 15:20:10,583:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8078 (1.8730)
+2022-11-18 15:20:10,890:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8528 (1.8528)
+2022-11-18 15:20:10,969:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9261 (1.8910)
+2022-11-18 15:20:11,046:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8725 (1.8849)
+2022-11-18 15:20:11,124:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8695 (1.8809)
+2022-11-18 15:20:11,203:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9352 (1.8915)
+2022-11-18 15:20:11,256:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_383.pth.tar
+2022-11-18 15:20:11,256:INFO: 
+===> EPOCH: 384 (P2)
+2022-11-18 15:20:11,257:INFO: - Computing loss (training)
+2022-11-18 15:20:11,593:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8886 (1.8886)
+2022-11-18 15:20:11,743:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9921 (1.9368)
+2022-11-18 15:20:11,896:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9393 (1.9377)
+2022-11-18 15:20:12,011:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8266 (1.9185)
+2022-11-18 15:20:12,423:INFO: Dataset: univ                Batch:  1/15	Loss 1.9117 (1.9117)
+2022-11-18 15:20:12,582:INFO: Dataset: univ                Batch:  2/15	Loss 1.8931 (1.9028)
+2022-11-18 15:20:12,740:INFO: Dataset: univ                Batch:  3/15	Loss 1.8714 (1.8922)
+2022-11-18 15:20:12,899:INFO: Dataset: univ                Batch:  4/15	Loss 1.9356 (1.9027)
+2022-11-18 15:20:13,058:INFO: Dataset: univ                Batch:  5/15	Loss 1.8747 (1.8974)
+2022-11-18 15:20:13,218:INFO: Dataset: univ                Batch:  6/15	Loss 1.9011 (1.8980)
+2022-11-18 15:20:13,375:INFO: Dataset: univ                Batch:  7/15	Loss 1.8872 (1.8966)
+2022-11-18 15:20:13,529:INFO: Dataset: univ                Batch:  8/15	Loss 1.8386 (1.8902)
+2022-11-18 15:20:13,686:INFO: Dataset: univ                Batch:  9/15	Loss 1.9267 (1.8943)
+2022-11-18 15:20:13,841:INFO: Dataset: univ                Batch: 10/15	Loss 1.8687 (1.8916)
+2022-11-18 15:20:13,997:INFO: Dataset: univ                Batch: 11/15	Loss 1.9529 (1.8970)
+2022-11-18 15:20:14,153:INFO: Dataset: univ                Batch: 12/15	Loss 1.8606 (1.8938)
+2022-11-18 15:20:14,311:INFO: Dataset: univ                Batch: 13/15	Loss 1.9026 (1.8945)
+2022-11-18 15:20:14,467:INFO: Dataset: univ                Batch: 14/15	Loss 1.8559 (1.8920)
+2022-11-18 15:20:14,550:INFO: Dataset: univ                Batch: 15/15	Loss 1.7947 (1.8908)
+2022-11-18 15:20:14,938:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9526 (1.9526)
+2022-11-18 15:20:15,088:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8165 (1.8849)
+2022-11-18 15:20:15,239:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8585 (1.8758)
+2022-11-18 15:20:15,390:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0012 (1.9080)
+2022-11-18 15:20:15,540:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8603 (1.8978)
+2022-11-18 15:20:15,691:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9070 (1.8994)
+2022-11-18 15:20:15,841:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9524 (1.9070)
+2022-11-18 15:20:15,977:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9204 (1.9085)
+2022-11-18 15:20:16,364:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8345 (1.8345)
+2022-11-18 15:20:16,518:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8991 (1.8649)
+2022-11-18 15:20:16,670:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8067 (1.8454)
+2022-11-18 15:20:16,826:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9276 (1.8650)
+2022-11-18 15:20:16,979:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8208 (1.8559)
+2022-11-18 15:20:17,131:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8030 (1.8468)
+2022-11-18 15:20:17,285:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9004 (1.8540)
+2022-11-18 15:20:17,435:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8233 (1.8503)
+2022-11-18 15:20:17,587:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9142 (1.8575)
+2022-11-18 15:20:17,736:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8356 (1.8551)
+2022-11-18 15:20:17,891:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8646 (1.8560)
+2022-11-18 15:20:18,042:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9450 (1.8637)
+2022-11-18 15:20:18,195:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8964 (1.8663)
+2022-11-18 15:20:18,346:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8602 (1.8659)
+2022-11-18 15:20:18,498:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9425 (1.8712)
+2022-11-18 15:20:18,649:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8333 (1.8690)
+2022-11-18 15:20:18,803:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9010 (1.8708)
+2022-11-18 15:20:18,942:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0128 (1.8776)
+2022-11-18 15:20:18,987:INFO: - Computing loss (validation)
+2022-11-18 15:20:19,253:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9048 (1.9048)
+2022-11-18 15:20:19,288:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1171 (1.9243)
+2022-11-18 15:20:19,601:INFO: Dataset: univ                Batch: 1/3	Loss 1.8668 (1.8668)
+2022-11-18 15:20:19,677:INFO: Dataset: univ                Batch: 2/3	Loss 1.8877 (1.8770)
+2022-11-18 15:20:19,747:INFO: Dataset: univ                Batch: 3/3	Loss 1.8604 (1.8728)
+2022-11-18 15:20:20,046:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9209 (1.9209)
+2022-11-18 15:20:20,090:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8970 (1.9146)
+2022-11-18 15:20:20,399:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9306 (1.9306)
+2022-11-18 15:20:20,473:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9447 (1.9374)
+2022-11-18 15:20:20,546:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9216 (1.9324)
+2022-11-18 15:20:20,620:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8686 (1.9158)
+2022-11-18 15:20:20,693:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8951 (1.9120)
+2022-11-18 15:20:20,746:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_384.pth.tar
+2022-11-18 15:20:20,746:INFO: 
+===> EPOCH: 385 (P2)
+2022-11-18 15:20:20,747:INFO: - Computing loss (training)
+2022-11-18 15:20:21,091:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0095 (2.0095)
+2022-11-18 15:20:21,243:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9938 (2.0020)
+2022-11-18 15:20:21,396:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9609 (1.9882)
+2022-11-18 15:20:21,514:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8547 (1.9664)
+2022-11-18 15:20:21,918:INFO: Dataset: univ                Batch:  1/15	Loss 1.8795 (1.8795)
+2022-11-18 15:20:22,076:INFO: Dataset: univ                Batch:  2/15	Loss 1.9202 (1.9006)
+2022-11-18 15:20:22,237:INFO: Dataset: univ                Batch:  3/15	Loss 1.8604 (1.8881)
+2022-11-18 15:20:22,394:INFO: Dataset: univ                Batch:  4/15	Loss 1.9017 (1.8915)
+2022-11-18 15:20:22,553:INFO: Dataset: univ                Batch:  5/15	Loss 1.8654 (1.8860)
+2022-11-18 15:20:22,709:INFO: Dataset: univ                Batch:  6/15	Loss 1.9135 (1.8909)
+2022-11-18 15:20:22,868:INFO: Dataset: univ                Batch:  7/15	Loss 1.9455 (1.8991)
+2022-11-18 15:20:23,022:INFO: Dataset: univ                Batch:  8/15	Loss 1.8798 (1.8967)
+2022-11-18 15:20:23,178:INFO: Dataset: univ                Batch:  9/15	Loss 1.9076 (1.8978)
+2022-11-18 15:20:23,334:INFO: Dataset: univ                Batch: 10/15	Loss 1.8610 (1.8943)
+2022-11-18 15:20:23,493:INFO: Dataset: univ                Batch: 11/15	Loss 1.8946 (1.8943)
+2022-11-18 15:20:23,646:INFO: Dataset: univ                Batch: 12/15	Loss 1.8726 (1.8925)
+2022-11-18 15:20:23,807:INFO: Dataset: univ                Batch: 13/15	Loss 1.8376 (1.8884)
+2022-11-18 15:20:23,964:INFO: Dataset: univ                Batch: 14/15	Loss 1.8652 (1.8866)
+2022-11-18 15:20:24,047:INFO: Dataset: univ                Batch: 15/15	Loss 1.9011 (1.8867)
+2022-11-18 15:20:24,437:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9695 (1.9695)
+2022-11-18 15:20:24,588:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9129 (1.9414)
+2022-11-18 15:20:24,739:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9362 (1.9396)
+2022-11-18 15:20:24,888:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9594 (1.9451)
+2022-11-18 15:20:25,041:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0455 (1.9667)
+2022-11-18 15:20:25,193:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9484 (1.9639)
+2022-11-18 15:20:25,344:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9604 (1.9635)
+2022-11-18 15:20:25,482:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9708 (1.9643)
+2022-11-18 15:20:25,894:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9648 (1.9648)
+2022-11-18 15:20:26,047:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9046 (1.9354)
+2022-11-18 15:20:26,205:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9046 (1.9248)
+2022-11-18 15:20:26,359:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9330 (1.9269)
+2022-11-18 15:20:26,513:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9396 (1.9294)
+2022-11-18 15:20:26,670:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7887 (1.9066)
+2022-11-18 15:20:26,824:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8519 (1.8977)
+2022-11-18 15:20:26,975:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9495 (1.9045)
+2022-11-18 15:20:27,128:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0026 (1.9147)
+2022-11-18 15:20:27,280:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8323 (1.9068)
+2022-11-18 15:20:27,436:INFO: Dataset: zara2               Batch: 11/18	Loss 2.0077 (1.9155)
+2022-11-18 15:20:27,587:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0080 (1.9231)
+2022-11-18 15:20:27,740:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8287 (1.9161)
+2022-11-18 15:20:27,894:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9254 (1.9168)
+2022-11-18 15:20:28,048:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9023 (1.9159)
+2022-11-18 15:20:28,200:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7894 (1.9078)
+2022-11-18 15:20:28,354:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8840 (1.9064)
+2022-11-18 15:20:28,496:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8389 (1.9031)
+2022-11-18 15:20:28,540:INFO: - Computing loss (validation)
+2022-11-18 15:20:28,803:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9065 (1.9065)
+2022-11-18 15:20:28,838:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8650 (1.9025)
+2022-11-18 15:20:29,158:INFO: Dataset: univ                Batch: 1/3	Loss 1.9043 (1.9043)
+2022-11-18 15:20:29,241:INFO: Dataset: univ                Batch: 2/3	Loss 1.8748 (1.8883)
+2022-11-18 15:20:29,317:INFO: Dataset: univ                Batch: 3/3	Loss 1.9013 (1.8921)
+2022-11-18 15:20:29,626:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8730 (1.8730)
+2022-11-18 15:20:29,672:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8674 (1.8716)
+2022-11-18 15:20:29,973:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8508 (1.8508)
+2022-11-18 15:20:30,050:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8897 (1.8708)
+2022-11-18 15:20:30,125:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8785 (1.8734)
+2022-11-18 15:20:30,201:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9425 (1.8894)
+2022-11-18 15:20:30,276:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9370 (1.8992)
+2022-11-18 15:20:30,333:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_385.pth.tar
+2022-11-18 15:20:30,333:INFO: 
+===> EPOCH: 386 (P2)
+2022-11-18 15:20:30,333:INFO: - Computing loss (training)
+2022-11-18 15:20:30,688:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9227 (1.9227)
+2022-11-18 15:20:30,840:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8317 (1.8770)
+2022-11-18 15:20:30,989:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0626 (1.9408)
+2022-11-18 15:20:31,106:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9430 (1.9412)
+2022-11-18 15:20:31,516:INFO: Dataset: univ                Batch:  1/15	Loss 1.8861 (1.8861)
+2022-11-18 15:20:31,680:INFO: Dataset: univ                Batch:  2/15	Loss 1.8725 (1.8796)
+2022-11-18 15:20:31,837:INFO: Dataset: univ                Batch:  3/15	Loss 1.8441 (1.8679)
+2022-11-18 15:20:31,993:INFO: Dataset: univ                Batch:  4/15	Loss 1.9112 (1.8785)
+2022-11-18 15:20:32,149:INFO: Dataset: univ                Batch:  5/15	Loss 1.8664 (1.8763)
+2022-11-18 15:20:32,308:INFO: Dataset: univ                Batch:  6/15	Loss 1.8616 (1.8737)
+2022-11-18 15:20:32,465:INFO: Dataset: univ                Batch:  7/15	Loss 1.8777 (1.8743)
+2022-11-18 15:20:32,619:INFO: Dataset: univ                Batch:  8/15	Loss 1.8874 (1.8760)
+2022-11-18 15:20:32,777:INFO: Dataset: univ                Batch:  9/15	Loss 1.9162 (1.8806)
+2022-11-18 15:20:32,932:INFO: Dataset: univ                Batch: 10/15	Loss 1.8854 (1.8811)
+2022-11-18 15:20:33,089:INFO: Dataset: univ                Batch: 11/15	Loss 1.8815 (1.8811)
+2022-11-18 15:20:33,247:INFO: Dataset: univ                Batch: 12/15	Loss 1.8812 (1.8812)
+2022-11-18 15:20:33,403:INFO: Dataset: univ                Batch: 13/15	Loss 1.9124 (1.8835)
+2022-11-18 15:20:33,560:INFO: Dataset: univ                Batch: 14/15	Loss 1.8722 (1.8827)
+2022-11-18 15:20:33,643:INFO: Dataset: univ                Batch: 15/15	Loss 1.9021 (1.8830)
+2022-11-18 15:20:34,021:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8846 (1.8846)
+2022-11-18 15:20:34,174:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9995 (1.9491)
+2022-11-18 15:20:34,332:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9287 (1.9421)
+2022-11-18 15:20:34,485:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9373 (1.9409)
+2022-11-18 15:20:34,641:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9632 (1.9451)
+2022-11-18 15:20:34,795:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8686 (1.9328)
+2022-11-18 15:20:34,950:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8887 (1.9272)
+2022-11-18 15:20:35,089:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8869 (1.9225)
+2022-11-18 15:20:35,480:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9629 (1.9629)
+2022-11-18 15:20:35,632:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9028 (1.9346)
+2022-11-18 15:20:35,786:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8240 (1.8947)
+2022-11-18 15:20:35,937:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9281 (1.9034)
+2022-11-18 15:20:36,088:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8939 (1.9014)
+2022-11-18 15:20:36,244:INFO: Dataset: zara2               Batch:  6/18	Loss 2.0170 (1.9215)
+2022-11-18 15:20:36,402:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9725 (1.9293)
+2022-11-18 15:20:36,558:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9040 (1.9262)
+2022-11-18 15:20:36,713:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9035 (1.9236)
+2022-11-18 15:20:36,864:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8419 (1.9150)
+2022-11-18 15:20:37,017:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9365 (1.9169)
+2022-11-18 15:20:37,168:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8393 (1.9108)
+2022-11-18 15:20:37,319:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9117 (1.9109)
+2022-11-18 15:20:37,471:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9532 (1.9137)
+2022-11-18 15:20:37,623:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9359 (1.9153)
+2022-11-18 15:20:37,773:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9674 (1.9185)
+2022-11-18 15:20:37,927:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8189 (1.9127)
+2022-11-18 15:20:38,065:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8690 (1.9107)
+2022-11-18 15:20:38,111:INFO: - Computing loss (validation)
+2022-11-18 15:20:38,369:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8480 (1.8480)
+2022-11-18 15:20:38,403:INFO: Dataset: hotel               Batch: 2/2	Loss 2.4850 (1.8828)
+2022-11-18 15:20:38,712:INFO: Dataset: univ                Batch: 1/3	Loss 1.8612 (1.8612)
+2022-11-18 15:20:38,788:INFO: Dataset: univ                Batch: 2/3	Loss 1.9021 (1.8815)
+2022-11-18 15:20:38,862:INFO: Dataset: univ                Batch: 3/3	Loss 1.8463 (1.8693)
+2022-11-18 15:20:39,169:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8526 (1.8526)
+2022-11-18 15:20:39,216:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9794 (1.8828)
+2022-11-18 15:20:39,521:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8260 (1.8260)
+2022-11-18 15:20:39,598:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9977 (1.9106)
+2022-11-18 15:20:39,672:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8662 (1.8961)
+2022-11-18 15:20:39,747:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9480 (1.9093)
+2022-11-18 15:20:39,821:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8877 (1.9048)
+2022-11-18 15:20:39,874:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_386.pth.tar
+2022-11-18 15:20:39,874:INFO: 
+===> EPOCH: 387 (P2)
+2022-11-18 15:20:39,875:INFO: - Computing loss (training)
+2022-11-18 15:20:40,209:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9351 (1.9351)
+2022-11-18 15:20:40,363:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0093 (1.9716)
+2022-11-18 15:20:40,513:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9423 (1.9617)
+2022-11-18 15:20:40,637:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0850 (1.9840)
+2022-11-18 15:20:41,031:INFO: Dataset: univ                Batch:  1/15	Loss 1.8510 (1.8510)
+2022-11-18 15:20:41,189:INFO: Dataset: univ                Batch:  2/15	Loss 1.8550 (1.8529)
+2022-11-18 15:20:41,346:INFO: Dataset: univ                Batch:  3/15	Loss 1.9001 (1.8682)
+2022-11-18 15:20:41,501:INFO: Dataset: univ                Batch:  4/15	Loss 1.8898 (1.8734)
+2022-11-18 15:20:41,733:INFO: Dataset: univ                Batch:  5/15	Loss 1.8832 (1.8755)
+2022-11-18 15:20:41,892:INFO: Dataset: univ                Batch:  6/15	Loss 1.9043 (1.8805)
+2022-11-18 15:20:42,045:INFO: Dataset: univ                Batch:  7/15	Loss 1.8754 (1.8798)
+2022-11-18 15:20:42,199:INFO: Dataset: univ                Batch:  8/15	Loss 1.9032 (1.8831)
+2022-11-18 15:20:42,353:INFO: Dataset: univ                Batch:  9/15	Loss 1.9018 (1.8853)
+2022-11-18 15:20:42,506:INFO: Dataset: univ                Batch: 10/15	Loss 1.8627 (1.8829)
+2022-11-18 15:20:42,660:INFO: Dataset: univ                Batch: 11/15	Loss 1.9069 (1.8851)
+2022-11-18 15:20:42,815:INFO: Dataset: univ                Batch: 12/15	Loss 1.9063 (1.8868)
+2022-11-18 15:20:42,970:INFO: Dataset: univ                Batch: 13/15	Loss 1.9181 (1.8895)
+2022-11-18 15:20:43,124:INFO: Dataset: univ                Batch: 14/15	Loss 1.8605 (1.8875)
+2022-11-18 15:20:43,206:INFO: Dataset: univ                Batch: 15/15	Loss 1.9074 (1.8878)
+2022-11-18 15:20:43,601:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8468 (1.8468)
+2022-11-18 15:20:43,753:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8660 (1.8559)
+2022-11-18 15:20:43,905:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8464 (1.8527)
+2022-11-18 15:20:44,055:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9378 (1.8751)
+2022-11-18 15:20:44,207:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9229 (1.8847)
+2022-11-18 15:20:44,357:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8730 (1.8829)
+2022-11-18 15:20:44,507:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8866 (1.8834)
+2022-11-18 15:20:44,642:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9824 (1.8936)
+2022-11-18 15:20:45,040:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9321 (1.9321)
+2022-11-18 15:20:45,199:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8035 (1.8648)
+2022-11-18 15:20:45,351:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9355 (1.8883)
+2022-11-18 15:20:45,501:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8576 (1.8816)
+2022-11-18 15:20:45,652:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9598 (1.8978)
+2022-11-18 15:20:45,806:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9225 (1.9021)
+2022-11-18 15:20:45,957:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8654 (1.8966)
+2022-11-18 15:20:46,106:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8734 (1.8938)
+2022-11-18 15:20:46,257:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8963 (1.8941)
+2022-11-18 15:20:46,407:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8382 (1.8885)
+2022-11-18 15:20:46,558:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9041 (1.8900)
+2022-11-18 15:20:46,708:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8890 (1.8899)
+2022-11-18 15:20:46,859:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8431 (1.8861)
+2022-11-18 15:20:47,010:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9109 (1.8878)
+2022-11-18 15:20:47,163:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9313 (1.8905)
+2022-11-18 15:20:47,314:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9298 (1.8929)
+2022-11-18 15:20:47,466:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8848 (1.8924)
+2022-11-18 15:20:47,605:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8980 (1.8927)
+2022-11-18 15:20:47,651:INFO: - Computing loss (validation)
+2022-11-18 15:20:47,920:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8705 (1.8705)
+2022-11-18 15:20:47,956:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0622 (1.8849)
+2022-11-18 15:20:48,268:INFO: Dataset: univ                Batch: 1/3	Loss 1.8456 (1.8456)
+2022-11-18 15:20:48,346:INFO: Dataset: univ                Batch: 2/3	Loss 1.8929 (1.8697)
+2022-11-18 15:20:48,420:INFO: Dataset: univ                Batch: 3/3	Loss 1.8824 (1.8738)
+2022-11-18 15:20:48,722:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8238 (1.8238)
+2022-11-18 15:20:48,769:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8598 (1.8328)
+2022-11-18 15:20:49,087:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8996 (1.8996)
+2022-11-18 15:20:49,169:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8252 (1.8641)
+2022-11-18 15:20:49,248:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8636 (1.8639)
+2022-11-18 15:20:49,326:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8990 (1.8728)
+2022-11-18 15:20:49,405:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8433 (1.8665)
+2022-11-18 15:20:49,460:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_387.pth.tar
+2022-11-18 15:20:49,460:INFO: 
+===> EPOCH: 388 (P2)
+2022-11-18 15:20:49,461:INFO: - Computing loss (training)
+2022-11-18 15:20:49,801:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9315 (1.9315)
+2022-11-18 15:20:49,954:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9336 (1.9325)
+2022-11-18 15:20:50,104:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9627 (1.9423)
+2022-11-18 15:20:50,221:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9593 (1.9452)
+2022-11-18 15:20:50,633:INFO: Dataset: univ                Batch:  1/15	Loss 1.8525 (1.8525)
+2022-11-18 15:20:50,790:INFO: Dataset: univ                Batch:  2/15	Loss 1.8637 (1.8585)
+2022-11-18 15:20:50,946:INFO: Dataset: univ                Batch:  3/15	Loss 1.8584 (1.8585)
+2022-11-18 15:20:51,098:INFO: Dataset: univ                Batch:  4/15	Loss 1.8715 (1.8621)
+2022-11-18 15:20:51,256:INFO: Dataset: univ                Batch:  5/15	Loss 1.8612 (1.8619)
+2022-11-18 15:20:51,411:INFO: Dataset: univ                Batch:  6/15	Loss 1.9016 (1.8683)
+2022-11-18 15:20:51,567:INFO: Dataset: univ                Batch:  7/15	Loss 1.9105 (1.8746)
+2022-11-18 15:20:51,724:INFO: Dataset: univ                Batch:  8/15	Loss 1.9052 (1.8790)
+2022-11-18 15:20:51,879:INFO: Dataset: univ                Batch:  9/15	Loss 1.9229 (1.8836)
+2022-11-18 15:20:52,031:INFO: Dataset: univ                Batch: 10/15	Loss 1.9233 (1.8876)
+2022-11-18 15:20:52,188:INFO: Dataset: univ                Batch: 11/15	Loss 1.9219 (1.8909)
+2022-11-18 15:20:52,343:INFO: Dataset: univ                Batch: 12/15	Loss 1.8919 (1.8910)
+2022-11-18 15:20:52,499:INFO: Dataset: univ                Batch: 13/15	Loss 1.9291 (1.8944)
+2022-11-18 15:20:52,652:INFO: Dataset: univ                Batch: 14/15	Loss 1.8357 (1.8907)
+2022-11-18 15:20:52,734:INFO: Dataset: univ                Batch: 15/15	Loss 1.8956 (1.8907)
+2022-11-18 15:20:53,123:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0656 (2.0656)
+2022-11-18 15:20:53,269:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8580 (1.9554)
+2022-11-18 15:20:53,417:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8473 (1.9199)
+2022-11-18 15:20:53,564:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8843 (1.9107)
+2022-11-18 15:20:53,713:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9013 (1.9087)
+2022-11-18 15:20:53,860:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0535 (1.9324)
+2022-11-18 15:20:54,008:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9097 (1.9289)
+2022-11-18 15:20:54,141:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9030 (1.9261)
+2022-11-18 15:20:54,525:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8110 (1.8110)
+2022-11-18 15:20:54,680:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8768 (1.8409)
+2022-11-18 15:20:54,830:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8975 (1.8597)
+2022-11-18 15:20:54,978:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7642 (1.8360)
+2022-11-18 15:20:55,127:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9366 (1.8548)
+2022-11-18 15:20:55,279:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9160 (1.8652)
+2022-11-18 15:20:55,429:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8612 (1.8646)
+2022-11-18 15:20:55,576:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9292 (1.8726)
+2022-11-18 15:20:55,725:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7834 (1.8629)
+2022-11-18 15:20:55,873:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8404 (1.8608)
+2022-11-18 15:20:56,021:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8418 (1.8590)
+2022-11-18 15:20:56,170:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8677 (1.8598)
+2022-11-18 15:20:56,320:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9562 (1.8661)
+2022-11-18 15:20:56,472:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9031 (1.8685)
+2022-11-18 15:20:56,623:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9021 (1.8707)
+2022-11-18 15:20:56,771:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8487 (1.8693)
+2022-11-18 15:20:56,922:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8175 (1.8664)
+2022-11-18 15:20:57,059:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8675 (1.8664)
+2022-11-18 15:20:57,103:INFO: - Computing loss (validation)
+2022-11-18 15:20:57,379:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9631 (1.9631)
+2022-11-18 15:20:57,414:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2170 (1.9769)
+2022-11-18 15:20:57,730:INFO: Dataset: univ                Batch: 1/3	Loss 1.9491 (1.9491)
+2022-11-18 15:20:57,806:INFO: Dataset: univ                Batch: 2/3	Loss 1.8200 (1.8891)
+2022-11-18 15:20:57,879:INFO: Dataset: univ                Batch: 3/3	Loss 1.8969 (1.8915)
+2022-11-18 15:20:58,176:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9603 (1.9603)
+2022-11-18 15:20:58,220:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8712 (1.9383)
+2022-11-18 15:20:58,524:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9628 (1.9628)
+2022-11-18 15:20:58,597:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8169 (1.8923)
+2022-11-18 15:20:58,671:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8854 (1.8900)
+2022-11-18 15:20:58,746:INFO: Dataset: zara2               Batch: 4/5	Loss 2.0030 (1.9167)
+2022-11-18 15:20:58,819:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9132 (1.9160)
+2022-11-18 15:20:58,875:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_388.pth.tar
+2022-11-18 15:20:58,875:INFO: 
+===> EPOCH: 389 (P2)
+2022-11-18 15:20:58,875:INFO: - Computing loss (training)
+2022-11-18 15:20:59,216:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9200 (1.9200)
+2022-11-18 15:20:59,366:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9534 (1.9375)
+2022-11-18 15:20:59,515:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9036 (1.9262)
+2022-11-18 15:20:59,630:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8475 (1.9135)
+2022-11-18 15:21:00,026:INFO: Dataset: univ                Batch:  1/15	Loss 1.8420 (1.8420)
+2022-11-18 15:21:00,188:INFO: Dataset: univ                Batch:  2/15	Loss 1.8946 (1.8700)
+2022-11-18 15:21:00,351:INFO: Dataset: univ                Batch:  3/15	Loss 1.9037 (1.8814)
+2022-11-18 15:21:00,513:INFO: Dataset: univ                Batch:  4/15	Loss 1.9217 (1.8920)
+2022-11-18 15:21:00,675:INFO: Dataset: univ                Batch:  5/15	Loss 1.8928 (1.8922)
+2022-11-18 15:21:00,834:INFO: Dataset: univ                Batch:  6/15	Loss 1.8404 (1.8835)
+2022-11-18 15:21:01,000:INFO: Dataset: univ                Batch:  7/15	Loss 1.8678 (1.8814)
+2022-11-18 15:21:01,156:INFO: Dataset: univ                Batch:  8/15	Loss 1.8695 (1.8799)
+2022-11-18 15:21:01,315:INFO: Dataset: univ                Batch:  9/15	Loss 1.8824 (1.8802)
+2022-11-18 15:21:01,472:INFO: Dataset: univ                Batch: 10/15	Loss 1.8408 (1.8765)
+2022-11-18 15:21:01,631:INFO: Dataset: univ                Batch: 11/15	Loss 1.8720 (1.8761)
+2022-11-18 15:21:01,791:INFO: Dataset: univ                Batch: 12/15	Loss 1.9251 (1.8801)
+2022-11-18 15:21:01,949:INFO: Dataset: univ                Batch: 13/15	Loss 1.9168 (1.8829)
+2022-11-18 15:21:02,106:INFO: Dataset: univ                Batch: 14/15	Loss 1.9061 (1.8848)
+2022-11-18 15:21:02,192:INFO: Dataset: univ                Batch: 15/15	Loss 1.8662 (1.8845)
+2022-11-18 15:21:02,578:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9403 (1.9403)
+2022-11-18 15:21:02,728:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7848 (1.8575)
+2022-11-18 15:21:02,881:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8603 (1.8584)
+2022-11-18 15:21:03,034:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8545 (1.8575)
+2022-11-18 15:21:03,186:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8046 (1.8479)
+2022-11-18 15:21:03,337:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0018 (1.8745)
+2022-11-18 15:21:03,488:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9534 (1.8852)
+2022-11-18 15:21:03,625:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8003 (1.8755)
+2022-11-18 15:21:04,013:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8317 (1.8317)
+2022-11-18 15:21:04,162:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8831 (1.8563)
+2022-11-18 15:21:04,316:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8700 (1.8611)
+2022-11-18 15:21:04,464:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8798 (1.8654)
+2022-11-18 15:21:04,614:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8490 (1.8621)
+2022-11-18 15:21:04,764:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8506 (1.8605)
+2022-11-18 15:21:04,913:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8401 (1.8577)
+2022-11-18 15:21:05,059:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9321 (1.8665)
+2022-11-18 15:21:05,207:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7809 (1.8574)
+2022-11-18 15:21:05,354:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7931 (1.8504)
+2022-11-18 15:21:05,502:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8346 (1.8489)
+2022-11-18 15:21:05,649:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9891 (1.8602)
+2022-11-18 15:21:05,797:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9531 (1.8673)
+2022-11-18 15:21:05,946:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9038 (1.8700)
+2022-11-18 15:21:06,096:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9682 (1.8766)
+2022-11-18 15:21:06,244:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8853 (1.8772)
+2022-11-18 15:21:06,393:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8764 (1.8772)
+2022-11-18 15:21:06,529:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9745 (1.8815)
+2022-11-18 15:21:06,575:INFO: - Computing loss (validation)
+2022-11-18 15:21:06,835:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8729 (1.8729)
+2022-11-18 15:21:06,870:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1464 (1.8972)
+2022-11-18 15:21:07,182:INFO: Dataset: univ                Batch: 1/3	Loss 1.8773 (1.8773)
+2022-11-18 15:21:07,262:INFO: Dataset: univ                Batch: 2/3	Loss 1.9142 (1.8953)
+2022-11-18 15:21:07,340:INFO: Dataset: univ                Batch: 3/3	Loss 1.8913 (1.8938)
+2022-11-18 15:21:07,639:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9161 (1.9161)
+2022-11-18 15:21:07,685:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9309 (1.9197)
+2022-11-18 15:21:08,008:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7830 (1.7830)
+2022-11-18 15:21:08,086:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9412 (1.8593)
+2022-11-18 15:21:08,164:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9554 (1.8922)
+2022-11-18 15:21:08,242:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9079 (1.8960)
+2022-11-18 15:21:08,318:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9359 (1.9043)
+2022-11-18 15:21:08,370:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_389.pth.tar
+2022-11-18 15:21:08,370:INFO: 
+===> EPOCH: 390 (P2)
+2022-11-18 15:21:08,370:INFO: - Computing loss (training)
+2022-11-18 15:21:08,711:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9358 (1.9358)
+2022-11-18 15:21:08,863:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8519 (1.8944)
+2022-11-18 15:21:09,011:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8645 (1.8839)
+2022-11-18 15:21:09,125:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9179 (1.8900)
+2022-11-18 15:21:09,510:INFO: Dataset: univ                Batch:  1/15	Loss 1.8756 (1.8756)
+2022-11-18 15:21:09,667:INFO: Dataset: univ                Batch:  2/15	Loss 1.8769 (1.8762)
+2022-11-18 15:21:09,826:INFO: Dataset: univ                Batch:  3/15	Loss 1.8907 (1.8812)
+2022-11-18 15:21:09,984:INFO: Dataset: univ                Batch:  4/15	Loss 1.8861 (1.8825)
+2022-11-18 15:21:10,141:INFO: Dataset: univ                Batch:  5/15	Loss 1.8687 (1.8799)
+2022-11-18 15:21:10,297:INFO: Dataset: univ                Batch:  6/15	Loss 1.9073 (1.8843)
+2022-11-18 15:21:10,453:INFO: Dataset: univ                Batch:  7/15	Loss 1.8952 (1.8859)
+2022-11-18 15:21:10,607:INFO: Dataset: univ                Batch:  8/15	Loss 1.8673 (1.8834)
+2022-11-18 15:21:10,762:INFO: Dataset: univ                Batch:  9/15	Loss 1.8866 (1.8838)
+2022-11-18 15:21:10,916:INFO: Dataset: univ                Batch: 10/15	Loss 1.8946 (1.8848)
+2022-11-18 15:21:11,075:INFO: Dataset: univ                Batch: 11/15	Loss 1.8866 (1.8850)
+2022-11-18 15:21:11,230:INFO: Dataset: univ                Batch: 12/15	Loss 1.9007 (1.8863)
+2022-11-18 15:21:11,386:INFO: Dataset: univ                Batch: 13/15	Loss 1.8526 (1.8837)
+2022-11-18 15:21:11,541:INFO: Dataset: univ                Batch: 14/15	Loss 1.9013 (1.8849)
+2022-11-18 15:21:11,624:INFO: Dataset: univ                Batch: 15/15	Loss 1.9633 (1.8857)
+2022-11-18 15:21:12,006:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9895 (1.9895)
+2022-11-18 15:21:12,156:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9894 (1.9895)
+2022-11-18 15:21:12,305:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8790 (1.9520)
+2022-11-18 15:21:12,454:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8855 (1.9358)
+2022-11-18 15:21:12,605:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0219 (1.9517)
+2022-11-18 15:21:12,753:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8904 (1.9422)
+2022-11-18 15:21:12,902:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9401 (1.9419)
+2022-11-18 15:21:13,037:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8222 (1.9280)
+2022-11-18 15:21:13,441:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8145 (1.8145)
+2022-11-18 15:21:13,591:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9165 (1.8652)
+2022-11-18 15:21:13,744:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7203 (1.8165)
+2022-11-18 15:21:13,898:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8894 (1.8334)
+2022-11-18 15:21:14,049:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9077 (1.8479)
+2022-11-18 15:21:14,203:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9462 (1.8625)
+2022-11-18 15:21:14,354:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8603 (1.8622)
+2022-11-18 15:21:14,503:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0115 (1.8792)
+2022-11-18 15:21:14,654:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8548 (1.8765)
+2022-11-18 15:21:14,802:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9546 (1.8850)
+2022-11-18 15:21:14,955:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8436 (1.8812)
+2022-11-18 15:21:15,103:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8477 (1.8783)
+2022-11-18 15:21:15,255:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8432 (1.8756)
+2022-11-18 15:21:15,406:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8831 (1.8761)
+2022-11-18 15:21:15,558:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9100 (1.8784)
+2022-11-18 15:21:15,709:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8637 (1.8775)
+2022-11-18 15:21:15,862:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9301 (1.8804)
+2022-11-18 15:21:16,000:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9130 (1.8819)
+2022-11-18 15:21:16,047:INFO: - Computing loss (validation)
+2022-11-18 15:21:16,324:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9179 (1.9179)
+2022-11-18 15:21:16,361:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2008 (1.9411)
+2022-11-18 15:21:16,662:INFO: Dataset: univ                Batch: 1/3	Loss 1.9035 (1.9035)
+2022-11-18 15:21:16,739:INFO: Dataset: univ                Batch: 2/3	Loss 1.9128 (1.9081)
+2022-11-18 15:21:16,811:INFO: Dataset: univ                Batch: 3/3	Loss 1.9113 (1.9090)
+2022-11-18 15:21:17,103:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9418 (1.9418)
+2022-11-18 15:21:17,149:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9145 (1.9357)
+2022-11-18 15:21:17,462:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8996 (1.8996)
+2022-11-18 15:21:17,541:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9070 (1.9034)
+2022-11-18 15:21:17,619:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9476 (1.9186)
+2022-11-18 15:21:17,698:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8242 (1.8935)
+2022-11-18 15:21:17,776:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8363 (1.8819)
+2022-11-18 15:21:17,829:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_390.pth.tar
+2022-11-18 15:21:17,829:INFO: 
+===> EPOCH: 391 (P2)
+2022-11-18 15:21:17,830:INFO: - Computing loss (training)
+2022-11-18 15:21:18,170:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0416 (2.0416)
+2022-11-18 15:21:18,319:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9255 (1.9825)
+2022-11-18 15:21:18,469:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9999 (1.9883)
+2022-11-18 15:21:18,582:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9668 (1.9847)
+2022-11-18 15:21:18,995:INFO: Dataset: univ                Batch:  1/15	Loss 1.8752 (1.8752)
+2022-11-18 15:21:19,159:INFO: Dataset: univ                Batch:  2/15	Loss 1.8646 (1.8699)
+2022-11-18 15:21:19,318:INFO: Dataset: univ                Batch:  3/15	Loss 1.9089 (1.8832)
+2022-11-18 15:21:19,474:INFO: Dataset: univ                Batch:  4/15	Loss 1.8963 (1.8862)
+2022-11-18 15:21:19,630:INFO: Dataset: univ                Batch:  5/15	Loss 1.8547 (1.8800)
+2022-11-18 15:21:19,789:INFO: Dataset: univ                Batch:  6/15	Loss 1.9051 (1.8842)
+2022-11-18 15:21:19,947:INFO: Dataset: univ                Batch:  7/15	Loss 1.8693 (1.8822)
+2022-11-18 15:21:20,102:INFO: Dataset: univ                Batch:  8/15	Loss 1.8585 (1.8792)
+2022-11-18 15:21:20,259:INFO: Dataset: univ                Batch:  9/15	Loss 1.9135 (1.8828)
+2022-11-18 15:21:20,414:INFO: Dataset: univ                Batch: 10/15	Loss 1.8784 (1.8824)
+2022-11-18 15:21:20,572:INFO: Dataset: univ                Batch: 11/15	Loss 1.8655 (1.8810)
+2022-11-18 15:21:20,729:INFO: Dataset: univ                Batch: 12/15	Loss 1.8852 (1.8813)
+2022-11-18 15:21:20,886:INFO: Dataset: univ                Batch: 13/15	Loss 1.8473 (1.8787)
+2022-11-18 15:21:21,043:INFO: Dataset: univ                Batch: 14/15	Loss 1.8571 (1.8772)
+2022-11-18 15:21:21,127:INFO: Dataset: univ                Batch: 15/15	Loss 1.9211 (1.8778)
+2022-11-18 15:21:21,512:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8886 (1.8886)
+2022-11-18 15:21:21,660:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0357 (1.9590)
+2022-11-18 15:21:21,816:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0204 (1.9784)
+2022-11-18 15:21:21,965:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8379 (1.9432)
+2022-11-18 15:21:22,114:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9795 (1.9508)
+2022-11-18 15:21:22,265:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8973 (1.9421)
+2022-11-18 15:21:22,415:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7718 (1.9178)
+2022-11-18 15:21:22,552:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0206 (1.9287)
+2022-11-18 15:21:22,932:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8711 (1.8711)
+2022-11-18 15:21:23,083:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8687 (1.8699)
+2022-11-18 15:21:23,234:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9889 (1.9087)
+2022-11-18 15:21:23,385:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9050 (1.9078)
+2022-11-18 15:21:23,537:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9738 (1.9196)
+2022-11-18 15:21:23,689:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9212 (1.9199)
+2022-11-18 15:21:23,839:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9123 (1.9189)
+2022-11-18 15:21:23,988:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8269 (1.9072)
+2022-11-18 15:21:24,138:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8589 (1.9017)
+2022-11-18 15:21:24,285:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9312 (1.9048)
+2022-11-18 15:21:24,435:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8946 (1.9037)
+2022-11-18 15:21:24,584:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9125 (1.9045)
+2022-11-18 15:21:24,733:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8725 (1.9019)
+2022-11-18 15:21:24,881:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9684 (1.9065)
+2022-11-18 15:21:25,031:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9788 (1.9112)
+2022-11-18 15:21:25,179:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8234 (1.9057)
+2022-11-18 15:21:25,330:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8223 (1.9007)
+2022-11-18 15:21:25,466:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9524 (1.9035)
+2022-11-18 15:21:25,516:INFO: - Computing loss (validation)
+2022-11-18 15:21:25,777:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8796 (1.8796)
+2022-11-18 15:21:25,812:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1100 (1.9016)
+2022-11-18 15:21:26,115:INFO: Dataset: univ                Batch: 1/3	Loss 1.8485 (1.8485)
+2022-11-18 15:21:26,193:INFO: Dataset: univ                Batch: 2/3	Loss 1.8705 (1.8598)
+2022-11-18 15:21:26,266:INFO: Dataset: univ                Batch: 3/3	Loss 1.8040 (1.8440)
+2022-11-18 15:21:26,565:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9320 (1.9320)
+2022-11-18 15:21:26,611:INFO: Dataset: zara1               Batch: 2/2	Loss 2.2028 (2.0008)
+2022-11-18 15:21:26,914:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9908 (1.9908)
+2022-11-18 15:21:26,987:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8483 (1.9239)
+2022-11-18 15:21:27,061:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8420 (1.8943)
+2022-11-18 15:21:27,135:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8944 (1.8943)
+2022-11-18 15:21:27,209:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9436 (1.9039)
+2022-11-18 15:21:27,261:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_391.pth.tar
+2022-11-18 15:21:27,261:INFO: 
+===> EPOCH: 392 (P2)
+2022-11-18 15:21:27,262:INFO: - Computing loss (training)
+2022-11-18 15:21:27,595:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7957 (1.7957)
+2022-11-18 15:21:27,746:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8367 (1.8154)
+2022-11-18 15:21:27,899:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8359 (1.8219)
+2022-11-18 15:21:28,017:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8632 (1.8291)
+2022-11-18 15:21:28,433:INFO: Dataset: univ                Batch:  1/15	Loss 1.8619 (1.8619)
+2022-11-18 15:21:28,591:INFO: Dataset: univ                Batch:  2/15	Loss 1.9079 (1.8841)
+2022-11-18 15:21:28,752:INFO: Dataset: univ                Batch:  3/15	Loss 1.9594 (1.9074)
+2022-11-18 15:21:28,914:INFO: Dataset: univ                Batch:  4/15	Loss 1.8765 (1.8991)
+2022-11-18 15:21:29,074:INFO: Dataset: univ                Batch:  5/15	Loss 1.8909 (1.8976)
+2022-11-18 15:21:29,233:INFO: Dataset: univ                Batch:  6/15	Loss 1.8724 (1.8935)
+2022-11-18 15:21:29,392:INFO: Dataset: univ                Batch:  7/15	Loss 1.9138 (1.8965)
+2022-11-18 15:21:29,550:INFO: Dataset: univ                Batch:  8/15	Loss 1.8845 (1.8951)
+2022-11-18 15:21:29,707:INFO: Dataset: univ                Batch:  9/15	Loss 1.9361 (1.8997)
+2022-11-18 15:21:29,864:INFO: Dataset: univ                Batch: 10/15	Loss 1.8585 (1.8952)
+2022-11-18 15:21:30,022:INFO: Dataset: univ                Batch: 11/15	Loss 1.8933 (1.8951)
+2022-11-18 15:21:30,180:INFO: Dataset: univ                Batch: 12/15	Loss 1.8268 (1.8893)
+2022-11-18 15:21:30,338:INFO: Dataset: univ                Batch: 13/15	Loss 1.9017 (1.8902)
+2022-11-18 15:21:30,496:INFO: Dataset: univ                Batch: 14/15	Loss 1.8548 (1.8880)
+2022-11-18 15:21:30,580:INFO: Dataset: univ                Batch: 15/15	Loss 1.9172 (1.8884)
+2022-11-18 15:21:30,966:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8674 (1.8674)
+2022-11-18 15:21:31,117:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7743 (1.8190)
+2022-11-18 15:21:31,270:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8938 (1.8424)
+2022-11-18 15:21:31,425:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8235 (1.8376)
+2022-11-18 15:21:31,577:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8806 (1.8469)
+2022-11-18 15:21:31,727:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0255 (1.8757)
+2022-11-18 15:21:31,879:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9391 (1.8838)
+2022-11-18 15:21:32,015:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9472 (1.8908)
+2022-11-18 15:21:32,429:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9241 (1.9241)
+2022-11-18 15:21:32,576:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9190 (1.9215)
+2022-11-18 15:21:32,728:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7757 (1.8707)
+2022-11-18 15:21:32,876:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9232 (1.8835)
+2022-11-18 15:21:33,025:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8931 (1.8853)
+2022-11-18 15:21:33,175:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8210 (1.8738)
+2022-11-18 15:21:33,323:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9164 (1.8800)
+2022-11-18 15:21:33,470:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8340 (1.8743)
+2022-11-18 15:21:33,617:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8812 (1.8750)
+2022-11-18 15:21:33,762:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9151 (1.8791)
+2022-11-18 15:21:33,909:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9671 (1.8869)
+2022-11-18 15:21:34,056:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9745 (1.8940)
+2022-11-18 15:21:34,204:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8711 (1.8924)
+2022-11-18 15:21:34,352:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8997 (1.8930)
+2022-11-18 15:21:34,501:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8323 (1.8891)
+2022-11-18 15:21:34,648:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8215 (1.8848)
+2022-11-18 15:21:34,799:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9656 (1.8898)
+2022-11-18 15:21:34,936:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8745 (1.8890)
+2022-11-18 15:21:34,981:INFO: - Computing loss (validation)
+2022-11-18 15:21:35,246:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9647 (1.9647)
+2022-11-18 15:21:35,281:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8460 (1.9554)
+2022-11-18 15:21:35,606:INFO: Dataset: univ                Batch: 1/3	Loss 1.9694 (1.9694)
+2022-11-18 15:21:35,689:INFO: Dataset: univ                Batch: 2/3	Loss 1.8795 (1.9260)
+2022-11-18 15:21:35,767:INFO: Dataset: univ                Batch: 3/3	Loss 1.9351 (1.9290)
+2022-11-18 15:21:36,072:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8272 (1.8272)
+2022-11-18 15:21:36,118:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8902 (1.8426)
+2022-11-18 15:21:36,423:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8876 (1.8876)
+2022-11-18 15:21:36,501:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9238 (1.9045)
+2022-11-18 15:21:36,580:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9245 (1.9109)
+2022-11-18 15:21:36,657:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9319 (1.9162)
+2022-11-18 15:21:36,737:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9824 (1.9300)
+2022-11-18 15:21:36,791:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_392.pth.tar
+2022-11-18 15:21:36,791:INFO: 
+===> EPOCH: 393 (P2)
+2022-11-18 15:21:36,791:INFO: - Computing loss (training)
+2022-11-18 15:21:37,131:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9720 (1.9720)
+2022-11-18 15:21:37,282:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8480 (1.9081)
+2022-11-18 15:21:37,432:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9472 (1.9210)
+2022-11-18 15:21:37,546:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9666 (1.9287)
+2022-11-18 15:21:37,940:INFO: Dataset: univ                Batch:  1/15	Loss 1.8525 (1.8525)
+2022-11-18 15:21:38,100:INFO: Dataset: univ                Batch:  2/15	Loss 1.8770 (1.8641)
+2022-11-18 15:21:38,262:INFO: Dataset: univ                Batch:  3/15	Loss 1.8887 (1.8732)
+2022-11-18 15:21:38,422:INFO: Dataset: univ                Batch:  4/15	Loss 1.8269 (1.8612)
+2022-11-18 15:21:38,581:INFO: Dataset: univ                Batch:  5/15	Loss 1.9306 (1.8747)
+2022-11-18 15:21:38,741:INFO: Dataset: univ                Batch:  6/15	Loss 1.9109 (1.8807)
+2022-11-18 15:21:38,903:INFO: Dataset: univ                Batch:  7/15	Loss 1.8385 (1.8752)
+2022-11-18 15:21:39,061:INFO: Dataset: univ                Batch:  8/15	Loss 1.8680 (1.8743)
+2022-11-18 15:21:39,221:INFO: Dataset: univ                Batch:  9/15	Loss 1.8512 (1.8714)
+2022-11-18 15:21:39,379:INFO: Dataset: univ                Batch: 10/15	Loss 1.8895 (1.8732)
+2022-11-18 15:21:39,539:INFO: Dataset: univ                Batch: 11/15	Loss 1.8254 (1.8683)
+2022-11-18 15:21:39,694:INFO: Dataset: univ                Batch: 12/15	Loss 1.8248 (1.8646)
+2022-11-18 15:21:39,853:INFO: Dataset: univ                Batch: 13/15	Loss 1.8502 (1.8635)
+2022-11-18 15:21:40,012:INFO: Dataset: univ                Batch: 14/15	Loss 1.9392 (1.8689)
+2022-11-18 15:21:40,097:INFO: Dataset: univ                Batch: 15/15	Loss 1.7904 (1.8678)
+2022-11-18 15:21:40,496:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9373 (1.9373)
+2022-11-18 15:21:40,645:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8344 (1.8886)
+2022-11-18 15:21:40,800:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9838 (1.9218)
+2022-11-18 15:21:40,963:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9090 (1.9183)
+2022-11-18 15:21:41,119:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7798 (1.8915)
+2022-11-18 15:21:41,273:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8500 (1.8842)
+2022-11-18 15:21:41,428:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9197 (1.8892)
+2022-11-18 15:21:41,565:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8993 (1.8902)
+2022-11-18 15:21:41,966:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8780 (1.8780)
+2022-11-18 15:21:42,114:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9116 (1.8940)
+2022-11-18 15:21:42,271:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9464 (1.9109)
+2022-11-18 15:21:42,422:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9275 (1.9148)
+2022-11-18 15:21:42,572:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8824 (1.9079)
+2022-11-18 15:21:42,724:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8980 (1.9061)
+2022-11-18 15:21:42,874:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9604 (1.9143)
+2022-11-18 15:21:43,023:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9142 (1.9143)
+2022-11-18 15:21:43,173:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8519 (1.9071)
+2022-11-18 15:21:43,321:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8411 (1.9008)
+2022-11-18 15:21:43,472:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8952 (1.9003)
+2022-11-18 15:21:43,622:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8653 (1.8975)
+2022-11-18 15:21:43,775:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9684 (1.9033)
+2022-11-18 15:21:43,925:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8476 (1.8994)
+2022-11-18 15:21:44,078:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8449 (1.8959)
+2022-11-18 15:21:44,228:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8141 (1.8911)
+2022-11-18 15:21:44,382:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8646 (1.8895)
+2022-11-18 15:21:44,520:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9110 (1.8905)
+2022-11-18 15:21:44,567:INFO: - Computing loss (validation)
+2022-11-18 15:21:44,840:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8880 (1.8880)
+2022-11-18 15:21:44,876:INFO: Dataset: hotel               Batch: 2/2	Loss 2.4838 (1.9307)
+2022-11-18 15:21:45,198:INFO: Dataset: univ                Batch: 1/3	Loss 1.8463 (1.8463)
+2022-11-18 15:21:45,278:INFO: Dataset: univ                Batch: 2/3	Loss 1.8275 (1.8368)
+2022-11-18 15:21:45,351:INFO: Dataset: univ                Batch: 3/3	Loss 1.8755 (1.8476)
+2022-11-18 15:21:45,667:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9861 (1.9861)
+2022-11-18 15:21:45,715:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9209 (1.9702)
+2022-11-18 15:21:46,016:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8946 (1.8946)
+2022-11-18 15:21:46,091:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9339 (1.9134)
+2022-11-18 15:21:46,167:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9280 (1.9181)
+2022-11-18 15:21:46,241:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9007 (1.9133)
+2022-11-18 15:21:46,315:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8795 (1.9063)
+2022-11-18 15:21:46,370:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_393.pth.tar
+2022-11-18 15:21:46,370:INFO: 
+===> EPOCH: 394 (P2)
+2022-11-18 15:21:46,371:INFO: - Computing loss (training)
+2022-11-18 15:21:46,715:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9018 (1.9018)
+2022-11-18 15:21:46,867:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9008 (1.9013)
+2022-11-18 15:21:47,018:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8741 (1.8921)
+2022-11-18 15:21:47,133:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9177 (1.8962)
+2022-11-18 15:21:47,546:INFO: Dataset: univ                Batch:  1/15	Loss 1.9234 (1.9234)
+2022-11-18 15:21:47,703:INFO: Dataset: univ                Batch:  2/15	Loss 1.9216 (1.9226)
+2022-11-18 15:21:47,864:INFO: Dataset: univ                Batch:  3/15	Loss 1.8510 (1.8983)
+2022-11-18 15:21:48,023:INFO: Dataset: univ                Batch:  4/15	Loss 1.8757 (1.8927)
+2022-11-18 15:21:48,203:INFO: Dataset: univ                Batch:  5/15	Loss 1.8558 (1.8857)
+2022-11-18 15:21:48,381:INFO: Dataset: univ                Batch:  6/15	Loss 1.8933 (1.8869)
+2022-11-18 15:21:48,548:INFO: Dataset: univ                Batch:  7/15	Loss 1.9427 (1.8942)
+2022-11-18 15:21:48,723:INFO: Dataset: univ                Batch:  8/15	Loss 1.8913 (1.8938)
+2022-11-18 15:21:48,890:INFO: Dataset: univ                Batch:  9/15	Loss 1.8899 (1.8934)
+2022-11-18 15:21:49,050:INFO: Dataset: univ                Batch: 10/15	Loss 1.9018 (1.8942)
+2022-11-18 15:21:49,212:INFO: Dataset: univ                Batch: 11/15	Loss 1.8606 (1.8909)
+2022-11-18 15:21:49,375:INFO: Dataset: univ                Batch: 12/15	Loss 1.8750 (1.8897)
+2022-11-18 15:21:49,575:INFO: Dataset: univ                Batch: 13/15	Loss 1.9022 (1.8907)
+2022-11-18 15:21:49,762:INFO: Dataset: univ                Batch: 14/15	Loss 1.9174 (1.8926)
+2022-11-18 15:21:49,868:INFO: Dataset: univ                Batch: 15/15	Loss 1.8998 (1.8927)
+2022-11-18 15:21:50,356:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9451 (1.9451)
+2022-11-18 15:21:50,513:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7305 (1.8408)
+2022-11-18 15:21:50,664:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9450 (1.8756)
+2022-11-18 15:21:50,814:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8461 (1.8679)
+2022-11-18 15:21:50,984:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7300 (1.8366)
+2022-11-18 15:21:51,153:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8913 (1.8459)
+2022-11-18 15:21:51,321:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8727 (1.8497)
+2022-11-18 15:21:51,460:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9157 (1.8568)
+2022-11-18 15:21:51,860:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9241 (1.9241)
+2022-11-18 15:21:52,011:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8180 (1.8657)
+2022-11-18 15:21:52,166:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7965 (1.8401)
+2022-11-18 15:21:52,315:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8280 (1.8369)
+2022-11-18 15:21:52,466:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8486 (1.8393)
+2022-11-18 15:21:52,616:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9121 (1.8516)
+2022-11-18 15:21:52,765:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9007 (1.8586)
+2022-11-18 15:21:52,993:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7766 (1.8487)
+2022-11-18 15:21:53,142:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9219 (1.8569)
+2022-11-18 15:21:53,291:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8660 (1.8578)
+2022-11-18 15:21:53,441:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8903 (1.8606)
+2022-11-18 15:21:53,594:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8737 (1.8617)
+2022-11-18 15:21:53,746:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9194 (1.8660)
+2022-11-18 15:21:53,921:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9125 (1.8691)
+2022-11-18 15:21:54,083:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8936 (1.8707)
+2022-11-18 15:21:54,282:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8325 (1.8684)
+2022-11-18 15:21:54,461:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9649 (1.8744)
+2022-11-18 15:21:54,619:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0136 (1.8811)
+2022-11-18 15:21:54,671:INFO: - Computing loss (validation)
+2022-11-18 15:21:54,957:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0647 (2.0647)
+2022-11-18 15:21:54,991:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6500 (2.0293)
+2022-11-18 15:21:55,322:INFO: Dataset: univ                Batch: 1/3	Loss 1.8865 (1.8865)
+2022-11-18 15:21:55,399:INFO: Dataset: univ                Batch: 2/3	Loss 1.8844 (1.8855)
+2022-11-18 15:21:55,479:INFO: Dataset: univ                Batch: 3/3	Loss 1.8872 (1.8862)
+2022-11-18 15:21:55,817:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8593 (1.8593)
+2022-11-18 15:21:55,863:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8714 (1.8625)
+2022-11-18 15:21:56,176:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9944 (1.9944)
+2022-11-18 15:21:56,259:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8658 (1.9278)
+2022-11-18 15:21:56,338:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8890 (1.9148)
+2022-11-18 15:21:56,418:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9194 (1.9160)
+2022-11-18 15:21:56,496:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9209 (1.9170)
+2022-11-18 15:21:56,554:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_394.pth.tar
+2022-11-18 15:21:56,554:INFO: 
+===> EPOCH: 395 (P2)
+2022-11-18 15:21:56,555:INFO: - Computing loss (training)
+2022-11-18 15:21:56,918:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0446 (2.0446)
+2022-11-18 15:21:57,088:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8193 (1.9363)
+2022-11-18 15:21:57,263:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9375 (1.9367)
+2022-11-18 15:21:57,387:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9194 (1.9335)
+2022-11-18 15:21:57,821:INFO: Dataset: univ                Batch:  1/15	Loss 1.8774 (1.8774)
+2022-11-18 15:21:58,006:INFO: Dataset: univ                Batch:  2/15	Loss 1.8615 (1.8691)
+2022-11-18 15:21:58,198:INFO: Dataset: univ                Batch:  3/15	Loss 1.8840 (1.8736)
+2022-11-18 15:21:58,369:INFO: Dataset: univ                Batch:  4/15	Loss 1.8999 (1.8803)
+2022-11-18 15:21:58,545:INFO: Dataset: univ                Batch:  5/15	Loss 1.8916 (1.8825)
+2022-11-18 15:21:58,732:INFO: Dataset: univ                Batch:  6/15	Loss 1.8482 (1.8766)
+2022-11-18 15:21:58,941:INFO: Dataset: univ                Batch:  7/15	Loss 1.9307 (1.8843)
+2022-11-18 15:21:59,130:INFO: Dataset: univ                Batch:  8/15	Loss 1.8840 (1.8843)
+2022-11-18 15:21:59,301:INFO: Dataset: univ                Batch:  9/15	Loss 1.9009 (1.8861)
+2022-11-18 15:21:59,470:INFO: Dataset: univ                Batch: 10/15	Loss 1.8604 (1.8834)
+2022-11-18 15:21:59,632:INFO: Dataset: univ                Batch: 11/15	Loss 1.8720 (1.8825)
+2022-11-18 15:21:59,792:INFO: Dataset: univ                Batch: 12/15	Loss 1.8664 (1.8810)
+2022-11-18 15:21:59,950:INFO: Dataset: univ                Batch: 13/15	Loss 1.8742 (1.8804)
+2022-11-18 15:22:00,108:INFO: Dataset: univ                Batch: 14/15	Loss 1.8904 (1.8811)
+2022-11-18 15:22:00,192:INFO: Dataset: univ                Batch: 15/15	Loss 1.8565 (1.8807)
+2022-11-18 15:22:00,582:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9807 (1.9807)
+2022-11-18 15:22:00,741:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9166 (1.9479)
+2022-11-18 15:22:00,915:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8876 (1.9284)
+2022-11-18 15:22:01,086:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9369 (1.9304)
+2022-11-18 15:22:01,246:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8812 (1.9202)
+2022-11-18 15:22:01,402:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8497 (1.9057)
+2022-11-18 15:22:01,557:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8630 (1.9001)
+2022-11-18 15:22:01,700:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9368 (1.9039)
+2022-11-18 15:22:02,106:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8434 (1.8434)
+2022-11-18 15:22:02,275:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9459 (1.8990)
+2022-11-18 15:22:02,440:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8457 (1.8805)
+2022-11-18 15:22:02,598:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8062 (1.8616)
+2022-11-18 15:22:02,752:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8535 (1.8600)
+2022-11-18 15:22:02,906:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9116 (1.8679)
+2022-11-18 15:22:03,057:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9607 (1.8808)
+2022-11-18 15:22:03,208:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9192 (1.8855)
+2022-11-18 15:22:03,359:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8915 (1.8861)
+2022-11-18 15:22:03,509:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9368 (1.8911)
+2022-11-18 15:22:03,662:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8581 (1.8883)
+2022-11-18 15:22:03,814:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8416 (1.8848)
+2022-11-18 15:22:03,965:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9201 (1.8875)
+2022-11-18 15:22:04,120:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8653 (1.8859)
+2022-11-18 15:22:04,273:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8479 (1.8833)
+2022-11-18 15:22:04,424:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8372 (1.8803)
+2022-11-18 15:22:04,575:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7935 (1.8750)
+2022-11-18 15:22:04,716:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8859 (1.8756)
+2022-11-18 15:22:04,762:INFO: - Computing loss (validation)
+2022-11-18 15:22:05,026:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0050 (2.0050)
+2022-11-18 15:22:05,059:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0079 (2.0052)
+2022-11-18 15:22:05,359:INFO: Dataset: univ                Batch: 1/3	Loss 1.8482 (1.8482)
+2022-11-18 15:22:05,436:INFO: Dataset: univ                Batch: 2/3	Loss 1.8743 (1.8603)
+2022-11-18 15:22:05,511:INFO: Dataset: univ                Batch: 3/3	Loss 1.8455 (1.8555)
+2022-11-18 15:22:05,812:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8893 (1.8893)
+2022-11-18 15:22:05,860:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0689 (1.9320)
+2022-11-18 15:22:06,177:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9242 (1.9242)
+2022-11-18 15:22:06,255:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8246 (1.8721)
+2022-11-18 15:22:06,333:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8799 (1.8746)
+2022-11-18 15:22:06,412:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8115 (1.8586)
+2022-11-18 15:22:06,490:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8199 (1.8512)
+2022-11-18 15:22:06,543:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_395.pth.tar
+2022-11-18 15:22:06,543:INFO: 
+===> EPOCH: 396 (P2)
+2022-11-18 15:22:06,544:INFO: - Computing loss (training)
+2022-11-18 15:22:06,890:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8651 (1.8651)
+2022-11-18 15:22:07,048:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9055 (1.8859)
+2022-11-18 15:22:07,205:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9304 (1.9003)
+2022-11-18 15:22:07,326:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9271 (1.9048)
+2022-11-18 15:22:07,731:INFO: Dataset: univ                Batch:  1/15	Loss 1.8615 (1.8615)
+2022-11-18 15:22:07,892:INFO: Dataset: univ                Batch:  2/15	Loss 1.8777 (1.8696)
+2022-11-18 15:22:08,056:INFO: Dataset: univ                Batch:  3/15	Loss 1.8552 (1.8643)
+2022-11-18 15:22:08,211:INFO: Dataset: univ                Batch:  4/15	Loss 1.8668 (1.8649)
+2022-11-18 15:22:08,367:INFO: Dataset: univ                Batch:  5/15	Loss 1.8819 (1.8682)
+2022-11-18 15:22:08,528:INFO: Dataset: univ                Batch:  6/15	Loss 1.8647 (1.8676)
+2022-11-18 15:22:08,684:INFO: Dataset: univ                Batch:  7/15	Loss 1.8432 (1.8641)
+2022-11-18 15:22:08,837:INFO: Dataset: univ                Batch:  8/15	Loss 1.8575 (1.8634)
+2022-11-18 15:22:08,997:INFO: Dataset: univ                Batch:  9/15	Loss 1.8893 (1.8663)
+2022-11-18 15:22:09,150:INFO: Dataset: univ                Batch: 10/15	Loss 1.8983 (1.8695)
+2022-11-18 15:22:09,307:INFO: Dataset: univ                Batch: 11/15	Loss 1.8793 (1.8704)
+2022-11-18 15:22:09,465:INFO: Dataset: univ                Batch: 12/15	Loss 1.8351 (1.8671)
+2022-11-18 15:22:09,621:INFO: Dataset: univ                Batch: 13/15	Loss 1.8699 (1.8673)
+2022-11-18 15:22:09,777:INFO: Dataset: univ                Batch: 14/15	Loss 1.8905 (1.8689)
+2022-11-18 15:22:09,859:INFO: Dataset: univ                Batch: 15/15	Loss 1.8565 (1.8687)
+2022-11-18 15:22:10,262:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8019 (1.8019)
+2022-11-18 15:22:10,427:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0194 (1.9077)
+2022-11-18 15:22:10,587:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9772 (1.9310)
+2022-11-18 15:22:10,750:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9788 (1.9433)
+2022-11-18 15:22:10,911:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9232 (1.9393)
+2022-11-18 15:22:11,071:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8237 (1.9214)
+2022-11-18 15:22:11,230:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8773 (1.9152)
+2022-11-18 15:22:11,375:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0229 (1.9271)
+2022-11-18 15:22:11,778:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8733 (1.8733)
+2022-11-18 15:22:11,932:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9418 (1.9077)
+2022-11-18 15:22:12,086:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9466 (1.9209)
+2022-11-18 15:22:12,240:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8367 (1.8990)
+2022-11-18 15:22:12,393:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9323 (1.9059)
+2022-11-18 15:22:12,550:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9136 (1.9072)
+2022-11-18 15:22:12,702:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8404 (1.8981)
+2022-11-18 15:22:12,853:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8324 (1.8899)
+2022-11-18 15:22:13,005:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9196 (1.8932)
+2022-11-18 15:22:13,155:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8352 (1.8869)
+2022-11-18 15:22:13,307:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9490 (1.8922)
+2022-11-18 15:22:13,459:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8591 (1.8894)
+2022-11-18 15:22:13,611:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8913 (1.8895)
+2022-11-18 15:22:13,763:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8743 (1.8884)
+2022-11-18 15:22:13,916:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9248 (1.8909)
+2022-11-18 15:22:14,072:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8379 (1.8872)
+2022-11-18 15:22:14,226:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8853 (1.8871)
+2022-11-18 15:22:14,366:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9205 (1.8887)
+2022-11-18 15:22:14,410:INFO: - Computing loss (validation)
+2022-11-18 15:22:14,674:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7838 (1.7838)
+2022-11-18 15:22:14,707:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7405 (1.7810)
+2022-11-18 15:22:15,003:INFO: Dataset: univ                Batch: 1/3	Loss 1.8893 (1.8893)
+2022-11-18 15:22:15,081:INFO: Dataset: univ                Batch: 2/3	Loss 1.9177 (1.9029)
+2022-11-18 15:22:15,153:INFO: Dataset: univ                Batch: 3/3	Loss 1.8377 (1.8827)
+2022-11-18 15:22:15,465:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7916 (1.7916)
+2022-11-18 15:22:15,513:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8825 (1.8159)
+2022-11-18 15:22:15,818:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8706 (1.8706)
+2022-11-18 15:22:15,900:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9133 (1.8919)
+2022-11-18 15:22:15,975:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7988 (1.8623)
+2022-11-18 15:22:16,050:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8467 (1.8586)
+2022-11-18 15:22:16,122:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9489 (1.8766)
+2022-11-18 15:22:16,178:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_396.pth.tar
+2022-11-18 15:22:16,178:INFO: 
+===> EPOCH: 397 (P2)
+2022-11-18 15:22:16,179:INFO: - Computing loss (training)
+2022-11-18 15:22:16,525:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9414 (1.9414)
+2022-11-18 15:22:16,681:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9038 (1.9230)
+2022-11-18 15:22:16,833:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9599 (1.9353)
+2022-11-18 15:22:16,951:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7991 (1.9121)
+2022-11-18 15:22:17,355:INFO: Dataset: univ                Batch:  1/15	Loss 1.8814 (1.8814)
+2022-11-18 15:22:17,522:INFO: Dataset: univ                Batch:  2/15	Loss 1.8690 (1.8750)
+2022-11-18 15:22:17,684:INFO: Dataset: univ                Batch:  3/15	Loss 1.9445 (1.8987)
+2022-11-18 15:22:17,850:INFO: Dataset: univ                Batch:  4/15	Loss 1.8921 (1.8971)
+2022-11-18 15:22:18,014:INFO: Dataset: univ                Batch:  5/15	Loss 1.9107 (1.9001)
+2022-11-18 15:22:18,176:INFO: Dataset: univ                Batch:  6/15	Loss 1.9064 (1.9011)
+2022-11-18 15:22:18,339:INFO: Dataset: univ                Batch:  7/15	Loss 1.8624 (1.8954)
+2022-11-18 15:22:18,500:INFO: Dataset: univ                Batch:  8/15	Loss 1.8634 (1.8913)
+2022-11-18 15:22:18,662:INFO: Dataset: univ                Batch:  9/15	Loss 1.8581 (1.8876)
+2022-11-18 15:22:18,823:INFO: Dataset: univ                Batch: 10/15	Loss 1.8718 (1.8859)
+2022-11-18 15:22:18,986:INFO: Dataset: univ                Batch: 11/15	Loss 1.8736 (1.8847)
+2022-11-18 15:22:19,148:INFO: Dataset: univ                Batch: 12/15	Loss 1.9425 (1.8896)
+2022-11-18 15:22:19,310:INFO: Dataset: univ                Batch: 13/15	Loss 1.9078 (1.8910)
+2022-11-18 15:22:19,473:INFO: Dataset: univ                Batch: 14/15	Loss 1.8251 (1.8869)
+2022-11-18 15:22:19,559:INFO: Dataset: univ                Batch: 15/15	Loss 2.0038 (1.8880)
+2022-11-18 15:22:19,958:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8456 (1.8456)
+2022-11-18 15:22:20,116:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9486 (1.8982)
+2022-11-18 15:22:20,274:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9299 (1.9096)
+2022-11-18 15:22:20,431:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9481 (1.9192)
+2022-11-18 15:22:20,589:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9275 (1.9208)
+2022-11-18 15:22:20,747:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9679 (1.9283)
+2022-11-18 15:22:20,905:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9391 (1.9297)
+2022-11-18 15:22:21,048:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8440 (1.9201)
+2022-11-18 15:22:21,487:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9075 (1.9075)
+2022-11-18 15:22:21,637:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9136 (1.9104)
+2022-11-18 15:22:21,792:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9537 (1.9246)
+2022-11-18 15:22:21,942:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9054 (1.9201)
+2022-11-18 15:22:22,095:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7702 (1.8926)
+2022-11-18 15:22:22,249:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9485 (1.9019)
+2022-11-18 15:22:22,401:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8448 (1.8935)
+2022-11-18 15:22:22,550:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8130 (1.8836)
+2022-11-18 15:22:22,702:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8749 (1.8826)
+2022-11-18 15:22:22,851:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9487 (1.8894)
+2022-11-18 15:22:23,003:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8774 (1.8883)
+2022-11-18 15:22:23,152:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9155 (1.8905)
+2022-11-18 15:22:23,303:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9209 (1.8928)
+2022-11-18 15:22:23,454:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7672 (1.8837)
+2022-11-18 15:22:23,607:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8581 (1.8821)
+2022-11-18 15:22:23,761:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8639 (1.8809)
+2022-11-18 15:22:23,915:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9745 (1.8869)
+2022-11-18 15:22:24,053:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8674 (1.8860)
+2022-11-18 15:22:24,100:INFO: - Computing loss (validation)
+2022-11-18 15:22:24,357:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9384 (1.9384)
+2022-11-18 15:22:24,391:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3599 (1.9701)
+2022-11-18 15:22:24,696:INFO: Dataset: univ                Batch: 1/3	Loss 1.8766 (1.8766)
+2022-11-18 15:22:24,773:INFO: Dataset: univ                Batch: 2/3	Loss 1.8821 (1.8794)
+2022-11-18 15:22:24,846:INFO: Dataset: univ                Batch: 3/3	Loss 1.8883 (1.8823)
+2022-11-18 15:22:25,143:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8958 (1.8958)
+2022-11-18 15:22:25,187:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8952 (1.8956)
+2022-11-18 15:22:25,493:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8204 (1.8204)
+2022-11-18 15:22:25,573:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8610 (1.8408)
+2022-11-18 15:22:25,653:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9336 (1.8739)
+2022-11-18 15:22:25,731:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8647 (1.8717)
+2022-11-18 15:22:25,808:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8705 (1.8715)
+2022-11-18 15:22:25,860:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_397.pth.tar
+2022-11-18 15:22:25,860:INFO: 
+===> EPOCH: 398 (P2)
+2022-11-18 15:22:25,861:INFO: - Computing loss (training)
+2022-11-18 15:22:26,200:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8329 (1.8329)
+2022-11-18 15:22:26,353:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9586 (1.8959)
+2022-11-18 15:22:26,503:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8503 (1.8810)
+2022-11-18 15:22:26,618:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0279 (1.9074)
+2022-11-18 15:22:27,021:INFO: Dataset: univ                Batch:  1/15	Loss 1.8835 (1.8835)
+2022-11-18 15:22:27,184:INFO: Dataset: univ                Batch:  2/15	Loss 1.8800 (1.8819)
+2022-11-18 15:22:27,342:INFO: Dataset: univ                Batch:  3/15	Loss 1.8886 (1.8843)
+2022-11-18 15:22:27,498:INFO: Dataset: univ                Batch:  4/15	Loss 1.8139 (1.8654)
+2022-11-18 15:22:27,654:INFO: Dataset: univ                Batch:  5/15	Loss 1.8728 (1.8670)
+2022-11-18 15:22:27,812:INFO: Dataset: univ                Batch:  6/15	Loss 1.9093 (1.8743)
+2022-11-18 15:22:27,972:INFO: Dataset: univ                Batch:  7/15	Loss 1.9169 (1.8804)
+2022-11-18 15:22:28,126:INFO: Dataset: univ                Batch:  8/15	Loss 1.8419 (1.8760)
+2022-11-18 15:22:28,282:INFO: Dataset: univ                Batch:  9/15	Loss 1.9144 (1.8793)
+2022-11-18 15:22:28,436:INFO: Dataset: univ                Batch: 10/15	Loss 1.8620 (1.8775)
+2022-11-18 15:22:28,594:INFO: Dataset: univ                Batch: 11/15	Loss 1.8684 (1.8767)
+2022-11-18 15:22:28,749:INFO: Dataset: univ                Batch: 12/15	Loss 1.9327 (1.8808)
+2022-11-18 15:22:28,910:INFO: Dataset: univ                Batch: 13/15	Loss 1.8771 (1.8805)
+2022-11-18 15:22:29,069:INFO: Dataset: univ                Batch: 14/15	Loss 1.8755 (1.8802)
+2022-11-18 15:22:29,153:INFO: Dataset: univ                Batch: 15/15	Loss 1.8794 (1.8802)
+2022-11-18 15:22:29,557:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8250 (1.8250)
+2022-11-18 15:22:29,717:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8756 (1.8506)
+2022-11-18 15:22:29,878:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8274 (1.8434)
+2022-11-18 15:22:30,039:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8981 (1.8581)
+2022-11-18 15:22:30,200:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8963 (1.8654)
+2022-11-18 15:22:30,359:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8050 (1.8555)
+2022-11-18 15:22:30,519:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8880 (1.8603)
+2022-11-18 15:22:30,664:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0052 (1.8766)
+2022-11-18 15:22:31,103:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9007 (1.9007)
+2022-11-18 15:22:31,259:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8216 (1.8628)
+2022-11-18 15:22:31,420:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9144 (1.8787)
+2022-11-18 15:22:31,575:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7375 (1.8476)
+2022-11-18 15:22:31,734:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9104 (1.8600)
+2022-11-18 15:22:31,893:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7973 (1.8505)
+2022-11-18 15:22:32,052:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9646 (1.8660)
+2022-11-18 15:22:32,208:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9867 (1.8811)
+2022-11-18 15:22:32,366:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9988 (1.8939)
+2022-11-18 15:22:32,522:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8485 (1.8894)
+2022-11-18 15:22:32,680:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8244 (1.8836)
+2022-11-18 15:22:32,837:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9186 (1.8865)
+2022-11-18 15:22:32,995:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9501 (1.8919)
+2022-11-18 15:22:33,144:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8735 (1.8905)
+2022-11-18 15:22:33,296:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8633 (1.8889)
+2022-11-18 15:22:33,447:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9951 (1.8954)
+2022-11-18 15:22:33,598:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9099 (1.8963)
+2022-11-18 15:22:33,736:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9532 (1.8988)
+2022-11-18 15:22:33,783:INFO: - Computing loss (validation)
+2022-11-18 15:22:34,044:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9419 (1.9419)
+2022-11-18 15:22:34,078:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7375 (1.9266)
+2022-11-18 15:22:34,394:INFO: Dataset: univ                Batch: 1/3	Loss 1.8997 (1.8997)
+2022-11-18 15:22:34,476:INFO: Dataset: univ                Batch: 2/3	Loss 1.8815 (1.8895)
+2022-11-18 15:22:34,552:INFO: Dataset: univ                Batch: 3/3	Loss 1.8853 (1.8881)
+2022-11-18 15:22:34,867:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8789 (1.8789)
+2022-11-18 15:22:34,915:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0606 (1.9233)
+2022-11-18 15:22:35,215:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7991 (1.7991)
+2022-11-18 15:22:35,291:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8621 (1.8313)
+2022-11-18 15:22:35,365:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9184 (1.8595)
+2022-11-18 15:22:35,440:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9036 (1.8716)
+2022-11-18 15:22:35,515:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8530 (1.8677)
+2022-11-18 15:22:35,566:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_398.pth.tar
+2022-11-18 15:22:35,566:INFO: 
+===> EPOCH: 399 (P2)
+2022-11-18 15:22:35,567:INFO: - Computing loss (training)
+2022-11-18 15:22:35,914:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8791 (1.8791)
+2022-11-18 15:22:36,067:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7748 (1.8266)
+2022-11-18 15:22:36,217:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8942 (1.8489)
+2022-11-18 15:22:36,335:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1184 (1.8948)
+2022-11-18 15:22:36,748:INFO: Dataset: univ                Batch:  1/15	Loss 1.8433 (1.8433)
+2022-11-18 15:22:36,910:INFO: Dataset: univ                Batch:  2/15	Loss 1.8858 (1.8639)
+2022-11-18 15:22:37,069:INFO: Dataset: univ                Batch:  3/15	Loss 1.8517 (1.8599)
+2022-11-18 15:22:37,224:INFO: Dataset: univ                Batch:  4/15	Loss 1.8883 (1.8668)
+2022-11-18 15:22:37,385:INFO: Dataset: univ                Batch:  5/15	Loss 1.9105 (1.8758)
+2022-11-18 15:22:37,543:INFO: Dataset: univ                Batch:  6/15	Loss 1.8885 (1.8777)
+2022-11-18 15:22:37,701:INFO: Dataset: univ                Batch:  7/15	Loss 1.8985 (1.8807)
+2022-11-18 15:22:37,857:INFO: Dataset: univ                Batch:  8/15	Loss 1.8894 (1.8817)
+2022-11-18 15:22:38,016:INFO: Dataset: univ                Batch:  9/15	Loss 1.8474 (1.8776)
+2022-11-18 15:22:38,171:INFO: Dataset: univ                Batch: 10/15	Loss 1.9093 (1.8809)
+2022-11-18 15:22:38,332:INFO: Dataset: univ                Batch: 11/15	Loss 1.8244 (1.8753)
+2022-11-18 15:22:38,488:INFO: Dataset: univ                Batch: 12/15	Loss 1.9121 (1.8785)
+2022-11-18 15:22:38,645:INFO: Dataset: univ                Batch: 13/15	Loss 1.8809 (1.8787)
+2022-11-18 15:22:38,802:INFO: Dataset: univ                Batch: 14/15	Loss 1.8875 (1.8794)
+2022-11-18 15:22:38,888:INFO: Dataset: univ                Batch: 15/15	Loss 1.9156 (1.8798)
+2022-11-18 15:22:39,275:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9876 (1.9876)
+2022-11-18 15:22:39,426:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8945 (1.9395)
+2022-11-18 15:22:39,579:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9336 (1.9375)
+2022-11-18 15:22:39,730:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9930 (1.9518)
+2022-11-18 15:22:39,882:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0326 (1.9694)
+2022-11-18 15:22:40,033:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9624 (1.9681)
+2022-11-18 15:22:40,183:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8676 (1.9531)
+2022-11-18 15:22:40,321:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9802 (1.9561)
+2022-11-18 15:22:40,727:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8795 (1.8795)
+2022-11-18 15:22:40,878:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9537 (1.9159)
+2022-11-18 15:22:41,032:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8707 (1.9022)
+2022-11-18 15:22:41,187:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9523 (1.9143)
+2022-11-18 15:22:41,339:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8738 (1.9064)
+2022-11-18 15:22:41,493:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9048 (1.9061)
+2022-11-18 15:22:41,644:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7999 (1.8891)
+2022-11-18 15:22:41,793:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8668 (1.8862)
+2022-11-18 15:22:41,944:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8764 (1.8850)
+2022-11-18 15:22:42,094:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9229 (1.8889)
+2022-11-18 15:22:42,247:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8857 (1.8886)
+2022-11-18 15:22:42,399:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8874 (1.8885)
+2022-11-18 15:22:42,551:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8251 (1.8837)
+2022-11-18 15:22:42,701:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8445 (1.8810)
+2022-11-18 15:22:42,853:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8976 (1.8822)
+2022-11-18 15:22:43,005:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8420 (1.8794)
+2022-11-18 15:22:43,156:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8628 (1.8784)
+2022-11-18 15:22:43,296:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0081 (1.8848)
+2022-11-18 15:22:43,341:INFO: - Computing loss (validation)
+2022-11-18 15:22:43,608:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9164 (1.9164)
+2022-11-18 15:22:43,643:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1922 (1.9399)
+2022-11-18 15:22:43,950:INFO: Dataset: univ                Batch: 1/3	Loss 1.8719 (1.8719)
+2022-11-18 15:22:44,031:INFO: Dataset: univ                Batch: 2/3	Loss 1.8819 (1.8769)
+2022-11-18 15:22:44,103:INFO: Dataset: univ                Batch: 3/3	Loss 1.7875 (1.8493)
+2022-11-18 15:22:44,413:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9157 (1.9157)
+2022-11-18 15:22:44,461:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8930 (1.9113)
+2022-11-18 15:22:44,760:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9509 (1.9509)
+2022-11-18 15:22:44,836:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8600 (1.9076)
+2022-11-18 15:22:44,912:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7972 (1.8697)
+2022-11-18 15:22:44,988:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9323 (1.8857)
+2022-11-18 15:22:45,064:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7980 (1.8689)
+2022-11-18 15:22:45,123:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_399.pth.tar
+2022-11-18 15:22:45,123:INFO: 
+===> EPOCH: 400 (P2)
+2022-11-18 15:22:45,123:INFO: - Computing loss (training)
+2022-11-18 15:22:45,500:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0263 (2.0263)
+2022-11-18 15:22:45,657:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7932 (1.9126)
+2022-11-18 15:22:45,811:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0692 (1.9666)
+2022-11-18 15:22:45,927:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9585 (1.9651)
+2022-11-18 15:22:46,324:INFO: Dataset: univ                Batch:  1/15	Loss 1.8987 (1.8987)
+2022-11-18 15:22:46,479:INFO: Dataset: univ                Batch:  2/15	Loss 1.9090 (1.9037)
+2022-11-18 15:22:46,635:INFO: Dataset: univ                Batch:  3/15	Loss 1.8861 (1.8978)
+2022-11-18 15:22:46,793:INFO: Dataset: univ                Batch:  4/15	Loss 1.8726 (1.8916)
+2022-11-18 15:22:46,947:INFO: Dataset: univ                Batch:  5/15	Loss 1.8711 (1.8880)
+2022-11-18 15:22:47,103:INFO: Dataset: univ                Batch:  6/15	Loss 1.8608 (1.8832)
+2022-11-18 15:22:47,257:INFO: Dataset: univ                Batch:  7/15	Loss 1.8650 (1.8806)
+2022-11-18 15:22:47,408:INFO: Dataset: univ                Batch:  8/15	Loss 1.8922 (1.8821)
+2022-11-18 15:22:47,562:INFO: Dataset: univ                Batch:  9/15	Loss 1.8963 (1.8837)
+2022-11-18 15:22:47,714:INFO: Dataset: univ                Batch: 10/15	Loss 1.8940 (1.8847)
+2022-11-18 15:22:47,868:INFO: Dataset: univ                Batch: 11/15	Loss 1.8808 (1.8844)
+2022-11-18 15:22:48,023:INFO: Dataset: univ                Batch: 12/15	Loss 1.9126 (1.8869)
+2022-11-18 15:22:48,177:INFO: Dataset: univ                Batch: 13/15	Loss 1.9216 (1.8896)
+2022-11-18 15:22:48,331:INFO: Dataset: univ                Batch: 14/15	Loss 1.9011 (1.8905)
+2022-11-18 15:22:48,413:INFO: Dataset: univ                Batch: 15/15	Loss 1.9006 (1.8906)
+2022-11-18 15:22:48,799:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8482 (1.8482)
+2022-11-18 15:22:48,949:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9417 (1.8910)
+2022-11-18 15:22:49,098:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0107 (1.9357)
+2022-11-18 15:22:49,248:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8844 (1.9228)
+2022-11-18 15:22:49,398:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9076 (1.9199)
+2022-11-18 15:22:49,554:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8795 (1.9129)
+2022-11-18 15:22:49,711:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7664 (1.8932)
+2022-11-18 15:22:49,847:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8608 (1.8900)
+2022-11-18 15:22:50,255:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9139 (1.9139)
+2022-11-18 15:22:50,405:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8460 (1.8826)
+2022-11-18 15:22:50,560:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8739 (1.8798)
+2022-11-18 15:22:50,715:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0282 (1.9179)
+2022-11-18 15:22:50,870:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9311 (1.9204)
+2022-11-18 15:22:51,023:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7942 (1.8999)
+2022-11-18 15:22:51,174:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9392 (1.9052)
+2022-11-18 15:22:51,324:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8081 (1.8931)
+2022-11-18 15:22:51,476:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9306 (1.8973)
+2022-11-18 15:22:51,625:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8203 (1.8897)
+2022-11-18 15:22:51,777:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9070 (1.8913)
+2022-11-18 15:22:51,926:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8660 (1.8892)
+2022-11-18 15:22:52,077:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8225 (1.8843)
+2022-11-18 15:22:52,228:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8777 (1.8839)
+2022-11-18 15:22:52,380:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8997 (1.8850)
+2022-11-18 15:22:52,531:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8322 (1.8820)
+2022-11-18 15:22:52,683:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8340 (1.8793)
+2022-11-18 15:22:52,822:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0180 (1.8862)
+2022-11-18 15:22:52,866:INFO: - Computing loss (validation)
+2022-11-18 15:22:53,124:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9106 (1.9106)
+2022-11-18 15:22:53,158:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8493 (1.9062)
+2022-11-18 15:22:53,465:INFO: Dataset: univ                Batch: 1/3	Loss 1.9303 (1.9303)
+2022-11-18 15:22:53,544:INFO: Dataset: univ                Batch: 2/3	Loss 1.8931 (1.9127)
+2022-11-18 15:22:53,616:INFO: Dataset: univ                Batch: 3/3	Loss 1.8340 (1.8889)
+2022-11-18 15:22:53,913:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8517 (1.8517)
+2022-11-18 15:22:53,959:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7781 (1.8347)
+2022-11-18 15:22:54,265:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8990 (1.8990)
+2022-11-18 15:22:54,343:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8278 (1.8659)
+2022-11-18 15:22:54,420:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9043 (1.8795)
+2022-11-18 15:22:54,497:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8377 (1.8690)
+2022-11-18 15:22:54,573:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8928 (1.8738)
+2022-11-18 15:22:54,625:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_400.pth.tar
+2022-11-18 15:22:54,625:INFO: 
+===> EPOCH: 401 (P2)
+2022-11-18 15:22:54,626:INFO: - Computing loss (training)
+2022-11-18 15:22:54,971:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8689 (1.8689)
+2022-11-18 15:22:55,121:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0287 (1.9480)
+2022-11-18 15:22:55,271:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8815 (1.9250)
+2022-11-18 15:22:55,386:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7625 (1.8989)
+2022-11-18 15:22:55,792:INFO: Dataset: univ                Batch:  1/15	Loss 1.8355 (1.8355)
+2022-11-18 15:22:55,953:INFO: Dataset: univ                Batch:  2/15	Loss 1.8714 (1.8546)
+2022-11-18 15:22:56,115:INFO: Dataset: univ                Batch:  3/15	Loss 1.9278 (1.8791)
+2022-11-18 15:22:56,275:INFO: Dataset: univ                Batch:  4/15	Loss 1.8725 (1.8773)
+2022-11-18 15:22:56,433:INFO: Dataset: univ                Batch:  5/15	Loss 1.8383 (1.8697)
+2022-11-18 15:22:56,591:INFO: Dataset: univ                Batch:  6/15	Loss 1.8944 (1.8744)
+2022-11-18 15:22:56,749:INFO: Dataset: univ                Batch:  7/15	Loss 1.8789 (1.8750)
+2022-11-18 15:22:56,904:INFO: Dataset: univ                Batch:  8/15	Loss 1.8690 (1.8743)
+2022-11-18 15:22:57,061:INFO: Dataset: univ                Batch:  9/15	Loss 1.8427 (1.8708)
+2022-11-18 15:22:57,217:INFO: Dataset: univ                Batch: 10/15	Loss 1.8401 (1.8676)
+2022-11-18 15:22:57,377:INFO: Dataset: univ                Batch: 11/15	Loss 1.8920 (1.8701)
+2022-11-18 15:22:57,534:INFO: Dataset: univ                Batch: 12/15	Loss 1.9029 (1.8731)
+2022-11-18 15:22:57,694:INFO: Dataset: univ                Batch: 13/15	Loss 1.9027 (1.8754)
+2022-11-18 15:22:57,852:INFO: Dataset: univ                Batch: 14/15	Loss 1.8773 (1.8756)
+2022-11-18 15:22:57,936:INFO: Dataset: univ                Batch: 15/15	Loss 1.8930 (1.8758)
+2022-11-18 15:22:58,335:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9150 (1.9150)
+2022-11-18 15:22:58,486:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9296 (1.9226)
+2022-11-18 15:22:58,637:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9373 (1.9273)
+2022-11-18 15:22:58,786:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9641 (1.9363)
+2022-11-18 15:22:58,942:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9414 (1.9372)
+2022-11-18 15:22:59,096:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8390 (1.9229)
+2022-11-18 15:22:59,248:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9814 (1.9313)
+2022-11-18 15:22:59,385:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9858 (1.9373)
+2022-11-18 15:22:59,779:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9212 (1.9212)
+2022-11-18 15:22:59,933:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9766 (1.9485)
+2022-11-18 15:23:00,090:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9510 (1.9493)
+2022-11-18 15:23:00,243:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8225 (1.9154)
+2022-11-18 15:23:00,399:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9147 (1.9152)
+2022-11-18 15:23:00,560:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8709 (1.9080)
+2022-11-18 15:23:00,712:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9074 (1.9079)
+2022-11-18 15:23:00,861:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9057 (1.9076)
+2022-11-18 15:23:01,014:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9215 (1.9091)
+2022-11-18 15:23:01,186:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9609 (1.9149)
+2022-11-18 15:23:01,350:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8770 (1.9115)
+2022-11-18 15:23:01,504:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9307 (1.9133)
+2022-11-18 15:23:01,658:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8301 (1.9071)
+2022-11-18 15:23:01,811:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9897 (1.9126)
+2022-11-18 15:23:01,967:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9242 (1.9133)
+2022-11-18 15:23:02,118:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9147 (1.9134)
+2022-11-18 15:23:02,278:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9259 (1.9142)
+2022-11-18 15:23:02,436:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0092 (1.9193)
+2022-11-18 15:23:02,492:INFO: - Computing loss (validation)
+2022-11-18 15:23:02,799:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8957 (1.8957)
+2022-11-18 15:23:02,837:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9796 (1.9028)
+2022-11-18 15:23:03,159:INFO: Dataset: univ                Batch: 1/3	Loss 1.8758 (1.8758)
+2022-11-18 15:23:03,242:INFO: Dataset: univ                Batch: 2/3	Loss 1.8807 (1.8783)
+2022-11-18 15:23:03,318:INFO: Dataset: univ                Batch: 3/3	Loss 1.8536 (1.8709)
+2022-11-18 15:23:03,631:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9260 (1.9260)
+2022-11-18 15:23:03,678:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8348 (1.9058)
+2022-11-18 15:23:03,991:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8800 (1.8800)
+2022-11-18 15:23:04,069:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9485 (1.9155)
+2022-11-18 15:23:04,149:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8055 (1.8796)
+2022-11-18 15:23:04,228:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9003 (1.8849)
+2022-11-18 15:23:04,305:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8603 (1.8802)
+2022-11-18 15:23:04,356:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_401.pth.tar
+2022-11-18 15:23:04,356:INFO: 
+===> EPOCH: 402 (P2)
+2022-11-18 15:23:04,357:INFO: - Computing loss (training)
+2022-11-18 15:23:04,699:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9467 (1.9467)
+2022-11-18 15:23:04,850:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8181 (1.8812)
+2022-11-18 15:23:05,000:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9199 (1.8941)
+2022-11-18 15:23:05,118:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8848 (1.8925)
+2022-11-18 15:23:05,548:INFO: Dataset: univ                Batch:  1/15	Loss 1.8578 (1.8578)
+2022-11-18 15:23:05,704:INFO: Dataset: univ                Batch:  2/15	Loss 1.9046 (1.8823)
+2022-11-18 15:23:05,860:INFO: Dataset: univ                Batch:  3/15	Loss 1.9005 (1.8883)
+2022-11-18 15:23:06,014:INFO: Dataset: univ                Batch:  4/15	Loss 1.8372 (1.8751)
+2022-11-18 15:23:06,170:INFO: Dataset: univ                Batch:  5/15	Loss 1.8800 (1.8761)
+2022-11-18 15:23:06,326:INFO: Dataset: univ                Batch:  6/15	Loss 1.8889 (1.8781)
+2022-11-18 15:23:06,482:INFO: Dataset: univ                Batch:  7/15	Loss 1.9044 (1.8816)
+2022-11-18 15:23:06,637:INFO: Dataset: univ                Batch:  8/15	Loss 1.9612 (1.8907)
+2022-11-18 15:23:06,793:INFO: Dataset: univ                Batch:  9/15	Loss 1.8852 (1.8901)
+2022-11-18 15:23:06,947:INFO: Dataset: univ                Batch: 10/15	Loss 1.9134 (1.8926)
+2022-11-18 15:23:07,105:INFO: Dataset: univ                Batch: 11/15	Loss 1.8519 (1.8886)
+2022-11-18 15:23:07,261:INFO: Dataset: univ                Batch: 12/15	Loss 1.8203 (1.8831)
+2022-11-18 15:23:07,418:INFO: Dataset: univ                Batch: 13/15	Loss 1.9025 (1.8846)
+2022-11-18 15:23:07,575:INFO: Dataset: univ                Batch: 14/15	Loss 1.8608 (1.8827)
+2022-11-18 15:23:07,659:INFO: Dataset: univ                Batch: 15/15	Loss 1.8303 (1.8821)
+2022-11-18 15:23:08,052:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9483 (1.9483)
+2022-11-18 15:23:08,203:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9016 (1.9241)
+2022-11-18 15:23:08,355:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9967 (1.9471)
+2022-11-18 15:23:08,506:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8684 (1.9264)
+2022-11-18 15:23:08,655:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8076 (1.9047)
+2022-11-18 15:23:08,804:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9137 (1.9061)
+2022-11-18 15:23:08,957:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8843 (1.9024)
+2022-11-18 15:23:09,093:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8685 (1.8983)
+2022-11-18 15:23:09,478:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7986 (1.7986)
+2022-11-18 15:23:09,629:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9466 (1.8717)
+2022-11-18 15:23:09,786:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9131 (1.8868)
+2022-11-18 15:23:09,937:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8684 (1.8821)
+2022-11-18 15:23:10,166:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8901 (1.8837)
+2022-11-18 15:23:10,318:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8020 (1.8702)
+2022-11-18 15:23:10,468:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8819 (1.8718)
+2022-11-18 15:23:10,617:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8762 (1.8723)
+2022-11-18 15:23:10,767:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8611 (1.8710)
+2022-11-18 15:23:10,915:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8061 (1.8648)
+2022-11-18 15:23:11,067:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9431 (1.8727)
+2022-11-18 15:23:11,216:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8887 (1.8742)
+2022-11-18 15:23:11,366:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8885 (1.8753)
+2022-11-18 15:23:11,516:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8533 (1.8739)
+2022-11-18 15:23:11,668:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8185 (1.8702)
+2022-11-18 15:23:11,817:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9022 (1.8721)
+2022-11-18 15:23:11,969:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9817 (1.8794)
+2022-11-18 15:23:12,107:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9098 (1.8809)
+2022-11-18 15:23:12,153:INFO: - Computing loss (validation)
+2022-11-18 15:23:12,426:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9345 (1.9345)
+2022-11-18 15:23:12,461:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8339 (1.9273)
+2022-11-18 15:23:12,762:INFO: Dataset: univ                Batch: 1/3	Loss 1.8972 (1.8972)
+2022-11-18 15:23:12,837:INFO: Dataset: univ                Batch: 2/3	Loss 1.8937 (1.8955)
+2022-11-18 15:23:12,910:INFO: Dataset: univ                Batch: 3/3	Loss 1.8879 (1.8927)
+2022-11-18 15:23:13,221:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9093 (1.9093)
+2022-11-18 15:23:13,269:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7750 (1.8726)
+2022-11-18 15:23:13,585:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9099 (1.9099)
+2022-11-18 15:23:13,662:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7563 (1.8338)
+2022-11-18 15:23:13,737:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8594 (1.8422)
+2022-11-18 15:23:13,811:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8971 (1.8570)
+2022-11-18 15:23:13,886:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8273 (1.8513)
+2022-11-18 15:23:13,941:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_402.pth.tar
+2022-11-18 15:23:13,942:INFO: 
+===> EPOCH: 403 (P2)
+2022-11-18 15:23:13,942:INFO: - Computing loss (training)
+2022-11-18 15:23:14,290:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9362 (1.9362)
+2022-11-18 15:23:14,439:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8091 (1.8735)
+2022-11-18 15:23:14,591:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9145 (1.8870)
+2022-11-18 15:23:14,707:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8397 (1.8790)
+2022-11-18 15:23:15,114:INFO: Dataset: univ                Batch:  1/15	Loss 1.9108 (1.9108)
+2022-11-18 15:23:15,275:INFO: Dataset: univ                Batch:  2/15	Loss 1.8813 (1.8956)
+2022-11-18 15:23:15,436:INFO: Dataset: univ                Batch:  3/15	Loss 1.8726 (1.8881)
+2022-11-18 15:23:15,600:INFO: Dataset: univ                Batch:  4/15	Loss 1.8842 (1.8871)
+2022-11-18 15:23:15,757:INFO: Dataset: univ                Batch:  5/15	Loss 1.8639 (1.8824)
+2022-11-18 15:23:15,915:INFO: Dataset: univ                Batch:  6/15	Loss 1.8837 (1.8826)
+2022-11-18 15:23:16,070:INFO: Dataset: univ                Batch:  7/15	Loss 1.9110 (1.8862)
+2022-11-18 15:23:16,226:INFO: Dataset: univ                Batch:  8/15	Loss 1.8403 (1.8806)
+2022-11-18 15:23:16,383:INFO: Dataset: univ                Batch:  9/15	Loss 1.9122 (1.8843)
+2022-11-18 15:23:16,539:INFO: Dataset: univ                Batch: 10/15	Loss 1.9090 (1.8869)
+2022-11-18 15:23:16,695:INFO: Dataset: univ                Batch: 11/15	Loss 1.8930 (1.8875)
+2022-11-18 15:23:16,852:INFO: Dataset: univ                Batch: 12/15	Loss 1.8844 (1.8872)
+2022-11-18 15:23:17,010:INFO: Dataset: univ                Batch: 13/15	Loss 1.8577 (1.8853)
+2022-11-18 15:23:17,166:INFO: Dataset: univ                Batch: 14/15	Loss 1.8763 (1.8847)
+2022-11-18 15:23:17,250:INFO: Dataset: univ                Batch: 15/15	Loss 1.7184 (1.8827)
+2022-11-18 15:23:17,654:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8210 (1.8210)
+2022-11-18 15:23:17,804:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0713 (1.9509)
+2022-11-18 15:23:17,955:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9448 (1.9487)
+2022-11-18 15:23:18,106:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9248 (1.9425)
+2022-11-18 15:23:18,257:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8979 (1.9342)
+2022-11-18 15:23:18,404:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8645 (1.9223)
+2022-11-18 15:23:18,555:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8396 (1.9112)
+2022-11-18 15:23:18,690:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9939 (1.9205)
+2022-11-18 15:23:19,080:INFO: Dataset: zara2               Batch:  1/18	Loss 2.0031 (2.0031)
+2022-11-18 15:23:19,234:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9205 (1.9622)
+2022-11-18 15:23:19,386:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9005 (1.9403)
+2022-11-18 15:23:19,537:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8942 (1.9278)
+2022-11-18 15:23:19,692:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8885 (1.9201)
+2022-11-18 15:23:19,843:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9105 (1.9185)
+2022-11-18 15:23:19,995:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9562 (1.9242)
+2022-11-18 15:23:20,144:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8855 (1.9189)
+2022-11-18 15:23:20,295:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9110 (1.9180)
+2022-11-18 15:23:20,443:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8507 (1.9110)
+2022-11-18 15:23:20,595:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8789 (1.9082)
+2022-11-18 15:23:20,745:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9443 (1.9110)
+2022-11-18 15:23:20,896:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9196 (1.9117)
+2022-11-18 15:23:21,047:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8367 (1.9063)
+2022-11-18 15:23:21,199:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9604 (1.9096)
+2022-11-18 15:23:21,350:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8736 (1.9074)
+2022-11-18 15:23:21,504:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7895 (1.9005)
+2022-11-18 15:23:21,643:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7659 (1.8935)
+2022-11-18 15:23:21,690:INFO: - Computing loss (validation)
+2022-11-18 15:23:21,963:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0374 (2.0374)
+2022-11-18 15:23:21,997:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7958 (2.0218)
+2022-11-18 15:23:22,298:INFO: Dataset: univ                Batch: 1/3	Loss 1.9013 (1.9013)
+2022-11-18 15:23:22,375:INFO: Dataset: univ                Batch: 2/3	Loss 1.8940 (1.8977)
+2022-11-18 15:23:22,448:INFO: Dataset: univ                Batch: 3/3	Loss 1.8792 (1.8918)
+2022-11-18 15:23:22,736:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0225 (2.0225)
+2022-11-18 15:23:22,781:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0905 (2.0385)
+2022-11-18 15:23:23,088:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8298 (1.8298)
+2022-11-18 15:23:23,162:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9243 (1.8770)
+2022-11-18 15:23:23,236:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8457 (1.8664)
+2022-11-18 15:23:23,310:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9483 (1.8856)
+2022-11-18 15:23:23,383:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9109 (1.8903)
+2022-11-18 15:23:23,435:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_403.pth.tar
+2022-11-18 15:23:23,435:INFO: 
+===> EPOCH: 404 (P2)
+2022-11-18 15:23:23,436:INFO: - Computing loss (training)
+2022-11-18 15:23:23,788:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8464 (1.8464)
+2022-11-18 15:23:23,940:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9143 (1.8805)
+2022-11-18 15:23:24,089:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9451 (1.9024)
+2022-11-18 15:23:24,206:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0068 (1.9201)
+2022-11-18 15:23:24,610:INFO: Dataset: univ                Batch:  1/15	Loss 1.8336 (1.8336)
+2022-11-18 15:23:24,768:INFO: Dataset: univ                Batch:  2/15	Loss 1.8971 (1.8653)
+2022-11-18 15:23:24,927:INFO: Dataset: univ                Batch:  3/15	Loss 1.8858 (1.8728)
+2022-11-18 15:23:25,081:INFO: Dataset: univ                Batch:  4/15	Loss 1.8582 (1.8692)
+2022-11-18 15:23:25,243:INFO: Dataset: univ                Batch:  5/15	Loss 1.8726 (1.8699)
+2022-11-18 15:23:25,401:INFO: Dataset: univ                Batch:  6/15	Loss 1.9297 (1.8793)
+2022-11-18 15:23:25,557:INFO: Dataset: univ                Batch:  7/15	Loss 1.8412 (1.8738)
+2022-11-18 15:23:25,711:INFO: Dataset: univ                Batch:  8/15	Loss 1.8877 (1.8756)
+2022-11-18 15:23:25,867:INFO: Dataset: univ                Batch:  9/15	Loss 1.9328 (1.8821)
+2022-11-18 15:23:26,022:INFO: Dataset: univ                Batch: 10/15	Loss 1.8204 (1.8757)
+2022-11-18 15:23:26,178:INFO: Dataset: univ                Batch: 11/15	Loss 1.8803 (1.8761)
+2022-11-18 15:23:26,334:INFO: Dataset: univ                Batch: 12/15	Loss 1.8792 (1.8763)
+2022-11-18 15:23:26,490:INFO: Dataset: univ                Batch: 13/15	Loss 1.8564 (1.8750)
+2022-11-18 15:23:26,646:INFO: Dataset: univ                Batch: 14/15	Loss 1.9044 (1.8771)
+2022-11-18 15:23:26,729:INFO: Dataset: univ                Batch: 15/15	Loss 1.8709 (1.8770)
+2022-11-18 15:23:27,130:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9968 (1.9968)
+2022-11-18 15:23:27,282:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8929 (1.9452)
+2022-11-18 15:23:27,433:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9243 (1.9383)
+2022-11-18 15:23:27,586:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0443 (1.9688)
+2022-11-18 15:23:27,736:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9647 (1.9680)
+2022-11-18 15:23:27,885:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9188 (1.9600)
+2022-11-18 15:23:28,036:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9307 (1.9562)
+2022-11-18 15:23:28,173:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8747 (1.9468)
+2022-11-18 15:23:28,571:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9550 (1.9550)
+2022-11-18 15:23:28,722:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9325 (1.9442)
+2022-11-18 15:23:28,881:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8563 (1.9144)
+2022-11-18 15:23:29,041:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8729 (1.9046)
+2022-11-18 15:23:29,195:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8086 (1.8857)
+2022-11-18 15:23:29,347:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8297 (1.8761)
+2022-11-18 15:23:29,500:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8612 (1.8740)
+2022-11-18 15:23:29,651:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9498 (1.8839)
+2022-11-18 15:23:29,803:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9031 (1.8860)
+2022-11-18 15:23:29,952:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9118 (1.8884)
+2022-11-18 15:23:30,105:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8692 (1.8865)
+2022-11-18 15:23:30,257:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8951 (1.8872)
+2022-11-18 15:23:30,409:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8341 (1.8830)
+2022-11-18 15:23:30,560:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8791 (1.8827)
+2022-11-18 15:23:30,713:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8637 (1.8815)
+2022-11-18 15:23:30,864:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9881 (1.8884)
+2022-11-18 15:23:31,016:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9031 (1.8893)
+2022-11-18 15:23:31,156:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9378 (1.8918)
+2022-11-18 15:23:31,203:INFO: - Computing loss (validation)
+2022-11-18 15:23:31,457:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9672 (1.9672)
+2022-11-18 15:23:31,493:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8471 (1.9565)
+2022-11-18 15:23:31,808:INFO: Dataset: univ                Batch: 1/3	Loss 1.8622 (1.8622)
+2022-11-18 15:23:31,887:INFO: Dataset: univ                Batch: 2/3	Loss 1.8680 (1.8653)
+2022-11-18 15:23:31,961:INFO: Dataset: univ                Batch: 3/3	Loss 1.8830 (1.8703)
+2022-11-18 15:23:32,267:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9118 (1.9118)
+2022-11-18 15:23:32,315:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9878 (1.9302)
+2022-11-18 15:23:32,632:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9253 (1.9253)
+2022-11-18 15:23:32,714:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8481 (1.8860)
+2022-11-18 15:23:32,793:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8421 (1.8703)
+2022-11-18 15:23:32,872:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9587 (1.8924)
+2022-11-18 15:23:32,950:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9032 (1.8944)
+2022-11-18 15:23:33,001:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_404.pth.tar
+2022-11-18 15:23:33,002:INFO: 
+===> EPOCH: 405 (P2)
+2022-11-18 15:23:33,002:INFO: - Computing loss (training)
+2022-11-18 15:23:33,338:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9607 (1.9607)
+2022-11-18 15:23:33,491:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9789 (1.9697)
+2022-11-18 15:23:33,644:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9673 (1.9690)
+2022-11-18 15:23:33,759:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9787 (1.9707)
+2022-11-18 15:23:34,188:INFO: Dataset: univ                Batch:  1/15	Loss 1.8097 (1.8097)
+2022-11-18 15:23:34,350:INFO: Dataset: univ                Batch:  2/15	Loss 1.8527 (1.8308)
+2022-11-18 15:23:34,509:INFO: Dataset: univ                Batch:  3/15	Loss 1.9113 (1.8566)
+2022-11-18 15:23:34,665:INFO: Dataset: univ                Batch:  4/15	Loss 1.8496 (1.8548)
+2022-11-18 15:23:34,823:INFO: Dataset: univ                Batch:  5/15	Loss 1.8558 (1.8550)
+2022-11-18 15:23:34,983:INFO: Dataset: univ                Batch:  6/15	Loss 1.8869 (1.8607)
+2022-11-18 15:23:35,140:INFO: Dataset: univ                Batch:  7/15	Loss 1.9062 (1.8672)
+2022-11-18 15:23:35,297:INFO: Dataset: univ                Batch:  8/15	Loss 1.8381 (1.8632)
+2022-11-18 15:23:35,454:INFO: Dataset: univ                Batch:  9/15	Loss 1.9173 (1.8689)
+2022-11-18 15:23:35,610:INFO: Dataset: univ                Batch: 10/15	Loss 1.8935 (1.8713)
+2022-11-18 15:23:35,767:INFO: Dataset: univ                Batch: 11/15	Loss 1.8872 (1.8728)
+2022-11-18 15:23:35,924:INFO: Dataset: univ                Batch: 12/15	Loss 1.8400 (1.8700)
+2022-11-18 15:23:36,082:INFO: Dataset: univ                Batch: 13/15	Loss 1.8944 (1.8718)
+2022-11-18 15:23:36,248:INFO: Dataset: univ                Batch: 14/15	Loss 1.8809 (1.8723)
+2022-11-18 15:23:36,340:INFO: Dataset: univ                Batch: 15/15	Loss 1.9204 (1.8730)
+2022-11-18 15:23:36,728:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8872 (1.8872)
+2022-11-18 15:23:36,880:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8999 (1.8938)
+2022-11-18 15:23:37,031:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9454 (1.9117)
+2022-11-18 15:23:37,181:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9339 (1.9172)
+2022-11-18 15:23:37,337:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7429 (1.8830)
+2022-11-18 15:23:37,510:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9217 (1.8892)
+2022-11-18 15:23:37,668:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8189 (1.8803)
+2022-11-18 15:23:37,808:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8427 (1.8763)
+2022-11-18 15:23:38,199:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9176 (1.9176)
+2022-11-18 15:23:38,351:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9322 (1.9246)
+2022-11-18 15:23:38,502:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8665 (1.9059)
+2022-11-18 15:23:38,669:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8471 (1.8919)
+2022-11-18 15:23:38,841:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8854 (1.8907)
+2022-11-18 15:23:39,004:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8446 (1.8826)
+2022-11-18 15:23:39,162:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9992 (1.9002)
+2022-11-18 15:23:39,312:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8302 (1.8910)
+2022-11-18 15:23:39,463:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8424 (1.8854)
+2022-11-18 15:23:39,613:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8422 (1.8807)
+2022-11-18 15:23:39,766:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9418 (1.8864)
+2022-11-18 15:23:39,915:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8960 (1.8872)
+2022-11-18 15:23:40,065:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9112 (1.8890)
+2022-11-18 15:23:40,228:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9361 (1.8925)
+2022-11-18 15:23:40,399:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9645 (1.8972)
+2022-11-18 15:23:40,569:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8944 (1.8970)
+2022-11-18 15:23:40,734:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9305 (1.8987)
+2022-11-18 15:23:40,898:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8524 (1.8966)
+2022-11-18 15:23:40,958:INFO: - Computing loss (validation)
+2022-11-18 15:23:41,247:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8669 (1.8669)
+2022-11-18 15:23:41,282:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3632 (1.8957)
+2022-11-18 15:23:41,603:INFO: Dataset: univ                Batch: 1/3	Loss 1.8909 (1.8909)
+2022-11-18 15:23:41,683:INFO: Dataset: univ                Batch: 2/3	Loss 1.9289 (1.9074)
+2022-11-18 15:23:41,758:INFO: Dataset: univ                Batch: 3/3	Loss 1.9396 (1.9158)
+2022-11-18 15:23:42,071:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8908 (1.8908)
+2022-11-18 15:23:42,118:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9930 (1.9151)
+2022-11-18 15:23:42,425:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9527 (1.9527)
+2022-11-18 15:23:42,498:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9305 (1.9415)
+2022-11-18 15:23:42,574:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8114 (1.9002)
+2022-11-18 15:23:42,653:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8830 (1.8962)
+2022-11-18 15:23:42,726:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7978 (1.8774)
+2022-11-18 15:23:42,779:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_405.pth.tar
+2022-11-18 15:23:42,779:INFO: 
+===> EPOCH: 406 (P2)
+2022-11-18 15:23:42,780:INFO: - Computing loss (training)
+2022-11-18 15:23:43,128:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9938 (1.9938)
+2022-11-18 15:23:43,282:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9395 (1.9651)
+2022-11-18 15:23:43,437:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8502 (1.9238)
+2022-11-18 15:23:43,558:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8458 (1.9099)
+2022-11-18 15:23:43,962:INFO: Dataset: univ                Batch:  1/15	Loss 1.9150 (1.9150)
+2022-11-18 15:23:44,127:INFO: Dataset: univ                Batch:  2/15	Loss 1.9120 (1.9134)
+2022-11-18 15:23:44,290:INFO: Dataset: univ                Batch:  3/15	Loss 1.8851 (1.9038)
+2022-11-18 15:23:44,445:INFO: Dataset: univ                Batch:  4/15	Loss 1.8378 (1.8871)
+2022-11-18 15:23:44,602:INFO: Dataset: univ                Batch:  5/15	Loss 1.8521 (1.8798)
+2022-11-18 15:23:44,770:INFO: Dataset: univ                Batch:  6/15	Loss 1.9011 (1.8837)
+2022-11-18 15:23:44,962:INFO: Dataset: univ                Batch:  7/15	Loss 1.9058 (1.8870)
+2022-11-18 15:23:45,142:INFO: Dataset: univ                Batch:  8/15	Loss 1.8947 (1.8880)
+2022-11-18 15:23:45,300:INFO: Dataset: univ                Batch:  9/15	Loss 1.9227 (1.8921)
+2022-11-18 15:23:45,466:INFO: Dataset: univ                Batch: 10/15	Loss 1.9157 (1.8944)
+2022-11-18 15:23:45,624:INFO: Dataset: univ                Batch: 11/15	Loss 1.8885 (1.8939)
+2022-11-18 15:23:45,781:INFO: Dataset: univ                Batch: 12/15	Loss 1.8377 (1.8883)
+2022-11-18 15:23:45,937:INFO: Dataset: univ                Batch: 13/15	Loss 1.8609 (1.8864)
+2022-11-18 15:23:46,095:INFO: Dataset: univ                Batch: 14/15	Loss 1.8548 (1.8840)
+2022-11-18 15:23:46,178:INFO: Dataset: univ                Batch: 15/15	Loss 1.8398 (1.8832)
+2022-11-18 15:23:46,582:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9709 (1.9709)
+2022-11-18 15:23:46,737:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8916 (1.9333)
+2022-11-18 15:23:46,895:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8981 (1.9228)
+2022-11-18 15:23:47,049:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9929 (1.9417)
+2022-11-18 15:23:47,206:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8979 (1.9333)
+2022-11-18 15:23:47,358:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8270 (1.9155)
+2022-11-18 15:23:47,510:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8186 (1.9016)
+2022-11-18 15:23:47,647:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9918 (1.9126)
+2022-11-18 15:23:48,089:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8453 (1.8453)
+2022-11-18 15:23:48,262:INFO: Dataset: zara2               Batch:  2/18	Loss 2.0123 (1.9252)
+2022-11-18 15:23:48,424:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8730 (1.9079)
+2022-11-18 15:23:48,579:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8214 (1.8843)
+2022-11-18 15:23:48,740:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8847 (1.8844)
+2022-11-18 15:23:48,901:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7496 (1.8633)
+2022-11-18 15:23:49,058:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8972 (1.8681)
+2022-11-18 15:23:49,217:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8389 (1.8643)
+2022-11-18 15:23:49,393:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8826 (1.8663)
+2022-11-18 15:23:49,580:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8136 (1.8613)
+2022-11-18 15:23:49,763:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8439 (1.8596)
+2022-11-18 15:23:49,956:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7785 (1.8527)
+2022-11-18 15:23:50,150:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8700 (1.8540)
+2022-11-18 15:23:50,357:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8555 (1.8541)
+2022-11-18 15:23:50,529:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9386 (1.8591)
+2022-11-18 15:23:50,692:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8734 (1.8601)
+2022-11-18 15:23:50,852:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9226 (1.8638)
+2022-11-18 15:23:51,008:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8044 (1.8612)
+2022-11-18 15:23:51,062:INFO: - Computing loss (validation)
+2022-11-18 15:23:51,347:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9459 (1.9459)
+2022-11-18 15:23:51,384:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6871 (1.9300)
+2022-11-18 15:23:51,699:INFO: Dataset: univ                Batch: 1/3	Loss 1.9053 (1.9053)
+2022-11-18 15:23:51,777:INFO: Dataset: univ                Batch: 2/3	Loss 1.8600 (1.8812)
+2022-11-18 15:23:51,851:INFO: Dataset: univ                Batch: 3/3	Loss 1.9302 (1.8976)
+2022-11-18 15:23:52,148:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9053 (1.9053)
+2022-11-18 15:23:52,194:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8231 (1.8815)
+2022-11-18 15:23:52,504:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9037 (1.9037)
+2022-11-18 15:23:52,581:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8736 (1.8882)
+2022-11-18 15:23:52,656:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8129 (1.8661)
+2022-11-18 15:23:52,732:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8739 (1.8680)
+2022-11-18 15:23:52,807:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9444 (1.8843)
+2022-11-18 15:23:52,863:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_406.pth.tar
+2022-11-18 15:23:52,863:INFO: 
+===> EPOCH: 407 (P2)
+2022-11-18 15:23:52,863:INFO: - Computing loss (training)
+2022-11-18 15:23:53,209:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0417 (2.0417)
+2022-11-18 15:23:53,367:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9863 (2.0132)
+2022-11-18 15:23:53,524:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8320 (1.9538)
+2022-11-18 15:23:53,646:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1664 (1.9886)
+2022-11-18 15:23:54,057:INFO: Dataset: univ                Batch:  1/15	Loss 1.8331 (1.8331)
+2022-11-18 15:23:54,234:INFO: Dataset: univ                Batch:  2/15	Loss 1.8562 (1.8448)
+2022-11-18 15:23:54,408:INFO: Dataset: univ                Batch:  3/15	Loss 1.8892 (1.8601)
+2022-11-18 15:23:54,573:INFO: Dataset: univ                Batch:  4/15	Loss 1.8492 (1.8574)
+2022-11-18 15:23:54,741:INFO: Dataset: univ                Batch:  5/15	Loss 1.8510 (1.8560)
+2022-11-18 15:23:54,912:INFO: Dataset: univ                Batch:  6/15	Loss 1.8799 (1.8601)
+2022-11-18 15:23:55,079:INFO: Dataset: univ                Batch:  7/15	Loss 1.8794 (1.8628)
+2022-11-18 15:23:55,247:INFO: Dataset: univ                Batch:  8/15	Loss 1.8759 (1.8645)
+2022-11-18 15:23:55,419:INFO: Dataset: univ                Batch:  9/15	Loss 1.8692 (1.8651)
+2022-11-18 15:23:55,584:INFO: Dataset: univ                Batch: 10/15	Loss 1.9147 (1.8703)
+2022-11-18 15:23:55,751:INFO: Dataset: univ                Batch: 11/15	Loss 1.9042 (1.8733)
+2022-11-18 15:23:55,919:INFO: Dataset: univ                Batch: 12/15	Loss 1.8584 (1.8721)
+2022-11-18 15:23:56,086:INFO: Dataset: univ                Batch: 13/15	Loss 1.8966 (1.8739)
+2022-11-18 15:23:56,254:INFO: Dataset: univ                Batch: 14/15	Loss 1.9042 (1.8761)
+2022-11-18 15:23:56,343:INFO: Dataset: univ                Batch: 15/15	Loss 1.8881 (1.8762)
+2022-11-18 15:23:56,745:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8248 (1.8248)
+2022-11-18 15:23:56,903:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0265 (1.9233)
+2022-11-18 15:23:57,061:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0297 (1.9602)
+2022-11-18 15:23:57,218:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9189 (1.9496)
+2022-11-18 15:23:57,375:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9016 (1.9400)
+2022-11-18 15:23:57,533:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8279 (1.9211)
+2022-11-18 15:23:57,695:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9830 (1.9299)
+2022-11-18 15:23:57,840:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8956 (1.9262)
+2022-11-18 15:23:58,233:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7922 (1.7922)
+2022-11-18 15:23:58,382:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9327 (1.8658)
+2022-11-18 15:23:58,536:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8166 (1.8502)
+2022-11-18 15:23:58,688:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8898 (1.8599)
+2022-11-18 15:23:58,841:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7990 (1.8489)
+2022-11-18 15:23:58,995:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8884 (1.8559)
+2022-11-18 15:23:59,146:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9052 (1.8629)
+2022-11-18 15:23:59,296:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9407 (1.8725)
+2022-11-18 15:23:59,445:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8776 (1.8732)
+2022-11-18 15:23:59,594:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8402 (1.8703)
+2022-11-18 15:23:59,746:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8697 (1.8702)
+2022-11-18 15:23:59,894:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8390 (1.8676)
+2022-11-18 15:24:00,047:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9076 (1.8707)
+2022-11-18 15:24:00,198:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9536 (1.8770)
+2022-11-18 15:24:00,350:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7869 (1.8715)
+2022-11-18 15:24:00,500:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9770 (1.8787)
+2022-11-18 15:24:00,651:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9058 (1.8803)
+2022-11-18 15:24:00,789:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8918 (1.8808)
+2022-11-18 15:24:00,834:INFO: - Computing loss (validation)
+2022-11-18 15:24:01,101:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8733 (1.8733)
+2022-11-18 15:24:01,135:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8945 (1.8750)
+2022-11-18 15:24:01,443:INFO: Dataset: univ                Batch: 1/3	Loss 1.8886 (1.8886)
+2022-11-18 15:24:01,525:INFO: Dataset: univ                Batch: 2/3	Loss 1.8253 (1.8580)
+2022-11-18 15:24:01,601:INFO: Dataset: univ                Batch: 3/3	Loss 1.9372 (1.8794)
+2022-11-18 15:24:01,910:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9961 (1.9961)
+2022-11-18 15:24:01,957:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8457 (1.9574)
+2022-11-18 15:24:02,254:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9388 (1.9388)
+2022-11-18 15:24:02,332:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9081 (1.9226)
+2022-11-18 15:24:02,406:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9568 (1.9341)
+2022-11-18 15:24:02,480:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8742 (1.9183)
+2022-11-18 15:24:02,553:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8944 (1.9140)
+2022-11-18 15:24:02,605:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_407.pth.tar
+2022-11-18 15:24:02,605:INFO: 
+===> EPOCH: 408 (P2)
+2022-11-18 15:24:02,605:INFO: - Computing loss (training)
+2022-11-18 15:24:02,940:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9529 (1.9529)
+2022-11-18 15:24:03,091:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9561 (1.9546)
+2022-11-18 15:24:03,242:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9152 (1.9409)
+2022-11-18 15:24:03,358:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9016 (1.9346)
+2022-11-18 15:24:03,804:INFO: Dataset: univ                Batch:  1/15	Loss 1.9116 (1.9116)
+2022-11-18 15:24:03,962:INFO: Dataset: univ                Batch:  2/15	Loss 1.8680 (1.8916)
+2022-11-18 15:24:04,122:INFO: Dataset: univ                Batch:  3/15	Loss 1.8494 (1.8771)
+2022-11-18 15:24:04,279:INFO: Dataset: univ                Batch:  4/15	Loss 1.8962 (1.8817)
+2022-11-18 15:24:04,438:INFO: Dataset: univ                Batch:  5/15	Loss 1.8690 (1.8791)
+2022-11-18 15:24:04,600:INFO: Dataset: univ                Batch:  6/15	Loss 1.8926 (1.8817)
+2022-11-18 15:24:04,758:INFO: Dataset: univ                Batch:  7/15	Loss 1.9190 (1.8870)
+2022-11-18 15:24:04,915:INFO: Dataset: univ                Batch:  8/15	Loss 1.8973 (1.8884)
+2022-11-18 15:24:05,081:INFO: Dataset: univ                Batch:  9/15	Loss 1.8588 (1.8854)
+2022-11-18 15:24:05,246:INFO: Dataset: univ                Batch: 10/15	Loss 1.8771 (1.8845)
+2022-11-18 15:24:05,404:INFO: Dataset: univ                Batch: 11/15	Loss 1.8715 (1.8833)
+2022-11-18 15:24:05,562:INFO: Dataset: univ                Batch: 12/15	Loss 1.8533 (1.8809)
+2022-11-18 15:24:05,721:INFO: Dataset: univ                Batch: 13/15	Loss 1.8778 (1.8806)
+2022-11-18 15:24:05,878:INFO: Dataset: univ                Batch: 14/15	Loss 1.8803 (1.8806)
+2022-11-18 15:24:05,961:INFO: Dataset: univ                Batch: 15/15	Loss 2.0271 (1.8831)
+2022-11-18 15:24:06,351:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9511 (1.9511)
+2022-11-18 15:24:06,503:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9272 (1.9391)
+2022-11-18 15:24:06,661:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9070 (1.9291)
+2022-11-18 15:24:06,812:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0750 (1.9649)
+2022-11-18 15:24:06,967:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8739 (1.9477)
+2022-11-18 15:24:07,119:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0395 (1.9631)
+2022-11-18 15:24:07,273:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8325 (1.9435)
+2022-11-18 15:24:07,411:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8037 (1.9268)
+2022-11-18 15:24:07,806:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9324 (1.9324)
+2022-11-18 15:24:07,957:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8955 (1.9145)
+2022-11-18 15:24:08,117:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8286 (1.8876)
+2022-11-18 15:24:08,268:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9481 (1.9017)
+2022-11-18 15:24:08,421:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9265 (1.9070)
+2022-11-18 15:24:08,573:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7789 (1.8847)
+2022-11-18 15:24:08,727:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8800 (1.8841)
+2022-11-18 15:24:08,879:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8301 (1.8766)
+2022-11-18 15:24:09,033:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8861 (1.8777)
+2022-11-18 15:24:09,183:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8979 (1.8798)
+2022-11-18 15:24:09,335:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9214 (1.8837)
+2022-11-18 15:24:09,484:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9290 (1.8877)
+2022-11-18 15:24:09,636:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8587 (1.8853)
+2022-11-18 15:24:09,786:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9119 (1.8871)
+2022-11-18 15:24:09,939:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7594 (1.8793)
+2022-11-18 15:24:10,088:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8002 (1.8744)
+2022-11-18 15:24:10,243:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8283 (1.8718)
+2022-11-18 15:24:10,381:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7685 (1.8668)
+2022-11-18 15:24:10,427:INFO: - Computing loss (validation)
+2022-11-18 15:24:10,693:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8731 (1.8731)
+2022-11-18 15:24:10,728:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6790 (1.8592)
+2022-11-18 15:24:11,044:INFO: Dataset: univ                Batch: 1/3	Loss 1.9158 (1.9158)
+2022-11-18 15:24:11,129:INFO: Dataset: univ                Batch: 2/3	Loss 1.8366 (1.8778)
+2022-11-18 15:24:11,205:INFO: Dataset: univ                Batch: 3/3	Loss 1.8999 (1.8848)
+2022-11-18 15:24:11,509:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8775 (1.8775)
+2022-11-18 15:24:11,554:INFO: Dataset: zara1               Batch: 2/2	Loss 1.5637 (1.7978)
+2022-11-18 15:24:11,857:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9023 (1.9023)
+2022-11-18 15:24:11,935:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9486 (1.9251)
+2022-11-18 15:24:12,010:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9011 (1.9171)
+2022-11-18 15:24:12,086:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9266 (1.9195)
+2022-11-18 15:24:12,162:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8171 (1.8993)
+2022-11-18 15:24:12,216:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_408.pth.tar
+2022-11-18 15:24:12,217:INFO: 
+===> EPOCH: 409 (P2)
+2022-11-18 15:24:12,217:INFO: - Computing loss (training)
+2022-11-18 15:24:12,556:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8745 (1.8745)
+2022-11-18 15:24:12,708:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8107 (1.8425)
+2022-11-18 15:24:12,861:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8754 (1.8530)
+2022-11-18 15:24:12,978:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9564 (1.8709)
+2022-11-18 15:24:13,393:INFO: Dataset: univ                Batch:  1/15	Loss 1.8963 (1.8963)
+2022-11-18 15:24:13,552:INFO: Dataset: univ                Batch:  2/15	Loss 1.8272 (1.8626)
+2022-11-18 15:24:13,709:INFO: Dataset: univ                Batch:  3/15	Loss 1.8851 (1.8697)
+2022-11-18 15:24:13,866:INFO: Dataset: univ                Batch:  4/15	Loss 1.8643 (1.8684)
+2022-11-18 15:24:14,027:INFO: Dataset: univ                Batch:  5/15	Loss 1.9105 (1.8769)
+2022-11-18 15:24:14,184:INFO: Dataset: univ                Batch:  6/15	Loss 1.8757 (1.8767)
+2022-11-18 15:24:14,341:INFO: Dataset: univ                Batch:  7/15	Loss 1.8870 (1.8782)
+2022-11-18 15:24:14,494:INFO: Dataset: univ                Batch:  8/15	Loss 1.8903 (1.8797)
+2022-11-18 15:24:14,651:INFO: Dataset: univ                Batch:  9/15	Loss 1.8817 (1.8799)
+2022-11-18 15:24:14,806:INFO: Dataset: univ                Batch: 10/15	Loss 1.8469 (1.8766)
+2022-11-18 15:24:14,964:INFO: Dataset: univ                Batch: 11/15	Loss 1.8255 (1.8718)
+2022-11-18 15:24:15,119:INFO: Dataset: univ                Batch: 12/15	Loss 1.8838 (1.8728)
+2022-11-18 15:24:15,278:INFO: Dataset: univ                Batch: 13/15	Loss 1.8496 (1.8710)
+2022-11-18 15:24:15,434:INFO: Dataset: univ                Batch: 14/15	Loss 1.9735 (1.8770)
+2022-11-18 15:24:15,517:INFO: Dataset: univ                Batch: 15/15	Loss 1.8684 (1.8770)
+2022-11-18 15:24:15,916:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8940 (1.8940)
+2022-11-18 15:24:16,074:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8066 (1.8476)
+2022-11-18 15:24:16,247:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8167 (1.8382)
+2022-11-18 15:24:16,409:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8454 (1.8400)
+2022-11-18 15:24:16,560:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9856 (1.8715)
+2022-11-18 15:24:16,717:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9191 (1.8803)
+2022-11-18 15:24:16,867:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8667 (1.8783)
+2022-11-18 15:24:17,003:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8397 (1.8738)
+2022-11-18 15:24:17,449:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9310 (1.9310)
+2022-11-18 15:24:17,601:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7890 (1.8612)
+2022-11-18 15:24:17,752:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9314 (1.8863)
+2022-11-18 15:24:17,902:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7993 (1.8662)
+2022-11-18 15:24:18,053:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8796 (1.8692)
+2022-11-18 15:24:18,206:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8792 (1.8710)
+2022-11-18 15:24:18,356:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8692 (1.8708)
+2022-11-18 15:24:18,505:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8417 (1.8674)
+2022-11-18 15:24:18,656:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8659 (1.8672)
+2022-11-18 15:24:18,806:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8108 (1.8616)
+2022-11-18 15:24:18,959:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8843 (1.8635)
+2022-11-18 15:24:19,107:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8364 (1.8613)
+2022-11-18 15:24:19,260:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9577 (1.8679)
+2022-11-18 15:24:19,410:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8705 (1.8681)
+2022-11-18 15:24:19,562:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9439 (1.8727)
+2022-11-18 15:24:19,713:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8313 (1.8701)
+2022-11-18 15:24:19,866:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9009 (1.8719)
+2022-11-18 15:24:20,004:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9132 (1.8740)
+2022-11-18 15:24:20,051:INFO: - Computing loss (validation)
+2022-11-18 15:24:20,327:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7814 (1.7814)
+2022-11-18 15:24:20,361:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7700 (1.7805)
+2022-11-18 15:24:20,670:INFO: Dataset: univ                Batch: 1/3	Loss 1.9469 (1.9469)
+2022-11-18 15:24:20,755:INFO: Dataset: univ                Batch: 2/3	Loss 1.8792 (1.9085)
+2022-11-18 15:24:20,831:INFO: Dataset: univ                Batch: 3/3	Loss 1.8817 (1.8999)
+2022-11-18 15:24:21,129:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8441 (1.8441)
+2022-11-18 15:24:21,175:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9193 (1.8612)
+2022-11-18 15:24:21,500:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9601 (1.9601)
+2022-11-18 15:24:21,580:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8906 (1.9260)
+2022-11-18 15:24:21,659:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9359 (1.9294)
+2022-11-18 15:24:21,739:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8703 (1.9146)
+2022-11-18 15:24:21,817:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9339 (1.9186)
+2022-11-18 15:24:21,873:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_409.pth.tar
+2022-11-18 15:24:21,873:INFO: 
+===> EPOCH: 410 (P2)
+2022-11-18 15:24:21,874:INFO: - Computing loss (training)
+2022-11-18 15:24:22,220:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9787 (1.9787)
+2022-11-18 15:24:22,373:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8316 (1.9067)
+2022-11-18 15:24:22,525:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8579 (1.8898)
+2022-11-18 15:24:22,645:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9584 (1.9007)
+2022-11-18 15:24:23,132:INFO: Dataset: univ                Batch:  1/15	Loss 1.8583 (1.8583)
+2022-11-18 15:24:23,336:INFO: Dataset: univ                Batch:  2/15	Loss 1.9400 (1.8975)
+2022-11-18 15:24:23,542:INFO: Dataset: univ                Batch:  3/15	Loss 1.8639 (1.8865)
+2022-11-18 15:24:23,730:INFO: Dataset: univ                Batch:  4/15	Loss 1.9282 (1.8962)
+2022-11-18 15:24:23,899:INFO: Dataset: univ                Batch:  5/15	Loss 1.8747 (1.8920)
+2022-11-18 15:24:24,061:INFO: Dataset: univ                Batch:  6/15	Loss 1.9159 (1.8962)
+2022-11-18 15:24:24,228:INFO: Dataset: univ                Batch:  7/15	Loss 1.9113 (1.8984)
+2022-11-18 15:24:24,411:INFO: Dataset: univ                Batch:  8/15	Loss 1.8744 (1.8951)
+2022-11-18 15:24:24,595:INFO: Dataset: univ                Batch:  9/15	Loss 1.8473 (1.8901)
+2022-11-18 15:24:24,764:INFO: Dataset: univ                Batch: 10/15	Loss 1.9386 (1.8954)
+2022-11-18 15:24:24,933:INFO: Dataset: univ                Batch: 11/15	Loss 1.9028 (1.8960)
+2022-11-18 15:24:25,182:INFO: Dataset: univ                Batch: 12/15	Loss 1.8711 (1.8939)
+2022-11-18 15:24:25,341:INFO: Dataset: univ                Batch: 13/15	Loss 1.8795 (1.8929)
+2022-11-18 15:24:25,501:INFO: Dataset: univ                Batch: 14/15	Loss 1.8559 (1.8902)
+2022-11-18 15:24:25,585:INFO: Dataset: univ                Batch: 15/15	Loss 1.7717 (1.8889)
+2022-11-18 15:24:25,988:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0116 (2.0116)
+2022-11-18 15:24:26,150:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9292 (1.9735)
+2022-11-18 15:24:26,315:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9054 (1.9513)
+2022-11-18 15:24:26,479:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9180 (1.9427)
+2022-11-18 15:24:26,667:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8813 (1.9307)
+2022-11-18 15:24:26,841:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9468 (1.9332)
+2022-11-18 15:24:27,077:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8546 (1.9222)
+2022-11-18 15:24:27,247:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9750 (1.9284)
+2022-11-18 15:24:27,841:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9647 (1.9647)
+2022-11-18 15:24:28,020:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8676 (1.9186)
+2022-11-18 15:24:28,184:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8826 (1.9057)
+2022-11-18 15:24:28,338:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8828 (1.8999)
+2022-11-18 15:24:28,505:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9243 (1.9048)
+2022-11-18 15:24:28,659:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9289 (1.9089)
+2022-11-18 15:24:28,813:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8584 (1.9018)
+2022-11-18 15:24:28,969:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8137 (1.8914)
+2022-11-18 15:24:29,124:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8788 (1.8900)
+2022-11-18 15:24:29,274:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9160 (1.8927)
+2022-11-18 15:24:29,430:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8233 (1.8864)
+2022-11-18 15:24:29,579:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8736 (1.8853)
+2022-11-18 15:24:29,733:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8942 (1.8860)
+2022-11-18 15:24:29,887:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8436 (1.8829)
+2022-11-18 15:24:30,044:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8021 (1.8773)
+2022-11-18 15:24:30,199:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9117 (1.8794)
+2022-11-18 15:24:30,355:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8848 (1.8797)
+2022-11-18 15:24:30,495:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8677 (1.8791)
+2022-11-18 15:24:30,540:INFO: - Computing loss (validation)
+2022-11-18 15:24:30,846:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0003 (2.0003)
+2022-11-18 15:24:30,880:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0900 (2.0077)
+2022-11-18 15:24:31,185:INFO: Dataset: univ                Batch: 1/3	Loss 1.9200 (1.9200)
+2022-11-18 15:24:31,262:INFO: Dataset: univ                Batch: 2/3	Loss 1.8456 (1.8843)
+2022-11-18 15:24:31,337:INFO: Dataset: univ                Batch: 3/3	Loss 1.8826 (1.8837)
+2022-11-18 15:24:31,641:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8636 (1.8636)
+2022-11-18 15:24:31,689:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9148 (1.8763)
+2022-11-18 15:24:32,002:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8131 (1.8131)
+2022-11-18 15:24:32,079:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8978 (1.8556)
+2022-11-18 15:24:32,159:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8924 (1.8675)
+2022-11-18 15:24:32,238:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9046 (1.8768)
+2022-11-18 15:24:32,315:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9173 (1.8848)
+2022-11-18 15:24:32,368:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_410.pth.tar
+2022-11-18 15:24:32,368:INFO: 
+===> EPOCH: 411 (P2)
+2022-11-18 15:24:32,369:INFO: - Computing loss (training)
+2022-11-18 15:24:32,707:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8143 (1.8143)
+2022-11-18 15:24:32,861:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0105 (1.9092)
+2022-11-18 15:24:33,015:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9018 (1.9068)
+2022-11-18 15:24:33,133:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0348 (1.9282)
+2022-11-18 15:24:33,542:INFO: Dataset: univ                Batch:  1/15	Loss 1.9075 (1.9075)
+2022-11-18 15:24:33,699:INFO: Dataset: univ                Batch:  2/15	Loss 1.8414 (1.8741)
+2022-11-18 15:24:33,859:INFO: Dataset: univ                Batch:  3/15	Loss 1.8680 (1.8722)
+2022-11-18 15:24:34,015:INFO: Dataset: univ                Batch:  4/15	Loss 1.9039 (1.8805)
+2022-11-18 15:24:34,175:INFO: Dataset: univ                Batch:  5/15	Loss 1.9131 (1.8870)
+2022-11-18 15:24:34,334:INFO: Dataset: univ                Batch:  6/15	Loss 1.8510 (1.8812)
+2022-11-18 15:24:34,492:INFO: Dataset: univ                Batch:  7/15	Loss 1.8723 (1.8799)
+2022-11-18 15:24:34,648:INFO: Dataset: univ                Batch:  8/15	Loss 1.8550 (1.8767)
+2022-11-18 15:24:34,807:INFO: Dataset: univ                Batch:  9/15	Loss 1.8920 (1.8786)
+2022-11-18 15:24:34,966:INFO: Dataset: univ                Batch: 10/15	Loss 1.8709 (1.8778)
+2022-11-18 15:24:35,128:INFO: Dataset: univ                Batch: 11/15	Loss 1.8783 (1.8779)
+2022-11-18 15:24:35,288:INFO: Dataset: univ                Batch: 12/15	Loss 1.8616 (1.8764)
+2022-11-18 15:24:35,446:INFO: Dataset: univ                Batch: 13/15	Loss 1.8569 (1.8752)
+2022-11-18 15:24:35,604:INFO: Dataset: univ                Batch: 14/15	Loss 1.9137 (1.8780)
+2022-11-18 15:24:35,698:INFO: Dataset: univ                Batch: 15/15	Loss 1.8731 (1.8779)
+2022-11-18 15:24:36,104:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0376 (2.0376)
+2022-11-18 15:24:36,259:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8189 (1.9263)
+2022-11-18 15:24:36,413:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9512 (1.9340)
+2022-11-18 15:24:36,562:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0143 (1.9544)
+2022-11-18 15:24:36,715:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8414 (1.9334)
+2022-11-18 15:24:36,868:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9003 (1.9273)
+2022-11-18 15:24:37,021:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9001 (1.9233)
+2022-11-18 15:24:37,159:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8141 (1.9127)
+2022-11-18 15:24:37,554:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9182 (1.9182)
+2022-11-18 15:24:37,714:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8676 (1.8922)
+2022-11-18 15:24:37,868:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8919 (1.8921)
+2022-11-18 15:24:38,020:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9263 (1.9009)
+2022-11-18 15:24:38,175:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9204 (1.9043)
+2022-11-18 15:24:38,333:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7931 (1.8860)
+2022-11-18 15:24:38,487:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8574 (1.8823)
+2022-11-18 15:24:38,637:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9226 (1.8873)
+2022-11-18 15:24:38,790:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8932 (1.8879)
+2022-11-18 15:24:38,943:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8181 (1.8804)
+2022-11-18 15:24:39,098:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8415 (1.8769)
+2022-11-18 15:24:39,249:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8981 (1.8789)
+2022-11-18 15:24:39,401:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8188 (1.8742)
+2022-11-18 15:24:39,553:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8756 (1.8743)
+2022-11-18 15:24:39,709:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8808 (1.8747)
+2022-11-18 15:24:39,862:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8293 (1.8716)
+2022-11-18 15:24:40,016:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8087 (1.8681)
+2022-11-18 15:24:40,156:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9001 (1.8698)
+2022-11-18 15:24:40,205:INFO: - Computing loss (validation)
+2022-11-18 15:24:40,484:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8917 (1.8917)
+2022-11-18 15:24:40,520:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8855 (1.8912)
+2022-11-18 15:24:40,834:INFO: Dataset: univ                Batch: 1/3	Loss 1.9355 (1.9355)
+2022-11-18 15:24:40,915:INFO: Dataset: univ                Batch: 2/3	Loss 1.9177 (1.9262)
+2022-11-18 15:24:40,990:INFO: Dataset: univ                Batch: 3/3	Loss 1.8816 (1.9129)
+2022-11-18 15:24:41,296:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8041 (1.8041)
+2022-11-18 15:24:41,342:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7832 (1.7986)
+2022-11-18 15:24:41,692:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9503 (1.9503)
+2022-11-18 15:24:41,786:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9241 (1.9374)
+2022-11-18 15:24:41,868:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7536 (1.8759)
+2022-11-18 15:24:41,951:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8903 (1.8798)
+2022-11-18 15:24:42,030:INFO: Dataset: zara2               Batch: 5/5	Loss 2.0108 (1.9060)
+2022-11-18 15:24:42,084:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_411.pth.tar
+2022-11-18 15:24:42,084:INFO: 
+===> EPOCH: 412 (P2)
+2022-11-18 15:24:42,084:INFO: - Computing loss (training)
+2022-11-18 15:24:42,426:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9506 (1.9506)
+2022-11-18 15:24:42,578:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8548 (1.9025)
+2022-11-18 15:24:42,729:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8912 (1.8987)
+2022-11-18 15:24:42,844:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8834 (1.8961)
+2022-11-18 15:24:43,252:INFO: Dataset: univ                Batch:  1/15	Loss 1.9480 (1.9480)
+2022-11-18 15:24:43,414:INFO: Dataset: univ                Batch:  2/15	Loss 1.8732 (1.9097)
+2022-11-18 15:24:43,575:INFO: Dataset: univ                Batch:  3/15	Loss 1.8952 (1.9049)
+2022-11-18 15:24:43,735:INFO: Dataset: univ                Batch:  4/15	Loss 1.8468 (1.8911)
+2022-11-18 15:24:43,895:INFO: Dataset: univ                Batch:  5/15	Loss 1.8516 (1.8825)
+2022-11-18 15:24:44,053:INFO: Dataset: univ                Batch:  6/15	Loss 1.8635 (1.8794)
+2022-11-18 15:24:44,212:INFO: Dataset: univ                Batch:  7/15	Loss 1.8555 (1.8761)
+2022-11-18 15:24:44,368:INFO: Dataset: univ                Batch:  8/15	Loss 1.8726 (1.8756)
+2022-11-18 15:24:44,527:INFO: Dataset: univ                Batch:  9/15	Loss 1.9027 (1.8789)
+2022-11-18 15:24:44,685:INFO: Dataset: univ                Batch: 10/15	Loss 1.8768 (1.8787)
+2022-11-18 15:24:44,845:INFO: Dataset: univ                Batch: 11/15	Loss 1.8482 (1.8760)
+2022-11-18 15:24:45,002:INFO: Dataset: univ                Batch: 12/15	Loss 1.8712 (1.8756)
+2022-11-18 15:24:45,160:INFO: Dataset: univ                Batch: 13/15	Loss 1.8517 (1.8739)
+2022-11-18 15:24:45,320:INFO: Dataset: univ                Batch: 14/15	Loss 1.9042 (1.8763)
+2022-11-18 15:24:45,406:INFO: Dataset: univ                Batch: 15/15	Loss 1.9019 (1.8766)
+2022-11-18 15:24:45,811:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9337 (1.9337)
+2022-11-18 15:24:45,966:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9146 (1.9242)
+2022-11-18 15:24:46,122:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9082 (1.9186)
+2022-11-18 15:24:46,272:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9623 (1.9296)
+2022-11-18 15:24:46,423:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8181 (1.9057)
+2022-11-18 15:24:46,574:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8941 (1.9039)
+2022-11-18 15:24:46,726:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8359 (1.8944)
+2022-11-18 15:24:46,864:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9487 (1.9001)
+2022-11-18 15:24:47,282:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9223 (1.9223)
+2022-11-18 15:24:47,440:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9007 (1.9117)
+2022-11-18 15:24:47,603:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9584 (1.9266)
+2022-11-18 15:24:47,761:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9735 (1.9388)
+2022-11-18 15:24:47,926:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9417 (1.9394)
+2022-11-18 15:24:48,092:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8444 (1.9236)
+2022-11-18 15:24:48,257:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8472 (1.9112)
+2022-11-18 15:24:48,413:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8661 (1.9059)
+2022-11-18 15:24:48,585:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8490 (1.8997)
+2022-11-18 15:24:48,736:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8978 (1.8995)
+2022-11-18 15:24:48,931:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8086 (1.8908)
+2022-11-18 15:24:49,150:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8970 (1.8913)
+2022-11-18 15:24:49,339:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8226 (1.8858)
+2022-11-18 15:24:49,553:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8422 (1.8824)
+2022-11-18 15:24:49,733:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8573 (1.8808)
+2022-11-18 15:24:49,892:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8650 (1.8798)
+2022-11-18 15:24:50,044:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7175 (1.8705)
+2022-11-18 15:24:50,184:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7811 (1.8661)
+2022-11-18 15:24:50,229:INFO: - Computing loss (validation)
+2022-11-18 15:24:50,514:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8471 (1.8471)
+2022-11-18 15:24:50,553:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6859 (1.8361)
+2022-11-18 15:24:50,892:INFO: Dataset: univ                Batch: 1/3	Loss 1.8841 (1.8841)
+2022-11-18 15:24:50,971:INFO: Dataset: univ                Batch: 2/3	Loss 1.8950 (1.8892)
+2022-11-18 15:24:51,047:INFO: Dataset: univ                Batch: 3/3	Loss 1.9548 (1.9091)
+2022-11-18 15:24:51,355:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9121 (1.9121)
+2022-11-18 15:24:51,415:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9823 (1.9299)
+2022-11-18 15:24:51,780:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8769 (1.8769)
+2022-11-18 15:24:51,857:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8294 (1.8529)
+2022-11-18 15:24:51,935:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9466 (1.8822)
+2022-11-18 15:24:52,013:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9014 (1.8868)
+2022-11-18 15:24:52,087:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9513 (1.8991)
+2022-11-18 15:24:52,145:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_412.pth.tar
+2022-11-18 15:24:52,145:INFO: 
+===> EPOCH: 413 (P2)
+2022-11-18 15:24:52,146:INFO: - Computing loss (training)
+2022-11-18 15:24:52,497:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9482 (1.9482)
+2022-11-18 15:24:52,654:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0039 (1.9770)
+2022-11-18 15:24:52,806:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9967 (1.9837)
+2022-11-18 15:24:52,938:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0658 (1.9975)
+2022-11-18 15:24:53,368:INFO: Dataset: univ                Batch:  1/15	Loss 1.8952 (1.8952)
+2022-11-18 15:24:53,568:INFO: Dataset: univ                Batch:  2/15	Loss 1.8592 (1.8775)
+2022-11-18 15:24:53,767:INFO: Dataset: univ                Batch:  3/15	Loss 1.8947 (1.8831)
+2022-11-18 15:24:53,958:INFO: Dataset: univ                Batch:  4/15	Loss 1.8555 (1.8764)
+2022-11-18 15:24:54,144:INFO: Dataset: univ                Batch:  5/15	Loss 1.8626 (1.8737)
+2022-11-18 15:24:54,340:INFO: Dataset: univ                Batch:  6/15	Loss 1.8183 (1.8639)
+2022-11-18 15:24:54,515:INFO: Dataset: univ                Batch:  7/15	Loss 1.8788 (1.8659)
+2022-11-18 15:24:54,685:INFO: Dataset: univ                Batch:  8/15	Loss 1.9039 (1.8705)
+2022-11-18 15:24:54,891:INFO: Dataset: univ                Batch:  9/15	Loss 1.8868 (1.8724)
+2022-11-18 15:24:55,059:INFO: Dataset: univ                Batch: 10/15	Loss 1.8850 (1.8737)
+2022-11-18 15:24:55,224:INFO: Dataset: univ                Batch: 11/15	Loss 1.9160 (1.8776)
+2022-11-18 15:24:55,390:INFO: Dataset: univ                Batch: 12/15	Loss 1.8683 (1.8768)
+2022-11-18 15:24:55,591:INFO: Dataset: univ                Batch: 13/15	Loss 1.8572 (1.8754)
+2022-11-18 15:24:55,783:INFO: Dataset: univ                Batch: 14/15	Loss 1.8990 (1.8772)
+2022-11-18 15:24:55,883:INFO: Dataset: univ                Batch: 15/15	Loss 1.8367 (1.8767)
+2022-11-18 15:24:56,301:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9110 (1.9110)
+2022-11-18 15:24:56,458:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9425 (1.9277)
+2022-11-18 15:24:56,622:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9328 (1.9293)
+2022-11-18 15:24:56,793:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9674 (1.9403)
+2022-11-18 15:24:56,979:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9164 (1.9355)
+2022-11-18 15:24:57,161:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9745 (1.9416)
+2022-11-18 15:24:57,332:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0404 (1.9559)
+2022-11-18 15:24:57,491:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9231 (1.9526)
+2022-11-18 15:24:57,903:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8939 (1.8939)
+2022-11-18 15:24:58,073:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7696 (1.8375)
+2022-11-18 15:24:58,244:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8946 (1.8547)
+2022-11-18 15:24:58,450:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8792 (1.8604)
+2022-11-18 15:24:58,628:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9341 (1.8746)
+2022-11-18 15:24:58,817:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9148 (1.8808)
+2022-11-18 15:24:59,003:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8507 (1.8770)
+2022-11-18 15:24:59,181:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8950 (1.8796)
+2022-11-18 15:24:59,360:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8196 (1.8733)
+2022-11-18 15:24:59,523:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8088 (1.8671)
+2022-11-18 15:24:59,677:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8300 (1.8636)
+2022-11-18 15:24:59,859:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8646 (1.8637)
+2022-11-18 15:25:00,040:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9394 (1.8695)
+2022-11-18 15:25:00,228:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9129 (1.8722)
+2022-11-18 15:25:00,403:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7403 (1.8639)
+2022-11-18 15:25:00,608:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9255 (1.8675)
+2022-11-18 15:25:00,770:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9194 (1.8707)
+2022-11-18 15:25:00,925:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8329 (1.8688)
+2022-11-18 15:25:00,971:INFO: - Computing loss (validation)
+2022-11-18 15:25:01,245:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8619 (1.8619)
+2022-11-18 15:25:01,280:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8155 (1.8584)
+2022-11-18 15:25:01,626:INFO: Dataset: univ                Batch: 1/3	Loss 1.9469 (1.9469)
+2022-11-18 15:25:01,711:INFO: Dataset: univ                Batch: 2/3	Loss 1.8594 (1.8994)
+2022-11-18 15:25:01,785:INFO: Dataset: univ                Batch: 3/3	Loss 1.8921 (1.8973)
+2022-11-18 15:25:02,086:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9028 (1.9028)
+2022-11-18 15:25:02,135:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0368 (1.9351)
+2022-11-18 15:25:02,441:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8345 (1.8345)
+2022-11-18 15:25:02,518:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8414 (1.8381)
+2022-11-18 15:25:02,595:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9890 (1.8884)
+2022-11-18 15:25:02,671:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8458 (1.8772)
+2022-11-18 15:25:02,747:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8849 (1.8788)
+2022-11-18 15:25:02,804:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_413.pth.tar
+2022-11-18 15:25:02,804:INFO: 
+===> EPOCH: 414 (P2)
+2022-11-18 15:25:02,805:INFO: - Computing loss (training)
+2022-11-18 15:25:03,163:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9017 (1.9017)
+2022-11-18 15:25:03,319:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8944 (1.8980)
+2022-11-18 15:25:03,473:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8391 (1.8776)
+2022-11-18 15:25:03,592:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7993 (1.8644)
+2022-11-18 15:25:04,001:INFO: Dataset: univ                Batch:  1/15	Loss 1.8761 (1.8761)
+2022-11-18 15:25:04,159:INFO: Dataset: univ                Batch:  2/15	Loss 1.9047 (1.8917)
+2022-11-18 15:25:04,318:INFO: Dataset: univ                Batch:  3/15	Loss 1.8580 (1.8815)
+2022-11-18 15:25:04,474:INFO: Dataset: univ                Batch:  4/15	Loss 1.8430 (1.8721)
+2022-11-18 15:25:04,637:INFO: Dataset: univ                Batch:  5/15	Loss 1.8648 (1.8708)
+2022-11-18 15:25:04,794:INFO: Dataset: univ                Batch:  6/15	Loss 1.8230 (1.8634)
+2022-11-18 15:25:04,952:INFO: Dataset: univ                Batch:  7/15	Loss 1.9049 (1.8693)
+2022-11-18 15:25:05,107:INFO: Dataset: univ                Batch:  8/15	Loss 1.8543 (1.8675)
+2022-11-18 15:25:05,265:INFO: Dataset: univ                Batch:  9/15	Loss 1.8585 (1.8665)
+2022-11-18 15:25:05,423:INFO: Dataset: univ                Batch: 10/15	Loss 1.8463 (1.8645)
+2022-11-18 15:25:05,589:INFO: Dataset: univ                Batch: 11/15	Loss 1.8744 (1.8655)
+2022-11-18 15:25:05,746:INFO: Dataset: univ                Batch: 12/15	Loss 1.8932 (1.8679)
+2022-11-18 15:25:05,905:INFO: Dataset: univ                Batch: 13/15	Loss 1.8897 (1.8695)
+2022-11-18 15:25:06,062:INFO: Dataset: univ                Batch: 14/15	Loss 1.8985 (1.8716)
+2022-11-18 15:25:06,147:INFO: Dataset: univ                Batch: 15/15	Loss 1.8474 (1.8712)
+2022-11-18 15:25:06,552:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8418 (1.8418)
+2022-11-18 15:25:06,703:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9075 (1.8732)
+2022-11-18 15:25:06,859:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9575 (1.9010)
+2022-11-18 15:25:07,011:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8600 (1.8898)
+2022-11-18 15:25:07,169:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9285 (1.8965)
+2022-11-18 15:25:07,319:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8993 (1.8970)
+2022-11-18 15:25:07,472:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7817 (1.8812)
+2022-11-18 15:25:07,609:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9326 (1.8865)
+2022-11-18 15:25:08,014:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9611 (1.9611)
+2022-11-18 15:25:08,167:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9161 (1.9404)
+2022-11-18 15:25:08,323:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0177 (1.9671)
+2022-11-18 15:25:08,477:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8697 (1.9443)
+2022-11-18 15:25:08,632:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9068 (1.9364)
+2022-11-18 15:25:08,785:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9133 (1.9327)
+2022-11-18 15:25:08,941:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8061 (1.9143)
+2022-11-18 15:25:09,090:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9089 (1.9136)
+2022-11-18 15:25:09,242:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8378 (1.9044)
+2022-11-18 15:25:09,393:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8509 (1.8993)
+2022-11-18 15:25:09,547:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8581 (1.8953)
+2022-11-18 15:25:09,696:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8538 (1.8916)
+2022-11-18 15:25:09,850:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8771 (1.8905)
+2022-11-18 15:25:10,002:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8592 (1.8882)
+2022-11-18 15:25:10,156:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9157 (1.8901)
+2022-11-18 15:25:10,308:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0250 (1.8987)
+2022-11-18 15:25:10,462:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9625 (1.9024)
+2022-11-18 15:25:10,602:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8562 (1.9001)
+2022-11-18 15:25:10,647:INFO: - Computing loss (validation)
+2022-11-18 15:25:10,904:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0057 (2.0057)
+2022-11-18 15:25:10,939:INFO: Dataset: hotel               Batch: 2/2	Loss 2.5119 (2.0437)
+2022-11-18 15:25:11,248:INFO: Dataset: univ                Batch: 1/3	Loss 1.8578 (1.8578)
+2022-11-18 15:25:11,327:INFO: Dataset: univ                Batch: 2/3	Loss 1.8780 (1.8669)
+2022-11-18 15:25:11,401:INFO: Dataset: univ                Batch: 3/3	Loss 1.9486 (1.8945)
+2022-11-18 15:25:11,702:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9797 (1.9797)
+2022-11-18 15:25:11,748:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9403 (1.9691)
+2022-11-18 15:25:12,085:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9627 (1.9627)
+2022-11-18 15:25:12,165:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8142 (1.8902)
+2022-11-18 15:25:12,245:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7895 (1.8558)
+2022-11-18 15:25:12,325:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8653 (1.8583)
+2022-11-18 15:25:12,403:INFO: Dataset: zara2               Batch: 5/5	Loss 2.0008 (1.8856)
+2022-11-18 15:25:12,455:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_414.pth.tar
+2022-11-18 15:25:12,455:INFO: 
+===> EPOCH: 415 (P2)
+2022-11-18 15:25:12,456:INFO: - Computing loss (training)
+2022-11-18 15:25:12,796:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8962 (1.8962)
+2022-11-18 15:25:12,952:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9412 (1.9187)
+2022-11-18 15:25:13,106:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8933 (1.9095)
+2022-11-18 15:25:13,224:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1446 (1.9482)
+2022-11-18 15:25:13,620:INFO: Dataset: univ                Batch:  1/15	Loss 1.8601 (1.8601)
+2022-11-18 15:25:13,780:INFO: Dataset: univ                Batch:  2/15	Loss 1.8497 (1.8552)
+2022-11-18 15:25:13,943:INFO: Dataset: univ                Batch:  3/15	Loss 1.8899 (1.8679)
+2022-11-18 15:25:14,106:INFO: Dataset: univ                Batch:  4/15	Loss 1.8711 (1.8687)
+2022-11-18 15:25:14,270:INFO: Dataset: univ                Batch:  5/15	Loss 1.8595 (1.8669)
+2022-11-18 15:25:14,427:INFO: Dataset: univ                Batch:  6/15	Loss 1.8618 (1.8661)
+2022-11-18 15:25:14,583:INFO: Dataset: univ                Batch:  7/15	Loss 1.9186 (1.8740)
+2022-11-18 15:25:14,736:INFO: Dataset: univ                Batch:  8/15	Loss 1.8694 (1.8735)
+2022-11-18 15:25:14,893:INFO: Dataset: univ                Batch:  9/15	Loss 1.8656 (1.8726)
+2022-11-18 15:25:15,047:INFO: Dataset: univ                Batch: 10/15	Loss 1.8350 (1.8689)
+2022-11-18 15:25:15,203:INFO: Dataset: univ                Batch: 11/15	Loss 1.8216 (1.8646)
+2022-11-18 15:25:15,358:INFO: Dataset: univ                Batch: 12/15	Loss 1.8469 (1.8633)
+2022-11-18 15:25:15,515:INFO: Dataset: univ                Batch: 13/15	Loss 1.8541 (1.8626)
+2022-11-18 15:25:15,668:INFO: Dataset: univ                Batch: 14/15	Loss 1.8878 (1.8644)
+2022-11-18 15:25:15,751:INFO: Dataset: univ                Batch: 15/15	Loss 1.8512 (1.8642)
+2022-11-18 15:25:16,135:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8922 (1.8922)
+2022-11-18 15:25:16,287:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8489 (1.8712)
+2022-11-18 15:25:16,443:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8256 (1.8556)
+2022-11-18 15:25:16,595:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0269 (1.8991)
+2022-11-18 15:25:16,746:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8249 (1.8850)
+2022-11-18 15:25:16,898:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8775 (1.8838)
+2022-11-18 15:25:17,050:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8234 (1.8757)
+2022-11-18 15:25:17,188:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9197 (1.8806)
+2022-11-18 15:25:17,595:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8911 (1.8911)
+2022-11-18 15:25:17,749:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8721 (1.8813)
+2022-11-18 15:25:17,902:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9114 (1.8918)
+2022-11-18 15:25:18,056:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7888 (1.8665)
+2022-11-18 15:25:18,212:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9211 (1.8791)
+2022-11-18 15:25:18,364:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8592 (1.8757)
+2022-11-18 15:25:18,517:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8322 (1.8687)
+2022-11-18 15:25:18,666:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8563 (1.8672)
+2022-11-18 15:25:18,818:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8876 (1.8697)
+2022-11-18 15:25:18,971:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8553 (1.8682)
+2022-11-18 15:25:19,122:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9356 (1.8745)
+2022-11-18 15:25:19,274:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9894 (1.8842)
+2022-11-18 15:25:19,428:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8338 (1.8805)
+2022-11-18 15:25:19,579:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8261 (1.8765)
+2022-11-18 15:25:19,732:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8484 (1.8745)
+2022-11-18 15:25:19,881:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8134 (1.8710)
+2022-11-18 15:25:20,035:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8567 (1.8702)
+2022-11-18 15:25:20,174:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8661 (1.8700)
+2022-11-18 15:25:20,219:INFO: - Computing loss (validation)
+2022-11-18 15:25:20,495:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0029 (2.0029)
+2022-11-18 15:25:20,531:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6472 (1.9725)
+2022-11-18 15:25:20,843:INFO: Dataset: univ                Batch: 1/3	Loss 1.8787 (1.8787)
+2022-11-18 15:25:20,922:INFO: Dataset: univ                Batch: 2/3	Loss 1.9169 (1.8983)
+2022-11-18 15:25:20,994:INFO: Dataset: univ                Batch: 3/3	Loss 1.8798 (1.8929)
+2022-11-18 15:25:21,293:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8293 (1.8293)
+2022-11-18 15:25:21,339:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9189 (1.8484)
+2022-11-18 15:25:21,659:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8720 (1.8720)
+2022-11-18 15:25:21,741:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8460 (1.8593)
+2022-11-18 15:25:21,819:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9322 (1.8833)
+2022-11-18 15:25:21,898:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9483 (1.8988)
+2022-11-18 15:25:21,976:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8934 (1.8978)
+2022-11-18 15:25:22,030:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_415.pth.tar
+2022-11-18 15:25:22,030:INFO: 
+===> EPOCH: 416 (P2)
+2022-11-18 15:25:22,031:INFO: - Computing loss (training)
+2022-11-18 15:25:22,379:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9177 (1.9177)
+2022-11-18 15:25:22,535:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8277 (1.8707)
+2022-11-18 15:25:22,687:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9927 (1.9102)
+2022-11-18 15:25:22,806:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8335 (1.8963)
+2022-11-18 15:25:23,225:INFO: Dataset: univ                Batch:  1/15	Loss 1.8624 (1.8624)
+2022-11-18 15:25:23,384:INFO: Dataset: univ                Batch:  2/15	Loss 1.9060 (1.8845)
+2022-11-18 15:25:23,545:INFO: Dataset: univ                Batch:  3/15	Loss 1.8361 (1.8689)
+2022-11-18 15:25:23,704:INFO: Dataset: univ                Batch:  4/15	Loss 1.9224 (1.8828)
+2022-11-18 15:25:23,866:INFO: Dataset: univ                Batch:  5/15	Loss 1.8879 (1.8838)
+2022-11-18 15:25:24,027:INFO: Dataset: univ                Batch:  6/15	Loss 1.9192 (1.8893)
+2022-11-18 15:25:24,186:INFO: Dataset: univ                Batch:  7/15	Loss 1.8441 (1.8824)
+2022-11-18 15:25:24,341:INFO: Dataset: univ                Batch:  8/15	Loss 1.8983 (1.8842)
+2022-11-18 15:25:24,500:INFO: Dataset: univ                Batch:  9/15	Loss 1.8630 (1.8817)
+2022-11-18 15:25:24,655:INFO: Dataset: univ                Batch: 10/15	Loss 1.8974 (1.8833)
+2022-11-18 15:25:24,816:INFO: Dataset: univ                Batch: 11/15	Loss 1.8896 (1.8839)
+2022-11-18 15:25:24,972:INFO: Dataset: univ                Batch: 12/15	Loss 1.8658 (1.8826)
+2022-11-18 15:25:25,131:INFO: Dataset: univ                Batch: 13/15	Loss 1.8612 (1.8810)
+2022-11-18 15:25:25,289:INFO: Dataset: univ                Batch: 14/15	Loss 1.9141 (1.8835)
+2022-11-18 15:25:25,375:INFO: Dataset: univ                Batch: 15/15	Loss 1.9312 (1.8841)
+2022-11-18 15:25:25,764:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7493 (1.7493)
+2022-11-18 15:25:25,914:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7605 (1.7552)
+2022-11-18 15:25:26,064:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8832 (1.7969)
+2022-11-18 15:25:26,212:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9559 (1.8398)
+2022-11-18 15:25:26,362:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9170 (1.8548)
+2022-11-18 15:25:26,514:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9638 (1.8759)
+2022-11-18 15:25:26,664:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8173 (1.8680)
+2022-11-18 15:25:26,799:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0114 (1.8840)
+2022-11-18 15:25:27,193:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8979 (1.8979)
+2022-11-18 15:25:27,349:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9103 (1.9045)
+2022-11-18 15:25:27,504:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8654 (1.8907)
+2022-11-18 15:25:27,655:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9586 (1.9059)
+2022-11-18 15:25:27,809:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8699 (1.8979)
+2022-11-18 15:25:27,963:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7919 (1.8790)
+2022-11-18 15:25:28,114:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7777 (1.8638)
+2022-11-18 15:25:28,264:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9049 (1.8691)
+2022-11-18 15:25:28,416:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8819 (1.8707)
+2022-11-18 15:25:28,567:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9143 (1.8750)
+2022-11-18 15:25:28,720:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9214 (1.8785)
+2022-11-18 15:25:28,872:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8865 (1.8792)
+2022-11-18 15:25:29,025:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9188 (1.8823)
+2022-11-18 15:25:29,181:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9048 (1.8840)
+2022-11-18 15:25:29,335:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8556 (1.8821)
+2022-11-18 15:25:29,486:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9251 (1.8848)
+2022-11-18 15:25:29,640:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9064 (1.8861)
+2022-11-18 15:25:29,779:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8531 (1.8846)
+2022-11-18 15:25:29,824:INFO: - Computing loss (validation)
+2022-11-18 15:25:30,101:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8466 (1.8466)
+2022-11-18 15:25:30,137:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7402 (1.8379)
+2022-11-18 15:25:30,453:INFO: Dataset: univ                Batch: 1/3	Loss 1.8925 (1.8925)
+2022-11-18 15:25:30,535:INFO: Dataset: univ                Batch: 2/3	Loss 1.8840 (1.8885)
+2022-11-18 15:25:30,610:INFO: Dataset: univ                Batch: 3/3	Loss 1.7957 (1.8627)
+2022-11-18 15:25:30,910:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8302 (1.8302)
+2022-11-18 15:25:30,956:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7578 (1.8125)
+2022-11-18 15:25:31,260:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8707 (1.8707)
+2022-11-18 15:25:31,337:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8543 (1.8630)
+2022-11-18 15:25:31,412:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8422 (1.8563)
+2022-11-18 15:25:31,487:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9179 (1.8717)
+2022-11-18 15:25:31,561:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9022 (1.8778)
+2022-11-18 15:25:31,616:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_416.pth.tar
+2022-11-18 15:25:31,617:INFO: 
+===> EPOCH: 417 (P2)
+2022-11-18 15:25:31,617:INFO: - Computing loss (training)
+2022-11-18 15:25:31,969:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8876 (1.8876)
+2022-11-18 15:25:32,124:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8396 (1.8636)
+2022-11-18 15:25:32,277:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8201 (1.8501)
+2022-11-18 15:25:32,395:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8503 (1.8502)
+2022-11-18 15:25:32,791:INFO: Dataset: univ                Batch:  1/15	Loss 1.8481 (1.8481)
+2022-11-18 15:25:32,954:INFO: Dataset: univ                Batch:  2/15	Loss 1.8647 (1.8565)
+2022-11-18 15:25:33,116:INFO: Dataset: univ                Batch:  3/15	Loss 1.8901 (1.8680)
+2022-11-18 15:25:33,274:INFO: Dataset: univ                Batch:  4/15	Loss 1.8901 (1.8735)
+2022-11-18 15:25:33,436:INFO: Dataset: univ                Batch:  5/15	Loss 1.9032 (1.8794)
+2022-11-18 15:25:33,592:INFO: Dataset: univ                Batch:  6/15	Loss 1.9205 (1.8862)
+2022-11-18 15:25:33,749:INFO: Dataset: univ                Batch:  7/15	Loss 1.8552 (1.8818)
+2022-11-18 15:25:33,903:INFO: Dataset: univ                Batch:  8/15	Loss 1.8979 (1.8838)
+2022-11-18 15:25:34,060:INFO: Dataset: univ                Batch:  9/15	Loss 1.8562 (1.8807)
+2022-11-18 15:25:34,214:INFO: Dataset: univ                Batch: 10/15	Loss 1.8263 (1.8759)
+2022-11-18 15:25:34,374:INFO: Dataset: univ                Batch: 11/15	Loss 1.8287 (1.8714)
+2022-11-18 15:25:34,530:INFO: Dataset: univ                Batch: 12/15	Loss 1.8323 (1.8684)
+2022-11-18 15:25:34,687:INFO: Dataset: univ                Batch: 13/15	Loss 1.8823 (1.8694)
+2022-11-18 15:25:34,842:INFO: Dataset: univ                Batch: 14/15	Loss 1.9094 (1.8720)
+2022-11-18 15:25:34,926:INFO: Dataset: univ                Batch: 15/15	Loss 1.9317 (1.8728)
+2022-11-18 15:25:35,320:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9293 (1.9293)
+2022-11-18 15:25:35,476:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9365 (1.9327)
+2022-11-18 15:25:35,629:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9664 (1.9436)
+2022-11-18 15:25:35,780:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9808 (1.9523)
+2022-11-18 15:25:35,936:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8855 (1.9379)
+2022-11-18 15:25:36,089:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8504 (1.9244)
+2022-11-18 15:25:36,243:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9215 (1.9240)
+2022-11-18 15:25:36,381:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9540 (1.9272)
+2022-11-18 15:25:36,780:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9114 (1.9114)
+2022-11-18 15:25:36,939:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8378 (1.8761)
+2022-11-18 15:25:37,093:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8276 (1.8603)
+2022-11-18 15:25:37,244:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8951 (1.8693)
+2022-11-18 15:25:37,399:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9177 (1.8792)
+2022-11-18 15:25:37,554:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8904 (1.8811)
+2022-11-18 15:25:37,707:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9585 (1.8925)
+2022-11-18 15:25:37,859:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8348 (1.8859)
+2022-11-18 15:25:38,013:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8291 (1.8805)
+2022-11-18 15:25:38,164:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8578 (1.8784)
+2022-11-18 15:25:38,320:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8130 (1.8726)
+2022-11-18 15:25:38,472:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8974 (1.8748)
+2022-11-18 15:25:38,627:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7372 (1.8647)
+2022-11-18 15:25:38,779:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8516 (1.8638)
+2022-11-18 15:25:38,938:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9060 (1.8667)
+2022-11-18 15:25:39,091:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8979 (1.8687)
+2022-11-18 15:25:39,246:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7884 (1.8637)
+2022-11-18 15:25:39,386:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8862 (1.8647)
+2022-11-18 15:25:39,434:INFO: - Computing loss (validation)
+2022-11-18 15:25:39,702:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9353 (1.9353)
+2022-11-18 15:25:39,736:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7815 (1.9263)
+2022-11-18 15:25:40,043:INFO: Dataset: univ                Batch: 1/3	Loss 1.9133 (1.9133)
+2022-11-18 15:25:40,120:INFO: Dataset: univ                Batch: 2/3	Loss 1.8505 (1.8864)
+2022-11-18 15:25:40,196:INFO: Dataset: univ                Batch: 3/3	Loss 1.8796 (1.8840)
+2022-11-18 15:25:40,508:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9517 (1.9517)
+2022-11-18 15:25:40,555:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8489 (1.9272)
+2022-11-18 15:25:40,863:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8475 (1.8475)
+2022-11-18 15:25:40,942:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8782 (1.8624)
+2022-11-18 15:25:41,021:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8535 (1.8593)
+2022-11-18 15:25:41,099:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9637 (1.8862)
+2022-11-18 15:25:41,176:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7677 (1.8635)
+2022-11-18 15:25:41,232:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_417.pth.tar
+2022-11-18 15:25:41,232:INFO: 
+===> EPOCH: 418 (P2)
+2022-11-18 15:25:41,233:INFO: - Computing loss (training)
+2022-11-18 15:25:41,581:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8822 (1.8822)
+2022-11-18 15:25:41,730:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8915 (1.8872)
+2022-11-18 15:25:41,880:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8749 (1.8832)
+2022-11-18 15:25:41,995:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8901 (1.8845)
+2022-11-18 15:25:42,407:INFO: Dataset: univ                Batch:  1/15	Loss 1.8638 (1.8638)
+2022-11-18 15:25:42,570:INFO: Dataset: univ                Batch:  2/15	Loss 1.8561 (1.8599)
+2022-11-18 15:25:42,733:INFO: Dataset: univ                Batch:  3/15	Loss 1.8384 (1.8528)
+2022-11-18 15:25:42,892:INFO: Dataset: univ                Batch:  4/15	Loss 1.8666 (1.8563)
+2022-11-18 15:25:43,055:INFO: Dataset: univ                Batch:  5/15	Loss 1.9327 (1.8712)
+2022-11-18 15:25:43,219:INFO: Dataset: univ                Batch:  6/15	Loss 1.8941 (1.8746)
+2022-11-18 15:25:43,379:INFO: Dataset: univ                Batch:  7/15	Loss 1.8988 (1.8781)
+2022-11-18 15:25:43,537:INFO: Dataset: univ                Batch:  8/15	Loss 1.8486 (1.8745)
+2022-11-18 15:25:43,696:INFO: Dataset: univ                Batch:  9/15	Loss 1.9218 (1.8798)
+2022-11-18 15:25:43,853:INFO: Dataset: univ                Batch: 10/15	Loss 1.8682 (1.8786)
+2022-11-18 15:25:44,014:INFO: Dataset: univ                Batch: 11/15	Loss 1.9022 (1.8808)
+2022-11-18 15:25:44,175:INFO: Dataset: univ                Batch: 12/15	Loss 1.9290 (1.8847)
+2022-11-18 15:25:44,334:INFO: Dataset: univ                Batch: 13/15	Loss 1.8265 (1.8805)
+2022-11-18 15:25:44,491:INFO: Dataset: univ                Batch: 14/15	Loss 1.8836 (1.8807)
+2022-11-18 15:25:44,577:INFO: Dataset: univ                Batch: 15/15	Loss 1.8343 (1.8801)
+2022-11-18 15:25:44,954:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7530 (1.7530)
+2022-11-18 15:25:45,104:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0270 (1.8805)
+2022-11-18 15:25:45,254:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8598 (1.8737)
+2022-11-18 15:25:45,405:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8653 (1.8716)
+2022-11-18 15:25:45,556:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9473 (1.8841)
+2022-11-18 15:25:45,705:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9365 (1.8932)
+2022-11-18 15:25:45,855:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9592 (1.9031)
+2022-11-18 15:25:45,992:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9258 (1.9055)
+2022-11-18 15:25:46,391:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8521 (1.8521)
+2022-11-18 15:25:46,550:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9652 (1.9086)
+2022-11-18 15:25:46,704:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9367 (1.9185)
+2022-11-18 15:25:46,855:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9825 (1.9343)
+2022-11-18 15:25:47,091:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7854 (1.9075)
+2022-11-18 15:25:47,241:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8996 (1.9062)
+2022-11-18 15:25:47,391:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9041 (1.9059)
+2022-11-18 15:25:47,539:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8355 (1.8965)
+2022-11-18 15:25:47,689:INFO: Dataset: zara2               Batch:  9/18	Loss 2.0015 (1.9083)
+2022-11-18 15:25:47,836:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8420 (1.9016)
+2022-11-18 15:25:47,988:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8289 (1.8949)
+2022-11-18 15:25:48,136:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8239 (1.8894)
+2022-11-18 15:25:48,286:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7797 (1.8809)
+2022-11-18 15:25:48,435:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9698 (1.8875)
+2022-11-18 15:25:48,586:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9079 (1.8889)
+2022-11-18 15:25:48,734:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8639 (1.8874)
+2022-11-18 15:25:48,889:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9489 (1.8909)
+2022-11-18 15:25:49,028:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7976 (1.8859)
+2022-11-18 15:25:49,076:INFO: - Computing loss (validation)
+2022-11-18 15:25:49,339:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8182 (1.8182)
+2022-11-18 15:25:49,373:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9423 (1.8270)
+2022-11-18 15:25:49,684:INFO: Dataset: univ                Batch: 1/3	Loss 1.8310 (1.8310)
+2022-11-18 15:25:49,766:INFO: Dataset: univ                Batch: 2/3	Loss 1.8753 (1.8528)
+2022-11-18 15:25:49,841:INFO: Dataset: univ                Batch: 3/3	Loss 1.9464 (1.8834)
+2022-11-18 15:25:50,146:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9123 (1.9123)
+2022-11-18 15:25:50,191:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8772 (1.9038)
+2022-11-18 15:25:50,518:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9906 (1.9906)
+2022-11-18 15:25:50,594:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9722 (1.9811)
+2022-11-18 15:25:50,671:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8473 (1.9370)
+2022-11-18 15:25:50,747:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8639 (1.9171)
+2022-11-18 15:25:50,823:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8164 (1.8968)
+2022-11-18 15:25:50,877:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_418.pth.tar
+2022-11-18 15:25:50,878:INFO: 
+===> EPOCH: 419 (P2)
+2022-11-18 15:25:50,878:INFO: - Computing loss (training)
+2022-11-18 15:25:51,228:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9259 (1.9259)
+2022-11-18 15:25:51,381:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7768 (1.8524)
+2022-11-18 15:25:51,533:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8731 (1.8590)
+2022-11-18 15:25:51,649:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9157 (1.8691)
+2022-11-18 15:25:52,043:INFO: Dataset: univ                Batch:  1/15	Loss 1.8787 (1.8787)
+2022-11-18 15:25:52,198:INFO: Dataset: univ                Batch:  2/15	Loss 1.9514 (1.9144)
+2022-11-18 15:25:52,359:INFO: Dataset: univ                Batch:  3/15	Loss 1.8542 (1.8954)
+2022-11-18 15:25:52,517:INFO: Dataset: univ                Batch:  4/15	Loss 1.9214 (1.9018)
+2022-11-18 15:25:52,674:INFO: Dataset: univ                Batch:  5/15	Loss 1.8747 (1.8960)
+2022-11-18 15:25:52,830:INFO: Dataset: univ                Batch:  6/15	Loss 1.9111 (1.8985)
+2022-11-18 15:25:52,989:INFO: Dataset: univ                Batch:  7/15	Loss 1.8931 (1.8978)
+2022-11-18 15:25:53,143:INFO: Dataset: univ                Batch:  8/15	Loss 1.8930 (1.8972)
+2022-11-18 15:25:53,300:INFO: Dataset: univ                Batch:  9/15	Loss 1.8283 (1.8897)
+2022-11-18 15:25:53,455:INFO: Dataset: univ                Batch: 10/15	Loss 1.8579 (1.8865)
+2022-11-18 15:25:53,614:INFO: Dataset: univ                Batch: 11/15	Loss 1.8739 (1.8853)
+2022-11-18 15:25:53,767:INFO: Dataset: univ                Batch: 12/15	Loss 1.8875 (1.8855)
+2022-11-18 15:25:53,924:INFO: Dataset: univ                Batch: 13/15	Loss 1.8416 (1.8821)
+2022-11-18 15:25:54,081:INFO: Dataset: univ                Batch: 14/15	Loss 1.8729 (1.8815)
+2022-11-18 15:25:54,164:INFO: Dataset: univ                Batch: 15/15	Loss 1.8590 (1.8811)
+2022-11-18 15:25:54,558:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9080 (1.9080)
+2022-11-18 15:25:54,708:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9588 (1.9333)
+2022-11-18 15:25:54,861:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9139 (1.9265)
+2022-11-18 15:25:55,013:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8775 (1.9160)
+2022-11-18 15:25:55,164:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9646 (1.9256)
+2022-11-18 15:25:55,315:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8121 (1.9080)
+2022-11-18 15:25:55,466:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9082 (1.9080)
+2022-11-18 15:25:55,603:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9843 (1.9165)
+2022-11-18 15:25:56,041:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9146 (1.9146)
+2022-11-18 15:25:56,193:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7970 (1.8520)
+2022-11-18 15:25:56,344:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9140 (1.8710)
+2022-11-18 15:25:56,496:INFO: Dataset: zara2               Batch:  4/18	Loss 2.0043 (1.9037)
+2022-11-18 15:25:56,647:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9189 (1.9067)
+2022-11-18 15:25:56,798:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9560 (1.9141)
+2022-11-18 15:25:56,950:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8074 (1.8981)
+2022-11-18 15:25:57,100:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9939 (1.9104)
+2022-11-18 15:25:57,252:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9153 (1.9109)
+2022-11-18 15:25:57,400:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9072 (1.9105)
+2022-11-18 15:25:57,554:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9011 (1.9096)
+2022-11-18 15:25:57,702:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9077 (1.9095)
+2022-11-18 15:25:57,854:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9056 (1.9091)
+2022-11-18 15:25:58,005:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8387 (1.9037)
+2022-11-18 15:25:58,157:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8946 (1.9032)
+2022-11-18 15:25:58,308:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0097 (1.9102)
+2022-11-18 15:25:58,461:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9134 (1.9104)
+2022-11-18 15:25:58,600:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8884 (1.9092)
+2022-11-18 15:25:58,648:INFO: - Computing loss (validation)
+2022-11-18 15:25:58,917:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8654 (1.8654)
+2022-11-18 15:25:58,952:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0323 (1.8808)
+2022-11-18 15:25:59,262:INFO: Dataset: univ                Batch: 1/3	Loss 1.7918 (1.7918)
+2022-11-18 15:25:59,338:INFO: Dataset: univ                Batch: 2/3	Loss 1.8774 (1.8301)
+2022-11-18 15:25:59,413:INFO: Dataset: univ                Batch: 3/3	Loss 1.8838 (1.8485)
+2022-11-18 15:25:59,716:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8870 (1.8870)
+2022-11-18 15:25:59,765:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9590 (1.9053)
+2022-11-18 15:26:00,068:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8537 (1.8537)
+2022-11-18 15:26:00,142:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7796 (1.8202)
+2022-11-18 15:26:00,218:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8690 (1.8372)
+2022-11-18 15:26:00,292:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8699 (1.8459)
+2022-11-18 15:26:00,365:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8826 (1.8531)
+2022-11-18 15:26:00,418:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_419.pth.tar
+2022-11-18 15:26:00,418:INFO: 
+===> EPOCH: 420 (P2)
+2022-11-18 15:26:00,419:INFO: - Computing loss (training)
+2022-11-18 15:26:00,764:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8314 (1.8314)
+2022-11-18 15:26:00,920:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9160 (1.8756)
+2022-11-18 15:26:01,075:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7665 (1.8371)
+2022-11-18 15:26:01,193:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8048 (1.8311)
+2022-11-18 15:26:01,588:INFO: Dataset: univ                Batch:  1/15	Loss 1.9059 (1.9059)
+2022-11-18 15:26:01,748:INFO: Dataset: univ                Batch:  2/15	Loss 1.9028 (1.9042)
+2022-11-18 15:26:01,910:INFO: Dataset: univ                Batch:  3/15	Loss 1.8480 (1.8861)
+2022-11-18 15:26:02,065:INFO: Dataset: univ                Batch:  4/15	Loss 1.8552 (1.8789)
+2022-11-18 15:26:02,224:INFO: Dataset: univ                Batch:  5/15	Loss 1.8412 (1.8708)
+2022-11-18 15:26:02,383:INFO: Dataset: univ                Batch:  6/15	Loss 1.8599 (1.8691)
+2022-11-18 15:26:02,540:INFO: Dataset: univ                Batch:  7/15	Loss 1.8460 (1.8657)
+2022-11-18 15:26:02,695:INFO: Dataset: univ                Batch:  8/15	Loss 1.8606 (1.8650)
+2022-11-18 15:26:02,853:INFO: Dataset: univ                Batch:  9/15	Loss 1.9233 (1.8719)
+2022-11-18 15:26:03,011:INFO: Dataset: univ                Batch: 10/15	Loss 1.8589 (1.8707)
+2022-11-18 15:26:03,179:INFO: Dataset: univ                Batch: 11/15	Loss 1.8724 (1.8709)
+2022-11-18 15:26:03,338:INFO: Dataset: univ                Batch: 12/15	Loss 1.9176 (1.8739)
+2022-11-18 15:26:03,496:INFO: Dataset: univ                Batch: 13/15	Loss 1.8719 (1.8738)
+2022-11-18 15:26:03,652:INFO: Dataset: univ                Batch: 14/15	Loss 1.9233 (1.8770)
+2022-11-18 15:26:03,737:INFO: Dataset: univ                Batch: 15/15	Loss 1.8407 (1.8765)
+2022-11-18 15:26:04,124:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8948 (1.8948)
+2022-11-18 15:26:04,281:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9764 (1.9329)
+2022-11-18 15:26:04,436:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8513 (1.9077)
+2022-11-18 15:26:04,588:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8022 (1.8817)
+2022-11-18 15:26:04,743:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9147 (1.8888)
+2022-11-18 15:26:04,895:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8719 (1.8863)
+2022-11-18 15:26:05,050:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9404 (1.8940)
+2022-11-18 15:26:05,188:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9409 (1.8992)
+2022-11-18 15:26:05,590:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9664 (1.9664)
+2022-11-18 15:26:05,754:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8692 (1.9174)
+2022-11-18 15:26:05,908:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9073 (1.9137)
+2022-11-18 15:26:06,061:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9001 (1.9103)
+2022-11-18 15:26:06,216:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9271 (1.9139)
+2022-11-18 15:26:06,372:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8935 (1.9102)
+2022-11-18 15:26:06,525:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8804 (1.9056)
+2022-11-18 15:26:06,677:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7690 (1.8881)
+2022-11-18 15:26:06,831:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8212 (1.8815)
+2022-11-18 15:26:06,982:INFO: Dataset: zara2               Batch: 10/18	Loss 2.0455 (1.8955)
+2022-11-18 15:26:07,137:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9340 (1.8990)
+2022-11-18 15:26:07,291:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9902 (1.9066)
+2022-11-18 15:26:07,446:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9560 (1.9105)
+2022-11-18 15:26:07,598:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8468 (1.9059)
+2022-11-18 15:26:07,753:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9015 (1.9056)
+2022-11-18 15:26:07,905:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8333 (1.9005)
+2022-11-18 15:26:08,059:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9005 (1.9005)
+2022-11-18 15:26:08,201:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9297 (1.9019)
+2022-11-18 15:26:08,249:INFO: - Computing loss (validation)
+2022-11-18 15:26:08,513:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9851 (1.9851)
+2022-11-18 15:26:08,546:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0765 (1.9907)
+2022-11-18 15:26:08,860:INFO: Dataset: univ                Batch: 1/3	Loss 1.8360 (1.8360)
+2022-11-18 15:26:08,939:INFO: Dataset: univ                Batch: 2/3	Loss 1.8207 (1.8290)
+2022-11-18 15:26:09,014:INFO: Dataset: univ                Batch: 3/3	Loss 1.9013 (1.8524)
+2022-11-18 15:26:09,317:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8199 (1.8199)
+2022-11-18 15:26:09,362:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8764 (1.8350)
+2022-11-18 15:26:09,685:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9246 (1.9246)
+2022-11-18 15:26:09,762:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8457 (1.8850)
+2022-11-18 15:26:09,838:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8204 (1.8630)
+2022-11-18 15:26:09,916:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8654 (1.8636)
+2022-11-18 15:26:09,992:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8538 (1.8617)
+2022-11-18 15:26:10,044:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_420.pth.tar
+2022-11-18 15:26:10,044:INFO: 
+===> EPOCH: 421 (P2)
+2022-11-18 15:26:10,045:INFO: - Computing loss (training)
+2022-11-18 15:26:10,403:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8870 (1.8870)
+2022-11-18 15:26:10,562:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8230 (1.8534)
+2022-11-18 15:26:10,721:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8015 (1.8369)
+2022-11-18 15:26:10,845:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8640 (1.8412)
+2022-11-18 15:26:11,261:INFO: Dataset: univ                Batch:  1/15	Loss 1.8396 (1.8396)
+2022-11-18 15:26:11,430:INFO: Dataset: univ                Batch:  2/15	Loss 1.8826 (1.8626)
+2022-11-18 15:26:11,594:INFO: Dataset: univ                Batch:  3/15	Loss 1.8508 (1.8583)
+2022-11-18 15:26:11,757:INFO: Dataset: univ                Batch:  4/15	Loss 1.8537 (1.8571)
+2022-11-18 15:26:11,923:INFO: Dataset: univ                Batch:  5/15	Loss 1.9265 (1.8724)
+2022-11-18 15:26:12,088:INFO: Dataset: univ                Batch:  6/15	Loss 1.8446 (1.8675)
+2022-11-18 15:26:12,253:INFO: Dataset: univ                Batch:  7/15	Loss 1.8758 (1.8686)
+2022-11-18 15:26:12,414:INFO: Dataset: univ                Batch:  8/15	Loss 1.8646 (1.8681)
+2022-11-18 15:26:12,579:INFO: Dataset: univ                Batch:  9/15	Loss 1.8718 (1.8685)
+2022-11-18 15:26:12,740:INFO: Dataset: univ                Batch: 10/15	Loss 1.8990 (1.8714)
+2022-11-18 15:26:12,904:INFO: Dataset: univ                Batch: 11/15	Loss 1.8503 (1.8695)
+2022-11-18 15:26:13,067:INFO: Dataset: univ                Batch: 12/15	Loss 1.9108 (1.8731)
+2022-11-18 15:26:13,230:INFO: Dataset: univ                Batch: 13/15	Loss 1.8694 (1.8728)
+2022-11-18 15:26:13,393:INFO: Dataset: univ                Batch: 14/15	Loss 1.8727 (1.8728)
+2022-11-18 15:26:13,481:INFO: Dataset: univ                Batch: 15/15	Loss 1.8299 (1.8722)
+2022-11-18 15:26:13,884:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9656 (1.9656)
+2022-11-18 15:26:14,040:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9558 (1.9609)
+2022-11-18 15:26:14,196:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8237 (1.9123)
+2022-11-18 15:26:14,352:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8725 (1.9031)
+2022-11-18 15:26:14,509:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8833 (1.8992)
+2022-11-18 15:26:14,661:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9012 (1.8995)
+2022-11-18 15:26:14,815:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9164 (1.9016)
+2022-11-18 15:26:14,958:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9573 (1.9079)
+2022-11-18 15:26:15,360:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9710 (1.9710)
+2022-11-18 15:26:15,521:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9665 (1.9686)
+2022-11-18 15:26:15,680:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7572 (1.8943)
+2022-11-18 15:26:15,854:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8702 (1.8884)
+2022-11-18 15:26:16,048:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8359 (1.8779)
+2022-11-18 15:26:16,244:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8311 (1.8713)
+2022-11-18 15:26:16,438:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9016 (1.8756)
+2022-11-18 15:26:16,642:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8430 (1.8717)
+2022-11-18 15:26:16,817:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9501 (1.8799)
+2022-11-18 15:26:16,980:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8088 (1.8731)
+2022-11-18 15:26:17,131:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8657 (1.8724)
+2022-11-18 15:26:17,288:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9068 (1.8752)
+2022-11-18 15:26:17,461:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7956 (1.8695)
+2022-11-18 15:26:17,641:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8937 (1.8711)
+2022-11-18 15:26:17,828:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9214 (1.8741)
+2022-11-18 15:26:17,994:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8479 (1.8724)
+2022-11-18 15:26:18,159:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0279 (1.8810)
+2022-11-18 15:26:18,307:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8897 (1.8814)
+2022-11-18 15:26:18,354:INFO: - Computing loss (validation)
+2022-11-18 15:26:18,624:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9515 (1.9515)
+2022-11-18 15:26:18,658:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6158 (1.9309)
+2022-11-18 15:26:18,963:INFO: Dataset: univ                Batch: 1/3	Loss 1.9053 (1.9053)
+2022-11-18 15:26:19,043:INFO: Dataset: univ                Batch: 2/3	Loss 1.9328 (1.9210)
+2022-11-18 15:26:19,115:INFO: Dataset: univ                Batch: 3/3	Loss 1.8384 (1.8961)
+2022-11-18 15:26:19,425:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9874 (1.9874)
+2022-11-18 15:26:19,473:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9291 (1.9720)
+2022-11-18 15:26:19,837:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8063 (1.8063)
+2022-11-18 15:26:19,924:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8080 (1.8072)
+2022-11-18 15:26:20,012:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9113 (1.8431)
+2022-11-18 15:26:20,102:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8602 (1.8473)
+2022-11-18 15:26:20,194:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8944 (1.8566)
+2022-11-18 15:26:20,267:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_421.pth.tar
+2022-11-18 15:26:20,267:INFO: 
+===> EPOCH: 422 (P2)
+2022-11-18 15:26:20,269:INFO: - Computing loss (training)
+2022-11-18 15:26:20,676:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8129 (1.8129)
+2022-11-18 15:26:20,838:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7720 (1.7935)
+2022-11-18 15:26:21,002:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8744 (1.8211)
+2022-11-18 15:26:21,127:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9642 (1.8428)
+2022-11-18 15:26:21,588:INFO: Dataset: univ                Batch:  1/15	Loss 1.8802 (1.8802)
+2022-11-18 15:26:21,751:INFO: Dataset: univ                Batch:  2/15	Loss 1.8993 (1.8898)
+2022-11-18 15:26:21,912:INFO: Dataset: univ                Batch:  3/15	Loss 1.8975 (1.8925)
+2022-11-18 15:26:22,082:INFO: Dataset: univ                Batch:  4/15	Loss 1.9088 (1.8968)
+2022-11-18 15:26:22,247:INFO: Dataset: univ                Batch:  5/15	Loss 1.8862 (1.8946)
+2022-11-18 15:26:22,407:INFO: Dataset: univ                Batch:  6/15	Loss 1.9082 (1.8968)
+2022-11-18 15:26:22,564:INFO: Dataset: univ                Batch:  7/15	Loss 1.8523 (1.8905)
+2022-11-18 15:26:22,721:INFO: Dataset: univ                Batch:  8/15	Loss 1.8872 (1.8901)
+2022-11-18 15:26:22,883:INFO: Dataset: univ                Batch:  9/15	Loss 1.8401 (1.8845)
+2022-11-18 15:26:23,044:INFO: Dataset: univ                Batch: 10/15	Loss 1.8800 (1.8841)
+2022-11-18 15:26:23,206:INFO: Dataset: univ                Batch: 11/15	Loss 1.8972 (1.8853)
+2022-11-18 15:26:23,379:INFO: Dataset: univ                Batch: 12/15	Loss 1.8914 (1.8858)
+2022-11-18 15:26:23,580:INFO: Dataset: univ                Batch: 13/15	Loss 1.8971 (1.8867)
+2022-11-18 15:26:23,741:INFO: Dataset: univ                Batch: 14/15	Loss 1.8638 (1.8851)
+2022-11-18 15:26:23,842:INFO: Dataset: univ                Batch: 15/15	Loss 1.8385 (1.8844)
+2022-11-18 15:26:24,316:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9467 (1.9467)
+2022-11-18 15:26:24,478:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8687 (1.9090)
+2022-11-18 15:26:24,635:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8820 (1.8999)
+2022-11-18 15:26:24,795:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8837 (1.8958)
+2022-11-18 15:26:24,953:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0153 (1.9225)
+2022-11-18 15:26:25,110:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8764 (1.9153)
+2022-11-18 15:26:25,277:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9513 (1.9209)
+2022-11-18 15:26:25,430:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8875 (1.9176)
+2022-11-18 15:26:25,865:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9985 (1.9985)
+2022-11-18 15:26:26,055:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8508 (1.9213)
+2022-11-18 15:26:26,217:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8753 (1.9053)
+2022-11-18 15:26:26,371:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8729 (1.8960)
+2022-11-18 15:26:26,529:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8027 (1.8782)
+2022-11-18 15:26:26,707:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7923 (1.8645)
+2022-11-18 15:26:26,874:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8844 (1.8672)
+2022-11-18 15:26:27,039:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9490 (1.8779)
+2022-11-18 15:26:27,204:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9327 (1.8840)
+2022-11-18 15:26:27,362:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8025 (1.8762)
+2022-11-18 15:26:27,546:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8809 (1.8766)
+2022-11-18 15:26:27,698:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9020 (1.8786)
+2022-11-18 15:26:27,851:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8706 (1.8779)
+2022-11-18 15:26:28,020:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8620 (1.8769)
+2022-11-18 15:26:28,221:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7609 (1.8683)
+2022-11-18 15:26:28,418:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7687 (1.8624)
+2022-11-18 15:26:28,610:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8946 (1.8644)
+2022-11-18 15:26:28,780:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9430 (1.8681)
+2022-11-18 15:26:28,835:INFO: - Computing loss (validation)
+2022-11-18 15:26:29,122:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9609 (1.9609)
+2022-11-18 15:26:29,157:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8836 (1.9548)
+2022-11-18 15:26:29,466:INFO: Dataset: univ                Batch: 1/3	Loss 1.8511 (1.8511)
+2022-11-18 15:26:29,547:INFO: Dataset: univ                Batch: 2/3	Loss 1.9077 (1.8834)
+2022-11-18 15:26:29,623:INFO: Dataset: univ                Batch: 3/3	Loss 1.8617 (1.8760)
+2022-11-18 15:26:29,965:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8771 (1.8771)
+2022-11-18 15:26:30,019:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9030 (1.8830)
+2022-11-18 15:26:30,381:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8650 (1.8650)
+2022-11-18 15:26:30,460:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9998 (1.9314)
+2022-11-18 15:26:30,538:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9663 (1.9431)
+2022-11-18 15:26:30,618:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8592 (1.9239)
+2022-11-18 15:26:30,700:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9535 (1.9296)
+2022-11-18 15:26:30,769:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_422.pth.tar
+2022-11-18 15:26:30,770:INFO: 
+===> EPOCH: 423 (P2)
+2022-11-18 15:26:30,770:INFO: - Computing loss (training)
+2022-11-18 15:26:31,151:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6507 (1.6507)
+2022-11-18 15:26:31,335:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0136 (1.8291)
+2022-11-18 15:26:31,502:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8891 (1.8491)
+2022-11-18 15:26:31,620:INFO: Dataset: hotel               Batch: 4/4	Loss 2.1028 (1.8933)
+2022-11-18 15:26:32,058:INFO: Dataset: univ                Batch:  1/15	Loss 1.8613 (1.8613)
+2022-11-18 15:26:32,239:INFO: Dataset: univ                Batch:  2/15	Loss 1.8959 (1.8775)
+2022-11-18 15:26:32,425:INFO: Dataset: univ                Batch:  3/15	Loss 1.8300 (1.8619)
+2022-11-18 15:26:32,593:INFO: Dataset: univ                Batch:  4/15	Loss 1.8363 (1.8550)
+2022-11-18 15:26:32,754:INFO: Dataset: univ                Batch:  5/15	Loss 1.8305 (1.8503)
+2022-11-18 15:26:32,935:INFO: Dataset: univ                Batch:  6/15	Loss 1.8650 (1.8527)
+2022-11-18 15:26:33,097:INFO: Dataset: univ                Batch:  7/15	Loss 1.9105 (1.8607)
+2022-11-18 15:26:33,251:INFO: Dataset: univ                Batch:  8/15	Loss 1.8427 (1.8585)
+2022-11-18 15:26:33,407:INFO: Dataset: univ                Batch:  9/15	Loss 1.8714 (1.8599)
+2022-11-18 15:26:33,561:INFO: Dataset: univ                Batch: 10/15	Loss 1.8560 (1.8595)
+2022-11-18 15:26:33,727:INFO: Dataset: univ                Batch: 11/15	Loss 1.8568 (1.8593)
+2022-11-18 15:26:33,916:INFO: Dataset: univ                Batch: 12/15	Loss 1.8242 (1.8563)
+2022-11-18 15:26:34,104:INFO: Dataset: univ                Batch: 13/15	Loss 1.8722 (1.8576)
+2022-11-18 15:26:34,286:INFO: Dataset: univ                Batch: 14/15	Loss 1.8755 (1.8589)
+2022-11-18 15:26:34,379:INFO: Dataset: univ                Batch: 15/15	Loss 1.8180 (1.8583)
+2022-11-18 15:26:34,780:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9992 (1.9992)
+2022-11-18 15:26:34,937:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8385 (1.9198)
+2022-11-18 15:26:35,092:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9233 (1.9210)
+2022-11-18 15:26:35,246:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9443 (1.9270)
+2022-11-18 15:26:35,415:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8740 (1.9158)
+2022-11-18 15:26:35,579:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9940 (1.9293)
+2022-11-18 15:26:35,760:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8984 (1.9253)
+2022-11-18 15:26:35,926:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6851 (1.8992)
+2022-11-18 15:26:36,417:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8322 (1.8322)
+2022-11-18 15:26:36,606:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8728 (1.8513)
+2022-11-18 15:26:36,775:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9499 (1.8822)
+2022-11-18 15:26:36,947:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8292 (1.8701)
+2022-11-18 15:26:37,107:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9118 (1.8787)
+2022-11-18 15:26:37,269:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9060 (1.8833)
+2022-11-18 15:26:37,426:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8546 (1.8791)
+2022-11-18 15:26:37,577:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9166 (1.8834)
+2022-11-18 15:26:37,734:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8165 (1.8758)
+2022-11-18 15:26:37,913:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8207 (1.8702)
+2022-11-18 15:26:38,077:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8943 (1.8726)
+2022-11-18 15:26:38,236:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8068 (1.8672)
+2022-11-18 15:26:38,394:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9437 (1.8736)
+2022-11-18 15:26:38,549:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8348 (1.8708)
+2022-11-18 15:26:38,700:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7509 (1.8626)
+2022-11-18 15:26:38,850:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8579 (1.8623)
+2022-11-18 15:26:39,017:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8873 (1.8637)
+2022-11-18 15:26:39,178:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8958 (1.8652)
+2022-11-18 15:26:39,238:INFO: - Computing loss (validation)
+2022-11-18 15:26:39,562:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8316 (1.8316)
+2022-11-18 15:26:39,605:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0306 (1.8459)
+2022-11-18 15:26:39,963:INFO: Dataset: univ                Batch: 1/3	Loss 1.9086 (1.9086)
+2022-11-18 15:26:40,098:INFO: Dataset: univ                Batch: 2/3	Loss 1.8766 (1.8932)
+2022-11-18 15:26:40,207:INFO: Dataset: univ                Batch: 3/3	Loss 1.8889 (1.8922)
+2022-11-18 15:26:40,567:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8462 (1.8462)
+2022-11-18 15:26:40,613:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8156 (1.8382)
+2022-11-18 15:26:40,930:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9176 (1.9176)
+2022-11-18 15:26:41,009:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8636 (1.8909)
+2022-11-18 15:26:41,087:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8645 (1.8826)
+2022-11-18 15:26:41,166:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9190 (1.8916)
+2022-11-18 15:26:41,244:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7554 (1.8641)
+2022-11-18 15:26:41,302:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_423.pth.tar
+2022-11-18 15:26:41,302:INFO: 
+===> EPOCH: 424 (P2)
+2022-11-18 15:26:41,303:INFO: - Computing loss (training)
+2022-11-18 15:26:41,644:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0930 (2.0930)
+2022-11-18 15:26:41,800:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9345 (2.0149)
+2022-11-18 15:26:41,956:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8807 (1.9692)
+2022-11-18 15:26:42,073:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0215 (1.9785)
+2022-11-18 15:26:42,489:INFO: Dataset: univ                Batch:  1/15	Loss 1.8443 (1.8443)
+2022-11-18 15:26:42,652:INFO: Dataset: univ                Batch:  2/15	Loss 1.8836 (1.8634)
+2022-11-18 15:26:42,812:INFO: Dataset: univ                Batch:  3/15	Loss 1.8743 (1.8667)
+2022-11-18 15:26:42,977:INFO: Dataset: univ                Batch:  4/15	Loss 1.8275 (1.8560)
+2022-11-18 15:26:43,159:INFO: Dataset: univ                Batch:  5/15	Loss 1.8333 (1.8508)
+2022-11-18 15:26:43,337:INFO: Dataset: univ                Batch:  6/15	Loss 1.9399 (1.8659)
+2022-11-18 15:26:43,500:INFO: Dataset: univ                Batch:  7/15	Loss 1.8898 (1.8695)
+2022-11-18 15:26:43,658:INFO: Dataset: univ                Batch:  8/15	Loss 1.8727 (1.8699)
+2022-11-18 15:26:43,822:INFO: Dataset: univ                Batch:  9/15	Loss 1.8938 (1.8726)
+2022-11-18 15:26:43,982:INFO: Dataset: univ                Batch: 10/15	Loss 1.8530 (1.8705)
+2022-11-18 15:26:44,157:INFO: Dataset: univ                Batch: 11/15	Loss 1.8460 (1.8680)
+2022-11-18 15:26:44,327:INFO: Dataset: univ                Batch: 12/15	Loss 1.8900 (1.8698)
+2022-11-18 15:26:44,496:INFO: Dataset: univ                Batch: 13/15	Loss 1.8801 (1.8706)
+2022-11-18 15:26:44,659:INFO: Dataset: univ                Batch: 14/15	Loss 1.8578 (1.8697)
+2022-11-18 15:26:44,765:INFO: Dataset: univ                Batch: 15/15	Loss 1.9053 (1.8701)
+2022-11-18 15:26:45,236:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9569 (1.9569)
+2022-11-18 15:26:45,424:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8635 (1.9104)
+2022-11-18 15:26:45,622:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8326 (1.8843)
+2022-11-18 15:26:45,818:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8455 (1.8751)
+2022-11-18 15:26:45,978:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9332 (1.8869)
+2022-11-18 15:26:46,132:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8975 (1.8885)
+2022-11-18 15:26:46,282:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8069 (1.8767)
+2022-11-18 15:26:46,420:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8677 (1.8758)
+2022-11-18 15:26:46,868:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8500 (1.8500)
+2022-11-18 15:26:47,029:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8718 (1.8609)
+2022-11-18 15:26:47,183:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9139 (1.8785)
+2022-11-18 15:26:47,334:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8619 (1.8741)
+2022-11-18 15:26:47,499:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8233 (1.8645)
+2022-11-18 15:26:47,655:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9465 (1.8774)
+2022-11-18 15:26:47,806:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8835 (1.8784)
+2022-11-18 15:26:47,954:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8129 (1.8699)
+2022-11-18 15:26:48,104:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8625 (1.8691)
+2022-11-18 15:26:48,252:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8010 (1.8621)
+2022-11-18 15:26:48,403:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8146 (1.8578)
+2022-11-18 15:26:48,553:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8579 (1.8578)
+2022-11-18 15:26:48,702:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9432 (1.8641)
+2022-11-18 15:26:48,852:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8278 (1.8616)
+2022-11-18 15:26:49,006:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9379 (1.8668)
+2022-11-18 15:26:49,155:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9375 (1.8713)
+2022-11-18 15:26:49,307:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8730 (1.8714)
+2022-11-18 15:26:49,448:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7468 (1.8655)
+2022-11-18 15:26:49,494:INFO: - Computing loss (validation)
+2022-11-18 15:26:49,760:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9490 (1.9490)
+2022-11-18 15:26:49,794:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8759 (1.9445)
+2022-11-18 15:26:50,109:INFO: Dataset: univ                Batch: 1/3	Loss 1.8551 (1.8551)
+2022-11-18 15:26:50,191:INFO: Dataset: univ                Batch: 2/3	Loss 1.8598 (1.8572)
+2022-11-18 15:26:50,269:INFO: Dataset: univ                Batch: 3/3	Loss 1.8921 (1.8691)
+2022-11-18 15:26:50,571:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9680 (1.9680)
+2022-11-18 15:26:50,616:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9967 (1.9761)
+2022-11-18 15:26:50,932:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9240 (1.9240)
+2022-11-18 15:26:51,011:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8569 (1.8906)
+2022-11-18 15:26:51,084:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8488 (1.8770)
+2022-11-18 15:26:51,159:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8417 (1.8677)
+2022-11-18 15:26:51,232:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7751 (1.8492)
+2022-11-18 15:26:51,290:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_424.pth.tar
+2022-11-18 15:26:51,290:INFO: 
+===> EPOCH: 425 (P2)
+2022-11-18 15:26:51,291:INFO: - Computing loss (training)
+2022-11-18 15:26:51,631:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9347 (1.9347)
+2022-11-18 15:26:51,782:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9308 (1.9327)
+2022-11-18 15:26:51,934:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9104 (1.9250)
+2022-11-18 15:26:52,051:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0069 (1.9386)
+2022-11-18 15:26:52,448:INFO: Dataset: univ                Batch:  1/15	Loss 1.8996 (1.8996)
+2022-11-18 15:26:52,606:INFO: Dataset: univ                Batch:  2/15	Loss 1.8947 (1.8969)
+2022-11-18 15:26:52,778:INFO: Dataset: univ                Batch:  3/15	Loss 1.8394 (1.8748)
+2022-11-18 15:26:52,936:INFO: Dataset: univ                Batch:  4/15	Loss 1.8590 (1.8708)
+2022-11-18 15:26:53,101:INFO: Dataset: univ                Batch:  5/15	Loss 1.8840 (1.8733)
+2022-11-18 15:26:53,271:INFO: Dataset: univ                Batch:  6/15	Loss 1.8951 (1.8773)
+2022-11-18 15:26:53,436:INFO: Dataset: univ                Batch:  7/15	Loss 1.8883 (1.8789)
+2022-11-18 15:26:53,592:INFO: Dataset: univ                Batch:  8/15	Loss 1.8388 (1.8738)
+2022-11-18 15:26:53,754:INFO: Dataset: univ                Batch:  9/15	Loss 1.8551 (1.8717)
+2022-11-18 15:26:53,938:INFO: Dataset: univ                Batch: 10/15	Loss 1.9146 (1.8764)
+2022-11-18 15:26:54,119:INFO: Dataset: univ                Batch: 11/15	Loss 1.8549 (1.8741)
+2022-11-18 15:26:54,291:INFO: Dataset: univ                Batch: 12/15	Loss 1.8929 (1.8758)
+2022-11-18 15:26:54,452:INFO: Dataset: univ                Batch: 13/15	Loss 1.8393 (1.8729)
+2022-11-18 15:26:54,610:INFO: Dataset: univ                Batch: 14/15	Loss 1.8907 (1.8741)
+2022-11-18 15:26:54,695:INFO: Dataset: univ                Batch: 15/15	Loss 1.8884 (1.8743)
+2022-11-18 15:26:55,144:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9535 (1.9535)
+2022-11-18 15:26:55,320:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8881 (1.9230)
+2022-11-18 15:26:55,486:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8653 (1.9049)
+2022-11-18 15:26:55,650:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8869 (1.9006)
+2022-11-18 15:26:55,809:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9890 (1.9176)
+2022-11-18 15:26:55,963:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9337 (1.9203)
+2022-11-18 15:26:56,118:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9344 (1.9222)
+2022-11-18 15:26:56,263:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7767 (1.9078)
+2022-11-18 15:26:56,677:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8292 (1.8292)
+2022-11-18 15:26:56,838:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9297 (1.8817)
+2022-11-18 15:26:56,996:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8727 (1.8785)
+2022-11-18 15:26:57,156:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8821 (1.8794)
+2022-11-18 15:26:57,320:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8581 (1.8754)
+2022-11-18 15:26:57,477:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8532 (1.8714)
+2022-11-18 15:26:57,627:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7897 (1.8588)
+2022-11-18 15:26:57,776:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8567 (1.8586)
+2022-11-18 15:26:57,926:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9260 (1.8660)
+2022-11-18 15:26:58,080:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7622 (1.8557)
+2022-11-18 15:26:58,232:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9118 (1.8606)
+2022-11-18 15:26:58,382:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9018 (1.8644)
+2022-11-18 15:26:58,532:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7850 (1.8587)
+2022-11-18 15:26:58,681:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9154 (1.8629)
+2022-11-18 15:26:58,833:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8562 (1.8625)
+2022-11-18 15:26:58,998:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8977 (1.8646)
+2022-11-18 15:26:59,153:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8158 (1.8621)
+2022-11-18 15:26:59,296:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8689 (1.8624)
+2022-11-18 15:26:59,341:INFO: - Computing loss (validation)
+2022-11-18 15:26:59,602:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8893 (1.8893)
+2022-11-18 15:26:59,638:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0575 (1.9014)
+2022-11-18 15:26:59,942:INFO: Dataset: univ                Batch: 1/3	Loss 1.8820 (1.8820)
+2022-11-18 15:27:00,019:INFO: Dataset: univ                Batch: 2/3	Loss 1.8185 (1.8456)
+2022-11-18 15:27:00,092:INFO: Dataset: univ                Batch: 3/3	Loss 1.8817 (1.8572)
+2022-11-18 15:27:00,399:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8997 (1.8997)
+2022-11-18 15:27:00,445:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9115 (1.9025)
+2022-11-18 15:27:00,737:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9078 (1.9078)
+2022-11-18 15:27:00,814:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8755 (1.8917)
+2022-11-18 15:27:00,889:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8850 (1.8895)
+2022-11-18 15:27:00,970:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9476 (1.9039)
+2022-11-18 15:27:01,046:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8651 (1.8960)
+2022-11-18 15:27:01,097:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_425.pth.tar
+2022-11-18 15:27:01,098:INFO: 
+===> EPOCH: 426 (P2)
+2022-11-18 15:27:01,098:INFO: - Computing loss (training)
+2022-11-18 15:27:01,434:INFO: Dataset: hotel               Batch: 1/4	Loss 2.1085 (2.1085)
+2022-11-18 15:27:01,587:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8188 (1.9732)
+2022-11-18 15:27:01,743:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8726 (1.9410)
+2022-11-18 15:27:01,860:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7502 (1.9065)
+2022-11-18 15:27:02,310:INFO: Dataset: univ                Batch:  1/15	Loss 1.8720 (1.8720)
+2022-11-18 15:27:02,477:INFO: Dataset: univ                Batch:  2/15	Loss 1.8352 (1.8558)
+2022-11-18 15:27:02,638:INFO: Dataset: univ                Batch:  3/15	Loss 1.8939 (1.8695)
+2022-11-18 15:27:02,799:INFO: Dataset: univ                Batch:  4/15	Loss 1.8594 (1.8668)
+2022-11-18 15:27:02,966:INFO: Dataset: univ                Batch:  5/15	Loss 1.9060 (1.8756)
+2022-11-18 15:27:03,131:INFO: Dataset: univ                Batch:  6/15	Loss 1.8365 (1.8693)
+2022-11-18 15:27:03,296:INFO: Dataset: univ                Batch:  7/15	Loss 1.8536 (1.8672)
+2022-11-18 15:27:03,466:INFO: Dataset: univ                Batch:  8/15	Loss 1.9013 (1.8717)
+2022-11-18 15:27:03,626:INFO: Dataset: univ                Batch:  9/15	Loss 1.8333 (1.8678)
+2022-11-18 15:27:03,785:INFO: Dataset: univ                Batch: 10/15	Loss 1.9158 (1.8729)
+2022-11-18 15:27:03,944:INFO: Dataset: univ                Batch: 11/15	Loss 1.8545 (1.8712)
+2022-11-18 15:27:04,111:INFO: Dataset: univ                Batch: 12/15	Loss 1.8604 (1.8703)
+2022-11-18 15:27:04,270:INFO: Dataset: univ                Batch: 13/15	Loss 1.8538 (1.8691)
+2022-11-18 15:27:04,431:INFO: Dataset: univ                Batch: 14/15	Loss 1.8767 (1.8696)
+2022-11-18 15:27:04,517:INFO: Dataset: univ                Batch: 15/15	Loss 1.8139 (1.8688)
+2022-11-18 15:27:04,907:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9049 (1.9049)
+2022-11-18 15:27:05,073:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7894 (1.8497)
+2022-11-18 15:27:05,242:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9382 (1.8783)
+2022-11-18 15:27:05,409:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8072 (1.8593)
+2022-11-18 15:27:05,565:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8788 (1.8634)
+2022-11-18 15:27:05,718:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7943 (1.8517)
+2022-11-18 15:27:05,873:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9898 (1.8726)
+2022-11-18 15:27:06,020:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8271 (1.8679)
+2022-11-18 15:27:06,418:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8277 (1.8277)
+2022-11-18 15:27:06,579:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8691 (1.8472)
+2022-11-18 15:27:06,732:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7827 (1.8230)
+2022-11-18 15:27:06,885:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8884 (1.8412)
+2022-11-18 15:27:07,044:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8260 (1.8384)
+2022-11-18 15:27:07,201:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8927 (1.8491)
+2022-11-18 15:27:07,356:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8360 (1.8472)
+2022-11-18 15:27:07,593:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8387 (1.8460)
+2022-11-18 15:27:07,752:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8613 (1.8475)
+2022-11-18 15:27:07,906:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8366 (1.8464)
+2022-11-18 15:27:08,070:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7965 (1.8419)
+2022-11-18 15:27:08,238:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8498 (1.8426)
+2022-11-18 15:27:08,403:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8243 (1.8412)
+2022-11-18 15:27:08,558:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9680 (1.8503)
+2022-11-18 15:27:08,712:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9095 (1.8544)
+2022-11-18 15:27:08,869:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8726 (1.8556)
+2022-11-18 15:27:09,032:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8562 (1.8556)
+2022-11-18 15:27:09,179:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9852 (1.8616)
+2022-11-18 15:27:09,226:INFO: - Computing loss (validation)
+2022-11-18 15:27:09,490:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9066 (1.9066)
+2022-11-18 15:27:09,525:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0890 (1.9166)
+2022-11-18 15:27:09,835:INFO: Dataset: univ                Batch: 1/3	Loss 1.8762 (1.8762)
+2022-11-18 15:27:09,915:INFO: Dataset: univ                Batch: 2/3	Loss 1.8495 (1.8626)
+2022-11-18 15:27:09,990:INFO: Dataset: univ                Batch: 3/3	Loss 1.9089 (1.8765)
+2022-11-18 15:27:10,295:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9455 (1.9455)
+2022-11-18 15:27:10,339:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0811 (1.9769)
+2022-11-18 15:27:10,650:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8660 (1.8660)
+2022-11-18 15:27:10,726:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8361 (1.8515)
+2022-11-18 15:27:10,801:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8864 (1.8641)
+2022-11-18 15:27:10,876:INFO: Dataset: zara2               Batch: 4/5	Loss 2.0071 (1.9008)
+2022-11-18 15:27:10,950:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8490 (1.8901)
+2022-11-18 15:27:11,010:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_426.pth.tar
+2022-11-18 15:27:11,010:INFO: 
+===> EPOCH: 427 (P2)
+2022-11-18 15:27:11,011:INFO: - Computing loss (training)
+2022-11-18 15:27:11,404:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9207 (1.9207)
+2022-11-18 15:27:11,569:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8656 (1.8940)
+2022-11-18 15:27:11,731:INFO: Dataset: hotel               Batch: 3/4	Loss 1.6695 (1.8165)
+2022-11-18 15:27:11,856:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8669 (1.8243)
+2022-11-18 15:27:12,275:INFO: Dataset: univ                Batch:  1/15	Loss 1.8261 (1.8261)
+2022-11-18 15:27:12,444:INFO: Dataset: univ                Batch:  2/15	Loss 1.8861 (1.8548)
+2022-11-18 15:27:12,613:INFO: Dataset: univ                Batch:  3/15	Loss 1.9206 (1.8777)
+2022-11-18 15:27:12,781:INFO: Dataset: univ                Batch:  4/15	Loss 1.9201 (1.8891)
+2022-11-18 15:27:12,946:INFO: Dataset: univ                Batch:  5/15	Loss 1.8609 (1.8830)
+2022-11-18 15:27:13,117:INFO: Dataset: univ                Batch:  6/15	Loss 1.8543 (1.8783)
+2022-11-18 15:27:13,283:INFO: Dataset: univ                Batch:  7/15	Loss 1.8558 (1.8750)
+2022-11-18 15:27:13,446:INFO: Dataset: univ                Batch:  8/15	Loss 1.8351 (1.8704)
+2022-11-18 15:27:13,613:INFO: Dataset: univ                Batch:  9/15	Loss 1.8659 (1.8699)
+2022-11-18 15:27:13,777:INFO: Dataset: univ                Batch: 10/15	Loss 1.8879 (1.8717)
+2022-11-18 15:27:13,945:INFO: Dataset: univ                Batch: 11/15	Loss 1.8812 (1.8725)
+2022-11-18 15:27:14,112:INFO: Dataset: univ                Batch: 12/15	Loss 1.8324 (1.8692)
+2022-11-18 15:27:14,277:INFO: Dataset: univ                Batch: 13/15	Loss 1.8626 (1.8687)
+2022-11-18 15:27:14,442:INFO: Dataset: univ                Batch: 14/15	Loss 1.8185 (1.8653)
+2022-11-18 15:27:14,530:INFO: Dataset: univ                Batch: 15/15	Loss 1.8204 (1.8647)
+2022-11-18 15:27:14,933:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7763 (1.7763)
+2022-11-18 15:27:15,107:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9479 (1.8552)
+2022-11-18 15:27:15,287:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0203 (1.9111)
+2022-11-18 15:27:15,459:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8656 (1.8988)
+2022-11-18 15:27:15,622:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8323 (1.8849)
+2022-11-18 15:27:15,785:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0009 (1.9060)
+2022-11-18 15:27:15,947:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9560 (1.9129)
+2022-11-18 15:27:16,093:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8012 (1.9015)
+2022-11-18 15:27:16,490:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9184 (1.9184)
+2022-11-18 15:27:16,656:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8127 (1.8647)
+2022-11-18 15:27:16,818:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8686 (1.8660)
+2022-11-18 15:27:16,978:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8604 (1.8647)
+2022-11-18 15:27:17,139:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9062 (1.8731)
+2022-11-18 15:27:17,302:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7575 (1.8533)
+2022-11-18 15:27:17,464:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8972 (1.8593)
+2022-11-18 15:27:17,624:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8851 (1.8626)
+2022-11-18 15:27:17,788:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7406 (1.8491)
+2022-11-18 15:27:17,947:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8226 (1.8464)
+2022-11-18 15:27:18,109:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8911 (1.8502)
+2022-11-18 15:27:18,271:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8843 (1.8530)
+2022-11-18 15:27:18,432:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9207 (1.8579)
+2022-11-18 15:27:18,592:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8209 (1.8551)
+2022-11-18 15:27:18,755:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8147 (1.8528)
+2022-11-18 15:27:18,919:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9616 (1.8595)
+2022-11-18 15:27:19,083:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9556 (1.8650)
+2022-11-18 15:27:19,233:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8356 (1.8636)
+2022-11-18 15:27:19,278:INFO: - Computing loss (validation)
+2022-11-18 15:27:19,538:INFO: Dataset: hotel               Batch: 1/2	Loss 2.1227 (2.1227)
+2022-11-18 15:27:19,573:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7178 (2.0937)
+2022-11-18 15:27:19,879:INFO: Dataset: univ                Batch: 1/3	Loss 1.8171 (1.8171)
+2022-11-18 15:27:19,957:INFO: Dataset: univ                Batch: 2/3	Loss 1.8975 (1.8581)
+2022-11-18 15:27:20,030:INFO: Dataset: univ                Batch: 3/3	Loss 1.8244 (1.8470)
+2022-11-18 15:27:20,330:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8513 (1.8513)
+2022-11-18 15:27:20,377:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8424 (1.8490)
+2022-11-18 15:27:20,689:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9026 (1.9026)
+2022-11-18 15:27:20,764:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9153 (1.9092)
+2022-11-18 15:27:20,838:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8980 (1.9055)
+2022-11-18 15:27:20,913:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8799 (1.8991)
+2022-11-18 15:27:20,987:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8487 (1.8893)
+2022-11-18 15:27:21,039:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_427.pth.tar
+2022-11-18 15:27:21,039:INFO: 
+===> EPOCH: 428 (P2)
+2022-11-18 15:27:21,040:INFO: - Computing loss (training)
+2022-11-18 15:27:21,382:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9406 (1.9406)
+2022-11-18 15:27:21,544:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9445 (1.9427)
+2022-11-18 15:27:21,704:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0317 (1.9733)
+2022-11-18 15:27:21,826:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9718 (1.9730)
+2022-11-18 15:27:22,239:INFO: Dataset: univ                Batch:  1/15	Loss 1.9055 (1.9055)
+2022-11-18 15:27:22,401:INFO: Dataset: univ                Batch:  2/15	Loss 1.8707 (1.8877)
+2022-11-18 15:27:22,561:INFO: Dataset: univ                Batch:  3/15	Loss 1.9091 (1.8946)
+2022-11-18 15:27:22,731:INFO: Dataset: univ                Batch:  4/15	Loss 1.8674 (1.8883)
+2022-11-18 15:27:22,897:INFO: Dataset: univ                Batch:  5/15	Loss 1.8937 (1.8893)
+2022-11-18 15:27:23,067:INFO: Dataset: univ                Batch:  6/15	Loss 1.8232 (1.8783)
+2022-11-18 15:27:23,232:INFO: Dataset: univ                Batch:  7/15	Loss 1.9057 (1.8824)
+2022-11-18 15:27:23,395:INFO: Dataset: univ                Batch:  8/15	Loss 1.8879 (1.8829)
+2022-11-18 15:27:23,561:INFO: Dataset: univ                Batch:  9/15	Loss 1.8925 (1.8841)
+2022-11-18 15:27:23,726:INFO: Dataset: univ                Batch: 10/15	Loss 1.8663 (1.8822)
+2022-11-18 15:27:23,895:INFO: Dataset: univ                Batch: 11/15	Loss 1.8618 (1.8805)
+2022-11-18 15:27:24,061:INFO: Dataset: univ                Batch: 12/15	Loss 1.8543 (1.8781)
+2022-11-18 15:27:24,226:INFO: Dataset: univ                Batch: 13/15	Loss 1.8555 (1.8763)
+2022-11-18 15:27:24,397:INFO: Dataset: univ                Batch: 14/15	Loss 1.8810 (1.8767)
+2022-11-18 15:27:24,498:INFO: Dataset: univ                Batch: 15/15	Loss 1.8139 (1.8759)
+2022-11-18 15:27:24,915:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9586 (1.9586)
+2022-11-18 15:27:25,076:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9427 (1.9503)
+2022-11-18 15:27:25,254:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8147 (1.9051)
+2022-11-18 15:27:25,433:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8450 (1.8897)
+2022-11-18 15:27:25,601:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9472 (1.9012)
+2022-11-18 15:27:25,770:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9870 (1.9152)
+2022-11-18 15:27:25,927:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7795 (1.8979)
+2022-11-18 15:27:26,067:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9363 (1.9022)
+2022-11-18 15:27:26,485:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8746 (1.8746)
+2022-11-18 15:27:26,643:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8535 (1.8640)
+2022-11-18 15:27:26,817:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9006 (1.8760)
+2022-11-18 15:27:26,980:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9249 (1.8891)
+2022-11-18 15:27:27,133:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9177 (1.8947)
+2022-11-18 15:27:27,288:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8666 (1.8896)
+2022-11-18 15:27:27,439:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8549 (1.8842)
+2022-11-18 15:27:27,593:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8690 (1.8823)
+2022-11-18 15:27:27,745:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8846 (1.8826)
+2022-11-18 15:27:27,895:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8604 (1.8802)
+2022-11-18 15:27:28,047:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7724 (1.8702)
+2022-11-18 15:27:28,198:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9797 (1.8797)
+2022-11-18 15:27:28,350:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8070 (1.8735)
+2022-11-18 15:27:28,500:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8949 (1.8751)
+2022-11-18 15:27:28,659:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8971 (1.8765)
+2022-11-18 15:27:28,829:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8488 (1.8748)
+2022-11-18 15:27:28,994:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9239 (1.8778)
+2022-11-18 15:27:29,149:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8718 (1.8776)
+2022-11-18 15:27:29,200:INFO: - Computing loss (validation)
+2022-11-18 15:27:29,472:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9697 (1.9697)
+2022-11-18 15:27:29,508:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6970 (1.9511)
+2022-11-18 15:27:29,812:INFO: Dataset: univ                Batch: 1/3	Loss 1.8429 (1.8429)
+2022-11-18 15:27:29,889:INFO: Dataset: univ                Batch: 2/3	Loss 1.8607 (1.8515)
+2022-11-18 15:27:29,962:INFO: Dataset: univ                Batch: 3/3	Loss 1.8758 (1.8590)
+2022-11-18 15:27:30,272:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8826 (1.8826)
+2022-11-18 15:27:30,317:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0271 (1.9170)
+2022-11-18 15:27:30,646:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8345 (1.8345)
+2022-11-18 15:27:30,721:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9074 (1.8703)
+2022-11-18 15:27:30,795:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8877 (1.8758)
+2022-11-18 15:27:30,871:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9612 (1.8969)
+2022-11-18 15:27:30,946:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8759 (1.8925)
+2022-11-18 15:27:30,999:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_428.pth.tar
+2022-11-18 15:27:30,999:INFO: 
+===> EPOCH: 429 (P2)
+2022-11-18 15:27:31,000:INFO: - Computing loss (training)
+2022-11-18 15:27:31,346:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7582 (1.7582)
+2022-11-18 15:27:31,502:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8991 (1.8301)
+2022-11-18 15:27:31,653:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9544 (1.8702)
+2022-11-18 15:27:31,768:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9283 (1.8798)
+2022-11-18 15:27:32,182:INFO: Dataset: univ                Batch:  1/15	Loss 1.8445 (1.8445)
+2022-11-18 15:27:32,354:INFO: Dataset: univ                Batch:  2/15	Loss 1.8717 (1.8585)
+2022-11-18 15:27:32,530:INFO: Dataset: univ                Batch:  3/15	Loss 1.8936 (1.8705)
+2022-11-18 15:27:32,701:INFO: Dataset: univ                Batch:  4/15	Loss 1.8930 (1.8768)
+2022-11-18 15:27:32,872:INFO: Dataset: univ                Batch:  5/15	Loss 1.8918 (1.8798)
+2022-11-18 15:27:33,049:INFO: Dataset: univ                Batch:  6/15	Loss 1.8235 (1.8695)
+2022-11-18 15:27:33,226:INFO: Dataset: univ                Batch:  7/15	Loss 1.8402 (1.8652)
+2022-11-18 15:27:33,386:INFO: Dataset: univ                Batch:  8/15	Loss 1.8726 (1.8662)
+2022-11-18 15:27:33,547:INFO: Dataset: univ                Batch:  9/15	Loss 1.8339 (1.8626)
+2022-11-18 15:27:33,714:INFO: Dataset: univ                Batch: 10/15	Loss 1.9429 (1.8713)
+2022-11-18 15:27:33,880:INFO: Dataset: univ                Batch: 11/15	Loss 1.8800 (1.8720)
+2022-11-18 15:27:34,044:INFO: Dataset: univ                Batch: 12/15	Loss 1.8345 (1.8687)
+2022-11-18 15:27:34,207:INFO: Dataset: univ                Batch: 13/15	Loss 1.8739 (1.8691)
+2022-11-18 15:27:34,373:INFO: Dataset: univ                Batch: 14/15	Loss 1.8698 (1.8692)
+2022-11-18 15:27:34,458:INFO: Dataset: univ                Batch: 15/15	Loss 1.9478 (1.8700)
+2022-11-18 15:27:34,872:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8844 (1.8844)
+2022-11-18 15:27:35,029:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8843 (1.8844)
+2022-11-18 15:27:35,193:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9747 (1.9166)
+2022-11-18 15:27:35,345:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9260 (1.9190)
+2022-11-18 15:27:35,505:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9570 (1.9268)
+2022-11-18 15:27:35,654:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8564 (1.9146)
+2022-11-18 15:27:35,804:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9307 (1.9169)
+2022-11-18 15:27:35,939:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9293 (1.9184)
+2022-11-18 15:27:36,354:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8843 (1.8843)
+2022-11-18 15:27:36,516:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8241 (1.8537)
+2022-11-18 15:27:36,682:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8939 (1.8677)
+2022-11-18 15:27:36,854:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8858 (1.8726)
+2022-11-18 15:27:37,027:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8287 (1.8637)
+2022-11-18 15:27:37,182:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9327 (1.8742)
+2022-11-18 15:27:37,334:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7866 (1.8624)
+2022-11-18 15:27:37,481:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8168 (1.8565)
+2022-11-18 15:27:37,630:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9053 (1.8616)
+2022-11-18 15:27:37,777:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8278 (1.8583)
+2022-11-18 15:27:37,938:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8552 (1.8581)
+2022-11-18 15:27:38,108:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8933 (1.8611)
+2022-11-18 15:27:38,276:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8398 (1.8598)
+2022-11-18 15:27:38,447:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8139 (1.8562)
+2022-11-18 15:27:38,609:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9290 (1.8608)
+2022-11-18 15:27:38,788:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9209 (1.8648)
+2022-11-18 15:27:38,953:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9546 (1.8705)
+2022-11-18 15:27:39,107:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8331 (1.8688)
+2022-11-18 15:27:39,157:INFO: - Computing loss (validation)
+2022-11-18 15:27:39,446:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8467 (1.8467)
+2022-11-18 15:27:39,481:INFO: Dataset: hotel               Batch: 2/2	Loss 2.8783 (1.8960)
+2022-11-18 15:27:39,795:INFO: Dataset: univ                Batch: 1/3	Loss 1.9042 (1.9042)
+2022-11-18 15:27:39,883:INFO: Dataset: univ                Batch: 2/3	Loss 1.9411 (1.9238)
+2022-11-18 15:27:39,964:INFO: Dataset: univ                Batch: 3/3	Loss 1.8329 (1.8922)
+2022-11-18 15:27:40,326:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8973 (1.8973)
+2022-11-18 15:27:40,379:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9793 (1.9179)
+2022-11-18 15:27:40,721:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8259 (1.8259)
+2022-11-18 15:27:40,806:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8580 (1.8425)
+2022-11-18 15:27:40,885:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9613 (1.8828)
+2022-11-18 15:27:40,967:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8889 (1.8843)
+2022-11-18 15:27:41,045:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8787 (1.8832)
+2022-11-18 15:27:41,099:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_429.pth.tar
+2022-11-18 15:27:41,099:INFO: 
+===> EPOCH: 430 (P2)
+2022-11-18 15:27:41,100:INFO: - Computing loss (training)
+2022-11-18 15:27:41,470:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9449 (1.9449)
+2022-11-18 15:27:41,629:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7559 (1.8531)
+2022-11-18 15:27:41,782:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9628 (1.8893)
+2022-11-18 15:27:41,908:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8813 (1.8880)
+2022-11-18 15:27:42,333:INFO: Dataset: univ                Batch:  1/15	Loss 1.8278 (1.8278)
+2022-11-18 15:27:42,503:INFO: Dataset: univ                Batch:  2/15	Loss 1.8585 (1.8434)
+2022-11-18 15:27:42,672:INFO: Dataset: univ                Batch:  3/15	Loss 1.8598 (1.8488)
+2022-11-18 15:27:42,852:INFO: Dataset: univ                Batch:  4/15	Loss 1.8769 (1.8557)
+2022-11-18 15:27:43,014:INFO: Dataset: univ                Batch:  5/15	Loss 1.8164 (1.8483)
+2022-11-18 15:27:43,178:INFO: Dataset: univ                Batch:  6/15	Loss 1.8834 (1.8531)
+2022-11-18 15:27:43,341:INFO: Dataset: univ                Batch:  7/15	Loss 1.8489 (1.8525)
+2022-11-18 15:27:43,500:INFO: Dataset: univ                Batch:  8/15	Loss 1.9001 (1.8581)
+2022-11-18 15:27:43,671:INFO: Dataset: univ                Batch:  9/15	Loss 1.8258 (1.8551)
+2022-11-18 15:27:43,844:INFO: Dataset: univ                Batch: 10/15	Loss 1.8361 (1.8530)
+2022-11-18 15:27:44,008:INFO: Dataset: univ                Batch: 11/15	Loss 1.8744 (1.8549)
+2022-11-18 15:27:44,172:INFO: Dataset: univ                Batch: 12/15	Loss 1.8052 (1.8507)
+2022-11-18 15:27:44,334:INFO: Dataset: univ                Batch: 13/15	Loss 1.8738 (1.8526)
+2022-11-18 15:27:44,502:INFO: Dataset: univ                Batch: 14/15	Loss 1.8317 (1.8511)
+2022-11-18 15:27:44,593:INFO: Dataset: univ                Batch: 15/15	Loss 1.9019 (1.8517)
+2022-11-18 15:27:45,029:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9432 (1.9432)
+2022-11-18 15:27:45,230:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8434 (1.8944)
+2022-11-18 15:27:45,411:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8452 (1.8782)
+2022-11-18 15:27:45,582:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9045 (1.8848)
+2022-11-18 15:27:45,766:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9549 (1.8986)
+2022-11-18 15:27:45,939:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8767 (1.8948)
+2022-11-18 15:27:46,117:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9137 (1.8980)
+2022-11-18 15:27:46,281:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0190 (1.9109)
+2022-11-18 15:27:46,780:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9002 (1.9002)
+2022-11-18 15:27:46,969:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8582 (1.8800)
+2022-11-18 15:27:47,151:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8171 (1.8600)
+2022-11-18 15:27:47,330:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9192 (1.8747)
+2022-11-18 15:27:47,514:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8703 (1.8739)
+2022-11-18 15:27:47,713:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7851 (1.8591)
+2022-11-18 15:27:47,947:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8865 (1.8629)
+2022-11-18 15:27:48,142:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8365 (1.8593)
+2022-11-18 15:27:48,325:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8562 (1.8590)
+2022-11-18 15:27:48,520:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8372 (1.8568)
+2022-11-18 15:27:48,682:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8328 (1.8544)
+2022-11-18 15:27:48,864:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9190 (1.8596)
+2022-11-18 15:27:49,039:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8789 (1.8610)
+2022-11-18 15:27:49,219:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8365 (1.8592)
+2022-11-18 15:27:49,402:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8326 (1.8575)
+2022-11-18 15:27:49,568:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8161 (1.8547)
+2022-11-18 15:27:49,755:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8339 (1.8535)
+2022-11-18 15:27:49,925:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8794 (1.8548)
+2022-11-18 15:27:49,982:INFO: - Computing loss (validation)
+2022-11-18 15:27:50,295:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9451 (1.9451)
+2022-11-18 15:27:50,339:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3286 (1.9635)
+2022-11-18 15:27:50,691:INFO: Dataset: univ                Batch: 1/3	Loss 1.9130 (1.9130)
+2022-11-18 15:27:50,770:INFO: Dataset: univ                Batch: 2/3	Loss 1.8780 (1.8949)
+2022-11-18 15:27:50,846:INFO: Dataset: univ                Batch: 3/3	Loss 1.8694 (1.8865)
+2022-11-18 15:27:51,137:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8072 (1.8072)
+2022-11-18 15:27:51,185:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8773 (1.8241)
+2022-11-18 15:27:51,536:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7742 (1.7742)
+2022-11-18 15:27:51,619:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8361 (1.8046)
+2022-11-18 15:27:51,701:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9224 (1.8427)
+2022-11-18 15:27:51,785:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8390 (1.8418)
+2022-11-18 15:27:51,866:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8186 (1.8370)
+2022-11-18 15:27:51,923:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_430.pth.tar
+2022-11-18 15:27:51,924:INFO: 
+===> EPOCH: 431 (P2)
+2022-11-18 15:27:51,924:INFO: - Computing loss (training)
+2022-11-18 15:27:52,319:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9024 (1.9024)
+2022-11-18 15:27:52,486:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8851 (1.8938)
+2022-11-18 15:27:52,658:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9669 (1.9168)
+2022-11-18 15:27:52,782:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7870 (1.8947)
+2022-11-18 15:27:53,216:INFO: Dataset: univ                Batch:  1/15	Loss 1.8858 (1.8858)
+2022-11-18 15:27:53,391:INFO: Dataset: univ                Batch:  2/15	Loss 1.8707 (1.8787)
+2022-11-18 15:27:53,554:INFO: Dataset: univ                Batch:  3/15	Loss 1.8901 (1.8824)
+2022-11-18 15:27:53,717:INFO: Dataset: univ                Batch:  4/15	Loss 1.8878 (1.8836)
+2022-11-18 15:27:53,879:INFO: Dataset: univ                Batch:  5/15	Loss 1.8698 (1.8807)
+2022-11-18 15:27:54,055:INFO: Dataset: univ                Batch:  6/15	Loss 1.8554 (1.8765)
+2022-11-18 15:27:54,223:INFO: Dataset: univ                Batch:  7/15	Loss 1.8143 (1.8683)
+2022-11-18 15:27:54,382:INFO: Dataset: univ                Batch:  8/15	Loss 1.8472 (1.8656)
+2022-11-18 15:27:54,542:INFO: Dataset: univ                Batch:  9/15	Loss 1.8255 (1.8618)
+2022-11-18 15:27:54,706:INFO: Dataset: univ                Batch: 10/15	Loss 1.8602 (1.8617)
+2022-11-18 15:27:54,873:INFO: Dataset: univ                Batch: 11/15	Loss 1.8814 (1.8635)
+2022-11-18 15:27:55,031:INFO: Dataset: univ                Batch: 12/15	Loss 1.8677 (1.8638)
+2022-11-18 15:27:55,198:INFO: Dataset: univ                Batch: 13/15	Loss 1.9239 (1.8682)
+2022-11-18 15:27:55,368:INFO: Dataset: univ                Batch: 14/15	Loss 1.8697 (1.8683)
+2022-11-18 15:27:55,458:INFO: Dataset: univ                Batch: 15/15	Loss 1.8909 (1.8686)
+2022-11-18 15:27:55,860:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9890 (1.9890)
+2022-11-18 15:27:56,015:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9948 (1.9920)
+2022-11-18 15:27:56,169:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9088 (1.9675)
+2022-11-18 15:27:56,323:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8505 (1.9411)
+2022-11-18 15:27:56,477:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0046 (1.9541)
+2022-11-18 15:27:56,631:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8907 (1.9439)
+2022-11-18 15:27:56,783:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9833 (1.9491)
+2022-11-18 15:27:56,922:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9723 (1.9514)
+2022-11-18 15:27:57,326:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9562 (1.9562)
+2022-11-18 15:27:57,493:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7675 (1.8566)
+2022-11-18 15:27:57,675:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8838 (1.8657)
+2022-11-18 15:27:57,835:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8091 (1.8520)
+2022-11-18 15:27:57,990:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7986 (1.8404)
+2022-11-18 15:27:58,146:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9325 (1.8554)
+2022-11-18 15:27:58,300:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8927 (1.8614)
+2022-11-18 15:27:58,453:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8504 (1.8600)
+2022-11-18 15:27:58,608:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8951 (1.8634)
+2022-11-18 15:27:58,769:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8505 (1.8621)
+2022-11-18 15:27:58,939:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8833 (1.8640)
+2022-11-18 15:27:59,100:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8081 (1.8592)
+2022-11-18 15:27:59,258:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8159 (1.8559)
+2022-11-18 15:27:59,412:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7805 (1.8505)
+2022-11-18 15:27:59,569:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8746 (1.8520)
+2022-11-18 15:27:59,722:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8999 (1.8551)
+2022-11-18 15:27:59,879:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8029 (1.8519)
+2022-11-18 15:28:00,023:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8259 (1.8506)
+2022-11-18 15:28:00,074:INFO: - Computing loss (validation)
+2022-11-18 15:28:00,359:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9108 (1.9108)
+2022-11-18 15:28:00,395:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1935 (1.9310)
+2022-11-18 15:28:00,699:INFO: Dataset: univ                Batch: 1/3	Loss 1.9069 (1.9069)
+2022-11-18 15:28:00,778:INFO: Dataset: univ                Batch: 2/3	Loss 1.9301 (1.9185)
+2022-11-18 15:28:00,851:INFO: Dataset: univ                Batch: 3/3	Loss 1.9092 (1.9158)
+2022-11-18 15:28:01,157:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0245 (2.0245)
+2022-11-18 15:28:01,205:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9632 (2.0073)
+2022-11-18 15:28:01,513:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9807 (1.9807)
+2022-11-18 15:28:01,590:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8862 (1.9325)
+2022-11-18 15:28:01,665:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8746 (1.9135)
+2022-11-18 15:28:01,741:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8671 (1.9020)
+2022-11-18 15:28:01,816:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9298 (1.9070)
+2022-11-18 15:28:01,867:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_431.pth.tar
+2022-11-18 15:28:01,867:INFO: 
+===> EPOCH: 432 (P2)
+2022-11-18 15:28:01,868:INFO: - Computing loss (training)
+2022-11-18 15:28:02,208:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8793 (1.8793)
+2022-11-18 15:28:02,366:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8332 (1.8560)
+2022-11-18 15:28:02,522:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8762 (1.8627)
+2022-11-18 15:28:02,642:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7613 (1.8474)
+2022-11-18 15:28:03,061:INFO: Dataset: univ                Batch:  1/15	Loss 1.8751 (1.8751)
+2022-11-18 15:28:03,225:INFO: Dataset: univ                Batch:  2/15	Loss 1.8665 (1.8708)
+2022-11-18 15:28:03,388:INFO: Dataset: univ                Batch:  3/15	Loss 1.8592 (1.8670)
+2022-11-18 15:28:03,548:INFO: Dataset: univ                Batch:  4/15	Loss 1.8862 (1.8717)
+2022-11-18 15:28:03,713:INFO: Dataset: univ                Batch:  5/15	Loss 1.9050 (1.8778)
+2022-11-18 15:28:03,877:INFO: Dataset: univ                Batch:  6/15	Loss 1.8705 (1.8766)
+2022-11-18 15:28:04,038:INFO: Dataset: univ                Batch:  7/15	Loss 1.9189 (1.8825)
+2022-11-18 15:28:04,196:INFO: Dataset: univ                Batch:  8/15	Loss 1.8888 (1.8832)
+2022-11-18 15:28:04,359:INFO: Dataset: univ                Batch:  9/15	Loss 1.8688 (1.8815)
+2022-11-18 15:28:04,518:INFO: Dataset: univ                Batch: 10/15	Loss 1.9159 (1.8851)
+2022-11-18 15:28:04,681:INFO: Dataset: univ                Batch: 11/15	Loss 1.8780 (1.8845)
+2022-11-18 15:28:04,842:INFO: Dataset: univ                Batch: 12/15	Loss 1.8894 (1.8849)
+2022-11-18 15:28:05,010:INFO: Dataset: univ                Batch: 13/15	Loss 1.8401 (1.8816)
+2022-11-18 15:28:05,171:INFO: Dataset: univ                Batch: 14/15	Loss 1.8937 (1.8825)
+2022-11-18 15:28:05,258:INFO: Dataset: univ                Batch: 15/15	Loss 2.0050 (1.8844)
+2022-11-18 15:28:05,644:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8047 (1.8047)
+2022-11-18 15:28:05,798:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8547 (1.8300)
+2022-11-18 15:28:05,957:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7992 (1.8196)
+2022-11-18 15:28:06,109:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9001 (1.8398)
+2022-11-18 15:28:06,267:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8678 (1.8458)
+2022-11-18 15:28:06,427:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9117 (1.8567)
+2022-11-18 15:28:06,587:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9614 (1.8730)
+2022-11-18 15:28:06,731:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8183 (1.8668)
+2022-11-18 15:28:07,137:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8071 (1.8071)
+2022-11-18 15:28:07,295:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8301 (1.8189)
+2022-11-18 15:28:07,452:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7714 (1.8032)
+2022-11-18 15:28:07,613:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8355 (1.8106)
+2022-11-18 15:28:07,775:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8857 (1.8265)
+2022-11-18 15:28:07,942:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9746 (1.8488)
+2022-11-18 15:28:08,101:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9246 (1.8605)
+2022-11-18 15:28:08,259:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8937 (1.8646)
+2022-11-18 15:28:08,418:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9198 (1.8703)
+2022-11-18 15:28:08,571:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8801 (1.8714)
+2022-11-18 15:28:08,725:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8505 (1.8694)
+2022-11-18 15:28:08,881:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9356 (1.8751)
+2022-11-18 15:28:09,035:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7904 (1.8682)
+2022-11-18 15:28:09,188:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8773 (1.8688)
+2022-11-18 15:28:09,346:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9619 (1.8745)
+2022-11-18 15:28:09,505:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7517 (1.8671)
+2022-11-18 15:28:09,662:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8205 (1.8644)
+2022-11-18 15:28:09,806:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9874 (1.8707)
+2022-11-18 15:28:09,860:INFO: - Computing loss (validation)
+2022-11-18 15:28:10,129:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9469 (1.9469)
+2022-11-18 15:28:10,167:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7253 (1.9295)
+2022-11-18 15:28:10,484:INFO: Dataset: univ                Batch: 1/3	Loss 1.8502 (1.8502)
+2022-11-18 15:28:10,569:INFO: Dataset: univ                Batch: 2/3	Loss 1.8539 (1.8520)
+2022-11-18 15:28:10,646:INFO: Dataset: univ                Batch: 3/3	Loss 1.8817 (1.8609)
+2022-11-18 15:28:10,957:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9203 (1.9203)
+2022-11-18 15:28:11,004:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8051 (1.8925)
+2022-11-18 15:28:11,313:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8294 (1.8294)
+2022-11-18 15:28:11,390:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8055 (1.8175)
+2022-11-18 15:28:11,466:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9285 (1.8570)
+2022-11-18 15:28:11,541:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7818 (1.8380)
+2022-11-18 15:28:11,616:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8937 (1.8489)
+2022-11-18 15:28:11,673:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_432.pth.tar
+2022-11-18 15:28:11,673:INFO: 
+===> EPOCH: 433 (P2)
+2022-11-18 15:28:11,674:INFO: - Computing loss (training)
+2022-11-18 15:28:12,023:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0121 (2.0121)
+2022-11-18 15:28:12,190:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9576 (1.9855)
+2022-11-18 15:28:12,350:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9861 (1.9857)
+2022-11-18 15:28:12,476:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7891 (1.9525)
+2022-11-18 15:28:12,900:INFO: Dataset: univ                Batch:  1/15	Loss 1.8669 (1.8669)
+2022-11-18 15:28:13,067:INFO: Dataset: univ                Batch:  2/15	Loss 1.9019 (1.8831)
+2022-11-18 15:28:13,228:INFO: Dataset: univ                Batch:  3/15	Loss 1.8659 (1.8777)
+2022-11-18 15:28:13,397:INFO: Dataset: univ                Batch:  4/15	Loss 1.8614 (1.8734)
+2022-11-18 15:28:13,561:INFO: Dataset: univ                Batch:  5/15	Loss 1.8764 (1.8740)
+2022-11-18 15:28:13,723:INFO: Dataset: univ                Batch:  6/15	Loss 1.9580 (1.8875)
+2022-11-18 15:28:13,887:INFO: Dataset: univ                Batch:  7/15	Loss 1.9068 (1.8903)
+2022-11-18 15:28:14,048:INFO: Dataset: univ                Batch:  8/15	Loss 1.9088 (1.8928)
+2022-11-18 15:28:14,211:INFO: Dataset: univ                Batch:  9/15	Loss 1.8464 (1.8878)
+2022-11-18 15:28:14,373:INFO: Dataset: univ                Batch: 10/15	Loss 1.8801 (1.8871)
+2022-11-18 15:28:14,549:INFO: Dataset: univ                Batch: 11/15	Loss 1.8446 (1.8829)
+2022-11-18 15:28:14,723:INFO: Dataset: univ                Batch: 12/15	Loss 1.8214 (1.8772)
+2022-11-18 15:28:14,897:INFO: Dataset: univ                Batch: 13/15	Loss 1.8586 (1.8760)
+2022-11-18 15:28:15,080:INFO: Dataset: univ                Batch: 14/15	Loss 1.8429 (1.8736)
+2022-11-18 15:28:15,179:INFO: Dataset: univ                Batch: 15/15	Loss 2.0258 (1.8751)
+2022-11-18 15:28:15,593:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9894 (1.9894)
+2022-11-18 15:28:15,743:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9723 (1.9805)
+2022-11-18 15:28:15,895:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8077 (1.9220)
+2022-11-18 15:28:16,043:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8487 (1.9033)
+2022-11-18 15:28:16,195:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8676 (1.8957)
+2022-11-18 15:28:16,356:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9293 (1.9015)
+2022-11-18 15:28:16,512:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8961 (1.9007)
+2022-11-18 15:28:16,650:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8854 (1.8991)
+2022-11-18 15:28:17,077:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8300 (1.8300)
+2022-11-18 15:28:17,235:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8864 (1.8581)
+2022-11-18 15:28:17,399:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9124 (1.8765)
+2022-11-18 15:28:17,566:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7853 (1.8542)
+2022-11-18 15:28:17,734:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8702 (1.8575)
+2022-11-18 15:28:17,907:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7880 (1.8473)
+2022-11-18 15:28:18,077:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9587 (1.8628)
+2022-11-18 15:28:18,243:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9297 (1.8716)
+2022-11-18 15:28:18,415:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7983 (1.8637)
+2022-11-18 15:28:18,570:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8819 (1.8657)
+2022-11-18 15:28:18,725:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8448 (1.8638)
+2022-11-18 15:28:18,884:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8014 (1.8582)
+2022-11-18 15:28:19,040:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9065 (1.8619)
+2022-11-18 15:28:19,196:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8925 (1.8640)
+2022-11-18 15:28:19,354:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8894 (1.8656)
+2022-11-18 15:28:19,512:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8563 (1.8651)
+2022-11-18 15:28:19,668:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8125 (1.8617)
+2022-11-18 15:28:19,810:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8756 (1.8624)
+2022-11-18 15:28:19,854:INFO: - Computing loss (validation)
+2022-11-18 15:28:20,118:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8789 (1.8789)
+2022-11-18 15:28:20,153:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9825 (1.8881)
+2022-11-18 15:28:20,464:INFO: Dataset: univ                Batch: 1/3	Loss 1.8020 (1.8020)
+2022-11-18 15:28:20,541:INFO: Dataset: univ                Batch: 2/3	Loss 1.8687 (1.8379)
+2022-11-18 15:28:20,613:INFO: Dataset: univ                Batch: 3/3	Loss 1.9195 (1.8640)
+2022-11-18 15:28:20,926:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8147 (1.8147)
+2022-11-18 15:28:20,975:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7711 (1.8050)
+2022-11-18 15:28:21,294:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8263 (1.8263)
+2022-11-18 15:28:21,375:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8282 (1.8272)
+2022-11-18 15:28:21,455:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9359 (1.8632)
+2022-11-18 15:28:21,534:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8705 (1.8650)
+2022-11-18 15:28:21,609:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8812 (1.8681)
+2022-11-18 15:28:21,662:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_433.pth.tar
+2022-11-18 15:28:21,663:INFO: 
+===> EPOCH: 434 (P2)
+2022-11-18 15:28:21,663:INFO: - Computing loss (training)
+2022-11-18 15:28:22,003:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8295 (1.8295)
+2022-11-18 15:28:22,157:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8319 (1.8307)
+2022-11-18 15:28:22,311:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9098 (1.8565)
+2022-11-18 15:28:22,429:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0055 (1.8821)
+2022-11-18 15:28:22,846:INFO: Dataset: univ                Batch:  1/15	Loss 1.8654 (1.8654)
+2022-11-18 15:28:23,009:INFO: Dataset: univ                Batch:  2/15	Loss 1.8482 (1.8576)
+2022-11-18 15:28:23,178:INFO: Dataset: univ                Batch:  3/15	Loss 1.8748 (1.8633)
+2022-11-18 15:28:23,349:INFO: Dataset: univ                Batch:  4/15	Loss 1.8713 (1.8653)
+2022-11-18 15:28:23,516:INFO: Dataset: univ                Batch:  5/15	Loss 1.8594 (1.8641)
+2022-11-18 15:28:23,676:INFO: Dataset: univ                Batch:  6/15	Loss 1.9133 (1.8726)
+2022-11-18 15:28:23,849:INFO: Dataset: univ                Batch:  7/15	Loss 1.8643 (1.8714)
+2022-11-18 15:28:24,020:INFO: Dataset: univ                Batch:  8/15	Loss 1.8445 (1.8679)
+2022-11-18 15:28:24,180:INFO: Dataset: univ                Batch:  9/15	Loss 1.8860 (1.8700)
+2022-11-18 15:28:24,339:INFO: Dataset: univ                Batch: 10/15	Loss 1.9075 (1.8733)
+2022-11-18 15:28:24,498:INFO: Dataset: univ                Batch: 11/15	Loss 1.8381 (1.8703)
+2022-11-18 15:28:24,656:INFO: Dataset: univ                Batch: 12/15	Loss 1.9011 (1.8730)
+2022-11-18 15:28:24,816:INFO: Dataset: univ                Batch: 13/15	Loss 1.8839 (1.8739)
+2022-11-18 15:28:24,976:INFO: Dataset: univ                Batch: 14/15	Loss 1.8525 (1.8722)
+2022-11-18 15:28:25,065:INFO: Dataset: univ                Batch: 15/15	Loss 1.8704 (1.8722)
+2022-11-18 15:28:25,485:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7516 (1.7516)
+2022-11-18 15:28:25,720:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8803 (1.8134)
+2022-11-18 15:28:25,879:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0220 (1.8864)
+2022-11-18 15:28:26,038:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8700 (1.8824)
+2022-11-18 15:28:26,192:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8856 (1.8829)
+2022-11-18 15:28:26,351:INFO: Dataset: zara1               Batch: 6/8	Loss 2.1030 (1.9196)
+2022-11-18 15:28:26,507:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8686 (1.9119)
+2022-11-18 15:28:26,649:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8205 (1.9032)
+2022-11-18 15:28:27,036:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8601 (1.8601)
+2022-11-18 15:28:27,189:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8729 (1.8660)
+2022-11-18 15:28:27,343:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8834 (1.8720)
+2022-11-18 15:28:27,500:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8649 (1.8702)
+2022-11-18 15:28:27,655:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8136 (1.8586)
+2022-11-18 15:28:27,812:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8549 (1.8580)
+2022-11-18 15:28:27,964:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8508 (1.8569)
+2022-11-18 15:28:28,126:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8417 (1.8551)
+2022-11-18 15:28:28,290:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8868 (1.8583)
+2022-11-18 15:28:28,447:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9019 (1.8627)
+2022-11-18 15:28:28,601:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8288 (1.8597)
+2022-11-18 15:28:28,755:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8550 (1.8593)
+2022-11-18 15:28:28,909:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9636 (1.8678)
+2022-11-18 15:28:29,060:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8667 (1.8677)
+2022-11-18 15:28:29,213:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8364 (1.8656)
+2022-11-18 15:28:29,365:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8929 (1.8674)
+2022-11-18 15:28:29,517:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8071 (1.8636)
+2022-11-18 15:28:29,656:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8537 (1.8631)
+2022-11-18 15:28:29,702:INFO: - Computing loss (validation)
+2022-11-18 15:28:29,959:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8828 (1.8828)
+2022-11-18 15:28:29,993:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0069 (1.8942)
+2022-11-18 15:28:30,302:INFO: Dataset: univ                Batch: 1/3	Loss 1.9291 (1.9291)
+2022-11-18 15:28:30,383:INFO: Dataset: univ                Batch: 2/3	Loss 1.8817 (1.9050)
+2022-11-18 15:28:30,457:INFO: Dataset: univ                Batch: 3/3	Loss 1.8802 (1.8982)
+2022-11-18 15:28:30,760:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8967 (1.8967)
+2022-11-18 15:28:30,806:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9636 (1.9141)
+2022-11-18 15:28:31,111:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8760 (1.8760)
+2022-11-18 15:28:31,189:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7989 (1.8378)
+2022-11-18 15:28:31,264:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8529 (1.8428)
+2022-11-18 15:28:31,339:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8561 (1.8463)
+2022-11-18 15:28:31,414:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8047 (1.8380)
+2022-11-18 15:28:31,468:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_434.pth.tar
+2022-11-18 15:28:31,468:INFO: 
+===> EPOCH: 435 (P2)
+2022-11-18 15:28:31,469:INFO: - Computing loss (training)
+2022-11-18 15:28:31,829:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7802 (1.7802)
+2022-11-18 15:28:31,983:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8102 (1.7955)
+2022-11-18 15:28:32,135:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8884 (1.8256)
+2022-11-18 15:28:32,253:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9703 (1.8492)
+2022-11-18 15:28:32,649:INFO: Dataset: univ                Batch:  1/15	Loss 1.8779 (1.8779)
+2022-11-18 15:28:32,812:INFO: Dataset: univ                Batch:  2/15	Loss 1.8270 (1.8529)
+2022-11-18 15:28:32,977:INFO: Dataset: univ                Batch:  3/15	Loss 1.8542 (1.8533)
+2022-11-18 15:28:33,141:INFO: Dataset: univ                Batch:  4/15	Loss 1.9023 (1.8638)
+2022-11-18 15:28:33,307:INFO: Dataset: univ                Batch:  5/15	Loss 1.8576 (1.8626)
+2022-11-18 15:28:33,468:INFO: Dataset: univ                Batch:  6/15	Loss 1.8872 (1.8667)
+2022-11-18 15:28:33,628:INFO: Dataset: univ                Batch:  7/15	Loss 1.9096 (1.8727)
+2022-11-18 15:28:33,787:INFO: Dataset: univ                Batch:  8/15	Loss 1.8602 (1.8710)
+2022-11-18 15:28:33,946:INFO: Dataset: univ                Batch:  9/15	Loss 1.8805 (1.8720)
+2022-11-18 15:28:34,105:INFO: Dataset: univ                Batch: 10/15	Loss 1.8498 (1.8698)
+2022-11-18 15:28:34,275:INFO: Dataset: univ                Batch: 11/15	Loss 1.8829 (1.8708)
+2022-11-18 15:28:34,445:INFO: Dataset: univ                Batch: 12/15	Loss 1.8477 (1.8688)
+2022-11-18 15:28:34,611:INFO: Dataset: univ                Batch: 13/15	Loss 1.8290 (1.8658)
+2022-11-18 15:28:34,771:INFO: Dataset: univ                Batch: 14/15	Loss 1.8679 (1.8659)
+2022-11-18 15:28:34,857:INFO: Dataset: univ                Batch: 15/15	Loss 1.9282 (1.8666)
+2022-11-18 15:28:35,255:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8913 (1.8913)
+2022-11-18 15:28:35,416:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9174 (1.9042)
+2022-11-18 15:28:35,570:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9578 (1.9228)
+2022-11-18 15:28:35,722:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8412 (1.9047)
+2022-11-18 15:28:35,877:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7919 (1.8797)
+2022-11-18 15:28:36,031:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9527 (1.8909)
+2022-11-18 15:28:36,188:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8761 (1.8888)
+2022-11-18 15:28:36,332:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8135 (1.8790)
+2022-11-18 15:28:36,735:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7266 (1.7266)
+2022-11-18 15:28:36,893:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7347 (1.7307)
+2022-11-18 15:28:37,053:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8083 (1.7565)
+2022-11-18 15:28:37,215:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8047 (1.7681)
+2022-11-18 15:28:37,383:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9523 (1.8010)
+2022-11-18 15:28:37,542:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8767 (1.8145)
+2022-11-18 15:28:37,702:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7900 (1.8113)
+2022-11-18 15:28:37,858:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9359 (1.8273)
+2022-11-18 15:28:38,014:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9021 (1.8360)
+2022-11-18 15:28:38,176:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9016 (1.8422)
+2022-11-18 15:28:38,345:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8279 (1.8409)
+2022-11-18 15:28:38,508:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9043 (1.8466)
+2022-11-18 15:28:38,667:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8101 (1.8436)
+2022-11-18 15:28:38,826:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8623 (1.8449)
+2022-11-18 15:28:38,989:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8374 (1.8445)
+2022-11-18 15:28:39,155:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8227 (1.8430)
+2022-11-18 15:28:39,322:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8193 (1.8416)
+2022-11-18 15:28:39,466:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8147 (1.8401)
+2022-11-18 15:28:39,511:INFO: - Computing loss (validation)
+2022-11-18 15:28:39,792:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9109 (1.9109)
+2022-11-18 15:28:39,831:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1292 (1.9265)
+2022-11-18 15:28:40,156:INFO: Dataset: univ                Batch: 1/3	Loss 1.8284 (1.8284)
+2022-11-18 15:28:40,245:INFO: Dataset: univ                Batch: 2/3	Loss 1.8468 (1.8382)
+2022-11-18 15:28:40,327:INFO: Dataset: univ                Batch: 3/3	Loss 1.8982 (1.8590)
+2022-11-18 15:28:40,647:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9190 (1.9190)
+2022-11-18 15:28:40,694:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8190 (1.8959)
+2022-11-18 15:28:41,001:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8482 (1.8482)
+2022-11-18 15:28:41,081:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9280 (1.8862)
+2022-11-18 15:28:41,161:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8890 (1.8872)
+2022-11-18 15:28:41,243:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9704 (1.9074)
+2022-11-18 15:28:41,328:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8097 (1.8871)
+2022-11-18 15:28:41,383:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_435.pth.tar
+2022-11-18 15:28:41,383:INFO: 
+===> EPOCH: 436 (P2)
+2022-11-18 15:28:41,383:INFO: - Computing loss (training)
+2022-11-18 15:28:41,770:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8616 (1.8616)
+2022-11-18 15:28:41,934:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9877 (1.9234)
+2022-11-18 15:28:42,095:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9204 (1.9224)
+2022-11-18 15:28:42,213:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8076 (1.9030)
+2022-11-18 15:28:42,635:INFO: Dataset: univ                Batch:  1/15	Loss 1.8552 (1.8552)
+2022-11-18 15:28:42,807:INFO: Dataset: univ                Batch:  2/15	Loss 1.9122 (1.8818)
+2022-11-18 15:28:42,989:INFO: Dataset: univ                Batch:  3/15	Loss 1.8809 (1.8815)
+2022-11-18 15:28:43,165:INFO: Dataset: univ                Batch:  4/15	Loss 1.8388 (1.8707)
+2022-11-18 15:28:43,338:INFO: Dataset: univ                Batch:  5/15	Loss 1.8579 (1.8681)
+2022-11-18 15:28:43,508:INFO: Dataset: univ                Batch:  6/15	Loss 1.8485 (1.8647)
+2022-11-18 15:28:43,677:INFO: Dataset: univ                Batch:  7/15	Loss 1.8928 (1.8686)
+2022-11-18 15:28:43,842:INFO: Dataset: univ                Batch:  8/15	Loss 1.8403 (1.8653)
+2022-11-18 15:28:44,002:INFO: Dataset: univ                Batch:  9/15	Loss 1.8302 (1.8617)
+2022-11-18 15:28:44,159:INFO: Dataset: univ                Batch: 10/15	Loss 1.9274 (1.8677)
+2022-11-18 15:28:44,320:INFO: Dataset: univ                Batch: 11/15	Loss 1.8797 (1.8688)
+2022-11-18 15:28:44,477:INFO: Dataset: univ                Batch: 12/15	Loss 1.8955 (1.8710)
+2022-11-18 15:28:44,636:INFO: Dataset: univ                Batch: 13/15	Loss 1.8608 (1.8701)
+2022-11-18 15:28:44,793:INFO: Dataset: univ                Batch: 14/15	Loss 1.8738 (1.8704)
+2022-11-18 15:28:44,879:INFO: Dataset: univ                Batch: 15/15	Loss 1.7468 (1.8691)
+2022-11-18 15:28:45,270:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8359 (1.8359)
+2022-11-18 15:28:45,424:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8893 (1.8624)
+2022-11-18 15:28:45,576:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8257 (1.8495)
+2022-11-18 15:28:45,729:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7713 (1.8309)
+2022-11-18 15:28:45,883:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8953 (1.8436)
+2022-11-18 15:28:46,033:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7858 (1.8346)
+2022-11-18 15:28:46,187:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8738 (1.8402)
+2022-11-18 15:28:46,326:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9628 (1.8543)
+2022-11-18 15:28:46,726:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9223 (1.9223)
+2022-11-18 15:28:46,879:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9115 (1.9168)
+2022-11-18 15:28:47,037:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9508 (1.9278)
+2022-11-18 15:28:47,191:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8596 (1.9119)
+2022-11-18 15:28:47,350:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8620 (1.9020)
+2022-11-18 15:28:47,509:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9702 (1.9134)
+2022-11-18 15:28:47,663:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8532 (1.9050)
+2022-11-18 15:28:47,816:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7161 (1.8816)
+2022-11-18 15:28:47,969:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8991 (1.8838)
+2022-11-18 15:28:48,122:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8199 (1.8773)
+2022-11-18 15:28:48,277:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8516 (1.8750)
+2022-11-18 15:28:48,439:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8581 (1.8735)
+2022-11-18 15:28:48,623:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8706 (1.8733)
+2022-11-18 15:28:48,798:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9526 (1.8780)
+2022-11-18 15:28:48,970:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8523 (1.8763)
+2022-11-18 15:28:49,136:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8633 (1.8755)
+2022-11-18 15:28:49,291:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9679 (1.8811)
+2022-11-18 15:28:49,433:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9352 (1.8838)
+2022-11-18 15:28:49,484:INFO: - Computing loss (validation)
+2022-11-18 15:28:49,746:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9954 (1.9954)
+2022-11-18 15:28:49,786:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6871 (1.9723)
+2022-11-18 15:28:50,121:INFO: Dataset: univ                Batch: 1/3	Loss 1.8708 (1.8708)
+2022-11-18 15:28:50,200:INFO: Dataset: univ                Batch: 2/3	Loss 1.8859 (1.8787)
+2022-11-18 15:28:50,273:INFO: Dataset: univ                Batch: 3/3	Loss 1.8132 (1.8582)
+2022-11-18 15:28:50,591:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9238 (1.9238)
+2022-11-18 15:28:50,636:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8813 (1.9130)
+2022-11-18 15:28:50,972:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8654 (1.8654)
+2022-11-18 15:28:51,059:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8171 (1.8402)
+2022-11-18 15:28:51,149:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9251 (1.8695)
+2022-11-18 15:28:51,255:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8524 (1.8653)
+2022-11-18 15:28:51,341:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8702 (1.8663)
+2022-11-18 15:28:51,397:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_436.pth.tar
+2022-11-18 15:28:51,397:INFO: 
+===> EPOCH: 437 (P2)
+2022-11-18 15:28:51,398:INFO: - Computing loss (training)
+2022-11-18 15:28:51,744:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9610 (1.9610)
+2022-11-18 15:28:51,927:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9485 (1.9550)
+2022-11-18 15:28:52,110:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9547 (1.9549)
+2022-11-18 15:28:52,250:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9444 (1.9530)
+2022-11-18 15:28:52,759:INFO: Dataset: univ                Batch:  1/15	Loss 1.8718 (1.8718)
+2022-11-18 15:28:52,944:INFO: Dataset: univ                Batch:  2/15	Loss 1.8425 (1.8560)
+2022-11-18 15:28:53,126:INFO: Dataset: univ                Batch:  3/15	Loss 1.8850 (1.8664)
+2022-11-18 15:28:53,302:INFO: Dataset: univ                Batch:  4/15	Loss 1.8751 (1.8686)
+2022-11-18 15:28:53,466:INFO: Dataset: univ                Batch:  5/15	Loss 1.9029 (1.8753)
+2022-11-18 15:28:53,623:INFO: Dataset: univ                Batch:  6/15	Loss 1.8831 (1.8766)
+2022-11-18 15:28:53,790:INFO: Dataset: univ                Batch:  7/15	Loss 1.9078 (1.8811)
+2022-11-18 15:28:53,948:INFO: Dataset: univ                Batch:  8/15	Loss 1.8897 (1.8821)
+2022-11-18 15:28:54,104:INFO: Dataset: univ                Batch:  9/15	Loss 1.8672 (1.8804)
+2022-11-18 15:28:54,269:INFO: Dataset: univ                Batch: 10/15	Loss 1.8648 (1.8788)
+2022-11-18 15:28:54,438:INFO: Dataset: univ                Batch: 11/15	Loss 1.8529 (1.8764)
+2022-11-18 15:28:54,605:INFO: Dataset: univ                Batch: 12/15	Loss 1.8578 (1.8749)
+2022-11-18 15:28:54,772:INFO: Dataset: univ                Batch: 13/15	Loss 1.8840 (1.8755)
+2022-11-18 15:28:54,933:INFO: Dataset: univ                Batch: 14/15	Loss 1.8801 (1.8758)
+2022-11-18 15:28:55,017:INFO: Dataset: univ                Batch: 15/15	Loss 1.9038 (1.8761)
+2022-11-18 15:28:55,416:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8879 (1.8879)
+2022-11-18 15:28:55,569:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0223 (1.9577)
+2022-11-18 15:28:55,725:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7828 (1.8996)
+2022-11-18 15:28:55,891:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8540 (1.8874)
+2022-11-18 15:28:56,060:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9631 (1.9010)
+2022-11-18 15:28:56,223:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7711 (1.8806)
+2022-11-18 15:28:56,392:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7440 (1.8612)
+2022-11-18 15:28:56,534:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7901 (1.8535)
+2022-11-18 15:28:56,937:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8883 (1.8883)
+2022-11-18 15:28:57,091:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9023 (1.8961)
+2022-11-18 15:28:57,257:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8741 (1.8888)
+2022-11-18 15:28:57,428:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9343 (1.9010)
+2022-11-18 15:28:57,596:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8459 (1.8906)
+2022-11-18 15:28:57,761:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8870 (1.8899)
+2022-11-18 15:28:57,931:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8597 (1.8853)
+2022-11-18 15:28:58,094:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9414 (1.8928)
+2022-11-18 15:28:58,257:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8616 (1.8897)
+2022-11-18 15:28:58,421:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8387 (1.8848)
+2022-11-18 15:28:58,606:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9385 (1.8897)
+2022-11-18 15:28:58,810:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9730 (1.8964)
+2022-11-18 15:28:58,999:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8717 (1.8944)
+2022-11-18 15:28:59,183:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7937 (1.8869)
+2022-11-18 15:28:59,367:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9717 (1.8927)
+2022-11-18 15:28:59,566:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8762 (1.8917)
+2022-11-18 15:28:59,761:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9669 (1.8961)
+2022-11-18 15:28:59,940:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8287 (1.8931)
+2022-11-18 15:29:00,000:INFO: - Computing loss (validation)
+2022-11-18 15:29:00,312:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8632 (1.8632)
+2022-11-18 15:29:00,353:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4411 (1.8358)
+2022-11-18 15:29:00,702:INFO: Dataset: univ                Batch: 1/3	Loss 1.8424 (1.8424)
+2022-11-18 15:29:00,790:INFO: Dataset: univ                Batch: 2/3	Loss 1.8111 (1.8251)
+2022-11-18 15:29:00,868:INFO: Dataset: univ                Batch: 3/3	Loss 1.8562 (1.8347)
+2022-11-18 15:29:01,195:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8461 (1.8461)
+2022-11-18 15:29:01,245:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9981 (1.8847)
+2022-11-18 15:29:01,617:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8128 (1.8128)
+2022-11-18 15:29:01,724:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8270 (1.8194)
+2022-11-18 15:29:01,819:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8887 (1.8426)
+2022-11-18 15:29:01,915:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8825 (1.8532)
+2022-11-18 15:29:02,013:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8716 (1.8567)
+2022-11-18 15:29:02,079:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_437.pth.tar
+2022-11-18 15:29:02,080:INFO: 
+===> EPOCH: 438 (P2)
+2022-11-18 15:29:02,081:INFO: - Computing loss (training)
+2022-11-18 15:29:02,464:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8837 (1.8837)
+2022-11-18 15:29:02,692:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8579 (1.8705)
+2022-11-18 15:29:02,944:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9301 (1.8902)
+2022-11-18 15:29:03,139:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9705 (1.9023)
+2022-11-18 15:29:03,801:INFO: Dataset: univ                Batch:  1/15	Loss 1.8545 (1.8545)
+2022-11-18 15:29:04,043:INFO: Dataset: univ                Batch:  2/15	Loss 1.8657 (1.8606)
+2022-11-18 15:29:04,309:INFO: Dataset: univ                Batch:  3/15	Loss 1.8893 (1.8703)
+2022-11-18 15:29:04,614:INFO: Dataset: univ                Batch:  4/15	Loss 1.8900 (1.8758)
+2022-11-18 15:29:04,916:INFO: Dataset: univ                Batch:  5/15	Loss 1.8763 (1.8759)
+2022-11-18 15:29:05,179:INFO: Dataset: univ                Batch:  6/15	Loss 1.8900 (1.8784)
+2022-11-18 15:29:05,390:INFO: Dataset: univ                Batch:  7/15	Loss 1.8685 (1.8771)
+2022-11-18 15:29:05,650:INFO: Dataset: univ                Batch:  8/15	Loss 1.8439 (1.8730)
+2022-11-18 15:29:05,868:INFO: Dataset: univ                Batch:  9/15	Loss 1.9271 (1.8788)
+2022-11-18 15:29:06,099:INFO: Dataset: univ                Batch: 10/15	Loss 1.8935 (1.8802)
+2022-11-18 15:29:06,393:INFO: Dataset: univ                Batch: 11/15	Loss 1.8736 (1.8795)
+2022-11-18 15:29:06,685:INFO: Dataset: univ                Batch: 12/15	Loss 1.8440 (1.8766)
+2022-11-18 15:29:06,984:INFO: Dataset: univ                Batch: 13/15	Loss 1.8527 (1.8749)
+2022-11-18 15:29:07,240:INFO: Dataset: univ                Batch: 14/15	Loss 1.8677 (1.8744)
+2022-11-18 15:29:07,383:INFO: Dataset: univ                Batch: 15/15	Loss 1.8674 (1.8743)
+2022-11-18 15:29:07,935:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8658 (1.8658)
+2022-11-18 15:29:08,126:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8802 (1.8730)
+2022-11-18 15:29:08,324:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8907 (1.8786)
+2022-11-18 15:29:08,518:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8830 (1.8796)
+2022-11-18 15:29:08,718:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0705 (1.9195)
+2022-11-18 15:29:08,919:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7938 (1.8995)
+2022-11-18 15:29:09,116:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8783 (1.8966)
+2022-11-18 15:29:09,285:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9283 (1.8999)
+2022-11-18 15:29:09,792:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8802 (1.8802)
+2022-11-18 15:29:09,976:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9242 (1.8999)
+2022-11-18 15:29:10,164:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8421 (1.8823)
+2022-11-18 15:29:10,351:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8108 (1.8636)
+2022-11-18 15:29:10,540:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9297 (1.8764)
+2022-11-18 15:29:10,732:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8449 (1.8712)
+2022-11-18 15:29:10,923:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8618 (1.8700)
+2022-11-18 15:29:11,109:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8525 (1.8678)
+2022-11-18 15:29:11,300:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8846 (1.8697)
+2022-11-18 15:29:11,502:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8604 (1.8688)
+2022-11-18 15:29:11,692:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8750 (1.8693)
+2022-11-18 15:29:11,879:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9169 (1.8732)
+2022-11-18 15:29:12,067:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8641 (1.8725)
+2022-11-18 15:29:12,256:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8494 (1.8712)
+2022-11-18 15:29:12,458:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9288 (1.8750)
+2022-11-18 15:29:12,647:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7808 (1.8696)
+2022-11-18 15:29:12,832:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8054 (1.8655)
+2022-11-18 15:29:13,014:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8852 (1.8665)
+2022-11-18 15:29:13,077:INFO: - Computing loss (validation)
+2022-11-18 15:29:13,421:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7736 (1.7736)
+2022-11-18 15:29:13,467:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0219 (1.7905)
+2022-11-18 15:29:13,859:INFO: Dataset: univ                Batch: 1/3	Loss 1.8592 (1.8592)
+2022-11-18 15:29:13,949:INFO: Dataset: univ                Batch: 2/3	Loss 1.8895 (1.8743)
+2022-11-18 15:29:14,035:INFO: Dataset: univ                Batch: 3/3	Loss 1.9068 (1.8857)
+2022-11-18 15:29:14,412:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0141 (2.0141)
+2022-11-18 15:29:14,469:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8269 (1.9648)
+2022-11-18 15:29:14,875:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8044 (1.8044)
+2022-11-18 15:29:14,972:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8896 (1.8472)
+2022-11-18 15:29:15,061:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9029 (1.8659)
+2022-11-18 15:29:15,147:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9117 (1.8771)
+2022-11-18 15:29:15,229:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8374 (1.8691)
+2022-11-18 15:29:15,297:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_438.pth.tar
+2022-11-18 15:29:15,297:INFO: 
+===> EPOCH: 439 (P2)
+2022-11-18 15:29:15,298:INFO: - Computing loss (training)
+2022-11-18 15:29:15,723:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9328 (1.9328)
+2022-11-18 15:29:15,915:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8440 (1.8880)
+2022-11-18 15:29:16,097:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8686 (1.8815)
+2022-11-18 15:29:16,248:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8980 (1.8842)
+2022-11-18 15:29:16,737:INFO: Dataset: univ                Batch:  1/15	Loss 1.8791 (1.8791)
+2022-11-18 15:29:16,933:INFO: Dataset: univ                Batch:  2/15	Loss 1.8404 (1.8581)
+2022-11-18 15:29:17,130:INFO: Dataset: univ                Batch:  3/15	Loss 1.8581 (1.8581)
+2022-11-18 15:29:17,327:INFO: Dataset: univ                Batch:  4/15	Loss 1.8696 (1.8611)
+2022-11-18 15:29:17,525:INFO: Dataset: univ                Batch:  5/15	Loss 1.8667 (1.8623)
+2022-11-18 15:29:17,721:INFO: Dataset: univ                Batch:  6/15	Loss 1.8438 (1.8592)
+2022-11-18 15:29:17,915:INFO: Dataset: univ                Batch:  7/15	Loss 1.8693 (1.8607)
+2022-11-18 15:29:18,108:INFO: Dataset: univ                Batch:  8/15	Loss 1.8722 (1.8622)
+2022-11-18 15:29:18,298:INFO: Dataset: univ                Batch:  9/15	Loss 1.8724 (1.8633)
+2022-11-18 15:29:18,495:INFO: Dataset: univ                Batch: 10/15	Loss 1.8910 (1.8664)
+2022-11-18 15:29:18,698:INFO: Dataset: univ                Batch: 11/15	Loss 1.8611 (1.8658)
+2022-11-18 15:29:18,889:INFO: Dataset: univ                Batch: 12/15	Loss 1.8632 (1.8656)
+2022-11-18 15:29:19,084:INFO: Dataset: univ                Batch: 13/15	Loss 1.9204 (1.8698)
+2022-11-18 15:29:19,281:INFO: Dataset: univ                Batch: 14/15	Loss 1.8598 (1.8691)
+2022-11-18 15:29:19,387:INFO: Dataset: univ                Batch: 15/15	Loss 1.9170 (1.8696)
+2022-11-18 15:29:19,853:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8770 (1.8770)
+2022-11-18 15:29:20,034:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8877 (1.8826)
+2022-11-18 15:29:20,224:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9061 (1.8907)
+2022-11-18 15:29:20,409:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9835 (1.9135)
+2022-11-18 15:29:20,596:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8844 (1.9077)
+2022-11-18 15:29:20,784:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8326 (1.8949)
+2022-11-18 15:29:20,970:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8609 (1.8899)
+2022-11-18 15:29:21,127:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9363 (1.8954)
+2022-11-18 15:29:21,587:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9251 (1.9251)
+2022-11-18 15:29:21,773:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7958 (1.8595)
+2022-11-18 15:29:21,964:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9086 (1.8771)
+2022-11-18 15:29:22,150:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8262 (1.8643)
+2022-11-18 15:29:22,342:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8985 (1.8710)
+2022-11-18 15:29:22,537:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7805 (1.8554)
+2022-11-18 15:29:22,724:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8015 (1.8471)
+2022-11-18 15:29:22,907:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8711 (1.8503)
+2022-11-18 15:29:23,094:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8275 (1.8476)
+2022-11-18 15:29:23,278:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8556 (1.8485)
+2022-11-18 15:29:23,463:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8002 (1.8439)
+2022-11-18 15:29:23,648:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8986 (1.8483)
+2022-11-18 15:29:23,836:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9212 (1.8537)
+2022-11-18 15:29:24,023:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8964 (1.8568)
+2022-11-18 15:29:24,198:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8353 (1.8553)
+2022-11-18 15:29:24,384:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7920 (1.8515)
+2022-11-18 15:29:24,569:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9530 (1.8573)
+2022-11-18 15:29:24,737:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8740 (1.8581)
+2022-11-18 15:29:24,795:INFO: - Computing loss (validation)
+2022-11-18 15:29:25,113:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8549 (1.8549)
+2022-11-18 15:29:25,155:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5103 (1.8314)
+2022-11-18 15:29:25,544:INFO: Dataset: univ                Batch: 1/3	Loss 1.8713 (1.8713)
+2022-11-18 15:29:25,646:INFO: Dataset: univ                Batch: 2/3	Loss 1.8480 (1.8591)
+2022-11-18 15:29:25,733:INFO: Dataset: univ                Batch: 3/3	Loss 1.8536 (1.8575)
+2022-11-18 15:29:26,106:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9358 (1.9358)
+2022-11-18 15:29:26,170:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8580 (1.9183)
+2022-11-18 15:29:26,549:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8149 (1.8149)
+2022-11-18 15:29:26,634:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8964 (1.8564)
+2022-11-18 15:29:26,719:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9311 (1.8813)
+2022-11-18 15:29:26,804:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8385 (1.8703)
+2022-11-18 15:29:26,889:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9623 (1.8873)
+2022-11-18 15:29:26,960:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_439.pth.tar
+2022-11-18 15:29:26,960:INFO: 
+===> EPOCH: 440 (P2)
+2022-11-18 15:29:26,961:INFO: - Computing loss (training)
+2022-11-18 15:29:27,345:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0029 (2.0029)
+2022-11-18 15:29:27,539:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9574 (1.9783)
+2022-11-18 15:29:27,725:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9461 (1.9676)
+2022-11-18 15:29:27,874:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9622 (1.9667)
+2022-11-18 15:29:28,384:INFO: Dataset: univ                Batch:  1/15	Loss 1.8675 (1.8675)
+2022-11-18 15:29:28,584:INFO: Dataset: univ                Batch:  2/15	Loss 1.8744 (1.8708)
+2022-11-18 15:29:28,786:INFO: Dataset: univ                Batch:  3/15	Loss 1.8258 (1.8559)
+2022-11-18 15:29:28,985:INFO: Dataset: univ                Batch:  4/15	Loss 1.8655 (1.8582)
+2022-11-18 15:29:29,182:INFO: Dataset: univ                Batch:  5/15	Loss 1.8885 (1.8646)
+2022-11-18 15:29:29,379:INFO: Dataset: univ                Batch:  6/15	Loss 1.8837 (1.8678)
+2022-11-18 15:29:29,575:INFO: Dataset: univ                Batch:  7/15	Loss 1.8898 (1.8711)
+2022-11-18 15:29:29,766:INFO: Dataset: univ                Batch:  8/15	Loss 1.8390 (1.8678)
+2022-11-18 15:29:29,959:INFO: Dataset: univ                Batch:  9/15	Loss 1.9336 (1.8742)
+2022-11-18 15:29:30,142:INFO: Dataset: univ                Batch: 10/15	Loss 1.8366 (1.8704)
+2022-11-18 15:29:30,333:INFO: Dataset: univ                Batch: 11/15	Loss 1.9179 (1.8747)
+2022-11-18 15:29:30,531:INFO: Dataset: univ                Batch: 12/15	Loss 1.9257 (1.8787)
+2022-11-18 15:29:30,723:INFO: Dataset: univ                Batch: 13/15	Loss 1.8843 (1.8791)
+2022-11-18 15:29:30,916:INFO: Dataset: univ                Batch: 14/15	Loss 1.8740 (1.8788)
+2022-11-18 15:29:31,022:INFO: Dataset: univ                Batch: 15/15	Loss 1.8995 (1.8791)
+2022-11-18 15:29:31,520:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9092 (1.9092)
+2022-11-18 15:29:31,702:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8721 (1.8906)
+2022-11-18 15:29:31,906:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8648 (1.8820)
+2022-11-18 15:29:32,088:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9813 (1.9056)
+2022-11-18 15:29:32,275:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7919 (1.8832)
+2022-11-18 15:29:32,488:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9365 (1.8915)
+2022-11-18 15:29:32,715:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8641 (1.8878)
+2022-11-18 15:29:32,922:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9017 (1.8893)
+2022-11-18 15:29:33,494:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8785 (1.8785)
+2022-11-18 15:29:33,699:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8696 (1.8742)
+2022-11-18 15:29:33,901:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8729 (1.8738)
+2022-11-18 15:29:34,119:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8924 (1.8787)
+2022-11-18 15:29:34,319:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8158 (1.8654)
+2022-11-18 15:29:34,513:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8406 (1.8611)
+2022-11-18 15:29:34,707:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9359 (1.8722)
+2022-11-18 15:29:34,892:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8395 (1.8681)
+2022-11-18 15:29:35,075:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8275 (1.8636)
+2022-11-18 15:29:35,258:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8466 (1.8618)
+2022-11-18 15:29:35,447:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8177 (1.8576)
+2022-11-18 15:29:35,638:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8186 (1.8547)
+2022-11-18 15:29:35,849:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8963 (1.8579)
+2022-11-18 15:29:36,042:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8783 (1.8595)
+2022-11-18 15:29:36,269:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7225 (1.8502)
+2022-11-18 15:29:36,454:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8886 (1.8522)
+2022-11-18 15:29:36,656:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9419 (1.8578)
+2022-11-18 15:29:36,824:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9637 (1.8628)
+2022-11-18 15:29:36,882:INFO: - Computing loss (validation)
+2022-11-18 15:29:37,207:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8549 (1.8549)
+2022-11-18 15:29:37,248:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1578 (1.8766)
+2022-11-18 15:29:37,611:INFO: Dataset: univ                Batch: 1/3	Loss 1.9166 (1.9166)
+2022-11-18 15:29:37,709:INFO: Dataset: univ                Batch: 2/3	Loss 1.8936 (1.9046)
+2022-11-18 15:29:37,805:INFO: Dataset: univ                Batch: 3/3	Loss 1.9113 (1.9064)
+2022-11-18 15:29:38,157:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9687 (1.9687)
+2022-11-18 15:29:38,210:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7945 (1.9228)
+2022-11-18 15:29:38,593:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7817 (1.7817)
+2022-11-18 15:29:38,689:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7382 (1.7593)
+2022-11-18 15:29:38,776:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9015 (1.8062)
+2022-11-18 15:29:38,865:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7894 (1.8021)
+2022-11-18 15:29:38,959:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9134 (1.8243)
+2022-11-18 15:29:39,033:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_440.pth.tar
+2022-11-18 15:29:39,034:INFO: 
+===> EPOCH: 441 (P2)
+2022-11-18 15:29:39,035:INFO: - Computing loss (training)
+2022-11-18 15:29:39,469:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8558 (1.8558)
+2022-11-18 15:29:39,653:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8455 (1.8504)
+2022-11-18 15:29:39,839:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9526 (1.8842)
+2022-11-18 15:29:39,998:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7571 (1.8631)
+2022-11-18 15:29:40,508:INFO: Dataset: univ                Batch:  1/15	Loss 1.8491 (1.8491)
+2022-11-18 15:29:40,704:INFO: Dataset: univ                Batch:  2/15	Loss 1.9089 (1.8776)
+2022-11-18 15:29:40,898:INFO: Dataset: univ                Batch:  3/15	Loss 1.8293 (1.8629)
+2022-11-18 15:29:41,098:INFO: Dataset: univ                Batch:  4/15	Loss 1.8434 (1.8578)
+2022-11-18 15:29:41,292:INFO: Dataset: univ                Batch:  5/15	Loss 1.8962 (1.8651)
+2022-11-18 15:29:41,487:INFO: Dataset: univ                Batch:  6/15	Loss 1.8818 (1.8679)
+2022-11-18 15:29:41,679:INFO: Dataset: univ                Batch:  7/15	Loss 1.9023 (1.8728)
+2022-11-18 15:29:41,874:INFO: Dataset: univ                Batch:  8/15	Loss 1.8600 (1.8713)
+2022-11-18 15:29:42,070:INFO: Dataset: univ                Batch:  9/15	Loss 1.8292 (1.8668)
+2022-11-18 15:29:42,260:INFO: Dataset: univ                Batch: 10/15	Loss 1.8518 (1.8652)
+2022-11-18 15:29:42,453:INFO: Dataset: univ                Batch: 11/15	Loss 1.8604 (1.8648)
+2022-11-18 15:29:42,643:INFO: Dataset: univ                Batch: 12/15	Loss 1.8893 (1.8667)
+2022-11-18 15:29:42,834:INFO: Dataset: univ                Batch: 13/15	Loss 1.8853 (1.8681)
+2022-11-18 15:29:43,027:INFO: Dataset: univ                Batch: 14/15	Loss 1.8234 (1.8649)
+2022-11-18 15:29:43,135:INFO: Dataset: univ                Batch: 15/15	Loss 1.8101 (1.8642)
+2022-11-18 15:29:43,599:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8998 (1.8998)
+2022-11-18 15:29:43,779:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8783 (1.8887)
+2022-11-18 15:29:43,968:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9113 (1.8970)
+2022-11-18 15:29:44,156:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9905 (1.9180)
+2022-11-18 15:29:44,339:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0265 (1.9373)
+2022-11-18 15:29:44,526:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7980 (1.9119)
+2022-11-18 15:29:44,708:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9241 (1.9138)
+2022-11-18 15:29:44,878:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8496 (1.9059)
+2022-11-18 15:29:45,370:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9038 (1.9038)
+2022-11-18 15:29:45,572:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7954 (1.8477)
+2022-11-18 15:29:45,764:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8271 (1.8411)
+2022-11-18 15:29:45,956:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9107 (1.8564)
+2022-11-18 15:29:46,145:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8225 (1.8491)
+2022-11-18 15:29:46,331:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9488 (1.8658)
+2022-11-18 15:29:46,517:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8166 (1.8593)
+2022-11-18 15:29:46,694:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9284 (1.8684)
+2022-11-18 15:29:46,876:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8302 (1.8638)
+2022-11-18 15:29:47,062:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8467 (1.8622)
+2022-11-18 15:29:47,241:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9011 (1.8659)
+2022-11-18 15:29:47,424:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9036 (1.8689)
+2022-11-18 15:29:47,609:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8356 (1.8664)
+2022-11-18 15:29:47,801:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9807 (1.8734)
+2022-11-18 15:29:47,993:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8971 (1.8748)
+2022-11-18 15:29:48,174:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7851 (1.8691)
+2022-11-18 15:29:48,357:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8977 (1.8705)
+2022-11-18 15:29:48,536:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8794 (1.8709)
+2022-11-18 15:29:48,596:INFO: - Computing loss (validation)
+2022-11-18 15:29:48,895:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8966 (1.8966)
+2022-11-18 15:29:48,934:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8129 (1.8892)
+2022-11-18 15:29:49,304:INFO: Dataset: univ                Batch: 1/3	Loss 1.8465 (1.8465)
+2022-11-18 15:29:49,403:INFO: Dataset: univ                Batch: 2/3	Loss 1.8802 (1.8643)
+2022-11-18 15:29:49,490:INFO: Dataset: univ                Batch: 3/3	Loss 1.8708 (1.8662)
+2022-11-18 15:29:49,876:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9365 (1.9365)
+2022-11-18 15:29:49,933:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0295 (1.9580)
+2022-11-18 15:29:50,315:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8681 (1.8681)
+2022-11-18 15:29:50,412:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8906 (1.8793)
+2022-11-18 15:29:50,504:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9040 (1.8876)
+2022-11-18 15:29:50,603:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8956 (1.8897)
+2022-11-18 15:29:50,701:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8068 (1.8743)
+2022-11-18 15:29:50,767:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_441.pth.tar
+2022-11-18 15:29:50,768:INFO: 
+===> EPOCH: 442 (P2)
+2022-11-18 15:29:50,768:INFO: - Computing loss (training)
+2022-11-18 15:29:51,227:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0279 (2.0279)
+2022-11-18 15:29:51,418:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7776 (1.9012)
+2022-11-18 15:29:51,579:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8607 (1.8878)
+2022-11-18 15:29:51,703:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9007 (1.8900)
+2022-11-18 15:29:52,138:INFO: Dataset: univ                Batch:  1/15	Loss 1.8536 (1.8536)
+2022-11-18 15:29:52,313:INFO: Dataset: univ                Batch:  2/15	Loss 1.8718 (1.8629)
+2022-11-18 15:29:52,486:INFO: Dataset: univ                Batch:  3/15	Loss 1.8967 (1.8749)
+2022-11-18 15:29:52,652:INFO: Dataset: univ                Batch:  4/15	Loss 1.8562 (1.8706)
+2022-11-18 15:29:52,817:INFO: Dataset: univ                Batch:  5/15	Loss 1.8644 (1.8695)
+2022-11-18 15:29:52,985:INFO: Dataset: univ                Batch:  6/15	Loss 1.8652 (1.8687)
+2022-11-18 15:29:53,150:INFO: Dataset: univ                Batch:  7/15	Loss 1.8385 (1.8646)
+2022-11-18 15:29:53,313:INFO: Dataset: univ                Batch:  8/15	Loss 1.9003 (1.8692)
+2022-11-18 15:29:53,479:INFO: Dataset: univ                Batch:  9/15	Loss 1.8877 (1.8712)
+2022-11-18 15:29:53,641:INFO: Dataset: univ                Batch: 10/15	Loss 1.8853 (1.8726)
+2022-11-18 15:29:53,803:INFO: Dataset: univ                Batch: 11/15	Loss 1.8918 (1.8743)
+2022-11-18 15:29:53,967:INFO: Dataset: univ                Batch: 12/15	Loss 1.8635 (1.8734)
+2022-11-18 15:29:54,129:INFO: Dataset: univ                Batch: 13/15	Loss 1.8379 (1.8707)
+2022-11-18 15:29:54,293:INFO: Dataset: univ                Batch: 14/15	Loss 1.9307 (1.8750)
+2022-11-18 15:29:54,383:INFO: Dataset: univ                Batch: 15/15	Loss 1.9056 (1.8754)
+2022-11-18 15:29:54,771:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9465 (1.9465)
+2022-11-18 15:29:54,930:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0167 (1.9816)
+2022-11-18 15:29:55,091:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8546 (1.9330)
+2022-11-18 15:29:55,254:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9159 (1.9289)
+2022-11-18 15:29:55,418:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9663 (1.9363)
+2022-11-18 15:29:55,579:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9557 (1.9392)
+2022-11-18 15:29:55,741:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8420 (1.9225)
+2022-11-18 15:29:55,887:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8747 (1.9173)
+2022-11-18 15:29:56,296:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8014 (1.8014)
+2022-11-18 15:29:56,459:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8681 (1.8349)
+2022-11-18 15:29:56,625:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8002 (1.8228)
+2022-11-18 15:29:56,783:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9355 (1.8501)
+2022-11-18 15:29:56,944:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8262 (1.8447)
+2022-11-18 15:29:57,106:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8210 (1.8405)
+2022-11-18 15:29:57,265:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8153 (1.8368)
+2022-11-18 15:29:57,426:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8902 (1.8427)
+2022-11-18 15:29:57,585:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8826 (1.8472)
+2022-11-18 15:29:57,742:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9273 (1.8548)
+2022-11-18 15:29:57,903:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8158 (1.8513)
+2022-11-18 15:29:58,075:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9011 (1.8556)
+2022-11-18 15:29:58,245:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8739 (1.8571)
+2022-11-18 15:29:58,435:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8989 (1.8599)
+2022-11-18 15:29:58,634:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7664 (1.8532)
+2022-11-18 15:29:58,822:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7184 (1.8446)
+2022-11-18 15:29:59,017:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8268 (1.8435)
+2022-11-18 15:29:59,203:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9304 (1.8475)
+2022-11-18 15:29:59,279:INFO: - Computing loss (validation)
+2022-11-18 15:29:59,597:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0041 (2.0041)
+2022-11-18 15:29:59,639:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9479 (2.0004)
+2022-11-18 15:30:00,001:INFO: Dataset: univ                Batch: 1/3	Loss 1.8703 (1.8703)
+2022-11-18 15:30:00,086:INFO: Dataset: univ                Batch: 2/3	Loss 1.8598 (1.8647)
+2022-11-18 15:30:00,168:INFO: Dataset: univ                Batch: 3/3	Loss 1.8527 (1.8604)
+2022-11-18 15:30:00,538:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8657 (1.8657)
+2022-11-18 15:30:00,590:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9775 (1.8948)
+2022-11-18 15:30:00,975:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8292 (1.8292)
+2022-11-18 15:30:01,080:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9304 (1.8778)
+2022-11-18 15:30:01,176:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8915 (1.8823)
+2022-11-18 15:30:01,264:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8654 (1.8778)
+2022-11-18 15:30:01,362:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8042 (1.8634)
+2022-11-18 15:30:01,537:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_442.pth.tar
+2022-11-18 15:30:01,538:INFO: 
+===> EPOCH: 443 (P2)
+2022-11-18 15:30:01,538:INFO: - Computing loss (training)
+2022-11-18 15:30:02,021:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9083 (1.9083)
+2022-11-18 15:30:02,232:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8640 (1.8854)
+2022-11-18 15:30:02,424:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0723 (1.9463)
+2022-11-18 15:30:02,587:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9816 (1.9520)
+2022-11-18 15:30:03,108:INFO: Dataset: univ                Batch:  1/15	Loss 1.8673 (1.8673)
+2022-11-18 15:30:03,302:INFO: Dataset: univ                Batch:  2/15	Loss 1.9123 (1.8894)
+2022-11-18 15:30:03,502:INFO: Dataset: univ                Batch:  3/15	Loss 1.8681 (1.8817)
+2022-11-18 15:30:03,693:INFO: Dataset: univ                Batch:  4/15	Loss 1.8728 (1.8794)
+2022-11-18 15:30:03,892:INFO: Dataset: univ                Batch:  5/15	Loss 1.8767 (1.8789)
+2022-11-18 15:30:04,085:INFO: Dataset: univ                Batch:  6/15	Loss 1.8723 (1.8777)
+2022-11-18 15:30:04,286:INFO: Dataset: univ                Batch:  7/15	Loss 1.8933 (1.8800)
+2022-11-18 15:30:04,503:INFO: Dataset: univ                Batch:  8/15	Loss 1.8705 (1.8789)
+2022-11-18 15:30:04,690:INFO: Dataset: univ                Batch:  9/15	Loss 1.8380 (1.8744)
+2022-11-18 15:30:04,917:INFO: Dataset: univ                Batch: 10/15	Loss 1.8423 (1.8716)
+2022-11-18 15:30:05,118:INFO: Dataset: univ                Batch: 11/15	Loss 1.8083 (1.8661)
+2022-11-18 15:30:05,323:INFO: Dataset: univ                Batch: 12/15	Loss 1.9047 (1.8689)
+2022-11-18 15:30:05,521:INFO: Dataset: univ                Batch: 13/15	Loss 1.8792 (1.8697)
+2022-11-18 15:30:05,717:INFO: Dataset: univ                Batch: 14/15	Loss 1.8657 (1.8694)
+2022-11-18 15:30:05,831:INFO: Dataset: univ                Batch: 15/15	Loss 1.7601 (1.8682)
+2022-11-18 15:30:06,364:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7518 (1.7518)
+2022-11-18 15:30:06,583:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8535 (1.8015)
+2022-11-18 15:30:06,795:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8987 (1.8375)
+2022-11-18 15:30:06,989:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8968 (1.8514)
+2022-11-18 15:30:07,189:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7603 (1.8328)
+2022-11-18 15:30:07,390:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8161 (1.8301)
+2022-11-18 15:30:07,581:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8225 (1.8290)
+2022-11-18 15:30:07,764:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8831 (1.8351)
+2022-11-18 15:30:08,238:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8467 (1.8467)
+2022-11-18 15:30:08,439:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7592 (1.8032)
+2022-11-18 15:30:08,635:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8310 (1.8125)
+2022-11-18 15:30:08,847:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8658 (1.8256)
+2022-11-18 15:30:09,036:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8357 (1.8275)
+2022-11-18 15:30:09,269:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8599 (1.8334)
+2022-11-18 15:30:09,504:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8822 (1.8406)
+2022-11-18 15:30:09,706:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9171 (1.8500)
+2022-11-18 15:30:09,901:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8519 (1.8502)
+2022-11-18 15:30:10,089:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8687 (1.8520)
+2022-11-18 15:30:10,277:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7896 (1.8460)
+2022-11-18 15:30:10,478:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8442 (1.8459)
+2022-11-18 15:30:10,687:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8674 (1.8475)
+2022-11-18 15:30:10,931:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8923 (1.8507)
+2022-11-18 15:30:11,193:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9100 (1.8546)
+2022-11-18 15:30:11,428:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8874 (1.8568)
+2022-11-18 15:30:11,667:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9081 (1.8598)
+2022-11-18 15:30:11,897:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8743 (1.8605)
+2022-11-18 15:30:11,962:INFO: - Computing loss (validation)
+2022-11-18 15:30:12,287:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9676 (1.9676)
+2022-11-18 15:30:12,328:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5845 (1.9350)
+2022-11-18 15:30:12,715:INFO: Dataset: univ                Batch: 1/3	Loss 1.8394 (1.8394)
+2022-11-18 15:30:12,808:INFO: Dataset: univ                Batch: 2/3	Loss 1.9000 (1.8744)
+2022-11-18 15:30:12,894:INFO: Dataset: univ                Batch: 3/3	Loss 1.8402 (1.8635)
+2022-11-18 15:30:13,252:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8632 (1.8632)
+2022-11-18 15:30:13,304:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9775 (1.8937)
+2022-11-18 15:30:13,703:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8256 (1.8256)
+2022-11-18 15:30:13,800:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9245 (1.8732)
+2022-11-18 15:30:13,896:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8609 (1.8692)
+2022-11-18 15:30:13,995:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8868 (1.8733)
+2022-11-18 15:30:14,092:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8777 (1.8742)
+2022-11-18 15:30:14,155:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_443.pth.tar
+2022-11-18 15:30:14,156:INFO: 
+===> EPOCH: 444 (P2)
+2022-11-18 15:30:14,156:INFO: - Computing loss (training)
+2022-11-18 15:30:14,597:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9525 (1.9525)
+2022-11-18 15:30:14,779:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9249 (1.9392)
+2022-11-18 15:30:14,965:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8612 (1.9133)
+2022-11-18 15:30:15,113:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9380 (1.9173)
+2022-11-18 15:30:15,613:INFO: Dataset: univ                Batch:  1/15	Loss 1.8770 (1.8770)
+2022-11-18 15:30:15,816:INFO: Dataset: univ                Batch:  2/15	Loss 1.7908 (1.8371)
+2022-11-18 15:30:16,019:INFO: Dataset: univ                Batch:  3/15	Loss 1.8791 (1.8502)
+2022-11-18 15:30:16,227:INFO: Dataset: univ                Batch:  4/15	Loss 1.8577 (1.8521)
+2022-11-18 15:30:16,461:INFO: Dataset: univ                Batch:  5/15	Loss 1.8362 (1.8485)
+2022-11-18 15:30:16,659:INFO: Dataset: univ                Batch:  6/15	Loss 1.8277 (1.8455)
+2022-11-18 15:30:16,864:INFO: Dataset: univ                Batch:  7/15	Loss 1.8342 (1.8439)
+2022-11-18 15:30:17,071:INFO: Dataset: univ                Batch:  8/15	Loss 1.8822 (1.8483)
+2022-11-18 15:30:17,267:INFO: Dataset: univ                Batch:  9/15	Loss 1.8512 (1.8486)
+2022-11-18 15:30:17,473:INFO: Dataset: univ                Batch: 10/15	Loss 1.8608 (1.8500)
+2022-11-18 15:30:17,665:INFO: Dataset: univ                Batch: 11/15	Loss 1.8897 (1.8532)
+2022-11-18 15:30:17,874:INFO: Dataset: univ                Batch: 12/15	Loss 1.8358 (1.8517)
+2022-11-18 15:30:18,062:INFO: Dataset: univ                Batch: 13/15	Loss 1.8650 (1.8527)
+2022-11-18 15:30:18,261:INFO: Dataset: univ                Batch: 14/15	Loss 1.8517 (1.8526)
+2022-11-18 15:30:18,370:INFO: Dataset: univ                Batch: 15/15	Loss 1.8398 (1.8524)
+2022-11-18 15:30:18,857:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8520 (1.8520)
+2022-11-18 15:30:19,041:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8928 (1.8717)
+2022-11-18 15:30:19,225:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9442 (1.8966)
+2022-11-18 15:30:19,409:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9700 (1.9151)
+2022-11-18 15:30:19,602:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9089 (1.9138)
+2022-11-18 15:30:19,793:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8347 (1.8990)
+2022-11-18 15:30:19,980:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8461 (1.8917)
+2022-11-18 15:30:20,146:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7851 (1.8794)
+2022-11-18 15:30:20,641:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8837 (1.8837)
+2022-11-18 15:30:20,835:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7911 (1.8356)
+2022-11-18 15:30:21,033:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9330 (1.8659)
+2022-11-18 15:30:21,245:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7938 (1.8493)
+2022-11-18 15:30:21,441:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8787 (1.8551)
+2022-11-18 15:30:21,659:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8936 (1.8618)
+2022-11-18 15:30:21,877:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9884 (1.8807)
+2022-11-18 15:30:22,089:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7558 (1.8654)
+2022-11-18 15:30:22,304:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9446 (1.8740)
+2022-11-18 15:30:22,569:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7665 (1.8638)
+2022-11-18 15:30:22,797:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9521 (1.8717)
+2022-11-18 15:30:23,029:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7747 (1.8634)
+2022-11-18 15:30:23,219:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7997 (1.8590)
+2022-11-18 15:30:23,467:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9861 (1.8688)
+2022-11-18 15:30:23,685:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8838 (1.8699)
+2022-11-18 15:30:23,872:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8658 (1.8697)
+2022-11-18 15:30:24,055:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8458 (1.8683)
+2022-11-18 15:30:24,227:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8548 (1.8676)
+2022-11-18 15:30:24,285:INFO: - Computing loss (validation)
+2022-11-18 15:30:24,591:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9042 (1.9042)
+2022-11-18 15:30:24,626:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7817 (1.8954)
+2022-11-18 15:30:24,954:INFO: Dataset: univ                Batch: 1/3	Loss 1.8474 (1.8474)
+2022-11-18 15:30:25,044:INFO: Dataset: univ                Batch: 2/3	Loss 1.8455 (1.8464)
+2022-11-18 15:30:25,129:INFO: Dataset: univ                Batch: 3/3	Loss 1.8047 (1.8331)
+2022-11-18 15:30:25,457:INFO: Dataset: zara1               Batch: 1/2	Loss 1.6951 (1.6951)
+2022-11-18 15:30:25,510:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9609 (1.7575)
+2022-11-18 15:30:25,842:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8838 (1.8838)
+2022-11-18 15:30:25,934:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9088 (1.8961)
+2022-11-18 15:30:26,025:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9055 (1.8995)
+2022-11-18 15:30:26,120:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8512 (1.8875)
+2022-11-18 15:30:26,213:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9674 (1.9032)
+2022-11-18 15:30:26,286:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_444.pth.tar
+2022-11-18 15:30:26,287:INFO: 
+===> EPOCH: 445 (P2)
+2022-11-18 15:30:26,288:INFO: - Computing loss (training)
+2022-11-18 15:30:26,686:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8886 (1.8886)
+2022-11-18 15:30:26,867:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9048 (1.8969)
+2022-11-18 15:30:27,066:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9277 (1.9070)
+2022-11-18 15:30:27,230:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9149 (1.9084)
+2022-11-18 15:30:27,731:INFO: Dataset: univ                Batch:  1/15	Loss 1.9129 (1.9129)
+2022-11-18 15:30:27,894:INFO: Dataset: univ                Batch:  2/15	Loss 1.8483 (1.8799)
+2022-11-18 15:30:28,059:INFO: Dataset: univ                Batch:  3/15	Loss 1.8464 (1.8689)
+2022-11-18 15:30:28,231:INFO: Dataset: univ                Batch:  4/15	Loss 1.8598 (1.8666)
+2022-11-18 15:30:28,434:INFO: Dataset: univ                Batch:  5/15	Loss 1.8901 (1.8713)
+2022-11-18 15:30:28,647:INFO: Dataset: univ                Batch:  6/15	Loss 1.8985 (1.8759)
+2022-11-18 15:30:28,868:INFO: Dataset: univ                Batch:  7/15	Loss 1.8411 (1.8708)
+2022-11-18 15:30:29,073:INFO: Dataset: univ                Batch:  8/15	Loss 1.8717 (1.8709)
+2022-11-18 15:30:29,257:INFO: Dataset: univ                Batch:  9/15	Loss 1.8766 (1.8716)
+2022-11-18 15:30:29,436:INFO: Dataset: univ                Batch: 10/15	Loss 1.8357 (1.8680)
+2022-11-18 15:30:29,609:INFO: Dataset: univ                Batch: 11/15	Loss 1.8851 (1.8697)
+2022-11-18 15:30:29,783:INFO: Dataset: univ                Batch: 12/15	Loss 1.8689 (1.8696)
+2022-11-18 15:30:29,945:INFO: Dataset: univ                Batch: 13/15	Loss 1.8417 (1.8676)
+2022-11-18 15:30:30,120:INFO: Dataset: univ                Batch: 14/15	Loss 1.8781 (1.8683)
+2022-11-18 15:30:30,212:INFO: Dataset: univ                Batch: 15/15	Loss 1.7520 (1.8665)
+2022-11-18 15:30:30,657:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9858 (1.9858)
+2022-11-18 15:30:30,826:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8551 (1.9152)
+2022-11-18 15:30:30,988:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9108 (1.9139)
+2022-11-18 15:30:31,155:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8446 (1.8950)
+2022-11-18 15:30:31,311:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9184 (1.8999)
+2022-11-18 15:30:31,486:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7426 (1.8730)
+2022-11-18 15:30:31,638:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9336 (1.8818)
+2022-11-18 15:30:31,777:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8943 (1.8831)
+2022-11-18 15:30:32,192:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8146 (1.8146)
+2022-11-18 15:30:32,356:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7555 (1.7848)
+2022-11-18 15:30:32,522:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8089 (1.7934)
+2022-11-18 15:30:32,708:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8900 (1.8148)
+2022-11-18 15:30:32,898:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8200 (1.8157)
+2022-11-18 15:30:33,087:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9261 (1.8363)
+2022-11-18 15:30:33,275:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7003 (1.8171)
+2022-11-18 15:30:33,449:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8427 (1.8205)
+2022-11-18 15:30:33,623:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8722 (1.8266)
+2022-11-18 15:30:33,805:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8250 (1.8264)
+2022-11-18 15:30:33,976:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8534 (1.8290)
+2022-11-18 15:30:34,133:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8532 (1.8309)
+2022-11-18 15:30:34,296:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8028 (1.8289)
+2022-11-18 15:30:34,465:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8270 (1.8288)
+2022-11-18 15:30:34,633:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9151 (1.8341)
+2022-11-18 15:30:34,794:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8073 (1.8324)
+2022-11-18 15:30:34,965:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9042 (1.8367)
+2022-11-18 15:30:35,123:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8551 (1.8377)
+2022-11-18 15:30:35,173:INFO: - Computing loss (validation)
+2022-11-18 15:30:35,454:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9599 (1.9599)
+2022-11-18 15:30:35,494:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8192 (1.9512)
+2022-11-18 15:30:35,818:INFO: Dataset: univ                Batch: 1/3	Loss 1.9089 (1.9089)
+2022-11-18 15:30:35,903:INFO: Dataset: univ                Batch: 2/3	Loss 1.9045 (1.9067)
+2022-11-18 15:30:35,980:INFO: Dataset: univ                Batch: 3/3	Loss 1.8827 (1.8997)
+2022-11-18 15:30:36,319:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8734 (1.8734)
+2022-11-18 15:30:36,370:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9465 (1.8908)
+2022-11-18 15:30:36,711:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9309 (1.9309)
+2022-11-18 15:30:36,797:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8737 (1.9021)
+2022-11-18 15:30:36,882:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8755 (1.8936)
+2022-11-18 15:30:36,966:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8882 (1.8922)
+2022-11-18 15:30:37,052:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8339 (1.8805)
+2022-11-18 15:30:37,111:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_445.pth.tar
+2022-11-18 15:30:37,111:INFO: 
+===> EPOCH: 446 (P2)
+2022-11-18 15:30:37,111:INFO: - Computing loss (training)
+2022-11-18 15:30:37,459:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8338 (1.8338)
+2022-11-18 15:30:37,622:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8931 (1.8637)
+2022-11-18 15:30:37,784:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9852 (1.9051)
+2022-11-18 15:30:37,908:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8723 (1.8997)
+2022-11-18 15:30:38,317:INFO: Dataset: univ                Batch:  1/15	Loss 1.8551 (1.8551)
+2022-11-18 15:30:38,491:INFO: Dataset: univ                Batch:  2/15	Loss 1.8457 (1.8502)
+2022-11-18 15:30:38,657:INFO: Dataset: univ                Batch:  3/15	Loss 1.8754 (1.8589)
+2022-11-18 15:30:38,822:INFO: Dataset: univ                Batch:  4/15	Loss 1.8430 (1.8549)
+2022-11-18 15:30:38,993:INFO: Dataset: univ                Batch:  5/15	Loss 1.8917 (1.8624)
+2022-11-18 15:30:39,170:INFO: Dataset: univ                Batch:  6/15	Loss 1.8518 (1.8606)
+2022-11-18 15:30:39,345:INFO: Dataset: univ                Batch:  7/15	Loss 1.8713 (1.8620)
+2022-11-18 15:30:39,519:INFO: Dataset: univ                Batch:  8/15	Loss 1.8469 (1.8603)
+2022-11-18 15:30:39,693:INFO: Dataset: univ                Batch:  9/15	Loss 1.8280 (1.8567)
+2022-11-18 15:30:39,857:INFO: Dataset: univ                Batch: 10/15	Loss 1.8738 (1.8585)
+2022-11-18 15:30:40,036:INFO: Dataset: univ                Batch: 11/15	Loss 1.8390 (1.8569)
+2022-11-18 15:30:40,208:INFO: Dataset: univ                Batch: 12/15	Loss 1.8599 (1.8571)
+2022-11-18 15:30:40,384:INFO: Dataset: univ                Batch: 13/15	Loss 1.8937 (1.8600)
+2022-11-18 15:30:40,560:INFO: Dataset: univ                Batch: 14/15	Loss 1.8966 (1.8626)
+2022-11-18 15:30:40,646:INFO: Dataset: univ                Batch: 15/15	Loss 1.8194 (1.8620)
+2022-11-18 15:30:41,063:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8294 (1.8294)
+2022-11-18 15:30:41,231:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8947 (1.8617)
+2022-11-18 15:30:41,403:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8302 (1.8511)
+2022-11-18 15:30:41,568:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7760 (1.8328)
+2022-11-18 15:30:41,733:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8768 (1.8414)
+2022-11-18 15:30:41,901:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8492 (1.8425)
+2022-11-18 15:30:42,073:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8429 (1.8425)
+2022-11-18 15:30:42,224:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8229 (1.8404)
+2022-11-18 15:30:42,695:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9059 (1.9059)
+2022-11-18 15:30:42,863:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8582 (1.8819)
+2022-11-18 15:30:43,039:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8366 (1.8669)
+2022-11-18 15:30:43,220:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8313 (1.8582)
+2022-11-18 15:30:43,401:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7886 (1.8454)
+2022-11-18 15:30:43,580:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9537 (1.8648)
+2022-11-18 15:30:43,752:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8960 (1.8690)
+2022-11-18 15:30:43,925:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8546 (1.8673)
+2022-11-18 15:30:44,088:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9412 (1.8760)
+2022-11-18 15:30:44,243:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7879 (1.8675)
+2022-11-18 15:30:44,411:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8048 (1.8624)
+2022-11-18 15:30:44,579:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8976 (1.8656)
+2022-11-18 15:30:44,740:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9718 (1.8736)
+2022-11-18 15:30:44,909:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8536 (1.8721)
+2022-11-18 15:30:45,065:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8062 (1.8684)
+2022-11-18 15:30:45,219:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9700 (1.8748)
+2022-11-18 15:30:45,375:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9687 (1.8800)
+2022-11-18 15:30:45,521:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8070 (1.8768)
+2022-11-18 15:30:45,567:INFO: - Computing loss (validation)
+2022-11-18 15:30:45,830:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0047 (2.0047)
+2022-11-18 15:30:45,866:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8378 (1.9933)
+2022-11-18 15:30:46,177:INFO: Dataset: univ                Batch: 1/3	Loss 1.8729 (1.8729)
+2022-11-18 15:30:46,255:INFO: Dataset: univ                Batch: 2/3	Loss 1.9347 (1.9038)
+2022-11-18 15:30:46,330:INFO: Dataset: univ                Batch: 3/3	Loss 1.8728 (1.8931)
+2022-11-18 15:30:46,644:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8972 (1.8972)
+2022-11-18 15:30:46,693:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9760 (1.9165)
+2022-11-18 15:30:47,017:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7989 (1.7989)
+2022-11-18 15:30:47,099:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8932 (1.8461)
+2022-11-18 15:30:47,180:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9247 (1.8740)
+2022-11-18 15:30:47,264:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8583 (1.8701)
+2022-11-18 15:30:47,341:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8250 (1.8609)
+2022-11-18 15:30:47,400:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_446.pth.tar
+2022-11-18 15:30:47,400:INFO: 
+===> EPOCH: 447 (P2)
+2022-11-18 15:30:47,401:INFO: - Computing loss (training)
+2022-11-18 15:30:47,757:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8871 (1.8871)
+2022-11-18 15:30:47,916:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9237 (1.9056)
+2022-11-18 15:30:48,075:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8808 (1.8976)
+2022-11-18 15:30:48,196:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8179 (1.8847)
+2022-11-18 15:30:48,620:INFO: Dataset: univ                Batch:  1/15	Loss 1.8491 (1.8491)
+2022-11-18 15:30:48,795:INFO: Dataset: univ                Batch:  2/15	Loss 1.8949 (1.8722)
+2022-11-18 15:30:48,970:INFO: Dataset: univ                Batch:  3/15	Loss 1.8717 (1.8721)
+2022-11-18 15:30:49,143:INFO: Dataset: univ                Batch:  4/15	Loss 1.8167 (1.8575)
+2022-11-18 15:30:49,314:INFO: Dataset: univ                Batch:  5/15	Loss 1.8741 (1.8605)
+2022-11-18 15:30:49,479:INFO: Dataset: univ                Batch:  6/15	Loss 1.8977 (1.8664)
+2022-11-18 15:30:49,680:INFO: Dataset: univ                Batch:  7/15	Loss 1.8381 (1.8621)
+2022-11-18 15:30:49,875:INFO: Dataset: univ                Batch:  8/15	Loss 1.9198 (1.8683)
+2022-11-18 15:30:50,076:INFO: Dataset: univ                Batch:  9/15	Loss 1.8882 (1.8705)
+2022-11-18 15:30:50,239:INFO: Dataset: univ                Batch: 10/15	Loss 1.8485 (1.8684)
+2022-11-18 15:30:50,408:INFO: Dataset: univ                Batch: 11/15	Loss 1.9085 (1.8721)
+2022-11-18 15:30:50,577:INFO: Dataset: univ                Batch: 12/15	Loss 1.8795 (1.8726)
+2022-11-18 15:30:50,741:INFO: Dataset: univ                Batch: 13/15	Loss 1.8878 (1.8737)
+2022-11-18 15:30:50,915:INFO: Dataset: univ                Batch: 14/15	Loss 1.8867 (1.8747)
+2022-11-18 15:30:51,005:INFO: Dataset: univ                Batch: 15/15	Loss 1.8465 (1.8743)
+2022-11-18 15:30:51,406:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8522 (1.8522)
+2022-11-18 15:30:51,556:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8009 (1.8238)
+2022-11-18 15:30:51,707:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8845 (1.8456)
+2022-11-18 15:30:51,859:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0684 (1.8940)
+2022-11-18 15:30:52,014:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7520 (1.8655)
+2022-11-18 15:30:52,170:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8519 (1.8631)
+2022-11-18 15:30:52,332:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8422 (1.8601)
+2022-11-18 15:30:52,479:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0792 (1.8822)
+2022-11-18 15:30:52,882:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9506 (1.9506)
+2022-11-18 15:30:53,042:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8559 (1.9050)
+2022-11-18 15:30:53,199:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8469 (1.8875)
+2022-11-18 15:30:53,351:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9265 (1.8976)
+2022-11-18 15:30:53,512:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8238 (1.8831)
+2022-11-18 15:30:53,674:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9742 (1.9002)
+2022-11-18 15:30:53,832:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8552 (1.8938)
+2022-11-18 15:30:53,995:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8556 (1.8892)
+2022-11-18 15:30:54,169:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9575 (1.8971)
+2022-11-18 15:30:54,329:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8807 (1.8953)
+2022-11-18 15:30:54,489:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8481 (1.8911)
+2022-11-18 15:30:54,640:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7861 (1.8822)
+2022-11-18 15:30:54,806:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8526 (1.8798)
+2022-11-18 15:30:54,973:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9255 (1.8832)
+2022-11-18 15:30:55,132:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8897 (1.8837)
+2022-11-18 15:30:55,288:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9726 (1.8885)
+2022-11-18 15:30:55,459:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8012 (1.8832)
+2022-11-18 15:30:55,622:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9374 (1.8857)
+2022-11-18 15:30:55,677:INFO: - Computing loss (validation)
+2022-11-18 15:30:56,007:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8545 (1.8545)
+2022-11-18 15:30:56,048:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7527 (1.8468)
+2022-11-18 15:30:56,406:INFO: Dataset: univ                Batch: 1/3	Loss 1.8920 (1.8920)
+2022-11-18 15:30:56,522:INFO: Dataset: univ                Batch: 2/3	Loss 1.9124 (1.9030)
+2022-11-18 15:30:56,609:INFO: Dataset: univ                Batch: 3/3	Loss 1.8939 (1.9005)
+2022-11-18 15:30:56,946:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7791 (1.7791)
+2022-11-18 15:30:56,998:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8465 (1.7954)
+2022-11-18 15:30:57,317:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8297 (1.8297)
+2022-11-18 15:30:57,395:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8313 (1.8305)
+2022-11-18 15:30:57,469:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8741 (1.8445)
+2022-11-18 15:30:57,546:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9285 (1.8652)
+2022-11-18 15:30:57,621:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9246 (1.8775)
+2022-11-18 15:30:57,676:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_447.pth.tar
+2022-11-18 15:30:57,676:INFO: 
+===> EPOCH: 448 (P2)
+2022-11-18 15:30:57,677:INFO: - Computing loss (training)
+2022-11-18 15:30:58,028:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8390 (1.8390)
+2022-11-18 15:30:58,184:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9360 (1.8889)
+2022-11-18 15:30:58,340:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8512 (1.8765)
+2022-11-18 15:30:58,468:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8668 (1.8748)
+2022-11-18 15:30:58,897:INFO: Dataset: univ                Batch:  1/15	Loss 1.8335 (1.8335)
+2022-11-18 15:30:59,062:INFO: Dataset: univ                Batch:  2/15	Loss 1.8549 (1.8449)
+2022-11-18 15:30:59,237:INFO: Dataset: univ                Batch:  3/15	Loss 1.9322 (1.8732)
+2022-11-18 15:30:59,418:INFO: Dataset: univ                Batch:  4/15	Loss 1.8795 (1.8749)
+2022-11-18 15:30:59,588:INFO: Dataset: univ                Batch:  5/15	Loss 1.8341 (1.8668)
+2022-11-18 15:30:59,758:INFO: Dataset: univ                Batch:  6/15	Loss 1.8906 (1.8704)
+2022-11-18 15:30:59,944:INFO: Dataset: univ                Batch:  7/15	Loss 1.8798 (1.8718)
+2022-11-18 15:31:00,120:INFO: Dataset: univ                Batch:  8/15	Loss 1.8676 (1.8713)
+2022-11-18 15:31:00,297:INFO: Dataset: univ                Batch:  9/15	Loss 1.8859 (1.8729)
+2022-11-18 15:31:00,483:INFO: Dataset: univ                Batch: 10/15	Loss 1.8516 (1.8710)
+2022-11-18 15:31:00,670:INFO: Dataset: univ                Batch: 11/15	Loss 1.9287 (1.8769)
+2022-11-18 15:31:00,842:INFO: Dataset: univ                Batch: 12/15	Loss 1.8909 (1.8782)
+2022-11-18 15:31:01,024:INFO: Dataset: univ                Batch: 13/15	Loss 1.9153 (1.8807)
+2022-11-18 15:31:01,201:INFO: Dataset: univ                Batch: 14/15	Loss 1.8972 (1.8820)
+2022-11-18 15:31:01,295:INFO: Dataset: univ                Batch: 15/15	Loss 1.8429 (1.8814)
+2022-11-18 15:31:01,733:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8693 (1.8693)
+2022-11-18 15:31:01,913:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8947 (1.8824)
+2022-11-18 15:31:02,105:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9666 (1.9104)
+2022-11-18 15:31:02,292:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9259 (1.9147)
+2022-11-18 15:31:02,476:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8327 (1.8997)
+2022-11-18 15:31:02,663:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8430 (1.8915)
+2022-11-18 15:31:02,849:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9267 (1.8965)
+2022-11-18 15:31:03,036:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8217 (1.8892)
+2022-11-18 15:31:03,508:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8789 (1.8789)
+2022-11-18 15:31:03,720:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8615 (1.8698)
+2022-11-18 15:31:03,941:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9377 (1.8908)
+2022-11-18 15:31:04,184:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8951 (1.8919)
+2022-11-18 15:31:04,385:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8197 (1.8775)
+2022-11-18 15:31:04,575:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9349 (1.8866)
+2022-11-18 15:31:04,773:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8806 (1.8858)
+2022-11-18 15:31:04,967:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9537 (1.8941)
+2022-11-18 15:31:05,148:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8407 (1.8883)
+2022-11-18 15:31:05,331:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8761 (1.8870)
+2022-11-18 15:31:05,496:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8529 (1.8840)
+2022-11-18 15:31:05,660:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9499 (1.8901)
+2022-11-18 15:31:05,820:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8214 (1.8840)
+2022-11-18 15:31:05,986:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8858 (1.8842)
+2022-11-18 15:31:06,178:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9202 (1.8866)
+2022-11-18 15:31:06,380:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9198 (1.8885)
+2022-11-18 15:31:06,554:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8140 (1.8847)
+2022-11-18 15:31:06,718:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9353 (1.8874)
+2022-11-18 15:31:06,773:INFO: - Computing loss (validation)
+2022-11-18 15:31:07,076:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0015 (2.0015)
+2022-11-18 15:31:07,113:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0841 (2.0085)
+2022-11-18 15:31:07,460:INFO: Dataset: univ                Batch: 1/3	Loss 1.9148 (1.9148)
+2022-11-18 15:31:07,550:INFO: Dataset: univ                Batch: 2/3	Loss 1.8589 (1.8898)
+2022-11-18 15:31:07,635:INFO: Dataset: univ                Batch: 3/3	Loss 1.8543 (1.8792)
+2022-11-18 15:31:07,957:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9799 (1.9799)
+2022-11-18 15:31:08,009:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8900 (1.9603)
+2022-11-18 15:31:08,378:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8828 (1.8828)
+2022-11-18 15:31:08,463:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8462 (1.8640)
+2022-11-18 15:31:08,546:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8202 (1.8496)
+2022-11-18 15:31:08,630:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7650 (1.8286)
+2022-11-18 15:31:08,714:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8982 (1.8412)
+2022-11-18 15:31:08,773:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_448.pth.tar
+2022-11-18 15:31:08,773:INFO: 
+===> EPOCH: 449 (P2)
+2022-11-18 15:31:08,774:INFO: - Computing loss (training)
+2022-11-18 15:31:09,116:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8661 (1.8661)
+2022-11-18 15:31:09,281:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8944 (1.8807)
+2022-11-18 15:31:09,444:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9251 (1.8955)
+2022-11-18 15:31:09,570:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9183 (1.8996)
+2022-11-18 15:31:09,992:INFO: Dataset: univ                Batch:  1/15	Loss 1.9178 (1.9178)
+2022-11-18 15:31:10,153:INFO: Dataset: univ                Batch:  2/15	Loss 1.8075 (1.8582)
+2022-11-18 15:31:10,318:INFO: Dataset: univ                Batch:  3/15	Loss 1.8231 (1.8471)
+2022-11-18 15:31:10,498:INFO: Dataset: univ                Batch:  4/15	Loss 1.8399 (1.8453)
+2022-11-18 15:31:10,667:INFO: Dataset: univ                Batch:  5/15	Loss 1.8677 (1.8503)
+2022-11-18 15:31:10,828:INFO: Dataset: univ                Batch:  6/15	Loss 1.8733 (1.8539)
+2022-11-18 15:31:10,998:INFO: Dataset: univ                Batch:  7/15	Loss 1.8691 (1.8560)
+2022-11-18 15:31:11,167:INFO: Dataset: univ                Batch:  8/15	Loss 1.8550 (1.8559)
+2022-11-18 15:31:11,340:INFO: Dataset: univ                Batch:  9/15	Loss 1.8797 (1.8586)
+2022-11-18 15:31:11,509:INFO: Dataset: univ                Batch: 10/15	Loss 1.8433 (1.8571)
+2022-11-18 15:31:11,669:INFO: Dataset: univ                Batch: 11/15	Loss 1.8635 (1.8577)
+2022-11-18 15:31:11,844:INFO: Dataset: univ                Batch: 12/15	Loss 1.8784 (1.8592)
+2022-11-18 15:31:12,029:INFO: Dataset: univ                Batch: 13/15	Loss 1.8392 (1.8578)
+2022-11-18 15:31:12,215:INFO: Dataset: univ                Batch: 14/15	Loss 1.8897 (1.8600)
+2022-11-18 15:31:12,319:INFO: Dataset: univ                Batch: 15/15	Loss 1.9548 (1.8611)
+2022-11-18 15:31:12,750:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8378 (1.8378)
+2022-11-18 15:31:12,916:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9342 (1.8842)
+2022-11-18 15:31:13,094:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7894 (1.8502)
+2022-11-18 15:31:13,267:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8864 (1.8588)
+2022-11-18 15:31:13,433:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8780 (1.8624)
+2022-11-18 15:31:13,593:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8855 (1.8663)
+2022-11-18 15:31:13,751:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8737 (1.8674)
+2022-11-18 15:31:13,894:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8877 (1.8697)
+2022-11-18 15:31:14,295:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7995 (1.7995)
+2022-11-18 15:31:14,456:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8808 (1.8413)
+2022-11-18 15:31:14,630:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9048 (1.8622)
+2022-11-18 15:31:14,795:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7867 (1.8444)
+2022-11-18 15:31:14,962:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8191 (1.8390)
+2022-11-18 15:31:15,128:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7489 (1.8246)
+2022-11-18 15:31:15,285:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8654 (1.8303)
+2022-11-18 15:31:15,437:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8700 (1.8352)
+2022-11-18 15:31:15,590:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8006 (1.8317)
+2022-11-18 15:31:15,746:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8983 (1.8385)
+2022-11-18 15:31:15,911:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8319 (1.8379)
+2022-11-18 15:31:16,075:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8899 (1.8422)
+2022-11-18 15:31:16,239:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9339 (1.8489)
+2022-11-18 15:31:16,405:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9274 (1.8541)
+2022-11-18 15:31:16,570:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8088 (1.8510)
+2022-11-18 15:31:16,734:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8880 (1.8531)
+2022-11-18 15:31:16,898:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9172 (1.8571)
+2022-11-18 15:31:17,047:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9105 (1.8599)
+2022-11-18 15:31:17,095:INFO: - Computing loss (validation)
+2022-11-18 15:31:17,358:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9438 (1.9438)
+2022-11-18 15:31:17,395:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7247 (1.9266)
+2022-11-18 15:31:17,697:INFO: Dataset: univ                Batch: 1/3	Loss 1.8409 (1.8409)
+2022-11-18 15:31:17,779:INFO: Dataset: univ                Batch: 2/3	Loss 1.8207 (1.8298)
+2022-11-18 15:31:17,857:INFO: Dataset: univ                Batch: 3/3	Loss 1.8694 (1.8437)
+2022-11-18 15:31:18,173:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8344 (1.8344)
+2022-11-18 15:31:18,224:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7887 (1.8229)
+2022-11-18 15:31:18,536:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8648 (1.8648)
+2022-11-18 15:31:18,612:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9524 (1.9063)
+2022-11-18 15:31:18,689:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8733 (1.8953)
+2022-11-18 15:31:18,781:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8393 (1.8818)
+2022-11-18 15:31:18,863:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8633 (1.8779)
+2022-11-18 15:31:18,922:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_449.pth.tar
+2022-11-18 15:31:18,922:INFO: 
+===> EPOCH: 450 (P2)
+2022-11-18 15:31:18,923:INFO: - Computing loss (training)
+2022-11-18 15:31:19,291:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8804 (1.8804)
+2022-11-18 15:31:19,478:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8867 (1.8833)
+2022-11-18 15:31:19,666:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9458 (1.9046)
+2022-11-18 15:31:19,809:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7776 (1.8827)
+2022-11-18 15:31:20,235:INFO: Dataset: univ                Batch:  1/15	Loss 1.8910 (1.8910)
+2022-11-18 15:31:20,438:INFO: Dataset: univ                Batch:  2/15	Loss 1.9027 (1.8966)
+2022-11-18 15:31:20,633:INFO: Dataset: univ                Batch:  3/15	Loss 1.8445 (1.8806)
+2022-11-18 15:31:20,858:INFO: Dataset: univ                Batch:  4/15	Loss 1.8688 (1.8776)
+2022-11-18 15:31:21,064:INFO: Dataset: univ                Batch:  5/15	Loss 1.8963 (1.8813)
+2022-11-18 15:31:21,262:INFO: Dataset: univ                Batch:  6/15	Loss 1.9157 (1.8869)
+2022-11-18 15:31:21,444:INFO: Dataset: univ                Batch:  7/15	Loss 1.9074 (1.8899)
+2022-11-18 15:31:21,637:INFO: Dataset: univ                Batch:  8/15	Loss 1.8112 (1.8804)
+2022-11-18 15:31:21,803:INFO: Dataset: univ                Batch:  9/15	Loss 1.8849 (1.8809)
+2022-11-18 15:31:21,989:INFO: Dataset: univ                Batch: 10/15	Loss 1.8620 (1.8790)
+2022-11-18 15:31:22,169:INFO: Dataset: univ                Batch: 11/15	Loss 1.9273 (1.8833)
+2022-11-18 15:31:22,344:INFO: Dataset: univ                Batch: 12/15	Loss 1.9054 (1.8852)
+2022-11-18 15:31:22,505:INFO: Dataset: univ                Batch: 13/15	Loss 1.8690 (1.8839)
+2022-11-18 15:31:22,674:INFO: Dataset: univ                Batch: 14/15	Loss 1.8791 (1.8836)
+2022-11-18 15:31:22,770:INFO: Dataset: univ                Batch: 15/15	Loss 1.7782 (1.8826)
+2022-11-18 15:31:23,202:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8454 (1.8454)
+2022-11-18 15:31:23,367:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8970 (1.8708)
+2022-11-18 15:31:23,526:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9212 (1.8880)
+2022-11-18 15:31:23,690:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7765 (1.8610)
+2022-11-18 15:31:23,855:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8135 (1.8514)
+2022-11-18 15:31:24,010:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7447 (1.8340)
+2022-11-18 15:31:24,163:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8638 (1.8382)
+2022-11-18 15:31:24,311:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7839 (1.8317)
+2022-11-18 15:31:24,725:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9458 (1.9458)
+2022-11-18 15:31:24,910:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9101 (1.9286)
+2022-11-18 15:31:25,079:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8090 (1.8871)
+2022-11-18 15:31:25,242:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9418 (1.9008)
+2022-11-18 15:31:25,414:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8624 (1.8931)
+2022-11-18 15:31:25,592:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9002 (1.8942)
+2022-11-18 15:31:25,786:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9076 (1.8961)
+2022-11-18 15:31:25,962:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8803 (1.8942)
+2022-11-18 15:31:26,130:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8526 (1.8894)
+2022-11-18 15:31:26,283:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9115 (1.8918)
+2022-11-18 15:31:26,438:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8852 (1.8911)
+2022-11-18 15:31:26,589:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9284 (1.8939)
+2022-11-18 15:31:26,745:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8433 (1.8898)
+2022-11-18 15:31:26,903:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8660 (1.8881)
+2022-11-18 15:31:27,062:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8889 (1.8882)
+2022-11-18 15:31:27,221:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8401 (1.8852)
+2022-11-18 15:31:27,383:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8445 (1.8828)
+2022-11-18 15:31:27,530:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8625 (1.8817)
+2022-11-18 15:31:27,580:INFO: - Computing loss (validation)
+2022-11-18 15:31:27,853:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9562 (1.9562)
+2022-11-18 15:31:27,887:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4941 (1.9294)
+2022-11-18 15:31:28,196:INFO: Dataset: univ                Batch: 1/3	Loss 1.8618 (1.8618)
+2022-11-18 15:31:28,274:INFO: Dataset: univ                Batch: 2/3	Loss 1.8484 (1.8553)
+2022-11-18 15:31:28,352:INFO: Dataset: univ                Batch: 3/3	Loss 1.9125 (1.8761)
+2022-11-18 15:31:28,661:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8621 (1.8621)
+2022-11-18 15:31:28,709:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0336 (1.9046)
+2022-11-18 15:31:29,026:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8811 (1.8811)
+2022-11-18 15:31:29,107:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8899 (1.8856)
+2022-11-18 15:31:29,189:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8973 (1.8892)
+2022-11-18 15:31:29,272:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8319 (1.8755)
+2022-11-18 15:31:29,354:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8497 (1.8702)
+2022-11-18 15:31:29,411:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_450.pth.tar
+2022-11-18 15:31:29,411:INFO: 
+===> EPOCH: 451 (P2)
+2022-11-18 15:31:29,412:INFO: - Computing loss (training)
+2022-11-18 15:31:29,760:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8602 (1.8602)
+2022-11-18 15:31:29,926:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8623 (1.8613)
+2022-11-18 15:31:30,199:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9688 (1.8968)
+2022-11-18 15:31:30,338:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7275 (1.8687)
+2022-11-18 15:31:30,843:INFO: Dataset: univ                Batch:  1/15	Loss 1.8540 (1.8540)
+2022-11-18 15:31:31,030:INFO: Dataset: univ                Batch:  2/15	Loss 1.8501 (1.8521)
+2022-11-18 15:31:31,205:INFO: Dataset: univ                Batch:  3/15	Loss 1.8707 (1.8580)
+2022-11-18 15:31:31,379:INFO: Dataset: univ                Batch:  4/15	Loss 1.9036 (1.8695)
+2022-11-18 15:31:31,553:INFO: Dataset: univ                Batch:  5/15	Loss 1.8622 (1.8680)
+2022-11-18 15:31:31,720:INFO: Dataset: univ                Batch:  6/15	Loss 1.8995 (1.8739)
+2022-11-18 15:31:31,888:INFO: Dataset: univ                Batch:  7/15	Loss 1.8950 (1.8769)
+2022-11-18 15:31:32,056:INFO: Dataset: univ                Batch:  8/15	Loss 1.8939 (1.8788)
+2022-11-18 15:31:32,221:INFO: Dataset: univ                Batch:  9/15	Loss 1.8756 (1.8785)
+2022-11-18 15:31:32,383:INFO: Dataset: univ                Batch: 10/15	Loss 1.8584 (1.8765)
+2022-11-18 15:31:32,551:INFO: Dataset: univ                Batch: 11/15	Loss 1.8664 (1.8755)
+2022-11-18 15:31:32,716:INFO: Dataset: univ                Batch: 12/15	Loss 1.8721 (1.8752)
+2022-11-18 15:31:32,890:INFO: Dataset: univ                Batch: 13/15	Loss 1.8639 (1.8744)
+2022-11-18 15:31:33,058:INFO: Dataset: univ                Batch: 14/15	Loss 1.8918 (1.8758)
+2022-11-18 15:31:33,145:INFO: Dataset: univ                Batch: 15/15	Loss 1.9013 (1.8761)
+2022-11-18 15:31:33,579:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9258 (1.9258)
+2022-11-18 15:31:33,766:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9416 (1.9338)
+2022-11-18 15:31:33,938:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8643 (1.9125)
+2022-11-18 15:31:34,119:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8445 (1.8963)
+2022-11-18 15:31:34,304:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8743 (1.8920)
+2022-11-18 15:31:34,468:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8397 (1.8832)
+2022-11-18 15:31:34,623:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8408 (1.8776)
+2022-11-18 15:31:34,764:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8319 (1.8724)
+2022-11-18 15:31:35,170:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9431 (1.9431)
+2022-11-18 15:31:35,327:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7924 (1.8742)
+2022-11-18 15:31:35,485:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8931 (1.8805)
+2022-11-18 15:31:35,641:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8625 (1.8758)
+2022-11-18 15:31:35,804:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9959 (1.8998)
+2022-11-18 15:31:35,973:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8784 (1.8962)
+2022-11-18 15:31:36,142:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8941 (1.8959)
+2022-11-18 15:31:36,303:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9124 (1.8979)
+2022-11-18 15:31:36,469:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9566 (1.9046)
+2022-11-18 15:31:36,671:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8202 (1.8952)
+2022-11-18 15:31:36,843:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8407 (1.8904)
+2022-11-18 15:31:37,017:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8778 (1.8894)
+2022-11-18 15:31:37,190:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8919 (1.8896)
+2022-11-18 15:31:37,352:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9326 (1.8926)
+2022-11-18 15:31:37,538:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7447 (1.8837)
+2022-11-18 15:31:37,744:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8126 (1.8797)
+2022-11-18 15:31:37,921:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8103 (1.8761)
+2022-11-18 15:31:38,099:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7947 (1.8721)
+2022-11-18 15:31:38,151:INFO: - Computing loss (validation)
+2022-11-18 15:31:38,454:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9446 (1.9446)
+2022-11-18 15:31:38,498:INFO: Dataset: hotel               Batch: 2/2	Loss 2.8413 (2.0211)
+2022-11-18 15:31:38,832:INFO: Dataset: univ                Batch: 1/3	Loss 1.8527 (1.8527)
+2022-11-18 15:31:38,923:INFO: Dataset: univ                Batch: 2/3	Loss 1.8531 (1.8529)
+2022-11-18 15:31:39,008:INFO: Dataset: univ                Batch: 3/3	Loss 1.8558 (1.8538)
+2022-11-18 15:31:39,387:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9709 (1.9709)
+2022-11-18 15:31:39,447:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8648 (1.9439)
+2022-11-18 15:31:39,809:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9410 (1.9410)
+2022-11-18 15:31:39,907:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8120 (1.8750)
+2022-11-18 15:31:39,999:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7607 (1.8345)
+2022-11-18 15:31:40,091:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8932 (1.8498)
+2022-11-18 15:31:40,179:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8428 (1.8484)
+2022-11-18 15:31:40,237:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_451.pth.tar
+2022-11-18 15:31:40,238:INFO: 
+===> EPOCH: 452 (P2)
+2022-11-18 15:31:40,238:INFO: - Computing loss (training)
+2022-11-18 15:31:40,598:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8706 (1.8706)
+2022-11-18 15:31:40,784:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8749 (1.8727)
+2022-11-18 15:31:40,986:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9705 (1.9050)
+2022-11-18 15:31:41,149:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8554 (1.8954)
+2022-11-18 15:31:41,656:INFO: Dataset: univ                Batch:  1/15	Loss 1.8133 (1.8133)
+2022-11-18 15:31:41,840:INFO: Dataset: univ                Batch:  2/15	Loss 1.8962 (1.8541)
+2022-11-18 15:31:42,059:INFO: Dataset: univ                Batch:  3/15	Loss 1.8742 (1.8601)
+2022-11-18 15:31:42,246:INFO: Dataset: univ                Batch:  4/15	Loss 1.8587 (1.8597)
+2022-11-18 15:31:42,457:INFO: Dataset: univ                Batch:  5/15	Loss 1.8380 (1.8556)
+2022-11-18 15:31:42,702:INFO: Dataset: univ                Batch:  6/15	Loss 1.8242 (1.8502)
+2022-11-18 15:31:42,935:INFO: Dataset: univ                Batch:  7/15	Loss 1.8579 (1.8512)
+2022-11-18 15:31:43,167:INFO: Dataset: univ                Batch:  8/15	Loss 1.8458 (1.8505)
+2022-11-18 15:31:43,411:INFO: Dataset: univ                Batch:  9/15	Loss 1.8759 (1.8530)
+2022-11-18 15:31:43,610:INFO: Dataset: univ                Batch: 10/15	Loss 1.9068 (1.8586)
+2022-11-18 15:31:43,838:INFO: Dataset: univ                Batch: 11/15	Loss 1.8857 (1.8607)
+2022-11-18 15:31:44,099:INFO: Dataset: univ                Batch: 12/15	Loss 1.8289 (1.8583)
+2022-11-18 15:31:44,319:INFO: Dataset: univ                Batch: 13/15	Loss 1.8488 (1.8575)
+2022-11-18 15:31:44,527:INFO: Dataset: univ                Batch: 14/15	Loss 1.8770 (1.8588)
+2022-11-18 15:31:44,628:INFO: Dataset: univ                Batch: 15/15	Loss 1.8062 (1.8580)
+2022-11-18 15:31:45,155:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9143 (1.9143)
+2022-11-18 15:31:45,367:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8623 (1.8889)
+2022-11-18 15:31:45,562:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9616 (1.9143)
+2022-11-18 15:31:45,745:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8050 (1.8848)
+2022-11-18 15:31:45,925:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8357 (1.8741)
+2022-11-18 15:31:46,112:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9360 (1.8845)
+2022-11-18 15:31:46,309:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8038 (1.8726)
+2022-11-18 15:31:46,490:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8568 (1.8709)
+2022-11-18 15:31:46,953:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8838 (1.8838)
+2022-11-18 15:31:47,128:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8610 (1.8722)
+2022-11-18 15:31:47,299:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8071 (1.8505)
+2022-11-18 15:31:47,467:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7889 (1.8360)
+2022-11-18 15:31:47,642:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9210 (1.8518)
+2022-11-18 15:31:47,820:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8931 (1.8589)
+2022-11-18 15:31:47,999:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8411 (1.8563)
+2022-11-18 15:31:48,184:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8592 (1.8566)
+2022-11-18 15:31:48,354:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8162 (1.8520)
+2022-11-18 15:31:48,523:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8596 (1.8527)
+2022-11-18 15:31:48,690:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9033 (1.8570)
+2022-11-18 15:31:48,858:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8064 (1.8523)
+2022-11-18 15:31:49,029:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8036 (1.8484)
+2022-11-18 15:31:49,225:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8972 (1.8520)
+2022-11-18 15:31:49,415:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8776 (1.8537)
+2022-11-18 15:31:49,594:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9056 (1.8572)
+2022-11-18 15:31:49,779:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8999 (1.8595)
+2022-11-18 15:31:49,947:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8379 (1.8585)
+2022-11-18 15:31:50,006:INFO: - Computing loss (validation)
+2022-11-18 15:31:50,295:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8321 (1.8321)
+2022-11-18 15:31:50,333:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9303 (1.8388)
+2022-11-18 15:31:50,678:INFO: Dataset: univ                Batch: 1/3	Loss 1.9086 (1.9086)
+2022-11-18 15:31:50,765:INFO: Dataset: univ                Batch: 2/3	Loss 1.8934 (1.9012)
+2022-11-18 15:31:50,844:INFO: Dataset: univ                Batch: 3/3	Loss 1.9178 (1.9058)
+2022-11-18 15:31:51,190:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9731 (1.9731)
+2022-11-18 15:31:51,248:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9743 (1.9734)
+2022-11-18 15:31:51,574:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9465 (1.9465)
+2022-11-18 15:31:51,653:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9079 (1.9277)
+2022-11-18 15:31:51,736:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8129 (1.8875)
+2022-11-18 15:31:51,821:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8959 (1.8896)
+2022-11-18 15:31:51,905:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8485 (1.8818)
+2022-11-18 15:31:51,964:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_452.pth.tar
+2022-11-18 15:31:51,964:INFO: 
+===> EPOCH: 453 (P2)
+2022-11-18 15:31:51,965:INFO: - Computing loss (training)
+2022-11-18 15:31:52,345:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7848 (1.7848)
+2022-11-18 15:31:52,520:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9097 (1.8486)
+2022-11-18 15:31:52,691:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9588 (1.8849)
+2022-11-18 15:31:52,828:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9781 (1.8998)
+2022-11-18 15:31:53,252:INFO: Dataset: univ                Batch:  1/15	Loss 1.9016 (1.9016)
+2022-11-18 15:31:53,427:INFO: Dataset: univ                Batch:  2/15	Loss 1.8810 (1.8907)
+2022-11-18 15:31:53,602:INFO: Dataset: univ                Batch:  3/15	Loss 1.8362 (1.8717)
+2022-11-18 15:31:53,781:INFO: Dataset: univ                Batch:  4/15	Loss 1.9242 (1.8846)
+2022-11-18 15:31:53,953:INFO: Dataset: univ                Batch:  5/15	Loss 1.8146 (1.8729)
+2022-11-18 15:31:54,127:INFO: Dataset: univ                Batch:  6/15	Loss 1.8910 (1.8761)
+2022-11-18 15:31:54,320:INFO: Dataset: univ                Batch:  7/15	Loss 1.8801 (1.8767)
+2022-11-18 15:31:54,506:INFO: Dataset: univ                Batch:  8/15	Loss 1.8615 (1.8749)
+2022-11-18 15:31:54,691:INFO: Dataset: univ                Batch:  9/15	Loss 1.8496 (1.8722)
+2022-11-18 15:31:54,865:INFO: Dataset: univ                Batch: 10/15	Loss 1.8771 (1.8727)
+2022-11-18 15:31:55,041:INFO: Dataset: univ                Batch: 11/15	Loss 1.8655 (1.8720)
+2022-11-18 15:31:55,211:INFO: Dataset: univ                Batch: 12/15	Loss 1.8509 (1.8705)
+2022-11-18 15:31:55,390:INFO: Dataset: univ                Batch: 13/15	Loss 1.8791 (1.8710)
+2022-11-18 15:31:55,565:INFO: Dataset: univ                Batch: 14/15	Loss 1.8932 (1.8726)
+2022-11-18 15:31:55,663:INFO: Dataset: univ                Batch: 15/15	Loss 1.8445 (1.8722)
+2022-11-18 15:31:56,106:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8902 (1.8902)
+2022-11-18 15:31:56,285:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7615 (1.8213)
+2022-11-18 15:31:56,463:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9060 (1.8456)
+2022-11-18 15:31:56,636:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9230 (1.8656)
+2022-11-18 15:31:56,800:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9249 (1.8778)
+2022-11-18 15:31:56,970:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7903 (1.8640)
+2022-11-18 15:31:57,139:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9300 (1.8744)
+2022-11-18 15:31:57,295:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8837 (1.8753)
+2022-11-18 15:31:57,727:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8442 (1.8442)
+2022-11-18 15:31:57,904:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9258 (1.8861)
+2022-11-18 15:31:58,084:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8746 (1.8820)
+2022-11-18 15:31:58,282:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7972 (1.8611)
+2022-11-18 15:31:58,457:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9499 (1.8805)
+2022-11-18 15:31:58,650:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8326 (1.8731)
+2022-11-18 15:31:58,846:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8588 (1.8710)
+2022-11-18 15:31:59,037:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8481 (1.8683)
+2022-11-18 15:31:59,228:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8629 (1.8677)
+2022-11-18 15:31:59,418:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8161 (1.8625)
+2022-11-18 15:31:59,609:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8599 (1.8622)
+2022-11-18 15:31:59,799:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8905 (1.8645)
+2022-11-18 15:31:59,987:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7863 (1.8585)
+2022-11-18 15:32:00,176:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8359 (1.8569)
+2022-11-18 15:32:00,353:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7903 (1.8520)
+2022-11-18 15:32:00,528:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8796 (1.8537)
+2022-11-18 15:32:00,699:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8279 (1.8521)
+2022-11-18 15:32:00,857:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8791 (1.8534)
+2022-11-18 15:32:00,905:INFO: - Computing loss (validation)
+2022-11-18 15:32:01,178:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8389 (1.8389)
+2022-11-18 15:32:01,215:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6077 (1.8239)
+2022-11-18 15:32:01,554:INFO: Dataset: univ                Batch: 1/3	Loss 1.8765 (1.8765)
+2022-11-18 15:32:01,643:INFO: Dataset: univ                Batch: 2/3	Loss 1.8503 (1.8642)
+2022-11-18 15:32:01,725:INFO: Dataset: univ                Batch: 3/3	Loss 1.9340 (1.8840)
+2022-11-18 15:32:02,069:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8916 (1.8916)
+2022-11-18 15:32:02,123:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9326 (1.9020)
+2022-11-18 15:32:02,443:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8717 (1.8717)
+2022-11-18 15:32:02,529:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8853 (1.8787)
+2022-11-18 15:32:02,612:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8308 (1.8628)
+2022-11-18 15:32:02,696:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8717 (1.8649)
+2022-11-18 15:32:02,773:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9216 (1.8756)
+2022-11-18 15:32:02,833:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_453.pth.tar
+2022-11-18 15:32:02,833:INFO: 
+===> EPOCH: 454 (P2)
+2022-11-18 15:32:02,834:INFO: - Computing loss (training)
+2022-11-18 15:32:03,202:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9248 (1.9248)
+2022-11-18 15:32:03,380:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9291 (1.9268)
+2022-11-18 15:32:03,559:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8996 (1.9174)
+2022-11-18 15:32:03,701:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8552 (1.9068)
+2022-11-18 15:32:04,147:INFO: Dataset: univ                Batch:  1/15	Loss 1.8705 (1.8705)
+2022-11-18 15:32:04,328:INFO: Dataset: univ                Batch:  2/15	Loss 1.8858 (1.8782)
+2022-11-18 15:32:04,507:INFO: Dataset: univ                Batch:  3/15	Loss 1.8387 (1.8650)
+2022-11-18 15:32:04,682:INFO: Dataset: univ                Batch:  4/15	Loss 1.8849 (1.8697)
+2022-11-18 15:32:04,870:INFO: Dataset: univ                Batch:  5/15	Loss 1.8580 (1.8675)
+2022-11-18 15:32:05,052:INFO: Dataset: univ                Batch:  6/15	Loss 1.8612 (1.8665)
+2022-11-18 15:32:05,240:INFO: Dataset: univ                Batch:  7/15	Loss 1.8632 (1.8660)
+2022-11-18 15:32:05,424:INFO: Dataset: univ                Batch:  8/15	Loss 1.8572 (1.8648)
+2022-11-18 15:32:05,594:INFO: Dataset: univ                Batch:  9/15	Loss 1.8458 (1.8626)
+2022-11-18 15:32:05,765:INFO: Dataset: univ                Batch: 10/15	Loss 1.8816 (1.8645)
+2022-11-18 15:32:05,938:INFO: Dataset: univ                Batch: 11/15	Loss 1.8658 (1.8646)
+2022-11-18 15:32:06,116:INFO: Dataset: univ                Batch: 12/15	Loss 1.8757 (1.8655)
+2022-11-18 15:32:06,299:INFO: Dataset: univ                Batch: 13/15	Loss 1.8547 (1.8647)
+2022-11-18 15:32:06,478:INFO: Dataset: univ                Batch: 14/15	Loss 1.8549 (1.8639)
+2022-11-18 15:32:06,574:INFO: Dataset: univ                Batch: 15/15	Loss 1.8656 (1.8640)
+2022-11-18 15:32:06,993:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7943 (1.7943)
+2022-11-18 15:32:07,158:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9011 (1.8523)
+2022-11-18 15:32:07,380:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9244 (1.8746)
+2022-11-18 15:32:07,599:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7493 (1.8443)
+2022-11-18 15:32:07,826:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9383 (1.8634)
+2022-11-18 15:32:08,019:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9952 (1.8860)
+2022-11-18 15:32:08,186:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8317 (1.8780)
+2022-11-18 15:32:08,346:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8362 (1.8740)
+2022-11-18 15:32:08,772:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9218 (1.9218)
+2022-11-18 15:32:08,934:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8781 (1.8987)
+2022-11-18 15:32:09,102:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7695 (1.8561)
+2022-11-18 15:32:09,263:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8147 (1.8455)
+2022-11-18 15:32:09,460:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8912 (1.8543)
+2022-11-18 15:32:09,655:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8935 (1.8605)
+2022-11-18 15:32:09,864:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8375 (1.8568)
+2022-11-18 15:32:10,092:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9917 (1.8722)
+2022-11-18 15:32:10,275:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8786 (1.8728)
+2022-11-18 15:32:10,456:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8983 (1.8754)
+2022-11-18 15:32:10,623:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9141 (1.8788)
+2022-11-18 15:32:10,804:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8592 (1.8773)
+2022-11-18 15:32:10,991:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9147 (1.8802)
+2022-11-18 15:32:11,177:INFO: Dataset: zara2               Batch: 14/18	Loss 1.6826 (1.8667)
+2022-11-18 15:32:11,339:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9043 (1.8691)
+2022-11-18 15:32:11,505:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7381 (1.8610)
+2022-11-18 15:32:11,684:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8620 (1.8611)
+2022-11-18 15:32:11,836:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8992 (1.8629)
+2022-11-18 15:32:11,883:INFO: - Computing loss (validation)
+2022-11-18 15:32:12,203:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8541 (1.8541)
+2022-11-18 15:32:12,242:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7830 (1.8495)
+2022-11-18 15:32:12,611:INFO: Dataset: univ                Batch: 1/3	Loss 1.8882 (1.8882)
+2022-11-18 15:32:12,693:INFO: Dataset: univ                Batch: 2/3	Loss 1.8402 (1.8633)
+2022-11-18 15:32:12,770:INFO: Dataset: univ                Batch: 3/3	Loss 1.8701 (1.8652)
+2022-11-18 15:32:13,140:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8529 (1.8529)
+2022-11-18 15:32:13,198:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9622 (1.8793)
+2022-11-18 15:32:13,542:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8615 (1.8615)
+2022-11-18 15:32:13,628:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8640 (1.8628)
+2022-11-18 15:32:13,719:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8700 (1.8652)
+2022-11-18 15:32:13,820:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8716 (1.8668)
+2022-11-18 15:32:13,915:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8990 (1.8733)
+2022-11-18 15:32:13,985:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_454.pth.tar
+2022-11-18 15:32:13,985:INFO: 
+===> EPOCH: 455 (P2)
+2022-11-18 15:32:13,985:INFO: - Computing loss (training)
+2022-11-18 15:32:14,379:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9217 (1.9217)
+2022-11-18 15:32:14,552:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8217 (1.8698)
+2022-11-18 15:32:14,704:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8467 (1.8620)
+2022-11-18 15:32:14,835:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8161 (1.8549)
+2022-11-18 15:32:15,306:INFO: Dataset: univ                Batch:  1/15	Loss 1.9007 (1.9007)
+2022-11-18 15:32:15,487:INFO: Dataset: univ                Batch:  2/15	Loss 1.8456 (1.8762)
+2022-11-18 15:32:15,662:INFO: Dataset: univ                Batch:  3/15	Loss 1.8963 (1.8833)
+2022-11-18 15:32:15,837:INFO: Dataset: univ                Batch:  4/15	Loss 1.9005 (1.8882)
+2022-11-18 15:32:16,009:INFO: Dataset: univ                Batch:  5/15	Loss 1.8378 (1.8774)
+2022-11-18 15:32:16,185:INFO: Dataset: univ                Batch:  6/15	Loss 1.9074 (1.8822)
+2022-11-18 15:32:16,398:INFO: Dataset: univ                Batch:  7/15	Loss 1.8180 (1.8726)
+2022-11-18 15:32:16,609:INFO: Dataset: univ                Batch:  8/15	Loss 1.8888 (1.8746)
+2022-11-18 15:32:16,821:INFO: Dataset: univ                Batch:  9/15	Loss 1.8493 (1.8720)
+2022-11-18 15:32:17,011:INFO: Dataset: univ                Batch: 10/15	Loss 1.8737 (1.8722)
+2022-11-18 15:32:17,187:INFO: Dataset: univ                Batch: 11/15	Loss 1.9008 (1.8748)
+2022-11-18 15:32:17,370:INFO: Dataset: univ                Batch: 12/15	Loss 1.8713 (1.8745)
+2022-11-18 15:32:17,541:INFO: Dataset: univ                Batch: 13/15	Loss 1.8483 (1.8723)
+2022-11-18 15:32:17,717:INFO: Dataset: univ                Batch: 14/15	Loss 1.8844 (1.8732)
+2022-11-18 15:32:17,820:INFO: Dataset: univ                Batch: 15/15	Loss 1.8544 (1.8730)
+2022-11-18 15:32:18,255:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8481 (1.8481)
+2022-11-18 15:32:18,413:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8957 (1.8720)
+2022-11-18 15:32:18,594:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9330 (1.8922)
+2022-11-18 15:32:18,774:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9638 (1.9107)
+2022-11-18 15:32:18,956:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9556 (1.9197)
+2022-11-18 15:32:19,113:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9498 (1.9250)
+2022-11-18 15:32:19,270:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8916 (1.9197)
+2022-11-18 15:32:19,410:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8719 (1.9142)
+2022-11-18 15:32:19,880:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8633 (1.8633)
+2022-11-18 15:32:20,070:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7841 (1.8242)
+2022-11-18 15:32:20,244:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8055 (1.8184)
+2022-11-18 15:32:20,408:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9189 (1.8420)
+2022-11-18 15:32:20,574:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8726 (1.8477)
+2022-11-18 15:32:20,757:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8771 (1.8526)
+2022-11-18 15:32:20,925:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9618 (1.8667)
+2022-11-18 15:32:21,083:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9085 (1.8720)
+2022-11-18 15:32:21,240:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8774 (1.8726)
+2022-11-18 15:32:21,400:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8488 (1.8702)
+2022-11-18 15:32:21,556:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8605 (1.8693)
+2022-11-18 15:32:21,714:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8632 (1.8687)
+2022-11-18 15:32:21,876:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9232 (1.8732)
+2022-11-18 15:32:22,033:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8286 (1.8703)
+2022-11-18 15:32:22,195:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7857 (1.8649)
+2022-11-18 15:32:22,371:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7761 (1.8591)
+2022-11-18 15:32:22,543:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8555 (1.8589)
+2022-11-18 15:32:22,699:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8984 (1.8606)
+2022-11-18 15:32:22,750:INFO: - Computing loss (validation)
+2022-11-18 15:32:23,019:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8794 (1.8794)
+2022-11-18 15:32:23,055:INFO: Dataset: hotel               Batch: 2/2	Loss 2.8693 (1.9300)
+2022-11-18 15:32:23,387:INFO: Dataset: univ                Batch: 1/3	Loss 1.9229 (1.9229)
+2022-11-18 15:32:23,479:INFO: Dataset: univ                Batch: 2/3	Loss 1.8483 (1.8907)
+2022-11-18 15:32:23,563:INFO: Dataset: univ                Batch: 3/3	Loss 1.8849 (1.8891)
+2022-11-18 15:32:23,900:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8578 (1.8578)
+2022-11-18 15:32:23,954:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7484 (1.8321)
+2022-11-18 15:32:24,274:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8109 (1.8109)
+2022-11-18 15:32:24,356:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8331 (1.8223)
+2022-11-18 15:32:24,436:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8699 (1.8389)
+2022-11-18 15:32:24,518:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9196 (1.8584)
+2022-11-18 15:32:24,601:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8475 (1.8561)
+2022-11-18 15:32:24,657:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_455.pth.tar
+2022-11-18 15:32:24,657:INFO: 
+===> EPOCH: 456 (P2)
+2022-11-18 15:32:24,657:INFO: - Computing loss (training)
+2022-11-18 15:32:24,999:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9141 (1.9141)
+2022-11-18 15:32:25,155:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9506 (1.9331)
+2022-11-18 15:32:25,311:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9212 (1.9292)
+2022-11-18 15:32:25,437:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0401 (1.9499)
+2022-11-18 15:32:25,882:INFO: Dataset: univ                Batch:  1/15	Loss 1.8790 (1.8790)
+2022-11-18 15:32:26,058:INFO: Dataset: univ                Batch:  2/15	Loss 1.8581 (1.8690)
+2022-11-18 15:32:26,242:INFO: Dataset: univ                Batch:  3/15	Loss 1.8518 (1.8635)
+2022-11-18 15:32:26,429:INFO: Dataset: univ                Batch:  4/15	Loss 1.8771 (1.8667)
+2022-11-18 15:32:26,608:INFO: Dataset: univ                Batch:  5/15	Loss 1.8834 (1.8701)
+2022-11-18 15:32:26,788:INFO: Dataset: univ                Batch:  6/15	Loss 1.8941 (1.8741)
+2022-11-18 15:32:26,960:INFO: Dataset: univ                Batch:  7/15	Loss 1.9140 (1.8796)
+2022-11-18 15:32:27,134:INFO: Dataset: univ                Batch:  8/15	Loss 1.9002 (1.8821)
+2022-11-18 15:32:27,338:INFO: Dataset: univ                Batch:  9/15	Loss 1.8743 (1.8813)
+2022-11-18 15:32:27,528:INFO: Dataset: univ                Batch: 10/15	Loss 1.8891 (1.8820)
+2022-11-18 15:32:27,709:INFO: Dataset: univ                Batch: 11/15	Loss 1.9007 (1.8835)
+2022-11-18 15:32:27,897:INFO: Dataset: univ                Batch: 12/15	Loss 1.8296 (1.8788)
+2022-11-18 15:32:28,080:INFO: Dataset: univ                Batch: 13/15	Loss 1.8364 (1.8752)
+2022-11-18 15:32:28,259:INFO: Dataset: univ                Batch: 14/15	Loss 1.8613 (1.8743)
+2022-11-18 15:32:28,345:INFO: Dataset: univ                Batch: 15/15	Loss 1.8091 (1.8735)
+2022-11-18 15:32:28,758:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9578 (1.9578)
+2022-11-18 15:32:28,925:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8768 (1.9181)
+2022-11-18 15:32:29,101:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9398 (1.9247)
+2022-11-18 15:32:29,273:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8838 (1.9139)
+2022-11-18 15:32:29,486:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8899 (1.9099)
+2022-11-18 15:32:29,725:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9561 (1.9174)
+2022-11-18 15:32:29,911:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7768 (1.8989)
+2022-11-18 15:32:30,087:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9882 (1.9106)
+2022-11-18 15:32:30,669:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8313 (1.8313)
+2022-11-18 15:32:30,862:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8832 (1.8564)
+2022-11-18 15:32:31,040:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8690 (1.8606)
+2022-11-18 15:32:31,200:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9410 (1.8808)
+2022-11-18 15:32:31,370:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8057 (1.8661)
+2022-11-18 15:32:31,535:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8505 (1.8637)
+2022-11-18 15:32:31,694:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9180 (1.8715)
+2022-11-18 15:32:31,862:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8599 (1.8699)
+2022-11-18 15:32:32,056:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8989 (1.8732)
+2022-11-18 15:32:32,239:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7538 (1.8615)
+2022-11-18 15:32:32,398:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8402 (1.8595)
+2022-11-18 15:32:32,566:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0372 (1.8750)
+2022-11-18 15:32:32,735:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8607 (1.8739)
+2022-11-18 15:32:32,887:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9077 (1.8762)
+2022-11-18 15:32:33,050:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7962 (1.8700)
+2022-11-18 15:32:33,210:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8307 (1.8675)
+2022-11-18 15:32:33,381:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9406 (1.8715)
+2022-11-18 15:32:33,534:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7932 (1.8679)
+2022-11-18 15:32:33,582:INFO: - Computing loss (validation)
+2022-11-18 15:32:33,882:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8939 (1.8939)
+2022-11-18 15:32:33,920:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9126 (1.8953)
+2022-11-18 15:32:34,247:INFO: Dataset: univ                Batch: 1/3	Loss 1.8748 (1.8748)
+2022-11-18 15:32:34,325:INFO: Dataset: univ                Batch: 2/3	Loss 1.9145 (1.8923)
+2022-11-18 15:32:34,398:INFO: Dataset: univ                Batch: 3/3	Loss 1.8642 (1.8837)
+2022-11-18 15:32:34,710:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9601 (1.9601)
+2022-11-18 15:32:34,761:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7070 (1.8942)
+2022-11-18 15:32:35,074:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8064 (1.8064)
+2022-11-18 15:32:35,152:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8156 (1.8108)
+2022-11-18 15:32:35,231:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8374 (1.8209)
+2022-11-18 15:32:35,309:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9081 (1.8428)
+2022-11-18 15:32:35,384:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8745 (1.8491)
+2022-11-18 15:32:35,440:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_456.pth.tar
+2022-11-18 15:32:35,441:INFO: 
+===> EPOCH: 457 (P2)
+2022-11-18 15:32:35,441:INFO: - Computing loss (training)
+2022-11-18 15:32:35,791:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8738 (1.8738)
+2022-11-18 15:32:35,943:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8107 (1.8418)
+2022-11-18 15:32:36,101:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8824 (1.8552)
+2022-11-18 15:32:36,219:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7646 (1.8404)
+2022-11-18 15:32:36,624:INFO: Dataset: univ                Batch:  1/15	Loss 1.8379 (1.8379)
+2022-11-18 15:32:36,789:INFO: Dataset: univ                Batch:  2/15	Loss 1.8519 (1.8453)
+2022-11-18 15:32:36,946:INFO: Dataset: univ                Batch:  3/15	Loss 1.8878 (1.8602)
+2022-11-18 15:32:37,102:INFO: Dataset: univ                Batch:  4/15	Loss 1.8808 (1.8656)
+2022-11-18 15:32:37,259:INFO: Dataset: univ                Batch:  5/15	Loss 1.8280 (1.8581)
+2022-11-18 15:32:37,429:INFO: Dataset: univ                Batch:  6/15	Loss 1.9070 (1.8669)
+2022-11-18 15:32:37,596:INFO: Dataset: univ                Batch:  7/15	Loss 1.9215 (1.8745)
+2022-11-18 15:32:37,761:INFO: Dataset: univ                Batch:  8/15	Loss 1.8767 (1.8748)
+2022-11-18 15:32:37,919:INFO: Dataset: univ                Batch:  9/15	Loss 1.8543 (1.8726)
+2022-11-18 15:32:38,075:INFO: Dataset: univ                Batch: 10/15	Loss 1.8688 (1.8722)
+2022-11-18 15:32:38,233:INFO: Dataset: univ                Batch: 11/15	Loss 1.8646 (1.8716)
+2022-11-18 15:32:38,390:INFO: Dataset: univ                Batch: 12/15	Loss 1.8555 (1.8703)
+2022-11-18 15:32:38,546:INFO: Dataset: univ                Batch: 13/15	Loss 1.8618 (1.8697)
+2022-11-18 15:32:38,704:INFO: Dataset: univ                Batch: 14/15	Loss 1.8786 (1.8703)
+2022-11-18 15:32:38,788:INFO: Dataset: univ                Batch: 15/15	Loss 1.8526 (1.8700)
+2022-11-18 15:32:39,197:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9996 (1.9996)
+2022-11-18 15:32:39,375:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8443 (1.9248)
+2022-11-18 15:32:39,545:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9075 (1.9191)
+2022-11-18 15:32:39,699:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8823 (1.9103)
+2022-11-18 15:32:39,855:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9440 (1.9174)
+2022-11-18 15:32:40,009:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8558 (1.9063)
+2022-11-18 15:32:40,161:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9720 (1.9155)
+2022-11-18 15:32:40,299:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9346 (1.9174)
+2022-11-18 15:32:40,713:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8526 (1.8526)
+2022-11-18 15:32:40,879:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8737 (1.8626)
+2022-11-18 15:32:41,037:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8005 (1.8426)
+2022-11-18 15:32:41,189:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9236 (1.8624)
+2022-11-18 15:32:41,347:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9214 (1.8725)
+2022-11-18 15:32:41,502:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8042 (1.8604)
+2022-11-18 15:32:41,677:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7891 (1.8516)
+2022-11-18 15:32:41,845:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9114 (1.8579)
+2022-11-18 15:32:41,999:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8624 (1.8584)
+2022-11-18 15:32:42,150:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8991 (1.8629)
+2022-11-18 15:32:42,303:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8834 (1.8646)
+2022-11-18 15:32:42,454:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9354 (1.8712)
+2022-11-18 15:32:42,606:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9266 (1.8752)
+2022-11-18 15:32:42,775:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9357 (1.8798)
+2022-11-18 15:32:42,951:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8840 (1.8800)
+2022-11-18 15:32:43,118:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7563 (1.8728)
+2022-11-18 15:32:43,273:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8358 (1.8705)
+2022-11-18 15:32:43,417:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8894 (1.8715)
+2022-11-18 15:32:43,466:INFO: - Computing loss (validation)
+2022-11-18 15:32:43,742:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8750 (1.8750)
+2022-11-18 15:32:43,780:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6268 (1.8606)
+2022-11-18 15:32:44,113:INFO: Dataset: univ                Batch: 1/3	Loss 1.8565 (1.8565)
+2022-11-18 15:32:44,201:INFO: Dataset: univ                Batch: 2/3	Loss 1.8645 (1.8605)
+2022-11-18 15:32:44,282:INFO: Dataset: univ                Batch: 3/3	Loss 1.8501 (1.8573)
+2022-11-18 15:32:44,594:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8977 (1.8977)
+2022-11-18 15:32:44,641:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0609 (1.9439)
+2022-11-18 15:32:44,970:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8266 (1.8266)
+2022-11-18 15:32:45,046:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8271 (1.8268)
+2022-11-18 15:32:45,123:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7889 (1.8144)
+2022-11-18 15:32:45,201:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9560 (1.8487)
+2022-11-18 15:32:45,275:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9267 (1.8649)
+2022-11-18 15:32:45,333:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_457.pth.tar
+2022-11-18 15:32:45,333:INFO: 
+===> EPOCH: 458 (P2)
+2022-11-18 15:32:45,334:INFO: - Computing loss (training)
+2022-11-18 15:32:45,709:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9390 (1.9390)
+2022-11-18 15:32:45,870:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8294 (1.8851)
+2022-11-18 15:32:46,023:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9475 (1.9058)
+2022-11-18 15:32:46,142:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9436 (1.9126)
+2022-11-18 15:32:46,621:INFO: Dataset: univ                Batch:  1/15	Loss 1.9080 (1.9080)
+2022-11-18 15:32:46,783:INFO: Dataset: univ                Batch:  2/15	Loss 1.8969 (1.9022)
+2022-11-18 15:32:46,960:INFO: Dataset: univ                Batch:  3/15	Loss 1.8284 (1.8766)
+2022-11-18 15:32:47,140:INFO: Dataset: univ                Batch:  4/15	Loss 1.8685 (1.8745)
+2022-11-18 15:32:47,308:INFO: Dataset: univ                Batch:  5/15	Loss 1.8985 (1.8798)
+2022-11-18 15:32:47,470:INFO: Dataset: univ                Batch:  6/15	Loss 1.8248 (1.8695)
+2022-11-18 15:32:47,644:INFO: Dataset: univ                Batch:  7/15	Loss 1.8919 (1.8725)
+2022-11-18 15:32:47,818:INFO: Dataset: univ                Batch:  8/15	Loss 1.8584 (1.8708)
+2022-11-18 15:32:47,999:INFO: Dataset: univ                Batch:  9/15	Loss 1.8386 (1.8673)
+2022-11-18 15:32:48,173:INFO: Dataset: univ                Batch: 10/15	Loss 1.8627 (1.8669)
+2022-11-18 15:32:48,367:INFO: Dataset: univ                Batch: 11/15	Loss 1.8893 (1.8689)
+2022-11-18 15:32:48,547:INFO: Dataset: univ                Batch: 12/15	Loss 1.8732 (1.8693)
+2022-11-18 15:32:48,723:INFO: Dataset: univ                Batch: 13/15	Loss 1.8824 (1.8704)
+2022-11-18 15:32:48,899:INFO: Dataset: univ                Batch: 14/15	Loss 1.9027 (1.8725)
+2022-11-18 15:32:48,995:INFO: Dataset: univ                Batch: 15/15	Loss 1.8344 (1.8721)
+2022-11-18 15:32:49,439:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8439 (1.8439)
+2022-11-18 15:32:49,613:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8543 (1.8493)
+2022-11-18 15:32:49,792:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9421 (1.8797)
+2022-11-18 15:32:49,982:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8794 (1.8796)
+2022-11-18 15:32:50,178:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9109 (1.8860)
+2022-11-18 15:32:50,387:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9242 (1.8921)
+2022-11-18 15:32:50,574:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8871 (1.8914)
+2022-11-18 15:32:50,725:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8805 (1.8903)
+2022-11-18 15:32:51,146:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9577 (1.9577)
+2022-11-18 15:32:51,304:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8705 (1.9147)
+2022-11-18 15:32:51,461:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8746 (1.9017)
+2022-11-18 15:32:51,620:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8849 (1.8975)
+2022-11-18 15:32:51,775:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9343 (1.9050)
+2022-11-18 15:32:51,943:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9141 (1.9065)
+2022-11-18 15:32:52,108:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8014 (1.8922)
+2022-11-18 15:32:52,266:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8631 (1.8885)
+2022-11-18 15:32:52,421:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9028 (1.8901)
+2022-11-18 15:32:52,574:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8189 (1.8829)
+2022-11-18 15:32:52,733:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9449 (1.8880)
+2022-11-18 15:32:52,891:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8269 (1.8824)
+2022-11-18 15:32:53,047:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8706 (1.8815)
+2022-11-18 15:32:53,210:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8661 (1.8803)
+2022-11-18 15:32:53,378:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8868 (1.8808)
+2022-11-18 15:32:53,547:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8368 (1.8780)
+2022-11-18 15:32:53,708:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8529 (1.8766)
+2022-11-18 15:32:53,855:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8675 (1.8762)
+2022-11-18 15:32:53,903:INFO: - Computing loss (validation)
+2022-11-18 15:32:54,190:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9091 (1.9091)
+2022-11-18 15:32:54,231:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1351 (1.9261)
+2022-11-18 15:32:54,557:INFO: Dataset: univ                Batch: 1/3	Loss 1.8942 (1.8942)
+2022-11-18 15:32:54,638:INFO: Dataset: univ                Batch: 2/3	Loss 1.8516 (1.8752)
+2022-11-18 15:32:54,712:INFO: Dataset: univ                Batch: 3/3	Loss 1.8355 (1.8625)
+2022-11-18 15:32:55,039:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8328 (1.8328)
+2022-11-18 15:32:55,090:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0588 (1.8844)
+2022-11-18 15:32:55,405:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7819 (1.7819)
+2022-11-18 15:32:55,482:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8489 (1.8156)
+2022-11-18 15:32:55,559:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8685 (1.8333)
+2022-11-18 15:32:55,637:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9272 (1.8557)
+2022-11-18 15:32:55,713:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8424 (1.8532)
+2022-11-18 15:32:55,766:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_458.pth.tar
+2022-11-18 15:32:55,766:INFO: 
+===> EPOCH: 459 (P2)
+2022-11-18 15:32:55,767:INFO: - Computing loss (training)
+2022-11-18 15:32:56,120:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9745 (1.9745)
+2022-11-18 15:32:56,288:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9584 (1.9665)
+2022-11-18 15:32:56,534:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8725 (1.9355)
+2022-11-18 15:32:56,661:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9982 (1.9463)
+2022-11-18 15:32:57,109:INFO: Dataset: univ                Batch:  1/15	Loss 1.8779 (1.8779)
+2022-11-18 15:32:57,290:INFO: Dataset: univ                Batch:  2/15	Loss 1.8568 (1.8672)
+2022-11-18 15:32:57,470:INFO: Dataset: univ                Batch:  3/15	Loss 1.9103 (1.8821)
+2022-11-18 15:32:57,651:INFO: Dataset: univ                Batch:  4/15	Loss 1.8811 (1.8819)
+2022-11-18 15:32:57,853:INFO: Dataset: univ                Batch:  5/15	Loss 1.8546 (1.8767)
+2022-11-18 15:32:58,042:INFO: Dataset: univ                Batch:  6/15	Loss 1.8637 (1.8745)
+2022-11-18 15:32:58,251:INFO: Dataset: univ                Batch:  7/15	Loss 1.8619 (1.8727)
+2022-11-18 15:32:58,449:INFO: Dataset: univ                Batch:  8/15	Loss 1.8915 (1.8750)
+2022-11-18 15:32:58,618:INFO: Dataset: univ                Batch:  9/15	Loss 1.8542 (1.8728)
+2022-11-18 15:32:58,787:INFO: Dataset: univ                Batch: 10/15	Loss 1.8816 (1.8737)
+2022-11-18 15:32:58,969:INFO: Dataset: univ                Batch: 11/15	Loss 1.8769 (1.8740)
+2022-11-18 15:32:59,142:INFO: Dataset: univ                Batch: 12/15	Loss 1.9266 (1.8781)
+2022-11-18 15:32:59,307:INFO: Dataset: univ                Batch: 13/15	Loss 1.8645 (1.8770)
+2022-11-18 15:32:59,474:INFO: Dataset: univ                Batch: 14/15	Loss 1.8529 (1.8752)
+2022-11-18 15:32:59,565:INFO: Dataset: univ                Batch: 15/15	Loss 1.9171 (1.8757)
+2022-11-18 15:33:00,000:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9298 (1.9298)
+2022-11-18 15:33:00,169:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9374 (1.9337)
+2022-11-18 15:33:00,335:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8827 (1.9181)
+2022-11-18 15:33:00,499:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9148 (1.9172)
+2022-11-18 15:33:00,657:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8153 (1.8968)
+2022-11-18 15:33:00,813:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9829 (1.9118)
+2022-11-18 15:33:00,969:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8701 (1.9063)
+2022-11-18 15:33:01,110:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9479 (1.9108)
+2022-11-18 15:33:01,529:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9438 (1.9438)
+2022-11-18 15:33:01,697:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8929 (1.9180)
+2022-11-18 15:33:01,878:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9396 (1.9250)
+2022-11-18 15:33:02,072:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9044 (1.9199)
+2022-11-18 15:33:02,282:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8244 (1.9012)
+2022-11-18 15:33:02,453:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9020 (1.9013)
+2022-11-18 15:33:02,611:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7893 (1.8857)
+2022-11-18 15:33:02,769:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9163 (1.8894)
+2022-11-18 15:33:02,933:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7932 (1.8798)
+2022-11-18 15:33:03,089:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8970 (1.8817)
+2022-11-18 15:33:03,254:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8999 (1.8833)
+2022-11-18 15:33:03,414:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8737 (1.8826)
+2022-11-18 15:33:03,577:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8686 (1.8815)
+2022-11-18 15:33:03,737:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8882 (1.8820)
+2022-11-18 15:33:03,896:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9040 (1.8835)
+2022-11-18 15:33:04,058:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8714 (1.8828)
+2022-11-18 15:33:04,222:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8671 (1.8819)
+2022-11-18 15:33:04,375:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9467 (1.8853)
+2022-11-18 15:33:04,423:INFO: - Computing loss (validation)
+2022-11-18 15:33:04,749:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8653 (1.8653)
+2022-11-18 15:33:04,789:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5501 (1.8438)
+2022-11-18 15:33:05,184:INFO: Dataset: univ                Batch: 1/3	Loss 1.8576 (1.8576)
+2022-11-18 15:33:05,282:INFO: Dataset: univ                Batch: 2/3	Loss 1.8236 (1.8421)
+2022-11-18 15:33:05,373:INFO: Dataset: univ                Batch: 3/3	Loss 1.9066 (1.8650)
+2022-11-18 15:33:05,740:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8941 (1.8941)
+2022-11-18 15:33:05,793:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9541 (1.9088)
+2022-11-18 15:33:06,154:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9737 (1.9737)
+2022-11-18 15:33:06,239:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7825 (1.8751)
+2022-11-18 15:33:06,324:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9552 (1.9019)
+2022-11-18 15:33:06,411:INFO: Dataset: zara2               Batch: 4/5	Loss 2.0141 (1.9293)
+2022-11-18 15:33:06,493:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9509 (1.9337)
+2022-11-18 15:33:06,554:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_459.pth.tar
+2022-11-18 15:33:06,555:INFO: 
+===> EPOCH: 460 (P2)
+2022-11-18 15:33:06,555:INFO: - Computing loss (training)
+2022-11-18 15:33:06,939:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9478 (1.9478)
+2022-11-18 15:33:07,117:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9227 (1.9347)
+2022-11-18 15:33:07,278:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9794 (1.9493)
+2022-11-18 15:33:07,400:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8706 (1.9369)
+2022-11-18 15:33:07,885:INFO: Dataset: univ                Batch:  1/15	Loss 1.8533 (1.8533)
+2022-11-18 15:33:08,064:INFO: Dataset: univ                Batch:  2/15	Loss 1.9035 (1.8775)
+2022-11-18 15:33:08,259:INFO: Dataset: univ                Batch:  3/15	Loss 1.8685 (1.8743)
+2022-11-18 15:33:08,434:INFO: Dataset: univ                Batch:  4/15	Loss 1.8289 (1.8627)
+2022-11-18 15:33:08,605:INFO: Dataset: univ                Batch:  5/15	Loss 1.8766 (1.8654)
+2022-11-18 15:33:08,781:INFO: Dataset: univ                Batch:  6/15	Loss 1.8808 (1.8679)
+2022-11-18 15:33:08,962:INFO: Dataset: univ                Batch:  7/15	Loss 1.8484 (1.8653)
+2022-11-18 15:33:09,146:INFO: Dataset: univ                Batch:  8/15	Loss 1.8516 (1.8636)
+2022-11-18 15:33:09,319:INFO: Dataset: univ                Batch:  9/15	Loss 1.9077 (1.8684)
+2022-11-18 15:33:09,500:INFO: Dataset: univ                Batch: 10/15	Loss 1.8456 (1.8662)
+2022-11-18 15:33:09,691:INFO: Dataset: univ                Batch: 11/15	Loss 1.8554 (1.8653)
+2022-11-18 15:33:09,865:INFO: Dataset: univ                Batch: 12/15	Loss 1.8399 (1.8631)
+2022-11-18 15:33:10,052:INFO: Dataset: univ                Batch: 13/15	Loss 1.8650 (1.8633)
+2022-11-18 15:33:10,225:INFO: Dataset: univ                Batch: 14/15	Loss 1.8951 (1.8657)
+2022-11-18 15:33:10,317:INFO: Dataset: univ                Batch: 15/15	Loss 1.9222 (1.8663)
+2022-11-18 15:33:10,743:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9809 (1.9809)
+2022-11-18 15:33:10,918:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8371 (1.9111)
+2022-11-18 15:33:11,093:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8264 (1.8840)
+2022-11-18 15:33:11,287:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9605 (1.9022)
+2022-11-18 15:33:11,479:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9470 (1.9107)
+2022-11-18 15:33:11,660:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9759 (1.9221)
+2022-11-18 15:33:11,825:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8222 (1.9090)
+2022-11-18 15:33:11,984:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7079 (1.8867)
+2022-11-18 15:33:12,552:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9668 (1.9668)
+2022-11-18 15:33:12,847:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7573 (1.8511)
+2022-11-18 15:33:13,072:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8304 (1.8443)
+2022-11-18 15:33:13,285:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7312 (1.8169)
+2022-11-18 15:33:13,518:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0033 (1.8577)
+2022-11-18 15:33:13,731:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8296 (1.8529)
+2022-11-18 15:33:13,921:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7544 (1.8395)
+2022-11-18 15:33:14,130:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8755 (1.8440)
+2022-11-18 15:33:14,318:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9245 (1.8522)
+2022-11-18 15:33:14,493:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7901 (1.8469)
+2022-11-18 15:33:14,685:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9229 (1.8537)
+2022-11-18 15:33:14,895:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8451 (1.8530)
+2022-11-18 15:33:15,099:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7710 (1.8470)
+2022-11-18 15:33:15,332:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7721 (1.8419)
+2022-11-18 15:33:15,549:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7976 (1.8389)
+2022-11-18 15:33:15,795:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8643 (1.8405)
+2022-11-18 15:33:15,979:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8923 (1.8436)
+2022-11-18 15:33:16,155:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8319 (1.8430)
+2022-11-18 15:33:16,209:INFO: - Computing loss (validation)
+2022-11-18 15:33:16,575:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9504 (1.9504)
+2022-11-18 15:33:16,619:INFO: Dataset: hotel               Batch: 2/2	Loss 2.4328 (1.9833)
+2022-11-18 15:33:16,936:INFO: Dataset: univ                Batch: 1/3	Loss 1.8253 (1.8253)
+2022-11-18 15:33:17,017:INFO: Dataset: univ                Batch: 2/3	Loss 1.8774 (1.8543)
+2022-11-18 15:33:17,092:INFO: Dataset: univ                Batch: 3/3	Loss 1.9612 (1.8850)
+2022-11-18 15:33:17,418:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7860 (1.7860)
+2022-11-18 15:33:17,468:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9301 (1.8199)
+2022-11-18 15:33:17,854:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8329 (1.8329)
+2022-11-18 15:33:17,942:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8990 (1.8660)
+2022-11-18 15:33:18,028:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8623 (1.8647)
+2022-11-18 15:33:18,110:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7923 (1.8456)
+2022-11-18 15:33:18,187:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9143 (1.8604)
+2022-11-18 15:33:18,242:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_460.pth.tar
+2022-11-18 15:33:18,242:INFO: 
+===> EPOCH: 461 (P2)
+2022-11-18 15:33:18,243:INFO: - Computing loss (training)
+2022-11-18 15:33:18,600:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8535 (1.8535)
+2022-11-18 15:33:18,772:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8922 (1.8723)
+2022-11-18 15:33:18,945:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8780 (1.8743)
+2022-11-18 15:33:19,074:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9743 (1.8913)
+2022-11-18 15:33:19,550:INFO: Dataset: univ                Batch:  1/15	Loss 1.8505 (1.8505)
+2022-11-18 15:33:19,746:INFO: Dataset: univ                Batch:  2/15	Loss 1.8225 (1.8367)
+2022-11-18 15:33:19,959:INFO: Dataset: univ                Batch:  3/15	Loss 1.8593 (1.8444)
+2022-11-18 15:33:20,137:INFO: Dataset: univ                Batch:  4/15	Loss 1.8685 (1.8504)
+2022-11-18 15:33:20,316:INFO: Dataset: univ                Batch:  5/15	Loss 1.7987 (1.8406)
+2022-11-18 15:33:20,513:INFO: Dataset: univ                Batch:  6/15	Loss 1.8511 (1.8423)
+2022-11-18 15:33:20,716:INFO: Dataset: univ                Batch:  7/15	Loss 1.8191 (1.8390)
+2022-11-18 15:33:20,904:INFO: Dataset: univ                Batch:  8/15	Loss 1.8505 (1.8402)
+2022-11-18 15:33:21,092:INFO: Dataset: univ                Batch:  9/15	Loss 1.8637 (1.8431)
+2022-11-18 15:33:21,280:INFO: Dataset: univ                Batch: 10/15	Loss 1.8422 (1.8431)
+2022-11-18 15:33:21,457:INFO: Dataset: univ                Batch: 11/15	Loss 1.8827 (1.8467)
+2022-11-18 15:33:21,628:INFO: Dataset: univ                Batch: 12/15	Loss 1.8635 (1.8481)
+2022-11-18 15:33:21,825:INFO: Dataset: univ                Batch: 13/15	Loss 1.8811 (1.8504)
+2022-11-18 15:33:22,007:INFO: Dataset: univ                Batch: 14/15	Loss 1.8743 (1.8521)
+2022-11-18 15:33:22,112:INFO: Dataset: univ                Batch: 15/15	Loss 1.8818 (1.8525)
+2022-11-18 15:33:22,567:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8176 (1.8176)
+2022-11-18 15:33:22,748:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7635 (1.7903)
+2022-11-18 15:33:22,949:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9580 (1.8492)
+2022-11-18 15:33:23,139:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9933 (1.8853)
+2022-11-18 15:33:23,327:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9706 (1.9021)
+2022-11-18 15:33:23,500:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8236 (1.8889)
+2022-11-18 15:33:23,685:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8918 (1.8893)
+2022-11-18 15:33:23,863:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8365 (1.8840)
+2022-11-18 15:33:24,382:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8363 (1.8363)
+2022-11-18 15:33:24,637:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8630 (1.8500)
+2022-11-18 15:33:24,876:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8974 (1.8654)
+2022-11-18 15:33:25,100:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9614 (1.8879)
+2022-11-18 15:33:25,316:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8890 (1.8881)
+2022-11-18 15:33:25,511:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8666 (1.8846)
+2022-11-18 15:33:25,719:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8423 (1.8781)
+2022-11-18 15:33:25,896:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8311 (1.8723)
+2022-11-18 15:33:26,080:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9105 (1.8764)
+2022-11-18 15:33:26,255:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9437 (1.8831)
+2022-11-18 15:33:26,418:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8331 (1.8785)
+2022-11-18 15:33:26,599:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8666 (1.8774)
+2022-11-18 15:33:26,770:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9160 (1.8803)
+2022-11-18 15:33:26,948:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8106 (1.8757)
+2022-11-18 15:33:27,158:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8393 (1.8733)
+2022-11-18 15:33:27,428:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9357 (1.8771)
+2022-11-18 15:33:27,658:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9143 (1.8795)
+2022-11-18 15:33:27,834:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9429 (1.8824)
+2022-11-18 15:33:27,908:INFO: - Computing loss (validation)
+2022-11-18 15:33:28,185:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7730 (1.7730)
+2022-11-18 15:33:28,221:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2643 (1.8099)
+2022-11-18 15:33:28,545:INFO: Dataset: univ                Batch: 1/3	Loss 1.8587 (1.8587)
+2022-11-18 15:33:28,627:INFO: Dataset: univ                Batch: 2/3	Loss 1.8681 (1.8640)
+2022-11-18 15:33:28,708:INFO: Dataset: univ                Batch: 3/3	Loss 1.8725 (1.8664)
+2022-11-18 15:33:29,039:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8674 (1.8674)
+2022-11-18 15:33:29,085:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7202 (1.8314)
+2022-11-18 15:33:29,423:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9125 (1.9125)
+2022-11-18 15:33:29,517:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8750 (1.8929)
+2022-11-18 15:33:29,603:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8971 (1.8944)
+2022-11-18 15:33:29,687:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8165 (1.8743)
+2022-11-18 15:33:29,769:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8792 (1.8753)
+2022-11-18 15:33:29,822:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_461.pth.tar
+2022-11-18 15:33:29,823:INFO: 
+===> EPOCH: 462 (P2)
+2022-11-18 15:33:29,823:INFO: - Computing loss (training)
+2022-11-18 15:33:30,172:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9067 (1.9067)
+2022-11-18 15:33:30,345:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9750 (1.9414)
+2022-11-18 15:33:30,511:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8560 (1.9130)
+2022-11-18 15:33:30,641:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7748 (1.8898)
+2022-11-18 15:33:31,058:INFO: Dataset: univ                Batch:  1/15	Loss 1.8563 (1.8563)
+2022-11-18 15:33:31,253:INFO: Dataset: univ                Batch:  2/15	Loss 1.8621 (1.8594)
+2022-11-18 15:33:31,459:INFO: Dataset: univ                Batch:  3/15	Loss 1.8389 (1.8525)
+2022-11-18 15:33:31,658:INFO: Dataset: univ                Batch:  4/15	Loss 1.8855 (1.8601)
+2022-11-18 15:33:31,830:INFO: Dataset: univ                Batch:  5/15	Loss 1.8394 (1.8560)
+2022-11-18 15:33:32,004:INFO: Dataset: univ                Batch:  6/15	Loss 1.8797 (1.8596)
+2022-11-18 15:33:32,169:INFO: Dataset: univ                Batch:  7/15	Loss 1.8417 (1.8570)
+2022-11-18 15:33:32,334:INFO: Dataset: univ                Batch:  8/15	Loss 1.8585 (1.8572)
+2022-11-18 15:33:32,502:INFO: Dataset: univ                Batch:  9/15	Loss 1.8014 (1.8517)
+2022-11-18 15:33:32,704:INFO: Dataset: univ                Batch: 10/15	Loss 1.8588 (1.8524)
+2022-11-18 15:33:32,897:INFO: Dataset: univ                Batch: 11/15	Loss 1.8951 (1.8562)
+2022-11-18 15:33:33,077:INFO: Dataset: univ                Batch: 12/15	Loss 1.8363 (1.8545)
+2022-11-18 15:33:33,264:INFO: Dataset: univ                Batch: 13/15	Loss 1.8642 (1.8553)
+2022-11-18 15:33:33,439:INFO: Dataset: univ                Batch: 14/15	Loss 1.8449 (1.8545)
+2022-11-18 15:33:33,533:INFO: Dataset: univ                Batch: 15/15	Loss 1.9412 (1.8555)
+2022-11-18 15:33:34,026:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9018 (1.9018)
+2022-11-18 15:33:34,209:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9342 (1.9182)
+2022-11-18 15:33:34,402:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8940 (1.9101)
+2022-11-18 15:33:34,577:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8421 (1.8938)
+2022-11-18 15:33:34,753:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8903 (1.8930)
+2022-11-18 15:33:34,913:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9318 (1.8988)
+2022-11-18 15:33:35,070:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8633 (1.8937)
+2022-11-18 15:33:35,213:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8668 (1.8907)
+2022-11-18 15:33:35,633:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9447 (1.9447)
+2022-11-18 15:33:35,798:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8096 (1.8796)
+2022-11-18 15:33:35,955:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8750 (1.8780)
+2022-11-18 15:33:36,113:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8432 (1.8700)
+2022-11-18 15:33:36,283:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8265 (1.8615)
+2022-11-18 15:33:36,449:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7991 (1.8523)
+2022-11-18 15:33:36,612:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9987 (1.8728)
+2022-11-18 15:33:36,769:INFO: Dataset: zara2               Batch:  8/18	Loss 2.0182 (1.8897)
+2022-11-18 15:33:36,944:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9577 (1.8968)
+2022-11-18 15:33:37,117:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8964 (1.8967)
+2022-11-18 15:33:37,276:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9010 (1.8971)
+2022-11-18 15:33:37,443:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7373 (1.8836)
+2022-11-18 15:33:37,612:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7310 (1.8724)
+2022-11-18 15:33:37,779:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8821 (1.8732)
+2022-11-18 15:33:37,942:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8579 (1.8720)
+2022-11-18 15:33:38,147:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8527 (1.8708)
+2022-11-18 15:33:38,330:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8499 (1.8696)
+2022-11-18 15:33:38,513:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9595 (1.8737)
+2022-11-18 15:33:38,581:INFO: - Computing loss (validation)
+2022-11-18 15:33:38,942:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9448 (1.9448)
+2022-11-18 15:33:38,982:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9253 (1.9429)
+2022-11-18 15:33:39,307:INFO: Dataset: univ                Batch: 1/3	Loss 1.8785 (1.8785)
+2022-11-18 15:33:39,388:INFO: Dataset: univ                Batch: 2/3	Loss 1.8889 (1.8837)
+2022-11-18 15:33:39,466:INFO: Dataset: univ                Batch: 3/3	Loss 1.8768 (1.8818)
+2022-11-18 15:33:39,813:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7809 (1.7809)
+2022-11-18 15:33:39,862:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8743 (1.8016)
+2022-11-18 15:33:40,172:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8523 (1.8523)
+2022-11-18 15:33:40,247:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8095 (1.8308)
+2022-11-18 15:33:40,325:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7654 (1.8089)
+2022-11-18 15:33:40,401:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8416 (1.8171)
+2022-11-18 15:33:40,478:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8166 (1.8170)
+2022-11-18 15:33:40,533:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_462.pth.tar
+2022-11-18 15:33:40,533:INFO: 
+===> EPOCH: 463 (P2)
+2022-11-18 15:33:40,534:INFO: - Computing loss (training)
+2022-11-18 15:33:40,907:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9417 (1.9417)
+2022-11-18 15:33:41,074:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9514 (1.9466)
+2022-11-18 15:33:41,229:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9642 (1.9522)
+2022-11-18 15:33:41,353:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0310 (1.9655)
+2022-11-18 15:33:41,847:INFO: Dataset: univ                Batch:  1/15	Loss 1.9070 (1.9070)
+2022-11-18 15:33:42,033:INFO: Dataset: univ                Batch:  2/15	Loss 1.8709 (1.8891)
+2022-11-18 15:33:42,226:INFO: Dataset: univ                Batch:  3/15	Loss 1.8162 (1.8653)
+2022-11-18 15:33:42,400:INFO: Dataset: univ                Batch:  4/15	Loss 1.9058 (1.8747)
+2022-11-18 15:33:42,566:INFO: Dataset: univ                Batch:  5/15	Loss 1.9030 (1.8802)
+2022-11-18 15:33:42,725:INFO: Dataset: univ                Batch:  6/15	Loss 1.8104 (1.8675)
+2022-11-18 15:33:42,886:INFO: Dataset: univ                Batch:  7/15	Loss 1.8229 (1.8615)
+2022-11-18 15:33:43,046:INFO: Dataset: univ                Batch:  8/15	Loss 1.8454 (1.8594)
+2022-11-18 15:33:43,226:INFO: Dataset: univ                Batch:  9/15	Loss 1.8052 (1.8534)
+2022-11-18 15:33:43,421:INFO: Dataset: univ                Batch: 10/15	Loss 1.8370 (1.8518)
+2022-11-18 15:33:43,594:INFO: Dataset: univ                Batch: 11/15	Loss 1.8671 (1.8531)
+2022-11-18 15:33:43,757:INFO: Dataset: univ                Batch: 12/15	Loss 1.8754 (1.8550)
+2022-11-18 15:33:43,922:INFO: Dataset: univ                Batch: 13/15	Loss 1.8625 (1.8556)
+2022-11-18 15:33:44,093:INFO: Dataset: univ                Batch: 14/15	Loss 1.8680 (1.8565)
+2022-11-18 15:33:44,180:INFO: Dataset: univ                Batch: 15/15	Loss 1.9551 (1.8580)
+2022-11-18 15:33:44,597:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0001 (2.0001)
+2022-11-18 15:33:44,793:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9090 (1.9548)
+2022-11-18 15:33:44,963:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8727 (1.9265)
+2022-11-18 15:33:45,132:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7603 (1.8835)
+2022-11-18 15:33:45,293:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8506 (1.8777)
+2022-11-18 15:33:45,472:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8831 (1.8785)
+2022-11-18 15:33:45,645:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8540 (1.8752)
+2022-11-18 15:33:45,794:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9167 (1.8799)
+2022-11-18 15:33:46,202:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8756 (1.8756)
+2022-11-18 15:33:46,376:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7612 (1.8167)
+2022-11-18 15:33:46,553:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8112 (1.8149)
+2022-11-18 15:33:46,726:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8320 (1.8188)
+2022-11-18 15:33:46,914:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9435 (1.8425)
+2022-11-18 15:33:47,108:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8419 (1.8424)
+2022-11-18 15:33:47,271:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7692 (1.8320)
+2022-11-18 15:33:47,430:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7668 (1.8237)
+2022-11-18 15:33:47,585:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9148 (1.8345)
+2022-11-18 15:33:47,740:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8950 (1.8410)
+2022-11-18 15:33:47,902:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8804 (1.8447)
+2022-11-18 15:33:48,057:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9888 (1.8563)
+2022-11-18 15:33:48,225:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8040 (1.8523)
+2022-11-18 15:33:48,388:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8837 (1.8546)
+2022-11-18 15:33:48,554:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8582 (1.8549)
+2022-11-18 15:33:48,744:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8673 (1.8556)
+2022-11-18 15:33:48,952:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8815 (1.8572)
+2022-11-18 15:33:49,137:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7854 (1.8536)
+2022-11-18 15:33:49,200:INFO: - Computing loss (validation)
+2022-11-18 15:33:49,490:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9170 (1.9170)
+2022-11-18 15:33:49,527:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9006 (1.9154)
+2022-11-18 15:33:49,869:INFO: Dataset: univ                Batch: 1/3	Loss 1.8428 (1.8428)
+2022-11-18 15:33:49,949:INFO: Dataset: univ                Batch: 2/3	Loss 1.7845 (1.8128)
+2022-11-18 15:33:50,025:INFO: Dataset: univ                Batch: 3/3	Loss 1.9019 (1.8411)
+2022-11-18 15:33:50,360:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8087 (1.8087)
+2022-11-18 15:33:50,410:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9457 (1.8404)
+2022-11-18 15:33:50,742:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8579 (1.8579)
+2022-11-18 15:33:50,823:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7879 (1.8218)
+2022-11-18 15:33:50,902:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8575 (1.8338)
+2022-11-18 15:33:50,984:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8709 (1.8433)
+2022-11-18 15:33:51,061:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9001 (1.8554)
+2022-11-18 15:33:51,114:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_463.pth.tar
+2022-11-18 15:33:51,114:INFO: 
+===> EPOCH: 464 (P2)
+2022-11-18 15:33:51,115:INFO: - Computing loss (training)
+2022-11-18 15:33:51,528:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0184 (2.0184)
+2022-11-18 15:33:51,703:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8471 (1.9342)
+2022-11-18 15:33:51,866:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9898 (1.9519)
+2022-11-18 15:33:51,992:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8538 (1.9346)
+2022-11-18 15:33:52,404:INFO: Dataset: univ                Batch:  1/15	Loss 1.8399 (1.8399)
+2022-11-18 15:33:52,592:INFO: Dataset: univ                Batch:  2/15	Loss 1.8497 (1.8444)
+2022-11-18 15:33:52,784:INFO: Dataset: univ                Batch:  3/15	Loss 1.8696 (1.8525)
+2022-11-18 15:33:52,995:INFO: Dataset: univ                Batch:  4/15	Loss 1.8845 (1.8603)
+2022-11-18 15:33:53,220:INFO: Dataset: univ                Batch:  5/15	Loss 1.8626 (1.8607)
+2022-11-18 15:33:53,396:INFO: Dataset: univ                Batch:  6/15	Loss 1.8942 (1.8663)
+2022-11-18 15:33:53,580:INFO: Dataset: univ                Batch:  7/15	Loss 1.8806 (1.8684)
+2022-11-18 15:33:53,766:INFO: Dataset: univ                Batch:  8/15	Loss 1.8350 (1.8643)
+2022-11-18 15:33:53,930:INFO: Dataset: univ                Batch:  9/15	Loss 1.8311 (1.8610)
+2022-11-18 15:33:54,102:INFO: Dataset: univ                Batch: 10/15	Loss 1.8699 (1.8619)
+2022-11-18 15:33:54,283:INFO: Dataset: univ                Batch: 11/15	Loss 1.8680 (1.8625)
+2022-11-18 15:33:54,454:INFO: Dataset: univ                Batch: 12/15	Loss 1.8855 (1.8644)
+2022-11-18 15:33:54,650:INFO: Dataset: univ                Batch: 13/15	Loss 1.8798 (1.8656)
+2022-11-18 15:33:54,820:INFO: Dataset: univ                Batch: 14/15	Loss 1.9059 (1.8684)
+2022-11-18 15:33:54,940:INFO: Dataset: univ                Batch: 15/15	Loss 1.8063 (1.8675)
+2022-11-18 15:33:55,360:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8567 (1.8567)
+2022-11-18 15:33:55,515:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8641 (1.8606)
+2022-11-18 15:33:55,668:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7779 (1.8341)
+2022-11-18 15:33:55,823:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9396 (1.8607)
+2022-11-18 15:33:55,976:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8386 (1.8561)
+2022-11-18 15:33:56,129:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7980 (1.8468)
+2022-11-18 15:33:56,281:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8779 (1.8508)
+2022-11-18 15:33:56,421:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9903 (1.8669)
+2022-11-18 15:33:56,826:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9006 (1.9006)
+2022-11-18 15:33:56,979:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8383 (1.8705)
+2022-11-18 15:33:57,136:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8304 (1.8562)
+2022-11-18 15:33:57,286:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9600 (1.8821)
+2022-11-18 15:33:57,437:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9039 (1.8861)
+2022-11-18 15:33:57,596:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9235 (1.8924)
+2022-11-18 15:33:57,746:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8303 (1.8829)
+2022-11-18 15:33:57,895:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8138 (1.8745)
+2022-11-18 15:33:58,047:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8683 (1.8739)
+2022-11-18 15:33:58,202:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8577 (1.8723)
+2022-11-18 15:33:58,352:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8599 (1.8711)
+2022-11-18 15:33:58,500:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8497 (1.8693)
+2022-11-18 15:33:58,650:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9105 (1.8724)
+2022-11-18 15:33:58,805:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8302 (1.8696)
+2022-11-18 15:33:58,956:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8280 (1.8668)
+2022-11-18 15:33:59,105:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8856 (1.8680)
+2022-11-18 15:33:59,255:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8287 (1.8654)
+2022-11-18 15:33:59,391:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8731 (1.8658)
+2022-11-18 15:33:59,436:INFO: - Computing loss (validation)
+2022-11-18 15:33:59,697:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9169 (1.9169)
+2022-11-18 15:33:59,732:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9334 (1.9182)
+2022-11-18 15:34:00,043:INFO: Dataset: univ                Batch: 1/3	Loss 1.8942 (1.8942)
+2022-11-18 15:34:00,123:INFO: Dataset: univ                Batch: 2/3	Loss 1.8728 (1.8835)
+2022-11-18 15:34:00,195:INFO: Dataset: univ                Batch: 3/3	Loss 1.8202 (1.8635)
+2022-11-18 15:34:00,506:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8229 (1.8229)
+2022-11-18 15:34:00,551:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0232 (1.8660)
+2022-11-18 15:34:00,879:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8136 (1.8136)
+2022-11-18 15:34:00,956:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8674 (1.8400)
+2022-11-18 15:34:01,033:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8327 (1.8375)
+2022-11-18 15:34:01,108:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9455 (1.8653)
+2022-11-18 15:34:01,182:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8444 (1.8614)
+2022-11-18 15:34:01,237:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_464.pth.tar
+2022-11-18 15:34:01,237:INFO: 
+===> EPOCH: 465 (P2)
+2022-11-18 15:34:01,238:INFO: - Computing loss (training)
+2022-11-18 15:34:01,581:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8698 (1.8698)
+2022-11-18 15:34:01,736:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9193 (1.8943)
+2022-11-18 15:34:01,891:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8210 (1.8697)
+2022-11-18 15:34:02,007:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6387 (1.8304)
+2022-11-18 15:34:02,406:INFO: Dataset: univ                Batch:  1/15	Loss 1.8705 (1.8705)
+2022-11-18 15:34:02,565:INFO: Dataset: univ                Batch:  2/15	Loss 1.8247 (1.8482)
+2022-11-18 15:34:02,742:INFO: Dataset: univ                Batch:  3/15	Loss 1.8551 (1.8503)
+2022-11-18 15:34:02,900:INFO: Dataset: univ                Batch:  4/15	Loss 1.8726 (1.8559)
+2022-11-18 15:34:03,058:INFO: Dataset: univ                Batch:  5/15	Loss 1.8764 (1.8602)
+2022-11-18 15:34:03,223:INFO: Dataset: univ                Batch:  6/15	Loss 1.8808 (1.8635)
+2022-11-18 15:34:03,384:INFO: Dataset: univ                Batch:  7/15	Loss 1.8427 (1.8604)
+2022-11-18 15:34:03,543:INFO: Dataset: univ                Batch:  8/15	Loss 1.8649 (1.8610)
+2022-11-18 15:34:03,703:INFO: Dataset: univ                Batch:  9/15	Loss 1.9013 (1.8649)
+2022-11-18 15:34:03,863:INFO: Dataset: univ                Batch: 10/15	Loss 1.8812 (1.8666)
+2022-11-18 15:34:04,031:INFO: Dataset: univ                Batch: 11/15	Loss 1.9055 (1.8699)
+2022-11-18 15:34:04,204:INFO: Dataset: univ                Batch: 12/15	Loss 1.8521 (1.8684)
+2022-11-18 15:34:04,389:INFO: Dataset: univ                Batch: 13/15	Loss 1.8443 (1.8666)
+2022-11-18 15:34:04,626:INFO: Dataset: univ                Batch: 14/15	Loss 1.8664 (1.8666)
+2022-11-18 15:34:04,719:INFO: Dataset: univ                Batch: 15/15	Loss 1.8969 (1.8670)
+2022-11-18 15:34:05,170:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8591 (1.8591)
+2022-11-18 15:34:05,328:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9830 (1.9226)
+2022-11-18 15:34:05,490:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7519 (1.8678)
+2022-11-18 15:34:05,644:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8482 (1.8629)
+2022-11-18 15:34:05,809:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9406 (1.8792)
+2022-11-18 15:34:05,975:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8851 (1.8802)
+2022-11-18 15:34:06,140:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8068 (1.8698)
+2022-11-18 15:34:06,284:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8980 (1.8732)
+2022-11-18 15:34:06,694:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8284 (1.8284)
+2022-11-18 15:34:06,852:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8692 (1.8488)
+2022-11-18 15:34:07,018:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9185 (1.8744)
+2022-11-18 15:34:07,180:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8587 (1.8704)
+2022-11-18 15:34:07,339:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7574 (1.8488)
+2022-11-18 15:34:07,493:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7887 (1.8394)
+2022-11-18 15:34:07,647:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7289 (1.8235)
+2022-11-18 15:34:07,800:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9126 (1.8349)
+2022-11-18 15:34:07,962:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8231 (1.8336)
+2022-11-18 15:34:08,116:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8829 (1.8385)
+2022-11-18 15:34:08,273:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8848 (1.8432)
+2022-11-18 15:34:08,432:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7452 (1.8349)
+2022-11-18 15:34:08,588:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8850 (1.8389)
+2022-11-18 15:34:08,741:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8566 (1.8403)
+2022-11-18 15:34:08,895:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8064 (1.8380)
+2022-11-18 15:34:09,048:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8467 (1.8385)
+2022-11-18 15:34:09,201:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9656 (1.8462)
+2022-11-18 15:34:09,342:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8855 (1.8480)
+2022-11-18 15:34:09,390:INFO: - Computing loss (validation)
+2022-11-18 15:34:09,664:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8449 (1.8449)
+2022-11-18 15:34:09,700:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6534 (1.8318)
+2022-11-18 15:34:10,032:INFO: Dataset: univ                Batch: 1/3	Loss 1.9003 (1.9003)
+2022-11-18 15:34:10,118:INFO: Dataset: univ                Batch: 2/3	Loss 1.8225 (1.8680)
+2022-11-18 15:34:10,201:INFO: Dataset: univ                Batch: 3/3	Loss 1.8778 (1.8714)
+2022-11-18 15:34:10,496:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8938 (1.8938)
+2022-11-18 15:34:10,542:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8714 (1.8872)
+2022-11-18 15:34:10,864:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8266 (1.8266)
+2022-11-18 15:34:10,940:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7649 (1.7980)
+2022-11-18 15:34:11,016:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9324 (1.8443)
+2022-11-18 15:34:11,097:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8726 (1.8511)
+2022-11-18 15:34:11,174:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8285 (1.8465)
+2022-11-18 15:34:11,228:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_465.pth.tar
+2022-11-18 15:34:11,229:INFO: 
+===> EPOCH: 466 (P2)
+2022-11-18 15:34:11,229:INFO: - Computing loss (training)
+2022-11-18 15:34:11,581:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8751 (1.8751)
+2022-11-18 15:34:11,737:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7465 (1.8131)
+2022-11-18 15:34:11,895:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8699 (1.8326)
+2022-11-18 15:34:12,014:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9237 (1.8473)
+2022-11-18 15:34:12,449:INFO: Dataset: univ                Batch:  1/15	Loss 1.9063 (1.9063)
+2022-11-18 15:34:12,615:INFO: Dataset: univ                Batch:  2/15	Loss 1.8566 (1.8821)
+2022-11-18 15:34:12,775:INFO: Dataset: univ                Batch:  3/15	Loss 1.8736 (1.8792)
+2022-11-18 15:34:12,933:INFO: Dataset: univ                Batch:  4/15	Loss 1.8750 (1.8782)
+2022-11-18 15:34:13,092:INFO: Dataset: univ                Batch:  5/15	Loss 1.8885 (1.8802)
+2022-11-18 15:34:13,256:INFO: Dataset: univ                Batch:  6/15	Loss 1.8382 (1.8729)
+2022-11-18 15:34:13,415:INFO: Dataset: univ                Batch:  7/15	Loss 1.8205 (1.8654)
+2022-11-18 15:34:13,571:INFO: Dataset: univ                Batch:  8/15	Loss 1.8389 (1.8624)
+2022-11-18 15:34:13,736:INFO: Dataset: univ                Batch:  9/15	Loss 1.8722 (1.8636)
+2022-11-18 15:34:13,904:INFO: Dataset: univ                Batch: 10/15	Loss 1.8558 (1.8627)
+2022-11-18 15:34:14,069:INFO: Dataset: univ                Batch: 11/15	Loss 1.8369 (1.8605)
+2022-11-18 15:34:14,232:INFO: Dataset: univ                Batch: 12/15	Loss 1.8473 (1.8594)
+2022-11-18 15:34:14,391:INFO: Dataset: univ                Batch: 13/15	Loss 1.8535 (1.8589)
+2022-11-18 15:34:14,550:INFO: Dataset: univ                Batch: 14/15	Loss 1.8785 (1.8602)
+2022-11-18 15:34:14,635:INFO: Dataset: univ                Batch: 15/15	Loss 1.8905 (1.8606)
+2022-11-18 15:34:15,035:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8787 (1.8787)
+2022-11-18 15:34:15,193:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9083 (1.8936)
+2022-11-18 15:34:15,349:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9375 (1.9085)
+2022-11-18 15:34:15,512:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7886 (1.8769)
+2022-11-18 15:34:15,669:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8644 (1.8744)
+2022-11-18 15:34:15,832:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8023 (1.8618)
+2022-11-18 15:34:15,992:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9788 (1.8759)
+2022-11-18 15:34:16,133:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9051 (1.8789)
+2022-11-18 15:34:16,535:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8763 (1.8763)
+2022-11-18 15:34:16,691:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9169 (1.8963)
+2022-11-18 15:34:16,849:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7962 (1.8605)
+2022-11-18 15:34:17,010:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9600 (1.8859)
+2022-11-18 15:34:17,165:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8703 (1.8826)
+2022-11-18 15:34:17,324:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7922 (1.8688)
+2022-11-18 15:34:17,478:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8352 (1.8640)
+2022-11-18 15:34:17,631:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8444 (1.8616)
+2022-11-18 15:34:17,787:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8187 (1.8564)
+2022-11-18 15:34:17,941:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8433 (1.8550)
+2022-11-18 15:34:18,095:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9069 (1.8589)
+2022-11-18 15:34:18,253:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9664 (1.8675)
+2022-11-18 15:34:18,408:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7514 (1.8589)
+2022-11-18 15:34:18,563:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8799 (1.8604)
+2022-11-18 15:34:18,720:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9091 (1.8636)
+2022-11-18 15:34:18,878:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7907 (1.8597)
+2022-11-18 15:34:19,035:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8253 (1.8575)
+2022-11-18 15:34:19,178:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8428 (1.8567)
+2022-11-18 15:34:19,223:INFO: - Computing loss (validation)
+2022-11-18 15:34:19,495:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9121 (1.9121)
+2022-11-18 15:34:19,530:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7645 (1.9015)
+2022-11-18 15:34:19,853:INFO: Dataset: univ                Batch: 1/3	Loss 1.9299 (1.9299)
+2022-11-18 15:34:19,938:INFO: Dataset: univ                Batch: 2/3	Loss 1.8522 (1.8885)
+2022-11-18 15:34:20,017:INFO: Dataset: univ                Batch: 3/3	Loss 1.9302 (1.9017)
+2022-11-18 15:34:20,327:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9140 (1.9140)
+2022-11-18 15:34:20,378:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7441 (1.8774)
+2022-11-18 15:34:20,701:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7775 (1.7775)
+2022-11-18 15:34:20,776:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8560 (1.8165)
+2022-11-18 15:34:20,853:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8679 (1.8338)
+2022-11-18 15:34:20,928:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9115 (1.8532)
+2022-11-18 15:34:21,003:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8553 (1.8537)
+2022-11-18 15:34:21,064:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_466.pth.tar
+2022-11-18 15:34:21,064:INFO: 
+===> EPOCH: 467 (P2)
+2022-11-18 15:34:21,065:INFO: - Computing loss (training)
+2022-11-18 15:34:21,418:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8584 (1.8584)
+2022-11-18 15:34:21,576:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9973 (1.9290)
+2022-11-18 15:34:21,730:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7754 (1.8805)
+2022-11-18 15:34:21,857:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9003 (1.8838)
+2022-11-18 15:34:22,267:INFO: Dataset: univ                Batch:  1/15	Loss 1.8818 (1.8818)
+2022-11-18 15:34:22,436:INFO: Dataset: univ                Batch:  2/15	Loss 1.8640 (1.8728)
+2022-11-18 15:34:22,600:INFO: Dataset: univ                Batch:  3/15	Loss 1.8866 (1.8774)
+2022-11-18 15:34:22,758:INFO: Dataset: univ                Batch:  4/15	Loss 1.8425 (1.8687)
+2022-11-18 15:34:22,918:INFO: Dataset: univ                Batch:  5/15	Loss 1.8653 (1.8680)
+2022-11-18 15:34:23,080:INFO: Dataset: univ                Batch:  6/15	Loss 1.8632 (1.8672)
+2022-11-18 15:34:23,240:INFO: Dataset: univ                Batch:  7/15	Loss 1.8363 (1.8628)
+2022-11-18 15:34:23,485:INFO: Dataset: univ                Batch:  8/15	Loss 1.8801 (1.8652)
+2022-11-18 15:34:23,643:INFO: Dataset: univ                Batch:  9/15	Loss 1.8900 (1.8679)
+2022-11-18 15:34:23,803:INFO: Dataset: univ                Batch: 10/15	Loss 1.8619 (1.8674)
+2022-11-18 15:34:23,961:INFO: Dataset: univ                Batch: 11/15	Loss 1.8969 (1.8700)
+2022-11-18 15:34:24,121:INFO: Dataset: univ                Batch: 12/15	Loss 1.8940 (1.8719)
+2022-11-18 15:34:24,279:INFO: Dataset: univ                Batch: 13/15	Loss 1.8380 (1.8691)
+2022-11-18 15:34:24,439:INFO: Dataset: univ                Batch: 14/15	Loss 1.8883 (1.8704)
+2022-11-18 15:34:24,524:INFO: Dataset: univ                Batch: 15/15	Loss 1.9009 (1.8709)
+2022-11-18 15:34:24,951:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9237 (1.9237)
+2022-11-18 15:34:25,103:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9529 (1.9386)
+2022-11-18 15:34:25,256:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7628 (1.8867)
+2022-11-18 15:34:25,407:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8598 (1.8795)
+2022-11-18 15:34:25,559:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9869 (1.9018)
+2022-11-18 15:34:25,711:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9671 (1.9122)
+2022-11-18 15:34:25,865:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9670 (1.9196)
+2022-11-18 15:34:26,005:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8240 (1.9088)
+2022-11-18 15:34:26,404:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8041 (1.8041)
+2022-11-18 15:34:26,559:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7672 (1.7879)
+2022-11-18 15:34:26,719:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8507 (1.8087)
+2022-11-18 15:34:26,878:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7883 (1.8035)
+2022-11-18 15:34:27,036:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8012 (1.8031)
+2022-11-18 15:34:27,199:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8015 (1.8028)
+2022-11-18 15:34:27,354:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8709 (1.8130)
+2022-11-18 15:34:27,507:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9681 (1.8330)
+2022-11-18 15:34:27,662:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8359 (1.8333)
+2022-11-18 15:34:27,823:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9358 (1.8437)
+2022-11-18 15:34:27,986:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8206 (1.8415)
+2022-11-18 15:34:28,152:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7457 (1.8334)
+2022-11-18 15:34:28,311:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8854 (1.8376)
+2022-11-18 15:34:28,468:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8459 (1.8383)
+2022-11-18 15:34:28,632:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9230 (1.8439)
+2022-11-18 15:34:28,803:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8877 (1.8467)
+2022-11-18 15:34:28,987:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8302 (1.8457)
+2022-11-18 15:34:29,146:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8022 (1.8436)
+2022-11-18 15:34:29,201:INFO: - Computing loss (validation)
+2022-11-18 15:34:29,479:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9260 (1.9260)
+2022-11-18 15:34:29,516:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7180 (1.9075)
+2022-11-18 15:34:29,833:INFO: Dataset: univ                Batch: 1/3	Loss 1.8791 (1.8791)
+2022-11-18 15:34:29,918:INFO: Dataset: univ                Batch: 2/3	Loss 1.8550 (1.8672)
+2022-11-18 15:34:29,994:INFO: Dataset: univ                Batch: 3/3	Loss 1.8429 (1.8595)
+2022-11-18 15:34:30,307:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9921 (1.9921)
+2022-11-18 15:34:30,354:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9410 (1.9782)
+2022-11-18 15:34:30,686:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7338 (1.7338)
+2022-11-18 15:34:30,767:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8079 (1.7722)
+2022-11-18 15:34:30,848:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8890 (1.8123)
+2022-11-18 15:34:30,928:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8574 (1.8235)
+2022-11-18 15:34:31,006:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8569 (1.8302)
+2022-11-18 15:34:31,058:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_467.pth.tar
+2022-11-18 15:34:31,058:INFO: 
+===> EPOCH: 468 (P2)
+2022-11-18 15:34:31,059:INFO: - Computing loss (training)
+2022-11-18 15:34:31,433:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8827 (1.8827)
+2022-11-18 15:34:31,593:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8605 (1.8714)
+2022-11-18 15:34:31,764:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8961 (1.8796)
+2022-11-18 15:34:31,900:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0016 (1.9002)
+2022-11-18 15:34:32,315:INFO: Dataset: univ                Batch:  1/15	Loss 1.8449 (1.8449)
+2022-11-18 15:34:32,475:INFO: Dataset: univ                Batch:  2/15	Loss 1.8725 (1.8578)
+2022-11-18 15:34:32,638:INFO: Dataset: univ                Batch:  3/15	Loss 1.8800 (1.8642)
+2022-11-18 15:34:32,809:INFO: Dataset: univ                Batch:  4/15	Loss 1.8452 (1.8593)
+2022-11-18 15:34:32,971:INFO: Dataset: univ                Batch:  5/15	Loss 1.8748 (1.8624)
+2022-11-18 15:34:33,130:INFO: Dataset: univ                Batch:  6/15	Loss 1.8255 (1.8562)
+2022-11-18 15:34:33,287:INFO: Dataset: univ                Batch:  7/15	Loss 1.8747 (1.8591)
+2022-11-18 15:34:33,444:INFO: Dataset: univ                Batch:  8/15	Loss 1.8720 (1.8607)
+2022-11-18 15:34:33,600:INFO: Dataset: univ                Batch:  9/15	Loss 1.8499 (1.8595)
+2022-11-18 15:34:33,756:INFO: Dataset: univ                Batch: 10/15	Loss 1.8869 (1.8622)
+2022-11-18 15:34:33,921:INFO: Dataset: univ                Batch: 11/15	Loss 1.9314 (1.8686)
+2022-11-18 15:34:34,091:INFO: Dataset: univ                Batch: 12/15	Loss 1.8629 (1.8681)
+2022-11-18 15:34:34,272:INFO: Dataset: univ                Batch: 13/15	Loss 1.8848 (1.8694)
+2022-11-18 15:34:34,438:INFO: Dataset: univ                Batch: 14/15	Loss 1.8416 (1.8674)
+2022-11-18 15:34:34,524:INFO: Dataset: univ                Batch: 15/15	Loss 1.8850 (1.8676)
+2022-11-18 15:34:34,949:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8410 (1.8410)
+2022-11-18 15:34:35,111:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7430 (1.7923)
+2022-11-18 15:34:35,276:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8860 (1.8236)
+2022-11-18 15:34:35,432:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7851 (1.8150)
+2022-11-18 15:34:35,582:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9085 (1.8324)
+2022-11-18 15:34:35,732:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8012 (1.8274)
+2022-11-18 15:34:35,882:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8701 (1.8332)
+2022-11-18 15:34:36,019:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9512 (1.8467)
+2022-11-18 15:34:36,412:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8514 (1.8514)
+2022-11-18 15:34:36,573:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8610 (1.8559)
+2022-11-18 15:34:36,737:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8495 (1.8539)
+2022-11-18 15:34:36,904:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8466 (1.8519)
+2022-11-18 15:34:37,073:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8324 (1.8478)
+2022-11-18 15:34:37,246:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7887 (1.8381)
+2022-11-18 15:34:37,413:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7714 (1.8277)
+2022-11-18 15:34:37,567:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8546 (1.8312)
+2022-11-18 15:34:37,722:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9361 (1.8427)
+2022-11-18 15:34:37,879:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9212 (1.8509)
+2022-11-18 15:34:38,034:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8729 (1.8532)
+2022-11-18 15:34:38,189:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9226 (1.8587)
+2022-11-18 15:34:38,345:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8996 (1.8618)
+2022-11-18 15:34:38,500:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9669 (1.8687)
+2022-11-18 15:34:38,656:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8705 (1.8688)
+2022-11-18 15:34:38,812:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7218 (1.8603)
+2022-11-18 15:34:38,969:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8724 (1.8610)
+2022-11-18 15:34:39,116:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8430 (1.8602)
+2022-11-18 15:34:39,171:INFO: - Computing loss (validation)
+2022-11-18 15:34:39,443:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9857 (1.9857)
+2022-11-18 15:34:39,481:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9093 (1.9805)
+2022-11-18 15:34:39,808:INFO: Dataset: univ                Batch: 1/3	Loss 1.9278 (1.9278)
+2022-11-18 15:34:39,887:INFO: Dataset: univ                Batch: 2/3	Loss 1.9348 (1.9312)
+2022-11-18 15:34:39,959:INFO: Dataset: univ                Batch: 3/3	Loss 1.8114 (1.8978)
+2022-11-18 15:34:40,285:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9606 (1.9606)
+2022-11-18 15:34:40,330:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8933 (1.9439)
+2022-11-18 15:34:40,647:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8546 (1.8546)
+2022-11-18 15:34:40,733:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8444 (1.8498)
+2022-11-18 15:34:40,820:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8022 (1.8339)
+2022-11-18 15:34:40,898:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8515 (1.8383)
+2022-11-18 15:34:40,976:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8230 (1.8354)
+2022-11-18 15:34:41,040:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_468.pth.tar
+2022-11-18 15:34:41,040:INFO: 
+===> EPOCH: 469 (P2)
+2022-11-18 15:34:41,041:INFO: - Computing loss (training)
+2022-11-18 15:34:41,396:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9686 (1.9686)
+2022-11-18 15:34:41,554:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8543 (1.9126)
+2022-11-18 15:34:41,715:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8747 (1.9002)
+2022-11-18 15:34:41,840:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8538 (1.8925)
+2022-11-18 15:34:42,266:INFO: Dataset: univ                Batch:  1/15	Loss 1.8579 (1.8579)
+2022-11-18 15:34:42,440:INFO: Dataset: univ                Batch:  2/15	Loss 1.8721 (1.8650)
+2022-11-18 15:34:42,611:INFO: Dataset: univ                Batch:  3/15	Loss 1.8640 (1.8647)
+2022-11-18 15:34:42,781:INFO: Dataset: univ                Batch:  4/15	Loss 1.8914 (1.8717)
+2022-11-18 15:34:42,952:INFO: Dataset: univ                Batch:  5/15	Loss 1.8570 (1.8685)
+2022-11-18 15:34:43,131:INFO: Dataset: univ                Batch:  6/15	Loss 1.8384 (1.8631)
+2022-11-18 15:34:43,301:INFO: Dataset: univ                Batch:  7/15	Loss 1.8287 (1.8589)
+2022-11-18 15:34:43,468:INFO: Dataset: univ                Batch:  8/15	Loss 1.8531 (1.8582)
+2022-11-18 15:34:43,637:INFO: Dataset: univ                Batch:  9/15	Loss 1.9147 (1.8649)
+2022-11-18 15:34:43,803:INFO: Dataset: univ                Batch: 10/15	Loss 1.8946 (1.8678)
+2022-11-18 15:34:43,972:INFO: Dataset: univ                Batch: 11/15	Loss 1.8864 (1.8694)
+2022-11-18 15:34:44,141:INFO: Dataset: univ                Batch: 12/15	Loss 1.8297 (1.8663)
+2022-11-18 15:34:44,311:INFO: Dataset: univ                Batch: 13/15	Loss 1.8875 (1.8680)
+2022-11-18 15:34:44,477:INFO: Dataset: univ                Batch: 14/15	Loss 1.8259 (1.8649)
+2022-11-18 15:34:44,563:INFO: Dataset: univ                Batch: 15/15	Loss 1.8626 (1.8649)
+2022-11-18 15:34:44,954:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9206 (1.9206)
+2022-11-18 15:34:45,110:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8339 (1.8748)
+2022-11-18 15:34:45,266:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8448 (1.8646)
+2022-11-18 15:34:45,417:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7875 (1.8454)
+2022-11-18 15:34:45,566:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7979 (1.8360)
+2022-11-18 15:34:45,715:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8634 (1.8400)
+2022-11-18 15:34:45,866:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8843 (1.8462)
+2022-11-18 15:34:46,001:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8568 (1.8475)
+2022-11-18 15:34:46,407:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7869 (1.7869)
+2022-11-18 15:34:46,570:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8406 (1.8151)
+2022-11-18 15:34:46,737:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8076 (1.8125)
+2022-11-18 15:34:46,908:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8692 (1.8262)
+2022-11-18 15:34:47,072:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9645 (1.8532)
+2022-11-18 15:34:47,232:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8869 (1.8585)
+2022-11-18 15:34:47,388:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8661 (1.8597)
+2022-11-18 15:34:47,573:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9569 (1.8709)
+2022-11-18 15:34:47,769:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8566 (1.8694)
+2022-11-18 15:34:47,954:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9504 (1.8771)
+2022-11-18 15:34:48,121:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8963 (1.8789)
+2022-11-18 15:34:48,292:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8407 (1.8757)
+2022-11-18 15:34:48,460:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8515 (1.8739)
+2022-11-18 15:34:48,626:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8840 (1.8746)
+2022-11-18 15:34:48,800:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8708 (1.8744)
+2022-11-18 15:34:48,960:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9141 (1.8770)
+2022-11-18 15:34:49,116:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8382 (1.8747)
+2022-11-18 15:34:49,257:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8141 (1.8719)
+2022-11-18 15:34:49,303:INFO: - Computing loss (validation)
+2022-11-18 15:34:49,581:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9187 (1.9187)
+2022-11-18 15:34:49,616:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5978 (1.8891)
+2022-11-18 15:34:49,958:INFO: Dataset: univ                Batch: 1/3	Loss 1.8555 (1.8555)
+2022-11-18 15:34:50,039:INFO: Dataset: univ                Batch: 2/3	Loss 1.8713 (1.8640)
+2022-11-18 15:34:50,112:INFO: Dataset: univ                Batch: 3/3	Loss 1.8138 (1.8487)
+2022-11-18 15:34:50,434:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7922 (1.7922)
+2022-11-18 15:34:50,481:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7899 (1.7916)
+2022-11-18 15:34:50,791:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9686 (1.9686)
+2022-11-18 15:34:50,871:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8060 (1.8826)
+2022-11-18 15:34:50,950:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9408 (1.9017)
+2022-11-18 15:34:51,030:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8867 (1.8975)
+2022-11-18 15:34:51,104:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8313 (1.8843)
+2022-11-18 15:34:51,158:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_469.pth.tar
+2022-11-18 15:34:51,158:INFO: 
+===> EPOCH: 470 (P2)
+2022-11-18 15:34:51,159:INFO: - Computing loss (training)
+2022-11-18 15:34:51,546:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7546 (1.7546)
+2022-11-18 15:34:51,723:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8945 (1.8250)
+2022-11-18 15:34:51,891:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9627 (1.8695)
+2022-11-18 15:34:52,047:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9118 (1.8766)
+2022-11-18 15:34:52,555:INFO: Dataset: univ                Batch:  1/15	Loss 1.8206 (1.8206)
+2022-11-18 15:34:52,747:INFO: Dataset: univ                Batch:  2/15	Loss 1.8801 (1.8497)
+2022-11-18 15:34:52,933:INFO: Dataset: univ                Batch:  3/15	Loss 1.8515 (1.8503)
+2022-11-18 15:34:53,137:INFO: Dataset: univ                Batch:  4/15	Loss 1.8430 (1.8484)
+2022-11-18 15:34:53,325:INFO: Dataset: univ                Batch:  5/15	Loss 1.8472 (1.8482)
+2022-11-18 15:34:53,502:INFO: Dataset: univ                Batch:  6/15	Loss 1.8628 (1.8508)
+2022-11-18 15:34:53,674:INFO: Dataset: univ                Batch:  7/15	Loss 1.8647 (1.8527)
+2022-11-18 15:34:53,857:INFO: Dataset: univ                Batch:  8/15	Loss 1.8929 (1.8579)
+2022-11-18 15:34:54,035:INFO: Dataset: univ                Batch:  9/15	Loss 1.8234 (1.8540)
+2022-11-18 15:34:54,212:INFO: Dataset: univ                Batch: 10/15	Loss 1.8520 (1.8538)
+2022-11-18 15:34:54,393:INFO: Dataset: univ                Batch: 11/15	Loss 1.8623 (1.8545)
+2022-11-18 15:34:54,562:INFO: Dataset: univ                Batch: 12/15	Loss 1.8222 (1.8518)
+2022-11-18 15:34:54,723:INFO: Dataset: univ                Batch: 13/15	Loss 1.8671 (1.8529)
+2022-11-18 15:34:54,892:INFO: Dataset: univ                Batch: 14/15	Loss 1.8686 (1.8541)
+2022-11-18 15:34:54,978:INFO: Dataset: univ                Batch: 15/15	Loss 1.9559 (1.8549)
+2022-11-18 15:34:55,389:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8850 (1.8850)
+2022-11-18 15:34:55,541:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8400 (1.8645)
+2022-11-18 15:34:55,699:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8529 (1.8606)
+2022-11-18 15:34:55,852:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9278 (1.8769)
+2022-11-18 15:34:56,008:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8701 (1.8756)
+2022-11-18 15:34:56,173:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9588 (1.8888)
+2022-11-18 15:34:56,328:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7482 (1.8683)
+2022-11-18 15:34:56,465:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9935 (1.8820)
+2022-11-18 15:34:56,858:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9022 (1.9022)
+2022-11-18 15:34:57,010:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8223 (1.8600)
+2022-11-18 15:34:57,164:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8186 (1.8451)
+2022-11-18 15:34:57,330:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9091 (1.8616)
+2022-11-18 15:34:57,500:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9044 (1.8700)
+2022-11-18 15:34:57,668:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8778 (1.8713)
+2022-11-18 15:34:57,825:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7839 (1.8591)
+2022-11-18 15:34:57,987:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8042 (1.8521)
+2022-11-18 15:34:58,152:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8497 (1.8519)
+2022-11-18 15:34:58,320:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8532 (1.8520)
+2022-11-18 15:34:58,487:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8554 (1.8523)
+2022-11-18 15:34:58,639:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8718 (1.8541)
+2022-11-18 15:34:58,792:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8417 (1.8533)
+2022-11-18 15:34:58,946:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8909 (1.8561)
+2022-11-18 15:34:59,102:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8234 (1.8539)
+2022-11-18 15:34:59,265:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8433 (1.8533)
+2022-11-18 15:34:59,423:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8779 (1.8547)
+2022-11-18 15:34:59,569:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9530 (1.8590)
+2022-11-18 15:34:59,620:INFO: - Computing loss (validation)
+2022-11-18 15:34:59,887:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8858 (1.8858)
+2022-11-18 15:34:59,922:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8373 (1.8821)
+2022-11-18 15:35:00,252:INFO: Dataset: univ                Batch: 1/3	Loss 1.8553 (1.8553)
+2022-11-18 15:35:00,339:INFO: Dataset: univ                Batch: 2/3	Loss 1.9080 (1.8818)
+2022-11-18 15:35:00,421:INFO: Dataset: univ                Batch: 3/3	Loss 1.8821 (1.8819)
+2022-11-18 15:35:00,735:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9506 (1.9506)
+2022-11-18 15:35:00,788:INFO: Dataset: zara1               Batch: 2/2	Loss 2.1173 (1.9918)
+2022-11-18 15:35:01,119:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8456 (1.8456)
+2022-11-18 15:35:01,214:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8489 (1.8471)
+2022-11-18 15:35:01,305:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8705 (1.8548)
+2022-11-18 15:35:01,397:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8877 (1.8625)
+2022-11-18 15:35:01,490:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8212 (1.8547)
+2022-11-18 15:35:01,547:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_470.pth.tar
+2022-11-18 15:35:01,547:INFO: 
+===> EPOCH: 471 (P2)
+2022-11-18 15:35:01,548:INFO: - Computing loss (training)
+2022-11-18 15:35:01,899:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9479 (1.9479)
+2022-11-18 15:35:02,054:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8214 (1.8884)
+2022-11-18 15:35:02,207:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8827 (1.8865)
+2022-11-18 15:35:02,324:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8758 (1.8848)
+2022-11-18 15:35:02,733:INFO: Dataset: univ                Batch:  1/15	Loss 1.8985 (1.8985)
+2022-11-18 15:35:02,906:INFO: Dataset: univ                Batch:  2/15	Loss 1.8167 (1.8568)
+2022-11-18 15:35:03,082:INFO: Dataset: univ                Batch:  3/15	Loss 1.8397 (1.8516)
+2022-11-18 15:35:03,263:INFO: Dataset: univ                Batch:  4/15	Loss 1.8479 (1.8507)
+2022-11-18 15:35:03,439:INFO: Dataset: univ                Batch:  5/15	Loss 1.8772 (1.8565)
+2022-11-18 15:35:03,598:INFO: Dataset: univ                Batch:  6/15	Loss 1.8998 (1.8632)
+2022-11-18 15:35:03,755:INFO: Dataset: univ                Batch:  7/15	Loss 1.8645 (1.8634)
+2022-11-18 15:35:03,909:INFO: Dataset: univ                Batch:  8/15	Loss 1.8525 (1.8621)
+2022-11-18 15:35:04,072:INFO: Dataset: univ                Batch:  9/15	Loss 1.8623 (1.8621)
+2022-11-18 15:35:04,242:INFO: Dataset: univ                Batch: 10/15	Loss 1.9038 (1.8659)
+2022-11-18 15:35:04,415:INFO: Dataset: univ                Batch: 11/15	Loss 1.9167 (1.8700)
+2022-11-18 15:35:04,588:INFO: Dataset: univ                Batch: 12/15	Loss 1.8787 (1.8707)
+2022-11-18 15:35:04,752:INFO: Dataset: univ                Batch: 13/15	Loss 1.8325 (1.8682)
+2022-11-18 15:35:04,917:INFO: Dataset: univ                Batch: 14/15	Loss 1.9079 (1.8709)
+2022-11-18 15:35:05,011:INFO: Dataset: univ                Batch: 15/15	Loss 1.9194 (1.8715)
+2022-11-18 15:35:05,447:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9419 (1.9419)
+2022-11-18 15:35:05,605:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8247 (1.8808)
+2022-11-18 15:35:05,758:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8867 (1.8830)
+2022-11-18 15:35:05,914:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8927 (1.8855)
+2022-11-18 15:35:06,084:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9528 (1.8979)
+2022-11-18 15:35:06,254:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8735 (1.8938)
+2022-11-18 15:35:06,423:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9104 (1.8961)
+2022-11-18 15:35:06,563:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8435 (1.8904)
+2022-11-18 15:35:06,979:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8305 (1.8305)
+2022-11-18 15:35:07,138:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7592 (1.7974)
+2022-11-18 15:35:07,304:INFO: Dataset: zara2               Batch:  3/18	Loss 2.0012 (1.8670)
+2022-11-18 15:35:07,472:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8054 (1.8516)
+2022-11-18 15:35:07,626:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8489 (1.8511)
+2022-11-18 15:35:07,780:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8226 (1.8458)
+2022-11-18 15:35:07,949:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9165 (1.8563)
+2022-11-18 15:35:08,119:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9206 (1.8651)
+2022-11-18 15:35:08,275:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7555 (1.8539)
+2022-11-18 15:35:08,438:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7718 (1.8454)
+2022-11-18 15:35:08,610:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8560 (1.8462)
+2022-11-18 15:35:08,762:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8586 (1.8473)
+2022-11-18 15:35:08,933:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7820 (1.8418)
+2022-11-18 15:35:09,089:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8618 (1.8431)
+2022-11-18 15:35:09,253:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9507 (1.8499)
+2022-11-18 15:35:09,419:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8310 (1.8486)
+2022-11-18 15:35:09,574:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8563 (1.8491)
+2022-11-18 15:35:09,730:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8801 (1.8506)
+2022-11-18 15:35:09,776:INFO: - Computing loss (validation)
+2022-11-18 15:35:10,053:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8383 (1.8383)
+2022-11-18 15:35:10,089:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8613 (1.8396)
+2022-11-18 15:35:10,419:INFO: Dataset: univ                Batch: 1/3	Loss 1.8855 (1.8855)
+2022-11-18 15:35:10,511:INFO: Dataset: univ                Batch: 2/3	Loss 1.9325 (1.9087)
+2022-11-18 15:35:10,594:INFO: Dataset: univ                Batch: 3/3	Loss 1.7725 (1.8624)
+2022-11-18 15:35:10,900:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8493 (1.8493)
+2022-11-18 15:35:10,945:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9140 (1.8657)
+2022-11-18 15:35:11,266:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8796 (1.8796)
+2022-11-18 15:35:11,341:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9200 (1.8987)
+2022-11-18 15:35:11,417:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8978 (1.8984)
+2022-11-18 15:35:11,494:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8666 (1.8902)
+2022-11-18 15:35:11,570:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7577 (1.8657)
+2022-11-18 15:35:11,625:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_471.pth.tar
+2022-11-18 15:35:11,625:INFO: 
+===> EPOCH: 472 (P2)
+2022-11-18 15:35:11,626:INFO: - Computing loss (training)
+2022-11-18 15:35:11,975:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9088 (1.9088)
+2022-11-18 15:35:12,131:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8361 (1.8747)
+2022-11-18 15:35:12,292:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9659 (1.9050)
+2022-11-18 15:35:12,423:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8068 (1.8884)
+2022-11-18 15:35:12,852:INFO: Dataset: univ                Batch:  1/15	Loss 1.8513 (1.8513)
+2022-11-18 15:35:13,015:INFO: Dataset: univ                Batch:  2/15	Loss 1.8343 (1.8423)
+2022-11-18 15:35:13,175:INFO: Dataset: univ                Batch:  3/15	Loss 1.8178 (1.8340)
+2022-11-18 15:35:13,333:INFO: Dataset: univ                Batch:  4/15	Loss 1.8847 (1.8475)
+2022-11-18 15:35:13,493:INFO: Dataset: univ                Batch:  5/15	Loss 1.8726 (1.8529)
+2022-11-18 15:35:13,669:INFO: Dataset: univ                Batch:  6/15	Loss 1.8917 (1.8605)
+2022-11-18 15:35:13,848:INFO: Dataset: univ                Batch:  7/15	Loss 1.8316 (1.8563)
+2022-11-18 15:35:14,014:INFO: Dataset: univ                Batch:  8/15	Loss 1.8868 (1.8596)
+2022-11-18 15:35:14,171:INFO: Dataset: univ                Batch:  9/15	Loss 1.8503 (1.8585)
+2022-11-18 15:35:14,326:INFO: Dataset: univ                Batch: 10/15	Loss 1.8975 (1.8620)
+2022-11-18 15:35:14,484:INFO: Dataset: univ                Batch: 11/15	Loss 1.8498 (1.8609)
+2022-11-18 15:35:14,637:INFO: Dataset: univ                Batch: 12/15	Loss 1.8536 (1.8603)
+2022-11-18 15:35:14,793:INFO: Dataset: univ                Batch: 13/15	Loss 1.8517 (1.8597)
+2022-11-18 15:35:14,956:INFO: Dataset: univ                Batch: 14/15	Loss 1.8447 (1.8586)
+2022-11-18 15:35:15,044:INFO: Dataset: univ                Batch: 15/15	Loss 1.9452 (1.8599)
+2022-11-18 15:35:15,439:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8398 (1.8398)
+2022-11-18 15:35:15,592:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8300 (1.8348)
+2022-11-18 15:35:15,746:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8952 (1.8533)
+2022-11-18 15:35:15,899:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8954 (1.8641)
+2022-11-18 15:35:16,050:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9620 (1.8851)
+2022-11-18 15:35:16,199:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9304 (1.8931)
+2022-11-18 15:35:16,350:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7963 (1.8801)
+2022-11-18 15:35:16,488:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8003 (1.8700)
+2022-11-18 15:35:16,912:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8097 (1.8097)
+2022-11-18 15:35:17,069:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8434 (1.8265)
+2022-11-18 15:35:17,225:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9865 (1.8794)
+2022-11-18 15:35:17,383:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9318 (1.8925)
+2022-11-18 15:35:17,533:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8961 (1.8932)
+2022-11-18 15:35:17,688:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8140 (1.8799)
+2022-11-18 15:35:17,838:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8448 (1.8749)
+2022-11-18 15:35:17,988:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7677 (1.8623)
+2022-11-18 15:35:18,140:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8545 (1.8615)
+2022-11-18 15:35:18,289:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8318 (1.8585)
+2022-11-18 15:35:18,440:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9333 (1.8656)
+2022-11-18 15:35:18,590:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8763 (1.8665)
+2022-11-18 15:35:18,740:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9495 (1.8725)
+2022-11-18 15:35:18,890:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8130 (1.8685)
+2022-11-18 15:35:19,041:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9523 (1.8750)
+2022-11-18 15:35:19,189:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8622 (1.8742)
+2022-11-18 15:35:19,340:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7839 (1.8695)
+2022-11-18 15:35:19,478:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8593 (1.8690)
+2022-11-18 15:35:19,523:INFO: - Computing loss (validation)
+2022-11-18 15:35:19,803:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8762 (1.8762)
+2022-11-18 15:35:19,841:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9622 (1.8833)
+2022-11-18 15:35:20,173:INFO: Dataset: univ                Batch: 1/3	Loss 1.8819 (1.8819)
+2022-11-18 15:35:20,254:INFO: Dataset: univ                Batch: 2/3	Loss 1.8357 (1.8593)
+2022-11-18 15:35:20,332:INFO: Dataset: univ                Batch: 3/3	Loss 1.9307 (1.8797)
+2022-11-18 15:35:20,646:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9999 (1.9999)
+2022-11-18 15:35:20,692:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9064 (1.9777)
+2022-11-18 15:35:21,006:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8070 (1.8070)
+2022-11-18 15:35:21,080:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9317 (1.8722)
+2022-11-18 15:35:21,153:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9556 (1.8987)
+2022-11-18 15:35:21,228:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8320 (1.8821)
+2022-11-18 15:35:21,303:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8926 (1.8841)
+2022-11-18 15:35:21,359:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_472.pth.tar
+2022-11-18 15:35:21,359:INFO: 
+===> EPOCH: 473 (P2)
+2022-11-18 15:35:21,360:INFO: - Computing loss (training)
+2022-11-18 15:35:21,712:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9230 (1.9230)
+2022-11-18 15:35:21,867:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8955 (1.9096)
+2022-11-18 15:35:22,017:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8479 (1.8897)
+2022-11-18 15:35:22,145:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7328 (1.8630)
+2022-11-18 15:35:22,679:INFO: Dataset: univ                Batch:  1/15	Loss 1.8396 (1.8396)
+2022-11-18 15:35:22,843:INFO: Dataset: univ                Batch:  2/15	Loss 1.8561 (1.8479)
+2022-11-18 15:35:23,006:INFO: Dataset: univ                Batch:  3/15	Loss 1.8415 (1.8459)
+2022-11-18 15:35:23,166:INFO: Dataset: univ                Batch:  4/15	Loss 1.8400 (1.8445)
+2022-11-18 15:35:23,334:INFO: Dataset: univ                Batch:  5/15	Loss 1.8527 (1.8460)
+2022-11-18 15:35:23,504:INFO: Dataset: univ                Batch:  6/15	Loss 1.8557 (1.8475)
+2022-11-18 15:35:23,666:INFO: Dataset: univ                Batch:  7/15	Loss 1.8276 (1.8445)
+2022-11-18 15:35:23,823:INFO: Dataset: univ                Batch:  8/15	Loss 1.8985 (1.8508)
+2022-11-18 15:35:23,982:INFO: Dataset: univ                Batch:  9/15	Loss 1.8430 (1.8499)
+2022-11-18 15:35:24,141:INFO: Dataset: univ                Batch: 10/15	Loss 1.8775 (1.8530)
+2022-11-18 15:35:24,302:INFO: Dataset: univ                Batch: 11/15	Loss 1.8625 (1.8539)
+2022-11-18 15:35:24,460:INFO: Dataset: univ                Batch: 12/15	Loss 1.8634 (1.8546)
+2022-11-18 15:35:24,627:INFO: Dataset: univ                Batch: 13/15	Loss 1.8974 (1.8575)
+2022-11-18 15:35:24,796:INFO: Dataset: univ                Batch: 14/15	Loss 1.8371 (1.8561)
+2022-11-18 15:35:24,885:INFO: Dataset: univ                Batch: 15/15	Loss 1.8513 (1.8560)
+2022-11-18 15:35:25,273:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9188 (1.9188)
+2022-11-18 15:35:25,423:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9112 (1.9149)
+2022-11-18 15:35:25,574:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7895 (1.8721)
+2022-11-18 15:35:25,726:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8592 (1.8688)
+2022-11-18 15:35:25,878:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8043 (1.8569)
+2022-11-18 15:35:26,027:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9139 (1.8655)
+2022-11-18 15:35:26,177:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8094 (1.8580)
+2022-11-18 15:35:26,313:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8928 (1.8621)
+2022-11-18 15:35:26,721:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7627 (1.7627)
+2022-11-18 15:35:26,874:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8488 (1.8036)
+2022-11-18 15:35:27,029:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8306 (1.8126)
+2022-11-18 15:35:27,182:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9303 (1.8419)
+2022-11-18 15:35:27,332:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8726 (1.8485)
+2022-11-18 15:35:27,484:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8221 (1.8442)
+2022-11-18 15:35:27,643:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9425 (1.8589)
+2022-11-18 15:35:27,807:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8302 (1.8554)
+2022-11-18 15:35:27,979:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8204 (1.8517)
+2022-11-18 15:35:28,131:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8781 (1.8545)
+2022-11-18 15:35:28,285:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8954 (1.8585)
+2022-11-18 15:35:28,435:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8504 (1.8578)
+2022-11-18 15:35:28,587:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9509 (1.8652)
+2022-11-18 15:35:28,737:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8210 (1.8618)
+2022-11-18 15:35:28,889:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7988 (1.8580)
+2022-11-18 15:35:29,047:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8348 (1.8565)
+2022-11-18 15:35:29,206:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8780 (1.8578)
+2022-11-18 15:35:29,360:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8042 (1.8554)
+2022-11-18 15:35:29,416:INFO: - Computing loss (validation)
+2022-11-18 15:35:29,692:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8608 (1.8608)
+2022-11-18 15:35:29,729:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1116 (1.8805)
+2022-11-18 15:35:30,070:INFO: Dataset: univ                Batch: 1/3	Loss 1.8740 (1.8740)
+2022-11-18 15:35:30,148:INFO: Dataset: univ                Batch: 2/3	Loss 1.8685 (1.8715)
+2022-11-18 15:35:30,222:INFO: Dataset: univ                Batch: 3/3	Loss 1.8380 (1.8622)
+2022-11-18 15:35:30,553:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9301 (1.9301)
+2022-11-18 15:35:30,600:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6946 (1.8741)
+2022-11-18 15:35:30,923:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7820 (1.7820)
+2022-11-18 15:35:30,997:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9623 (1.8667)
+2022-11-18 15:35:31,073:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9673 (1.9020)
+2022-11-18 15:35:31,147:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7761 (1.8699)
+2022-11-18 15:35:31,221:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9363 (1.8817)
+2022-11-18 15:35:31,276:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_473.pth.tar
+2022-11-18 15:35:31,276:INFO: 
+===> EPOCH: 474 (P2)
+2022-11-18 15:35:31,277:INFO: - Computing loss (training)
+2022-11-18 15:35:31,611:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9212 (1.9212)
+2022-11-18 15:35:31,770:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0690 (1.9967)
+2022-11-18 15:35:31,929:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7937 (1.9313)
+2022-11-18 15:35:32,052:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9737 (1.9395)
+2022-11-18 15:35:32,446:INFO: Dataset: univ                Batch:  1/15	Loss 1.8957 (1.8957)
+2022-11-18 15:35:32,606:INFO: Dataset: univ                Batch:  2/15	Loss 1.8589 (1.8783)
+2022-11-18 15:35:32,764:INFO: Dataset: univ                Batch:  3/15	Loss 1.8802 (1.8790)
+2022-11-18 15:35:32,927:INFO: Dataset: univ                Batch:  4/15	Loss 1.8632 (1.8756)
+2022-11-18 15:35:33,103:INFO: Dataset: univ                Batch:  5/15	Loss 1.8413 (1.8689)
+2022-11-18 15:35:33,274:INFO: Dataset: univ                Batch:  6/15	Loss 1.8514 (1.8659)
+2022-11-18 15:35:33,449:INFO: Dataset: univ                Batch:  7/15	Loss 1.8610 (1.8652)
+2022-11-18 15:35:33,609:INFO: Dataset: univ                Batch:  8/15	Loss 1.8666 (1.8654)
+2022-11-18 15:35:33,766:INFO: Dataset: univ                Batch:  9/15	Loss 1.8294 (1.8614)
+2022-11-18 15:35:33,922:INFO: Dataset: univ                Batch: 10/15	Loss 1.8287 (1.8581)
+2022-11-18 15:35:34,078:INFO: Dataset: univ                Batch: 11/15	Loss 1.9194 (1.8637)
+2022-11-18 15:35:34,233:INFO: Dataset: univ                Batch: 12/15	Loss 1.8117 (1.8599)
+2022-11-18 15:35:34,390:INFO: Dataset: univ                Batch: 13/15	Loss 1.9052 (1.8632)
+2022-11-18 15:35:34,545:INFO: Dataset: univ                Batch: 14/15	Loss 1.8457 (1.8620)
+2022-11-18 15:35:34,628:INFO: Dataset: univ                Batch: 15/15	Loss 1.8385 (1.8616)
+2022-11-18 15:35:35,022:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8301 (1.8301)
+2022-11-18 15:35:35,174:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8536 (1.8418)
+2022-11-18 15:35:35,328:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8819 (1.8553)
+2022-11-18 15:35:35,480:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9267 (1.8736)
+2022-11-18 15:35:35,642:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8989 (1.8785)
+2022-11-18 15:35:35,810:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0875 (1.9169)
+2022-11-18 15:35:35,979:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9900 (1.9276)
+2022-11-18 15:35:36,128:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8958 (1.9238)
+2022-11-18 15:35:36,536:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7872 (1.7872)
+2022-11-18 15:35:36,693:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8836 (1.8366)
+2022-11-18 15:35:36,844:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8802 (1.8501)
+2022-11-18 15:35:36,995:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8770 (1.8563)
+2022-11-18 15:35:37,149:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7970 (1.8428)
+2022-11-18 15:35:37,304:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7854 (1.8332)
+2022-11-18 15:35:37,463:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8470 (1.8350)
+2022-11-18 15:35:37,622:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8998 (1.8433)
+2022-11-18 15:35:37,777:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8332 (1.8422)
+2022-11-18 15:35:37,935:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8452 (1.8425)
+2022-11-18 15:35:38,090:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8020 (1.8391)
+2022-11-18 15:35:38,243:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8388 (1.8391)
+2022-11-18 15:35:38,398:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7930 (1.8355)
+2022-11-18 15:35:38,549:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8153 (1.8339)
+2022-11-18 15:35:38,704:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7854 (1.8309)
+2022-11-18 15:35:38,856:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8671 (1.8331)
+2022-11-18 15:35:39,008:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8980 (1.8374)
+2022-11-18 15:35:39,148:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9053 (1.8405)
+2022-11-18 15:35:39,194:INFO: - Computing loss (validation)
+2022-11-18 15:35:39,474:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9365 (1.9365)
+2022-11-18 15:35:39,508:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6647 (1.9189)
+2022-11-18 15:35:39,818:INFO: Dataset: univ                Batch: 1/3	Loss 1.8860 (1.8860)
+2022-11-18 15:35:39,899:INFO: Dataset: univ                Batch: 2/3	Loss 1.8645 (1.8750)
+2022-11-18 15:35:39,974:INFO: Dataset: univ                Batch: 3/3	Loss 1.8545 (1.8686)
+2022-11-18 15:35:40,278:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8895 (1.8895)
+2022-11-18 15:35:40,324:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0049 (1.9177)
+2022-11-18 15:35:40,637:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8648 (1.8648)
+2022-11-18 15:35:40,714:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8723 (1.8685)
+2022-11-18 15:35:40,791:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8438 (1.8606)
+2022-11-18 15:35:40,868:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9375 (1.8786)
+2022-11-18 15:35:40,944:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9171 (1.8864)
+2022-11-18 15:35:40,998:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_474.pth.tar
+2022-11-18 15:35:40,998:INFO: 
+===> EPOCH: 475 (P2)
+2022-11-18 15:35:40,998:INFO: - Computing loss (training)
+2022-11-18 15:35:41,411:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9479 (1.9479)
+2022-11-18 15:35:41,563:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7187 (1.8301)
+2022-11-18 15:35:41,715:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9598 (1.8720)
+2022-11-18 15:35:41,832:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8563 (1.8694)
+2022-11-18 15:35:42,233:INFO: Dataset: univ                Batch:  1/15	Loss 1.8552 (1.8552)
+2022-11-18 15:35:42,395:INFO: Dataset: univ                Batch:  2/15	Loss 1.9263 (1.8914)
+2022-11-18 15:35:42,554:INFO: Dataset: univ                Batch:  3/15	Loss 1.8332 (1.8729)
+2022-11-18 15:35:42,710:INFO: Dataset: univ                Batch:  4/15	Loss 1.8861 (1.8760)
+2022-11-18 15:35:42,870:INFO: Dataset: univ                Batch:  5/15	Loss 1.8653 (1.8739)
+2022-11-18 15:35:43,034:INFO: Dataset: univ                Batch:  6/15	Loss 1.8601 (1.8713)
+2022-11-18 15:35:43,193:INFO: Dataset: univ                Batch:  7/15	Loss 1.8662 (1.8706)
+2022-11-18 15:35:43,356:INFO: Dataset: univ                Batch:  8/15	Loss 1.8603 (1.8692)
+2022-11-18 15:35:43,520:INFO: Dataset: univ                Batch:  9/15	Loss 1.8261 (1.8642)
+2022-11-18 15:35:43,682:INFO: Dataset: univ                Batch: 10/15	Loss 1.8634 (1.8642)
+2022-11-18 15:35:43,859:INFO: Dataset: univ                Batch: 11/15	Loss 1.8678 (1.8645)
+2022-11-18 15:35:44,031:INFO: Dataset: univ                Batch: 12/15	Loss 1.8838 (1.8660)
+2022-11-18 15:35:44,190:INFO: Dataset: univ                Batch: 13/15	Loss 1.8618 (1.8657)
+2022-11-18 15:35:44,363:INFO: Dataset: univ                Batch: 14/15	Loss 1.8646 (1.8656)
+2022-11-18 15:35:44,458:INFO: Dataset: univ                Batch: 15/15	Loss 1.8503 (1.8654)
+2022-11-18 15:35:44,912:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8335 (1.8335)
+2022-11-18 15:35:45,069:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0283 (1.9366)
+2022-11-18 15:35:45,232:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9704 (1.9475)
+2022-11-18 15:35:45,389:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8254 (1.9156)
+2022-11-18 15:35:45,546:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7975 (1.8938)
+2022-11-18 15:35:45,708:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9839 (1.9085)
+2022-11-18 15:35:45,867:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8068 (1.8942)
+2022-11-18 15:35:46,018:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8054 (1.8845)
+2022-11-18 15:35:46,473:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9288 (1.9288)
+2022-11-18 15:35:46,643:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9010 (1.9155)
+2022-11-18 15:35:46,805:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8483 (1.8941)
+2022-11-18 15:35:46,965:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7625 (1.8613)
+2022-11-18 15:35:47,127:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9296 (1.8745)
+2022-11-18 15:35:47,288:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8751 (1.8746)
+2022-11-18 15:35:47,453:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8023 (1.8652)
+2022-11-18 15:35:47,619:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8057 (1.8578)
+2022-11-18 15:35:47,796:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8500 (1.8569)
+2022-11-18 15:35:47,976:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9218 (1.8634)
+2022-11-18 15:35:48,150:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8323 (1.8607)
+2022-11-18 15:35:48,327:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8015 (1.8559)
+2022-11-18 15:35:48,488:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8475 (1.8553)
+2022-11-18 15:35:48,650:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8315 (1.8537)
+2022-11-18 15:35:48,815:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8136 (1.8509)
+2022-11-18 15:35:48,972:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8728 (1.8523)
+2022-11-18 15:35:49,129:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9324 (1.8575)
+2022-11-18 15:35:49,273:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0290 (1.8652)
+2022-11-18 15:35:49,321:INFO: - Computing loss (validation)
+2022-11-18 15:35:49,597:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9163 (1.9163)
+2022-11-18 15:35:49,633:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7931 (1.9067)
+2022-11-18 15:35:49,948:INFO: Dataset: univ                Batch: 1/3	Loss 1.8237 (1.8237)
+2022-11-18 15:35:50,026:INFO: Dataset: univ                Batch: 2/3	Loss 1.8386 (1.8311)
+2022-11-18 15:35:50,103:INFO: Dataset: univ                Batch: 3/3	Loss 1.8157 (1.8255)
+2022-11-18 15:35:50,401:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8518 (1.8518)
+2022-11-18 15:35:50,447:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9271 (1.8699)
+2022-11-18 15:35:50,754:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8763 (1.8763)
+2022-11-18 15:35:50,831:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8079 (1.8417)
+2022-11-18 15:35:50,907:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8782 (1.8537)
+2022-11-18 15:35:50,984:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8113 (1.8433)
+2022-11-18 15:35:51,059:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8571 (1.8459)
+2022-11-18 15:35:51,115:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_475.pth.tar
+2022-11-18 15:35:51,115:INFO: 
+===> EPOCH: 476 (P2)
+2022-11-18 15:35:51,116:INFO: - Computing loss (training)
+2022-11-18 15:35:51,464:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8901 (1.8901)
+2022-11-18 15:35:51,625:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9295 (1.9100)
+2022-11-18 15:35:51,803:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8227 (1.8802)
+2022-11-18 15:35:51,950:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8381 (1.8728)
+2022-11-18 15:35:52,428:INFO: Dataset: univ                Batch:  1/15	Loss 1.9100 (1.9100)
+2022-11-18 15:35:52,592:INFO: Dataset: univ                Batch:  2/15	Loss 1.8442 (1.8782)
+2022-11-18 15:35:52,759:INFO: Dataset: univ                Batch:  3/15	Loss 1.8794 (1.8786)
+2022-11-18 15:35:52,924:INFO: Dataset: univ                Batch:  4/15	Loss 1.8560 (1.8727)
+2022-11-18 15:35:53,094:INFO: Dataset: univ                Batch:  5/15	Loss 1.8370 (1.8655)
+2022-11-18 15:35:53,274:INFO: Dataset: univ                Batch:  6/15	Loss 1.8717 (1.8665)
+2022-11-18 15:35:53,453:INFO: Dataset: univ                Batch:  7/15	Loss 1.8848 (1.8690)
+2022-11-18 15:35:53,618:INFO: Dataset: univ                Batch:  8/15	Loss 1.8390 (1.8651)
+2022-11-18 15:35:53,778:INFO: Dataset: univ                Batch:  9/15	Loss 1.8444 (1.8629)
+2022-11-18 15:35:53,941:INFO: Dataset: univ                Batch: 10/15	Loss 1.8370 (1.8601)
+2022-11-18 15:35:54,108:INFO: Dataset: univ                Batch: 11/15	Loss 1.8486 (1.8590)
+2022-11-18 15:35:54,274:INFO: Dataset: univ                Batch: 12/15	Loss 1.8459 (1.8580)
+2022-11-18 15:35:54,433:INFO: Dataset: univ                Batch: 13/15	Loss 1.8707 (1.8589)
+2022-11-18 15:35:54,593:INFO: Dataset: univ                Batch: 14/15	Loss 1.9197 (1.8630)
+2022-11-18 15:35:54,678:INFO: Dataset: univ                Batch: 15/15	Loss 1.8062 (1.8622)
+2022-11-18 15:35:55,066:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8511 (1.8511)
+2022-11-18 15:35:55,219:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7588 (1.8111)
+2022-11-18 15:35:55,379:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9234 (1.8501)
+2022-11-18 15:35:55,543:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8746 (1.8563)
+2022-11-18 15:35:55,707:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7494 (1.8353)
+2022-11-18 15:35:55,863:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9067 (1.8462)
+2022-11-18 15:35:56,015:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9611 (1.8620)
+2022-11-18 15:35:56,155:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9203 (1.8686)
+2022-11-18 15:35:56,547:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9065 (1.9065)
+2022-11-18 15:35:56,702:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8129 (1.8562)
+2022-11-18 15:35:56,859:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8262 (1.8458)
+2022-11-18 15:35:57,022:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8769 (1.8544)
+2022-11-18 15:35:57,185:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9453 (1.8722)
+2022-11-18 15:35:57,343:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7668 (1.8542)
+2022-11-18 15:35:57,497:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8508 (1.8538)
+2022-11-18 15:35:57,648:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8944 (1.8593)
+2022-11-18 15:35:57,800:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7945 (1.8521)
+2022-11-18 15:35:57,960:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7210 (1.8397)
+2022-11-18 15:35:58,133:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8207 (1.8381)
+2022-11-18 15:35:58,300:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7962 (1.8346)
+2022-11-18 15:35:58,466:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8660 (1.8370)
+2022-11-18 15:35:58,623:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8920 (1.8409)
+2022-11-18 15:35:58,777:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9102 (1.8450)
+2022-11-18 15:35:58,929:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7549 (1.8394)
+2022-11-18 15:35:59,083:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7610 (1.8353)
+2022-11-18 15:35:59,225:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9175 (1.8396)
+2022-11-18 15:35:59,278:INFO: - Computing loss (validation)
+2022-11-18 15:35:59,557:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8996 (1.8996)
+2022-11-18 15:35:59,596:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9799 (1.9040)
+2022-11-18 15:35:59,942:INFO: Dataset: univ                Batch: 1/3	Loss 1.8607 (1.8607)
+2022-11-18 15:36:00,019:INFO: Dataset: univ                Batch: 2/3	Loss 1.8871 (1.8730)
+2022-11-18 15:36:00,097:INFO: Dataset: univ                Batch: 3/3	Loss 1.8402 (1.8618)
+2022-11-18 15:36:00,432:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8490 (1.8490)
+2022-11-18 15:36:00,483:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8382 (1.8464)
+2022-11-18 15:36:00,794:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6986 (1.6986)
+2022-11-18 15:36:00,873:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8546 (1.7778)
+2022-11-18 15:36:00,947:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8289 (1.7952)
+2022-11-18 15:36:01,023:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8814 (1.8169)
+2022-11-18 15:36:01,098:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8153 (1.8166)
+2022-11-18 15:36:01,151:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_476.pth.tar
+2022-11-18 15:36:01,151:INFO: 
+===> EPOCH: 477 (P2)
+2022-11-18 15:36:01,151:INFO: - Computing loss (training)
+2022-11-18 15:36:01,500:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8156 (1.8156)
+2022-11-18 15:36:01,668:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9167 (1.8685)
+2022-11-18 15:36:01,839:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8019 (1.8467)
+2022-11-18 15:36:01,972:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9075 (1.8563)
+2022-11-18 15:36:02,393:INFO: Dataset: univ                Batch:  1/15	Loss 1.8497 (1.8497)
+2022-11-18 15:36:02,550:INFO: Dataset: univ                Batch:  2/15	Loss 1.8607 (1.8552)
+2022-11-18 15:36:02,711:INFO: Dataset: univ                Batch:  3/15	Loss 1.8517 (1.8540)
+2022-11-18 15:36:02,867:INFO: Dataset: univ                Batch:  4/15	Loss 1.8875 (1.8627)
+2022-11-18 15:36:03,032:INFO: Dataset: univ                Batch:  5/15	Loss 1.8647 (1.8631)
+2022-11-18 15:36:03,197:INFO: Dataset: univ                Batch:  6/15	Loss 1.8636 (1.8632)
+2022-11-18 15:36:03,364:INFO: Dataset: univ                Batch:  7/15	Loss 1.8714 (1.8643)
+2022-11-18 15:36:03,528:INFO: Dataset: univ                Batch:  8/15	Loss 1.8648 (1.8644)
+2022-11-18 15:36:03,685:INFO: Dataset: univ                Batch:  9/15	Loss 1.8757 (1.8657)
+2022-11-18 15:36:03,839:INFO: Dataset: univ                Batch: 10/15	Loss 1.8884 (1.8680)
+2022-11-18 15:36:03,997:INFO: Dataset: univ                Batch: 11/15	Loss 1.8487 (1.8662)
+2022-11-18 15:36:04,152:INFO: Dataset: univ                Batch: 12/15	Loss 1.8798 (1.8673)
+2022-11-18 15:36:04,309:INFO: Dataset: univ                Batch: 13/15	Loss 1.8448 (1.8656)
+2022-11-18 15:36:04,484:INFO: Dataset: univ                Batch: 14/15	Loss 1.8404 (1.8638)
+2022-11-18 15:36:04,577:INFO: Dataset: univ                Batch: 15/15	Loss 1.8509 (1.8637)
+2022-11-18 15:36:04,982:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8295 (1.8295)
+2022-11-18 15:36:05,133:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9143 (1.8722)
+2022-11-18 15:36:05,304:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8779 (1.8741)
+2022-11-18 15:36:05,484:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8066 (1.8570)
+2022-11-18 15:36:05,650:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9357 (1.8729)
+2022-11-18 15:36:05,802:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8648 (1.8712)
+2022-11-18 15:36:05,952:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8693 (1.8709)
+2022-11-18 15:36:06,089:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7966 (1.8632)
+2022-11-18 15:36:06,484:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8040 (1.8040)
+2022-11-18 15:36:06,643:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8431 (1.8239)
+2022-11-18 15:36:06,805:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8431 (1.8303)
+2022-11-18 15:36:06,967:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8486 (1.8348)
+2022-11-18 15:36:07,124:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7482 (1.8184)
+2022-11-18 15:36:07,285:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8063 (1.8164)
+2022-11-18 15:36:07,440:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8620 (1.8233)
+2022-11-18 15:36:07,589:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8628 (1.8285)
+2022-11-18 15:36:07,744:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7322 (1.8185)
+2022-11-18 15:36:07,900:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8856 (1.8249)
+2022-11-18 15:36:08,067:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8643 (1.8286)
+2022-11-18 15:36:08,243:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9784 (1.8407)
+2022-11-18 15:36:08,424:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9390 (1.8475)
+2022-11-18 15:36:08,592:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8596 (1.8485)
+2022-11-18 15:36:08,754:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8937 (1.8517)
+2022-11-18 15:36:08,923:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9429 (1.8575)
+2022-11-18 15:36:09,092:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7338 (1.8499)
+2022-11-18 15:36:09,250:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8246 (1.8486)
+2022-11-18 15:36:09,305:INFO: - Computing loss (validation)
+2022-11-18 15:36:09,593:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9104 (1.9104)
+2022-11-18 15:36:09,628:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1574 (1.9297)
+2022-11-18 15:36:09,935:INFO: Dataset: univ                Batch: 1/3	Loss 1.8367 (1.8367)
+2022-11-18 15:36:10,013:INFO: Dataset: univ                Batch: 2/3	Loss 1.8813 (1.8604)
+2022-11-18 15:36:10,088:INFO: Dataset: univ                Batch: 3/3	Loss 1.8590 (1.8599)
+2022-11-18 15:36:10,395:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8026 (1.8026)
+2022-11-18 15:36:10,443:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9367 (1.8375)
+2022-11-18 15:36:10,749:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8374 (1.8374)
+2022-11-18 15:36:10,824:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8944 (1.8651)
+2022-11-18 15:36:10,902:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8698 (1.8667)
+2022-11-18 15:36:10,978:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9039 (1.8756)
+2022-11-18 15:36:11,052:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7883 (1.8565)
+2022-11-18 15:36:11,103:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_477.pth.tar
+2022-11-18 15:36:11,103:INFO: 
+===> EPOCH: 478 (P2)
+2022-11-18 15:36:11,103:INFO: - Computing loss (training)
+2022-11-18 15:36:11,459:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9114 (1.9114)
+2022-11-18 15:36:11,615:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9039 (1.9075)
+2022-11-18 15:36:11,769:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9504 (1.9215)
+2022-11-18 15:36:11,889:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7605 (1.8960)
+2022-11-18 15:36:12,369:INFO: Dataset: univ                Batch:  1/15	Loss 1.8526 (1.8526)
+2022-11-18 15:36:12,533:INFO: Dataset: univ                Batch:  2/15	Loss 1.8211 (1.8377)
+2022-11-18 15:36:12,709:INFO: Dataset: univ                Batch:  3/15	Loss 1.8410 (1.8388)
+2022-11-18 15:36:12,872:INFO: Dataset: univ                Batch:  4/15	Loss 1.8590 (1.8438)
+2022-11-18 15:36:13,033:INFO: Dataset: univ                Batch:  5/15	Loss 1.8649 (1.8481)
+2022-11-18 15:36:13,193:INFO: Dataset: univ                Batch:  6/15	Loss 1.8520 (1.8488)
+2022-11-18 15:36:13,351:INFO: Dataset: univ                Batch:  7/15	Loss 1.8586 (1.8501)
+2022-11-18 15:36:13,508:INFO: Dataset: univ                Batch:  8/15	Loss 1.8387 (1.8487)
+2022-11-18 15:36:13,666:INFO: Dataset: univ                Batch:  9/15	Loss 1.8762 (1.8521)
+2022-11-18 15:36:13,820:INFO: Dataset: univ                Batch: 10/15	Loss 1.8800 (1.8546)
+2022-11-18 15:36:13,977:INFO: Dataset: univ                Batch: 11/15	Loss 1.8425 (1.8536)
+2022-11-18 15:36:14,132:INFO: Dataset: univ                Batch: 12/15	Loss 1.8641 (1.8545)
+2022-11-18 15:36:14,298:INFO: Dataset: univ                Batch: 13/15	Loss 1.8655 (1.8555)
+2022-11-18 15:36:14,465:INFO: Dataset: univ                Batch: 14/15	Loss 1.9035 (1.8586)
+2022-11-18 15:36:14,554:INFO: Dataset: univ                Batch: 15/15	Loss 1.9105 (1.8594)
+2022-11-18 15:36:14,967:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9271 (1.9271)
+2022-11-18 15:36:15,119:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8214 (1.8769)
+2022-11-18 15:36:15,271:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8576 (1.8698)
+2022-11-18 15:36:15,422:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7901 (1.8494)
+2022-11-18 15:36:15,579:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8573 (1.8510)
+2022-11-18 15:36:15,729:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8050 (1.8430)
+2022-11-18 15:36:15,879:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8155 (1.8392)
+2022-11-18 15:36:16,016:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9028 (1.8470)
+2022-11-18 15:36:16,423:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8825 (1.8825)
+2022-11-18 15:36:16,581:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8782 (1.8804)
+2022-11-18 15:36:16,738:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9429 (1.9013)
+2022-11-18 15:36:16,899:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8150 (1.8775)
+2022-11-18 15:36:17,053:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7817 (1.8577)
+2022-11-18 15:36:17,206:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8951 (1.8639)
+2022-11-18 15:36:17,356:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7997 (1.8542)
+2022-11-18 15:36:17,507:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8477 (1.8534)
+2022-11-18 15:36:17,657:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8154 (1.8491)
+2022-11-18 15:36:17,804:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8776 (1.8520)
+2022-11-18 15:36:17,956:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8413 (1.8510)
+2022-11-18 15:36:18,106:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7702 (1.8441)
+2022-11-18 15:36:18,267:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9045 (1.8490)
+2022-11-18 15:36:18,436:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8452 (1.8487)
+2022-11-18 15:36:18,591:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7754 (1.8442)
+2022-11-18 15:36:18,752:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8526 (1.8447)
+2022-11-18 15:36:18,914:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9147 (1.8490)
+2022-11-18 15:36:19,052:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8901 (1.8509)
+2022-11-18 15:36:19,102:INFO: - Computing loss (validation)
+2022-11-18 15:36:19,396:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9120 (1.9120)
+2022-11-18 15:36:19,431:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0954 (1.9214)
+2022-11-18 15:36:19,739:INFO: Dataset: univ                Batch: 1/3	Loss 1.8534 (1.8534)
+2022-11-18 15:36:19,817:INFO: Dataset: univ                Batch: 2/3	Loss 1.8568 (1.8552)
+2022-11-18 15:36:19,892:INFO: Dataset: univ                Batch: 3/3	Loss 1.8636 (1.8580)
+2022-11-18 15:36:20,190:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9250 (1.9250)
+2022-11-18 15:36:20,235:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8471 (1.9057)
+2022-11-18 15:36:20,558:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9068 (1.9068)
+2022-11-18 15:36:20,641:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8208 (1.8626)
+2022-11-18 15:36:20,725:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9046 (1.8759)
+2022-11-18 15:36:20,809:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8367 (1.8650)
+2022-11-18 15:36:20,889:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8944 (1.8707)
+2022-11-18 15:36:20,958:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_478.pth.tar
+2022-11-18 15:36:20,958:INFO: 
+===> EPOCH: 479 (P2)
+2022-11-18 15:36:20,959:INFO: - Computing loss (training)
+2022-11-18 15:36:21,306:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8888 (1.8888)
+2022-11-18 15:36:21,465:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9718 (1.9295)
+2022-11-18 15:36:21,637:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8489 (1.9021)
+2022-11-18 15:36:21,776:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8910 (1.9001)
+2022-11-18 15:36:22,218:INFO: Dataset: univ                Batch:  1/15	Loss 1.8203 (1.8203)
+2022-11-18 15:36:22,390:INFO: Dataset: univ                Batch:  2/15	Loss 1.8890 (1.8557)
+2022-11-18 15:36:22,548:INFO: Dataset: univ                Batch:  3/15	Loss 1.8415 (1.8513)
+2022-11-18 15:36:22,722:INFO: Dataset: univ                Batch:  4/15	Loss 1.8166 (1.8420)
+2022-11-18 15:36:22,892:INFO: Dataset: univ                Batch:  5/15	Loss 1.8395 (1.8415)
+2022-11-18 15:36:23,056:INFO: Dataset: univ                Batch:  6/15	Loss 1.8145 (1.8367)
+2022-11-18 15:36:23,218:INFO: Dataset: univ                Batch:  7/15	Loss 1.8629 (1.8407)
+2022-11-18 15:36:23,375:INFO: Dataset: univ                Batch:  8/15	Loss 1.8905 (1.8465)
+2022-11-18 15:36:23,536:INFO: Dataset: univ                Batch:  9/15	Loss 1.8592 (1.8479)
+2022-11-18 15:36:23,693:INFO: Dataset: univ                Batch: 10/15	Loss 1.8683 (1.8498)
+2022-11-18 15:36:23,852:INFO: Dataset: univ                Batch: 11/15	Loss 1.8511 (1.8499)
+2022-11-18 15:36:24,012:INFO: Dataset: univ                Batch: 12/15	Loss 1.8555 (1.8503)
+2022-11-18 15:36:24,171:INFO: Dataset: univ                Batch: 13/15	Loss 1.8723 (1.8521)
+2022-11-18 15:36:24,326:INFO: Dataset: univ                Batch: 14/15	Loss 1.8472 (1.8518)
+2022-11-18 15:36:24,409:INFO: Dataset: univ                Batch: 15/15	Loss 1.9174 (1.8528)
+2022-11-18 15:36:24,804:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8987 (1.8987)
+2022-11-18 15:36:24,957:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9119 (1.9049)
+2022-11-18 15:36:25,114:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8657 (1.8916)
+2022-11-18 15:36:25,275:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8393 (1.8793)
+2022-11-18 15:36:25,433:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9225 (1.8873)
+2022-11-18 15:36:25,589:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9118 (1.8913)
+2022-11-18 15:36:25,744:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8585 (1.8868)
+2022-11-18 15:36:25,884:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8880 (1.8870)
+2022-11-18 15:36:26,320:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8726 (1.8726)
+2022-11-18 15:36:26,470:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8981 (1.8856)
+2022-11-18 15:36:26,619:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8548 (1.8757)
+2022-11-18 15:36:26,767:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8275 (1.8635)
+2022-11-18 15:36:26,927:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8982 (1.8704)
+2022-11-18 15:36:27,088:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8015 (1.8581)
+2022-11-18 15:36:27,246:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8627 (1.8588)
+2022-11-18 15:36:27,393:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8844 (1.8620)
+2022-11-18 15:36:27,542:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9002 (1.8660)
+2022-11-18 15:36:27,689:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8295 (1.8623)
+2022-11-18 15:36:27,841:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7771 (1.8548)
+2022-11-18 15:36:27,993:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7915 (1.8494)
+2022-11-18 15:36:28,145:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9040 (1.8537)
+2022-11-18 15:36:28,295:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9274 (1.8592)
+2022-11-18 15:36:28,448:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8481 (1.8586)
+2022-11-18 15:36:28,599:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8089 (1.8554)
+2022-11-18 15:36:28,751:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8139 (1.8531)
+2022-11-18 15:36:28,889:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8263 (1.8517)
+2022-11-18 15:36:28,936:INFO: - Computing loss (validation)
+2022-11-18 15:36:29,197:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0227 (2.0227)
+2022-11-18 15:36:29,231:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8927 (2.0138)
+2022-11-18 15:36:29,543:INFO: Dataset: univ                Batch: 1/3	Loss 1.8835 (1.8835)
+2022-11-18 15:36:29,622:INFO: Dataset: univ                Batch: 2/3	Loss 1.8060 (1.8392)
+2022-11-18 15:36:29,698:INFO: Dataset: univ                Batch: 3/3	Loss 1.8500 (1.8428)
+2022-11-18 15:36:29,999:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9056 (1.9056)
+2022-11-18 15:36:30,045:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7637 (1.8709)
+2022-11-18 15:36:30,358:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9277 (1.9277)
+2022-11-18 15:36:30,435:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8358 (1.8832)
+2022-11-18 15:36:30,512:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8697 (1.8784)
+2022-11-18 15:36:30,590:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8982 (1.8833)
+2022-11-18 15:36:30,664:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9732 (1.9016)
+2022-11-18 15:36:30,717:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_479.pth.tar
+2022-11-18 15:36:30,718:INFO: 
+===> EPOCH: 480 (P2)
+2022-11-18 15:36:30,718:INFO: - Computing loss (training)
+2022-11-18 15:36:31,058:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8111 (1.8111)
+2022-11-18 15:36:31,214:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8680 (1.8399)
+2022-11-18 15:36:31,367:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8955 (1.8586)
+2022-11-18 15:36:31,487:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9090 (1.8670)
+2022-11-18 15:36:31,885:INFO: Dataset: univ                Batch:  1/15	Loss 1.8283 (1.8283)
+2022-11-18 15:36:32,046:INFO: Dataset: univ                Batch:  2/15	Loss 1.8798 (1.8543)
+2022-11-18 15:36:32,215:INFO: Dataset: univ                Batch:  3/15	Loss 1.8682 (1.8593)
+2022-11-18 15:36:32,383:INFO: Dataset: univ                Batch:  4/15	Loss 1.8550 (1.8582)
+2022-11-18 15:36:32,542:INFO: Dataset: univ                Batch:  5/15	Loss 1.8500 (1.8563)
+2022-11-18 15:36:32,701:INFO: Dataset: univ                Batch:  6/15	Loss 1.8203 (1.8499)
+2022-11-18 15:36:32,870:INFO: Dataset: univ                Batch:  7/15	Loss 1.8742 (1.8538)
+2022-11-18 15:36:33,029:INFO: Dataset: univ                Batch:  8/15	Loss 1.8695 (1.8559)
+2022-11-18 15:36:33,189:INFO: Dataset: univ                Batch:  9/15	Loss 1.8693 (1.8574)
+2022-11-18 15:36:33,354:INFO: Dataset: univ                Batch: 10/15	Loss 1.8920 (1.8607)
+2022-11-18 15:36:33,523:INFO: Dataset: univ                Batch: 11/15	Loss 1.8799 (1.8626)
+2022-11-18 15:36:33,695:INFO: Dataset: univ                Batch: 12/15	Loss 1.9014 (1.8658)
+2022-11-18 15:36:33,855:INFO: Dataset: univ                Batch: 13/15	Loss 1.8626 (1.8655)
+2022-11-18 15:36:34,021:INFO: Dataset: univ                Batch: 14/15	Loss 1.8826 (1.8668)
+2022-11-18 15:36:34,110:INFO: Dataset: univ                Batch: 15/15	Loss 1.8771 (1.8669)
+2022-11-18 15:36:34,528:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9230 (1.9230)
+2022-11-18 15:36:34,683:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7970 (1.8636)
+2022-11-18 15:36:34,846:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8024 (1.8416)
+2022-11-18 15:36:35,006:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8871 (1.8527)
+2022-11-18 15:36:35,162:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9152 (1.8656)
+2022-11-18 15:36:35,314:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8377 (1.8611)
+2022-11-18 15:36:35,465:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9551 (1.8758)
+2022-11-18 15:36:35,602:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8111 (1.8690)
+2022-11-18 15:36:36,000:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9047 (1.9047)
+2022-11-18 15:36:36,156:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8912 (1.8976)
+2022-11-18 15:36:36,310:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9227 (1.9063)
+2022-11-18 15:36:36,469:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9049 (1.9060)
+2022-11-18 15:36:36,626:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9307 (1.9106)
+2022-11-18 15:36:36,782:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8279 (1.8975)
+2022-11-18 15:36:36,937:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8494 (1.8901)
+2022-11-18 15:36:37,090:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9120 (1.8928)
+2022-11-18 15:36:37,243:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8057 (1.8839)
+2022-11-18 15:36:37,395:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8561 (1.8810)
+2022-11-18 15:36:37,550:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8345 (1.8766)
+2022-11-18 15:36:37,702:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8157 (1.8716)
+2022-11-18 15:36:37,856:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8339 (1.8687)
+2022-11-18 15:36:38,013:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8689 (1.8687)
+2022-11-18 15:36:38,174:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9401 (1.8738)
+2022-11-18 15:36:38,336:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9691 (1.8796)
+2022-11-18 15:36:38,494:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8538 (1.8780)
+2022-11-18 15:36:38,636:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8479 (1.8764)
+2022-11-18 15:36:38,682:INFO: - Computing loss (validation)
+2022-11-18 15:36:38,944:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9289 (1.9289)
+2022-11-18 15:36:38,980:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0027 (1.9342)
+2022-11-18 15:36:39,292:INFO: Dataset: univ                Batch: 1/3	Loss 1.8403 (1.8403)
+2022-11-18 15:36:39,373:INFO: Dataset: univ                Batch: 2/3	Loss 1.8388 (1.8396)
+2022-11-18 15:36:39,448:INFO: Dataset: univ                Batch: 3/3	Loss 1.8771 (1.8497)
+2022-11-18 15:36:39,750:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7918 (1.7918)
+2022-11-18 15:36:39,795:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9466 (1.8321)
+2022-11-18 15:36:40,103:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8820 (1.8820)
+2022-11-18 15:36:40,180:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8017 (1.8413)
+2022-11-18 15:36:40,257:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9116 (1.8649)
+2022-11-18 15:36:40,332:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8424 (1.8592)
+2022-11-18 15:36:40,406:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8882 (1.8649)
+2022-11-18 15:36:40,459:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_480.pth.tar
+2022-11-18 15:36:40,460:INFO: 
+===> EPOCH: 481 (P2)
+2022-11-18 15:36:40,460:INFO: - Computing loss (training)
+2022-11-18 15:36:40,810:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8730 (1.8730)
+2022-11-18 15:36:40,978:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8405 (1.8570)
+2022-11-18 15:36:41,147:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8527 (1.8556)
+2022-11-18 15:36:41,281:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0506 (1.8878)
+2022-11-18 15:36:41,716:INFO: Dataset: univ                Batch:  1/15	Loss 1.9408 (1.9408)
+2022-11-18 15:36:41,884:INFO: Dataset: univ                Batch:  2/15	Loss 1.8342 (1.8854)
+2022-11-18 15:36:42,047:INFO: Dataset: univ                Batch:  3/15	Loss 1.8169 (1.8617)
+2022-11-18 15:36:42,207:INFO: Dataset: univ                Batch:  4/15	Loss 1.8603 (1.8614)
+2022-11-18 15:36:42,370:INFO: Dataset: univ                Batch:  5/15	Loss 1.8551 (1.8602)
+2022-11-18 15:36:42,544:INFO: Dataset: univ                Batch:  6/15	Loss 1.9004 (1.8667)
+2022-11-18 15:36:42,712:INFO: Dataset: univ                Batch:  7/15	Loss 1.8545 (1.8649)
+2022-11-18 15:36:42,879:INFO: Dataset: univ                Batch:  8/15	Loss 1.8347 (1.8615)
+2022-11-18 15:36:43,048:INFO: Dataset: univ                Batch:  9/15	Loss 1.8279 (1.8577)
+2022-11-18 15:36:43,218:INFO: Dataset: univ                Batch: 10/15	Loss 1.8667 (1.8587)
+2022-11-18 15:36:43,392:INFO: Dataset: univ                Batch: 11/15	Loss 1.8382 (1.8570)
+2022-11-18 15:36:43,563:INFO: Dataset: univ                Batch: 12/15	Loss 1.8353 (1.8552)
+2022-11-18 15:36:43,732:INFO: Dataset: univ                Batch: 13/15	Loss 1.8480 (1.8546)
+2022-11-18 15:36:43,902:INFO: Dataset: univ                Batch: 14/15	Loss 1.8551 (1.8546)
+2022-11-18 15:36:43,998:INFO: Dataset: univ                Batch: 15/15	Loss 1.9284 (1.8557)
+2022-11-18 15:36:44,410:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7612 (1.7612)
+2022-11-18 15:36:44,562:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9460 (1.8465)
+2022-11-18 15:36:44,720:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0536 (1.9137)
+2022-11-18 15:36:44,887:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8904 (1.9082)
+2022-11-18 15:36:45,055:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8765 (1.9013)
+2022-11-18 15:36:45,210:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0019 (1.9192)
+2022-11-18 15:36:45,361:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8234 (1.9049)
+2022-11-18 15:36:45,498:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7389 (1.8878)
+2022-11-18 15:36:45,916:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9050 (1.9050)
+2022-11-18 15:36:46,078:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8523 (1.8791)
+2022-11-18 15:36:46,248:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8618 (1.8732)
+2022-11-18 15:36:46,398:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8296 (1.8628)
+2022-11-18 15:36:46,551:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8883 (1.8681)
+2022-11-18 15:36:46,706:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9208 (1.8772)
+2022-11-18 15:36:46,858:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9512 (1.8880)
+2022-11-18 15:36:47,019:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8735 (1.8861)
+2022-11-18 15:36:47,179:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7772 (1.8726)
+2022-11-18 15:36:47,339:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8181 (1.8662)
+2022-11-18 15:36:47,497:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8626 (1.8659)
+2022-11-18 15:36:47,647:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8607 (1.8654)
+2022-11-18 15:36:47,800:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7883 (1.8593)
+2022-11-18 15:36:47,952:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9481 (1.8654)
+2022-11-18 15:36:48,106:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8874 (1.8669)
+2022-11-18 15:36:48,261:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8644 (1.8668)
+2022-11-18 15:36:48,416:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7841 (1.8615)
+2022-11-18 15:36:48,558:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8526 (1.8611)
+2022-11-18 15:36:48,610:INFO: - Computing loss (validation)
+2022-11-18 15:36:48,863:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9695 (1.9695)
+2022-11-18 15:36:48,898:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9744 (1.9698)
+2022-11-18 15:36:49,215:INFO: Dataset: univ                Batch: 1/3	Loss 1.8737 (1.8737)
+2022-11-18 15:36:49,300:INFO: Dataset: univ                Batch: 2/3	Loss 1.8545 (1.8645)
+2022-11-18 15:36:49,379:INFO: Dataset: univ                Batch: 3/3	Loss 1.8530 (1.8607)
+2022-11-18 15:36:49,707:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9102 (1.9102)
+2022-11-18 15:36:49,757:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8087 (1.8838)
+2022-11-18 15:36:50,061:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8348 (1.8348)
+2022-11-18 15:36:50,139:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8480 (1.8413)
+2022-11-18 15:36:50,213:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8807 (1.8543)
+2022-11-18 15:36:50,288:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8854 (1.8623)
+2022-11-18 15:36:50,362:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8743 (1.8646)
+2022-11-18 15:36:50,414:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_481.pth.tar
+2022-11-18 15:36:50,414:INFO: 
+===> EPOCH: 482 (P2)
+2022-11-18 15:36:50,415:INFO: - Computing loss (training)
+2022-11-18 15:36:50,759:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8949 (1.8949)
+2022-11-18 15:36:50,909:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0057 (1.9508)
+2022-11-18 15:36:51,060:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9270 (1.9424)
+2022-11-18 15:36:51,176:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8806 (1.9329)
+2022-11-18 15:36:51,587:INFO: Dataset: univ                Batch:  1/15	Loss 1.8806 (1.8806)
+2022-11-18 15:36:51,752:INFO: Dataset: univ                Batch:  2/15	Loss 1.8617 (1.8704)
+2022-11-18 15:36:51,917:INFO: Dataset: univ                Batch:  3/15	Loss 1.8870 (1.8762)
+2022-11-18 15:36:52,087:INFO: Dataset: univ                Batch:  4/15	Loss 1.9152 (1.8853)
+2022-11-18 15:36:52,253:INFO: Dataset: univ                Batch:  5/15	Loss 1.8716 (1.8825)
+2022-11-18 15:36:52,418:INFO: Dataset: univ                Batch:  6/15	Loss 1.8512 (1.8772)
+2022-11-18 15:36:52,590:INFO: Dataset: univ                Batch:  7/15	Loss 1.8585 (1.8745)
+2022-11-18 15:36:52,766:INFO: Dataset: univ                Batch:  8/15	Loss 1.8605 (1.8724)
+2022-11-18 15:36:52,940:INFO: Dataset: univ                Batch:  9/15	Loss 1.8305 (1.8680)
+2022-11-18 15:36:53,109:INFO: Dataset: univ                Batch: 10/15	Loss 1.8570 (1.8669)
+2022-11-18 15:36:53,277:INFO: Dataset: univ                Batch: 11/15	Loss 1.8304 (1.8637)
+2022-11-18 15:36:53,442:INFO: Dataset: univ                Batch: 12/15	Loss 1.8673 (1.8640)
+2022-11-18 15:36:53,606:INFO: Dataset: univ                Batch: 13/15	Loss 1.8635 (1.8640)
+2022-11-18 15:36:53,770:INFO: Dataset: univ                Batch: 14/15	Loss 1.8472 (1.8628)
+2022-11-18 15:36:53,857:INFO: Dataset: univ                Batch: 15/15	Loss 1.7828 (1.8619)
+2022-11-18 15:36:54,250:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8257 (1.8257)
+2022-11-18 15:36:54,406:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8535 (1.8392)
+2022-11-18 15:36:54,564:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8668 (1.8490)
+2022-11-18 15:36:54,725:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9658 (1.8778)
+2022-11-18 15:36:54,879:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0023 (1.9012)
+2022-11-18 15:36:55,033:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8447 (1.8925)
+2022-11-18 15:36:55,184:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9178 (1.8959)
+2022-11-18 15:36:55,321:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7876 (1.8849)
+2022-11-18 15:36:55,732:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8905 (1.8905)
+2022-11-18 15:36:55,887:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8585 (1.8750)
+2022-11-18 15:36:56,050:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8932 (1.8809)
+2022-11-18 15:36:56,217:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8360 (1.8695)
+2022-11-18 15:36:56,382:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8691 (1.8694)
+2022-11-18 15:36:56,541:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8748 (1.8703)
+2022-11-18 15:36:56,694:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9460 (1.8823)
+2022-11-18 15:36:56,847:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8398 (1.8773)
+2022-11-18 15:36:57,001:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7915 (1.8685)
+2022-11-18 15:36:57,153:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8846 (1.8701)
+2022-11-18 15:36:57,312:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7385 (1.8583)
+2022-11-18 15:36:57,468:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9283 (1.8637)
+2022-11-18 15:36:57,635:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8557 (1.8631)
+2022-11-18 15:36:57,794:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8175 (1.8601)
+2022-11-18 15:36:57,951:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8848 (1.8618)
+2022-11-18 15:36:58,107:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7594 (1.8557)
+2022-11-18 15:36:58,265:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9006 (1.8583)
+2022-11-18 15:36:58,491:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8128 (1.8561)
+2022-11-18 15:36:58,541:INFO: - Computing loss (validation)
+2022-11-18 15:36:58,825:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8593 (1.8593)
+2022-11-18 15:36:58,864:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6852 (1.8486)
+2022-11-18 15:36:59,174:INFO: Dataset: univ                Batch: 1/3	Loss 1.8589 (1.8589)
+2022-11-18 15:36:59,252:INFO: Dataset: univ                Batch: 2/3	Loss 1.9355 (1.9004)
+2022-11-18 15:36:59,325:INFO: Dataset: univ                Batch: 3/3	Loss 1.8750 (1.8929)
+2022-11-18 15:36:59,648:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8147 (1.8147)
+2022-11-18 15:36:59,701:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9208 (1.8400)
+2022-11-18 15:37:00,038:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7463 (1.7463)
+2022-11-18 15:37:00,122:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8306 (1.7910)
+2022-11-18 15:37:00,205:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8790 (1.8208)
+2022-11-18 15:37:00,289:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8423 (1.8259)
+2022-11-18 15:37:00,371:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8645 (1.8338)
+2022-11-18 15:37:00,430:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_482.pth.tar
+2022-11-18 15:37:00,430:INFO: 
+===> EPOCH: 483 (P2)
+2022-11-18 15:37:00,430:INFO: - Computing loss (training)
+2022-11-18 15:37:00,803:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8982 (1.8982)
+2022-11-18 15:37:00,973:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8518 (1.8760)
+2022-11-18 15:37:01,138:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9641 (1.9037)
+2022-11-18 15:37:01,264:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8368 (1.8929)
+2022-11-18 15:37:01,733:INFO: Dataset: univ                Batch:  1/15	Loss 1.8873 (1.8873)
+2022-11-18 15:37:01,892:INFO: Dataset: univ                Batch:  2/15	Loss 1.8991 (1.8942)
+2022-11-18 15:37:02,049:INFO: Dataset: univ                Batch:  3/15	Loss 1.8653 (1.8841)
+2022-11-18 15:37:02,204:INFO: Dataset: univ                Batch:  4/15	Loss 1.8977 (1.8875)
+2022-11-18 15:37:02,362:INFO: Dataset: univ                Batch:  5/15	Loss 1.8891 (1.8878)
+2022-11-18 15:37:02,518:INFO: Dataset: univ                Batch:  6/15	Loss 1.8269 (1.8776)
+2022-11-18 15:37:02,681:INFO: Dataset: univ                Batch:  7/15	Loss 1.9143 (1.8820)
+2022-11-18 15:37:02,845:INFO: Dataset: univ                Batch:  8/15	Loss 1.8530 (1.8781)
+2022-11-18 15:37:03,010:INFO: Dataset: univ                Batch:  9/15	Loss 1.8713 (1.8774)
+2022-11-18 15:37:03,169:INFO: Dataset: univ                Batch: 10/15	Loss 1.8595 (1.8756)
+2022-11-18 15:37:03,330:INFO: Dataset: univ                Batch: 11/15	Loss 1.8480 (1.8732)
+2022-11-18 15:37:03,487:INFO: Dataset: univ                Batch: 12/15	Loss 1.8519 (1.8715)
+2022-11-18 15:37:03,645:INFO: Dataset: univ                Batch: 13/15	Loss 1.8545 (1.8703)
+2022-11-18 15:37:03,802:INFO: Dataset: univ                Batch: 14/15	Loss 1.8505 (1.8688)
+2022-11-18 15:37:03,888:INFO: Dataset: univ                Batch: 15/15	Loss 1.8290 (1.8682)
+2022-11-18 15:37:04,267:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7726 (1.7726)
+2022-11-18 15:37:04,425:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9075 (1.8432)
+2022-11-18 15:37:04,577:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0036 (1.9014)
+2022-11-18 15:37:04,727:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8560 (1.8903)
+2022-11-18 15:37:04,880:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9007 (1.8922)
+2022-11-18 15:37:05,032:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9441 (1.9010)
+2022-11-18 15:37:05,183:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8827 (1.8983)
+2022-11-18 15:37:05,322:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9974 (1.9082)
+2022-11-18 15:37:05,738:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8241 (1.8241)
+2022-11-18 15:37:05,905:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8025 (1.8137)
+2022-11-18 15:37:06,080:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9262 (1.8496)
+2022-11-18 15:37:06,233:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8876 (1.8584)
+2022-11-18 15:37:06,399:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8636 (1.8594)
+2022-11-18 15:37:06,566:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9345 (1.8715)
+2022-11-18 15:37:06,716:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7888 (1.8615)
+2022-11-18 15:37:06,878:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8334 (1.8577)
+2022-11-18 15:37:07,042:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8637 (1.8582)
+2022-11-18 15:37:07,191:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8696 (1.8594)
+2022-11-18 15:37:07,350:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8700 (1.8604)
+2022-11-18 15:37:07,517:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6992 (1.8469)
+2022-11-18 15:37:07,668:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8204 (1.8450)
+2022-11-18 15:37:07,820:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7687 (1.8401)
+2022-11-18 15:37:07,976:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8830 (1.8430)
+2022-11-18 15:37:08,138:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8287 (1.8421)
+2022-11-18 15:37:08,299:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8512 (1.8426)
+2022-11-18 15:37:08,444:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9112 (1.8457)
+2022-11-18 15:37:08,490:INFO: - Computing loss (validation)
+2022-11-18 15:37:08,757:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9939 (1.9939)
+2022-11-18 15:37:08,791:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0836 (2.0000)
+2022-11-18 15:37:09,112:INFO: Dataset: univ                Batch: 1/3	Loss 1.8219 (1.8219)
+2022-11-18 15:37:09,197:INFO: Dataset: univ                Batch: 2/3	Loss 1.8945 (1.8604)
+2022-11-18 15:37:09,276:INFO: Dataset: univ                Batch: 3/3	Loss 1.8985 (1.8709)
+2022-11-18 15:37:09,593:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9586 (1.9586)
+2022-11-18 15:37:09,639:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0846 (1.9874)
+2022-11-18 15:37:09,989:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8821 (1.8821)
+2022-11-18 15:37:10,078:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8004 (1.8425)
+2022-11-18 15:37:10,159:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8122 (1.8326)
+2022-11-18 15:37:10,238:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8882 (1.8469)
+2022-11-18 15:37:10,313:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8957 (1.8570)
+2022-11-18 15:37:10,369:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_483.pth.tar
+2022-11-18 15:37:10,369:INFO: 
+===> EPOCH: 484 (P2)
+2022-11-18 15:37:10,370:INFO: - Computing loss (training)
+2022-11-18 15:37:10,741:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8690 (1.8690)
+2022-11-18 15:37:10,903:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7458 (1.8117)
+2022-11-18 15:37:11,065:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8079 (1.8105)
+2022-11-18 15:37:11,194:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8076 (1.8100)
+2022-11-18 15:37:11,629:INFO: Dataset: univ                Batch:  1/15	Loss 1.8679 (1.8679)
+2022-11-18 15:37:11,786:INFO: Dataset: univ                Batch:  2/15	Loss 1.8635 (1.8657)
+2022-11-18 15:37:11,943:INFO: Dataset: univ                Batch:  3/15	Loss 1.8893 (1.8726)
+2022-11-18 15:37:12,096:INFO: Dataset: univ                Batch:  4/15	Loss 1.8834 (1.8753)
+2022-11-18 15:37:12,254:INFO: Dataset: univ                Batch:  5/15	Loss 1.8077 (1.8622)
+2022-11-18 15:37:12,419:INFO: Dataset: univ                Batch:  6/15	Loss 1.8585 (1.8615)
+2022-11-18 15:37:12,582:INFO: Dataset: univ                Batch:  7/15	Loss 1.8442 (1.8589)
+2022-11-18 15:37:12,745:INFO: Dataset: univ                Batch:  8/15	Loss 1.8607 (1.8591)
+2022-11-18 15:37:12,904:INFO: Dataset: univ                Batch:  9/15	Loss 1.8719 (1.8606)
+2022-11-18 15:37:13,060:INFO: Dataset: univ                Batch: 10/15	Loss 1.7986 (1.8547)
+2022-11-18 15:37:13,221:INFO: Dataset: univ                Batch: 11/15	Loss 1.8455 (1.8538)
+2022-11-18 15:37:13,380:INFO: Dataset: univ                Batch: 12/15	Loss 1.8092 (1.8501)
+2022-11-18 15:37:13,540:INFO: Dataset: univ                Batch: 13/15	Loss 1.8469 (1.8499)
+2022-11-18 15:37:13,705:INFO: Dataset: univ                Batch: 14/15	Loss 1.8778 (1.8519)
+2022-11-18 15:37:13,794:INFO: Dataset: univ                Batch: 15/15	Loss 1.8863 (1.8525)
+2022-11-18 15:37:14,191:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9638 (1.9638)
+2022-11-18 15:37:14,347:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8833 (1.9283)
+2022-11-18 15:37:14,499:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8003 (1.8837)
+2022-11-18 15:37:14,649:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7533 (1.8500)
+2022-11-18 15:37:14,808:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8709 (1.8542)
+2022-11-18 15:37:14,979:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9179 (1.8653)
+2022-11-18 15:37:15,145:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7517 (1.8496)
+2022-11-18 15:37:15,290:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9586 (1.8618)
+2022-11-18 15:37:15,763:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8534 (1.8534)
+2022-11-18 15:37:15,942:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7814 (1.8169)
+2022-11-18 15:37:16,103:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9976 (1.8813)
+2022-11-18 15:37:16,263:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8789 (1.8807)
+2022-11-18 15:37:16,431:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8067 (1.8644)
+2022-11-18 15:37:16,584:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9222 (1.8746)
+2022-11-18 15:37:16,740:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7593 (1.8578)
+2022-11-18 15:37:16,897:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8219 (1.8531)
+2022-11-18 15:37:17,051:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8379 (1.8515)
+2022-11-18 15:37:17,207:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9123 (1.8576)
+2022-11-18 15:37:17,368:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8192 (1.8538)
+2022-11-18 15:37:17,530:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8160 (1.8504)
+2022-11-18 15:37:17,704:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9545 (1.8589)
+2022-11-18 15:37:17,867:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9309 (1.8634)
+2022-11-18 15:37:18,042:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8011 (1.8595)
+2022-11-18 15:37:18,205:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8338 (1.8581)
+2022-11-18 15:37:18,366:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7906 (1.8538)
+2022-11-18 15:37:18,517:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8824 (1.8554)
+2022-11-18 15:37:18,565:INFO: - Computing loss (validation)
+2022-11-18 15:37:18,851:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7670 (1.7670)
+2022-11-18 15:37:18,889:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4788 (1.7483)
+2022-11-18 15:37:19,234:INFO: Dataset: univ                Batch: 1/3	Loss 1.8537 (1.8537)
+2022-11-18 15:37:19,323:INFO: Dataset: univ                Batch: 2/3	Loss 1.8280 (1.8410)
+2022-11-18 15:37:19,405:INFO: Dataset: univ                Batch: 3/3	Loss 1.8775 (1.8516)
+2022-11-18 15:37:19,762:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9247 (1.9247)
+2022-11-18 15:37:19,807:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9402 (1.9290)
+2022-11-18 15:37:20,130:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8546 (1.8546)
+2022-11-18 15:37:20,208:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8615 (1.8582)
+2022-11-18 15:37:20,287:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7968 (1.8373)
+2022-11-18 15:37:20,365:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8068 (1.8297)
+2022-11-18 15:37:20,441:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9385 (1.8507)
+2022-11-18 15:37:20,501:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_484.pth.tar
+2022-11-18 15:37:20,501:INFO: 
+===> EPOCH: 485 (P2)
+2022-11-18 15:37:20,502:INFO: - Computing loss (training)
+2022-11-18 15:37:20,846:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8854 (1.8854)
+2022-11-18 15:37:20,998:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9678 (1.9282)
+2022-11-18 15:37:21,151:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8791 (1.9116)
+2022-11-18 15:37:21,267:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9336 (1.9155)
+2022-11-18 15:37:21,723:INFO: Dataset: univ                Batch:  1/15	Loss 1.9097 (1.9097)
+2022-11-18 15:37:21,884:INFO: Dataset: univ                Batch:  2/15	Loss 1.8748 (1.8918)
+2022-11-18 15:37:22,043:INFO: Dataset: univ                Batch:  3/15	Loss 1.8971 (1.8937)
+2022-11-18 15:37:22,202:INFO: Dataset: univ                Batch:  4/15	Loss 1.8522 (1.8831)
+2022-11-18 15:37:22,360:INFO: Dataset: univ                Batch:  5/15	Loss 1.8288 (1.8731)
+2022-11-18 15:37:22,520:INFO: Dataset: univ                Batch:  6/15	Loss 1.8622 (1.8714)
+2022-11-18 15:37:22,686:INFO: Dataset: univ                Batch:  7/15	Loss 1.8850 (1.8733)
+2022-11-18 15:37:22,850:INFO: Dataset: univ                Batch:  8/15	Loss 1.8895 (1.8752)
+2022-11-18 15:37:23,016:INFO: Dataset: univ                Batch:  9/15	Loss 1.8648 (1.8741)
+2022-11-18 15:37:23,174:INFO: Dataset: univ                Batch: 10/15	Loss 1.8518 (1.8717)
+2022-11-18 15:37:23,332:INFO: Dataset: univ                Batch: 11/15	Loss 1.8574 (1.8703)
+2022-11-18 15:37:23,492:INFO: Dataset: univ                Batch: 12/15	Loss 1.8579 (1.8693)
+2022-11-18 15:37:23,659:INFO: Dataset: univ                Batch: 13/15	Loss 1.8558 (1.8683)
+2022-11-18 15:37:23,821:INFO: Dataset: univ                Batch: 14/15	Loss 1.8541 (1.8672)
+2022-11-18 15:37:23,907:INFO: Dataset: univ                Batch: 15/15	Loss 1.8335 (1.8667)
+2022-11-18 15:37:24,306:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8529 (1.8529)
+2022-11-18 15:37:24,459:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7911 (1.8229)
+2022-11-18 15:37:24,614:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9115 (1.8528)
+2022-11-18 15:37:24,782:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9023 (1.8646)
+2022-11-18 15:37:24,962:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8510 (1.8620)
+2022-11-18 15:37:25,128:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9433 (1.8771)
+2022-11-18 15:37:25,288:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8551 (1.8739)
+2022-11-18 15:37:25,434:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9008 (1.8768)
+2022-11-18 15:37:25,843:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9057 (1.9057)
+2022-11-18 15:37:25,999:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8583 (1.8804)
+2022-11-18 15:37:26,155:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8760 (1.8789)
+2022-11-18 15:37:26,311:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8110 (1.8619)
+2022-11-18 15:37:26,469:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7549 (1.8404)
+2022-11-18 15:37:26,633:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8546 (1.8429)
+2022-11-18 15:37:26,786:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9427 (1.8565)
+2022-11-18 15:37:26,937:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8450 (1.8552)
+2022-11-18 15:37:27,090:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8909 (1.8590)
+2022-11-18 15:37:27,244:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8234 (1.8555)
+2022-11-18 15:37:27,399:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8407 (1.8542)
+2022-11-18 15:37:27,560:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8208 (1.8514)
+2022-11-18 15:37:27,722:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8235 (1.8492)
+2022-11-18 15:37:27,877:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7637 (1.8428)
+2022-11-18 15:37:28,030:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9501 (1.8504)
+2022-11-18 15:37:28,180:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8101 (1.8477)
+2022-11-18 15:37:28,333:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9241 (1.8526)
+2022-11-18 15:37:28,475:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8670 (1.8532)
+2022-11-18 15:37:28,522:INFO: - Computing loss (validation)
+2022-11-18 15:37:28,783:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8159 (1.8159)
+2022-11-18 15:37:28,817:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7079 (1.8082)
+2022-11-18 15:37:29,126:INFO: Dataset: univ                Batch: 1/3	Loss 1.8333 (1.8333)
+2022-11-18 15:37:29,203:INFO: Dataset: univ                Batch: 2/3	Loss 1.8289 (1.8311)
+2022-11-18 15:37:29,275:INFO: Dataset: univ                Batch: 3/3	Loss 1.8496 (1.8365)
+2022-11-18 15:37:29,571:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9880 (1.9880)
+2022-11-18 15:37:29,619:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8532 (1.9568)
+2022-11-18 15:37:29,920:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8120 (1.8120)
+2022-11-18 15:37:29,995:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9105 (1.8585)
+2022-11-18 15:37:30,072:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8273 (1.8486)
+2022-11-18 15:37:30,151:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7907 (1.8337)
+2022-11-18 15:37:30,226:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8868 (1.8448)
+2022-11-18 15:37:30,285:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_485.pth.tar
+2022-11-18 15:37:30,285:INFO: 
+===> EPOCH: 486 (P2)
+2022-11-18 15:37:30,285:INFO: - Computing loss (training)
+2022-11-18 15:37:30,650:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9096 (1.9096)
+2022-11-18 15:37:30,816:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8623 (1.8864)
+2022-11-18 15:37:30,982:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9706 (1.9148)
+2022-11-18 15:37:31,131:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7457 (1.8865)
+2022-11-18 15:37:31,578:INFO: Dataset: univ                Batch:  1/15	Loss 1.8585 (1.8585)
+2022-11-18 15:37:31,754:INFO: Dataset: univ                Batch:  2/15	Loss 1.8650 (1.8619)
+2022-11-18 15:37:31,934:INFO: Dataset: univ                Batch:  3/15	Loss 1.8222 (1.8483)
+2022-11-18 15:37:32,094:INFO: Dataset: univ                Batch:  4/15	Loss 1.7965 (1.8370)
+2022-11-18 15:37:32,265:INFO: Dataset: univ                Batch:  5/15	Loss 1.8718 (1.8442)
+2022-11-18 15:37:32,441:INFO: Dataset: univ                Batch:  6/15	Loss 1.8338 (1.8424)
+2022-11-18 15:37:32,615:INFO: Dataset: univ                Batch:  7/15	Loss 1.8518 (1.8438)
+2022-11-18 15:37:32,777:INFO: Dataset: univ                Batch:  8/15	Loss 1.8823 (1.8490)
+2022-11-18 15:37:32,983:INFO: Dataset: univ                Batch:  9/15	Loss 1.8304 (1.8468)
+2022-11-18 15:37:33,168:INFO: Dataset: univ                Batch: 10/15	Loss 1.8262 (1.8450)
+2022-11-18 15:37:33,348:INFO: Dataset: univ                Batch: 11/15	Loss 1.8796 (1.8479)
+2022-11-18 15:37:33,524:INFO: Dataset: univ                Batch: 12/15	Loss 1.8762 (1.8506)
+2022-11-18 15:37:33,690:INFO: Dataset: univ                Batch: 13/15	Loss 1.8581 (1.8511)
+2022-11-18 15:37:33,854:INFO: Dataset: univ                Batch: 14/15	Loss 1.8595 (1.8518)
+2022-11-18 15:37:33,943:INFO: Dataset: univ                Batch: 15/15	Loss 1.8246 (1.8514)
+2022-11-18 15:37:34,352:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9169 (1.9169)
+2022-11-18 15:37:34,515:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9891 (1.9526)
+2022-11-18 15:37:34,681:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9165 (1.9404)
+2022-11-18 15:37:34,843:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8978 (1.9300)
+2022-11-18 15:37:35,000:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7879 (1.9041)
+2022-11-18 15:37:35,170:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9821 (1.9176)
+2022-11-18 15:37:35,341:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8543 (1.9082)
+2022-11-18 15:37:35,485:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0347 (1.9232)
+2022-11-18 15:37:35,932:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9163 (1.9163)
+2022-11-18 15:37:36,130:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9693 (1.9437)
+2022-11-18 15:37:36,339:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8954 (1.9279)
+2022-11-18 15:37:36,529:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7363 (1.8817)
+2022-11-18 15:37:36,710:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8703 (1.8792)
+2022-11-18 15:37:36,918:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8731 (1.8782)
+2022-11-18 15:37:37,093:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8314 (1.8719)
+2022-11-18 15:37:37,247:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7865 (1.8609)
+2022-11-18 15:37:37,399:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8613 (1.8609)
+2022-11-18 15:37:37,548:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8841 (1.8634)
+2022-11-18 15:37:37,696:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8774 (1.8646)
+2022-11-18 15:37:37,855:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9570 (1.8722)
+2022-11-18 15:37:38,013:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7661 (1.8641)
+2022-11-18 15:37:38,172:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8678 (1.8643)
+2022-11-18 15:37:38,336:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8556 (1.8638)
+2022-11-18 15:37:38,488:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8713 (1.8642)
+2022-11-18 15:37:38,637:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7673 (1.8584)
+2022-11-18 15:37:38,777:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8795 (1.8595)
+2022-11-18 15:37:38,823:INFO: - Computing loss (validation)
+2022-11-18 15:37:39,079:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8742 (1.8742)
+2022-11-18 15:37:39,114:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7660 (1.8664)
+2022-11-18 15:37:39,455:INFO: Dataset: univ                Batch: 1/3	Loss 1.8320 (1.8320)
+2022-11-18 15:37:39,543:INFO: Dataset: univ                Batch: 2/3	Loss 1.8650 (1.8480)
+2022-11-18 15:37:39,623:INFO: Dataset: univ                Batch: 3/3	Loss 1.8747 (1.8565)
+2022-11-18 15:37:39,934:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8770 (1.8770)
+2022-11-18 15:37:39,979:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7205 (1.8428)
+2022-11-18 15:37:40,308:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8840 (1.8840)
+2022-11-18 15:37:40,391:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7488 (1.8200)
+2022-11-18 15:37:40,475:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8542 (1.8307)
+2022-11-18 15:37:40,561:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8442 (1.8342)
+2022-11-18 15:37:40,640:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8329 (1.8339)
+2022-11-18 15:37:40,697:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_486.pth.tar
+2022-11-18 15:37:40,697:INFO: 
+===> EPOCH: 487 (P2)
+2022-11-18 15:37:40,698:INFO: - Computing loss (training)
+2022-11-18 15:37:41,055:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9202 (1.9202)
+2022-11-18 15:37:41,216:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9404 (1.9304)
+2022-11-18 15:37:41,383:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0422 (1.9675)
+2022-11-18 15:37:41,501:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0121 (1.9752)
+2022-11-18 15:37:41,896:INFO: Dataset: univ                Batch:  1/15	Loss 1.8850 (1.8850)
+2022-11-18 15:37:42,056:INFO: Dataset: univ                Batch:  2/15	Loss 1.8469 (1.8662)
+2022-11-18 15:37:42,223:INFO: Dataset: univ                Batch:  3/15	Loss 1.8148 (1.8490)
+2022-11-18 15:37:42,398:INFO: Dataset: univ                Batch:  4/15	Loss 1.8714 (1.8545)
+2022-11-18 15:37:42,573:INFO: Dataset: univ                Batch:  5/15	Loss 1.8542 (1.8544)
+2022-11-18 15:37:42,737:INFO: Dataset: univ                Batch:  6/15	Loss 1.8240 (1.8492)
+2022-11-18 15:37:42,894:INFO: Dataset: univ                Batch:  7/15	Loss 1.8382 (1.8477)
+2022-11-18 15:37:43,053:INFO: Dataset: univ                Batch:  8/15	Loss 1.8998 (1.8543)
+2022-11-18 15:37:43,220:INFO: Dataset: univ                Batch:  9/15	Loss 1.8777 (1.8570)
+2022-11-18 15:37:43,388:INFO: Dataset: univ                Batch: 10/15	Loss 1.8999 (1.8618)
+2022-11-18 15:37:43,557:INFO: Dataset: univ                Batch: 11/15	Loss 1.8483 (1.8606)
+2022-11-18 15:37:43,728:INFO: Dataset: univ                Batch: 12/15	Loss 1.8794 (1.8623)
+2022-11-18 15:37:43,889:INFO: Dataset: univ                Batch: 13/15	Loss 1.8200 (1.8594)
+2022-11-18 15:37:44,048:INFO: Dataset: univ                Batch: 14/15	Loss 1.9166 (1.8634)
+2022-11-18 15:37:44,134:INFO: Dataset: univ                Batch: 15/15	Loss 1.8598 (1.8634)
+2022-11-18 15:37:44,547:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9361 (1.9361)
+2022-11-18 15:37:44,704:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8322 (1.8803)
+2022-11-18 15:37:44,861:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9393 (1.8994)
+2022-11-18 15:37:45,013:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7946 (1.8745)
+2022-11-18 15:37:45,167:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8277 (1.8660)
+2022-11-18 15:37:45,337:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8429 (1.8621)
+2022-11-18 15:37:45,506:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7508 (1.8469)
+2022-11-18 15:37:45,663:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9838 (1.8630)
+2022-11-18 15:37:46,090:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8104 (1.8104)
+2022-11-18 15:37:46,255:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7399 (1.7772)
+2022-11-18 15:37:46,428:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8193 (1.7914)
+2022-11-18 15:37:46,593:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8573 (1.8074)
+2022-11-18 15:37:46,747:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8025 (1.8064)
+2022-11-18 15:37:46,903:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8740 (1.8184)
+2022-11-18 15:37:47,055:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8499 (1.8230)
+2022-11-18 15:37:47,204:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8706 (1.8294)
+2022-11-18 15:37:47,369:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8909 (1.8359)
+2022-11-18 15:37:47,531:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8699 (1.8392)
+2022-11-18 15:37:47,695:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9125 (1.8451)
+2022-11-18 15:37:47,852:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9462 (1.8539)
+2022-11-18 15:37:48,005:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8653 (1.8549)
+2022-11-18 15:37:48,159:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8457 (1.8542)
+2022-11-18 15:37:48,327:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8328 (1.8528)
+2022-11-18 15:37:48,499:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9182 (1.8568)
+2022-11-18 15:37:48,670:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7442 (1.8507)
+2022-11-18 15:37:48,812:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7825 (1.8476)
+2022-11-18 15:37:48,863:INFO: - Computing loss (validation)
+2022-11-18 15:37:49,135:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8401 (1.8401)
+2022-11-18 15:37:49,169:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1171 (1.8600)
+2022-11-18 15:37:49,479:INFO: Dataset: univ                Batch: 1/3	Loss 1.9304 (1.9304)
+2022-11-18 15:37:49,564:INFO: Dataset: univ                Batch: 2/3	Loss 1.9038 (1.9168)
+2022-11-18 15:37:49,643:INFO: Dataset: univ                Batch: 3/3	Loss 1.8339 (1.8879)
+2022-11-18 15:37:49,961:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8344 (1.8344)
+2022-11-18 15:37:50,006:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8867 (1.8458)
+2022-11-18 15:37:50,326:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9056 (1.9056)
+2022-11-18 15:37:50,404:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8800 (1.8929)
+2022-11-18 15:37:50,480:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8744 (1.8871)
+2022-11-18 15:37:50,556:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8951 (1.8892)
+2022-11-18 15:37:50,632:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9161 (1.8942)
+2022-11-18 15:37:50,685:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_487.pth.tar
+2022-11-18 15:37:50,685:INFO: 
+===> EPOCH: 488 (P2)
+2022-11-18 15:37:50,685:INFO: - Computing loss (training)
+2022-11-18 15:37:51,033:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8780 (1.8780)
+2022-11-18 15:37:51,190:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0071 (1.9418)
+2022-11-18 15:37:51,348:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8514 (1.9135)
+2022-11-18 15:37:51,467:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8516 (1.9038)
+2022-11-18 15:37:51,865:INFO: Dataset: univ                Batch:  1/15	Loss 1.8532 (1.8532)
+2022-11-18 15:37:52,028:INFO: Dataset: univ                Batch:  2/15	Loss 1.8546 (1.8538)
+2022-11-18 15:37:52,190:INFO: Dataset: univ                Batch:  3/15	Loss 1.8570 (1.8549)
+2022-11-18 15:37:52,365:INFO: Dataset: univ                Batch:  4/15	Loss 1.8964 (1.8649)
+2022-11-18 15:37:52,553:INFO: Dataset: univ                Batch:  5/15	Loss 1.8683 (1.8656)
+2022-11-18 15:37:52,721:INFO: Dataset: univ                Batch:  6/15	Loss 1.8523 (1.8635)
+2022-11-18 15:37:52,882:INFO: Dataset: univ                Batch:  7/15	Loss 1.8511 (1.8618)
+2022-11-18 15:37:53,041:INFO: Dataset: univ                Batch:  8/15	Loss 1.8375 (1.8588)
+2022-11-18 15:37:53,200:INFO: Dataset: univ                Batch:  9/15	Loss 1.8372 (1.8564)
+2022-11-18 15:37:53,361:INFO: Dataset: univ                Batch: 10/15	Loss 1.8611 (1.8568)
+2022-11-18 15:37:53,524:INFO: Dataset: univ                Batch: 11/15	Loss 1.8568 (1.8568)
+2022-11-18 15:37:53,684:INFO: Dataset: univ                Batch: 12/15	Loss 1.8970 (1.8602)
+2022-11-18 15:37:53,844:INFO: Dataset: univ                Batch: 13/15	Loss 1.8945 (1.8628)
+2022-11-18 15:37:54,004:INFO: Dataset: univ                Batch: 14/15	Loss 1.8577 (1.8624)
+2022-11-18 15:37:54,088:INFO: Dataset: univ                Batch: 15/15	Loss 1.8482 (1.8622)
+2022-11-18 15:37:54,473:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9381 (1.9381)
+2022-11-18 15:37:54,628:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8286 (1.8800)
+2022-11-18 15:37:54,782:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8141 (1.8601)
+2022-11-18 15:37:54,938:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8995 (1.8699)
+2022-11-18 15:37:55,090:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7723 (1.8494)
+2022-11-18 15:37:55,245:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7890 (1.8399)
+2022-11-18 15:37:55,416:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9578 (1.8559)
+2022-11-18 15:37:55,570:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0329 (1.8739)
+2022-11-18 15:37:55,992:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8884 (1.8884)
+2022-11-18 15:37:56,152:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8261 (1.8573)
+2022-11-18 15:37:56,329:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8974 (1.8713)
+2022-11-18 15:37:56,494:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7546 (1.8397)
+2022-11-18 15:37:56,660:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9155 (1.8553)
+2022-11-18 15:37:56,812:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7928 (1.8447)
+2022-11-18 15:37:56,964:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8195 (1.8408)
+2022-11-18 15:37:57,119:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8689 (1.8445)
+2022-11-18 15:37:57,283:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8563 (1.8458)
+2022-11-18 15:37:57,471:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7851 (1.8393)
+2022-11-18 15:37:57,657:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8971 (1.8447)
+2022-11-18 15:37:57,864:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8910 (1.8485)
+2022-11-18 15:37:58,062:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9528 (1.8561)
+2022-11-18 15:37:58,232:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8540 (1.8560)
+2022-11-18 15:37:58,389:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8488 (1.8555)
+2022-11-18 15:37:58,545:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8197 (1.8534)
+2022-11-18 15:37:58,701:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8802 (1.8551)
+2022-11-18 15:37:58,867:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7488 (1.8494)
+2022-11-18 15:37:58,920:INFO: - Computing loss (validation)
+2022-11-18 15:37:59,241:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8386 (1.8386)
+2022-11-18 15:37:59,285:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7687 (1.8346)
+2022-11-18 15:37:59,672:INFO: Dataset: univ                Batch: 1/3	Loss 1.8358 (1.8358)
+2022-11-18 15:37:59,762:INFO: Dataset: univ                Batch: 2/3	Loss 1.8145 (1.8242)
+2022-11-18 15:37:59,841:INFO: Dataset: univ                Batch: 3/3	Loss 1.9020 (1.8459)
+2022-11-18 15:38:00,145:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8629 (1.8629)
+2022-11-18 15:38:00,196:INFO: Dataset: zara1               Batch: 2/2	Loss 2.1294 (1.9298)
+2022-11-18 15:38:00,542:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8340 (1.8340)
+2022-11-18 15:38:00,622:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8111 (1.8228)
+2022-11-18 15:38:00,701:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8990 (1.8477)
+2022-11-18 15:38:00,777:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8885 (1.8576)
+2022-11-18 15:38:00,851:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7609 (1.8396)
+2022-11-18 15:38:00,904:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_488.pth.tar
+2022-11-18 15:38:00,904:INFO: 
+===> EPOCH: 489 (P2)
+2022-11-18 15:38:00,905:INFO: - Computing loss (training)
+2022-11-18 15:38:01,255:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9260 (1.9260)
+2022-11-18 15:38:01,440:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9396 (1.9331)
+2022-11-18 15:38:01,621:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8225 (1.8965)
+2022-11-18 15:38:01,746:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0073 (1.9154)
+2022-11-18 15:38:02,148:INFO: Dataset: univ                Batch:  1/15	Loss 1.8322 (1.8322)
+2022-11-18 15:38:02,308:INFO: Dataset: univ                Batch:  2/15	Loss 1.8703 (1.8510)
+2022-11-18 15:38:02,471:INFO: Dataset: univ                Batch:  3/15	Loss 1.8855 (1.8616)
+2022-11-18 15:38:02,639:INFO: Dataset: univ                Batch:  4/15	Loss 1.8779 (1.8655)
+2022-11-18 15:38:02,810:INFO: Dataset: univ                Batch:  5/15	Loss 1.8337 (1.8592)
+2022-11-18 15:38:02,988:INFO: Dataset: univ                Batch:  6/15	Loss 1.9038 (1.8663)
+2022-11-18 15:38:03,176:INFO: Dataset: univ                Batch:  7/15	Loss 1.8643 (1.8661)
+2022-11-18 15:38:03,341:INFO: Dataset: univ                Batch:  8/15	Loss 1.8107 (1.8587)
+2022-11-18 15:38:03,500:INFO: Dataset: univ                Batch:  9/15	Loss 1.8468 (1.8574)
+2022-11-18 15:38:03,655:INFO: Dataset: univ                Batch: 10/15	Loss 1.8583 (1.8575)
+2022-11-18 15:38:03,817:INFO: Dataset: univ                Batch: 11/15	Loss 1.8708 (1.8586)
+2022-11-18 15:38:03,979:INFO: Dataset: univ                Batch: 12/15	Loss 1.8644 (1.8591)
+2022-11-18 15:38:04,136:INFO: Dataset: univ                Batch: 13/15	Loss 1.8869 (1.8612)
+2022-11-18 15:38:04,295:INFO: Dataset: univ                Batch: 14/15	Loss 1.8976 (1.8638)
+2022-11-18 15:38:04,380:INFO: Dataset: univ                Batch: 15/15	Loss 1.8686 (1.8639)
+2022-11-18 15:38:04,759:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8248 (1.8248)
+2022-11-18 15:38:04,920:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8691 (1.8460)
+2022-11-18 15:38:05,102:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9247 (1.8722)
+2022-11-18 15:38:05,268:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8926 (1.8773)
+2022-11-18 15:38:05,430:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8994 (1.8817)
+2022-11-18 15:38:05,587:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8818 (1.8817)
+2022-11-18 15:38:05,749:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8929 (1.8834)
+2022-11-18 15:38:05,888:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8990 (1.8853)
+2022-11-18 15:38:06,332:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8523 (1.8523)
+2022-11-18 15:38:06,488:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8304 (1.8416)
+2022-11-18 15:38:06,652:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8944 (1.8579)
+2022-11-18 15:38:06,808:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8302 (1.8510)
+2022-11-18 15:38:06,963:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8159 (1.8439)
+2022-11-18 15:38:07,120:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7646 (1.8306)
+2022-11-18 15:38:07,276:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8809 (1.8378)
+2022-11-18 15:38:07,434:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8907 (1.8446)
+2022-11-18 15:38:07,611:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7821 (1.8367)
+2022-11-18 15:38:07,764:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9548 (1.8485)
+2022-11-18 15:38:07,939:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8164 (1.8458)
+2022-11-18 15:38:08,101:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8532 (1.8463)
+2022-11-18 15:38:08,271:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8124 (1.8436)
+2022-11-18 15:38:08,453:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9015 (1.8481)
+2022-11-18 15:38:08,626:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8127 (1.8458)
+2022-11-18 15:38:08,800:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8532 (1.8463)
+2022-11-18 15:38:08,970:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7938 (1.8432)
+2022-11-18 15:38:09,136:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9069 (1.8465)
+2022-11-18 15:38:09,193:INFO: - Computing loss (validation)
+2022-11-18 15:38:09,465:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8095 (1.8095)
+2022-11-18 15:38:09,499:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8550 (1.8123)
+2022-11-18 15:38:09,828:INFO: Dataset: univ                Batch: 1/3	Loss 1.8503 (1.8503)
+2022-11-18 15:38:09,904:INFO: Dataset: univ                Batch: 2/3	Loss 1.8556 (1.8528)
+2022-11-18 15:38:09,978:INFO: Dataset: univ                Batch: 3/3	Loss 1.8732 (1.8599)
+2022-11-18 15:38:10,325:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7932 (1.7932)
+2022-11-18 15:38:10,373:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9074 (1.8237)
+2022-11-18 15:38:10,692:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7771 (1.7771)
+2022-11-18 15:38:10,769:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8803 (1.8313)
+2022-11-18 15:38:10,845:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7787 (1.8141)
+2022-11-18 15:38:10,923:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9763 (1.8546)
+2022-11-18 15:38:10,996:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8313 (1.8502)
+2022-11-18 15:38:11,052:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_489.pth.tar
+2022-11-18 15:38:11,053:INFO: 
+===> EPOCH: 490 (P2)
+2022-11-18 15:38:11,053:INFO: - Computing loss (training)
+2022-11-18 15:38:11,466:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8590 (1.8590)
+2022-11-18 15:38:11,631:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7965 (1.8268)
+2022-11-18 15:38:11,793:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8750 (1.8426)
+2022-11-18 15:38:11,919:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8763 (1.8483)
+2022-11-18 15:38:12,341:INFO: Dataset: univ                Batch:  1/15	Loss 1.8747 (1.8747)
+2022-11-18 15:38:12,509:INFO: Dataset: univ                Batch:  2/15	Loss 1.8971 (1.8851)
+2022-11-18 15:38:12,694:INFO: Dataset: univ                Batch:  3/15	Loss 1.8160 (1.8619)
+2022-11-18 15:38:12,889:INFO: Dataset: univ                Batch:  4/15	Loss 1.8451 (1.8576)
+2022-11-18 15:38:13,058:INFO: Dataset: univ                Batch:  5/15	Loss 1.8413 (1.8544)
+2022-11-18 15:38:13,218:INFO: Dataset: univ                Batch:  6/15	Loss 1.8261 (1.8500)
+2022-11-18 15:38:13,382:INFO: Dataset: univ                Batch:  7/15	Loss 1.8184 (1.8458)
+2022-11-18 15:38:13,543:INFO: Dataset: univ                Batch:  8/15	Loss 1.8700 (1.8488)
+2022-11-18 15:38:13,706:INFO: Dataset: univ                Batch:  9/15	Loss 1.8165 (1.8450)
+2022-11-18 15:38:13,871:INFO: Dataset: univ                Batch: 10/15	Loss 1.8310 (1.8437)
+2022-11-18 15:38:14,031:INFO: Dataset: univ                Batch: 11/15	Loss 1.8732 (1.8461)
+2022-11-18 15:38:14,195:INFO: Dataset: univ                Batch: 12/15	Loss 1.8904 (1.8502)
+2022-11-18 15:38:14,361:INFO: Dataset: univ                Batch: 13/15	Loss 1.8491 (1.8501)
+2022-11-18 15:38:14,538:INFO: Dataset: univ                Batch: 14/15	Loss 1.8594 (1.8508)
+2022-11-18 15:38:14,626:INFO: Dataset: univ                Batch: 15/15	Loss 2.0223 (1.8532)
+2022-11-18 15:38:15,013:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7249 (1.7249)
+2022-11-18 15:38:15,168:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0139 (1.8738)
+2022-11-18 15:38:15,345:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8813 (1.8765)
+2022-11-18 15:38:15,511:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9544 (1.8970)
+2022-11-18 15:38:15,700:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8715 (1.8923)
+2022-11-18 15:38:15,878:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9102 (1.8953)
+2022-11-18 15:38:16,060:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8818 (1.8934)
+2022-11-18 15:38:16,219:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9561 (1.9005)
+2022-11-18 15:38:16,698:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9241 (1.9241)
+2022-11-18 15:38:16,861:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9603 (1.9424)
+2022-11-18 15:38:17,031:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9509 (1.9453)
+2022-11-18 15:38:17,193:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8364 (1.9179)
+2022-11-18 15:38:17,356:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7959 (1.8928)
+2022-11-18 15:38:17,514:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8586 (1.8868)
+2022-11-18 15:38:17,669:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8639 (1.8836)
+2022-11-18 15:38:17,822:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7380 (1.8645)
+2022-11-18 15:38:17,976:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8049 (1.8577)
+2022-11-18 15:38:18,137:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8204 (1.8539)
+2022-11-18 15:38:18,294:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7835 (1.8476)
+2022-11-18 15:38:18,452:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8623 (1.8488)
+2022-11-18 15:38:18,630:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8339 (1.8475)
+2022-11-18 15:38:18,791:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9201 (1.8526)
+2022-11-18 15:38:18,954:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7709 (1.8469)
+2022-11-18 15:38:19,113:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7148 (1.8384)
+2022-11-18 15:38:19,273:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8109 (1.8367)
+2022-11-18 15:38:19,418:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8276 (1.8363)
+2022-11-18 15:38:19,465:INFO: - Computing loss (validation)
+2022-11-18 15:38:19,743:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8584 (1.8584)
+2022-11-18 15:38:19,778:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9747 (1.8671)
+2022-11-18 15:38:20,097:INFO: Dataset: univ                Batch: 1/3	Loss 1.8614 (1.8614)
+2022-11-18 15:38:20,177:INFO: Dataset: univ                Batch: 2/3	Loss 1.8056 (1.8355)
+2022-11-18 15:38:20,249:INFO: Dataset: univ                Batch: 3/3	Loss 1.8878 (1.8510)
+2022-11-18 15:38:20,547:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9340 (1.9340)
+2022-11-18 15:38:20,594:INFO: Dataset: zara1               Batch: 2/2	Loss 2.1838 (2.0031)
+2022-11-18 15:38:20,947:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8073 (1.8073)
+2022-11-18 15:38:21,026:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8602 (1.8317)
+2022-11-18 15:38:21,120:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8040 (1.8225)
+2022-11-18 15:38:21,219:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8301 (1.8244)
+2022-11-18 15:38:21,304:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9074 (1.8416)
+2022-11-18 15:38:21,366:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_490.pth.tar
+2022-11-18 15:38:21,366:INFO: 
+===> EPOCH: 491 (P2)
+2022-11-18 15:38:21,367:INFO: - Computing loss (training)
+2022-11-18 15:38:21,774:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8942 (1.8942)
+2022-11-18 15:38:21,930:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9693 (1.9298)
+2022-11-18 15:38:22,095:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9510 (1.9371)
+2022-11-18 15:38:22,211:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8516 (1.9216)
+2022-11-18 15:38:22,617:INFO: Dataset: univ                Batch:  1/15	Loss 1.8599 (1.8599)
+2022-11-18 15:38:22,777:INFO: Dataset: univ                Batch:  2/15	Loss 1.8314 (1.8461)
+2022-11-18 15:38:22,942:INFO: Dataset: univ                Batch:  3/15	Loss 1.8735 (1.8554)
+2022-11-18 15:38:23,099:INFO: Dataset: univ                Batch:  4/15	Loss 1.8437 (1.8527)
+2022-11-18 15:38:23,259:INFO: Dataset: univ                Batch:  5/15	Loss 1.8572 (1.8536)
+2022-11-18 15:38:23,417:INFO: Dataset: univ                Batch:  6/15	Loss 1.8773 (1.8572)
+2022-11-18 15:38:23,577:INFO: Dataset: univ                Batch:  7/15	Loss 1.8677 (1.8587)
+2022-11-18 15:38:23,737:INFO: Dataset: univ                Batch:  8/15	Loss 1.8424 (1.8567)
+2022-11-18 15:38:23,894:INFO: Dataset: univ                Batch:  9/15	Loss 1.8710 (1.8583)
+2022-11-18 15:38:24,049:INFO: Dataset: univ                Batch: 10/15	Loss 1.8898 (1.8613)
+2022-11-18 15:38:24,209:INFO: Dataset: univ                Batch: 11/15	Loss 1.8652 (1.8617)
+2022-11-18 15:38:24,367:INFO: Dataset: univ                Batch: 12/15	Loss 1.8343 (1.8593)
+2022-11-18 15:38:24,525:INFO: Dataset: univ                Batch: 13/15	Loss 1.8610 (1.8594)
+2022-11-18 15:38:24,682:INFO: Dataset: univ                Batch: 14/15	Loss 1.8784 (1.8608)
+2022-11-18 15:38:24,765:INFO: Dataset: univ                Batch: 15/15	Loss 1.8610 (1.8608)
+2022-11-18 15:38:25,150:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8607 (1.8607)
+2022-11-18 15:38:25,304:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9194 (1.8905)
+2022-11-18 15:38:25,455:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8262 (1.8706)
+2022-11-18 15:38:25,608:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9080 (1.8793)
+2022-11-18 15:38:25,759:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8783 (1.8791)
+2022-11-18 15:38:25,908:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9513 (1.8905)
+2022-11-18 15:38:26,058:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8852 (1.8899)
+2022-11-18 15:38:26,197:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8772 (1.8884)
+2022-11-18 15:38:26,592:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9043 (1.9043)
+2022-11-18 15:38:26,742:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9465 (1.9259)
+2022-11-18 15:38:26,893:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9113 (1.9215)
+2022-11-18 15:38:27,046:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8740 (1.9092)
+2022-11-18 15:38:27,198:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8383 (1.8951)
+2022-11-18 15:38:27,350:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8571 (1.8885)
+2022-11-18 15:38:27,502:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8442 (1.8826)
+2022-11-18 15:38:27,652:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8097 (1.8732)
+2022-11-18 15:38:27,803:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8656 (1.8724)
+2022-11-18 15:38:27,950:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8948 (1.8748)
+2022-11-18 15:38:28,103:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8616 (1.8737)
+2022-11-18 15:38:28,254:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8201 (1.8694)
+2022-11-18 15:38:28,405:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8698 (1.8694)
+2022-11-18 15:38:28,559:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8210 (1.8659)
+2022-11-18 15:38:28,714:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8925 (1.8678)
+2022-11-18 15:38:28,870:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8784 (1.8685)
+2022-11-18 15:38:29,024:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8430 (1.8671)
+2022-11-18 15:38:29,163:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7942 (1.8632)
+2022-11-18 15:38:29,208:INFO: - Computing loss (validation)
+2022-11-18 15:38:29,469:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8810 (1.8810)
+2022-11-18 15:38:29,504:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7634 (1.8746)
+2022-11-18 15:38:29,814:INFO: Dataset: univ                Batch: 1/3	Loss 1.9669 (1.9669)
+2022-11-18 15:38:29,897:INFO: Dataset: univ                Batch: 2/3	Loss 1.8835 (1.9287)
+2022-11-18 15:38:29,976:INFO: Dataset: univ                Batch: 3/3	Loss 1.7897 (1.8774)
+2022-11-18 15:38:30,286:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8936 (1.8936)
+2022-11-18 15:38:30,337:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9145 (1.8985)
+2022-11-18 15:38:30,653:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8604 (1.8604)
+2022-11-18 15:38:30,729:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7259 (1.7943)
+2022-11-18 15:38:30,806:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8123 (1.8005)
+2022-11-18 15:38:30,881:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9290 (1.8317)
+2022-11-18 15:38:30,956:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7851 (1.8217)
+2022-11-18 15:38:31,011:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_491.pth.tar
+2022-11-18 15:38:31,011:INFO: 
+===> EPOCH: 492 (P2)
+2022-11-18 15:38:31,011:INFO: - Computing loss (training)
+2022-11-18 15:38:31,350:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8581 (1.8581)
+2022-11-18 15:38:31,505:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0669 (1.9662)
+2022-11-18 15:38:31,657:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9505 (1.9609)
+2022-11-18 15:38:31,776:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8368 (1.9416)
+2022-11-18 15:38:32,182:INFO: Dataset: univ                Batch:  1/15	Loss 1.8214 (1.8214)
+2022-11-18 15:38:32,337:INFO: Dataset: univ                Batch:  2/15	Loss 1.8594 (1.8400)
+2022-11-18 15:38:32,498:INFO: Dataset: univ                Batch:  3/15	Loss 1.8348 (1.8385)
+2022-11-18 15:38:32,655:INFO: Dataset: univ                Batch:  4/15	Loss 1.9089 (1.8574)
+2022-11-18 15:38:32,812:INFO: Dataset: univ                Batch:  5/15	Loss 1.8826 (1.8628)
+2022-11-18 15:38:32,969:INFO: Dataset: univ                Batch:  6/15	Loss 1.8497 (1.8607)
+2022-11-18 15:38:33,124:INFO: Dataset: univ                Batch:  7/15	Loss 1.8477 (1.8590)
+2022-11-18 15:38:33,280:INFO: Dataset: univ                Batch:  8/15	Loss 1.8611 (1.8593)
+2022-11-18 15:38:33,435:INFO: Dataset: univ                Batch:  9/15	Loss 1.8482 (1.8581)
+2022-11-18 15:38:33,592:INFO: Dataset: univ                Batch: 10/15	Loss 1.8528 (1.8575)
+2022-11-18 15:38:33,753:INFO: Dataset: univ                Batch: 11/15	Loss 1.8975 (1.8612)
+2022-11-18 15:38:33,906:INFO: Dataset: univ                Batch: 12/15	Loss 1.8666 (1.8615)
+2022-11-18 15:38:34,062:INFO: Dataset: univ                Batch: 13/15	Loss 1.8768 (1.8628)
+2022-11-18 15:38:34,219:INFO: Dataset: univ                Batch: 14/15	Loss 1.8397 (1.8609)
+2022-11-18 15:38:34,304:INFO: Dataset: univ                Batch: 15/15	Loss 1.8115 (1.8602)
+2022-11-18 15:38:34,701:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7954 (1.7954)
+2022-11-18 15:38:34,856:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8562 (1.8262)
+2022-11-18 15:38:35,009:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7914 (1.8153)
+2022-11-18 15:38:35,160:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8689 (1.8290)
+2022-11-18 15:38:35,314:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0461 (1.8733)
+2022-11-18 15:38:35,467:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9569 (1.8878)
+2022-11-18 15:38:35,619:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8219 (1.8786)
+2022-11-18 15:38:35,759:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8762 (1.8783)
+2022-11-18 15:38:36,149:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9691 (1.9691)
+2022-11-18 15:38:36,306:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8564 (1.9129)
+2022-11-18 15:38:36,460:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8800 (1.9023)
+2022-11-18 15:38:36,613:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7873 (1.8745)
+2022-11-18 15:38:36,772:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8789 (1.8755)
+2022-11-18 15:38:36,934:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8804 (1.8763)
+2022-11-18 15:38:37,093:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7494 (1.8576)
+2022-11-18 15:38:37,248:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8239 (1.8529)
+2022-11-18 15:38:37,403:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9796 (1.8663)
+2022-11-18 15:38:37,557:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9380 (1.8728)
+2022-11-18 15:38:37,717:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7874 (1.8653)
+2022-11-18 15:38:37,874:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8023 (1.8602)
+2022-11-18 15:38:38,031:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9019 (1.8636)
+2022-11-18 15:38:38,187:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9106 (1.8670)
+2022-11-18 15:38:38,345:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9463 (1.8719)
+2022-11-18 15:38:38,520:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8488 (1.8705)
+2022-11-18 15:38:38,679:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7623 (1.8641)
+2022-11-18 15:38:38,837:INFO: Dataset: zara2               Batch: 18/18	Loss 2.0072 (1.8706)
+2022-11-18 15:38:38,884:INFO: - Computing loss (validation)
+2022-11-18 15:38:39,182:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9228 (1.9228)
+2022-11-18 15:38:39,220:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9520 (1.9252)
+2022-11-18 15:38:39,546:INFO: Dataset: univ                Batch: 1/3	Loss 1.9138 (1.9138)
+2022-11-18 15:38:39,634:INFO: Dataset: univ                Batch: 2/3	Loss 1.8708 (1.8904)
+2022-11-18 15:38:39,715:INFO: Dataset: univ                Batch: 3/3	Loss 1.7883 (1.8575)
+2022-11-18 15:38:40,029:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8900 (1.8900)
+2022-11-18 15:38:40,081:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8844 (1.8885)
+2022-11-18 15:38:40,401:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8631 (1.8631)
+2022-11-18 15:38:40,481:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7746 (1.8198)
+2022-11-18 15:38:40,562:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8583 (1.8324)
+2022-11-18 15:38:40,646:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8790 (1.8444)
+2022-11-18 15:38:40,726:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8499 (1.8454)
+2022-11-18 15:38:40,783:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_492.pth.tar
+2022-11-18 15:38:40,783:INFO: 
+===> EPOCH: 493 (P2)
+2022-11-18 15:38:40,784:INFO: - Computing loss (training)
+2022-11-18 15:38:41,136:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8693 (1.8693)
+2022-11-18 15:38:41,288:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8736 (1.8716)
+2022-11-18 15:38:41,437:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9649 (1.9024)
+2022-11-18 15:38:41,551:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8764 (1.8978)
+2022-11-18 15:38:42,024:INFO: Dataset: univ                Batch:  1/15	Loss 1.8818 (1.8818)
+2022-11-18 15:38:42,185:INFO: Dataset: univ                Batch:  2/15	Loss 1.8567 (1.8692)
+2022-11-18 15:38:42,346:INFO: Dataset: univ                Batch:  3/15	Loss 1.8730 (1.8705)
+2022-11-18 15:38:42,503:INFO: Dataset: univ                Batch:  4/15	Loss 1.8852 (1.8740)
+2022-11-18 15:38:42,662:INFO: Dataset: univ                Batch:  5/15	Loss 1.8717 (1.8735)
+2022-11-18 15:38:42,829:INFO: Dataset: univ                Batch:  6/15	Loss 1.9025 (1.8778)
+2022-11-18 15:38:42,993:INFO: Dataset: univ                Batch:  7/15	Loss 1.8135 (1.8683)
+2022-11-18 15:38:43,164:INFO: Dataset: univ                Batch:  8/15	Loss 1.8434 (1.8649)
+2022-11-18 15:38:43,329:INFO: Dataset: univ                Batch:  9/15	Loss 1.8311 (1.8612)
+2022-11-18 15:38:43,503:INFO: Dataset: univ                Batch: 10/15	Loss 1.8644 (1.8615)
+2022-11-18 15:38:43,675:INFO: Dataset: univ                Batch: 11/15	Loss 1.8551 (1.8609)
+2022-11-18 15:38:43,840:INFO: Dataset: univ                Batch: 12/15	Loss 1.8631 (1.8611)
+2022-11-18 15:38:44,018:INFO: Dataset: univ                Batch: 13/15	Loss 1.8976 (1.8640)
+2022-11-18 15:38:44,189:INFO: Dataset: univ                Batch: 14/15	Loss 1.8596 (1.8637)
+2022-11-18 15:38:44,277:INFO: Dataset: univ                Batch: 15/15	Loss 1.7986 (1.8627)
+2022-11-18 15:38:44,703:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0023 (2.0023)
+2022-11-18 15:38:44,883:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9325 (1.9663)
+2022-11-18 15:38:45,037:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8287 (1.9206)
+2022-11-18 15:38:45,192:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8607 (1.9063)
+2022-11-18 15:38:45,346:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9265 (1.9100)
+2022-11-18 15:38:45,498:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8265 (1.8955)
+2022-11-18 15:38:45,651:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8738 (1.8924)
+2022-11-18 15:38:45,789:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8902 (1.8921)
+2022-11-18 15:38:46,183:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8474 (1.8474)
+2022-11-18 15:38:46,349:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8058 (1.8253)
+2022-11-18 15:38:46,511:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8877 (1.8457)
+2022-11-18 15:38:46,672:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9145 (1.8627)
+2022-11-18 15:38:46,826:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8798 (1.8661)
+2022-11-18 15:38:46,978:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7442 (1.8474)
+2022-11-18 15:38:47,129:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8917 (1.8536)
+2022-11-18 15:38:47,278:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8200 (1.8495)
+2022-11-18 15:38:47,429:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8413 (1.8487)
+2022-11-18 15:38:47,579:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7705 (1.8410)
+2022-11-18 15:38:47,730:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8775 (1.8441)
+2022-11-18 15:38:47,881:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7997 (1.8398)
+2022-11-18 15:38:48,031:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9307 (1.8462)
+2022-11-18 15:38:48,182:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7924 (1.8424)
+2022-11-18 15:38:48,335:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7772 (1.8383)
+2022-11-18 15:38:48,490:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8842 (1.8415)
+2022-11-18 15:38:48,643:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8324 (1.8410)
+2022-11-18 15:38:48,780:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8191 (1.8399)
+2022-11-18 15:38:48,831:INFO: - Computing loss (validation)
+2022-11-18 15:38:49,093:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8331 (1.8331)
+2022-11-18 15:38:49,129:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8955 (1.8378)
+2022-11-18 15:38:49,434:INFO: Dataset: univ                Batch: 1/3	Loss 1.8602 (1.8602)
+2022-11-18 15:38:49,511:INFO: Dataset: univ                Batch: 2/3	Loss 1.8601 (1.8601)
+2022-11-18 15:38:49,582:INFO: Dataset: univ                Batch: 3/3	Loss 1.8037 (1.8443)
+2022-11-18 15:38:49,897:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9083 (1.9083)
+2022-11-18 15:38:49,943:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8054 (1.8851)
+2022-11-18 15:38:50,257:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7707 (1.7707)
+2022-11-18 15:38:50,334:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8741 (1.8255)
+2022-11-18 15:38:50,408:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8551 (1.8356)
+2022-11-18 15:38:50,482:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8308 (1.8343)
+2022-11-18 15:38:50,555:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8839 (1.8451)
+2022-11-18 15:38:50,607:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_493.pth.tar
+2022-11-18 15:38:50,607:INFO: 
+===> EPOCH: 494 (P2)
+2022-11-18 15:38:50,608:INFO: - Computing loss (training)
+2022-11-18 15:38:50,962:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7810 (1.7810)
+2022-11-18 15:38:51,118:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9315 (1.8550)
+2022-11-18 15:38:51,272:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8665 (1.8589)
+2022-11-18 15:38:51,388:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0401 (1.8900)
+2022-11-18 15:38:51,782:INFO: Dataset: univ                Batch:  1/15	Loss 1.8545 (1.8545)
+2022-11-18 15:38:51,939:INFO: Dataset: univ                Batch:  2/15	Loss 1.8619 (1.8581)
+2022-11-18 15:38:52,100:INFO: Dataset: univ                Batch:  3/15	Loss 1.8617 (1.8594)
+2022-11-18 15:38:52,265:INFO: Dataset: univ                Batch:  4/15	Loss 1.9068 (1.8707)
+2022-11-18 15:38:52,429:INFO: Dataset: univ                Batch:  5/15	Loss 1.8632 (1.8691)
+2022-11-18 15:38:52,606:INFO: Dataset: univ                Batch:  6/15	Loss 1.8455 (1.8653)
+2022-11-18 15:38:52,767:INFO: Dataset: univ                Batch:  7/15	Loss 1.8367 (1.8610)
+2022-11-18 15:38:52,925:INFO: Dataset: univ                Batch:  8/15	Loss 1.8243 (1.8565)
+2022-11-18 15:38:53,084:INFO: Dataset: univ                Batch:  9/15	Loss 1.8436 (1.8550)
+2022-11-18 15:38:53,241:INFO: Dataset: univ                Batch: 10/15	Loss 1.9029 (1.8594)
+2022-11-18 15:38:53,406:INFO: Dataset: univ                Batch: 11/15	Loss 1.8661 (1.8601)
+2022-11-18 15:38:53,576:INFO: Dataset: univ                Batch: 12/15	Loss 1.8493 (1.8591)
+2022-11-18 15:38:53,745:INFO: Dataset: univ                Batch: 13/15	Loss 1.7986 (1.8545)
+2022-11-18 15:38:53,908:INFO: Dataset: univ                Batch: 14/15	Loss 1.8388 (1.8535)
+2022-11-18 15:38:53,992:INFO: Dataset: univ                Batch: 15/15	Loss 1.7921 (1.8525)
+2022-11-18 15:38:54,387:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9328 (1.9328)
+2022-11-18 15:38:54,544:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9410 (1.9372)
+2022-11-18 15:38:54,694:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0363 (1.9704)
+2022-11-18 15:38:54,850:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8153 (1.9308)
+2022-11-18 15:38:55,008:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9109 (1.9271)
+2022-11-18 15:38:55,159:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8568 (1.9152)
+2022-11-18 15:38:55,308:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8895 (1.9117)
+2022-11-18 15:38:55,446:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9464 (1.9161)
+2022-11-18 15:38:55,867:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9099 (1.9099)
+2022-11-18 15:38:56,026:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8383 (1.8719)
+2022-11-18 15:38:56,181:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7113 (1.8161)
+2022-11-18 15:38:56,339:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9204 (1.8399)
+2022-11-18 15:38:56,502:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9112 (1.8534)
+2022-11-18 15:38:56,660:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9853 (1.8752)
+2022-11-18 15:38:56,815:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8967 (1.8781)
+2022-11-18 15:38:56,967:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8284 (1.8724)
+2022-11-18 15:38:57,120:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7749 (1.8628)
+2022-11-18 15:38:57,278:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8091 (1.8576)
+2022-11-18 15:38:57,431:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8859 (1.8601)
+2022-11-18 15:38:57,583:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8530 (1.8595)
+2022-11-18 15:38:57,735:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8406 (1.8581)
+2022-11-18 15:38:57,886:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8614 (1.8583)
+2022-11-18 15:38:58,039:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8454 (1.8575)
+2022-11-18 15:38:58,193:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8732 (1.8584)
+2022-11-18 15:38:58,346:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8640 (1.8588)
+2022-11-18 15:38:58,486:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9101 (1.8612)
+2022-11-18 15:38:58,531:INFO: - Computing loss (validation)
+2022-11-18 15:38:58,802:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8931 (1.8931)
+2022-11-18 15:38:58,836:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8431 (1.8895)
+2022-11-18 15:38:59,148:INFO: Dataset: univ                Batch: 1/3	Loss 1.8835 (1.8835)
+2022-11-18 15:38:59,235:INFO: Dataset: univ                Batch: 2/3	Loss 1.9271 (1.9071)
+2022-11-18 15:38:59,314:INFO: Dataset: univ                Batch: 3/3	Loss 1.8571 (1.8880)
+2022-11-18 15:38:59,617:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8977 (1.8977)
+2022-11-18 15:38:59,664:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9111 (1.9009)
+2022-11-18 15:38:59,980:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8164 (1.8164)
+2022-11-18 15:39:00,057:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7198 (1.7686)
+2022-11-18 15:39:00,135:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8235 (1.7870)
+2022-11-18 15:39:00,212:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8795 (1.8084)
+2022-11-18 15:39:00,288:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8752 (1.8216)
+2022-11-18 15:39:00,350:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_494.pth.tar
+2022-11-18 15:39:00,350:INFO: 
+===> EPOCH: 495 (P2)
+2022-11-18 15:39:00,351:INFO: - Computing loss (training)
+2022-11-18 15:39:00,764:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9529 (1.9529)
+2022-11-18 15:39:00,919:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9353 (1.9438)
+2022-11-18 15:39:01,085:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8680 (1.9191)
+2022-11-18 15:39:01,217:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9431 (1.9235)
+2022-11-18 15:39:01,621:INFO: Dataset: univ                Batch:  1/15	Loss 1.8972 (1.8972)
+2022-11-18 15:39:01,785:INFO: Dataset: univ                Batch:  2/15	Loss 1.8222 (1.8594)
+2022-11-18 15:39:01,947:INFO: Dataset: univ                Batch:  3/15	Loss 1.8662 (1.8614)
+2022-11-18 15:39:02,104:INFO: Dataset: univ                Batch:  4/15	Loss 1.8550 (1.8598)
+2022-11-18 15:39:02,262:INFO: Dataset: univ                Batch:  5/15	Loss 1.8597 (1.8598)
+2022-11-18 15:39:02,421:INFO: Dataset: univ                Batch:  6/15	Loss 1.8882 (1.8648)
+2022-11-18 15:39:02,578:INFO: Dataset: univ                Batch:  7/15	Loss 1.8697 (1.8655)
+2022-11-18 15:39:02,735:INFO: Dataset: univ                Batch:  8/15	Loss 1.8305 (1.8608)
+2022-11-18 15:39:02,892:INFO: Dataset: univ                Batch:  9/15	Loss 1.8911 (1.8639)
+2022-11-18 15:39:03,047:INFO: Dataset: univ                Batch: 10/15	Loss 1.8730 (1.8648)
+2022-11-18 15:39:03,205:INFO: Dataset: univ                Batch: 11/15	Loss 1.8174 (1.8610)
+2022-11-18 15:39:03,362:INFO: Dataset: univ                Batch: 12/15	Loss 1.8729 (1.8621)
+2022-11-18 15:39:03,518:INFO: Dataset: univ                Batch: 13/15	Loss 1.8938 (1.8646)
+2022-11-18 15:39:03,676:INFO: Dataset: univ                Batch: 14/15	Loss 1.8348 (1.8626)
+2022-11-18 15:39:03,762:INFO: Dataset: univ                Batch: 15/15	Loss 1.7991 (1.8619)
+2022-11-18 15:39:04,163:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8907 (1.8907)
+2022-11-18 15:39:04,319:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8662 (1.8783)
+2022-11-18 15:39:04,485:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9232 (1.8938)
+2022-11-18 15:39:04,637:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8071 (1.8725)
+2022-11-18 15:39:04,795:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8441 (1.8674)
+2022-11-18 15:39:04,950:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8154 (1.8594)
+2022-11-18 15:39:05,110:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7485 (1.8449)
+2022-11-18 15:39:05,255:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9227 (1.8535)
+2022-11-18 15:39:05,659:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8723 (1.8723)
+2022-11-18 15:39:05,839:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8110 (1.8419)
+2022-11-18 15:39:06,015:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8750 (1.8531)
+2022-11-18 15:39:06,185:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8798 (1.8593)
+2022-11-18 15:39:06,355:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9048 (1.8681)
+2022-11-18 15:39:06,522:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7781 (1.8527)
+2022-11-18 15:39:06,697:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8934 (1.8588)
+2022-11-18 15:39:06,851:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8193 (1.8548)
+2022-11-18 15:39:07,024:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8571 (1.8551)
+2022-11-18 15:39:07,194:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8150 (1.8514)
+2022-11-18 15:39:07,363:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8262 (1.8492)
+2022-11-18 15:39:07,536:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8098 (1.8455)
+2022-11-18 15:39:07,694:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9531 (1.8543)
+2022-11-18 15:39:07,846:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8577 (1.8546)
+2022-11-18 15:39:08,008:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8395 (1.8536)
+2022-11-18 15:39:08,163:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7950 (1.8501)
+2022-11-18 15:39:08,324:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8737 (1.8515)
+2022-11-18 15:39:08,466:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9122 (1.8542)
+2022-11-18 15:39:08,514:INFO: - Computing loss (validation)
+2022-11-18 15:39:08,787:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8903 (1.8903)
+2022-11-18 15:39:08,825:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7089 (1.8773)
+2022-11-18 15:39:09,136:INFO: Dataset: univ                Batch: 1/3	Loss 1.8199 (1.8199)
+2022-11-18 15:39:09,215:INFO: Dataset: univ                Batch: 2/3	Loss 1.8495 (1.8355)
+2022-11-18 15:39:09,290:INFO: Dataset: univ                Batch: 3/3	Loss 1.8020 (1.8253)
+2022-11-18 15:39:09,607:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9374 (1.9374)
+2022-11-18 15:39:09,660:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0126 (1.9563)
+2022-11-18 15:39:10,007:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9519 (1.9519)
+2022-11-18 15:39:10,094:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8365 (1.8938)
+2022-11-18 15:39:10,182:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9008 (1.8961)
+2022-11-18 15:39:10,271:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8827 (1.8927)
+2022-11-18 15:39:10,356:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8088 (1.8754)
+2022-11-18 15:39:10,418:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_495.pth.tar
+2022-11-18 15:39:10,418:INFO: 
+===> EPOCH: 496 (P2)
+2022-11-18 15:39:10,419:INFO: - Computing loss (training)
+2022-11-18 15:39:10,773:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9217 (1.9217)
+2022-11-18 15:39:10,933:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8865 (1.9040)
+2022-11-18 15:39:11,091:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0085 (1.9396)
+2022-11-18 15:39:11,212:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9516 (1.9415)
+2022-11-18 15:39:11,617:INFO: Dataset: univ                Batch:  1/15	Loss 1.9147 (1.9147)
+2022-11-18 15:39:11,777:INFO: Dataset: univ                Batch:  2/15	Loss 1.8334 (1.8708)
+2022-11-18 15:39:11,938:INFO: Dataset: univ                Batch:  3/15	Loss 1.9053 (1.8825)
+2022-11-18 15:39:12,107:INFO: Dataset: univ                Batch:  4/15	Loss 1.8038 (1.8644)
+2022-11-18 15:39:12,270:INFO: Dataset: univ                Batch:  5/15	Loss 1.8407 (1.8599)
+2022-11-18 15:39:12,439:INFO: Dataset: univ                Batch:  6/15	Loss 1.8299 (1.8546)
+2022-11-18 15:39:12,604:INFO: Dataset: univ                Batch:  7/15	Loss 1.9083 (1.8632)
+2022-11-18 15:39:12,767:INFO: Dataset: univ                Batch:  8/15	Loss 1.8649 (1.8634)
+2022-11-18 15:39:12,933:INFO: Dataset: univ                Batch:  9/15	Loss 1.8817 (1.8656)
+2022-11-18 15:39:13,098:INFO: Dataset: univ                Batch: 10/15	Loss 1.8603 (1.8651)
+2022-11-18 15:39:13,261:INFO: Dataset: univ                Batch: 11/15	Loss 1.8641 (1.8650)
+2022-11-18 15:39:13,418:INFO: Dataset: univ                Batch: 12/15	Loss 1.8741 (1.8657)
+2022-11-18 15:39:13,574:INFO: Dataset: univ                Batch: 13/15	Loss 1.8950 (1.8679)
+2022-11-18 15:39:13,731:INFO: Dataset: univ                Batch: 14/15	Loss 1.8362 (1.8655)
+2022-11-18 15:39:13,815:INFO: Dataset: univ                Batch: 15/15	Loss 1.6821 (1.8632)
+2022-11-18 15:39:14,202:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8787 (1.8787)
+2022-11-18 15:39:14,360:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8004 (1.8419)
+2022-11-18 15:39:14,519:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8365 (1.8401)
+2022-11-18 15:39:14,678:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8090 (1.8329)
+2022-11-18 15:39:14,834:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8766 (1.8422)
+2022-11-18 15:39:14,984:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8525 (1.8438)
+2022-11-18 15:39:15,134:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8690 (1.8475)
+2022-11-18 15:39:15,270:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8760 (1.8507)
+2022-11-18 15:39:15,690:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8926 (1.8926)
+2022-11-18 15:39:15,839:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8464 (1.8703)
+2022-11-18 15:39:15,992:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8573 (1.8660)
+2022-11-18 15:39:16,143:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8805 (1.8694)
+2022-11-18 15:39:16,294:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8598 (1.8675)
+2022-11-18 15:39:16,448:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8088 (1.8573)
+2022-11-18 15:39:16,599:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8910 (1.8624)
+2022-11-18 15:39:16,748:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8290 (1.8584)
+2022-11-18 15:39:16,898:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7876 (1.8496)
+2022-11-18 15:39:17,047:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8654 (1.8512)
+2022-11-18 15:39:17,198:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8365 (1.8499)
+2022-11-18 15:39:17,348:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9398 (1.8572)
+2022-11-18 15:39:17,499:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8575 (1.8573)
+2022-11-18 15:39:17,652:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8245 (1.8550)
+2022-11-18 15:39:17,805:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7408 (1.8474)
+2022-11-18 15:39:17,955:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9637 (1.8542)
+2022-11-18 15:39:18,108:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8035 (1.8513)
+2022-11-18 15:39:18,247:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8664 (1.8521)
+2022-11-18 15:39:18,292:INFO: - Computing loss (validation)
+2022-11-18 15:39:18,548:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7892 (1.7892)
+2022-11-18 15:39:18,583:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8734 (1.7972)
+2022-11-18 15:39:18,891:INFO: Dataset: univ                Batch: 1/3	Loss 1.8542 (1.8542)
+2022-11-18 15:39:18,969:INFO: Dataset: univ                Batch: 2/3	Loss 1.8932 (1.8725)
+2022-11-18 15:39:19,041:INFO: Dataset: univ                Batch: 3/3	Loss 1.8736 (1.8728)
+2022-11-18 15:39:19,356:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9229 (1.9229)
+2022-11-18 15:39:19,407:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9666 (1.9337)
+2022-11-18 15:39:19,708:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8774 (1.8774)
+2022-11-18 15:39:19,784:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9079 (1.8929)
+2022-11-18 15:39:19,858:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7558 (1.8474)
+2022-11-18 15:39:19,933:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8898 (1.8580)
+2022-11-18 15:39:20,008:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8185 (1.8494)
+2022-11-18 15:39:20,063:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_496.pth.tar
+2022-11-18 15:39:20,063:INFO: 
+===> EPOCH: 497 (P2)
+2022-11-18 15:39:20,064:INFO: - Computing loss (training)
+2022-11-18 15:39:20,415:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8874 (1.8874)
+2022-11-18 15:39:20,575:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7474 (1.8169)
+2022-11-18 15:39:20,728:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9616 (1.8651)
+2022-11-18 15:39:20,847:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8684 (1.8657)
+2022-11-18 15:39:21,324:INFO: Dataset: univ                Batch:  1/15	Loss 1.8955 (1.8955)
+2022-11-18 15:39:21,489:INFO: Dataset: univ                Batch:  2/15	Loss 1.8381 (1.8681)
+2022-11-18 15:39:21,656:INFO: Dataset: univ                Batch:  3/15	Loss 1.8737 (1.8699)
+2022-11-18 15:39:21,820:INFO: Dataset: univ                Batch:  4/15	Loss 1.8066 (1.8543)
+2022-11-18 15:39:21,988:INFO: Dataset: univ                Batch:  5/15	Loss 1.7992 (1.8432)
+2022-11-18 15:39:22,168:INFO: Dataset: univ                Batch:  6/15	Loss 1.8591 (1.8458)
+2022-11-18 15:39:22,337:INFO: Dataset: univ                Batch:  7/15	Loss 1.8795 (1.8505)
+2022-11-18 15:39:22,497:INFO: Dataset: univ                Batch:  8/15	Loss 1.8647 (1.8521)
+2022-11-18 15:39:22,660:INFO: Dataset: univ                Batch:  9/15	Loss 1.9170 (1.8590)
+2022-11-18 15:39:22,819:INFO: Dataset: univ                Batch: 10/15	Loss 1.8350 (1.8565)
+2022-11-18 15:39:22,978:INFO: Dataset: univ                Batch: 11/15	Loss 1.8078 (1.8519)
+2022-11-18 15:39:23,139:INFO: Dataset: univ                Batch: 12/15	Loss 1.8551 (1.8521)
+2022-11-18 15:39:23,302:INFO: Dataset: univ                Batch: 13/15	Loss 1.8853 (1.8547)
+2022-11-18 15:39:23,464:INFO: Dataset: univ                Batch: 14/15	Loss 1.8274 (1.8528)
+2022-11-18 15:39:23,550:INFO: Dataset: univ                Batch: 15/15	Loss 1.8179 (1.8523)
+2022-11-18 15:39:23,959:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8722 (1.8722)
+2022-11-18 15:39:24,118:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8522 (1.8627)
+2022-11-18 15:39:24,279:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9382 (1.8885)
+2022-11-18 15:39:24,431:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8622 (1.8816)
+2022-11-18 15:39:24,586:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0042 (1.9078)
+2022-11-18 15:39:24,739:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8282 (1.8923)
+2022-11-18 15:39:24,891:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8441 (1.8861)
+2022-11-18 15:39:25,031:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9010 (1.8877)
+2022-11-18 15:39:25,419:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8151 (1.8151)
+2022-11-18 15:39:25,582:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8682 (1.8412)
+2022-11-18 15:39:25,741:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9127 (1.8654)
+2022-11-18 15:39:25,896:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8352 (1.8577)
+2022-11-18 15:39:26,049:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8379 (1.8534)
+2022-11-18 15:39:26,207:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7798 (1.8404)
+2022-11-18 15:39:26,360:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9004 (1.8481)
+2022-11-18 15:39:26,512:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9016 (1.8550)
+2022-11-18 15:39:26,667:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9755 (1.8699)
+2022-11-18 15:39:26,824:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8546 (1.8684)
+2022-11-18 15:39:26,986:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8160 (1.8637)
+2022-11-18 15:39:27,148:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8526 (1.8628)
+2022-11-18 15:39:27,304:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8769 (1.8638)
+2022-11-18 15:39:27,458:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8513 (1.8629)
+2022-11-18 15:39:27,613:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0181 (1.8736)
+2022-11-18 15:39:27,766:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8529 (1.8723)
+2022-11-18 15:39:27,920:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7704 (1.8661)
+2022-11-18 15:39:28,062:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8278 (1.8641)
+2022-11-18 15:39:28,108:INFO: - Computing loss (validation)
+2022-11-18 15:39:28,385:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8815 (1.8815)
+2022-11-18 15:39:28,421:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2355 (1.8529)
+2022-11-18 15:39:28,729:INFO: Dataset: univ                Batch: 1/3	Loss 1.8796 (1.8796)
+2022-11-18 15:39:28,805:INFO: Dataset: univ                Batch: 2/3	Loss 1.8664 (1.8736)
+2022-11-18 15:39:28,879:INFO: Dataset: univ                Batch: 3/3	Loss 1.8897 (1.8795)
+2022-11-18 15:39:29,180:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8861 (1.8861)
+2022-11-18 15:39:29,226:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9686 (1.9065)
+2022-11-18 15:39:29,552:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8816 (1.8816)
+2022-11-18 15:39:29,633:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8011 (1.8428)
+2022-11-18 15:39:29,712:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8703 (1.8519)
+2022-11-18 15:39:29,795:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7964 (1.8379)
+2022-11-18 15:39:29,875:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7814 (1.8268)
+2022-11-18 15:39:29,930:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_497.pth.tar
+2022-11-18 15:39:29,930:INFO: 
+===> EPOCH: 498 (P2)
+2022-11-18 15:39:29,930:INFO: - Computing loss (training)
+2022-11-18 15:39:30,269:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8957 (1.8957)
+2022-11-18 15:39:30,421:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9126 (1.9039)
+2022-11-18 15:39:30,573:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9639 (1.9230)
+2022-11-18 15:39:30,690:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8307 (1.9071)
+2022-11-18 15:39:31,152:INFO: Dataset: univ                Batch:  1/15	Loss 1.8360 (1.8360)
+2022-11-18 15:39:31,312:INFO: Dataset: univ                Batch:  2/15	Loss 1.9086 (1.8723)
+2022-11-18 15:39:31,470:INFO: Dataset: univ                Batch:  3/15	Loss 1.8614 (1.8685)
+2022-11-18 15:39:31,624:INFO: Dataset: univ                Batch:  4/15	Loss 1.8483 (1.8636)
+2022-11-18 15:39:31,781:INFO: Dataset: univ                Batch:  5/15	Loss 1.8824 (1.8673)
+2022-11-18 15:39:31,941:INFO: Dataset: univ                Batch:  6/15	Loss 1.8098 (1.8577)
+2022-11-18 15:39:32,098:INFO: Dataset: univ                Batch:  7/15	Loss 1.9019 (1.8643)
+2022-11-18 15:39:32,254:INFO: Dataset: univ                Batch:  8/15	Loss 1.9093 (1.8699)
+2022-11-18 15:39:32,411:INFO: Dataset: univ                Batch:  9/15	Loss 1.8577 (1.8685)
+2022-11-18 15:39:32,565:INFO: Dataset: univ                Batch: 10/15	Loss 1.8810 (1.8697)
+2022-11-18 15:39:32,723:INFO: Dataset: univ                Batch: 11/15	Loss 1.8359 (1.8666)
+2022-11-18 15:39:32,881:INFO: Dataset: univ                Batch: 12/15	Loss 1.8750 (1.8674)
+2022-11-18 15:39:33,037:INFO: Dataset: univ                Batch: 13/15	Loss 1.8595 (1.8668)
+2022-11-18 15:39:33,194:INFO: Dataset: univ                Batch: 14/15	Loss 1.8300 (1.8643)
+2022-11-18 15:39:33,277:INFO: Dataset: univ                Batch: 15/15	Loss 1.8629 (1.8643)
+2022-11-18 15:39:33,669:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8111 (1.8111)
+2022-11-18 15:39:33,823:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8049 (1.8078)
+2022-11-18 15:39:33,978:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7906 (1.8017)
+2022-11-18 15:39:34,132:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9059 (1.8263)
+2022-11-18 15:39:34,288:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9363 (1.8486)
+2022-11-18 15:39:34,442:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9328 (1.8642)
+2022-11-18 15:39:34,596:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8475 (1.8615)
+2022-11-18 15:39:34,736:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9614 (1.8741)
+2022-11-18 15:39:35,160:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8964 (1.8964)
+2022-11-18 15:39:35,314:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8633 (1.8800)
+2022-11-18 15:39:35,469:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7958 (1.8502)
+2022-11-18 15:39:35,625:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6960 (1.8110)
+2022-11-18 15:39:35,782:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8943 (1.8268)
+2022-11-18 15:39:35,938:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9214 (1.8417)
+2022-11-18 15:39:36,092:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8570 (1.8441)
+2022-11-18 15:39:36,244:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8064 (1.8391)
+2022-11-18 15:39:36,398:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9082 (1.8473)
+2022-11-18 15:39:36,550:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8727 (1.8499)
+2022-11-18 15:39:36,703:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8177 (1.8470)
+2022-11-18 15:39:36,856:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7295 (1.8365)
+2022-11-18 15:39:37,009:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8574 (1.8381)
+2022-11-18 15:39:37,163:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9495 (1.8467)
+2022-11-18 15:39:37,317:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7930 (1.8427)
+2022-11-18 15:39:37,470:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8374 (1.8423)
+2022-11-18 15:39:37,625:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8916 (1.8454)
+2022-11-18 15:39:37,768:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8033 (1.8434)
+2022-11-18 15:39:37,819:INFO: - Computing loss (validation)
+2022-11-18 15:39:38,090:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8955 (1.8955)
+2022-11-18 15:39:38,125:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5660 (1.8742)
+2022-11-18 15:39:38,436:INFO: Dataset: univ                Batch: 1/3	Loss 1.8918 (1.8918)
+2022-11-18 15:39:38,515:INFO: Dataset: univ                Batch: 2/3	Loss 1.8651 (1.8772)
+2022-11-18 15:39:38,587:INFO: Dataset: univ                Batch: 3/3	Loss 1.8838 (1.8792)
+2022-11-18 15:39:38,902:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9541 (1.9541)
+2022-11-18 15:39:38,950:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0715 (1.9858)
+2022-11-18 15:39:39,256:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7686 (1.7686)
+2022-11-18 15:39:39,339:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7424 (1.7553)
+2022-11-18 15:39:39,419:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8936 (1.8025)
+2022-11-18 15:39:39,499:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9099 (1.8282)
+2022-11-18 15:39:39,578:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8627 (1.8349)
+2022-11-18 15:39:39,699:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_498.pth.tar
+2022-11-18 15:39:39,699:INFO: 
+===> EPOCH: 499 (P2)
+2022-11-18 15:39:39,700:INFO: - Computing loss (training)
+2022-11-18 15:39:40,037:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8333 (1.8333)
+2022-11-18 15:39:40,189:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9325 (1.8818)
+2022-11-18 15:39:40,341:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9351 (1.8978)
+2022-11-18 15:39:40,456:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8502 (1.8894)
+2022-11-18 15:39:40,865:INFO: Dataset: univ                Batch:  1/15	Loss 1.8807 (1.8807)
+2022-11-18 15:39:41,027:INFO: Dataset: univ                Batch:  2/15	Loss 1.8807 (1.8807)
+2022-11-18 15:39:41,191:INFO: Dataset: univ                Batch:  3/15	Loss 1.8533 (1.8728)
+2022-11-18 15:39:41,348:INFO: Dataset: univ                Batch:  4/15	Loss 1.7948 (1.8530)
+2022-11-18 15:39:41,506:INFO: Dataset: univ                Batch:  5/15	Loss 1.8331 (1.8487)
+2022-11-18 15:39:41,668:INFO: Dataset: univ                Batch:  6/15	Loss 1.7783 (1.8358)
+2022-11-18 15:39:41,827:INFO: Dataset: univ                Batch:  7/15	Loss 1.8378 (1.8361)
+2022-11-18 15:39:41,983:INFO: Dataset: univ                Batch:  8/15	Loss 1.8919 (1.8433)
+2022-11-18 15:39:42,142:INFO: Dataset: univ                Batch:  9/15	Loss 1.8692 (1.8461)
+2022-11-18 15:39:42,299:INFO: Dataset: univ                Batch: 10/15	Loss 1.8732 (1.8489)
+2022-11-18 15:39:42,459:INFO: Dataset: univ                Batch: 11/15	Loss 1.8303 (1.8474)
+2022-11-18 15:39:42,615:INFO: Dataset: univ                Batch: 12/15	Loss 1.8966 (1.8513)
+2022-11-18 15:39:42,774:INFO: Dataset: univ                Batch: 13/15	Loss 1.8077 (1.8481)
+2022-11-18 15:39:42,931:INFO: Dataset: univ                Batch: 14/15	Loss 1.8872 (1.8505)
+2022-11-18 15:39:43,015:INFO: Dataset: univ                Batch: 15/15	Loss 1.7979 (1.8498)
+2022-11-18 15:39:43,402:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8661 (1.8661)
+2022-11-18 15:39:43,553:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9724 (1.9176)
+2022-11-18 15:39:43,706:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9037 (1.9130)
+2022-11-18 15:39:43,854:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0727 (1.9498)
+2022-11-18 15:39:44,005:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9300 (1.9459)
+2022-11-18 15:39:44,156:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8213 (1.9238)
+2022-11-18 15:39:44,305:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9331 (1.9250)
+2022-11-18 15:39:44,442:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8440 (1.9154)
+2022-11-18 15:39:44,829:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7767 (1.7767)
+2022-11-18 15:39:44,982:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8776 (1.8310)
+2022-11-18 15:39:45,138:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8384 (1.8334)
+2022-11-18 15:39:45,292:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8133 (1.8286)
+2022-11-18 15:39:45,451:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8237 (1.8276)
+2022-11-18 15:39:45,604:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8349 (1.8289)
+2022-11-18 15:39:45,758:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8747 (1.8353)
+2022-11-18 15:39:45,908:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8908 (1.8428)
+2022-11-18 15:39:46,061:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7873 (1.8365)
+2022-11-18 15:39:46,213:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8695 (1.8396)
+2022-11-18 15:39:46,367:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8037 (1.8363)
+2022-11-18 15:39:46,518:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7773 (1.8310)
+2022-11-18 15:39:46,672:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9071 (1.8369)
+2022-11-18 15:39:46,824:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8628 (1.8388)
+2022-11-18 15:39:46,978:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7724 (1.8343)
+2022-11-18 15:39:47,130:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8697 (1.8366)
+2022-11-18 15:39:47,285:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9210 (1.8417)
+2022-11-18 15:39:47,426:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8284 (1.8411)
+2022-11-18 15:39:47,470:INFO: - Computing loss (validation)
+2022-11-18 15:39:47,737:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9108 (1.9108)
+2022-11-18 15:39:47,771:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8923 (1.9097)
+2022-11-18 15:39:48,088:INFO: Dataset: univ                Batch: 1/3	Loss 1.8520 (1.8520)
+2022-11-18 15:39:48,174:INFO: Dataset: univ                Batch: 2/3	Loss 1.8878 (1.8706)
+2022-11-18 15:39:48,253:INFO: Dataset: univ                Batch: 3/3	Loss 1.8851 (1.8753)
+2022-11-18 15:39:48,561:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8633 (1.8633)
+2022-11-18 15:39:48,609:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7990 (1.8460)
+2022-11-18 15:39:48,911:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8296 (1.8296)
+2022-11-18 15:39:48,988:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8476 (1.8385)
+2022-11-18 15:39:49,062:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8507 (1.8427)
+2022-11-18 15:39:49,136:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9207 (1.8616)
+2022-11-18 15:39:49,211:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8207 (1.8536)
+2022-11-18 15:39:49,262:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_499.pth.tar
+2022-11-18 15:39:49,262:INFO: 
+===> EPOCH: 500 (P2)
+2022-11-18 15:39:49,263:INFO: - Computing loss (training)
+2022-11-18 15:39:49,603:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9732 (1.9732)
+2022-11-18 15:39:49,756:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9964 (1.9849)
+2022-11-18 15:39:49,908:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8637 (1.9450)
+2022-11-18 15:39:50,025:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8820 (1.9348)
+2022-11-18 15:39:50,437:INFO: Dataset: univ                Batch:  1/15	Loss 1.8013 (1.8013)
+2022-11-18 15:39:50,595:INFO: Dataset: univ                Batch:  2/15	Loss 1.9178 (1.8613)
+2022-11-18 15:39:50,756:INFO: Dataset: univ                Batch:  3/15	Loss 1.8434 (1.8549)
+2022-11-18 15:39:50,915:INFO: Dataset: univ                Batch:  4/15	Loss 1.8758 (1.8596)
+2022-11-18 15:39:51,073:INFO: Dataset: univ                Batch:  5/15	Loss 1.8872 (1.8649)
+2022-11-18 15:39:51,231:INFO: Dataset: univ                Batch:  6/15	Loss 1.9235 (1.8734)
+2022-11-18 15:39:51,388:INFO: Dataset: univ                Batch:  7/15	Loss 1.8430 (1.8696)
+2022-11-18 15:39:51,544:INFO: Dataset: univ                Batch:  8/15	Loss 1.8683 (1.8695)
+2022-11-18 15:39:51,702:INFO: Dataset: univ                Batch:  9/15	Loss 1.8410 (1.8662)
+2022-11-18 15:39:51,856:INFO: Dataset: univ                Batch: 10/15	Loss 1.8671 (1.8663)
+2022-11-18 15:39:52,015:INFO: Dataset: univ                Batch: 11/15	Loss 1.8541 (1.8653)
+2022-11-18 15:39:52,172:INFO: Dataset: univ                Batch: 12/15	Loss 1.8246 (1.8617)
+2022-11-18 15:39:52,328:INFO: Dataset: univ                Batch: 13/15	Loss 1.8576 (1.8613)
+2022-11-18 15:39:52,485:INFO: Dataset: univ                Batch: 14/15	Loss 1.8056 (1.8571)
+2022-11-18 15:39:52,571:INFO: Dataset: univ                Batch: 15/15	Loss 1.8622 (1.8571)
+2022-11-18 15:39:52,973:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8585 (1.8585)
+2022-11-18 15:39:53,130:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9527 (1.9102)
+2022-11-18 15:39:53,284:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8540 (1.8932)
+2022-11-18 15:39:53,436:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8207 (1.8734)
+2022-11-18 15:39:53,590:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9593 (1.8893)
+2022-11-18 15:39:53,745:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9189 (1.8938)
+2022-11-18 15:39:53,899:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7901 (1.8768)
+2022-11-18 15:39:54,038:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8774 (1.8769)
+2022-11-18 15:39:54,428:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8197 (1.8197)
+2022-11-18 15:39:54,582:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8158 (1.8179)
+2022-11-18 15:39:54,738:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9408 (1.8618)
+2022-11-18 15:39:54,888:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8536 (1.8598)
+2022-11-18 15:39:55,039:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9123 (1.8708)
+2022-11-18 15:39:55,193:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7823 (1.8565)
+2022-11-18 15:39:55,344:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7789 (1.8453)
+2022-11-18 15:39:55,494:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8816 (1.8500)
+2022-11-18 15:39:55,645:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8920 (1.8550)
+2022-11-18 15:39:55,798:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8286 (1.8527)
+2022-11-18 15:39:55,950:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8484 (1.8523)
+2022-11-18 15:39:56,101:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7843 (1.8463)
+2022-11-18 15:39:56,253:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8409 (1.8459)
+2022-11-18 15:39:56,404:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9565 (1.8534)
+2022-11-18 15:39:56,555:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8252 (1.8514)
+2022-11-18 15:39:56,710:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7575 (1.8449)
+2022-11-18 15:39:56,863:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9356 (1.8501)
+2022-11-18 15:39:57,003:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8946 (1.8524)
+2022-11-18 15:39:57,049:INFO: - Computing loss (validation)
+2022-11-18 15:39:57,316:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9302 (1.9302)
+2022-11-18 15:39:57,350:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7333 (1.9167)
+2022-11-18 15:39:57,668:INFO: Dataset: univ                Batch: 1/3	Loss 1.8595 (1.8595)
+2022-11-18 15:39:57,746:INFO: Dataset: univ                Batch: 2/3	Loss 1.8777 (1.8677)
+2022-11-18 15:39:57,822:INFO: Dataset: univ                Batch: 3/3	Loss 1.8257 (1.8540)
+2022-11-18 15:39:58,126:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8747 (1.8747)
+2022-11-18 15:39:58,174:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0796 (1.9188)
+2022-11-18 15:39:58,487:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7919 (1.7919)
+2022-11-18 15:39:58,563:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8842 (1.8364)
+2022-11-18 15:39:58,637:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8050 (1.8259)
+2022-11-18 15:39:58,711:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8702 (1.8376)
+2022-11-18 15:39:58,786:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8513 (1.8404)
+2022-11-18 15:39:58,838:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_500.pth.tar
+2022-11-18 15:39:58,838:INFO: 
+===> EPOCH: 501 (P2)
+2022-11-18 15:39:58,839:INFO: - Computing loss (training)
+2022-11-18 15:39:59,195:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9891 (1.9891)
+2022-11-18 15:39:59,352:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9011 (1.9443)
+2022-11-18 15:39:59,507:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9131 (1.9340)
+2022-11-18 15:39:59,624:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0182 (1.9480)
+2022-11-18 15:40:00,039:INFO: Dataset: univ                Batch:  1/15	Loss 1.8255 (1.8255)
+2022-11-18 15:40:00,198:INFO: Dataset: univ                Batch:  2/15	Loss 1.8608 (1.8423)
+2022-11-18 15:40:00,356:INFO: Dataset: univ                Batch:  3/15	Loss 1.9160 (1.8650)
+2022-11-18 15:40:00,519:INFO: Dataset: univ                Batch:  4/15	Loss 1.8924 (1.8721)
+2022-11-18 15:40:00,684:INFO: Dataset: univ                Batch:  5/15	Loss 1.8339 (1.8641)
+2022-11-18 15:40:00,842:INFO: Dataset: univ                Batch:  6/15	Loss 1.8664 (1.8645)
+2022-11-18 15:40:00,999:INFO: Dataset: univ                Batch:  7/15	Loss 1.8372 (1.8605)
+2022-11-18 15:40:01,156:INFO: Dataset: univ                Batch:  8/15	Loss 1.8591 (1.8604)
+2022-11-18 15:40:01,316:INFO: Dataset: univ                Batch:  9/15	Loss 1.8818 (1.8627)
+2022-11-18 15:40:01,474:INFO: Dataset: univ                Batch: 10/15	Loss 1.8658 (1.8630)
+2022-11-18 15:40:01,639:INFO: Dataset: univ                Batch: 11/15	Loss 1.8381 (1.8606)
+2022-11-18 15:40:01,798:INFO: Dataset: univ                Batch: 12/15	Loss 1.8849 (1.8626)
+2022-11-18 15:40:01,961:INFO: Dataset: univ                Batch: 13/15	Loss 1.8835 (1.8643)
+2022-11-18 15:40:02,122:INFO: Dataset: univ                Batch: 14/15	Loss 1.8648 (1.8643)
+2022-11-18 15:40:02,210:INFO: Dataset: univ                Batch: 15/15	Loss 1.8724 (1.8644)
+2022-11-18 15:40:02,616:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7873 (1.7873)
+2022-11-18 15:40:02,769:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8339 (1.8110)
+2022-11-18 15:40:02,923:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8696 (1.8295)
+2022-11-18 15:40:03,075:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9319 (1.8553)
+2022-11-18 15:40:03,241:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7919 (1.8429)
+2022-11-18 15:40:03,397:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0470 (1.8753)
+2022-11-18 15:40:03,550:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9115 (1.8805)
+2022-11-18 15:40:03,692:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9169 (1.8844)
+2022-11-18 15:40:04,095:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8065 (1.8065)
+2022-11-18 15:40:04,250:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8846 (1.8459)
+2022-11-18 15:40:04,401:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7899 (1.8272)
+2022-11-18 15:40:04,552:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8594 (1.8355)
+2022-11-18 15:40:04,707:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9605 (1.8568)
+2022-11-18 15:40:04,864:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8129 (1.8495)
+2022-11-18 15:40:05,015:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8607 (1.8511)
+2022-11-18 15:40:05,164:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8994 (1.8566)
+2022-11-18 15:40:05,314:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9437 (1.8665)
+2022-11-18 15:40:05,463:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8166 (1.8616)
+2022-11-18 15:40:05,615:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8656 (1.8620)
+2022-11-18 15:40:05,764:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8736 (1.8630)
+2022-11-18 15:40:05,914:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8672 (1.8633)
+2022-11-18 15:40:06,064:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8120 (1.8596)
+2022-11-18 15:40:06,216:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8948 (1.8620)
+2022-11-18 15:40:06,366:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8530 (1.8614)
+2022-11-18 15:40:06,518:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8025 (1.8578)
+2022-11-18 15:40:06,657:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8900 (1.8594)
+2022-11-18 15:40:06,701:INFO: - Computing loss (validation)
+2022-11-18 15:40:06,973:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8656 (1.8656)
+2022-11-18 15:40:07,009:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6349 (1.8467)
+2022-11-18 15:40:07,315:INFO: Dataset: univ                Batch: 1/3	Loss 1.8463 (1.8463)
+2022-11-18 15:40:07,392:INFO: Dataset: univ                Batch: 2/3	Loss 1.8476 (1.8469)
+2022-11-18 15:40:07,465:INFO: Dataset: univ                Batch: 3/3	Loss 1.8407 (1.8451)
+2022-11-18 15:40:07,758:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9286 (1.9286)
+2022-11-18 15:40:07,803:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8524 (1.9087)
+2022-11-18 15:40:08,111:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8036 (1.8036)
+2022-11-18 15:40:08,187:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8594 (1.8319)
+2022-11-18 15:40:08,261:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8623 (1.8422)
+2022-11-18 15:40:08,337:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8531 (1.8449)
+2022-11-18 15:40:08,411:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8537 (1.8467)
+2022-11-18 15:40:08,468:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_501.pth.tar
+2022-11-18 15:40:08,468:INFO: 
+===> EPOCH: 502 (P2)
+2022-11-18 15:40:08,469:INFO: - Computing loss (training)
+2022-11-18 15:40:08,805:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9220 (1.9220)
+2022-11-18 15:40:08,958:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9097 (1.9158)
+2022-11-18 15:40:09,109:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9417 (1.9244)
+2022-11-18 15:40:09,225:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9429 (1.9274)
+2022-11-18 15:40:09,669:INFO: Dataset: univ                Batch:  1/15	Loss 1.8650 (1.8650)
+2022-11-18 15:40:09,827:INFO: Dataset: univ                Batch:  2/15	Loss 1.8706 (1.8679)
+2022-11-18 15:40:09,986:INFO: Dataset: univ                Batch:  3/15	Loss 1.8788 (1.8715)
+2022-11-18 15:40:10,140:INFO: Dataset: univ                Batch:  4/15	Loss 1.8136 (1.8580)
+2022-11-18 15:40:10,298:INFO: Dataset: univ                Batch:  5/15	Loss 1.8504 (1.8565)
+2022-11-18 15:40:10,457:INFO: Dataset: univ                Batch:  6/15	Loss 1.8284 (1.8517)
+2022-11-18 15:40:10,613:INFO: Dataset: univ                Batch:  7/15	Loss 1.8250 (1.8480)
+2022-11-18 15:40:10,771:INFO: Dataset: univ                Batch:  8/15	Loss 1.8660 (1.8502)
+2022-11-18 15:40:10,930:INFO: Dataset: univ                Batch:  9/15	Loss 1.8170 (1.8464)
+2022-11-18 15:40:11,084:INFO: Dataset: univ                Batch: 10/15	Loss 1.8566 (1.8474)
+2022-11-18 15:40:11,242:INFO: Dataset: univ                Batch: 11/15	Loss 1.9097 (1.8532)
+2022-11-18 15:40:11,398:INFO: Dataset: univ                Batch: 12/15	Loss 1.8556 (1.8534)
+2022-11-18 15:40:11,556:INFO: Dataset: univ                Batch: 13/15	Loss 1.8359 (1.8520)
+2022-11-18 15:40:11,713:INFO: Dataset: univ                Batch: 14/15	Loss 1.8311 (1.8507)
+2022-11-18 15:40:11,797:INFO: Dataset: univ                Batch: 15/15	Loss 1.9384 (1.8520)
+2022-11-18 15:40:12,193:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8563 (1.8563)
+2022-11-18 15:40:12,342:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9225 (1.8881)
+2022-11-18 15:40:12,493:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7649 (1.8466)
+2022-11-18 15:40:12,642:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9104 (1.8626)
+2022-11-18 15:40:12,793:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7902 (1.8480)
+2022-11-18 15:40:12,945:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7649 (1.8344)
+2022-11-18 15:40:13,095:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7913 (1.8282)
+2022-11-18 15:40:13,233:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8070 (1.8260)
+2022-11-18 15:40:13,621:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8028 (1.8028)
+2022-11-18 15:40:13,773:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9541 (1.8839)
+2022-11-18 15:40:13,927:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8670 (1.8783)
+2022-11-18 15:40:14,080:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7883 (1.8539)
+2022-11-18 15:40:14,231:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8827 (1.8601)
+2022-11-18 15:40:14,384:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7322 (1.8395)
+2022-11-18 15:40:14,535:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8025 (1.8345)
+2022-11-18 15:40:14,685:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8388 (1.8350)
+2022-11-18 15:40:14,836:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8592 (1.8379)
+2022-11-18 15:40:14,985:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8680 (1.8410)
+2022-11-18 15:40:15,136:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7702 (1.8339)
+2022-11-18 15:40:15,286:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8721 (1.8374)
+2022-11-18 15:40:15,437:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8759 (1.8403)
+2022-11-18 15:40:15,587:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8523 (1.8412)
+2022-11-18 15:40:15,739:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8514 (1.8419)
+2022-11-18 15:40:15,895:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9336 (1.8466)
+2022-11-18 15:40:16,048:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8879 (1.8491)
+2022-11-18 15:40:16,187:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8632 (1.8498)
+2022-11-18 15:40:16,238:INFO: - Computing loss (validation)
+2022-11-18 15:40:16,515:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9322 (1.9322)
+2022-11-18 15:40:16,549:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2620 (1.9570)
+2022-11-18 15:40:16,855:INFO: Dataset: univ                Batch: 1/3	Loss 1.8522 (1.8522)
+2022-11-18 15:40:16,933:INFO: Dataset: univ                Batch: 2/3	Loss 1.8646 (1.8582)
+2022-11-18 15:40:17,005:INFO: Dataset: univ                Batch: 3/3	Loss 1.9110 (1.8748)
+2022-11-18 15:40:17,310:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8306 (1.8306)
+2022-11-18 15:40:17,356:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0371 (1.8897)
+2022-11-18 15:40:17,673:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8547 (1.8547)
+2022-11-18 15:40:17,747:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8630 (1.8587)
+2022-11-18 15:40:17,824:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7991 (1.8390)
+2022-11-18 15:40:17,898:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8920 (1.8527)
+2022-11-18 15:40:17,973:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8270 (1.8472)
+2022-11-18 15:40:18,028:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_502.pth.tar
+2022-11-18 15:40:18,028:INFO: 
+===> EPOCH: 503 (P2)
+2022-11-18 15:40:18,029:INFO: - Computing loss (training)
+2022-11-18 15:40:18,366:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9184 (1.9184)
+2022-11-18 15:40:18,518:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8185 (1.8694)
+2022-11-18 15:40:18,669:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8589 (1.8659)
+2022-11-18 15:40:18,786:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7587 (1.8489)
+2022-11-18 15:40:19,188:INFO: Dataset: univ                Batch:  1/15	Loss 1.8455 (1.8455)
+2022-11-18 15:40:19,349:INFO: Dataset: univ                Batch:  2/15	Loss 1.8778 (1.8625)
+2022-11-18 15:40:19,511:INFO: Dataset: univ                Batch:  3/15	Loss 1.8854 (1.8697)
+2022-11-18 15:40:19,671:INFO: Dataset: univ                Batch:  4/15	Loss 1.8593 (1.8671)
+2022-11-18 15:40:19,831:INFO: Dataset: univ                Batch:  5/15	Loss 1.8390 (1.8621)
+2022-11-18 15:40:19,991:INFO: Dataset: univ                Batch:  6/15	Loss 1.8418 (1.8584)
+2022-11-18 15:40:20,150:INFO: Dataset: univ                Batch:  7/15	Loss 1.8741 (1.8606)
+2022-11-18 15:40:20,306:INFO: Dataset: univ                Batch:  8/15	Loss 1.8769 (1.8627)
+2022-11-18 15:40:20,463:INFO: Dataset: univ                Batch:  9/15	Loss 1.8417 (1.8603)
+2022-11-18 15:40:20,618:INFO: Dataset: univ                Batch: 10/15	Loss 1.9118 (1.8654)
+2022-11-18 15:40:20,778:INFO: Dataset: univ                Batch: 11/15	Loss 1.8403 (1.8633)
+2022-11-18 15:40:20,936:INFO: Dataset: univ                Batch: 12/15	Loss 1.8456 (1.8619)
+2022-11-18 15:40:21,094:INFO: Dataset: univ                Batch: 13/15	Loss 1.8788 (1.8632)
+2022-11-18 15:40:21,251:INFO: Dataset: univ                Batch: 14/15	Loss 1.8421 (1.8619)
+2022-11-18 15:40:21,337:INFO: Dataset: univ                Batch: 15/15	Loss 1.7918 (1.8609)
+2022-11-18 15:40:21,727:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8039 (1.8039)
+2022-11-18 15:40:21,877:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9208 (1.8632)
+2022-11-18 15:40:22,029:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8290 (1.8515)
+2022-11-18 15:40:22,180:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8627 (1.8541)
+2022-11-18 15:40:22,331:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8196 (1.8463)
+2022-11-18 15:40:22,481:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8314 (1.8438)
+2022-11-18 15:40:22,632:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8047 (1.8384)
+2022-11-18 15:40:22,769:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9405 (1.8503)
+2022-11-18 15:40:23,159:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9614 (1.9614)
+2022-11-18 15:40:23,319:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8053 (1.8858)
+2022-11-18 15:40:23,471:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7985 (1.8595)
+2022-11-18 15:40:23,621:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8317 (1.8522)
+2022-11-18 15:40:23,773:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8237 (1.8464)
+2022-11-18 15:40:23,927:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7674 (1.8332)
+2022-11-18 15:40:24,078:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8244 (1.8320)
+2022-11-18 15:40:24,229:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8216 (1.8307)
+2022-11-18 15:40:24,379:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8695 (1.8352)
+2022-11-18 15:40:24,528:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8529 (1.8371)
+2022-11-18 15:40:24,679:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8134 (1.8348)
+2022-11-18 15:40:24,831:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9573 (1.8449)
+2022-11-18 15:40:24,982:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8131 (1.8425)
+2022-11-18 15:40:25,133:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7685 (1.8375)
+2022-11-18 15:40:25,285:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7735 (1.8331)
+2022-11-18 15:40:25,436:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8209 (1.8324)
+2022-11-18 15:40:25,588:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7427 (1.8270)
+2022-11-18 15:40:25,728:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8566 (1.8282)
+2022-11-18 15:40:25,774:INFO: - Computing loss (validation)
+2022-11-18 15:40:26,044:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8837 (1.8837)
+2022-11-18 15:40:26,078:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7811 (1.8742)
+2022-11-18 15:40:26,393:INFO: Dataset: univ                Batch: 1/3	Loss 1.8542 (1.8542)
+2022-11-18 15:40:26,476:INFO: Dataset: univ                Batch: 2/3	Loss 1.8581 (1.8562)
+2022-11-18 15:40:26,555:INFO: Dataset: univ                Batch: 3/3	Loss 1.8491 (1.8539)
+2022-11-18 15:40:26,875:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9590 (1.9590)
+2022-11-18 15:40:26,924:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8804 (1.9405)
+2022-11-18 15:40:27,229:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8576 (1.8576)
+2022-11-18 15:40:27,306:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8352 (1.8461)
+2022-11-18 15:40:27,383:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8917 (1.8608)
+2022-11-18 15:40:27,460:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8444 (1.8566)
+2022-11-18 15:40:27,536:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8313 (1.8514)
+2022-11-18 15:40:27,592:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_503.pth.tar
+2022-11-18 15:40:27,592:INFO: 
+===> EPOCH: 504 (P2)
+2022-11-18 15:40:27,593:INFO: - Computing loss (training)
+2022-11-18 15:40:27,948:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9006 (1.9006)
+2022-11-18 15:40:28,098:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7282 (1.8129)
+2022-11-18 15:40:28,248:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9461 (1.8594)
+2022-11-18 15:40:28,363:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9535 (1.8763)
+2022-11-18 15:40:28,758:INFO: Dataset: univ                Batch:  1/15	Loss 1.8622 (1.8622)
+2022-11-18 15:40:28,918:INFO: Dataset: univ                Batch:  2/15	Loss 1.8225 (1.8431)
+2022-11-18 15:40:29,077:INFO: Dataset: univ                Batch:  3/15	Loss 1.8748 (1.8537)
+2022-11-18 15:40:29,235:INFO: Dataset: univ                Batch:  4/15	Loss 1.9090 (1.8670)
+2022-11-18 15:40:29,395:INFO: Dataset: univ                Batch:  5/15	Loss 1.8510 (1.8635)
+2022-11-18 15:40:29,557:INFO: Dataset: univ                Batch:  6/15	Loss 1.8526 (1.8617)
+2022-11-18 15:40:29,714:INFO: Dataset: univ                Batch:  7/15	Loss 1.8119 (1.8551)
+2022-11-18 15:40:29,870:INFO: Dataset: univ                Batch:  8/15	Loss 1.8609 (1.8558)
+2022-11-18 15:40:30,027:INFO: Dataset: univ                Batch:  9/15	Loss 1.8551 (1.8557)
+2022-11-18 15:40:30,183:INFO: Dataset: univ                Batch: 10/15	Loss 1.8325 (1.8535)
+2022-11-18 15:40:30,343:INFO: Dataset: univ                Batch: 11/15	Loss 1.8614 (1.8542)
+2022-11-18 15:40:30,499:INFO: Dataset: univ                Batch: 12/15	Loss 1.8401 (1.8530)
+2022-11-18 15:40:30,656:INFO: Dataset: univ                Batch: 13/15	Loss 1.8504 (1.8528)
+2022-11-18 15:40:30,814:INFO: Dataset: univ                Batch: 14/15	Loss 1.9251 (1.8579)
+2022-11-18 15:40:30,898:INFO: Dataset: univ                Batch: 15/15	Loss 1.8869 (1.8584)
+2022-11-18 15:40:31,284:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8942 (1.8942)
+2022-11-18 15:40:31,435:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8894 (1.8918)
+2022-11-18 15:40:31,588:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9348 (1.9071)
+2022-11-18 15:40:31,738:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8869 (1.9024)
+2022-11-18 15:40:31,888:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8634 (1.8948)
+2022-11-18 15:40:32,041:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8845 (1.8929)
+2022-11-18 15:40:32,191:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9767 (1.9044)
+2022-11-18 15:40:32,329:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8454 (1.8979)
+2022-11-18 15:40:32,721:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9033 (1.9033)
+2022-11-18 15:40:32,874:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8426 (1.8716)
+2022-11-18 15:40:33,029:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7733 (1.8397)
+2022-11-18 15:40:33,184:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7228 (1.8107)
+2022-11-18 15:40:33,338:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8027 (1.8090)
+2022-11-18 15:40:33,490:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7993 (1.8074)
+2022-11-18 15:40:33,641:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7910 (1.8049)
+2022-11-18 15:40:33,791:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9172 (1.8191)
+2022-11-18 15:40:33,941:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8310 (1.8204)
+2022-11-18 15:40:34,090:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8778 (1.8263)
+2022-11-18 15:40:34,244:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8687 (1.8304)
+2022-11-18 15:40:34,394:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8703 (1.8338)
+2022-11-18 15:40:34,545:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7868 (1.8306)
+2022-11-18 15:40:34,696:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9014 (1.8358)
+2022-11-18 15:40:34,849:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8613 (1.8375)
+2022-11-18 15:40:34,999:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8742 (1.8399)
+2022-11-18 15:40:35,151:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9352 (1.8452)
+2022-11-18 15:40:35,291:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7339 (1.8396)
+2022-11-18 15:40:35,339:INFO: - Computing loss (validation)
+2022-11-18 15:40:35,616:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9014 (1.9014)
+2022-11-18 15:40:35,650:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0871 (1.9141)
+2022-11-18 15:40:35,967:INFO: Dataset: univ                Batch: 1/3	Loss 1.8442 (1.8442)
+2022-11-18 15:40:36,051:INFO: Dataset: univ                Batch: 2/3	Loss 1.8221 (1.8330)
+2022-11-18 15:40:36,131:INFO: Dataset: univ                Batch: 3/3	Loss 1.8338 (1.8332)
+2022-11-18 15:40:36,452:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9454 (1.9454)
+2022-11-18 15:40:36,497:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8810 (1.9292)
+2022-11-18 15:40:36,803:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7259 (1.7259)
+2022-11-18 15:40:36,879:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8724 (1.8037)
+2022-11-18 15:40:36,954:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8114 (1.8064)
+2022-11-18 15:40:37,030:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8456 (1.8163)
+2022-11-18 15:40:37,104:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7790 (1.8090)
+2022-11-18 15:40:37,163:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_504.pth.tar
+2022-11-18 15:40:37,163:INFO: 
+===> EPOCH: 505 (P2)
+2022-11-18 15:40:37,164:INFO: - Computing loss (training)
+2022-11-18 15:40:37,506:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7377 (1.7377)
+2022-11-18 15:40:37,660:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9645 (1.8500)
+2022-11-18 15:40:37,810:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0427 (1.9139)
+2022-11-18 15:40:37,926:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8284 (1.8995)
+2022-11-18 15:40:38,323:INFO: Dataset: univ                Batch:  1/15	Loss 1.8775 (1.8775)
+2022-11-18 15:40:38,488:INFO: Dataset: univ                Batch:  2/15	Loss 1.8459 (1.8619)
+2022-11-18 15:40:38,646:INFO: Dataset: univ                Batch:  3/15	Loss 1.8741 (1.8659)
+2022-11-18 15:40:38,804:INFO: Dataset: univ                Batch:  4/15	Loss 1.8499 (1.8617)
+2022-11-18 15:40:38,962:INFO: Dataset: univ                Batch:  5/15	Loss 1.8473 (1.8585)
+2022-11-18 15:40:39,121:INFO: Dataset: univ                Batch:  6/15	Loss 1.8514 (1.8574)
+2022-11-18 15:40:39,278:INFO: Dataset: univ                Batch:  7/15	Loss 1.8883 (1.8615)
+2022-11-18 15:40:39,436:INFO: Dataset: univ                Batch:  8/15	Loss 1.8831 (1.8644)
+2022-11-18 15:40:39,593:INFO: Dataset: univ                Batch:  9/15	Loss 1.9102 (1.8698)
+2022-11-18 15:40:39,750:INFO: Dataset: univ                Batch: 10/15	Loss 1.8588 (1.8688)
+2022-11-18 15:40:39,907:INFO: Dataset: univ                Batch: 11/15	Loss 1.8591 (1.8679)
+2022-11-18 15:40:40,065:INFO: Dataset: univ                Batch: 12/15	Loss 1.8547 (1.8667)
+2022-11-18 15:40:40,223:INFO: Dataset: univ                Batch: 13/15	Loss 1.8418 (1.8648)
+2022-11-18 15:40:40,381:INFO: Dataset: univ                Batch: 14/15	Loss 1.9051 (1.8678)
+2022-11-18 15:40:40,465:INFO: Dataset: univ                Batch: 15/15	Loss 1.8311 (1.8674)
+2022-11-18 15:40:40,855:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9007 (1.9007)
+2022-11-18 15:40:41,010:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7537 (1.8228)
+2022-11-18 15:40:41,165:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9800 (1.8776)
+2022-11-18 15:40:41,316:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8740 (1.8767)
+2022-11-18 15:40:41,468:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7984 (1.8596)
+2022-11-18 15:40:41,620:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8109 (1.8522)
+2022-11-18 15:40:41,773:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8203 (1.8475)
+2022-11-18 15:40:41,912:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8859 (1.8519)
+2022-11-18 15:40:42,320:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9382 (1.9382)
+2022-11-18 15:40:42,472:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9435 (1.9411)
+2022-11-18 15:40:42,626:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8575 (1.9121)
+2022-11-18 15:40:42,775:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8195 (1.8878)
+2022-11-18 15:40:42,928:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7841 (1.8664)
+2022-11-18 15:40:43,080:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9446 (1.8783)
+2022-11-18 15:40:43,230:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8229 (1.8708)
+2022-11-18 15:40:43,381:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8384 (1.8673)
+2022-11-18 15:40:43,530:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8433 (1.8646)
+2022-11-18 15:40:43,678:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7705 (1.8545)
+2022-11-18 15:40:43,828:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8677 (1.8556)
+2022-11-18 15:40:43,978:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8889 (1.8585)
+2022-11-18 15:40:44,128:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8195 (1.8556)
+2022-11-18 15:40:44,277:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0024 (1.8655)
+2022-11-18 15:40:44,429:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8266 (1.8630)
+2022-11-18 15:40:44,578:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7458 (1.8557)
+2022-11-18 15:40:44,730:INFO: Dataset: zara2               Batch: 17/18	Loss 2.0158 (1.8653)
+2022-11-18 15:40:44,869:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8493 (1.8645)
+2022-11-18 15:40:44,914:INFO: - Computing loss (validation)
+2022-11-18 15:40:45,178:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0267 (2.0267)
+2022-11-18 15:40:45,212:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8618 (2.0149)
+2022-11-18 15:40:45,522:INFO: Dataset: univ                Batch: 1/3	Loss 1.9031 (1.9031)
+2022-11-18 15:40:45,597:INFO: Dataset: univ                Batch: 2/3	Loss 1.8134 (1.8598)
+2022-11-18 15:40:45,671:INFO: Dataset: univ                Batch: 3/3	Loss 1.9059 (1.8761)
+2022-11-18 15:40:45,971:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8395 (1.8395)
+2022-11-18 15:40:46,020:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0433 (1.8913)
+2022-11-18 15:40:46,343:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9060 (1.9060)
+2022-11-18 15:40:46,430:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8954 (1.9006)
+2022-11-18 15:40:46,510:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8182 (1.8727)
+2022-11-18 15:40:46,591:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8835 (1.8756)
+2022-11-18 15:40:46,670:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7983 (1.8607)
+2022-11-18 15:40:46,728:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_505.pth.tar
+2022-11-18 15:40:46,728:INFO: 
+===> EPOCH: 506 (P2)
+2022-11-18 15:40:46,729:INFO: - Computing loss (training)
+2022-11-18 15:40:47,096:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9755 (1.9755)
+2022-11-18 15:40:47,285:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8548 (1.9170)
+2022-11-18 15:40:47,449:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8854 (1.9068)
+2022-11-18 15:40:47,573:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9905 (1.9220)
+2022-11-18 15:40:47,989:INFO: Dataset: univ                Batch:  1/15	Loss 1.8384 (1.8384)
+2022-11-18 15:40:48,165:INFO: Dataset: univ                Batch:  2/15	Loss 1.8328 (1.8355)
+2022-11-18 15:40:48,350:INFO: Dataset: univ                Batch:  3/15	Loss 1.8374 (1.8361)
+2022-11-18 15:40:48,530:INFO: Dataset: univ                Batch:  4/15	Loss 1.8555 (1.8406)
+2022-11-18 15:40:48,698:INFO: Dataset: univ                Batch:  5/15	Loss 1.8737 (1.8465)
+2022-11-18 15:40:48,880:INFO: Dataset: univ                Batch:  6/15	Loss 1.8592 (1.8486)
+2022-11-18 15:40:49,072:INFO: Dataset: univ                Batch:  7/15	Loss 1.8876 (1.8542)
+2022-11-18 15:40:49,251:INFO: Dataset: univ                Batch:  8/15	Loss 1.8412 (1.8524)
+2022-11-18 15:40:49,424:INFO: Dataset: univ                Batch:  9/15	Loss 1.8359 (1.8505)
+2022-11-18 15:40:49,590:INFO: Dataset: univ                Batch: 10/15	Loss 1.8305 (1.8486)
+2022-11-18 15:40:49,758:INFO: Dataset: univ                Batch: 11/15	Loss 1.8740 (1.8510)
+2022-11-18 15:40:49,915:INFO: Dataset: univ                Batch: 12/15	Loss 1.8501 (1.8509)
+2022-11-18 15:40:50,073:INFO: Dataset: univ                Batch: 13/15	Loss 1.8550 (1.8512)
+2022-11-18 15:40:50,240:INFO: Dataset: univ                Batch: 14/15	Loss 1.8577 (1.8517)
+2022-11-18 15:40:50,326:INFO: Dataset: univ                Batch: 15/15	Loss 1.8329 (1.8515)
+2022-11-18 15:40:50,736:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9046 (1.9046)
+2022-11-18 15:40:50,895:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8785 (1.8917)
+2022-11-18 15:40:51,045:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8287 (1.8714)
+2022-11-18 15:40:51,195:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9088 (1.8803)
+2022-11-18 15:40:51,354:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9304 (1.8903)
+2022-11-18 15:40:51,513:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9170 (1.8946)
+2022-11-18 15:40:51,669:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9509 (1.9027)
+2022-11-18 15:40:51,817:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8877 (1.9010)
+2022-11-18 15:40:52,228:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8473 (1.8473)
+2022-11-18 15:40:52,411:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8632 (1.8554)
+2022-11-18 15:40:52,575:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8559 (1.8555)
+2022-11-18 15:40:52,732:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9966 (1.8888)
+2022-11-18 15:40:52,890:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8565 (1.8820)
+2022-11-18 15:40:53,178:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9217 (1.8879)
+2022-11-18 15:40:53,380:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8296 (1.8793)
+2022-11-18 15:40:53,568:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8358 (1.8739)
+2022-11-18 15:40:53,738:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9109 (1.8782)
+2022-11-18 15:40:53,894:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7513 (1.8665)
+2022-11-18 15:40:54,049:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7545 (1.8560)
+2022-11-18 15:40:54,210:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9230 (1.8611)
+2022-11-18 15:40:54,366:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8765 (1.8622)
+2022-11-18 15:40:54,525:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7970 (1.8577)
+2022-11-18 15:40:54,687:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7609 (1.8517)
+2022-11-18 15:40:54,845:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7928 (1.8483)
+2022-11-18 15:40:55,002:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9021 (1.8517)
+2022-11-18 15:40:55,152:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7817 (1.8485)
+2022-11-18 15:40:55,205:INFO: - Computing loss (validation)
+2022-11-18 15:40:55,478:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8958 (1.8958)
+2022-11-18 15:40:55,516:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6253 (1.8792)
+2022-11-18 15:40:55,830:INFO: Dataset: univ                Batch: 1/3	Loss 1.9432 (1.9432)
+2022-11-18 15:40:55,916:INFO: Dataset: univ                Batch: 2/3	Loss 1.8931 (1.9171)
+2022-11-18 15:40:55,992:INFO: Dataset: univ                Batch: 3/3	Loss 1.8614 (1.8988)
+2022-11-18 15:40:56,324:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9089 (1.9089)
+2022-11-18 15:40:56,380:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7589 (1.8723)
+2022-11-18 15:40:56,732:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8916 (1.8916)
+2022-11-18 15:40:56,822:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8156 (1.8531)
+2022-11-18 15:40:56,917:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7715 (1.8254)
+2022-11-18 15:40:57,018:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8640 (1.8346)
+2022-11-18 15:40:57,112:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8111 (1.8302)
+2022-11-18 15:40:57,176:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_506.pth.tar
+2022-11-18 15:40:57,176:INFO: 
+===> EPOCH: 507 (P2)
+2022-11-18 15:40:57,177:INFO: - Computing loss (training)
+2022-11-18 15:40:57,585:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0167 (2.0167)
+2022-11-18 15:40:57,769:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8643 (1.9374)
+2022-11-18 15:40:57,943:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9863 (1.9538)
+2022-11-18 15:40:58,087:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9656 (1.9558)
+2022-11-18 15:40:58,568:INFO: Dataset: univ                Batch:  1/15	Loss 1.8500 (1.8500)
+2022-11-18 15:40:58,745:INFO: Dataset: univ                Batch:  2/15	Loss 1.8753 (1.8634)
+2022-11-18 15:40:58,925:INFO: Dataset: univ                Batch:  3/15	Loss 1.8447 (1.8568)
+2022-11-18 15:40:59,092:INFO: Dataset: univ                Batch:  4/15	Loss 1.8523 (1.8557)
+2022-11-18 15:40:59,272:INFO: Dataset: univ                Batch:  5/15	Loss 1.7974 (1.8452)
+2022-11-18 15:40:59,451:INFO: Dataset: univ                Batch:  6/15	Loss 1.8448 (1.8451)
+2022-11-18 15:40:59,651:INFO: Dataset: univ                Batch:  7/15	Loss 1.8191 (1.8410)
+2022-11-18 15:40:59,849:INFO: Dataset: univ                Batch:  8/15	Loss 1.8930 (1.8475)
+2022-11-18 15:41:00,028:INFO: Dataset: univ                Batch:  9/15	Loss 1.8727 (1.8503)
+2022-11-18 15:41:00,205:INFO: Dataset: univ                Batch: 10/15	Loss 1.8562 (1.8509)
+2022-11-18 15:41:00,380:INFO: Dataset: univ                Batch: 11/15	Loss 1.8774 (1.8535)
+2022-11-18 15:41:00,564:INFO: Dataset: univ                Batch: 12/15	Loss 1.8867 (1.8562)
+2022-11-18 15:41:00,730:INFO: Dataset: univ                Batch: 13/15	Loss 1.8722 (1.8575)
+2022-11-18 15:41:00,913:INFO: Dataset: univ                Batch: 14/15	Loss 1.9011 (1.8607)
+2022-11-18 15:41:01,011:INFO: Dataset: univ                Batch: 15/15	Loss 1.9628 (1.8623)
+2022-11-18 15:41:01,469:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8300 (1.8300)
+2022-11-18 15:41:01,662:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9471 (1.8835)
+2022-11-18 15:41:01,836:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8380 (1.8691)
+2022-11-18 15:41:02,004:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8670 (1.8685)
+2022-11-18 15:41:02,174:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8935 (1.8734)
+2022-11-18 15:41:02,348:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8660 (1.8721)
+2022-11-18 15:41:02,510:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8701 (1.8718)
+2022-11-18 15:41:02,671:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7738 (1.8604)
+2022-11-18 15:41:03,128:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8556 (1.8556)
+2022-11-18 15:41:03,300:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8594 (1.8575)
+2022-11-18 15:41:03,460:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7850 (1.8325)
+2022-11-18 15:41:03,618:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8174 (1.8282)
+2022-11-18 15:41:03,783:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8208 (1.8267)
+2022-11-18 15:41:03,955:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8637 (1.8323)
+2022-11-18 15:41:04,117:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9190 (1.8460)
+2022-11-18 15:41:04,290:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8372 (1.8448)
+2022-11-18 15:41:04,465:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8511 (1.8455)
+2022-11-18 15:41:04,635:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8309 (1.8441)
+2022-11-18 15:41:04,793:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8240 (1.8423)
+2022-11-18 15:41:04,949:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7684 (1.8357)
+2022-11-18 15:41:05,104:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8405 (1.8360)
+2022-11-18 15:41:05,267:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8940 (1.8403)
+2022-11-18 15:41:05,427:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8702 (1.8422)
+2022-11-18 15:41:05,586:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8056 (1.8398)
+2022-11-18 15:41:05,744:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8820 (1.8423)
+2022-11-18 15:41:05,890:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8464 (1.8426)
+2022-11-18 15:41:05,937:INFO: - Computing loss (validation)
+2022-11-18 15:41:06,200:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8721 (1.8721)
+2022-11-18 15:41:06,236:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7306 (1.8572)
+2022-11-18 15:41:06,538:INFO: Dataset: univ                Batch: 1/3	Loss 1.8275 (1.8275)
+2022-11-18 15:41:06,625:INFO: Dataset: univ                Batch: 2/3	Loss 1.8849 (1.8566)
+2022-11-18 15:41:06,705:INFO: Dataset: univ                Batch: 3/3	Loss 1.8543 (1.8560)
+2022-11-18 15:41:07,017:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8282 (1.8282)
+2022-11-18 15:41:07,062:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0358 (1.8822)
+2022-11-18 15:41:07,373:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9102 (1.9102)
+2022-11-18 15:41:07,453:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8059 (1.8601)
+2022-11-18 15:41:07,533:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8715 (1.8637)
+2022-11-18 15:41:07,614:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8510 (1.8603)
+2022-11-18 15:41:07,693:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8966 (1.8668)
+2022-11-18 15:41:07,747:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_507.pth.tar
+2022-11-18 15:41:07,747:INFO: 
+===> EPOCH: 508 (P2)
+2022-11-18 15:41:07,747:INFO: - Computing loss (training)
+2022-11-18 15:41:08,094:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9712 (1.9712)
+2022-11-18 15:41:08,259:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9409 (1.9557)
+2022-11-18 15:41:08,431:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8072 (1.9041)
+2022-11-18 15:41:08,560:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9509 (1.9118)
+2022-11-18 15:41:08,978:INFO: Dataset: univ                Batch:  1/15	Loss 1.8656 (1.8656)
+2022-11-18 15:41:09,142:INFO: Dataset: univ                Batch:  2/15	Loss 1.8535 (1.8595)
+2022-11-18 15:41:09,305:INFO: Dataset: univ                Batch:  3/15	Loss 1.8592 (1.8594)
+2022-11-18 15:41:09,469:INFO: Dataset: univ                Batch:  4/15	Loss 1.8216 (1.8505)
+2022-11-18 15:41:09,634:INFO: Dataset: univ                Batch:  5/15	Loss 1.8576 (1.8520)
+2022-11-18 15:41:09,797:INFO: Dataset: univ                Batch:  6/15	Loss 1.8786 (1.8564)
+2022-11-18 15:41:09,965:INFO: Dataset: univ                Batch:  7/15	Loss 1.8691 (1.8581)
+2022-11-18 15:41:10,131:INFO: Dataset: univ                Batch:  8/15	Loss 1.8391 (1.8556)
+2022-11-18 15:41:10,296:INFO: Dataset: univ                Batch:  9/15	Loss 1.8694 (1.8572)
+2022-11-18 15:41:10,460:INFO: Dataset: univ                Batch: 10/15	Loss 1.8185 (1.8530)
+2022-11-18 15:41:10,625:INFO: Dataset: univ                Batch: 11/15	Loss 1.8858 (1.8561)
+2022-11-18 15:41:10,789:INFO: Dataset: univ                Batch: 12/15	Loss 1.8131 (1.8525)
+2022-11-18 15:41:10,983:INFO: Dataset: univ                Batch: 13/15	Loss 1.8242 (1.8503)
+2022-11-18 15:41:11,160:INFO: Dataset: univ                Batch: 14/15	Loss 1.8647 (1.8514)
+2022-11-18 15:41:11,245:INFO: Dataset: univ                Batch: 15/15	Loss 2.0264 (1.8536)
+2022-11-18 15:41:11,642:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8562 (1.8562)
+2022-11-18 15:41:11,812:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0109 (1.9325)
+2022-11-18 15:41:11,988:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9433 (1.9365)
+2022-11-18 15:41:12,155:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9236 (1.9333)
+2022-11-18 15:41:12,324:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8833 (1.9232)
+2022-11-18 15:41:12,484:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8635 (1.9139)
+2022-11-18 15:41:12,648:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7989 (1.8974)
+2022-11-18 15:41:12,812:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0050 (1.9073)
+2022-11-18 15:41:13,260:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8871 (1.8871)
+2022-11-18 15:41:13,423:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8588 (1.8724)
+2022-11-18 15:41:13,586:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8552 (1.8673)
+2022-11-18 15:41:13,744:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8066 (1.8524)
+2022-11-18 15:41:13,912:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8502 (1.8519)
+2022-11-18 15:41:14,074:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8509 (1.8518)
+2022-11-18 15:41:14,252:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7812 (1.8411)
+2022-11-18 15:41:14,412:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8574 (1.8433)
+2022-11-18 15:41:14,577:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8375 (1.8426)
+2022-11-18 15:41:14,743:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8834 (1.8466)
+2022-11-18 15:41:14,906:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8103 (1.8434)
+2022-11-18 15:41:15,087:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8208 (1.8416)
+2022-11-18 15:41:15,258:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8011 (1.8383)
+2022-11-18 15:41:15,419:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9168 (1.8437)
+2022-11-18 15:41:15,577:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8037 (1.8413)
+2022-11-18 15:41:15,759:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8040 (1.8389)
+2022-11-18 15:41:15,948:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8886 (1.8419)
+2022-11-18 15:41:16,124:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8812 (1.8439)
+2022-11-18 15:41:16,180:INFO: - Computing loss (validation)
+2022-11-18 15:41:16,489:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8995 (1.8995)
+2022-11-18 15:41:16,530:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8414 (1.8955)
+2022-11-18 15:41:16,863:INFO: Dataset: univ                Batch: 1/3	Loss 1.7898 (1.7898)
+2022-11-18 15:41:16,945:INFO: Dataset: univ                Batch: 2/3	Loss 1.8754 (1.8341)
+2022-11-18 15:41:17,022:INFO: Dataset: univ                Batch: 3/3	Loss 1.8178 (1.8290)
+2022-11-18 15:41:17,350:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9439 (1.9439)
+2022-11-18 15:41:17,395:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9456 (1.9444)
+2022-11-18 15:41:17,711:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8303 (1.8303)
+2022-11-18 15:41:17,791:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8768 (1.8529)
+2022-11-18 15:41:17,873:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8448 (1.8502)
+2022-11-18 15:41:17,956:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7518 (1.8260)
+2022-11-18 15:41:18,037:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7718 (1.8152)
+2022-11-18 15:41:18,093:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_508.pth.tar
+2022-11-18 15:41:18,093:INFO: 
+===> EPOCH: 509 (P2)
+2022-11-18 15:41:18,094:INFO: - Computing loss (training)
+2022-11-18 15:41:18,468:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7897 (1.7897)
+2022-11-18 15:41:18,624:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8485 (1.8179)
+2022-11-18 15:41:18,779:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8119 (1.8159)
+2022-11-18 15:41:18,898:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9243 (1.8339)
+2022-11-18 15:41:19,364:INFO: Dataset: univ                Batch:  1/15	Loss 1.8452 (1.8452)
+2022-11-18 15:41:19,524:INFO: Dataset: univ                Batch:  2/15	Loss 1.8686 (1.8565)
+2022-11-18 15:41:19,684:INFO: Dataset: univ                Batch:  3/15	Loss 1.8592 (1.8574)
+2022-11-18 15:41:19,846:INFO: Dataset: univ                Batch:  4/15	Loss 1.8144 (1.8467)
+2022-11-18 15:41:20,014:INFO: Dataset: univ                Batch:  5/15	Loss 1.8387 (1.8451)
+2022-11-18 15:41:20,200:INFO: Dataset: univ                Batch:  6/15	Loss 1.8452 (1.8451)
+2022-11-18 15:41:20,385:INFO: Dataset: univ                Batch:  7/15	Loss 1.9211 (1.8564)
+2022-11-18 15:41:20,545:INFO: Dataset: univ                Batch:  8/15	Loss 1.8503 (1.8557)
+2022-11-18 15:41:20,701:INFO: Dataset: univ                Batch:  9/15	Loss 1.8946 (1.8597)
+2022-11-18 15:41:20,857:INFO: Dataset: univ                Batch: 10/15	Loss 1.8437 (1.8583)
+2022-11-18 15:41:21,021:INFO: Dataset: univ                Batch: 11/15	Loss 1.8816 (1.8603)
+2022-11-18 15:41:21,185:INFO: Dataset: univ                Batch: 12/15	Loss 1.8795 (1.8620)
+2022-11-18 15:41:21,345:INFO: Dataset: univ                Batch: 13/15	Loss 1.8747 (1.8629)
+2022-11-18 15:41:21,501:INFO: Dataset: univ                Batch: 14/15	Loss 1.7971 (1.8582)
+2022-11-18 15:41:21,584:INFO: Dataset: univ                Batch: 15/15	Loss 1.8667 (1.8583)
+2022-11-18 15:41:22,001:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7978 (1.7978)
+2022-11-18 15:41:22,171:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9128 (1.8573)
+2022-11-18 15:41:22,343:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9821 (1.9010)
+2022-11-18 15:41:22,505:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8764 (1.8954)
+2022-11-18 15:41:22,664:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8911 (1.8945)
+2022-11-18 15:41:22,819:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8784 (1.8919)
+2022-11-18 15:41:22,975:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8439 (1.8854)
+2022-11-18 15:41:23,131:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8252 (1.8792)
+2022-11-18 15:41:23,533:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8354 (1.8354)
+2022-11-18 15:41:23,685:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9686 (1.9017)
+2022-11-18 15:41:23,847:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9009 (1.9014)
+2022-11-18 15:41:24,020:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8217 (1.8812)
+2022-11-18 15:41:24,174:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9264 (1.8903)
+2022-11-18 15:41:24,338:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8278 (1.8793)
+2022-11-18 15:41:24,510:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8059 (1.8701)
+2022-11-18 15:41:24,668:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7998 (1.8608)
+2022-11-18 15:41:24,821:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8583 (1.8605)
+2022-11-18 15:41:24,984:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7800 (1.8520)
+2022-11-18 15:41:25,168:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8479 (1.8516)
+2022-11-18 15:41:25,344:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8427 (1.8509)
+2022-11-18 15:41:25,513:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8021 (1.8469)
+2022-11-18 15:41:25,676:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7473 (1.8398)
+2022-11-18 15:41:25,843:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9147 (1.8449)
+2022-11-18 15:41:26,000:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8354 (1.8443)
+2022-11-18 15:41:26,166:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9362 (1.8499)
+2022-11-18 15:41:26,309:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8637 (1.8505)
+2022-11-18 15:41:26,359:INFO: - Computing loss (validation)
+2022-11-18 15:41:26,633:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9849 (1.9849)
+2022-11-18 15:41:26,673:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8221 (1.9743)
+2022-11-18 15:41:26,987:INFO: Dataset: univ                Batch: 1/3	Loss 1.8731 (1.8731)
+2022-11-18 15:41:27,066:INFO: Dataset: univ                Batch: 2/3	Loss 1.8762 (1.8744)
+2022-11-18 15:41:27,139:INFO: Dataset: univ                Batch: 3/3	Loss 1.8912 (1.8796)
+2022-11-18 15:41:27,441:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9741 (1.9741)
+2022-11-18 15:41:27,488:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7568 (1.9154)
+2022-11-18 15:41:27,809:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8282 (1.8282)
+2022-11-18 15:41:27,892:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9339 (1.8794)
+2022-11-18 15:41:27,972:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8638 (1.8742)
+2022-11-18 15:41:28,059:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7731 (1.8484)
+2022-11-18 15:41:28,144:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7783 (1.8340)
+2022-11-18 15:41:28,203:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_509.pth.tar
+2022-11-18 15:41:28,203:INFO: 
+===> EPOCH: 510 (P2)
+2022-11-18 15:41:28,204:INFO: - Computing loss (training)
+2022-11-18 15:41:28,583:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8848 (1.8848)
+2022-11-18 15:41:28,743:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8752 (1.8799)
+2022-11-18 15:41:28,916:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8979 (1.8860)
+2022-11-18 15:41:29,044:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9439 (1.8955)
+2022-11-18 15:41:29,443:INFO: Dataset: univ                Batch:  1/15	Loss 1.8888 (1.8888)
+2022-11-18 15:41:29,611:INFO: Dataset: univ                Batch:  2/15	Loss 1.8544 (1.8692)
+2022-11-18 15:41:29,773:INFO: Dataset: univ                Batch:  3/15	Loss 1.8423 (1.8602)
+2022-11-18 15:41:29,943:INFO: Dataset: univ                Batch:  4/15	Loss 1.8354 (1.8538)
+2022-11-18 15:41:30,105:INFO: Dataset: univ                Batch:  5/15	Loss 1.8646 (1.8560)
+2022-11-18 15:41:30,282:INFO: Dataset: univ                Batch:  6/15	Loss 1.9248 (1.8675)
+2022-11-18 15:41:30,463:INFO: Dataset: univ                Batch:  7/15	Loss 1.8040 (1.8597)
+2022-11-18 15:41:30,626:INFO: Dataset: univ                Batch:  8/15	Loss 1.8651 (1.8604)
+2022-11-18 15:41:30,783:INFO: Dataset: univ                Batch:  9/15	Loss 1.8419 (1.8584)
+2022-11-18 15:41:30,953:INFO: Dataset: univ                Batch: 10/15	Loss 1.8480 (1.8574)
+2022-11-18 15:41:31,145:INFO: Dataset: univ                Batch: 11/15	Loss 1.8426 (1.8561)
+2022-11-18 15:41:31,345:INFO: Dataset: univ                Batch: 12/15	Loss 1.8197 (1.8531)
+2022-11-18 15:41:31,550:INFO: Dataset: univ                Batch: 13/15	Loss 1.7964 (1.8486)
+2022-11-18 15:41:31,735:INFO: Dataset: univ                Batch: 14/15	Loss 1.8532 (1.8489)
+2022-11-18 15:41:31,836:INFO: Dataset: univ                Batch: 15/15	Loss 1.9420 (1.8501)
+2022-11-18 15:41:32,304:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9878 (1.9878)
+2022-11-18 15:41:32,480:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8993 (1.9413)
+2022-11-18 15:41:32,641:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9247 (1.9358)
+2022-11-18 15:41:32,796:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9275 (1.9340)
+2022-11-18 15:41:32,953:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8284 (1.9123)
+2022-11-18 15:41:33,117:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8709 (1.9056)
+2022-11-18 15:41:33,281:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0241 (1.9241)
+2022-11-18 15:41:33,431:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8535 (1.9164)
+2022-11-18 15:41:33,827:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9793 (1.9793)
+2022-11-18 15:41:33,983:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7916 (1.8805)
+2022-11-18 15:41:34,140:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8811 (1.8807)
+2022-11-18 15:41:34,296:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8731 (1.8786)
+2022-11-18 15:41:34,454:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6966 (1.8426)
+2022-11-18 15:41:34,614:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9239 (1.8567)
+2022-11-18 15:41:34,768:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9738 (1.8728)
+2022-11-18 15:41:34,921:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8243 (1.8670)
+2022-11-18 15:41:35,076:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7753 (1.8569)
+2022-11-18 15:41:35,230:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8328 (1.8542)
+2022-11-18 15:41:35,386:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8505 (1.8539)
+2022-11-18 15:41:35,552:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9313 (1.8599)
+2022-11-18 15:41:35,713:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8350 (1.8576)
+2022-11-18 15:41:35,868:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9270 (1.8628)
+2022-11-18 15:41:36,023:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9892 (1.8710)
+2022-11-18 15:41:36,177:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8086 (1.8667)
+2022-11-18 15:41:36,336:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9477 (1.8718)
+2022-11-18 15:41:36,478:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8975 (1.8730)
+2022-11-18 15:41:36,524:INFO: - Computing loss (validation)
+2022-11-18 15:41:36,807:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8719 (1.8719)
+2022-11-18 15:41:36,843:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8371 (1.8694)
+2022-11-18 15:41:37,152:INFO: Dataset: univ                Batch: 1/3	Loss 1.8243 (1.8243)
+2022-11-18 15:41:37,226:INFO: Dataset: univ                Batch: 2/3	Loss 1.8433 (1.8326)
+2022-11-18 15:41:37,300:INFO: Dataset: univ                Batch: 3/3	Loss 1.8400 (1.8350)
+2022-11-18 15:41:37,615:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8223 (1.8223)
+2022-11-18 15:41:37,664:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9140 (1.8447)
+2022-11-18 15:41:37,971:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8592 (1.8592)
+2022-11-18 15:41:38,046:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8716 (1.8654)
+2022-11-18 15:41:38,122:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8842 (1.8715)
+2022-11-18 15:41:38,196:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8074 (1.8541)
+2022-11-18 15:41:38,271:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7466 (1.8328)
+2022-11-18 15:41:38,326:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_510.pth.tar
+2022-11-18 15:41:38,326:INFO: 
+===> EPOCH: 511 (P2)
+2022-11-18 15:41:38,327:INFO: - Computing loss (training)
+2022-11-18 15:41:38,688:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7832 (1.7832)
+2022-11-18 15:41:38,850:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8743 (1.8285)
+2022-11-18 15:41:39,007:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9779 (1.8766)
+2022-11-18 15:41:39,123:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7724 (1.8587)
+2022-11-18 15:41:39,523:INFO: Dataset: univ                Batch:  1/15	Loss 1.8634 (1.8634)
+2022-11-18 15:41:39,690:INFO: Dataset: univ                Batch:  2/15	Loss 1.8438 (1.8529)
+2022-11-18 15:41:39,854:INFO: Dataset: univ                Batch:  3/15	Loss 1.8896 (1.8646)
+2022-11-18 15:41:40,015:INFO: Dataset: univ                Batch:  4/15	Loss 1.8559 (1.8625)
+2022-11-18 15:41:40,177:INFO: Dataset: univ                Batch:  5/15	Loss 1.8597 (1.8619)
+2022-11-18 15:41:40,341:INFO: Dataset: univ                Batch:  6/15	Loss 1.8720 (1.8636)
+2022-11-18 15:41:40,503:INFO: Dataset: univ                Batch:  7/15	Loss 1.8064 (1.8562)
+2022-11-18 15:41:40,663:INFO: Dataset: univ                Batch:  8/15	Loss 1.8142 (1.8509)
+2022-11-18 15:41:40,823:INFO: Dataset: univ                Batch:  9/15	Loss 1.8381 (1.8494)
+2022-11-18 15:41:40,985:INFO: Dataset: univ                Batch: 10/15	Loss 1.8437 (1.8488)
+2022-11-18 15:41:41,148:INFO: Dataset: univ                Batch: 11/15	Loss 1.8159 (1.8457)
+2022-11-18 15:41:41,307:INFO: Dataset: univ                Batch: 12/15	Loss 1.8064 (1.8426)
+2022-11-18 15:41:41,468:INFO: Dataset: univ                Batch: 13/15	Loss 1.8938 (1.8463)
+2022-11-18 15:41:41,629:INFO: Dataset: univ                Batch: 14/15	Loss 1.8160 (1.8440)
+2022-11-18 15:41:41,715:INFO: Dataset: univ                Batch: 15/15	Loss 1.9091 (1.8450)
+2022-11-18 15:41:42,106:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9217 (1.9217)
+2022-11-18 15:41:42,253:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7623 (1.8391)
+2022-11-18 15:41:42,404:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9786 (1.8874)
+2022-11-18 15:41:42,553:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9381 (1.9015)
+2022-11-18 15:41:42,701:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9542 (1.9123)
+2022-11-18 15:41:42,849:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8963 (1.9096)
+2022-11-18 15:41:42,999:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7834 (1.8908)
+2022-11-18 15:41:43,138:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9809 (1.9008)
+2022-11-18 15:41:43,527:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8611 (1.8611)
+2022-11-18 15:41:43,681:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8214 (1.8419)
+2022-11-18 15:41:43,830:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8861 (1.8559)
+2022-11-18 15:41:43,979:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8968 (1.8656)
+2022-11-18 15:41:44,128:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9472 (1.8828)
+2022-11-18 15:41:44,278:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8828 (1.8828)
+2022-11-18 15:41:44,426:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8594 (1.8801)
+2022-11-18 15:41:44,573:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7647 (1.8666)
+2022-11-18 15:41:44,722:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8757 (1.8675)
+2022-11-18 15:41:44,870:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9453 (1.8764)
+2022-11-18 15:41:45,020:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8757 (1.8763)
+2022-11-18 15:41:45,168:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8451 (1.8739)
+2022-11-18 15:41:45,316:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8265 (1.8701)
+2022-11-18 15:41:45,468:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8992 (1.8719)
+2022-11-18 15:41:45,618:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7619 (1.8652)
+2022-11-18 15:41:45,766:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8379 (1.8635)
+2022-11-18 15:41:45,917:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8875 (1.8649)
+2022-11-18 15:41:46,054:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9485 (1.8687)
+2022-11-18 15:41:46,099:INFO: - Computing loss (validation)
+2022-11-18 15:41:46,370:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8484 (1.8484)
+2022-11-18 15:41:46,406:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5472 (1.8299)
+2022-11-18 15:41:46,737:INFO: Dataset: univ                Batch: 1/3	Loss 1.8204 (1.8204)
+2022-11-18 15:41:46,823:INFO: Dataset: univ                Batch: 2/3	Loss 1.7735 (1.7969)
+2022-11-18 15:41:46,903:INFO: Dataset: univ                Batch: 3/3	Loss 1.9155 (1.8377)
+2022-11-18 15:41:47,204:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8221 (1.8221)
+2022-11-18 15:41:47,250:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7922 (1.8150)
+2022-11-18 15:41:47,622:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8632 (1.8632)
+2022-11-18 15:41:47,711:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9334 (1.8976)
+2022-11-18 15:41:47,800:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8739 (1.8900)
+2022-11-18 15:41:47,893:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9046 (1.8938)
+2022-11-18 15:41:47,982:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9437 (1.9028)
+2022-11-18 15:41:48,047:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_511.pth.tar
+2022-11-18 15:41:48,048:INFO: 
+===> EPOCH: 512 (P2)
+2022-11-18 15:41:48,048:INFO: - Computing loss (training)
+2022-11-18 15:41:48,434:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9213 (1.9213)
+2022-11-18 15:41:48,611:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8275 (1.8760)
+2022-11-18 15:41:48,787:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8728 (1.8749)
+2022-11-18 15:41:48,918:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0608 (1.9043)
+2022-11-18 15:41:49,342:INFO: Dataset: univ                Batch:  1/15	Loss 1.8227 (1.8227)
+2022-11-18 15:41:49,517:INFO: Dataset: univ                Batch:  2/15	Loss 1.8421 (1.8330)
+2022-11-18 15:41:49,687:INFO: Dataset: univ                Batch:  3/15	Loss 1.8285 (1.8315)
+2022-11-18 15:41:49,866:INFO: Dataset: univ                Batch:  4/15	Loss 1.8530 (1.8371)
+2022-11-18 15:41:50,034:INFO: Dataset: univ                Batch:  5/15	Loss 1.8435 (1.8383)
+2022-11-18 15:41:50,201:INFO: Dataset: univ                Batch:  6/15	Loss 1.8540 (1.8409)
+2022-11-18 15:41:50,372:INFO: Dataset: univ                Batch:  7/15	Loss 1.8477 (1.8419)
+2022-11-18 15:41:50,534:INFO: Dataset: univ                Batch:  8/15	Loss 1.8997 (1.8499)
+2022-11-18 15:41:50,693:INFO: Dataset: univ                Batch:  9/15	Loss 1.8960 (1.8551)
+2022-11-18 15:41:50,855:INFO: Dataset: univ                Batch: 10/15	Loss 1.8023 (1.8499)
+2022-11-18 15:41:51,013:INFO: Dataset: univ                Batch: 11/15	Loss 1.8526 (1.8501)
+2022-11-18 15:41:51,168:INFO: Dataset: univ                Batch: 12/15	Loss 1.8211 (1.8480)
+2022-11-18 15:41:51,323:INFO: Dataset: univ                Batch: 13/15	Loss 1.8571 (1.8486)
+2022-11-18 15:41:51,480:INFO: Dataset: univ                Batch: 14/15	Loss 1.8434 (1.8482)
+2022-11-18 15:41:51,563:INFO: Dataset: univ                Batch: 15/15	Loss 1.8925 (1.8487)
+2022-11-18 15:41:51,968:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8613 (1.8613)
+2022-11-18 15:41:52,122:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8152 (1.8367)
+2022-11-18 15:41:52,291:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9629 (1.8780)
+2022-11-18 15:41:52,463:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9137 (1.8866)
+2022-11-18 15:41:52,629:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8552 (1.8799)
+2022-11-18 15:41:52,783:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9405 (1.8906)
+2022-11-18 15:41:52,935:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8656 (1.8872)
+2022-11-18 15:41:53,074:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8971 (1.8883)
+2022-11-18 15:41:53,477:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8299 (1.8299)
+2022-11-18 15:41:53,646:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8332 (1.8316)
+2022-11-18 15:41:53,812:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8008 (1.8208)
+2022-11-18 15:41:53,988:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8149 (1.8193)
+2022-11-18 15:41:54,147:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7867 (1.8126)
+2022-11-18 15:41:54,315:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8597 (1.8210)
+2022-11-18 15:41:54,482:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8723 (1.8286)
+2022-11-18 15:41:54,634:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9202 (1.8406)
+2022-11-18 15:41:54,787:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7941 (1.8350)
+2022-11-18 15:41:54,945:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8188 (1.8335)
+2022-11-18 15:41:55,099:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9777 (1.8456)
+2022-11-18 15:41:55,253:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8039 (1.8421)
+2022-11-18 15:41:55,409:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8866 (1.8451)
+2022-11-18 15:41:55,571:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8192 (1.8434)
+2022-11-18 15:41:55,733:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7992 (1.8406)
+2022-11-18 15:41:55,895:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9054 (1.8445)
+2022-11-18 15:41:56,050:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8146 (1.8428)
+2022-11-18 15:41:56,207:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8554 (1.8434)
+2022-11-18 15:41:56,266:INFO: - Computing loss (validation)
+2022-11-18 15:41:56,536:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8549 (1.8549)
+2022-11-18 15:41:56,572:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7734 (1.8493)
+2022-11-18 15:41:56,878:INFO: Dataset: univ                Batch: 1/3	Loss 1.8729 (1.8729)
+2022-11-18 15:41:56,955:INFO: Dataset: univ                Batch: 2/3	Loss 1.9245 (1.8975)
+2022-11-18 15:41:57,028:INFO: Dataset: univ                Batch: 3/3	Loss 1.8407 (1.8786)
+2022-11-18 15:41:57,329:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9864 (1.9864)
+2022-11-18 15:41:57,376:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8883 (1.9625)
+2022-11-18 15:41:57,697:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7640 (1.7640)
+2022-11-18 15:41:57,775:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8510 (1.8043)
+2022-11-18 15:41:57,857:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9067 (1.8387)
+2022-11-18 15:41:57,935:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7537 (1.8172)
+2022-11-18 15:41:58,012:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8183 (1.8175)
+2022-11-18 15:41:58,067:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_512.pth.tar
+2022-11-18 15:41:58,067:INFO: 
+===> EPOCH: 513 (P2)
+2022-11-18 15:41:58,068:INFO: - Computing loss (training)
+2022-11-18 15:41:58,420:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0787 (2.0787)
+2022-11-18 15:41:58,576:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9015 (1.9879)
+2022-11-18 15:41:58,729:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8584 (1.9429)
+2022-11-18 15:41:58,858:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9646 (1.9468)
+2022-11-18 15:41:59,248:INFO: Dataset: univ                Batch:  1/15	Loss 1.8515 (1.8515)
+2022-11-18 15:41:59,408:INFO: Dataset: univ                Batch:  2/15	Loss 1.9174 (1.8801)
+2022-11-18 15:41:59,576:INFO: Dataset: univ                Batch:  3/15	Loss 1.8396 (1.8663)
+2022-11-18 15:41:59,739:INFO: Dataset: univ                Batch:  4/15	Loss 1.7919 (1.8488)
+2022-11-18 15:41:59,901:INFO: Dataset: univ                Batch:  5/15	Loss 1.8573 (1.8506)
+2022-11-18 15:42:00,067:INFO: Dataset: univ                Batch:  6/15	Loss 1.8847 (1.8563)
+2022-11-18 15:42:00,227:INFO: Dataset: univ                Batch:  7/15	Loss 1.8559 (1.8562)
+2022-11-18 15:42:00,386:INFO: Dataset: univ                Batch:  8/15	Loss 1.8206 (1.8522)
+2022-11-18 15:42:00,557:INFO: Dataset: univ                Batch:  9/15	Loss 1.8511 (1.8520)
+2022-11-18 15:42:00,719:INFO: Dataset: univ                Batch: 10/15	Loss 1.8681 (1.8535)
+2022-11-18 15:42:00,882:INFO: Dataset: univ                Batch: 11/15	Loss 1.8506 (1.8533)
+2022-11-18 15:42:01,044:INFO: Dataset: univ                Batch: 12/15	Loss 1.8444 (1.8525)
+2022-11-18 15:42:01,204:INFO: Dataset: univ                Batch: 13/15	Loss 1.8707 (1.8539)
+2022-11-18 15:42:01,364:INFO: Dataset: univ                Batch: 14/15	Loss 1.8813 (1.8557)
+2022-11-18 15:42:01,452:INFO: Dataset: univ                Batch: 15/15	Loss 1.9918 (1.8577)
+2022-11-18 15:42:01,866:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8573 (1.8573)
+2022-11-18 15:42:02,032:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9808 (1.9195)
+2022-11-18 15:42:02,193:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8287 (1.8892)
+2022-11-18 15:42:02,345:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8806 (1.8870)
+2022-11-18 15:42:02,501:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9010 (1.8900)
+2022-11-18 15:42:02,657:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9064 (1.8926)
+2022-11-18 15:42:02,811:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7603 (1.8716)
+2022-11-18 15:42:02,953:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9482 (1.8798)
+2022-11-18 15:42:03,350:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8363 (1.8363)
+2022-11-18 15:42:03,507:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8093 (1.8233)
+2022-11-18 15:42:03,666:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8418 (1.8299)
+2022-11-18 15:42:03,821:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8484 (1.8345)
+2022-11-18 15:42:03,988:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8053 (1.8289)
+2022-11-18 15:42:04,143:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8103 (1.8256)
+2022-11-18 15:42:04,298:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8974 (1.8360)
+2022-11-18 15:42:04,453:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8109 (1.8327)
+2022-11-18 15:42:04,609:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8035 (1.8294)
+2022-11-18 15:42:04,764:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8499 (1.8315)
+2022-11-18 15:42:04,922:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8455 (1.8328)
+2022-11-18 15:42:05,075:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9019 (1.8381)
+2022-11-18 15:42:05,230:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8829 (1.8420)
+2022-11-18 15:42:05,383:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8855 (1.8449)
+2022-11-18 15:42:05,541:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8511 (1.8454)
+2022-11-18 15:42:05,696:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8214 (1.8439)
+2022-11-18 15:42:05,853:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8676 (1.8453)
+2022-11-18 15:42:05,996:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8760 (1.8468)
+2022-11-18 15:42:06,042:INFO: - Computing loss (validation)
+2022-11-18 15:42:06,312:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9902 (1.9902)
+2022-11-18 15:42:06,348:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6012 (1.9544)
+2022-11-18 15:42:06,660:INFO: Dataset: univ                Batch: 1/3	Loss 1.8790 (1.8790)
+2022-11-18 15:42:06,737:INFO: Dataset: univ                Batch: 2/3	Loss 1.8449 (1.8620)
+2022-11-18 15:42:06,810:INFO: Dataset: univ                Batch: 3/3	Loss 1.8968 (1.8723)
+2022-11-18 15:42:07,106:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9094 (1.9094)
+2022-11-18 15:42:07,153:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8789 (1.9032)
+2022-11-18 15:42:07,462:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9169 (1.9169)
+2022-11-18 15:42:07,537:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8234 (1.8702)
+2022-11-18 15:42:07,611:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7926 (1.8440)
+2022-11-18 15:42:07,686:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9484 (1.8717)
+2022-11-18 15:42:07,761:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8953 (1.8765)
+2022-11-18 15:42:07,813:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_513.pth.tar
+2022-11-18 15:42:07,813:INFO: 
+===> EPOCH: 514 (P2)
+2022-11-18 15:42:07,814:INFO: - Computing loss (training)
+2022-11-18 15:42:08,164:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7469 (1.7469)
+2022-11-18 15:42:08,316:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7889 (1.7674)
+2022-11-18 15:42:08,468:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9267 (1.8196)
+2022-11-18 15:42:08,584:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9891 (1.8507)
+2022-11-18 15:42:09,002:INFO: Dataset: univ                Batch:  1/15	Loss 1.8766 (1.8766)
+2022-11-18 15:42:09,159:INFO: Dataset: univ                Batch:  2/15	Loss 1.8775 (1.8771)
+2022-11-18 15:42:09,318:INFO: Dataset: univ                Batch:  3/15	Loss 1.8273 (1.8606)
+2022-11-18 15:42:09,473:INFO: Dataset: univ                Batch:  4/15	Loss 1.9110 (1.8731)
+2022-11-18 15:42:09,629:INFO: Dataset: univ                Batch:  5/15	Loss 1.9032 (1.8792)
+2022-11-18 15:42:09,788:INFO: Dataset: univ                Batch:  6/15	Loss 1.7948 (1.8644)
+2022-11-18 15:42:09,944:INFO: Dataset: univ                Batch:  7/15	Loss 1.8735 (1.8657)
+2022-11-18 15:42:10,177:INFO: Dataset: univ                Batch:  8/15	Loss 1.8592 (1.8650)
+2022-11-18 15:42:10,331:INFO: Dataset: univ                Batch:  9/15	Loss 1.8438 (1.8625)
+2022-11-18 15:42:10,489:INFO: Dataset: univ                Batch: 10/15	Loss 1.8547 (1.8617)
+2022-11-18 15:42:10,642:INFO: Dataset: univ                Batch: 11/15	Loss 1.8645 (1.8619)
+2022-11-18 15:42:10,800:INFO: Dataset: univ                Batch: 12/15	Loss 1.9257 (1.8669)
+2022-11-18 15:42:10,954:INFO: Dataset: univ                Batch: 13/15	Loss 1.8389 (1.8650)
+2022-11-18 15:42:11,110:INFO: Dataset: univ                Batch: 14/15	Loss 1.8758 (1.8657)
+2022-11-18 15:42:11,191:INFO: Dataset: univ                Batch: 15/15	Loss 1.7589 (1.8644)
+2022-11-18 15:42:11,590:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9817 (1.9817)
+2022-11-18 15:42:11,737:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8843 (1.9315)
+2022-11-18 15:42:11,886:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8882 (1.9166)
+2022-11-18 15:42:12,034:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8503 (1.8995)
+2022-11-18 15:42:12,182:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9138 (1.9023)
+2022-11-18 15:42:12,332:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8754 (1.8980)
+2022-11-18 15:42:12,482:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8854 (1.8961)
+2022-11-18 15:42:12,617:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9605 (1.9028)
+2022-11-18 15:42:13,015:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8618 (1.8618)
+2022-11-18 15:42:13,164:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8665 (1.8642)
+2022-11-18 15:42:13,315:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7855 (1.8377)
+2022-11-18 15:42:13,470:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9336 (1.8631)
+2022-11-18 15:42:13,620:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9017 (1.8705)
+2022-11-18 15:42:13,771:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7730 (1.8549)
+2022-11-18 15:42:13,920:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8321 (1.8514)
+2022-11-18 15:42:14,068:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9610 (1.8643)
+2022-11-18 15:42:14,216:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7819 (1.8547)
+2022-11-18 15:42:14,363:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7847 (1.8478)
+2022-11-18 15:42:14,513:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8042 (1.8436)
+2022-11-18 15:42:14,661:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7783 (1.8382)
+2022-11-18 15:42:14,810:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9291 (1.8442)
+2022-11-18 15:42:14,960:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8331 (1.8434)
+2022-11-18 15:42:15,110:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8556 (1.8443)
+2022-11-18 15:42:15,259:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7902 (1.8410)
+2022-11-18 15:42:15,408:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8774 (1.8433)
+2022-11-18 15:42:15,547:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8105 (1.8415)
+2022-11-18 15:42:15,591:INFO: - Computing loss (validation)
+2022-11-18 15:42:15,846:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8838 (1.8838)
+2022-11-18 15:42:15,881:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3531 (1.9206)
+2022-11-18 15:42:16,188:INFO: Dataset: univ                Batch: 1/3	Loss 1.8853 (1.8853)
+2022-11-18 15:42:16,266:INFO: Dataset: univ                Batch: 2/3	Loss 1.8369 (1.8604)
+2022-11-18 15:42:16,338:INFO: Dataset: univ                Batch: 3/3	Loss 1.8566 (1.8591)
+2022-11-18 15:42:16,648:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9726 (1.9726)
+2022-11-18 15:42:16,697:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9396 (1.9647)
+2022-11-18 15:42:17,004:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8723 (1.8723)
+2022-11-18 15:42:17,081:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7441 (1.8121)
+2022-11-18 15:42:17,155:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8550 (1.8267)
+2022-11-18 15:42:17,231:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8106 (1.8226)
+2022-11-18 15:42:17,305:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8055 (1.8193)
+2022-11-18 15:42:17,357:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_514.pth.tar
+2022-11-18 15:42:17,357:INFO: 
+===> EPOCH: 515 (P2)
+2022-11-18 15:42:17,357:INFO: - Computing loss (training)
+2022-11-18 15:42:17,704:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7883 (1.7883)
+2022-11-18 15:42:17,860:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0430 (1.9132)
+2022-11-18 15:42:18,015:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8715 (1.8994)
+2022-11-18 15:42:18,134:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0012 (1.9182)
+2022-11-18 15:42:18,528:INFO: Dataset: univ                Batch:  1/15	Loss 1.8877 (1.8877)
+2022-11-18 15:42:18,684:INFO: Dataset: univ                Batch:  2/15	Loss 1.8783 (1.8831)
+2022-11-18 15:42:18,842:INFO: Dataset: univ                Batch:  3/15	Loss 1.8265 (1.8649)
+2022-11-18 15:42:18,998:INFO: Dataset: univ                Batch:  4/15	Loss 1.8655 (1.8651)
+2022-11-18 15:42:19,161:INFO: Dataset: univ                Batch:  5/15	Loss 1.8899 (1.8702)
+2022-11-18 15:42:19,325:INFO: Dataset: univ                Batch:  6/15	Loss 1.8989 (1.8748)
+2022-11-18 15:42:19,484:INFO: Dataset: univ                Batch:  7/15	Loss 1.8166 (1.8664)
+2022-11-18 15:42:19,638:INFO: Dataset: univ                Batch:  8/15	Loss 1.8495 (1.8642)
+2022-11-18 15:42:19,795:INFO: Dataset: univ                Batch:  9/15	Loss 1.9040 (1.8690)
+2022-11-18 15:42:19,949:INFO: Dataset: univ                Batch: 10/15	Loss 1.8162 (1.8639)
+2022-11-18 15:42:20,108:INFO: Dataset: univ                Batch: 11/15	Loss 1.8401 (1.8618)
+2022-11-18 15:42:20,262:INFO: Dataset: univ                Batch: 12/15	Loss 1.8835 (1.8637)
+2022-11-18 15:42:20,417:INFO: Dataset: univ                Batch: 13/15	Loss 1.8807 (1.8649)
+2022-11-18 15:42:20,573:INFO: Dataset: univ                Batch: 14/15	Loss 1.8902 (1.8667)
+2022-11-18 15:42:20,656:INFO: Dataset: univ                Batch: 15/15	Loss 1.8564 (1.8665)
+2022-11-18 15:42:21,051:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6769 (1.6769)
+2022-11-18 15:42:21,202:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8423 (1.7548)
+2022-11-18 15:42:21,354:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9114 (1.8112)
+2022-11-18 15:42:21,509:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8989 (1.8322)
+2022-11-18 15:42:21,660:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9073 (1.8482)
+2022-11-18 15:42:21,813:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9779 (1.8709)
+2022-11-18 15:42:21,966:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8585 (1.8690)
+2022-11-18 15:42:22,104:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9343 (1.8766)
+2022-11-18 15:42:22,492:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7920 (1.7920)
+2022-11-18 15:42:22,644:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7942 (1.7931)
+2022-11-18 15:42:22,801:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8808 (1.8221)
+2022-11-18 15:42:22,952:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9396 (1.8507)
+2022-11-18 15:42:23,108:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7176 (1.8227)
+2022-11-18 15:42:23,260:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8940 (1.8354)
+2022-11-18 15:42:23,412:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7956 (1.8296)
+2022-11-18 15:42:23,562:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9068 (1.8396)
+2022-11-18 15:42:23,713:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7768 (1.8328)
+2022-11-18 15:42:23,861:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8886 (1.8386)
+2022-11-18 15:42:24,012:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8493 (1.8395)
+2022-11-18 15:42:24,162:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8605 (1.8412)
+2022-11-18 15:42:24,312:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8833 (1.8445)
+2022-11-18 15:42:24,464:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8164 (1.8422)
+2022-11-18 15:42:24,615:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8102 (1.8397)
+2022-11-18 15:42:24,764:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8201 (1.8384)
+2022-11-18 15:42:24,917:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8226 (1.8376)
+2022-11-18 15:42:25,056:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8213 (1.8369)
+2022-11-18 15:42:25,102:INFO: - Computing loss (validation)
+2022-11-18 15:42:25,362:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9873 (1.9873)
+2022-11-18 15:42:25,398:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9088 (1.9809)
+2022-11-18 15:42:25,722:INFO: Dataset: univ                Batch: 1/3	Loss 1.8263 (1.8263)
+2022-11-18 15:42:25,802:INFO: Dataset: univ                Batch: 2/3	Loss 1.9479 (1.8858)
+2022-11-18 15:42:25,879:INFO: Dataset: univ                Batch: 3/3	Loss 1.8183 (1.8605)
+2022-11-18 15:42:26,184:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8635 (1.8635)
+2022-11-18 15:42:26,232:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8143 (1.8501)
+2022-11-18 15:42:26,546:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9041 (1.9041)
+2022-11-18 15:42:26,624:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8054 (1.8568)
+2022-11-18 15:42:26,703:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8372 (1.8498)
+2022-11-18 15:42:26,783:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8656 (1.8539)
+2022-11-18 15:42:26,860:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7895 (1.8401)
+2022-11-18 15:42:26,918:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_515.pth.tar
+2022-11-18 15:42:26,918:INFO: 
+===> EPOCH: 516 (P2)
+2022-11-18 15:42:26,918:INFO: - Computing loss (training)
+2022-11-18 15:42:27,263:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9399 (1.9399)
+2022-11-18 15:42:27,419:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9659 (1.9536)
+2022-11-18 15:42:27,573:INFO: Dataset: hotel               Batch: 3/4	Loss 2.0667 (1.9903)
+2022-11-18 15:42:27,691:INFO: Dataset: hotel               Batch: 4/4	Loss 1.6887 (1.9365)
+2022-11-18 15:42:28,096:INFO: Dataset: univ                Batch:  1/15	Loss 1.7986 (1.7986)
+2022-11-18 15:42:28,254:INFO: Dataset: univ                Batch:  2/15	Loss 1.8774 (1.8357)
+2022-11-18 15:42:28,414:INFO: Dataset: univ                Batch:  3/15	Loss 1.8564 (1.8429)
+2022-11-18 15:42:28,572:INFO: Dataset: univ                Batch:  4/15	Loss 1.8573 (1.8464)
+2022-11-18 15:42:28,728:INFO: Dataset: univ                Batch:  5/15	Loss 1.8388 (1.8450)
+2022-11-18 15:42:28,890:INFO: Dataset: univ                Batch:  6/15	Loss 1.8503 (1.8459)
+2022-11-18 15:42:29,046:INFO: Dataset: univ                Batch:  7/15	Loss 1.9089 (1.8552)
+2022-11-18 15:42:29,200:INFO: Dataset: univ                Batch:  8/15	Loss 1.8432 (1.8537)
+2022-11-18 15:42:29,356:INFO: Dataset: univ                Batch:  9/15	Loss 1.8777 (1.8562)
+2022-11-18 15:42:29,512:INFO: Dataset: univ                Batch: 10/15	Loss 1.8728 (1.8579)
+2022-11-18 15:42:29,668:INFO: Dataset: univ                Batch: 11/15	Loss 1.8392 (1.8562)
+2022-11-18 15:42:29,824:INFO: Dataset: univ                Batch: 12/15	Loss 1.9079 (1.8607)
+2022-11-18 15:42:29,982:INFO: Dataset: univ                Batch: 13/15	Loss 1.8113 (1.8564)
+2022-11-18 15:42:30,138:INFO: Dataset: univ                Batch: 14/15	Loss 1.8465 (1.8558)
+2022-11-18 15:42:30,220:INFO: Dataset: univ                Batch: 15/15	Loss 1.8295 (1.8553)
+2022-11-18 15:42:30,615:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9102 (1.9102)
+2022-11-18 15:42:30,769:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7553 (1.8323)
+2022-11-18 15:42:30,932:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8929 (1.8510)
+2022-11-18 15:42:31,100:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7893 (1.8374)
+2022-11-18 15:42:31,252:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8652 (1.8431)
+2022-11-18 15:42:31,402:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9185 (1.8553)
+2022-11-18 15:42:31,554:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9466 (1.8692)
+2022-11-18 15:42:31,702:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9124 (1.8744)
+2022-11-18 15:42:32,141:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8718 (1.8718)
+2022-11-18 15:42:32,298:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8136 (1.8452)
+2022-11-18 15:42:32,458:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8345 (1.8417)
+2022-11-18 15:42:32,612:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8413 (1.8416)
+2022-11-18 15:42:32,774:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8552 (1.8440)
+2022-11-18 15:42:32,933:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7923 (1.8359)
+2022-11-18 15:42:33,095:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8129 (1.8326)
+2022-11-18 15:42:33,258:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8528 (1.8350)
+2022-11-18 15:42:33,424:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8537 (1.8371)
+2022-11-18 15:42:33,591:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8724 (1.8405)
+2022-11-18 15:42:33,775:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8143 (1.8381)
+2022-11-18 15:42:33,943:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9438 (1.8467)
+2022-11-18 15:42:34,099:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8202 (1.8447)
+2022-11-18 15:42:34,260:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8813 (1.8473)
+2022-11-18 15:42:34,418:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8193 (1.8454)
+2022-11-18 15:42:34,576:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8200 (1.8438)
+2022-11-18 15:42:34,736:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8671 (1.8451)
+2022-11-18 15:42:34,885:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8953 (1.8475)
+2022-11-18 15:42:34,935:INFO: - Computing loss (validation)
+2022-11-18 15:42:35,213:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8788 (1.8788)
+2022-11-18 15:42:35,249:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6528 (1.8588)
+2022-11-18 15:42:35,568:INFO: Dataset: univ                Batch: 1/3	Loss 1.8145 (1.8145)
+2022-11-18 15:42:35,653:INFO: Dataset: univ                Batch: 2/3	Loss 1.8569 (1.8356)
+2022-11-18 15:42:35,734:INFO: Dataset: univ                Batch: 3/3	Loss 1.8435 (1.8383)
+2022-11-18 15:42:36,069:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9514 (1.9514)
+2022-11-18 15:42:36,118:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8716 (1.9324)
+2022-11-18 15:42:36,454:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8760 (1.8760)
+2022-11-18 15:42:36,529:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8425 (1.8590)
+2022-11-18 15:42:36,603:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9165 (1.8784)
+2022-11-18 15:42:36,677:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8187 (1.8630)
+2022-11-18 15:42:36,751:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8487 (1.8602)
+2022-11-18 15:42:36,803:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_516.pth.tar
+2022-11-18 15:42:36,803:INFO: 
+===> EPOCH: 517 (P2)
+2022-11-18 15:42:36,804:INFO: - Computing loss (training)
+2022-11-18 15:42:37,170:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8576 (1.8576)
+2022-11-18 15:42:37,342:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7846 (1.8214)
+2022-11-18 15:42:37,511:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9147 (1.8514)
+2022-11-18 15:42:37,634:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9176 (1.8626)
+2022-11-18 15:42:38,071:INFO: Dataset: univ                Batch:  1/15	Loss 1.8894 (1.8894)
+2022-11-18 15:42:38,242:INFO: Dataset: univ                Batch:  2/15	Loss 1.8736 (1.8815)
+2022-11-18 15:42:38,419:INFO: Dataset: univ                Batch:  3/15	Loss 1.8646 (1.8758)
+2022-11-18 15:42:38,588:INFO: Dataset: univ                Batch:  4/15	Loss 1.8096 (1.8599)
+2022-11-18 15:42:38,748:INFO: Dataset: univ                Batch:  5/15	Loss 1.8221 (1.8526)
+2022-11-18 15:42:38,913:INFO: Dataset: univ                Batch:  6/15	Loss 1.8741 (1.8562)
+2022-11-18 15:42:39,079:INFO: Dataset: univ                Batch:  7/15	Loss 1.8397 (1.8538)
+2022-11-18 15:42:39,235:INFO: Dataset: univ                Batch:  8/15	Loss 1.8719 (1.8560)
+2022-11-18 15:42:39,391:INFO: Dataset: univ                Batch:  9/15	Loss 1.8623 (1.8567)
+2022-11-18 15:42:39,548:INFO: Dataset: univ                Batch: 10/15	Loss 1.8308 (1.8542)
+2022-11-18 15:42:39,711:INFO: Dataset: univ                Batch: 11/15	Loss 1.8601 (1.8547)
+2022-11-18 15:42:39,869:INFO: Dataset: univ                Batch: 12/15	Loss 1.8520 (1.8545)
+2022-11-18 15:42:40,027:INFO: Dataset: univ                Batch: 13/15	Loss 1.8399 (1.8532)
+2022-11-18 15:42:40,183:INFO: Dataset: univ                Batch: 14/15	Loss 1.8902 (1.8557)
+2022-11-18 15:42:40,265:INFO: Dataset: univ                Batch: 15/15	Loss 1.7746 (1.8547)
+2022-11-18 15:42:40,668:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7942 (1.7942)
+2022-11-18 15:42:40,818:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9239 (1.8554)
+2022-11-18 15:42:40,970:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9067 (1.8722)
+2022-11-18 15:42:41,120:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8799 (1.8740)
+2022-11-18 15:42:41,272:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9810 (1.8971)
+2022-11-18 15:42:41,426:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7488 (1.8741)
+2022-11-18 15:42:41,595:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8474 (1.8707)
+2022-11-18 15:42:41,753:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7490 (1.8589)
+2022-11-18 15:42:42,176:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7809 (1.7809)
+2022-11-18 15:42:42,347:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8707 (1.8296)
+2022-11-18 15:42:42,507:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8131 (1.8243)
+2022-11-18 15:42:42,684:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8034 (1.8194)
+2022-11-18 15:42:42,848:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8662 (1.8287)
+2022-11-18 15:42:43,019:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7923 (1.8222)
+2022-11-18 15:42:43,199:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8531 (1.8266)
+2022-11-18 15:42:43,355:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8440 (1.8287)
+2022-11-18 15:42:43,516:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9089 (1.8377)
+2022-11-18 15:42:43,695:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8473 (1.8387)
+2022-11-18 15:42:43,874:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8022 (1.8356)
+2022-11-18 15:42:44,033:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8100 (1.8334)
+2022-11-18 15:42:44,191:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8096 (1.8315)
+2022-11-18 15:42:44,345:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9333 (1.8391)
+2022-11-18 15:42:44,501:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8639 (1.8408)
+2022-11-18 15:42:44,654:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8003 (1.8381)
+2022-11-18 15:42:44,808:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9117 (1.8422)
+2022-11-18 15:42:44,950:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9148 (1.8459)
+2022-11-18 15:42:45,001:INFO: - Computing loss (validation)
+2022-11-18 15:42:45,276:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9009 (1.9009)
+2022-11-18 15:42:45,312:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2389 (1.9240)
+2022-11-18 15:42:45,626:INFO: Dataset: univ                Batch: 1/3	Loss 1.8995 (1.8995)
+2022-11-18 15:42:45,703:INFO: Dataset: univ                Batch: 2/3	Loss 1.7948 (1.8453)
+2022-11-18 15:42:45,777:INFO: Dataset: univ                Batch: 3/3	Loss 1.8322 (1.8408)
+2022-11-18 15:42:46,083:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9422 (1.9422)
+2022-11-18 15:42:46,128:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8424 (1.9156)
+2022-11-18 15:42:46,456:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7702 (1.7702)
+2022-11-18 15:42:46,536:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8306 (1.8004)
+2022-11-18 15:42:46,617:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9041 (1.8347)
+2022-11-18 15:42:46,699:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9032 (1.8514)
+2022-11-18 15:42:46,778:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8090 (1.8431)
+2022-11-18 15:42:46,831:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_517.pth.tar
+2022-11-18 15:42:46,831:INFO: 
+===> EPOCH: 518 (P2)
+2022-11-18 15:42:46,832:INFO: - Computing loss (training)
+2022-11-18 15:42:47,177:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9751 (1.9751)
+2022-11-18 15:42:47,329:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8835 (1.9288)
+2022-11-18 15:42:47,481:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9593 (1.9400)
+2022-11-18 15:42:47,597:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8338 (1.9228)
+2022-11-18 15:42:47,994:INFO: Dataset: univ                Batch:  1/15	Loss 1.7880 (1.7880)
+2022-11-18 15:42:48,157:INFO: Dataset: univ                Batch:  2/15	Loss 1.8104 (1.8002)
+2022-11-18 15:42:48,315:INFO: Dataset: univ                Batch:  3/15	Loss 1.8776 (1.8265)
+2022-11-18 15:42:48,473:INFO: Dataset: univ                Batch:  4/15	Loss 1.8730 (1.8381)
+2022-11-18 15:42:48,634:INFO: Dataset: univ                Batch:  5/15	Loss 1.8541 (1.8413)
+2022-11-18 15:42:48,792:INFO: Dataset: univ                Batch:  6/15	Loss 1.8707 (1.8469)
+2022-11-18 15:42:48,950:INFO: Dataset: univ                Batch:  7/15	Loss 1.8521 (1.8477)
+2022-11-18 15:42:49,104:INFO: Dataset: univ                Batch:  8/15	Loss 1.8331 (1.8460)
+2022-11-18 15:42:49,261:INFO: Dataset: univ                Batch:  9/15	Loss 1.8623 (1.8479)
+2022-11-18 15:42:49,424:INFO: Dataset: univ                Batch: 10/15	Loss 1.8666 (1.8498)
+2022-11-18 15:42:49,586:INFO: Dataset: univ                Batch: 11/15	Loss 1.8709 (1.8516)
+2022-11-18 15:42:49,751:INFO: Dataset: univ                Batch: 12/15	Loss 1.8707 (1.8533)
+2022-11-18 15:42:49,910:INFO: Dataset: univ                Batch: 13/15	Loss 1.8696 (1.8546)
+2022-11-18 15:42:50,070:INFO: Dataset: univ                Batch: 14/15	Loss 1.8732 (1.8559)
+2022-11-18 15:42:50,155:INFO: Dataset: univ                Batch: 15/15	Loss 1.8268 (1.8555)
+2022-11-18 15:42:50,572:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9086 (1.9086)
+2022-11-18 15:42:50,732:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9652 (1.9333)
+2022-11-18 15:42:50,897:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7705 (1.8809)
+2022-11-18 15:42:51,057:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7307 (1.8478)
+2022-11-18 15:42:51,219:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7784 (1.8334)
+2022-11-18 15:42:51,379:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8876 (1.8430)
+2022-11-18 15:42:51,541:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8382 (1.8422)
+2022-11-18 15:42:51,687:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0030 (1.8604)
+2022-11-18 15:42:52,087:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8277 (1.8277)
+2022-11-18 15:42:52,243:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7776 (1.8040)
+2022-11-18 15:42:52,395:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8153 (1.8077)
+2022-11-18 15:42:52,549:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7814 (1.8014)
+2022-11-18 15:42:52,701:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7789 (1.7960)
+2022-11-18 15:42:52,856:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7840 (1.7941)
+2022-11-18 15:42:53,008:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8029 (1.7955)
+2022-11-18 15:42:53,157:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8073 (1.7969)
+2022-11-18 15:42:53,310:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9192 (1.8101)
+2022-11-18 15:42:53,463:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7699 (1.8059)
+2022-11-18 15:42:53,614:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8114 (1.8064)
+2022-11-18 15:42:53,764:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8455 (1.8100)
+2022-11-18 15:42:53,915:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8355 (1.8120)
+2022-11-18 15:42:54,067:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9161 (1.8202)
+2022-11-18 15:42:54,219:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7837 (1.8178)
+2022-11-18 15:42:54,369:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8460 (1.8198)
+2022-11-18 15:42:54,524:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7451 (1.8161)
+2022-11-18 15:42:54,667:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8064 (1.8157)
+2022-11-18 15:42:54,713:INFO: - Computing loss (validation)
+2022-11-18 15:42:55,012:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8338 (1.8338)
+2022-11-18 15:42:55,050:INFO: Dataset: hotel               Batch: 2/2	Loss 1.3478 (1.7990)
+2022-11-18 15:42:55,402:INFO: Dataset: univ                Batch: 1/3	Loss 1.8897 (1.8897)
+2022-11-18 15:42:55,488:INFO: Dataset: univ                Batch: 2/3	Loss 1.8806 (1.8849)
+2022-11-18 15:42:55,569:INFO: Dataset: univ                Batch: 3/3	Loss 1.8640 (1.8781)
+2022-11-18 15:42:55,875:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8848 (1.8848)
+2022-11-18 15:42:55,923:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7625 (1.8569)
+2022-11-18 15:42:56,239:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8424 (1.8424)
+2022-11-18 15:42:56,321:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8096 (1.8256)
+2022-11-18 15:42:56,402:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8275 (1.8262)
+2022-11-18 15:42:56,486:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8930 (1.8422)
+2022-11-18 15:42:56,567:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7899 (1.8318)
+2022-11-18 15:42:56,624:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_518.pth.tar
+2022-11-18 15:42:56,624:INFO: 
+===> EPOCH: 519 (P2)
+2022-11-18 15:42:56,625:INFO: - Computing loss (training)
+2022-11-18 15:42:56,986:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8814 (1.8814)
+2022-11-18 15:42:57,143:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9427 (1.9123)
+2022-11-18 15:42:57,299:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9486 (1.9238)
+2022-11-18 15:42:57,418:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0415 (1.9435)
+2022-11-18 15:42:57,823:INFO: Dataset: univ                Batch:  1/15	Loss 1.8378 (1.8378)
+2022-11-18 15:42:57,988:INFO: Dataset: univ                Batch:  2/15	Loss 1.8960 (1.8657)
+2022-11-18 15:42:58,155:INFO: Dataset: univ                Batch:  3/15	Loss 1.8548 (1.8620)
+2022-11-18 15:42:58,312:INFO: Dataset: univ                Batch:  4/15	Loss 1.8729 (1.8650)
+2022-11-18 15:42:58,474:INFO: Dataset: univ                Batch:  5/15	Loss 1.9090 (1.8735)
+2022-11-18 15:42:58,633:INFO: Dataset: univ                Batch:  6/15	Loss 1.8365 (1.8677)
+2022-11-18 15:42:58,792:INFO: Dataset: univ                Batch:  7/15	Loss 1.8526 (1.8656)
+2022-11-18 15:42:58,951:INFO: Dataset: univ                Batch:  8/15	Loss 1.8617 (1.8650)
+2022-11-18 15:42:59,111:INFO: Dataset: univ                Batch:  9/15	Loss 1.8222 (1.8602)
+2022-11-18 15:42:59,268:INFO: Dataset: univ                Batch: 10/15	Loss 1.9269 (1.8664)
+2022-11-18 15:42:59,432:INFO: Dataset: univ                Batch: 11/15	Loss 1.8476 (1.8645)
+2022-11-18 15:42:59,589:INFO: Dataset: univ                Batch: 12/15	Loss 1.8679 (1.8647)
+2022-11-18 15:42:59,748:INFO: Dataset: univ                Batch: 13/15	Loss 1.8353 (1.8624)
+2022-11-18 15:42:59,907:INFO: Dataset: univ                Batch: 14/15	Loss 1.8759 (1.8634)
+2022-11-18 15:42:59,992:INFO: Dataset: univ                Batch: 15/15	Loss 1.8071 (1.8627)
+2022-11-18 15:43:00,391:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7572 (1.7572)
+2022-11-18 15:43:00,558:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9283 (1.8442)
+2022-11-18 15:43:00,711:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7968 (1.8292)
+2022-11-18 15:43:00,859:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7755 (1.8151)
+2022-11-18 15:43:01,013:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7985 (1.8117)
+2022-11-18 15:43:01,163:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8686 (1.8219)
+2022-11-18 15:43:01,321:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9202 (1.8356)
+2022-11-18 15:43:01,468:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8530 (1.8375)
+2022-11-18 15:43:01,872:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8949 (1.8949)
+2022-11-18 15:43:02,035:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8799 (1.8871)
+2022-11-18 15:43:02,214:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7182 (1.8337)
+2022-11-18 15:43:02,381:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9191 (1.8562)
+2022-11-18 15:43:02,552:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8090 (1.8459)
+2022-11-18 15:43:02,728:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8180 (1.8416)
+2022-11-18 15:43:02,894:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8460 (1.8422)
+2022-11-18 15:43:03,104:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8301 (1.8406)
+2022-11-18 15:43:03,307:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8935 (1.8460)
+2022-11-18 15:43:03,499:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8753 (1.8492)
+2022-11-18 15:43:03,664:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8733 (1.8513)
+2022-11-18 15:43:03,818:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7689 (1.8446)
+2022-11-18 15:43:03,975:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8944 (1.8487)
+2022-11-18 15:43:04,135:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7742 (1.8433)
+2022-11-18 15:43:04,293:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9674 (1.8512)
+2022-11-18 15:43:04,454:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8468 (1.8510)
+2022-11-18 15:43:04,612:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9014 (1.8541)
+2022-11-18 15:43:04,751:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8162 (1.8522)
+2022-11-18 15:43:04,795:INFO: - Computing loss (validation)
+2022-11-18 15:43:05,067:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8891 (1.8891)
+2022-11-18 15:43:05,103:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0012 (1.8952)
+2022-11-18 15:43:05,414:INFO: Dataset: univ                Batch: 1/3	Loss 1.8450 (1.8450)
+2022-11-18 15:43:05,491:INFO: Dataset: univ                Batch: 2/3	Loss 1.8858 (1.8626)
+2022-11-18 15:43:05,564:INFO: Dataset: univ                Batch: 3/3	Loss 1.8897 (1.8700)
+2022-11-18 15:43:05,875:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8669 (1.8669)
+2022-11-18 15:43:05,922:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8120 (1.8528)
+2022-11-18 15:43:06,236:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8133 (1.8133)
+2022-11-18 15:43:06,311:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8193 (1.8161)
+2022-11-18 15:43:06,386:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8296 (1.8205)
+2022-11-18 15:43:06,462:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7751 (1.8093)
+2022-11-18 15:43:06,536:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7988 (1.8072)
+2022-11-18 15:43:06,587:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_519.pth.tar
+2022-11-18 15:43:06,587:INFO: 
+===> EPOCH: 520 (P2)
+2022-11-18 15:43:06,588:INFO: - Computing loss (training)
+2022-11-18 15:43:06,929:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8331 (1.8331)
+2022-11-18 15:43:07,090:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9459 (1.8882)
+2022-11-18 15:43:07,247:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9848 (1.9197)
+2022-11-18 15:43:07,367:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0039 (1.9330)
+2022-11-18 15:43:07,810:INFO: Dataset: univ                Batch:  1/15	Loss 1.8442 (1.8442)
+2022-11-18 15:43:07,985:INFO: Dataset: univ                Batch:  2/15	Loss 1.9093 (1.8771)
+2022-11-18 15:43:08,154:INFO: Dataset: univ                Batch:  3/15	Loss 1.8784 (1.8775)
+2022-11-18 15:43:08,322:INFO: Dataset: univ                Batch:  4/15	Loss 1.8725 (1.8763)
+2022-11-18 15:43:08,484:INFO: Dataset: univ                Batch:  5/15	Loss 1.8544 (1.8721)
+2022-11-18 15:43:08,646:INFO: Dataset: univ                Batch:  6/15	Loss 1.8283 (1.8641)
+2022-11-18 15:43:08,804:INFO: Dataset: univ                Batch:  7/15	Loss 1.8198 (1.8580)
+2022-11-18 15:43:08,961:INFO: Dataset: univ                Batch:  8/15	Loss 1.8112 (1.8516)
+2022-11-18 15:43:09,121:INFO: Dataset: univ                Batch:  9/15	Loss 1.8081 (1.8466)
+2022-11-18 15:43:09,278:INFO: Dataset: univ                Batch: 10/15	Loss 1.8134 (1.8432)
+2022-11-18 15:43:09,441:INFO: Dataset: univ                Batch: 11/15	Loss 1.8323 (1.8422)
+2022-11-18 15:43:09,609:INFO: Dataset: univ                Batch: 12/15	Loss 1.8647 (1.8441)
+2022-11-18 15:43:09,772:INFO: Dataset: univ                Batch: 13/15	Loss 1.8216 (1.8422)
+2022-11-18 15:43:09,932:INFO: Dataset: univ                Batch: 14/15	Loss 1.8969 (1.8457)
+2022-11-18 15:43:10,020:INFO: Dataset: univ                Batch: 15/15	Loss 1.7881 (1.8448)
+2022-11-18 15:43:10,418:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9361 (1.9361)
+2022-11-18 15:43:10,595:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9130 (1.9251)
+2022-11-18 15:43:10,763:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8220 (1.8927)
+2022-11-18 15:43:10,921:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9334 (1.9026)
+2022-11-18 15:43:11,083:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8567 (1.8947)
+2022-11-18 15:43:11,253:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7762 (1.8752)
+2022-11-18 15:43:11,441:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8093 (1.8661)
+2022-11-18 15:43:11,602:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9328 (1.8737)
+2022-11-18 15:43:12,054:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8249 (1.8249)
+2022-11-18 15:43:12,230:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8799 (1.8508)
+2022-11-18 15:43:12,402:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8468 (1.8493)
+2022-11-18 15:43:12,571:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8621 (1.8524)
+2022-11-18 15:43:12,726:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8684 (1.8556)
+2022-11-18 15:43:12,881:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8517 (1.8550)
+2022-11-18 15:43:13,035:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8823 (1.8589)
+2022-11-18 15:43:13,185:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8446 (1.8570)
+2022-11-18 15:43:13,337:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7654 (1.8476)
+2022-11-18 15:43:13,490:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8737 (1.8502)
+2022-11-18 15:43:13,647:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7556 (1.8415)
+2022-11-18 15:43:13,799:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8846 (1.8447)
+2022-11-18 15:43:13,953:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8388 (1.8442)
+2022-11-18 15:43:14,108:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8457 (1.8443)
+2022-11-18 15:43:14,261:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8397 (1.8440)
+2022-11-18 15:43:14,414:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8741 (1.8458)
+2022-11-18 15:43:14,567:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8886 (1.8480)
+2022-11-18 15:43:14,714:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8924 (1.8499)
+2022-11-18 15:43:14,763:INFO: - Computing loss (validation)
+2022-11-18 15:43:15,040:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9116 (1.9116)
+2022-11-18 15:43:15,076:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0574 (1.9221)
+2022-11-18 15:43:15,406:INFO: Dataset: univ                Batch: 1/3	Loss 1.9178 (1.9178)
+2022-11-18 15:43:15,487:INFO: Dataset: univ                Batch: 2/3	Loss 1.8416 (1.8858)
+2022-11-18 15:43:15,565:INFO: Dataset: univ                Batch: 3/3	Loss 1.8409 (1.8716)
+2022-11-18 15:43:15,883:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8908 (1.8908)
+2022-11-18 15:43:15,930:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8684 (1.8852)
+2022-11-18 15:43:16,310:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9261 (1.9261)
+2022-11-18 15:43:16,400:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9062 (1.9162)
+2022-11-18 15:43:16,507:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8272 (1.8851)
+2022-11-18 15:43:16,590:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8936 (1.8873)
+2022-11-18 15:43:16,674:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9479 (1.8987)
+2022-11-18 15:43:16,746:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_520.pth.tar
+2022-11-18 15:43:16,747:INFO: 
+===> EPOCH: 521 (P2)
+2022-11-18 15:43:16,748:INFO: - Computing loss (training)
+2022-11-18 15:43:17,167:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9282 (1.9282)
+2022-11-18 15:43:17,347:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7327 (1.8314)
+2022-11-18 15:43:17,513:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9303 (1.8639)
+2022-11-18 15:43:17,644:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7421 (1.8432)
+2022-11-18 15:43:18,125:INFO: Dataset: univ                Batch:  1/15	Loss 1.8334 (1.8334)
+2022-11-18 15:43:18,311:INFO: Dataset: univ                Batch:  2/15	Loss 1.8670 (1.8505)
+2022-11-18 15:43:18,498:INFO: Dataset: univ                Batch:  3/15	Loss 1.8400 (1.8470)
+2022-11-18 15:43:18,685:INFO: Dataset: univ                Batch:  4/15	Loss 1.8741 (1.8538)
+2022-11-18 15:43:18,887:INFO: Dataset: univ                Batch:  5/15	Loss 1.8986 (1.8628)
+2022-11-18 15:43:19,111:INFO: Dataset: univ                Batch:  6/15	Loss 1.8242 (1.8561)
+2022-11-18 15:43:19,303:INFO: Dataset: univ                Batch:  7/15	Loss 1.8926 (1.8611)
+2022-11-18 15:43:19,486:INFO: Dataset: univ                Batch:  8/15	Loss 1.8882 (1.8647)
+2022-11-18 15:43:19,681:INFO: Dataset: univ                Batch:  9/15	Loss 1.8318 (1.8611)
+2022-11-18 15:43:19,876:INFO: Dataset: univ                Batch: 10/15	Loss 1.8690 (1.8620)
+2022-11-18 15:43:20,115:INFO: Dataset: univ                Batch: 11/15	Loss 1.8905 (1.8644)
+2022-11-18 15:43:20,327:INFO: Dataset: univ                Batch: 12/15	Loss 1.8554 (1.8637)
+2022-11-18 15:43:20,554:INFO: Dataset: univ                Batch: 13/15	Loss 1.8414 (1.8621)
+2022-11-18 15:43:20,817:INFO: Dataset: univ                Batch: 14/15	Loss 1.8666 (1.8624)
+2022-11-18 15:43:20,940:INFO: Dataset: univ                Batch: 15/15	Loss 1.8922 (1.8628)
+2022-11-18 15:43:21,407:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7537 (1.7537)
+2022-11-18 15:43:21,638:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7545 (1.7541)
+2022-11-18 15:43:21,862:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8178 (1.7762)
+2022-11-18 15:43:22,092:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9935 (1.8308)
+2022-11-18 15:43:22,353:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8115 (1.8270)
+2022-11-18 15:43:22,553:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0688 (1.8679)
+2022-11-18 15:43:22,723:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8104 (1.8603)
+2022-11-18 15:43:22,879:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7525 (1.8484)
+2022-11-18 15:43:23,369:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7873 (1.7873)
+2022-11-18 15:43:23,534:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8339 (1.8114)
+2022-11-18 15:43:23,701:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8023 (1.8084)
+2022-11-18 15:43:23,871:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8625 (1.8220)
+2022-11-18 15:43:24,038:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8263 (1.8229)
+2022-11-18 15:43:24,215:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8494 (1.8275)
+2022-11-18 15:43:24,380:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7576 (1.8175)
+2022-11-18 15:43:24,554:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7718 (1.8123)
+2022-11-18 15:43:24,727:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8805 (1.8195)
+2022-11-18 15:43:24,905:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8260 (1.8201)
+2022-11-18 15:43:25,078:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8340 (1.8213)
+2022-11-18 15:43:25,249:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7594 (1.8161)
+2022-11-18 15:43:25,420:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8508 (1.8189)
+2022-11-18 15:43:25,590:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8287 (1.8196)
+2022-11-18 15:43:25,760:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8386 (1.8210)
+2022-11-18 15:43:25,934:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7699 (1.8177)
+2022-11-18 15:43:26,168:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9510 (1.8257)
+2022-11-18 15:43:26,363:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8041 (1.8246)
+2022-11-18 15:43:26,425:INFO: - Computing loss (validation)
+2022-11-18 15:43:26,720:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9135 (1.9135)
+2022-11-18 15:43:26,756:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8341 (1.9073)
+2022-11-18 15:43:27,067:INFO: Dataset: univ                Batch: 1/3	Loss 1.8689 (1.8689)
+2022-11-18 15:43:27,145:INFO: Dataset: univ                Batch: 2/3	Loss 1.8169 (1.8417)
+2022-11-18 15:43:27,221:INFO: Dataset: univ                Batch: 3/3	Loss 1.8542 (1.8454)
+2022-11-18 15:43:27,537:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8515 (1.8515)
+2022-11-18 15:43:27,583:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8600 (1.8535)
+2022-11-18 15:43:27,898:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8339 (1.8339)
+2022-11-18 15:43:27,973:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8306 (1.8324)
+2022-11-18 15:43:28,048:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8310 (1.8319)
+2022-11-18 15:43:28,124:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8745 (1.8433)
+2022-11-18 15:43:28,198:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8446 (1.8436)
+2022-11-18 15:43:28,251:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_521.pth.tar
+2022-11-18 15:43:28,251:INFO: 
+===> EPOCH: 522 (P2)
+2022-11-18 15:43:28,251:INFO: - Computing loss (training)
+2022-11-18 15:43:28,602:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9000 (1.9000)
+2022-11-18 15:43:28,757:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9336 (1.9163)
+2022-11-18 15:43:28,917:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9263 (1.9194)
+2022-11-18 15:43:29,036:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8886 (1.9144)
+2022-11-18 15:43:29,447:INFO: Dataset: univ                Batch:  1/15	Loss 1.8583 (1.8583)
+2022-11-18 15:43:29,617:INFO: Dataset: univ                Batch:  2/15	Loss 1.8121 (1.8346)
+2022-11-18 15:43:29,788:INFO: Dataset: univ                Batch:  3/15	Loss 1.8514 (1.8399)
+2022-11-18 15:43:29,953:INFO: Dataset: univ                Batch:  4/15	Loss 1.8445 (1.8411)
+2022-11-18 15:43:30,109:INFO: Dataset: univ                Batch:  5/15	Loss 1.8261 (1.8381)
+2022-11-18 15:43:30,273:INFO: Dataset: univ                Batch:  6/15	Loss 1.8501 (1.8402)
+2022-11-18 15:43:30,452:INFO: Dataset: univ                Batch:  7/15	Loss 1.8931 (1.8474)
+2022-11-18 15:43:30,611:INFO: Dataset: univ                Batch:  8/15	Loss 1.8210 (1.8442)
+2022-11-18 15:43:30,767:INFO: Dataset: univ                Batch:  9/15	Loss 1.8651 (1.8466)
+2022-11-18 15:43:30,927:INFO: Dataset: univ                Batch: 10/15	Loss 1.8801 (1.8501)
+2022-11-18 15:43:31,174:INFO: Dataset: univ                Batch: 11/15	Loss 1.8663 (1.8515)
+2022-11-18 15:43:31,338:INFO: Dataset: univ                Batch: 12/15	Loss 1.8506 (1.8514)
+2022-11-18 15:43:31,495:INFO: Dataset: univ                Batch: 13/15	Loss 1.7982 (1.8472)
+2022-11-18 15:43:31,651:INFO: Dataset: univ                Batch: 14/15	Loss 1.8674 (1.8485)
+2022-11-18 15:43:31,734:INFO: Dataset: univ                Batch: 15/15	Loss 1.7091 (1.8472)
+2022-11-18 15:43:32,129:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7116 (1.7116)
+2022-11-18 15:43:32,283:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8969 (1.8046)
+2022-11-18 15:43:32,433:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9272 (1.8437)
+2022-11-18 15:43:32,583:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8193 (1.8375)
+2022-11-18 15:43:32,733:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9636 (1.8642)
+2022-11-18 15:43:32,884:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9837 (1.8834)
+2022-11-18 15:43:33,036:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8463 (1.8779)
+2022-11-18 15:43:33,174:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8087 (1.8700)
+2022-11-18 15:43:33,576:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8520 (1.8520)
+2022-11-18 15:43:33,743:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8138 (1.8324)
+2022-11-18 15:43:33,907:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8537 (1.8389)
+2022-11-18 15:43:34,066:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8331 (1.8374)
+2022-11-18 15:43:34,224:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7958 (1.8290)
+2022-11-18 15:43:34,386:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8401 (1.8308)
+2022-11-18 15:43:34,542:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8483 (1.8333)
+2022-11-18 15:43:34,692:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9135 (1.8424)
+2022-11-18 15:43:34,844:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8482 (1.8430)
+2022-11-18 15:43:34,995:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7709 (1.8355)
+2022-11-18 15:43:35,149:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9253 (1.8439)
+2022-11-18 15:43:35,307:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8940 (1.8480)
+2022-11-18 15:43:35,467:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9243 (1.8546)
+2022-11-18 15:43:35,627:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8489 (1.8542)
+2022-11-18 15:43:35,778:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7822 (1.8496)
+2022-11-18 15:43:35,926:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8769 (1.8515)
+2022-11-18 15:43:36,080:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8697 (1.8526)
+2022-11-18 15:43:36,217:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8796 (1.8538)
+2022-11-18 15:43:36,263:INFO: - Computing loss (validation)
+2022-11-18 15:43:36,543:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7982 (1.7982)
+2022-11-18 15:43:36,577:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7643 (1.7959)
+2022-11-18 15:43:36,935:INFO: Dataset: univ                Batch: 1/3	Loss 1.9154 (1.9154)
+2022-11-18 15:43:37,027:INFO: Dataset: univ                Batch: 2/3	Loss 1.8674 (1.8911)
+2022-11-18 15:43:37,110:INFO: Dataset: univ                Batch: 3/3	Loss 1.8985 (1.8934)
+2022-11-18 15:43:37,421:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8429 (1.8429)
+2022-11-18 15:43:37,467:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9687 (1.8761)
+2022-11-18 15:43:37,769:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9111 (1.9111)
+2022-11-18 15:43:37,847:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7674 (1.8368)
+2022-11-18 15:43:37,922:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8476 (1.8404)
+2022-11-18 15:43:37,996:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8618 (1.8455)
+2022-11-18 15:43:38,070:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8665 (1.8498)
+2022-11-18 15:43:38,122:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_522.pth.tar
+2022-11-18 15:43:38,123:INFO: 
+===> EPOCH: 523 (P2)
+2022-11-18 15:43:38,123:INFO: - Computing loss (training)
+2022-11-18 15:43:38,470:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9336 (1.9336)
+2022-11-18 15:43:38,624:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8017 (1.8682)
+2022-11-18 15:43:38,775:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9101 (1.8814)
+2022-11-18 15:43:38,893:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0008 (1.9016)
+2022-11-18 15:43:39,288:INFO: Dataset: univ                Batch:  1/15	Loss 1.8059 (1.8059)
+2022-11-18 15:43:39,453:INFO: Dataset: univ                Batch:  2/15	Loss 1.8627 (1.8336)
+2022-11-18 15:43:39,618:INFO: Dataset: univ                Batch:  3/15	Loss 1.8671 (1.8462)
+2022-11-18 15:43:39,789:INFO: Dataset: univ                Batch:  4/15	Loss 1.8612 (1.8502)
+2022-11-18 15:43:39,955:INFO: Dataset: univ                Batch:  5/15	Loss 1.8542 (1.8511)
+2022-11-18 15:43:40,115:INFO: Dataset: univ                Batch:  6/15	Loss 1.8791 (1.8561)
+2022-11-18 15:43:40,273:INFO: Dataset: univ                Batch:  7/15	Loss 1.8304 (1.8523)
+2022-11-18 15:43:40,430:INFO: Dataset: univ                Batch:  8/15	Loss 1.8687 (1.8544)
+2022-11-18 15:43:40,592:INFO: Dataset: univ                Batch:  9/15	Loss 1.8720 (1.8565)
+2022-11-18 15:43:40,758:INFO: Dataset: univ                Batch: 10/15	Loss 1.8257 (1.8538)
+2022-11-18 15:43:40,929:INFO: Dataset: univ                Batch: 11/15	Loss 1.9094 (1.8591)
+2022-11-18 15:43:41,091:INFO: Dataset: univ                Batch: 12/15	Loss 1.8469 (1.8580)
+2022-11-18 15:43:41,251:INFO: Dataset: univ                Batch: 13/15	Loss 1.8876 (1.8605)
+2022-11-18 15:43:41,409:INFO: Dataset: univ                Batch: 14/15	Loss 1.8588 (1.8603)
+2022-11-18 15:43:41,493:INFO: Dataset: univ                Batch: 15/15	Loss 1.8961 (1.8607)
+2022-11-18 15:43:41,898:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9665 (1.9665)
+2022-11-18 15:43:42,056:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9187 (1.9428)
+2022-11-18 15:43:42,214:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9450 (1.9435)
+2022-11-18 15:43:42,381:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8776 (1.9276)
+2022-11-18 15:43:42,547:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8695 (1.9150)
+2022-11-18 15:43:42,727:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9955 (1.9271)
+2022-11-18 15:43:42,889:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9417 (1.9291)
+2022-11-18 15:43:43,044:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8807 (1.9231)
+2022-11-18 15:43:43,497:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8219 (1.8219)
+2022-11-18 15:43:43,674:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8476 (1.8342)
+2022-11-18 15:43:43,856:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8160 (1.8283)
+2022-11-18 15:43:44,040:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7891 (1.8188)
+2022-11-18 15:43:44,204:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8219 (1.8194)
+2022-11-18 15:43:44,364:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8794 (1.8309)
+2022-11-18 15:43:44,548:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8712 (1.8363)
+2022-11-18 15:43:44,742:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8936 (1.8434)
+2022-11-18 15:43:44,936:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8577 (1.8451)
+2022-11-18 15:43:45,102:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8980 (1.8504)
+2022-11-18 15:43:45,269:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8896 (1.8543)
+2022-11-18 15:43:45,433:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8516 (1.8540)
+2022-11-18 15:43:45,618:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9348 (1.8600)
+2022-11-18 15:43:45,822:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8144 (1.8568)
+2022-11-18 15:43:46,003:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8878 (1.8587)
+2022-11-18 15:43:46,188:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8087 (1.8558)
+2022-11-18 15:43:46,379:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7652 (1.8501)
+2022-11-18 15:43:46,553:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9016 (1.8524)
+2022-11-18 15:43:46,603:INFO: - Computing loss (validation)
+2022-11-18 15:43:46,908:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9618 (1.9618)
+2022-11-18 15:43:46,946:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8036 (1.9542)
+2022-11-18 15:43:47,259:INFO: Dataset: univ                Batch: 1/3	Loss 1.8542 (1.8542)
+2022-11-18 15:43:47,341:INFO: Dataset: univ                Batch: 2/3	Loss 1.8145 (1.8343)
+2022-11-18 15:43:47,417:INFO: Dataset: univ                Batch: 3/3	Loss 1.8365 (1.8351)
+2022-11-18 15:43:47,740:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8845 (1.8845)
+2022-11-18 15:43:47,794:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8260 (1.8696)
+2022-11-18 15:43:48,114:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8858 (1.8858)
+2022-11-18 15:43:48,196:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8865 (1.8862)
+2022-11-18 15:43:48,273:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9864 (1.9204)
+2022-11-18 15:43:48,351:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8189 (1.8938)
+2022-11-18 15:43:48,428:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8192 (1.8789)
+2022-11-18 15:43:48,485:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_523.pth.tar
+2022-11-18 15:43:48,485:INFO: 
+===> EPOCH: 524 (P2)
+2022-11-18 15:43:48,486:INFO: - Computing loss (training)
+2022-11-18 15:43:48,851:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9394 (1.9394)
+2022-11-18 15:43:49,028:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8845 (1.9106)
+2022-11-18 15:43:49,201:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9804 (1.9343)
+2022-11-18 15:43:49,331:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8452 (1.9179)
+2022-11-18 15:43:49,754:INFO: Dataset: univ                Batch:  1/15	Loss 1.8631 (1.8631)
+2022-11-18 15:43:49,931:INFO: Dataset: univ                Batch:  2/15	Loss 1.8484 (1.8554)
+2022-11-18 15:43:50,118:INFO: Dataset: univ                Batch:  3/15	Loss 1.8830 (1.8648)
+2022-11-18 15:43:50,302:INFO: Dataset: univ                Batch:  4/15	Loss 1.8254 (1.8543)
+2022-11-18 15:43:50,486:INFO: Dataset: univ                Batch:  5/15	Loss 1.8245 (1.8481)
+2022-11-18 15:43:50,655:INFO: Dataset: univ                Batch:  6/15	Loss 1.8614 (1.8504)
+2022-11-18 15:43:50,824:INFO: Dataset: univ                Batch:  7/15	Loss 1.8513 (1.8505)
+2022-11-18 15:43:50,992:INFO: Dataset: univ                Batch:  8/15	Loss 1.8575 (1.8514)
+2022-11-18 15:43:51,162:INFO: Dataset: univ                Batch:  9/15	Loss 1.8596 (1.8523)
+2022-11-18 15:43:51,328:INFO: Dataset: univ                Batch: 10/15	Loss 1.8673 (1.8539)
+2022-11-18 15:43:51,497:INFO: Dataset: univ                Batch: 11/15	Loss 1.8068 (1.8496)
+2022-11-18 15:43:51,665:INFO: Dataset: univ                Batch: 12/15	Loss 1.8643 (1.8510)
+2022-11-18 15:43:51,833:INFO: Dataset: univ                Batch: 13/15	Loss 1.8505 (1.8509)
+2022-11-18 15:43:52,001:INFO: Dataset: univ                Batch: 14/15	Loss 1.8562 (1.8513)
+2022-11-18 15:43:52,091:INFO: Dataset: univ                Batch: 15/15	Loss 1.9308 (1.8523)
+2022-11-18 15:43:52,513:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8723 (1.8723)
+2022-11-18 15:43:52,673:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9133 (1.8926)
+2022-11-18 15:43:52,832:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9663 (1.9187)
+2022-11-18 15:43:52,991:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8700 (1.9071)
+2022-11-18 15:43:53,159:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8650 (1.8986)
+2022-11-18 15:43:53,329:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0079 (1.9173)
+2022-11-18 15:43:53,501:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8372 (1.9053)
+2022-11-18 15:43:53,661:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7635 (1.8916)
+2022-11-18 15:43:54,145:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8810 (1.8810)
+2022-11-18 15:43:54,308:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8984 (1.8895)
+2022-11-18 15:43:54,472:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9817 (1.9179)
+2022-11-18 15:43:54,633:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9063 (1.9147)
+2022-11-18 15:43:54,792:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8831 (1.9078)
+2022-11-18 15:43:54,956:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8871 (1.9044)
+2022-11-18 15:43:55,118:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8066 (1.8902)
+2022-11-18 15:43:55,279:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7913 (1.8772)
+2022-11-18 15:43:55,439:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8862 (1.8782)
+2022-11-18 15:43:55,599:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8598 (1.8764)
+2022-11-18 15:43:55,760:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7997 (1.8697)
+2022-11-18 15:43:55,921:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8211 (1.8658)
+2022-11-18 15:43:56,084:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8477 (1.8645)
+2022-11-18 15:43:56,246:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7728 (1.8581)
+2022-11-18 15:43:56,414:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8172 (1.8551)
+2022-11-18 15:43:56,582:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8284 (1.8532)
+2022-11-18 15:43:56,759:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8271 (1.8518)
+2022-11-18 15:43:56,922:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8338 (1.8510)
+2022-11-18 15:43:56,975:INFO: - Computing loss (validation)
+2022-11-18 15:43:57,264:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9346 (1.9346)
+2022-11-18 15:43:57,300:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6302 (1.9107)
+2022-11-18 15:43:57,623:INFO: Dataset: univ                Batch: 1/3	Loss 1.8984 (1.8984)
+2022-11-18 15:43:57,709:INFO: Dataset: univ                Batch: 2/3	Loss 1.8733 (1.8842)
+2022-11-18 15:43:57,791:INFO: Dataset: univ                Batch: 3/3	Loss 1.8979 (1.8887)
+2022-11-18 15:43:58,104:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8392 (1.8392)
+2022-11-18 15:43:58,154:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7489 (1.8177)
+2022-11-18 15:43:58,480:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8425 (1.8425)
+2022-11-18 15:43:58,566:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8607 (1.8512)
+2022-11-18 15:43:58,647:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7934 (1.8315)
+2022-11-18 15:43:58,729:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9028 (1.8481)
+2022-11-18 15:43:58,809:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9164 (1.8612)
+2022-11-18 15:43:58,865:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_524.pth.tar
+2022-11-18 15:43:58,866:INFO: 
+===> EPOCH: 525 (P2)
+2022-11-18 15:43:58,866:INFO: - Computing loss (training)
+2022-11-18 15:43:59,229:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8810 (1.8810)
+2022-11-18 15:43:59,394:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8069 (1.8436)
+2022-11-18 15:43:59,557:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7754 (1.8208)
+2022-11-18 15:43:59,685:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9496 (1.8431)
+2022-11-18 15:44:00,125:INFO: Dataset: univ                Batch:  1/15	Loss 1.8637 (1.8637)
+2022-11-18 15:44:00,309:INFO: Dataset: univ                Batch:  2/15	Loss 1.8202 (1.8414)
+2022-11-18 15:44:00,489:INFO: Dataset: univ                Batch:  3/15	Loss 1.8618 (1.8478)
+2022-11-18 15:44:00,660:INFO: Dataset: univ                Batch:  4/15	Loss 1.9015 (1.8599)
+2022-11-18 15:44:00,842:INFO: Dataset: univ                Batch:  5/15	Loss 1.8259 (1.8527)
+2022-11-18 15:44:01,023:INFO: Dataset: univ                Batch:  6/15	Loss 1.8264 (1.8484)
+2022-11-18 15:44:01,196:INFO: Dataset: univ                Batch:  7/15	Loss 1.8660 (1.8510)
+2022-11-18 15:44:01,364:INFO: Dataset: univ                Batch:  8/15	Loss 1.8630 (1.8523)
+2022-11-18 15:44:01,537:INFO: Dataset: univ                Batch:  9/15	Loss 1.7972 (1.8459)
+2022-11-18 15:44:01,708:INFO: Dataset: univ                Batch: 10/15	Loss 1.8647 (1.8476)
+2022-11-18 15:44:01,866:INFO: Dataset: univ                Batch: 11/15	Loss 1.8650 (1.8493)
+2022-11-18 15:44:02,058:INFO: Dataset: univ                Batch: 12/15	Loss 1.8488 (1.8492)
+2022-11-18 15:44:02,273:INFO: Dataset: univ                Batch: 13/15	Loss 1.8650 (1.8504)
+2022-11-18 15:44:02,481:INFO: Dataset: univ                Batch: 14/15	Loss 1.8754 (1.8523)
+2022-11-18 15:44:02,587:INFO: Dataset: univ                Batch: 15/15	Loss 1.8257 (1.8520)
+2022-11-18 15:44:03,003:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9215 (1.9215)
+2022-11-18 15:44:03,170:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7572 (1.8363)
+2022-11-18 15:44:03,341:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8682 (1.8476)
+2022-11-18 15:44:03,514:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9001 (1.8583)
+2022-11-18 15:44:03,689:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8331 (1.8530)
+2022-11-18 15:44:03,866:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8872 (1.8587)
+2022-11-18 15:44:04,030:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9292 (1.8694)
+2022-11-18 15:44:04,177:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9744 (1.8810)
+2022-11-18 15:44:04,599:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8646 (1.8646)
+2022-11-18 15:44:04,760:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9074 (1.8865)
+2022-11-18 15:44:04,922:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9208 (1.8981)
+2022-11-18 15:44:05,092:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8252 (1.8802)
+2022-11-18 15:44:05,264:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8364 (1.8713)
+2022-11-18 15:44:05,423:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8800 (1.8728)
+2022-11-18 15:44:05,584:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9016 (1.8769)
+2022-11-18 15:44:05,750:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9147 (1.8814)
+2022-11-18 15:44:05,914:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8108 (1.8729)
+2022-11-18 15:44:06,077:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8394 (1.8696)
+2022-11-18 15:44:06,246:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8648 (1.8691)
+2022-11-18 15:44:06,398:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9215 (1.8733)
+2022-11-18 15:44:06,549:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9864 (1.8821)
+2022-11-18 15:44:06,699:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7946 (1.8751)
+2022-11-18 15:44:06,853:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9193 (1.8781)
+2022-11-18 15:44:07,007:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8304 (1.8751)
+2022-11-18 15:44:07,165:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8805 (1.8754)
+2022-11-18 15:44:07,314:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8006 (1.8714)
+2022-11-18 15:44:07,363:INFO: - Computing loss (validation)
+2022-11-18 15:44:07,633:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9114 (1.9114)
+2022-11-18 15:44:07,669:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9038 (1.9109)
+2022-11-18 15:44:07,985:INFO: Dataset: univ                Batch: 1/3	Loss 1.7988 (1.7988)
+2022-11-18 15:44:08,068:INFO: Dataset: univ                Batch: 2/3	Loss 1.9129 (1.8507)
+2022-11-18 15:44:08,147:INFO: Dataset: univ                Batch: 3/3	Loss 1.8932 (1.8638)
+2022-11-18 15:44:08,489:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8865 (1.8865)
+2022-11-18 15:44:08,534:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0612 (1.9309)
+2022-11-18 15:44:08,852:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8146 (1.8146)
+2022-11-18 15:44:08,928:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8707 (1.8436)
+2022-11-18 15:44:09,006:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8186 (1.8351)
+2022-11-18 15:44:09,087:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8738 (1.8449)
+2022-11-18 15:44:09,165:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8239 (1.8409)
+2022-11-18 15:44:09,219:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_525.pth.tar
+2022-11-18 15:44:09,219:INFO: 
+===> EPOCH: 526 (P2)
+2022-11-18 15:44:09,219:INFO: - Computing loss (training)
+2022-11-18 15:44:09,569:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8275 (1.8275)
+2022-11-18 15:44:09,721:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8508 (1.8393)
+2022-11-18 15:44:09,874:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9935 (1.8912)
+2022-11-18 15:44:09,994:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7258 (1.8637)
+2022-11-18 15:44:10,391:INFO: Dataset: univ                Batch:  1/15	Loss 1.8496 (1.8496)
+2022-11-18 15:44:10,563:INFO: Dataset: univ                Batch:  2/15	Loss 1.8262 (1.8370)
+2022-11-18 15:44:10,725:INFO: Dataset: univ                Batch:  3/15	Loss 1.8848 (1.8555)
+2022-11-18 15:44:10,884:INFO: Dataset: univ                Batch:  4/15	Loss 1.8602 (1.8567)
+2022-11-18 15:44:11,052:INFO: Dataset: univ                Batch:  5/15	Loss 1.8494 (1.8551)
+2022-11-18 15:44:11,241:INFO: Dataset: univ                Batch:  6/15	Loss 1.9305 (1.8683)
+2022-11-18 15:44:11,421:INFO: Dataset: univ                Batch:  7/15	Loss 1.8136 (1.8607)
+2022-11-18 15:44:11,588:INFO: Dataset: univ                Batch:  8/15	Loss 1.8442 (1.8586)
+2022-11-18 15:44:11,748:INFO: Dataset: univ                Batch:  9/15	Loss 1.8718 (1.8601)
+2022-11-18 15:44:11,905:INFO: Dataset: univ                Batch: 10/15	Loss 1.8827 (1.8621)
+2022-11-18 15:44:12,065:INFO: Dataset: univ                Batch: 11/15	Loss 1.8832 (1.8638)
+2022-11-18 15:44:12,223:INFO: Dataset: univ                Batch: 12/15	Loss 1.8116 (1.8595)
+2022-11-18 15:44:12,397:INFO: Dataset: univ                Batch: 13/15	Loss 1.8441 (1.8584)
+2022-11-18 15:44:12,571:INFO: Dataset: univ                Batch: 14/15	Loss 1.8971 (1.8610)
+2022-11-18 15:44:12,662:INFO: Dataset: univ                Batch: 15/15	Loss 1.8369 (1.8607)
+2022-11-18 15:44:13,104:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8270 (1.8270)
+2022-11-18 15:44:13,261:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8249 (1.8260)
+2022-11-18 15:44:13,425:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9327 (1.8634)
+2022-11-18 15:44:13,593:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7966 (1.8465)
+2022-11-18 15:44:13,751:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8967 (1.8569)
+2022-11-18 15:44:13,904:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9200 (1.8667)
+2022-11-18 15:44:14,062:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8066 (1.8587)
+2022-11-18 15:44:14,218:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8773 (1.8608)
+2022-11-18 15:44:14,636:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7959 (1.7959)
+2022-11-18 15:44:14,806:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8206 (1.8080)
+2022-11-18 15:44:14,990:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7003 (1.7713)
+2022-11-18 15:44:15,155:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8298 (1.7870)
+2022-11-18 15:44:15,323:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9035 (1.8104)
+2022-11-18 15:44:15,500:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8760 (1.8214)
+2022-11-18 15:44:15,661:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8883 (1.8311)
+2022-11-18 15:44:15,818:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9325 (1.8447)
+2022-11-18 15:44:15,976:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8188 (1.8414)
+2022-11-18 15:44:16,134:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8535 (1.8427)
+2022-11-18 15:44:16,295:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9366 (1.8510)
+2022-11-18 15:44:16,451:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8453 (1.8504)
+2022-11-18 15:44:16,617:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8689 (1.8520)
+2022-11-18 15:44:16,785:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9117 (1.8562)
+2022-11-18 15:44:16,954:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9226 (1.8602)
+2022-11-18 15:44:17,122:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8706 (1.8609)
+2022-11-18 15:44:17,302:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8020 (1.8574)
+2022-11-18 15:44:17,449:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8041 (1.8546)
+2022-11-18 15:44:17,496:INFO: - Computing loss (validation)
+2022-11-18 15:44:17,768:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8440 (1.8440)
+2022-11-18 15:44:17,805:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1079 (1.8602)
+2022-11-18 15:44:18,147:INFO: Dataset: univ                Batch: 1/3	Loss 1.8735 (1.8735)
+2022-11-18 15:44:18,231:INFO: Dataset: univ                Batch: 2/3	Loss 1.8463 (1.8597)
+2022-11-18 15:44:18,307:INFO: Dataset: univ                Batch: 3/3	Loss 1.8186 (1.8485)
+2022-11-18 15:44:18,621:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7672 (1.7672)
+2022-11-18 15:44:18,670:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8846 (1.7986)
+2022-11-18 15:44:18,990:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9215 (1.9215)
+2022-11-18 15:44:19,065:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8061 (1.8623)
+2022-11-18 15:44:19,139:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8305 (1.8514)
+2022-11-18 15:44:19,213:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8619 (1.8541)
+2022-11-18 15:44:19,289:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9739 (1.8777)
+2022-11-18 15:44:19,342:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_526.pth.tar
+2022-11-18 15:44:19,342:INFO: 
+===> EPOCH: 527 (P2)
+2022-11-18 15:44:19,343:INFO: - Computing loss (training)
+2022-11-18 15:44:19,682:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9822 (1.9822)
+2022-11-18 15:44:19,844:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9960 (1.9891)
+2022-11-18 15:44:20,005:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8801 (1.9537)
+2022-11-18 15:44:20,129:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8435 (1.9363)
+2022-11-18 15:44:20,545:INFO: Dataset: univ                Batch:  1/15	Loss 1.8766 (1.8766)
+2022-11-18 15:44:20,704:INFO: Dataset: univ                Batch:  2/15	Loss 1.8072 (1.8437)
+2022-11-18 15:44:20,862:INFO: Dataset: univ                Batch:  3/15	Loss 1.8743 (1.8542)
+2022-11-18 15:44:21,020:INFO: Dataset: univ                Batch:  4/15	Loss 1.8429 (1.8516)
+2022-11-18 15:44:21,183:INFO: Dataset: univ                Batch:  5/15	Loss 1.8318 (1.8473)
+2022-11-18 15:44:21,351:INFO: Dataset: univ                Batch:  6/15	Loss 1.8118 (1.8410)
+2022-11-18 15:44:21,522:INFO: Dataset: univ                Batch:  7/15	Loss 1.8141 (1.8372)
+2022-11-18 15:44:21,680:INFO: Dataset: univ                Batch:  8/15	Loss 1.8332 (1.8366)
+2022-11-18 15:44:21,837:INFO: Dataset: univ                Batch:  9/15	Loss 1.8052 (1.8333)
+2022-11-18 15:44:21,993:INFO: Dataset: univ                Batch: 10/15	Loss 1.8514 (1.8351)
+2022-11-18 15:44:22,154:INFO: Dataset: univ                Batch: 11/15	Loss 1.9142 (1.8429)
+2022-11-18 15:44:22,309:INFO: Dataset: univ                Batch: 12/15	Loss 1.7931 (1.8387)
+2022-11-18 15:44:22,465:INFO: Dataset: univ                Batch: 13/15	Loss 1.9060 (1.8435)
+2022-11-18 15:44:22,633:INFO: Dataset: univ                Batch: 14/15	Loss 1.8234 (1.8420)
+2022-11-18 15:44:22,721:INFO: Dataset: univ                Batch: 15/15	Loss 1.9417 (1.8431)
+2022-11-18 15:44:23,118:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8484 (1.8484)
+2022-11-18 15:44:23,278:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8928 (1.8713)
+2022-11-18 15:44:23,442:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9135 (1.8858)
+2022-11-18 15:44:23,600:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8093 (1.8662)
+2022-11-18 15:44:23,781:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8122 (1.8547)
+2022-11-18 15:44:23,940:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9365 (1.8689)
+2022-11-18 15:44:24,102:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7982 (1.8607)
+2022-11-18 15:44:24,246:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9674 (1.8726)
+2022-11-18 15:44:24,658:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8331 (1.8331)
+2022-11-18 15:44:24,811:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8496 (1.8417)
+2022-11-18 15:44:24,965:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8745 (1.8521)
+2022-11-18 15:44:25,114:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9035 (1.8659)
+2022-11-18 15:44:25,282:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7569 (1.8455)
+2022-11-18 15:44:25,449:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8396 (1.8445)
+2022-11-18 15:44:25,601:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8152 (1.8406)
+2022-11-18 15:44:25,751:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8481 (1.8415)
+2022-11-18 15:44:25,921:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8730 (1.8450)
+2022-11-18 15:44:26,088:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8318 (1.8438)
+2022-11-18 15:44:26,242:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8559 (1.8449)
+2022-11-18 15:44:26,394:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9132 (1.8504)
+2022-11-18 15:44:26,545:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8594 (1.8511)
+2022-11-18 15:44:26,695:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8679 (1.8524)
+2022-11-18 15:44:26,849:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7918 (1.8485)
+2022-11-18 15:44:27,018:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8639 (1.8494)
+2022-11-18 15:44:27,184:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9504 (1.8555)
+2022-11-18 15:44:27,329:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8596 (1.8557)
+2022-11-18 15:44:27,383:INFO: - Computing loss (validation)
+2022-11-18 15:44:27,689:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8282 (1.8282)
+2022-11-18 15:44:27,728:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2146 (1.8533)
+2022-11-18 15:44:28,086:INFO: Dataset: univ                Batch: 1/3	Loss 1.8716 (1.8716)
+2022-11-18 15:44:28,167:INFO: Dataset: univ                Batch: 2/3	Loss 1.8498 (1.8617)
+2022-11-18 15:44:28,246:INFO: Dataset: univ                Batch: 3/3	Loss 1.8004 (1.8450)
+2022-11-18 15:44:28,580:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8074 (1.8074)
+2022-11-18 15:44:28,625:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8223 (1.8108)
+2022-11-18 15:44:28,956:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9035 (1.9035)
+2022-11-18 15:44:29,035:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8905 (1.8967)
+2022-11-18 15:44:29,114:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7873 (1.8607)
+2022-11-18 15:44:29,194:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8493 (1.8579)
+2022-11-18 15:44:29,275:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9013 (1.8665)
+2022-11-18 15:44:29,343:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_527.pth.tar
+2022-11-18 15:44:29,344:INFO: 
+===> EPOCH: 528 (P2)
+2022-11-18 15:44:29,344:INFO: - Computing loss (training)
+2022-11-18 15:44:29,710:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8790 (1.8790)
+2022-11-18 15:44:29,887:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7436 (1.8135)
+2022-11-18 15:44:30,048:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7956 (1.8077)
+2022-11-18 15:44:30,168:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9365 (1.8291)
+2022-11-18 15:44:30,572:INFO: Dataset: univ                Batch:  1/15	Loss 1.8511 (1.8511)
+2022-11-18 15:44:30,735:INFO: Dataset: univ                Batch:  2/15	Loss 1.8269 (1.8395)
+2022-11-18 15:44:30,895:INFO: Dataset: univ                Batch:  3/15	Loss 1.8565 (1.8450)
+2022-11-18 15:44:31,056:INFO: Dataset: univ                Batch:  4/15	Loss 1.7945 (1.8335)
+2022-11-18 15:44:31,224:INFO: Dataset: univ                Batch:  5/15	Loss 1.8528 (1.8371)
+2022-11-18 15:44:31,393:INFO: Dataset: univ                Batch:  6/15	Loss 1.8613 (1.8411)
+2022-11-18 15:44:31,561:INFO: Dataset: univ                Batch:  7/15	Loss 1.8271 (1.8391)
+2022-11-18 15:44:31,718:INFO: Dataset: univ                Batch:  8/15	Loss 1.8070 (1.8358)
+2022-11-18 15:44:31,881:INFO: Dataset: univ                Batch:  9/15	Loss 1.8438 (1.8366)
+2022-11-18 15:44:32,042:INFO: Dataset: univ                Batch: 10/15	Loss 1.8916 (1.8423)
+2022-11-18 15:44:32,202:INFO: Dataset: univ                Batch: 11/15	Loss 1.8581 (1.8437)
+2022-11-18 15:44:32,363:INFO: Dataset: univ                Batch: 12/15	Loss 1.8849 (1.8471)
+2022-11-18 15:44:32,538:INFO: Dataset: univ                Batch: 13/15	Loss 1.8390 (1.8464)
+2022-11-18 15:44:32,709:INFO: Dataset: univ                Batch: 14/15	Loss 1.8574 (1.8472)
+2022-11-18 15:44:32,799:INFO: Dataset: univ                Batch: 15/15	Loss 1.8073 (1.8466)
+2022-11-18 15:44:33,223:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9757 (1.9757)
+2022-11-18 15:44:33,374:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0595 (2.0188)
+2022-11-18 15:44:33,525:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8768 (1.9687)
+2022-11-18 15:44:33,679:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8299 (1.9319)
+2022-11-18 15:44:33,830:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9253 (1.9307)
+2022-11-18 15:44:33,980:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0125 (1.9442)
+2022-11-18 15:44:34,130:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8861 (1.9366)
+2022-11-18 15:44:34,270:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8018 (1.9204)
+2022-11-18 15:44:34,710:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7592 (1.7592)
+2022-11-18 15:44:34,886:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8642 (1.8113)
+2022-11-18 15:44:35,048:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8972 (1.8387)
+2022-11-18 15:44:35,206:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8179 (1.8338)
+2022-11-18 15:44:35,370:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8556 (1.8381)
+2022-11-18 15:44:35,536:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8967 (1.8479)
+2022-11-18 15:44:35,699:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8456 (1.8475)
+2022-11-18 15:44:35,865:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7814 (1.8396)
+2022-11-18 15:44:36,024:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7374 (1.8294)
+2022-11-18 15:44:36,183:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8193 (1.8284)
+2022-11-18 15:44:36,339:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9161 (1.8361)
+2022-11-18 15:44:36,493:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8573 (1.8379)
+2022-11-18 15:44:36,657:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8153 (1.8363)
+2022-11-18 15:44:36,828:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9661 (1.8468)
+2022-11-18 15:44:37,000:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8527 (1.8472)
+2022-11-18 15:44:37,157:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8178 (1.8455)
+2022-11-18 15:44:37,318:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8224 (1.8441)
+2022-11-18 15:44:37,461:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8043 (1.8423)
+2022-11-18 15:44:37,507:INFO: - Computing loss (validation)
+2022-11-18 15:44:37,781:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8540 (1.8540)
+2022-11-18 15:44:37,816:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7911 (1.8506)
+2022-11-18 15:44:38,121:INFO: Dataset: univ                Batch: 1/3	Loss 1.8540 (1.8540)
+2022-11-18 15:44:38,201:INFO: Dataset: univ                Batch: 2/3	Loss 1.9196 (1.8862)
+2022-11-18 15:44:38,278:INFO: Dataset: univ                Batch: 3/3	Loss 1.8503 (1.8737)
+2022-11-18 15:44:38,611:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8121 (1.8121)
+2022-11-18 15:44:38,659:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9400 (1.8405)
+2022-11-18 15:44:38,985:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7647 (1.7647)
+2022-11-18 15:44:39,070:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8161 (1.7904)
+2022-11-18 15:44:39,154:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9023 (1.8274)
+2022-11-18 15:44:39,237:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9222 (1.8500)
+2022-11-18 15:44:39,320:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7938 (1.8383)
+2022-11-18 15:44:39,382:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_528.pth.tar
+2022-11-18 15:44:39,382:INFO: 
+===> EPOCH: 529 (P2)
+2022-11-18 15:44:39,383:INFO: - Computing loss (training)
+2022-11-18 15:44:39,752:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9824 (1.9824)
+2022-11-18 15:44:39,920:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0176 (2.0003)
+2022-11-18 15:44:40,084:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7465 (1.9112)
+2022-11-18 15:44:40,205:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8078 (1.8929)
+2022-11-18 15:44:40,664:INFO: Dataset: univ                Batch:  1/15	Loss 1.8354 (1.8354)
+2022-11-18 15:44:40,859:INFO: Dataset: univ                Batch:  2/15	Loss 1.8412 (1.8381)
+2022-11-18 15:44:41,067:INFO: Dataset: univ                Batch:  3/15	Loss 1.8457 (1.8405)
+2022-11-18 15:44:41,275:INFO: Dataset: univ                Batch:  4/15	Loss 1.8495 (1.8429)
+2022-11-18 15:44:41,441:INFO: Dataset: univ                Batch:  5/15	Loss 1.9122 (1.8570)
+2022-11-18 15:44:41,605:INFO: Dataset: univ                Batch:  6/15	Loss 1.8738 (1.8598)
+2022-11-18 15:44:41,765:INFO: Dataset: univ                Batch:  7/15	Loss 1.8446 (1.8576)
+2022-11-18 15:44:41,930:INFO: Dataset: univ                Batch:  8/15	Loss 1.8529 (1.8570)
+2022-11-18 15:44:42,117:INFO: Dataset: univ                Batch:  9/15	Loss 1.8540 (1.8566)
+2022-11-18 15:44:42,309:INFO: Dataset: univ                Batch: 10/15	Loss 1.8326 (1.8542)
+2022-11-18 15:44:42,472:INFO: Dataset: univ                Batch: 11/15	Loss 1.8579 (1.8545)
+2022-11-18 15:44:42,637:INFO: Dataset: univ                Batch: 12/15	Loss 1.8336 (1.8529)
+2022-11-18 15:44:42,803:INFO: Dataset: univ                Batch: 13/15	Loss 1.8499 (1.8527)
+2022-11-18 15:44:42,993:INFO: Dataset: univ                Batch: 14/15	Loss 1.8790 (1.8546)
+2022-11-18 15:44:43,094:INFO: Dataset: univ                Batch: 15/15	Loss 1.9038 (1.8551)
+2022-11-18 15:44:43,525:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7906 (1.7906)
+2022-11-18 15:44:43,673:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8361 (1.8132)
+2022-11-18 15:44:43,827:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8387 (1.8214)
+2022-11-18 15:44:43,981:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8695 (1.8334)
+2022-11-18 15:44:44,133:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9258 (1.8512)
+2022-11-18 15:44:44,284:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9440 (1.8663)
+2022-11-18 15:44:44,447:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9487 (1.8793)
+2022-11-18 15:44:44,609:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8861 (1.8801)
+2022-11-18 15:44:45,073:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8823 (1.8823)
+2022-11-18 15:44:45,230:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7674 (1.8235)
+2022-11-18 15:44:45,385:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7917 (1.8127)
+2022-11-18 15:44:45,556:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7928 (1.8071)
+2022-11-18 15:44:45,750:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8213 (1.8100)
+2022-11-18 15:44:45,924:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8798 (1.8213)
+2022-11-18 15:44:46,093:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7658 (1.8143)
+2022-11-18 15:44:46,269:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7942 (1.8120)
+2022-11-18 15:44:46,467:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8048 (1.8112)
+2022-11-18 15:44:46,641:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7886 (1.8088)
+2022-11-18 15:44:46,807:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9124 (1.8183)
+2022-11-18 15:44:46,966:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9165 (1.8258)
+2022-11-18 15:44:47,165:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8093 (1.8245)
+2022-11-18 15:44:47,330:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7460 (1.8187)
+2022-11-18 15:44:47,519:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8869 (1.8231)
+2022-11-18 15:44:47,704:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8304 (1.8236)
+2022-11-18 15:44:47,884:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8290 (1.8239)
+2022-11-18 15:44:48,035:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8158 (1.8235)
+2022-11-18 15:44:48,087:INFO: - Computing loss (validation)
+2022-11-18 15:44:48,356:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8339 (1.8339)
+2022-11-18 15:44:48,393:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8835 (1.8366)
+2022-11-18 15:44:48,727:INFO: Dataset: univ                Batch: 1/3	Loss 1.8470 (1.8470)
+2022-11-18 15:44:48,810:INFO: Dataset: univ                Batch: 2/3	Loss 1.8950 (1.8723)
+2022-11-18 15:44:48,888:INFO: Dataset: univ                Batch: 3/3	Loss 1.8979 (1.8810)
+2022-11-18 15:44:49,203:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8419 (1.8419)
+2022-11-18 15:44:49,248:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8285 (1.8386)
+2022-11-18 15:44:49,595:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8302 (1.8302)
+2022-11-18 15:44:49,678:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8759 (1.8524)
+2022-11-18 15:44:49,760:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9038 (1.8688)
+2022-11-18 15:44:49,841:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8577 (1.8660)
+2022-11-18 15:44:49,919:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8697 (1.8668)
+2022-11-18 15:44:49,976:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_529.pth.tar
+2022-11-18 15:44:49,976:INFO: 
+===> EPOCH: 530 (P2)
+2022-11-18 15:44:49,977:INFO: - Computing loss (training)
+2022-11-18 15:44:50,330:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8821 (1.8821)
+2022-11-18 15:44:50,496:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9469 (1.9151)
+2022-11-18 15:44:50,667:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8499 (1.8921)
+2022-11-18 15:44:50,799:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8369 (1.8824)
+2022-11-18 15:44:51,242:INFO: Dataset: univ                Batch:  1/15	Loss 1.8892 (1.8892)
+2022-11-18 15:44:51,469:INFO: Dataset: univ                Batch:  2/15	Loss 1.8236 (1.8556)
+2022-11-18 15:44:51,653:INFO: Dataset: univ                Batch:  3/15	Loss 1.8018 (1.8371)
+2022-11-18 15:44:51,855:INFO: Dataset: univ                Batch:  4/15	Loss 1.8722 (1.8464)
+2022-11-18 15:44:52,046:INFO: Dataset: univ                Batch:  5/15	Loss 1.8328 (1.8439)
+2022-11-18 15:44:52,239:INFO: Dataset: univ                Batch:  6/15	Loss 1.8884 (1.8515)
+2022-11-18 15:44:52,405:INFO: Dataset: univ                Batch:  7/15	Loss 1.8245 (1.8477)
+2022-11-18 15:44:52,574:INFO: Dataset: univ                Batch:  8/15	Loss 1.8364 (1.8464)
+2022-11-18 15:44:52,747:INFO: Dataset: univ                Batch:  9/15	Loss 1.8704 (1.8490)
+2022-11-18 15:44:52,927:INFO: Dataset: univ                Batch: 10/15	Loss 1.8309 (1.8472)
+2022-11-18 15:44:53,102:INFO: Dataset: univ                Batch: 11/15	Loss 1.8420 (1.8467)
+2022-11-18 15:44:53,266:INFO: Dataset: univ                Batch: 12/15	Loss 1.8203 (1.8446)
+2022-11-18 15:44:53,432:INFO: Dataset: univ                Batch: 13/15	Loss 1.8217 (1.8429)
+2022-11-18 15:44:53,704:INFO: Dataset: univ                Batch: 14/15	Loss 1.8665 (1.8446)
+2022-11-18 15:44:53,809:INFO: Dataset: univ                Batch: 15/15	Loss 1.7987 (1.8440)
+2022-11-18 15:44:54,250:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8160 (1.8160)
+2022-11-18 15:44:54,406:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9453 (1.8861)
+2022-11-18 15:44:54,560:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8590 (1.8776)
+2022-11-18 15:44:54,735:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9213 (1.8885)
+2022-11-18 15:44:54,932:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8139 (1.8743)
+2022-11-18 15:44:55,104:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8790 (1.8752)
+2022-11-18 15:44:55,263:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9168 (1.8813)
+2022-11-18 15:44:55,404:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7645 (1.8683)
+2022-11-18 15:44:55,801:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8236 (1.8236)
+2022-11-18 15:44:55,959:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7458 (1.7850)
+2022-11-18 15:44:56,117:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7460 (1.7708)
+2022-11-18 15:44:56,274:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8560 (1.7890)
+2022-11-18 15:44:56,434:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9091 (1.8126)
+2022-11-18 15:44:56,591:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8981 (1.8267)
+2022-11-18 15:44:56,746:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9546 (1.8452)
+2022-11-18 15:44:56,900:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7947 (1.8394)
+2022-11-18 15:44:57,079:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8290 (1.8383)
+2022-11-18 15:44:57,259:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8393 (1.8384)
+2022-11-18 15:44:57,426:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7760 (1.8329)
+2022-11-18 15:44:57,621:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8479 (1.8342)
+2022-11-18 15:44:57,780:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8333 (1.8341)
+2022-11-18 15:44:57,960:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9008 (1.8387)
+2022-11-18 15:44:58,128:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7759 (1.8346)
+2022-11-18 15:44:58,281:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7450 (1.8296)
+2022-11-18 15:44:58,435:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9072 (1.8339)
+2022-11-18 15:44:58,580:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7727 (1.8314)
+2022-11-18 15:44:58,629:INFO: - Computing loss (validation)
+2022-11-18 15:44:58,906:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7860 (1.7860)
+2022-11-18 15:44:58,946:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1326 (1.8061)
+2022-11-18 15:44:59,410:INFO: Dataset: univ                Batch: 1/3	Loss 1.8701 (1.8701)
+2022-11-18 15:44:59,535:INFO: Dataset: univ                Batch: 2/3	Loss 1.8299 (1.8506)
+2022-11-18 15:44:59,620:INFO: Dataset: univ                Batch: 3/3	Loss 1.7938 (1.8338)
+2022-11-18 15:44:59,995:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8774 (1.8774)
+2022-11-18 15:45:00,044:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9446 (1.8938)
+2022-11-18 15:45:00,373:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8412 (1.8412)
+2022-11-18 15:45:00,456:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8061 (1.8237)
+2022-11-18 15:45:00,541:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7588 (1.8026)
+2022-11-18 15:45:00,622:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9326 (1.8355)
+2022-11-18 15:45:00,702:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8499 (1.8385)
+2022-11-18 15:45:00,755:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_530.pth.tar
+2022-11-18 15:45:00,755:INFO: 
+===> EPOCH: 531 (P2)
+2022-11-18 15:45:00,756:INFO: - Computing loss (training)
+2022-11-18 15:45:01,105:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9013 (1.9013)
+2022-11-18 15:45:01,259:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8765 (1.8893)
+2022-11-18 15:45:01,411:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8986 (1.8926)
+2022-11-18 15:45:01,525:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8336 (1.8832)
+2022-11-18 15:45:01,970:INFO: Dataset: univ                Batch:  1/15	Loss 1.8578 (1.8578)
+2022-11-18 15:45:02,133:INFO: Dataset: univ                Batch:  2/15	Loss 1.8963 (1.8756)
+2022-11-18 15:45:02,297:INFO: Dataset: univ                Batch:  3/15	Loss 1.8952 (1.8825)
+2022-11-18 15:45:02,458:INFO: Dataset: univ                Batch:  4/15	Loss 1.8828 (1.8825)
+2022-11-18 15:45:02,618:INFO: Dataset: univ                Batch:  5/15	Loss 1.8910 (1.8842)
+2022-11-18 15:45:02,783:INFO: Dataset: univ                Batch:  6/15	Loss 1.8793 (1.8834)
+2022-11-18 15:45:02,944:INFO: Dataset: univ                Batch:  7/15	Loss 1.8618 (1.8804)
+2022-11-18 15:45:03,104:INFO: Dataset: univ                Batch:  8/15	Loss 1.8070 (1.8700)
+2022-11-18 15:45:03,265:INFO: Dataset: univ                Batch:  9/15	Loss 1.8263 (1.8646)
+2022-11-18 15:45:03,424:INFO: Dataset: univ                Batch: 10/15	Loss 1.8737 (1.8656)
+2022-11-18 15:45:03,587:INFO: Dataset: univ                Batch: 11/15	Loss 1.8896 (1.8679)
+2022-11-18 15:45:03,748:INFO: Dataset: univ                Batch: 12/15	Loss 1.9146 (1.8717)
+2022-11-18 15:45:03,908:INFO: Dataset: univ                Batch: 13/15	Loss 1.8553 (1.8705)
+2022-11-18 15:45:04,068:INFO: Dataset: univ                Batch: 14/15	Loss 1.8913 (1.8719)
+2022-11-18 15:45:04,153:INFO: Dataset: univ                Batch: 15/15	Loss 1.9682 (1.8731)
+2022-11-18 15:45:04,541:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7827 (1.7827)
+2022-11-18 15:45:04,697:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7732 (1.7781)
+2022-11-18 15:45:04,853:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7857 (1.7808)
+2022-11-18 15:45:05,005:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9332 (1.8214)
+2022-11-18 15:45:05,169:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9559 (1.8501)
+2022-11-18 15:45:05,323:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8086 (1.8432)
+2022-11-18 15:45:05,478:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7915 (1.8362)
+2022-11-18 15:45:05,622:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8527 (1.8382)
+2022-11-18 15:45:06,024:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8087 (1.8087)
+2022-11-18 15:45:06,186:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8947 (1.8503)
+2022-11-18 15:45:06,349:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9120 (1.8712)
+2022-11-18 15:45:06,505:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7923 (1.8513)
+2022-11-18 15:45:06,674:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8530 (1.8516)
+2022-11-18 15:45:06,838:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8373 (1.8493)
+2022-11-18 15:45:07,004:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8578 (1.8504)
+2022-11-18 15:45:07,170:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9159 (1.8587)
+2022-11-18 15:45:07,329:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8768 (1.8606)
+2022-11-18 15:45:07,522:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8249 (1.8573)
+2022-11-18 15:45:07,718:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9663 (1.8681)
+2022-11-18 15:45:07,909:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8184 (1.8640)
+2022-11-18 15:45:08,123:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9008 (1.8668)
+2022-11-18 15:45:08,327:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8706 (1.8670)
+2022-11-18 15:45:08,500:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8010 (1.8623)
+2022-11-18 15:45:08,655:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8894 (1.8640)
+2022-11-18 15:45:08,809:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8537 (1.8634)
+2022-11-18 15:45:08,957:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8353 (1.8621)
+2022-11-18 15:45:09,009:INFO: - Computing loss (validation)
+2022-11-18 15:45:09,300:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8531 (1.8531)
+2022-11-18 15:45:09,338:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9408 (1.8609)
+2022-11-18 15:45:09,660:INFO: Dataset: univ                Batch: 1/3	Loss 1.8471 (1.8471)
+2022-11-18 15:45:09,746:INFO: Dataset: univ                Batch: 2/3	Loss 1.8892 (1.8692)
+2022-11-18 15:45:09,826:INFO: Dataset: univ                Batch: 3/3	Loss 1.8476 (1.8621)
+2022-11-18 15:45:10,142:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8940 (1.8940)
+2022-11-18 15:45:10,187:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7867 (1.8688)
+2022-11-18 15:45:10,546:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7863 (1.7863)
+2022-11-18 15:45:10,633:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8298 (1.8066)
+2022-11-18 15:45:10,716:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8348 (1.8159)
+2022-11-18 15:45:10,803:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7785 (1.8066)
+2022-11-18 15:45:10,885:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7963 (1.8046)
+2022-11-18 15:45:10,941:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_531.pth.tar
+2022-11-18 15:45:10,941:INFO: 
+===> EPOCH: 532 (P2)
+2022-11-18 15:45:10,942:INFO: - Computing loss (training)
+2022-11-18 15:45:11,283:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8406 (1.8406)
+2022-11-18 15:45:11,434:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7873 (1.8142)
+2022-11-18 15:45:11,584:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9754 (1.8674)
+2022-11-18 15:45:11,712:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8499 (1.8644)
+2022-11-18 15:45:12,115:INFO: Dataset: univ                Batch:  1/15	Loss 1.8277 (1.8277)
+2022-11-18 15:45:12,277:INFO: Dataset: univ                Batch:  2/15	Loss 1.8247 (1.8262)
+2022-11-18 15:45:12,435:INFO: Dataset: univ                Batch:  3/15	Loss 1.8942 (1.8482)
+2022-11-18 15:45:12,604:INFO: Dataset: univ                Batch:  4/15	Loss 1.8622 (1.8516)
+2022-11-18 15:45:12,766:INFO: Dataset: univ                Batch:  5/15	Loss 1.8731 (1.8562)
+2022-11-18 15:45:12,925:INFO: Dataset: univ                Batch:  6/15	Loss 1.8519 (1.8554)
+2022-11-18 15:45:13,082:INFO: Dataset: univ                Batch:  7/15	Loss 1.8764 (1.8581)
+2022-11-18 15:45:13,243:INFO: Dataset: univ                Batch:  8/15	Loss 1.8187 (1.8536)
+2022-11-18 15:45:13,411:INFO: Dataset: univ                Batch:  9/15	Loss 1.8731 (1.8558)
+2022-11-18 15:45:13,578:INFO: Dataset: univ                Batch: 10/15	Loss 1.8846 (1.8588)
+2022-11-18 15:45:13,743:INFO: Dataset: univ                Batch: 11/15	Loss 1.8320 (1.8567)
+2022-11-18 15:45:13,914:INFO: Dataset: univ                Batch: 12/15	Loss 1.8235 (1.8537)
+2022-11-18 15:45:14,093:INFO: Dataset: univ                Batch: 13/15	Loss 1.8525 (1.8536)
+2022-11-18 15:45:14,253:INFO: Dataset: univ                Batch: 14/15	Loss 1.8365 (1.8524)
+2022-11-18 15:45:14,336:INFO: Dataset: univ                Batch: 15/15	Loss 1.9546 (1.8539)
+2022-11-18 15:45:14,730:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8318 (1.8318)
+2022-11-18 15:45:14,879:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8198 (1.8252)
+2022-11-18 15:45:15,042:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8246 (1.8250)
+2022-11-18 15:45:15,196:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8941 (1.8424)
+2022-11-18 15:45:15,371:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9595 (1.8657)
+2022-11-18 15:45:15,532:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9065 (1.8735)
+2022-11-18 15:45:15,690:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8825 (1.8747)
+2022-11-18 15:45:15,841:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9328 (1.8809)
+2022-11-18 15:45:16,241:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8649 (1.8649)
+2022-11-18 15:45:16,398:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8730 (1.8691)
+2022-11-18 15:45:16,563:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8664 (1.8682)
+2022-11-18 15:45:16,736:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8270 (1.8577)
+2022-11-18 15:45:16,898:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8480 (1.8556)
+2022-11-18 15:45:17,058:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8749 (1.8587)
+2022-11-18 15:45:17,232:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8032 (1.8511)
+2022-11-18 15:45:17,404:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7871 (1.8435)
+2022-11-18 15:45:17,581:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7896 (1.8372)
+2022-11-18 15:45:17,746:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8063 (1.8344)
+2022-11-18 15:45:17,897:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8482 (1.8358)
+2022-11-18 15:45:18,048:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8455 (1.8366)
+2022-11-18 15:45:18,203:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8622 (1.8387)
+2022-11-18 15:45:18,363:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7192 (1.8299)
+2022-11-18 15:45:18,543:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8227 (1.8294)
+2022-11-18 15:45:18,721:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8200 (1.8288)
+2022-11-18 15:45:18,886:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8734 (1.8312)
+2022-11-18 15:45:19,026:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8584 (1.8327)
+2022-11-18 15:45:19,078:INFO: - Computing loss (validation)
+2022-11-18 15:45:19,351:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8537 (1.8537)
+2022-11-18 15:45:19,386:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7485 (1.8465)
+2022-11-18 15:45:19,736:INFO: Dataset: univ                Batch: 1/3	Loss 1.8397 (1.8397)
+2022-11-18 15:45:19,827:INFO: Dataset: univ                Batch: 2/3	Loss 1.8940 (1.8677)
+2022-11-18 15:45:19,909:INFO: Dataset: univ                Batch: 3/3	Loss 1.8398 (1.8589)
+2022-11-18 15:45:20,292:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9872 (1.9872)
+2022-11-18 15:45:20,351:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8393 (1.9501)
+2022-11-18 15:45:20,732:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8044 (1.8044)
+2022-11-18 15:45:20,821:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8266 (1.8154)
+2022-11-18 15:45:20,907:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9285 (1.8511)
+2022-11-18 15:45:20,984:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7824 (1.8331)
+2022-11-18 15:45:21,061:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8149 (1.8295)
+2022-11-18 15:45:21,127:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_532.pth.tar
+2022-11-18 15:45:21,127:INFO: 
+===> EPOCH: 533 (P2)
+2022-11-18 15:45:21,128:INFO: - Computing loss (training)
+2022-11-18 15:45:21,492:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8097 (1.8097)
+2022-11-18 15:45:21,651:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8043 (1.8070)
+2022-11-18 15:45:21,821:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9890 (1.8705)
+2022-11-18 15:45:21,968:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8836 (1.8728)
+2022-11-18 15:45:22,400:INFO: Dataset: univ                Batch:  1/15	Loss 1.8705 (1.8705)
+2022-11-18 15:45:22,573:INFO: Dataset: univ                Batch:  2/15	Loss 1.8834 (1.8768)
+2022-11-18 15:45:22,734:INFO: Dataset: univ                Batch:  3/15	Loss 1.8154 (1.8558)
+2022-11-18 15:45:22,898:INFO: Dataset: univ                Batch:  4/15	Loss 1.8829 (1.8622)
+2022-11-18 15:45:23,096:INFO: Dataset: univ                Batch:  5/15	Loss 1.8764 (1.8650)
+2022-11-18 15:45:23,280:INFO: Dataset: univ                Batch:  6/15	Loss 1.8589 (1.8639)
+2022-11-18 15:45:23,459:INFO: Dataset: univ                Batch:  7/15	Loss 1.8454 (1.8614)
+2022-11-18 15:45:23,626:INFO: Dataset: univ                Batch:  8/15	Loss 1.8696 (1.8624)
+2022-11-18 15:45:23,796:INFO: Dataset: univ                Batch:  9/15	Loss 1.8250 (1.8582)
+2022-11-18 15:45:23,977:INFO: Dataset: univ                Batch: 10/15	Loss 1.8742 (1.8597)
+2022-11-18 15:45:24,153:INFO: Dataset: univ                Batch: 11/15	Loss 1.8851 (1.8620)
+2022-11-18 15:45:24,341:INFO: Dataset: univ                Batch: 12/15	Loss 1.8403 (1.8601)
+2022-11-18 15:45:24,514:INFO: Dataset: univ                Batch: 13/15	Loss 1.8777 (1.8614)
+2022-11-18 15:45:24,702:INFO: Dataset: univ                Batch: 14/15	Loss 1.8224 (1.8586)
+2022-11-18 15:45:24,811:INFO: Dataset: univ                Batch: 15/15	Loss 1.8699 (1.8588)
+2022-11-18 15:45:25,229:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8520 (1.8520)
+2022-11-18 15:45:25,383:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8279 (1.8400)
+2022-11-18 15:45:25,545:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9116 (1.8619)
+2022-11-18 15:45:25,699:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9111 (1.8736)
+2022-11-18 15:45:25,886:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8155 (1.8618)
+2022-11-18 15:45:26,051:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8130 (1.8534)
+2022-11-18 15:45:26,204:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7745 (1.8425)
+2022-11-18 15:45:26,340:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8751 (1.8459)
+2022-11-18 15:45:26,776:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7832 (1.7832)
+2022-11-18 15:45:26,943:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8681 (1.8250)
+2022-11-18 15:45:27,099:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8554 (1.8342)
+2022-11-18 15:45:27,256:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8499 (1.8382)
+2022-11-18 15:45:27,418:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7872 (1.8285)
+2022-11-18 15:45:27,580:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8648 (1.8343)
+2022-11-18 15:45:27,749:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8958 (1.8440)
+2022-11-18 15:45:27,944:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8733 (1.8475)
+2022-11-18 15:45:28,117:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7785 (1.8391)
+2022-11-18 15:45:28,334:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9337 (1.8484)
+2022-11-18 15:45:28,507:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8728 (1.8507)
+2022-11-18 15:45:28,745:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9751 (1.8608)
+2022-11-18 15:45:28,914:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7814 (1.8549)
+2022-11-18 15:45:29,112:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8808 (1.8568)
+2022-11-18 15:45:29,273:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7413 (1.8495)
+2022-11-18 15:45:29,433:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8354 (1.8486)
+2022-11-18 15:45:29,587:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8173 (1.8468)
+2022-11-18 15:45:29,731:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8748 (1.8481)
+2022-11-18 15:45:29,782:INFO: - Computing loss (validation)
+2022-11-18 15:45:30,051:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7387 (1.7387)
+2022-11-18 15:45:30,085:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6526 (1.7316)
+2022-11-18 15:45:30,409:INFO: Dataset: univ                Batch: 1/3	Loss 1.8790 (1.8790)
+2022-11-18 15:45:30,493:INFO: Dataset: univ                Batch: 2/3	Loss 1.8513 (1.8653)
+2022-11-18 15:45:30,571:INFO: Dataset: univ                Batch: 3/3	Loss 1.7893 (1.8429)
+2022-11-18 15:45:30,887:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9099 (1.9099)
+2022-11-18 15:45:30,934:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8063 (1.8822)
+2022-11-18 15:45:31,265:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8312 (1.8312)
+2022-11-18 15:45:31,346:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8484 (1.8397)
+2022-11-18 15:45:31,419:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9148 (1.8625)
+2022-11-18 15:45:31,495:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8657 (1.8633)
+2022-11-18 15:45:31,568:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8401 (1.8586)
+2022-11-18 15:45:31,621:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_533.pth.tar
+2022-11-18 15:45:31,621:INFO: 
+===> EPOCH: 534 (P2)
+2022-11-18 15:45:31,622:INFO: - Computing loss (training)
+2022-11-18 15:45:31,958:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8517 (1.8517)
+2022-11-18 15:45:32,111:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8931 (1.8727)
+2022-11-18 15:45:32,263:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8458 (1.8639)
+2022-11-18 15:45:32,378:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9823 (1.8848)
+2022-11-18 15:45:32,772:INFO: Dataset: univ                Batch:  1/15	Loss 1.8866 (1.8866)
+2022-11-18 15:45:32,938:INFO: Dataset: univ                Batch:  2/15	Loss 1.8421 (1.8654)
+2022-11-18 15:45:33,104:INFO: Dataset: univ                Batch:  3/15	Loss 1.8387 (1.8569)
+2022-11-18 15:45:33,261:INFO: Dataset: univ                Batch:  4/15	Loss 1.8386 (1.8522)
+2022-11-18 15:45:33,422:INFO: Dataset: univ                Batch:  5/15	Loss 1.8581 (1.8534)
+2022-11-18 15:45:33,578:INFO: Dataset: univ                Batch:  6/15	Loss 1.8672 (1.8554)
+2022-11-18 15:45:33,736:INFO: Dataset: univ                Batch:  7/15	Loss 1.8542 (1.8553)
+2022-11-18 15:45:33,890:INFO: Dataset: univ                Batch:  8/15	Loss 1.8389 (1.8532)
+2022-11-18 15:45:34,047:INFO: Dataset: univ                Batch:  9/15	Loss 1.8920 (1.8577)
+2022-11-18 15:45:34,203:INFO: Dataset: univ                Batch: 10/15	Loss 1.8338 (1.8553)
+2022-11-18 15:45:34,361:INFO: Dataset: univ                Batch: 11/15	Loss 1.8849 (1.8581)
+2022-11-18 15:45:34,517:INFO: Dataset: univ                Batch: 12/15	Loss 1.8272 (1.8556)
+2022-11-18 15:45:34,673:INFO: Dataset: univ                Batch: 13/15	Loss 1.8274 (1.8536)
+2022-11-18 15:45:34,829:INFO: Dataset: univ                Batch: 14/15	Loss 1.8490 (1.8532)
+2022-11-18 15:45:34,912:INFO: Dataset: univ                Batch: 15/15	Loss 1.8047 (1.8525)
+2022-11-18 15:45:35,296:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8433 (1.8433)
+2022-11-18 15:45:35,445:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7723 (1.8079)
+2022-11-18 15:45:35,598:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7601 (1.7947)
+2022-11-18 15:45:35,747:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8318 (1.8045)
+2022-11-18 15:45:35,898:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9215 (1.8300)
+2022-11-18 15:45:36,046:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8656 (1.8360)
+2022-11-18 15:45:36,196:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8973 (1.8448)
+2022-11-18 15:45:36,331:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8988 (1.8509)
+2022-11-18 15:45:36,732:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8042 (1.8042)
+2022-11-18 15:45:36,883:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7673 (1.7857)
+2022-11-18 15:45:37,036:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7995 (1.7900)
+2022-11-18 15:45:37,189:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7783 (1.7872)
+2022-11-18 15:45:37,343:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8647 (1.8030)
+2022-11-18 15:45:37,494:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8970 (1.8187)
+2022-11-18 15:45:37,645:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8582 (1.8246)
+2022-11-18 15:45:37,794:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8895 (1.8323)
+2022-11-18 15:45:37,945:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8204 (1.8310)
+2022-11-18 15:45:38,093:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7621 (1.8240)
+2022-11-18 15:45:38,245:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9616 (1.8366)
+2022-11-18 15:45:38,394:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7840 (1.8323)
+2022-11-18 15:45:38,544:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8406 (1.8331)
+2022-11-18 15:45:38,694:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8331 (1.8331)
+2022-11-18 15:45:38,846:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8144 (1.8318)
+2022-11-18 15:45:38,996:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8776 (1.8348)
+2022-11-18 15:45:39,149:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8752 (1.8375)
+2022-11-18 15:45:39,288:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8461 (1.8380)
+2022-11-18 15:45:39,334:INFO: - Computing loss (validation)
+2022-11-18 15:45:39,612:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8687 (1.8687)
+2022-11-18 15:45:39,650:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4272 (1.8386)
+2022-11-18 15:45:39,982:INFO: Dataset: univ                Batch: 1/3	Loss 1.8636 (1.8636)
+2022-11-18 15:45:40,058:INFO: Dataset: univ                Batch: 2/3	Loss 1.8116 (1.8404)
+2022-11-18 15:45:40,132:INFO: Dataset: univ                Batch: 3/3	Loss 1.7726 (1.8178)
+2022-11-18 15:45:40,433:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9048 (1.9048)
+2022-11-18 15:45:40,482:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0178 (1.9309)
+2022-11-18 15:45:40,793:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8567 (1.8567)
+2022-11-18 15:45:40,875:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7956 (1.8285)
+2022-11-18 15:45:40,962:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8653 (1.8402)
+2022-11-18 15:45:41,043:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8801 (1.8500)
+2022-11-18 15:45:41,121:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7955 (1.8388)
+2022-11-18 15:45:41,175:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_534.pth.tar
+2022-11-18 15:45:41,175:INFO: 
+===> EPOCH: 535 (P2)
+2022-11-18 15:45:41,175:INFO: - Computing loss (training)
+2022-11-18 15:45:41,528:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7981 (1.7981)
+2022-11-18 15:45:41,679:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8647 (1.8317)
+2022-11-18 15:45:41,829:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7109 (1.7903)
+2022-11-18 15:45:41,946:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9451 (1.8175)
+2022-11-18 15:45:42,335:INFO: Dataset: univ                Batch:  1/15	Loss 1.8690 (1.8690)
+2022-11-18 15:45:42,499:INFO: Dataset: univ                Batch:  2/15	Loss 1.8668 (1.8679)
+2022-11-18 15:45:42,659:INFO: Dataset: univ                Batch:  3/15	Loss 1.8442 (1.8596)
+2022-11-18 15:45:42,812:INFO: Dataset: univ                Batch:  4/15	Loss 1.8626 (1.8604)
+2022-11-18 15:45:42,971:INFO: Dataset: univ                Batch:  5/15	Loss 1.8075 (1.8497)
+2022-11-18 15:45:43,132:INFO: Dataset: univ                Batch:  6/15	Loss 1.8514 (1.8500)
+2022-11-18 15:45:43,289:INFO: Dataset: univ                Batch:  7/15	Loss 1.8808 (1.8541)
+2022-11-18 15:45:43,443:INFO: Dataset: univ                Batch:  8/15	Loss 1.8429 (1.8527)
+2022-11-18 15:45:43,601:INFO: Dataset: univ                Batch:  9/15	Loss 1.8651 (1.8541)
+2022-11-18 15:45:43,756:INFO: Dataset: univ                Batch: 10/15	Loss 1.8268 (1.8513)
+2022-11-18 15:45:43,913:INFO: Dataset: univ                Batch: 11/15	Loss 1.8312 (1.8494)
+2022-11-18 15:45:44,070:INFO: Dataset: univ                Batch: 12/15	Loss 1.8783 (1.8518)
+2022-11-18 15:45:44,228:INFO: Dataset: univ                Batch: 13/15	Loss 1.8773 (1.8539)
+2022-11-18 15:45:44,383:INFO: Dataset: univ                Batch: 14/15	Loss 1.8605 (1.8544)
+2022-11-18 15:45:44,468:INFO: Dataset: univ                Batch: 15/15	Loss 1.7847 (1.8536)
+2022-11-18 15:45:44,854:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8486 (1.8486)
+2022-11-18 15:45:45,005:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7899 (1.8193)
+2022-11-18 15:45:45,158:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8668 (1.8358)
+2022-11-18 15:45:45,306:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7960 (1.8245)
+2022-11-18 15:45:45,456:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8825 (1.8362)
+2022-11-18 15:45:45,607:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8771 (1.8426)
+2022-11-18 15:45:45,758:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8435 (1.8427)
+2022-11-18 15:45:45,894:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8370 (1.8420)
+2022-11-18 15:45:46,286:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8227 (1.8227)
+2022-11-18 15:45:46,438:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7917 (1.8088)
+2022-11-18 15:45:46,595:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8340 (1.8176)
+2022-11-18 15:45:46,746:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7987 (1.8131)
+2022-11-18 15:45:46,897:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8957 (1.8295)
+2022-11-18 15:45:47,051:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8707 (1.8362)
+2022-11-18 15:45:47,202:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9482 (1.8512)
+2022-11-18 15:45:47,350:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9145 (1.8593)
+2022-11-18 15:45:47,500:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7704 (1.8498)
+2022-11-18 15:45:47,650:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9358 (1.8582)
+2022-11-18 15:45:47,801:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8342 (1.8561)
+2022-11-18 15:45:47,952:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9458 (1.8632)
+2022-11-18 15:45:48,102:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7766 (1.8577)
+2022-11-18 15:45:48,254:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7787 (1.8526)
+2022-11-18 15:45:48,406:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7185 (1.8428)
+2022-11-18 15:45:48,556:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8841 (1.8454)
+2022-11-18 15:45:48,708:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8133 (1.8435)
+2022-11-18 15:45:48,845:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8356 (1.8432)
+2022-11-18 15:45:48,891:INFO: - Computing loss (validation)
+2022-11-18 15:45:49,158:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9286 (1.9286)
+2022-11-18 15:45:49,193:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9768 (1.9316)
+2022-11-18 15:45:49,529:INFO: Dataset: univ                Batch: 1/3	Loss 1.8898 (1.8898)
+2022-11-18 15:45:49,612:INFO: Dataset: univ                Batch: 2/3	Loss 1.8019 (1.8418)
+2022-11-18 15:45:49,686:INFO: Dataset: univ                Batch: 3/3	Loss 1.8264 (1.8364)
+2022-11-18 15:45:49,990:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8568 (1.8568)
+2022-11-18 15:45:50,035:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9621 (1.8835)
+2022-11-18 15:45:50,352:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8783 (1.8783)
+2022-11-18 15:45:50,433:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8541 (1.8654)
+2022-11-18 15:45:50,512:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7825 (1.8371)
+2022-11-18 15:45:50,593:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9021 (1.8531)
+2022-11-18 15:45:50,672:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8578 (1.8541)
+2022-11-18 15:45:50,723:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_535.pth.tar
+2022-11-18 15:45:50,723:INFO: 
+===> EPOCH: 536 (P2)
+2022-11-18 15:45:50,724:INFO: - Computing loss (training)
+2022-11-18 15:45:51,083:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8814 (1.8814)
+2022-11-18 15:45:51,238:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9665 (1.9229)
+2022-11-18 15:45:51,393:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8912 (1.9127)
+2022-11-18 15:45:51,510:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9262 (1.9150)
+2022-11-18 15:45:51,961:INFO: Dataset: univ                Batch:  1/15	Loss 1.8352 (1.8352)
+2022-11-18 15:45:52,117:INFO: Dataset: univ                Batch:  2/15	Loss 1.8697 (1.8528)
+2022-11-18 15:45:52,277:INFO: Dataset: univ                Batch:  3/15	Loss 1.8537 (1.8531)
+2022-11-18 15:45:52,430:INFO: Dataset: univ                Batch:  4/15	Loss 1.8451 (1.8511)
+2022-11-18 15:45:52,585:INFO: Dataset: univ                Batch:  5/15	Loss 1.8287 (1.8467)
+2022-11-18 15:45:52,743:INFO: Dataset: univ                Batch:  6/15	Loss 1.8565 (1.8484)
+2022-11-18 15:45:52,898:INFO: Dataset: univ                Batch:  7/15	Loss 1.8246 (1.8453)
+2022-11-18 15:45:53,053:INFO: Dataset: univ                Batch:  8/15	Loss 1.8497 (1.8459)
+2022-11-18 15:45:53,208:INFO: Dataset: univ                Batch:  9/15	Loss 1.8613 (1.8475)
+2022-11-18 15:45:53,369:INFO: Dataset: univ                Batch: 10/15	Loss 1.8140 (1.8440)
+2022-11-18 15:45:53,525:INFO: Dataset: univ                Batch: 11/15	Loss 1.8938 (1.8484)
+2022-11-18 15:45:53,681:INFO: Dataset: univ                Batch: 12/15	Loss 1.8538 (1.8488)
+2022-11-18 15:45:53,837:INFO: Dataset: univ                Batch: 13/15	Loss 1.8196 (1.8468)
+2022-11-18 15:45:53,993:INFO: Dataset: univ                Batch: 14/15	Loss 1.8460 (1.8468)
+2022-11-18 15:45:54,076:INFO: Dataset: univ                Batch: 15/15	Loss 1.8446 (1.8467)
+2022-11-18 15:45:54,471:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8578 (1.8578)
+2022-11-18 15:45:54,619:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8252 (1.8416)
+2022-11-18 15:45:54,768:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8468 (1.8434)
+2022-11-18 15:45:54,915:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8311 (1.8406)
+2022-11-18 15:45:55,064:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8215 (1.8368)
+2022-11-18 15:45:55,215:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8726 (1.8424)
+2022-11-18 15:45:55,364:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9403 (1.8564)
+2022-11-18 15:45:55,499:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9547 (1.8671)
+2022-11-18 15:45:55,907:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7845 (1.7845)
+2022-11-18 15:45:56,057:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8871 (1.8395)
+2022-11-18 15:45:56,211:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9137 (1.8642)
+2022-11-18 15:45:56,361:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8801 (1.8679)
+2022-11-18 15:45:56,513:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7935 (1.8524)
+2022-11-18 15:45:56,666:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8833 (1.8574)
+2022-11-18 15:45:56,815:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8501 (1.8562)
+2022-11-18 15:45:56,963:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8396 (1.8541)
+2022-11-18 15:45:57,113:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8529 (1.8540)
+2022-11-18 15:45:57,262:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8892 (1.8571)
+2022-11-18 15:45:57,413:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7809 (1.8509)
+2022-11-18 15:45:57,561:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7976 (1.8470)
+2022-11-18 15:45:57,711:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8299 (1.8456)
+2022-11-18 15:45:57,859:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8313 (1.8446)
+2022-11-18 15:45:58,010:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9080 (1.8483)
+2022-11-18 15:45:58,160:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8633 (1.8492)
+2022-11-18 15:45:58,311:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8259 (1.8477)
+2022-11-18 15:45:58,448:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9156 (1.8510)
+2022-11-18 15:45:58,493:INFO: - Computing loss (validation)
+2022-11-18 15:45:58,763:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8546 (1.8546)
+2022-11-18 15:45:58,797:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9618 (1.8626)
+2022-11-18 15:45:59,113:INFO: Dataset: univ                Batch: 1/3	Loss 1.8308 (1.8308)
+2022-11-18 15:45:59,196:INFO: Dataset: univ                Batch: 2/3	Loss 1.8742 (1.8503)
+2022-11-18 15:45:59,272:INFO: Dataset: univ                Batch: 3/3	Loss 1.8745 (1.8578)
+2022-11-18 15:45:59,593:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8847 (1.8847)
+2022-11-18 15:45:59,642:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9583 (1.9032)
+2022-11-18 15:45:59,945:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9066 (1.9066)
+2022-11-18 15:46:00,023:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8505 (1.8788)
+2022-11-18 15:46:00,099:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8700 (1.8759)
+2022-11-18 15:46:00,176:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8691 (1.8744)
+2022-11-18 15:46:00,252:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7933 (1.8586)
+2022-11-18 15:46:00,305:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_536.pth.tar
+2022-11-18 15:46:00,305:INFO: 
+===> EPOCH: 537 (P2)
+2022-11-18 15:46:00,306:INFO: - Computing loss (training)
+2022-11-18 15:46:00,648:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8028 (1.8028)
+2022-11-18 15:46:00,799:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9403 (1.8756)
+2022-11-18 15:46:00,949:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9653 (1.9054)
+2022-11-18 15:46:01,066:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7630 (1.8813)
+2022-11-18 15:46:01,469:INFO: Dataset: univ                Batch:  1/15	Loss 1.8535 (1.8535)
+2022-11-18 15:46:01,633:INFO: Dataset: univ                Batch:  2/15	Loss 1.8876 (1.8701)
+2022-11-18 15:46:01,798:INFO: Dataset: univ                Batch:  3/15	Loss 1.8671 (1.8691)
+2022-11-18 15:46:01,960:INFO: Dataset: univ                Batch:  4/15	Loss 1.8989 (1.8763)
+2022-11-18 15:46:02,125:INFO: Dataset: univ                Batch:  5/15	Loss 1.8536 (1.8719)
+2022-11-18 15:46:02,287:INFO: Dataset: univ                Batch:  6/15	Loss 1.8569 (1.8692)
+2022-11-18 15:46:02,448:INFO: Dataset: univ                Batch:  7/15	Loss 1.8320 (1.8645)
+2022-11-18 15:46:02,607:INFO: Dataset: univ                Batch:  8/15	Loss 1.8346 (1.8605)
+2022-11-18 15:46:02,768:INFO: Dataset: univ                Batch:  9/15	Loss 1.8391 (1.8583)
+2022-11-18 15:46:02,926:INFO: Dataset: univ                Batch: 10/15	Loss 1.8288 (1.8553)
+2022-11-18 15:46:03,090:INFO: Dataset: univ                Batch: 11/15	Loss 1.8166 (1.8515)
+2022-11-18 15:46:03,251:INFO: Dataset: univ                Batch: 12/15	Loss 1.8953 (1.8552)
+2022-11-18 15:46:03,411:INFO: Dataset: univ                Batch: 13/15	Loss 1.8385 (1.8542)
+2022-11-18 15:46:03,570:INFO: Dataset: univ                Batch: 14/15	Loss 1.8504 (1.8539)
+2022-11-18 15:46:03,657:INFO: Dataset: univ                Batch: 15/15	Loss 1.8281 (1.8536)
+2022-11-18 15:46:04,043:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8781 (1.8781)
+2022-11-18 15:46:04,193:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9290 (1.9040)
+2022-11-18 15:46:04,344:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8439 (1.8831)
+2022-11-18 15:46:04,493:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8667 (1.8786)
+2022-11-18 15:46:04,644:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0347 (1.9153)
+2022-11-18 15:46:04,794:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8396 (1.9035)
+2022-11-18 15:46:04,944:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8633 (1.8976)
+2022-11-18 15:46:05,080:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8935 (1.8971)
+2022-11-18 15:46:05,481:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7931 (1.7931)
+2022-11-18 15:46:05,634:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8564 (1.8256)
+2022-11-18 15:46:05,791:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8677 (1.8380)
+2022-11-18 15:46:05,948:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9814 (1.8699)
+2022-11-18 15:46:06,103:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8258 (1.8614)
+2022-11-18 15:46:06,260:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8805 (1.8647)
+2022-11-18 15:46:06,414:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8955 (1.8684)
+2022-11-18 15:46:06,565:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8055 (1.8597)
+2022-11-18 15:46:06,718:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8891 (1.8635)
+2022-11-18 15:46:06,870:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7694 (1.8540)
+2022-11-18 15:46:07,024:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7809 (1.8478)
+2022-11-18 15:46:07,179:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7992 (1.8435)
+2022-11-18 15:46:07,333:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8734 (1.8456)
+2022-11-18 15:46:07,486:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7339 (1.8377)
+2022-11-18 15:46:07,641:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8844 (1.8409)
+2022-11-18 15:46:07,795:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8063 (1.8389)
+2022-11-18 15:46:07,949:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8030 (1.8368)
+2022-11-18 15:46:08,090:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7770 (1.8337)
+2022-11-18 15:46:08,137:INFO: - Computing loss (validation)
+2022-11-18 15:46:08,404:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8701 (1.8701)
+2022-11-18 15:46:08,438:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6767 (1.8563)
+2022-11-18 15:46:08,759:INFO: Dataset: univ                Batch: 1/3	Loss 1.8602 (1.8602)
+2022-11-18 15:46:08,844:INFO: Dataset: univ                Batch: 2/3	Loss 1.8330 (1.8474)
+2022-11-18 15:46:08,920:INFO: Dataset: univ                Batch: 3/3	Loss 1.8948 (1.8615)
+2022-11-18 15:46:09,256:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8435 (1.8435)
+2022-11-18 15:46:09,305:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7974 (1.8316)
+2022-11-18 15:46:09,604:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8978 (1.8978)
+2022-11-18 15:46:09,680:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8478 (1.8724)
+2022-11-18 15:46:09,755:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8606 (1.8687)
+2022-11-18 15:46:09,830:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8113 (1.8534)
+2022-11-18 15:46:09,905:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8959 (1.8621)
+2022-11-18 15:46:09,955:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_537.pth.tar
+2022-11-18 15:46:09,956:INFO: 
+===> EPOCH: 538 (P2)
+2022-11-18 15:46:09,956:INFO: - Computing loss (training)
+2022-11-18 15:46:10,290:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9627 (1.9627)
+2022-11-18 15:46:10,439:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8972 (1.9294)
+2022-11-18 15:46:10,587:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7816 (1.8797)
+2022-11-18 15:46:10,703:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9036 (1.8836)
+2022-11-18 15:46:11,104:INFO: Dataset: univ                Batch:  1/15	Loss 1.8738 (1.8738)
+2022-11-18 15:46:11,264:INFO: Dataset: univ                Batch:  2/15	Loss 1.8966 (1.8845)
+2022-11-18 15:46:11,419:INFO: Dataset: univ                Batch:  3/15	Loss 1.8488 (1.8734)
+2022-11-18 15:46:11,571:INFO: Dataset: univ                Batch:  4/15	Loss 1.8311 (1.8632)
+2022-11-18 15:46:11,726:INFO: Dataset: univ                Batch:  5/15	Loss 1.8733 (1.8651)
+2022-11-18 15:46:11,880:INFO: Dataset: univ                Batch:  6/15	Loss 1.8279 (1.8591)
+2022-11-18 15:46:12,035:INFO: Dataset: univ                Batch:  7/15	Loss 1.8659 (1.8600)
+2022-11-18 15:46:12,191:INFO: Dataset: univ                Batch:  8/15	Loss 1.8021 (1.8530)
+2022-11-18 15:46:12,346:INFO: Dataset: univ                Batch:  9/15	Loss 1.8271 (1.8501)
+2022-11-18 15:46:12,578:INFO: Dataset: univ                Batch: 10/15	Loss 1.8899 (1.8539)
+2022-11-18 15:46:12,735:INFO: Dataset: univ                Batch: 11/15	Loss 1.8221 (1.8508)
+2022-11-18 15:46:12,890:INFO: Dataset: univ                Batch: 12/15	Loss 1.8546 (1.8511)
+2022-11-18 15:46:13,043:INFO: Dataset: univ                Batch: 13/15	Loss 1.8642 (1.8522)
+2022-11-18 15:46:13,201:INFO: Dataset: univ                Batch: 14/15	Loss 1.8208 (1.8499)
+2022-11-18 15:46:13,282:INFO: Dataset: univ                Batch: 15/15	Loss 1.8471 (1.8498)
+2022-11-18 15:46:13,678:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9414 (1.9414)
+2022-11-18 15:46:13,833:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9052 (1.9230)
+2022-11-18 15:46:13,989:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8556 (1.8992)
+2022-11-18 15:46:14,153:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8272 (1.8799)
+2022-11-18 15:46:14,308:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8224 (1.8691)
+2022-11-18 15:46:14,460:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9193 (1.8780)
+2022-11-18 15:46:14,615:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8371 (1.8724)
+2022-11-18 15:46:14,757:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9955 (1.8881)
+2022-11-18 15:46:15,167:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8717 (1.8717)
+2022-11-18 15:46:15,328:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8532 (1.8626)
+2022-11-18 15:46:15,484:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8989 (1.8747)
+2022-11-18 15:46:15,638:INFO: Dataset: zara2               Batch:  4/18	Loss 1.6907 (1.8300)
+2022-11-18 15:46:15,797:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8019 (1.8240)
+2022-11-18 15:46:15,952:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9096 (1.8379)
+2022-11-18 15:46:16,108:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9143 (1.8484)
+2022-11-18 15:46:16,262:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8228 (1.8454)
+2022-11-18 15:46:16,417:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9097 (1.8528)
+2022-11-18 15:46:16,569:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8521 (1.8527)
+2022-11-18 15:46:16,730:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9018 (1.8569)
+2022-11-18 15:46:16,882:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8848 (1.8590)
+2022-11-18 15:46:17,038:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7897 (1.8538)
+2022-11-18 15:46:17,194:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7872 (1.8491)
+2022-11-18 15:46:17,351:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8640 (1.8501)
+2022-11-18 15:46:17,504:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7389 (1.8430)
+2022-11-18 15:46:17,661:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7595 (1.8378)
+2022-11-18 15:46:17,804:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9384 (1.8432)
+2022-11-18 15:46:17,850:INFO: - Computing loss (validation)
+2022-11-18 15:46:18,142:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8696 (1.8696)
+2022-11-18 15:46:18,178:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6545 (1.8549)
+2022-11-18 15:46:18,500:INFO: Dataset: univ                Batch: 1/3	Loss 1.8958 (1.8958)
+2022-11-18 15:46:18,586:INFO: Dataset: univ                Batch: 2/3	Loss 1.8298 (1.8619)
+2022-11-18 15:46:18,666:INFO: Dataset: univ                Batch: 3/3	Loss 1.8577 (1.8605)
+2022-11-18 15:46:18,979:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9629 (1.9629)
+2022-11-18 15:46:19,026:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7535 (1.9131)
+2022-11-18 15:46:19,340:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8283 (1.8283)
+2022-11-18 15:46:19,413:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8417 (1.8351)
+2022-11-18 15:46:19,488:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7447 (1.8045)
+2022-11-18 15:46:19,562:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8738 (1.8214)
+2022-11-18 15:46:19,635:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9544 (1.8479)
+2022-11-18 15:46:19,690:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_538.pth.tar
+2022-11-18 15:46:19,690:INFO: 
+===> EPOCH: 539 (P2)
+2022-11-18 15:46:19,691:INFO: - Computing loss (training)
+2022-11-18 15:46:20,028:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9517 (1.9517)
+2022-11-18 15:46:20,181:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8774 (1.9154)
+2022-11-18 15:46:20,330:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8931 (1.9080)
+2022-11-18 15:46:20,443:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0781 (1.9360)
+2022-11-18 15:46:20,857:INFO: Dataset: univ                Batch:  1/15	Loss 1.8634 (1.8634)
+2022-11-18 15:46:21,020:INFO: Dataset: univ                Batch:  2/15	Loss 1.9082 (1.8836)
+2022-11-18 15:46:21,186:INFO: Dataset: univ                Batch:  3/15	Loss 1.8226 (1.8651)
+2022-11-18 15:46:21,349:INFO: Dataset: univ                Batch:  4/15	Loss 1.8901 (1.8717)
+2022-11-18 15:46:21,511:INFO: Dataset: univ                Batch:  5/15	Loss 1.8592 (1.8692)
+2022-11-18 15:46:21,677:INFO: Dataset: univ                Batch:  6/15	Loss 1.8847 (1.8718)
+2022-11-18 15:46:21,838:INFO: Dataset: univ                Batch:  7/15	Loss 1.8815 (1.8730)
+2022-11-18 15:46:21,998:INFO: Dataset: univ                Batch:  8/15	Loss 1.8366 (1.8681)
+2022-11-18 15:46:22,163:INFO: Dataset: univ                Batch:  9/15	Loss 1.8051 (1.8608)
+2022-11-18 15:46:22,323:INFO: Dataset: univ                Batch: 10/15	Loss 1.8958 (1.8643)
+2022-11-18 15:46:22,483:INFO: Dataset: univ                Batch: 11/15	Loss 1.8696 (1.8648)
+2022-11-18 15:46:22,644:INFO: Dataset: univ                Batch: 12/15	Loss 1.8324 (1.8622)
+2022-11-18 15:46:22,806:INFO: Dataset: univ                Batch: 13/15	Loss 1.8132 (1.8581)
+2022-11-18 15:46:22,966:INFO: Dataset: univ                Batch: 14/15	Loss 1.8649 (1.8585)
+2022-11-18 15:46:23,052:INFO: Dataset: univ                Batch: 15/15	Loss 1.8863 (1.8588)
+2022-11-18 15:46:23,451:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8693 (1.8693)
+2022-11-18 15:46:23,598:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9648 (1.9102)
+2022-11-18 15:46:23,747:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7310 (1.8478)
+2022-11-18 15:46:23,893:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7823 (1.8300)
+2022-11-18 15:46:24,043:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8630 (1.8365)
+2022-11-18 15:46:24,203:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8103 (1.8316)
+2022-11-18 15:46:24,352:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9453 (1.8481)
+2022-11-18 15:46:24,486:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9263 (1.8558)
+2022-11-18 15:46:24,872:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7737 (1.7737)
+2022-11-18 15:46:25,021:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8160 (1.7943)
+2022-11-18 15:46:25,178:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9061 (1.8321)
+2022-11-18 15:46:25,326:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8286 (1.8312)
+2022-11-18 15:46:25,477:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8017 (1.8259)
+2022-11-18 15:46:25,628:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8038 (1.8221)
+2022-11-18 15:46:25,779:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8115 (1.8208)
+2022-11-18 15:46:25,930:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8900 (1.8299)
+2022-11-18 15:46:26,080:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8482 (1.8317)
+2022-11-18 15:46:26,230:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9128 (1.8400)
+2022-11-18 15:46:26,381:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8101 (1.8370)
+2022-11-18 15:46:26,529:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6988 (1.8267)
+2022-11-18 15:46:26,681:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9294 (1.8346)
+2022-11-18 15:46:26,830:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9088 (1.8397)
+2022-11-18 15:46:26,982:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8217 (1.8384)
+2022-11-18 15:46:27,131:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8608 (1.8397)
+2022-11-18 15:46:27,285:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8057 (1.8378)
+2022-11-18 15:46:27,422:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8137 (1.8366)
+2022-11-18 15:46:27,471:INFO: - Computing loss (validation)
+2022-11-18 15:46:27,740:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7907 (1.7907)
+2022-11-18 15:46:27,775:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6587 (1.7844)
+2022-11-18 15:46:28,081:INFO: Dataset: univ                Batch: 1/3	Loss 1.8506 (1.8506)
+2022-11-18 15:46:28,160:INFO: Dataset: univ                Batch: 2/3	Loss 1.8399 (1.8448)
+2022-11-18 15:46:28,233:INFO: Dataset: univ                Batch: 3/3	Loss 1.8500 (1.8465)
+2022-11-18 15:46:28,526:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8540 (1.8540)
+2022-11-18 15:46:28,571:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0815 (1.9140)
+2022-11-18 15:46:28,876:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8075 (1.8075)
+2022-11-18 15:46:28,953:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8247 (1.8161)
+2022-11-18 15:46:29,028:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8698 (1.8332)
+2022-11-18 15:46:29,106:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7710 (1.8191)
+2022-11-18 15:46:29,181:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7049 (1.7980)
+2022-11-18 15:46:29,235:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_539.pth.tar
+2022-11-18 15:46:29,235:INFO: 
+===> EPOCH: 540 (P2)
+2022-11-18 15:46:29,236:INFO: - Computing loss (training)
+2022-11-18 15:46:29,584:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8537 (1.8537)
+2022-11-18 15:46:29,740:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9469 (1.8982)
+2022-11-18 15:46:29,896:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9734 (1.9233)
+2022-11-18 15:46:30,014:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8755 (1.9153)
+2022-11-18 15:46:30,405:INFO: Dataset: univ                Batch:  1/15	Loss 1.8295 (1.8295)
+2022-11-18 15:46:30,561:INFO: Dataset: univ                Batch:  2/15	Loss 1.8552 (1.8430)
+2022-11-18 15:46:30,723:INFO: Dataset: univ                Batch:  3/15	Loss 1.8881 (1.8586)
+2022-11-18 15:46:30,879:INFO: Dataset: univ                Batch:  4/15	Loss 1.8581 (1.8585)
+2022-11-18 15:46:31,045:INFO: Dataset: univ                Batch:  5/15	Loss 1.8415 (1.8552)
+2022-11-18 15:46:31,206:INFO: Dataset: univ                Batch:  6/15	Loss 1.9118 (1.8650)
+2022-11-18 15:46:31,367:INFO: Dataset: univ                Batch:  7/15	Loss 1.8832 (1.8673)
+2022-11-18 15:46:31,527:INFO: Dataset: univ                Batch:  8/15	Loss 1.8357 (1.8632)
+2022-11-18 15:46:31,684:INFO: Dataset: univ                Batch:  9/15	Loss 1.8702 (1.8640)
+2022-11-18 15:46:31,838:INFO: Dataset: univ                Batch: 10/15	Loss 1.8144 (1.8590)
+2022-11-18 15:46:31,995:INFO: Dataset: univ                Batch: 11/15	Loss 1.8621 (1.8593)
+2022-11-18 15:46:32,178:INFO: Dataset: univ                Batch: 12/15	Loss 1.8261 (1.8566)
+2022-11-18 15:46:32,343:INFO: Dataset: univ                Batch: 13/15	Loss 1.8547 (1.8565)
+2022-11-18 15:46:32,502:INFO: Dataset: univ                Batch: 14/15	Loss 1.8560 (1.8564)
+2022-11-18 15:46:32,586:INFO: Dataset: univ                Batch: 15/15	Loss 1.8817 (1.8568)
+2022-11-18 15:46:32,992:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9223 (1.9223)
+2022-11-18 15:46:33,144:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7238 (1.8347)
+2022-11-18 15:46:33,294:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8026 (1.8240)
+2022-11-18 15:46:33,441:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7582 (1.8089)
+2022-11-18 15:46:33,591:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8489 (1.8163)
+2022-11-18 15:46:33,740:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8433 (1.8206)
+2022-11-18 15:46:33,888:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9482 (1.8386)
+2022-11-18 15:46:34,023:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9176 (1.8463)
+2022-11-18 15:46:34,414:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7993 (1.7993)
+2022-11-18 15:46:34,563:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8090 (1.8039)
+2022-11-18 15:46:34,717:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8461 (1.8171)
+2022-11-18 15:46:34,865:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8517 (1.8263)
+2022-11-18 15:46:35,018:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9067 (1.8423)
+2022-11-18 15:46:35,168:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8514 (1.8438)
+2022-11-18 15:46:35,317:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8554 (1.8455)
+2022-11-18 15:46:35,464:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8199 (1.8419)
+2022-11-18 15:46:35,613:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9131 (1.8499)
+2022-11-18 15:46:35,761:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8575 (1.8507)
+2022-11-18 15:46:35,912:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8382 (1.8496)
+2022-11-18 15:46:36,061:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8641 (1.8507)
+2022-11-18 15:46:36,212:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8585 (1.8513)
+2022-11-18 15:46:36,361:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8361 (1.8503)
+2022-11-18 15:46:36,512:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8195 (1.8483)
+2022-11-18 15:46:36,660:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7528 (1.8427)
+2022-11-18 15:46:36,810:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7816 (1.8392)
+2022-11-18 15:46:36,946:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8342 (1.8389)
+2022-11-18 15:46:36,994:INFO: - Computing loss (validation)
+2022-11-18 15:46:37,262:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7759 (1.7759)
+2022-11-18 15:46:37,297:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8616 (1.7800)
+2022-11-18 15:46:37,613:INFO: Dataset: univ                Batch: 1/3	Loss 1.8532 (1.8532)
+2022-11-18 15:46:37,695:INFO: Dataset: univ                Batch: 2/3	Loss 1.8443 (1.8490)
+2022-11-18 15:46:37,775:INFO: Dataset: univ                Batch: 3/3	Loss 1.9110 (1.8711)
+2022-11-18 15:46:38,086:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9155 (1.9155)
+2022-11-18 15:46:38,134:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7985 (1.8914)
+2022-11-18 15:46:38,461:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8141 (1.8141)
+2022-11-18 15:46:38,542:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8809 (1.8470)
+2022-11-18 15:46:38,621:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9156 (1.8685)
+2022-11-18 15:46:38,702:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8350 (1.8595)
+2022-11-18 15:46:38,782:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8893 (1.8651)
+2022-11-18 15:46:38,839:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_540.pth.tar
+2022-11-18 15:46:38,840:INFO: 
+===> EPOCH: 541 (P2)
+2022-11-18 15:46:38,840:INFO: - Computing loss (training)
+2022-11-18 15:46:39,183:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9159 (1.9159)
+2022-11-18 15:46:39,338:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9748 (1.9461)
+2022-11-18 15:46:39,492:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8065 (1.9001)
+2022-11-18 15:46:39,611:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8233 (1.8869)
+2022-11-18 15:46:40,011:INFO: Dataset: univ                Batch:  1/15	Loss 1.8401 (1.8401)
+2022-11-18 15:46:40,177:INFO: Dataset: univ                Batch:  2/15	Loss 1.8427 (1.8413)
+2022-11-18 15:46:40,339:INFO: Dataset: univ                Batch:  3/15	Loss 1.8063 (1.8292)
+2022-11-18 15:46:40,499:INFO: Dataset: univ                Batch:  4/15	Loss 1.8328 (1.8302)
+2022-11-18 15:46:40,661:INFO: Dataset: univ                Batch:  5/15	Loss 1.8364 (1.8315)
+2022-11-18 15:46:40,825:INFO: Dataset: univ                Batch:  6/15	Loss 1.8790 (1.8392)
+2022-11-18 15:46:40,987:INFO: Dataset: univ                Batch:  7/15	Loss 1.8613 (1.8424)
+2022-11-18 15:46:41,149:INFO: Dataset: univ                Batch:  8/15	Loss 1.7938 (1.8365)
+2022-11-18 15:46:41,318:INFO: Dataset: univ                Batch:  9/15	Loss 1.8573 (1.8391)
+2022-11-18 15:46:41,485:INFO: Dataset: univ                Batch: 10/15	Loss 1.8342 (1.8386)
+2022-11-18 15:46:41,648:INFO: Dataset: univ                Batch: 11/15	Loss 1.9049 (1.8448)
+2022-11-18 15:46:41,812:INFO: Dataset: univ                Batch: 12/15	Loss 1.8980 (1.8490)
+2022-11-18 15:46:41,977:INFO: Dataset: univ                Batch: 13/15	Loss 1.8179 (1.8465)
+2022-11-18 15:46:42,142:INFO: Dataset: univ                Batch: 14/15	Loss 1.8052 (1.8433)
+2022-11-18 15:46:42,230:INFO: Dataset: univ                Batch: 15/15	Loss 1.8433 (1.8433)
+2022-11-18 15:46:42,628:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8830 (1.8830)
+2022-11-18 15:46:42,783:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9260 (1.9034)
+2022-11-18 15:46:42,938:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8843 (1.8970)
+2022-11-18 15:46:43,106:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8997 (1.8976)
+2022-11-18 15:46:43,260:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7824 (1.8729)
+2022-11-18 15:46:43,431:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7738 (1.8547)
+2022-11-18 15:46:43,587:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9010 (1.8614)
+2022-11-18 15:46:43,741:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8228 (1.8574)
+2022-11-18 15:46:44,132:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8727 (1.8727)
+2022-11-18 15:46:44,300:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7872 (1.8287)
+2022-11-18 15:46:44,459:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7616 (1.8079)
+2022-11-18 15:46:44,617:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7819 (1.8015)
+2022-11-18 15:46:44,774:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8666 (1.8148)
+2022-11-18 15:46:44,930:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9255 (1.8347)
+2022-11-18 15:46:45,082:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8494 (1.8367)
+2022-11-18 15:46:45,234:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8187 (1.8344)
+2022-11-18 15:46:45,401:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8831 (1.8396)
+2022-11-18 15:46:45,582:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8394 (1.8396)
+2022-11-18 15:46:45,747:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7565 (1.8321)
+2022-11-18 15:46:45,904:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8166 (1.8308)
+2022-11-18 15:46:46,063:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7121 (1.8213)
+2022-11-18 15:46:46,218:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8591 (1.8238)
+2022-11-18 15:46:46,373:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8982 (1.8283)
+2022-11-18 15:46:46,543:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8093 (1.8271)
+2022-11-18 15:46:46,723:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8411 (1.8279)
+2022-11-18 15:46:46,872:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8012 (1.8266)
+2022-11-18 15:46:46,920:INFO: - Computing loss (validation)
+2022-11-18 15:46:47,182:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8525 (1.8525)
+2022-11-18 15:46:47,217:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8249 (1.8505)
+2022-11-18 15:46:47,531:INFO: Dataset: univ                Batch: 1/3	Loss 1.8560 (1.8560)
+2022-11-18 15:46:47,608:INFO: Dataset: univ                Batch: 2/3	Loss 1.8718 (1.8634)
+2022-11-18 15:46:47,683:INFO: Dataset: univ                Batch: 3/3	Loss 1.8724 (1.8663)
+2022-11-18 15:46:48,005:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7993 (1.7993)
+2022-11-18 15:46:48,055:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0490 (1.8668)
+2022-11-18 15:46:48,369:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9109 (1.9109)
+2022-11-18 15:46:48,446:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8022 (1.8535)
+2022-11-18 15:46:48,525:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7758 (1.8268)
+2022-11-18 15:46:48,603:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8330 (1.8283)
+2022-11-18 15:46:48,680:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9028 (1.8442)
+2022-11-18 15:46:48,732:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_541.pth.tar
+2022-11-18 15:46:48,732:INFO: 
+===> EPOCH: 542 (P2)
+2022-11-18 15:46:48,733:INFO: - Computing loss (training)
+2022-11-18 15:46:49,081:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7730 (1.7730)
+2022-11-18 15:46:49,244:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9201 (1.8474)
+2022-11-18 15:46:49,400:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8139 (1.8365)
+2022-11-18 15:46:49,520:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8419 (1.8375)
+2022-11-18 15:46:49,941:INFO: Dataset: univ                Batch:  1/15	Loss 1.7939 (1.7939)
+2022-11-18 15:46:50,100:INFO: Dataset: univ                Batch:  2/15	Loss 1.8504 (1.8223)
+2022-11-18 15:46:50,259:INFO: Dataset: univ                Batch:  3/15	Loss 1.9198 (1.8535)
+2022-11-18 15:46:50,410:INFO: Dataset: univ                Batch:  4/15	Loss 1.8600 (1.8552)
+2022-11-18 15:46:50,563:INFO: Dataset: univ                Batch:  5/15	Loss 1.8470 (1.8535)
+2022-11-18 15:46:50,719:INFO: Dataset: univ                Batch:  6/15	Loss 1.8500 (1.8529)
+2022-11-18 15:46:50,874:INFO: Dataset: univ                Batch:  7/15	Loss 1.8560 (1.8534)
+2022-11-18 15:46:51,030:INFO: Dataset: univ                Batch:  8/15	Loss 1.8746 (1.8560)
+2022-11-18 15:46:51,186:INFO: Dataset: univ                Batch:  9/15	Loss 1.8296 (1.8530)
+2022-11-18 15:46:51,341:INFO: Dataset: univ                Batch: 10/15	Loss 1.8320 (1.8510)
+2022-11-18 15:46:51,497:INFO: Dataset: univ                Batch: 11/15	Loss 1.8648 (1.8524)
+2022-11-18 15:46:51,652:INFO: Dataset: univ                Batch: 12/15	Loss 1.8783 (1.8544)
+2022-11-18 15:46:51,809:INFO: Dataset: univ                Batch: 13/15	Loss 1.8776 (1.8561)
+2022-11-18 15:46:51,965:INFO: Dataset: univ                Batch: 14/15	Loss 1.8298 (1.8540)
+2022-11-18 15:46:52,046:INFO: Dataset: univ                Batch: 15/15	Loss 1.9208 (1.8548)
+2022-11-18 15:46:52,441:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8624 (1.8624)
+2022-11-18 15:46:52,591:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7502 (1.8105)
+2022-11-18 15:46:52,740:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8540 (1.8247)
+2022-11-18 15:46:52,888:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8116 (1.8216)
+2022-11-18 15:46:53,050:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8872 (1.8343)
+2022-11-18 15:46:53,239:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9576 (1.8544)
+2022-11-18 15:46:53,434:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8418 (1.8525)
+2022-11-18 15:46:53,607:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8595 (1.8533)
+2022-11-18 15:46:54,068:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9469 (1.9469)
+2022-11-18 15:46:54,261:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7826 (1.8636)
+2022-11-18 15:46:54,441:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8578 (1.8617)
+2022-11-18 15:46:54,628:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7359 (1.8283)
+2022-11-18 15:46:54,817:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9012 (1.8435)
+2022-11-18 15:46:55,012:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8679 (1.8474)
+2022-11-18 15:46:55,202:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8517 (1.8480)
+2022-11-18 15:46:55,391:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9154 (1.8573)
+2022-11-18 15:46:55,580:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7454 (1.8441)
+2022-11-18 15:46:55,769:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7022 (1.8306)
+2022-11-18 15:46:55,939:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7874 (1.8269)
+2022-11-18 15:46:56,115:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9385 (1.8359)
+2022-11-18 15:46:56,284:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8377 (1.8360)
+2022-11-18 15:46:56,442:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9221 (1.8424)
+2022-11-18 15:46:56,596:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8589 (1.8434)
+2022-11-18 15:46:56,750:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8275 (1.8423)
+2022-11-18 15:46:56,906:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8065 (1.8401)
+2022-11-18 15:46:57,051:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7335 (1.8347)
+2022-11-18 15:46:57,097:INFO: - Computing loss (validation)
+2022-11-18 15:46:57,367:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9021 (1.9021)
+2022-11-18 15:46:57,402:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9698 (1.9074)
+2022-11-18 15:46:57,719:INFO: Dataset: univ                Batch: 1/3	Loss 1.8417 (1.8417)
+2022-11-18 15:46:57,803:INFO: Dataset: univ                Batch: 2/3	Loss 1.8006 (1.8216)
+2022-11-18 15:46:57,884:INFO: Dataset: univ                Batch: 3/3	Loss 1.8954 (1.8470)
+2022-11-18 15:46:58,181:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9125 (1.9125)
+2022-11-18 15:46:58,226:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8394 (1.8953)
+2022-11-18 15:46:58,532:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8409 (1.8409)
+2022-11-18 15:46:58,608:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8078 (1.8238)
+2022-11-18 15:46:58,684:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8704 (1.8396)
+2022-11-18 15:46:58,758:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9602 (1.8722)
+2022-11-18 15:46:58,832:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8500 (1.8675)
+2022-11-18 15:46:58,883:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_542.pth.tar
+2022-11-18 15:46:58,883:INFO: 
+===> EPOCH: 543 (P2)
+2022-11-18 15:46:58,884:INFO: - Computing loss (training)
+2022-11-18 15:46:59,231:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8933 (1.8933)
+2022-11-18 15:46:59,384:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9471 (1.9211)
+2022-11-18 15:46:59,535:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8993 (1.9138)
+2022-11-18 15:46:59,650:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8611 (1.9050)
+2022-11-18 15:47:00,062:INFO: Dataset: univ                Batch:  1/15	Loss 1.8671 (1.8671)
+2022-11-18 15:47:00,223:INFO: Dataset: univ                Batch:  2/15	Loss 1.7921 (1.8309)
+2022-11-18 15:47:00,388:INFO: Dataset: univ                Batch:  3/15	Loss 1.7967 (1.8192)
+2022-11-18 15:47:00,550:INFO: Dataset: univ                Batch:  4/15	Loss 1.8344 (1.8227)
+2022-11-18 15:47:00,708:INFO: Dataset: univ                Batch:  5/15	Loss 1.8863 (1.8339)
+2022-11-18 15:47:00,871:INFO: Dataset: univ                Batch:  6/15	Loss 1.8196 (1.8315)
+2022-11-18 15:47:01,029:INFO: Dataset: univ                Batch:  7/15	Loss 1.8573 (1.8354)
+2022-11-18 15:47:01,184:INFO: Dataset: univ                Batch:  8/15	Loss 1.8788 (1.8404)
+2022-11-18 15:47:01,340:INFO: Dataset: univ                Batch:  9/15	Loss 1.8878 (1.8459)
+2022-11-18 15:47:01,495:INFO: Dataset: univ                Batch: 10/15	Loss 1.8713 (1.8486)
+2022-11-18 15:47:01,650:INFO: Dataset: univ                Batch: 11/15	Loss 1.8363 (1.8474)
+2022-11-18 15:47:01,806:INFO: Dataset: univ                Batch: 12/15	Loss 1.8498 (1.8477)
+2022-11-18 15:47:01,962:INFO: Dataset: univ                Batch: 13/15	Loss 1.8390 (1.8470)
+2022-11-18 15:47:02,119:INFO: Dataset: univ                Batch: 14/15	Loss 1.8673 (1.8484)
+2022-11-18 15:47:02,201:INFO: Dataset: univ                Batch: 15/15	Loss 1.8439 (1.8483)
+2022-11-18 15:47:02,598:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8059 (1.8059)
+2022-11-18 15:47:02,750:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8859 (1.8424)
+2022-11-18 15:47:02,901:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7924 (1.8253)
+2022-11-18 15:47:03,056:INFO: Dataset: zara1               Batch: 4/8	Loss 2.0132 (1.8684)
+2022-11-18 15:47:03,206:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9385 (1.8816)
+2022-11-18 15:47:03,357:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8386 (1.8739)
+2022-11-18 15:47:03,510:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8553 (1.8709)
+2022-11-18 15:47:03,648:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9166 (1.8758)
+2022-11-18 15:47:04,072:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9059 (1.9059)
+2022-11-18 15:47:04,236:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8912 (1.8985)
+2022-11-18 15:47:04,395:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8522 (1.8839)
+2022-11-18 15:47:04,545:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8662 (1.8799)
+2022-11-18 15:47:04,695:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8904 (1.8822)
+2022-11-18 15:47:04,851:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8283 (1.8737)
+2022-11-18 15:47:05,005:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8614 (1.8719)
+2022-11-18 15:47:05,159:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8384 (1.8677)
+2022-11-18 15:47:05,309:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8204 (1.8625)
+2022-11-18 15:47:05,458:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8786 (1.8642)
+2022-11-18 15:47:05,610:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8422 (1.8621)
+2022-11-18 15:47:05,759:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8693 (1.8626)
+2022-11-18 15:47:05,907:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8768 (1.8637)
+2022-11-18 15:47:06,056:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8585 (1.8634)
+2022-11-18 15:47:06,207:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7620 (1.8565)
+2022-11-18 15:47:06,360:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7867 (1.8520)
+2022-11-18 15:47:06,513:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7676 (1.8476)
+2022-11-18 15:47:06,652:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8952 (1.8501)
+2022-11-18 15:47:06,696:INFO: - Computing loss (validation)
+2022-11-18 15:47:06,966:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8830 (1.8830)
+2022-11-18 15:47:07,002:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5879 (1.8598)
+2022-11-18 15:47:07,329:INFO: Dataset: univ                Batch: 1/3	Loss 1.8319 (1.8319)
+2022-11-18 15:47:07,415:INFO: Dataset: univ                Batch: 2/3	Loss 1.8659 (1.8492)
+2022-11-18 15:47:07,493:INFO: Dataset: univ                Batch: 3/3	Loss 1.8022 (1.8379)
+2022-11-18 15:47:07,832:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9385 (1.9385)
+2022-11-18 15:47:07,882:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6999 (1.8771)
+2022-11-18 15:47:08,196:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8631 (1.8631)
+2022-11-18 15:47:08,271:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8649 (1.8640)
+2022-11-18 15:47:08,345:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8219 (1.8498)
+2022-11-18 15:47:08,419:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8238 (1.8434)
+2022-11-18 15:47:08,492:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8868 (1.8521)
+2022-11-18 15:47:08,544:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_543.pth.tar
+2022-11-18 15:47:08,544:INFO: 
+===> EPOCH: 544 (P2)
+2022-11-18 15:47:08,545:INFO: - Computing loss (training)
+2022-11-18 15:47:08,883:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8129 (1.8129)
+2022-11-18 15:47:09,038:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8844 (1.8475)
+2022-11-18 15:47:09,189:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8848 (1.8600)
+2022-11-18 15:47:09,308:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8827 (1.8640)
+2022-11-18 15:47:09,716:INFO: Dataset: univ                Batch:  1/15	Loss 1.8559 (1.8559)
+2022-11-18 15:47:09,882:INFO: Dataset: univ                Batch:  2/15	Loss 1.8278 (1.8413)
+2022-11-18 15:47:10,049:INFO: Dataset: univ                Batch:  3/15	Loss 1.8198 (1.8350)
+2022-11-18 15:47:10,214:INFO: Dataset: univ                Batch:  4/15	Loss 1.8517 (1.8389)
+2022-11-18 15:47:10,375:INFO: Dataset: univ                Batch:  5/15	Loss 1.8334 (1.8379)
+2022-11-18 15:47:10,540:INFO: Dataset: univ                Batch:  6/15	Loss 1.8626 (1.8417)
+2022-11-18 15:47:10,728:INFO: Dataset: univ                Batch:  7/15	Loss 1.8197 (1.8384)
+2022-11-18 15:47:10,906:INFO: Dataset: univ                Batch:  8/15	Loss 1.8651 (1.8412)
+2022-11-18 15:47:11,073:INFO: Dataset: univ                Batch:  9/15	Loss 1.8232 (1.8392)
+2022-11-18 15:47:11,238:INFO: Dataset: univ                Batch: 10/15	Loss 1.8681 (1.8419)
+2022-11-18 15:47:11,401:INFO: Dataset: univ                Batch: 11/15	Loss 1.8932 (1.8473)
+2022-11-18 15:47:11,562:INFO: Dataset: univ                Batch: 12/15	Loss 1.8630 (1.8487)
+2022-11-18 15:47:11,720:INFO: Dataset: univ                Batch: 13/15	Loss 1.8077 (1.8455)
+2022-11-18 15:47:11,879:INFO: Dataset: univ                Batch: 14/15	Loss 1.8868 (1.8485)
+2022-11-18 15:47:11,963:INFO: Dataset: univ                Batch: 15/15	Loss 1.9052 (1.8494)
+2022-11-18 15:47:12,354:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8608 (1.8608)
+2022-11-18 15:47:12,500:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8867 (1.8740)
+2022-11-18 15:47:12,647:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7781 (1.8433)
+2022-11-18 15:47:12,793:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9051 (1.8590)
+2022-11-18 15:47:12,945:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8550 (1.8582)
+2022-11-18 15:47:13,098:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9259 (1.8689)
+2022-11-18 15:47:13,248:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8598 (1.8675)
+2022-11-18 15:47:13,383:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9708 (1.8811)
+2022-11-18 15:47:13,786:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9047 (1.9047)
+2022-11-18 15:47:13,943:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8709 (1.8883)
+2022-11-18 15:47:14,101:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7658 (1.8477)
+2022-11-18 15:47:14,253:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8170 (1.8401)
+2022-11-18 15:47:14,409:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8032 (1.8328)
+2022-11-18 15:47:14,560:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8288 (1.8322)
+2022-11-18 15:47:14,708:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8271 (1.8315)
+2022-11-18 15:47:14,858:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8162 (1.8296)
+2022-11-18 15:47:15,006:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8218 (1.8287)
+2022-11-18 15:47:15,154:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8876 (1.8344)
+2022-11-18 15:47:15,304:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7915 (1.8308)
+2022-11-18 15:47:15,452:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8056 (1.8289)
+2022-11-18 15:47:15,599:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7027 (1.8185)
+2022-11-18 15:47:15,747:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8537 (1.8212)
+2022-11-18 15:47:15,897:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8106 (1.8205)
+2022-11-18 15:47:16,046:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8874 (1.8245)
+2022-11-18 15:47:16,196:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8241 (1.8245)
+2022-11-18 15:47:16,333:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8721 (1.8268)
+2022-11-18 15:47:16,380:INFO: - Computing loss (validation)
+2022-11-18 15:47:16,656:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8307 (1.8307)
+2022-11-18 15:47:16,690:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9785 (1.8418)
+2022-11-18 15:47:17,009:INFO: Dataset: univ                Batch: 1/3	Loss 1.8207 (1.8207)
+2022-11-18 15:47:17,094:INFO: Dataset: univ                Batch: 2/3	Loss 1.8271 (1.8240)
+2022-11-18 15:47:17,170:INFO: Dataset: univ                Batch: 3/3	Loss 1.8037 (1.8181)
+2022-11-18 15:47:17,474:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8472 (1.8472)
+2022-11-18 15:47:17,523:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9706 (1.8801)
+2022-11-18 15:47:17,862:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8359 (1.8359)
+2022-11-18 15:47:17,940:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8422 (1.8388)
+2022-11-18 15:47:18,018:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8390 (1.8389)
+2022-11-18 15:47:18,095:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8972 (1.8538)
+2022-11-18 15:47:18,171:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8649 (1.8560)
+2022-11-18 15:47:18,226:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_544.pth.tar
+2022-11-18 15:47:18,226:INFO: 
+===> EPOCH: 545 (P2)
+2022-11-18 15:47:18,227:INFO: - Computing loss (training)
+2022-11-18 15:47:18,587:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9043 (1.9043)
+2022-11-18 15:47:18,750:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8561 (1.8806)
+2022-11-18 15:47:18,905:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8327 (1.8654)
+2022-11-18 15:47:19,029:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9725 (1.8827)
+2022-11-18 15:47:19,457:INFO: Dataset: univ                Batch:  1/15	Loss 1.8414 (1.8414)
+2022-11-18 15:47:19,626:INFO: Dataset: univ                Batch:  2/15	Loss 1.9099 (1.8773)
+2022-11-18 15:47:19,792:INFO: Dataset: univ                Batch:  3/15	Loss 1.8648 (1.8733)
+2022-11-18 15:47:19,951:INFO: Dataset: univ                Batch:  4/15	Loss 1.8511 (1.8678)
+2022-11-18 15:47:20,113:INFO: Dataset: univ                Batch:  5/15	Loss 1.8601 (1.8663)
+2022-11-18 15:47:20,288:INFO: Dataset: univ                Batch:  6/15	Loss 1.8483 (1.8630)
+2022-11-18 15:47:20,451:INFO: Dataset: univ                Batch:  7/15	Loss 1.8916 (1.8670)
+2022-11-18 15:47:20,611:INFO: Dataset: univ                Batch:  8/15	Loss 1.8051 (1.8593)
+2022-11-18 15:47:20,775:INFO: Dataset: univ                Batch:  9/15	Loss 1.8581 (1.8592)
+2022-11-18 15:47:20,936:INFO: Dataset: univ                Batch: 10/15	Loss 1.8540 (1.8587)
+2022-11-18 15:47:21,095:INFO: Dataset: univ                Batch: 11/15	Loss 1.8596 (1.8588)
+2022-11-18 15:47:21,257:INFO: Dataset: univ                Batch: 12/15	Loss 1.8744 (1.8600)
+2022-11-18 15:47:21,434:INFO: Dataset: univ                Batch: 13/15	Loss 1.8569 (1.8598)
+2022-11-18 15:47:21,617:INFO: Dataset: univ                Batch: 14/15	Loss 1.8432 (1.8586)
+2022-11-18 15:47:21,715:INFO: Dataset: univ                Batch: 15/15	Loss 1.9455 (1.8597)
+2022-11-18 15:47:22,157:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9789 (1.9789)
+2022-11-18 15:47:22,317:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9156 (1.9473)
+2022-11-18 15:47:22,473:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7591 (1.8840)
+2022-11-18 15:47:22,645:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8407 (1.8732)
+2022-11-18 15:47:22,809:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8855 (1.8755)
+2022-11-18 15:47:22,975:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8278 (1.8681)
+2022-11-18 15:47:23,145:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9578 (1.8806)
+2022-11-18 15:47:23,290:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9190 (1.8849)
+2022-11-18 15:47:23,696:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8056 (1.8056)
+2022-11-18 15:47:23,852:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8807 (1.8455)
+2022-11-18 15:47:24,010:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9230 (1.8725)
+2022-11-18 15:47:24,164:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8965 (1.8785)
+2022-11-18 15:47:24,318:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7974 (1.8634)
+2022-11-18 15:47:24,475:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8324 (1.8582)
+2022-11-18 15:47:24,637:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8417 (1.8557)
+2022-11-18 15:47:24,796:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8089 (1.8501)
+2022-11-18 15:47:24,951:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9752 (1.8629)
+2022-11-18 15:47:25,110:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8866 (1.8651)
+2022-11-18 15:47:25,287:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8538 (1.8640)
+2022-11-18 15:47:25,467:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8526 (1.8630)
+2022-11-18 15:47:25,634:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8249 (1.8599)
+2022-11-18 15:47:25,793:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7822 (1.8547)
+2022-11-18 15:47:25,951:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7750 (1.8496)
+2022-11-18 15:47:26,111:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7893 (1.8463)
+2022-11-18 15:47:26,277:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8672 (1.8477)
+2022-11-18 15:47:26,423:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9600 (1.8537)
+2022-11-18 15:47:26,468:INFO: - Computing loss (validation)
+2022-11-18 15:47:26,754:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8213 (1.8213)
+2022-11-18 15:47:26,790:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8527 (1.8232)
+2022-11-18 15:47:27,099:INFO: Dataset: univ                Batch: 1/3	Loss 1.8911 (1.8911)
+2022-11-18 15:47:27,177:INFO: Dataset: univ                Batch: 2/3	Loss 1.8671 (1.8787)
+2022-11-18 15:47:27,253:INFO: Dataset: univ                Batch: 3/3	Loss 1.8632 (1.8735)
+2022-11-18 15:47:27,562:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8807 (1.8807)
+2022-11-18 15:47:27,608:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0388 (1.9224)
+2022-11-18 15:47:27,955:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9444 (1.9444)
+2022-11-18 15:47:28,036:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8482 (1.8977)
+2022-11-18 15:47:28,115:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8427 (1.8791)
+2022-11-18 15:47:28,193:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8288 (1.8666)
+2022-11-18 15:47:28,271:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8859 (1.8705)
+2022-11-18 15:47:28,340:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_545.pth.tar
+2022-11-18 15:47:28,340:INFO: 
+===> EPOCH: 546 (P2)
+2022-11-18 15:47:28,341:INFO: - Computing loss (training)
+2022-11-18 15:47:28,688:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9486 (1.9486)
+2022-11-18 15:47:28,838:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8005 (1.8730)
+2022-11-18 15:47:28,986:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8210 (1.8562)
+2022-11-18 15:47:29,099:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8886 (1.8619)
+2022-11-18 15:47:29,499:INFO: Dataset: univ                Batch:  1/15	Loss 1.8151 (1.8151)
+2022-11-18 15:47:29,660:INFO: Dataset: univ                Batch:  2/15	Loss 1.8588 (1.8369)
+2022-11-18 15:47:29,817:INFO: Dataset: univ                Batch:  3/15	Loss 1.8585 (1.8440)
+2022-11-18 15:47:29,971:INFO: Dataset: univ                Batch:  4/15	Loss 1.8383 (1.8427)
+2022-11-18 15:47:30,131:INFO: Dataset: univ                Batch:  5/15	Loss 1.8440 (1.8430)
+2022-11-18 15:47:30,293:INFO: Dataset: univ                Batch:  6/15	Loss 1.8286 (1.8405)
+2022-11-18 15:47:30,451:INFO: Dataset: univ                Batch:  7/15	Loss 1.8738 (1.8455)
+2022-11-18 15:47:30,605:INFO: Dataset: univ                Batch:  8/15	Loss 1.8464 (1.8456)
+2022-11-18 15:47:30,761:INFO: Dataset: univ                Batch:  9/15	Loss 1.8507 (1.8462)
+2022-11-18 15:47:30,916:INFO: Dataset: univ                Batch: 10/15	Loss 1.8337 (1.8450)
+2022-11-18 15:47:31,072:INFO: Dataset: univ                Batch: 11/15	Loss 1.8128 (1.8420)
+2022-11-18 15:47:31,228:INFO: Dataset: univ                Batch: 12/15	Loss 1.8622 (1.8438)
+2022-11-18 15:47:31,382:INFO: Dataset: univ                Batch: 13/15	Loss 1.8223 (1.8422)
+2022-11-18 15:47:31,537:INFO: Dataset: univ                Batch: 14/15	Loss 1.8847 (1.8451)
+2022-11-18 15:47:31,619:INFO: Dataset: univ                Batch: 15/15	Loss 1.8623 (1.8454)
+2022-11-18 15:47:32,019:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8074 (1.8074)
+2022-11-18 15:47:32,169:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8854 (1.8464)
+2022-11-18 15:47:32,316:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8574 (1.8503)
+2022-11-18 15:47:32,462:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8542 (1.8512)
+2022-11-18 15:47:32,691:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8999 (1.8610)
+2022-11-18 15:47:32,840:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8430 (1.8579)
+2022-11-18 15:47:32,989:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7952 (1.8489)
+2022-11-18 15:47:33,125:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9112 (1.8555)
+2022-11-18 15:47:33,522:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8075 (1.8075)
+2022-11-18 15:47:33,678:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7976 (1.8029)
+2022-11-18 15:47:33,839:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7911 (1.7991)
+2022-11-18 15:47:33,994:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9440 (1.8370)
+2022-11-18 15:47:34,155:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8591 (1.8418)
+2022-11-18 15:47:34,313:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8534 (1.8439)
+2022-11-18 15:47:34,469:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8313 (1.8421)
+2022-11-18 15:47:34,623:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7854 (1.8348)
+2022-11-18 15:47:34,777:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9324 (1.8461)
+2022-11-18 15:47:34,931:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8711 (1.8486)
+2022-11-18 15:47:35,087:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8722 (1.8507)
+2022-11-18 15:47:35,242:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9170 (1.8559)
+2022-11-18 15:47:35,393:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8775 (1.8577)
+2022-11-18 15:47:35,545:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8474 (1.8569)
+2022-11-18 15:47:35,700:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8027 (1.8535)
+2022-11-18 15:47:35,853:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8240 (1.8517)
+2022-11-18 15:47:36,011:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8119 (1.8495)
+2022-11-18 15:47:36,156:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8664 (1.8503)
+2022-11-18 15:47:36,203:INFO: - Computing loss (validation)
+2022-11-18 15:47:36,465:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8732 (1.8732)
+2022-11-18 15:47:36,500:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2271 (1.8937)
+2022-11-18 15:47:36,809:INFO: Dataset: univ                Batch: 1/3	Loss 1.8017 (1.8017)
+2022-11-18 15:47:36,887:INFO: Dataset: univ                Batch: 2/3	Loss 1.8580 (1.8297)
+2022-11-18 15:47:36,961:INFO: Dataset: univ                Batch: 3/3	Loss 1.8568 (1.8373)
+2022-11-18 15:47:37,265:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8687 (1.8687)
+2022-11-18 15:47:37,312:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7547 (1.8364)
+2022-11-18 15:47:37,621:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7962 (1.7962)
+2022-11-18 15:47:37,699:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8334 (1.8152)
+2022-11-18 15:47:37,776:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8786 (1.8354)
+2022-11-18 15:47:37,851:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8899 (1.8486)
+2022-11-18 15:47:37,924:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8835 (1.8554)
+2022-11-18 15:47:37,977:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_546.pth.tar
+2022-11-18 15:47:37,977:INFO: 
+===> EPOCH: 547 (P2)
+2022-11-18 15:47:37,977:INFO: - Computing loss (training)
+2022-11-18 15:47:38,320:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8509 (1.8509)
+2022-11-18 15:47:38,478:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7310 (1.7917)
+2022-11-18 15:47:38,633:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9471 (1.8429)
+2022-11-18 15:47:38,753:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0439 (1.8734)
+2022-11-18 15:47:39,160:INFO: Dataset: univ                Batch:  1/15	Loss 1.8370 (1.8370)
+2022-11-18 15:47:39,318:INFO: Dataset: univ                Batch:  2/15	Loss 1.8383 (1.8377)
+2022-11-18 15:47:39,475:INFO: Dataset: univ                Batch:  3/15	Loss 1.8611 (1.8449)
+2022-11-18 15:47:39,640:INFO: Dataset: univ                Batch:  4/15	Loss 1.8238 (1.8396)
+2022-11-18 15:47:39,805:INFO: Dataset: univ                Batch:  5/15	Loss 1.8444 (1.8406)
+2022-11-18 15:47:39,978:INFO: Dataset: univ                Batch:  6/15	Loss 1.8380 (1.8401)
+2022-11-18 15:47:40,140:INFO: Dataset: univ                Batch:  7/15	Loss 1.8529 (1.8420)
+2022-11-18 15:47:40,297:INFO: Dataset: univ                Batch:  8/15	Loss 1.8810 (1.8469)
+2022-11-18 15:47:40,454:INFO: Dataset: univ                Batch:  9/15	Loss 1.8703 (1.8495)
+2022-11-18 15:47:40,613:INFO: Dataset: univ                Batch: 10/15	Loss 1.8315 (1.8475)
+2022-11-18 15:47:40,773:INFO: Dataset: univ                Batch: 11/15	Loss 1.8208 (1.8449)
+2022-11-18 15:47:40,932:INFO: Dataset: univ                Batch: 12/15	Loss 1.8406 (1.8445)
+2022-11-18 15:47:41,088:INFO: Dataset: univ                Batch: 13/15	Loss 1.8853 (1.8476)
+2022-11-18 15:47:41,247:INFO: Dataset: univ                Batch: 14/15	Loss 1.8631 (1.8487)
+2022-11-18 15:47:41,329:INFO: Dataset: univ                Batch: 15/15	Loss 1.8495 (1.8487)
+2022-11-18 15:47:41,725:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8275 (1.8275)
+2022-11-18 15:47:41,884:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8404 (1.8342)
+2022-11-18 15:47:42,043:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9240 (1.8607)
+2022-11-18 15:47:42,203:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8691 (1.8630)
+2022-11-18 15:47:42,359:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9473 (1.8811)
+2022-11-18 15:47:42,516:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9661 (1.8956)
+2022-11-18 15:47:42,674:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9347 (1.9011)
+2022-11-18 15:47:42,818:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9110 (1.9022)
+2022-11-18 15:47:43,245:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9519 (1.9519)
+2022-11-18 15:47:43,401:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9115 (1.9312)
+2022-11-18 15:47:43,559:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8687 (1.9114)
+2022-11-18 15:47:43,710:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8258 (1.8914)
+2022-11-18 15:47:43,865:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8706 (1.8875)
+2022-11-18 15:47:44,021:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8408 (1.8793)
+2022-11-18 15:47:44,175:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8443 (1.8743)
+2022-11-18 15:47:44,325:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8268 (1.8684)
+2022-11-18 15:47:44,475:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8015 (1.8611)
+2022-11-18 15:47:44,628:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9742 (1.8723)
+2022-11-18 15:47:44,782:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8883 (1.8739)
+2022-11-18 15:47:44,936:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7404 (1.8632)
+2022-11-18 15:47:45,087:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9420 (1.8692)
+2022-11-18 15:47:45,238:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7838 (1.8629)
+2022-11-18 15:47:45,391:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8653 (1.8631)
+2022-11-18 15:47:45,546:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8743 (1.8638)
+2022-11-18 15:47:45,700:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8731 (1.8644)
+2022-11-18 15:47:45,841:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9341 (1.8674)
+2022-11-18 15:47:45,886:INFO: - Computing loss (validation)
+2022-11-18 15:47:46,163:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9059 (1.9059)
+2022-11-18 15:47:46,201:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0798 (1.9189)
+2022-11-18 15:47:46,503:INFO: Dataset: univ                Batch: 1/3	Loss 1.8386 (1.8386)
+2022-11-18 15:47:46,582:INFO: Dataset: univ                Batch: 2/3	Loss 1.8191 (1.8293)
+2022-11-18 15:47:46,653:INFO: Dataset: univ                Batch: 3/3	Loss 1.8524 (1.8363)
+2022-11-18 15:47:46,958:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9035 (1.9035)
+2022-11-18 15:47:47,005:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8592 (1.8938)
+2022-11-18 15:47:47,319:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8616 (1.8616)
+2022-11-18 15:47:47,393:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9009 (1.8809)
+2022-11-18 15:47:47,470:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8354 (1.8659)
+2022-11-18 15:47:47,544:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7910 (1.8474)
+2022-11-18 15:47:47,630:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8615 (1.8501)
+2022-11-18 15:47:47,702:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_547.pth.tar
+2022-11-18 15:47:47,702:INFO: 
+===> EPOCH: 548 (P2)
+2022-11-18 15:47:47,702:INFO: - Computing loss (training)
+2022-11-18 15:47:48,077:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0243 (2.0243)
+2022-11-18 15:47:48,241:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8587 (1.9431)
+2022-11-18 15:47:48,400:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8548 (1.9131)
+2022-11-18 15:47:48,517:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9407 (1.9177)
+2022-11-18 15:47:48,933:INFO: Dataset: univ                Batch:  1/15	Loss 1.8238 (1.8238)
+2022-11-18 15:47:49,108:INFO: Dataset: univ                Batch:  2/15	Loss 1.8257 (1.8248)
+2022-11-18 15:47:49,289:INFO: Dataset: univ                Batch:  3/15	Loss 1.8701 (1.8379)
+2022-11-18 15:47:49,474:INFO: Dataset: univ                Batch:  4/15	Loss 1.8695 (1.8458)
+2022-11-18 15:47:49,652:INFO: Dataset: univ                Batch:  5/15	Loss 1.8604 (1.8486)
+2022-11-18 15:47:49,823:INFO: Dataset: univ                Batch:  6/15	Loss 1.8485 (1.8486)
+2022-11-18 15:47:49,985:INFO: Dataset: univ                Batch:  7/15	Loss 1.8993 (1.8554)
+2022-11-18 15:47:50,153:INFO: Dataset: univ                Batch:  8/15	Loss 1.8520 (1.8549)
+2022-11-18 15:47:50,317:INFO: Dataset: univ                Batch:  9/15	Loss 1.9163 (1.8618)
+2022-11-18 15:47:50,482:INFO: Dataset: univ                Batch: 10/15	Loss 1.8615 (1.8618)
+2022-11-18 15:47:50,660:INFO: Dataset: univ                Batch: 11/15	Loss 1.8383 (1.8596)
+2022-11-18 15:47:50,833:INFO: Dataset: univ                Batch: 12/15	Loss 1.9042 (1.8633)
+2022-11-18 15:47:51,006:INFO: Dataset: univ                Batch: 13/15	Loss 1.8524 (1.8625)
+2022-11-18 15:47:51,185:INFO: Dataset: univ                Batch: 14/15	Loss 1.8572 (1.8621)
+2022-11-18 15:47:51,278:INFO: Dataset: univ                Batch: 15/15	Loss 1.8644 (1.8622)
+2022-11-18 15:47:51,697:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8794 (1.8794)
+2022-11-18 15:47:51,867:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8445 (1.8623)
+2022-11-18 15:47:52,039:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7777 (1.8343)
+2022-11-18 15:47:52,207:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8732 (1.8437)
+2022-11-18 15:47:52,375:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8701 (1.8492)
+2022-11-18 15:47:52,543:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8037 (1.8419)
+2022-11-18 15:47:52,710:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8414 (1.8418)
+2022-11-18 15:47:52,874:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8085 (1.8381)
+2022-11-18 15:47:53,290:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8904 (1.8904)
+2022-11-18 15:47:53,456:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8673 (1.8791)
+2022-11-18 15:47:53,648:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7565 (1.8384)
+2022-11-18 15:47:53,831:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8293 (1.8362)
+2022-11-18 15:47:54,008:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8650 (1.8416)
+2022-11-18 15:47:54,215:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8260 (1.8388)
+2022-11-18 15:47:54,391:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9897 (1.8605)
+2022-11-18 15:47:54,567:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9609 (1.8722)
+2022-11-18 15:47:54,737:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8185 (1.8662)
+2022-11-18 15:47:54,908:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8429 (1.8639)
+2022-11-18 15:47:55,079:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8279 (1.8604)
+2022-11-18 15:47:55,245:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8375 (1.8585)
+2022-11-18 15:47:55,428:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8856 (1.8605)
+2022-11-18 15:47:55,605:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7890 (1.8555)
+2022-11-18 15:47:55,780:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8845 (1.8574)
+2022-11-18 15:47:55,953:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8923 (1.8595)
+2022-11-18 15:47:56,127:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8992 (1.8618)
+2022-11-18 15:47:56,290:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8236 (1.8597)
+2022-11-18 15:47:56,355:INFO: - Computing loss (validation)
+2022-11-18 15:47:56,649:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7772 (1.7772)
+2022-11-18 15:47:56,689:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1435 (1.8072)
+2022-11-18 15:47:57,039:INFO: Dataset: univ                Batch: 1/3	Loss 1.8156 (1.8156)
+2022-11-18 15:47:57,123:INFO: Dataset: univ                Batch: 2/3	Loss 1.8365 (1.8254)
+2022-11-18 15:47:57,200:INFO: Dataset: univ                Batch: 3/3	Loss 1.8864 (1.8411)
+2022-11-18 15:47:57,539:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8618 (1.8618)
+2022-11-18 15:47:57,584:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9415 (1.8820)
+2022-11-18 15:47:57,897:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9060 (1.9060)
+2022-11-18 15:47:57,975:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8333 (1.8714)
+2022-11-18 15:47:58,052:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8381 (1.8608)
+2022-11-18 15:47:58,128:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8511 (1.8584)
+2022-11-18 15:47:58,203:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8535 (1.8575)
+2022-11-18 15:47:58,256:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_548.pth.tar
+2022-11-18 15:47:58,257:INFO: 
+===> EPOCH: 549 (P2)
+2022-11-18 15:47:58,257:INFO: - Computing loss (training)
+2022-11-18 15:47:58,635:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9265 (1.9265)
+2022-11-18 15:47:58,801:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7992 (1.8633)
+2022-11-18 15:47:58,970:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8748 (1.8670)
+2022-11-18 15:47:59,101:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9663 (1.8836)
+2022-11-18 15:47:59,499:INFO: Dataset: univ                Batch:  1/15	Loss 1.8126 (1.8126)
+2022-11-18 15:47:59,665:INFO: Dataset: univ                Batch:  2/15	Loss 1.9063 (1.8591)
+2022-11-18 15:47:59,845:INFO: Dataset: univ                Batch:  3/15	Loss 1.8611 (1.8598)
+2022-11-18 15:48:00,023:INFO: Dataset: univ                Batch:  4/15	Loss 1.8237 (1.8508)
+2022-11-18 15:48:00,184:INFO: Dataset: univ                Batch:  5/15	Loss 1.8739 (1.8559)
+2022-11-18 15:48:00,342:INFO: Dataset: univ                Batch:  6/15	Loss 1.9008 (1.8637)
+2022-11-18 15:48:00,499:INFO: Dataset: univ                Batch:  7/15	Loss 1.8595 (1.8630)
+2022-11-18 15:48:00,654:INFO: Dataset: univ                Batch:  8/15	Loss 1.8406 (1.8602)
+2022-11-18 15:48:00,826:INFO: Dataset: univ                Batch:  9/15	Loss 1.8911 (1.8645)
+2022-11-18 15:48:00,983:INFO: Dataset: univ                Batch: 10/15	Loss 1.8583 (1.8638)
+2022-11-18 15:48:01,142:INFO: Dataset: univ                Batch: 11/15	Loss 1.8069 (1.8586)
+2022-11-18 15:48:01,295:INFO: Dataset: univ                Batch: 12/15	Loss 1.8315 (1.8563)
+2022-11-18 15:48:01,452:INFO: Dataset: univ                Batch: 13/15	Loss 1.8842 (1.8585)
+2022-11-18 15:48:01,607:INFO: Dataset: univ                Batch: 14/15	Loss 1.8587 (1.8585)
+2022-11-18 15:48:01,689:INFO: Dataset: univ                Batch: 15/15	Loss 1.8949 (1.8591)
+2022-11-18 15:48:02,121:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9542 (1.9542)
+2022-11-18 15:48:02,287:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8131 (1.8852)
+2022-11-18 15:48:02,442:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9140 (1.8948)
+2022-11-18 15:48:02,601:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9276 (1.9027)
+2022-11-18 15:48:02,757:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9484 (1.9109)
+2022-11-18 15:48:02,916:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8823 (1.9061)
+2022-11-18 15:48:03,066:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8684 (1.9010)
+2022-11-18 15:48:03,203:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8751 (1.8986)
+2022-11-18 15:48:03,611:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8229 (1.8229)
+2022-11-18 15:48:03,786:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7814 (1.8013)
+2022-11-18 15:48:03,952:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7680 (1.7889)
+2022-11-18 15:48:04,104:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8218 (1.7968)
+2022-11-18 15:48:04,255:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7947 (1.7964)
+2022-11-18 15:48:04,408:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7890 (1.7950)
+2022-11-18 15:48:04,558:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9114 (1.8099)
+2022-11-18 15:48:04,707:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8271 (1.8120)
+2022-11-18 15:48:04,877:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8474 (1.8160)
+2022-11-18 15:48:05,046:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8426 (1.8186)
+2022-11-18 15:48:05,210:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8790 (1.8242)
+2022-11-18 15:48:05,361:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8671 (1.8276)
+2022-11-18 15:48:05,514:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8289 (1.8277)
+2022-11-18 15:48:05,668:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8336 (1.8282)
+2022-11-18 15:48:05,826:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8750 (1.8315)
+2022-11-18 15:48:05,984:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7772 (1.8282)
+2022-11-18 15:48:06,139:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8345 (1.8286)
+2022-11-18 15:48:06,279:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8229 (1.8283)
+2022-11-18 15:48:06,324:INFO: - Computing loss (validation)
+2022-11-18 15:48:06,599:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7962 (1.7962)
+2022-11-18 15:48:06,634:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8858 (1.8017)
+2022-11-18 15:48:06,951:INFO: Dataset: univ                Batch: 1/3	Loss 1.8981 (1.8981)
+2022-11-18 15:48:07,029:INFO: Dataset: univ                Batch: 2/3	Loss 1.8508 (1.8754)
+2022-11-18 15:48:07,103:INFO: Dataset: univ                Batch: 3/3	Loss 1.8788 (1.8763)
+2022-11-18 15:48:07,462:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9376 (1.9376)
+2022-11-18 15:48:07,526:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8978 (1.9283)
+2022-11-18 15:48:07,910:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7960 (1.7960)
+2022-11-18 15:48:08,001:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8567 (1.8270)
+2022-11-18 15:48:08,093:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9096 (1.8566)
+2022-11-18 15:48:08,183:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7953 (1.8406)
+2022-11-18 15:48:08,271:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8458 (1.8416)
+2022-11-18 15:48:08,335:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_549.pth.tar
+2022-11-18 15:48:08,336:INFO: 
+===> EPOCH: 550 (P2)
+2022-11-18 15:48:08,337:INFO: - Computing loss (training)
+2022-11-18 15:48:08,712:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9770 (1.9770)
+2022-11-18 15:48:08,888:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8722 (1.9229)
+2022-11-18 15:48:09,059:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8437 (1.8961)
+2022-11-18 15:48:09,192:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7978 (1.8801)
+2022-11-18 15:48:09,630:INFO: Dataset: univ                Batch:  1/15	Loss 1.8005 (1.8005)
+2022-11-18 15:48:09,811:INFO: Dataset: univ                Batch:  2/15	Loss 1.8937 (1.8455)
+2022-11-18 15:48:10,002:INFO: Dataset: univ                Batch:  3/15	Loss 1.8846 (1.8582)
+2022-11-18 15:48:10,175:INFO: Dataset: univ                Batch:  4/15	Loss 1.8242 (1.8494)
+2022-11-18 15:48:10,350:INFO: Dataset: univ                Batch:  5/15	Loss 1.8332 (1.8464)
+2022-11-18 15:48:10,527:INFO: Dataset: univ                Batch:  6/15	Loss 1.8734 (1.8513)
+2022-11-18 15:48:10,696:INFO: Dataset: univ                Batch:  7/15	Loss 1.7993 (1.8432)
+2022-11-18 15:48:10,865:INFO: Dataset: univ                Batch:  8/15	Loss 1.8379 (1.8425)
+2022-11-18 15:48:11,035:INFO: Dataset: univ                Batch:  9/15	Loss 1.8631 (1.8448)
+2022-11-18 15:48:11,201:INFO: Dataset: univ                Batch: 10/15	Loss 1.8445 (1.8448)
+2022-11-18 15:48:11,372:INFO: Dataset: univ                Batch: 11/15	Loss 1.8510 (1.8453)
+2022-11-18 15:48:11,542:INFO: Dataset: univ                Batch: 12/15	Loss 1.8756 (1.8478)
+2022-11-18 15:48:11,709:INFO: Dataset: univ                Batch: 13/15	Loss 1.8248 (1.8459)
+2022-11-18 15:48:11,877:INFO: Dataset: univ                Batch: 14/15	Loss 1.8676 (1.8476)
+2022-11-18 15:48:11,966:INFO: Dataset: univ                Batch: 15/15	Loss 1.9039 (1.8483)
+2022-11-18 15:48:12,362:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8455 (1.8455)
+2022-11-18 15:48:12,511:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9781 (1.9215)
+2022-11-18 15:48:12,662:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0147 (1.9562)
+2022-11-18 15:48:12,811:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8831 (1.9376)
+2022-11-18 15:48:12,960:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9170 (1.9337)
+2022-11-18 15:48:13,110:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7657 (1.9068)
+2022-11-18 15:48:13,259:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8101 (1.8922)
+2022-11-18 15:48:13,394:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9010 (1.8932)
+2022-11-18 15:48:13,807:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8017 (1.8017)
+2022-11-18 15:48:13,982:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8794 (1.8392)
+2022-11-18 15:48:14,149:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7384 (1.8036)
+2022-11-18 15:48:14,317:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9069 (1.8290)
+2022-11-18 15:48:14,474:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8144 (1.8260)
+2022-11-18 15:48:14,633:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8288 (1.8265)
+2022-11-18 15:48:14,789:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8554 (1.8299)
+2022-11-18 15:48:14,946:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8192 (1.8286)
+2022-11-18 15:48:15,102:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8656 (1.8327)
+2022-11-18 15:48:15,258:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8230 (1.8317)
+2022-11-18 15:48:15,415:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8909 (1.8371)
+2022-11-18 15:48:15,571:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8499 (1.8382)
+2022-11-18 15:48:15,731:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8178 (1.8367)
+2022-11-18 15:48:15,890:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8593 (1.8384)
+2022-11-18 15:48:16,049:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7974 (1.8357)
+2022-11-18 15:48:16,210:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8313 (1.8354)
+2022-11-18 15:48:16,372:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8347 (1.8354)
+2022-11-18 15:48:16,537:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7656 (1.8324)
+2022-11-18 15:48:16,582:INFO: - Computing loss (validation)
+2022-11-18 15:48:16,881:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7721 (1.7721)
+2022-11-18 15:48:16,920:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9532 (1.7838)
+2022-11-18 15:48:17,243:INFO: Dataset: univ                Batch: 1/3	Loss 1.8886 (1.8886)
+2022-11-18 15:48:17,321:INFO: Dataset: univ                Batch: 2/3	Loss 1.8972 (1.8930)
+2022-11-18 15:48:17,395:INFO: Dataset: univ                Batch: 3/3	Loss 1.8655 (1.8839)
+2022-11-18 15:48:17,701:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8618 (1.8618)
+2022-11-18 15:48:17,747:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7856 (1.8439)
+2022-11-18 15:48:18,062:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8550 (1.8550)
+2022-11-18 15:48:18,148:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8812 (1.8683)
+2022-11-18 15:48:18,239:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9077 (1.8813)
+2022-11-18 15:48:18,338:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7640 (1.8526)
+2022-11-18 15:48:18,430:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8615 (1.8544)
+2022-11-18 15:48:18,498:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_550.pth.tar
+2022-11-18 15:48:18,498:INFO: 
+===> EPOCH: 551 (P2)
+2022-11-18 15:48:18,499:INFO: - Computing loss (training)
+2022-11-18 15:48:18,902:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8090 (1.8090)
+2022-11-18 15:48:19,072:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8270 (1.8176)
+2022-11-18 15:48:19,234:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8172 (1.8175)
+2022-11-18 15:48:19,353:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8395 (1.8213)
+2022-11-18 15:48:19,753:INFO: Dataset: univ                Batch:  1/15	Loss 1.8721 (1.8721)
+2022-11-18 15:48:19,927:INFO: Dataset: univ                Batch:  2/15	Loss 1.8302 (1.8526)
+2022-11-18 15:48:20,105:INFO: Dataset: univ                Batch:  3/15	Loss 1.8449 (1.8500)
+2022-11-18 15:48:20,272:INFO: Dataset: univ                Batch:  4/15	Loss 1.8307 (1.8451)
+2022-11-18 15:48:20,434:INFO: Dataset: univ                Batch:  5/15	Loss 1.8712 (1.8502)
+2022-11-18 15:48:20,604:INFO: Dataset: univ                Batch:  6/15	Loss 1.8289 (1.8465)
+2022-11-18 15:48:20,771:INFO: Dataset: univ                Batch:  7/15	Loss 1.8880 (1.8529)
+2022-11-18 15:48:20,936:INFO: Dataset: univ                Batch:  8/15	Loss 1.8789 (1.8563)
+2022-11-18 15:48:21,112:INFO: Dataset: univ                Batch:  9/15	Loss 1.8917 (1.8600)
+2022-11-18 15:48:21,289:INFO: Dataset: univ                Batch: 10/15	Loss 1.8763 (1.8618)
+2022-11-18 15:48:21,456:INFO: Dataset: univ                Batch: 11/15	Loss 1.8220 (1.8582)
+2022-11-18 15:48:21,621:INFO: Dataset: univ                Batch: 12/15	Loss 1.8254 (1.8555)
+2022-11-18 15:48:21,782:INFO: Dataset: univ                Batch: 13/15	Loss 1.8446 (1.8547)
+2022-11-18 15:48:21,946:INFO: Dataset: univ                Batch: 14/15	Loss 1.7971 (1.8509)
+2022-11-18 15:48:22,034:INFO: Dataset: univ                Batch: 15/15	Loss 1.8302 (1.8507)
+2022-11-18 15:48:22,508:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8227 (1.8227)
+2022-11-18 15:48:22,694:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8732 (1.8493)
+2022-11-18 15:48:22,876:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7957 (1.8296)
+2022-11-18 15:48:23,061:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8800 (1.8419)
+2022-11-18 15:48:23,234:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8588 (1.8454)
+2022-11-18 15:48:23,392:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8341 (1.8436)
+2022-11-18 15:48:23,568:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9547 (1.8596)
+2022-11-18 15:48:23,727:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9015 (1.8641)
+2022-11-18 15:48:24,137:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8988 (1.8988)
+2022-11-18 15:48:24,289:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8260 (1.8610)
+2022-11-18 15:48:24,444:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7852 (1.8367)
+2022-11-18 15:48:24,605:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7963 (1.8268)
+2022-11-18 15:48:24,756:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8691 (1.8352)
+2022-11-18 15:48:24,920:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8461 (1.8369)
+2022-11-18 15:48:25,089:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8939 (1.8453)
+2022-11-18 15:48:25,257:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9041 (1.8528)
+2022-11-18 15:48:25,412:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8822 (1.8562)
+2022-11-18 15:48:25,562:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8114 (1.8521)
+2022-11-18 15:48:25,715:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7316 (1.8394)
+2022-11-18 15:48:25,865:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8406 (1.8395)
+2022-11-18 15:48:26,015:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8231 (1.8382)
+2022-11-18 15:48:26,167:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8133 (1.8365)
+2022-11-18 15:48:26,319:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9272 (1.8423)
+2022-11-18 15:48:26,470:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7457 (1.8363)
+2022-11-18 15:48:26,627:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8056 (1.8345)
+2022-11-18 15:48:26,772:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7706 (1.8314)
+2022-11-18 15:48:26,819:INFO: - Computing loss (validation)
+2022-11-18 15:48:27,092:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8159 (1.8159)
+2022-11-18 15:48:27,127:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7128 (1.8064)
+2022-11-18 15:48:27,451:INFO: Dataset: univ                Batch: 1/3	Loss 1.8658 (1.8658)
+2022-11-18 15:48:27,538:INFO: Dataset: univ                Batch: 2/3	Loss 1.8933 (1.8783)
+2022-11-18 15:48:27,618:INFO: Dataset: univ                Batch: 3/3	Loss 1.8342 (1.8649)
+2022-11-18 15:48:27,947:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9741 (1.9741)
+2022-11-18 15:48:28,003:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6457 (1.8949)
+2022-11-18 15:48:28,313:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8169 (1.8169)
+2022-11-18 15:48:28,390:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8110 (1.8140)
+2022-11-18 15:48:28,464:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9230 (1.8512)
+2022-11-18 15:48:28,540:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7544 (1.8273)
+2022-11-18 15:48:28,614:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8240 (1.8268)
+2022-11-18 15:48:28,666:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_551.pth.tar
+2022-11-18 15:48:28,666:INFO: 
+===> EPOCH: 552 (P2)
+2022-11-18 15:48:28,667:INFO: - Computing loss (training)
+2022-11-18 15:48:29,017:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0533 (2.0533)
+2022-11-18 15:48:29,182:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8682 (1.9657)
+2022-11-18 15:48:29,348:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8667 (1.9344)
+2022-11-18 15:48:29,480:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7787 (1.9087)
+2022-11-18 15:48:29,927:INFO: Dataset: univ                Batch:  1/15	Loss 1.8511 (1.8511)
+2022-11-18 15:48:30,107:INFO: Dataset: univ                Batch:  2/15	Loss 1.8234 (1.8366)
+2022-11-18 15:48:30,276:INFO: Dataset: univ                Batch:  3/15	Loss 1.8291 (1.8341)
+2022-11-18 15:48:30,452:INFO: Dataset: univ                Batch:  4/15	Loss 1.8453 (1.8370)
+2022-11-18 15:48:30,624:INFO: Dataset: univ                Batch:  5/15	Loss 1.8117 (1.8318)
+2022-11-18 15:48:30,789:INFO: Dataset: univ                Batch:  6/15	Loss 1.8230 (1.8303)
+2022-11-18 15:48:30,945:INFO: Dataset: univ                Batch:  7/15	Loss 1.8574 (1.8340)
+2022-11-18 15:48:31,103:INFO: Dataset: univ                Batch:  8/15	Loss 1.8668 (1.8384)
+2022-11-18 15:48:31,263:INFO: Dataset: univ                Batch:  9/15	Loss 1.8027 (1.8343)
+2022-11-18 15:48:31,437:INFO: Dataset: univ                Batch: 10/15	Loss 1.8854 (1.8396)
+2022-11-18 15:48:31,604:INFO: Dataset: univ                Batch: 11/15	Loss 1.8553 (1.8411)
+2022-11-18 15:48:31,759:INFO: Dataset: univ                Batch: 12/15	Loss 1.8423 (1.8411)
+2022-11-18 15:48:31,933:INFO: Dataset: univ                Batch: 13/15	Loss 1.8579 (1.8425)
+2022-11-18 15:48:32,106:INFO: Dataset: univ                Batch: 14/15	Loss 1.8632 (1.8440)
+2022-11-18 15:48:32,195:INFO: Dataset: univ                Batch: 15/15	Loss 1.8257 (1.8437)
+2022-11-18 15:48:32,628:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9011 (1.9011)
+2022-11-18 15:48:32,779:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8049 (1.8535)
+2022-11-18 15:48:32,934:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9298 (1.8759)
+2022-11-18 15:48:33,087:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8042 (1.8580)
+2022-11-18 15:48:33,240:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9593 (1.8787)
+2022-11-18 15:48:33,391:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8455 (1.8729)
+2022-11-18 15:48:33,543:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8066 (1.8631)
+2022-11-18 15:48:33,689:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9422 (1.8723)
+2022-11-18 15:48:34,088:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8568 (1.8568)
+2022-11-18 15:48:34,242:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9017 (1.8797)
+2022-11-18 15:48:34,396:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8134 (1.8594)
+2022-11-18 15:48:34,554:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9347 (1.8788)
+2022-11-18 15:48:34,706:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7814 (1.8592)
+2022-11-18 15:48:34,867:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8420 (1.8563)
+2022-11-18 15:48:35,037:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9169 (1.8661)
+2022-11-18 15:48:35,192:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8592 (1.8653)
+2022-11-18 15:48:35,349:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8957 (1.8686)
+2022-11-18 15:48:35,505:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9638 (1.8797)
+2022-11-18 15:48:35,665:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8579 (1.8777)
+2022-11-18 15:48:35,817:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9091 (1.8804)
+2022-11-18 15:48:35,977:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8478 (1.8776)
+2022-11-18 15:48:36,158:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8214 (1.8734)
+2022-11-18 15:48:36,337:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7554 (1.8650)
+2022-11-18 15:48:36,511:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8546 (1.8645)
+2022-11-18 15:48:36,675:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8129 (1.8615)
+2022-11-18 15:48:36,821:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8577 (1.8613)
+2022-11-18 15:48:36,876:INFO: - Computing loss (validation)
+2022-11-18 15:48:37,174:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8986 (1.8986)
+2022-11-18 15:48:37,218:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3086 (1.9308)
+2022-11-18 15:48:37,581:INFO: Dataset: univ                Batch: 1/3	Loss 1.9037 (1.9037)
+2022-11-18 15:48:37,667:INFO: Dataset: univ                Batch: 2/3	Loss 1.8096 (1.8557)
+2022-11-18 15:48:37,746:INFO: Dataset: univ                Batch: 3/3	Loss 1.8825 (1.8650)
+2022-11-18 15:48:38,065:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9423 (1.9423)
+2022-11-18 15:48:38,114:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7666 (1.9011)
+2022-11-18 15:48:38,449:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9127 (1.9127)
+2022-11-18 15:48:38,526:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8211 (1.8654)
+2022-11-18 15:48:38,603:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8357 (1.8557)
+2022-11-18 15:48:38,681:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7755 (1.8361)
+2022-11-18 15:48:38,758:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8196 (1.8328)
+2022-11-18 15:48:38,818:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_552.pth.tar
+2022-11-18 15:48:38,818:INFO: 
+===> EPOCH: 553 (P2)
+2022-11-18 15:48:38,818:INFO: - Computing loss (training)
+2022-11-18 15:48:39,163:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9486 (1.9486)
+2022-11-18 15:48:39,317:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8778 (1.9135)
+2022-11-18 15:48:39,467:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8660 (1.8982)
+2022-11-18 15:48:39,586:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8910 (1.8970)
+2022-11-18 15:48:39,980:INFO: Dataset: univ                Batch:  1/15	Loss 1.8413 (1.8413)
+2022-11-18 15:48:40,141:INFO: Dataset: univ                Batch:  2/15	Loss 1.9043 (1.8706)
+2022-11-18 15:48:40,295:INFO: Dataset: univ                Batch:  3/15	Loss 1.8340 (1.8582)
+2022-11-18 15:48:40,453:INFO: Dataset: univ                Batch:  4/15	Loss 1.8189 (1.8479)
+2022-11-18 15:48:40,608:INFO: Dataset: univ                Batch:  5/15	Loss 1.8546 (1.8492)
+2022-11-18 15:48:40,777:INFO: Dataset: univ                Batch:  6/15	Loss 1.8206 (1.8439)
+2022-11-18 15:48:40,949:INFO: Dataset: univ                Batch:  7/15	Loss 1.8505 (1.8449)
+2022-11-18 15:48:41,123:INFO: Dataset: univ                Batch:  8/15	Loss 1.8323 (1.8432)
+2022-11-18 15:48:41,286:INFO: Dataset: univ                Batch:  9/15	Loss 1.8375 (1.8426)
+2022-11-18 15:48:41,449:INFO: Dataset: univ                Batch: 10/15	Loss 1.8868 (1.8469)
+2022-11-18 15:48:41,606:INFO: Dataset: univ                Batch: 11/15	Loss 1.8832 (1.8503)
+2022-11-18 15:48:41,760:INFO: Dataset: univ                Batch: 12/15	Loss 1.8476 (1.8501)
+2022-11-18 15:48:41,915:INFO: Dataset: univ                Batch: 13/15	Loss 1.8641 (1.8511)
+2022-11-18 15:48:42,069:INFO: Dataset: univ                Batch: 14/15	Loss 1.9076 (1.8548)
+2022-11-18 15:48:42,157:INFO: Dataset: univ                Batch: 15/15	Loss 1.9150 (1.8556)
+2022-11-18 15:48:42,555:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8638 (1.8638)
+2022-11-18 15:48:42,709:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8805 (1.8719)
+2022-11-18 15:48:42,864:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9234 (1.8910)
+2022-11-18 15:48:43,021:INFO: Dataset: zara1               Batch: 4/8	Loss 1.6879 (1.8440)
+2022-11-18 15:48:43,175:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8862 (1.8523)
+2022-11-18 15:48:43,328:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8353 (1.8495)
+2022-11-18 15:48:43,480:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8345 (1.8471)
+2022-11-18 15:48:43,620:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9438 (1.8567)
+2022-11-18 15:48:44,010:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8912 (1.8912)
+2022-11-18 15:48:44,164:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8305 (1.8638)
+2022-11-18 15:48:44,318:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8703 (1.8658)
+2022-11-18 15:48:44,474:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8085 (1.8510)
+2022-11-18 15:48:44,629:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8971 (1.8597)
+2022-11-18 15:48:44,784:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7528 (1.8392)
+2022-11-18 15:48:44,938:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8144 (1.8353)
+2022-11-18 15:48:45,098:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8070 (1.8317)
+2022-11-18 15:48:45,250:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9200 (1.8411)
+2022-11-18 15:48:45,397:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8800 (1.8450)
+2022-11-18 15:48:45,548:INFO: Dataset: zara2               Batch: 11/18	Loss 1.6681 (1.8306)
+2022-11-18 15:48:45,696:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7102 (1.8205)
+2022-11-18 15:48:45,843:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8242 (1.8207)
+2022-11-18 15:48:45,991:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7701 (1.8169)
+2022-11-18 15:48:46,142:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9112 (1.8232)
+2022-11-18 15:48:46,301:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9071 (1.8286)
+2022-11-18 15:48:46,459:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7632 (1.8249)
+2022-11-18 15:48:46,605:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8040 (1.8239)
+2022-11-18 15:48:46,651:INFO: - Computing loss (validation)
+2022-11-18 15:48:46,917:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8147 (1.8147)
+2022-11-18 15:48:46,951:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9372 (1.8218)
+2022-11-18 15:48:47,252:INFO: Dataset: univ                Batch: 1/3	Loss 1.8896 (1.8896)
+2022-11-18 15:48:47,330:INFO: Dataset: univ                Batch: 2/3	Loss 1.8404 (1.8643)
+2022-11-18 15:48:47,403:INFO: Dataset: univ                Batch: 3/3	Loss 1.8711 (1.8665)
+2022-11-18 15:48:47,700:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8898 (1.8898)
+2022-11-18 15:48:47,744:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9064 (1.8943)
+2022-11-18 15:48:48,055:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8835 (1.8835)
+2022-11-18 15:48:48,131:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8541 (1.8684)
+2022-11-18 15:48:48,206:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8114 (1.8493)
+2022-11-18 15:48:48,283:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9594 (1.8755)
+2022-11-18 15:48:48,357:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8246 (1.8664)
+2022-11-18 15:48:48,410:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_553.pth.tar
+2022-11-18 15:48:48,410:INFO: 
+===> EPOCH: 554 (P2)
+2022-11-18 15:48:48,410:INFO: - Computing loss (training)
+2022-11-18 15:48:48,753:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8394 (1.8394)
+2022-11-18 15:48:48,902:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9951 (1.9146)
+2022-11-18 15:48:49,049:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7985 (1.8724)
+2022-11-18 15:48:49,165:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8940 (1.8756)
+2022-11-18 15:48:49,578:INFO: Dataset: univ                Batch:  1/15	Loss 1.8898 (1.8898)
+2022-11-18 15:48:49,739:INFO: Dataset: univ                Batch:  2/15	Loss 1.8423 (1.8651)
+2022-11-18 15:48:49,896:INFO: Dataset: univ                Batch:  3/15	Loss 1.8477 (1.8594)
+2022-11-18 15:48:50,057:INFO: Dataset: univ                Batch:  4/15	Loss 1.8580 (1.8590)
+2022-11-18 15:48:50,221:INFO: Dataset: univ                Batch:  5/15	Loss 1.8700 (1.8611)
+2022-11-18 15:48:50,378:INFO: Dataset: univ                Batch:  6/15	Loss 1.8721 (1.8631)
+2022-11-18 15:48:50,533:INFO: Dataset: univ                Batch:  7/15	Loss 1.8615 (1.8629)
+2022-11-18 15:48:50,687:INFO: Dataset: univ                Batch:  8/15	Loss 1.7912 (1.8537)
+2022-11-18 15:48:50,841:INFO: Dataset: univ                Batch:  9/15	Loss 1.8225 (1.8506)
+2022-11-18 15:48:50,995:INFO: Dataset: univ                Batch: 10/15	Loss 1.8604 (1.8516)
+2022-11-18 15:48:51,151:INFO: Dataset: univ                Batch: 11/15	Loss 1.8532 (1.8517)
+2022-11-18 15:48:51,319:INFO: Dataset: univ                Batch: 12/15	Loss 1.8794 (1.8541)
+2022-11-18 15:48:51,484:INFO: Dataset: univ                Batch: 13/15	Loss 1.8017 (1.8502)
+2022-11-18 15:48:51,645:INFO: Dataset: univ                Batch: 14/15	Loss 1.8490 (1.8502)
+2022-11-18 15:48:51,729:INFO: Dataset: univ                Batch: 15/15	Loss 1.8502 (1.8502)
+2022-11-18 15:48:52,127:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8663 (1.8663)
+2022-11-18 15:48:52,277:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8905 (1.8777)
+2022-11-18 15:48:52,426:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0046 (1.9204)
+2022-11-18 15:48:52,572:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8468 (1.9021)
+2022-11-18 15:48:52,722:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7962 (1.8814)
+2022-11-18 15:48:52,873:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0218 (1.9037)
+2022-11-18 15:48:53,022:INFO: Dataset: zara1               Batch: 7/8	Loss 2.0184 (1.9203)
+2022-11-18 15:48:53,164:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9324 (1.9216)
+2022-11-18 15:48:53,561:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8477 (1.8477)
+2022-11-18 15:48:53,715:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8963 (1.8723)
+2022-11-18 15:48:53,872:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8254 (1.8563)
+2022-11-18 15:48:54,023:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8300 (1.8497)
+2022-11-18 15:48:54,180:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8351 (1.8470)
+2022-11-18 15:48:54,339:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8118 (1.8408)
+2022-11-18 15:48:54,493:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8151 (1.8372)
+2022-11-18 15:48:54,650:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8197 (1.8352)
+2022-11-18 15:48:54,804:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8192 (1.8335)
+2022-11-18 15:48:54,960:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8629 (1.8362)
+2022-11-18 15:48:55,201:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9868 (1.8507)
+2022-11-18 15:48:55,376:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8162 (1.8476)
+2022-11-18 15:48:55,561:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8016 (1.8439)
+2022-11-18 15:48:55,731:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8320 (1.8429)
+2022-11-18 15:48:55,898:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8114 (1.8408)
+2022-11-18 15:48:56,065:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7868 (1.8374)
+2022-11-18 15:48:56,226:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7790 (1.8341)
+2022-11-18 15:48:56,363:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7708 (1.8311)
+2022-11-18 15:48:56,408:INFO: - Computing loss (validation)
+2022-11-18 15:48:56,680:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9595 (1.9595)
+2022-11-18 15:48:56,716:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8093 (1.9507)
+2022-11-18 15:48:57,032:INFO: Dataset: univ                Batch: 1/3	Loss 1.8606 (1.8606)
+2022-11-18 15:48:57,121:INFO: Dataset: univ                Batch: 2/3	Loss 1.9103 (1.8842)
+2022-11-18 15:48:57,199:INFO: Dataset: univ                Batch: 3/3	Loss 1.8676 (1.8791)
+2022-11-18 15:48:57,501:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7659 (1.7659)
+2022-11-18 15:48:57,547:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7649 (1.7657)
+2022-11-18 15:48:57,869:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7884 (1.7884)
+2022-11-18 15:48:57,955:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7463 (1.7678)
+2022-11-18 15:48:58,042:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8379 (1.7900)
+2022-11-18 15:48:58,127:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8315 (1.8004)
+2022-11-18 15:48:58,209:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8452 (1.8087)
+2022-11-18 15:48:58,272:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_554.pth.tar
+2022-11-18 15:48:58,273:INFO: 
+===> EPOCH: 555 (P2)
+2022-11-18 15:48:58,273:INFO: - Computing loss (training)
+2022-11-18 15:48:58,640:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9814 (1.9814)
+2022-11-18 15:48:58,826:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8585 (1.9198)
+2022-11-18 15:48:59,000:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8330 (1.8918)
+2022-11-18 15:48:59,123:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9225 (1.8971)
+2022-11-18 15:48:59,554:INFO: Dataset: univ                Batch:  1/15	Loss 1.7867 (1.7867)
+2022-11-18 15:48:59,729:INFO: Dataset: univ                Batch:  2/15	Loss 1.8385 (1.8114)
+2022-11-18 15:48:59,913:INFO: Dataset: univ                Batch:  3/15	Loss 1.9115 (1.8498)
+2022-11-18 15:49:00,094:INFO: Dataset: univ                Batch:  4/15	Loss 1.8294 (1.8445)
+2022-11-18 15:49:00,260:INFO: Dataset: univ                Batch:  5/15	Loss 1.8613 (1.8479)
+2022-11-18 15:49:00,422:INFO: Dataset: univ                Batch:  6/15	Loss 1.8327 (1.8452)
+2022-11-18 15:49:00,580:INFO: Dataset: univ                Batch:  7/15	Loss 1.8391 (1.8443)
+2022-11-18 15:49:00,737:INFO: Dataset: univ                Batch:  8/15	Loss 1.8095 (1.8399)
+2022-11-18 15:49:00,893:INFO: Dataset: univ                Batch:  9/15	Loss 1.7990 (1.8353)
+2022-11-18 15:49:01,049:INFO: Dataset: univ                Batch: 10/15	Loss 1.8632 (1.8382)
+2022-11-18 15:49:01,207:INFO: Dataset: univ                Batch: 11/15	Loss 1.8475 (1.8391)
+2022-11-18 15:49:01,365:INFO: Dataset: univ                Batch: 12/15	Loss 1.8447 (1.8395)
+2022-11-18 15:49:01,521:INFO: Dataset: univ                Batch: 13/15	Loss 1.8132 (1.8374)
+2022-11-18 15:49:01,678:INFO: Dataset: univ                Batch: 14/15	Loss 1.8481 (1.8382)
+2022-11-18 15:49:01,760:INFO: Dataset: univ                Batch: 15/15	Loss 1.7778 (1.8374)
+2022-11-18 15:49:02,143:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8179 (1.8179)
+2022-11-18 15:49:02,298:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9422 (1.8831)
+2022-11-18 15:49:02,454:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9686 (1.9093)
+2022-11-18 15:49:02,617:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9698 (1.9238)
+2022-11-18 15:49:02,771:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8365 (1.9059)
+2022-11-18 15:49:02,920:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8357 (1.8924)
+2022-11-18 15:49:03,069:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8042 (1.8791)
+2022-11-18 15:49:03,205:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9525 (1.8867)
+2022-11-18 15:49:03,607:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9417 (1.9417)
+2022-11-18 15:49:03,765:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8182 (1.8773)
+2022-11-18 15:49:03,916:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8101 (1.8545)
+2022-11-18 15:49:04,066:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9026 (1.8669)
+2022-11-18 15:49:04,216:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7810 (1.8498)
+2022-11-18 15:49:04,376:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8911 (1.8560)
+2022-11-18 15:49:04,529:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8335 (1.8529)
+2022-11-18 15:49:04,680:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7305 (1.8367)
+2022-11-18 15:49:04,831:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8575 (1.8389)
+2022-11-18 15:49:04,980:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8288 (1.8379)
+2022-11-18 15:49:05,132:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8842 (1.8419)
+2022-11-18 15:49:05,282:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7697 (1.8358)
+2022-11-18 15:49:05,432:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8410 (1.8363)
+2022-11-18 15:49:05,583:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0372 (1.8501)
+2022-11-18 15:49:05,742:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8120 (1.8475)
+2022-11-18 15:49:05,895:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8795 (1.8493)
+2022-11-18 15:49:06,046:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8365 (1.8486)
+2022-11-18 15:49:06,188:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8328 (1.8478)
+2022-11-18 15:49:06,241:INFO: - Computing loss (validation)
+2022-11-18 15:49:06,528:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9424 (1.9424)
+2022-11-18 15:49:06,563:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7199 (1.9310)
+2022-11-18 15:49:06,896:INFO: Dataset: univ                Batch: 1/3	Loss 1.8233 (1.8233)
+2022-11-18 15:49:06,977:INFO: Dataset: univ                Batch: 2/3	Loss 1.8485 (1.8360)
+2022-11-18 15:49:07,055:INFO: Dataset: univ                Batch: 3/3	Loss 1.8211 (1.8311)
+2022-11-18 15:49:07,381:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8387 (1.8387)
+2022-11-18 15:49:07,427:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6532 (1.7940)
+2022-11-18 15:49:07,738:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8473 (1.8473)
+2022-11-18 15:49:07,814:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8239 (1.8357)
+2022-11-18 15:49:07,890:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8413 (1.8376)
+2022-11-18 15:49:07,970:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8270 (1.8349)
+2022-11-18 15:49:08,048:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8639 (1.8405)
+2022-11-18 15:49:08,109:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_555.pth.tar
+2022-11-18 15:49:08,109:INFO: 
+===> EPOCH: 556 (P2)
+2022-11-18 15:49:08,110:INFO: - Computing loss (training)
+2022-11-18 15:49:08,482:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9179 (1.9179)
+2022-11-18 15:49:08,643:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7979 (1.8576)
+2022-11-18 15:49:08,799:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8236 (1.8465)
+2022-11-18 15:49:08,919:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8742 (1.8514)
+2022-11-18 15:49:09,366:INFO: Dataset: univ                Batch:  1/15	Loss 1.8536 (1.8536)
+2022-11-18 15:49:09,539:INFO: Dataset: univ                Batch:  2/15	Loss 1.8847 (1.8682)
+2022-11-18 15:49:09,708:INFO: Dataset: univ                Batch:  3/15	Loss 1.8655 (1.8672)
+2022-11-18 15:49:09,869:INFO: Dataset: univ                Batch:  4/15	Loss 1.7757 (1.8427)
+2022-11-18 15:49:10,034:INFO: Dataset: univ                Batch:  5/15	Loss 1.8563 (1.8454)
+2022-11-18 15:49:10,194:INFO: Dataset: univ                Batch:  6/15	Loss 1.8286 (1.8426)
+2022-11-18 15:49:10,351:INFO: Dataset: univ                Batch:  7/15	Loss 1.8391 (1.8421)
+2022-11-18 15:49:10,509:INFO: Dataset: univ                Batch:  8/15	Loss 1.8303 (1.8406)
+2022-11-18 15:49:10,667:INFO: Dataset: univ                Batch:  9/15	Loss 1.9036 (1.8473)
+2022-11-18 15:49:10,826:INFO: Dataset: univ                Batch: 10/15	Loss 1.8064 (1.8433)
+2022-11-18 15:49:11,004:INFO: Dataset: univ                Batch: 11/15	Loss 1.8655 (1.8451)
+2022-11-18 15:49:11,187:INFO: Dataset: univ                Batch: 12/15	Loss 1.8407 (1.8448)
+2022-11-18 15:49:11,352:INFO: Dataset: univ                Batch: 13/15	Loss 1.8539 (1.8454)
+2022-11-18 15:49:11,510:INFO: Dataset: univ                Batch: 14/15	Loss 1.8400 (1.8450)
+2022-11-18 15:49:11,594:INFO: Dataset: univ                Batch: 15/15	Loss 1.8296 (1.8448)
+2022-11-18 15:49:12,014:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8952 (1.8952)
+2022-11-18 15:49:12,172:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9034 (1.8989)
+2022-11-18 15:49:12,323:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9453 (1.9148)
+2022-11-18 15:49:12,486:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8470 (1.8960)
+2022-11-18 15:49:12,646:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8074 (1.8775)
+2022-11-18 15:49:12,798:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8968 (1.8807)
+2022-11-18 15:49:12,950:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7688 (1.8632)
+2022-11-18 15:49:13,096:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8407 (1.8609)
+2022-11-18 15:49:13,511:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8905 (1.8905)
+2022-11-18 15:49:13,670:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7688 (1.8278)
+2022-11-18 15:49:13,828:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7985 (1.8181)
+2022-11-18 15:49:13,995:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8789 (1.8331)
+2022-11-18 15:49:14,154:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8411 (1.8346)
+2022-11-18 15:49:14,312:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8120 (1.8307)
+2022-11-18 15:49:14,466:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8917 (1.8391)
+2022-11-18 15:49:14,614:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8792 (1.8443)
+2022-11-18 15:49:14,765:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8460 (1.8445)
+2022-11-18 15:49:14,959:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8230 (1.8422)
+2022-11-18 15:49:15,155:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8328 (1.8414)
+2022-11-18 15:49:15,338:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9232 (1.8477)
+2022-11-18 15:49:15,543:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8811 (1.8501)
+2022-11-18 15:49:15,768:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9062 (1.8542)
+2022-11-18 15:49:15,985:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7662 (1.8483)
+2022-11-18 15:49:16,184:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9427 (1.8543)
+2022-11-18 15:49:16,370:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8825 (1.8558)
+2022-11-18 15:49:16,552:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8907 (1.8574)
+2022-11-18 15:49:16,610:INFO: - Computing loss (validation)
+2022-11-18 15:49:16,910:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8739 (1.8739)
+2022-11-18 15:49:16,947:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6952 (1.8636)
+2022-11-18 15:49:17,298:INFO: Dataset: univ                Batch: 1/3	Loss 1.8953 (1.8953)
+2022-11-18 15:49:17,390:INFO: Dataset: univ                Batch: 2/3	Loss 1.8508 (1.8724)
+2022-11-18 15:49:17,473:INFO: Dataset: univ                Batch: 3/3	Loss 1.8163 (1.8572)
+2022-11-18 15:49:17,795:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0000 (2.0000)
+2022-11-18 15:49:17,846:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7607 (1.9315)
+2022-11-18 15:49:18,189:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7593 (1.7593)
+2022-11-18 15:49:18,272:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8577 (1.8054)
+2022-11-18 15:49:18,357:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8050 (1.8053)
+2022-11-18 15:49:18,440:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8368 (1.8135)
+2022-11-18 15:49:18,521:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8071 (1.8123)
+2022-11-18 15:49:18,589:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_556.pth.tar
+2022-11-18 15:49:18,589:INFO: 
+===> EPOCH: 557 (P2)
+2022-11-18 15:49:18,590:INFO: - Computing loss (training)
+2022-11-18 15:49:18,986:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8162 (1.8162)
+2022-11-18 15:49:19,165:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8852 (1.8491)
+2022-11-18 15:49:19,342:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9037 (1.8669)
+2022-11-18 15:49:19,472:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8716 (1.8677)
+2022-11-18 15:49:19,907:INFO: Dataset: univ                Batch:  1/15	Loss 1.8330 (1.8330)
+2022-11-18 15:49:20,077:INFO: Dataset: univ                Batch:  2/15	Loss 1.8651 (1.8489)
+2022-11-18 15:49:20,242:INFO: Dataset: univ                Batch:  3/15	Loss 1.8190 (1.8388)
+2022-11-18 15:49:20,405:INFO: Dataset: univ                Batch:  4/15	Loss 1.8156 (1.8329)
+2022-11-18 15:49:20,577:INFO: Dataset: univ                Batch:  5/15	Loss 1.8538 (1.8369)
+2022-11-18 15:49:20,751:INFO: Dataset: univ                Batch:  6/15	Loss 1.8573 (1.8406)
+2022-11-18 15:49:20,951:INFO: Dataset: univ                Batch:  7/15	Loss 1.8768 (1.8467)
+2022-11-18 15:49:21,130:INFO: Dataset: univ                Batch:  8/15	Loss 1.8413 (1.8460)
+2022-11-18 15:49:21,302:INFO: Dataset: univ                Batch:  9/15	Loss 1.8559 (1.8471)
+2022-11-18 15:49:21,477:INFO: Dataset: univ                Batch: 10/15	Loss 1.8375 (1.8461)
+2022-11-18 15:49:21,645:INFO: Dataset: univ                Batch: 11/15	Loss 1.8401 (1.8455)
+2022-11-18 15:49:21,802:INFO: Dataset: univ                Batch: 12/15	Loss 1.8595 (1.8466)
+2022-11-18 15:49:21,966:INFO: Dataset: univ                Batch: 13/15	Loss 1.8153 (1.8442)
+2022-11-18 15:49:22,134:INFO: Dataset: univ                Batch: 14/15	Loss 1.7839 (1.8400)
+2022-11-18 15:49:22,217:INFO: Dataset: univ                Batch: 15/15	Loss 1.9246 (1.8411)
+2022-11-18 15:49:22,626:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8585 (1.8585)
+2022-11-18 15:49:22,782:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9610 (1.9103)
+2022-11-18 15:49:22,936:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9396 (1.9211)
+2022-11-18 15:49:23,099:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8000 (1.8908)
+2022-11-18 15:49:23,285:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8123 (1.8744)
+2022-11-18 15:49:23,467:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9424 (1.8857)
+2022-11-18 15:49:23,643:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7927 (1.8731)
+2022-11-18 15:49:23,830:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8131 (1.8667)
+2022-11-18 15:49:24,422:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7281 (1.7281)
+2022-11-18 15:49:24,631:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8434 (1.7891)
+2022-11-18 15:49:24,824:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8285 (1.8018)
+2022-11-18 15:49:25,029:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8102 (1.8039)
+2022-11-18 15:49:25,210:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8327 (1.8103)
+2022-11-18 15:49:25,382:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8355 (1.8151)
+2022-11-18 15:49:25,538:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8087 (1.8142)
+2022-11-18 15:49:25,701:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8846 (1.8242)
+2022-11-18 15:49:25,855:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8131 (1.8230)
+2022-11-18 15:49:26,015:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7730 (1.8183)
+2022-11-18 15:49:26,177:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8345 (1.8198)
+2022-11-18 15:49:26,333:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8602 (1.8232)
+2022-11-18 15:49:26,505:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8020 (1.8216)
+2022-11-18 15:49:26,682:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8414 (1.8230)
+2022-11-18 15:49:26,862:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8627 (1.8255)
+2022-11-18 15:49:27,038:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9336 (1.8322)
+2022-11-18 15:49:27,221:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7649 (1.8284)
+2022-11-18 15:49:27,372:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8442 (1.8291)
+2022-11-18 15:49:27,422:INFO: - Computing loss (validation)
+2022-11-18 15:49:27,698:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9494 (1.9494)
+2022-11-18 15:49:27,735:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0942 (1.9597)
+2022-11-18 15:49:28,072:INFO: Dataset: univ                Batch: 1/3	Loss 1.7160 (1.7160)
+2022-11-18 15:49:28,153:INFO: Dataset: univ                Batch: 2/3	Loss 1.8675 (1.7969)
+2022-11-18 15:49:28,230:INFO: Dataset: univ                Batch: 3/3	Loss 1.8700 (1.8225)
+2022-11-18 15:49:28,556:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8227 (1.8227)
+2022-11-18 15:49:28,610:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8230 (1.8228)
+2022-11-18 15:49:28,941:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8782 (1.8782)
+2022-11-18 15:49:29,016:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7651 (1.8208)
+2022-11-18 15:49:29,091:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8054 (1.8158)
+2022-11-18 15:49:29,166:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7553 (1.8009)
+2022-11-18 15:49:29,241:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8094 (1.8026)
+2022-11-18 15:49:29,296:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_557.pth.tar
+2022-11-18 15:49:29,296:INFO: 
+===> EPOCH: 558 (P2)
+2022-11-18 15:49:29,297:INFO: - Computing loss (training)
+2022-11-18 15:49:29,660:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8135 (1.8135)
+2022-11-18 15:49:29,824:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9234 (1.8668)
+2022-11-18 15:49:29,983:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9705 (1.9011)
+2022-11-18 15:49:30,101:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9681 (1.9119)
+2022-11-18 15:49:30,485:INFO: Dataset: univ                Batch:  1/15	Loss 1.8559 (1.8559)
+2022-11-18 15:49:30,647:INFO: Dataset: univ                Batch:  2/15	Loss 1.8435 (1.8497)
+2022-11-18 15:49:30,803:INFO: Dataset: univ                Batch:  3/15	Loss 1.8532 (1.8509)
+2022-11-18 15:49:30,959:INFO: Dataset: univ                Batch:  4/15	Loss 1.7982 (1.8372)
+2022-11-18 15:49:31,112:INFO: Dataset: univ                Batch:  5/15	Loss 1.8679 (1.8428)
+2022-11-18 15:49:31,271:INFO: Dataset: univ                Batch:  6/15	Loss 1.8673 (1.8473)
+2022-11-18 15:49:31,429:INFO: Dataset: univ                Batch:  7/15	Loss 1.8822 (1.8525)
+2022-11-18 15:49:31,593:INFO: Dataset: univ                Batch:  8/15	Loss 1.8419 (1.8511)
+2022-11-18 15:49:31,759:INFO: Dataset: univ                Batch:  9/15	Loss 1.8422 (1.8501)
+2022-11-18 15:49:31,919:INFO: Dataset: univ                Batch: 10/15	Loss 1.8384 (1.8490)
+2022-11-18 15:49:32,076:INFO: Dataset: univ                Batch: 11/15	Loss 1.8536 (1.8495)
+2022-11-18 15:49:32,237:INFO: Dataset: univ                Batch: 12/15	Loss 1.8062 (1.8457)
+2022-11-18 15:49:32,392:INFO: Dataset: univ                Batch: 13/15	Loss 1.8316 (1.8446)
+2022-11-18 15:49:32,546:INFO: Dataset: univ                Batch: 14/15	Loss 1.8274 (1.8435)
+2022-11-18 15:49:32,633:INFO: Dataset: univ                Batch: 15/15	Loss 1.8686 (1.8438)
+2022-11-18 15:49:33,069:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8748 (1.8748)
+2022-11-18 15:49:33,236:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8942 (1.8844)
+2022-11-18 15:49:33,387:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8075 (1.8612)
+2022-11-18 15:49:33,570:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8490 (1.8582)
+2022-11-18 15:49:33,744:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9081 (1.8693)
+2022-11-18 15:49:33,915:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8803 (1.8712)
+2022-11-18 15:49:34,085:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8727 (1.8714)
+2022-11-18 15:49:34,241:INFO: Dataset: zara1               Batch: 8/8	Loss 2.1186 (1.8989)
+2022-11-18 15:49:34,704:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8129 (1.8129)
+2022-11-18 15:49:34,878:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7599 (1.7860)
+2022-11-18 15:49:35,043:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8005 (1.7909)
+2022-11-18 15:49:35,219:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7692 (1.7856)
+2022-11-18 15:49:35,387:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8704 (1.8007)
+2022-11-18 15:49:35,547:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7441 (1.7919)
+2022-11-18 15:49:35,699:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8557 (1.8009)
+2022-11-18 15:49:35,848:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8590 (1.8078)
+2022-11-18 15:49:35,996:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8462 (1.8121)
+2022-11-18 15:49:36,163:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9573 (1.8262)
+2022-11-18 15:49:36,330:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8222 (1.8258)
+2022-11-18 15:49:36,491:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8787 (1.8299)
+2022-11-18 15:49:36,650:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8308 (1.8300)
+2022-11-18 15:49:36,810:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8544 (1.8316)
+2022-11-18 15:49:36,971:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8979 (1.8360)
+2022-11-18 15:49:37,139:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7964 (1.8334)
+2022-11-18 15:49:37,307:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8063 (1.8317)
+2022-11-18 15:49:37,458:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8538 (1.8329)
+2022-11-18 15:49:37,513:INFO: - Computing loss (validation)
+2022-11-18 15:49:37,807:INFO: Dataset: hotel               Batch: 1/2	Loss 2.0096 (2.0096)
+2022-11-18 15:49:37,844:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8333 (1.9970)
+2022-11-18 15:49:38,157:INFO: Dataset: univ                Batch: 1/3	Loss 1.8399 (1.8399)
+2022-11-18 15:49:38,242:INFO: Dataset: univ                Batch: 2/3	Loss 1.8425 (1.8412)
+2022-11-18 15:49:38,320:INFO: Dataset: univ                Batch: 3/3	Loss 1.7766 (1.8234)
+2022-11-18 15:49:38,635:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8034 (1.8034)
+2022-11-18 15:49:38,683:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6036 (1.7572)
+2022-11-18 15:49:39,004:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8157 (1.8157)
+2022-11-18 15:49:39,085:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8718 (1.8443)
+2022-11-18 15:49:39,168:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8973 (1.8631)
+2022-11-18 15:49:39,248:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8763 (1.8662)
+2022-11-18 15:49:39,328:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7790 (1.8485)
+2022-11-18 15:49:39,383:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_558.pth.tar
+2022-11-18 15:49:39,383:INFO: 
+===> EPOCH: 559 (P2)
+2022-11-18 15:49:39,383:INFO: - Computing loss (training)
+2022-11-18 15:49:39,728:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9019 (1.9019)
+2022-11-18 15:49:39,880:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8904 (1.8963)
+2022-11-18 15:49:40,029:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8644 (1.8856)
+2022-11-18 15:49:40,142:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8948 (1.8871)
+2022-11-18 15:49:40,546:INFO: Dataset: univ                Batch:  1/15	Loss 1.7747 (1.7747)
+2022-11-18 15:49:40,709:INFO: Dataset: univ                Batch:  2/15	Loss 1.8446 (1.8095)
+2022-11-18 15:49:40,896:INFO: Dataset: univ                Batch:  3/15	Loss 1.8161 (1.8119)
+2022-11-18 15:49:41,070:INFO: Dataset: univ                Batch:  4/15	Loss 1.8583 (1.8237)
+2022-11-18 15:49:41,245:INFO: Dataset: univ                Batch:  5/15	Loss 1.8684 (1.8324)
+2022-11-18 15:49:41,424:INFO: Dataset: univ                Batch:  6/15	Loss 1.8367 (1.8330)
+2022-11-18 15:49:41,603:INFO: Dataset: univ                Batch:  7/15	Loss 1.8481 (1.8355)
+2022-11-18 15:49:41,781:INFO: Dataset: univ                Batch:  8/15	Loss 1.8620 (1.8389)
+2022-11-18 15:49:41,944:INFO: Dataset: univ                Batch:  9/15	Loss 1.8440 (1.8395)
+2022-11-18 15:49:42,104:INFO: Dataset: univ                Batch: 10/15	Loss 1.8362 (1.8392)
+2022-11-18 15:49:42,264:INFO: Dataset: univ                Batch: 11/15	Loss 1.8831 (1.8430)
+2022-11-18 15:49:42,426:INFO: Dataset: univ                Batch: 12/15	Loss 1.8866 (1.8465)
+2022-11-18 15:49:42,587:INFO: Dataset: univ                Batch: 13/15	Loss 1.9355 (1.8531)
+2022-11-18 15:49:42,748:INFO: Dataset: univ                Batch: 14/15	Loss 1.8994 (1.8567)
+2022-11-18 15:49:42,836:INFO: Dataset: univ                Batch: 15/15	Loss 1.8667 (1.8569)
+2022-11-18 15:49:43,237:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9211 (1.9211)
+2022-11-18 15:49:43,388:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8929 (1.9076)
+2022-11-18 15:49:43,539:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8591 (1.8912)
+2022-11-18 15:49:43,690:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7606 (1.8592)
+2022-11-18 15:49:43,846:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8562 (1.8587)
+2022-11-18 15:49:43,998:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9373 (1.8715)
+2022-11-18 15:49:44,148:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8145 (1.8635)
+2022-11-18 15:49:44,286:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0495 (1.8835)
+2022-11-18 15:49:44,674:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9158 (1.9158)
+2022-11-18 15:49:44,828:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8395 (1.8784)
+2022-11-18 15:49:44,979:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8355 (1.8623)
+2022-11-18 15:49:45,130:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8090 (1.8483)
+2022-11-18 15:49:45,281:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8358 (1.8457)
+2022-11-18 15:49:45,443:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7963 (1.8367)
+2022-11-18 15:49:45,600:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9489 (1.8542)
+2022-11-18 15:49:45,749:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8849 (1.8583)
+2022-11-18 15:49:45,900:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8411 (1.8565)
+2022-11-18 15:49:46,050:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8330 (1.8542)
+2022-11-18 15:49:46,201:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8755 (1.8560)
+2022-11-18 15:49:46,353:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7932 (1.8510)
+2022-11-18 15:49:46,507:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7538 (1.8433)
+2022-11-18 15:49:46,658:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8289 (1.8422)
+2022-11-18 15:49:46,811:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7974 (1.8393)
+2022-11-18 15:49:46,983:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8798 (1.8421)
+2022-11-18 15:49:47,155:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8517 (1.8426)
+2022-11-18 15:49:47,308:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8473 (1.8429)
+2022-11-18 15:49:47,362:INFO: - Computing loss (validation)
+2022-11-18 15:49:47,656:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9179 (1.9179)
+2022-11-18 15:49:47,694:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7719 (1.9069)
+2022-11-18 15:49:48,011:INFO: Dataset: univ                Batch: 1/3	Loss 1.8367 (1.8367)
+2022-11-18 15:49:48,087:INFO: Dataset: univ                Batch: 2/3	Loss 1.8306 (1.8337)
+2022-11-18 15:49:48,162:INFO: Dataset: univ                Batch: 3/3	Loss 1.8160 (1.8269)
+2022-11-18 15:49:48,478:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8464 (1.8464)
+2022-11-18 15:49:48,524:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9059 (1.8613)
+2022-11-18 15:49:48,829:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7249 (1.7249)
+2022-11-18 15:49:48,909:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7650 (1.7452)
+2022-11-18 15:49:48,987:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7543 (1.7482)
+2022-11-18 15:49:49,071:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7905 (1.7587)
+2022-11-18 15:49:49,158:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8245 (1.7717)
+2022-11-18 15:49:49,225:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_559.pth.tar
+2022-11-18 15:49:49,225:INFO: 
+===> EPOCH: 560 (P2)
+2022-11-18 15:49:49,226:INFO: - Computing loss (training)
+2022-11-18 15:49:49,631:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0110 (2.0110)
+2022-11-18 15:49:49,813:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9130 (1.9638)
+2022-11-18 15:49:49,989:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8140 (1.9142)
+2022-11-18 15:49:50,127:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8072 (1.8964)
+2022-11-18 15:49:50,603:INFO: Dataset: univ                Batch:  1/15	Loss 1.8502 (1.8502)
+2022-11-18 15:49:50,785:INFO: Dataset: univ                Batch:  2/15	Loss 1.8247 (1.8387)
+2022-11-18 15:49:50,977:INFO: Dataset: univ                Batch:  3/15	Loss 1.7918 (1.8225)
+2022-11-18 15:49:51,149:INFO: Dataset: univ                Batch:  4/15	Loss 1.8510 (1.8297)
+2022-11-18 15:49:51,310:INFO: Dataset: univ                Batch:  5/15	Loss 1.8680 (1.8380)
+2022-11-18 15:49:51,474:INFO: Dataset: univ                Batch:  6/15	Loss 1.8180 (1.8349)
+2022-11-18 15:49:51,634:INFO: Dataset: univ                Batch:  7/15	Loss 1.8351 (1.8349)
+2022-11-18 15:49:51,791:INFO: Dataset: univ                Batch:  8/15	Loss 1.8185 (1.8328)
+2022-11-18 15:49:51,951:INFO: Dataset: univ                Batch:  9/15	Loss 1.8632 (1.8361)
+2022-11-18 15:49:52,130:INFO: Dataset: univ                Batch: 10/15	Loss 1.8454 (1.8371)
+2022-11-18 15:49:52,331:INFO: Dataset: univ                Batch: 11/15	Loss 1.9215 (1.8448)
+2022-11-18 15:49:52,540:INFO: Dataset: univ                Batch: 12/15	Loss 1.8719 (1.8468)
+2022-11-18 15:49:52,766:INFO: Dataset: univ                Batch: 13/15	Loss 1.8731 (1.8488)
+2022-11-18 15:49:52,972:INFO: Dataset: univ                Batch: 14/15	Loss 1.8484 (1.8488)
+2022-11-18 15:49:53,086:INFO: Dataset: univ                Batch: 15/15	Loss 1.8693 (1.8491)
+2022-11-18 15:49:53,573:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8096 (1.8096)
+2022-11-18 15:49:53,757:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9340 (1.8717)
+2022-11-18 15:49:53,919:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9875 (1.9063)
+2022-11-18 15:49:54,069:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8573 (1.8930)
+2022-11-18 15:49:54,229:INFO: Dataset: zara1               Batch: 5/8	Loss 2.0066 (1.9143)
+2022-11-18 15:49:54,383:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9811 (1.9268)
+2022-11-18 15:49:54,534:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8252 (1.9128)
+2022-11-18 15:49:54,671:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8954 (1.9110)
+2022-11-18 15:49:55,064:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7454 (1.7454)
+2022-11-18 15:49:55,224:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7222 (1.7333)
+2022-11-18 15:49:55,394:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8383 (1.7693)
+2022-11-18 15:49:55,569:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8404 (1.7866)
+2022-11-18 15:49:55,756:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8611 (1.8030)
+2022-11-18 15:49:55,938:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8768 (1.8171)
+2022-11-18 15:49:56,109:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7747 (1.8114)
+2022-11-18 15:49:56,271:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8174 (1.8122)
+2022-11-18 15:49:56,432:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9009 (1.8218)
+2022-11-18 15:49:56,592:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9229 (1.8316)
+2022-11-18 15:49:56,752:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7324 (1.8232)
+2022-11-18 15:49:56,904:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7336 (1.8162)
+2022-11-18 15:49:57,060:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8018 (1.8152)
+2022-11-18 15:49:57,220:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8386 (1.8168)
+2022-11-18 15:49:57,378:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8041 (1.8160)
+2022-11-18 15:49:57,533:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7788 (1.8136)
+2022-11-18 15:49:57,684:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9368 (1.8202)
+2022-11-18 15:49:57,823:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9398 (1.8262)
+2022-11-18 15:49:57,868:INFO: - Computing loss (validation)
+2022-11-18 15:49:58,147:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9020 (1.9020)
+2022-11-18 15:49:58,184:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8581 (1.8987)
+2022-11-18 15:49:58,488:INFO: Dataset: univ                Batch: 1/3	Loss 1.8470 (1.8470)
+2022-11-18 15:49:58,567:INFO: Dataset: univ                Batch: 2/3	Loss 1.8425 (1.8444)
+2022-11-18 15:49:58,640:INFO: Dataset: univ                Batch: 3/3	Loss 1.8798 (1.8543)
+2022-11-18 15:49:58,966:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8350 (1.8350)
+2022-11-18 15:49:59,014:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7917 (1.8232)
+2022-11-18 15:49:59,358:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7529 (1.7529)
+2022-11-18 15:49:59,436:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7983 (1.7750)
+2022-11-18 15:49:59,514:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8515 (1.7997)
+2022-11-18 15:49:59,588:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8335 (1.8078)
+2022-11-18 15:49:59,662:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8412 (1.8145)
+2022-11-18 15:49:59,717:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_560.pth.tar
+2022-11-18 15:49:59,718:INFO: 
+===> EPOCH: 561 (P2)
+2022-11-18 15:49:59,718:INFO: - Computing loss (training)
+2022-11-18 15:50:00,053:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8549 (1.8549)
+2022-11-18 15:50:00,221:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9031 (1.8794)
+2022-11-18 15:50:00,380:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9188 (1.8926)
+2022-11-18 15:50:00,507:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9319 (1.8991)
+2022-11-18 15:50:00,949:INFO: Dataset: univ                Batch:  1/15	Loss 1.8180 (1.8180)
+2022-11-18 15:50:01,121:INFO: Dataset: univ                Batch:  2/15	Loss 1.8190 (1.8185)
+2022-11-18 15:50:01,298:INFO: Dataset: univ                Batch:  3/15	Loss 1.8177 (1.8183)
+2022-11-18 15:50:01,473:INFO: Dataset: univ                Batch:  4/15	Loss 1.8785 (1.8330)
+2022-11-18 15:50:01,653:INFO: Dataset: univ                Batch:  5/15	Loss 1.8386 (1.8342)
+2022-11-18 15:50:01,823:INFO: Dataset: univ                Batch:  6/15	Loss 1.8435 (1.8357)
+2022-11-18 15:50:01,991:INFO: Dataset: univ                Batch:  7/15	Loss 1.8653 (1.8397)
+2022-11-18 15:50:02,152:INFO: Dataset: univ                Batch:  8/15	Loss 1.8172 (1.8371)
+2022-11-18 15:50:02,311:INFO: Dataset: univ                Batch:  9/15	Loss 1.8507 (1.8385)
+2022-11-18 15:50:02,486:INFO: Dataset: univ                Batch: 10/15	Loss 1.8782 (1.8423)
+2022-11-18 15:50:02,667:INFO: Dataset: univ                Batch: 11/15	Loss 1.8386 (1.8420)
+2022-11-18 15:50:02,839:INFO: Dataset: univ                Batch: 12/15	Loss 1.8783 (1.8451)
+2022-11-18 15:50:03,009:INFO: Dataset: univ                Batch: 13/15	Loss 1.8157 (1.8427)
+2022-11-18 15:50:03,191:INFO: Dataset: univ                Batch: 14/15	Loss 1.9025 (1.8468)
+2022-11-18 15:50:03,290:INFO: Dataset: univ                Batch: 15/15	Loss 1.8725 (1.8472)
+2022-11-18 15:50:03,767:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8138 (1.8138)
+2022-11-18 15:50:03,943:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9096 (1.8596)
+2022-11-18 15:50:04,121:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8792 (1.8666)
+2022-11-18 15:50:04,287:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7907 (1.8483)
+2022-11-18 15:50:04,457:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8605 (1.8506)
+2022-11-18 15:50:04,622:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8493 (1.8504)
+2022-11-18 15:50:04,792:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9044 (1.8590)
+2022-11-18 15:50:04,945:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7418 (1.8474)
+2022-11-18 15:50:05,368:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8594 (1.8594)
+2022-11-18 15:50:05,538:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8349 (1.8472)
+2022-11-18 15:50:05,702:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9234 (1.8723)
+2022-11-18 15:50:05,869:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8254 (1.8610)
+2022-11-18 15:50:06,038:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9511 (1.8787)
+2022-11-18 15:50:06,208:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8975 (1.8819)
+2022-11-18 15:50:06,372:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8242 (1.8726)
+2022-11-18 15:50:06,537:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8046 (1.8639)
+2022-11-18 15:50:06,700:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7856 (1.8544)
+2022-11-18 15:50:06,879:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7468 (1.8444)
+2022-11-18 15:50:07,063:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7876 (1.8389)
+2022-11-18 15:50:07,238:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7207 (1.8291)
+2022-11-18 15:50:07,420:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7317 (1.8222)
+2022-11-18 15:50:07,608:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8683 (1.8255)
+2022-11-18 15:50:07,787:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8005 (1.8237)
+2022-11-18 15:50:07,955:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7688 (1.8203)
+2022-11-18 15:50:08,134:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8020 (1.8192)
+2022-11-18 15:50:08,316:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8787 (1.8220)
+2022-11-18 15:50:08,369:INFO: - Computing loss (validation)
+2022-11-18 15:50:08,646:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8256 (1.8256)
+2022-11-18 15:50:08,683:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0665 (1.8404)
+2022-11-18 15:50:08,998:INFO: Dataset: univ                Batch: 1/3	Loss 1.8455 (1.8455)
+2022-11-18 15:50:09,084:INFO: Dataset: univ                Batch: 2/3	Loss 1.8333 (1.8388)
+2022-11-18 15:50:09,162:INFO: Dataset: univ                Batch: 3/3	Loss 1.8635 (1.8461)
+2022-11-18 15:50:09,466:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9143 (1.9143)
+2022-11-18 15:50:09,512:INFO: Dataset: zara1               Batch: 2/2	Loss 1.5863 (1.8383)
+2022-11-18 15:50:09,866:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7634 (1.7634)
+2022-11-18 15:50:09,951:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8395 (1.7992)
+2022-11-18 15:50:10,031:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8217 (1.8072)
+2022-11-18 15:50:10,111:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8291 (1.8126)
+2022-11-18 15:50:10,187:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8682 (1.8229)
+2022-11-18 15:50:10,243:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_561.pth.tar
+2022-11-18 15:50:10,243:INFO: 
+===> EPOCH: 562 (P2)
+2022-11-18 15:50:10,244:INFO: - Computing loss (training)
+2022-11-18 15:50:10,612:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8567 (1.8567)
+2022-11-18 15:50:10,765:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8229 (1.8412)
+2022-11-18 15:50:10,916:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8527 (1.8448)
+2022-11-18 15:50:11,032:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9167 (1.8569)
+2022-11-18 15:50:11,423:INFO: Dataset: univ                Batch:  1/15	Loss 1.8361 (1.8361)
+2022-11-18 15:50:11,597:INFO: Dataset: univ                Batch:  2/15	Loss 1.8900 (1.8609)
+2022-11-18 15:50:11,780:INFO: Dataset: univ                Batch:  3/15	Loss 1.8334 (1.8521)
+2022-11-18 15:50:11,958:INFO: Dataset: univ                Batch:  4/15	Loss 1.8847 (1.8598)
+2022-11-18 15:50:12,121:INFO: Dataset: univ                Batch:  5/15	Loss 1.8501 (1.8579)
+2022-11-18 15:50:12,286:INFO: Dataset: univ                Batch:  6/15	Loss 1.8458 (1.8560)
+2022-11-18 15:50:12,445:INFO: Dataset: univ                Batch:  7/15	Loss 1.9381 (1.8671)
+2022-11-18 15:50:12,601:INFO: Dataset: univ                Batch:  8/15	Loss 1.8297 (1.8630)
+2022-11-18 15:50:12,759:INFO: Dataset: univ                Batch:  9/15	Loss 1.7971 (1.8555)
+2022-11-18 15:50:12,914:INFO: Dataset: univ                Batch: 10/15	Loss 1.8571 (1.8557)
+2022-11-18 15:50:13,074:INFO: Dataset: univ                Batch: 11/15	Loss 1.7928 (1.8497)
+2022-11-18 15:50:13,231:INFO: Dataset: univ                Batch: 12/15	Loss 1.8105 (1.8466)
+2022-11-18 15:50:13,391:INFO: Dataset: univ                Batch: 13/15	Loss 1.8253 (1.8449)
+2022-11-18 15:50:13,555:INFO: Dataset: univ                Batch: 14/15	Loss 1.8463 (1.8450)
+2022-11-18 15:50:13,638:INFO: Dataset: univ                Batch: 15/15	Loss 1.9065 (1.8461)
+2022-11-18 15:50:14,063:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8480 (1.8480)
+2022-11-18 15:50:14,229:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8865 (1.8677)
+2022-11-18 15:50:14,381:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9649 (1.8966)
+2022-11-18 15:50:14,543:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7996 (1.8726)
+2022-11-18 15:50:14,696:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7810 (1.8556)
+2022-11-18 15:50:14,851:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0363 (1.8861)
+2022-11-18 15:50:15,006:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8107 (1.8763)
+2022-11-18 15:50:15,163:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8713 (1.8758)
+2022-11-18 15:50:15,701:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8031 (1.8031)
+2022-11-18 15:50:15,855:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8911 (1.8477)
+2022-11-18 15:50:16,023:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9270 (1.8709)
+2022-11-18 15:50:16,185:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8178 (1.8584)
+2022-11-18 15:50:16,345:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8680 (1.8603)
+2022-11-18 15:50:16,530:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7854 (1.8490)
+2022-11-18 15:50:16,704:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8469 (1.8487)
+2022-11-18 15:50:16,897:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9499 (1.8601)
+2022-11-18 15:50:17,088:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8703 (1.8613)
+2022-11-18 15:50:17,270:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9044 (1.8655)
+2022-11-18 15:50:17,454:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7388 (1.8538)
+2022-11-18 15:50:17,632:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8828 (1.8561)
+2022-11-18 15:50:17,798:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7207 (1.8450)
+2022-11-18 15:50:17,948:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7949 (1.8413)
+2022-11-18 15:50:18,104:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8379 (1.8411)
+2022-11-18 15:50:18,266:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8017 (1.8388)
+2022-11-18 15:50:18,453:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8577 (1.8399)
+2022-11-18 15:50:18,623:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8387 (1.8399)
+2022-11-18 15:50:18,673:INFO: - Computing loss (validation)
+2022-11-18 15:50:18,951:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9614 (1.9614)
+2022-11-18 15:50:18,990:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0730 (1.9709)
+2022-11-18 15:50:19,313:INFO: Dataset: univ                Batch: 1/3	Loss 1.7985 (1.7985)
+2022-11-18 15:50:19,396:INFO: Dataset: univ                Batch: 2/3	Loss 1.8848 (1.8424)
+2022-11-18 15:50:19,472:INFO: Dataset: univ                Batch: 3/3	Loss 1.8127 (1.8335)
+2022-11-18 15:50:19,855:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9936 (1.9936)
+2022-11-18 15:50:19,913:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7713 (1.9371)
+2022-11-18 15:50:20,314:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8290 (1.8290)
+2022-11-18 15:50:20,411:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8690 (1.8487)
+2022-11-18 15:50:20,515:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9017 (1.8674)
+2022-11-18 15:50:20,622:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8929 (1.8734)
+2022-11-18 15:50:20,722:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8461 (1.8679)
+2022-11-18 15:50:20,798:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_562.pth.tar
+2022-11-18 15:50:20,798:INFO: 
+===> EPOCH: 563 (P2)
+2022-11-18 15:50:20,799:INFO: - Computing loss (training)
+2022-11-18 15:50:21,231:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9499 (1.9499)
+2022-11-18 15:50:21,410:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8740 (1.9124)
+2022-11-18 15:50:21,582:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8634 (1.8961)
+2022-11-18 15:50:21,708:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9276 (1.9016)
+2022-11-18 15:50:22,120:INFO: Dataset: univ                Batch:  1/15	Loss 1.8578 (1.8578)
+2022-11-18 15:50:22,284:INFO: Dataset: univ                Batch:  2/15	Loss 1.8463 (1.8522)
+2022-11-18 15:50:22,452:INFO: Dataset: univ                Batch:  3/15	Loss 1.7934 (1.8318)
+2022-11-18 15:50:22,622:INFO: Dataset: univ                Batch:  4/15	Loss 1.8228 (1.8297)
+2022-11-18 15:50:22,789:INFO: Dataset: univ                Batch:  5/15	Loss 1.8755 (1.8396)
+2022-11-18 15:50:22,955:INFO: Dataset: univ                Batch:  6/15	Loss 1.8377 (1.8393)
+2022-11-18 15:50:23,119:INFO: Dataset: univ                Batch:  7/15	Loss 1.8701 (1.8436)
+2022-11-18 15:50:23,295:INFO: Dataset: univ                Batch:  8/15	Loss 1.8202 (1.8408)
+2022-11-18 15:50:23,462:INFO: Dataset: univ                Batch:  9/15	Loss 1.8328 (1.8399)
+2022-11-18 15:50:23,625:INFO: Dataset: univ                Batch: 10/15	Loss 1.8372 (1.8396)
+2022-11-18 15:50:23,791:INFO: Dataset: univ                Batch: 11/15	Loss 1.8439 (1.8400)
+2022-11-18 15:50:23,952:INFO: Dataset: univ                Batch: 12/15	Loss 1.8531 (1.8410)
+2022-11-18 15:50:24,114:INFO: Dataset: univ                Batch: 13/15	Loss 1.8206 (1.8395)
+2022-11-18 15:50:24,296:INFO: Dataset: univ                Batch: 14/15	Loss 1.8882 (1.8428)
+2022-11-18 15:50:24,385:INFO: Dataset: univ                Batch: 15/15	Loss 1.9932 (1.8447)
+2022-11-18 15:50:24,835:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9082 (1.9082)
+2022-11-18 15:50:25,014:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9439 (1.9254)
+2022-11-18 15:50:25,200:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9050 (1.9189)
+2022-11-18 15:50:25,395:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7335 (1.8765)
+2022-11-18 15:50:25,578:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8323 (1.8680)
+2022-11-18 15:50:25,769:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8521 (1.8654)
+2022-11-18 15:50:25,970:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8550 (1.8639)
+2022-11-18 15:50:26,141:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6733 (1.8440)
+2022-11-18 15:50:26,609:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7884 (1.7884)
+2022-11-18 15:50:26,776:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8352 (1.8117)
+2022-11-18 15:50:26,944:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8285 (1.8168)
+2022-11-18 15:50:27,098:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9014 (1.8389)
+2022-11-18 15:50:27,253:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7806 (1.8276)
+2022-11-18 15:50:27,412:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8075 (1.8244)
+2022-11-18 15:50:27,580:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8470 (1.8278)
+2022-11-18 15:50:27,737:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7572 (1.8192)
+2022-11-18 15:50:27,893:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8864 (1.8269)
+2022-11-18 15:50:28,062:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8606 (1.8305)
+2022-11-18 15:50:28,220:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8508 (1.8325)
+2022-11-18 15:50:28,378:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9992 (1.8459)
+2022-11-18 15:50:28,541:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7787 (1.8405)
+2022-11-18 15:50:28,703:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8448 (1.8409)
+2022-11-18 15:50:28,864:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7981 (1.8382)
+2022-11-18 15:50:29,021:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8530 (1.8391)
+2022-11-18 15:50:29,177:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7262 (1.8330)
+2022-11-18 15:50:29,320:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8416 (1.8335)
+2022-11-18 15:50:29,371:INFO: - Computing loss (validation)
+2022-11-18 15:50:29,640:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9662 (1.9662)
+2022-11-18 15:50:29,680:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7379 (1.9490)
+2022-11-18 15:50:29,990:INFO: Dataset: univ                Batch: 1/3	Loss 1.8119 (1.8119)
+2022-11-18 15:50:30,069:INFO: Dataset: univ                Batch: 2/3	Loss 1.8566 (1.8336)
+2022-11-18 15:50:30,145:INFO: Dataset: univ                Batch: 3/3	Loss 1.7766 (1.8143)
+2022-11-18 15:50:30,451:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8669 (1.8669)
+2022-11-18 15:50:30,500:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9419 (1.8857)
+2022-11-18 15:50:30,812:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8592 (1.8592)
+2022-11-18 15:50:30,892:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8845 (1.8719)
+2022-11-18 15:50:30,971:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8383 (1.8605)
+2022-11-18 15:50:31,051:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8637 (1.8613)
+2022-11-18 15:50:31,130:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7320 (1.8365)
+2022-11-18 15:50:31,185:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_563.pth.tar
+2022-11-18 15:50:31,185:INFO: 
+===> EPOCH: 564 (P2)
+2022-11-18 15:50:31,186:INFO: - Computing loss (training)
+2022-11-18 15:50:31,582:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0219 (2.0219)
+2022-11-18 15:50:31,771:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9739 (1.9982)
+2022-11-18 15:50:31,952:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7911 (1.9290)
+2022-11-18 15:50:32,110:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9203 (1.9275)
+2022-11-18 15:50:32,660:INFO: Dataset: univ                Batch:  1/15	Loss 1.8331 (1.8331)
+2022-11-18 15:50:32,900:INFO: Dataset: univ                Batch:  2/15	Loss 1.8576 (1.8456)
+2022-11-18 15:50:33,080:INFO: Dataset: univ                Batch:  3/15	Loss 1.8214 (1.8382)
+2022-11-18 15:50:33,261:INFO: Dataset: univ                Batch:  4/15	Loss 1.8948 (1.8534)
+2022-11-18 15:50:33,460:INFO: Dataset: univ                Batch:  5/15	Loss 1.8333 (1.8494)
+2022-11-18 15:50:33,645:INFO: Dataset: univ                Batch:  6/15	Loss 1.8463 (1.8488)
+2022-11-18 15:50:33,845:INFO: Dataset: univ                Batch:  7/15	Loss 1.8437 (1.8482)
+2022-11-18 15:50:34,046:INFO: Dataset: univ                Batch:  8/15	Loss 1.8631 (1.8501)
+2022-11-18 15:50:34,232:INFO: Dataset: univ                Batch:  9/15	Loss 1.8864 (1.8542)
+2022-11-18 15:50:34,420:INFO: Dataset: univ                Batch: 10/15	Loss 1.8806 (1.8567)
+2022-11-18 15:50:34,594:INFO: Dataset: univ                Batch: 11/15	Loss 1.8814 (1.8591)
+2022-11-18 15:50:34,772:INFO: Dataset: univ                Batch: 12/15	Loss 1.8358 (1.8573)
+2022-11-18 15:50:34,939:INFO: Dataset: univ                Batch: 13/15	Loss 1.8810 (1.8593)
+2022-11-18 15:50:35,102:INFO: Dataset: univ                Batch: 14/15	Loss 1.8591 (1.8593)
+2022-11-18 15:50:35,187:INFO: Dataset: univ                Batch: 15/15	Loss 1.9470 (1.8605)
+2022-11-18 15:50:35,578:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8250 (1.8250)
+2022-11-18 15:50:35,731:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8225 (1.8237)
+2022-11-18 15:50:35,888:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9145 (1.8535)
+2022-11-18 15:50:36,058:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8850 (1.8621)
+2022-11-18 15:50:36,209:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9573 (1.8805)
+2022-11-18 15:50:36,365:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0181 (1.9075)
+2022-11-18 15:50:36,529:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8795 (1.9034)
+2022-11-18 15:50:36,683:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8009 (1.8921)
+2022-11-18 15:50:37,146:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9303 (1.9303)
+2022-11-18 15:50:37,313:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8970 (1.9134)
+2022-11-18 15:50:37,466:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8108 (1.8789)
+2022-11-18 15:50:37,620:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8593 (1.8738)
+2022-11-18 15:50:37,772:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7689 (1.8561)
+2022-11-18 15:50:37,939:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7631 (1.8402)
+2022-11-18 15:50:38,092:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8797 (1.8461)
+2022-11-18 15:50:38,246:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8636 (1.8482)
+2022-11-18 15:50:38,400:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8421 (1.8475)
+2022-11-18 15:50:38,548:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8152 (1.8440)
+2022-11-18 15:50:38,705:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8339 (1.8430)
+2022-11-18 15:50:38,870:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8507 (1.8437)
+2022-11-18 15:50:39,038:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9798 (1.8541)
+2022-11-18 15:50:39,215:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9701 (1.8627)
+2022-11-18 15:50:39,375:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8869 (1.8644)
+2022-11-18 15:50:39,526:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7982 (1.8605)
+2022-11-18 15:50:39,677:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8841 (1.8620)
+2022-11-18 15:50:39,816:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8141 (1.8595)
+2022-11-18 15:50:39,868:INFO: - Computing loss (validation)
+2022-11-18 15:50:40,138:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9737 (1.9737)
+2022-11-18 15:50:40,174:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7598 (1.9606)
+2022-11-18 15:50:40,489:INFO: Dataset: univ                Batch: 1/3	Loss 1.8798 (1.8798)
+2022-11-18 15:50:40,569:INFO: Dataset: univ                Batch: 2/3	Loss 1.7925 (1.8355)
+2022-11-18 15:50:40,642:INFO: Dataset: univ                Batch: 3/3	Loss 1.8561 (1.8424)
+2022-11-18 15:50:40,955:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9319 (1.9319)
+2022-11-18 15:50:41,003:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7737 (1.8923)
+2022-11-18 15:50:41,312:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8773 (1.8773)
+2022-11-18 15:50:41,390:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7969 (1.8367)
+2022-11-18 15:50:41,468:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8263 (1.8334)
+2022-11-18 15:50:41,545:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8599 (1.8399)
+2022-11-18 15:50:41,619:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8455 (1.8409)
+2022-11-18 15:50:41,671:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_564.pth.tar
+2022-11-18 15:50:41,671:INFO: 
+===> EPOCH: 565 (P2)
+2022-11-18 15:50:41,672:INFO: - Computing loss (training)
+2022-11-18 15:50:42,014:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8870 (1.8870)
+2022-11-18 15:50:42,174:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9644 (1.9251)
+2022-11-18 15:50:42,338:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8138 (1.8875)
+2022-11-18 15:50:42,461:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9298 (1.8943)
+2022-11-18 15:50:42,874:INFO: Dataset: univ                Batch:  1/15	Loss 1.8375 (1.8375)
+2022-11-18 15:50:43,049:INFO: Dataset: univ                Batch:  2/15	Loss 1.8435 (1.8406)
+2022-11-18 15:50:43,212:INFO: Dataset: univ                Batch:  3/15	Loss 1.7974 (1.8261)
+2022-11-18 15:50:43,370:INFO: Dataset: univ                Batch:  4/15	Loss 1.8416 (1.8299)
+2022-11-18 15:50:43,530:INFO: Dataset: univ                Batch:  5/15	Loss 1.8434 (1.8326)
+2022-11-18 15:50:43,690:INFO: Dataset: univ                Batch:  6/15	Loss 1.8294 (1.8320)
+2022-11-18 15:50:43,849:INFO: Dataset: univ                Batch:  7/15	Loss 1.8446 (1.8338)
+2022-11-18 15:50:44,015:INFO: Dataset: univ                Batch:  8/15	Loss 1.8402 (1.8346)
+2022-11-18 15:50:44,173:INFO: Dataset: univ                Batch:  9/15	Loss 1.8347 (1.8346)
+2022-11-18 15:50:44,328:INFO: Dataset: univ                Batch: 10/15	Loss 1.8516 (1.8362)
+2022-11-18 15:50:44,498:INFO: Dataset: univ                Batch: 11/15	Loss 1.8990 (1.8414)
+2022-11-18 15:50:44,657:INFO: Dataset: univ                Batch: 12/15	Loss 1.8393 (1.8412)
+2022-11-18 15:50:44,816:INFO: Dataset: univ                Batch: 13/15	Loss 1.8365 (1.8408)
+2022-11-18 15:50:44,971:INFO: Dataset: univ                Batch: 14/15	Loss 1.8130 (1.8390)
+2022-11-18 15:50:45,052:INFO: Dataset: univ                Batch: 15/15	Loss 1.8217 (1.8387)
+2022-11-18 15:50:45,448:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7579 (1.7579)
+2022-11-18 15:50:45,601:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8239 (1.7910)
+2022-11-18 15:50:45,753:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9382 (1.8402)
+2022-11-18 15:50:45,907:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9107 (1.8545)
+2022-11-18 15:50:46,067:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8142 (1.8457)
+2022-11-18 15:50:46,226:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8573 (1.8478)
+2022-11-18 15:50:46,380:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9150 (1.8584)
+2022-11-18 15:50:46,520:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8048 (1.8534)
+2022-11-18 15:50:46,929:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9580 (1.9580)
+2022-11-18 15:50:47,079:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9723 (1.9653)
+2022-11-18 15:50:47,237:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8942 (1.9391)
+2022-11-18 15:50:47,389:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8215 (1.9063)
+2022-11-18 15:50:47,543:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7994 (1.8832)
+2022-11-18 15:50:47,699:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8388 (1.8752)
+2022-11-18 15:50:47,850:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8092 (1.8656)
+2022-11-18 15:50:48,002:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8164 (1.8593)
+2022-11-18 15:50:48,153:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8226 (1.8555)
+2022-11-18 15:50:48,303:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7770 (1.8485)
+2022-11-18 15:50:48,453:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8645 (1.8499)
+2022-11-18 15:50:48,605:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7446 (1.8419)
+2022-11-18 15:50:48,763:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7071 (1.8325)
+2022-11-18 15:50:48,915:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8194 (1.8316)
+2022-11-18 15:50:49,067:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8895 (1.8350)
+2022-11-18 15:50:49,233:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6839 (1.8262)
+2022-11-18 15:50:49,386:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8840 (1.8296)
+2022-11-18 15:50:49,529:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8066 (1.8286)
+2022-11-18 15:50:49,574:INFO: - Computing loss (validation)
+2022-11-18 15:50:49,850:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7881 (1.7881)
+2022-11-18 15:50:49,886:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0713 (1.8113)
+2022-11-18 15:50:50,199:INFO: Dataset: univ                Batch: 1/3	Loss 1.8666 (1.8666)
+2022-11-18 15:50:50,281:INFO: Dataset: univ                Batch: 2/3	Loss 1.8714 (1.8691)
+2022-11-18 15:50:50,357:INFO: Dataset: univ                Batch: 3/3	Loss 1.8685 (1.8689)
+2022-11-18 15:50:50,668:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8774 (1.8774)
+2022-11-18 15:50:50,716:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7656 (1.8504)
+2022-11-18 15:50:51,030:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8588 (1.8588)
+2022-11-18 15:50:51,111:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9318 (1.8951)
+2022-11-18 15:50:51,194:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7789 (1.8575)
+2022-11-18 15:50:51,274:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9123 (1.8710)
+2022-11-18 15:50:51,353:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8614 (1.8691)
+2022-11-18 15:50:51,409:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_565.pth.tar
+2022-11-18 15:50:51,409:INFO: 
+===> EPOCH: 566 (P2)
+2022-11-18 15:50:51,410:INFO: - Computing loss (training)
+2022-11-18 15:50:51,769:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8091 (1.8091)
+2022-11-18 15:50:51,921:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8932 (1.8508)
+2022-11-18 15:50:52,071:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8989 (1.8662)
+2022-11-18 15:50:52,199:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8064 (1.8570)
+2022-11-18 15:50:52,625:INFO: Dataset: univ                Batch:  1/15	Loss 1.8627 (1.8627)
+2022-11-18 15:50:52,791:INFO: Dataset: univ                Batch:  2/15	Loss 1.7920 (1.8292)
+2022-11-18 15:50:52,959:INFO: Dataset: univ                Batch:  3/15	Loss 1.8470 (1.8355)
+2022-11-18 15:50:53,134:INFO: Dataset: univ                Batch:  4/15	Loss 1.8098 (1.8295)
+2022-11-18 15:50:53,303:INFO: Dataset: univ                Batch:  5/15	Loss 1.8733 (1.8382)
+2022-11-18 15:50:53,478:INFO: Dataset: univ                Batch:  6/15	Loss 1.8181 (1.8349)
+2022-11-18 15:50:53,655:INFO: Dataset: univ                Batch:  7/15	Loss 1.8718 (1.8408)
+2022-11-18 15:50:53,823:INFO: Dataset: univ                Batch:  8/15	Loss 1.8239 (1.8389)
+2022-11-18 15:50:53,998:INFO: Dataset: univ                Batch:  9/15	Loss 1.8417 (1.8392)
+2022-11-18 15:50:54,170:INFO: Dataset: univ                Batch: 10/15	Loss 1.8673 (1.8419)
+2022-11-18 15:50:54,337:INFO: Dataset: univ                Batch: 11/15	Loss 1.8723 (1.8445)
+2022-11-18 15:50:54,504:INFO: Dataset: univ                Batch: 12/15	Loss 1.7914 (1.8402)
+2022-11-18 15:50:54,670:INFO: Dataset: univ                Batch: 13/15	Loss 1.8576 (1.8415)
+2022-11-18 15:50:54,835:INFO: Dataset: univ                Batch: 14/15	Loss 1.7745 (1.8374)
+2022-11-18 15:50:54,921:INFO: Dataset: univ                Batch: 15/15	Loss 1.7820 (1.8367)
+2022-11-18 15:50:55,336:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8636 (1.8636)
+2022-11-18 15:50:55,498:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8412 (1.8515)
+2022-11-18 15:50:55,661:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8647 (1.8560)
+2022-11-18 15:50:55,821:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8309 (1.8504)
+2022-11-18 15:50:55,983:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8283 (1.8465)
+2022-11-18 15:50:56,152:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9096 (1.8569)
+2022-11-18 15:50:56,311:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7555 (1.8438)
+2022-11-18 15:50:56,458:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9460 (1.8546)
+2022-11-18 15:50:56,885:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9289 (1.9289)
+2022-11-18 15:50:57,037:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8344 (1.8804)
+2022-11-18 15:50:57,194:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8440 (1.8675)
+2022-11-18 15:50:57,346:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9276 (1.8817)
+2022-11-18 15:50:57,499:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8745 (1.8804)
+2022-11-18 15:50:57,655:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8165 (1.8694)
+2022-11-18 15:50:57,807:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8416 (1.8649)
+2022-11-18 15:50:57,959:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8607 (1.8644)
+2022-11-18 15:50:58,110:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8760 (1.8657)
+2022-11-18 15:50:58,262:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9041 (1.8696)
+2022-11-18 15:50:58,414:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7771 (1.8622)
+2022-11-18 15:50:58,567:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7833 (1.8558)
+2022-11-18 15:50:58,720:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8632 (1.8564)
+2022-11-18 15:50:58,874:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8122 (1.8534)
+2022-11-18 15:50:59,028:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7996 (1.8499)
+2022-11-18 15:50:59,181:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8257 (1.8485)
+2022-11-18 15:50:59,335:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8434 (1.8482)
+2022-11-18 15:50:59,475:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7547 (1.8441)
+2022-11-18 15:50:59,520:INFO: - Computing loss (validation)
+2022-11-18 15:50:59,780:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9147 (1.9147)
+2022-11-18 15:50:59,815:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7686 (1.9037)
+2022-11-18 15:51:00,122:INFO: Dataset: univ                Batch: 1/3	Loss 1.9181 (1.9181)
+2022-11-18 15:51:00,197:INFO: Dataset: univ                Batch: 2/3	Loss 1.8751 (1.8983)
+2022-11-18 15:51:00,271:INFO: Dataset: univ                Batch: 3/3	Loss 1.9146 (1.9038)
+2022-11-18 15:51:00,570:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8613 (1.8613)
+2022-11-18 15:51:00,615:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7495 (1.8354)
+2022-11-18 15:51:00,924:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8803 (1.8803)
+2022-11-18 15:51:01,000:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7999 (1.8424)
+2022-11-18 15:51:01,086:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8786 (1.8549)
+2022-11-18 15:51:01,162:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8579 (1.8557)
+2022-11-18 15:51:01,235:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8682 (1.8582)
+2022-11-18 15:51:01,286:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_566.pth.tar
+2022-11-18 15:51:01,287:INFO: 
+===> EPOCH: 567 (P2)
+2022-11-18 15:51:01,287:INFO: - Computing loss (training)
+2022-11-18 15:51:01,646:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8821 (1.8821)
+2022-11-18 15:51:01,802:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8421 (1.8613)
+2022-11-18 15:51:01,951:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8431 (1.8553)
+2022-11-18 15:51:02,066:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8191 (1.8492)
+2022-11-18 15:51:02,515:INFO: Dataset: univ                Batch:  1/15	Loss 1.8062 (1.8062)
+2022-11-18 15:51:02,691:INFO: Dataset: univ                Batch:  2/15	Loss 1.8727 (1.8383)
+2022-11-18 15:51:02,870:INFO: Dataset: univ                Batch:  3/15	Loss 1.8170 (1.8312)
+2022-11-18 15:51:03,039:INFO: Dataset: univ                Batch:  4/15	Loss 1.7834 (1.8200)
+2022-11-18 15:51:03,207:INFO: Dataset: univ                Batch:  5/15	Loss 1.8323 (1.8225)
+2022-11-18 15:51:03,374:INFO: Dataset: univ                Batch:  6/15	Loss 1.8644 (1.8294)
+2022-11-18 15:51:03,539:INFO: Dataset: univ                Batch:  7/15	Loss 1.8247 (1.8287)
+2022-11-18 15:51:03,706:INFO: Dataset: univ                Batch:  8/15	Loss 1.8199 (1.8276)
+2022-11-18 15:51:03,875:INFO: Dataset: univ                Batch:  9/15	Loss 1.8125 (1.8259)
+2022-11-18 15:51:04,045:INFO: Dataset: univ                Batch: 10/15	Loss 1.8516 (1.8287)
+2022-11-18 15:51:04,218:INFO: Dataset: univ                Batch: 11/15	Loss 1.7953 (1.8257)
+2022-11-18 15:51:04,387:INFO: Dataset: univ                Batch: 12/15	Loss 1.8449 (1.8274)
+2022-11-18 15:51:04,554:INFO: Dataset: univ                Batch: 13/15	Loss 1.8146 (1.8264)
+2022-11-18 15:51:04,720:INFO: Dataset: univ                Batch: 14/15	Loss 1.8369 (1.8271)
+2022-11-18 15:51:04,806:INFO: Dataset: univ                Batch: 15/15	Loss 1.9642 (1.8286)
+2022-11-18 15:51:05,196:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8505 (1.8505)
+2022-11-18 15:51:05,347:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7449 (1.7949)
+2022-11-18 15:51:05,500:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9199 (1.8350)
+2022-11-18 15:51:05,668:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9257 (1.8568)
+2022-11-18 15:51:05,836:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7671 (1.8376)
+2022-11-18 15:51:05,997:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9368 (1.8544)
+2022-11-18 15:51:06,148:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8010 (1.8465)
+2022-11-18 15:51:06,286:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8819 (1.8506)
+2022-11-18 15:51:06,676:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7689 (1.7689)
+2022-11-18 15:51:06,828:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7552 (1.7617)
+2022-11-18 15:51:06,982:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8879 (1.8058)
+2022-11-18 15:51:07,131:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8547 (1.8180)
+2022-11-18 15:51:07,287:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8600 (1.8259)
+2022-11-18 15:51:07,438:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8405 (1.8284)
+2022-11-18 15:51:07,587:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9761 (1.8492)
+2022-11-18 15:51:07,736:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8740 (1.8523)
+2022-11-18 15:51:07,885:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8996 (1.8577)
+2022-11-18 15:51:08,033:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7276 (1.8446)
+2022-11-18 15:51:08,185:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8163 (1.8421)
+2022-11-18 15:51:08,335:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8428 (1.8422)
+2022-11-18 15:51:08,492:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7727 (1.8368)
+2022-11-18 15:51:08,647:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8410 (1.8370)
+2022-11-18 15:51:08,802:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9178 (1.8423)
+2022-11-18 15:51:08,956:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8820 (1.8447)
+2022-11-18 15:51:09,112:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8963 (1.8476)
+2022-11-18 15:51:09,256:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7634 (1.8435)
+2022-11-18 15:51:09,301:INFO: - Computing loss (validation)
+2022-11-18 15:51:09,572:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8711 (1.8711)
+2022-11-18 15:51:09,608:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8820 (1.8719)
+2022-11-18 15:51:09,915:INFO: Dataset: univ                Batch: 1/3	Loss 1.8304 (1.8304)
+2022-11-18 15:51:09,992:INFO: Dataset: univ                Batch: 2/3	Loss 1.8318 (1.8311)
+2022-11-18 15:51:10,065:INFO: Dataset: univ                Batch: 3/3	Loss 1.8552 (1.8375)
+2022-11-18 15:51:10,372:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8577 (1.8577)
+2022-11-18 15:51:10,417:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7824 (1.8381)
+2022-11-18 15:51:10,726:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8462 (1.8462)
+2022-11-18 15:51:10,802:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8219 (1.8345)
+2022-11-18 15:51:10,878:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8336 (1.8342)
+2022-11-18 15:51:10,955:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8581 (1.8400)
+2022-11-18 15:51:11,036:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7499 (1.8214)
+2022-11-18 15:51:11,090:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_567.pth.tar
+2022-11-18 15:51:11,090:INFO: 
+===> EPOCH: 568 (P2)
+2022-11-18 15:51:11,091:INFO: - Computing loss (training)
+2022-11-18 15:51:11,455:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9494 (1.9494)
+2022-11-18 15:51:11,619:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9293 (1.9393)
+2022-11-18 15:51:11,777:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8291 (1.9016)
+2022-11-18 15:51:11,898:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8835 (1.8986)
+2022-11-18 15:51:12,349:INFO: Dataset: univ                Batch:  1/15	Loss 1.8662 (1.8662)
+2022-11-18 15:51:12,515:INFO: Dataset: univ                Batch:  2/15	Loss 1.8732 (1.8698)
+2022-11-18 15:51:12,675:INFO: Dataset: univ                Batch:  3/15	Loss 1.8088 (1.8496)
+2022-11-18 15:51:12,833:INFO: Dataset: univ                Batch:  4/15	Loss 1.8598 (1.8522)
+2022-11-18 15:51:13,006:INFO: Dataset: univ                Batch:  5/15	Loss 1.8255 (1.8466)
+2022-11-18 15:51:13,167:INFO: Dataset: univ                Batch:  6/15	Loss 1.7998 (1.8396)
+2022-11-18 15:51:13,338:INFO: Dataset: univ                Batch:  7/15	Loss 1.8576 (1.8421)
+2022-11-18 15:51:13,502:INFO: Dataset: univ                Batch:  8/15	Loss 1.8566 (1.8439)
+2022-11-18 15:51:13,662:INFO: Dataset: univ                Batch:  9/15	Loss 1.8308 (1.8425)
+2022-11-18 15:51:13,833:INFO: Dataset: univ                Batch: 10/15	Loss 1.8902 (1.8473)
+2022-11-18 15:51:13,999:INFO: Dataset: univ                Batch: 11/15	Loss 1.8882 (1.8511)
+2022-11-18 15:51:14,162:INFO: Dataset: univ                Batch: 12/15	Loss 1.9045 (1.8559)
+2022-11-18 15:51:14,345:INFO: Dataset: univ                Batch: 13/15	Loss 1.8625 (1.8564)
+2022-11-18 15:51:14,517:INFO: Dataset: univ                Batch: 14/15	Loss 1.8544 (1.8563)
+2022-11-18 15:51:14,605:INFO: Dataset: univ                Batch: 15/15	Loss 1.8193 (1.8557)
+2022-11-18 15:51:15,009:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9014 (1.9014)
+2022-11-18 15:51:15,174:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8381 (1.8701)
+2022-11-18 15:51:15,366:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9459 (1.8930)
+2022-11-18 15:51:15,548:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7669 (1.8606)
+2022-11-18 15:51:15,740:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8001 (1.8483)
+2022-11-18 15:51:15,941:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7791 (1.8362)
+2022-11-18 15:51:16,131:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7894 (1.8293)
+2022-11-18 15:51:16,302:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8928 (1.8360)
+2022-11-18 15:51:16,744:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8811 (1.8811)
+2022-11-18 15:51:16,901:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8967 (1.8891)
+2022-11-18 15:51:17,065:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8762 (1.8849)
+2022-11-18 15:51:17,225:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8543 (1.8770)
+2022-11-18 15:51:17,387:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9142 (1.8844)
+2022-11-18 15:51:17,549:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8560 (1.8799)
+2022-11-18 15:51:17,720:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7312 (1.8612)
+2022-11-18 15:51:17,901:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7431 (1.8466)
+2022-11-18 15:51:18,078:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8482 (1.8468)
+2022-11-18 15:51:18,247:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8830 (1.8508)
+2022-11-18 15:51:18,414:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8907 (1.8546)
+2022-11-18 15:51:18,584:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8119 (1.8508)
+2022-11-18 15:51:18,748:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8806 (1.8528)
+2022-11-18 15:51:18,919:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8320 (1.8514)
+2022-11-18 15:51:19,090:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8301 (1.8499)
+2022-11-18 15:51:19,241:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7160 (1.8411)
+2022-11-18 15:51:19,393:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7185 (1.8348)
+2022-11-18 15:51:19,543:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7778 (1.8318)
+2022-11-18 15:51:19,600:INFO: - Computing loss (validation)
+2022-11-18 15:51:19,891:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8219 (1.8219)
+2022-11-18 15:51:19,927:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6106 (1.8046)
+2022-11-18 15:51:20,265:INFO: Dataset: univ                Batch: 1/3	Loss 1.8209 (1.8209)
+2022-11-18 15:51:20,349:INFO: Dataset: univ                Batch: 2/3	Loss 1.8268 (1.8238)
+2022-11-18 15:51:20,433:INFO: Dataset: univ                Batch: 3/3	Loss 1.8691 (1.8389)
+2022-11-18 15:51:20,775:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0173 (2.0173)
+2022-11-18 15:51:20,820:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6439 (1.9321)
+2022-11-18 15:51:21,133:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9309 (1.9309)
+2022-11-18 15:51:21,214:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7999 (1.8648)
+2022-11-18 15:51:21,296:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8353 (1.8544)
+2022-11-18 15:51:21,376:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9112 (1.8679)
+2022-11-18 15:51:21,455:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7755 (1.8483)
+2022-11-18 15:51:21,509:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_568.pth.tar
+2022-11-18 15:51:21,509:INFO: 
+===> EPOCH: 569 (P2)
+2022-11-18 15:51:21,510:INFO: - Computing loss (training)
+2022-11-18 15:51:21,854:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9386 (1.9386)
+2022-11-18 15:51:22,033:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0820 (2.0073)
+2022-11-18 15:51:22,209:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8598 (1.9589)
+2022-11-18 15:51:22,333:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7809 (1.9284)
+2022-11-18 15:51:22,762:INFO: Dataset: univ                Batch:  1/15	Loss 1.8542 (1.8542)
+2022-11-18 15:51:22,936:INFO: Dataset: univ                Batch:  2/15	Loss 1.8017 (1.8269)
+2022-11-18 15:51:23,094:INFO: Dataset: univ                Batch:  3/15	Loss 1.8866 (1.8481)
+2022-11-18 15:51:23,266:INFO: Dataset: univ                Batch:  4/15	Loss 1.8250 (1.8422)
+2022-11-18 15:51:23,442:INFO: Dataset: univ                Batch:  5/15	Loss 1.8567 (1.8452)
+2022-11-18 15:51:23,606:INFO: Dataset: univ                Batch:  6/15	Loss 1.8125 (1.8404)
+2022-11-18 15:51:23,782:INFO: Dataset: univ                Batch:  7/15	Loss 1.8862 (1.8470)
+2022-11-18 15:51:23,957:INFO: Dataset: univ                Batch:  8/15	Loss 1.8237 (1.8441)
+2022-11-18 15:51:24,130:INFO: Dataset: univ                Batch:  9/15	Loss 1.8515 (1.8449)
+2022-11-18 15:51:24,299:INFO: Dataset: univ                Batch: 10/15	Loss 1.9222 (1.8525)
+2022-11-18 15:51:24,464:INFO: Dataset: univ                Batch: 11/15	Loss 1.8286 (1.8504)
+2022-11-18 15:51:24,641:INFO: Dataset: univ                Batch: 12/15	Loss 1.8445 (1.8500)
+2022-11-18 15:51:24,804:INFO: Dataset: univ                Batch: 13/15	Loss 1.8878 (1.8532)
+2022-11-18 15:51:24,981:INFO: Dataset: univ                Batch: 14/15	Loss 1.8623 (1.8538)
+2022-11-18 15:51:25,076:INFO: Dataset: univ                Batch: 15/15	Loss 1.8535 (1.8538)
+2022-11-18 15:51:25,502:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8889 (1.8889)
+2022-11-18 15:51:25,680:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8623 (1.8756)
+2022-11-18 15:51:25,849:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9025 (1.8841)
+2022-11-18 15:51:26,003:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8944 (1.8867)
+2022-11-18 15:51:26,157:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8247 (1.8744)
+2022-11-18 15:51:26,310:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9484 (1.8852)
+2022-11-18 15:51:26,462:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9244 (1.8903)
+2022-11-18 15:51:26,601:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8457 (1.8856)
+2022-11-18 15:51:27,015:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8256 (1.8256)
+2022-11-18 15:51:27,175:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9785 (1.9035)
+2022-11-18 15:51:27,333:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8249 (1.8777)
+2022-11-18 15:51:27,493:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8462 (1.8698)
+2022-11-18 15:51:27,653:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9402 (1.8843)
+2022-11-18 15:51:27,817:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8628 (1.8807)
+2022-11-18 15:51:27,979:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9001 (1.8834)
+2022-11-18 15:51:28,140:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8780 (1.8827)
+2022-11-18 15:51:28,291:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8247 (1.8761)
+2022-11-18 15:51:28,441:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8164 (1.8703)
+2022-11-18 15:51:28,593:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7971 (1.8638)
+2022-11-18 15:51:28,742:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8612 (1.8636)
+2022-11-18 15:51:28,898:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8807 (1.8651)
+2022-11-18 15:51:29,058:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8627 (1.8649)
+2022-11-18 15:51:29,220:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8253 (1.8624)
+2022-11-18 15:51:29,376:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8054 (1.8590)
+2022-11-18 15:51:29,528:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8310 (1.8574)
+2022-11-18 15:51:29,676:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8039 (1.8548)
+2022-11-18 15:51:29,725:INFO: - Computing loss (validation)
+2022-11-18 15:51:29,998:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9520 (1.9520)
+2022-11-18 15:51:30,034:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9164 (1.9491)
+2022-11-18 15:51:30,387:INFO: Dataset: univ                Batch: 1/3	Loss 1.8629 (1.8629)
+2022-11-18 15:51:30,470:INFO: Dataset: univ                Batch: 2/3	Loss 1.8420 (1.8515)
+2022-11-18 15:51:30,545:INFO: Dataset: univ                Batch: 3/3	Loss 1.8505 (1.8512)
+2022-11-18 15:51:30,905:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8912 (1.8912)
+2022-11-18 15:51:30,958:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7484 (1.8531)
+2022-11-18 15:51:31,342:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7822 (1.7822)
+2022-11-18 15:51:31,432:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7506 (1.7672)
+2022-11-18 15:51:31,524:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9063 (1.8145)
+2022-11-18 15:51:31,623:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7814 (1.8059)
+2022-11-18 15:51:31,712:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8508 (1.8148)
+2022-11-18 15:51:31,786:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_569.pth.tar
+2022-11-18 15:51:31,786:INFO: 
+===> EPOCH: 570 (P2)
+2022-11-18 15:51:31,787:INFO: - Computing loss (training)
+2022-11-18 15:51:32,194:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9513 (1.9513)
+2022-11-18 15:51:32,371:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9354 (1.9437)
+2022-11-18 15:51:32,529:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9555 (1.9475)
+2022-11-18 15:51:32,747:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9752 (1.9524)
+2022-11-18 15:51:33,173:INFO: Dataset: univ                Batch:  1/15	Loss 1.8079 (1.8079)
+2022-11-18 15:51:33,333:INFO: Dataset: univ                Batch:  2/15	Loss 1.8598 (1.8358)
+2022-11-18 15:51:33,498:INFO: Dataset: univ                Batch:  3/15	Loss 1.8307 (1.8340)
+2022-11-18 15:51:33,666:INFO: Dataset: univ                Batch:  4/15	Loss 1.8572 (1.8403)
+2022-11-18 15:51:33,841:INFO: Dataset: univ                Batch:  5/15	Loss 1.8051 (1.8329)
+2022-11-18 15:51:34,021:INFO: Dataset: univ                Batch:  6/15	Loss 1.8569 (1.8367)
+2022-11-18 15:51:34,201:INFO: Dataset: univ                Batch:  7/15	Loss 1.8045 (1.8319)
+2022-11-18 15:51:34,378:INFO: Dataset: univ                Batch:  8/15	Loss 1.8161 (1.8298)
+2022-11-18 15:51:34,547:INFO: Dataset: univ                Batch:  9/15	Loss 1.8504 (1.8320)
+2022-11-18 15:51:34,713:INFO: Dataset: univ                Batch: 10/15	Loss 1.8422 (1.8330)
+2022-11-18 15:51:34,879:INFO: Dataset: univ                Batch: 11/15	Loss 1.8453 (1.8341)
+2022-11-18 15:51:35,050:INFO: Dataset: univ                Batch: 12/15	Loss 1.8615 (1.8362)
+2022-11-18 15:51:35,216:INFO: Dataset: univ                Batch: 13/15	Loss 1.8006 (1.8334)
+2022-11-18 15:51:35,382:INFO: Dataset: univ                Batch: 14/15	Loss 1.8635 (1.8357)
+2022-11-18 15:51:35,469:INFO: Dataset: univ                Batch: 15/15	Loss 1.9218 (1.8368)
+2022-11-18 15:51:35,909:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7825 (1.7825)
+2022-11-18 15:51:36,069:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8949 (1.8417)
+2022-11-18 15:51:36,225:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8554 (1.8463)
+2022-11-18 15:51:36,395:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7941 (1.8341)
+2022-11-18 15:51:36,561:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9253 (1.8515)
+2022-11-18 15:51:36,714:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7257 (1.8296)
+2022-11-18 15:51:36,865:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8563 (1.8333)
+2022-11-18 15:51:37,003:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7488 (1.8245)
+2022-11-18 15:51:37,436:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8767 (1.8767)
+2022-11-18 15:51:37,605:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8158 (1.8463)
+2022-11-18 15:51:37,766:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8138 (1.8353)
+2022-11-18 15:51:37,924:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8793 (1.8472)
+2022-11-18 15:51:38,083:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7645 (1.8308)
+2022-11-18 15:51:38,246:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8774 (1.8381)
+2022-11-18 15:51:38,412:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8207 (1.8356)
+2022-11-18 15:51:38,576:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8852 (1.8412)
+2022-11-18 15:51:38,736:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9070 (1.8490)
+2022-11-18 15:51:38,897:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8671 (1.8507)
+2022-11-18 15:51:39,060:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7306 (1.8401)
+2022-11-18 15:51:39,223:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8206 (1.8385)
+2022-11-18 15:51:39,383:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8224 (1.8372)
+2022-11-18 15:51:39,543:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8516 (1.8383)
+2022-11-18 15:51:39,704:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8756 (1.8406)
+2022-11-18 15:51:39,863:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9060 (1.8453)
+2022-11-18 15:51:40,022:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8138 (1.8434)
+2022-11-18 15:51:40,169:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8530 (1.8438)
+2022-11-18 15:51:40,215:INFO: - Computing loss (validation)
+2022-11-18 15:51:40,495:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8970 (1.8970)
+2022-11-18 15:51:40,533:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9551 (1.9017)
+2022-11-18 15:51:40,843:INFO: Dataset: univ                Batch: 1/3	Loss 1.8319 (1.8319)
+2022-11-18 15:51:40,923:INFO: Dataset: univ                Batch: 2/3	Loss 1.8941 (1.8597)
+2022-11-18 15:51:40,995:INFO: Dataset: univ                Batch: 3/3	Loss 1.8539 (1.8581)
+2022-11-18 15:51:41,298:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8406 (1.8406)
+2022-11-18 15:51:41,344:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9510 (1.8690)
+2022-11-18 15:51:41,658:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8824 (1.8824)
+2022-11-18 15:51:41,734:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8866 (1.8846)
+2022-11-18 15:51:41,808:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9023 (1.8906)
+2022-11-18 15:51:41,883:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8762 (1.8870)
+2022-11-18 15:51:41,957:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8048 (1.8702)
+2022-11-18 15:51:42,011:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_570.pth.tar
+2022-11-18 15:51:42,011:INFO: 
+===> EPOCH: 571 (P2)
+2022-11-18 15:51:42,012:INFO: - Computing loss (training)
+2022-11-18 15:51:42,362:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8072 (1.8072)
+2022-11-18 15:51:42,523:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9143 (1.8602)
+2022-11-18 15:51:42,676:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8271 (1.8490)
+2022-11-18 15:51:42,793:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8349 (1.8466)
+2022-11-18 15:51:43,191:INFO: Dataset: univ                Batch:  1/15	Loss 1.8702 (1.8702)
+2022-11-18 15:51:43,354:INFO: Dataset: univ                Batch:  2/15	Loss 1.8436 (1.8572)
+2022-11-18 15:51:43,522:INFO: Dataset: univ                Batch:  3/15	Loss 1.8397 (1.8516)
+2022-11-18 15:51:43,684:INFO: Dataset: univ                Batch:  4/15	Loss 1.8210 (1.8440)
+2022-11-18 15:51:43,844:INFO: Dataset: univ                Batch:  5/15	Loss 1.8463 (1.8445)
+2022-11-18 15:51:44,005:INFO: Dataset: univ                Batch:  6/15	Loss 1.8332 (1.8427)
+2022-11-18 15:51:44,163:INFO: Dataset: univ                Batch:  7/15	Loss 1.8363 (1.8416)
+2022-11-18 15:51:44,320:INFO: Dataset: univ                Batch:  8/15	Loss 1.8730 (1.8449)
+2022-11-18 15:51:44,479:INFO: Dataset: univ                Batch:  9/15	Loss 1.8244 (1.8429)
+2022-11-18 15:51:44,656:INFO: Dataset: univ                Batch: 10/15	Loss 1.8370 (1.8422)
+2022-11-18 15:51:44,834:INFO: Dataset: univ                Batch: 11/15	Loss 1.8098 (1.8393)
+2022-11-18 15:51:45,000:INFO: Dataset: univ                Batch: 12/15	Loss 1.8462 (1.8399)
+2022-11-18 15:51:45,168:INFO: Dataset: univ                Batch: 13/15	Loss 1.8608 (1.8416)
+2022-11-18 15:51:45,337:INFO: Dataset: univ                Batch: 14/15	Loss 1.8162 (1.8397)
+2022-11-18 15:51:45,427:INFO: Dataset: univ                Batch: 15/15	Loss 1.8195 (1.8394)
+2022-11-18 15:51:45,829:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8039 (1.8039)
+2022-11-18 15:51:45,987:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9450 (1.8767)
+2022-11-18 15:51:46,146:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8163 (1.8541)
+2022-11-18 15:51:46,309:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8854 (1.8621)
+2022-11-18 15:51:46,474:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8931 (1.8682)
+2022-11-18 15:51:46,630:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9202 (1.8770)
+2022-11-18 15:51:46,779:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9021 (1.8806)
+2022-11-18 15:51:46,914:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9232 (1.8857)
+2022-11-18 15:51:47,310:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8587 (1.8587)
+2022-11-18 15:51:47,463:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8690 (1.8642)
+2022-11-18 15:51:47,627:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8607 (1.8630)
+2022-11-18 15:51:47,788:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8375 (1.8562)
+2022-11-18 15:51:47,948:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7978 (1.8452)
+2022-11-18 15:51:48,105:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8349 (1.8436)
+2022-11-18 15:51:48,272:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8459 (1.8439)
+2022-11-18 15:51:48,425:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7632 (1.8343)
+2022-11-18 15:51:48,573:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8501 (1.8362)
+2022-11-18 15:51:48,721:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8502 (1.8376)
+2022-11-18 15:51:48,873:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9208 (1.8453)
+2022-11-18 15:51:49,019:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7977 (1.8411)
+2022-11-18 15:51:49,167:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8517 (1.8419)
+2022-11-18 15:51:49,315:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7691 (1.8369)
+2022-11-18 15:51:49,482:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8178 (1.8355)
+2022-11-18 15:51:49,651:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7804 (1.8319)
+2022-11-18 15:51:49,822:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8664 (1.8339)
+2022-11-18 15:51:49,971:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7749 (1.8308)
+2022-11-18 15:51:50,016:INFO: - Computing loss (validation)
+2022-11-18 15:51:50,298:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9335 (1.9335)
+2022-11-18 15:51:50,334:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6820 (1.9155)
+2022-11-18 15:51:50,671:INFO: Dataset: univ                Batch: 1/3	Loss 1.8125 (1.8125)
+2022-11-18 15:51:50,758:INFO: Dataset: univ                Batch: 2/3	Loss 1.8293 (1.8215)
+2022-11-18 15:51:50,835:INFO: Dataset: univ                Batch: 3/3	Loss 1.9314 (1.8555)
+2022-11-18 15:51:51,142:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8395 (1.8395)
+2022-11-18 15:51:51,189:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8291 (1.8369)
+2022-11-18 15:51:51,510:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9045 (1.9045)
+2022-11-18 15:51:51,586:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7659 (1.8343)
+2022-11-18 15:51:51,666:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8813 (1.8498)
+2022-11-18 15:51:51,743:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9554 (1.8760)
+2022-11-18 15:51:51,818:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7852 (1.8575)
+2022-11-18 15:51:51,871:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_571.pth.tar
+2022-11-18 15:51:51,871:INFO: 
+===> EPOCH: 572 (P2)
+2022-11-18 15:51:51,872:INFO: - Computing loss (training)
+2022-11-18 15:51:52,213:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7715 (1.7715)
+2022-11-18 15:51:52,366:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9423 (1.8571)
+2022-11-18 15:51:52,514:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9141 (1.8755)
+2022-11-18 15:51:52,630:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9922 (1.8957)
+2022-11-18 15:51:53,048:INFO: Dataset: univ                Batch:  1/15	Loss 1.8400 (1.8400)
+2022-11-18 15:51:53,214:INFO: Dataset: univ                Batch:  2/15	Loss 1.8592 (1.8490)
+2022-11-18 15:51:53,376:INFO: Dataset: univ                Batch:  3/15	Loss 1.8365 (1.8454)
+2022-11-18 15:51:53,537:INFO: Dataset: univ                Batch:  4/15	Loss 1.8164 (1.8381)
+2022-11-18 15:51:53,702:INFO: Dataset: univ                Batch:  5/15	Loss 1.8678 (1.8441)
+2022-11-18 15:51:53,871:INFO: Dataset: univ                Batch:  6/15	Loss 1.8069 (1.8375)
+2022-11-18 15:51:54,033:INFO: Dataset: univ                Batch:  7/15	Loss 1.8170 (1.8346)
+2022-11-18 15:51:54,192:INFO: Dataset: univ                Batch:  8/15	Loss 1.8421 (1.8355)
+2022-11-18 15:51:54,353:INFO: Dataset: univ                Batch:  9/15	Loss 1.8888 (1.8418)
+2022-11-18 15:51:54,513:INFO: Dataset: univ                Batch: 10/15	Loss 1.8475 (1.8424)
+2022-11-18 15:51:54,673:INFO: Dataset: univ                Batch: 11/15	Loss 1.8456 (1.8427)
+2022-11-18 15:51:54,833:INFO: Dataset: univ                Batch: 12/15	Loss 1.8999 (1.8474)
+2022-11-18 15:51:54,993:INFO: Dataset: univ                Batch: 13/15	Loss 1.8460 (1.8473)
+2022-11-18 15:51:55,161:INFO: Dataset: univ                Batch: 14/15	Loss 1.8564 (1.8480)
+2022-11-18 15:51:55,251:INFO: Dataset: univ                Batch: 15/15	Loss 1.8588 (1.8481)
+2022-11-18 15:51:55,649:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7974 (1.7974)
+2022-11-18 15:51:55,797:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8712 (1.8366)
+2022-11-18 15:51:55,950:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8437 (1.8388)
+2022-11-18 15:51:56,097:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8341 (1.8376)
+2022-11-18 15:51:56,244:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8807 (1.8461)
+2022-11-18 15:51:56,393:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8630 (1.8487)
+2022-11-18 15:51:56,540:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7649 (1.8360)
+2022-11-18 15:51:56,675:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7876 (1.8304)
+2022-11-18 15:51:57,091:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8216 (1.8216)
+2022-11-18 15:51:57,249:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7666 (1.7936)
+2022-11-18 15:51:57,408:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8031 (1.7970)
+2022-11-18 15:51:57,562:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8064 (1.7994)
+2022-11-18 15:51:57,716:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8354 (1.8063)
+2022-11-18 15:51:57,874:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7048 (1.7928)
+2022-11-18 15:51:58,026:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7872 (1.7920)
+2022-11-18 15:51:58,178:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8532 (1.7990)
+2022-11-18 15:51:58,334:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8254 (1.8020)
+2022-11-18 15:51:58,496:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7753 (1.7994)
+2022-11-18 15:51:58,661:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8579 (1.8040)
+2022-11-18 15:51:58,820:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7534 (1.8002)
+2022-11-18 15:51:58,974:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8378 (1.8031)
+2022-11-18 15:51:59,128:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7594 (1.7999)
+2022-11-18 15:51:59,282:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7717 (1.7980)
+2022-11-18 15:51:59,436:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7920 (1.7976)
+2022-11-18 15:51:59,589:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9095 (1.8045)
+2022-11-18 15:51:59,730:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7365 (1.8014)
+2022-11-18 15:51:59,777:INFO: - Computing loss (validation)
+2022-11-18 15:52:00,055:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8805 (1.8805)
+2022-11-18 15:52:00,091:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0914 (1.8963)
+2022-11-18 15:52:00,404:INFO: Dataset: univ                Batch: 1/3	Loss 1.8605 (1.8605)
+2022-11-18 15:52:00,481:INFO: Dataset: univ                Batch: 2/3	Loss 1.8990 (1.8819)
+2022-11-18 15:52:00,553:INFO: Dataset: univ                Batch: 3/3	Loss 1.8414 (1.8693)
+2022-11-18 15:52:00,869:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8665 (1.8665)
+2022-11-18 15:52:00,916:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8819 (1.8701)
+2022-11-18 15:52:01,270:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8817 (1.8817)
+2022-11-18 15:52:01,351:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9388 (1.9118)
+2022-11-18 15:52:01,433:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7978 (1.8716)
+2022-11-18 15:52:01,514:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9106 (1.8817)
+2022-11-18 15:52:01,593:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8327 (1.8725)
+2022-11-18 15:52:01,655:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_572.pth.tar
+2022-11-18 15:52:01,655:INFO: 
+===> EPOCH: 573 (P2)
+2022-11-18 15:52:01,656:INFO: - Computing loss (training)
+2022-11-18 15:52:01,994:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9010 (1.9010)
+2022-11-18 15:52:02,144:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9384 (1.9207)
+2022-11-18 15:52:02,293:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8190 (1.8863)
+2022-11-18 15:52:02,408:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8834 (1.8858)
+2022-11-18 15:52:02,829:INFO: Dataset: univ                Batch:  1/15	Loss 1.8502 (1.8502)
+2022-11-18 15:52:02,998:INFO: Dataset: univ                Batch:  2/15	Loss 1.8612 (1.8557)
+2022-11-18 15:52:03,169:INFO: Dataset: univ                Batch:  3/15	Loss 1.8495 (1.8536)
+2022-11-18 15:52:03,331:INFO: Dataset: univ                Batch:  4/15	Loss 1.8435 (1.8512)
+2022-11-18 15:52:03,491:INFO: Dataset: univ                Batch:  5/15	Loss 1.8994 (1.8604)
+2022-11-18 15:52:03,654:INFO: Dataset: univ                Batch:  6/15	Loss 1.8071 (1.8511)
+2022-11-18 15:52:03,810:INFO: Dataset: univ                Batch:  7/15	Loss 1.8292 (1.8477)
+2022-11-18 15:52:03,966:INFO: Dataset: univ                Batch:  8/15	Loss 1.8303 (1.8453)
+2022-11-18 15:52:04,122:INFO: Dataset: univ                Batch:  9/15	Loss 1.8255 (1.8434)
+2022-11-18 15:52:04,279:INFO: Dataset: univ                Batch: 10/15	Loss 1.7847 (1.8376)
+2022-11-18 15:52:04,437:INFO: Dataset: univ                Batch: 11/15	Loss 1.8647 (1.8401)
+2022-11-18 15:52:04,592:INFO: Dataset: univ                Batch: 12/15	Loss 1.8401 (1.8401)
+2022-11-18 15:52:04,748:INFO: Dataset: univ                Batch: 13/15	Loss 1.8147 (1.8381)
+2022-11-18 15:52:04,904:INFO: Dataset: univ                Batch: 14/15	Loss 1.8612 (1.8397)
+2022-11-18 15:52:04,988:INFO: Dataset: univ                Batch: 15/15	Loss 1.8236 (1.8395)
+2022-11-18 15:52:05,388:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8963 (1.8963)
+2022-11-18 15:52:05,543:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8110 (1.8554)
+2022-11-18 15:52:05,697:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8210 (1.8442)
+2022-11-18 15:52:05,848:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8289 (1.8404)
+2022-11-18 15:52:06,001:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8245 (1.8368)
+2022-11-18 15:52:06,154:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7678 (1.8257)
+2022-11-18 15:52:06,316:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7522 (1.8145)
+2022-11-18 15:52:06,465:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8475 (1.8180)
+2022-11-18 15:52:06,874:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8122 (1.8122)
+2022-11-18 15:52:07,031:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7498 (1.7832)
+2022-11-18 15:52:07,191:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9372 (1.8320)
+2022-11-18 15:52:07,343:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8219 (1.8294)
+2022-11-18 15:52:07,499:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7763 (1.8187)
+2022-11-18 15:52:07,655:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8227 (1.8194)
+2022-11-18 15:52:07,805:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7942 (1.8160)
+2022-11-18 15:52:07,953:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8447 (1.8197)
+2022-11-18 15:52:08,101:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8073 (1.8183)
+2022-11-18 15:52:08,252:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8240 (1.8189)
+2022-11-18 15:52:08,405:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9019 (1.8262)
+2022-11-18 15:52:08,561:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9759 (1.8375)
+2022-11-18 15:52:08,722:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8127 (1.8355)
+2022-11-18 15:52:08,874:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9211 (1.8419)
+2022-11-18 15:52:09,030:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8699 (1.8437)
+2022-11-18 15:52:09,182:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7681 (1.8388)
+2022-11-18 15:52:09,349:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8526 (1.8396)
+2022-11-18 15:52:09,504:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9259 (1.8436)
+2022-11-18 15:52:09,554:INFO: - Computing loss (validation)
+2022-11-18 15:52:09,817:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9399 (1.9399)
+2022-11-18 15:52:09,851:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8885 (1.9362)
+2022-11-18 15:52:10,173:INFO: Dataset: univ                Batch: 1/3	Loss 1.8566 (1.8566)
+2022-11-18 15:52:10,252:INFO: Dataset: univ                Batch: 2/3	Loss 1.8729 (1.8647)
+2022-11-18 15:52:10,324:INFO: Dataset: univ                Batch: 3/3	Loss 1.8715 (1.8669)
+2022-11-18 15:52:10,648:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8615 (1.8615)
+2022-11-18 15:52:10,701:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0219 (1.8981)
+2022-11-18 15:52:11,034:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7905 (1.7905)
+2022-11-18 15:52:11,112:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7500 (1.7703)
+2022-11-18 15:52:11,188:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8269 (1.7894)
+2022-11-18 15:52:11,265:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8016 (1.7927)
+2022-11-18 15:52:11,340:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8916 (1.8116)
+2022-11-18 15:52:11,400:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_573.pth.tar
+2022-11-18 15:52:11,400:INFO: 
+===> EPOCH: 574 (P2)
+2022-11-18 15:52:11,401:INFO: - Computing loss (training)
+2022-11-18 15:52:11,744:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8937 (1.8937)
+2022-11-18 15:52:11,902:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8757 (1.8848)
+2022-11-18 15:52:12,056:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8170 (1.8632)
+2022-11-18 15:52:12,178:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8102 (1.8542)
+2022-11-18 15:52:12,621:INFO: Dataset: univ                Batch:  1/15	Loss 1.8408 (1.8408)
+2022-11-18 15:52:12,784:INFO: Dataset: univ                Batch:  2/15	Loss 1.8736 (1.8574)
+2022-11-18 15:52:12,943:INFO: Dataset: univ                Batch:  3/15	Loss 1.8358 (1.8501)
+2022-11-18 15:52:13,103:INFO: Dataset: univ                Batch:  4/15	Loss 1.8337 (1.8458)
+2022-11-18 15:52:13,262:INFO: Dataset: univ                Batch:  5/15	Loss 1.8681 (1.8499)
+2022-11-18 15:52:13,422:INFO: Dataset: univ                Batch:  6/15	Loss 1.8830 (1.8548)
+2022-11-18 15:52:13,579:INFO: Dataset: univ                Batch:  7/15	Loss 1.8015 (1.8470)
+2022-11-18 15:52:13,736:INFO: Dataset: univ                Batch:  8/15	Loss 1.8250 (1.8441)
+2022-11-18 15:52:13,891:INFO: Dataset: univ                Batch:  9/15	Loss 1.8210 (1.8420)
+2022-11-18 15:52:14,047:INFO: Dataset: univ                Batch: 10/15	Loss 1.8649 (1.8444)
+2022-11-18 15:52:14,206:INFO: Dataset: univ                Batch: 11/15	Loss 1.8277 (1.8428)
+2022-11-18 15:52:14,369:INFO: Dataset: univ                Batch: 12/15	Loss 1.8320 (1.8419)
+2022-11-18 15:52:14,529:INFO: Dataset: univ                Batch: 13/15	Loss 1.8365 (1.8415)
+2022-11-18 15:52:14,689:INFO: Dataset: univ                Batch: 14/15	Loss 1.8454 (1.8417)
+2022-11-18 15:52:14,772:INFO: Dataset: univ                Batch: 15/15	Loss 1.8822 (1.8422)
+2022-11-18 15:52:15,170:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8460 (1.8460)
+2022-11-18 15:52:15,322:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9552 (1.8997)
+2022-11-18 15:52:15,474:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9121 (1.9040)
+2022-11-18 15:52:15,625:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8859 (1.8999)
+2022-11-18 15:52:15,779:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9077 (1.9015)
+2022-11-18 15:52:15,931:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8682 (1.8965)
+2022-11-18 15:52:16,081:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8302 (1.8869)
+2022-11-18 15:52:16,219:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8736 (1.8853)
+2022-11-18 15:52:16,611:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8442 (1.8442)
+2022-11-18 15:52:16,766:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8229 (1.8341)
+2022-11-18 15:52:16,920:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8376 (1.8353)
+2022-11-18 15:52:17,071:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8210 (1.8317)
+2022-11-18 15:52:17,227:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8742 (1.8405)
+2022-11-18 15:52:17,380:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8451 (1.8412)
+2022-11-18 15:52:17,576:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8412 (1.8412)
+2022-11-18 15:52:17,730:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8059 (1.8367)
+2022-11-18 15:52:17,881:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8772 (1.8411)
+2022-11-18 15:52:18,030:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8198 (1.8391)
+2022-11-18 15:52:18,183:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8897 (1.8439)
+2022-11-18 15:52:18,336:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7706 (1.8381)
+2022-11-18 15:52:18,487:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8064 (1.8354)
+2022-11-18 15:52:18,639:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7774 (1.8311)
+2022-11-18 15:52:18,803:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7843 (1.8280)
+2022-11-18 15:52:18,973:INFO: Dataset: zara2               Batch: 16/18	Loss 1.6918 (1.8199)
+2022-11-18 15:52:19,144:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7854 (1.8179)
+2022-11-18 15:52:19,286:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8920 (1.8215)
+2022-11-18 15:52:19,332:INFO: - Computing loss (validation)
+2022-11-18 15:52:19,602:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8707 (1.8707)
+2022-11-18 15:52:19,639:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9215 (1.8741)
+2022-11-18 15:52:19,959:INFO: Dataset: univ                Batch: 1/3	Loss 1.8502 (1.8502)
+2022-11-18 15:52:20,042:INFO: Dataset: univ                Batch: 2/3	Loss 1.7981 (1.8247)
+2022-11-18 15:52:20,119:INFO: Dataset: univ                Batch: 3/3	Loss 1.8196 (1.8232)
+2022-11-18 15:52:20,427:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8396 (1.8396)
+2022-11-18 15:52:20,472:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9290 (1.8652)
+2022-11-18 15:52:20,771:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8306 (1.8306)
+2022-11-18 15:52:20,848:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8356 (1.8332)
+2022-11-18 15:52:20,926:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8747 (1.8470)
+2022-11-18 15:52:21,005:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8180 (1.8397)
+2022-11-18 15:52:21,080:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8867 (1.8487)
+2022-11-18 15:52:21,132:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_574.pth.tar
+2022-11-18 15:52:21,132:INFO: 
+===> EPOCH: 575 (P2)
+2022-11-18 15:52:21,132:INFO: - Computing loss (training)
+2022-11-18 15:52:21,475:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7535 (1.7535)
+2022-11-18 15:52:21,641:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8650 (1.8098)
+2022-11-18 15:52:21,801:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8513 (1.8234)
+2022-11-18 15:52:21,927:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8010 (1.8201)
+2022-11-18 15:52:22,342:INFO: Dataset: univ                Batch:  1/15	Loss 1.8366 (1.8366)
+2022-11-18 15:52:22,498:INFO: Dataset: univ                Batch:  2/15	Loss 1.8663 (1.8509)
+2022-11-18 15:52:22,653:INFO: Dataset: univ                Batch:  3/15	Loss 1.8844 (1.8624)
+2022-11-18 15:52:22,807:INFO: Dataset: univ                Batch:  4/15	Loss 1.8216 (1.8516)
+2022-11-18 15:52:22,964:INFO: Dataset: univ                Batch:  5/15	Loss 1.8883 (1.8591)
+2022-11-18 15:52:23,119:INFO: Dataset: univ                Batch:  6/15	Loss 1.8507 (1.8577)
+2022-11-18 15:52:23,272:INFO: Dataset: univ                Batch:  7/15	Loss 1.8190 (1.8522)
+2022-11-18 15:52:23,427:INFO: Dataset: univ                Batch:  8/15	Loss 1.8569 (1.8528)
+2022-11-18 15:52:23,598:INFO: Dataset: univ                Batch:  9/15	Loss 1.8650 (1.8543)
+2022-11-18 15:52:23,766:INFO: Dataset: univ                Batch: 10/15	Loss 1.8538 (1.8542)
+2022-11-18 15:52:23,930:INFO: Dataset: univ                Batch: 11/15	Loss 1.8012 (1.8493)
+2022-11-18 15:52:24,084:INFO: Dataset: univ                Batch: 12/15	Loss 1.8475 (1.8492)
+2022-11-18 15:52:24,237:INFO: Dataset: univ                Batch: 13/15	Loss 1.8089 (1.8461)
+2022-11-18 15:52:24,392:INFO: Dataset: univ                Batch: 14/15	Loss 1.8338 (1.8452)
+2022-11-18 15:52:24,472:INFO: Dataset: univ                Batch: 15/15	Loss 1.8274 (1.8449)
+2022-11-18 15:52:24,848:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7460 (1.7460)
+2022-11-18 15:52:24,994:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8812 (1.8115)
+2022-11-18 15:52:25,142:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9717 (1.8638)
+2022-11-18 15:52:25,292:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7623 (1.8372)
+2022-11-18 15:52:25,441:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7854 (1.8286)
+2022-11-18 15:52:25,591:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9520 (1.8496)
+2022-11-18 15:52:25,740:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8089 (1.8432)
+2022-11-18 15:52:25,875:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9359 (1.8540)
+2022-11-18 15:52:26,290:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9173 (1.9173)
+2022-11-18 15:52:26,441:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8273 (1.8737)
+2022-11-18 15:52:26,592:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7849 (1.8438)
+2022-11-18 15:52:26,742:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8507 (1.8456)
+2022-11-18 15:52:26,893:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8397 (1.8444)
+2022-11-18 15:52:27,046:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7725 (1.8317)
+2022-11-18 15:52:27,196:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7847 (1.8248)
+2022-11-18 15:52:27,346:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8247 (1.8248)
+2022-11-18 15:52:27,497:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7430 (1.8163)
+2022-11-18 15:52:27,647:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8247 (1.8171)
+2022-11-18 15:52:27,797:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8223 (1.8176)
+2022-11-18 15:52:27,951:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8063 (1.8166)
+2022-11-18 15:52:28,102:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8753 (1.8208)
+2022-11-18 15:52:28,253:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8259 (1.8211)
+2022-11-18 15:52:28,405:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8762 (1.8246)
+2022-11-18 15:52:28,556:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9147 (1.8297)
+2022-11-18 15:52:28,708:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8182 (1.8291)
+2022-11-18 15:52:28,847:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7568 (1.8259)
+2022-11-18 15:52:28,894:INFO: - Computing loss (validation)
+2022-11-18 15:52:29,164:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8498 (1.8498)
+2022-11-18 15:52:29,201:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1844 (1.8692)
+2022-11-18 15:52:29,530:INFO: Dataset: univ                Batch: 1/3	Loss 1.7942 (1.7942)
+2022-11-18 15:52:29,605:INFO: Dataset: univ                Batch: 2/3	Loss 1.8684 (1.8312)
+2022-11-18 15:52:29,677:INFO: Dataset: univ                Batch: 3/3	Loss 1.8960 (1.8543)
+2022-11-18 15:52:29,983:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8601 (1.8601)
+2022-11-18 15:52:30,028:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0165 (1.8988)
+2022-11-18 15:52:30,350:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9141 (1.9141)
+2022-11-18 15:52:30,424:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7804 (1.8455)
+2022-11-18 15:52:30,497:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8590 (1.8502)
+2022-11-18 15:52:30,571:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8624 (1.8531)
+2022-11-18 15:52:30,643:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7727 (1.8363)
+2022-11-18 15:52:30,695:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_575.pth.tar
+2022-11-18 15:52:30,695:INFO: 
+===> EPOCH: 576 (P2)
+2022-11-18 15:52:30,695:INFO: - Computing loss (training)
+2022-11-18 15:52:31,029:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7772 (1.7772)
+2022-11-18 15:52:31,179:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7220 (1.7504)
+2022-11-18 15:52:31,331:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9054 (1.8047)
+2022-11-18 15:52:31,465:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9008 (1.8204)
+2022-11-18 15:52:31,876:INFO: Dataset: univ                Batch:  1/15	Loss 1.7899 (1.7899)
+2022-11-18 15:52:32,034:INFO: Dataset: univ                Batch:  2/15	Loss 1.8704 (1.8289)
+2022-11-18 15:52:32,207:INFO: Dataset: univ                Batch:  3/15	Loss 1.8125 (1.8230)
+2022-11-18 15:52:32,384:INFO: Dataset: univ                Batch:  4/15	Loss 1.9077 (1.8423)
+2022-11-18 15:52:32,541:INFO: Dataset: univ                Batch:  5/15	Loss 1.8580 (1.8451)
+2022-11-18 15:52:32,699:INFO: Dataset: univ                Batch:  6/15	Loss 1.8945 (1.8532)
+2022-11-18 15:52:32,859:INFO: Dataset: univ                Batch:  7/15	Loss 1.8177 (1.8483)
+2022-11-18 15:52:33,020:INFO: Dataset: univ                Batch:  8/15	Loss 1.8428 (1.8476)
+2022-11-18 15:52:33,176:INFO: Dataset: univ                Batch:  9/15	Loss 1.8487 (1.8477)
+2022-11-18 15:52:33,336:INFO: Dataset: univ                Batch: 10/15	Loss 1.8312 (1.8462)
+2022-11-18 15:52:33,497:INFO: Dataset: univ                Batch: 11/15	Loss 1.8032 (1.8422)
+2022-11-18 15:52:33,667:INFO: Dataset: univ                Batch: 12/15	Loss 1.8015 (1.8388)
+2022-11-18 15:52:33,828:INFO: Dataset: univ                Batch: 13/15	Loss 1.8561 (1.8401)
+2022-11-18 15:52:33,989:INFO: Dataset: univ                Batch: 14/15	Loss 1.8818 (1.8433)
+2022-11-18 15:52:34,075:INFO: Dataset: univ                Batch: 15/15	Loss 1.8460 (1.8433)
+2022-11-18 15:52:34,500:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8373 (1.8373)
+2022-11-18 15:52:34,661:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8686 (1.8523)
+2022-11-18 15:52:34,818:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8654 (1.8573)
+2022-11-18 15:52:34,971:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9789 (1.8886)
+2022-11-18 15:52:35,150:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9927 (1.9123)
+2022-11-18 15:52:35,314:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8204 (1.8977)
+2022-11-18 15:52:35,467:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8569 (1.8923)
+2022-11-18 15:52:35,608:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9126 (1.8945)
+2022-11-18 15:52:36,021:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8060 (1.8060)
+2022-11-18 15:52:36,188:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9081 (1.8544)
+2022-11-18 15:52:36,340:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7045 (1.8060)
+2022-11-18 15:52:36,504:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8477 (1.8166)
+2022-11-18 15:52:36,665:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8055 (1.8144)
+2022-11-18 15:52:36,821:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8522 (1.8203)
+2022-11-18 15:52:36,985:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8824 (1.8284)
+2022-11-18 15:52:37,145:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7545 (1.8191)
+2022-11-18 15:52:37,295:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7356 (1.8105)
+2022-11-18 15:52:37,451:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7923 (1.8088)
+2022-11-18 15:52:37,602:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7845 (1.8066)
+2022-11-18 15:52:37,756:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7734 (1.8039)
+2022-11-18 15:52:37,911:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9702 (1.8156)
+2022-11-18 15:52:38,067:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8338 (1.8169)
+2022-11-18 15:52:38,230:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7945 (1.8154)
+2022-11-18 15:52:38,393:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8302 (1.8163)
+2022-11-18 15:52:38,551:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8222 (1.8166)
+2022-11-18 15:52:38,717:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8530 (1.8183)
+2022-11-18 15:52:38,764:INFO: - Computing loss (validation)
+2022-11-18 15:52:39,041:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9928 (1.9928)
+2022-11-18 15:52:39,077:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1574 (2.0046)
+2022-11-18 15:52:39,395:INFO: Dataset: univ                Batch: 1/3	Loss 1.8543 (1.8543)
+2022-11-18 15:52:39,475:INFO: Dataset: univ                Batch: 2/3	Loss 1.8919 (1.8731)
+2022-11-18 15:52:39,551:INFO: Dataset: univ                Batch: 3/3	Loss 1.8244 (1.8585)
+2022-11-18 15:52:39,865:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7834 (1.7834)
+2022-11-18 15:52:39,927:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0939 (1.8633)
+2022-11-18 15:52:40,309:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8654 (1.8654)
+2022-11-18 15:52:40,397:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7606 (1.8131)
+2022-11-18 15:52:40,486:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8198 (1.8154)
+2022-11-18 15:52:40,578:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8231 (1.8171)
+2022-11-18 15:52:40,670:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8204 (1.8178)
+2022-11-18 15:52:40,752:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_576.pth.tar
+2022-11-18 15:52:40,753:INFO: 
+===> EPOCH: 577 (P2)
+2022-11-18 15:52:40,754:INFO: - Computing loss (training)
+2022-11-18 15:52:41,115:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9186 (1.9186)
+2022-11-18 15:52:41,265:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9307 (1.9248)
+2022-11-18 15:52:41,416:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8109 (1.8848)
+2022-11-18 15:52:41,548:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7660 (1.8643)
+2022-11-18 15:52:42,006:INFO: Dataset: univ                Batch:  1/15	Loss 1.8501 (1.8501)
+2022-11-18 15:52:42,179:INFO: Dataset: univ                Batch:  2/15	Loss 1.8087 (1.8295)
+2022-11-18 15:52:42,344:INFO: Dataset: univ                Batch:  3/15	Loss 1.8228 (1.8272)
+2022-11-18 15:52:42,511:INFO: Dataset: univ                Batch:  4/15	Loss 1.8335 (1.8288)
+2022-11-18 15:52:42,681:INFO: Dataset: univ                Batch:  5/15	Loss 1.8543 (1.8343)
+2022-11-18 15:52:42,849:INFO: Dataset: univ                Batch:  6/15	Loss 1.8787 (1.8415)
+2022-11-18 15:52:43,017:INFO: Dataset: univ                Batch:  7/15	Loss 1.8472 (1.8423)
+2022-11-18 15:52:43,183:INFO: Dataset: univ                Batch:  8/15	Loss 1.8518 (1.8435)
+2022-11-18 15:52:43,363:INFO: Dataset: univ                Batch:  9/15	Loss 1.7923 (1.8376)
+2022-11-18 15:52:43,526:INFO: Dataset: univ                Batch: 10/15	Loss 1.8688 (1.8406)
+2022-11-18 15:52:43,705:INFO: Dataset: univ                Batch: 11/15	Loss 1.8506 (1.8416)
+2022-11-18 15:52:43,877:INFO: Dataset: univ                Batch: 12/15	Loss 1.8368 (1.8412)
+2022-11-18 15:52:44,039:INFO: Dataset: univ                Batch: 13/15	Loss 1.8231 (1.8400)
+2022-11-18 15:52:44,199:INFO: Dataset: univ                Batch: 14/15	Loss 1.8652 (1.8417)
+2022-11-18 15:52:44,282:INFO: Dataset: univ                Batch: 15/15	Loss 1.9151 (1.8425)
+2022-11-18 15:52:44,696:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9873 (1.9873)
+2022-11-18 15:52:44,853:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7271 (1.8514)
+2022-11-18 15:52:45,009:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9418 (1.8831)
+2022-11-18 15:52:45,162:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8745 (1.8809)
+2022-11-18 15:52:45,313:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9738 (1.8996)
+2022-11-18 15:52:45,466:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9314 (1.9051)
+2022-11-18 15:52:45,618:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8816 (1.9016)
+2022-11-18 15:52:45,758:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9311 (1.9052)
+2022-11-18 15:52:46,169:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7799 (1.7799)
+2022-11-18 15:52:46,328:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8241 (1.8024)
+2022-11-18 15:52:46,479:INFO: Dataset: zara2               Batch:  3/18	Loss 1.6734 (1.7629)
+2022-11-18 15:52:46,628:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9169 (1.8019)
+2022-11-18 15:52:46,790:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8082 (1.8030)
+2022-11-18 15:52:47,021:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9093 (1.8205)
+2022-11-18 15:52:47,168:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8644 (1.8268)
+2022-11-18 15:52:47,317:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8051 (1.8241)
+2022-11-18 15:52:47,471:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7754 (1.8184)
+2022-11-18 15:52:47,621:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7441 (1.8109)
+2022-11-18 15:52:47,769:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8173 (1.8115)
+2022-11-18 15:52:47,917:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8175 (1.8120)
+2022-11-18 15:52:48,075:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9173 (1.8198)
+2022-11-18 15:52:48,231:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7824 (1.8172)
+2022-11-18 15:52:48,382:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8316 (1.8181)
+2022-11-18 15:52:48,535:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8516 (1.8201)
+2022-11-18 15:52:48,705:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7329 (1.8147)
+2022-11-18 15:52:48,870:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7923 (1.8136)
+2022-11-18 15:52:48,917:INFO: - Computing loss (validation)
+2022-11-18 15:52:49,206:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8154 (1.8154)
+2022-11-18 15:52:49,240:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2152 (1.8372)
+2022-11-18 15:52:49,622:INFO: Dataset: univ                Batch: 1/3	Loss 1.8787 (1.8787)
+2022-11-18 15:52:49,713:INFO: Dataset: univ                Batch: 2/3	Loss 1.8452 (1.8636)
+2022-11-18 15:52:49,798:INFO: Dataset: univ                Batch: 3/3	Loss 1.7961 (1.8425)
+2022-11-18 15:52:50,142:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7202 (1.7202)
+2022-11-18 15:52:50,190:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9387 (1.7687)
+2022-11-18 15:52:50,505:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8605 (1.8605)
+2022-11-18 15:52:50,586:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7922 (1.8267)
+2022-11-18 15:52:50,665:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7461 (1.8009)
+2022-11-18 15:52:50,745:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7779 (1.7948)
+2022-11-18 15:52:50,822:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8186 (1.7995)
+2022-11-18 15:52:50,880:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_577.pth.tar
+2022-11-18 15:52:50,880:INFO: 
+===> EPOCH: 578 (P2)
+2022-11-18 15:52:50,881:INFO: - Computing loss (training)
+2022-11-18 15:52:51,265:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8395 (1.8395)
+2022-11-18 15:52:51,439:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9551 (1.8981)
+2022-11-18 15:52:51,602:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9029 (1.8998)
+2022-11-18 15:52:51,728:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7622 (1.8786)
+2022-11-18 15:52:52,164:INFO: Dataset: univ                Batch:  1/15	Loss 1.8594 (1.8594)
+2022-11-18 15:52:52,332:INFO: Dataset: univ                Batch:  2/15	Loss 1.8602 (1.8598)
+2022-11-18 15:52:52,510:INFO: Dataset: univ                Batch:  3/15	Loss 1.8928 (1.8706)
+2022-11-18 15:52:52,683:INFO: Dataset: univ                Batch:  4/15	Loss 1.8810 (1.8730)
+2022-11-18 15:52:52,878:INFO: Dataset: univ                Batch:  5/15	Loss 1.8424 (1.8666)
+2022-11-18 15:52:53,059:INFO: Dataset: univ                Batch:  6/15	Loss 1.8254 (1.8594)
+2022-11-18 15:52:53,225:INFO: Dataset: univ                Batch:  7/15	Loss 1.8311 (1.8554)
+2022-11-18 15:52:53,394:INFO: Dataset: univ                Batch:  8/15	Loss 1.8640 (1.8564)
+2022-11-18 15:52:53,558:INFO: Dataset: univ                Batch:  9/15	Loss 1.8719 (1.8581)
+2022-11-18 15:52:53,723:INFO: Dataset: univ                Batch: 10/15	Loss 1.8241 (1.8550)
+2022-11-18 15:52:53,890:INFO: Dataset: univ                Batch: 11/15	Loss 1.8960 (1.8588)
+2022-11-18 15:52:54,052:INFO: Dataset: univ                Batch: 12/15	Loss 1.8358 (1.8568)
+2022-11-18 15:52:54,211:INFO: Dataset: univ                Batch: 13/15	Loss 1.8354 (1.8551)
+2022-11-18 15:52:54,380:INFO: Dataset: univ                Batch: 14/15	Loss 1.8516 (1.8549)
+2022-11-18 15:52:54,469:INFO: Dataset: univ                Batch: 15/15	Loss 1.8428 (1.8548)
+2022-11-18 15:52:54,892:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9095 (1.9095)
+2022-11-18 15:52:55,070:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0203 (1.9723)
+2022-11-18 15:52:55,255:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8068 (1.9116)
+2022-11-18 15:52:55,427:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9125 (1.9118)
+2022-11-18 15:52:55,597:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8200 (1.8934)
+2022-11-18 15:52:55,772:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7487 (1.8685)
+2022-11-18 15:52:55,926:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7520 (1.8507)
+2022-11-18 15:52:56,084:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9477 (1.8621)
+2022-11-18 15:52:56,504:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8248 (1.8248)
+2022-11-18 15:52:56,685:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8505 (1.8368)
+2022-11-18 15:52:56,867:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8805 (1.8504)
+2022-11-18 15:52:57,038:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9360 (1.8716)
+2022-11-18 15:52:57,195:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7372 (1.8457)
+2022-11-18 15:52:57,351:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8443 (1.8454)
+2022-11-18 15:52:57,499:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8086 (1.8404)
+2022-11-18 15:52:57,651:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7727 (1.8322)
+2022-11-18 15:52:57,813:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7764 (1.8262)
+2022-11-18 15:52:57,973:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8369 (1.8272)
+2022-11-18 15:52:58,137:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8640 (1.8305)
+2022-11-18 15:52:58,305:INFO: Dataset: zara2               Batch: 12/18	Loss 1.9038 (1.8371)
+2022-11-18 15:52:58,473:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9633 (1.8466)
+2022-11-18 15:52:58,626:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7900 (1.8419)
+2022-11-18 15:52:58,775:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7280 (1.8343)
+2022-11-18 15:52:58,936:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8154 (1.8332)
+2022-11-18 15:52:59,096:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7944 (1.8308)
+2022-11-18 15:52:59,252:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8959 (1.8339)
+2022-11-18 15:52:59,308:INFO: - Computing loss (validation)
+2022-11-18 15:52:59,615:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9350 (1.9350)
+2022-11-18 15:52:59,653:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6251 (1.9086)
+2022-11-18 15:52:59,984:INFO: Dataset: univ                Batch: 1/3	Loss 1.8374 (1.8374)
+2022-11-18 15:53:00,069:INFO: Dataset: univ                Batch: 2/3	Loss 1.8118 (1.8256)
+2022-11-18 15:53:00,147:INFO: Dataset: univ                Batch: 3/3	Loss 1.8636 (1.8366)
+2022-11-18 15:53:00,458:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8579 (1.8579)
+2022-11-18 15:53:00,516:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9653 (1.8820)
+2022-11-18 15:53:00,877:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8752 (1.8752)
+2022-11-18 15:53:00,971:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8088 (1.8412)
+2022-11-18 15:53:01,064:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7497 (1.8102)
+2022-11-18 15:53:01,153:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7828 (1.8032)
+2022-11-18 15:53:01,241:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8490 (1.8124)
+2022-11-18 15:53:01,306:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_578.pth.tar
+2022-11-18 15:53:01,307:INFO: 
+===> EPOCH: 579 (P2)
+2022-11-18 15:53:01,308:INFO: - Computing loss (training)
+2022-11-18 15:53:01,685:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8686 (1.8686)
+2022-11-18 15:53:01,842:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8367 (1.8527)
+2022-11-18 15:53:02,007:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9729 (1.8907)
+2022-11-18 15:53:02,140:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8120 (1.8767)
+2022-11-18 15:53:02,596:INFO: Dataset: univ                Batch:  1/15	Loss 1.8422 (1.8422)
+2022-11-18 15:53:02,789:INFO: Dataset: univ                Batch:  2/15	Loss 1.8094 (1.8255)
+2022-11-18 15:53:02,985:INFO: Dataset: univ                Batch:  3/15	Loss 1.8192 (1.8233)
+2022-11-18 15:53:03,166:INFO: Dataset: univ                Batch:  4/15	Loss 1.8727 (1.8354)
+2022-11-18 15:53:03,329:INFO: Dataset: univ                Batch:  5/15	Loss 1.8383 (1.8360)
+2022-11-18 15:53:03,493:INFO: Dataset: univ                Batch:  6/15	Loss 1.8453 (1.8376)
+2022-11-18 15:53:03,660:INFO: Dataset: univ                Batch:  7/15	Loss 1.8592 (1.8408)
+2022-11-18 15:53:03,854:INFO: Dataset: univ                Batch:  8/15	Loss 1.8140 (1.8375)
+2022-11-18 15:53:04,046:INFO: Dataset: univ                Batch:  9/15	Loss 1.8593 (1.8400)
+2022-11-18 15:53:04,251:INFO: Dataset: univ                Batch: 10/15	Loss 1.8288 (1.8389)
+2022-11-18 15:53:04,474:INFO: Dataset: univ                Batch: 11/15	Loss 1.8381 (1.8388)
+2022-11-18 15:53:04,727:INFO: Dataset: univ                Batch: 12/15	Loss 1.8992 (1.8438)
+2022-11-18 15:53:04,914:INFO: Dataset: univ                Batch: 13/15	Loss 1.8065 (1.8405)
+2022-11-18 15:53:05,075:INFO: Dataset: univ                Batch: 14/15	Loss 1.8136 (1.8385)
+2022-11-18 15:53:05,158:INFO: Dataset: univ                Batch: 15/15	Loss 1.8036 (1.8381)
+2022-11-18 15:53:05,557:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9209 (1.9209)
+2022-11-18 15:53:05,712:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0030 (1.9659)
+2022-11-18 15:53:05,877:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9359 (1.9551)
+2022-11-18 15:53:06,038:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8602 (1.9286)
+2022-11-18 15:53:06,199:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7916 (1.9013)
+2022-11-18 15:53:06,359:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8880 (1.8990)
+2022-11-18 15:53:06,523:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9007 (1.8992)
+2022-11-18 15:53:06,663:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0060 (1.9095)
+2022-11-18 15:53:07,129:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8200 (1.8200)
+2022-11-18 15:53:07,289:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8051 (1.8123)
+2022-11-18 15:53:07,455:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8270 (1.8170)
+2022-11-18 15:53:07,657:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7675 (1.8050)
+2022-11-18 15:53:07,839:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7702 (1.7979)
+2022-11-18 15:53:08,028:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7739 (1.7943)
+2022-11-18 15:53:08,248:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9608 (1.8186)
+2022-11-18 15:53:08,412:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8445 (1.8219)
+2022-11-18 15:53:08,571:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8519 (1.8253)
+2022-11-18 15:53:08,721:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8244 (1.8252)
+2022-11-18 15:53:08,884:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8072 (1.8238)
+2022-11-18 15:53:09,050:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8740 (1.8280)
+2022-11-18 15:53:09,227:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8770 (1.8321)
+2022-11-18 15:53:09,402:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7668 (1.8274)
+2022-11-18 15:53:09,561:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9309 (1.8343)
+2022-11-18 15:53:09,718:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8552 (1.8356)
+2022-11-18 15:53:09,875:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8925 (1.8388)
+2022-11-18 15:53:10,017:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9163 (1.8425)
+2022-11-18 15:53:10,067:INFO: - Computing loss (validation)
+2022-11-18 15:53:10,339:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8578 (1.8578)
+2022-11-18 15:53:10,373:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9456 (1.8650)
+2022-11-18 15:53:10,683:INFO: Dataset: univ                Batch: 1/3	Loss 1.8242 (1.8242)
+2022-11-18 15:53:10,762:INFO: Dataset: univ                Batch: 2/3	Loss 1.7872 (1.8047)
+2022-11-18 15:53:10,838:INFO: Dataset: univ                Batch: 3/3	Loss 1.8522 (1.8204)
+2022-11-18 15:53:11,155:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8729 (1.8729)
+2022-11-18 15:53:11,206:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9664 (1.8976)
+2022-11-18 15:53:11,523:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7552 (1.7552)
+2022-11-18 15:53:11,606:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8668 (1.8116)
+2022-11-18 15:53:11,688:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8489 (1.8247)
+2022-11-18 15:53:11,769:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9042 (1.8445)
+2022-11-18 15:53:11,849:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7586 (1.8282)
+2022-11-18 15:53:11,902:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_579.pth.tar
+2022-11-18 15:53:11,902:INFO: 
+===> EPOCH: 580 (P2)
+2022-11-18 15:53:11,903:INFO: - Computing loss (training)
+2022-11-18 15:53:12,247:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9222 (1.9222)
+2022-11-18 15:53:12,399:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8621 (1.8922)
+2022-11-18 15:53:12,551:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8778 (1.8874)
+2022-11-18 15:53:12,668:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9514 (1.8980)
+2022-11-18 15:53:13,115:INFO: Dataset: univ                Batch:  1/15	Loss 1.7997 (1.7997)
+2022-11-18 15:53:13,281:INFO: Dataset: univ                Batch:  2/15	Loss 1.8322 (1.8176)
+2022-11-18 15:53:13,492:INFO: Dataset: univ                Batch:  3/15	Loss 1.8314 (1.8219)
+2022-11-18 15:53:13,698:INFO: Dataset: univ                Batch:  4/15	Loss 1.8610 (1.8318)
+2022-11-18 15:53:13,869:INFO: Dataset: univ                Batch:  5/15	Loss 1.8690 (1.8395)
+2022-11-18 15:53:14,031:INFO: Dataset: univ                Batch:  6/15	Loss 1.8116 (1.8344)
+2022-11-18 15:53:14,187:INFO: Dataset: univ                Batch:  7/15	Loss 1.8410 (1.8353)
+2022-11-18 15:53:14,345:INFO: Dataset: univ                Batch:  8/15	Loss 1.7946 (1.8300)
+2022-11-18 15:53:14,498:INFO: Dataset: univ                Batch:  9/15	Loss 1.8432 (1.8314)
+2022-11-18 15:53:14,654:INFO: Dataset: univ                Batch: 10/15	Loss 1.7691 (1.8250)
+2022-11-18 15:53:14,827:INFO: Dataset: univ                Batch: 11/15	Loss 1.8601 (1.8281)
+2022-11-18 15:53:15,003:INFO: Dataset: univ                Batch: 12/15	Loss 1.8454 (1.8296)
+2022-11-18 15:53:15,171:INFO: Dataset: univ                Batch: 13/15	Loss 1.8692 (1.8328)
+2022-11-18 15:53:15,347:INFO: Dataset: univ                Batch: 14/15	Loss 1.8582 (1.8347)
+2022-11-18 15:53:15,448:INFO: Dataset: univ                Batch: 15/15	Loss 1.7975 (1.8342)
+2022-11-18 15:53:15,884:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8085 (1.8085)
+2022-11-18 15:53:16,067:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8606 (1.8329)
+2022-11-18 15:53:16,229:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8476 (1.8378)
+2022-11-18 15:53:16,400:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8030 (1.8290)
+2022-11-18 15:53:16,564:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8576 (1.8352)
+2022-11-18 15:53:16,729:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9010 (1.8469)
+2022-11-18 15:53:16,893:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9327 (1.8591)
+2022-11-18 15:53:17,047:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8755 (1.8609)
+2022-11-18 15:53:17,460:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8345 (1.8345)
+2022-11-18 15:53:17,615:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8728 (1.8520)
+2022-11-18 15:53:17,766:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8036 (1.8349)
+2022-11-18 15:53:17,913:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7340 (1.8090)
+2022-11-18 15:53:18,065:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9610 (1.8388)
+2022-11-18 15:53:18,219:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8442 (1.8397)
+2022-11-18 15:53:18,369:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7866 (1.8319)
+2022-11-18 15:53:18,516:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8091 (1.8289)
+2022-11-18 15:53:18,679:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7516 (1.8196)
+2022-11-18 15:53:18,847:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8136 (1.8191)
+2022-11-18 15:53:19,011:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7773 (1.8150)
+2022-11-18 15:53:19,161:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8351 (1.8166)
+2022-11-18 15:53:19,310:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9154 (1.8245)
+2022-11-18 15:53:19,459:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7104 (1.8159)
+2022-11-18 15:53:19,608:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8193 (1.8161)
+2022-11-18 15:53:19,756:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8272 (1.8169)
+2022-11-18 15:53:19,906:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8517 (1.8189)
+2022-11-18 15:53:20,042:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8905 (1.8225)
+2022-11-18 15:53:20,087:INFO: - Computing loss (validation)
+2022-11-18 15:53:20,356:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8818 (1.8818)
+2022-11-18 15:53:20,392:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8123 (1.8768)
+2022-11-18 15:53:20,704:INFO: Dataset: univ                Batch: 1/3	Loss 1.8146 (1.8146)
+2022-11-18 15:53:20,783:INFO: Dataset: univ                Batch: 2/3	Loss 1.9429 (1.8716)
+2022-11-18 15:53:20,859:INFO: Dataset: univ                Batch: 3/3	Loss 1.7781 (1.8422)
+2022-11-18 15:53:21,157:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9060 (1.9060)
+2022-11-18 15:53:21,202:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7063 (1.8591)
+2022-11-18 15:53:21,533:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7860 (1.7860)
+2022-11-18 15:53:21,613:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8825 (1.8327)
+2022-11-18 15:53:21,694:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7911 (1.8198)
+2022-11-18 15:53:21,781:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8275 (1.8218)
+2022-11-18 15:53:21,861:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8994 (1.8375)
+2022-11-18 15:53:21,918:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_580.pth.tar
+2022-11-18 15:53:21,918:INFO: 
+===> EPOCH: 581 (P2)
+2022-11-18 15:53:21,919:INFO: - Computing loss (training)
+2022-11-18 15:53:22,273:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9176 (1.9176)
+2022-11-18 15:53:22,428:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8437 (1.8792)
+2022-11-18 15:53:22,582:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8929 (1.8842)
+2022-11-18 15:53:22,700:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9806 (1.9002)
+2022-11-18 15:53:23,093:INFO: Dataset: univ                Batch:  1/15	Loss 1.8189 (1.8189)
+2022-11-18 15:53:23,252:INFO: Dataset: univ                Batch:  2/15	Loss 1.8478 (1.8332)
+2022-11-18 15:53:23,413:INFO: Dataset: univ                Batch:  3/15	Loss 1.8370 (1.8345)
+2022-11-18 15:53:23,570:INFO: Dataset: univ                Batch:  4/15	Loss 1.8095 (1.8283)
+2022-11-18 15:53:23,730:INFO: Dataset: univ                Batch:  5/15	Loss 1.7792 (1.8184)
+2022-11-18 15:53:23,887:INFO: Dataset: univ                Batch:  6/15	Loss 1.8557 (1.8247)
+2022-11-18 15:53:24,043:INFO: Dataset: univ                Batch:  7/15	Loss 1.8152 (1.8233)
+2022-11-18 15:53:24,197:INFO: Dataset: univ                Batch:  8/15	Loss 1.8300 (1.8241)
+2022-11-18 15:53:24,354:INFO: Dataset: univ                Batch:  9/15	Loss 1.8321 (1.8250)
+2022-11-18 15:53:24,509:INFO: Dataset: univ                Batch: 10/15	Loss 1.8041 (1.8228)
+2022-11-18 15:53:24,668:INFO: Dataset: univ                Batch: 11/15	Loss 1.8519 (1.8256)
+2022-11-18 15:53:24,822:INFO: Dataset: univ                Batch: 12/15	Loss 1.8400 (1.8267)
+2022-11-18 15:53:24,979:INFO: Dataset: univ                Batch: 13/15	Loss 1.8163 (1.8259)
+2022-11-18 15:53:25,134:INFO: Dataset: univ                Batch: 14/15	Loss 1.8282 (1.8260)
+2022-11-18 15:53:25,217:INFO: Dataset: univ                Batch: 15/15	Loss 1.7645 (1.8252)
+2022-11-18 15:53:25,611:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7626 (1.7626)
+2022-11-18 15:53:25,756:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8662 (1.8162)
+2022-11-18 15:53:25,906:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8274 (1.8196)
+2022-11-18 15:53:26,052:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9403 (1.8536)
+2022-11-18 15:53:26,204:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7845 (1.8397)
+2022-11-18 15:53:26,352:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9114 (1.8528)
+2022-11-18 15:53:26,501:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6921 (1.8305)
+2022-11-18 15:53:26,635:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9129 (1.8392)
+2022-11-18 15:53:27,050:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8524 (1.8524)
+2022-11-18 15:53:27,204:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7428 (1.7914)
+2022-11-18 15:53:27,356:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8911 (1.8221)
+2022-11-18 15:53:27,508:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7604 (1.8061)
+2022-11-18 15:53:27,659:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8605 (1.8169)
+2022-11-18 15:53:27,811:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9082 (1.8320)
+2022-11-18 15:53:27,962:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7312 (1.8189)
+2022-11-18 15:53:28,111:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7943 (1.8159)
+2022-11-18 15:53:28,260:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7389 (1.8071)
+2022-11-18 15:53:28,409:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8158 (1.8080)
+2022-11-18 15:53:28,560:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8492 (1.8119)
+2022-11-18 15:53:28,715:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7579 (1.8074)
+2022-11-18 15:53:28,869:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7850 (1.8058)
+2022-11-18 15:53:29,020:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7594 (1.8027)
+2022-11-18 15:53:29,172:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8513 (1.8058)
+2022-11-18 15:53:29,321:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8399 (1.8080)
+2022-11-18 15:53:29,473:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9068 (1.8138)
+2022-11-18 15:53:29,612:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8928 (1.8174)
+2022-11-18 15:53:29,658:INFO: - Computing loss (validation)
+2022-11-18 15:53:29,921:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9747 (1.9747)
+2022-11-18 15:53:29,956:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0467 (1.9796)
+2022-11-18 15:53:30,267:INFO: Dataset: univ                Batch: 1/3	Loss 1.8100 (1.8100)
+2022-11-18 15:53:30,344:INFO: Dataset: univ                Batch: 2/3	Loss 1.8798 (1.8443)
+2022-11-18 15:53:30,419:INFO: Dataset: univ                Batch: 3/3	Loss 1.8371 (1.8420)
+2022-11-18 15:53:30,718:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9403 (1.9403)
+2022-11-18 15:53:30,763:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9088 (1.9312)
+2022-11-18 15:53:31,082:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9074 (1.9074)
+2022-11-18 15:53:31,164:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8444 (1.8750)
+2022-11-18 15:53:31,245:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8906 (1.8804)
+2022-11-18 15:53:31,326:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8283 (1.8689)
+2022-11-18 15:53:31,405:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7418 (1.8445)
+2022-11-18 15:53:31,460:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_581.pth.tar
+2022-11-18 15:53:31,460:INFO: 
+===> EPOCH: 582 (P2)
+2022-11-18 15:53:31,461:INFO: - Computing loss (training)
+2022-11-18 15:53:31,799:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9236 (1.9236)
+2022-11-18 15:53:31,950:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8948 (1.9093)
+2022-11-18 15:53:32,099:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7997 (1.8723)
+2022-11-18 15:53:32,215:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7964 (1.8591)
+2022-11-18 15:53:32,623:INFO: Dataset: univ                Batch:  1/15	Loss 1.8520 (1.8520)
+2022-11-18 15:53:32,783:INFO: Dataset: univ                Batch:  2/15	Loss 1.8805 (1.8667)
+2022-11-18 15:53:32,941:INFO: Dataset: univ                Batch:  3/15	Loss 1.8253 (1.8518)
+2022-11-18 15:53:33,102:INFO: Dataset: univ                Batch:  4/15	Loss 1.8373 (1.8478)
+2022-11-18 15:53:33,258:INFO: Dataset: univ                Batch:  5/15	Loss 1.8636 (1.8510)
+2022-11-18 15:53:33,420:INFO: Dataset: univ                Batch:  6/15	Loss 1.8396 (1.8491)
+2022-11-18 15:53:33,577:INFO: Dataset: univ                Batch:  7/15	Loss 1.8399 (1.8478)
+2022-11-18 15:53:33,732:INFO: Dataset: univ                Batch:  8/15	Loss 1.8610 (1.8495)
+2022-11-18 15:53:33,889:INFO: Dataset: univ                Batch:  9/15	Loss 1.7896 (1.8432)
+2022-11-18 15:53:34,043:INFO: Dataset: univ                Batch: 10/15	Loss 1.8111 (1.8402)
+2022-11-18 15:53:34,201:INFO: Dataset: univ                Batch: 11/15	Loss 1.8179 (1.8380)
+2022-11-18 15:53:34,358:INFO: Dataset: univ                Batch: 12/15	Loss 1.8371 (1.8379)
+2022-11-18 15:53:34,516:INFO: Dataset: univ                Batch: 13/15	Loss 1.8937 (1.8421)
+2022-11-18 15:53:34,672:INFO: Dataset: univ                Batch: 14/15	Loss 1.8106 (1.8399)
+2022-11-18 15:53:34,755:INFO: Dataset: univ                Batch: 15/15	Loss 1.8232 (1.8397)
+2022-11-18 15:53:35,141:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8778 (1.8778)
+2022-11-18 15:53:35,293:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7405 (1.8024)
+2022-11-18 15:53:35,448:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8493 (1.8199)
+2022-11-18 15:53:35,597:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9956 (1.8570)
+2022-11-18 15:53:35,747:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8530 (1.8562)
+2022-11-18 15:53:35,899:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8965 (1.8634)
+2022-11-18 15:53:36,050:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9054 (1.8696)
+2022-11-18 15:53:36,187:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8580 (1.8684)
+2022-11-18 15:53:36,598:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7973 (1.7973)
+2022-11-18 15:53:36,748:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7352 (1.7634)
+2022-11-18 15:53:36,900:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8375 (1.7890)
+2022-11-18 15:53:37,054:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7979 (1.7912)
+2022-11-18 15:53:37,205:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8197 (1.7967)
+2022-11-18 15:53:37,358:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8455 (1.8054)
+2022-11-18 15:53:37,508:INFO: Dataset: zara2               Batch:  7/18	Loss 1.9132 (1.8214)
+2022-11-18 15:53:37,657:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9154 (1.8320)
+2022-11-18 15:53:37,807:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8647 (1.8356)
+2022-11-18 15:53:37,956:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9034 (1.8429)
+2022-11-18 15:53:38,106:INFO: Dataset: zara2               Batch: 11/18	Loss 1.9398 (1.8516)
+2022-11-18 15:53:38,255:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8957 (1.8555)
+2022-11-18 15:53:38,405:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7848 (1.8502)
+2022-11-18 15:53:38,555:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8541 (1.8505)
+2022-11-18 15:53:38,707:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7480 (1.8433)
+2022-11-18 15:53:38,858:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8535 (1.8439)
+2022-11-18 15:53:39,009:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8541 (1.8446)
+2022-11-18 15:53:39,145:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8179 (1.8433)
+2022-11-18 15:53:39,190:INFO: - Computing loss (validation)
+2022-11-18 15:53:39,461:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9029 (1.9029)
+2022-11-18 15:53:39,498:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5897 (1.8815)
+2022-11-18 15:53:39,812:INFO: Dataset: univ                Batch: 1/3	Loss 1.8573 (1.8573)
+2022-11-18 15:53:39,894:INFO: Dataset: univ                Batch: 2/3	Loss 1.8971 (1.8794)
+2022-11-18 15:53:39,972:INFO: Dataset: univ                Batch: 3/3	Loss 1.8941 (1.8847)
+2022-11-18 15:53:40,285:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8045 (1.8045)
+2022-11-18 15:53:40,332:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7859 (1.7997)
+2022-11-18 15:53:40,637:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8762 (1.8762)
+2022-11-18 15:53:40,713:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8652 (1.8707)
+2022-11-18 15:53:40,790:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7838 (1.8403)
+2022-11-18 15:53:40,866:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8214 (1.8357)
+2022-11-18 15:53:40,942:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8035 (1.8293)
+2022-11-18 15:53:40,995:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_582.pth.tar
+2022-11-18 15:53:40,995:INFO: 
+===> EPOCH: 583 (P2)
+2022-11-18 15:53:40,995:INFO: - Computing loss (training)
+2022-11-18 15:53:41,333:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9082 (1.9082)
+2022-11-18 15:53:41,488:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8826 (1.8954)
+2022-11-18 15:53:41,638:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8626 (1.8850)
+2022-11-18 15:53:41,755:INFO: Dataset: hotel               Batch: 4/4	Loss 2.0312 (1.9072)
+2022-11-18 15:53:42,205:INFO: Dataset: univ                Batch:  1/15	Loss 1.9003 (1.9003)
+2022-11-18 15:53:42,364:INFO: Dataset: univ                Batch:  2/15	Loss 1.8673 (1.8827)
+2022-11-18 15:53:42,521:INFO: Dataset: univ                Batch:  3/15	Loss 1.8303 (1.8665)
+2022-11-18 15:53:42,676:INFO: Dataset: univ                Batch:  4/15	Loss 1.8287 (1.8565)
+2022-11-18 15:53:42,833:INFO: Dataset: univ                Batch:  5/15	Loss 1.8003 (1.8457)
+2022-11-18 15:53:42,993:INFO: Dataset: univ                Batch:  6/15	Loss 1.8483 (1.8461)
+2022-11-18 15:53:43,150:INFO: Dataset: univ                Batch:  7/15	Loss 1.8675 (1.8491)
+2022-11-18 15:53:43,303:INFO: Dataset: univ                Batch:  8/15	Loss 1.8735 (1.8519)
+2022-11-18 15:53:43,460:INFO: Dataset: univ                Batch:  9/15	Loss 1.8222 (1.8489)
+2022-11-18 15:53:43,615:INFO: Dataset: univ                Batch: 10/15	Loss 1.8345 (1.8475)
+2022-11-18 15:53:43,775:INFO: Dataset: univ                Batch: 11/15	Loss 1.8611 (1.8488)
+2022-11-18 15:53:43,932:INFO: Dataset: univ                Batch: 12/15	Loss 1.8411 (1.8483)
+2022-11-18 15:53:44,089:INFO: Dataset: univ                Batch: 13/15	Loss 1.8254 (1.8465)
+2022-11-18 15:53:44,245:INFO: Dataset: univ                Batch: 14/15	Loss 1.8941 (1.8497)
+2022-11-18 15:53:44,327:INFO: Dataset: univ                Batch: 15/15	Loss 1.8409 (1.8496)
+2022-11-18 15:53:44,726:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9419 (1.9419)
+2022-11-18 15:53:44,875:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7478 (1.8378)
+2022-11-18 15:53:45,023:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8952 (1.8572)
+2022-11-18 15:53:45,170:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8014 (1.8437)
+2022-11-18 15:53:45,319:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9511 (1.8662)
+2022-11-18 15:53:45,469:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9386 (1.8784)
+2022-11-18 15:53:45,618:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9610 (1.8899)
+2022-11-18 15:53:45,752:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9659 (1.8976)
+2022-11-18 15:53:46,136:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8145 (1.8145)
+2022-11-18 15:53:46,286:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7993 (1.8069)
+2022-11-18 15:53:46,438:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7913 (1.8015)
+2022-11-18 15:53:46,590:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8055 (1.8026)
+2022-11-18 15:53:46,740:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8141 (1.8051)
+2022-11-18 15:53:46,892:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8442 (1.8118)
+2022-11-18 15:53:47,042:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8288 (1.8146)
+2022-11-18 15:53:47,190:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8048 (1.8134)
+2022-11-18 15:53:47,339:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8625 (1.8182)
+2022-11-18 15:53:47,487:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7604 (1.8125)
+2022-11-18 15:53:47,639:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8238 (1.8137)
+2022-11-18 15:53:47,786:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8180 (1.8140)
+2022-11-18 15:53:47,935:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8174 (1.8143)
+2022-11-18 15:53:48,091:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8412 (1.8163)
+2022-11-18 15:53:48,253:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8269 (1.8170)
+2022-11-18 15:53:48,419:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8104 (1.8165)
+2022-11-18 15:53:48,574:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7927 (1.8151)
+2022-11-18 15:53:48,714:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8888 (1.8186)
+2022-11-18 15:53:48,762:INFO: - Computing loss (validation)
+2022-11-18 15:53:49,028:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9427 (1.9427)
+2022-11-18 15:53:49,063:INFO: Dataset: hotel               Batch: 2/2	Loss 2.2667 (1.9726)
+2022-11-18 15:53:49,373:INFO: Dataset: univ                Batch: 1/3	Loss 1.9277 (1.9277)
+2022-11-18 15:53:49,459:INFO: Dataset: univ                Batch: 2/3	Loss 1.8560 (1.8888)
+2022-11-18 15:53:49,531:INFO: Dataset: univ                Batch: 3/3	Loss 1.8606 (1.8799)
+2022-11-18 15:53:49,829:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9390 (1.9390)
+2022-11-18 15:53:49,874:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8662 (1.9210)
+2022-11-18 15:53:50,175:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8144 (1.8144)
+2022-11-18 15:53:50,251:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7594 (1.7867)
+2022-11-18 15:53:50,325:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7946 (1.7893)
+2022-11-18 15:53:50,401:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9242 (1.8228)
+2022-11-18 15:53:50,475:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8598 (1.8292)
+2022-11-18 15:53:50,527:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_583.pth.tar
+2022-11-18 15:53:50,527:INFO: 
+===> EPOCH: 584 (P2)
+2022-11-18 15:53:50,528:INFO: - Computing loss (training)
+2022-11-18 15:53:50,871:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8686 (1.8686)
+2022-11-18 15:53:51,024:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9300 (1.8981)
+2022-11-18 15:53:51,176:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9183 (1.9046)
+2022-11-18 15:53:51,294:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8549 (1.8961)
+2022-11-18 15:53:51,702:INFO: Dataset: univ                Batch:  1/15	Loss 1.8638 (1.8638)
+2022-11-18 15:53:51,857:INFO: Dataset: univ                Batch:  2/15	Loss 1.8028 (1.8339)
+2022-11-18 15:53:52,014:INFO: Dataset: univ                Batch:  3/15	Loss 1.8320 (1.8332)
+2022-11-18 15:53:52,172:INFO: Dataset: univ                Batch:  4/15	Loss 1.8449 (1.8362)
+2022-11-18 15:53:52,335:INFO: Dataset: univ                Batch:  5/15	Loss 1.8579 (1.8401)
+2022-11-18 15:53:52,493:INFO: Dataset: univ                Batch:  6/15	Loss 1.8302 (1.8385)
+2022-11-18 15:53:52,650:INFO: Dataset: univ                Batch:  7/15	Loss 1.8389 (1.8386)
+2022-11-18 15:53:52,804:INFO: Dataset: univ                Batch:  8/15	Loss 1.7926 (1.8334)
+2022-11-18 15:53:52,971:INFO: Dataset: univ                Batch:  9/15	Loss 1.8893 (1.8400)
+2022-11-18 15:53:53,125:INFO: Dataset: univ                Batch: 10/15	Loss 1.8208 (1.8381)
+2022-11-18 15:53:53,285:INFO: Dataset: univ                Batch: 11/15	Loss 1.8585 (1.8400)
+2022-11-18 15:53:53,441:INFO: Dataset: univ                Batch: 12/15	Loss 1.8587 (1.8416)
+2022-11-18 15:53:53,598:INFO: Dataset: univ                Batch: 13/15	Loss 1.8348 (1.8411)
+2022-11-18 15:53:53,755:INFO: Dataset: univ                Batch: 14/15	Loss 1.8316 (1.8404)
+2022-11-18 15:53:53,838:INFO: Dataset: univ                Batch: 15/15	Loss 1.8355 (1.8403)
+2022-11-18 15:53:54,241:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8203 (1.8203)
+2022-11-18 15:53:54,394:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7709 (1.7961)
+2022-11-18 15:53:54,549:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7348 (1.7744)
+2022-11-18 15:53:54,701:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8168 (1.7855)
+2022-11-18 15:53:54,855:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8955 (1.8087)
+2022-11-18 15:53:55,010:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8672 (1.8187)
+2022-11-18 15:53:55,164:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9075 (1.8312)
+2022-11-18 15:53:55,303:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8178 (1.8298)
+2022-11-18 15:53:55,703:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7812 (1.7812)
+2022-11-18 15:53:55,857:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9356 (1.8590)
+2022-11-18 15:53:56,016:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8910 (1.8692)
+2022-11-18 15:53:56,167:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7757 (1.8457)
+2022-11-18 15:53:56,320:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8323 (1.8430)
+2022-11-18 15:53:56,475:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8121 (1.8380)
+2022-11-18 15:53:56,628:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8399 (1.8382)
+2022-11-18 15:53:56,777:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8560 (1.8406)
+2022-11-18 15:53:56,930:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8433 (1.8409)
+2022-11-18 15:53:57,081:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8252 (1.8393)
+2022-11-18 15:53:57,232:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8440 (1.8398)
+2022-11-18 15:53:57,385:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8171 (1.8380)
+2022-11-18 15:53:57,537:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8931 (1.8422)
+2022-11-18 15:53:57,689:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9064 (1.8469)
+2022-11-18 15:53:57,842:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8612 (1.8479)
+2022-11-18 15:53:57,996:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8225 (1.8464)
+2022-11-18 15:53:58,149:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7536 (1.8403)
+2022-11-18 15:53:58,289:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8770 (1.8422)
+2022-11-18 15:53:58,335:INFO: - Computing loss (validation)
+2022-11-18 15:53:58,617:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8773 (1.8773)
+2022-11-18 15:53:58,654:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9544 (1.8847)
+2022-11-18 15:53:58,970:INFO: Dataset: univ                Batch: 1/3	Loss 1.8294 (1.8294)
+2022-11-18 15:53:59,046:INFO: Dataset: univ                Batch: 2/3	Loss 1.8436 (1.8366)
+2022-11-18 15:53:59,120:INFO: Dataset: univ                Batch: 3/3	Loss 1.7939 (1.8208)
+2022-11-18 15:53:59,421:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9086 (1.9086)
+2022-11-18 15:53:59,466:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9162 (1.9103)
+2022-11-18 15:53:59,767:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9276 (1.9276)
+2022-11-18 15:53:59,847:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7491 (1.8383)
+2022-11-18 15:53:59,928:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8809 (1.8521)
+2022-11-18 15:54:00,007:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8203 (1.8441)
+2022-11-18 15:54:00,085:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8002 (1.8353)
+2022-11-18 15:54:00,139:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_584.pth.tar
+2022-11-18 15:54:00,139:INFO: 
+===> EPOCH: 585 (P2)
+2022-11-18 15:54:00,140:INFO: - Computing loss (training)
+2022-11-18 15:54:00,507:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9254 (1.9254)
+2022-11-18 15:54:00,666:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8594 (1.8930)
+2022-11-18 15:54:00,824:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8898 (1.8919)
+2022-11-18 15:54:00,947:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8365 (1.8827)
+2022-11-18 15:54:01,340:INFO: Dataset: univ                Batch:  1/15	Loss 1.7975 (1.7975)
+2022-11-18 15:54:01,498:INFO: Dataset: univ                Batch:  2/15	Loss 1.8755 (1.8359)
+2022-11-18 15:54:01,655:INFO: Dataset: univ                Batch:  3/15	Loss 1.8606 (1.8445)
+2022-11-18 15:54:01,812:INFO: Dataset: univ                Batch:  4/15	Loss 1.7972 (1.8326)
+2022-11-18 15:54:01,970:INFO: Dataset: univ                Batch:  5/15	Loss 1.8318 (1.8324)
+2022-11-18 15:54:02,128:INFO: Dataset: univ                Batch:  6/15	Loss 1.7807 (1.8239)
+2022-11-18 15:54:02,284:INFO: Dataset: univ                Batch:  7/15	Loss 1.8434 (1.8269)
+2022-11-18 15:54:02,439:INFO: Dataset: univ                Batch:  8/15	Loss 1.8106 (1.8248)
+2022-11-18 15:54:02,594:INFO: Dataset: univ                Batch:  9/15	Loss 1.8159 (1.8238)
+2022-11-18 15:54:02,748:INFO: Dataset: univ                Batch: 10/15	Loss 1.8344 (1.8249)
+2022-11-18 15:54:02,905:INFO: Dataset: univ                Batch: 11/15	Loss 1.7997 (1.8225)
+2022-11-18 15:54:03,061:INFO: Dataset: univ                Batch: 12/15	Loss 1.8769 (1.8276)
+2022-11-18 15:54:03,217:INFO: Dataset: univ                Batch: 13/15	Loss 1.8554 (1.8297)
+2022-11-18 15:54:03,373:INFO: Dataset: univ                Batch: 14/15	Loss 1.8715 (1.8330)
+2022-11-18 15:54:03,456:INFO: Dataset: univ                Batch: 15/15	Loss 1.8026 (1.8326)
+2022-11-18 15:54:03,846:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9374 (1.9374)
+2022-11-18 15:54:04,001:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8621 (1.8975)
+2022-11-18 15:54:04,157:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9327 (1.9086)
+2022-11-18 15:54:04,309:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8521 (1.8938)
+2022-11-18 15:54:04,462:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8649 (1.8881)
+2022-11-18 15:54:04,614:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9601 (1.8995)
+2022-11-18 15:54:04,768:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8520 (1.8922)
+2022-11-18 15:54:04,908:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8678 (1.8897)
+2022-11-18 15:54:05,383:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8230 (1.8230)
+2022-11-18 15:54:05,534:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8340 (1.8284)
+2022-11-18 15:54:05,686:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8185 (1.8251)
+2022-11-18 15:54:05,838:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8641 (1.8343)
+2022-11-18 15:54:05,992:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8834 (1.8439)
+2022-11-18 15:54:06,148:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8069 (1.8380)
+2022-11-18 15:54:06,299:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8079 (1.8336)
+2022-11-18 15:54:06,449:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7380 (1.8203)
+2022-11-18 15:54:06,600:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8503 (1.8234)
+2022-11-18 15:54:06,749:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7816 (1.8193)
+2022-11-18 15:54:06,901:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8354 (1.8209)
+2022-11-18 15:54:07,053:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7113 (1.8117)
+2022-11-18 15:54:07,204:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7978 (1.8108)
+2022-11-18 15:54:07,354:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8987 (1.8173)
+2022-11-18 15:54:07,507:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8121 (1.8169)
+2022-11-18 15:54:07,659:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7605 (1.8135)
+2022-11-18 15:54:07,812:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8693 (1.8166)
+2022-11-18 15:54:07,952:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9299 (1.8217)
+2022-11-18 15:54:07,998:INFO: - Computing loss (validation)
+2022-11-18 15:54:08,267:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8387 (1.8387)
+2022-11-18 15:54:08,303:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8808 (1.8410)
+2022-11-18 15:54:08,616:INFO: Dataset: univ                Batch: 1/3	Loss 1.8239 (1.8239)
+2022-11-18 15:54:08,698:INFO: Dataset: univ                Batch: 2/3	Loss 1.8377 (1.8306)
+2022-11-18 15:54:08,773:INFO: Dataset: univ                Batch: 3/3	Loss 1.8964 (1.8488)
+2022-11-18 15:54:09,071:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7752 (1.7752)
+2022-11-18 15:54:09,115:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8853 (1.8028)
+2022-11-18 15:54:09,436:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8815 (1.8815)
+2022-11-18 15:54:09,514:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8491 (1.8648)
+2022-11-18 15:54:09,591:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8294 (1.8524)
+2022-11-18 15:54:09,668:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8267 (1.8456)
+2022-11-18 15:54:09,745:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8845 (1.8529)
+2022-11-18 15:54:09,797:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_585.pth.tar
+2022-11-18 15:54:09,798:INFO: 
+===> EPOCH: 586 (P2)
+2022-11-18 15:54:09,798:INFO: - Computing loss (training)
+2022-11-18 15:54:10,149:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9527 (1.9527)
+2022-11-18 15:54:10,302:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9303 (1.9408)
+2022-11-18 15:54:10,454:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9199 (1.9340)
+2022-11-18 15:54:10,568:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9137 (1.9304)
+2022-11-18 15:54:10,976:INFO: Dataset: univ                Batch:  1/15	Loss 1.8230 (1.8230)
+2022-11-18 15:54:11,131:INFO: Dataset: univ                Batch:  2/15	Loss 1.8388 (1.8309)
+2022-11-18 15:54:11,291:INFO: Dataset: univ                Batch:  3/15	Loss 1.8254 (1.8291)
+2022-11-18 15:54:11,447:INFO: Dataset: univ                Batch:  4/15	Loss 1.8467 (1.8335)
+2022-11-18 15:54:11,603:INFO: Dataset: univ                Batch:  5/15	Loss 1.8703 (1.8402)
+2022-11-18 15:54:11,762:INFO: Dataset: univ                Batch:  6/15	Loss 1.8551 (1.8427)
+2022-11-18 15:54:11,919:INFO: Dataset: univ                Batch:  7/15	Loss 1.8562 (1.8447)
+2022-11-18 15:54:12,073:INFO: Dataset: univ                Batch:  8/15	Loss 1.8727 (1.8487)
+2022-11-18 15:54:12,228:INFO: Dataset: univ                Batch:  9/15	Loss 1.8077 (1.8446)
+2022-11-18 15:54:12,382:INFO: Dataset: univ                Batch: 10/15	Loss 1.9108 (1.8511)
+2022-11-18 15:54:12,539:INFO: Dataset: univ                Batch: 11/15	Loss 1.8151 (1.8475)
+2022-11-18 15:54:12,695:INFO: Dataset: univ                Batch: 12/15	Loss 1.8489 (1.8476)
+2022-11-18 15:54:12,850:INFO: Dataset: univ                Batch: 13/15	Loss 1.8024 (1.8445)
+2022-11-18 15:54:13,006:INFO: Dataset: univ                Batch: 14/15	Loss 1.8929 (1.8477)
+2022-11-18 15:54:13,089:INFO: Dataset: univ                Batch: 15/15	Loss 1.8748 (1.8480)
+2022-11-18 15:54:13,495:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9372 (1.9372)
+2022-11-18 15:54:13,644:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8373 (1.8876)
+2022-11-18 15:54:13,798:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8352 (1.8698)
+2022-11-18 15:54:13,947:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9268 (1.8853)
+2022-11-18 15:54:14,098:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9213 (1.8929)
+2022-11-18 15:54:14,249:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8734 (1.8896)
+2022-11-18 15:54:14,400:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8729 (1.8871)
+2022-11-18 15:54:14,538:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9319 (1.8926)
+2022-11-18 15:54:14,956:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7822 (1.7822)
+2022-11-18 15:54:15,104:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9056 (1.8417)
+2022-11-18 15:54:15,252:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9031 (1.8611)
+2022-11-18 15:54:15,399:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8802 (1.8661)
+2022-11-18 15:54:15,548:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8105 (1.8544)
+2022-11-18 15:54:15,700:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8545 (1.8544)
+2022-11-18 15:54:15,848:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7513 (1.8403)
+2022-11-18 15:54:16,001:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8336 (1.8395)
+2022-11-18 15:54:16,149:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8409 (1.8396)
+2022-11-18 15:54:16,295:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8888 (1.8444)
+2022-11-18 15:54:16,442:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8093 (1.8410)
+2022-11-18 15:54:16,589:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8393 (1.8409)
+2022-11-18 15:54:16,736:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8371 (1.8406)
+2022-11-18 15:54:16,884:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8016 (1.8379)
+2022-11-18 15:54:17,034:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7951 (1.8350)
+2022-11-18 15:54:17,181:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7748 (1.8310)
+2022-11-18 15:54:17,331:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7895 (1.8286)
+2022-11-18 15:54:17,468:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6629 (1.8201)
+2022-11-18 15:54:17,512:INFO: - Computing loss (validation)
+2022-11-18 15:54:17,772:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9152 (1.9152)
+2022-11-18 15:54:17,807:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0868 (1.9287)
+2022-11-18 15:54:18,117:INFO: Dataset: univ                Batch: 1/3	Loss 1.8013 (1.8013)
+2022-11-18 15:54:18,203:INFO: Dataset: univ                Batch: 2/3	Loss 1.8576 (1.8309)
+2022-11-18 15:54:18,279:INFO: Dataset: univ                Batch: 3/3	Loss 1.8339 (1.8318)
+2022-11-18 15:54:18,585:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9135 (1.9135)
+2022-11-18 15:54:18,632:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8502 (1.8985)
+2022-11-18 15:54:18,967:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8309 (1.8309)
+2022-11-18 15:54:19,049:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8407 (1.8357)
+2022-11-18 15:54:19,127:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7959 (1.8227)
+2022-11-18 15:54:19,207:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7237 (1.7961)
+2022-11-18 15:54:19,287:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8857 (1.8143)
+2022-11-18 15:54:19,340:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_586.pth.tar
+2022-11-18 15:54:19,340:INFO: 
+===> EPOCH: 587 (P2)
+2022-11-18 15:54:19,341:INFO: - Computing loss (training)
+2022-11-18 15:54:19,678:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8362 (1.8362)
+2022-11-18 15:54:19,865:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8401 (1.8382)
+2022-11-18 15:54:20,051:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8447 (1.8404)
+2022-11-18 15:54:20,204:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8411 (1.8406)
+2022-11-18 15:54:20,740:INFO: Dataset: univ                Batch:  1/15	Loss 1.8364 (1.8364)
+2022-11-18 15:54:20,898:INFO: Dataset: univ                Batch:  2/15	Loss 1.8214 (1.8287)
+2022-11-18 15:54:21,062:INFO: Dataset: univ                Batch:  3/15	Loss 1.8661 (1.8424)
+2022-11-18 15:54:21,237:INFO: Dataset: univ                Batch:  4/15	Loss 1.8189 (1.8364)
+2022-11-18 15:54:21,417:INFO: Dataset: univ                Batch:  5/15	Loss 1.8517 (1.8396)
+2022-11-18 15:54:21,606:INFO: Dataset: univ                Batch:  6/15	Loss 1.8350 (1.8389)
+2022-11-18 15:54:21,780:INFO: Dataset: univ                Batch:  7/15	Loss 1.8242 (1.8368)
+2022-11-18 15:54:21,946:INFO: Dataset: univ                Batch:  8/15	Loss 1.9032 (1.8443)
+2022-11-18 15:54:22,111:INFO: Dataset: univ                Batch:  9/15	Loss 1.8477 (1.8446)
+2022-11-18 15:54:22,278:INFO: Dataset: univ                Batch: 10/15	Loss 1.8477 (1.8449)
+2022-11-18 15:54:22,452:INFO: Dataset: univ                Batch: 11/15	Loss 1.8160 (1.8423)
+2022-11-18 15:54:22,635:INFO: Dataset: univ                Batch: 12/15	Loss 1.8473 (1.8427)
+2022-11-18 15:54:22,806:INFO: Dataset: univ                Batch: 13/15	Loss 1.8212 (1.8411)
+2022-11-18 15:54:22,973:INFO: Dataset: univ                Batch: 14/15	Loss 1.8480 (1.8416)
+2022-11-18 15:54:23,062:INFO: Dataset: univ                Batch: 15/15	Loss 1.8463 (1.8416)
+2022-11-18 15:54:23,444:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9005 (1.9005)
+2022-11-18 15:54:23,597:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8925 (1.8966)
+2022-11-18 15:54:23,747:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9121 (1.9019)
+2022-11-18 15:54:23,899:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7253 (1.8525)
+2022-11-18 15:54:24,068:INFO: Dataset: zara1               Batch: 5/8	Loss 1.7741 (1.8359)
+2022-11-18 15:54:24,230:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7996 (1.8298)
+2022-11-18 15:54:24,384:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7323 (1.8153)
+2022-11-18 15:54:24,525:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8471 (1.8185)
+2022-11-18 15:54:24,935:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8109 (1.8109)
+2022-11-18 15:54:25,108:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8828 (1.8473)
+2022-11-18 15:54:25,295:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7877 (1.8257)
+2022-11-18 15:54:25,491:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8703 (1.8378)
+2022-11-18 15:54:25,658:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7296 (1.8165)
+2022-11-18 15:54:25,816:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7601 (1.8069)
+2022-11-18 15:54:25,991:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8265 (1.8097)
+2022-11-18 15:54:26,158:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9321 (1.8259)
+2022-11-18 15:54:26,318:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8423 (1.8276)
+2022-11-18 15:54:26,479:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7593 (1.8207)
+2022-11-18 15:54:26,671:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7754 (1.8165)
+2022-11-18 15:54:26,869:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8881 (1.8230)
+2022-11-18 15:54:27,046:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8275 (1.8234)
+2022-11-18 15:54:27,223:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7626 (1.8194)
+2022-11-18 15:54:27,410:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8961 (1.8252)
+2022-11-18 15:54:27,595:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7739 (1.8219)
+2022-11-18 15:54:27,770:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7192 (1.8155)
+2022-11-18 15:54:27,930:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8275 (1.8161)
+2022-11-18 15:54:27,975:INFO: - Computing loss (validation)
+2022-11-18 15:54:28,264:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8694 (1.8694)
+2022-11-18 15:54:28,308:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8992 (1.8718)
+2022-11-18 15:54:28,679:INFO: Dataset: univ                Batch: 1/3	Loss 1.7887 (1.7887)
+2022-11-18 15:54:28,763:INFO: Dataset: univ                Batch: 2/3	Loss 1.8402 (1.8148)
+2022-11-18 15:54:28,840:INFO: Dataset: univ                Batch: 3/3	Loss 1.9107 (1.8439)
+2022-11-18 15:54:29,151:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7883 (1.7883)
+2022-11-18 15:54:29,196:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9051 (1.8184)
+2022-11-18 15:54:29,520:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7721 (1.7721)
+2022-11-18 15:54:29,598:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7405 (1.7570)
+2022-11-18 15:54:29,681:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7836 (1.7658)
+2022-11-18 15:54:29,763:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8615 (1.7899)
+2022-11-18 15:54:29,843:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7834 (1.7886)
+2022-11-18 15:54:29,914:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_587.pth.tar
+2022-11-18 15:54:29,914:INFO: 
+===> EPOCH: 588 (P2)
+2022-11-18 15:54:29,915:INFO: - Computing loss (training)
+2022-11-18 15:54:30,304:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8513 (1.8513)
+2022-11-18 15:54:30,474:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7528 (1.8011)
+2022-11-18 15:54:30,637:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9644 (1.8573)
+2022-11-18 15:54:30,765:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9453 (1.8724)
+2022-11-18 15:54:31,189:INFO: Dataset: univ                Batch:  1/15	Loss 1.8392 (1.8392)
+2022-11-18 15:54:31,357:INFO: Dataset: univ                Batch:  2/15	Loss 1.8137 (1.8260)
+2022-11-18 15:54:31,531:INFO: Dataset: univ                Batch:  3/15	Loss 1.8767 (1.8435)
+2022-11-18 15:54:31,732:INFO: Dataset: univ                Batch:  4/15	Loss 1.8516 (1.8458)
+2022-11-18 15:54:31,922:INFO: Dataset: univ                Batch:  5/15	Loss 1.8394 (1.8444)
+2022-11-18 15:54:32,122:INFO: Dataset: univ                Batch:  6/15	Loss 1.8397 (1.8436)
+2022-11-18 15:54:32,307:INFO: Dataset: univ                Batch:  7/15	Loss 1.8212 (1.8402)
+2022-11-18 15:54:32,506:INFO: Dataset: univ                Batch:  8/15	Loss 1.8588 (1.8423)
+2022-11-18 15:54:32,684:INFO: Dataset: univ                Batch:  9/15	Loss 1.8310 (1.8411)
+2022-11-18 15:54:32,868:INFO: Dataset: univ                Batch: 10/15	Loss 1.8242 (1.8394)
+2022-11-18 15:54:33,040:INFO: Dataset: univ                Batch: 11/15	Loss 1.8337 (1.8389)
+2022-11-18 15:54:33,210:INFO: Dataset: univ                Batch: 12/15	Loss 1.8430 (1.8392)
+2022-11-18 15:54:33,389:INFO: Dataset: univ                Batch: 13/15	Loss 1.8412 (1.8394)
+2022-11-18 15:54:33,569:INFO: Dataset: univ                Batch: 14/15	Loss 1.8556 (1.8406)
+2022-11-18 15:54:33,663:INFO: Dataset: univ                Batch: 15/15	Loss 1.8222 (1.8403)
+2022-11-18 15:54:34,063:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8512 (1.8512)
+2022-11-18 15:54:34,218:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8658 (1.8583)
+2022-11-18 15:54:34,410:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7103 (1.8097)
+2022-11-18 15:54:34,592:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9714 (1.8511)
+2022-11-18 15:54:34,788:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9531 (1.8693)
+2022-11-18 15:54:34,992:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7814 (1.8565)
+2022-11-18 15:54:35,179:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8490 (1.8555)
+2022-11-18 15:54:35,330:INFO: Dataset: zara1               Batch: 8/8	Loss 1.7905 (1.8485)
+2022-11-18 15:54:35,768:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7736 (1.7736)
+2022-11-18 15:54:35,944:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8513 (1.8131)
+2022-11-18 15:54:36,126:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8739 (1.8325)
+2022-11-18 15:54:36,300:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8573 (1.8384)
+2022-11-18 15:54:36,455:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8829 (1.8474)
+2022-11-18 15:54:36,614:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9730 (1.8667)
+2022-11-18 15:54:36,765:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7588 (1.8504)
+2022-11-18 15:54:36,916:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9503 (1.8634)
+2022-11-18 15:54:37,068:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8668 (1.8638)
+2022-11-18 15:54:37,215:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8500 (1.8625)
+2022-11-18 15:54:37,377:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7785 (1.8551)
+2022-11-18 15:54:37,542:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7648 (1.8479)
+2022-11-18 15:54:37,695:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7369 (1.8393)
+2022-11-18 15:54:37,843:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7825 (1.8354)
+2022-11-18 15:54:37,995:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9423 (1.8428)
+2022-11-18 15:54:38,143:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8549 (1.8436)
+2022-11-18 15:54:38,296:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7800 (1.8401)
+2022-11-18 15:54:38,433:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8716 (1.8419)
+2022-11-18 15:54:38,479:INFO: - Computing loss (validation)
+2022-11-18 15:54:38,768:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8803 (1.8803)
+2022-11-18 15:54:38,806:INFO: Dataset: hotel               Batch: 2/2	Loss 2.1985 (1.9053)
+2022-11-18 15:54:39,138:INFO: Dataset: univ                Batch: 1/3	Loss 1.8541 (1.8541)
+2022-11-18 15:54:39,223:INFO: Dataset: univ                Batch: 2/3	Loss 1.8945 (1.8748)
+2022-11-18 15:54:39,302:INFO: Dataset: univ                Batch: 3/3	Loss 1.7944 (1.8483)
+2022-11-18 15:54:39,611:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9537 (1.9537)
+2022-11-18 15:54:39,656:INFO: Dataset: zara1               Batch: 2/2	Loss 2.1083 (1.9910)
+2022-11-18 15:54:39,979:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8049 (1.8049)
+2022-11-18 15:54:40,054:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8759 (1.8407)
+2022-11-18 15:54:40,130:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8337 (1.8383)
+2022-11-18 15:54:40,206:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7547 (1.8175)
+2022-11-18 15:54:40,281:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8661 (1.8274)
+2022-11-18 15:54:40,333:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_588.pth.tar
+2022-11-18 15:54:40,333:INFO: 
+===> EPOCH: 589 (P2)
+2022-11-18 15:54:40,334:INFO: - Computing loss (training)
+2022-11-18 15:54:40,694:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9832 (1.9832)
+2022-11-18 15:54:40,847:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9097 (1.9467)
+2022-11-18 15:54:40,998:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7369 (1.8782)
+2022-11-18 15:54:41,114:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7857 (1.8631)
+2022-11-18 15:54:41,574:INFO: Dataset: univ                Batch:  1/15	Loss 1.7838 (1.7838)
+2022-11-18 15:54:41,729:INFO: Dataset: univ                Batch:  2/15	Loss 1.8863 (1.8345)
+2022-11-18 15:54:41,885:INFO: Dataset: univ                Batch:  3/15	Loss 1.8592 (1.8429)
+2022-11-18 15:54:42,038:INFO: Dataset: univ                Batch:  4/15	Loss 1.8515 (1.8449)
+2022-11-18 15:54:42,193:INFO: Dataset: univ                Batch:  5/15	Loss 1.7991 (1.8365)
+2022-11-18 15:54:42,348:INFO: Dataset: univ                Batch:  6/15	Loss 1.8773 (1.8434)
+2022-11-18 15:54:42,502:INFO: Dataset: univ                Batch:  7/15	Loss 1.7661 (1.8322)
+2022-11-18 15:54:42,655:INFO: Dataset: univ                Batch:  8/15	Loss 1.8510 (1.8344)
+2022-11-18 15:54:42,810:INFO: Dataset: univ                Batch:  9/15	Loss 1.8397 (1.8350)
+2022-11-18 15:54:42,964:INFO: Dataset: univ                Batch: 10/15	Loss 1.8494 (1.8365)
+2022-11-18 15:54:43,121:INFO: Dataset: univ                Batch: 11/15	Loss 1.8804 (1.8401)
+2022-11-18 15:54:43,274:INFO: Dataset: univ                Batch: 12/15	Loss 1.8639 (1.8421)
+2022-11-18 15:54:43,430:INFO: Dataset: univ                Batch: 13/15	Loss 1.8550 (1.8430)
+2022-11-18 15:54:43,586:INFO: Dataset: univ                Batch: 14/15	Loss 1.8414 (1.8429)
+2022-11-18 15:54:43,668:INFO: Dataset: univ                Batch: 15/15	Loss 1.8850 (1.8434)
+2022-11-18 15:54:44,048:INFO: Dataset: zara1               Batch: 1/8	Loss 2.0325 (2.0325)
+2022-11-18 15:54:44,199:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8343 (1.9353)
+2022-11-18 15:54:44,348:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7174 (1.8610)
+2022-11-18 15:54:44,496:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9295 (1.8782)
+2022-11-18 15:54:44,649:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8439 (1.8714)
+2022-11-18 15:54:44,798:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0062 (1.8932)
+2022-11-18 15:54:44,948:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8682 (1.8895)
+2022-11-18 15:54:45,083:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8906 (1.8896)
+2022-11-18 15:54:45,469:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7570 (1.7570)
+2022-11-18 15:54:45,621:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9565 (1.8522)
+2022-11-18 15:54:45,772:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8369 (1.8472)
+2022-11-18 15:54:45,921:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8024 (1.8365)
+2022-11-18 15:54:46,076:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8988 (1.8501)
+2022-11-18 15:54:46,226:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8588 (1.8515)
+2022-11-18 15:54:46,375:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8988 (1.8586)
+2022-11-18 15:54:46,523:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7880 (1.8507)
+2022-11-18 15:54:46,673:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7902 (1.8441)
+2022-11-18 15:54:46,820:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9877 (1.8592)
+2022-11-18 15:54:46,972:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8779 (1.8610)
+2022-11-18 15:54:47,119:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7768 (1.8534)
+2022-11-18 15:54:47,269:INFO: Dataset: zara2               Batch: 13/18	Loss 1.9103 (1.8574)
+2022-11-18 15:54:47,418:INFO: Dataset: zara2               Batch: 14/18	Loss 1.7925 (1.8531)
+2022-11-18 15:54:47,569:INFO: Dataset: zara2               Batch: 15/18	Loss 1.9180 (1.8574)
+2022-11-18 15:54:47,719:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8609 (1.8576)
+2022-11-18 15:54:47,870:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7760 (1.8529)
+2022-11-18 15:54:48,009:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8365 (1.8521)
+2022-11-18 15:54:48,054:INFO: - Computing loss (validation)
+2022-11-18 15:54:48,337:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9664 (1.9664)
+2022-11-18 15:54:48,372:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8584 (1.9594)
+2022-11-18 15:54:48,683:INFO: Dataset: univ                Batch: 1/3	Loss 1.8202 (1.8202)
+2022-11-18 15:54:48,761:INFO: Dataset: univ                Batch: 2/3	Loss 1.8179 (1.8191)
+2022-11-18 15:54:48,832:INFO: Dataset: univ                Batch: 3/3	Loss 1.8175 (1.8186)
+2022-11-18 15:54:49,149:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8345 (1.8345)
+2022-11-18 15:54:49,195:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8708 (1.8440)
+2022-11-18 15:54:49,499:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7451 (1.7451)
+2022-11-18 15:54:49,577:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8996 (1.8224)
+2022-11-18 15:54:49,650:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7679 (1.8037)
+2022-11-18 15:54:49,724:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8137 (1.8062)
+2022-11-18 15:54:49,797:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8390 (1.8128)
+2022-11-18 15:54:49,855:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_589.pth.tar
+2022-11-18 15:54:49,855:INFO: 
+===> EPOCH: 590 (P2)
+2022-11-18 15:54:49,856:INFO: - Computing loss (training)
+2022-11-18 15:54:50,208:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9867 (1.9867)
+2022-11-18 15:54:50,356:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8685 (1.9238)
+2022-11-18 15:54:50,504:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8452 (1.8973)
+2022-11-18 15:54:50,618:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9892 (1.9133)
+2022-11-18 15:54:51,014:INFO: Dataset: univ                Batch:  1/15	Loss 1.8253 (1.8253)
+2022-11-18 15:54:51,171:INFO: Dataset: univ                Batch:  2/15	Loss 1.9042 (1.8644)
+2022-11-18 15:54:51,328:INFO: Dataset: univ                Batch:  3/15	Loss 1.8583 (1.8621)
+2022-11-18 15:54:51,487:INFO: Dataset: univ                Batch:  4/15	Loss 1.8233 (1.8520)
+2022-11-18 15:54:51,643:INFO: Dataset: univ                Batch:  5/15	Loss 1.8278 (1.8471)
+2022-11-18 15:54:51,798:INFO: Dataset: univ                Batch:  6/15	Loss 1.8287 (1.8441)
+2022-11-18 15:54:51,955:INFO: Dataset: univ                Batch:  7/15	Loss 1.8584 (1.8461)
+2022-11-18 15:54:52,108:INFO: Dataset: univ                Batch:  8/15	Loss 1.8220 (1.8430)
+2022-11-18 15:54:52,262:INFO: Dataset: univ                Batch:  9/15	Loss 1.8397 (1.8427)
+2022-11-18 15:54:52,449:INFO: Dataset: univ                Batch: 10/15	Loss 1.8287 (1.8413)
+2022-11-18 15:54:52,635:INFO: Dataset: univ                Batch: 11/15	Loss 1.8153 (1.8389)
+2022-11-18 15:54:52,796:INFO: Dataset: univ                Batch: 12/15	Loss 1.8433 (1.8393)
+2022-11-18 15:54:52,954:INFO: Dataset: univ                Batch: 13/15	Loss 1.8653 (1.8411)
+2022-11-18 15:54:53,110:INFO: Dataset: univ                Batch: 14/15	Loss 1.8153 (1.8392)
+2022-11-18 15:54:53,192:INFO: Dataset: univ                Batch: 15/15	Loss 1.8681 (1.8396)
+2022-11-18 15:54:53,594:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8467 (1.8467)
+2022-11-18 15:54:53,746:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7253 (1.7872)
+2022-11-18 15:54:53,896:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8811 (1.8193)
+2022-11-18 15:54:54,046:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9642 (1.8520)
+2022-11-18 15:54:54,196:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8523 (1.8521)
+2022-11-18 15:54:54,347:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8216 (1.8468)
+2022-11-18 15:54:54,497:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8077 (1.8413)
+2022-11-18 15:54:54,633:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8157 (1.8384)
+2022-11-18 15:54:55,035:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8444 (1.8444)
+2022-11-18 15:54:55,189:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8921 (1.8672)
+2022-11-18 15:54:55,344:INFO: Dataset: zara2               Batch:  3/18	Loss 1.9241 (1.8865)
+2022-11-18 15:54:55,500:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8883 (1.8870)
+2022-11-18 15:54:55,652:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7539 (1.8610)
+2022-11-18 15:54:55,806:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8215 (1.8540)
+2022-11-18 15:54:55,958:INFO: Dataset: zara2               Batch:  7/18	Loss 1.7931 (1.8454)
+2022-11-18 15:54:56,108:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9205 (1.8544)
+2022-11-18 15:54:56,258:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7998 (1.8480)
+2022-11-18 15:54:56,407:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7937 (1.8430)
+2022-11-18 15:54:56,566:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8023 (1.8393)
+2022-11-18 15:54:56,718:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7880 (1.8352)
+2022-11-18 15:54:56,870:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8043 (1.8331)
+2022-11-18 15:54:57,031:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8059 (1.8312)
+2022-11-18 15:54:57,192:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8430 (1.8319)
+2022-11-18 15:54:57,350:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8053 (1.8303)
+2022-11-18 15:54:57,511:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7165 (1.8235)
+2022-11-18 15:54:57,658:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7853 (1.8217)
+2022-11-18 15:54:57,703:INFO: - Computing loss (validation)
+2022-11-18 15:54:57,964:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9371 (1.9371)
+2022-11-18 15:54:57,998:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8683 (1.9312)
+2022-11-18 15:54:58,307:INFO: Dataset: univ                Batch: 1/3	Loss 1.8603 (1.8603)
+2022-11-18 15:54:58,386:INFO: Dataset: univ                Batch: 2/3	Loss 1.8488 (1.8545)
+2022-11-18 15:54:58,457:INFO: Dataset: univ                Batch: 3/3	Loss 1.8535 (1.8542)
+2022-11-18 15:54:58,764:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8228 (1.8228)
+2022-11-18 15:54:58,810:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9132 (1.8461)
+2022-11-18 15:54:59,125:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8501 (1.8501)
+2022-11-18 15:54:59,200:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8851 (1.8675)
+2022-11-18 15:54:59,276:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8025 (1.8456)
+2022-11-18 15:54:59,351:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8169 (1.8384)
+2022-11-18 15:54:59,425:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8202 (1.8349)
+2022-11-18 15:54:59,476:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_590.pth.tar
+2022-11-18 15:54:59,476:INFO: 
+===> EPOCH: 591 (P2)
+2022-11-18 15:54:59,477:INFO: - Computing loss (training)
+2022-11-18 15:54:59,830:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8730 (1.8730)
+2022-11-18 15:54:59,994:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9713 (1.9214)
+2022-11-18 15:55:00,156:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8614 (1.9009)
+2022-11-18 15:55:00,280:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8604 (1.8944)
+2022-11-18 15:55:00,685:INFO: Dataset: univ                Batch:  1/15	Loss 1.8590 (1.8590)
+2022-11-18 15:55:00,844:INFO: Dataset: univ                Batch:  2/15	Loss 1.8586 (1.8588)
+2022-11-18 15:55:01,005:INFO: Dataset: univ                Batch:  3/15	Loss 1.8275 (1.8481)
+2022-11-18 15:55:01,160:INFO: Dataset: univ                Batch:  4/15	Loss 1.8734 (1.8547)
+2022-11-18 15:55:01,321:INFO: Dataset: univ                Batch:  5/15	Loss 1.8606 (1.8557)
+2022-11-18 15:55:01,480:INFO: Dataset: univ                Batch:  6/15	Loss 1.8193 (1.8491)
+2022-11-18 15:55:01,637:INFO: Dataset: univ                Batch:  7/15	Loss 1.8997 (1.8565)
+2022-11-18 15:55:01,791:INFO: Dataset: univ                Batch:  8/15	Loss 1.8785 (1.8591)
+2022-11-18 15:55:01,950:INFO: Dataset: univ                Batch:  9/15	Loss 1.8310 (1.8557)
+2022-11-18 15:55:02,105:INFO: Dataset: univ                Batch: 10/15	Loss 1.8305 (1.8530)
+2022-11-18 15:55:02,264:INFO: Dataset: univ                Batch: 11/15	Loss 1.8597 (1.8536)
+2022-11-18 15:55:02,418:INFO: Dataset: univ                Batch: 12/15	Loss 1.9009 (1.8576)
+2022-11-18 15:55:02,576:INFO: Dataset: univ                Batch: 13/15	Loss 1.7906 (1.8526)
+2022-11-18 15:55:02,733:INFO: Dataset: univ                Batch: 14/15	Loss 1.8594 (1.8531)
+2022-11-18 15:55:02,816:INFO: Dataset: univ                Batch: 15/15	Loss 1.8879 (1.8536)
+2022-11-18 15:55:03,209:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9599 (1.9599)
+2022-11-18 15:55:03,361:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8809 (1.9204)
+2022-11-18 15:55:03,516:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9042 (1.9149)
+2022-11-18 15:55:03,664:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8739 (1.9040)
+2022-11-18 15:55:03,815:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9152 (1.9063)
+2022-11-18 15:55:03,966:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8340 (1.8928)
+2022-11-18 15:55:04,117:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8404 (1.8860)
+2022-11-18 15:55:04,254:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9899 (1.8978)
+2022-11-18 15:55:04,638:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7959 (1.7959)
+2022-11-18 15:55:04,790:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8768 (1.8348)
+2022-11-18 15:55:04,947:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8112 (1.8274)
+2022-11-18 15:55:05,100:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8374 (1.8300)
+2022-11-18 15:55:05,254:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8716 (1.8382)
+2022-11-18 15:55:05,404:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8980 (1.8480)
+2022-11-18 15:55:05,554:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8422 (1.8472)
+2022-11-18 15:55:05,701:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8280 (1.8448)
+2022-11-18 15:55:05,851:INFO: Dataset: zara2               Batch:  9/18	Loss 1.9675 (1.8581)
+2022-11-18 15:55:05,999:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7160 (1.8436)
+2022-11-18 15:55:06,151:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8538 (1.8445)
+2022-11-18 15:55:06,297:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8650 (1.8462)
+2022-11-18 15:55:06,447:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8955 (1.8499)
+2022-11-18 15:55:06,597:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8247 (1.8482)
+2022-11-18 15:55:06,748:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7267 (1.8394)
+2022-11-18 15:55:06,896:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8083 (1.8374)
+2022-11-18 15:55:07,047:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8447 (1.8379)
+2022-11-18 15:55:07,184:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9424 (1.8428)
+2022-11-18 15:55:07,234:INFO: - Computing loss (validation)
+2022-11-18 15:55:07,497:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9911 (1.9911)
+2022-11-18 15:55:07,532:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8597 (1.9813)
+2022-11-18 15:55:07,845:INFO: Dataset: univ                Batch: 1/3	Loss 1.8100 (1.8100)
+2022-11-18 15:55:07,923:INFO: Dataset: univ                Batch: 2/3	Loss 1.8485 (1.8295)
+2022-11-18 15:55:07,995:INFO: Dataset: univ                Batch: 3/3	Loss 1.8991 (1.8514)
+2022-11-18 15:55:08,295:INFO: Dataset: zara1               Batch: 1/2	Loss 1.8306 (1.8306)
+2022-11-18 15:55:08,339:INFO: Dataset: zara1               Batch: 2/2	Loss 1.8469 (1.8343)
+2022-11-18 15:55:08,645:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9072 (1.9072)
+2022-11-18 15:55:08,720:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7770 (1.8421)
+2022-11-18 15:55:08,795:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8233 (1.8358)
+2022-11-18 15:55:08,871:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8400 (1.8369)
+2022-11-18 15:55:08,946:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8649 (1.8426)
+2022-11-18 15:55:08,998:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_591.pth.tar
+2022-11-18 15:55:08,998:INFO: 
+===> EPOCH: 592 (P2)
+2022-11-18 15:55:08,998:INFO: - Computing loss (training)
+2022-11-18 15:55:09,349:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7564 (1.7564)
+2022-11-18 15:55:09,504:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9279 (1.8455)
+2022-11-18 15:55:09,655:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7649 (1.8169)
+2022-11-18 15:55:09,772:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8652 (1.8248)
+2022-11-18 15:55:10,215:INFO: Dataset: univ                Batch:  1/15	Loss 1.8608 (1.8608)
+2022-11-18 15:55:10,373:INFO: Dataset: univ                Batch:  2/15	Loss 1.8445 (1.8525)
+2022-11-18 15:55:10,532:INFO: Dataset: univ                Batch:  3/15	Loss 1.9196 (1.8714)
+2022-11-18 15:55:10,687:INFO: Dataset: univ                Batch:  4/15	Loss 1.8103 (1.8561)
+2022-11-18 15:55:10,843:INFO: Dataset: univ                Batch:  5/15	Loss 1.8690 (1.8588)
+2022-11-18 15:55:11,002:INFO: Dataset: univ                Batch:  6/15	Loss 1.8490 (1.8573)
+2022-11-18 15:55:11,159:INFO: Dataset: univ                Batch:  7/15	Loss 1.7816 (1.8461)
+2022-11-18 15:55:11,312:INFO: Dataset: univ                Batch:  8/15	Loss 1.8866 (1.8509)
+2022-11-18 15:55:11,469:INFO: Dataset: univ                Batch:  9/15	Loss 1.8569 (1.8516)
+2022-11-18 15:55:11,622:INFO: Dataset: univ                Batch: 10/15	Loss 1.8363 (1.8499)
+2022-11-18 15:55:11,778:INFO: Dataset: univ                Batch: 11/15	Loss 1.8280 (1.8479)
+2022-11-18 15:55:11,934:INFO: Dataset: univ                Batch: 12/15	Loss 1.8509 (1.8482)
+2022-11-18 15:55:12,090:INFO: Dataset: univ                Batch: 13/15	Loss 1.8383 (1.8473)
+2022-11-18 15:55:12,246:INFO: Dataset: univ                Batch: 14/15	Loss 1.8184 (1.8452)
+2022-11-18 15:55:12,328:INFO: Dataset: univ                Batch: 15/15	Loss 1.7998 (1.8446)
+2022-11-18 15:55:12,715:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7032 (1.7032)
+2022-11-18 15:55:12,868:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8033 (1.7559)
+2022-11-18 15:55:13,017:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7668 (1.7592)
+2022-11-18 15:55:13,164:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8794 (1.7891)
+2022-11-18 15:55:13,314:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8528 (1.8037)
+2022-11-18 15:55:13,464:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8076 (1.8044)
+2022-11-18 15:55:13,614:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8502 (1.8106)
+2022-11-18 15:55:13,750:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8348 (1.8132)
+2022-11-18 15:55:14,148:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8224 (1.8224)
+2022-11-18 15:55:14,300:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8784 (1.8495)
+2022-11-18 15:55:14,451:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8658 (1.8545)
+2022-11-18 15:55:14,604:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8943 (1.8648)
+2022-11-18 15:55:14,757:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8436 (1.8607)
+2022-11-18 15:55:14,909:INFO: Dataset: zara2               Batch:  6/18	Loss 1.9080 (1.8684)
+2022-11-18 15:55:15,059:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8384 (1.8641)
+2022-11-18 15:55:15,207:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7541 (1.8504)
+2022-11-18 15:55:15,356:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8115 (1.8462)
+2022-11-18 15:55:15,505:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8365 (1.8452)
+2022-11-18 15:55:15,656:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8160 (1.8427)
+2022-11-18 15:55:15,805:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8749 (1.8451)
+2022-11-18 15:55:15,956:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8113 (1.8425)
+2022-11-18 15:55:16,106:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8367 (1.8421)
+2022-11-18 15:55:16,257:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7507 (1.8365)
+2022-11-18 15:55:16,406:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7752 (1.8326)
+2022-11-18 15:55:16,557:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9020 (1.8364)
+2022-11-18 15:55:16,695:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8381 (1.8365)
+2022-11-18 15:55:16,740:INFO: - Computing loss (validation)
+2022-11-18 15:55:17,016:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8579 (1.8579)
+2022-11-18 15:55:17,052:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9815 (1.8664)
+2022-11-18 15:55:17,372:INFO: Dataset: univ                Batch: 1/3	Loss 1.8627 (1.8627)
+2022-11-18 15:55:17,447:INFO: Dataset: univ                Batch: 2/3	Loss 1.8587 (1.8608)
+2022-11-18 15:55:17,522:INFO: Dataset: univ                Batch: 3/3	Loss 1.8055 (1.8430)
+2022-11-18 15:55:17,824:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9191 (1.9191)
+2022-11-18 15:55:17,872:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0016 (1.9390)
+2022-11-18 15:55:18,173:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7971 (1.7971)
+2022-11-18 15:55:18,248:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8364 (1.8160)
+2022-11-18 15:55:18,325:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8850 (1.8376)
+2022-11-18 15:55:18,401:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8727 (1.8463)
+2022-11-18 15:55:18,476:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8409 (1.8452)
+2022-11-18 15:55:18,528:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_592.pth.tar
+2022-11-18 15:55:18,528:INFO: 
+===> EPOCH: 593 (P2)
+2022-11-18 15:55:18,529:INFO: - Computing loss (training)
+2022-11-18 15:55:18,864:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8211 (1.8211)
+2022-11-18 15:55:19,015:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8573 (1.8384)
+2022-11-18 15:55:19,164:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9172 (1.8652)
+2022-11-18 15:55:19,279:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8858 (1.8688)
+2022-11-18 15:55:19,727:INFO: Dataset: univ                Batch:  1/15	Loss 1.8498 (1.8498)
+2022-11-18 15:55:19,882:INFO: Dataset: univ                Batch:  2/15	Loss 1.8604 (1.8549)
+2022-11-18 15:55:20,041:INFO: Dataset: univ                Batch:  3/15	Loss 1.8489 (1.8530)
+2022-11-18 15:55:20,194:INFO: Dataset: univ                Batch:  4/15	Loss 1.8475 (1.8517)
+2022-11-18 15:55:20,349:INFO: Dataset: univ                Batch:  5/15	Loss 1.8279 (1.8466)
+2022-11-18 15:55:20,505:INFO: Dataset: univ                Batch:  6/15	Loss 1.8592 (1.8486)
+2022-11-18 15:55:20,661:INFO: Dataset: univ                Batch:  7/15	Loss 1.8089 (1.8426)
+2022-11-18 15:55:20,814:INFO: Dataset: univ                Batch:  8/15	Loss 1.8391 (1.8422)
+2022-11-18 15:55:20,971:INFO: Dataset: univ                Batch:  9/15	Loss 1.8353 (1.8414)
+2022-11-18 15:55:21,126:INFO: Dataset: univ                Batch: 10/15	Loss 1.8249 (1.8396)
+2022-11-18 15:55:21,283:INFO: Dataset: univ                Batch: 11/15	Loss 1.8795 (1.8429)
+2022-11-18 15:55:21,436:INFO: Dataset: univ                Batch: 12/15	Loss 1.7916 (1.8383)
+2022-11-18 15:55:21,593:INFO: Dataset: univ                Batch: 13/15	Loss 1.8472 (1.8389)
+2022-11-18 15:55:21,748:INFO: Dataset: univ                Batch: 14/15	Loss 1.8408 (1.8391)
+2022-11-18 15:55:21,831:INFO: Dataset: univ                Batch: 15/15	Loss 1.8325 (1.8390)
+2022-11-18 15:55:22,215:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8647 (1.8647)
+2022-11-18 15:55:22,365:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8693 (1.8671)
+2022-11-18 15:55:22,518:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7214 (1.8197)
+2022-11-18 15:55:22,669:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8420 (1.8245)
+2022-11-18 15:55:22,819:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8720 (1.8349)
+2022-11-18 15:55:22,971:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8490 (1.8371)
+2022-11-18 15:55:23,121:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8574 (1.8401)
+2022-11-18 15:55:23,258:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8942 (1.8461)
+2022-11-18 15:55:23,658:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7436 (1.7436)
+2022-11-18 15:55:23,810:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8016 (1.7744)
+2022-11-18 15:55:23,965:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8044 (1.7843)
+2022-11-18 15:55:24,114:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8564 (1.8031)
+2022-11-18 15:55:24,267:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8134 (1.8052)
+2022-11-18 15:55:24,418:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8743 (1.8170)
+2022-11-18 15:55:24,568:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8900 (1.8273)
+2022-11-18 15:55:24,717:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8781 (1.8329)
+2022-11-18 15:55:24,867:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8836 (1.8383)
+2022-11-18 15:55:25,017:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9069 (1.8446)
+2022-11-18 15:55:25,169:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8465 (1.8448)
+2022-11-18 15:55:25,317:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8989 (1.8491)
+2022-11-18 15:55:25,468:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8224 (1.8470)
+2022-11-18 15:55:25,618:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9279 (1.8527)
+2022-11-18 15:55:25,770:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8014 (1.8494)
+2022-11-18 15:55:25,919:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8191 (1.8474)
+2022-11-18 15:55:26,071:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8230 (1.8461)
+2022-11-18 15:55:26,210:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8299 (1.8453)
+2022-11-18 15:55:26,255:INFO: - Computing loss (validation)
+2022-11-18 15:55:26,525:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8931 (1.8931)
+2022-11-18 15:55:26,561:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7369 (1.8803)
+2022-11-18 15:55:26,876:INFO: Dataset: univ                Batch: 1/3	Loss 1.8449 (1.8449)
+2022-11-18 15:55:26,952:INFO: Dataset: univ                Batch: 2/3	Loss 1.7370 (1.7977)
+2022-11-18 15:55:27,028:INFO: Dataset: univ                Batch: 3/3	Loss 1.8268 (1.8075)
+2022-11-18 15:55:27,332:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9443 (1.9443)
+2022-11-18 15:55:27,378:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7032 (1.8815)
+2022-11-18 15:55:27,712:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8323 (1.8323)
+2022-11-18 15:55:27,788:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8704 (1.8507)
+2022-11-18 15:55:27,862:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9417 (1.8818)
+2022-11-18 15:55:27,937:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8703 (1.8790)
+2022-11-18 15:55:28,011:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9007 (1.8831)
+2022-11-18 15:55:28,067:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_593.pth.tar
+2022-11-18 15:55:28,067:INFO: 
+===> EPOCH: 594 (P2)
+2022-11-18 15:55:28,068:INFO: - Computing loss (training)
+2022-11-18 15:55:28,485:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8278 (1.8278)
+2022-11-18 15:55:28,635:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8972 (1.8617)
+2022-11-18 15:55:28,787:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8791 (1.8679)
+2022-11-18 15:55:28,902:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8793 (1.8700)
+2022-11-18 15:55:29,303:INFO: Dataset: univ                Batch:  1/15	Loss 1.8565 (1.8565)
+2022-11-18 15:55:29,461:INFO: Dataset: univ                Batch:  2/15	Loss 1.8437 (1.8505)
+2022-11-18 15:55:29,620:INFO: Dataset: univ                Batch:  3/15	Loss 1.8501 (1.8504)
+2022-11-18 15:55:29,777:INFO: Dataset: univ                Batch:  4/15	Loss 1.8520 (1.8508)
+2022-11-18 15:55:29,936:INFO: Dataset: univ                Batch:  5/15	Loss 1.8454 (1.8498)
+2022-11-18 15:55:30,094:INFO: Dataset: univ                Batch:  6/15	Loss 1.8336 (1.8471)
+2022-11-18 15:55:30,250:INFO: Dataset: univ                Batch:  7/15	Loss 1.8063 (1.8415)
+2022-11-18 15:55:30,403:INFO: Dataset: univ                Batch:  8/15	Loss 1.8331 (1.8405)
+2022-11-18 15:55:30,560:INFO: Dataset: univ                Batch:  9/15	Loss 1.8330 (1.8396)
+2022-11-18 15:55:30,713:INFO: Dataset: univ                Batch: 10/15	Loss 1.8397 (1.8397)
+2022-11-18 15:55:30,869:INFO: Dataset: univ                Batch: 11/15	Loss 1.8502 (1.8406)
+2022-11-18 15:55:31,024:INFO: Dataset: univ                Batch: 12/15	Loss 1.8381 (1.8404)
+2022-11-18 15:55:31,181:INFO: Dataset: univ                Batch: 13/15	Loss 1.8391 (1.8403)
+2022-11-18 15:55:31,337:INFO: Dataset: univ                Batch: 14/15	Loss 1.8704 (1.8425)
+2022-11-18 15:55:31,420:INFO: Dataset: univ                Batch: 15/15	Loss 1.8629 (1.8427)
+2022-11-18 15:55:31,808:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8534 (1.8534)
+2022-11-18 15:55:31,956:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8459 (1.8497)
+2022-11-18 15:55:32,108:INFO: Dataset: zara1               Batch: 3/8	Loss 2.0233 (1.9014)
+2022-11-18 15:55:32,255:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7993 (1.8743)
+2022-11-18 15:55:32,405:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8727 (1.8740)
+2022-11-18 15:55:32,555:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9321 (1.8840)
+2022-11-18 15:55:32,703:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7490 (1.8661)
+2022-11-18 15:55:32,839:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8874 (1.8683)
+2022-11-18 15:55:33,233:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7986 (1.7986)
+2022-11-18 15:55:33,383:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8804 (1.8387)
+2022-11-18 15:55:33,534:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8561 (1.8446)
+2022-11-18 15:55:33,688:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8897 (1.8557)
+2022-11-18 15:55:33,839:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8228 (1.8488)
+2022-11-18 15:55:33,992:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8515 (1.8493)
+2022-11-18 15:55:34,142:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8111 (1.8437)
+2022-11-18 15:55:34,290:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7736 (1.8355)
+2022-11-18 15:55:34,441:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8027 (1.8319)
+2022-11-18 15:55:34,590:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7869 (1.8273)
+2022-11-18 15:55:34,741:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8147 (1.8262)
+2022-11-18 15:55:34,890:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8410 (1.8274)
+2022-11-18 15:55:35,044:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8942 (1.8328)
+2022-11-18 15:55:35,194:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8485 (1.8340)
+2022-11-18 15:55:35,345:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8637 (1.8362)
+2022-11-18 15:55:35,496:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8167 (1.8349)
+2022-11-18 15:55:35,648:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9041 (1.8388)
+2022-11-18 15:55:35,786:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8879 (1.8413)
+2022-11-18 15:55:35,831:INFO: - Computing loss (validation)
+2022-11-18 15:55:36,094:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8829 (1.8829)
+2022-11-18 15:55:36,128:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6594 (1.8669)
+2022-11-18 15:55:36,448:INFO: Dataset: univ                Batch: 1/3	Loss 1.8492 (1.8492)
+2022-11-18 15:55:36,529:INFO: Dataset: univ                Batch: 2/3	Loss 1.8169 (1.8359)
+2022-11-18 15:55:36,605:INFO: Dataset: univ                Batch: 3/3	Loss 1.8353 (1.8357)
+2022-11-18 15:55:36,908:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9199 (1.9199)
+2022-11-18 15:55:36,952:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6857 (1.8620)
+2022-11-18 15:55:37,286:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8410 (1.8410)
+2022-11-18 15:55:37,366:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8619 (1.8515)
+2022-11-18 15:55:37,446:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8999 (1.8672)
+2022-11-18 15:55:37,527:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8641 (1.8664)
+2022-11-18 15:55:37,606:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8474 (1.8624)
+2022-11-18 15:55:37,659:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_594.pth.tar
+2022-11-18 15:55:37,659:INFO: 
+===> EPOCH: 595 (P2)
+2022-11-18 15:55:37,660:INFO: - Computing loss (training)
+2022-11-18 15:55:38,015:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8627 (1.8627)
+2022-11-18 15:55:38,167:INFO: Dataset: hotel               Batch: 2/4	Loss 1.9358 (1.8989)
+2022-11-18 15:55:38,317:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9219 (1.9067)
+2022-11-18 15:55:38,433:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8580 (1.8980)
+2022-11-18 15:55:38,858:INFO: Dataset: univ                Batch:  1/15	Loss 1.8376 (1.8376)
+2022-11-18 15:55:39,028:INFO: Dataset: univ                Batch:  2/15	Loss 1.8307 (1.8343)
+2022-11-18 15:55:39,202:INFO: Dataset: univ                Batch:  3/15	Loss 1.9040 (1.8578)
+2022-11-18 15:55:39,368:INFO: Dataset: univ                Batch:  4/15	Loss 1.8575 (1.8577)
+2022-11-18 15:55:39,536:INFO: Dataset: univ                Batch:  5/15	Loss 1.8172 (1.8492)
+2022-11-18 15:55:39,705:INFO: Dataset: univ                Batch:  6/15	Loss 1.8546 (1.8501)
+2022-11-18 15:55:39,871:INFO: Dataset: univ                Batch:  7/15	Loss 1.7935 (1.8416)
+2022-11-18 15:55:40,036:INFO: Dataset: univ                Batch:  8/15	Loss 1.8134 (1.8384)
+2022-11-18 15:55:40,202:INFO: Dataset: univ                Batch:  9/15	Loss 1.8692 (1.8417)
+2022-11-18 15:55:40,366:INFO: Dataset: univ                Batch: 10/15	Loss 1.7763 (1.8358)
+2022-11-18 15:55:40,533:INFO: Dataset: univ                Batch: 11/15	Loss 1.9254 (1.8448)
+2022-11-18 15:55:40,700:INFO: Dataset: univ                Batch: 12/15	Loss 1.8025 (1.8412)
+2022-11-18 15:55:40,866:INFO: Dataset: univ                Batch: 13/15	Loss 1.8518 (1.8421)
+2022-11-18 15:55:41,033:INFO: Dataset: univ                Batch: 14/15	Loss 1.7802 (1.8377)
+2022-11-18 15:55:41,122:INFO: Dataset: univ                Batch: 15/15	Loss 1.8339 (1.8376)
+2022-11-18 15:55:41,514:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7970 (1.7970)
+2022-11-18 15:55:41,666:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8031 (1.8001)
+2022-11-18 15:55:41,817:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8711 (1.8238)
+2022-11-18 15:55:41,968:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7963 (1.8175)
+2022-11-18 15:55:42,118:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8477 (1.8237)
+2022-11-18 15:55:42,268:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9232 (1.8422)
+2022-11-18 15:55:42,417:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8527 (1.8436)
+2022-11-18 15:55:42,554:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0162 (1.8630)
+2022-11-18 15:55:42,944:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8183 (1.8183)
+2022-11-18 15:55:43,100:INFO: Dataset: zara2               Batch:  2/18	Loss 1.7368 (1.7760)
+2022-11-18 15:55:43,251:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7361 (1.7637)
+2022-11-18 15:55:43,400:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8959 (1.7964)
+2022-11-18 15:55:43,554:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7265 (1.7833)
+2022-11-18 15:55:43,705:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7841 (1.7834)
+2022-11-18 15:55:43,856:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8476 (1.7922)
+2022-11-18 15:55:44,004:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7555 (1.7880)
+2022-11-18 15:55:44,155:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7770 (1.7867)
+2022-11-18 15:55:44,303:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9037 (1.7972)
+2022-11-18 15:55:44,455:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8113 (1.7985)
+2022-11-18 15:55:44,604:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7510 (1.7948)
+2022-11-18 15:55:44,753:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7777 (1.7935)
+2022-11-18 15:55:44,902:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8409 (1.7968)
+2022-11-18 15:55:45,053:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7913 (1.7964)
+2022-11-18 15:55:45,202:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8934 (1.8024)
+2022-11-18 15:55:45,354:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8789 (1.8068)
+2022-11-18 15:55:45,491:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9191 (1.8124)
+2022-11-18 15:55:45,535:INFO: - Computing loss (validation)
+2022-11-18 15:55:45,812:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9226 (1.9226)
+2022-11-18 15:55:45,846:INFO: Dataset: hotel               Batch: 2/2	Loss 1.7759 (1.9111)
+2022-11-18 15:55:46,170:INFO: Dataset: univ                Batch: 1/3	Loss 1.8904 (1.8904)
+2022-11-18 15:55:46,253:INFO: Dataset: univ                Batch: 2/3	Loss 1.8259 (1.8601)
+2022-11-18 15:55:46,330:INFO: Dataset: univ                Batch: 3/3	Loss 1.8670 (1.8621)
+2022-11-18 15:55:46,638:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9933 (1.9933)
+2022-11-18 15:55:46,683:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7344 (1.9225)
+2022-11-18 15:55:46,986:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8207 (1.8207)
+2022-11-18 15:55:47,061:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9568 (1.8878)
+2022-11-18 15:55:47,138:INFO: Dataset: zara2               Batch: 3/5	Loss 1.8538 (1.8761)
+2022-11-18 15:55:47,214:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8897 (1.8795)
+2022-11-18 15:55:47,289:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8420 (1.8721)
+2022-11-18 15:55:47,343:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_595.pth.tar
+2022-11-18 15:55:47,343:INFO: 
+===> EPOCH: 596 (P2)
+2022-11-18 15:55:47,344:INFO: - Computing loss (training)
+2022-11-18 15:55:47,694:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9289 (1.9289)
+2022-11-18 15:55:47,851:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8252 (1.8758)
+2022-11-18 15:55:48,006:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7761 (1.8442)
+2022-11-18 15:55:48,124:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8089 (1.8383)
+2022-11-18 15:55:48,536:INFO: Dataset: univ                Batch:  1/15	Loss 1.8567 (1.8567)
+2022-11-18 15:55:48,704:INFO: Dataset: univ                Batch:  2/15	Loss 1.7724 (1.8145)
+2022-11-18 15:55:48,872:INFO: Dataset: univ                Batch:  3/15	Loss 1.8238 (1.8175)
+2022-11-18 15:55:49,043:INFO: Dataset: univ                Batch:  4/15	Loss 1.8345 (1.8219)
+2022-11-18 15:55:49,211:INFO: Dataset: univ                Batch:  5/15	Loss 1.8094 (1.8195)
+2022-11-18 15:55:49,378:INFO: Dataset: univ                Batch:  6/15	Loss 1.8200 (1.8195)
+2022-11-18 15:55:49,544:INFO: Dataset: univ                Batch:  7/15	Loss 1.8832 (1.8286)
+2022-11-18 15:55:49,708:INFO: Dataset: univ                Batch:  8/15	Loss 1.8793 (1.8355)
+2022-11-18 15:55:49,874:INFO: Dataset: univ                Batch:  9/15	Loss 1.8720 (1.8401)
+2022-11-18 15:55:50,041:INFO: Dataset: univ                Batch: 10/15	Loss 1.8037 (1.8364)
+2022-11-18 15:55:50,207:INFO: Dataset: univ                Batch: 11/15	Loss 1.8124 (1.8340)
+2022-11-18 15:55:50,373:INFO: Dataset: univ                Batch: 12/15	Loss 1.8355 (1.8341)
+2022-11-18 15:55:50,540:INFO: Dataset: univ                Batch: 13/15	Loss 1.8320 (1.8340)
+2022-11-18 15:55:50,706:INFO: Dataset: univ                Batch: 14/15	Loss 1.8170 (1.8327)
+2022-11-18 15:55:50,795:INFO: Dataset: univ                Batch: 15/15	Loss 1.8518 (1.8330)
+2022-11-18 15:55:51,187:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8950 (1.8950)
+2022-11-18 15:55:51,337:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8402 (1.8694)
+2022-11-18 15:55:51,491:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8338 (1.8570)
+2022-11-18 15:55:51,641:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9570 (1.8828)
+2022-11-18 15:55:51,793:INFO: Dataset: zara1               Batch: 5/8	Loss 1.8892 (1.8840)
+2022-11-18 15:55:51,943:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8556 (1.8793)
+2022-11-18 15:55:52,094:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8954 (1.8815)
+2022-11-18 15:55:52,230:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0108 (1.8953)
+2022-11-18 15:55:52,643:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8512 (1.8512)
+2022-11-18 15:55:52,800:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8581 (1.8544)
+2022-11-18 15:55:52,962:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7633 (1.8249)
+2022-11-18 15:55:53,121:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9121 (1.8452)
+2022-11-18 15:55:53,279:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7977 (1.8348)
+2022-11-18 15:55:53,437:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8290 (1.8338)
+2022-11-18 15:55:53,594:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8439 (1.8353)
+2022-11-18 15:55:53,750:INFO: Dataset: zara2               Batch:  8/18	Loss 1.7662 (1.8264)
+2022-11-18 15:55:53,908:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8399 (1.8278)
+2022-11-18 15:55:54,064:INFO: Dataset: zara2               Batch: 10/18	Loss 1.9027 (1.8351)
+2022-11-18 15:55:54,223:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8214 (1.8340)
+2022-11-18 15:55:54,378:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7856 (1.8299)
+2022-11-18 15:55:54,536:INFO: Dataset: zara2               Batch: 13/18	Loss 1.7695 (1.8252)
+2022-11-18 15:55:54,691:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8877 (1.8298)
+2022-11-18 15:55:54,849:INFO: Dataset: zara2               Batch: 15/18	Loss 1.7613 (1.8248)
+2022-11-18 15:55:55,007:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8329 (1.8253)
+2022-11-18 15:55:55,166:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8377 (1.8261)
+2022-11-18 15:55:55,310:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9065 (1.8302)
+2022-11-18 15:55:55,354:INFO: - Computing loss (validation)
+2022-11-18 15:55:55,620:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8076 (1.8076)
+2022-11-18 15:55:55,656:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0277 (1.8241)
+2022-11-18 15:55:55,960:INFO: Dataset: univ                Batch: 1/3	Loss 1.8891 (1.8891)
+2022-11-18 15:55:56,035:INFO: Dataset: univ                Batch: 2/3	Loss 1.8738 (1.8825)
+2022-11-18 15:55:56,115:INFO: Dataset: univ                Batch: 3/3	Loss 1.8399 (1.8696)
+2022-11-18 15:55:56,421:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9310 (1.9310)
+2022-11-18 15:55:56,468:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6392 (1.8625)
+2022-11-18 15:55:56,785:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8985 (1.8985)
+2022-11-18 15:55:56,860:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8023 (1.8514)
+2022-11-18 15:55:56,936:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7643 (1.8216)
+2022-11-18 15:55:57,012:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8165 (1.8203)
+2022-11-18 15:55:57,089:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8126 (1.8189)
+2022-11-18 15:55:57,142:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_596.pth.tar
+2022-11-18 15:55:57,142:INFO: 
+===> EPOCH: 597 (P2)
+2022-11-18 15:55:57,142:INFO: - Computing loss (training)
+2022-11-18 15:55:57,490:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8168 (1.8168)
+2022-11-18 15:55:57,652:INFO: Dataset: hotel               Batch: 2/4	Loss 1.7867 (1.8013)
+2022-11-18 15:55:57,809:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9502 (1.8500)
+2022-11-18 15:55:57,931:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9125 (1.8608)
+2022-11-18 15:55:58,328:INFO: Dataset: univ                Batch:  1/15	Loss 1.8088 (1.8088)
+2022-11-18 15:55:58,489:INFO: Dataset: univ                Batch:  2/15	Loss 1.8085 (1.8086)
+2022-11-18 15:55:58,649:INFO: Dataset: univ                Batch:  3/15	Loss 1.8077 (1.8083)
+2022-11-18 15:55:58,807:INFO: Dataset: univ                Batch:  4/15	Loss 1.8590 (1.8209)
+2022-11-18 15:55:58,967:INFO: Dataset: univ                Batch:  5/15	Loss 1.7827 (1.8128)
+2022-11-18 15:55:59,128:INFO: Dataset: univ                Batch:  6/15	Loss 1.8804 (1.8236)
+2022-11-18 15:55:59,286:INFO: Dataset: univ                Batch:  7/15	Loss 1.8766 (1.8318)
+2022-11-18 15:55:59,441:INFO: Dataset: univ                Batch:  8/15	Loss 1.8266 (1.8312)
+2022-11-18 15:55:59,598:INFO: Dataset: univ                Batch:  9/15	Loss 1.7943 (1.8271)
+2022-11-18 15:55:59,753:INFO: Dataset: univ                Batch: 10/15	Loss 1.8837 (1.8329)
+2022-11-18 15:55:59,911:INFO: Dataset: univ                Batch: 11/15	Loss 1.8597 (1.8355)
+2022-11-18 15:56:00,069:INFO: Dataset: univ                Batch: 12/15	Loss 1.8339 (1.8354)
+2022-11-18 15:56:00,227:INFO: Dataset: univ                Batch: 13/15	Loss 1.8143 (1.8338)
+2022-11-18 15:56:00,384:INFO: Dataset: univ                Batch: 14/15	Loss 1.8537 (1.8354)
+2022-11-18 15:56:00,468:INFO: Dataset: univ                Batch: 15/15	Loss 1.7512 (1.8339)
+2022-11-18 15:56:00,863:INFO: Dataset: zara1               Batch: 1/8	Loss 1.8437 (1.8437)
+2022-11-18 15:56:01,014:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9121 (1.8770)
+2022-11-18 15:56:01,178:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9630 (1.9073)
+2022-11-18 15:56:01,327:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8643 (1.8966)
+2022-11-18 15:56:01,477:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9207 (1.9016)
+2022-11-18 15:56:01,628:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8072 (1.8865)
+2022-11-18 15:56:01,778:INFO: Dataset: zara1               Batch: 7/8	Loss 1.7744 (1.8698)
+2022-11-18 15:56:01,914:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8438 (1.8672)
+2022-11-18 15:56:02,362:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7722 (1.7722)
+2022-11-18 15:56:02,517:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8464 (1.8093)
+2022-11-18 15:56:02,669:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7204 (1.7786)
+2022-11-18 15:56:02,819:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7754 (1.7778)
+2022-11-18 15:56:02,970:INFO: Dataset: zara2               Batch:  5/18	Loss 1.7827 (1.7788)
+2022-11-18 15:56:03,123:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7187 (1.7691)
+2022-11-18 15:56:03,274:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8877 (1.7851)
+2022-11-18 15:56:03,422:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8509 (1.7936)
+2022-11-18 15:56:03,572:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8485 (1.8002)
+2022-11-18 15:56:03,722:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8768 (1.8071)
+2022-11-18 15:56:03,875:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8259 (1.8087)
+2022-11-18 15:56:04,024:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7756 (1.8058)
+2022-11-18 15:56:04,176:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8965 (1.8130)
+2022-11-18 15:56:04,326:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8363 (1.8146)
+2022-11-18 15:56:04,478:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8840 (1.8198)
+2022-11-18 15:56:04,629:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7722 (1.8170)
+2022-11-18 15:56:04,780:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8176 (1.8171)
+2022-11-18 15:56:04,922:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7242 (1.8125)
+2022-11-18 15:56:04,970:INFO: - Computing loss (validation)
+2022-11-18 15:56:05,233:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9492 (1.9492)
+2022-11-18 15:56:05,266:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0779 (1.9558)
+2022-11-18 15:56:05,578:INFO: Dataset: univ                Batch: 1/3	Loss 1.8614 (1.8614)
+2022-11-18 15:56:05,658:INFO: Dataset: univ                Batch: 2/3	Loss 1.8542 (1.8578)
+2022-11-18 15:56:05,733:INFO: Dataset: univ                Batch: 3/3	Loss 1.8566 (1.8574)
+2022-11-18 15:56:06,036:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9168 (1.9168)
+2022-11-18 15:56:06,082:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9446 (1.9233)
+2022-11-18 15:56:06,388:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8162 (1.8162)
+2022-11-18 15:56:06,463:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9236 (1.8709)
+2022-11-18 15:56:06,539:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7660 (1.8340)
+2022-11-18 15:56:06,614:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7928 (1.8234)
+2022-11-18 15:56:06,688:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8221 (1.8231)
+2022-11-18 15:56:06,741:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_597.pth.tar
+2022-11-18 15:56:06,741:INFO: 
+===> EPOCH: 598 (P2)
+2022-11-18 15:56:06,742:INFO: - Computing loss (training)
+2022-11-18 15:56:07,084:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8574 (1.8574)
+2022-11-18 15:56:07,241:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8135 (1.8356)
+2022-11-18 15:56:07,393:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8756 (1.8488)
+2022-11-18 15:56:07,510:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8584 (1.8503)
+2022-11-18 15:56:07,930:INFO: Dataset: univ                Batch:  1/15	Loss 1.8116 (1.8116)
+2022-11-18 15:56:08,095:INFO: Dataset: univ                Batch:  2/15	Loss 1.8525 (1.8302)
+2022-11-18 15:56:08,253:INFO: Dataset: univ                Batch:  3/15	Loss 1.8211 (1.8273)
+2022-11-18 15:56:08,412:INFO: Dataset: univ                Batch:  4/15	Loss 1.8811 (1.8393)
+2022-11-18 15:56:08,577:INFO: Dataset: univ                Batch:  5/15	Loss 1.8108 (1.8335)
+2022-11-18 15:56:08,747:INFO: Dataset: univ                Batch:  6/15	Loss 1.8956 (1.8435)
+2022-11-18 15:56:08,905:INFO: Dataset: univ                Batch:  7/15	Loss 1.8583 (1.8455)
+2022-11-18 15:56:09,073:INFO: Dataset: univ                Batch:  8/15	Loss 1.8325 (1.8438)
+2022-11-18 15:56:09,234:INFO: Dataset: univ                Batch:  9/15	Loss 1.8363 (1.8430)
+2022-11-18 15:56:09,405:INFO: Dataset: univ                Batch: 10/15	Loss 1.8143 (1.8400)
+2022-11-18 15:56:09,611:INFO: Dataset: univ                Batch: 11/15	Loss 1.8422 (1.8402)
+2022-11-18 15:56:09,805:INFO: Dataset: univ                Batch: 12/15	Loss 1.8647 (1.8422)
+2022-11-18 15:56:09,991:INFO: Dataset: univ                Batch: 13/15	Loss 1.8529 (1.8430)
+2022-11-18 15:56:10,196:INFO: Dataset: univ                Batch: 14/15	Loss 1.8413 (1.8429)
+2022-11-18 15:56:10,294:INFO: Dataset: univ                Batch: 15/15	Loss 2.0370 (1.8449)
+2022-11-18 15:56:10,711:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9072 (1.9072)
+2022-11-18 15:56:10,873:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8703 (1.8894)
+2022-11-18 15:56:11,040:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8567 (1.8777)
+2022-11-18 15:56:11,224:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7501 (1.8482)
+2022-11-18 15:56:11,398:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9270 (1.8631)
+2022-11-18 15:56:11,567:INFO: Dataset: zara1               Batch: 6/8	Loss 2.0178 (1.8921)
+2022-11-18 15:56:11,739:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9727 (1.9041)
+2022-11-18 15:56:11,890:INFO: Dataset: zara1               Batch: 8/8	Loss 1.8714 (1.9006)
+2022-11-18 15:56:12,281:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8541 (1.8541)
+2022-11-18 15:56:12,437:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9210 (1.8892)
+2022-11-18 15:56:12,589:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8208 (1.8678)
+2022-11-18 15:56:12,757:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9069 (1.8778)
+2022-11-18 15:56:12,943:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8533 (1.8727)
+2022-11-18 15:56:13,118:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8606 (1.8708)
+2022-11-18 15:56:13,280:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8158 (1.8620)
+2022-11-18 15:56:13,432:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9087 (1.8679)
+2022-11-18 15:56:13,607:INFO: Dataset: zara2               Batch:  9/18	Loss 1.7983 (1.8607)
+2022-11-18 15:56:13,783:INFO: Dataset: zara2               Batch: 10/18	Loss 1.7892 (1.8535)
+2022-11-18 15:56:13,951:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8175 (1.8500)
+2022-11-18 15:56:14,113:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8216 (1.8479)
+2022-11-18 15:56:14,268:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8468 (1.8478)
+2022-11-18 15:56:14,443:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8612 (1.8488)
+2022-11-18 15:56:14,616:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8094 (1.8462)
+2022-11-18 15:56:14,793:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8366 (1.8457)
+2022-11-18 15:56:14,963:INFO: Dataset: zara2               Batch: 17/18	Loss 1.8760 (1.8475)
+2022-11-18 15:56:15,128:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8332 (1.8468)
+2022-11-18 15:56:15,203:INFO: - Computing loss (validation)
+2022-11-18 15:56:15,468:INFO: Dataset: hotel               Batch: 1/2	Loss 1.8770 (1.8770)
+2022-11-18 15:56:15,502:INFO: Dataset: hotel               Batch: 2/2	Loss 2.3320 (1.9112)
+2022-11-18 15:56:15,818:INFO: Dataset: univ                Batch: 1/3	Loss 1.8799 (1.8799)
+2022-11-18 15:56:15,903:INFO: Dataset: univ                Batch: 2/3	Loss 1.8180 (1.8511)
+2022-11-18 15:56:15,981:INFO: Dataset: univ                Batch: 3/3	Loss 1.8993 (1.8659)
+2022-11-18 15:56:16,303:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9033 (1.9033)
+2022-11-18 15:56:16,352:INFO: Dataset: zara1               Batch: 2/2	Loss 1.9525 (1.9145)
+2022-11-18 15:56:16,662:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8217 (1.8217)
+2022-11-18 15:56:16,739:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8825 (1.8519)
+2022-11-18 15:56:16,815:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9829 (1.8949)
+2022-11-18 15:56:16,892:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8142 (1.8756)
+2022-11-18 15:56:16,970:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8118 (1.8626)
+2022-11-18 15:56:17,031:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_598.pth.tar
+2022-11-18 15:56:17,031:INFO: 
+===> EPOCH: 599 (P2)
+2022-11-18 15:56:17,032:INFO: - Computing loss (training)
+2022-11-18 15:56:17,444:INFO: Dataset: hotel               Batch: 1/4	Loss 1.7788 (1.7788)
+2022-11-18 15:56:17,617:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8693 (1.8256)
+2022-11-18 15:56:17,775:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8139 (1.8219)
+2022-11-18 15:56:17,895:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8316 (1.8236)
+2022-11-18 15:56:18,327:INFO: Dataset: univ                Batch:  1/15	Loss 1.8269 (1.8269)
+2022-11-18 15:56:18,499:INFO: Dataset: univ                Batch:  2/15	Loss 1.8485 (1.8369)
+2022-11-18 15:56:18,683:INFO: Dataset: univ                Batch:  3/15	Loss 1.8287 (1.8344)
+2022-11-18 15:56:18,857:INFO: Dataset: univ                Batch:  4/15	Loss 1.8716 (1.8436)
+2022-11-18 15:56:19,018:INFO: Dataset: univ                Batch:  5/15	Loss 1.8068 (1.8362)
+2022-11-18 15:56:19,199:INFO: Dataset: univ                Batch:  6/15	Loss 1.8004 (1.8306)
+2022-11-18 15:56:19,368:INFO: Dataset: univ                Batch:  7/15	Loss 1.8306 (1.8306)
+2022-11-18 15:56:19,530:INFO: Dataset: univ                Batch:  8/15	Loss 1.8391 (1.8317)
+2022-11-18 15:56:19,695:INFO: Dataset: univ                Batch:  9/15	Loss 1.8030 (1.8287)
+2022-11-18 15:56:19,852:INFO: Dataset: univ                Batch: 10/15	Loss 1.8073 (1.8267)
+2022-11-18 15:56:20,013:INFO: Dataset: univ                Batch: 11/15	Loss 1.8648 (1.8302)
+2022-11-18 15:56:20,176:INFO: Dataset: univ                Batch: 12/15	Loss 1.8095 (1.8286)
+2022-11-18 15:56:20,344:INFO: Dataset: univ                Batch: 13/15	Loss 1.8588 (1.8310)
+2022-11-18 15:56:20,509:INFO: Dataset: univ                Batch: 14/15	Loss 1.8614 (1.8332)
+2022-11-18 15:56:20,599:INFO: Dataset: univ                Batch: 15/15	Loss 1.8523 (1.8334)
+2022-11-18 15:56:21,011:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7501 (1.7501)
+2022-11-18 15:56:21,171:INFO: Dataset: zara1               Batch: 2/8	Loss 1.9596 (1.8551)
+2022-11-18 15:56:21,328:INFO: Dataset: zara1               Batch: 3/8	Loss 1.8406 (1.8505)
+2022-11-18 15:56:21,479:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8866 (1.8595)
+2022-11-18 15:56:21,648:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9236 (1.8733)
+2022-11-18 15:56:21,820:INFO: Dataset: zara1               Batch: 6/8	Loss 1.8556 (1.8701)
+2022-11-18 15:56:21,991:INFO: Dataset: zara1               Batch: 7/8	Loss 1.8439 (1.8661)
+2022-11-18 15:56:22,131:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9363 (1.8732)
+2022-11-18 15:56:22,558:INFO: Dataset: zara2               Batch:  1/18	Loss 1.8216 (1.8216)
+2022-11-18 15:56:22,710:INFO: Dataset: zara2               Batch:  2/18	Loss 1.8615 (1.8407)
+2022-11-18 15:56:22,863:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8465 (1.8427)
+2022-11-18 15:56:23,012:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7634 (1.8234)
+2022-11-18 15:56:23,162:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8405 (1.8269)
+2022-11-18 15:56:23,316:INFO: Dataset: zara2               Batch:  6/18	Loss 1.8316 (1.8278)
+2022-11-18 15:56:23,466:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8324 (1.8284)
+2022-11-18 15:56:23,615:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8253 (1.8281)
+2022-11-18 15:56:23,770:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8587 (1.8314)
+2022-11-18 15:56:23,917:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8324 (1.8315)
+2022-11-18 15:56:24,068:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8113 (1.8294)
+2022-11-18 15:56:24,217:INFO: Dataset: zara2               Batch: 12/18	Loss 1.7519 (1.8224)
+2022-11-18 15:56:24,367:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8870 (1.8272)
+2022-11-18 15:56:24,517:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9541 (1.8356)
+2022-11-18 15:56:24,669:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8201 (1.8346)
+2022-11-18 15:56:24,820:INFO: Dataset: zara2               Batch: 16/18	Loss 1.8049 (1.8329)
+2022-11-18 15:56:24,997:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7329 (1.8270)
+2022-11-18 15:56:25,148:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9050 (1.8303)
+2022-11-18 15:56:25,199:INFO: - Computing loss (validation)
+2022-11-18 15:56:25,481:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9356 (1.9356)
+2022-11-18 15:56:25,518:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0127 (1.9414)
+2022-11-18 15:56:25,883:INFO: Dataset: univ                Batch: 1/3	Loss 1.9060 (1.9060)
+2022-11-18 15:56:25,974:INFO: Dataset: univ                Batch: 2/3	Loss 1.8539 (1.8815)
+2022-11-18 15:56:26,058:INFO: Dataset: univ                Batch: 3/3	Loss 1.9083 (1.8886)
+2022-11-18 15:56:26,375:INFO: Dataset: zara1               Batch: 1/2	Loss 1.9031 (1.9031)
+2022-11-18 15:56:26,422:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0153 (1.9284)
+2022-11-18 15:56:26,746:INFO: Dataset: zara2               Batch: 1/5	Loss 1.8185 (1.8185)
+2022-11-18 15:56:26,824:INFO: Dataset: zara2               Batch: 2/5	Loss 1.8795 (1.8483)
+2022-11-18 15:56:26,900:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7289 (1.8103)
+2022-11-18 15:56:26,976:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8202 (1.8130)
+2022-11-18 15:56:27,050:INFO: Dataset: zara2               Batch: 5/5	Loss 1.8230 (1.8149)
+2022-11-18 15:56:27,104:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_599.pth.tar
+2022-11-18 15:56:27,104:INFO: 
+===> EPOCH: 600 (P2)
+2022-11-18 15:56:27,105:INFO: - Computing loss (training)
+2022-11-18 15:56:27,456:INFO: Dataset: hotel               Batch: 1/4	Loss 1.9935 (1.9935)
+2022-11-18 15:56:27,625:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8862 (1.9400)
+2022-11-18 15:56:27,828:INFO: Dataset: hotel               Batch: 3/4	Loss 1.8972 (1.9260)
+2022-11-18 15:56:27,968:INFO: Dataset: hotel               Batch: 4/4	Loss 1.9402 (1.9282)
+2022-11-18 15:56:28,506:INFO: Dataset: univ                Batch:  1/15	Loss 1.8362 (1.8362)
+2022-11-18 15:56:28,676:INFO: Dataset: univ                Batch:  2/15	Loss 1.8394 (1.8378)
+2022-11-18 15:56:28,846:INFO: Dataset: univ                Batch:  3/15	Loss 1.8682 (1.8473)
+2022-11-18 15:56:29,024:INFO: Dataset: univ                Batch:  4/15	Loss 1.8304 (1.8427)
+2022-11-18 15:56:29,207:INFO: Dataset: univ                Batch:  5/15	Loss 1.8411 (1.8424)
+2022-11-18 15:56:29,403:INFO: Dataset: univ                Batch:  6/15	Loss 1.8416 (1.8423)
+2022-11-18 15:56:29,593:INFO: Dataset: univ                Batch:  7/15	Loss 1.8819 (1.8476)
+2022-11-18 15:56:29,772:INFO: Dataset: univ                Batch:  8/15	Loss 1.8702 (1.8505)
+2022-11-18 15:56:29,936:INFO: Dataset: univ                Batch:  9/15	Loss 1.8322 (1.8486)
+2022-11-18 15:56:30,099:INFO: Dataset: univ                Batch: 10/15	Loss 1.8334 (1.8472)
+2022-11-18 15:56:30,274:INFO: Dataset: univ                Batch: 11/15	Loss 1.8390 (1.8464)
+2022-11-18 15:56:30,457:INFO: Dataset: univ                Batch: 12/15	Loss 1.8769 (1.8488)
+2022-11-18 15:56:30,640:INFO: Dataset: univ                Batch: 13/15	Loss 1.8387 (1.8480)
+2022-11-18 15:56:30,821:INFO: Dataset: univ                Batch: 14/15	Loss 1.8370 (1.8473)
+2022-11-18 15:56:30,929:INFO: Dataset: univ                Batch: 15/15	Loss 1.9134 (1.8483)
+2022-11-18 15:56:31,394:INFO: Dataset: zara1               Batch: 1/8	Loss 1.7424 (1.7424)
+2022-11-18 15:56:31,551:INFO: Dataset: zara1               Batch: 2/8	Loss 1.8774 (1.8125)
+2022-11-18 15:56:31,709:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9013 (1.8405)
+2022-11-18 15:56:31,883:INFO: Dataset: zara1               Batch: 4/8	Loss 1.8164 (1.8351)
+2022-11-18 15:56:32,046:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9420 (1.8563)
+2022-11-18 15:56:32,201:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7906 (1.8443)
+2022-11-18 15:56:32,357:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9523 (1.8602)
+2022-11-18 15:56:32,498:INFO: Dataset: zara1               Batch: 8/8	Loss 1.9188 (1.8653)
+2022-11-18 15:56:32,913:INFO: Dataset: zara2               Batch:  1/18	Loss 1.7964 (1.7964)
+2022-11-18 15:56:33,095:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9062 (1.8474)
+2022-11-18 15:56:33,248:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7699 (1.8203)
+2022-11-18 15:56:33,400:INFO: Dataset: zara2               Batch:  4/18	Loss 1.8156 (1.8191)
+2022-11-18 15:56:33,571:INFO: Dataset: zara2               Batch:  5/18	Loss 1.8082 (1.8168)
+2022-11-18 15:56:33,763:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6944 (1.7952)
+2022-11-18 15:56:33,949:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8287 (1.8001)
+2022-11-18 15:56:34,123:INFO: Dataset: zara2               Batch:  8/18	Loss 1.8191 (1.8025)
+2022-11-18 15:56:34,291:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8227 (1.8049)
+2022-11-18 15:56:34,465:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8119 (1.8056)
+2022-11-18 15:56:34,632:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8209 (1.8071)
+2022-11-18 15:56:34,785:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8812 (1.8135)
+2022-11-18 15:56:34,938:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8284 (1.8146)
+2022-11-18 15:56:35,089:INFO: Dataset: zara2               Batch: 14/18	Loss 1.9065 (1.8216)
+2022-11-18 15:56:35,242:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8579 (1.8239)
+2022-11-18 15:56:35,402:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9086 (1.8286)
+2022-11-18 15:56:35,557:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7441 (1.8239)
+2022-11-18 15:56:35,701:INFO: Dataset: zara2               Batch: 18/18	Loss 1.8797 (1.8264)
+2022-11-18 15:56:35,748:INFO: - Computing loss (validation)
+2022-11-18 15:56:36,039:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9189 (1.9189)
+2022-11-18 15:56:36,077:INFO: Dataset: hotel               Batch: 2/2	Loss 1.8720 (1.9157)
+2022-11-18 15:56:36,458:INFO: Dataset: univ                Batch: 1/3	Loss 1.8305 (1.8305)
+2022-11-18 15:56:36,546:INFO: Dataset: univ                Batch: 2/3	Loss 1.7881 (1.8089)
+2022-11-18 15:56:36,630:INFO: Dataset: univ                Batch: 3/3	Loss 1.8633 (1.8267)
+2022-11-18 15:56:36,942:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7465 (1.7465)
+2022-11-18 15:56:36,987:INFO: Dataset: zara1               Batch: 2/2	Loss 1.6349 (1.7153)
+2022-11-18 15:56:37,363:INFO: Dataset: zara2               Batch: 1/5	Loss 1.7791 (1.7791)
+2022-11-18 15:56:37,448:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7715 (1.7756)
+2022-11-18 15:56:37,526:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9041 (1.8192)
+2022-11-18 15:56:37,604:INFO: Dataset: zara2               Batch: 4/5	Loss 1.8392 (1.8238)
+2022-11-18 15:56:37,681:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7776 (1.8151)
+2022-11-18 15:56:37,743:INFO:  --> Model Saved in ./models/E1//P2/CRMF_epoch_600.pth.tar
+2022-11-18 15:56:37,743:INFO: 
+===> EPOCH: 601 (P3)
+2022-11-18 15:56:37,744:INFO: - Computing loss (training)
+2022-11-18 15:56:38,035:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0408 (0.0408)
+2022-11-18 15:56:38,109:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0463 (0.0436)
+2022-11-18 15:56:38,179:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0509 (0.0460)
+2022-11-18 15:56:38,230:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0462 (0.0460)
+2022-11-18 15:56:38,549:INFO: Dataset: univ                Batch:  1/15	Loss 0.0391 (0.0391)
+2022-11-18 15:56:38,640:INFO: Dataset: univ                Batch:  2/15	Loss 0.0385 (0.0388)
+2022-11-18 15:56:38,713:INFO: Dataset: univ                Batch:  3/15	Loss 0.0367 (0.0381)
+2022-11-18 15:56:38,786:INFO: Dataset: univ                Batch:  4/15	Loss 0.0384 (0.0382)
+2022-11-18 15:56:38,862:INFO: Dataset: univ                Batch:  5/15	Loss 0.0379 (0.0381)
+2022-11-18 15:56:38,936:INFO: Dataset: univ                Batch:  6/15	Loss 0.0384 (0.0382)
+2022-11-18 15:56:39,007:INFO: Dataset: univ                Batch:  7/15	Loss 0.0374 (0.0381)
+2022-11-18 15:56:39,082:INFO: Dataset: univ                Batch:  8/15	Loss 0.0331 (0.0374)
+2022-11-18 15:56:39,158:INFO: Dataset: univ                Batch:  9/15	Loss 0.0385 (0.0375)
+2022-11-18 15:56:39,234:INFO: Dataset: univ                Batch: 10/15	Loss 0.0388 (0.0376)
+2022-11-18 15:56:39,316:INFO: Dataset: univ                Batch: 11/15	Loss 0.0364 (0.0375)
+2022-11-18 15:56:39,400:INFO: Dataset: univ                Batch: 12/15	Loss 0.0407 (0.0378)
+2022-11-18 15:56:39,487:INFO: Dataset: univ                Batch: 13/15	Loss 0.0349 (0.0376)
+2022-11-18 15:56:39,571:INFO: Dataset: univ                Batch: 14/15	Loss 0.0350 (0.0374)
+2022-11-18 15:56:39,610:INFO: Dataset: univ                Batch: 15/15	Loss 0.0433 (0.0375)
+2022-11-18 15:56:39,930:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0898 (0.0898)
+2022-11-18 15:56:39,997:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0869 (0.0883)
+2022-11-18 15:56:40,088:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0890 (0.0886)
+2022-11-18 15:56:40,159:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0860 (0.0879)
+2022-11-18 15:56:40,238:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0896 (0.0882)
+2022-11-18 15:56:40,310:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0911 (0.0887)
+2022-11-18 15:56:40,379:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0797 (0.0874)
+2022-11-18 15:56:40,441:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0807 (0.0867)
+2022-11-18 15:56:40,758:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0570 (0.0570)
+2022-11-18 15:56:40,830:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0558 (0.0564)
+2022-11-18 15:56:40,905:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0557 (0.0562)
+2022-11-18 15:56:40,975:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0549 (0.0559)
+2022-11-18 15:56:41,053:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0605 (0.0568)
+2022-11-18 15:56:41,124:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0645 (0.0580)
+2022-11-18 15:56:41,192:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0652 (0.0589)
+2022-11-18 15:56:41,258:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0657 (0.0597)
+2022-11-18 15:56:41,325:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0543 (0.0591)
+2022-11-18 15:56:41,390:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0640 (0.0596)
+2022-11-18 15:56:41,456:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0547 (0.0592)
+2022-11-18 15:56:41,523:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0611 (0.0593)
+2022-11-18 15:56:41,589:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0624 (0.0596)
+2022-11-18 15:56:41,660:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0622 (0.0598)
+2022-11-18 15:56:41,728:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0680 (0.0603)
+2022-11-18 15:56:41,805:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0578 (0.0602)
+2022-11-18 15:56:41,879:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0557 (0.0599)
+2022-11-18 15:56:41,946:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0681 (0.0603)
+2022-11-18 15:56:41,992:INFO: - Computing ADE (validation)
+2022-11-18 15:56:42,321:INFO: 		 ADE on hotel                     dataset:	 0.9899857044219971
+2022-11-18 15:56:42,789:INFO: 		 ADE on univ                      dataset:	 1.516422986984253
+2022-11-18 15:56:43,115:INFO: 		 ADE on zara1                     dataset:	 2.5536258220672607
+2022-11-18 15:56:43,580:INFO: 		 ADE on zara2                     dataset:	 1.4460583925247192
+2022-11-18 15:56:43,580:INFO: Average validation:	ADE  1.5221	FDE  2.7367
+2022-11-18 15:56:43,594:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_601.pth.tar
+2022-11-18 15:56:43,594:INFO: 
+===> EPOCH: 602 (P3)
+2022-11-18 15:56:43,595:INFO: - Computing loss (training)
+2022-11-18 15:56:43,864:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0470 (0.0470)
+2022-11-18 15:56:43,937:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0473 (0.0471)
+2022-11-18 15:56:44,006:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0495 (0.0479)
+2022-11-18 15:56:44,058:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0360 (0.0459)
+2022-11-18 15:56:44,373:INFO: Dataset: univ                Batch:  1/15	Loss 0.0373 (0.0373)
+2022-11-18 15:56:44,447:INFO: Dataset: univ                Batch:  2/15	Loss 0.0386 (0.0380)
+2022-11-18 15:56:44,526:INFO: Dataset: univ                Batch:  3/15	Loss 0.0351 (0.0370)
+2022-11-18 15:56:44,598:INFO: Dataset: univ                Batch:  4/15	Loss 0.0371 (0.0370)
+2022-11-18 15:56:44,672:INFO: Dataset: univ                Batch:  5/15	Loss 0.0389 (0.0374)
+2022-11-18 15:56:44,754:INFO: Dataset: univ                Batch:  6/15	Loss 0.0356 (0.0371)
+2022-11-18 15:56:44,826:INFO: Dataset: univ                Batch:  7/15	Loss 0.0373 (0.0371)
+2022-11-18 15:56:44,899:INFO: Dataset: univ                Batch:  8/15	Loss 0.0356 (0.0369)
+2022-11-18 15:56:44,971:INFO: Dataset: univ                Batch:  9/15	Loss 0.0384 (0.0371)
+2022-11-18 15:56:45,042:INFO: Dataset: univ                Batch: 10/15	Loss 0.0375 (0.0371)
+2022-11-18 15:56:45,115:INFO: Dataset: univ                Batch: 11/15	Loss 0.0372 (0.0371)
+2022-11-18 15:56:45,189:INFO: Dataset: univ                Batch: 12/15	Loss 0.0371 (0.0371)
+2022-11-18 15:56:45,263:INFO: Dataset: univ                Batch: 13/15	Loss 0.0376 (0.0372)
+2022-11-18 15:56:45,336:INFO: Dataset: univ                Batch: 14/15	Loss 0.0402 (0.0374)
+2022-11-18 15:56:45,369:INFO: Dataset: univ                Batch: 15/15	Loss 0.0355 (0.0373)
+2022-11-18 15:56:45,676:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0830 (0.0830)
+2022-11-18 15:56:45,745:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0846 (0.0838)
+2022-11-18 15:56:45,810:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0919 (0.0862)
+2022-11-18 15:56:45,876:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0856 (0.0861)
+2022-11-18 15:56:45,943:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0867 (0.0862)
+2022-11-18 15:56:46,013:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0885 (0.0866)
+2022-11-18 15:56:46,082:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0881 (0.0868)
+2022-11-18 15:56:46,145:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0848 (0.0866)
+2022-11-18 15:56:46,448:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0671 (0.0671)
+2022-11-18 15:56:46,523:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0620 (0.0645)
+2022-11-18 15:56:46,592:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0597 (0.0629)
+2022-11-18 15:56:46,661:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0595 (0.0620)
+2022-11-18 15:56:46,731:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0637 (0.0624)
+2022-11-18 15:56:46,804:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0542 (0.0610)
+2022-11-18 15:56:46,873:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0584 (0.0606)
+2022-11-18 15:56:46,942:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0647 (0.0611)
+2022-11-18 15:56:47,012:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0621 (0.0612)
+2022-11-18 15:56:47,080:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0641 (0.0615)
+2022-11-18 15:56:47,150:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0593 (0.0613)
+2022-11-18 15:56:47,220:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0584 (0.0611)
+2022-11-18 15:56:47,288:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0547 (0.0605)
+2022-11-18 15:56:47,361:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0624 (0.0607)
+2022-11-18 15:56:47,435:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0592 (0.0606)
+2022-11-18 15:56:47,508:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0554 (0.0602)
+2022-11-18 15:56:47,580:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0593 (0.0602)
+2022-11-18 15:56:47,644:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0604 (0.0602)
+2022-11-18 15:56:47,692:INFO: - Computing ADE (validation)
+2022-11-18 15:56:47,980:INFO: 		 ADE on hotel                     dataset:	 0.9829847812652588
+2022-11-18 15:56:48,348:INFO: 		 ADE on univ                      dataset:	 1.5107464790344238
+2022-11-18 15:56:48,632:INFO: 		 ADE on zara1                     dataset:	 2.5448813438415527
+2022-11-18 15:56:49,095:INFO: 		 ADE on zara2                     dataset:	 1.4391361474990845
+2022-11-18 15:56:49,095:INFO: Average validation:	ADE  1.5157	FDE  2.7290
+2022-11-18 15:56:49,108:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_602.pth.tar
+2022-11-18 15:56:49,108:INFO: 
+===> EPOCH: 603 (P3)
+2022-11-18 15:56:49,108:INFO: - Computing loss (training)
+2022-11-18 15:56:49,378:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0394 (0.0394)
+2022-11-18 15:56:49,444:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0532 (0.0459)
+2022-11-18 15:56:49,510:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0480 (0.0466)
+2022-11-18 15:56:49,558:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0418 (0.0458)
+2022-11-18 15:56:49,889:INFO: Dataset: univ                Batch:  1/15	Loss 0.0411 (0.0411)
+2022-11-18 15:56:49,964:INFO: Dataset: univ                Batch:  2/15	Loss 0.0354 (0.0382)
+2022-11-18 15:56:50,039:INFO: Dataset: univ                Batch:  3/15	Loss 0.0344 (0.0369)
+2022-11-18 15:56:50,112:INFO: Dataset: univ                Batch:  4/15	Loss 0.0376 (0.0371)
+2022-11-18 15:56:50,189:INFO: Dataset: univ                Batch:  5/15	Loss 0.0372 (0.0371)
+2022-11-18 15:56:50,262:INFO: Dataset: univ                Batch:  6/15	Loss 0.0374 (0.0371)
+2022-11-18 15:56:50,334:INFO: Dataset: univ                Batch:  7/15	Loss 0.0380 (0.0372)
+2022-11-18 15:56:50,407:INFO: Dataset: univ                Batch:  8/15	Loss 0.0358 (0.0371)
+2022-11-18 15:56:50,479:INFO: Dataset: univ                Batch:  9/15	Loss 0.0374 (0.0371)
+2022-11-18 15:56:50,551:INFO: Dataset: univ                Batch: 10/15	Loss 0.0365 (0.0370)
+2022-11-18 15:56:50,623:INFO: Dataset: univ                Batch: 11/15	Loss 0.0399 (0.0373)
+2022-11-18 15:56:50,695:INFO: Dataset: univ                Batch: 12/15	Loss 0.0368 (0.0372)
+2022-11-18 15:56:50,767:INFO: Dataset: univ                Batch: 13/15	Loss 0.0384 (0.0373)
+2022-11-18 15:56:50,841:INFO: Dataset: univ                Batch: 14/15	Loss 0.0354 (0.0372)
+2022-11-18 15:56:50,874:INFO: Dataset: univ                Batch: 15/15	Loss 0.0434 (0.0373)
+2022-11-18 15:56:51,178:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0884 (0.0884)
+2022-11-18 15:56:51,246:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0872 (0.0878)
+2022-11-18 15:56:51,316:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0790 (0.0848)
+2022-11-18 15:56:51,383:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0838 (0.0845)
+2022-11-18 15:56:51,450:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0839 (0.0844)
+2022-11-18 15:56:51,518:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0913 (0.0855)
+2022-11-18 15:56:51,584:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0857 (0.0855)
+2022-11-18 15:56:51,645:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0944 (0.0864)
+2022-11-18 15:56:51,954:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0601 (0.0601)
+2022-11-18 15:56:52,027:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0548 (0.0574)
+2022-11-18 15:56:52,098:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0551 (0.0566)
+2022-11-18 15:56:52,168:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0698 (0.0596)
+2022-11-18 15:56:52,238:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0612 (0.0599)
+2022-11-18 15:56:52,310:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0702 (0.0617)
+2022-11-18 15:56:52,378:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0559 (0.0608)
+2022-11-18 15:56:52,447:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0634 (0.0611)
+2022-11-18 15:56:52,516:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0611 (0.0611)
+2022-11-18 15:56:52,583:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0521 (0.0602)
+2022-11-18 15:56:52,653:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0595 (0.0601)
+2022-11-18 15:56:52,722:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0608 (0.0602)
+2022-11-18 15:56:52,789:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0598 (0.0601)
+2022-11-18 15:56:52,860:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0572 (0.0599)
+2022-11-18 15:56:52,929:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0615 (0.0600)
+2022-11-18 15:56:52,998:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0564 (0.0598)
+2022-11-18 15:56:53,068:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0692 (0.0604)
+2022-11-18 15:56:53,130:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0537 (0.0601)
+2022-11-18 15:56:53,175:INFO: - Computing ADE (validation)
+2022-11-18 15:56:53,466:INFO: 		 ADE on hotel                     dataset:	 0.9689210057258606
+2022-11-18 15:56:53,844:INFO: 		 ADE on univ                      dataset:	 1.5084141492843628
+2022-11-18 15:56:54,140:INFO: 		 ADE on zara1                     dataset:	 2.523404359817505
+2022-11-18 15:56:54,599:INFO: 		 ADE on zara2                     dataset:	 1.4340583086013794
+2022-11-18 15:56:54,599:INFO: Average validation:	ADE  1.5106	FDE  2.7241
+2022-11-18 15:56:54,614:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_603.pth.tar
+2022-11-18 15:56:54,614:INFO: 
+===> EPOCH: 604 (P3)
+2022-11-18 15:56:54,615:INFO: - Computing loss (training)
+2022-11-18 15:56:54,901:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0479 (0.0479)
+2022-11-18 15:56:54,969:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0456 (0.0467)
+2022-11-18 15:56:55,038:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0394 (0.0443)
+2022-11-18 15:56:55,088:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0533 (0.0458)
+2022-11-18 15:56:55,400:INFO: Dataset: univ                Batch:  1/15	Loss 0.0373 (0.0373)
+2022-11-18 15:56:55,479:INFO: Dataset: univ                Batch:  2/15	Loss 0.0341 (0.0356)
+2022-11-18 15:56:55,560:INFO: Dataset: univ                Batch:  3/15	Loss 0.0389 (0.0366)
+2022-11-18 15:56:55,639:INFO: Dataset: univ                Batch:  4/15	Loss 0.0364 (0.0366)
+2022-11-18 15:56:55,713:INFO: Dataset: univ                Batch:  5/15	Loss 0.0354 (0.0363)
+2022-11-18 15:56:55,791:INFO: Dataset: univ                Batch:  6/15	Loss 0.0371 (0.0365)
+2022-11-18 15:56:55,866:INFO: Dataset: univ                Batch:  7/15	Loss 0.0423 (0.0372)
+2022-11-18 15:56:55,941:INFO: Dataset: univ                Batch:  8/15	Loss 0.0364 (0.0371)
+2022-11-18 15:56:56,015:INFO: Dataset: univ                Batch:  9/15	Loss 0.0383 (0.0373)
+2022-11-18 15:56:56,090:INFO: Dataset: univ                Batch: 10/15	Loss 0.0378 (0.0373)
+2022-11-18 15:56:56,168:INFO: Dataset: univ                Batch: 11/15	Loss 0.0361 (0.0372)
+2022-11-18 15:56:56,242:INFO: Dataset: univ                Batch: 12/15	Loss 0.0376 (0.0372)
+2022-11-18 15:56:56,316:INFO: Dataset: univ                Batch: 13/15	Loss 0.0347 (0.0370)
+2022-11-18 15:56:56,391:INFO: Dataset: univ                Batch: 14/15	Loss 0.0384 (0.0371)
+2022-11-18 15:56:56,424:INFO: Dataset: univ                Batch: 15/15	Loss 0.0420 (0.0372)
+2022-11-18 15:56:56,734:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0844 (0.0844)
+2022-11-18 15:56:56,803:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0859 (0.0851)
+2022-11-18 15:56:56,875:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0845 (0.0849)
+2022-11-18 15:56:56,945:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0861 (0.0852)
+2022-11-18 15:56:57,015:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0964 (0.0873)
+2022-11-18 15:56:57,085:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0832 (0.0866)
+2022-11-18 15:56:57,153:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0868 (0.0866)
+2022-11-18 15:56:57,216:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0831 (0.0862)
+2022-11-18 15:56:57,532:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0618 (0.0618)
+2022-11-18 15:56:57,601:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0588 (0.0603)
+2022-11-18 15:56:57,671:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0586 (0.0597)
+2022-11-18 15:56:57,737:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0618 (0.0603)
+2022-11-18 15:56:57,808:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0585 (0.0600)
+2022-11-18 15:56:57,877:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0599 (0.0599)
+2022-11-18 15:56:57,944:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0601 (0.0600)
+2022-11-18 15:56:58,012:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0638 (0.0605)
+2022-11-18 15:56:58,079:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0600 (0.0604)
+2022-11-18 15:56:58,145:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0663 (0.0610)
+2022-11-18 15:56:58,215:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0543 (0.0604)
+2022-11-18 15:56:58,282:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0517 (0.0598)
+2022-11-18 15:56:58,348:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0622 (0.0599)
+2022-11-18 15:56:58,417:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0633 (0.0602)
+2022-11-18 15:56:58,485:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0505 (0.0595)
+2022-11-18 15:56:58,554:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0583 (0.0594)
+2022-11-18 15:56:58,623:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0691 (0.0599)
+2022-11-18 15:56:58,685:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0621 (0.0600)
+2022-11-18 15:56:58,730:INFO: - Computing ADE (validation)
+2022-11-18 15:56:59,025:INFO: 		 ADE on hotel                     dataset:	 0.969819962978363
+2022-11-18 15:56:59,386:INFO: 		 ADE on univ                      dataset:	 1.498254656791687
+2022-11-18 15:56:59,684:INFO: 		 ADE on zara1                     dataset:	 2.519136667251587
+2022-11-18 15:57:00,167:INFO: 		 ADE on zara2                     dataset:	 1.4329107999801636
+2022-11-18 15:57:00,167:INFO: Average validation:	ADE  1.5047	FDE  2.7169
+2022-11-18 15:57:00,180:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_604.pth.tar
+2022-11-18 15:57:00,180:INFO: 
+===> EPOCH: 605 (P3)
+2022-11-18 15:57:00,181:INFO: - Computing loss (training)
+2022-11-18 15:57:00,447:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0475 (0.0475)
+2022-11-18 15:57:00,520:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0400 (0.0438)
+2022-11-18 15:57:00,590:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0539 (0.0472)
+2022-11-18 15:57:00,641:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0387 (0.0457)
+2022-11-18 15:57:00,977:INFO: Dataset: univ                Batch:  1/15	Loss 0.0377 (0.0377)
+2022-11-18 15:57:01,060:INFO: Dataset: univ                Batch:  2/15	Loss 0.0352 (0.0364)
+2022-11-18 15:57:01,143:INFO: Dataset: univ                Batch:  3/15	Loss 0.0315 (0.0347)
+2022-11-18 15:57:01,226:INFO: Dataset: univ                Batch:  4/15	Loss 0.0377 (0.0354)
+2022-11-18 15:57:01,305:INFO: Dataset: univ                Batch:  5/15	Loss 0.0381 (0.0359)
+2022-11-18 15:57:01,383:INFO: Dataset: univ                Batch:  6/15	Loss 0.0402 (0.0366)
+2022-11-18 15:57:01,483:INFO: Dataset: univ                Batch:  7/15	Loss 0.0402 (0.0370)
+2022-11-18 15:57:01,560:INFO: Dataset: univ                Batch:  8/15	Loss 0.0357 (0.0369)
+2022-11-18 15:57:01,636:INFO: Dataset: univ                Batch:  9/15	Loss 0.0371 (0.0369)
+2022-11-18 15:57:01,716:INFO: Dataset: univ                Batch: 10/15	Loss 0.0370 (0.0369)
+2022-11-18 15:57:01,820:INFO: Dataset: univ                Batch: 11/15	Loss 0.0380 (0.0370)
+2022-11-18 15:57:01,912:INFO: Dataset: univ                Batch: 12/15	Loss 0.0389 (0.0371)
+2022-11-18 15:57:01,993:INFO: Dataset: univ                Batch: 13/15	Loss 0.0368 (0.0371)
+2022-11-18 15:57:02,080:INFO: Dataset: univ                Batch: 14/15	Loss 0.0366 (0.0371)
+2022-11-18 15:57:02,120:INFO: Dataset: univ                Batch: 15/15	Loss 0.0409 (0.0371)
+2022-11-18 15:57:02,500:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0866 (0.0866)
+2022-11-18 15:57:02,583:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0767 (0.0817)
+2022-11-18 15:57:02,657:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0829 (0.0821)
+2022-11-18 15:57:02,733:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0892 (0.0838)
+2022-11-18 15:57:02,803:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0890 (0.0849)
+2022-11-18 15:57:02,874:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0907 (0.0858)
+2022-11-18 15:57:02,947:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0876 (0.0860)
+2022-11-18 15:57:03,013:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0870 (0.0861)
+2022-11-18 15:57:03,353:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0599 (0.0599)
+2022-11-18 15:57:03,432:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0575 (0.0587)
+2022-11-18 15:57:03,508:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0588 (0.0587)
+2022-11-18 15:57:03,582:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0689 (0.0611)
+2022-11-18 15:57:03,652:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0545 (0.0597)
+2022-11-18 15:57:03,722:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0626 (0.0601)
+2022-11-18 15:57:03,789:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0646 (0.0607)
+2022-11-18 15:57:03,857:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0630 (0.0610)
+2022-11-18 15:57:03,924:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0572 (0.0606)
+2022-11-18 15:57:03,991:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0613 (0.0606)
+2022-11-18 15:57:04,061:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0600 (0.0606)
+2022-11-18 15:57:04,129:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0599 (0.0605)
+2022-11-18 15:57:04,199:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0542 (0.0600)
+2022-11-18 15:57:04,272:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0615 (0.0601)
+2022-11-18 15:57:04,346:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0580 (0.0600)
+2022-11-18 15:57:04,421:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0614 (0.0601)
+2022-11-18 15:57:04,493:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0568 (0.0599)
+2022-11-18 15:57:04,556:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0613 (0.0600)
+2022-11-18 15:57:04,601:INFO: - Computing ADE (validation)
+2022-11-18 15:57:04,891:INFO: 		 ADE on hotel                     dataset:	 0.9594536423683167
+2022-11-18 15:57:05,282:INFO: 		 ADE on univ                      dataset:	 1.5039087533950806
+2022-11-18 15:57:05,574:INFO: 		 ADE on zara1                     dataset:	 2.5369277000427246
+2022-11-18 15:57:06,047:INFO: 		 ADE on zara2                     dataset:	 1.427556037902832
+2022-11-18 15:57:06,047:INFO: Average validation:	ADE  1.5061	FDE  2.7184
+2022-11-18 15:57:06,059:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_605.pth.tar
+2022-11-18 15:57:06,060:INFO: 
+===> EPOCH: 606 (P3)
+2022-11-18 15:57:06,060:INFO: - Computing loss (training)
+2022-11-18 15:57:06,329:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0431 (0.0431)
+2022-11-18 15:57:06,400:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0544 (0.0487)
+2022-11-18 15:57:06,470:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0428 (0.0467)
+2022-11-18 15:57:06,522:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0407 (0.0457)
+2022-11-18 15:57:06,849:INFO: Dataset: univ                Batch:  1/15	Loss 0.0379 (0.0379)
+2022-11-18 15:57:06,927:INFO: Dataset: univ                Batch:  2/15	Loss 0.0362 (0.0370)
+2022-11-18 15:57:07,008:INFO: Dataset: univ                Batch:  3/15	Loss 0.0367 (0.0369)
+2022-11-18 15:57:07,082:INFO: Dataset: univ                Batch:  4/15	Loss 0.0354 (0.0365)
+2022-11-18 15:57:07,156:INFO: Dataset: univ                Batch:  5/15	Loss 0.0363 (0.0365)
+2022-11-18 15:57:07,232:INFO: Dataset: univ                Batch:  6/15	Loss 0.0360 (0.0364)
+2022-11-18 15:57:07,309:INFO: Dataset: univ                Batch:  7/15	Loss 0.0374 (0.0365)
+2022-11-18 15:57:07,387:INFO: Dataset: univ                Batch:  8/15	Loss 0.0352 (0.0364)
+2022-11-18 15:57:07,465:INFO: Dataset: univ                Batch:  9/15	Loss 0.0368 (0.0364)
+2022-11-18 15:57:07,550:INFO: Dataset: univ                Batch: 10/15	Loss 0.0371 (0.0365)
+2022-11-18 15:57:07,626:INFO: Dataset: univ                Batch: 11/15	Loss 0.0389 (0.0367)
+2022-11-18 15:57:07,699:INFO: Dataset: univ                Batch: 12/15	Loss 0.0362 (0.0367)
+2022-11-18 15:57:07,772:INFO: Dataset: univ                Batch: 13/15	Loss 0.0392 (0.0368)
+2022-11-18 15:57:07,846:INFO: Dataset: univ                Batch: 14/15	Loss 0.0392 (0.0370)
+2022-11-18 15:57:07,878:INFO: Dataset: univ                Batch: 15/15	Loss 0.0407 (0.0371)
+2022-11-18 15:57:08,199:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0844 (0.0844)
+2022-11-18 15:57:08,266:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0788 (0.0816)
+2022-11-18 15:57:08,336:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0823 (0.0818)
+2022-11-18 15:57:08,401:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0854 (0.0827)
+2022-11-18 15:57:08,469:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0914 (0.0846)
+2022-11-18 15:57:08,537:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0955 (0.0864)
+2022-11-18 15:57:08,602:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0905 (0.0869)
+2022-11-18 15:57:08,663:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0785 (0.0861)
+2022-11-18 15:57:08,985:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0620 (0.0620)
+2022-11-18 15:57:09,061:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0602 (0.0611)
+2022-11-18 15:57:09,135:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0608 (0.0610)
+2022-11-18 15:57:09,204:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0605 (0.0608)
+2022-11-18 15:57:09,284:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0591 (0.0605)
+2022-11-18 15:57:09,355:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0664 (0.0614)
+2022-11-18 15:57:09,424:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0573 (0.0608)
+2022-11-18 15:57:09,498:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0663 (0.0615)
+2022-11-18 15:57:09,572:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0551 (0.0608)
+2022-11-18 15:57:09,649:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0566 (0.0603)
+2022-11-18 15:57:09,727:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0653 (0.0608)
+2022-11-18 15:57:09,807:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0566 (0.0605)
+2022-11-18 15:57:09,887:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0552 (0.0600)
+2022-11-18 15:57:09,959:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0589 (0.0600)
+2022-11-18 15:57:10,030:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0563 (0.0597)
+2022-11-18 15:57:10,100:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0597 (0.0597)
+2022-11-18 15:57:10,171:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0658 (0.0601)
+2022-11-18 15:57:10,235:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0549 (0.0598)
+2022-11-18 15:57:10,284:INFO: - Computing ADE (validation)
+2022-11-18 15:57:10,571:INFO: 		 ADE on hotel                     dataset:	 0.9633236527442932
+2022-11-18 15:57:10,969:INFO: 		 ADE on univ                      dataset:	 1.504157304763794
+2022-11-18 15:57:11,266:INFO: 		 ADE on zara1                     dataset:	 2.522390842437744
+2022-11-18 15:57:11,746:INFO: 		 ADE on zara2                     dataset:	 1.426964521408081
+2022-11-18 15:57:11,746:INFO: Average validation:	ADE  1.5054	FDE  2.7185
+2022-11-18 15:57:11,760:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_606.pth.tar
+2022-11-18 15:57:11,760:INFO: 
+===> EPOCH: 607 (P3)
+2022-11-18 15:57:11,761:INFO: - Computing loss (training)
+2022-11-18 15:57:12,020:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0460 (0.0460)
+2022-11-18 15:57:12,086:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0463 (0.0461)
+2022-11-18 15:57:12,153:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0406 (0.0441)
+2022-11-18 15:57:12,203:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0523 (0.0456)
+2022-11-18 15:57:12,514:INFO: Dataset: univ                Batch:  1/15	Loss 0.0362 (0.0362)
+2022-11-18 15:57:12,591:INFO: Dataset: univ                Batch:  2/15	Loss 0.0376 (0.0369)
+2022-11-18 15:57:12,668:INFO: Dataset: univ                Batch:  3/15	Loss 0.0402 (0.0379)
+2022-11-18 15:57:12,742:INFO: Dataset: univ                Batch:  4/15	Loss 0.0351 (0.0372)
+2022-11-18 15:57:12,815:INFO: Dataset: univ                Batch:  5/15	Loss 0.0376 (0.0373)
+2022-11-18 15:57:12,888:INFO: Dataset: univ                Batch:  6/15	Loss 0.0362 (0.0371)
+2022-11-18 15:57:12,960:INFO: Dataset: univ                Batch:  7/15	Loss 0.0383 (0.0373)
+2022-11-18 15:57:13,033:INFO: Dataset: univ                Batch:  8/15	Loss 0.0356 (0.0371)
+2022-11-18 15:57:13,104:INFO: Dataset: univ                Batch:  9/15	Loss 0.0376 (0.0371)
+2022-11-18 15:57:13,177:INFO: Dataset: univ                Batch: 10/15	Loss 0.0347 (0.0369)
+2022-11-18 15:57:13,251:INFO: Dataset: univ                Batch: 11/15	Loss 0.0339 (0.0366)
+2022-11-18 15:57:13,322:INFO: Dataset: univ                Batch: 12/15	Loss 0.0420 (0.0371)
+2022-11-18 15:57:13,393:INFO: Dataset: univ                Batch: 13/15	Loss 0.0359 (0.0370)
+2022-11-18 15:57:13,467:INFO: Dataset: univ                Batch: 14/15	Loss 0.0388 (0.0371)
+2022-11-18 15:57:13,499:INFO: Dataset: univ                Batch: 15/15	Loss 0.0312 (0.0370)
+2022-11-18 15:57:13,798:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0886 (0.0886)
+2022-11-18 15:57:13,869:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0778 (0.0827)
+2022-11-18 15:57:13,961:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0895 (0.0850)
+2022-11-18 15:57:14,036:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0905 (0.0863)
+2022-11-18 15:57:14,105:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0871 (0.0865)
+2022-11-18 15:57:14,174:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0913 (0.0874)
+2022-11-18 15:57:14,242:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0813 (0.0864)
+2022-11-18 15:57:14,304:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0829 (0.0860)
+2022-11-18 15:57:14,614:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0614 (0.0614)
+2022-11-18 15:57:14,689:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0598 (0.0605)
+2022-11-18 15:57:14,763:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0564 (0.0591)
+2022-11-18 15:57:14,835:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0508 (0.0571)
+2022-11-18 15:57:14,903:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0672 (0.0590)
+2022-11-18 15:57:14,975:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0553 (0.0584)
+2022-11-18 15:57:15,044:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0630 (0.0590)
+2022-11-18 15:57:15,117:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0579 (0.0589)
+2022-11-18 15:57:15,186:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0563 (0.0586)
+2022-11-18 15:57:15,254:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0604 (0.0588)
+2022-11-18 15:57:15,323:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0641 (0.0593)
+2022-11-18 15:57:15,393:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0648 (0.0598)
+2022-11-18 15:57:15,461:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0582 (0.0596)
+2022-11-18 15:57:15,531:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0672 (0.0602)
+2022-11-18 15:57:15,601:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0593 (0.0601)
+2022-11-18 15:57:15,679:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0600 (0.0601)
+2022-11-18 15:57:15,752:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0569 (0.0599)
+2022-11-18 15:57:15,819:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0579 (0.0598)
+2022-11-18 15:57:15,861:INFO: - Computing ADE (validation)
+2022-11-18 15:57:16,150:INFO: 		 ADE on hotel                     dataset:	 0.9598833322525024
+2022-11-18 15:57:16,526:INFO: 		 ADE on univ                      dataset:	 1.4981586933135986
+2022-11-18 15:57:16,813:INFO: 		 ADE on zara1                     dataset:	 2.5147576332092285
+2022-11-18 15:57:17,295:INFO: 		 ADE on zara2                     dataset:	 1.4188870191574097
+2022-11-18 15:57:17,295:INFO: Average validation:	ADE  1.4987	FDE  2.7114
+2022-11-18 15:57:17,309:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_607.pth.tar
+2022-11-18 15:57:17,309:INFO: 
+===> EPOCH: 608 (P3)
+2022-11-18 15:57:17,310:INFO: - Computing loss (training)
+2022-11-18 15:57:17,660:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0395 (0.0395)
+2022-11-18 15:57:17,733:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0411 (0.0402)
+2022-11-18 15:57:17,816:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0540 (0.0447)
+2022-11-18 15:57:17,871:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0502 (0.0456)
+2022-11-18 15:57:18,200:INFO: Dataset: univ                Batch:  1/15	Loss 0.0374 (0.0374)
+2022-11-18 15:57:18,280:INFO: Dataset: univ                Batch:  2/15	Loss 0.0349 (0.0361)
+2022-11-18 15:57:18,363:INFO: Dataset: univ                Batch:  3/15	Loss 0.0362 (0.0362)
+2022-11-18 15:57:18,451:INFO: Dataset: univ                Batch:  4/15	Loss 0.0362 (0.0362)
+2022-11-18 15:57:18,538:INFO: Dataset: univ                Batch:  5/15	Loss 0.0358 (0.0361)
+2022-11-18 15:57:18,619:INFO: Dataset: univ                Batch:  6/15	Loss 0.0371 (0.0363)
+2022-11-18 15:57:18,699:INFO: Dataset: univ                Batch:  7/15	Loss 0.0383 (0.0365)
+2022-11-18 15:57:18,778:INFO: Dataset: univ                Batch:  8/15	Loss 0.0365 (0.0365)
+2022-11-18 15:57:18,855:INFO: Dataset: univ                Batch:  9/15	Loss 0.0345 (0.0363)
+2022-11-18 15:57:18,937:INFO: Dataset: univ                Batch: 10/15	Loss 0.0376 (0.0364)
+2022-11-18 15:57:19,017:INFO: Dataset: univ                Batch: 11/15	Loss 0.0400 (0.0367)
+2022-11-18 15:57:19,092:INFO: Dataset: univ                Batch: 12/15	Loss 0.0387 (0.0369)
+2022-11-18 15:57:19,169:INFO: Dataset: univ                Batch: 13/15	Loss 0.0377 (0.0370)
+2022-11-18 15:57:19,254:INFO: Dataset: univ                Batch: 14/15	Loss 0.0361 (0.0369)
+2022-11-18 15:57:19,294:INFO: Dataset: univ                Batch: 15/15	Loss 0.0411 (0.0369)
+2022-11-18 15:57:19,649:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0895 (0.0895)
+2022-11-18 15:57:19,727:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0852 (0.0872)
+2022-11-18 15:57:19,801:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0903 (0.0882)
+2022-11-18 15:57:19,873:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0846 (0.0872)
+2022-11-18 15:57:19,945:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0788 (0.0856)
+2022-11-18 15:57:20,015:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0886 (0.0861)
+2022-11-18 15:57:20,086:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0841 (0.0858)
+2022-11-18 15:57:20,151:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0870 (0.0859)
+2022-11-18 15:57:20,466:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0575 (0.0575)
+2022-11-18 15:57:20,539:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0624 (0.0601)
+2022-11-18 15:57:20,610:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0578 (0.0593)
+2022-11-18 15:57:20,686:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0541 (0.0580)
+2022-11-18 15:57:20,761:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0570 (0.0578)
+2022-11-18 15:57:20,848:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0641 (0.0588)
+2022-11-18 15:57:20,927:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0630 (0.0593)
+2022-11-18 15:57:21,004:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0563 (0.0590)
+2022-11-18 15:57:21,073:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0531 (0.0584)
+2022-11-18 15:57:21,171:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0536 (0.0579)
+2022-11-18 15:57:21,284:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0669 (0.0587)
+2022-11-18 15:57:21,365:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0621 (0.0590)
+2022-11-18 15:57:21,443:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0622 (0.0593)
+2022-11-18 15:57:21,555:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0642 (0.0596)
+2022-11-18 15:57:21,628:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0590 (0.0596)
+2022-11-18 15:57:21,699:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0615 (0.0597)
+2022-11-18 15:57:21,769:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0626 (0.0599)
+2022-11-18 15:57:21,835:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0577 (0.0597)
+2022-11-18 15:57:21,881:INFO: - Computing ADE (validation)
+2022-11-18 15:57:22,182:INFO: 		 ADE on hotel                     dataset:	 0.9522435069084167
+2022-11-18 15:57:22,579:INFO: 		 ADE on univ                      dataset:	 1.494242548942566
+2022-11-18 15:57:22,911:INFO: 		 ADE on zara1                     dataset:	 2.5130276679992676
+2022-11-18 15:57:23,508:INFO: 		 ADE on zara2                     dataset:	 1.4185963869094849
+2022-11-18 15:57:23,508:INFO: Average validation:	ADE  1.4960	FDE  2.7078
+2022-11-18 15:57:23,523:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_608.pth.tar
+2022-11-18 15:57:23,523:INFO: 
+===> EPOCH: 609 (P3)
+2022-11-18 15:57:23,524:INFO: - Computing loss (training)
+2022-11-18 15:57:23,935:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0436 (0.0436)
+2022-11-18 15:57:24,074:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0457 (0.0446)
+2022-11-18 15:57:24,184:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0524 (0.0471)
+2022-11-18 15:57:24,247:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0383 (0.0456)
+2022-11-18 15:57:24,599:INFO: Dataset: univ                Batch:  1/15	Loss 0.0368 (0.0368)
+2022-11-18 15:57:24,682:INFO: Dataset: univ                Batch:  2/15	Loss 0.0385 (0.0376)
+2022-11-18 15:57:24,764:INFO: Dataset: univ                Batch:  3/15	Loss 0.0362 (0.0371)
+2022-11-18 15:57:24,848:INFO: Dataset: univ                Batch:  4/15	Loss 0.0364 (0.0370)
+2022-11-18 15:57:24,926:INFO: Dataset: univ                Batch:  5/15	Loss 0.0357 (0.0367)
+2022-11-18 15:57:25,008:INFO: Dataset: univ                Batch:  6/15	Loss 0.0340 (0.0362)
+2022-11-18 15:57:25,087:INFO: Dataset: univ                Batch:  7/15	Loss 0.0374 (0.0364)
+2022-11-18 15:57:25,162:INFO: Dataset: univ                Batch:  8/15	Loss 0.0379 (0.0366)
+2022-11-18 15:57:25,252:INFO: Dataset: univ                Batch:  9/15	Loss 0.0389 (0.0368)
+2022-11-18 15:57:25,341:INFO: Dataset: univ                Batch: 10/15	Loss 0.0359 (0.0367)
+2022-11-18 15:57:25,428:INFO: Dataset: univ                Batch: 11/15	Loss 0.0362 (0.0367)
+2022-11-18 15:57:25,509:INFO: Dataset: univ                Batch: 12/15	Loss 0.0367 (0.0367)
+2022-11-18 15:57:25,610:INFO: Dataset: univ                Batch: 13/15	Loss 0.0387 (0.0368)
+2022-11-18 15:57:25,708:INFO: Dataset: univ                Batch: 14/15	Loss 0.0376 (0.0369)
+2022-11-18 15:57:25,751:INFO: Dataset: univ                Batch: 15/15	Loss 0.0383 (0.0369)
+2022-11-18 15:57:26,068:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0864 (0.0864)
+2022-11-18 15:57:26,141:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0866 (0.0865)
+2022-11-18 15:57:26,212:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0840 (0.0857)
+2022-11-18 15:57:26,281:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0799 (0.0844)
+2022-11-18 15:57:26,353:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0889 (0.0853)
+2022-11-18 15:57:26,422:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0783 (0.0841)
+2022-11-18 15:57:26,488:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0901 (0.0849)
+2022-11-18 15:57:26,550:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0949 (0.0859)
+2022-11-18 15:57:26,855:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0624 (0.0624)
+2022-11-18 15:57:26,925:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0615 (0.0619)
+2022-11-18 15:57:26,995:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0606 (0.0614)
+2022-11-18 15:57:27,060:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0570 (0.0603)
+2022-11-18 15:57:27,130:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0554 (0.0593)
+2022-11-18 15:57:27,199:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0621 (0.0597)
+2022-11-18 15:57:27,267:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0735 (0.0614)
+2022-11-18 15:57:27,335:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0591 (0.0612)
+2022-11-18 15:57:27,402:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0581 (0.0608)
+2022-11-18 15:57:27,469:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0436 (0.0591)
+2022-11-18 15:57:27,540:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0691 (0.0600)
+2022-11-18 15:57:27,607:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0627 (0.0603)
+2022-11-18 15:57:27,673:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0486 (0.0593)
+2022-11-18 15:57:27,742:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0620 (0.0595)
+2022-11-18 15:57:27,810:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0563 (0.0593)
+2022-11-18 15:57:27,877:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0579 (0.0592)
+2022-11-18 15:57:27,948:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0554 (0.0590)
+2022-11-18 15:57:28,012:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0723 (0.0597)
+2022-11-18 15:57:28,054:INFO: - Computing ADE (validation)
+2022-11-18 15:57:28,343:INFO: 		 ADE on hotel                     dataset:	 0.9541906118392944
+2022-11-18 15:57:28,735:INFO: 		 ADE on univ                      dataset:	 1.4978471994400024
+2022-11-18 15:57:29,034:INFO: 		 ADE on zara1                     dataset:	 2.5076098442077637
+2022-11-18 15:57:29,520:INFO: 		 ADE on zara2                     dataset:	 1.4151415824890137
+2022-11-18 15:57:29,520:INFO: Average validation:	ADE  1.4964	FDE  2.7095
+2022-11-18 15:57:29,534:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_609.pth.tar
+2022-11-18 15:57:29,534:INFO: 
+===> EPOCH: 610 (P3)
+2022-11-18 15:57:29,535:INFO: - Computing loss (training)
+2022-11-18 15:57:29,813:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0491 (0.0491)
+2022-11-18 15:57:29,885:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0447 (0.0469)
+2022-11-18 15:57:29,955:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0392 (0.0443)
+2022-11-18 15:57:30,007:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0515 (0.0455)
+2022-11-18 15:57:30,326:INFO: Dataset: univ                Batch:  1/15	Loss 0.0376 (0.0376)
+2022-11-18 15:57:30,401:INFO: Dataset: univ                Batch:  2/15	Loss 0.0391 (0.0383)
+2022-11-18 15:57:30,475:INFO: Dataset: univ                Batch:  3/15	Loss 0.0350 (0.0372)
+2022-11-18 15:57:30,552:INFO: Dataset: univ                Batch:  4/15	Loss 0.0363 (0.0370)
+2022-11-18 15:57:30,628:INFO: Dataset: univ                Batch:  5/15	Loss 0.0373 (0.0370)
+2022-11-18 15:57:30,703:INFO: Dataset: univ                Batch:  6/15	Loss 0.0343 (0.0366)
+2022-11-18 15:57:30,778:INFO: Dataset: univ                Batch:  7/15	Loss 0.0379 (0.0367)
+2022-11-18 15:57:30,854:INFO: Dataset: univ                Batch:  8/15	Loss 0.0348 (0.0365)
+2022-11-18 15:57:30,927:INFO: Dataset: univ                Batch:  9/15	Loss 0.0387 (0.0367)
+2022-11-18 15:57:31,001:INFO: Dataset: univ                Batch: 10/15	Loss 0.0392 (0.0370)
+2022-11-18 15:57:31,075:INFO: Dataset: univ                Batch: 11/15	Loss 0.0365 (0.0369)
+2022-11-18 15:57:31,148:INFO: Dataset: univ                Batch: 12/15	Loss 0.0368 (0.0369)
+2022-11-18 15:57:31,223:INFO: Dataset: univ                Batch: 13/15	Loss 0.0371 (0.0370)
+2022-11-18 15:57:31,297:INFO: Dataset: univ                Batch: 14/15	Loss 0.0361 (0.0369)
+2022-11-18 15:57:31,331:INFO: Dataset: univ                Batch: 15/15	Loss 0.0322 (0.0368)
+2022-11-18 15:57:31,634:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0843 (0.0843)
+2022-11-18 15:57:31,701:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0923 (0.0883)
+2022-11-18 15:57:31,789:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0832 (0.0867)
+2022-11-18 15:57:31,872:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0872 (0.0868)
+2022-11-18 15:57:31,946:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0905 (0.0876)
+2022-11-18 15:57:32,017:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0851 (0.0872)
+2022-11-18 15:57:32,085:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0833 (0.0866)
+2022-11-18 15:57:32,149:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0784 (0.0857)
+2022-11-18 15:57:32,585:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0606 (0.0606)
+2022-11-18 15:57:32,656:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0553 (0.0579)
+2022-11-18 15:57:32,739:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0606 (0.0588)
+2022-11-18 15:57:32,811:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0565 (0.0582)
+2022-11-18 15:57:32,883:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0560 (0.0578)
+2022-11-18 15:57:32,955:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0644 (0.0589)
+2022-11-18 15:57:33,026:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0649 (0.0598)
+2022-11-18 15:57:33,094:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0682 (0.0609)
+2022-11-18 15:57:33,162:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0651 (0.0614)
+2022-11-18 15:57:33,230:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0541 (0.0606)
+2022-11-18 15:57:33,301:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0551 (0.0602)
+2022-11-18 15:57:33,367:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0536 (0.0595)
+2022-11-18 15:57:33,434:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0596 (0.0595)
+2022-11-18 15:57:33,503:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0615 (0.0597)
+2022-11-18 15:57:33,571:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0660 (0.0601)
+2022-11-18 15:57:33,641:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0608 (0.0602)
+2022-11-18 15:57:33,709:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0589 (0.0601)
+2022-11-18 15:57:33,772:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0493 (0.0596)
+2022-11-18 15:57:33,819:INFO: - Computing ADE (validation)
+2022-11-18 15:57:34,106:INFO: 		 ADE on hotel                     dataset:	 0.9475428462028503
+2022-11-18 15:57:34,484:INFO: 		 ADE on univ                      dataset:	 1.4896059036254883
+2022-11-18 15:57:34,791:INFO: 		 ADE on zara1                     dataset:	 2.5095059871673584
+2022-11-18 15:57:35,455:INFO: 		 ADE on zara2                     dataset:	 1.4139835834503174
+2022-11-18 15:57:35,455:INFO: Average validation:	ADE  1.4915	FDE  2.7039
+2022-11-18 15:57:35,468:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_610.pth.tar
+2022-11-18 15:57:35,469:INFO: 
+===> EPOCH: 611 (P3)
+2022-11-18 15:57:35,469:INFO: - Computing loss (training)
+2022-11-18 15:57:35,748:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0470 (0.0470)
+2022-11-18 15:57:35,817:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0316 (0.0390)
+2022-11-18 15:57:35,883:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0522 (0.0432)
+2022-11-18 15:57:35,932:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0557 (0.0455)
+2022-11-18 15:57:36,414:INFO: Dataset: univ                Batch:  1/15	Loss 0.0370 (0.0370)
+2022-11-18 15:57:36,509:INFO: Dataset: univ                Batch:  2/15	Loss 0.0351 (0.0360)
+2022-11-18 15:57:36,590:INFO: Dataset: univ                Batch:  3/15	Loss 0.0376 (0.0366)
+2022-11-18 15:57:36,671:INFO: Dataset: univ                Batch:  4/15	Loss 0.0387 (0.0371)
+2022-11-18 15:57:36,749:INFO: Dataset: univ                Batch:  5/15	Loss 0.0346 (0.0366)
+2022-11-18 15:57:36,850:INFO: Dataset: univ                Batch:  6/15	Loss 0.0379 (0.0368)
+2022-11-18 15:57:36,997:INFO: Dataset: univ                Batch:  7/15	Loss 0.0371 (0.0368)
+2022-11-18 15:57:37,097:INFO: Dataset: univ                Batch:  8/15	Loss 0.0345 (0.0365)
+2022-11-18 15:57:37,181:INFO: Dataset: univ                Batch:  9/15	Loss 0.0369 (0.0366)
+2022-11-18 15:57:37,259:INFO: Dataset: univ                Batch: 10/15	Loss 0.0373 (0.0367)
+2022-11-18 15:57:37,338:INFO: Dataset: univ                Batch: 11/15	Loss 0.0385 (0.0368)
+2022-11-18 15:57:37,416:INFO: Dataset: univ                Batch: 12/15	Loss 0.0374 (0.0369)
+2022-11-18 15:57:37,496:INFO: Dataset: univ                Batch: 13/15	Loss 0.0350 (0.0367)
+2022-11-18 15:57:37,576:INFO: Dataset: univ                Batch: 14/15	Loss 0.0375 (0.0368)
+2022-11-18 15:57:37,610:INFO: Dataset: univ                Batch: 15/15	Loss 0.0381 (0.0368)
+2022-11-18 15:57:37,935:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0856 (0.0856)
+2022-11-18 15:57:38,011:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0827 (0.0842)
+2022-11-18 15:57:38,080:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0853 (0.0846)
+2022-11-18 15:57:38,148:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0869 (0.0852)
+2022-11-18 15:57:38,219:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0908 (0.0862)
+2022-11-18 15:57:38,289:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0850 (0.0860)
+2022-11-18 15:57:38,357:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0814 (0.0854)
+2022-11-18 15:57:38,421:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0876 (0.0857)
+2022-11-18 15:57:38,731:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0619 (0.0619)
+2022-11-18 15:57:38,809:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0665 (0.0642)
+2022-11-18 15:57:38,881:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0597 (0.0627)
+2022-11-18 15:57:38,955:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0607 (0.0621)
+2022-11-18 15:57:39,032:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0580 (0.0613)
+2022-11-18 15:57:39,103:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0611 (0.0613)
+2022-11-18 15:57:39,173:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0597 (0.0610)
+2022-11-18 15:57:39,245:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0456 (0.0591)
+2022-11-18 15:57:39,314:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0634 (0.0596)
+2022-11-18 15:57:39,384:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0630 (0.0599)
+2022-11-18 15:57:39,458:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0586 (0.0598)
+2022-11-18 15:57:39,528:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0646 (0.0602)
+2022-11-18 15:57:39,598:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0642 (0.0605)
+2022-11-18 15:57:39,670:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0525 (0.0599)
+2022-11-18 15:57:39,740:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0632 (0.0601)
+2022-11-18 15:57:39,811:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0575 (0.0600)
+2022-11-18 15:57:39,883:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0575 (0.0598)
+2022-11-18 15:57:39,947:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0543 (0.0595)
+2022-11-18 15:57:39,992:INFO: - Computing ADE (validation)
+2022-11-18 15:57:40,291:INFO: 		 ADE on hotel                     dataset:	 0.9509141445159912
+2022-11-18 15:57:40,663:INFO: 		 ADE on univ                      dataset:	 1.492961049079895
+2022-11-18 15:57:40,964:INFO: 		 ADE on zara1                     dataset:	 2.5149993896484375
+2022-11-18 15:57:41,440:INFO: 		 ADE on zara2                     dataset:	 1.4126405715942383
+2022-11-18 15:57:41,440:INFO: Average validation:	ADE  1.4932	FDE  2.7059
+2022-11-18 15:57:41,454:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_611.pth.tar
+2022-11-18 15:57:41,454:INFO: 
+===> EPOCH: 612 (P3)
+2022-11-18 15:57:41,455:INFO: - Computing loss (training)
+2022-11-18 15:57:41,728:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0472 (0.0472)
+2022-11-18 15:57:41,795:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0541 (0.0506)
+2022-11-18 15:57:41,864:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0413 (0.0474)
+2022-11-18 15:57:41,914:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0365 (0.0455)
+2022-11-18 15:57:42,299:INFO: Dataset: univ                Batch:  1/15	Loss 0.0379 (0.0379)
+2022-11-18 15:57:42,372:INFO: Dataset: univ                Batch:  2/15	Loss 0.0373 (0.0376)
+2022-11-18 15:57:42,446:INFO: Dataset: univ                Batch:  3/15	Loss 0.0341 (0.0364)
+2022-11-18 15:57:42,518:INFO: Dataset: univ                Batch:  4/15	Loss 0.0397 (0.0372)
+2022-11-18 15:57:42,591:INFO: Dataset: univ                Batch:  5/15	Loss 0.0367 (0.0371)
+2022-11-18 15:57:42,665:INFO: Dataset: univ                Batch:  6/15	Loss 0.0347 (0.0367)
+2022-11-18 15:57:42,736:INFO: Dataset: univ                Batch:  7/15	Loss 0.0351 (0.0365)
+2022-11-18 15:57:42,808:INFO: Dataset: univ                Batch:  8/15	Loss 0.0385 (0.0367)
+2022-11-18 15:57:42,880:INFO: Dataset: univ                Batch:  9/15	Loss 0.0367 (0.0367)
+2022-11-18 15:57:42,952:INFO: Dataset: univ                Batch: 10/15	Loss 0.0377 (0.0368)
+2022-11-18 15:57:43,028:INFO: Dataset: univ                Batch: 11/15	Loss 0.0357 (0.0367)
+2022-11-18 15:57:43,100:INFO: Dataset: univ                Batch: 12/15	Loss 0.0356 (0.0366)
+2022-11-18 15:57:43,172:INFO: Dataset: univ                Batch: 13/15	Loss 0.0388 (0.0368)
+2022-11-18 15:57:43,246:INFO: Dataset: univ                Batch: 14/15	Loss 0.0355 (0.0367)
+2022-11-18 15:57:43,278:INFO: Dataset: univ                Batch: 15/15	Loss 0.0433 (0.0368)
+2022-11-18 15:57:43,581:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0896 (0.0896)
+2022-11-18 15:57:43,652:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0969 (0.0933)
+2022-11-18 15:57:43,718:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0812 (0.0890)
+2022-11-18 15:57:43,784:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0875 (0.0886)
+2022-11-18 15:57:43,851:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0866 (0.0882)
+2022-11-18 15:57:43,918:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0855 (0.0878)
+2022-11-18 15:57:43,984:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0801 (0.0868)
+2022-11-18 15:57:44,044:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0754 (0.0855)
+2022-11-18 15:57:44,340:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0645 (0.0645)
+2022-11-18 15:57:44,408:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0584 (0.0612)
+2022-11-18 15:57:44,476:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0577 (0.0600)
+2022-11-18 15:57:44,548:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0553 (0.0590)
+2022-11-18 15:57:44,619:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0510 (0.0574)
+2022-11-18 15:57:44,690:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0606 (0.0579)
+2022-11-18 15:57:44,757:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0600 (0.0582)
+2022-11-18 15:57:44,825:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0607 (0.0586)
+2022-11-18 15:57:44,893:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0574 (0.0584)
+2022-11-18 15:57:44,961:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0634 (0.0589)
+2022-11-18 15:57:45,029:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0583 (0.0588)
+2022-11-18 15:57:45,098:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0583 (0.0588)
+2022-11-18 15:57:45,166:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0620 (0.0590)
+2022-11-18 15:57:45,235:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0624 (0.0593)
+2022-11-18 15:57:45,304:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0634 (0.0595)
+2022-11-18 15:57:45,373:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0569 (0.0594)
+2022-11-18 15:57:45,443:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0609 (0.0595)
+2022-11-18 15:57:45,506:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0594 (0.0595)
+2022-11-18 15:57:45,548:INFO: - Computing ADE (validation)
+2022-11-18 15:57:45,831:INFO: 		 ADE on hotel                     dataset:	 0.942776620388031
+2022-11-18 15:57:46,238:INFO: 		 ADE on univ                      dataset:	 1.4919415712356567
+2022-11-18 15:57:46,528:INFO: 		 ADE on zara1                     dataset:	 2.500627040863037
+2022-11-18 15:57:46,987:INFO: 		 ADE on zara2                     dataset:	 1.4120551347732544
+2022-11-18 15:57:46,987:INFO: Average validation:	ADE  1.4912	FDE  2.7046
+2022-11-18 15:57:47,000:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_612.pth.tar
+2022-11-18 15:57:47,000:INFO: 
+===> EPOCH: 613 (P3)
+2022-11-18 15:57:47,000:INFO: - Computing loss (training)
+2022-11-18 15:57:47,264:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0559 (0.0559)
+2022-11-18 15:57:47,331:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0412 (0.0484)
+2022-11-18 15:57:47,397:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0384 (0.0448)
+2022-11-18 15:57:47,445:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0482 (0.0455)
+2022-11-18 15:57:47,755:INFO: Dataset: univ                Batch:  1/15	Loss 0.0367 (0.0367)
+2022-11-18 15:57:47,830:INFO: Dataset: univ                Batch:  2/15	Loss 0.0351 (0.0358)
+2022-11-18 15:57:47,906:INFO: Dataset: univ                Batch:  3/15	Loss 0.0361 (0.0359)
+2022-11-18 15:57:47,985:INFO: Dataset: univ                Batch:  4/15	Loss 0.0361 (0.0360)
+2022-11-18 15:57:48,058:INFO: Dataset: univ                Batch:  5/15	Loss 0.0370 (0.0362)
+2022-11-18 15:57:48,134:INFO: Dataset: univ                Batch:  6/15	Loss 0.0388 (0.0367)
+2022-11-18 15:57:48,208:INFO: Dataset: univ                Batch:  7/15	Loss 0.0371 (0.0367)
+2022-11-18 15:57:48,283:INFO: Dataset: univ                Batch:  8/15	Loss 0.0373 (0.0368)
+2022-11-18 15:57:48,358:INFO: Dataset: univ                Batch:  9/15	Loss 0.0374 (0.0368)
+2022-11-18 15:57:48,433:INFO: Dataset: univ                Batch: 10/15	Loss 0.0353 (0.0367)
+2022-11-18 15:57:48,509:INFO: Dataset: univ                Batch: 11/15	Loss 0.0353 (0.0366)
+2022-11-18 15:57:48,582:INFO: Dataset: univ                Batch: 12/15	Loss 0.0392 (0.0368)
+2022-11-18 15:57:48,656:INFO: Dataset: univ                Batch: 13/15	Loss 0.0367 (0.0368)
+2022-11-18 15:57:48,772:INFO: Dataset: univ                Batch: 14/15	Loss 0.0367 (0.0368)
+2022-11-18 15:57:48,844:INFO: Dataset: univ                Batch: 15/15	Loss 0.0331 (0.0367)
+2022-11-18 15:57:49,267:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0816 (0.0816)
+2022-11-18 15:57:49,349:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0873 (0.0844)
+2022-11-18 15:57:49,414:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0857 (0.0848)
+2022-11-18 15:57:49,482:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0801 (0.0836)
+2022-11-18 15:57:49,548:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0842 (0.0837)
+2022-11-18 15:57:49,616:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0850 (0.0839)
+2022-11-18 15:57:49,682:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0960 (0.0857)
+2022-11-18 15:57:49,745:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0827 (0.0854)
+2022-11-18 15:57:50,061:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0553 (0.0553)
+2022-11-18 15:57:50,132:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0563 (0.0558)
+2022-11-18 15:57:50,202:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0556 (0.0558)
+2022-11-18 15:57:50,273:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0579 (0.0563)
+2022-11-18 15:57:50,348:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0626 (0.0576)
+2022-11-18 15:57:50,419:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0520 (0.0567)
+2022-11-18 15:57:50,488:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0556 (0.0565)
+2022-11-18 15:57:50,558:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0663 (0.0577)
+2022-11-18 15:57:50,627:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0579 (0.0578)
+2022-11-18 15:57:50,696:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0606 (0.0580)
+2022-11-18 15:57:50,767:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0568 (0.0579)
+2022-11-18 15:57:50,838:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0645 (0.0585)
+2022-11-18 15:57:50,906:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0588 (0.0585)
+2022-11-18 15:57:50,979:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0632 (0.0588)
+2022-11-18 15:57:51,049:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0594 (0.0589)
+2022-11-18 15:57:51,119:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0563 (0.0587)
+2022-11-18 15:57:51,190:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0644 (0.0591)
+2022-11-18 15:57:51,255:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0661 (0.0594)
+2022-11-18 15:57:51,301:INFO: - Computing ADE (validation)
+2022-11-18 15:57:51,583:INFO: 		 ADE on hotel                     dataset:	 0.9509425163269043
+2022-11-18 15:57:51,978:INFO: 		 ADE on univ                      dataset:	 1.4883837699890137
+2022-11-18 15:57:52,372:INFO: 		 ADE on zara1                     dataset:	 2.505465030670166
+2022-11-18 15:57:52,874:INFO: 		 ADE on zara2                     dataset:	 1.406551480293274
+2022-11-18 15:57:52,874:INFO: Average validation:	ADE  1.4881	FDE  2.7004
+2022-11-18 15:57:52,889:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_613.pth.tar
+2022-11-18 15:57:52,890:INFO: 
+===> EPOCH: 614 (P3)
+2022-11-18 15:57:52,890:INFO: - Computing loss (training)
+2022-11-18 15:57:53,194:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0472 (0.0472)
+2022-11-18 15:57:53,271:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0418 (0.0444)
+2022-11-18 15:57:53,339:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0487 (0.0458)
+2022-11-18 15:57:53,388:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0445 (0.0455)
+2022-11-18 15:57:53,750:INFO: Dataset: univ                Batch:  1/15	Loss 0.0366 (0.0366)
+2022-11-18 15:57:53,828:INFO: Dataset: univ                Batch:  2/15	Loss 0.0358 (0.0362)
+2022-11-18 15:57:53,909:INFO: Dataset: univ                Batch:  3/15	Loss 0.0359 (0.0361)
+2022-11-18 15:57:53,985:INFO: Dataset: univ                Batch:  4/15	Loss 0.0352 (0.0359)
+2022-11-18 15:57:54,059:INFO: Dataset: univ                Batch:  5/15	Loss 0.0360 (0.0359)
+2022-11-18 15:57:54,133:INFO: Dataset: univ                Batch:  6/15	Loss 0.0394 (0.0364)
+2022-11-18 15:57:54,204:INFO: Dataset: univ                Batch:  7/15	Loss 0.0353 (0.0363)
+2022-11-18 15:57:54,277:INFO: Dataset: univ                Batch:  8/15	Loss 0.0385 (0.0365)
+2022-11-18 15:57:54,349:INFO: Dataset: univ                Batch:  9/15	Loss 0.0358 (0.0365)
+2022-11-18 15:57:54,421:INFO: Dataset: univ                Batch: 10/15	Loss 0.0362 (0.0364)
+2022-11-18 15:57:54,496:INFO: Dataset: univ                Batch: 11/15	Loss 0.0389 (0.0367)
+2022-11-18 15:57:54,569:INFO: Dataset: univ                Batch: 12/15	Loss 0.0406 (0.0370)
+2022-11-18 15:57:54,642:INFO: Dataset: univ                Batch: 13/15	Loss 0.0362 (0.0369)
+2022-11-18 15:57:54,718:INFO: Dataset: univ                Batch: 14/15	Loss 0.0347 (0.0368)
+2022-11-18 15:57:54,751:INFO: Dataset: univ                Batch: 15/15	Loss 0.0319 (0.0367)
+2022-11-18 15:57:55,077:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0845 (0.0845)
+2022-11-18 15:57:55,152:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0929 (0.0888)
+2022-11-18 15:57:55,224:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0857 (0.0878)
+2022-11-18 15:57:55,297:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0850 (0.0871)
+2022-11-18 15:57:55,367:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0807 (0.0859)
+2022-11-18 15:57:55,436:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0829 (0.0854)
+2022-11-18 15:57:55,505:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0891 (0.0860)
+2022-11-18 15:57:55,568:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0799 (0.0854)
+2022-11-18 15:57:55,877:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0562 (0.0562)
+2022-11-18 15:57:55,947:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0548 (0.0555)
+2022-11-18 15:57:56,017:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0587 (0.0566)
+2022-11-18 15:57:56,091:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0545 (0.0560)
+2022-11-18 15:57:56,162:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0656 (0.0579)
+2022-11-18 15:57:56,235:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0592 (0.0582)
+2022-11-18 15:57:56,307:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0590 (0.0583)
+2022-11-18 15:57:56,379:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0621 (0.0587)
+2022-11-18 15:57:56,448:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0582 (0.0587)
+2022-11-18 15:57:56,518:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0670 (0.0595)
+2022-11-18 15:57:56,589:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0595 (0.0595)
+2022-11-18 15:57:56,660:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0575 (0.0593)
+2022-11-18 15:57:56,730:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0550 (0.0590)
+2022-11-18 15:57:56,801:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0578 (0.0589)
+2022-11-18 15:57:56,873:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0668 (0.0594)
+2022-11-18 15:57:56,944:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0583 (0.0594)
+2022-11-18 15:57:57,016:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0553 (0.0591)
+2022-11-18 15:57:57,079:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0620 (0.0593)
+2022-11-18 15:57:57,121:INFO: - Computing ADE (validation)
+2022-11-18 15:57:57,427:INFO: 		 ADE on hotel                     dataset:	 0.9408462047576904
+2022-11-18 15:57:57,816:INFO: 		 ADE on univ                      dataset:	 1.4886960983276367
+2022-11-18 15:57:58,113:INFO: 		 ADE on zara1                     dataset:	 2.496976137161255
+2022-11-18 15:57:58,601:INFO: 		 ADE on zara2                     dataset:	 1.406823754310608
+2022-11-18 15:57:58,601:INFO: Average validation:	ADE  1.4873	FDE  2.7002
+2022-11-18 15:57:58,614:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_614.pth.tar
+2022-11-18 15:57:58,615:INFO: 
+===> EPOCH: 615 (P3)
+2022-11-18 15:57:58,615:INFO: - Computing loss (training)
+2022-11-18 15:57:58,865:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0402 (0.0402)
+2022-11-18 15:57:58,935:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0381 (0.0391)
+2022-11-18 15:57:59,002:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0581 (0.0455)
+2022-11-18 15:57:59,051:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0451 (0.0454)
+2022-11-18 15:57:59,448:INFO: Dataset: univ                Batch:  1/15	Loss 0.0384 (0.0384)
+2022-11-18 15:57:59,526:INFO: Dataset: univ                Batch:  2/15	Loss 0.0367 (0.0375)
+2022-11-18 15:57:59,598:INFO: Dataset: univ                Batch:  3/15	Loss 0.0375 (0.0375)
+2022-11-18 15:57:59,674:INFO: Dataset: univ                Batch:  4/15	Loss 0.0392 (0.0379)
+2022-11-18 15:57:59,764:INFO: Dataset: univ                Batch:  5/15	Loss 0.0364 (0.0376)
+2022-11-18 15:57:59,842:INFO: Dataset: univ                Batch:  6/15	Loss 0.0370 (0.0375)
+2022-11-18 15:57:59,917:INFO: Dataset: univ                Batch:  7/15	Loss 0.0330 (0.0368)
+2022-11-18 15:57:59,991:INFO: Dataset: univ                Batch:  8/15	Loss 0.0346 (0.0365)
+2022-11-18 15:58:00,066:INFO: Dataset: univ                Batch:  9/15	Loss 0.0330 (0.0361)
+2022-11-18 15:58:00,140:INFO: Dataset: univ                Batch: 10/15	Loss 0.0397 (0.0364)
+2022-11-18 15:58:00,214:INFO: Dataset: univ                Batch: 11/15	Loss 0.0385 (0.0366)
+2022-11-18 15:58:00,288:INFO: Dataset: univ                Batch: 12/15	Loss 0.0376 (0.0367)
+2022-11-18 15:58:00,360:INFO: Dataset: univ                Batch: 13/15	Loss 0.0357 (0.0366)
+2022-11-18 15:58:00,434:INFO: Dataset: univ                Batch: 14/15	Loss 0.0360 (0.0366)
+2022-11-18 15:58:00,467:INFO: Dataset: univ                Batch: 15/15	Loss 0.0402 (0.0366)
+2022-11-18 15:58:00,776:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0902 (0.0902)
+2022-11-18 15:58:00,889:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0802 (0.0854)
+2022-11-18 15:58:01,021:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0913 (0.0872)
+2022-11-18 15:58:01,129:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0797 (0.0853)
+2022-11-18 15:58:01,232:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0887 (0.0860)
+2022-11-18 15:58:01,310:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0797 (0.0849)
+2022-11-18 15:58:01,377:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0848 (0.0849)
+2022-11-18 15:58:01,437:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0879 (0.0852)
+2022-11-18 15:58:01,746:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0629 (0.0629)
+2022-11-18 15:58:01,816:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0638 (0.0634)
+2022-11-18 15:58:01,889:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0594 (0.0622)
+2022-11-18 15:58:01,960:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0596 (0.0615)
+2022-11-18 15:58:02,034:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0590 (0.0610)
+2022-11-18 15:58:02,105:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0589 (0.0606)
+2022-11-18 15:58:02,192:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0542 (0.0598)
+2022-11-18 15:58:02,308:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0595 (0.0597)
+2022-11-18 15:58:02,398:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0639 (0.0602)
+2022-11-18 15:58:02,477:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0538 (0.0595)
+2022-11-18 15:58:02,549:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0633 (0.0599)
+2022-11-18 15:58:02,621:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0632 (0.0602)
+2022-11-18 15:58:02,694:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0543 (0.0597)
+2022-11-18 15:58:02,765:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0585 (0.0596)
+2022-11-18 15:58:02,840:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0619 (0.0597)
+2022-11-18 15:58:02,916:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0564 (0.0595)
+2022-11-18 15:58:02,987:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0564 (0.0593)
+2022-11-18 15:58:03,052:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0574 (0.0592)
+2022-11-18 15:58:03,097:INFO: - Computing ADE (validation)
+2022-11-18 15:58:03,412:INFO: 		 ADE on hotel                     dataset:	 0.9483057856559753
+2022-11-18 15:58:03,803:INFO: 		 ADE on univ                      dataset:	 1.48249351978302
+2022-11-18 15:58:04,123:INFO: 		 ADE on zara1                     dataset:	 2.4978296756744385
+2022-11-18 15:58:04,662:INFO: 		 ADE on zara2                     dataset:	 1.4035316705703735
+2022-11-18 15:58:04,662:INFO: Average validation:	ADE  1.4833	FDE  2.6955
+2022-11-18 15:58:04,676:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_615.pth.tar
+2022-11-18 15:58:04,676:INFO: 
+===> EPOCH: 616 (P3)
+2022-11-18 15:58:04,677:INFO: - Computing loss (training)
+2022-11-18 15:58:04,945:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0497 (0.0497)
+2022-11-18 15:58:05,020:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0363 (0.0428)
+2022-11-18 15:58:05,086:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0470 (0.0443)
+2022-11-18 15:58:05,135:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0507 (0.0454)
+2022-11-18 15:58:05,454:INFO: Dataset: univ                Batch:  1/15	Loss 0.0363 (0.0363)
+2022-11-18 15:58:05,529:INFO: Dataset: univ                Batch:  2/15	Loss 0.0370 (0.0366)
+2022-11-18 15:58:05,623:INFO: Dataset: univ                Batch:  3/15	Loss 0.0355 (0.0363)
+2022-11-18 15:58:05,697:INFO: Dataset: univ                Batch:  4/15	Loss 0.0375 (0.0366)
+2022-11-18 15:58:05,773:INFO: Dataset: univ                Batch:  5/15	Loss 0.0378 (0.0368)
+2022-11-18 15:58:05,850:INFO: Dataset: univ                Batch:  6/15	Loss 0.0346 (0.0364)
+2022-11-18 15:58:05,924:INFO: Dataset: univ                Batch:  7/15	Loss 0.0366 (0.0364)
+2022-11-18 15:58:05,997:INFO: Dataset: univ                Batch:  8/15	Loss 0.0397 (0.0368)
+2022-11-18 15:58:06,068:INFO: Dataset: univ                Batch:  9/15	Loss 0.0384 (0.0370)
+2022-11-18 15:58:06,141:INFO: Dataset: univ                Batch: 10/15	Loss 0.0375 (0.0370)
+2022-11-18 15:58:06,214:INFO: Dataset: univ                Batch: 11/15	Loss 0.0364 (0.0370)
+2022-11-18 15:58:06,287:INFO: Dataset: univ                Batch: 12/15	Loss 0.0346 (0.0368)
+2022-11-18 15:58:06,359:INFO: Dataset: univ                Batch: 13/15	Loss 0.0365 (0.0368)
+2022-11-18 15:58:06,454:INFO: Dataset: univ                Batch: 14/15	Loss 0.0346 (0.0366)
+2022-11-18 15:58:06,488:INFO: Dataset: univ                Batch: 15/15	Loss 0.0357 (0.0366)
+2022-11-18 15:58:06,875:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0838 (0.0838)
+2022-11-18 15:58:07,029:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0767 (0.0805)
+2022-11-18 15:58:07,157:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0872 (0.0827)
+2022-11-18 15:58:07,245:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0898 (0.0845)
+2022-11-18 15:58:07,319:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0890 (0.0852)
+2022-11-18 15:58:07,408:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0815 (0.0846)
+2022-11-18 15:58:07,480:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0864 (0.0848)
+2022-11-18 15:58:07,543:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0873 (0.0851)
+2022-11-18 15:58:07,949:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0665 (0.0665)
+2022-11-18 15:58:08,034:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0592 (0.0627)
+2022-11-18 15:58:08,103:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0482 (0.0578)
+2022-11-18 15:58:08,175:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0584 (0.0579)
+2022-11-18 15:58:08,252:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0530 (0.0569)
+2022-11-18 15:58:08,326:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0545 (0.0565)
+2022-11-18 15:58:08,416:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0593 (0.0569)
+2022-11-18 15:58:08,496:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0641 (0.0578)
+2022-11-18 15:58:08,586:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0621 (0.0583)
+2022-11-18 15:58:08,671:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0582 (0.0583)
+2022-11-18 15:58:08,742:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0580 (0.0582)
+2022-11-18 15:58:08,813:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0606 (0.0584)
+2022-11-18 15:58:08,884:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0620 (0.0587)
+2022-11-18 15:58:08,956:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0488 (0.0580)
+2022-11-18 15:58:09,028:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0612 (0.0582)
+2022-11-18 15:58:09,099:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0592 (0.0583)
+2022-11-18 15:58:09,171:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0713 (0.0591)
+2022-11-18 15:58:09,236:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0601 (0.0591)
+2022-11-18 15:58:09,280:INFO: - Computing ADE (validation)
+2022-11-18 15:58:09,606:INFO: 		 ADE on hotel                     dataset:	 0.9434725642204285
+2022-11-18 15:58:10,000:INFO: 		 ADE on univ                      dataset:	 1.4868783950805664
+2022-11-18 15:58:10,312:INFO: 		 ADE on zara1                     dataset:	 2.4969427585601807
+2022-11-18 15:58:10,796:INFO: 		 ADE on zara2                     dataset:	 1.4063116312026978
+2022-11-18 15:58:10,797:INFO: Average validation:	ADE  1.4863	FDE  2.6992
+2022-11-18 15:58:10,810:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_616.pth.tar
+2022-11-18 15:58:10,810:INFO: 
+===> EPOCH: 617 (P3)
+2022-11-18 15:58:10,810:INFO: - Computing loss (training)
+2022-11-18 15:58:11,086:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0438 (0.0438)
+2022-11-18 15:58:11,156:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0427 (0.0433)
+2022-11-18 15:58:11,223:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0520 (0.0461)
+2022-11-18 15:58:11,275:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0421 (0.0455)
+2022-11-18 15:58:11,632:INFO: Dataset: univ                Batch:  1/15	Loss 0.0355 (0.0355)
+2022-11-18 15:58:11,709:INFO: Dataset: univ                Batch:  2/15	Loss 0.0339 (0.0347)
+2022-11-18 15:58:11,813:INFO: Dataset: univ                Batch:  3/15	Loss 0.0382 (0.0358)
+2022-11-18 15:58:11,914:INFO: Dataset: univ                Batch:  4/15	Loss 0.0358 (0.0358)
+2022-11-18 15:58:11,988:INFO: Dataset: univ                Batch:  5/15	Loss 0.0347 (0.0356)
+2022-11-18 15:58:12,065:INFO: Dataset: univ                Batch:  6/15	Loss 0.0404 (0.0364)
+2022-11-18 15:58:12,140:INFO: Dataset: univ                Batch:  7/15	Loss 0.0350 (0.0362)
+2022-11-18 15:58:12,214:INFO: Dataset: univ                Batch:  8/15	Loss 0.0360 (0.0362)
+2022-11-18 15:58:12,304:INFO: Dataset: univ                Batch:  9/15	Loss 0.0389 (0.0364)
+2022-11-18 15:58:12,399:INFO: Dataset: univ                Batch: 10/15	Loss 0.0381 (0.0366)
+2022-11-18 15:58:12,473:INFO: Dataset: univ                Batch: 11/15	Loss 0.0386 (0.0368)
+2022-11-18 15:58:12,546:INFO: Dataset: univ                Batch: 12/15	Loss 0.0368 (0.0368)
+2022-11-18 15:58:12,619:INFO: Dataset: univ                Batch: 13/15	Loss 0.0357 (0.0367)
+2022-11-18 15:58:12,697:INFO: Dataset: univ                Batch: 14/15	Loss 0.0351 (0.0366)
+2022-11-18 15:58:12,746:INFO: Dataset: univ                Batch: 15/15	Loss 0.0359 (0.0366)
+2022-11-18 15:58:13,077:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0835 (0.0835)
+2022-11-18 15:58:13,151:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0864 (0.0850)
+2022-11-18 15:58:13,221:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0833 (0.0844)
+2022-11-18 15:58:13,288:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0831 (0.0840)
+2022-11-18 15:58:13,355:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0884 (0.0849)
+2022-11-18 15:58:13,422:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0856 (0.0850)
+2022-11-18 15:58:13,488:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0869 (0.0853)
+2022-11-18 15:58:13,549:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0823 (0.0849)
+2022-11-18 15:58:13,869:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0552 (0.0552)
+2022-11-18 15:58:13,949:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0579 (0.0565)
+2022-11-18 15:58:14,051:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0583 (0.0571)
+2022-11-18 15:58:14,218:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0588 (0.0575)
+2022-11-18 15:58:14,327:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0576 (0.0576)
+2022-11-18 15:58:14,413:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0607 (0.0581)
+2022-11-18 15:58:14,483:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0567 (0.0579)
+2022-11-18 15:58:14,551:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0564 (0.0577)
+2022-11-18 15:58:14,618:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0687 (0.0588)
+2022-11-18 15:58:14,685:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0530 (0.0582)
+2022-11-18 15:58:14,767:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0662 (0.0589)
+2022-11-18 15:58:14,868:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0637 (0.0593)
+2022-11-18 15:58:15,001:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0606 (0.0594)
+2022-11-18 15:58:15,119:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0609 (0.0595)
+2022-11-18 15:58:15,198:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0558 (0.0593)
+2022-11-18 15:58:15,276:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0645 (0.0596)
+2022-11-18 15:58:15,354:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0531 (0.0592)
+2022-11-18 15:58:15,426:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0571 (0.0591)
+2022-11-18 15:58:15,481:INFO: - Computing ADE (validation)
+2022-11-18 15:58:15,797:INFO: 		 ADE on hotel                     dataset:	 0.9443000555038452
+2022-11-18 15:58:16,196:INFO: 		 ADE on univ                      dataset:	 1.48173987865448
+2022-11-18 15:58:16,624:INFO: 		 ADE on zara1                     dataset:	 2.5004568099975586
+2022-11-18 15:58:17,122:INFO: 		 ADE on zara2                     dataset:	 1.4032167196273804
+2022-11-18 15:58:17,123:INFO: Average validation:	ADE  1.4827	FDE  2.6946
+2022-11-18 15:58:17,136:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_617.pth.tar
+2022-11-18 15:58:17,136:INFO: 
+===> EPOCH: 618 (P3)
+2022-11-18 15:58:17,136:INFO: - Computing loss (training)
+2022-11-18 15:58:17,400:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0415 (0.0415)
+2022-11-18 15:58:17,469:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0429 (0.0422)
+2022-11-18 15:58:17,535:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0547 (0.0463)
+2022-11-18 15:58:17,587:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0410 (0.0454)
+2022-11-18 15:58:17,938:INFO: Dataset: univ                Batch:  1/15	Loss 0.0342 (0.0342)
+2022-11-18 15:58:18,021:INFO: Dataset: univ                Batch:  2/15	Loss 0.0361 (0.0351)
+2022-11-18 15:58:18,102:INFO: Dataset: univ                Batch:  3/15	Loss 0.0390 (0.0363)
+2022-11-18 15:58:18,182:INFO: Dataset: univ                Batch:  4/15	Loss 0.0375 (0.0366)
+2022-11-18 15:58:18,258:INFO: Dataset: univ                Batch:  5/15	Loss 0.0381 (0.0369)
+2022-11-18 15:58:18,336:INFO: Dataset: univ                Batch:  6/15	Loss 0.0353 (0.0366)
+2022-11-18 15:58:18,414:INFO: Dataset: univ                Batch:  7/15	Loss 0.0367 (0.0366)
+2022-11-18 15:58:18,493:INFO: Dataset: univ                Batch:  8/15	Loss 0.0367 (0.0366)
+2022-11-18 15:58:18,573:INFO: Dataset: univ                Batch:  9/15	Loss 0.0345 (0.0364)
+2022-11-18 15:58:18,651:INFO: Dataset: univ                Batch: 10/15	Loss 0.0387 (0.0366)
+2022-11-18 15:58:18,728:INFO: Dataset: univ                Batch: 11/15	Loss 0.0367 (0.0366)
+2022-11-18 15:58:18,821:INFO: Dataset: univ                Batch: 12/15	Loss 0.0347 (0.0365)
+2022-11-18 15:58:18,931:INFO: Dataset: univ                Batch: 13/15	Loss 0.0373 (0.0365)
+2022-11-18 15:58:19,081:INFO: Dataset: univ                Batch: 14/15	Loss 0.0343 (0.0364)
+2022-11-18 15:58:19,145:INFO: Dataset: univ                Batch: 15/15	Loss 0.0475 (0.0365)
+2022-11-18 15:58:19,505:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0878 (0.0878)
+2022-11-18 15:58:19,573:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0808 (0.0840)
+2022-11-18 15:58:19,645:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0855 (0.0845)
+2022-11-18 15:58:19,731:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0773 (0.0828)
+2022-11-18 15:58:19,830:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0857 (0.0833)
+2022-11-18 15:58:19,958:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0834 (0.0833)
+2022-11-18 15:58:20,056:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0914 (0.0843)
+2022-11-18 15:58:20,122:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0898 (0.0849)
+2022-11-18 15:58:20,480:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0562 (0.0562)
+2022-11-18 15:58:20,551:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0628 (0.0592)
+2022-11-18 15:58:20,623:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0513 (0.0566)
+2022-11-18 15:58:20,697:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0644 (0.0585)
+2022-11-18 15:58:20,769:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0588 (0.0585)
+2022-11-18 15:58:20,843:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0630 (0.0593)
+2022-11-18 15:58:20,915:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0643 (0.0600)
+2022-11-18 15:58:20,987:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0582 (0.0598)
+2022-11-18 15:58:21,057:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0578 (0.0595)
+2022-11-18 15:58:21,127:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0624 (0.0598)
+2022-11-18 15:58:21,206:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0589 (0.0598)
+2022-11-18 15:58:21,279:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0641 (0.0601)
+2022-11-18 15:58:21,350:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0603 (0.0601)
+2022-11-18 15:58:21,422:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0596 (0.0601)
+2022-11-18 15:58:21,501:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0560 (0.0598)
+2022-11-18 15:58:21,583:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0543 (0.0595)
+2022-11-18 15:58:21,656:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0521 (0.0591)
+2022-11-18 15:58:21,722:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0567 (0.0590)
+2022-11-18 15:58:21,767:INFO: - Computing ADE (validation)
+2022-11-18 15:58:22,105:INFO: 		 ADE on hotel                     dataset:	 0.9487602114677429
+2022-11-18 15:58:22,538:INFO: 		 ADE on univ                      dataset:	 1.4835196733474731
+2022-11-18 15:58:22,846:INFO: 		 ADE on zara1                     dataset:	 2.48256516456604
+2022-11-18 15:58:23,322:INFO: 		 ADE on zara2                     dataset:	 1.4034016132354736
+2022-11-18 15:58:23,322:INFO: Average validation:	ADE  1.4829	FDE  2.6947
+2022-11-18 15:58:23,337:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_618.pth.tar
+2022-11-18 15:58:23,337:INFO: 
+===> EPOCH: 619 (P3)
+2022-11-18 15:58:23,337:INFO: - Computing loss (training)
+2022-11-18 15:58:23,634:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0433 (0.0433)
+2022-11-18 15:58:23,793:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0449 (0.0441)
+2022-11-18 15:58:23,897:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0492 (0.0459)
+2022-11-18 15:58:23,966:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0434 (0.0454)
+2022-11-18 15:58:24,374:INFO: Dataset: univ                Batch:  1/15	Loss 0.0373 (0.0373)
+2022-11-18 15:58:24,449:INFO: Dataset: univ                Batch:  2/15	Loss 0.0354 (0.0364)
+2022-11-18 15:58:24,545:INFO: Dataset: univ                Batch:  3/15	Loss 0.0370 (0.0366)
+2022-11-18 15:58:24,675:INFO: Dataset: univ                Batch:  4/15	Loss 0.0376 (0.0368)
+2022-11-18 15:58:24,763:INFO: Dataset: univ                Batch:  5/15	Loss 0.0329 (0.0360)
+2022-11-18 15:58:24,842:INFO: Dataset: univ                Batch:  6/15	Loss 0.0346 (0.0358)
+2022-11-18 15:58:24,918:INFO: Dataset: univ                Batch:  7/15	Loss 0.0353 (0.0357)
+2022-11-18 15:58:24,997:INFO: Dataset: univ                Batch:  8/15	Loss 0.0369 (0.0359)
+2022-11-18 15:58:25,079:INFO: Dataset: univ                Batch:  9/15	Loss 0.0360 (0.0359)
+2022-11-18 15:58:25,154:INFO: Dataset: univ                Batch: 10/15	Loss 0.0372 (0.0360)
+2022-11-18 15:58:25,246:INFO: Dataset: univ                Batch: 11/15	Loss 0.0372 (0.0361)
+2022-11-18 15:58:25,320:INFO: Dataset: univ                Batch: 12/15	Loss 0.0374 (0.0362)
+2022-11-18 15:58:25,399:INFO: Dataset: univ                Batch: 13/15	Loss 0.0353 (0.0362)
+2022-11-18 15:58:25,474:INFO: Dataset: univ                Batch: 14/15	Loss 0.0412 (0.0365)
+2022-11-18 15:58:25,507:INFO: Dataset: univ                Batch: 15/15	Loss 0.0384 (0.0365)
+2022-11-18 15:58:25,826:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0845 (0.0845)
+2022-11-18 15:58:25,906:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0865 (0.0855)
+2022-11-18 15:58:25,987:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0908 (0.0872)
+2022-11-18 15:58:26,087:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0842 (0.0864)
+2022-11-18 15:58:26,164:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0829 (0.0858)
+2022-11-18 15:58:26,234:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0803 (0.0848)
+2022-11-18 15:58:26,315:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0862 (0.0850)
+2022-11-18 15:58:26,395:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0829 (0.0848)
+2022-11-18 15:58:26,753:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0633 (0.0633)
+2022-11-18 15:58:26,828:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0555 (0.0599)
+2022-11-18 15:58:26,907:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0596 (0.0598)
+2022-11-18 15:58:26,983:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0602 (0.0599)
+2022-11-18 15:58:27,057:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0596 (0.0598)
+2022-11-18 15:58:27,134:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0525 (0.0586)
+2022-11-18 15:58:27,209:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0590 (0.0587)
+2022-11-18 15:58:27,281:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0572 (0.0585)
+2022-11-18 15:58:27,354:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0593 (0.0586)
+2022-11-18 15:58:27,442:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0580 (0.0585)
+2022-11-18 15:58:27,531:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0607 (0.0587)
+2022-11-18 15:58:27,618:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0597 (0.0588)
+2022-11-18 15:58:27,700:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0573 (0.0587)
+2022-11-18 15:58:27,775:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0608 (0.0589)
+2022-11-18 15:58:27,858:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0659 (0.0592)
+2022-11-18 15:58:27,946:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0587 (0.0592)
+2022-11-18 15:58:28,031:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0543 (0.0589)
+2022-11-18 15:58:28,102:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0582 (0.0589)
+2022-11-18 15:58:28,150:INFO: - Computing ADE (validation)
+2022-11-18 15:58:28,471:INFO: 		 ADE on hotel                     dataset:	 0.9495575428009033
+2022-11-18 15:58:28,851:INFO: 		 ADE on univ                      dataset:	 1.480077862739563
+2022-11-18 15:58:29,160:INFO: 		 ADE on zara1                     dataset:	 2.4856669902801514
+2022-11-18 15:58:29,640:INFO: 		 ADE on zara2                     dataset:	 1.4018371105194092
+2022-11-18 15:58:29,640:INFO: Average validation:	ADE  1.4808	FDE  2.6927
+2022-11-18 15:58:29,653:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_619.pth.tar
+2022-11-18 15:58:29,653:INFO: 
+===> EPOCH: 620 (P3)
+2022-11-18 15:58:29,653:INFO: - Computing loss (training)
+2022-11-18 15:58:29,917:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0454 (0.0454)
+2022-11-18 15:58:29,989:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0425 (0.0439)
+2022-11-18 15:58:30,061:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0488 (0.0455)
+2022-11-18 15:58:30,113:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0451 (0.0454)
+2022-11-18 15:58:30,453:INFO: Dataset: univ                Batch:  1/15	Loss 0.0369 (0.0369)
+2022-11-18 15:58:30,538:INFO: Dataset: univ                Batch:  2/15	Loss 0.0378 (0.0374)
+2022-11-18 15:58:30,617:INFO: Dataset: univ                Batch:  3/15	Loss 0.0372 (0.0373)
+2022-11-18 15:58:30,700:INFO: Dataset: univ                Batch:  4/15	Loss 0.0348 (0.0366)
+2022-11-18 15:58:30,784:INFO: Dataset: univ                Batch:  5/15	Loss 0.0355 (0.0364)
+2022-11-18 15:58:30,870:INFO: Dataset: univ                Batch:  6/15	Loss 0.0350 (0.0361)
+2022-11-18 15:58:31,005:INFO: Dataset: univ                Batch:  7/15	Loss 0.0355 (0.0360)
+2022-11-18 15:58:31,089:INFO: Dataset: univ                Batch:  8/15	Loss 0.0362 (0.0361)
+2022-11-18 15:58:31,168:INFO: Dataset: univ                Batch:  9/15	Loss 0.0375 (0.0362)
+2022-11-18 15:58:31,248:INFO: Dataset: univ                Batch: 10/15	Loss 0.0337 (0.0360)
+2022-11-18 15:58:31,341:INFO: Dataset: univ                Batch: 11/15	Loss 0.0382 (0.0362)
+2022-11-18 15:58:31,424:INFO: Dataset: univ                Batch: 12/15	Loss 0.0361 (0.0362)
+2022-11-18 15:58:31,505:INFO: Dataset: univ                Batch: 13/15	Loss 0.0392 (0.0364)
+2022-11-18 15:58:31,585:INFO: Dataset: univ                Batch: 14/15	Loss 0.0375 (0.0364)
+2022-11-18 15:58:31,621:INFO: Dataset: univ                Batch: 15/15	Loss 0.0347 (0.0364)
+2022-11-18 15:58:31,931:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0867 (0.0867)
+2022-11-18 15:58:32,062:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0835 (0.0851)
+2022-11-18 15:58:32,230:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0881 (0.0861)
+2022-11-18 15:58:32,337:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0815 (0.0849)
+2022-11-18 15:58:32,414:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0848 (0.0849)
+2022-11-18 15:58:32,484:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0880 (0.0854)
+2022-11-18 15:58:32,552:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0816 (0.0848)
+2022-11-18 15:58:32,612:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0829 (0.0846)
+2022-11-18 15:58:32,919:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0608 (0.0608)
+2022-11-18 15:58:32,989:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0579 (0.0592)
+2022-11-18 15:58:33,060:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0586 (0.0590)
+2022-11-18 15:58:33,129:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0559 (0.0581)
+2022-11-18 15:58:33,203:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0633 (0.0591)
+2022-11-18 15:58:33,274:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0653 (0.0602)
+2022-11-18 15:58:33,343:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0545 (0.0594)
+2022-11-18 15:58:33,414:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0715 (0.0609)
+2022-11-18 15:58:33,484:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0597 (0.0608)
+2022-11-18 15:58:33,559:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0604 (0.0607)
+2022-11-18 15:58:33,654:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0520 (0.0598)
+2022-11-18 15:58:33,739:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0591 (0.0598)
+2022-11-18 15:58:33,809:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0512 (0.0592)
+2022-11-18 15:58:33,902:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0610 (0.0593)
+2022-11-18 15:58:34,054:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0606 (0.0594)
+2022-11-18 15:58:34,137:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0519 (0.0589)
+2022-11-18 15:58:34,212:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0561 (0.0587)
+2022-11-18 15:58:34,277:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0602 (0.0588)
+2022-11-18 15:58:34,323:INFO: - Computing ADE (validation)
+2022-11-18 15:58:34,621:INFO: 		 ADE on hotel                     dataset:	 0.9471526145935059
+2022-11-18 15:58:35,008:INFO: 		 ADE on univ                      dataset:	 1.478691816329956
+2022-11-18 15:58:35,314:INFO: 		 ADE on zara1                     dataset:	 2.4765233993530273
+2022-11-18 15:58:35,814:INFO: 		 ADE on zara2                     dataset:	 1.399348497390747
+2022-11-18 15:58:35,814:INFO: Average validation:	ADE  1.4785	FDE  2.6900
+2022-11-18 15:58:35,828:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_620.pth.tar
+2022-11-18 15:58:35,828:INFO: 
+===> EPOCH: 621 (P3)
+2022-11-18 15:58:35,829:INFO: - Computing loss (training)
+2022-11-18 15:58:36,121:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0353 (0.0353)
+2022-11-18 15:58:36,190:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0435 (0.0393)
+2022-11-18 15:58:36,261:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0568 (0.0451)
+2022-11-18 15:58:36,314:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0469 (0.0454)
+2022-11-18 15:58:36,702:INFO: Dataset: univ                Batch:  1/15	Loss 0.0358 (0.0358)
+2022-11-18 15:58:36,821:INFO: Dataset: univ                Batch:  2/15	Loss 0.0349 (0.0353)
+2022-11-18 15:58:36,906:INFO: Dataset: univ                Batch:  3/15	Loss 0.0358 (0.0355)
+2022-11-18 15:58:36,987:INFO: Dataset: univ                Batch:  4/15	Loss 0.0393 (0.0364)
+2022-11-18 15:58:37,066:INFO: Dataset: univ                Batch:  5/15	Loss 0.0377 (0.0366)
+2022-11-18 15:58:37,144:INFO: Dataset: univ                Batch:  6/15	Loss 0.0381 (0.0369)
+2022-11-18 15:58:37,240:INFO: Dataset: univ                Batch:  7/15	Loss 0.0360 (0.0368)
+2022-11-18 15:58:37,392:INFO: Dataset: univ                Batch:  8/15	Loss 0.0360 (0.0367)
+2022-11-18 15:58:37,504:INFO: Dataset: univ                Batch:  9/15	Loss 0.0356 (0.0365)
+2022-11-18 15:58:37,600:INFO: Dataset: univ                Batch: 10/15	Loss 0.0376 (0.0366)
+2022-11-18 15:58:37,690:INFO: Dataset: univ                Batch: 11/15	Loss 0.0339 (0.0364)
+2022-11-18 15:58:37,764:INFO: Dataset: univ                Batch: 12/15	Loss 0.0368 (0.0364)
+2022-11-18 15:58:37,850:INFO: Dataset: univ                Batch: 13/15	Loss 0.0364 (0.0364)
+2022-11-18 15:58:37,927:INFO: Dataset: univ                Batch: 14/15	Loss 0.0379 (0.0365)
+2022-11-18 15:58:37,959:INFO: Dataset: univ                Batch: 15/15	Loss 0.0284 (0.0364)
+2022-11-18 15:58:38,265:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0888 (0.0888)
+2022-11-18 15:58:38,331:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0899 (0.0894)
+2022-11-18 15:58:38,399:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0750 (0.0847)
+2022-11-18 15:58:38,465:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0850 (0.0848)
+2022-11-18 15:58:38,531:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0898 (0.0858)
+2022-11-18 15:58:38,598:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0823 (0.0852)
+2022-11-18 15:58:38,663:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0746 (0.0837)
+2022-11-18 15:58:38,722:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0911 (0.0845)
+2022-11-18 15:58:39,030:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0584 (0.0584)
+2022-11-18 15:58:39,101:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0607 (0.0596)
+2022-11-18 15:58:39,172:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0555 (0.0583)
+2022-11-18 15:58:39,240:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0611 (0.0590)
+2022-11-18 15:58:39,311:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0606 (0.0593)
+2022-11-18 15:58:39,383:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0636 (0.0600)
+2022-11-18 15:58:39,450:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0515 (0.0589)
+2022-11-18 15:58:39,518:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0656 (0.0598)
+2022-11-18 15:58:39,587:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0554 (0.0593)
+2022-11-18 15:58:39,655:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0623 (0.0596)
+2022-11-18 15:58:39,725:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0602 (0.0597)
+2022-11-18 15:58:39,794:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0602 (0.0597)
+2022-11-18 15:58:39,863:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0538 (0.0592)
+2022-11-18 15:58:39,933:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0566 (0.0591)
+2022-11-18 15:58:40,002:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0538 (0.0587)
+2022-11-18 15:58:40,070:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0595 (0.0588)
+2022-11-18 15:58:40,140:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0553 (0.0585)
+2022-11-18 15:58:40,203:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0633 (0.0587)
+2022-11-18 15:58:40,250:INFO: - Computing ADE (validation)
+2022-11-18 15:58:40,545:INFO: 		 ADE on hotel                     dataset:	 0.9584129452705383
+2022-11-18 15:58:40,923:INFO: 		 ADE on univ                      dataset:	 1.477623701095581
+2022-11-18 15:58:41,223:INFO: 		 ADE on zara1                     dataset:	 2.482124090194702
+2022-11-18 15:58:41,710:INFO: 		 ADE on zara2                     dataset:	 1.3982733488082886
+2022-11-18 15:58:41,710:INFO: Average validation:	ADE  1.4785	FDE  2.6900
+2022-11-18 15:58:41,724:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_621.pth.tar
+2022-11-18 15:58:41,724:INFO: 
+===> EPOCH: 622 (P3)
+2022-11-18 15:58:41,725:INFO: - Computing loss (training)
+2022-11-18 15:58:41,987:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0403 (0.0403)
+2022-11-18 15:58:42,056:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0468 (0.0434)
+2022-11-18 15:58:42,127:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0437 (0.0435)
+2022-11-18 15:58:42,180:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0547 (0.0454)
+2022-11-18 15:58:42,503:INFO: Dataset: univ                Batch:  1/15	Loss 0.0347 (0.0347)
+2022-11-18 15:58:42,577:INFO: Dataset: univ                Batch:  2/15	Loss 0.0395 (0.0369)
+2022-11-18 15:58:42,652:INFO: Dataset: univ                Batch:  3/15	Loss 0.0343 (0.0360)
+2022-11-18 15:58:42,724:INFO: Dataset: univ                Batch:  4/15	Loss 0.0378 (0.0364)
+2022-11-18 15:58:42,799:INFO: Dataset: univ                Batch:  5/15	Loss 0.0371 (0.0365)
+2022-11-18 15:58:42,876:INFO: Dataset: univ                Batch:  6/15	Loss 0.0359 (0.0364)
+2022-11-18 15:58:42,948:INFO: Dataset: univ                Batch:  7/15	Loss 0.0359 (0.0364)
+2022-11-18 15:58:43,022:INFO: Dataset: univ                Batch:  8/15	Loss 0.0338 (0.0360)
+2022-11-18 15:58:43,096:INFO: Dataset: univ                Batch:  9/15	Loss 0.0357 (0.0360)
+2022-11-18 15:58:43,175:INFO: Dataset: univ                Batch: 10/15	Loss 0.0368 (0.0360)
+2022-11-18 15:58:43,258:INFO: Dataset: univ                Batch: 11/15	Loss 0.0354 (0.0360)
+2022-11-18 15:58:43,341:INFO: Dataset: univ                Batch: 12/15	Loss 0.0355 (0.0359)
+2022-11-18 15:58:43,416:INFO: Dataset: univ                Batch: 13/15	Loss 0.0382 (0.0361)
+2022-11-18 15:58:43,491:INFO: Dataset: univ                Batch: 14/15	Loss 0.0384 (0.0363)
+2022-11-18 15:58:43,523:INFO: Dataset: univ                Batch: 15/15	Loss 0.0422 (0.0363)
+2022-11-18 15:58:43,873:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0832 (0.0832)
+2022-11-18 15:58:43,948:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0824 (0.0828)
+2022-11-18 15:58:44,026:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0946 (0.0863)
+2022-11-18 15:58:44,107:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0790 (0.0844)
+2022-11-18 15:58:44,181:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0812 (0.0838)
+2022-11-18 15:58:44,252:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0828 (0.0836)
+2022-11-18 15:58:44,319:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0838 (0.0836)
+2022-11-18 15:58:44,381:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0913 (0.0844)
+2022-11-18 15:58:44,709:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0593 (0.0593)
+2022-11-18 15:58:44,786:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0624 (0.0608)
+2022-11-18 15:58:44,861:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0495 (0.0567)
+2022-11-18 15:58:44,945:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0633 (0.0583)
+2022-11-18 15:58:45,022:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0682 (0.0601)
+2022-11-18 15:58:45,109:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0617 (0.0604)
+2022-11-18 15:58:45,186:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0598 (0.0603)
+2022-11-18 15:58:45,257:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0560 (0.0598)
+2022-11-18 15:58:45,328:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0531 (0.0590)
+2022-11-18 15:58:45,400:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0496 (0.0580)
+2022-11-18 15:58:45,476:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0638 (0.0585)
+2022-11-18 15:58:45,559:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0602 (0.0587)
+2022-11-18 15:58:45,643:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0549 (0.0584)
+2022-11-18 15:58:45,723:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0658 (0.0589)
+2022-11-18 15:58:45,797:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0652 (0.0594)
+2022-11-18 15:58:45,870:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0633 (0.0596)
+2022-11-18 15:58:45,943:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0461 (0.0588)
+2022-11-18 15:58:46,009:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0529 (0.0586)
+2022-11-18 15:58:46,056:INFO: - Computing ADE (validation)
+2022-11-18 15:58:46,343:INFO: 		 ADE on hotel                     dataset:	 0.9578903913497925
+2022-11-18 15:58:46,722:INFO: 		 ADE on univ                      dataset:	 1.4805775880813599
+2022-11-18 15:58:47,034:INFO: 		 ADE on zara1                     dataset:	 2.4842798709869385
+2022-11-18 15:58:47,500:INFO: 		 ADE on zara2                     dataset:	 1.3984216451644897
+2022-11-18 15:58:47,500:INFO: Average validation:	ADE  1.4802	FDE  2.6919
+2022-11-18 15:58:47,513:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_622.pth.tar
+2022-11-18 15:58:47,513:INFO: 
+===> EPOCH: 623 (P3)
+2022-11-18 15:58:47,514:INFO: - Computing loss (training)
+2022-11-18 15:58:47,802:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0481 (0.0481)
+2022-11-18 15:58:47,882:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0437 (0.0459)
+2022-11-18 15:58:47,962:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0401 (0.0440)
+2022-11-18 15:58:48,021:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0526 (0.0454)
+2022-11-18 15:58:48,348:INFO: Dataset: univ                Batch:  1/15	Loss 0.0340 (0.0340)
+2022-11-18 15:58:48,430:INFO: Dataset: univ                Batch:  2/15	Loss 0.0348 (0.0344)
+2022-11-18 15:58:48,507:INFO: Dataset: univ                Batch:  3/15	Loss 0.0364 (0.0350)
+2022-11-18 15:58:48,593:INFO: Dataset: univ                Batch:  4/15	Loss 0.0363 (0.0353)
+2022-11-18 15:58:48,671:INFO: Dataset: univ                Batch:  5/15	Loss 0.0394 (0.0362)
+2022-11-18 15:58:48,753:INFO: Dataset: univ                Batch:  6/15	Loss 0.0364 (0.0362)
+2022-11-18 15:58:48,830:INFO: Dataset: univ                Batch:  7/15	Loss 0.0360 (0.0362)
+2022-11-18 15:58:48,908:INFO: Dataset: univ                Batch:  8/15	Loss 0.0372 (0.0363)
+2022-11-18 15:58:48,985:INFO: Dataset: univ                Batch:  9/15	Loss 0.0369 (0.0364)
+2022-11-18 15:58:49,062:INFO: Dataset: univ                Batch: 10/15	Loss 0.0383 (0.0365)
+2022-11-18 15:58:49,138:INFO: Dataset: univ                Batch: 11/15	Loss 0.0377 (0.0366)
+2022-11-18 15:58:49,220:INFO: Dataset: univ                Batch: 12/15	Loss 0.0348 (0.0365)
+2022-11-18 15:58:49,301:INFO: Dataset: univ                Batch: 13/15	Loss 0.0368 (0.0365)
+2022-11-18 15:58:49,382:INFO: Dataset: univ                Batch: 14/15	Loss 0.0339 (0.0363)
+2022-11-18 15:58:49,416:INFO: Dataset: univ                Batch: 15/15	Loss 0.0375 (0.0363)
+2022-11-18 15:58:49,840:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0854 (0.0854)
+2022-11-18 15:58:49,919:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0892 (0.0873)
+2022-11-18 15:58:49,991:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0816 (0.0856)
+2022-11-18 15:58:50,067:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0822 (0.0847)
+2022-11-18 15:58:50,143:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0849 (0.0848)
+2022-11-18 15:58:50,216:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0817 (0.0842)
+2022-11-18 15:58:50,291:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0868 (0.0845)
+2022-11-18 15:58:50,368:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0802 (0.0841)
+2022-11-18 15:58:50,695:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0602 (0.0602)
+2022-11-18 15:58:50,767:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0592 (0.0597)
+2022-11-18 15:58:50,841:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0559 (0.0586)
+2022-11-18 15:58:50,912:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0622 (0.0594)
+2022-11-18 15:58:50,984:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0536 (0.0581)
+2022-11-18 15:58:51,057:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0697 (0.0600)
+2022-11-18 15:58:51,127:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0485 (0.0583)
+2022-11-18 15:58:51,197:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0615 (0.0587)
+2022-11-18 15:58:51,266:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0569 (0.0585)
+2022-11-18 15:58:51,336:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0617 (0.0588)
+2022-11-18 15:58:51,407:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0561 (0.0586)
+2022-11-18 15:58:51,478:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0591 (0.0586)
+2022-11-18 15:58:51,548:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0643 (0.0590)
+2022-11-18 15:58:51,620:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0680 (0.0597)
+2022-11-18 15:58:51,689:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0484 (0.0588)
+2022-11-18 15:58:51,759:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0580 (0.0588)
+2022-11-18 15:58:51,831:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0584 (0.0588)
+2022-11-18 15:58:51,896:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0525 (0.0585)
+2022-11-18 15:58:51,940:INFO: - Computing ADE (validation)
+2022-11-18 15:58:52,221:INFO: 		 ADE on hotel                     dataset:	 0.9522334933280945
+2022-11-18 15:58:52,595:INFO: 		 ADE on univ                      dataset:	 1.4749789237976074
+2022-11-18 15:58:52,895:INFO: 		 ADE on zara1                     dataset:	 2.476409435272217
+2022-11-18 15:58:53,406:INFO: 		 ADE on zara2                     dataset:	 1.3924344778060913
+2022-11-18 15:58:53,406:INFO: Average validation:	ADE  1.4743	FDE  2.6838
+2022-11-18 15:58:53,419:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_623.pth.tar
+2022-11-18 15:58:53,419:INFO: 
+===> EPOCH: 624 (P3)
+2022-11-18 15:58:53,420:INFO: - Computing loss (training)
+2022-11-18 15:58:53,681:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0421 (0.0421)
+2022-11-18 15:58:53,751:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0504 (0.0460)
+2022-11-18 15:58:53,818:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0502 (0.0474)
+2022-11-18 15:58:53,869:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0364 (0.0454)
+2022-11-18 15:58:54,190:INFO: Dataset: univ                Batch:  1/15	Loss 0.0344 (0.0344)
+2022-11-18 15:58:54,265:INFO: Dataset: univ                Batch:  2/15	Loss 0.0381 (0.0363)
+2022-11-18 15:58:54,340:INFO: Dataset: univ                Batch:  3/15	Loss 0.0365 (0.0363)
+2022-11-18 15:58:54,416:INFO: Dataset: univ                Batch:  4/15	Loss 0.0363 (0.0363)
+2022-11-18 15:58:54,493:INFO: Dataset: univ                Batch:  5/15	Loss 0.0371 (0.0365)
+2022-11-18 15:58:54,575:INFO: Dataset: univ                Batch:  6/15	Loss 0.0378 (0.0367)
+2022-11-18 15:58:54,651:INFO: Dataset: univ                Batch:  7/15	Loss 0.0352 (0.0364)
+2022-11-18 15:58:54,727:INFO: Dataset: univ                Batch:  8/15	Loss 0.0383 (0.0367)
+2022-11-18 15:58:54,803:INFO: Dataset: univ                Batch:  9/15	Loss 0.0356 (0.0365)
+2022-11-18 15:58:54,879:INFO: Dataset: univ                Batch: 10/15	Loss 0.0352 (0.0364)
+2022-11-18 15:58:54,955:INFO: Dataset: univ                Batch: 11/15	Loss 0.0337 (0.0361)
+2022-11-18 15:58:55,035:INFO: Dataset: univ                Batch: 12/15	Loss 0.0372 (0.0362)
+2022-11-18 15:58:55,200:INFO: Dataset: univ                Batch: 13/15	Loss 0.0360 (0.0362)
+2022-11-18 15:58:55,297:INFO: Dataset: univ                Batch: 14/15	Loss 0.0371 (0.0363)
+2022-11-18 15:58:55,337:INFO: Dataset: univ                Batch: 15/15	Loss 0.0340 (0.0363)
+2022-11-18 15:58:55,675:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0829 (0.0829)
+2022-11-18 15:58:55,748:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0796 (0.0813)
+2022-11-18 15:58:55,819:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0843 (0.0823)
+2022-11-18 15:58:55,893:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0823 (0.0823)
+2022-11-18 15:58:55,963:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0867 (0.0831)
+2022-11-18 15:58:56,034:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0855 (0.0835)
+2022-11-18 15:58:56,118:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0855 (0.0838)
+2022-11-18 15:58:56,192:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0851 (0.0839)
+2022-11-18 15:58:56,541:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0642 (0.0642)
+2022-11-18 15:58:56,611:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0646 (0.0644)
+2022-11-18 15:58:56,678:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0612 (0.0633)
+2022-11-18 15:58:56,751:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0573 (0.0617)
+2022-11-18 15:58:56,821:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0618 (0.0618)
+2022-11-18 15:58:56,890:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0543 (0.0606)
+2022-11-18 15:58:56,956:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0580 (0.0602)
+2022-11-18 15:58:57,025:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0555 (0.0596)
+2022-11-18 15:58:57,093:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0633 (0.0600)
+2022-11-18 15:58:57,160:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0627 (0.0603)
+2022-11-18 15:58:57,229:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0602 (0.0603)
+2022-11-18 15:58:57,296:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0617 (0.0604)
+2022-11-18 15:58:57,365:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0542 (0.0599)
+2022-11-18 15:58:57,439:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0546 (0.0595)
+2022-11-18 15:58:57,509:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0550 (0.0592)
+2022-11-18 15:58:57,578:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0517 (0.0587)
+2022-11-18 15:58:57,647:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0562 (0.0586)
+2022-11-18 15:58:57,709:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0535 (0.0583)
+2022-11-18 15:58:57,758:INFO: - Computing ADE (validation)
+2022-11-18 15:58:58,132:INFO: 		 ADE on hotel                     dataset:	 0.9675933122634888
+2022-11-18 15:58:58,525:INFO: 		 ADE on univ                      dataset:	 1.4807530641555786
+2022-11-18 15:58:58,823:INFO: 		 ADE on zara1                     dataset:	 2.4768450260162354
+2022-11-18 15:58:59,303:INFO: 		 ADE on zara2                     dataset:	 1.398733139038086
+2022-11-18 15:58:59,303:INFO: Average validation:	ADE  1.4805	FDE  2.6919
+2022-11-18 15:58:59,316:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_624.pth.tar
+2022-11-18 15:58:59,316:INFO: 
+===> EPOCH: 625 (P3)
+2022-11-18 15:58:59,316:INFO: - Computing loss (training)
+2022-11-18 15:58:59,575:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0490 (0.0490)
+2022-11-18 15:58:59,644:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0487 (0.0488)
+2022-11-18 15:58:59,714:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0378 (0.0451)
+2022-11-18 15:58:59,767:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0474 (0.0454)
+2022-11-18 15:59:00,092:INFO: Dataset: univ                Batch:  1/15	Loss 0.0352 (0.0352)
+2022-11-18 15:59:00,168:INFO: Dataset: univ                Batch:  2/15	Loss 0.0347 (0.0349)
+2022-11-18 15:59:00,244:INFO: Dataset: univ                Batch:  3/15	Loss 0.0386 (0.0361)
+2022-11-18 15:59:00,319:INFO: Dataset: univ                Batch:  4/15	Loss 0.0383 (0.0366)
+2022-11-18 15:59:00,392:INFO: Dataset: univ                Batch:  5/15	Loss 0.0383 (0.0369)
+2022-11-18 15:59:00,467:INFO: Dataset: univ                Batch:  6/15	Loss 0.0357 (0.0367)
+2022-11-18 15:59:00,541:INFO: Dataset: univ                Batch:  7/15	Loss 0.0349 (0.0365)
+2022-11-18 15:59:00,615:INFO: Dataset: univ                Batch:  8/15	Loss 0.0354 (0.0363)
+2022-11-18 15:59:00,688:INFO: Dataset: univ                Batch:  9/15	Loss 0.0340 (0.0361)
+2022-11-18 15:59:00,760:INFO: Dataset: univ                Batch: 10/15	Loss 0.0375 (0.0362)
+2022-11-18 15:59:00,834:INFO: Dataset: univ                Batch: 11/15	Loss 0.0334 (0.0359)
+2022-11-18 15:59:00,907:INFO: Dataset: univ                Batch: 12/15	Loss 0.0355 (0.0359)
+2022-11-18 15:59:00,980:INFO: Dataset: univ                Batch: 13/15	Loss 0.0407 (0.0362)
+2022-11-18 15:59:01,054:INFO: Dataset: univ                Batch: 14/15	Loss 0.0351 (0.0362)
+2022-11-18 15:59:01,087:INFO: Dataset: univ                Batch: 15/15	Loss 0.0383 (0.0362)
+2022-11-18 15:59:01,385:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0844 (0.0844)
+2022-11-18 15:59:01,452:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0843 (0.0844)
+2022-11-18 15:59:01,521:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0769 (0.0818)
+2022-11-18 15:59:01,587:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0845 (0.0825)
+2022-11-18 15:59:01,654:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0838 (0.0828)
+2022-11-18 15:59:01,721:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0820 (0.0826)
+2022-11-18 15:59:01,787:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0880 (0.0834)
+2022-11-18 15:59:01,848:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0865 (0.0837)
+2022-11-18 15:59:02,147:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0573 (0.0573)
+2022-11-18 15:59:02,218:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0550 (0.0562)
+2022-11-18 15:59:02,284:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0521 (0.0549)
+2022-11-18 15:59:02,350:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0531 (0.0545)
+2022-11-18 15:59:02,420:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0525 (0.0541)
+2022-11-18 15:59:02,489:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0651 (0.0558)
+2022-11-18 15:59:02,554:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0574 (0.0560)
+2022-11-18 15:59:02,621:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0640 (0.0570)
+2022-11-18 15:59:02,686:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0631 (0.0577)
+2022-11-18 15:59:02,751:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0475 (0.0566)
+2022-11-18 15:59:02,818:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0614 (0.0571)
+2022-11-18 15:59:02,885:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0540 (0.0569)
+2022-11-18 15:59:02,951:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0626 (0.0573)
+2022-11-18 15:59:03,017:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0593 (0.0575)
+2022-11-18 15:59:03,085:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0558 (0.0574)
+2022-11-18 15:59:03,152:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0618 (0.0576)
+2022-11-18 15:59:03,219:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0616 (0.0579)
+2022-11-18 15:59:03,279:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0653 (0.0582)
+2022-11-18 15:59:03,326:INFO: - Computing ADE (validation)
+2022-11-18 15:59:03,604:INFO: 		 ADE on hotel                     dataset:	 0.9623929262161255
+2022-11-18 15:59:03,984:INFO: 		 ADE on univ                      dataset:	 1.4775934219360352
+2022-11-18 15:59:04,282:INFO: 		 ADE on zara1                     dataset:	 2.4600982666015625
+2022-11-18 15:59:04,742:INFO: 		 ADE on zara2                     dataset:	 1.4017671346664429
+2022-11-18 15:59:04,743:INFO: Average validation:	ADE  1.4787	FDE  2.6885
+2022-11-18 15:59:04,755:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_625.pth.tar
+2022-11-18 15:59:04,755:INFO: 
+===> EPOCH: 626 (P3)
+2022-11-18 15:59:04,756:INFO: - Computing loss (training)
+2022-11-18 15:59:05,017:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0477 (0.0477)
+2022-11-18 15:59:05,085:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0446 (0.0461)
+2022-11-18 15:59:05,153:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0458 (0.0460)
+2022-11-18 15:59:05,203:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0423 (0.0455)
+2022-11-18 15:59:05,520:INFO: Dataset: univ                Batch:  1/15	Loss 0.0366 (0.0366)
+2022-11-18 15:59:05,595:INFO: Dataset: univ                Batch:  2/15	Loss 0.0394 (0.0379)
+2022-11-18 15:59:05,669:INFO: Dataset: univ                Batch:  3/15	Loss 0.0383 (0.0380)
+2022-11-18 15:59:05,746:INFO: Dataset: univ                Batch:  4/15	Loss 0.0358 (0.0375)
+2022-11-18 15:59:05,822:INFO: Dataset: univ                Batch:  5/15	Loss 0.0389 (0.0378)
+2022-11-18 15:59:05,896:INFO: Dataset: univ                Batch:  6/15	Loss 0.0338 (0.0371)
+2022-11-18 15:59:05,969:INFO: Dataset: univ                Batch:  7/15	Loss 0.0379 (0.0372)
+2022-11-18 15:59:06,042:INFO: Dataset: univ                Batch:  8/15	Loss 0.0348 (0.0369)
+2022-11-18 15:59:06,117:INFO: Dataset: univ                Batch:  9/15	Loss 0.0344 (0.0366)
+2022-11-18 15:59:06,189:INFO: Dataset: univ                Batch: 10/15	Loss 0.0363 (0.0365)
+2022-11-18 15:59:06,265:INFO: Dataset: univ                Batch: 11/15	Loss 0.0365 (0.0365)
+2022-11-18 15:59:06,339:INFO: Dataset: univ                Batch: 12/15	Loss 0.0344 (0.0363)
+2022-11-18 15:59:06,411:INFO: Dataset: univ                Batch: 13/15	Loss 0.0374 (0.0364)
+2022-11-18 15:59:06,486:INFO: Dataset: univ                Batch: 14/15	Loss 0.0333 (0.0361)
+2022-11-18 15:59:06,519:INFO: Dataset: univ                Batch: 15/15	Loss 0.0373 (0.0362)
+2022-11-18 15:59:06,822:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0851 (0.0851)
+2022-11-18 15:59:06,891:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0810 (0.0828)
+2022-11-18 15:59:06,960:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0874 (0.0844)
+2022-11-18 15:59:07,029:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0821 (0.0838)
+2022-11-18 15:59:07,102:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0814 (0.0833)
+2022-11-18 15:59:07,173:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0871 (0.0840)
+2022-11-18 15:59:07,246:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0862 (0.0843)
+2022-11-18 15:59:07,309:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0778 (0.0835)
+2022-11-18 15:59:07,617:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0591 (0.0591)
+2022-11-18 15:59:07,687:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0532 (0.0562)
+2022-11-18 15:59:07,758:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0572 (0.0565)
+2022-11-18 15:59:07,827:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0538 (0.0559)
+2022-11-18 15:59:07,898:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0588 (0.0565)
+2022-11-18 15:59:07,968:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0662 (0.0581)
+2022-11-18 15:59:08,035:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0610 (0.0585)
+2022-11-18 15:59:08,105:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0557 (0.0581)
+2022-11-18 15:59:08,173:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0577 (0.0581)
+2022-11-18 15:59:08,241:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0585 (0.0581)
+2022-11-18 15:59:08,309:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0608 (0.0584)
+2022-11-18 15:59:08,378:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0524 (0.0579)
+2022-11-18 15:59:08,445:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0536 (0.0576)
+2022-11-18 15:59:08,515:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0592 (0.0577)
+2022-11-18 15:59:08,583:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0564 (0.0576)
+2022-11-18 15:59:08,651:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0691 (0.0583)
+2022-11-18 15:59:08,721:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0594 (0.0584)
+2022-11-18 15:59:08,786:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0516 (0.0580)
+2022-11-18 15:59:08,831:INFO: - Computing ADE (validation)
+2022-11-18 15:59:09,108:INFO: 		 ADE on hotel                     dataset:	 0.9769113659858704
+2022-11-18 15:59:09,487:INFO: 		 ADE on univ                      dataset:	 1.4729076623916626
+2022-11-18 15:59:09,794:INFO: 		 ADE on zara1                     dataset:	 2.462977647781372
+2022-11-18 15:59:10,279:INFO: 		 ADE on zara2                     dataset:	 1.396847128868103
+2022-11-18 15:59:10,279:INFO: Average validation:	ADE  1.4754	FDE  2.6853
+2022-11-18 15:59:10,291:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_626.pth.tar
+2022-11-18 15:59:10,291:INFO: 
+===> EPOCH: 627 (P3)
+2022-11-18 15:59:10,292:INFO: - Computing loss (training)
+2022-11-18 15:59:10,563:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0500 (0.0500)
+2022-11-18 15:59:10,635:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0447 (0.0474)
+2022-11-18 15:59:10,706:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0474 (0.0474)
+2022-11-18 15:59:10,758:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0356 (0.0455)
+2022-11-18 15:59:11,082:INFO: Dataset: univ                Batch:  1/15	Loss 0.0368 (0.0368)
+2022-11-18 15:59:11,161:INFO: Dataset: univ                Batch:  2/15	Loss 0.0353 (0.0360)
+2022-11-18 15:59:11,243:INFO: Dataset: univ                Batch:  3/15	Loss 0.0367 (0.0362)
+2022-11-18 15:59:11,326:INFO: Dataset: univ                Batch:  4/15	Loss 0.0354 (0.0360)
+2022-11-18 15:59:11,414:INFO: Dataset: univ                Batch:  5/15	Loss 0.0384 (0.0365)
+2022-11-18 15:59:11,509:INFO: Dataset: univ                Batch:  6/15	Loss 0.0362 (0.0364)
+2022-11-18 15:59:11,597:INFO: Dataset: univ                Batch:  7/15	Loss 0.0345 (0.0361)
+2022-11-18 15:59:11,679:INFO: Dataset: univ                Batch:  8/15	Loss 0.0342 (0.0359)
+2022-11-18 15:59:11,756:INFO: Dataset: univ                Batch:  9/15	Loss 0.0388 (0.0362)
+2022-11-18 15:59:11,832:INFO: Dataset: univ                Batch: 10/15	Loss 0.0372 (0.0363)
+2022-11-18 15:59:11,913:INFO: Dataset: univ                Batch: 11/15	Loss 0.0384 (0.0365)
+2022-11-18 15:59:12,003:INFO: Dataset: univ                Batch: 12/15	Loss 0.0407 (0.0368)
+2022-11-18 15:59:12,085:INFO: Dataset: univ                Batch: 13/15	Loss 0.0319 (0.0364)
+2022-11-18 15:59:12,163:INFO: Dataset: univ                Batch: 14/15	Loss 0.0334 (0.0362)
+2022-11-18 15:59:12,196:INFO: Dataset: univ                Batch: 15/15	Loss 0.0321 (0.0361)
+2022-11-18 15:59:12,520:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0824 (0.0824)
+2022-11-18 15:59:12,592:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0821 (0.0823)
+2022-11-18 15:59:12,666:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0841 (0.0829)
+2022-11-18 15:59:12,737:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0769 (0.0816)
+2022-11-18 15:59:12,812:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0865 (0.0826)
+2022-11-18 15:59:12,882:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0828 (0.0827)
+2022-11-18 15:59:12,952:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0847 (0.0830)
+2022-11-18 15:59:13,017:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0856 (0.0833)
+2022-11-18 15:59:13,323:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0599 (0.0599)
+2022-11-18 15:59:13,391:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0575 (0.0587)
+2022-11-18 15:59:13,461:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0597 (0.0591)
+2022-11-18 15:59:13,531:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0654 (0.0604)
+2022-11-18 15:59:13,599:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0632 (0.0610)
+2022-11-18 15:59:13,669:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0586 (0.0606)
+2022-11-18 15:59:13,735:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0618 (0.0608)
+2022-11-18 15:59:13,803:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0466 (0.0590)
+2022-11-18 15:59:13,871:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0562 (0.0587)
+2022-11-18 15:59:13,937:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0611 (0.0589)
+2022-11-18 15:59:14,003:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0526 (0.0583)
+2022-11-18 15:59:14,072:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0683 (0.0591)
+2022-11-18 15:59:14,140:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0522 (0.0585)
+2022-11-18 15:59:14,214:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0541 (0.0582)
+2022-11-18 15:59:14,286:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0547 (0.0580)
+2022-11-18 15:59:14,357:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0546 (0.0578)
+2022-11-18 15:59:14,430:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0612 (0.0580)
+2022-11-18 15:59:14,495:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0562 (0.0579)
+2022-11-18 15:59:14,539:INFO: - Computing ADE (validation)
+2022-11-18 15:59:14,820:INFO: 		 ADE on hotel                     dataset:	 0.9795747399330139
+2022-11-18 15:59:15,210:INFO: 		 ADE on univ                      dataset:	 1.4756526947021484
+2022-11-18 15:59:15,540:INFO: 		 ADE on zara1                     dataset:	 2.462489366531372
+2022-11-18 15:59:16,098:INFO: 		 ADE on zara2                     dataset:	 1.4010242223739624
+2022-11-18 15:59:16,098:INFO: Average validation:	ADE  1.4785	FDE  2.6882
+2022-11-18 15:59:16,111:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_627.pth.tar
+2022-11-18 15:59:16,111:INFO: 
+===> EPOCH: 628 (P3)
+2022-11-18 15:59:16,112:INFO: - Computing loss (training)
+2022-11-18 15:59:16,414:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0427 (0.0427)
+2022-11-18 15:59:16,487:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0473 (0.0450)
+2022-11-18 15:59:16,563:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0415 (0.0438)
+2022-11-18 15:59:16,620:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0529 (0.0455)
+2022-11-18 15:59:17,022:INFO: Dataset: univ                Batch:  1/15	Loss 0.0354 (0.0354)
+2022-11-18 15:59:17,105:INFO: Dataset: univ                Batch:  2/15	Loss 0.0351 (0.0353)
+2022-11-18 15:59:17,185:INFO: Dataset: univ                Batch:  3/15	Loss 0.0361 (0.0356)
+2022-11-18 15:59:17,267:INFO: Dataset: univ                Batch:  4/15	Loss 0.0327 (0.0349)
+2022-11-18 15:59:17,346:INFO: Dataset: univ                Batch:  5/15	Loss 0.0379 (0.0354)
+2022-11-18 15:59:17,423:INFO: Dataset: univ                Batch:  6/15	Loss 0.0348 (0.0353)
+2022-11-18 15:59:17,499:INFO: Dataset: univ                Batch:  7/15	Loss 0.0365 (0.0355)
+2022-11-18 15:59:17,575:INFO: Dataset: univ                Batch:  8/15	Loss 0.0350 (0.0355)
+2022-11-18 15:59:17,651:INFO: Dataset: univ                Batch:  9/15	Loss 0.0334 (0.0352)
+2022-11-18 15:59:17,727:INFO: Dataset: univ                Batch: 10/15	Loss 0.0382 (0.0355)
+2022-11-18 15:59:17,809:INFO: Dataset: univ                Batch: 11/15	Loss 0.0377 (0.0357)
+2022-11-18 15:59:17,893:INFO: Dataset: univ                Batch: 12/15	Loss 0.0363 (0.0357)
+2022-11-18 15:59:17,975:INFO: Dataset: univ                Batch: 13/15	Loss 0.0384 (0.0359)
+2022-11-18 15:59:18,058:INFO: Dataset: univ                Batch: 14/15	Loss 0.0389 (0.0361)
+2022-11-18 15:59:18,095:INFO: Dataset: univ                Batch: 15/15	Loss 0.0321 (0.0361)
+2022-11-18 15:59:18,401:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0787 (0.0787)
+2022-11-18 15:59:18,472:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0881 (0.0832)
+2022-11-18 15:59:18,541:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0750 (0.0803)
+2022-11-18 15:59:18,616:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0897 (0.0825)
+2022-11-18 15:59:18,687:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0841 (0.0828)
+2022-11-18 15:59:18,756:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0828 (0.0828)
+2022-11-18 15:59:18,824:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0817 (0.0827)
+2022-11-18 15:59:18,887:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0841 (0.0828)
+2022-11-18 15:59:19,212:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0615 (0.0615)
+2022-11-18 15:59:19,316:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0606 (0.0610)
+2022-11-18 15:59:19,390:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0589 (0.0604)
+2022-11-18 15:59:19,466:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0587 (0.0600)
+2022-11-18 15:59:19,538:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0594 (0.0598)
+2022-11-18 15:59:19,610:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0588 (0.0597)
+2022-11-18 15:59:19,685:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0565 (0.0592)
+2022-11-18 15:59:19,764:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0613 (0.0595)
+2022-11-18 15:59:19,845:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0508 (0.0585)
+2022-11-18 15:59:19,917:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0511 (0.0578)
+2022-11-18 15:59:19,986:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0548 (0.0576)
+2022-11-18 15:59:20,060:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0485 (0.0568)
+2022-11-18 15:59:20,136:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0552 (0.0567)
+2022-11-18 15:59:20,240:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0660 (0.0574)
+2022-11-18 15:59:20,317:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0535 (0.0571)
+2022-11-18 15:59:20,391:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0657 (0.0578)
+2022-11-18 15:59:20,462:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0585 (0.0578)
+2022-11-18 15:59:20,527:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0560 (0.0577)
+2022-11-18 15:59:20,572:INFO: - Computing ADE (validation)
+2022-11-18 15:59:20,870:INFO: 		 ADE on hotel                     dataset:	 0.9782513380050659
+2022-11-18 15:59:21,275:INFO: 		 ADE on univ                      dataset:	 1.4721604585647583
+2022-11-18 15:59:21,590:INFO: 		 ADE on zara1                     dataset:	 2.4189980030059814
+2022-11-18 15:59:22,091:INFO: 		 ADE on zara2                     dataset:	 1.3987993001937866
+2022-11-18 15:59:22,091:INFO: Average validation:	ADE  1.4732	FDE  2.6812
+2022-11-18 15:59:22,104:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_628.pth.tar
+2022-11-18 15:59:22,104:INFO: 
+===> EPOCH: 629 (P3)
+2022-11-18 15:59:22,105:INFO: - Computing loss (training)
+2022-11-18 15:59:22,369:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0495 (0.0495)
+2022-11-18 15:59:22,437:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0416 (0.0456)
+2022-11-18 15:59:22,506:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0503 (0.0471)
+2022-11-18 15:59:22,558:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0373 (0.0456)
+2022-11-18 15:59:22,882:INFO: Dataset: univ                Batch:  1/15	Loss 0.0349 (0.0349)
+2022-11-18 15:59:22,963:INFO: Dataset: univ                Batch:  2/15	Loss 0.0382 (0.0365)
+2022-11-18 15:59:23,048:INFO: Dataset: univ                Batch:  3/15	Loss 0.0342 (0.0357)
+2022-11-18 15:59:23,125:INFO: Dataset: univ                Batch:  4/15	Loss 0.0367 (0.0359)
+2022-11-18 15:59:23,203:INFO: Dataset: univ                Batch:  5/15	Loss 0.0379 (0.0363)
+2022-11-18 15:59:23,290:INFO: Dataset: univ                Batch:  6/15	Loss 0.0372 (0.0365)
+2022-11-18 15:59:23,377:INFO: Dataset: univ                Batch:  7/15	Loss 0.0330 (0.0359)
+2022-11-18 15:59:23,456:INFO: Dataset: univ                Batch:  8/15	Loss 0.0361 (0.0359)
+2022-11-18 15:59:23,534:INFO: Dataset: univ                Batch:  9/15	Loss 0.0350 (0.0358)
+2022-11-18 15:59:23,614:INFO: Dataset: univ                Batch: 10/15	Loss 0.0348 (0.0357)
+2022-11-18 15:59:23,691:INFO: Dataset: univ                Batch: 11/15	Loss 0.0381 (0.0359)
+2022-11-18 15:59:23,769:INFO: Dataset: univ                Batch: 12/15	Loss 0.0377 (0.0360)
+2022-11-18 15:59:23,849:INFO: Dataset: univ                Batch: 13/15	Loss 0.0356 (0.0360)
+2022-11-18 15:59:23,927:INFO: Dataset: univ                Batch: 14/15	Loss 0.0354 (0.0360)
+2022-11-18 15:59:23,961:INFO: Dataset: univ                Batch: 15/15	Loss 0.0379 (0.0360)
+2022-11-18 15:59:24,325:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0775 (0.0775)
+2022-11-18 15:59:24,406:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0815 (0.0794)
+2022-11-18 15:59:24,486:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0800 (0.0796)
+2022-11-18 15:59:24,586:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0894 (0.0820)
+2022-11-18 15:59:24,679:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0849 (0.0825)
+2022-11-18 15:59:24,760:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0855 (0.0830)
+2022-11-18 15:59:24,837:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0823 (0.0829)
+2022-11-18 15:59:24,903:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0801 (0.0826)
+2022-11-18 15:59:25,473:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0581 (0.0581)
+2022-11-18 15:59:25,544:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0599 (0.0590)
+2022-11-18 15:59:25,612:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0562 (0.0580)
+2022-11-18 15:59:25,686:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0527 (0.0567)
+2022-11-18 15:59:25,760:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0565 (0.0566)
+2022-11-18 15:59:25,829:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0612 (0.0573)
+2022-11-18 15:59:25,895:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0586 (0.0575)
+2022-11-18 15:59:25,963:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0607 (0.0579)
+2022-11-18 15:59:26,030:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0650 (0.0586)
+2022-11-18 15:59:26,096:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0541 (0.0581)
+2022-11-18 15:59:26,166:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0629 (0.0586)
+2022-11-18 15:59:26,234:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0515 (0.0580)
+2022-11-18 15:59:26,300:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0553 (0.0578)
+2022-11-18 15:59:26,367:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0615 (0.0580)
+2022-11-18 15:59:26,435:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0526 (0.0576)
+2022-11-18 15:59:26,505:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0636 (0.0580)
+2022-11-18 15:59:26,577:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0505 (0.0574)
+2022-11-18 15:59:26,649:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0596 (0.0575)
+2022-11-18 15:59:26,700:INFO: - Computing ADE (validation)
+2022-11-18 15:59:27,006:INFO: 		 ADE on hotel                     dataset:	 0.9796419143676758
+2022-11-18 15:59:27,435:INFO: 		 ADE on univ                      dataset:	 1.4769632816314697
+2022-11-18 15:59:27,762:INFO: 		 ADE on zara1                     dataset:	 2.426068067550659
+2022-11-18 15:59:28,266:INFO: 		 ADE on zara2                     dataset:	 1.399401068687439
+2022-11-18 15:59:28,266:INFO: Average validation:	ADE  1.4765	FDE  2.6851
+2022-11-18 15:59:28,280:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_629.pth.tar
+2022-11-18 15:59:28,280:INFO: 
+===> EPOCH: 630 (P3)
+2022-11-18 15:59:28,281:INFO: - Computing loss (training)
+2022-11-18 15:59:28,540:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0508 (0.0508)
+2022-11-18 15:59:28,611:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0416 (0.0463)
+2022-11-18 15:59:28,684:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0456 (0.0460)
+2022-11-18 15:59:28,735:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0432 (0.0456)
+2022-11-18 15:59:29,065:INFO: Dataset: univ                Batch:  1/15	Loss 0.0351 (0.0351)
+2022-11-18 15:59:29,141:INFO: Dataset: univ                Batch:  2/15	Loss 0.0382 (0.0365)
+2022-11-18 15:59:29,220:INFO: Dataset: univ                Batch:  3/15	Loss 0.0402 (0.0377)
+2022-11-18 15:59:29,294:INFO: Dataset: univ                Batch:  4/15	Loss 0.0344 (0.0368)
+2022-11-18 15:59:29,369:INFO: Dataset: univ                Batch:  5/15	Loss 0.0347 (0.0364)
+2022-11-18 15:59:29,445:INFO: Dataset: univ                Batch:  6/15	Loss 0.0357 (0.0362)
+2022-11-18 15:59:29,518:INFO: Dataset: univ                Batch:  7/15	Loss 0.0346 (0.0360)
+2022-11-18 15:59:29,591:INFO: Dataset: univ                Batch:  8/15	Loss 0.0343 (0.0358)
+2022-11-18 15:59:29,664:INFO: Dataset: univ                Batch:  9/15	Loss 0.0362 (0.0359)
+2022-11-18 15:59:29,738:INFO: Dataset: univ                Batch: 10/15	Loss 0.0363 (0.0359)
+2022-11-18 15:59:29,818:INFO: Dataset: univ                Batch: 11/15	Loss 0.0366 (0.0360)
+2022-11-18 15:59:29,893:INFO: Dataset: univ                Batch: 12/15	Loss 0.0378 (0.0361)
+2022-11-18 15:59:29,967:INFO: Dataset: univ                Batch: 13/15	Loss 0.0385 (0.0363)
+2022-11-18 15:59:30,042:INFO: Dataset: univ                Batch: 14/15	Loss 0.0317 (0.0359)
+2022-11-18 15:59:30,076:INFO: Dataset: univ                Batch: 15/15	Loss 0.0350 (0.0359)
+2022-11-18 15:59:30,402:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0756 (0.0756)
+2022-11-18 15:59:30,473:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0879 (0.0818)
+2022-11-18 15:59:30,543:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0784 (0.0807)
+2022-11-18 15:59:30,613:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0847 (0.0816)
+2022-11-18 15:59:30,683:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0861 (0.0824)
+2022-11-18 15:59:30,755:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0777 (0.0816)
+2022-11-18 15:59:30,824:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0860 (0.0823)
+2022-11-18 15:59:30,887:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0829 (0.0823)
+2022-11-18 15:59:31,204:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0687 (0.0687)
+2022-11-18 15:59:31,280:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0605 (0.0644)
+2022-11-18 15:59:31,359:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0559 (0.0617)
+2022-11-18 15:59:31,428:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0573 (0.0606)
+2022-11-18 15:59:31,495:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0539 (0.0592)
+2022-11-18 15:59:31,566:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0572 (0.0589)
+2022-11-18 15:59:31,634:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0564 (0.0585)
+2022-11-18 15:59:31,701:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0576 (0.0584)
+2022-11-18 15:59:31,767:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0492 (0.0574)
+2022-11-18 15:59:31,835:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0578 (0.0574)
+2022-11-18 15:59:31,905:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0552 (0.0572)
+2022-11-18 15:59:31,972:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0647 (0.0578)
+2022-11-18 15:59:32,045:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0503 (0.0572)
+2022-11-18 15:59:32,115:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0579 (0.0573)
+2022-11-18 15:59:32,184:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0567 (0.0572)
+2022-11-18 15:59:32,253:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0619 (0.0575)
+2022-11-18 15:59:32,331:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0589 (0.0576)
+2022-11-18 15:59:32,403:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0513 (0.0573)
+2022-11-18 15:59:32,447:INFO: - Computing ADE (validation)
+2022-11-18 15:59:32,862:INFO: 		 ADE on hotel                     dataset:	 0.9951646327972412
+2022-11-18 15:59:33,263:INFO: 		 ADE on univ                      dataset:	 1.479106068611145
+2022-11-18 15:59:33,546:INFO: 		 ADE on zara1                     dataset:	 2.4192159175872803
+2022-11-18 15:59:34,050:INFO: 		 ADE on zara2                     dataset:	 1.4028003215789795
+2022-11-18 15:59:34,050:INFO: Average validation:	ADE  1.4793	FDE  2.6882
+2022-11-18 15:59:34,062:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_630.pth.tar
+2022-11-18 15:59:34,062:INFO: 
+===> EPOCH: 631 (P3)
+2022-11-18 15:59:34,063:INFO: - Computing loss (training)
+2022-11-18 15:59:34,318:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0509 (0.0509)
+2022-11-18 15:59:34,387:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0496 (0.0503)
+2022-11-18 15:59:34,455:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0353 (0.0451)
+2022-11-18 15:59:34,505:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0483 (0.0457)
+2022-11-18 15:59:34,913:INFO: Dataset: univ                Batch:  1/15	Loss 0.0335 (0.0335)
+2022-11-18 15:59:34,988:INFO: Dataset: univ                Batch:  2/15	Loss 0.0386 (0.0360)
+2022-11-18 15:59:35,064:INFO: Dataset: univ                Batch:  3/15	Loss 0.0354 (0.0358)
+2022-11-18 15:59:35,139:INFO: Dataset: univ                Batch:  4/15	Loss 0.0354 (0.0357)
+2022-11-18 15:59:35,214:INFO: Dataset: univ                Batch:  5/15	Loss 0.0364 (0.0358)
+2022-11-18 15:59:35,290:INFO: Dataset: univ                Batch:  6/15	Loss 0.0350 (0.0357)
+2022-11-18 15:59:35,365:INFO: Dataset: univ                Batch:  7/15	Loss 0.0354 (0.0356)
+2022-11-18 15:59:35,437:INFO: Dataset: univ                Batch:  8/15	Loss 0.0380 (0.0359)
+2022-11-18 15:59:35,510:INFO: Dataset: univ                Batch:  9/15	Loss 0.0345 (0.0358)
+2022-11-18 15:59:35,584:INFO: Dataset: univ                Batch: 10/15	Loss 0.0381 (0.0360)
+2022-11-18 15:59:35,659:INFO: Dataset: univ                Batch: 11/15	Loss 0.0347 (0.0358)
+2022-11-18 15:59:35,730:INFO: Dataset: univ                Batch: 12/15	Loss 0.0384 (0.0360)
+2022-11-18 15:59:35,804:INFO: Dataset: univ                Batch: 13/15	Loss 0.0334 (0.0358)
+2022-11-18 15:59:35,879:INFO: Dataset: univ                Batch: 14/15	Loss 0.0353 (0.0358)
+2022-11-18 15:59:35,914:INFO: Dataset: univ                Batch: 15/15	Loss 0.0397 (0.0359)
+2022-11-18 15:59:36,218:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0766 (0.0766)
+2022-11-18 15:59:36,286:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0826 (0.0795)
+2022-11-18 15:59:36,356:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0836 (0.0808)
+2022-11-18 15:59:36,421:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0823 (0.0812)
+2022-11-18 15:59:36,488:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0815 (0.0813)
+2022-11-18 15:59:36,554:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0834 (0.0817)
+2022-11-18 15:59:36,620:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0855 (0.0822)
+2022-11-18 15:59:36,680:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0810 (0.0820)
+2022-11-18 15:59:36,990:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0664 (0.0664)
+2022-11-18 15:59:37,061:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0529 (0.0594)
+2022-11-18 15:59:37,133:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0591 (0.0593)
+2022-11-18 15:59:37,201:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0591 (0.0592)
+2022-11-18 15:59:37,270:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0563 (0.0586)
+2022-11-18 15:59:37,341:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0560 (0.0582)
+2022-11-18 15:59:37,411:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0627 (0.0588)
+2022-11-18 15:59:37,481:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0514 (0.0578)
+2022-11-18 15:59:37,550:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0559 (0.0576)
+2022-11-18 15:59:37,620:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0555 (0.0574)
+2022-11-18 15:59:37,693:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0588 (0.0575)
+2022-11-18 15:59:37,762:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0536 (0.0572)
+2022-11-18 15:59:37,832:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0563 (0.0571)
+2022-11-18 15:59:37,903:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0544 (0.0569)
+2022-11-18 15:59:37,973:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0508 (0.0565)
+2022-11-18 15:59:38,043:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0560 (0.0565)
+2022-11-18 15:59:38,114:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0642 (0.0569)
+2022-11-18 15:59:38,178:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0607 (0.0571)
+2022-11-18 15:59:38,224:INFO: - Computing ADE (validation)
+2022-11-18 15:59:38,502:INFO: 		 ADE on hotel                     dataset:	 1.013013482093811
+2022-11-18 15:59:38,906:INFO: 		 ADE on univ                      dataset:	 1.4722537994384766
+2022-11-18 15:59:39,193:INFO: 		 ADE on zara1                     dataset:	 2.411712169647217
+2022-11-18 15:59:39,671:INFO: 		 ADE on zara2                     dataset:	 1.3955237865447998
+2022-11-18 15:59:39,671:INFO: Average validation:	ADE  1.4736	FDE  2.6789
+2022-11-18 15:59:39,683:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_631.pth.tar
+2022-11-18 15:59:39,684:INFO: 
+===> EPOCH: 632 (P3)
+2022-11-18 15:59:39,684:INFO: - Computing loss (training)
+2022-11-18 15:59:39,947:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0541 (0.0541)
+2022-11-18 15:59:40,022:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0451 (0.0496)
+2022-11-18 15:59:40,113:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0374 (0.0455)
+2022-11-18 15:59:40,183:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0466 (0.0457)
+2022-11-18 15:59:40,532:INFO: Dataset: univ                Batch:  1/15	Loss 0.0359 (0.0359)
+2022-11-18 15:59:40,611:INFO: Dataset: univ                Batch:  2/15	Loss 0.0391 (0.0374)
+2022-11-18 15:59:40,691:INFO: Dataset: univ                Batch:  3/15	Loss 0.0368 (0.0373)
+2022-11-18 15:59:40,773:INFO: Dataset: univ                Batch:  4/15	Loss 0.0353 (0.0368)
+2022-11-18 15:59:40,849:INFO: Dataset: univ                Batch:  5/15	Loss 0.0351 (0.0365)
+2022-11-18 15:59:40,928:INFO: Dataset: univ                Batch:  6/15	Loss 0.0361 (0.0364)
+2022-11-18 15:59:41,006:INFO: Dataset: univ                Batch:  7/15	Loss 0.0342 (0.0361)
+2022-11-18 15:59:41,082:INFO: Dataset: univ                Batch:  8/15	Loss 0.0382 (0.0363)
+2022-11-18 15:59:41,158:INFO: Dataset: univ                Batch:  9/15	Loss 0.0370 (0.0364)
+2022-11-18 15:59:41,235:INFO: Dataset: univ                Batch: 10/15	Loss 0.0334 (0.0361)
+2022-11-18 15:59:41,315:INFO: Dataset: univ                Batch: 11/15	Loss 0.0330 (0.0358)
+2022-11-18 15:59:41,389:INFO: Dataset: univ                Batch: 12/15	Loss 0.0362 (0.0358)
+2022-11-18 15:59:41,466:INFO: Dataset: univ                Batch: 13/15	Loss 0.0359 (0.0358)
+2022-11-18 15:59:41,543:INFO: Dataset: univ                Batch: 14/15	Loss 0.0355 (0.0358)
+2022-11-18 15:59:41,576:INFO: Dataset: univ                Batch: 15/15	Loss 0.0347 (0.0358)
+2022-11-18 15:59:41,881:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0826 (0.0826)
+2022-11-18 15:59:41,953:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0854 (0.0840)
+2022-11-18 15:59:42,027:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0813 (0.0831)
+2022-11-18 15:59:42,099:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0752 (0.0811)
+2022-11-18 15:59:42,171:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0745 (0.0797)
+2022-11-18 15:59:42,243:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0829 (0.0804)
+2022-11-18 15:59:42,312:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0879 (0.0815)
+2022-11-18 15:59:42,376:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0825 (0.0816)
+2022-11-18 15:59:42,724:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0522 (0.0522)
+2022-11-18 15:59:42,791:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0611 (0.0567)
+2022-11-18 15:59:42,857:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0599 (0.0578)
+2022-11-18 15:59:42,924:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0506 (0.0561)
+2022-11-18 15:59:42,991:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0552 (0.0559)
+2022-11-18 15:59:43,062:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0620 (0.0569)
+2022-11-18 15:59:43,128:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0648 (0.0582)
+2022-11-18 15:59:43,194:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0610 (0.0585)
+2022-11-18 15:59:43,261:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0553 (0.0582)
+2022-11-18 15:59:43,327:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0511 (0.0574)
+2022-11-18 15:59:43,393:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0563 (0.0573)
+2022-11-18 15:59:43,461:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0544 (0.0570)
+2022-11-18 15:59:43,527:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0564 (0.0570)
+2022-11-18 15:59:43,594:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0530 (0.0567)
+2022-11-18 15:59:43,662:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0575 (0.0567)
+2022-11-18 15:59:43,731:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0552 (0.0566)
+2022-11-18 15:59:43,798:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0611 (0.0569)
+2022-11-18 15:59:43,860:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0566 (0.0569)
+2022-11-18 15:59:43,914:INFO: - Computing ADE (validation)
+2022-11-18 15:59:44,210:INFO: 		 ADE on hotel                     dataset:	 1.021064281463623
+2022-11-18 15:59:44,585:INFO: 		 ADE on univ                      dataset:	 1.4740254878997803
+2022-11-18 15:59:44,882:INFO: 		 ADE on zara1                     dataset:	 2.445814847946167
+2022-11-18 15:59:45,379:INFO: 		 ADE on zara2                     dataset:	 1.402545690536499
+2022-11-18 15:59:45,379:INFO: Average validation:	ADE  1.4795	FDE  2.6882
+2022-11-18 15:59:45,393:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_632.pth.tar
+2022-11-18 15:59:45,393:INFO: 
+===> EPOCH: 633 (P3)
+2022-11-18 15:59:45,394:INFO: - Computing loss (training)
+2022-11-18 15:59:45,661:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0533 (0.0533)
+2022-11-18 15:59:45,733:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0347 (0.0439)
+2022-11-18 15:59:45,804:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0533 (0.0470)
+2022-11-18 15:59:45,857:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0393 (0.0457)
+2022-11-18 15:59:46,261:INFO: Dataset: univ                Batch:  1/15	Loss 0.0377 (0.0377)
+2022-11-18 15:59:46,335:INFO: Dataset: univ                Batch:  2/15	Loss 0.0386 (0.0382)
+2022-11-18 15:59:46,408:INFO: Dataset: univ                Batch:  3/15	Loss 0.0371 (0.0378)
+2022-11-18 15:59:46,480:INFO: Dataset: univ                Batch:  4/15	Loss 0.0342 (0.0370)
+2022-11-18 15:59:46,554:INFO: Dataset: univ                Batch:  5/15	Loss 0.0333 (0.0362)
+2022-11-18 15:59:46,629:INFO: Dataset: univ                Batch:  6/15	Loss 0.0349 (0.0359)
+2022-11-18 15:59:46,701:INFO: Dataset: univ                Batch:  7/15	Loss 0.0355 (0.0359)
+2022-11-18 15:59:46,774:INFO: Dataset: univ                Batch:  8/15	Loss 0.0356 (0.0358)
+2022-11-18 15:59:46,845:INFO: Dataset: univ                Batch:  9/15	Loss 0.0367 (0.0359)
+2022-11-18 15:59:46,917:INFO: Dataset: univ                Batch: 10/15	Loss 0.0345 (0.0358)
+2022-11-18 15:59:46,990:INFO: Dataset: univ                Batch: 11/15	Loss 0.0368 (0.0359)
+2022-11-18 15:59:47,062:INFO: Dataset: univ                Batch: 12/15	Loss 0.0342 (0.0357)
+2022-11-18 15:59:47,135:INFO: Dataset: univ                Batch: 13/15	Loss 0.0350 (0.0357)
+2022-11-18 15:59:47,210:INFO: Dataset: univ                Batch: 14/15	Loss 0.0354 (0.0356)
+2022-11-18 15:59:47,243:INFO: Dataset: univ                Batch: 15/15	Loss 0.0400 (0.0357)
+2022-11-18 15:59:47,548:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0866 (0.0866)
+2022-11-18 15:59:47,618:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0780 (0.0821)
+2022-11-18 15:59:47,683:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0788 (0.0810)
+2022-11-18 15:59:47,749:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0847 (0.0818)
+2022-11-18 15:59:47,816:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0804 (0.0815)
+2022-11-18 15:59:47,886:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0800 (0.0813)
+2022-11-18 15:59:47,952:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0822 (0.0814)
+2022-11-18 15:59:48,013:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0792 (0.0812)
+2022-11-18 15:59:48,330:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0548 (0.0548)
+2022-11-18 15:59:48,399:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0549 (0.0549)
+2022-11-18 15:59:48,469:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0569 (0.0556)
+2022-11-18 15:59:48,539:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0572 (0.0560)
+2022-11-18 15:59:48,611:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0524 (0.0553)
+2022-11-18 15:59:48,683:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0479 (0.0539)
+2022-11-18 15:59:48,754:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0599 (0.0549)
+2022-11-18 15:59:48,824:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0576 (0.0552)
+2022-11-18 15:59:48,893:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0607 (0.0558)
+2022-11-18 15:59:48,964:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0569 (0.0559)
+2022-11-18 15:59:49,034:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0590 (0.0562)
+2022-11-18 15:59:49,103:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0595 (0.0565)
+2022-11-18 15:59:49,169:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0585 (0.0567)
+2022-11-18 15:59:49,239:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0577 (0.0567)
+2022-11-18 15:59:49,308:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0507 (0.0563)
+2022-11-18 15:59:49,378:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0551 (0.0562)
+2022-11-18 15:59:49,446:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0639 (0.0567)
+2022-11-18 15:59:49,508:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0530 (0.0565)
+2022-11-18 15:59:49,555:INFO: - Computing ADE (validation)
+2022-11-18 15:59:49,844:INFO: 		 ADE on hotel                     dataset:	 1.0287175178527832
+2022-11-18 15:59:50,229:INFO: 		 ADE on univ                      dataset:	 1.464651346206665
+2022-11-18 15:59:50,523:INFO: 		 ADE on zara1                     dataset:	 2.400418758392334
+2022-11-18 15:59:51,003:INFO: 		 ADE on zara2                     dataset:	 1.4004604816436768
+2022-11-18 15:59:51,004:INFO: Average validation:	ADE  1.4716	FDE  2.6737
+2022-11-18 15:59:51,017:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_633.pth.tar
+2022-11-18 15:59:51,017:INFO: 
+===> EPOCH: 634 (P3)
+2022-11-18 15:59:51,017:INFO: - Computing loss (training)
+2022-11-18 15:59:51,280:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0465 (0.0465)
+2022-11-18 15:59:51,348:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0529 (0.0497)
+2022-11-18 15:59:51,416:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0422 (0.0472)
+2022-11-18 15:59:51,468:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0390 (0.0458)
+2022-11-18 15:59:51,805:INFO: Dataset: univ                Batch:  1/15	Loss 0.0347 (0.0347)
+2022-11-18 15:59:51,885:INFO: Dataset: univ                Batch:  2/15	Loss 0.0349 (0.0348)
+2022-11-18 15:59:51,962:INFO: Dataset: univ                Batch:  3/15	Loss 0.0374 (0.0355)
+2022-11-18 15:59:52,037:INFO: Dataset: univ                Batch:  4/15	Loss 0.0359 (0.0356)
+2022-11-18 15:59:52,113:INFO: Dataset: univ                Batch:  5/15	Loss 0.0366 (0.0358)
+2022-11-18 15:59:52,189:INFO: Dataset: univ                Batch:  6/15	Loss 0.0345 (0.0356)
+2022-11-18 15:59:52,262:INFO: Dataset: univ                Batch:  7/15	Loss 0.0350 (0.0355)
+2022-11-18 15:59:52,335:INFO: Dataset: univ                Batch:  8/15	Loss 0.0370 (0.0356)
+2022-11-18 15:59:52,409:INFO: Dataset: univ                Batch:  9/15	Loss 0.0332 (0.0354)
+2022-11-18 15:59:52,482:INFO: Dataset: univ                Batch: 10/15	Loss 0.0349 (0.0353)
+2022-11-18 15:59:52,560:INFO: Dataset: univ                Batch: 11/15	Loss 0.0360 (0.0354)
+2022-11-18 15:59:52,635:INFO: Dataset: univ                Batch: 12/15	Loss 0.0368 (0.0355)
+2022-11-18 15:59:52,711:INFO: Dataset: univ                Batch: 13/15	Loss 0.0370 (0.0356)
+2022-11-18 15:59:52,790:INFO: Dataset: univ                Batch: 14/15	Loss 0.0359 (0.0356)
+2022-11-18 15:59:52,823:INFO: Dataset: univ                Batch: 15/15	Loss 0.0363 (0.0356)
+2022-11-18 15:59:53,115:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0813 (0.0813)
+2022-11-18 15:59:53,182:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0810 (0.0812)
+2022-11-18 15:59:53,249:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0850 (0.0823)
+2022-11-18 15:59:53,319:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0883 (0.0838)
+2022-11-18 15:59:53,395:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0734 (0.0815)
+2022-11-18 15:59:53,465:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0779 (0.0809)
+2022-11-18 15:59:53,536:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0836 (0.0813)
+2022-11-18 15:59:53,601:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0762 (0.0807)
+2022-11-18 15:59:53,911:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0554 (0.0554)
+2022-11-18 15:59:53,983:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0553 (0.0553)
+2022-11-18 15:59:54,055:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0572 (0.0560)
+2022-11-18 15:59:54,139:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0505 (0.0546)
+2022-11-18 15:59:54,212:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0561 (0.0549)
+2022-11-18 15:59:54,285:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0609 (0.0560)
+2022-11-18 15:59:54,356:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0564 (0.0561)
+2022-11-18 15:59:54,427:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0531 (0.0557)
+2022-11-18 15:59:54,497:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0634 (0.0566)
+2022-11-18 15:59:54,566:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0554 (0.0565)
+2022-11-18 15:59:54,637:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0527 (0.0562)
+2022-11-18 15:59:54,709:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0555 (0.0561)
+2022-11-18 15:59:54,780:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0511 (0.0557)
+2022-11-18 15:59:54,853:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0619 (0.0561)
+2022-11-18 15:59:54,924:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0557 (0.0561)
+2022-11-18 15:59:54,995:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0594 (0.0563)
+2022-11-18 15:59:55,066:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0574 (0.0563)
+2022-11-18 15:59:55,131:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0550 (0.0563)
+2022-11-18 15:59:55,176:INFO: - Computing ADE (validation)
+2022-11-18 15:59:55,478:INFO: 		 ADE on hotel                     dataset:	 1.0411887168884277
+2022-11-18 15:59:55,851:INFO: 		 ADE on univ                      dataset:	 1.4720932245254517
+2022-11-18 15:59:56,152:INFO: 		 ADE on zara1                     dataset:	 2.3711183071136475
+2022-11-18 15:59:56,645:INFO: 		 ADE on zara2                     dataset:	 1.4132087230682373
+2022-11-18 15:59:56,645:INFO: Average validation:	ADE  1.4792	FDE  2.6858
+2022-11-18 15:59:56,659:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_634.pth.tar
+2022-11-18 15:59:56,659:INFO: 
+===> EPOCH: 635 (P3)
+2022-11-18 15:59:56,660:INFO: - Computing loss (training)
+2022-11-18 15:59:56,913:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0493 (0.0493)
+2022-11-18 15:59:56,981:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0531 (0.0512)
+2022-11-18 15:59:57,051:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0400 (0.0476)
+2022-11-18 15:59:57,101:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0377 (0.0459)
+2022-11-18 15:59:57,415:INFO: Dataset: univ                Batch:  1/15	Loss 0.0324 (0.0324)
+2022-11-18 15:59:57,494:INFO: Dataset: univ                Batch:  2/15	Loss 0.0408 (0.0363)
+2022-11-18 15:59:57,575:INFO: Dataset: univ                Batch:  3/15	Loss 0.0333 (0.0352)
+2022-11-18 15:59:57,659:INFO: Dataset: univ                Batch:  4/15	Loss 0.0344 (0.0350)
+2022-11-18 15:59:57,738:INFO: Dataset: univ                Batch:  5/15	Loss 0.0354 (0.0351)
+2022-11-18 15:59:57,820:INFO: Dataset: univ                Batch:  6/15	Loss 0.0360 (0.0352)
+2022-11-18 15:59:57,901:INFO: Dataset: univ                Batch:  7/15	Loss 0.0313 (0.0346)
+2022-11-18 15:59:57,981:INFO: Dataset: univ                Batch:  8/15	Loss 0.0361 (0.0348)
+2022-11-18 15:59:58,062:INFO: Dataset: univ                Batch:  9/15	Loss 0.0367 (0.0350)
+2022-11-18 15:59:58,145:INFO: Dataset: univ                Batch: 10/15	Loss 0.0360 (0.0351)
+2022-11-18 15:59:58,226:INFO: Dataset: univ                Batch: 11/15	Loss 0.0355 (0.0351)
+2022-11-18 15:59:58,304:INFO: Dataset: univ                Batch: 12/15	Loss 0.0373 (0.0353)
+2022-11-18 15:59:58,382:INFO: Dataset: univ                Batch: 13/15	Loss 0.0361 (0.0354)
+2022-11-18 15:59:58,460:INFO: Dataset: univ                Batch: 14/15	Loss 0.0384 (0.0356)
+2022-11-18 15:59:58,493:INFO: Dataset: univ                Batch: 15/15	Loss 0.0367 (0.0356)
+2022-11-18 15:59:58,806:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0817 (0.0817)
+2022-11-18 15:59:58,878:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0813 (0.0815)
+2022-11-18 15:59:58,948:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0816 (0.0815)
+2022-11-18 15:59:59,020:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0788 (0.0809)
+2022-11-18 15:59:59,092:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0834 (0.0814)
+2022-11-18 15:59:59,174:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0779 (0.0808)
+2022-11-18 15:59:59,244:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0766 (0.0802)
+2022-11-18 15:59:59,307:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0796 (0.0801)
+2022-11-18 15:59:59,616:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0505 (0.0505)
+2022-11-18 15:59:59,686:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0544 (0.0525)
+2022-11-18 15:59:59,757:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0492 (0.0514)
+2022-11-18 15:59:59,827:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0567 (0.0528)
+2022-11-18 15:59:59,895:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0579 (0.0538)
+2022-11-18 15:59:59,966:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0570 (0.0543)
+2022-11-18 16:00:00,032:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0561 (0.0546)
+2022-11-18 16:00:00,099:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0496 (0.0540)
+2022-11-18 16:00:00,165:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0622 (0.0549)
+2022-11-18 16:00:00,232:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0568 (0.0550)
+2022-11-18 16:00:00,299:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0518 (0.0547)
+2022-11-18 16:00:00,367:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0569 (0.0549)
+2022-11-18 16:00:00,435:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0533 (0.0548)
+2022-11-18 16:00:00,503:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0624 (0.0553)
+2022-11-18 16:00:00,571:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0541 (0.0553)
+2022-11-18 16:00:00,638:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0653 (0.0559)
+2022-11-18 16:00:00,706:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0592 (0.0561)
+2022-11-18 16:00:00,767:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0549 (0.0560)
+2022-11-18 16:00:00,809:INFO: - Computing ADE (validation)
+2022-11-18 16:00:01,086:INFO: 		 ADE on hotel                     dataset:	 1.055789589881897
+2022-11-18 16:00:01,459:INFO: 		 ADE on univ                      dataset:	 1.4716765880584717
+2022-11-18 16:00:01,750:INFO: 		 ADE on zara1                     dataset:	 2.3770318031311035
+2022-11-18 16:00:02,246:INFO: 		 ADE on zara2                     dataset:	 1.412617564201355
+2022-11-18 16:00:02,246:INFO: Average validation:	ADE  1.4799	FDE  2.6873
+2022-11-18 16:00:02,259:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_635.pth.tar
+2022-11-18 16:00:02,259:INFO: 
+===> EPOCH: 636 (P3)
+2022-11-18 16:00:02,260:INFO: - Computing loss (training)
+2022-11-18 16:00:02,532:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0536 (0.0536)
+2022-11-18 16:00:02,607:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0407 (0.0471)
+2022-11-18 16:00:02,679:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0470 (0.0471)
+2022-11-18 16:00:02,730:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0419 (0.0460)
+2022-11-18 16:00:03,049:INFO: Dataset: univ                Batch:  1/15	Loss 0.0355 (0.0355)
+2022-11-18 16:00:03,126:INFO: Dataset: univ                Batch:  2/15	Loss 0.0335 (0.0345)
+2022-11-18 16:00:03,207:INFO: Dataset: univ                Batch:  3/15	Loss 0.0354 (0.0348)
+2022-11-18 16:00:03,283:INFO: Dataset: univ                Batch:  4/15	Loss 0.0361 (0.0351)
+2022-11-18 16:00:03,356:INFO: Dataset: univ                Batch:  5/15	Loss 0.0349 (0.0350)
+2022-11-18 16:00:03,432:INFO: Dataset: univ                Batch:  6/15	Loss 0.0359 (0.0352)
+2022-11-18 16:00:03,505:INFO: Dataset: univ                Batch:  7/15	Loss 0.0355 (0.0352)
+2022-11-18 16:00:03,578:INFO: Dataset: univ                Batch:  8/15	Loss 0.0342 (0.0351)
+2022-11-18 16:00:03,651:INFO: Dataset: univ                Batch:  9/15	Loss 0.0355 (0.0352)
+2022-11-18 16:00:03,726:INFO: Dataset: univ                Batch: 10/15	Loss 0.0367 (0.0353)
+2022-11-18 16:00:03,803:INFO: Dataset: univ                Batch: 11/15	Loss 0.0375 (0.0355)
+2022-11-18 16:00:03,875:INFO: Dataset: univ                Batch: 12/15	Loss 0.0361 (0.0356)
+2022-11-18 16:00:03,948:INFO: Dataset: univ                Batch: 13/15	Loss 0.0345 (0.0355)
+2022-11-18 16:00:04,024:INFO: Dataset: univ                Batch: 14/15	Loss 0.0358 (0.0355)
+2022-11-18 16:00:04,058:INFO: Dataset: univ                Batch: 15/15	Loss 0.0344 (0.0355)
+2022-11-18 16:00:04,366:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0781 (0.0781)
+2022-11-18 16:00:04,435:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0712 (0.0747)
+2022-11-18 16:00:04,504:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0803 (0.0766)
+2022-11-18 16:00:04,574:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0826 (0.0780)
+2022-11-18 16:00:04,642:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0796 (0.0782)
+2022-11-18 16:00:04,710:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0808 (0.0787)
+2022-11-18 16:00:04,777:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0862 (0.0796)
+2022-11-18 16:00:04,838:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0804 (0.0797)
+2022-11-18 16:00:05,154:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0565 (0.0565)
+2022-11-18 16:00:05,236:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0604 (0.0584)
+2022-11-18 16:00:05,307:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0586 (0.0584)
+2022-11-18 16:00:05,377:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0566 (0.0580)
+2022-11-18 16:00:05,448:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0489 (0.0561)
+2022-11-18 16:00:05,520:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0536 (0.0556)
+2022-11-18 16:00:05,588:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0503 (0.0548)
+2022-11-18 16:00:05,657:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0527 (0.0546)
+2022-11-18 16:00:05,727:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0554 (0.0547)
+2022-11-18 16:00:05,795:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0596 (0.0551)
+2022-11-18 16:00:05,866:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0589 (0.0555)
+2022-11-18 16:00:05,935:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0510 (0.0551)
+2022-11-18 16:00:06,002:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0564 (0.0552)
+2022-11-18 16:00:06,073:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0521 (0.0549)
+2022-11-18 16:00:06,144:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0585 (0.0551)
+2022-11-18 16:00:06,214:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0542 (0.0551)
+2022-11-18 16:00:06,286:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0560 (0.0551)
+2022-11-18 16:00:06,350:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0672 (0.0557)
+2022-11-18 16:00:06,397:INFO: - Computing ADE (validation)
+2022-11-18 16:00:06,693:INFO: 		 ADE on hotel                     dataset:	 1.072648525238037
+2022-11-18 16:00:07,061:INFO: 		 ADE on univ                      dataset:	 1.4720102548599243
+2022-11-18 16:00:07,355:INFO: 		 ADE on zara1                     dataset:	 2.385117530822754
+2022-11-18 16:00:07,830:INFO: 		 ADE on zara2                     dataset:	 1.4247888326644897
+2022-11-18 16:00:07,831:INFO: Average validation:	ADE  1.4859	FDE  2.6951
+2022-11-18 16:00:07,843:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_636.pth.tar
+2022-11-18 16:00:07,843:INFO: 
+===> EPOCH: 637 (P3)
+2022-11-18 16:00:07,844:INFO: - Computing loss (training)
+2022-11-18 16:00:08,100:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0430 (0.0430)
+2022-11-18 16:00:08,169:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0495 (0.0462)
+2022-11-18 16:00:08,236:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0460 (0.0461)
+2022-11-18 16:00:08,286:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0461 (0.0461)
+2022-11-18 16:00:08,605:INFO: Dataset: univ                Batch:  1/15	Loss 0.0349 (0.0349)
+2022-11-18 16:00:08,683:INFO: Dataset: univ                Batch:  2/15	Loss 0.0372 (0.0360)
+2022-11-18 16:00:08,759:INFO: Dataset: univ                Batch:  3/15	Loss 0.0355 (0.0359)
+2022-11-18 16:00:08,837:INFO: Dataset: univ                Batch:  4/15	Loss 0.0368 (0.0361)
+2022-11-18 16:00:08,915:INFO: Dataset: univ                Batch:  5/15	Loss 0.0344 (0.0358)
+2022-11-18 16:00:08,990:INFO: Dataset: univ                Batch:  6/15	Loss 0.0357 (0.0358)
+2022-11-18 16:00:09,065:INFO: Dataset: univ                Batch:  7/15	Loss 0.0357 (0.0358)
+2022-11-18 16:00:09,138:INFO: Dataset: univ                Batch:  8/15	Loss 0.0359 (0.0358)
+2022-11-18 16:00:09,210:INFO: Dataset: univ                Batch:  9/15	Loss 0.0332 (0.0355)
+2022-11-18 16:00:09,284:INFO: Dataset: univ                Batch: 10/15	Loss 0.0375 (0.0357)
+2022-11-18 16:00:09,358:INFO: Dataset: univ                Batch: 11/15	Loss 0.0348 (0.0356)
+2022-11-18 16:00:09,432:INFO: Dataset: univ                Batch: 12/15	Loss 0.0363 (0.0357)
+2022-11-18 16:00:09,505:INFO: Dataset: univ                Batch: 13/15	Loss 0.0336 (0.0355)
+2022-11-18 16:00:09,579:INFO: Dataset: univ                Batch: 14/15	Loss 0.0347 (0.0354)
+2022-11-18 16:00:09,612:INFO: Dataset: univ                Batch: 15/15	Loss 0.0322 (0.0354)
+2022-11-18 16:00:09,934:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0836 (0.0836)
+2022-11-18 16:00:10,013:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0791 (0.0812)
+2022-11-18 16:00:10,084:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0846 (0.0823)
+2022-11-18 16:00:10,156:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0777 (0.0812)
+2022-11-18 16:00:10,233:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0762 (0.0803)
+2022-11-18 16:00:10,337:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0862 (0.0812)
+2022-11-18 16:00:10,435:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0755 (0.0804)
+2022-11-18 16:00:10,530:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0714 (0.0793)
+2022-11-18 16:00:10,947:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0563 (0.0563)
+2022-11-18 16:00:11,045:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0598 (0.0580)
+2022-11-18 16:00:11,136:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0542 (0.0568)
+2022-11-18 16:00:11,240:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0530 (0.0559)
+2022-11-18 16:00:11,355:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0570 (0.0561)
+2022-11-18 16:00:11,439:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0479 (0.0547)
+2022-11-18 16:00:11,508:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0637 (0.0559)
+2022-11-18 16:00:11,576:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0578 (0.0562)
+2022-11-18 16:00:11,644:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0484 (0.0553)
+2022-11-18 16:00:11,711:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0533 (0.0551)
+2022-11-18 16:00:11,778:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0546 (0.0550)
+2022-11-18 16:00:11,845:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0610 (0.0555)
+2022-11-18 16:00:11,911:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0490 (0.0550)
+2022-11-18 16:00:11,978:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0531 (0.0549)
+2022-11-18 16:00:12,045:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0586 (0.0551)
+2022-11-18 16:00:12,111:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0581 (0.0553)
+2022-11-18 16:00:12,182:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0547 (0.0552)
+2022-11-18 16:00:12,248:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0580 (0.0554)
+2022-11-18 16:00:12,298:INFO: - Computing ADE (validation)
+2022-11-18 16:00:12,612:INFO: 		 ADE on hotel                     dataset:	 1.0767537355422974
+2022-11-18 16:00:12,984:INFO: 		 ADE on univ                      dataset:	 1.4674192667007446
+2022-11-18 16:00:13,274:INFO: 		 ADE on zara1                     dataset:	 2.358257293701172
+2022-11-18 16:00:13,736:INFO: 		 ADE on zara2                     dataset:	 1.406638741493225
+2022-11-18 16:00:13,736:INFO: Average validation:	ADE  1.4755	FDE  2.6803
+2022-11-18 16:00:13,749:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_637.pth.tar
+2022-11-18 16:00:13,749:INFO: 
+===> EPOCH: 638 (P3)
+2022-11-18 16:00:13,750:INFO: - Computing loss (training)
+2022-11-18 16:00:14,010:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0494 (0.0494)
+2022-11-18 16:00:14,080:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0466 (0.0480)
+2022-11-18 16:00:14,148:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0473 (0.0478)
+2022-11-18 16:00:14,200:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0388 (0.0463)
+2022-11-18 16:00:14,542:INFO: Dataset: univ                Batch:  1/15	Loss 0.0343 (0.0343)
+2022-11-18 16:00:14,618:INFO: Dataset: univ                Batch:  2/15	Loss 0.0338 (0.0341)
+2022-11-18 16:00:14,693:INFO: Dataset: univ                Batch:  3/15	Loss 0.0344 (0.0342)
+2022-11-18 16:00:14,776:INFO: Dataset: univ                Batch:  4/15	Loss 0.0351 (0.0344)
+2022-11-18 16:00:14,851:INFO: Dataset: univ                Batch:  5/15	Loss 0.0387 (0.0352)
+2022-11-18 16:00:14,928:INFO: Dataset: univ                Batch:  6/15	Loss 0.0369 (0.0355)
+2022-11-18 16:00:15,003:INFO: Dataset: univ                Batch:  7/15	Loss 0.0347 (0.0354)
+2022-11-18 16:00:15,076:INFO: Dataset: univ                Batch:  8/15	Loss 0.0350 (0.0353)
+2022-11-18 16:00:15,149:INFO: Dataset: univ                Batch:  9/15	Loss 0.0340 (0.0352)
+2022-11-18 16:00:15,223:INFO: Dataset: univ                Batch: 10/15	Loss 0.0355 (0.0352)
+2022-11-18 16:00:15,298:INFO: Dataset: univ                Batch: 11/15	Loss 0.0357 (0.0353)
+2022-11-18 16:00:15,373:INFO: Dataset: univ                Batch: 12/15	Loss 0.0359 (0.0353)
+2022-11-18 16:00:15,446:INFO: Dataset: univ                Batch: 13/15	Loss 0.0376 (0.0355)
+2022-11-18 16:00:15,521:INFO: Dataset: univ                Batch: 14/15	Loss 0.0339 (0.0354)
+2022-11-18 16:00:15,555:INFO: Dataset: univ                Batch: 15/15	Loss 0.0322 (0.0353)
+2022-11-18 16:00:15,901:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0824 (0.0824)
+2022-11-18 16:00:15,975:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0824 (0.0824)
+2022-11-18 16:00:16,048:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0760 (0.0804)
+2022-11-18 16:00:16,125:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0743 (0.0788)
+2022-11-18 16:00:16,198:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0790 (0.0788)
+2022-11-18 16:00:16,269:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0773 (0.0786)
+2022-11-18 16:00:16,340:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0808 (0.0789)
+2022-11-18 16:00:16,406:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0778 (0.0788)
+2022-11-18 16:00:16,760:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0577 (0.0577)
+2022-11-18 16:00:16,834:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0561 (0.0568)
+2022-11-18 16:00:16,908:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0535 (0.0558)
+2022-11-18 16:00:17,009:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0548 (0.0555)
+2022-11-18 16:00:17,099:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0564 (0.0557)
+2022-11-18 16:00:17,176:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0450 (0.0538)
+2022-11-18 16:00:17,250:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0547 (0.0539)
+2022-11-18 16:00:17,338:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0564 (0.0542)
+2022-11-18 16:00:17,493:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0516 (0.0539)
+2022-11-18 16:00:17,612:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0530 (0.0538)
+2022-11-18 16:00:17,725:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0618 (0.0545)
+2022-11-18 16:00:17,801:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0486 (0.0540)
+2022-11-18 16:00:17,871:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0506 (0.0537)
+2022-11-18 16:00:17,946:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0553 (0.0538)
+2022-11-18 16:00:18,015:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0657 (0.0546)
+2022-11-18 16:00:18,082:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0565 (0.0547)
+2022-11-18 16:00:18,151:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0535 (0.0547)
+2022-11-18 16:00:18,214:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0605 (0.0549)
+2022-11-18 16:00:18,257:INFO: - Computing ADE (validation)
+2022-11-18 16:00:18,549:INFO: 		 ADE on hotel                     dataset:	 1.080167531967163
+2022-11-18 16:00:18,933:INFO: 		 ADE on univ                      dataset:	 1.46427321434021
+2022-11-18 16:00:19,219:INFO: 		 ADE on zara1                     dataset:	 2.3726894855499268
+2022-11-18 16:00:19,683:INFO: 		 ADE on zara2                     dataset:	 1.4206653833389282
+2022-11-18 16:00:19,683:INFO: Average validation:	ADE  1.4801	FDE  2.6861
+2022-11-18 16:00:19,695:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_638.pth.tar
+2022-11-18 16:00:19,695:INFO: 
+===> EPOCH: 639 (P3)
+2022-11-18 16:00:19,696:INFO: - Computing loss (training)
+2022-11-18 16:00:19,956:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0453 (0.0453)
+2022-11-18 16:00:20,035:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0510 (0.0481)
+2022-11-18 16:00:20,102:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0516 (0.0493)
+2022-11-18 16:00:20,152:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0324 (0.0463)
+2022-11-18 16:00:20,484:INFO: Dataset: univ                Batch:  1/15	Loss 0.0330 (0.0330)
+2022-11-18 16:00:20,571:INFO: Dataset: univ                Batch:  2/15	Loss 0.0345 (0.0337)
+2022-11-18 16:00:20,650:INFO: Dataset: univ                Batch:  3/15	Loss 0.0347 (0.0340)
+2022-11-18 16:00:20,729:INFO: Dataset: univ                Batch:  4/15	Loss 0.0374 (0.0347)
+2022-11-18 16:00:20,807:INFO: Dataset: univ                Batch:  5/15	Loss 0.0357 (0.0349)
+2022-11-18 16:00:20,886:INFO: Dataset: univ                Batch:  6/15	Loss 0.0371 (0.0352)
+2022-11-18 16:00:20,961:INFO: Dataset: univ                Batch:  7/15	Loss 0.0362 (0.0354)
+2022-11-18 16:00:21,038:INFO: Dataset: univ                Batch:  8/15	Loss 0.0350 (0.0353)
+2022-11-18 16:00:21,113:INFO: Dataset: univ                Batch:  9/15	Loss 0.0385 (0.0356)
+2022-11-18 16:00:21,188:INFO: Dataset: univ                Batch: 10/15	Loss 0.0338 (0.0355)
+2022-11-18 16:00:21,267:INFO: Dataset: univ                Batch: 11/15	Loss 0.0351 (0.0354)
+2022-11-18 16:00:21,344:INFO: Dataset: univ                Batch: 12/15	Loss 0.0360 (0.0355)
+2022-11-18 16:00:21,421:INFO: Dataset: univ                Batch: 13/15	Loss 0.0356 (0.0355)
+2022-11-18 16:00:21,500:INFO: Dataset: univ                Batch: 14/15	Loss 0.0325 (0.0352)
+2022-11-18 16:00:21,532:INFO: Dataset: univ                Batch: 15/15	Loss 0.0332 (0.0352)
+2022-11-18 16:00:21,836:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0761 (0.0761)
+2022-11-18 16:00:21,905:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0852 (0.0806)
+2022-11-18 16:00:21,971:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0765 (0.0791)
+2022-11-18 16:00:22,040:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0774 (0.0786)
+2022-11-18 16:00:22,109:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0805 (0.0790)
+2022-11-18 16:00:22,178:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0777 (0.0787)
+2022-11-18 16:00:22,245:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0776 (0.0786)
+2022-11-18 16:00:22,307:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0745 (0.0782)
+2022-11-18 16:00:22,604:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0540 (0.0540)
+2022-11-18 16:00:22,673:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0613 (0.0576)
+2022-11-18 16:00:22,744:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0570 (0.0574)
+2022-11-18 16:00:22,821:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0530 (0.0563)
+2022-11-18 16:00:22,907:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0554 (0.0561)
+2022-11-18 16:00:22,981:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0537 (0.0557)
+2022-11-18 16:00:23,048:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0500 (0.0548)
+2022-11-18 16:00:23,115:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0507 (0.0544)
+2022-11-18 16:00:23,182:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0515 (0.0540)
+2022-11-18 16:00:23,251:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0549 (0.0541)
+2022-11-18 16:00:23,319:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0512 (0.0539)
+2022-11-18 16:00:23,387:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0645 (0.0547)
+2022-11-18 16:00:23,453:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0505 (0.0543)
+2022-11-18 16:00:23,521:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0528 (0.0542)
+2022-11-18 16:00:23,589:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0530 (0.0541)
+2022-11-18 16:00:23,657:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0572 (0.0543)
+2022-11-18 16:00:23,725:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0567 (0.0545)
+2022-11-18 16:00:23,787:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0577 (0.0546)
+2022-11-18 16:00:23,829:INFO: - Computing ADE (validation)
+2022-11-18 16:00:24,114:INFO: 		 ADE on hotel                     dataset:	 1.0713261365890503
+2022-11-18 16:00:24,491:INFO: 		 ADE on univ                      dataset:	 1.468877911567688
+2022-11-18 16:00:24,789:INFO: 		 ADE on zara1                     dataset:	 2.358736753463745
+2022-11-18 16:00:25,271:INFO: 		 ADE on zara2                     dataset:	 1.4184043407440186
+2022-11-18 16:00:25,272:INFO: Average validation:	ADE  1.4803	FDE  2.6857
+2022-11-18 16:00:25,285:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_639.pth.tar
+2022-11-18 16:00:25,285:INFO: 
+===> EPOCH: 640 (P3)
+2022-11-18 16:00:25,285:INFO: - Computing loss (training)
+2022-11-18 16:00:25,554:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0517 (0.0517)
+2022-11-18 16:00:25,625:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0426 (0.0471)
+2022-11-18 16:00:25,695:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0504 (0.0482)
+2022-11-18 16:00:25,750:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0379 (0.0464)
+2022-11-18 16:00:26,086:INFO: Dataset: univ                Batch:  1/15	Loss 0.0348 (0.0348)
+2022-11-18 16:00:26,168:INFO: Dataset: univ                Batch:  2/15	Loss 0.0327 (0.0338)
+2022-11-18 16:00:26,246:INFO: Dataset: univ                Batch:  3/15	Loss 0.0375 (0.0350)
+2022-11-18 16:00:26,329:INFO: Dataset: univ                Batch:  4/15	Loss 0.0325 (0.0343)
+2022-11-18 16:00:26,413:INFO: Dataset: univ                Batch:  5/15	Loss 0.0358 (0.0346)
+2022-11-18 16:00:26,491:INFO: Dataset: univ                Batch:  6/15	Loss 0.0350 (0.0347)
+2022-11-18 16:00:26,569:INFO: Dataset: univ                Batch:  7/15	Loss 0.0354 (0.0348)
+2022-11-18 16:00:26,646:INFO: Dataset: univ                Batch:  8/15	Loss 0.0362 (0.0349)
+2022-11-18 16:00:26,722:INFO: Dataset: univ                Batch:  9/15	Loss 0.0343 (0.0349)
+2022-11-18 16:00:26,799:INFO: Dataset: univ                Batch: 10/15	Loss 0.0343 (0.0348)
+2022-11-18 16:00:26,876:INFO: Dataset: univ                Batch: 11/15	Loss 0.0361 (0.0349)
+2022-11-18 16:00:26,954:INFO: Dataset: univ                Batch: 12/15	Loss 0.0362 (0.0350)
+2022-11-18 16:00:27,032:INFO: Dataset: univ                Batch: 13/15	Loss 0.0370 (0.0352)
+2022-11-18 16:00:27,111:INFO: Dataset: univ                Batch: 14/15	Loss 0.0337 (0.0351)
+2022-11-18 16:00:27,145:INFO: Dataset: univ                Batch: 15/15	Loss 0.0389 (0.0351)
+2022-11-18 16:00:27,466:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0771 (0.0771)
+2022-11-18 16:00:27,533:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0747 (0.0757)
+2022-11-18 16:00:27,601:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0797 (0.0770)
+2022-11-18 16:00:27,669:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0774 (0.0771)
+2022-11-18 16:00:27,739:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0774 (0.0772)
+2022-11-18 16:00:27,809:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0778 (0.0773)
+2022-11-18 16:00:27,876:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0746 (0.0769)
+2022-11-18 16:00:27,939:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0803 (0.0773)
+2022-11-18 16:00:28,257:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0526 (0.0526)
+2022-11-18 16:00:28,333:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0567 (0.0546)
+2022-11-18 16:00:28,405:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0557 (0.0550)
+2022-11-18 16:00:28,482:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0547 (0.0549)
+2022-11-18 16:00:28,552:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0515 (0.0542)
+2022-11-18 16:00:28,626:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0495 (0.0534)
+2022-11-18 16:00:28,696:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0541 (0.0535)
+2022-11-18 16:00:28,768:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0555 (0.0538)
+2022-11-18 16:00:28,839:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0631 (0.0548)
+2022-11-18 16:00:28,911:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0567 (0.0549)
+2022-11-18 16:00:28,984:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0516 (0.0546)
+2022-11-18 16:00:29,056:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0519 (0.0544)
+2022-11-18 16:00:29,125:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0552 (0.0545)
+2022-11-18 16:00:29,198:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0545 (0.0545)
+2022-11-18 16:00:29,270:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0471 (0.0540)
+2022-11-18 16:00:29,342:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0531 (0.0540)
+2022-11-18 16:00:29,417:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0563 (0.0541)
+2022-11-18 16:00:29,484:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0566 (0.0542)
+2022-11-18 16:00:29,533:INFO: - Computing ADE (validation)
+2022-11-18 16:00:29,826:INFO: 		 ADE on hotel                     dataset:	 1.118809461593628
+2022-11-18 16:00:30,200:INFO: 		 ADE on univ                      dataset:	 1.4698125123977661
+2022-11-18 16:00:30,506:INFO: 		 ADE on zara1                     dataset:	 2.350761651992798
+2022-11-18 16:00:31,001:INFO: 		 ADE on zara2                     dataset:	 1.4216188192367554
+2022-11-18 16:00:31,002:INFO: Average validation:	ADE  1.4841	FDE  2.6940
+2022-11-18 16:00:31,016:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_640.pth.tar
+2022-11-18 16:00:31,016:INFO: 
+===> EPOCH: 641 (P3)
+2022-11-18 16:00:31,017:INFO: - Computing loss (training)
+2022-11-18 16:00:31,282:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0496 (0.0496)
+2022-11-18 16:00:31,349:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0489 (0.0493)
+2022-11-18 16:00:31,416:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0430 (0.0473)
+2022-11-18 16:00:31,466:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0429 (0.0465)
+2022-11-18 16:00:31,785:INFO: Dataset: univ                Batch:  1/15	Loss 0.0387 (0.0387)
+2022-11-18 16:00:31,860:INFO: Dataset: univ                Batch:  2/15	Loss 0.0342 (0.0364)
+2022-11-18 16:00:31,938:INFO: Dataset: univ                Batch:  3/15	Loss 0.0322 (0.0350)
+2022-11-18 16:00:32,011:INFO: Dataset: univ                Batch:  4/15	Loss 0.0351 (0.0350)
+2022-11-18 16:00:32,085:INFO: Dataset: univ                Batch:  5/15	Loss 0.0357 (0.0352)
+2022-11-18 16:00:32,159:INFO: Dataset: univ                Batch:  6/15	Loss 0.0368 (0.0354)
+2022-11-18 16:00:32,232:INFO: Dataset: univ                Batch:  7/15	Loss 0.0334 (0.0352)
+2022-11-18 16:00:32,305:INFO: Dataset: univ                Batch:  8/15	Loss 0.0348 (0.0351)
+2022-11-18 16:00:32,378:INFO: Dataset: univ                Batch:  9/15	Loss 0.0364 (0.0353)
+2022-11-18 16:00:32,450:INFO: Dataset: univ                Batch: 10/15	Loss 0.0347 (0.0352)
+2022-11-18 16:00:32,526:INFO: Dataset: univ                Batch: 11/15	Loss 0.0337 (0.0351)
+2022-11-18 16:00:32,598:INFO: Dataset: univ                Batch: 12/15	Loss 0.0367 (0.0352)
+2022-11-18 16:00:32,672:INFO: Dataset: univ                Batch: 13/15	Loss 0.0353 (0.0352)
+2022-11-18 16:00:32,749:INFO: Dataset: univ                Batch: 14/15	Loss 0.0328 (0.0350)
+2022-11-18 16:00:32,784:INFO: Dataset: univ                Batch: 15/15	Loss 0.0332 (0.0350)
+2022-11-18 16:00:33,108:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0756 (0.0756)
+2022-11-18 16:00:33,179:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0754 (0.0755)
+2022-11-18 16:00:33,247:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0724 (0.0744)
+2022-11-18 16:00:33,316:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0762 (0.0749)
+2022-11-18 16:00:33,386:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0812 (0.0762)
+2022-11-18 16:00:33,457:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0768 (0.0763)
+2022-11-18 16:00:33,525:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0781 (0.0765)
+2022-11-18 16:00:33,588:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0787 (0.0768)
+2022-11-18 16:00:33,899:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0509 (0.0509)
+2022-11-18 16:00:33,968:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0515 (0.0512)
+2022-11-18 16:00:34,039:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0579 (0.0534)
+2022-11-18 16:00:34,107:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0618 (0.0552)
+2022-11-18 16:00:34,179:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0550 (0.0552)
+2022-11-18 16:00:34,248:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0607 (0.0561)
+2022-11-18 16:00:34,315:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0500 (0.0552)
+2022-11-18 16:00:34,383:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0488 (0.0545)
+2022-11-18 16:00:34,450:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0502 (0.0540)
+2022-11-18 16:00:34,517:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0562 (0.0542)
+2022-11-18 16:00:34,587:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0508 (0.0539)
+2022-11-18 16:00:34,654:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0553 (0.0540)
+2022-11-18 16:00:34,721:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0576 (0.0543)
+2022-11-18 16:00:34,792:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0474 (0.0538)
+2022-11-18 16:00:34,860:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0549 (0.0539)
+2022-11-18 16:00:34,928:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0571 (0.0541)
+2022-11-18 16:00:34,997:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0493 (0.0538)
+2022-11-18 16:00:35,059:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0547 (0.0538)
+2022-11-18 16:00:35,112:INFO: - Computing ADE (validation)
+2022-11-18 16:00:35,387:INFO: 		 ADE on hotel                     dataset:	 1.0927447080612183
+2022-11-18 16:00:35,777:INFO: 		 ADE on univ                      dataset:	 1.4737772941589355
+2022-11-18 16:00:36,071:INFO: 		 ADE on zara1                     dataset:	 2.31978178024292
+2022-11-18 16:00:36,559:INFO: 		 ADE on zara2                     dataset:	 1.4067018032073975
+2022-11-18 16:00:36,559:INFO: Average validation:	ADE  1.4775	FDE  2.6832
+2022-11-18 16:00:36,573:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_641.pth.tar
+2022-11-18 16:00:36,573:INFO: 
+===> EPOCH: 642 (P3)
+2022-11-18 16:00:36,574:INFO: - Computing loss (training)
+2022-11-18 16:00:36,849:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0437 (0.0437)
+2022-11-18 16:00:36,926:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0472 (0.0454)
+2022-11-18 16:00:36,997:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0493 (0.0468)
+2022-11-18 16:00:37,051:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0456 (0.0466)
+2022-11-18 16:00:37,364:INFO: Dataset: univ                Batch:  1/15	Loss 0.0330 (0.0330)
+2022-11-18 16:00:37,447:INFO: Dataset: univ                Batch:  2/15	Loss 0.0362 (0.0347)
+2022-11-18 16:00:37,530:INFO: Dataset: univ                Batch:  3/15	Loss 0.0331 (0.0342)
+2022-11-18 16:00:37,611:INFO: Dataset: univ                Batch:  4/15	Loss 0.0344 (0.0342)
+2022-11-18 16:00:37,692:INFO: Dataset: univ                Batch:  5/15	Loss 0.0351 (0.0344)
+2022-11-18 16:00:37,769:INFO: Dataset: univ                Batch:  6/15	Loss 0.0341 (0.0343)
+2022-11-18 16:00:37,847:INFO: Dataset: univ                Batch:  7/15	Loss 0.0346 (0.0344)
+2022-11-18 16:00:37,922:INFO: Dataset: univ                Batch:  8/15	Loss 0.0353 (0.0345)
+2022-11-18 16:00:37,998:INFO: Dataset: univ                Batch:  9/15	Loss 0.0337 (0.0344)
+2022-11-18 16:00:38,073:INFO: Dataset: univ                Batch: 10/15	Loss 0.0377 (0.0347)
+2022-11-18 16:00:38,151:INFO: Dataset: univ                Batch: 11/15	Loss 0.0347 (0.0347)
+2022-11-18 16:00:38,226:INFO: Dataset: univ                Batch: 12/15	Loss 0.0363 (0.0348)
+2022-11-18 16:00:38,304:INFO: Dataset: univ                Batch: 13/15	Loss 0.0369 (0.0350)
+2022-11-18 16:00:38,382:INFO: Dataset: univ                Batch: 14/15	Loss 0.0323 (0.0348)
+2022-11-18 16:00:38,416:INFO: Dataset: univ                Batch: 15/15	Loss 0.0381 (0.0348)
+2022-11-18 16:00:38,726:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0781 (0.0781)
+2022-11-18 16:00:38,793:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0750 (0.0766)
+2022-11-18 16:00:38,862:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0738 (0.0757)
+2022-11-18 16:00:38,927:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0750 (0.0756)
+2022-11-18 16:00:38,994:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0726 (0.0749)
+2022-11-18 16:00:39,064:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0777 (0.0754)
+2022-11-18 16:00:39,130:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0822 (0.0763)
+2022-11-18 16:00:39,191:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0703 (0.0756)
+2022-11-18 16:00:39,485:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0486 (0.0486)
+2022-11-18 16:00:39,554:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0582 (0.0537)
+2022-11-18 16:00:39,623:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0580 (0.0553)
+2022-11-18 16:00:39,695:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0556 (0.0554)
+2022-11-18 16:00:39,762:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0480 (0.0538)
+2022-11-18 16:00:39,832:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0484 (0.0528)
+2022-11-18 16:00:39,899:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0492 (0.0524)
+2022-11-18 16:00:39,966:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0596 (0.0533)
+2022-11-18 16:00:40,032:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0524 (0.0532)
+2022-11-18 16:00:40,099:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0572 (0.0537)
+2022-11-18 16:00:40,166:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0514 (0.0535)
+2022-11-18 16:00:40,234:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0492 (0.0531)
+2022-11-18 16:00:40,301:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0585 (0.0535)
+2022-11-18 16:00:40,370:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0548 (0.0536)
+2022-11-18 16:00:40,437:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0510 (0.0534)
+2022-11-18 16:00:40,505:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0573 (0.0536)
+2022-11-18 16:00:40,572:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0496 (0.0534)
+2022-11-18 16:00:40,635:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0486 (0.0532)
+2022-11-18 16:00:40,680:INFO: - Computing ADE (validation)
+2022-11-18 16:00:40,966:INFO: 		 ADE on hotel                     dataset:	 1.0982071161270142
+2022-11-18 16:00:41,339:INFO: 		 ADE on univ                      dataset:	 1.467151165008545
+2022-11-18 16:00:41,639:INFO: 		 ADE on zara1                     dataset:	 2.263152837753296
+2022-11-18 16:00:42,135:INFO: 		 ADE on zara2                     dataset:	 1.408795714378357
+2022-11-18 16:00:42,135:INFO: Average validation:	ADE  1.4718	FDE  2.6718
+2022-11-18 16:00:42,148:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_642.pth.tar
+2022-11-18 16:00:42,148:INFO: 
+===> EPOCH: 643 (P3)
+2022-11-18 16:00:42,149:INFO: - Computing loss (training)
+2022-11-18 16:00:42,407:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0443 (0.0443)
+2022-11-18 16:00:42,476:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0557 (0.0500)
+2022-11-18 16:00:42,542:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0379 (0.0458)
+2022-11-18 16:00:42,592:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0512 (0.0468)
+2022-11-18 16:00:42,916:INFO: Dataset: univ                Batch:  1/15	Loss 0.0343 (0.0343)
+2022-11-18 16:00:42,996:INFO: Dataset: univ                Batch:  2/15	Loss 0.0348 (0.0345)
+2022-11-18 16:00:43,078:INFO: Dataset: univ                Batch:  3/15	Loss 0.0332 (0.0341)
+2022-11-18 16:00:43,155:INFO: Dataset: univ                Batch:  4/15	Loss 0.0352 (0.0343)
+2022-11-18 16:00:43,235:INFO: Dataset: univ                Batch:  5/15	Loss 0.0350 (0.0345)
+2022-11-18 16:00:43,311:INFO: Dataset: univ                Batch:  6/15	Loss 0.0365 (0.0348)
+2022-11-18 16:00:43,387:INFO: Dataset: univ                Batch:  7/15	Loss 0.0375 (0.0351)
+2022-11-18 16:00:43,463:INFO: Dataset: univ                Batch:  8/15	Loss 0.0329 (0.0348)
+2022-11-18 16:00:43,538:INFO: Dataset: univ                Batch:  9/15	Loss 0.0325 (0.0346)
+2022-11-18 16:00:43,615:INFO: Dataset: univ                Batch: 10/15	Loss 0.0357 (0.0347)
+2022-11-18 16:00:43,692:INFO: Dataset: univ                Batch: 11/15	Loss 0.0356 (0.0348)
+2022-11-18 16:00:43,767:INFO: Dataset: univ                Batch: 12/15	Loss 0.0351 (0.0348)
+2022-11-18 16:00:43,844:INFO: Dataset: univ                Batch: 13/15	Loss 0.0329 (0.0346)
+2022-11-18 16:00:43,920:INFO: Dataset: univ                Batch: 14/15	Loss 0.0345 (0.0346)
+2022-11-18 16:00:43,954:INFO: Dataset: univ                Batch: 15/15	Loss 0.0372 (0.0347)
+2022-11-18 16:00:44,264:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0751 (0.0751)
+2022-11-18 16:00:44,334:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0810 (0.0783)
+2022-11-18 16:00:44,406:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0787 (0.0784)
+2022-11-18 16:00:44,476:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0700 (0.0763)
+2022-11-18 16:00:44,548:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0751 (0.0760)
+2022-11-18 16:00:44,618:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0758 (0.0760)
+2022-11-18 16:00:44,687:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0759 (0.0760)
+2022-11-18 16:00:44,750:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0698 (0.0753)
+2022-11-18 16:00:45,075:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0481 (0.0481)
+2022-11-18 16:00:45,146:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0517 (0.0499)
+2022-11-18 16:00:45,218:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0422 (0.0476)
+2022-11-18 16:00:45,287:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0574 (0.0503)
+2022-11-18 16:00:45,358:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0479 (0.0498)
+2022-11-18 16:00:45,433:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0656 (0.0523)
+2022-11-18 16:00:45,503:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0655 (0.0540)
+2022-11-18 16:00:45,572:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0466 (0.0531)
+2022-11-18 16:00:45,641:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0480 (0.0525)
+2022-11-18 16:00:45,710:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0513 (0.0524)
+2022-11-18 16:00:45,782:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0531 (0.0525)
+2022-11-18 16:00:45,851:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0531 (0.0525)
+2022-11-18 16:00:45,919:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0527 (0.0526)
+2022-11-18 16:00:45,991:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0580 (0.0530)
+2022-11-18 16:00:46,061:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0479 (0.0526)
+2022-11-18 16:00:46,131:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0553 (0.0528)
+2022-11-18 16:00:46,202:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0496 (0.0526)
+2022-11-18 16:00:46,266:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0539 (0.0527)
+2022-11-18 16:00:46,311:INFO: - Computing ADE (validation)
+2022-11-18 16:00:46,601:INFO: 		 ADE on hotel                     dataset:	 1.1166234016418457
+2022-11-18 16:00:46,968:INFO: 		 ADE on univ                      dataset:	 1.4730377197265625
+2022-11-18 16:00:47,279:INFO: 		 ADE on zara1                     dataset:	 2.287585735321045
+2022-11-18 16:00:47,734:INFO: 		 ADE on zara2                     dataset:	 1.4021724462509155
+2022-11-18 16:00:47,734:INFO: Average validation:	ADE  1.4749	FDE  2.6804
+2022-11-18 16:00:47,748:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_643.pth.tar
+2022-11-18 16:00:47,748:INFO: 
+===> EPOCH: 644 (P3)
+2022-11-18 16:00:47,748:INFO: - Computing loss (training)
+2022-11-18 16:00:48,014:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0442 (0.0442)
+2022-11-18 16:00:48,086:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0401 (0.0421)
+2022-11-18 16:00:48,155:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0514 (0.0449)
+2022-11-18 16:00:48,206:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0565 (0.0467)
+2022-11-18 16:00:48,521:INFO: Dataset: univ                Batch:  1/15	Loss 0.0353 (0.0353)
+2022-11-18 16:00:48,599:INFO: Dataset: univ                Batch:  2/15	Loss 0.0332 (0.0343)
+2022-11-18 16:00:48,681:INFO: Dataset: univ                Batch:  3/15	Loss 0.0377 (0.0354)
+2022-11-18 16:00:48,760:INFO: Dataset: univ                Batch:  4/15	Loss 0.0343 (0.0351)
+2022-11-18 16:00:48,836:INFO: Dataset: univ                Batch:  5/15	Loss 0.0354 (0.0351)
+2022-11-18 16:00:48,916:INFO: Dataset: univ                Batch:  6/15	Loss 0.0358 (0.0352)
+2022-11-18 16:00:48,994:INFO: Dataset: univ                Batch:  7/15	Loss 0.0361 (0.0354)
+2022-11-18 16:00:49,073:INFO: Dataset: univ                Batch:  8/15	Loss 0.0349 (0.0353)
+2022-11-18 16:00:49,151:INFO: Dataset: univ                Batch:  9/15	Loss 0.0341 (0.0352)
+2022-11-18 16:00:49,230:INFO: Dataset: univ                Batch: 10/15	Loss 0.0335 (0.0350)
+2022-11-18 16:00:49,309:INFO: Dataset: univ                Batch: 11/15	Loss 0.0328 (0.0348)
+2022-11-18 16:00:49,388:INFO: Dataset: univ                Batch: 12/15	Loss 0.0345 (0.0347)
+2022-11-18 16:00:49,466:INFO: Dataset: univ                Batch: 13/15	Loss 0.0342 (0.0347)
+2022-11-18 16:00:49,545:INFO: Dataset: univ                Batch: 14/15	Loss 0.0325 (0.0345)
+2022-11-18 16:00:49,578:INFO: Dataset: univ                Batch: 15/15	Loss 0.0358 (0.0346)
+2022-11-18 16:00:49,896:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0775 (0.0775)
+2022-11-18 16:00:49,967:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0793 (0.0785)
+2022-11-18 16:00:50,036:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0736 (0.0768)
+2022-11-18 16:00:50,103:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0682 (0.0747)
+2022-11-18 16:00:50,176:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0751 (0.0748)
+2022-11-18 16:00:50,247:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0753 (0.0749)
+2022-11-18 16:00:50,315:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0737 (0.0747)
+2022-11-18 16:00:50,377:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0728 (0.0745)
+2022-11-18 16:00:50,752:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0546 (0.0546)
+2022-11-18 16:00:50,883:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0478 (0.0514)
+2022-11-18 16:00:50,961:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0466 (0.0498)
+2022-11-18 16:00:51,039:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0501 (0.0499)
+2022-11-18 16:00:51,111:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0531 (0.0505)
+2022-11-18 16:00:51,184:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0505 (0.0505)
+2022-11-18 16:00:51,259:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0542 (0.0510)
+2022-11-18 16:00:51,325:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0480 (0.0507)
+2022-11-18 16:00:51,391:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0487 (0.0504)
+2022-11-18 16:00:51,460:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0569 (0.0510)
+2022-11-18 16:00:51,527:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0542 (0.0513)
+2022-11-18 16:00:51,613:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0490 (0.0511)
+2022-11-18 16:00:51,695:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0525 (0.0512)
+2022-11-18 16:00:51,769:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0545 (0.0515)
+2022-11-18 16:00:51,845:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0484 (0.0513)
+2022-11-18 16:00:51,915:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0501 (0.0512)
+2022-11-18 16:00:51,983:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0612 (0.0518)
+2022-11-18 16:00:52,044:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0581 (0.0521)
+2022-11-18 16:00:52,089:INFO: - Computing ADE (validation)
+2022-11-18 16:00:52,366:INFO: 		 ADE on hotel                     dataset:	 1.1433453559875488
+2022-11-18 16:00:52,754:INFO: 		 ADE on univ                      dataset:	 1.4577209949493408
+2022-11-18 16:00:53,084:INFO: 		 ADE on zara1                     dataset:	 2.2372586727142334
+2022-11-18 16:00:53,553:INFO: 		 ADE on zara2                     dataset:	 1.3996829986572266
+2022-11-18 16:00:53,553:INFO: Average validation:	ADE  1.4645	FDE  2.6636
+2022-11-18 16:00:53,566:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_644.pth.tar
+2022-11-18 16:00:53,566:INFO: 
+===> EPOCH: 645 (P3)
+2022-11-18 16:00:53,566:INFO: - Computing loss (training)
+2022-11-18 16:00:53,842:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0444 (0.0444)
+2022-11-18 16:00:53,920:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0428 (0.0436)
+2022-11-18 16:00:53,993:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0587 (0.0488)
+2022-11-18 16:00:54,047:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0383 (0.0468)
+2022-11-18 16:00:54,387:INFO: Dataset: univ                Batch:  1/15	Loss 0.0342 (0.0342)
+2022-11-18 16:00:54,469:INFO: Dataset: univ                Batch:  2/15	Loss 0.0316 (0.0329)
+2022-11-18 16:00:54,550:INFO: Dataset: univ                Batch:  3/15	Loss 0.0351 (0.0336)
+2022-11-18 16:00:54,628:INFO: Dataset: univ                Batch:  4/15	Loss 0.0345 (0.0338)
+2022-11-18 16:00:54,703:INFO: Dataset: univ                Batch:  5/15	Loss 0.0367 (0.0344)
+2022-11-18 16:00:54,780:INFO: Dataset: univ                Batch:  6/15	Loss 0.0340 (0.0343)
+2022-11-18 16:00:54,856:INFO: Dataset: univ                Batch:  7/15	Loss 0.0341 (0.0343)
+2022-11-18 16:00:54,933:INFO: Dataset: univ                Batch:  8/15	Loss 0.0354 (0.0344)
+2022-11-18 16:00:55,008:INFO: Dataset: univ                Batch:  9/15	Loss 0.0347 (0.0345)
+2022-11-18 16:00:55,084:INFO: Dataset: univ                Batch: 10/15	Loss 0.0333 (0.0344)
+2022-11-18 16:00:55,165:INFO: Dataset: univ                Batch: 11/15	Loss 0.0324 (0.0342)
+2022-11-18 16:00:55,240:INFO: Dataset: univ                Batch: 12/15	Loss 0.0366 (0.0343)
+2022-11-18 16:00:55,317:INFO: Dataset: univ                Batch: 13/15	Loss 0.0352 (0.0344)
+2022-11-18 16:00:55,394:INFO: Dataset: univ                Batch: 14/15	Loss 0.0357 (0.0345)
+2022-11-18 16:00:55,428:INFO: Dataset: univ                Batch: 15/15	Loss 0.0273 (0.0344)
+2022-11-18 16:00:55,732:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0737 (0.0737)
+2022-11-18 16:00:55,802:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0758 (0.0747)
+2022-11-18 16:00:55,869:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0744 (0.0746)
+2022-11-18 16:00:55,938:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0710 (0.0737)
+2022-11-18 16:00:56,008:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0696 (0.0728)
+2022-11-18 16:00:56,076:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0738 (0.0730)
+2022-11-18 16:00:56,143:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0699 (0.0725)
+2022-11-18 16:00:56,205:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0785 (0.0731)
+2022-11-18 16:00:56,552:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0545 (0.0545)
+2022-11-18 16:00:56,618:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0523 (0.0535)
+2022-11-18 16:00:56,687:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0455 (0.0508)
+2022-11-18 16:00:56,753:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0558 (0.0519)
+2022-11-18 16:00:56,820:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0516 (0.0519)
+2022-11-18 16:00:56,898:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0540 (0.0522)
+2022-11-18 16:00:56,964:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0500 (0.0519)
+2022-11-18 16:00:57,030:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0523 (0.0519)
+2022-11-18 16:00:57,095:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0513 (0.0519)
+2022-11-18 16:00:57,161:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0563 (0.0524)
+2022-11-18 16:00:57,227:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0470 (0.0518)
+2022-11-18 16:00:57,294:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0520 (0.0519)
+2022-11-18 16:00:57,359:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0509 (0.0518)
+2022-11-18 16:00:57,425:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0555 (0.0521)
+2022-11-18 16:00:57,492:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0561 (0.0523)
+2022-11-18 16:00:57,560:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0495 (0.0521)
+2022-11-18 16:00:57,627:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0459 (0.0517)
+2022-11-18 16:00:57,688:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0471 (0.0515)
+2022-11-18 16:00:57,733:INFO: - Computing ADE (validation)
+2022-11-18 16:00:58,010:INFO: 		 ADE on hotel                     dataset:	 1.1311224699020386
+2022-11-18 16:00:58,383:INFO: 		 ADE on univ                      dataset:	 1.4570484161376953
+2022-11-18 16:00:58,688:INFO: 		 ADE on zara1                     dataset:	 2.2653145790100098
+2022-11-18 16:00:59,160:INFO: 		 ADE on zara2                     dataset:	 1.4002680778503418
+2022-11-18 16:00:59,160:INFO: Average validation:	ADE  1.4654	FDE  2.6630
+2022-11-18 16:00:59,173:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_645.pth.tar
+2022-11-18 16:00:59,173:INFO: 
+===> EPOCH: 646 (P3)
+2022-11-18 16:00:59,174:INFO: - Computing loss (training)
+2022-11-18 16:00:59,456:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0486 (0.0486)
+2022-11-18 16:00:59,528:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0465 (0.0476)
+2022-11-18 16:00:59,598:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0473 (0.0475)
+2022-11-18 16:00:59,650:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0440 (0.0469)
+2022-11-18 16:00:59,991:INFO: Dataset: univ                Batch:  1/15	Loss 0.0342 (0.0342)
+2022-11-18 16:01:00,066:INFO: Dataset: univ                Batch:  2/15	Loss 0.0346 (0.0344)
+2022-11-18 16:01:00,138:INFO: Dataset: univ                Batch:  3/15	Loss 0.0361 (0.0349)
+2022-11-18 16:01:00,216:INFO: Dataset: univ                Batch:  4/15	Loss 0.0331 (0.0344)
+2022-11-18 16:01:00,288:INFO: Dataset: univ                Batch:  5/15	Loss 0.0372 (0.0350)
+2022-11-18 16:01:00,366:INFO: Dataset: univ                Batch:  6/15	Loss 0.0359 (0.0351)
+2022-11-18 16:01:00,440:INFO: Dataset: univ                Batch:  7/15	Loss 0.0332 (0.0348)
+2022-11-18 16:01:00,512:INFO: Dataset: univ                Batch:  8/15	Loss 0.0307 (0.0344)
+2022-11-18 16:01:00,585:INFO: Dataset: univ                Batch:  9/15	Loss 0.0355 (0.0345)
+2022-11-18 16:01:00,658:INFO: Dataset: univ                Batch: 10/15	Loss 0.0348 (0.0345)
+2022-11-18 16:01:00,740:INFO: Dataset: univ                Batch: 11/15	Loss 0.0341 (0.0345)
+2022-11-18 16:01:00,817:INFO: Dataset: univ                Batch: 12/15	Loss 0.0328 (0.0343)
+2022-11-18 16:01:00,895:INFO: Dataset: univ                Batch: 13/15	Loss 0.0332 (0.0342)
+2022-11-18 16:01:00,973:INFO: Dataset: univ                Batch: 14/15	Loss 0.0331 (0.0342)
+2022-11-18 16:01:01,009:INFO: Dataset: univ                Batch: 15/15	Loss 0.0363 (0.0342)
+2022-11-18 16:01:01,327:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0768 (0.0768)
+2022-11-18 16:01:01,393:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0765 (0.0767)
+2022-11-18 16:01:01,459:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0724 (0.0752)
+2022-11-18 16:01:01,527:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0722 (0.0745)
+2022-11-18 16:01:01,596:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0696 (0.0736)
+2022-11-18 16:01:01,665:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0685 (0.0727)
+2022-11-18 16:01:01,740:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0707 (0.0724)
+2022-11-18 16:01:01,811:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0741 (0.0726)
+2022-11-18 16:01:02,154:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0459 (0.0459)
+2022-11-18 16:01:02,228:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0542 (0.0498)
+2022-11-18 16:01:02,299:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0583 (0.0527)
+2022-11-18 16:01:02,374:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0441 (0.0505)
+2022-11-18 16:01:02,450:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0458 (0.0495)
+2022-11-18 16:01:02,527:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0464 (0.0490)
+2022-11-18 16:01:02,603:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0543 (0.0498)
+2022-11-18 16:01:02,679:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0596 (0.0510)
+2022-11-18 16:01:02,756:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0556 (0.0515)
+2022-11-18 16:01:02,826:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0456 (0.0508)
+2022-11-18 16:01:02,898:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0482 (0.0506)
+2022-11-18 16:01:02,970:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0519 (0.0507)
+2022-11-18 16:01:03,044:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0480 (0.0505)
+2022-11-18 16:01:03,114:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0506 (0.0505)
+2022-11-18 16:01:03,192:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0547 (0.0508)
+2022-11-18 16:01:03,268:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0510 (0.0508)
+2022-11-18 16:01:03,339:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0542 (0.0510)
+2022-11-18 16:01:03,402:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0522 (0.0511)
+2022-11-18 16:01:03,452:INFO: - Computing ADE (validation)
+2022-11-18 16:01:03,746:INFO: 		 ADE on hotel                     dataset:	 1.1379207372665405
+2022-11-18 16:01:04,192:INFO: 		 ADE on univ                      dataset:	 1.4517382383346558
+2022-11-18 16:01:04,535:INFO: 		 ADE on zara1                     dataset:	 2.2080225944519043
+2022-11-18 16:01:05,058:INFO: 		 ADE on zara2                     dataset:	 1.398914098739624
+2022-11-18 16:01:05,058:INFO: Average validation:	ADE  1.4591	FDE  2.6550
+2022-11-18 16:01:05,072:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_646.pth.tar
+2022-11-18 16:01:05,072:INFO: 
+===> EPOCH: 647 (P3)
+2022-11-18 16:01:05,073:INFO: - Computing loss (training)
+2022-11-18 16:01:05,354:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0387 (0.0387)
+2022-11-18 16:01:05,430:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0390 (0.0389)
+2022-11-18 16:01:05,507:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0545 (0.0442)
+2022-11-18 16:01:05,568:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0616 (0.0469)
+2022-11-18 16:01:05,898:INFO: Dataset: univ                Batch:  1/15	Loss 0.0333 (0.0333)
+2022-11-18 16:01:05,991:INFO: Dataset: univ                Batch:  2/15	Loss 0.0339 (0.0336)
+2022-11-18 16:01:06,084:INFO: Dataset: univ                Batch:  3/15	Loss 0.0317 (0.0330)
+2022-11-18 16:01:06,170:INFO: Dataset: univ                Batch:  4/15	Loss 0.0338 (0.0332)
+2022-11-18 16:01:06,257:INFO: Dataset: univ                Batch:  5/15	Loss 0.0331 (0.0332)
+2022-11-18 16:01:06,341:INFO: Dataset: univ                Batch:  6/15	Loss 0.0334 (0.0332)
+2022-11-18 16:01:06,424:INFO: Dataset: univ                Batch:  7/15	Loss 0.0351 (0.0335)
+2022-11-18 16:01:06,508:INFO: Dataset: univ                Batch:  8/15	Loss 0.0304 (0.0331)
+2022-11-18 16:01:06,595:INFO: Dataset: univ                Batch:  9/15	Loss 0.0337 (0.0332)
+2022-11-18 16:01:06,679:INFO: Dataset: univ                Batch: 10/15	Loss 0.0364 (0.0335)
+2022-11-18 16:01:06,764:INFO: Dataset: univ                Batch: 11/15	Loss 0.0341 (0.0335)
+2022-11-18 16:01:06,852:INFO: Dataset: univ                Batch: 12/15	Loss 0.0349 (0.0336)
+2022-11-18 16:01:06,938:INFO: Dataset: univ                Batch: 13/15	Loss 0.0360 (0.0338)
+2022-11-18 16:01:07,025:INFO: Dataset: univ                Batch: 14/15	Loss 0.0366 (0.0340)
+2022-11-18 16:01:07,062:INFO: Dataset: univ                Batch: 15/15	Loss 0.0351 (0.0340)
+2022-11-18 16:01:07,369:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0795 (0.0795)
+2022-11-18 16:01:07,439:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0690 (0.0743)
+2022-11-18 16:01:07,511:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0681 (0.0721)
+2022-11-18 16:01:07,579:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0680 (0.0710)
+2022-11-18 16:01:07,648:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0749 (0.0718)
+2022-11-18 16:01:07,719:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0681 (0.0711)
+2022-11-18 16:01:07,788:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0719 (0.0712)
+2022-11-18 16:01:07,848:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0772 (0.0718)
+2022-11-18 16:01:08,199:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0530 (0.0530)
+2022-11-18 16:01:08,271:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0557 (0.0544)
+2022-11-18 16:01:08,352:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0465 (0.0519)
+2022-11-18 16:01:08,440:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0542 (0.0524)
+2022-11-18 16:01:08,515:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0495 (0.0519)
+2022-11-18 16:01:08,586:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0534 (0.0521)
+2022-11-18 16:01:08,653:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0529 (0.0523)
+2022-11-18 16:01:08,726:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0467 (0.0516)
+2022-11-18 16:01:08,799:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0517 (0.0516)
+2022-11-18 16:01:08,873:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0466 (0.0511)
+2022-11-18 16:01:08,952:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0471 (0.0508)
+2022-11-18 16:01:09,027:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0452 (0.0503)
+2022-11-18 16:01:09,102:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0486 (0.0502)
+2022-11-18 16:01:09,174:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0531 (0.0504)
+2022-11-18 16:01:09,247:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0525 (0.0505)
+2022-11-18 16:01:09,319:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0517 (0.0506)
+2022-11-18 16:01:09,391:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0482 (0.0505)
+2022-11-18 16:01:09,455:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0490 (0.0504)
+2022-11-18 16:01:09,499:INFO: - Computing ADE (validation)
+2022-11-18 16:01:09,787:INFO: 		 ADE on hotel                     dataset:	 1.1603227853775024
+2022-11-18 16:01:10,180:INFO: 		 ADE on univ                      dataset:	 1.4445710182189941
+2022-11-18 16:01:10,483:INFO: 		 ADE on zara1                     dataset:	 2.138019561767578
+2022-11-18 16:01:10,979:INFO: 		 ADE on zara2                     dataset:	 1.3822052478790283
+2022-11-18 16:01:10,980:INFO: Average validation:	ADE  1.4464	FDE  2.6333
+2022-11-18 16:01:10,995:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_647.pth.tar
+2022-11-18 16:01:10,995:INFO: 
+===> EPOCH: 648 (P3)
+2022-11-18 16:01:10,996:INFO: - Computing loss (training)
+2022-11-18 16:01:11,283:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0433 (0.0433)
+2022-11-18 16:01:11,359:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0435 (0.0434)
+2022-11-18 16:01:11,438:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0526 (0.0465)
+2022-11-18 16:01:11,500:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0500 (0.0470)
+2022-11-18 16:01:11,858:INFO: Dataset: univ                Batch:  1/15	Loss 0.0334 (0.0334)
+2022-11-18 16:01:11,941:INFO: Dataset: univ                Batch:  2/15	Loss 0.0327 (0.0331)
+2022-11-18 16:01:12,017:INFO: Dataset: univ                Batch:  3/15	Loss 0.0347 (0.0336)
+2022-11-18 16:01:12,099:INFO: Dataset: univ                Batch:  4/15	Loss 0.0337 (0.0336)
+2022-11-18 16:01:12,178:INFO: Dataset: univ                Batch:  5/15	Loss 0.0328 (0.0334)
+2022-11-18 16:01:12,260:INFO: Dataset: univ                Batch:  6/15	Loss 0.0332 (0.0334)
+2022-11-18 16:01:12,335:INFO: Dataset: univ                Batch:  7/15	Loss 0.0341 (0.0335)
+2022-11-18 16:01:12,409:INFO: Dataset: univ                Batch:  8/15	Loss 0.0343 (0.0336)
+2022-11-18 16:01:12,484:INFO: Dataset: univ                Batch:  9/15	Loss 0.0338 (0.0336)
+2022-11-18 16:01:12,556:INFO: Dataset: univ                Batch: 10/15	Loss 0.0350 (0.0338)
+2022-11-18 16:01:12,629:INFO: Dataset: univ                Batch: 11/15	Loss 0.0359 (0.0340)
+2022-11-18 16:01:12,706:INFO: Dataset: univ                Batch: 12/15	Loss 0.0305 (0.0337)
+2022-11-18 16:01:12,782:INFO: Dataset: univ                Batch: 13/15	Loss 0.0351 (0.0338)
+2022-11-18 16:01:12,864:INFO: Dataset: univ                Batch: 14/15	Loss 0.0333 (0.0337)
+2022-11-18 16:01:12,904:INFO: Dataset: univ                Batch: 15/15	Loss 0.0341 (0.0337)
+2022-11-18 16:01:13,224:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0700 (0.0700)
+2022-11-18 16:01:13,292:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0728 (0.0714)
+2022-11-18 16:01:13,362:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0685 (0.0705)
+2022-11-18 16:01:13,431:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0665 (0.0695)
+2022-11-18 16:01:13,503:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0719 (0.0700)
+2022-11-18 16:01:13,571:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0774 (0.0713)
+2022-11-18 16:01:13,639:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0678 (0.0708)
+2022-11-18 16:01:13,701:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0694 (0.0706)
+2022-11-18 16:01:14,014:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0477 (0.0477)
+2022-11-18 16:01:14,083:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0510 (0.0495)
+2022-11-18 16:01:14,152:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0581 (0.0524)
+2022-11-18 16:01:14,224:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0468 (0.0510)
+2022-11-18 16:01:14,293:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0513 (0.0511)
+2022-11-18 16:01:14,367:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0476 (0.0504)
+2022-11-18 16:01:14,454:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0493 (0.0503)
+2022-11-18 16:01:14,531:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0519 (0.0505)
+2022-11-18 16:01:14,608:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0488 (0.0503)
+2022-11-18 16:01:14,689:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0534 (0.0506)
+2022-11-18 16:01:14,771:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0478 (0.0504)
+2022-11-18 16:01:14,852:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0556 (0.0508)
+2022-11-18 16:01:14,959:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0449 (0.0503)
+2022-11-18 16:01:15,050:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0520 (0.0504)
+2022-11-18 16:01:15,129:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0514 (0.0505)
+2022-11-18 16:01:15,209:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0440 (0.0501)
+2022-11-18 16:01:15,285:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0477 (0.0499)
+2022-11-18 16:01:15,353:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0444 (0.0497)
+2022-11-18 16:01:15,399:INFO: - Computing ADE (validation)
+2022-11-18 16:01:15,684:INFO: 		 ADE on hotel                     dataset:	 1.1637135744094849
+2022-11-18 16:01:16,070:INFO: 		 ADE on univ                      dataset:	 1.439027190208435
+2022-11-18 16:01:16,372:INFO: 		 ADE on zara1                     dataset:	 2.1760690212249756
+2022-11-18 16:01:16,846:INFO: 		 ADE on zara2                     dataset:	 1.3705321550369263
+2022-11-18 16:01:16,847:INFO: Average validation:	ADE  1.4417	FDE  2.6272
+2022-11-18 16:01:16,859:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_648.pth.tar
+2022-11-18 16:01:16,859:INFO: 
+===> EPOCH: 649 (P3)
+2022-11-18 16:01:16,860:INFO: - Computing loss (training)
+2022-11-18 16:01:17,119:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0474 (0.0474)
+2022-11-18 16:01:17,187:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0375 (0.0425)
+2022-11-18 16:01:17,255:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0536 (0.0462)
+2022-11-18 16:01:17,306:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0510 (0.0470)
+2022-11-18 16:01:17,683:INFO: Dataset: univ                Batch:  1/15	Loss 0.0331 (0.0331)
+2022-11-18 16:01:17,765:INFO: Dataset: univ                Batch:  2/15	Loss 0.0312 (0.0321)
+2022-11-18 16:01:17,843:INFO: Dataset: univ                Batch:  3/15	Loss 0.0336 (0.0325)
+2022-11-18 16:01:17,924:INFO: Dataset: univ                Batch:  4/15	Loss 0.0327 (0.0326)
+2022-11-18 16:01:18,001:INFO: Dataset: univ                Batch:  5/15	Loss 0.0344 (0.0329)
+2022-11-18 16:01:18,082:INFO: Dataset: univ                Batch:  6/15	Loss 0.0374 (0.0336)
+2022-11-18 16:01:18,160:INFO: Dataset: univ                Batch:  7/15	Loss 0.0335 (0.0336)
+2022-11-18 16:01:18,239:INFO: Dataset: univ                Batch:  8/15	Loss 0.0303 (0.0331)
+2022-11-18 16:01:18,315:INFO: Dataset: univ                Batch:  9/15	Loss 0.0330 (0.0331)
+2022-11-18 16:01:18,392:INFO: Dataset: univ                Batch: 10/15	Loss 0.0331 (0.0331)
+2022-11-18 16:01:18,470:INFO: Dataset: univ                Batch: 11/15	Loss 0.0346 (0.0332)
+2022-11-18 16:01:18,550:INFO: Dataset: univ                Batch: 12/15	Loss 0.0319 (0.0331)
+2022-11-18 16:01:18,629:INFO: Dataset: univ                Batch: 13/15	Loss 0.0348 (0.0333)
+2022-11-18 16:01:18,708:INFO: Dataset: univ                Batch: 14/15	Loss 0.0364 (0.0335)
+2022-11-18 16:01:18,742:INFO: Dataset: univ                Batch: 15/15	Loss 0.0384 (0.0335)
+2022-11-18 16:01:19,068:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0711 (0.0711)
+2022-11-18 16:01:19,144:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0695 (0.0704)
+2022-11-18 16:01:19,211:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0609 (0.0671)
+2022-11-18 16:01:19,278:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0728 (0.0687)
+2022-11-18 16:01:19,346:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0693 (0.0688)
+2022-11-18 16:01:19,413:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0741 (0.0697)
+2022-11-18 16:01:19,478:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0653 (0.0691)
+2022-11-18 16:01:19,538:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0709 (0.0692)
+2022-11-18 16:01:19,854:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0457 (0.0457)
+2022-11-18 16:01:19,924:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0474 (0.0466)
+2022-11-18 16:01:19,990:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0454 (0.0462)
+2022-11-18 16:01:20,061:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0528 (0.0479)
+2022-11-18 16:01:20,132:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0546 (0.0492)
+2022-11-18 16:01:20,205:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0467 (0.0488)
+2022-11-18 16:01:20,275:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0512 (0.0491)
+2022-11-18 16:01:20,346:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0521 (0.0495)
+2022-11-18 16:01:20,417:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0485 (0.0494)
+2022-11-18 16:01:20,487:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0494 (0.0494)
+2022-11-18 16:01:20,560:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0503 (0.0495)
+2022-11-18 16:01:20,630:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0460 (0.0492)
+2022-11-18 16:01:20,700:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0456 (0.0489)
+2022-11-18 16:01:20,772:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0512 (0.0491)
+2022-11-18 16:01:20,844:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0462 (0.0489)
+2022-11-18 16:01:20,915:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0484 (0.0489)
+2022-11-18 16:01:20,988:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0493 (0.0489)
+2022-11-18 16:01:21,053:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0487 (0.0489)
+2022-11-18 16:01:21,096:INFO: - Computing ADE (validation)
+2022-11-18 16:01:21,386:INFO: 		 ADE on hotel                     dataset:	 1.163644552230835
+2022-11-18 16:01:21,785:INFO: 		 ADE on univ                      dataset:	 1.4265661239624023
+2022-11-18 16:01:22,090:INFO: 		 ADE on zara1                     dataset:	 2.1341500282287598
+2022-11-18 16:01:22,560:INFO: 		 ADE on zara2                     dataset:	 1.3845094442367554
+2022-11-18 16:01:22,560:INFO: Average validation:	ADE  1.4379	FDE  2.6175
+2022-11-18 16:01:22,574:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_649.pth.tar
+2022-11-18 16:01:22,574:INFO: 
+===> EPOCH: 650 (P3)
+2022-11-18 16:01:22,574:INFO: - Computing loss (training)
+2022-11-18 16:01:22,843:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0464 (0.0464)
+2022-11-18 16:01:22,916:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0493 (0.0478)
+2022-11-18 16:01:22,989:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0432 (0.0462)
+2022-11-18 16:01:23,043:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0505 (0.0470)
+2022-11-18 16:01:23,377:INFO: Dataset: univ                Batch:  1/15	Loss 0.0333 (0.0333)
+2022-11-18 16:01:23,460:INFO: Dataset: univ                Batch:  2/15	Loss 0.0344 (0.0339)
+2022-11-18 16:01:23,541:INFO: Dataset: univ                Batch:  3/15	Loss 0.0301 (0.0326)
+2022-11-18 16:01:23,627:INFO: Dataset: univ                Batch:  4/15	Loss 0.0319 (0.0324)
+2022-11-18 16:01:23,706:INFO: Dataset: univ                Batch:  5/15	Loss 0.0319 (0.0323)
+2022-11-18 16:01:23,785:INFO: Dataset: univ                Batch:  6/15	Loss 0.0338 (0.0325)
+2022-11-18 16:01:23,863:INFO: Dataset: univ                Batch:  7/15	Loss 0.0343 (0.0328)
+2022-11-18 16:01:23,939:INFO: Dataset: univ                Batch:  8/15	Loss 0.0355 (0.0331)
+2022-11-18 16:01:24,016:INFO: Dataset: univ                Batch:  9/15	Loss 0.0334 (0.0331)
+2022-11-18 16:01:24,094:INFO: Dataset: univ                Batch: 10/15	Loss 0.0338 (0.0332)
+2022-11-18 16:01:24,172:INFO: Dataset: univ                Batch: 11/15	Loss 0.0337 (0.0332)
+2022-11-18 16:01:24,250:INFO: Dataset: univ                Batch: 12/15	Loss 0.0330 (0.0332)
+2022-11-18 16:01:24,327:INFO: Dataset: univ                Batch: 13/15	Loss 0.0339 (0.0333)
+2022-11-18 16:01:24,405:INFO: Dataset: univ                Batch: 14/15	Loss 0.0318 (0.0331)
+2022-11-18 16:01:24,440:INFO: Dataset: univ                Batch: 15/15	Loss 0.0387 (0.0332)
+2022-11-18 16:01:24,747:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0671 (0.0671)
+2022-11-18 16:01:24,821:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0689 (0.0680)
+2022-11-18 16:01:24,892:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0738 (0.0698)
+2022-11-18 16:01:24,967:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0633 (0.0681)
+2022-11-18 16:01:25,040:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0689 (0.0683)
+2022-11-18 16:01:25,112:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0640 (0.0676)
+2022-11-18 16:01:25,182:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0738 (0.0684)
+2022-11-18 16:01:25,247:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0661 (0.0682)
+2022-11-18 16:01:25,561:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0461 (0.0461)
+2022-11-18 16:01:25,627:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0478 (0.0469)
+2022-11-18 16:01:25,693:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0473 (0.0470)
+2022-11-18 16:01:25,766:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0486 (0.0474)
+2022-11-18 16:01:25,836:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0532 (0.0485)
+2022-11-18 16:01:25,904:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0460 (0.0481)
+2022-11-18 16:01:25,970:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0493 (0.0483)
+2022-11-18 16:01:26,036:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0479 (0.0482)
+2022-11-18 16:01:26,101:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0502 (0.0485)
+2022-11-18 16:01:26,166:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0500 (0.0486)
+2022-11-18 16:01:26,235:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0480 (0.0486)
+2022-11-18 16:01:26,301:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0508 (0.0487)
+2022-11-18 16:01:26,366:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0437 (0.0483)
+2022-11-18 16:01:26,433:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0463 (0.0482)
+2022-11-18 16:01:26,501:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0463 (0.0481)
+2022-11-18 16:01:26,568:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0457 (0.0479)
+2022-11-18 16:01:26,634:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0488 (0.0480)
+2022-11-18 16:01:26,695:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0500 (0.0481)
+2022-11-18 16:01:26,742:INFO: - Computing ADE (validation)
+2022-11-18 16:01:27,026:INFO: 		 ADE on hotel                     dataset:	 1.1848604679107666
+2022-11-18 16:01:27,405:INFO: 		 ADE on univ                      dataset:	 1.435505986213684
+2022-11-18 16:01:27,687:INFO: 		 ADE on zara1                     dataset:	 2.1377875804901123
+2022-11-18 16:01:28,155:INFO: 		 ADE on zara2                     dataset:	 1.3699774742126465
+2022-11-18 16:01:28,155:INFO: Average validation:	ADE  1.4386	FDE  2.6206
+2022-11-18 16:01:28,168:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_650.pth.tar
+2022-11-18 16:01:28,168:INFO: 
+===> EPOCH: 651 (P3)
+2022-11-18 16:01:28,169:INFO: - Computing loss (training)
+2022-11-18 16:01:28,427:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0463 (0.0463)
+2022-11-18 16:01:28,497:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0398 (0.0431)
+2022-11-18 16:01:28,567:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0514 (0.0460)
+2022-11-18 16:01:28,617:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0525 (0.0470)
+2022-11-18 16:01:28,954:INFO: Dataset: univ                Batch:  1/15	Loss 0.0319 (0.0319)
+2022-11-18 16:01:29,031:INFO: Dataset: univ                Batch:  2/15	Loss 0.0315 (0.0317)
+2022-11-18 16:01:29,113:INFO: Dataset: univ                Batch:  3/15	Loss 0.0343 (0.0327)
+2022-11-18 16:01:29,194:INFO: Dataset: univ                Batch:  4/15	Loss 0.0323 (0.0326)
+2022-11-18 16:01:29,272:INFO: Dataset: univ                Batch:  5/15	Loss 0.0339 (0.0328)
+2022-11-18 16:01:29,348:INFO: Dataset: univ                Batch:  6/15	Loss 0.0328 (0.0328)
+2022-11-18 16:01:29,425:INFO: Dataset: univ                Batch:  7/15	Loss 0.0353 (0.0331)
+2022-11-18 16:01:29,499:INFO: Dataset: univ                Batch:  8/15	Loss 0.0335 (0.0332)
+2022-11-18 16:01:29,573:INFO: Dataset: univ                Batch:  9/15	Loss 0.0313 (0.0329)
+2022-11-18 16:01:29,648:INFO: Dataset: univ                Batch: 10/15	Loss 0.0338 (0.0330)
+2022-11-18 16:01:29,722:INFO: Dataset: univ                Batch: 11/15	Loss 0.0333 (0.0330)
+2022-11-18 16:01:29,805:INFO: Dataset: univ                Batch: 12/15	Loss 0.0352 (0.0332)
+2022-11-18 16:01:29,887:INFO: Dataset: univ                Batch: 13/15	Loss 0.0320 (0.0331)
+2022-11-18 16:01:29,969:INFO: Dataset: univ                Batch: 14/15	Loss 0.0317 (0.0330)
+2022-11-18 16:01:30,008:INFO: Dataset: univ                Batch: 15/15	Loss 0.0289 (0.0329)
+2022-11-18 16:01:30,411:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0648 (0.0648)
+2022-11-18 16:01:30,500:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0637 (0.0643)
+2022-11-18 16:01:30,586:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0657 (0.0647)
+2022-11-18 16:01:30,659:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0690 (0.0658)
+2022-11-18 16:01:30,733:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0676 (0.0662)
+2022-11-18 16:01:30,804:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0676 (0.0664)
+2022-11-18 16:01:30,873:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0695 (0.0668)
+2022-11-18 16:01:30,938:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0667 (0.0668)
+2022-11-18 16:01:31,260:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0481 (0.0481)
+2022-11-18 16:01:31,353:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0480 (0.0481)
+2022-11-18 16:01:31,450:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0497 (0.0486)
+2022-11-18 16:01:31,539:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0474 (0.0482)
+2022-11-18 16:01:31,625:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0425 (0.0470)
+2022-11-18 16:01:31,708:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0448 (0.0467)
+2022-11-18 16:01:31,798:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0488 (0.0470)
+2022-11-18 16:01:31,884:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0426 (0.0464)
+2022-11-18 16:01:31,963:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0468 (0.0465)
+2022-11-18 16:01:32,043:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0558 (0.0473)
+2022-11-18 16:01:32,125:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0439 (0.0470)
+2022-11-18 16:01:32,201:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0488 (0.0471)
+2022-11-18 16:01:32,274:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0505 (0.0474)
+2022-11-18 16:01:32,349:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0426 (0.0471)
+2022-11-18 16:01:32,425:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0500 (0.0473)
+2022-11-18 16:01:32,498:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0475 (0.0473)
+2022-11-18 16:01:32,573:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0457 (0.0472)
+2022-11-18 16:01:32,642:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0446 (0.0471)
+2022-11-18 16:01:32,689:INFO: - Computing ADE (validation)
+2022-11-18 16:01:32,990:INFO: 		 ADE on hotel                     dataset:	 1.1829286813735962
+2022-11-18 16:01:33,410:INFO: 		 ADE on univ                      dataset:	 1.4150686264038086
+2022-11-18 16:01:33,702:INFO: 		 ADE on zara1                     dataset:	 2.0559682846069336
+2022-11-18 16:01:34,188:INFO: 		 ADE on zara2                     dataset:	 1.3615614175796509
+2022-11-18 16:01:34,188:INFO: Average validation:	ADE  1.4200	FDE  2.5860
+2022-11-18 16:01:34,201:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_651.pth.tar
+2022-11-18 16:01:34,202:INFO: 
+===> EPOCH: 652 (P3)
+2022-11-18 16:01:34,202:INFO: - Computing loss (training)
+2022-11-18 16:01:34,486:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0478 (0.0478)
+2022-11-18 16:01:34,558:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0405 (0.0441)
+2022-11-18 16:01:34,629:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0534 (0.0474)
+2022-11-18 16:01:34,681:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0464 (0.0473)
+2022-11-18 16:01:35,002:INFO: Dataset: univ                Batch:  1/15	Loss 0.0318 (0.0318)
+2022-11-18 16:01:35,087:INFO: Dataset: univ                Batch:  2/15	Loss 0.0314 (0.0316)
+2022-11-18 16:01:35,173:INFO: Dataset: univ                Batch:  3/15	Loss 0.0315 (0.0316)
+2022-11-18 16:01:35,263:INFO: Dataset: univ                Batch:  4/15	Loss 0.0329 (0.0319)
+2022-11-18 16:01:35,351:INFO: Dataset: univ                Batch:  5/15	Loss 0.0341 (0.0323)
+2022-11-18 16:01:35,433:INFO: Dataset: univ                Batch:  6/15	Loss 0.0322 (0.0323)
+2022-11-18 16:01:35,515:INFO: Dataset: univ                Batch:  7/15	Loss 0.0332 (0.0324)
+2022-11-18 16:01:35,595:INFO: Dataset: univ                Batch:  8/15	Loss 0.0339 (0.0326)
+2022-11-18 16:01:35,677:INFO: Dataset: univ                Batch:  9/15	Loss 0.0335 (0.0327)
+2022-11-18 16:01:35,758:INFO: Dataset: univ                Batch: 10/15	Loss 0.0327 (0.0327)
+2022-11-18 16:01:35,835:INFO: Dataset: univ                Batch: 11/15	Loss 0.0317 (0.0326)
+2022-11-18 16:01:35,910:INFO: Dataset: univ                Batch: 12/15	Loss 0.0323 (0.0326)
+2022-11-18 16:01:35,985:INFO: Dataset: univ                Batch: 13/15	Loss 0.0335 (0.0327)
+2022-11-18 16:01:36,062:INFO: Dataset: univ                Batch: 14/15	Loss 0.0323 (0.0326)
+2022-11-18 16:01:36,101:INFO: Dataset: univ                Batch: 15/15	Loss 0.0298 (0.0326)
+2022-11-18 16:01:36,462:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0635 (0.0635)
+2022-11-18 16:01:36,537:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0686 (0.0661)
+2022-11-18 16:01:36,609:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0656 (0.0659)
+2022-11-18 16:01:36,684:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0638 (0.0654)
+2022-11-18 16:01:36,754:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0636 (0.0650)
+2022-11-18 16:01:36,825:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0670 (0.0653)
+2022-11-18 16:01:36,896:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0615 (0.0648)
+2022-11-18 16:01:36,967:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0698 (0.0654)
+2022-11-18 16:01:37,328:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0452 (0.0452)
+2022-11-18 16:01:37,406:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0485 (0.0469)
+2022-11-18 16:01:37,479:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0426 (0.0454)
+2022-11-18 16:01:37,553:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0436 (0.0449)
+2022-11-18 16:01:37,626:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0502 (0.0459)
+2022-11-18 16:01:37,696:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0454 (0.0458)
+2022-11-18 16:01:37,762:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0490 (0.0463)
+2022-11-18 16:01:37,836:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0452 (0.0462)
+2022-11-18 16:01:37,913:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0502 (0.0466)
+2022-11-18 16:01:37,994:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0416 (0.0461)
+2022-11-18 16:01:38,073:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0438 (0.0459)
+2022-11-18 16:01:38,152:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0470 (0.0460)
+2022-11-18 16:01:38,229:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0442 (0.0459)
+2022-11-18 16:01:38,310:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0454 (0.0458)
+2022-11-18 16:01:38,395:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0484 (0.0460)
+2022-11-18 16:01:38,480:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0493 (0.0462)
+2022-11-18 16:01:38,564:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0442 (0.0461)
+2022-11-18 16:01:38,639:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0494 (0.0463)
+2022-11-18 16:01:38,686:INFO: - Computing ADE (validation)
+2022-11-18 16:01:38,989:INFO: 		 ADE on hotel                     dataset:	 1.1661529541015625
+2022-11-18 16:01:39,405:INFO: 		 ADE on univ                      dataset:	 1.4133442640304565
+2022-11-18 16:01:39,723:INFO: 		 ADE on zara1                     dataset:	 2.0852949619293213
+2022-11-18 16:01:40,261:INFO: 		 ADE on zara2                     dataset:	 1.346781849861145
+2022-11-18 16:01:40,261:INFO: Average validation:	ADE  1.4145	FDE  2.5778
+2022-11-18 16:01:40,275:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_652.pth.tar
+2022-11-18 16:01:40,275:INFO: 
+===> EPOCH: 653 (P3)
+2022-11-18 16:01:40,275:INFO: - Computing loss (training)
+2022-11-18 16:01:40,535:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0455 (0.0455)
+2022-11-18 16:01:40,604:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0435 (0.0445)
+2022-11-18 16:01:40,669:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0554 (0.0480)
+2022-11-18 16:01:40,719:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0433 (0.0472)
+2022-11-18 16:01:41,041:INFO: Dataset: univ                Batch:  1/15	Loss 0.0355 (0.0355)
+2022-11-18 16:01:41,124:INFO: Dataset: univ                Batch:  2/15	Loss 0.0335 (0.0345)
+2022-11-18 16:01:41,200:INFO: Dataset: univ                Batch:  3/15	Loss 0.0328 (0.0339)
+2022-11-18 16:01:41,282:INFO: Dataset: univ                Batch:  4/15	Loss 0.0296 (0.0328)
+2022-11-18 16:01:41,358:INFO: Dataset: univ                Batch:  5/15	Loss 0.0310 (0.0324)
+2022-11-18 16:01:41,434:INFO: Dataset: univ                Batch:  6/15	Loss 0.0289 (0.0318)
+2022-11-18 16:01:41,511:INFO: Dataset: univ                Batch:  7/15	Loss 0.0327 (0.0319)
+2022-11-18 16:01:41,585:INFO: Dataset: univ                Batch:  8/15	Loss 0.0335 (0.0321)
+2022-11-18 16:01:41,659:INFO: Dataset: univ                Batch:  9/15	Loss 0.0302 (0.0319)
+2022-11-18 16:01:41,739:INFO: Dataset: univ                Batch: 10/15	Loss 0.0331 (0.0320)
+2022-11-18 16:01:41,822:INFO: Dataset: univ                Batch: 11/15	Loss 0.0331 (0.0321)
+2022-11-18 16:01:41,905:INFO: Dataset: univ                Batch: 12/15	Loss 0.0324 (0.0321)
+2022-11-18 16:01:41,984:INFO: Dataset: univ                Batch: 13/15	Loss 0.0325 (0.0322)
+2022-11-18 16:01:42,072:INFO: Dataset: univ                Batch: 14/15	Loss 0.0320 (0.0322)
+2022-11-18 16:01:42,109:INFO: Dataset: univ                Batch: 15/15	Loss 0.0341 (0.0322)
+2022-11-18 16:01:42,472:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0653 (0.0653)
+2022-11-18 16:01:42,559:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0639 (0.0646)
+2022-11-18 16:01:42,638:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0657 (0.0649)
+2022-11-18 16:01:42,711:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0641 (0.0647)
+2022-11-18 16:01:42,782:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0605 (0.0639)
+2022-11-18 16:01:42,853:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0646 (0.0641)
+2022-11-18 16:01:42,924:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0672 (0.0645)
+2022-11-18 16:01:42,989:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0606 (0.0641)
+2022-11-18 16:01:43,302:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0459 (0.0459)
+2022-11-18 16:01:43,371:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0442 (0.0450)
+2022-11-18 16:01:43,440:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0424 (0.0442)
+2022-11-18 16:01:43,508:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0404 (0.0433)
+2022-11-18 16:01:43,577:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0474 (0.0442)
+2022-11-18 16:01:43,653:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0394 (0.0434)
+2022-11-18 16:01:43,727:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0498 (0.0444)
+2022-11-18 16:01:43,801:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0484 (0.0449)
+2022-11-18 16:01:43,873:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0492 (0.0454)
+2022-11-18 16:01:43,946:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0415 (0.0450)
+2022-11-18 16:01:44,018:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0392 (0.0445)
+2022-11-18 16:01:44,087:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0482 (0.0448)
+2022-11-18 16:01:44,156:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0509 (0.0453)
+2022-11-18 16:01:44,227:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0468 (0.0454)
+2022-11-18 16:01:44,302:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0463 (0.0455)
+2022-11-18 16:01:44,374:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0421 (0.0453)
+2022-11-18 16:01:44,444:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0473 (0.0454)
+2022-11-18 16:01:44,507:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0425 (0.0452)
+2022-11-18 16:01:44,553:INFO: - Computing ADE (validation)
+2022-11-18 16:01:44,855:INFO: 		 ADE on hotel                     dataset:	 1.1677287817001343
+2022-11-18 16:01:45,272:INFO: 		 ADE on univ                      dataset:	 1.4011602401733398
+2022-11-18 16:01:45,596:INFO: 		 ADE on zara1                     dataset:	 2.0269532203674316
+2022-11-18 16:01:46,100:INFO: 		 ADE on zara2                     dataset:	 1.3378201723098755
+2022-11-18 16:01:46,100:INFO: Average validation:	ADE  1.4015	FDE  2.5535
+2022-11-18 16:01:46,113:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_653.pth.tar
+2022-11-18 16:01:46,113:INFO: 
+===> EPOCH: 654 (P3)
+2022-11-18 16:01:46,114:INFO: - Computing loss (training)
+2022-11-18 16:01:46,406:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0564 (0.0564)
+2022-11-18 16:01:46,477:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0412 (0.0487)
+2022-11-18 16:01:46,554:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0421 (0.0465)
+2022-11-18 16:01:46,610:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0491 (0.0470)
+2022-11-18 16:01:46,942:INFO: Dataset: univ                Batch:  1/15	Loss 0.0301 (0.0301)
+2022-11-18 16:01:47,018:INFO: Dataset: univ                Batch:  2/15	Loss 0.0334 (0.0317)
+2022-11-18 16:01:47,095:INFO: Dataset: univ                Batch:  3/15	Loss 0.0309 (0.0314)
+2022-11-18 16:01:47,170:INFO: Dataset: univ                Batch:  4/15	Loss 0.0323 (0.0316)
+2022-11-18 16:01:47,248:INFO: Dataset: univ                Batch:  5/15	Loss 0.0308 (0.0315)
+2022-11-18 16:01:47,322:INFO: Dataset: univ                Batch:  6/15	Loss 0.0319 (0.0315)
+2022-11-18 16:01:47,394:INFO: Dataset: univ                Batch:  7/15	Loss 0.0324 (0.0317)
+2022-11-18 16:01:47,467:INFO: Dataset: univ                Batch:  8/15	Loss 0.0306 (0.0315)
+2022-11-18 16:01:47,538:INFO: Dataset: univ                Batch:  9/15	Loss 0.0319 (0.0316)
+2022-11-18 16:01:47,613:INFO: Dataset: univ                Batch: 10/15	Loss 0.0308 (0.0315)
+2022-11-18 16:01:47,694:INFO: Dataset: univ                Batch: 11/15	Loss 0.0332 (0.0316)
+2022-11-18 16:01:47,775:INFO: Dataset: univ                Batch: 12/15	Loss 0.0316 (0.0316)
+2022-11-18 16:01:47,858:INFO: Dataset: univ                Batch: 13/15	Loss 0.0316 (0.0316)
+2022-11-18 16:01:47,932:INFO: Dataset: univ                Batch: 14/15	Loss 0.0342 (0.0318)
+2022-11-18 16:01:47,965:INFO: Dataset: univ                Batch: 15/15	Loss 0.0348 (0.0318)
+2022-11-18 16:01:48,267:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0597 (0.0597)
+2022-11-18 16:01:48,333:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0687 (0.0640)
+2022-11-18 16:01:48,399:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0612 (0.0631)
+2022-11-18 16:01:48,467:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0584 (0.0620)
+2022-11-18 16:01:48,537:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0649 (0.0626)
+2022-11-18 16:01:48,605:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0632 (0.0627)
+2022-11-18 16:01:48,670:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0646 (0.0630)
+2022-11-18 16:01:48,729:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0608 (0.0627)
+2022-11-18 16:01:49,076:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0452 (0.0452)
+2022-11-18 16:01:49,147:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0491 (0.0472)
+2022-11-18 16:01:49,215:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0352 (0.0432)
+2022-11-18 16:01:49,281:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0441 (0.0434)
+2022-11-18 16:01:49,349:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0448 (0.0437)
+2022-11-18 16:01:49,420:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0441 (0.0438)
+2022-11-18 16:01:49,487:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0422 (0.0436)
+2022-11-18 16:01:49,555:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0491 (0.0443)
+2022-11-18 16:01:49,621:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0449 (0.0443)
+2022-11-18 16:01:49,687:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0462 (0.0445)
+2022-11-18 16:01:49,758:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0457 (0.0446)
+2022-11-18 16:01:49,843:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0449 (0.0446)
+2022-11-18 16:01:49,920:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0410 (0.0444)
+2022-11-18 16:01:49,995:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0457 (0.0445)
+2022-11-18 16:01:50,067:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0461 (0.0446)
+2022-11-18 16:01:50,139:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0459 (0.0446)
+2022-11-18 16:01:50,216:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0410 (0.0444)
+2022-11-18 16:01:50,291:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0434 (0.0444)
+2022-11-18 16:01:50,352:INFO: - Computing ADE (validation)
+2022-11-18 16:01:50,672:INFO: 		 ADE on hotel                     dataset:	 1.1535134315490723
+2022-11-18 16:01:51,098:INFO: 		 ADE on univ                      dataset:	 1.3775498867034912
+2022-11-18 16:01:51,490:INFO: 		 ADE on zara1                     dataset:	 2.024202346801758
+2022-11-18 16:01:51,998:INFO: 		 ADE on zara2                     dataset:	 1.3057059049606323
+2022-11-18 16:01:51,998:INFO: Average validation:	ADE  1.3765	FDE  2.5107
+2022-11-18 16:01:52,012:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_654.pth.tar
+2022-11-18 16:01:52,012:INFO: 
+===> EPOCH: 655 (P3)
+2022-11-18 16:01:52,012:INFO: - Computing loss (training)
+2022-11-18 16:01:52,266:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0482 (0.0482)
+2022-11-18 16:01:52,340:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0471 (0.0476)
+2022-11-18 16:01:52,421:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0519 (0.0490)
+2022-11-18 16:01:52,476:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0389 (0.0471)
+2022-11-18 16:01:52,820:INFO: Dataset: univ                Batch:  1/15	Loss 0.0333 (0.0333)
+2022-11-18 16:01:52,911:INFO: Dataset: univ                Batch:  2/15	Loss 0.0315 (0.0325)
+2022-11-18 16:01:53,004:INFO: Dataset: univ                Batch:  3/15	Loss 0.0308 (0.0319)
+2022-11-18 16:01:53,092:INFO: Dataset: univ                Batch:  4/15	Loss 0.0308 (0.0316)
+2022-11-18 16:01:53,189:INFO: Dataset: univ                Batch:  5/15	Loss 0.0298 (0.0312)
+2022-11-18 16:01:53,283:INFO: Dataset: univ                Batch:  6/15	Loss 0.0326 (0.0314)
+2022-11-18 16:01:53,372:INFO: Dataset: univ                Batch:  7/15	Loss 0.0322 (0.0316)
+2022-11-18 16:01:53,453:INFO: Dataset: univ                Batch:  8/15	Loss 0.0318 (0.0316)
+2022-11-18 16:01:53,537:INFO: Dataset: univ                Batch:  9/15	Loss 0.0309 (0.0315)
+2022-11-18 16:01:53,628:INFO: Dataset: univ                Batch: 10/15	Loss 0.0308 (0.0314)
+2022-11-18 16:01:53,712:INFO: Dataset: univ                Batch: 11/15	Loss 0.0287 (0.0312)
+2022-11-18 16:01:53,801:INFO: Dataset: univ                Batch: 12/15	Loss 0.0313 (0.0312)
+2022-11-18 16:01:53,883:INFO: Dataset: univ                Batch: 13/15	Loss 0.0339 (0.0314)
+2022-11-18 16:01:53,968:INFO: Dataset: univ                Batch: 14/15	Loss 0.0316 (0.0314)
+2022-11-18 16:01:54,006:INFO: Dataset: univ                Batch: 15/15	Loss 0.0295 (0.0314)
+2022-11-18 16:01:54,360:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0613 (0.0613)
+2022-11-18 16:01:54,444:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0582 (0.0596)
+2022-11-18 16:01:54,527:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0623 (0.0604)
+2022-11-18 16:01:54,609:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0607 (0.0605)
+2022-11-18 16:01:54,688:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0540 (0.0593)
+2022-11-18 16:01:54,771:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0650 (0.0601)
+2022-11-18 16:01:54,853:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0597 (0.0601)
+2022-11-18 16:01:54,932:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0661 (0.0608)
+2022-11-18 16:01:55,270:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0419 (0.0419)
+2022-11-18 16:01:55,348:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0377 (0.0397)
+2022-11-18 16:01:55,421:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0471 (0.0423)
+2022-11-18 16:01:55,498:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0427 (0.0424)
+2022-11-18 16:01:55,582:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0428 (0.0425)
+2022-11-18 16:01:55,665:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0432 (0.0426)
+2022-11-18 16:01:55,740:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0440 (0.0428)
+2022-11-18 16:01:55,819:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0383 (0.0422)
+2022-11-18 16:01:55,901:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0429 (0.0423)
+2022-11-18 16:01:55,979:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0426 (0.0423)
+2022-11-18 16:01:56,053:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0532 (0.0433)
+2022-11-18 16:01:56,128:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0434 (0.0433)
+2022-11-18 16:01:56,207:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0431 (0.0433)
+2022-11-18 16:01:56,288:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0447 (0.0434)
+2022-11-18 16:01:56,365:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0384 (0.0430)
+2022-11-18 16:01:56,449:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0449 (0.0431)
+2022-11-18 16:01:56,528:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0390 (0.0429)
+2022-11-18 16:01:56,599:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0450 (0.0430)
+2022-11-18 16:01:56,657:INFO: - Computing ADE (validation)
+2022-11-18 16:01:56,987:INFO: 		 ADE on hotel                     dataset:	 1.176594614982605
+2022-11-18 16:01:57,380:INFO: 		 ADE on univ                      dataset:	 1.3740283250808716
+2022-11-18 16:01:57,673:INFO: 		 ADE on zara1                     dataset:	 1.9428939819335938
+2022-11-18 16:01:58,172:INFO: 		 ADE on zara2                     dataset:	 1.2908045053482056
+2022-11-18 16:01:58,172:INFO: Average validation:	ADE  1.3658	FDE  2.4912
+2022-11-18 16:01:58,187:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_655.pth.tar
+2022-11-18 16:01:58,188:INFO: 
+===> EPOCH: 656 (P3)
+2022-11-18 16:01:58,189:INFO: - Computing loss (training)
+2022-11-18 16:01:58,489:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0486 (0.0486)
+2022-11-18 16:01:58,560:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0517 (0.0501)
+2022-11-18 16:01:58,630:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0395 (0.0466)
+2022-11-18 16:01:58,682:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0470 (0.0466)
+2022-11-18 16:01:59,061:INFO: Dataset: univ                Batch:  1/15	Loss 0.0303 (0.0303)
+2022-11-18 16:01:59,147:INFO: Dataset: univ                Batch:  2/15	Loss 0.0298 (0.0300)
+2022-11-18 16:01:59,247:INFO: Dataset: univ                Batch:  3/15	Loss 0.0332 (0.0311)
+2022-11-18 16:01:59,340:INFO: Dataset: univ                Batch:  4/15	Loss 0.0285 (0.0304)
+2022-11-18 16:01:59,421:INFO: Dataset: univ                Batch:  5/15	Loss 0.0314 (0.0306)
+2022-11-18 16:01:59,504:INFO: Dataset: univ                Batch:  6/15	Loss 0.0323 (0.0309)
+2022-11-18 16:01:59,588:INFO: Dataset: univ                Batch:  7/15	Loss 0.0302 (0.0308)
+2022-11-18 16:01:59,669:INFO: Dataset: univ                Batch:  8/15	Loss 0.0312 (0.0308)
+2022-11-18 16:01:59,744:INFO: Dataset: univ                Batch:  9/15	Loss 0.0306 (0.0308)
+2022-11-18 16:01:59,821:INFO: Dataset: univ                Batch: 10/15	Loss 0.0314 (0.0309)
+2022-11-18 16:01:59,896:INFO: Dataset: univ                Batch: 11/15	Loss 0.0276 (0.0306)
+2022-11-18 16:01:59,980:INFO: Dataset: univ                Batch: 12/15	Loss 0.0327 (0.0307)
+2022-11-18 16:02:00,065:INFO: Dataset: univ                Batch: 13/15	Loss 0.0327 (0.0309)
+2022-11-18 16:02:00,152:INFO: Dataset: univ                Batch: 14/15	Loss 0.0323 (0.0310)
+2022-11-18 16:02:00,193:INFO: Dataset: univ                Batch: 15/15	Loss 0.0262 (0.0309)
+2022-11-18 16:02:00,522:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0634 (0.0634)
+2022-11-18 16:02:00,591:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0560 (0.0599)
+2022-11-18 16:02:00,659:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0581 (0.0594)
+2022-11-18 16:02:00,728:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0614 (0.0598)
+2022-11-18 16:02:00,796:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0588 (0.0596)
+2022-11-18 16:02:00,863:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0575 (0.0592)
+2022-11-18 16:02:00,929:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0570 (0.0589)
+2022-11-18 16:02:00,990:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0590 (0.0589)
+2022-11-18 16:02:01,328:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0395 (0.0395)
+2022-11-18 16:02:01,400:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0442 (0.0417)
+2022-11-18 16:02:01,468:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0457 (0.0430)
+2022-11-18 16:02:01,536:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0409 (0.0425)
+2022-11-18 16:02:01,608:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0406 (0.0421)
+2022-11-18 16:02:01,679:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0413 (0.0420)
+2022-11-18 16:02:01,748:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0443 (0.0423)
+2022-11-18 16:02:01,817:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0425 (0.0424)
+2022-11-18 16:02:01,887:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0437 (0.0425)
+2022-11-18 16:02:01,959:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0412 (0.0424)
+2022-11-18 16:02:02,030:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0441 (0.0425)
+2022-11-18 16:02:02,102:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0398 (0.0423)
+2022-11-18 16:02:02,174:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0424 (0.0423)
+2022-11-18 16:02:02,252:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0421 (0.0423)
+2022-11-18 16:02:02,331:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0389 (0.0421)
+2022-11-18 16:02:02,414:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0430 (0.0421)
+2022-11-18 16:02:02,498:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0409 (0.0420)
+2022-11-18 16:02:02,571:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0367 (0.0418)
+2022-11-18 16:02:02,621:INFO: - Computing ADE (validation)
+2022-11-18 16:02:02,952:INFO: 		 ADE on hotel                     dataset:	 1.1643180847167969
+2022-11-18 16:02:03,382:INFO: 		 ADE on univ                      dataset:	 1.370867371559143
+2022-11-18 16:02:03,711:INFO: 		 ADE on zara1                     dataset:	 1.9202325344085693
+2022-11-18 16:02:04,248:INFO: 		 ADE on zara2                     dataset:	 1.2978861331939697
+2022-11-18 16:02:04,248:INFO: Average validation:	ADE  1.3647	FDE  2.4886
+2022-11-18 16:02:04,263:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_656.pth.tar
+2022-11-18 16:02:04,263:INFO: 
+===> EPOCH: 657 (P3)
+2022-11-18 16:02:04,263:INFO: - Computing loss (training)
+2022-11-18 16:02:04,541:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0434 (0.0434)
+2022-11-18 16:02:04,610:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0408 (0.0421)
+2022-11-18 16:02:04,678:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0573 (0.0468)
+2022-11-18 16:02:04,734:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0463 (0.0467)
+2022-11-18 16:02:05,088:INFO: Dataset: univ                Batch:  1/15	Loss 0.0324 (0.0324)
+2022-11-18 16:02:05,170:INFO: Dataset: univ                Batch:  2/15	Loss 0.0330 (0.0327)
+2022-11-18 16:02:05,251:INFO: Dataset: univ                Batch:  3/15	Loss 0.0335 (0.0330)
+2022-11-18 16:02:05,336:INFO: Dataset: univ                Batch:  4/15	Loss 0.0322 (0.0328)
+2022-11-18 16:02:05,419:INFO: Dataset: univ                Batch:  5/15	Loss 0.0296 (0.0321)
+2022-11-18 16:02:05,504:INFO: Dataset: univ                Batch:  6/15	Loss 0.0290 (0.0314)
+2022-11-18 16:02:05,581:INFO: Dataset: univ                Batch:  7/15	Loss 0.0315 (0.0315)
+2022-11-18 16:02:05,659:INFO: Dataset: univ                Batch:  8/15	Loss 0.0265 (0.0307)
+2022-11-18 16:02:05,735:INFO: Dataset: univ                Batch:  9/15	Loss 0.0304 (0.0307)
+2022-11-18 16:02:05,811:INFO: Dataset: univ                Batch: 10/15	Loss 0.0302 (0.0306)
+2022-11-18 16:02:05,918:INFO: Dataset: univ                Batch: 11/15	Loss 0.0302 (0.0306)
+2022-11-18 16:02:06,008:INFO: Dataset: univ                Batch: 12/15	Loss 0.0292 (0.0305)
+2022-11-18 16:02:06,099:INFO: Dataset: univ                Batch: 13/15	Loss 0.0315 (0.0306)
+2022-11-18 16:02:06,195:INFO: Dataset: univ                Batch: 14/15	Loss 0.0291 (0.0305)
+2022-11-18 16:02:06,240:INFO: Dataset: univ                Batch: 15/15	Loss 0.0268 (0.0304)
+2022-11-18 16:02:06,636:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0572 (0.0572)
+2022-11-18 16:02:06,710:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0622 (0.0596)
+2022-11-18 16:02:06,782:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0557 (0.0582)
+2022-11-18 16:02:06,852:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0547 (0.0573)
+2022-11-18 16:02:06,924:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0569 (0.0572)
+2022-11-18 16:02:06,995:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0598 (0.0576)
+2022-11-18 16:02:07,068:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0557 (0.0573)
+2022-11-18 16:02:07,132:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0529 (0.0569)
+2022-11-18 16:02:07,450:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0407 (0.0407)
+2022-11-18 16:02:07,529:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0416 (0.0412)
+2022-11-18 16:02:07,622:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0424 (0.0416)
+2022-11-18 16:02:07,716:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0464 (0.0427)
+2022-11-18 16:02:07,803:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0385 (0.0419)
+2022-11-18 16:02:07,875:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0408 (0.0417)
+2022-11-18 16:02:07,944:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0392 (0.0414)
+2022-11-18 16:02:08,012:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0402 (0.0412)
+2022-11-18 16:02:08,085:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0400 (0.0411)
+2022-11-18 16:02:08,164:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0392 (0.0409)
+2022-11-18 16:02:08,244:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0461 (0.0414)
+2022-11-18 16:02:08,317:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0387 (0.0412)
+2022-11-18 16:02:08,389:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0397 (0.0411)
+2022-11-18 16:02:08,460:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0396 (0.0410)
+2022-11-18 16:02:08,532:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0407 (0.0409)
+2022-11-18 16:02:08,604:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0375 (0.0407)
+2022-11-18 16:02:08,681:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0375 (0.0405)
+2022-11-18 16:02:08,750:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0381 (0.0404)
+2022-11-18 16:02:08,797:INFO: - Computing ADE (validation)
+2022-11-18 16:02:09,102:INFO: 		 ADE on hotel                     dataset:	 1.1798206567764282
+2022-11-18 16:02:09,499:INFO: 		 ADE on univ                      dataset:	 1.35552978515625
+2022-11-18 16:02:09,809:INFO: 		 ADE on zara1                     dataset:	 1.8428075313568115
+2022-11-18 16:02:10,387:INFO: 		 ADE on zara2                     dataset:	 1.27164626121521
+2022-11-18 16:02:10,387:INFO: Average validation:	ADE  1.3435	FDE  2.4511
+2022-11-18 16:02:10,401:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_657.pth.tar
+2022-11-18 16:02:10,401:INFO: 
+===> EPOCH: 658 (P3)
+2022-11-18 16:02:10,402:INFO: - Computing loss (training)
+2022-11-18 16:02:10,872:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0448 (0.0448)
+2022-11-18 16:02:10,954:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0443 (0.0446)
+2022-11-18 16:02:11,025:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0470 (0.0454)
+2022-11-18 16:02:11,081:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0521 (0.0466)
+2022-11-18 16:02:11,413:INFO: Dataset: univ                Batch:  1/15	Loss 0.0301 (0.0301)
+2022-11-18 16:02:11,490:INFO: Dataset: univ                Batch:  2/15	Loss 0.0297 (0.0299)
+2022-11-18 16:02:11,568:INFO: Dataset: univ                Batch:  3/15	Loss 0.0314 (0.0304)
+2022-11-18 16:02:11,648:INFO: Dataset: univ                Batch:  4/15	Loss 0.0290 (0.0301)
+2022-11-18 16:02:11,726:INFO: Dataset: univ                Batch:  5/15	Loss 0.0306 (0.0302)
+2022-11-18 16:02:11,800:INFO: Dataset: univ                Batch:  6/15	Loss 0.0324 (0.0305)
+2022-11-18 16:02:11,874:INFO: Dataset: univ                Batch:  7/15	Loss 0.0299 (0.0304)
+2022-11-18 16:02:11,948:INFO: Dataset: univ                Batch:  8/15	Loss 0.0282 (0.0301)
+2022-11-18 16:02:12,020:INFO: Dataset: univ                Batch:  9/15	Loss 0.0302 (0.0302)
+2022-11-18 16:02:12,095:INFO: Dataset: univ                Batch: 10/15	Loss 0.0292 (0.0301)
+2022-11-18 16:02:12,171:INFO: Dataset: univ                Batch: 11/15	Loss 0.0306 (0.0301)
+2022-11-18 16:02:12,243:INFO: Dataset: univ                Batch: 12/15	Loss 0.0301 (0.0301)
+2022-11-18 16:02:12,318:INFO: Dataset: univ                Batch: 13/15	Loss 0.0297 (0.0301)
+2022-11-18 16:02:12,392:INFO: Dataset: univ                Batch: 14/15	Loss 0.0283 (0.0299)
+2022-11-18 16:02:12,424:INFO: Dataset: univ                Batch: 15/15	Loss 0.0266 (0.0299)
+2022-11-18 16:02:12,737:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0549 (0.0549)
+2022-11-18 16:02:12,806:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0588 (0.0568)
+2022-11-18 16:02:12,877:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0550 (0.0562)
+2022-11-18 16:02:12,947:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0505 (0.0548)
+2022-11-18 16:02:13,019:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0514 (0.0540)
+2022-11-18 16:02:13,090:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0546 (0.0541)
+2022-11-18 16:02:13,160:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0591 (0.0548)
+2022-11-18 16:02:13,223:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0588 (0.0552)
+2022-11-18 16:02:13,533:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0431 (0.0431)
+2022-11-18 16:02:13,603:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0407 (0.0418)
+2022-11-18 16:02:13,673:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0361 (0.0399)
+2022-11-18 16:02:13,747:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0389 (0.0396)
+2022-11-18 16:02:13,818:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0345 (0.0386)
+2022-11-18 16:02:13,891:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0400 (0.0389)
+2022-11-18 16:02:13,962:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0399 (0.0390)
+2022-11-18 16:02:14,033:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0393 (0.0391)
+2022-11-18 16:02:14,102:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0411 (0.0393)
+2022-11-18 16:02:14,172:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0395 (0.0393)
+2022-11-18 16:02:14,245:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0397 (0.0394)
+2022-11-18 16:02:14,314:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0380 (0.0393)
+2022-11-18 16:02:14,384:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0397 (0.0393)
+2022-11-18 16:02:14,456:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0382 (0.0392)
+2022-11-18 16:02:14,526:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0408 (0.0393)
+2022-11-18 16:02:14,597:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0390 (0.0393)
+2022-11-18 16:02:14,669:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0376 (0.0392)
+2022-11-18 16:02:14,733:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0387 (0.0392)
+2022-11-18 16:02:14,777:INFO: - Computing ADE (validation)
+2022-11-18 16:02:15,074:INFO: 		 ADE on hotel                     dataset:	 1.1518125534057617
+2022-11-18 16:02:15,453:INFO: 		 ADE on univ                      dataset:	 1.338282823562622
+2022-11-18 16:02:15,765:INFO: 		 ADE on zara1                     dataset:	 1.8003898859024048
+2022-11-18 16:02:16,226:INFO: 		 ADE on zara2                     dataset:	 1.2606221437454224
+2022-11-18 16:02:16,226:INFO: Average validation:	ADE  1.3265	FDE  2.4166
+2022-11-18 16:02:16,238:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_658.pth.tar
+2022-11-18 16:02:16,239:INFO: 
+===> EPOCH: 659 (P3)
+2022-11-18 16:02:16,239:INFO: - Computing loss (training)
+2022-11-18 16:02:16,497:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0493 (0.0493)
+2022-11-18 16:02:16,566:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0448 (0.0471)
+2022-11-18 16:02:16,633:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0482 (0.0475)
+2022-11-18 16:02:16,683:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0410 (0.0463)
+2022-11-18 16:02:16,993:INFO: Dataset: univ                Batch:  1/15	Loss 0.0306 (0.0306)
+2022-11-18 16:02:17,076:INFO: Dataset: univ                Batch:  2/15	Loss 0.0307 (0.0306)
+2022-11-18 16:02:17,155:INFO: Dataset: univ                Batch:  3/15	Loss 0.0319 (0.0310)
+2022-11-18 16:02:17,232:INFO: Dataset: univ                Batch:  4/15	Loss 0.0285 (0.0304)
+2022-11-18 16:02:17,307:INFO: Dataset: univ                Batch:  5/15	Loss 0.0298 (0.0303)
+2022-11-18 16:02:17,387:INFO: Dataset: univ                Batch:  6/15	Loss 0.0284 (0.0300)
+2022-11-18 16:02:17,463:INFO: Dataset: univ                Batch:  7/15	Loss 0.0297 (0.0299)
+2022-11-18 16:02:17,538:INFO: Dataset: univ                Batch:  8/15	Loss 0.0296 (0.0299)
+2022-11-18 16:02:17,612:INFO: Dataset: univ                Batch:  9/15	Loss 0.0291 (0.0298)
+2022-11-18 16:02:17,689:INFO: Dataset: univ                Batch: 10/15	Loss 0.0280 (0.0296)
+2022-11-18 16:02:17,766:INFO: Dataset: univ                Batch: 11/15	Loss 0.0300 (0.0296)
+2022-11-18 16:02:17,840:INFO: Dataset: univ                Batch: 12/15	Loss 0.0289 (0.0296)
+2022-11-18 16:02:17,915:INFO: Dataset: univ                Batch: 13/15	Loss 0.0284 (0.0295)
+2022-11-18 16:02:17,993:INFO: Dataset: univ                Batch: 14/15	Loss 0.0276 (0.0293)
+2022-11-18 16:02:18,025:INFO: Dataset: univ                Batch: 15/15	Loss 0.0263 (0.0293)
+2022-11-18 16:02:18,335:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0527 (0.0527)
+2022-11-18 16:02:18,403:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0546 (0.0536)
+2022-11-18 16:02:18,471:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0546 (0.0539)
+2022-11-18 16:02:18,538:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0512 (0.0533)
+2022-11-18 16:02:18,606:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0510 (0.0528)
+2022-11-18 16:02:18,675:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0577 (0.0537)
+2022-11-18 16:02:18,742:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0501 (0.0532)
+2022-11-18 16:02:18,804:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0528 (0.0532)
+2022-11-18 16:02:19,120:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0401 (0.0401)
+2022-11-18 16:02:19,192:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0430 (0.0416)
+2022-11-18 16:02:19,266:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0365 (0.0398)
+2022-11-18 16:02:19,336:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0355 (0.0387)
+2022-11-18 16:02:19,406:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0364 (0.0382)
+2022-11-18 16:02:19,479:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0390 (0.0384)
+2022-11-18 16:02:19,548:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0330 (0.0376)
+2022-11-18 16:02:19,619:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0362 (0.0375)
+2022-11-18 16:02:19,688:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0353 (0.0372)
+2022-11-18 16:02:19,757:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0391 (0.0374)
+2022-11-18 16:02:19,829:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0422 (0.0378)
+2022-11-18 16:02:19,900:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0357 (0.0376)
+2022-11-18 16:02:19,969:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0358 (0.0375)
+2022-11-18 16:02:20,041:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0408 (0.0377)
+2022-11-18 16:02:20,111:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0361 (0.0376)
+2022-11-18 16:02:20,182:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0352 (0.0375)
+2022-11-18 16:02:20,254:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0397 (0.0376)
+2022-11-18 16:02:20,318:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0405 (0.0378)
+2022-11-18 16:02:20,360:INFO: - Computing ADE (validation)
+2022-11-18 16:02:20,639:INFO: 		 ADE on hotel                     dataset:	 1.170210599899292
+2022-11-18 16:02:21,013:INFO: 		 ADE on univ                      dataset:	 1.3084787130355835
+2022-11-18 16:02:21,314:INFO: 		 ADE on zara1                     dataset:	 1.7873508930206299
+2022-11-18 16:02:21,798:INFO: 		 ADE on zara2                     dataset:	 1.2235541343688965
+2022-11-18 16:02:21,799:INFO: Average validation:	ADE  1.2976	FDE  2.3580
+2022-11-18 16:02:21,811:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_659.pth.tar
+2022-11-18 16:02:21,811:INFO: 
+===> EPOCH: 660 (P3)
+2022-11-18 16:02:21,812:INFO: - Computing loss (training)
+2022-11-18 16:02:22,079:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0480 (0.0480)
+2022-11-18 16:02:22,150:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0445 (0.0463)
+2022-11-18 16:02:22,219:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0426 (0.0450)
+2022-11-18 16:02:22,271:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0513 (0.0461)
+2022-11-18 16:02:22,610:INFO: Dataset: univ                Batch:  1/15	Loss 0.0285 (0.0285)
+2022-11-18 16:02:22,693:INFO: Dataset: univ                Batch:  2/15	Loss 0.0299 (0.0292)
+2022-11-18 16:02:22,770:INFO: Dataset: univ                Batch:  3/15	Loss 0.0271 (0.0285)
+2022-11-18 16:02:22,851:INFO: Dataset: univ                Batch:  4/15	Loss 0.0273 (0.0282)
+2022-11-18 16:02:22,928:INFO: Dataset: univ                Batch:  5/15	Loss 0.0297 (0.0285)
+2022-11-18 16:02:23,010:INFO: Dataset: univ                Batch:  6/15	Loss 0.0281 (0.0284)
+2022-11-18 16:02:23,087:INFO: Dataset: univ                Batch:  7/15	Loss 0.0297 (0.0286)
+2022-11-18 16:02:23,164:INFO: Dataset: univ                Batch:  8/15	Loss 0.0288 (0.0286)
+2022-11-18 16:02:23,238:INFO: Dataset: univ                Batch:  9/15	Loss 0.0298 (0.0287)
+2022-11-18 16:02:23,317:INFO: Dataset: univ                Batch: 10/15	Loss 0.0297 (0.0288)
+2022-11-18 16:02:23,397:INFO: Dataset: univ                Batch: 11/15	Loss 0.0285 (0.0288)
+2022-11-18 16:02:23,473:INFO: Dataset: univ                Batch: 12/15	Loss 0.0279 (0.0287)
+2022-11-18 16:02:23,550:INFO: Dataset: univ                Batch: 13/15	Loss 0.0268 (0.0286)
+2022-11-18 16:02:23,630:INFO: Dataset: univ                Batch: 14/15	Loss 0.0298 (0.0286)
+2022-11-18 16:02:23,663:INFO: Dataset: univ                Batch: 15/15	Loss 0.0285 (0.0286)
+2022-11-18 16:02:23,984:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0511 (0.0511)
+2022-11-18 16:02:24,052:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0507 (0.0509)
+2022-11-18 16:02:24,123:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0528 (0.0515)
+2022-11-18 16:02:24,190:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0530 (0.0518)
+2022-11-18 16:02:24,260:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0478 (0.0510)
+2022-11-18 16:02:24,329:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0524 (0.0512)
+2022-11-18 16:02:24,397:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0502 (0.0511)
+2022-11-18 16:02:24,460:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0508 (0.0511)
+2022-11-18 16:02:24,772:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0351 (0.0351)
+2022-11-18 16:02:24,843:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0404 (0.0378)
+2022-11-18 16:02:24,916:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0376 (0.0378)
+2022-11-18 16:02:24,985:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0354 (0.0373)
+2022-11-18 16:02:25,058:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0357 (0.0369)
+2022-11-18 16:02:25,130:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0329 (0.0363)
+2022-11-18 16:02:25,199:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0392 (0.0367)
+2022-11-18 16:02:25,269:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0402 (0.0372)
+2022-11-18 16:02:25,337:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0358 (0.0370)
+2022-11-18 16:02:25,405:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0358 (0.0369)
+2022-11-18 16:02:25,475:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0366 (0.0369)
+2022-11-18 16:02:25,544:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0348 (0.0367)
+2022-11-18 16:02:25,613:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0348 (0.0366)
+2022-11-18 16:02:25,683:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0347 (0.0364)
+2022-11-18 16:02:25,753:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0363 (0.0364)
+2022-11-18 16:02:25,822:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0357 (0.0364)
+2022-11-18 16:02:25,893:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0355 (0.0363)
+2022-11-18 16:02:25,956:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0319 (0.0361)
+2022-11-18 16:02:25,999:INFO: - Computing ADE (validation)
+2022-11-18 16:02:26,288:INFO: 		 ADE on hotel                     dataset:	 1.1637375354766846
+2022-11-18 16:02:26,668:INFO: 		 ADE on univ                      dataset:	 1.2799209356307983
+2022-11-18 16:02:26,970:INFO: 		 ADE on zara1                     dataset:	 1.7284969091415405
+2022-11-18 16:02:27,455:INFO: 		 ADE on zara2                     dataset:	 1.2085391283035278
+2022-11-18 16:02:27,455:INFO: Average validation:	ADE  1.2735	FDE  2.3109
+2022-11-18 16:02:27,467:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_660.pth.tar
+2022-11-18 16:02:27,467:INFO: 
+===> EPOCH: 661 (P3)
+2022-11-18 16:02:27,468:INFO: - Computing loss (training)
+2022-11-18 16:02:27,727:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0450 (0.0450)
+2022-11-18 16:02:27,797:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0501 (0.0474)
+2022-11-18 16:02:27,880:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0427 (0.0457)
+2022-11-18 16:02:27,933:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0463 (0.0458)
+2022-11-18 16:02:28,256:INFO: Dataset: univ                Batch:  1/15	Loss 0.0317 (0.0317)
+2022-11-18 16:02:28,335:INFO: Dataset: univ                Batch:  2/15	Loss 0.0274 (0.0293)
+2022-11-18 16:02:28,417:INFO: Dataset: univ                Batch:  3/15	Loss 0.0284 (0.0290)
+2022-11-18 16:02:28,500:INFO: Dataset: univ                Batch:  4/15	Loss 0.0286 (0.0289)
+2022-11-18 16:02:28,577:INFO: Dataset: univ                Batch:  5/15	Loss 0.0271 (0.0285)
+2022-11-18 16:02:28,654:INFO: Dataset: univ                Batch:  6/15	Loss 0.0271 (0.0283)
+2022-11-18 16:02:28,731:INFO: Dataset: univ                Batch:  7/15	Loss 0.0283 (0.0283)
+2022-11-18 16:02:28,808:INFO: Dataset: univ                Batch:  8/15	Loss 0.0280 (0.0283)
+2022-11-18 16:02:28,884:INFO: Dataset: univ                Batch:  9/15	Loss 0.0269 (0.0281)
+2022-11-18 16:02:28,962:INFO: Dataset: univ                Batch: 10/15	Loss 0.0276 (0.0280)
+2022-11-18 16:02:29,038:INFO: Dataset: univ                Batch: 11/15	Loss 0.0266 (0.0279)
+2022-11-18 16:02:29,116:INFO: Dataset: univ                Batch: 12/15	Loss 0.0289 (0.0280)
+2022-11-18 16:02:29,193:INFO: Dataset: univ                Batch: 13/15	Loss 0.0286 (0.0280)
+2022-11-18 16:02:29,272:INFO: Dataset: univ                Batch: 14/15	Loss 0.0272 (0.0280)
+2022-11-18 16:02:29,305:INFO: Dataset: univ                Batch: 15/15	Loss 0.0282 (0.0280)
+2022-11-18 16:02:29,605:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0460 (0.0460)
+2022-11-18 16:02:29,673:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0490 (0.0475)
+2022-11-18 16:02:29,740:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0465 (0.0472)
+2022-11-18 16:02:29,807:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0487 (0.0476)
+2022-11-18 16:02:29,878:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0516 (0.0484)
+2022-11-18 16:02:29,946:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0500 (0.0487)
+2022-11-18 16:02:30,012:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0507 (0.0489)
+2022-11-18 16:02:30,074:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0474 (0.0488)
+2022-11-18 16:02:30,377:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0334 (0.0334)
+2022-11-18 16:02:30,448:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0384 (0.0359)
+2022-11-18 16:02:30,518:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0328 (0.0350)
+2022-11-18 16:02:30,588:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0340 (0.0348)
+2022-11-18 16:02:30,663:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0308 (0.0340)
+2022-11-18 16:02:30,733:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0349 (0.0341)
+2022-11-18 16:02:30,803:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0324 (0.0339)
+2022-11-18 16:02:30,872:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0341 (0.0339)
+2022-11-18 16:02:30,941:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0375 (0.0343)
+2022-11-18 16:02:31,010:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0311 (0.0340)
+2022-11-18 16:02:31,081:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0330 (0.0339)
+2022-11-18 16:02:31,151:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0355 (0.0341)
+2022-11-18 16:02:31,220:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0370 (0.0343)
+2022-11-18 16:02:31,291:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0364 (0.0345)
+2022-11-18 16:02:31,360:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0393 (0.0348)
+2022-11-18 16:02:31,430:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0328 (0.0347)
+2022-11-18 16:02:31,501:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0341 (0.0346)
+2022-11-18 16:02:31,565:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0303 (0.0344)
+2022-11-18 16:02:31,608:INFO: - Computing ADE (validation)
+2022-11-18 16:02:31,901:INFO: 		 ADE on hotel                     dataset:	 1.1628491878509521
+2022-11-18 16:02:32,276:INFO: 		 ADE on univ                      dataset:	 1.28032386302948
+2022-11-18 16:02:32,591:INFO: 		 ADE on zara1                     dataset:	 1.7560780048370361
+2022-11-18 16:02:33,076:INFO: 		 ADE on zara2                     dataset:	 1.1612523794174194
+2022-11-18 16:02:33,076:INFO: Average validation:	ADE  1.2579	FDE  2.2784
+2022-11-18 16:02:33,090:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_661.pth.tar
+2022-11-18 16:02:33,090:INFO: 
+===> EPOCH: 662 (P3)
+2022-11-18 16:02:33,090:INFO: - Computing loss (training)
+2022-11-18 16:02:33,356:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0499 (0.0499)
+2022-11-18 16:02:33,429:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0461 (0.0481)
+2022-11-18 16:02:33,500:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0442 (0.0468)
+2022-11-18 16:02:33,551:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0409 (0.0457)
+2022-11-18 16:02:33,887:INFO: Dataset: univ                Batch:  1/15	Loss 0.0279 (0.0279)
+2022-11-18 16:02:33,969:INFO: Dataset: univ                Batch:  2/15	Loss 0.0264 (0.0271)
+2022-11-18 16:02:34,049:INFO: Dataset: univ                Batch:  3/15	Loss 0.0260 (0.0268)
+2022-11-18 16:02:34,134:INFO: Dataset: univ                Batch:  4/15	Loss 0.0255 (0.0265)
+2022-11-18 16:02:34,210:INFO: Dataset: univ                Batch:  5/15	Loss 0.0294 (0.0270)
+2022-11-18 16:02:34,293:INFO: Dataset: univ                Batch:  6/15	Loss 0.0262 (0.0269)
+2022-11-18 16:02:34,373:INFO: Dataset: univ                Batch:  7/15	Loss 0.0263 (0.0268)
+2022-11-18 16:02:34,449:INFO: Dataset: univ                Batch:  8/15	Loss 0.0290 (0.0270)
+2022-11-18 16:02:34,526:INFO: Dataset: univ                Batch:  9/15	Loss 0.0267 (0.0270)
+2022-11-18 16:02:34,605:INFO: Dataset: univ                Batch: 10/15	Loss 0.0301 (0.0273)
+2022-11-18 16:02:34,681:INFO: Dataset: univ                Batch: 11/15	Loss 0.0281 (0.0273)
+2022-11-18 16:02:34,757:INFO: Dataset: univ                Batch: 12/15	Loss 0.0268 (0.0273)
+2022-11-18 16:02:34,836:INFO: Dataset: univ                Batch: 13/15	Loss 0.0280 (0.0273)
+2022-11-18 16:02:34,912:INFO: Dataset: univ                Batch: 14/15	Loss 0.0256 (0.0272)
+2022-11-18 16:02:34,946:INFO: Dataset: univ                Batch: 15/15	Loss 0.0284 (0.0272)
+2022-11-18 16:02:35,259:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0481 (0.0481)
+2022-11-18 16:02:35,328:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0468 (0.0474)
+2022-11-18 16:02:35,396:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0436 (0.0461)
+2022-11-18 16:02:35,467:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0484 (0.0466)
+2022-11-18 16:02:35,535:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0457 (0.0464)
+2022-11-18 16:02:35,603:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0486 (0.0468)
+2022-11-18 16:02:35,669:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0442 (0.0464)
+2022-11-18 16:02:35,731:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0459 (0.0463)
+2022-11-18 16:02:36,045:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0370 (0.0370)
+2022-11-18 16:02:36,119:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0330 (0.0350)
+2022-11-18 16:02:36,188:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0320 (0.0341)
+2022-11-18 16:02:36,262:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0297 (0.0330)
+2022-11-18 16:02:36,332:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0321 (0.0329)
+2022-11-18 16:02:36,405:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0352 (0.0333)
+2022-11-18 16:02:36,475:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0303 (0.0328)
+2022-11-18 16:02:36,545:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0326 (0.0328)
+2022-11-18 16:02:36,614:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0312 (0.0326)
+2022-11-18 16:02:36,683:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0319 (0.0325)
+2022-11-18 16:02:36,754:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0319 (0.0325)
+2022-11-18 16:02:36,825:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0360 (0.0327)
+2022-11-18 16:02:36,894:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0318 (0.0326)
+2022-11-18 16:02:36,965:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0310 (0.0325)
+2022-11-18 16:02:37,035:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0327 (0.0325)
+2022-11-18 16:02:37,106:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0345 (0.0327)
+2022-11-18 16:02:37,177:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0390 (0.0330)
+2022-11-18 16:02:37,241:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0318 (0.0330)
+2022-11-18 16:02:37,284:INFO: - Computing ADE (validation)
+2022-11-18 16:02:37,567:INFO: 		 ADE on hotel                     dataset:	 1.156980276107788
+2022-11-18 16:02:37,955:INFO: 		 ADE on univ                      dataset:	 1.2599496841430664
+2022-11-18 16:02:38,277:INFO: 		 ADE on zara1                     dataset:	 1.693756341934204
+2022-11-18 16:02:38,762:INFO: 		 ADE on zara2                     dataset:	 1.1194959878921509
+2022-11-18 16:02:38,762:INFO: Average validation:	ADE  1.2280	FDE  2.2168
+2022-11-18 16:02:38,775:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_662.pth.tar
+2022-11-18 16:02:38,775:INFO: 
+===> EPOCH: 663 (P3)
+2022-11-18 16:02:38,776:INFO: - Computing loss (training)
+2022-11-18 16:02:39,045:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0445 (0.0445)
+2022-11-18 16:02:39,116:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0388 (0.0417)
+2022-11-18 16:02:39,184:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0450 (0.0428)
+2022-11-18 16:02:39,235:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0590 (0.0453)
+2022-11-18 16:02:39,601:INFO: Dataset: univ                Batch:  1/15	Loss 0.0251 (0.0251)
+2022-11-18 16:02:39,678:INFO: Dataset: univ                Batch:  2/15	Loss 0.0279 (0.0265)
+2022-11-18 16:02:39,756:INFO: Dataset: univ                Batch:  3/15	Loss 0.0261 (0.0264)
+2022-11-18 16:02:39,834:INFO: Dataset: univ                Batch:  4/15	Loss 0.0268 (0.0265)
+2022-11-18 16:02:39,909:INFO: Dataset: univ                Batch:  5/15	Loss 0.0274 (0.0266)
+2022-11-18 16:02:39,988:INFO: Dataset: univ                Batch:  6/15	Loss 0.0262 (0.0266)
+2022-11-18 16:02:40,063:INFO: Dataset: univ                Batch:  7/15	Loss 0.0254 (0.0264)
+2022-11-18 16:02:40,139:INFO: Dataset: univ                Batch:  8/15	Loss 0.0257 (0.0263)
+2022-11-18 16:02:40,212:INFO: Dataset: univ                Batch:  9/15	Loss 0.0290 (0.0266)
+2022-11-18 16:02:40,287:INFO: Dataset: univ                Batch: 10/15	Loss 0.0279 (0.0267)
+2022-11-18 16:02:40,361:INFO: Dataset: univ                Batch: 11/15	Loss 0.0258 (0.0266)
+2022-11-18 16:02:40,438:INFO: Dataset: univ                Batch: 12/15	Loss 0.0274 (0.0267)
+2022-11-18 16:02:40,514:INFO: Dataset: univ                Batch: 13/15	Loss 0.0256 (0.0266)
+2022-11-18 16:02:40,590:INFO: Dataset: univ                Batch: 14/15	Loss 0.0255 (0.0265)
+2022-11-18 16:02:40,623:INFO: Dataset: univ                Batch: 15/15	Loss 0.0298 (0.0266)
+2022-11-18 16:02:40,956:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0455 (0.0455)
+2022-11-18 16:02:41,024:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0396 (0.0425)
+2022-11-18 16:02:41,091:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0468 (0.0439)
+2022-11-18 16:02:41,158:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0428 (0.0436)
+2022-11-18 16:02:41,226:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0450 (0.0438)
+2022-11-18 16:02:41,296:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0459 (0.0442)
+2022-11-18 16:02:41,362:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0460 (0.0444)
+2022-11-18 16:02:41,425:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0415 (0.0441)
+2022-11-18 16:02:41,729:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0318 (0.0318)
+2022-11-18 16:02:41,798:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0301 (0.0310)
+2022-11-18 16:02:41,868:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0317 (0.0312)
+2022-11-18 16:02:41,937:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0312 (0.0312)
+2022-11-18 16:02:42,005:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0304 (0.0310)
+2022-11-18 16:02:42,074:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0307 (0.0310)
+2022-11-18 16:02:42,139:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0334 (0.0313)
+2022-11-18 16:02:42,206:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0329 (0.0315)
+2022-11-18 16:02:42,272:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0358 (0.0320)
+2022-11-18 16:02:42,342:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0327 (0.0320)
+2022-11-18 16:02:42,412:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0332 (0.0321)
+2022-11-18 16:02:42,488:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0292 (0.0319)
+2022-11-18 16:02:42,560:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0289 (0.0317)
+2022-11-18 16:02:42,628:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0302 (0.0316)
+2022-11-18 16:02:42,695:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0301 (0.0315)
+2022-11-18 16:02:42,761:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0290 (0.0313)
+2022-11-18 16:02:42,829:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0306 (0.0312)
+2022-11-18 16:02:42,890:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0311 (0.0312)
+2022-11-18 16:02:42,943:INFO: - Computing ADE (validation)
+2022-11-18 16:02:43,228:INFO: 		 ADE on hotel                     dataset:	 1.1298679113388062
+2022-11-18 16:02:43,596:INFO: 		 ADE on univ                      dataset:	 1.2282601594924927
+2022-11-18 16:02:43,892:INFO: 		 ADE on zara1                     dataset:	 1.6154054403305054
+2022-11-18 16:02:44,358:INFO: 		 ADE on zara2                     dataset:	 1.1234325170516968
+2022-11-18 16:02:44,358:INFO: Average validation:	ADE  1.2069	FDE  2.1716
+2022-11-18 16:02:44,372:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_663.pth.tar
+2022-11-18 16:02:44,372:INFO: 
+===> EPOCH: 664 (P3)
+2022-11-18 16:02:44,372:INFO: - Computing loss (training)
+2022-11-18 16:02:44,647:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0416 (0.0416)
+2022-11-18 16:02:44,720:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0443 (0.0430)
+2022-11-18 16:02:44,790:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0458 (0.0439)
+2022-11-18 16:02:44,841:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0509 (0.0450)
+2022-11-18 16:02:45,170:INFO: Dataset: univ                Batch:  1/15	Loss 0.0243 (0.0243)
+2022-11-18 16:02:45,247:INFO: Dataset: univ                Batch:  2/15	Loss 0.0261 (0.0253)
+2022-11-18 16:02:45,323:INFO: Dataset: univ                Batch:  3/15	Loss 0.0260 (0.0255)
+2022-11-18 16:02:45,400:INFO: Dataset: univ                Batch:  4/15	Loss 0.0254 (0.0255)
+2022-11-18 16:02:45,475:INFO: Dataset: univ                Batch:  5/15	Loss 0.0268 (0.0258)
+2022-11-18 16:02:45,549:INFO: Dataset: univ                Batch:  6/15	Loss 0.0277 (0.0261)
+2022-11-18 16:02:45,622:INFO: Dataset: univ                Batch:  7/15	Loss 0.0261 (0.0261)
+2022-11-18 16:02:45,696:INFO: Dataset: univ                Batch:  8/15	Loss 0.0251 (0.0260)
+2022-11-18 16:02:45,767:INFO: Dataset: univ                Batch:  9/15	Loss 0.0268 (0.0260)
+2022-11-18 16:02:45,839:INFO: Dataset: univ                Batch: 10/15	Loss 0.0265 (0.0261)
+2022-11-18 16:02:45,913:INFO: Dataset: univ                Batch: 11/15	Loss 0.0253 (0.0260)
+2022-11-18 16:02:45,986:INFO: Dataset: univ                Batch: 12/15	Loss 0.0263 (0.0260)
+2022-11-18 16:02:46,059:INFO: Dataset: univ                Batch: 13/15	Loss 0.0246 (0.0259)
+2022-11-18 16:02:46,133:INFO: Dataset: univ                Batch: 14/15	Loss 0.0259 (0.0259)
+2022-11-18 16:02:46,165:INFO: Dataset: univ                Batch: 15/15	Loss 0.0252 (0.0259)
+2022-11-18 16:02:46,487:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0421 (0.0421)
+2022-11-18 16:02:46,556:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0398 (0.0411)
+2022-11-18 16:02:46,627:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0418 (0.0413)
+2022-11-18 16:02:46,699:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0441 (0.0421)
+2022-11-18 16:02:46,770:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0411 (0.0419)
+2022-11-18 16:02:46,841:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0425 (0.0420)
+2022-11-18 16:02:46,910:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0421 (0.0420)
+2022-11-18 16:02:46,973:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0394 (0.0417)
+2022-11-18 16:02:47,296:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0314 (0.0314)
+2022-11-18 16:02:47,367:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0290 (0.0302)
+2022-11-18 16:02:47,444:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0286 (0.0297)
+2022-11-18 16:02:47,514:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0335 (0.0307)
+2022-11-18 16:02:47,585:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0274 (0.0300)
+2022-11-18 16:02:47,658:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0319 (0.0303)
+2022-11-18 16:02:47,727:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0270 (0.0299)
+2022-11-18 16:02:47,797:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0266 (0.0295)
+2022-11-18 16:02:47,866:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0317 (0.0297)
+2022-11-18 16:02:47,935:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0281 (0.0296)
+2022-11-18 16:02:48,007:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0306 (0.0297)
+2022-11-18 16:02:48,076:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0312 (0.0298)
+2022-11-18 16:02:48,145:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0284 (0.0297)
+2022-11-18 16:02:48,216:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0293 (0.0297)
+2022-11-18 16:02:48,286:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0276 (0.0295)
+2022-11-18 16:02:48,356:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0281 (0.0294)
+2022-11-18 16:02:48,428:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0334 (0.0297)
+2022-11-18 16:02:48,493:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0322 (0.0298)
+2022-11-18 16:02:48,541:INFO: - Computing ADE (validation)
+2022-11-18 16:02:48,832:INFO: 		 ADE on hotel                     dataset:	 1.1339397430419922
+2022-11-18 16:02:49,228:INFO: 		 ADE on univ                      dataset:	 1.20688796043396
+2022-11-18 16:02:49,532:INFO: 		 ADE on zara1                     dataset:	 1.5663565397262573
+2022-11-18 16:02:50,038:INFO: 		 ADE on zara2                     dataset:	 1.0851294994354248
+2022-11-18 16:02:50,038:INFO: Average validation:	ADE  1.1791	FDE  2.1160
+2022-11-18 16:02:50,051:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_664.pth.tar
+2022-11-18 16:02:50,051:INFO: 
+===> EPOCH: 665 (P3)
+2022-11-18 16:02:50,052:INFO: - Computing loss (training)
+2022-11-18 16:02:50,309:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0451 (0.0451)
+2022-11-18 16:02:50,378:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0449 (0.0450)
+2022-11-18 16:02:50,445:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0373 (0.0424)
+2022-11-18 16:02:50,495:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0564 (0.0447)
+2022-11-18 16:02:50,819:INFO: Dataset: univ                Batch:  1/15	Loss 0.0254 (0.0254)
+2022-11-18 16:02:50,899:INFO: Dataset: univ                Batch:  2/15	Loss 0.0257 (0.0255)
+2022-11-18 16:02:50,979:INFO: Dataset: univ                Batch:  3/15	Loss 0.0250 (0.0253)
+2022-11-18 16:02:51,060:INFO: Dataset: univ                Batch:  4/15	Loss 0.0243 (0.0251)
+2022-11-18 16:02:51,141:INFO: Dataset: univ                Batch:  5/15	Loss 0.0238 (0.0249)
+2022-11-18 16:02:51,222:INFO: Dataset: univ                Batch:  6/15	Loss 0.0253 (0.0249)
+2022-11-18 16:02:51,302:INFO: Dataset: univ                Batch:  7/15	Loss 0.0269 (0.0252)
+2022-11-18 16:02:51,382:INFO: Dataset: univ                Batch:  8/15	Loss 0.0246 (0.0251)
+2022-11-18 16:02:51,460:INFO: Dataset: univ                Batch:  9/15	Loss 0.0255 (0.0252)
+2022-11-18 16:02:51,540:INFO: Dataset: univ                Batch: 10/15	Loss 0.0240 (0.0250)
+2022-11-18 16:02:51,619:INFO: Dataset: univ                Batch: 11/15	Loss 0.0260 (0.0251)
+2022-11-18 16:02:51,698:INFO: Dataset: univ                Batch: 12/15	Loss 0.0261 (0.0252)
+2022-11-18 16:02:51,778:INFO: Dataset: univ                Batch: 13/15	Loss 0.0250 (0.0252)
+2022-11-18 16:02:51,858:INFO: Dataset: univ                Batch: 14/15	Loss 0.0250 (0.0252)
+2022-11-18 16:02:51,894:INFO: Dataset: univ                Batch: 15/15	Loss 0.0244 (0.0252)
+2022-11-18 16:02:52,220:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0383 (0.0383)
+2022-11-18 16:02:52,293:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0401 (0.0392)
+2022-11-18 16:02:52,363:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0403 (0.0396)
+2022-11-18 16:02:52,433:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0414 (0.0400)
+2022-11-18 16:02:52,507:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0392 (0.0398)
+2022-11-18 16:02:52,577:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0400 (0.0399)
+2022-11-18 16:02:52,649:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0397 (0.0398)
+2022-11-18 16:02:52,713:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0378 (0.0396)
+2022-11-18 16:02:53,030:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0288 (0.0288)
+2022-11-18 16:02:53,100:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0282 (0.0285)
+2022-11-18 16:02:53,169:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0261 (0.0277)
+2022-11-18 16:02:53,243:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0276 (0.0277)
+2022-11-18 16:02:53,314:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0309 (0.0283)
+2022-11-18 16:02:53,384:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0265 (0.0280)
+2022-11-18 16:02:53,453:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0303 (0.0283)
+2022-11-18 16:02:53,523:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0290 (0.0284)
+2022-11-18 16:02:53,591:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0305 (0.0286)
+2022-11-18 16:02:53,659:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0258 (0.0283)
+2022-11-18 16:02:53,730:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0280 (0.0283)
+2022-11-18 16:02:53,799:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0292 (0.0284)
+2022-11-18 16:02:53,867:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0295 (0.0285)
+2022-11-18 16:02:53,938:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0287 (0.0285)
+2022-11-18 16:02:54,008:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0280 (0.0285)
+2022-11-18 16:02:54,077:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0268 (0.0284)
+2022-11-18 16:02:54,148:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0274 (0.0283)
+2022-11-18 16:02:54,211:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0280 (0.0283)
+2022-11-18 16:02:54,254:INFO: - Computing ADE (validation)
+2022-11-18 16:02:54,536:INFO: 		 ADE on hotel                     dataset:	 1.1024768352508545
+2022-11-18 16:02:54,907:INFO: 		 ADE on univ                      dataset:	 1.1838539838790894
+2022-11-18 16:02:55,210:INFO: 		 ADE on zara1                     dataset:	 1.5414303541183472
+2022-11-18 16:02:55,690:INFO: 		 ADE on zara2                     dataset:	 1.0541446208953857
+2022-11-18 16:02:55,690:INFO: Average validation:	ADE  1.1526	FDE  2.0576
+2022-11-18 16:02:55,702:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_665.pth.tar
+2022-11-18 16:02:55,702:INFO: 
+===> EPOCH: 666 (P3)
+2022-11-18 16:02:55,703:INFO: - Computing loss (training)
+2022-11-18 16:02:55,973:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0423 (0.0423)
+2022-11-18 16:02:56,040:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0507 (0.0463)
+2022-11-18 16:02:56,106:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0377 (0.0435)
+2022-11-18 16:02:56,155:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0493 (0.0444)
+2022-11-18 16:02:56,517:INFO: Dataset: univ                Batch:  1/15	Loss 0.0252 (0.0252)
+2022-11-18 16:02:56,597:INFO: Dataset: univ                Batch:  2/15	Loss 0.0243 (0.0248)
+2022-11-18 16:02:56,675:INFO: Dataset: univ                Batch:  3/15	Loss 0.0237 (0.0244)
+2022-11-18 16:02:56,754:INFO: Dataset: univ                Batch:  4/15	Loss 0.0253 (0.0246)
+2022-11-18 16:02:56,830:INFO: Dataset: univ                Batch:  5/15	Loss 0.0257 (0.0248)
+2022-11-18 16:02:56,912:INFO: Dataset: univ                Batch:  6/15	Loss 0.0243 (0.0247)
+2022-11-18 16:02:56,989:INFO: Dataset: univ                Batch:  7/15	Loss 0.0255 (0.0248)
+2022-11-18 16:02:57,065:INFO: Dataset: univ                Batch:  8/15	Loss 0.0233 (0.0246)
+2022-11-18 16:02:57,141:INFO: Dataset: univ                Batch:  9/15	Loss 0.0250 (0.0247)
+2022-11-18 16:02:57,218:INFO: Dataset: univ                Batch: 10/15	Loss 0.0249 (0.0247)
+2022-11-18 16:02:57,294:INFO: Dataset: univ                Batch: 11/15	Loss 0.0255 (0.0248)
+2022-11-18 16:02:57,372:INFO: Dataset: univ                Batch: 12/15	Loss 0.0245 (0.0247)
+2022-11-18 16:02:57,450:INFO: Dataset: univ                Batch: 13/15	Loss 0.0241 (0.0247)
+2022-11-18 16:02:57,528:INFO: Dataset: univ                Batch: 14/15	Loss 0.0230 (0.0246)
+2022-11-18 16:02:57,561:INFO: Dataset: univ                Batch: 15/15	Loss 0.0234 (0.0246)
+2022-11-18 16:02:57,865:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0364 (0.0364)
+2022-11-18 16:02:57,935:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0349 (0.0356)
+2022-11-18 16:02:58,004:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0369 (0.0361)
+2022-11-18 16:02:58,072:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0405 (0.0371)
+2022-11-18 16:02:58,141:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0367 (0.0370)
+2022-11-18 16:02:58,211:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0373 (0.0371)
+2022-11-18 16:02:58,280:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0387 (0.0373)
+2022-11-18 16:02:58,342:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0396 (0.0375)
+2022-11-18 16:02:58,659:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0292 (0.0292)
+2022-11-18 16:02:58,735:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0267 (0.0279)
+2022-11-18 16:02:58,808:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0325 (0.0294)
+2022-11-18 16:02:58,883:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0286 (0.0292)
+2022-11-18 16:02:58,960:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0259 (0.0285)
+2022-11-18 16:02:59,036:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0288 (0.0286)
+2022-11-18 16:02:59,109:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0279 (0.0285)
+2022-11-18 16:02:59,180:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0256 (0.0281)
+2022-11-18 16:02:59,251:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0283 (0.0281)
+2022-11-18 16:02:59,321:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0248 (0.0278)
+2022-11-18 16:02:59,393:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0279 (0.0278)
+2022-11-18 16:02:59,465:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0263 (0.0277)
+2022-11-18 16:02:59,536:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0273 (0.0276)
+2022-11-18 16:02:59,609:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0257 (0.0275)
+2022-11-18 16:02:59,681:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0258 (0.0274)
+2022-11-18 16:02:59,753:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0286 (0.0275)
+2022-11-18 16:02:59,827:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0221 (0.0272)
+2022-11-18 16:02:59,892:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0237 (0.0270)
+2022-11-18 16:02:59,937:INFO: - Computing ADE (validation)
+2022-11-18 16:03:00,236:INFO: 		 ADE on hotel                     dataset:	 1.0878044366836548
+2022-11-18 16:03:00,646:INFO: 		 ADE on univ                      dataset:	 1.168150544166565
+2022-11-18 16:03:00,941:INFO: 		 ADE on zara1                     dataset:	 1.475364327430725
+2022-11-18 16:03:01,438:INFO: 		 ADE on zara2                     dataset:	 1.0213297605514526
+2022-11-18 16:03:01,438:INFO: Average validation:	ADE  1.1278	FDE  2.0010
+2022-11-18 16:03:01,452:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_666.pth.tar
+2022-11-18 16:03:01,452:INFO: 
+===> EPOCH: 667 (P3)
+2022-11-18 16:03:01,453:INFO: - Computing loss (training)
+2022-11-18 16:03:01,718:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0411 (0.0411)
+2022-11-18 16:03:01,789:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0437 (0.0424)
+2022-11-18 16:03:01,857:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0435 (0.0428)
+2022-11-18 16:03:01,909:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0507 (0.0441)
+2022-11-18 16:03:02,235:INFO: Dataset: univ                Batch:  1/15	Loss 0.0237 (0.0237)
+2022-11-18 16:03:02,315:INFO: Dataset: univ                Batch:  2/15	Loss 0.0237 (0.0237)
+2022-11-18 16:03:02,396:INFO: Dataset: univ                Batch:  3/15	Loss 0.0245 (0.0240)
+2022-11-18 16:03:02,476:INFO: Dataset: univ                Batch:  4/15	Loss 0.0257 (0.0244)
+2022-11-18 16:03:02,553:INFO: Dataset: univ                Batch:  5/15	Loss 0.0245 (0.0244)
+2022-11-18 16:03:02,631:INFO: Dataset: univ                Batch:  6/15	Loss 0.0229 (0.0242)
+2022-11-18 16:03:02,707:INFO: Dataset: univ                Batch:  7/15	Loss 0.0249 (0.0243)
+2022-11-18 16:03:02,783:INFO: Dataset: univ                Batch:  8/15	Loss 0.0231 (0.0241)
+2022-11-18 16:03:02,860:INFO: Dataset: univ                Batch:  9/15	Loss 0.0247 (0.0242)
+2022-11-18 16:03:02,938:INFO: Dataset: univ                Batch: 10/15	Loss 0.0236 (0.0241)
+2022-11-18 16:03:03,014:INFO: Dataset: univ                Batch: 11/15	Loss 0.0235 (0.0241)
+2022-11-18 16:03:03,091:INFO: Dataset: univ                Batch: 12/15	Loss 0.0235 (0.0240)
+2022-11-18 16:03:03,167:INFO: Dataset: univ                Batch: 13/15	Loss 0.0238 (0.0240)
+2022-11-18 16:03:03,243:INFO: Dataset: univ                Batch: 14/15	Loss 0.0228 (0.0239)
+2022-11-18 16:03:03,276:INFO: Dataset: univ                Batch: 15/15	Loss 0.0255 (0.0240)
+2022-11-18 16:03:03,585:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0346 (0.0346)
+2022-11-18 16:03:03,652:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0366 (0.0357)
+2022-11-18 16:03:03,720:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0363 (0.0359)
+2022-11-18 16:03:03,787:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0359 (0.0359)
+2022-11-18 16:03:03,858:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0343 (0.0355)
+2022-11-18 16:03:03,925:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0332 (0.0351)
+2022-11-18 16:03:03,992:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0401 (0.0358)
+2022-11-18 16:03:04,055:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0347 (0.0357)
+2022-11-18 16:03:04,367:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0281 (0.0281)
+2022-11-18 16:03:04,441:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0283 (0.0282)
+2022-11-18 16:03:04,510:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0255 (0.0273)
+2022-11-18 16:03:04,579:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0258 (0.0269)
+2022-11-18 16:03:04,653:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0266 (0.0269)
+2022-11-18 16:03:04,723:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0280 (0.0270)
+2022-11-18 16:03:04,792:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0270 (0.0270)
+2022-11-18 16:03:04,863:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0253 (0.0268)
+2022-11-18 16:03:04,931:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0236 (0.0264)
+2022-11-18 16:03:05,000:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0257 (0.0264)
+2022-11-18 16:03:05,073:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0236 (0.0261)
+2022-11-18 16:03:05,140:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0253 (0.0260)
+2022-11-18 16:03:05,208:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0260 (0.0260)
+2022-11-18 16:03:05,280:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0252 (0.0260)
+2022-11-18 16:03:05,349:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0219 (0.0257)
+2022-11-18 16:03:05,418:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0259 (0.0257)
+2022-11-18 16:03:05,491:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0255 (0.0257)
+2022-11-18 16:03:05,554:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0233 (0.0256)
+2022-11-18 16:03:05,597:INFO: - Computing ADE (validation)
+2022-11-18 16:03:05,887:INFO: 		 ADE on hotel                     dataset:	 1.0604169368743896
+2022-11-18 16:03:06,287:INFO: 		 ADE on univ                      dataset:	 1.1479946374893188
+2022-11-18 16:03:06,584:INFO: 		 ADE on zara1                     dataset:	 1.4485338926315308
+2022-11-18 16:03:07,045:INFO: 		 ADE on zara2                     dataset:	 1.0151357650756836
+2022-11-18 16:03:07,045:INFO: Average validation:	ADE  1.1119	FDE  1.9658
+2022-11-18 16:03:07,058:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_667.pth.tar
+2022-11-18 16:03:07,058:INFO: 
+===> EPOCH: 668 (P3)
+2022-11-18 16:03:07,058:INFO: - Computing loss (training)
+2022-11-18 16:03:07,316:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0493 (0.0493)
+2022-11-18 16:03:07,385:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0388 (0.0441)
+2022-11-18 16:03:07,453:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0474 (0.0451)
+2022-11-18 16:03:07,505:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0391 (0.0441)
+2022-11-18 16:03:07,820:INFO: Dataset: univ                Batch:  1/15	Loss 0.0236 (0.0236)
+2022-11-18 16:03:07,897:INFO: Dataset: univ                Batch:  2/15	Loss 0.0242 (0.0239)
+2022-11-18 16:03:07,970:INFO: Dataset: univ                Batch:  3/15	Loss 0.0236 (0.0238)
+2022-11-18 16:03:08,045:INFO: Dataset: univ                Batch:  4/15	Loss 0.0229 (0.0236)
+2022-11-18 16:03:08,123:INFO: Dataset: univ                Batch:  5/15	Loss 0.0243 (0.0237)
+2022-11-18 16:03:08,197:INFO: Dataset: univ                Batch:  6/15	Loss 0.0234 (0.0237)
+2022-11-18 16:03:08,272:INFO: Dataset: univ                Batch:  7/15	Loss 0.0225 (0.0235)
+2022-11-18 16:03:08,345:INFO: Dataset: univ                Batch:  8/15	Loss 0.0218 (0.0233)
+2022-11-18 16:03:08,418:INFO: Dataset: univ                Batch:  9/15	Loss 0.0243 (0.0234)
+2022-11-18 16:03:08,491:INFO: Dataset: univ                Batch: 10/15	Loss 0.0231 (0.0234)
+2022-11-18 16:03:08,565:INFO: Dataset: univ                Batch: 11/15	Loss 0.0234 (0.0234)
+2022-11-18 16:03:08,638:INFO: Dataset: univ                Batch: 12/15	Loss 0.0237 (0.0234)
+2022-11-18 16:03:08,711:INFO: Dataset: univ                Batch: 13/15	Loss 0.0257 (0.0236)
+2022-11-18 16:03:08,785:INFO: Dataset: univ                Batch: 14/15	Loss 0.0227 (0.0235)
+2022-11-18 16:03:08,818:INFO: Dataset: univ                Batch: 15/15	Loss 0.0217 (0.0235)
+2022-11-18 16:03:09,126:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0372 (0.0372)
+2022-11-18 16:03:09,194:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0327 (0.0347)
+2022-11-18 16:03:09,260:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0326 (0.0340)
+2022-11-18 16:03:09,328:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0351 (0.0342)
+2022-11-18 16:03:09,395:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0345 (0.0343)
+2022-11-18 16:03:09,464:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0322 (0.0339)
+2022-11-18 16:03:09,529:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0327 (0.0338)
+2022-11-18 16:03:09,590:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0373 (0.0341)
+2022-11-18 16:03:09,900:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0273 (0.0273)
+2022-11-18 16:03:09,969:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0219 (0.0244)
+2022-11-18 16:03:10,036:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0254 (0.0248)
+2022-11-18 16:03:10,109:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0270 (0.0253)
+2022-11-18 16:03:10,176:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0242 (0.0251)
+2022-11-18 16:03:10,246:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0257 (0.0252)
+2022-11-18 16:03:10,313:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0218 (0.0247)
+2022-11-18 16:03:10,379:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0239 (0.0246)
+2022-11-18 16:03:10,446:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0237 (0.0245)
+2022-11-18 16:03:10,512:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0260 (0.0247)
+2022-11-18 16:03:10,579:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0237 (0.0246)
+2022-11-18 16:03:10,647:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0236 (0.0245)
+2022-11-18 16:03:10,712:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0247 (0.0245)
+2022-11-18 16:03:10,779:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0229 (0.0244)
+2022-11-18 16:03:10,847:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0237 (0.0244)
+2022-11-18 16:03:10,914:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0265 (0.0245)
+2022-11-18 16:03:10,981:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0230 (0.0244)
+2022-11-18 16:03:11,045:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0272 (0.0245)
+2022-11-18 16:03:11,088:INFO: - Computing ADE (validation)
+2022-11-18 16:03:11,380:INFO: 		 ADE on hotel                     dataset:	 1.05372154712677
+2022-11-18 16:03:11,780:INFO: 		 ADE on univ                      dataset:	 1.1167676448822021
+2022-11-18 16:03:12,076:INFO: 		 ADE on zara1                     dataset:	 1.4562368392944336
+2022-11-18 16:03:12,527:INFO: 		 ADE on zara2                     dataset:	 0.9652765393257141
+2022-11-18 16:03:12,527:INFO: Average validation:	ADE  1.0775	FDE  1.8938
+2022-11-18 16:03:12,540:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_668.pth.tar
+2022-11-18 16:03:12,540:INFO: 
+===> EPOCH: 669 (P3)
+2022-11-18 16:03:12,541:INFO: - Computing loss (training)
+2022-11-18 16:03:12,794:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0416 (0.0416)
+2022-11-18 16:03:12,863:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0499 (0.0457)
+2022-11-18 16:03:12,931:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0376 (0.0429)
+2022-11-18 16:03:12,983:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0497 (0.0440)
+2022-11-18 16:03:13,393:INFO: Dataset: univ                Batch:  1/15	Loss 0.0227 (0.0227)
+2022-11-18 16:03:13,468:INFO: Dataset: univ                Batch:  2/15	Loss 0.0228 (0.0228)
+2022-11-18 16:03:13,544:INFO: Dataset: univ                Batch:  3/15	Loss 0.0224 (0.0226)
+2022-11-18 16:03:13,619:INFO: Dataset: univ                Batch:  4/15	Loss 0.0224 (0.0226)
+2022-11-18 16:03:13,695:INFO: Dataset: univ                Batch:  5/15	Loss 0.0253 (0.0231)
+2022-11-18 16:03:13,773:INFO: Dataset: univ                Batch:  6/15	Loss 0.0222 (0.0229)
+2022-11-18 16:03:13,848:INFO: Dataset: univ                Batch:  7/15	Loss 0.0227 (0.0229)
+2022-11-18 16:03:13,923:INFO: Dataset: univ                Batch:  8/15	Loss 0.0204 (0.0226)
+2022-11-18 16:03:13,998:INFO: Dataset: univ                Batch:  9/15	Loss 0.0237 (0.0227)
+2022-11-18 16:03:14,072:INFO: Dataset: univ                Batch: 10/15	Loss 0.0222 (0.0226)
+2022-11-18 16:03:14,150:INFO: Dataset: univ                Batch: 11/15	Loss 0.0236 (0.0227)
+2022-11-18 16:03:14,225:INFO: Dataset: univ                Batch: 12/15	Loss 0.0229 (0.0227)
+2022-11-18 16:03:14,301:INFO: Dataset: univ                Batch: 13/15	Loss 0.0250 (0.0229)
+2022-11-18 16:03:14,376:INFO: Dataset: univ                Batch: 14/15	Loss 0.0244 (0.0230)
+2022-11-18 16:03:14,409:INFO: Dataset: univ                Batch: 15/15	Loss 0.0211 (0.0230)
+2022-11-18 16:03:14,719:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0334 (0.0334)
+2022-11-18 16:03:14,788:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0338 (0.0336)
+2022-11-18 16:03:14,857:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0328 (0.0333)
+2022-11-18 16:03:14,925:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0312 (0.0328)
+2022-11-18 16:03:14,995:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0326 (0.0328)
+2022-11-18 16:03:15,065:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0320 (0.0327)
+2022-11-18 16:03:15,134:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0325 (0.0326)
+2022-11-18 16:03:15,197:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0324 (0.0326)
+2022-11-18 16:03:15,502:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0230 (0.0230)
+2022-11-18 16:03:15,573:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0215 (0.0223)
+2022-11-18 16:03:15,645:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0267 (0.0237)
+2022-11-18 16:03:15,718:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0245 (0.0239)
+2022-11-18 16:03:15,791:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0251 (0.0241)
+2022-11-18 16:03:15,863:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0237 (0.0240)
+2022-11-18 16:03:15,934:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0238 (0.0240)
+2022-11-18 16:03:16,005:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0222 (0.0238)
+2022-11-18 16:03:16,074:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0244 (0.0239)
+2022-11-18 16:03:16,144:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0227 (0.0237)
+2022-11-18 16:03:16,214:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0219 (0.0236)
+2022-11-18 16:03:16,284:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0211 (0.0234)
+2022-11-18 16:03:16,355:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0238 (0.0234)
+2022-11-18 16:03:16,427:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0221 (0.0233)
+2022-11-18 16:03:16,498:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0232 (0.0233)
+2022-11-18 16:03:16,570:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0222 (0.0232)
+2022-11-18 16:03:16,641:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0244 (0.0233)
+2022-11-18 16:03:16,705:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0284 (0.0235)
+2022-11-18 16:03:16,748:INFO: - Computing ADE (validation)
+2022-11-18 16:03:17,051:INFO: 		 ADE on hotel                     dataset:	 1.0535873174667358
+2022-11-18 16:03:17,426:INFO: 		 ADE on univ                      dataset:	 1.1039148569107056
+2022-11-18 16:03:17,719:INFO: 		 ADE on zara1                     dataset:	 1.3708069324493408
+2022-11-18 16:03:18,194:INFO: 		 ADE on zara2                     dataset:	 0.9566543698310852
+2022-11-18 16:03:18,194:INFO: Average validation:	ADE  1.0627	FDE  1.8607
+2022-11-18 16:03:18,206:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_669.pth.tar
+2022-11-18 16:03:18,206:INFO: 
+===> EPOCH: 670 (P3)
+2022-11-18 16:03:18,207:INFO: - Computing loss (training)
+2022-11-18 16:03:18,471:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0466 (0.0466)
+2022-11-18 16:03:18,540:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0444 (0.0455)
+2022-11-18 16:03:18,609:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0450 (0.0453)
+2022-11-18 16:03:18,659:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0377 (0.0440)
+2022-11-18 16:03:18,982:INFO: Dataset: univ                Batch:  1/15	Loss 0.0224 (0.0224)
+2022-11-18 16:03:19,060:INFO: Dataset: univ                Batch:  2/15	Loss 0.0238 (0.0231)
+2022-11-18 16:03:19,142:INFO: Dataset: univ                Batch:  3/15	Loss 0.0230 (0.0231)
+2022-11-18 16:03:19,226:INFO: Dataset: univ                Batch:  4/15	Loss 0.0237 (0.0232)
+2022-11-18 16:03:19,303:INFO: Dataset: univ                Batch:  5/15	Loss 0.0225 (0.0231)
+2022-11-18 16:03:19,381:INFO: Dataset: univ                Batch:  6/15	Loss 0.0219 (0.0229)
+2022-11-18 16:03:19,459:INFO: Dataset: univ                Batch:  7/15	Loss 0.0235 (0.0230)
+2022-11-18 16:03:19,536:INFO: Dataset: univ                Batch:  8/15	Loss 0.0226 (0.0229)
+2022-11-18 16:03:19,611:INFO: Dataset: univ                Batch:  9/15	Loss 0.0239 (0.0230)
+2022-11-18 16:03:19,690:INFO: Dataset: univ                Batch: 10/15	Loss 0.0219 (0.0229)
+2022-11-18 16:03:19,768:INFO: Dataset: univ                Batch: 11/15	Loss 0.0215 (0.0228)
+2022-11-18 16:03:19,845:INFO: Dataset: univ                Batch: 12/15	Loss 0.0232 (0.0228)
+2022-11-18 16:03:19,925:INFO: Dataset: univ                Batch: 13/15	Loss 0.0216 (0.0227)
+2022-11-18 16:03:20,003:INFO: Dataset: univ                Batch: 14/15	Loss 0.0227 (0.0227)
+2022-11-18 16:03:20,036:INFO: Dataset: univ                Batch: 15/15	Loss 0.0215 (0.0227)
+2022-11-18 16:03:20,341:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0320 (0.0320)
+2022-11-18 16:03:20,408:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0292 (0.0307)
+2022-11-18 16:03:20,478:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0327 (0.0314)
+2022-11-18 16:03:20,544:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0291 (0.0308)
+2022-11-18 16:03:20,610:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0306 (0.0308)
+2022-11-18 16:03:20,677:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0344 (0.0313)
+2022-11-18 16:03:20,742:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0317 (0.0314)
+2022-11-18 16:03:20,803:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0302 (0.0312)
+2022-11-18 16:03:21,117:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0235 (0.0235)
+2022-11-18 16:03:21,189:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0229 (0.0232)
+2022-11-18 16:03:21,256:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0236 (0.0233)
+2022-11-18 16:03:21,322:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0232 (0.0233)
+2022-11-18 16:03:21,390:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0226 (0.0231)
+2022-11-18 16:03:21,460:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0219 (0.0230)
+2022-11-18 16:03:21,527:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0219 (0.0228)
+2022-11-18 16:03:21,595:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0214 (0.0227)
+2022-11-18 16:03:21,661:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0262 (0.0231)
+2022-11-18 16:03:21,727:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0226 (0.0231)
+2022-11-18 16:03:21,795:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0225 (0.0230)
+2022-11-18 16:03:21,862:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0228 (0.0230)
+2022-11-18 16:03:21,928:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0216 (0.0229)
+2022-11-18 16:03:21,996:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0214 (0.0228)
+2022-11-18 16:03:22,064:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0255 (0.0230)
+2022-11-18 16:03:22,132:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0227 (0.0229)
+2022-11-18 16:03:22,199:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0243 (0.0230)
+2022-11-18 16:03:22,261:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0198 (0.0229)
+2022-11-18 16:03:22,305:INFO: - Computing ADE (validation)
+2022-11-18 16:03:22,594:INFO: 		 ADE on hotel                     dataset:	 1.0476235151290894
+2022-11-18 16:03:22,970:INFO: 		 ADE on univ                      dataset:	 1.0893735885620117
+2022-11-18 16:03:23,264:INFO: 		 ADE on zara1                     dataset:	 1.2941782474517822
+2022-11-18 16:03:23,747:INFO: 		 ADE on zara2                     dataset:	 0.928413987159729
+2022-11-18 16:03:23,748:INFO: Average validation:	ADE  1.0400	FDE  1.8146
+2022-11-18 16:03:23,760:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_670.pth.tar
+2022-11-18 16:03:23,760:INFO: 
+===> EPOCH: 671 (P3)
+2022-11-18 16:03:23,761:INFO: - Computing loss (training)
+2022-11-18 16:03:24,024:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0459 (0.0459)
+2022-11-18 16:03:24,096:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0411 (0.0434)
+2022-11-18 16:03:24,165:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0487 (0.0451)
+2022-11-18 16:03:24,216:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0400 (0.0442)
+2022-11-18 16:03:24,530:INFO: Dataset: univ                Batch:  1/15	Loss 0.0221 (0.0221)
+2022-11-18 16:03:24,609:INFO: Dataset: univ                Batch:  2/15	Loss 0.0227 (0.0224)
+2022-11-18 16:03:24,685:INFO: Dataset: univ                Batch:  3/15	Loss 0.0229 (0.0226)
+2022-11-18 16:03:24,759:INFO: Dataset: univ                Batch:  4/15	Loss 0.0212 (0.0222)
+2022-11-18 16:03:24,835:INFO: Dataset: univ                Batch:  5/15	Loss 0.0209 (0.0219)
+2022-11-18 16:03:24,909:INFO: Dataset: univ                Batch:  6/15	Loss 0.0223 (0.0220)
+2022-11-18 16:03:24,982:INFO: Dataset: univ                Batch:  7/15	Loss 0.0233 (0.0222)
+2022-11-18 16:03:25,055:INFO: Dataset: univ                Batch:  8/15	Loss 0.0239 (0.0224)
+2022-11-18 16:03:25,127:INFO: Dataset: univ                Batch:  9/15	Loss 0.0229 (0.0224)
+2022-11-18 16:03:25,200:INFO: Dataset: univ                Batch: 10/15	Loss 0.0222 (0.0224)
+2022-11-18 16:03:25,276:INFO: Dataset: univ                Batch: 11/15	Loss 0.0225 (0.0224)
+2022-11-18 16:03:25,348:INFO: Dataset: univ                Batch: 12/15	Loss 0.0224 (0.0224)
+2022-11-18 16:03:25,421:INFO: Dataset: univ                Batch: 13/15	Loss 0.0213 (0.0223)
+2022-11-18 16:03:25,497:INFO: Dataset: univ                Batch: 14/15	Loss 0.0230 (0.0224)
+2022-11-18 16:03:25,530:INFO: Dataset: univ                Batch: 15/15	Loss 0.0233 (0.0224)
+2022-11-18 16:03:25,847:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0311 (0.0311)
+2022-11-18 16:03:25,923:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0294 (0.0302)
+2022-11-18 16:03:25,995:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0316 (0.0307)
+2022-11-18 16:03:26,068:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0298 (0.0305)
+2022-11-18 16:03:26,139:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0294 (0.0302)
+2022-11-18 16:03:26,210:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0287 (0.0300)
+2022-11-18 16:03:26,281:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0298 (0.0300)
+2022-11-18 16:03:26,345:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0332 (0.0303)
+2022-11-18 16:03:26,656:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0235 (0.0235)
+2022-11-18 16:03:26,726:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0246 (0.0240)
+2022-11-18 16:03:26,800:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0237 (0.0239)
+2022-11-18 16:03:26,871:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0197 (0.0228)
+2022-11-18 16:03:26,942:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0215 (0.0225)
+2022-11-18 16:03:27,016:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0206 (0.0222)
+2022-11-18 16:03:27,086:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0213 (0.0221)
+2022-11-18 16:03:27,168:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0207 (0.0219)
+2022-11-18 16:03:27,239:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0222 (0.0219)
+2022-11-18 16:03:27,309:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0214 (0.0219)
+2022-11-18 16:03:27,381:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0217 (0.0219)
+2022-11-18 16:03:27,460:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0218 (0.0219)
+2022-11-18 16:03:27,563:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0227 (0.0219)
+2022-11-18 16:03:27,651:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0227 (0.0220)
+2022-11-18 16:03:27,758:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0232 (0.0221)
+2022-11-18 16:03:27,931:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0238 (0.0222)
+2022-11-18 16:03:28,026:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0209 (0.0221)
+2022-11-18 16:03:28,089:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0227 (0.0221)
+2022-11-18 16:03:28,134:INFO: - Computing ADE (validation)
+2022-11-18 16:03:28,447:INFO: 		 ADE on hotel                     dataset:	 1.0223398208618164
+2022-11-18 16:03:28,884:INFO: 		 ADE on univ                      dataset:	 1.0800544023513794
+2022-11-18 16:03:29,287:INFO: 		 ADE on zara1                     dataset:	 1.2778582572937012
+2022-11-18 16:03:29,813:INFO: 		 ADE on zara2                     dataset:	 0.9092121124267578
+2022-11-18 16:03:29,813:INFO: Average validation:	ADE  1.0257	FDE  1.7821
+2022-11-18 16:03:29,827:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_671.pth.tar
+2022-11-18 16:03:29,827:INFO: 
+===> EPOCH: 672 (P3)
+2022-11-18 16:03:29,828:INFO: - Computing loss (training)
+2022-11-18 16:03:30,127:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0435 (0.0435)
+2022-11-18 16:03:30,209:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0427 (0.0431)
+2022-11-18 16:03:30,287:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0512 (0.0458)
+2022-11-18 16:03:30,340:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0370 (0.0443)
+2022-11-18 16:03:30,665:INFO: Dataset: univ                Batch:  1/15	Loss 0.0222 (0.0222)
+2022-11-18 16:03:30,752:INFO: Dataset: univ                Batch:  2/15	Loss 0.0221 (0.0222)
+2022-11-18 16:03:30,842:INFO: Dataset: univ                Batch:  3/15	Loss 0.0232 (0.0225)
+2022-11-18 16:03:30,923:INFO: Dataset: univ                Batch:  4/15	Loss 0.0220 (0.0224)
+2022-11-18 16:03:31,002:INFO: Dataset: univ                Batch:  5/15	Loss 0.0208 (0.0221)
+2022-11-18 16:03:31,084:INFO: Dataset: univ                Batch:  6/15	Loss 0.0224 (0.0221)
+2022-11-18 16:03:31,164:INFO: Dataset: univ                Batch:  7/15	Loss 0.0228 (0.0222)
+2022-11-18 16:03:31,244:INFO: Dataset: univ                Batch:  8/15	Loss 0.0226 (0.0223)
+2022-11-18 16:03:31,322:INFO: Dataset: univ                Batch:  9/15	Loss 0.0236 (0.0224)
+2022-11-18 16:03:31,400:INFO: Dataset: univ                Batch: 10/15	Loss 0.0206 (0.0222)
+2022-11-18 16:03:31,477:INFO: Dataset: univ                Batch: 11/15	Loss 0.0223 (0.0223)
+2022-11-18 16:03:31,550:INFO: Dataset: univ                Batch: 12/15	Loss 0.0219 (0.0222)
+2022-11-18 16:03:31,623:INFO: Dataset: univ                Batch: 13/15	Loss 0.0221 (0.0222)
+2022-11-18 16:03:31,698:INFO: Dataset: univ                Batch: 14/15	Loss 0.0211 (0.0221)
+2022-11-18 16:03:31,731:INFO: Dataset: univ                Batch: 15/15	Loss 0.0211 (0.0221)
+2022-11-18 16:03:32,060:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0289 (0.0289)
+2022-11-18 16:03:32,132:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0268 (0.0279)
+2022-11-18 16:03:32,208:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0290 (0.0282)
+2022-11-18 16:03:32,275:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0309 (0.0289)
+2022-11-18 16:03:32,343:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0299 (0.0291)
+2022-11-18 16:03:32,410:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0290 (0.0290)
+2022-11-18 16:03:32,476:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0316 (0.0294)
+2022-11-18 16:03:32,537:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0295 (0.0294)
+2022-11-18 16:03:32,836:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0233 (0.0233)
+2022-11-18 16:03:32,903:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0230 (0.0231)
+2022-11-18 16:03:32,971:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0211 (0.0224)
+2022-11-18 16:03:33,041:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0235 (0.0227)
+2022-11-18 16:03:33,110:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0193 (0.0220)
+2022-11-18 16:03:33,179:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0232 (0.0222)
+2022-11-18 16:03:33,244:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0201 (0.0219)
+2022-11-18 16:03:33,311:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0211 (0.0218)
+2022-11-18 16:03:33,378:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0195 (0.0216)
+2022-11-18 16:03:33,443:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0229 (0.0217)
+2022-11-18 16:03:33,512:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0170 (0.0213)
+2022-11-18 16:03:33,579:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0225 (0.0214)
+2022-11-18 16:03:33,644:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0231 (0.0215)
+2022-11-18 16:03:33,713:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0203 (0.0214)
+2022-11-18 16:03:33,781:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0219 (0.0215)
+2022-11-18 16:03:33,848:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0220 (0.0215)
+2022-11-18 16:03:33,915:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0216 (0.0215)
+2022-11-18 16:03:33,978:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0205 (0.0215)
+2022-11-18 16:03:34,025:INFO: - Computing ADE (validation)
+2022-11-18 16:03:34,309:INFO: 		 ADE on hotel                     dataset:	 1.0360041856765747
+2022-11-18 16:03:34,696:INFO: 		 ADE on univ                      dataset:	 1.0776997804641724
+2022-11-18 16:03:34,998:INFO: 		 ADE on zara1                     dataset:	 1.2875378131866455
+2022-11-18 16:03:35,472:INFO: 		 ADE on zara2                     dataset:	 0.9006147980690002
+2022-11-18 16:03:35,472:INFO: Average validation:	ADE  1.0227	FDE  1.7740
+2022-11-18 16:03:35,485:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_672.pth.tar
+2022-11-18 16:03:35,485:INFO: 
+===> EPOCH: 673 (P3)
+2022-11-18 16:03:35,485:INFO: - Computing loss (training)
+2022-11-18 16:03:35,745:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0447 (0.0447)
+2022-11-18 16:03:35,814:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0585 (0.0514)
+2022-11-18 16:03:35,881:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0369 (0.0464)
+2022-11-18 16:03:35,931:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0343 (0.0443)
+2022-11-18 16:03:36,255:INFO: Dataset: univ                Batch:  1/15	Loss 0.0215 (0.0215)
+2022-11-18 16:03:36,336:INFO: Dataset: univ                Batch:  2/15	Loss 0.0225 (0.0221)
+2022-11-18 16:03:36,415:INFO: Dataset: univ                Batch:  3/15	Loss 0.0226 (0.0223)
+2022-11-18 16:03:36,498:INFO: Dataset: univ                Batch:  4/15	Loss 0.0216 (0.0221)
+2022-11-18 16:03:36,589:INFO: Dataset: univ                Batch:  5/15	Loss 0.0219 (0.0221)
+2022-11-18 16:03:36,667:INFO: Dataset: univ                Batch:  6/15	Loss 0.0216 (0.0220)
+2022-11-18 16:03:36,742:INFO: Dataset: univ                Batch:  7/15	Loss 0.0218 (0.0219)
+2022-11-18 16:03:36,818:INFO: Dataset: univ                Batch:  8/15	Loss 0.0214 (0.0219)
+2022-11-18 16:03:36,892:INFO: Dataset: univ                Batch:  9/15	Loss 0.0224 (0.0219)
+2022-11-18 16:03:36,967:INFO: Dataset: univ                Batch: 10/15	Loss 0.0222 (0.0220)
+2022-11-18 16:03:37,045:INFO: Dataset: univ                Batch: 11/15	Loss 0.0221 (0.0220)
+2022-11-18 16:03:37,121:INFO: Dataset: univ                Batch: 12/15	Loss 0.0216 (0.0219)
+2022-11-18 16:03:37,196:INFO: Dataset: univ                Batch: 13/15	Loss 0.0234 (0.0220)
+2022-11-18 16:03:37,272:INFO: Dataset: univ                Batch: 14/15	Loss 0.0208 (0.0220)
+2022-11-18 16:03:37,305:INFO: Dataset: univ                Batch: 15/15	Loss 0.0219 (0.0220)
+2022-11-18 16:03:37,623:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0260 (0.0260)
+2022-11-18 16:03:37,698:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0286 (0.0274)
+2022-11-18 16:03:37,770:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0301 (0.0282)
+2022-11-18 16:03:37,842:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0265 (0.0278)
+2022-11-18 16:03:37,916:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0283 (0.0279)
+2022-11-18 16:03:37,988:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0305 (0.0283)
+2022-11-18 16:03:38,059:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0291 (0.0284)
+2022-11-18 16:03:38,124:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0295 (0.0285)
+2022-11-18 16:03:38,455:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0267 (0.0267)
+2022-11-18 16:03:38,531:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0206 (0.0235)
+2022-11-18 16:03:38,608:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0240 (0.0237)
+2022-11-18 16:03:38,680:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0208 (0.0229)
+2022-11-18 16:03:38,752:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0206 (0.0225)
+2022-11-18 16:03:38,827:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0208 (0.0222)
+2022-11-18 16:03:38,898:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0191 (0.0217)
+2022-11-18 16:03:38,970:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0199 (0.0215)
+2022-11-18 16:03:39,041:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0219 (0.0215)
+2022-11-18 16:03:39,111:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0200 (0.0214)
+2022-11-18 16:03:39,183:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0198 (0.0212)
+2022-11-18 16:03:39,254:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0206 (0.0212)
+2022-11-18 16:03:39,325:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0201 (0.0211)
+2022-11-18 16:03:39,398:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0210 (0.0211)
+2022-11-18 16:03:39,470:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0236 (0.0213)
+2022-11-18 16:03:39,541:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0203 (0.0212)
+2022-11-18 16:03:39,614:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0216 (0.0212)
+2022-11-18 16:03:39,680:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0191 (0.0211)
+2022-11-18 16:03:39,726:INFO: - Computing ADE (validation)
+2022-11-18 16:03:40,020:INFO: 		 ADE on hotel                     dataset:	 1.0390501022338867
+2022-11-18 16:03:40,412:INFO: 		 ADE on univ                      dataset:	 1.0572845935821533
+2022-11-18 16:03:40,724:INFO: 		 ADE on zara1                     dataset:	 1.2499887943267822
+2022-11-18 16:03:41,217:INFO: 		 ADE on zara2                     dataset:	 0.9040980339050293
+2022-11-18 16:03:41,217:INFO: Average validation:	ADE  1.0113	FDE  1.7447
+2022-11-18 16:03:41,231:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_673.pth.tar
+2022-11-18 16:03:41,231:INFO: 
+===> EPOCH: 674 (P3)
+2022-11-18 16:03:41,232:INFO: - Computing loss (training)
+2022-11-18 16:03:41,499:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0385 (0.0385)
+2022-11-18 16:03:41,569:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0553 (0.0472)
+2022-11-18 16:03:41,635:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0450 (0.0464)
+2022-11-18 16:03:41,684:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0361 (0.0447)
+2022-11-18 16:03:42,019:INFO: Dataset: univ                Batch:  1/15	Loss 0.0226 (0.0226)
+2022-11-18 16:03:42,106:INFO: Dataset: univ                Batch:  2/15	Loss 0.0223 (0.0224)
+2022-11-18 16:03:42,190:INFO: Dataset: univ                Batch:  3/15	Loss 0.0229 (0.0226)
+2022-11-18 16:03:42,272:INFO: Dataset: univ                Batch:  4/15	Loss 0.0219 (0.0224)
+2022-11-18 16:03:42,353:INFO: Dataset: univ                Batch:  5/15	Loss 0.0220 (0.0223)
+2022-11-18 16:03:42,436:INFO: Dataset: univ                Batch:  6/15	Loss 0.0219 (0.0222)
+2022-11-18 16:03:42,517:INFO: Dataset: univ                Batch:  7/15	Loss 0.0223 (0.0222)
+2022-11-18 16:03:42,598:INFO: Dataset: univ                Batch:  8/15	Loss 0.0219 (0.0222)
+2022-11-18 16:03:42,677:INFO: Dataset: univ                Batch:  9/15	Loss 0.0219 (0.0222)
+2022-11-18 16:03:42,759:INFO: Dataset: univ                Batch: 10/15	Loss 0.0209 (0.0220)
+2022-11-18 16:03:42,841:INFO: Dataset: univ                Batch: 11/15	Loss 0.0216 (0.0220)
+2022-11-18 16:03:42,922:INFO: Dataset: univ                Batch: 12/15	Loss 0.0207 (0.0219)
+2022-11-18 16:03:43,003:INFO: Dataset: univ                Batch: 13/15	Loss 0.0203 (0.0218)
+2022-11-18 16:03:43,083:INFO: Dataset: univ                Batch: 14/15	Loss 0.0225 (0.0218)
+2022-11-18 16:03:43,116:INFO: Dataset: univ                Batch: 15/15	Loss 0.0216 (0.0218)
+2022-11-18 16:03:43,415:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0275 (0.0275)
+2022-11-18 16:03:43,483:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0264 (0.0269)
+2022-11-18 16:03:43,549:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0272 (0.0270)
+2022-11-18 16:03:43,617:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0295 (0.0276)
+2022-11-18 16:03:43,685:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0291 (0.0279)
+2022-11-18 16:03:43,752:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0269 (0.0277)
+2022-11-18 16:03:43,817:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0286 (0.0279)
+2022-11-18 16:03:43,877:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0286 (0.0279)
+2022-11-18 16:03:44,190:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0206 (0.0206)
+2022-11-18 16:03:44,259:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0218 (0.0212)
+2022-11-18 16:03:44,324:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0202 (0.0209)
+2022-11-18 16:03:44,393:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0229 (0.0214)
+2022-11-18 16:03:44,461:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0194 (0.0210)
+2022-11-18 16:03:44,529:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0200 (0.0208)
+2022-11-18 16:03:44,594:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0188 (0.0205)
+2022-11-18 16:03:44,660:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0217 (0.0207)
+2022-11-18 16:03:44,726:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0199 (0.0206)
+2022-11-18 16:03:44,791:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0222 (0.0207)
+2022-11-18 16:03:44,860:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0199 (0.0207)
+2022-11-18 16:03:44,925:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0200 (0.0206)
+2022-11-18 16:03:44,990:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0209 (0.0206)
+2022-11-18 16:03:45,057:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0218 (0.0207)
+2022-11-18 16:03:45,124:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0202 (0.0207)
+2022-11-18 16:03:45,191:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0195 (0.0206)
+2022-11-18 16:03:45,257:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0204 (0.0206)
+2022-11-18 16:03:45,318:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0203 (0.0206)
+2022-11-18 16:03:45,363:INFO: - Computing ADE (validation)
+2022-11-18 16:03:45,640:INFO: 		 ADE on hotel                     dataset:	 1.0147591829299927
+2022-11-18 16:03:46,047:INFO: 		 ADE on univ                      dataset:	 1.0631874799728394
+2022-11-18 16:03:46,340:INFO: 		 ADE on zara1                     dataset:	 1.2703598737716675
+2022-11-18 16:03:46,812:INFO: 		 ADE on zara2                     dataset:	 0.8841615915298462
+2022-11-18 16:03:46,812:INFO: Average validation:	ADE  1.0069	FDE  1.7367
+2022-11-18 16:03:46,825:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_674.pth.tar
+2022-11-18 16:03:46,825:INFO: 
+===> EPOCH: 675 (P3)
+2022-11-18 16:03:46,826:INFO: - Computing loss (training)
+2022-11-18 16:03:47,091:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0430 (0.0430)
+2022-11-18 16:03:47,162:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0425 (0.0427)
+2022-11-18 16:03:47,233:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0449 (0.0434)
+2022-11-18 16:03:47,285:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0515 (0.0447)
+2022-11-18 16:03:47,605:INFO: Dataset: univ                Batch:  1/15	Loss 0.0231 (0.0231)
+2022-11-18 16:03:47,684:INFO: Dataset: univ                Batch:  2/15	Loss 0.0223 (0.0228)
+2022-11-18 16:03:47,763:INFO: Dataset: univ                Batch:  3/15	Loss 0.0209 (0.0221)
+2022-11-18 16:03:47,843:INFO: Dataset: univ                Batch:  4/15	Loss 0.0214 (0.0219)
+2022-11-18 16:03:47,918:INFO: Dataset: univ                Batch:  5/15	Loss 0.0218 (0.0219)
+2022-11-18 16:03:47,998:INFO: Dataset: univ                Batch:  6/15	Loss 0.0208 (0.0217)
+2022-11-18 16:03:48,077:INFO: Dataset: univ                Batch:  7/15	Loss 0.0233 (0.0219)
+2022-11-18 16:03:48,152:INFO: Dataset: univ                Batch:  8/15	Loss 0.0219 (0.0219)
+2022-11-18 16:03:48,226:INFO: Dataset: univ                Batch:  9/15	Loss 0.0202 (0.0217)
+2022-11-18 16:03:48,303:INFO: Dataset: univ                Batch: 10/15	Loss 0.0214 (0.0217)
+2022-11-18 16:03:48,379:INFO: Dataset: univ                Batch: 11/15	Loss 0.0215 (0.0217)
+2022-11-18 16:03:48,455:INFO: Dataset: univ                Batch: 12/15	Loss 0.0211 (0.0216)
+2022-11-18 16:03:48,532:INFO: Dataset: univ                Batch: 13/15	Loss 0.0221 (0.0217)
+2022-11-18 16:03:48,609:INFO: Dataset: univ                Batch: 14/15	Loss 0.0223 (0.0217)
+2022-11-18 16:03:48,643:INFO: Dataset: univ                Batch: 15/15	Loss 0.0209 (0.0217)
+2022-11-18 16:03:48,957:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0278 (0.0278)
+2022-11-18 16:03:49,025:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0271 (0.0274)
+2022-11-18 16:03:49,093:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0248 (0.0265)
+2022-11-18 16:03:49,160:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0284 (0.0269)
+2022-11-18 16:03:49,228:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0288 (0.0273)
+2022-11-18 16:03:49,297:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0292 (0.0276)
+2022-11-18 16:03:49,364:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0254 (0.0273)
+2022-11-18 16:03:49,426:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0275 (0.0273)
+2022-11-18 16:03:49,736:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0219 (0.0219)
+2022-11-18 16:03:49,809:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0204 (0.0211)
+2022-11-18 16:03:49,883:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0231 (0.0217)
+2022-11-18 16:03:49,959:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0216 (0.0217)
+2022-11-18 16:03:50,035:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0208 (0.0215)
+2022-11-18 16:03:50,110:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0189 (0.0211)
+2022-11-18 16:03:50,182:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0196 (0.0209)
+2022-11-18 16:03:50,254:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0176 (0.0205)
+2022-11-18 16:03:50,327:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0196 (0.0204)
+2022-11-18 16:03:50,399:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0201 (0.0204)
+2022-11-18 16:03:50,473:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0205 (0.0204)
+2022-11-18 16:03:50,547:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0199 (0.0203)
+2022-11-18 16:03:50,619:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0208 (0.0204)
+2022-11-18 16:03:50,694:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0191 (0.0203)
+2022-11-18 16:03:50,767:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0204 (0.0203)
+2022-11-18 16:03:50,841:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0177 (0.0201)
+2022-11-18 16:03:50,915:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0206 (0.0202)
+2022-11-18 16:03:50,982:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0214 (0.0202)
+2022-11-18 16:03:51,028:INFO: - Computing ADE (validation)
+2022-11-18 16:03:51,308:INFO: 		 ADE on hotel                     dataset:	 1.0225452184677124
+2022-11-18 16:03:51,688:INFO: 		 ADE on univ                      dataset:	 1.0531809329986572
+2022-11-18 16:03:51,983:INFO: 		 ADE on zara1                     dataset:	 1.224361538887024
+2022-11-18 16:03:52,448:INFO: 		 ADE on zara2                     dataset:	 0.8804282546043396
+2022-11-18 16:03:52,448:INFO: Average validation:	ADE  0.9981	FDE  1.7170
+2022-11-18 16:03:52,461:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_675.pth.tar
+2022-11-18 16:03:52,461:INFO: 
+===> EPOCH: 676 (P3)
+2022-11-18 16:03:52,461:INFO: - Computing loss (training)
+2022-11-18 16:03:52,715:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0411 (0.0411)
+2022-11-18 16:03:52,783:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0462 (0.0437)
+2022-11-18 16:03:52,849:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0483 (0.0452)
+2022-11-18 16:03:52,898:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0445 (0.0450)
+2022-11-18 16:03:53,219:INFO: Dataset: univ                Batch:  1/15	Loss 0.0227 (0.0227)
+2022-11-18 16:03:53,292:INFO: Dataset: univ                Batch:  2/15	Loss 0.0223 (0.0225)
+2022-11-18 16:03:53,365:INFO: Dataset: univ                Batch:  3/15	Loss 0.0218 (0.0223)
+2022-11-18 16:03:53,439:INFO: Dataset: univ                Batch:  4/15	Loss 0.0210 (0.0219)
+2022-11-18 16:03:53,517:INFO: Dataset: univ                Batch:  5/15	Loss 0.0209 (0.0217)
+2022-11-18 16:03:53,589:INFO: Dataset: univ                Batch:  6/15	Loss 0.0217 (0.0217)
+2022-11-18 16:03:53,662:INFO: Dataset: univ                Batch:  7/15	Loss 0.0207 (0.0216)
+2022-11-18 16:03:53,733:INFO: Dataset: univ                Batch:  8/15	Loss 0.0239 (0.0219)
+2022-11-18 16:03:53,806:INFO: Dataset: univ                Batch:  9/15	Loss 0.0201 (0.0217)
+2022-11-18 16:03:53,879:INFO: Dataset: univ                Batch: 10/15	Loss 0.0210 (0.0216)
+2022-11-18 16:03:53,952:INFO: Dataset: univ                Batch: 11/15	Loss 0.0216 (0.0216)
+2022-11-18 16:03:54,025:INFO: Dataset: univ                Batch: 12/15	Loss 0.0222 (0.0216)
+2022-11-18 16:03:54,098:INFO: Dataset: univ                Batch: 13/15	Loss 0.0225 (0.0217)
+2022-11-18 16:03:54,171:INFO: Dataset: univ                Batch: 14/15	Loss 0.0210 (0.0217)
+2022-11-18 16:03:54,203:INFO: Dataset: univ                Batch: 15/15	Loss 0.0184 (0.0216)
+2022-11-18 16:03:54,508:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0273 (0.0273)
+2022-11-18 16:03:54,576:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0274 (0.0274)
+2022-11-18 16:03:54,642:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0289 (0.0278)
+2022-11-18 16:03:54,707:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0254 (0.0272)
+2022-11-18 16:03:54,773:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0282 (0.0274)
+2022-11-18 16:03:54,841:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0255 (0.0271)
+2022-11-18 16:03:54,906:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0272 (0.0271)
+2022-11-18 16:03:54,965:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0251 (0.0269)
+2022-11-18 16:03:55,276:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0200 (0.0200)
+2022-11-18 16:03:55,354:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0193 (0.0197)
+2022-11-18 16:03:55,431:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0207 (0.0200)
+2022-11-18 16:03:55,502:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0191 (0.0198)
+2022-11-18 16:03:55,570:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0218 (0.0202)
+2022-11-18 16:03:55,639:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0231 (0.0206)
+2022-11-18 16:03:55,704:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0184 (0.0203)
+2022-11-18 16:03:55,777:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0203 (0.0203)
+2022-11-18 16:03:55,845:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0195 (0.0203)
+2022-11-18 16:03:55,914:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0207 (0.0203)
+2022-11-18 16:03:55,983:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0205 (0.0203)
+2022-11-18 16:03:56,052:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0195 (0.0202)
+2022-11-18 16:03:56,120:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0213 (0.0203)
+2022-11-18 16:03:56,190:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0205 (0.0203)
+2022-11-18 16:03:56,257:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0212 (0.0204)
+2022-11-18 16:03:56,325:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0169 (0.0201)
+2022-11-18 16:03:56,395:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0184 (0.0200)
+2022-11-18 16:03:56,458:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0183 (0.0200)
+2022-11-18 16:03:56,507:INFO: - Computing ADE (validation)
+2022-11-18 16:03:56,818:INFO: 		 ADE on hotel                     dataset:	 1.0450519323349
+2022-11-18 16:03:57,206:INFO: 		 ADE on univ                      dataset:	 1.0518617630004883
+2022-11-18 16:03:57,534:INFO: 		 ADE on zara1                     dataset:	 1.1854381561279297
+2022-11-18 16:03:58,084:INFO: 		 ADE on zara2                     dataset:	 0.8588083386421204
+2022-11-18 16:03:58,084:INFO: Average validation:	ADE  0.9884	FDE  1.6923
+2022-11-18 16:03:58,107:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_676.pth.tar
+2022-11-18 16:03:58,107:INFO: 
+===> EPOCH: 677 (P3)
+2022-11-18 16:03:58,108:INFO: - Computing loss (training)
+2022-11-18 16:03:58,403:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0522 (0.0522)
+2022-11-18 16:03:58,475:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0414 (0.0469)
+2022-11-18 16:03:58,541:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0411 (0.0449)
+2022-11-18 16:03:58,591:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0461 (0.0451)
+2022-11-18 16:03:58,912:INFO: Dataset: univ                Batch:  1/15	Loss 0.0216 (0.0216)
+2022-11-18 16:03:58,993:INFO: Dataset: univ                Batch:  2/15	Loss 0.0221 (0.0218)
+2022-11-18 16:03:59,073:INFO: Dataset: univ                Batch:  3/15	Loss 0.0226 (0.0221)
+2022-11-18 16:03:59,153:INFO: Dataset: univ                Batch:  4/15	Loss 0.0216 (0.0220)
+2022-11-18 16:03:59,232:INFO: Dataset: univ                Batch:  5/15	Loss 0.0214 (0.0218)
+2022-11-18 16:03:59,313:INFO: Dataset: univ                Batch:  6/15	Loss 0.0209 (0.0217)
+2022-11-18 16:03:59,385:INFO: Dataset: univ                Batch:  7/15	Loss 0.0234 (0.0219)
+2022-11-18 16:03:59,462:INFO: Dataset: univ                Batch:  8/15	Loss 0.0210 (0.0218)
+2022-11-18 16:03:59,545:INFO: Dataset: univ                Batch:  9/15	Loss 0.0204 (0.0216)
+2022-11-18 16:03:59,627:INFO: Dataset: univ                Batch: 10/15	Loss 0.0217 (0.0216)
+2022-11-18 16:03:59,703:INFO: Dataset: univ                Batch: 11/15	Loss 0.0212 (0.0216)
+2022-11-18 16:03:59,781:INFO: Dataset: univ                Batch: 12/15	Loss 0.0217 (0.0216)
+2022-11-18 16:03:59,857:INFO: Dataset: univ                Batch: 13/15	Loss 0.0217 (0.0216)
+2022-11-18 16:03:59,932:INFO: Dataset: univ                Batch: 14/15	Loss 0.0204 (0.0215)
+2022-11-18 16:03:59,967:INFO: Dataset: univ                Batch: 15/15	Loss 0.0228 (0.0215)
+2022-11-18 16:04:00,286:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0250 (0.0250)
+2022-11-18 16:04:00,356:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0275 (0.0264)
+2022-11-18 16:04:00,422:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0248 (0.0259)
+2022-11-18 16:04:00,488:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0277 (0.0263)
+2022-11-18 16:04:00,558:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0272 (0.0265)
+2022-11-18 16:04:00,628:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0277 (0.0267)
+2022-11-18 16:04:00,700:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0247 (0.0264)
+2022-11-18 16:04:00,769:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0268 (0.0265)
+2022-11-18 16:04:01,118:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0203 (0.0203)
+2022-11-18 16:04:01,190:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0198 (0.0201)
+2022-11-18 16:04:01,259:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0208 (0.0203)
+2022-11-18 16:04:01,329:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0192 (0.0200)
+2022-11-18 16:04:01,398:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0173 (0.0195)
+2022-11-18 16:04:01,471:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0191 (0.0194)
+2022-11-18 16:04:01,542:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0193 (0.0194)
+2022-11-18 16:04:01,615:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0170 (0.0191)
+2022-11-18 16:04:01,684:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0160 (0.0188)
+2022-11-18 16:04:01,753:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0206 (0.0190)
+2022-11-18 16:04:01,820:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0209 (0.0191)
+2022-11-18 16:04:01,887:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0195 (0.0191)
+2022-11-18 16:04:01,953:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0197 (0.0192)
+2022-11-18 16:04:02,024:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0200 (0.0192)
+2022-11-18 16:04:02,095:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0188 (0.0192)
+2022-11-18 16:04:02,167:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0212 (0.0193)
+2022-11-18 16:04:02,235:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0224 (0.0195)
+2022-11-18 16:04:02,297:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0214 (0.0196)
+2022-11-18 16:04:02,340:INFO: - Computing ADE (validation)
+2022-11-18 16:04:02,632:INFO: 		 ADE on hotel                     dataset:	 1.0518211126327515
+2022-11-18 16:04:03,008:INFO: 		 ADE on univ                      dataset:	 1.042839765548706
+2022-11-18 16:04:03,304:INFO: 		 ADE on zara1                     dataset:	 1.1967103481292725
+2022-11-18 16:04:03,793:INFO: 		 ADE on zara2                     dataset:	 0.8472343683242798
+2022-11-18 16:04:03,793:INFO: Average validation:	ADE  0.9805	FDE  1.6756
+2022-11-18 16:04:03,806:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_677.pth.tar
+2022-11-18 16:04:03,806:INFO: 
+===> EPOCH: 678 (P3)
+2022-11-18 16:04:03,807:INFO: - Computing loss (training)
+2022-11-18 16:04:04,083:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0427 (0.0427)
+2022-11-18 16:04:04,150:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0358 (0.0393)
+2022-11-18 16:04:04,217:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0466 (0.0417)
+2022-11-18 16:04:04,266:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0638 (0.0453)
+2022-11-18 16:04:04,604:INFO: Dataset: univ                Batch:  1/15	Loss 0.0213 (0.0213)
+2022-11-18 16:04:04,698:INFO: Dataset: univ                Batch:  2/15	Loss 0.0220 (0.0216)
+2022-11-18 16:04:04,788:INFO: Dataset: univ                Batch:  3/15	Loss 0.0217 (0.0217)
+2022-11-18 16:04:04,874:INFO: Dataset: univ                Batch:  4/15	Loss 0.0221 (0.0218)
+2022-11-18 16:04:04,965:INFO: Dataset: univ                Batch:  5/15	Loss 0.0205 (0.0215)
+2022-11-18 16:04:05,052:INFO: Dataset: univ                Batch:  6/15	Loss 0.0228 (0.0217)
+2022-11-18 16:04:05,135:INFO: Dataset: univ                Batch:  7/15	Loss 0.0215 (0.0217)
+2022-11-18 16:04:05,219:INFO: Dataset: univ                Batch:  8/15	Loss 0.0214 (0.0217)
+2022-11-18 16:04:05,300:INFO: Dataset: univ                Batch:  9/15	Loss 0.0201 (0.0215)
+2022-11-18 16:04:05,381:INFO: Dataset: univ                Batch: 10/15	Loss 0.0197 (0.0213)
+2022-11-18 16:04:05,468:INFO: Dataset: univ                Batch: 11/15	Loss 0.0200 (0.0212)
+2022-11-18 16:04:05,562:INFO: Dataset: univ                Batch: 12/15	Loss 0.0235 (0.0214)
+2022-11-18 16:04:05,652:INFO: Dataset: univ                Batch: 13/15	Loss 0.0215 (0.0214)
+2022-11-18 16:04:05,741:INFO: Dataset: univ                Batch: 14/15	Loss 0.0213 (0.0214)
+2022-11-18 16:04:05,781:INFO: Dataset: univ                Batch: 15/15	Loss 0.0201 (0.0214)
+2022-11-18 16:04:06,122:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0265 (0.0265)
+2022-11-18 16:04:06,192:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0251 (0.0258)
+2022-11-18 16:04:06,260:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0265 (0.0260)
+2022-11-18 16:04:06,326:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0261 (0.0260)
+2022-11-18 16:04:06,396:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0256 (0.0259)
+2022-11-18 16:04:06,463:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0254 (0.0258)
+2022-11-18 16:04:06,530:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0253 (0.0258)
+2022-11-18 16:04:06,592:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0268 (0.0259)
+2022-11-18 16:04:06,928:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0237 (0.0237)
+2022-11-18 16:04:06,996:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0208 (0.0222)
+2022-11-18 16:04:07,064:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0189 (0.0211)
+2022-11-18 16:04:07,133:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0202 (0.0209)
+2022-11-18 16:04:07,207:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0196 (0.0206)
+2022-11-18 16:04:07,278:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0176 (0.0201)
+2022-11-18 16:04:07,347:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0207 (0.0202)
+2022-11-18 16:04:07,416:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0174 (0.0198)
+2022-11-18 16:04:07,483:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0193 (0.0198)
+2022-11-18 16:04:07,551:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0158 (0.0194)
+2022-11-18 16:04:07,623:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0204 (0.0195)
+2022-11-18 16:04:07,692:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0195 (0.0195)
+2022-11-18 16:04:07,769:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0210 (0.0196)
+2022-11-18 16:04:07,855:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0195 (0.0196)
+2022-11-18 16:04:07,929:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0175 (0.0194)
+2022-11-18 16:04:08,002:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0189 (0.0194)
+2022-11-18 16:04:08,072:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0204 (0.0195)
+2022-11-18 16:04:08,139:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0161 (0.0193)
+2022-11-18 16:04:08,193:INFO: - Computing ADE (validation)
+2022-11-18 16:04:08,482:INFO: 		 ADE on hotel                     dataset:	 1.0320199728012085
+2022-11-18 16:04:08,866:INFO: 		 ADE on univ                      dataset:	 1.0377269983291626
+2022-11-18 16:04:09,188:INFO: 		 ADE on zara1                     dataset:	 1.180545449256897
+2022-11-18 16:04:09,697:INFO: 		 ADE on zara2                     dataset:	 0.8415910601615906
+2022-11-18 16:04:09,698:INFO: Average validation:	ADE  0.9738	FDE  1.6657
+2022-11-18 16:04:09,711:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_678.pth.tar
+2022-11-18 16:04:09,711:INFO: 
+===> EPOCH: 679 (P3)
+2022-11-18 16:04:09,711:INFO: - Computing loss (training)
+2022-11-18 16:04:09,990:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0383 (0.0383)
+2022-11-18 16:04:10,057:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0464 (0.0424)
+2022-11-18 16:04:10,123:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0515 (0.0455)
+2022-11-18 16:04:10,173:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0451 (0.0454)
+2022-11-18 16:04:10,501:INFO: Dataset: univ                Batch:  1/15	Loss 0.0227 (0.0227)
+2022-11-18 16:04:10,588:INFO: Dataset: univ                Batch:  2/15	Loss 0.0204 (0.0215)
+2022-11-18 16:04:10,674:INFO: Dataset: univ                Batch:  3/15	Loss 0.0207 (0.0212)
+2022-11-18 16:04:10,766:INFO: Dataset: univ                Batch:  4/15	Loss 0.0209 (0.0211)
+2022-11-18 16:04:10,861:INFO: Dataset: univ                Batch:  5/15	Loss 0.0210 (0.0211)
+2022-11-18 16:04:10,950:INFO: Dataset: univ                Batch:  6/15	Loss 0.0225 (0.0213)
+2022-11-18 16:04:11,034:INFO: Dataset: univ                Batch:  7/15	Loss 0.0224 (0.0215)
+2022-11-18 16:04:11,116:INFO: Dataset: univ                Batch:  8/15	Loss 0.0208 (0.0214)
+2022-11-18 16:04:11,197:INFO: Dataset: univ                Batch:  9/15	Loss 0.0214 (0.0214)
+2022-11-18 16:04:11,279:INFO: Dataset: univ                Batch: 10/15	Loss 0.0217 (0.0214)
+2022-11-18 16:04:11,363:INFO: Dataset: univ                Batch: 11/15	Loss 0.0219 (0.0215)
+2022-11-18 16:04:11,444:INFO: Dataset: univ                Batch: 12/15	Loss 0.0215 (0.0215)
+2022-11-18 16:04:11,529:INFO: Dataset: univ                Batch: 13/15	Loss 0.0207 (0.0214)
+2022-11-18 16:04:11,616:INFO: Dataset: univ                Batch: 14/15	Loss 0.0203 (0.0213)
+2022-11-18 16:04:11,653:INFO: Dataset: univ                Batch: 15/15	Loss 0.0200 (0.0213)
+2022-11-18 16:04:11,962:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0259 (0.0259)
+2022-11-18 16:04:12,037:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0266 (0.0263)
+2022-11-18 16:04:12,115:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0226 (0.0252)
+2022-11-18 16:04:12,187:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0243 (0.0250)
+2022-11-18 16:04:12,258:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0259 (0.0251)
+2022-11-18 16:04:12,328:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0265 (0.0254)
+2022-11-18 16:04:12,397:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0261 (0.0255)
+2022-11-18 16:04:12,460:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0257 (0.0255)
+2022-11-18 16:04:12,781:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0182 (0.0182)
+2022-11-18 16:04:12,863:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0201 (0.0192)
+2022-11-18 16:04:12,934:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0219 (0.0200)
+2022-11-18 16:04:13,003:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0174 (0.0193)
+2022-11-18 16:04:13,075:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0192 (0.0193)
+2022-11-18 16:04:13,147:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0186 (0.0192)
+2022-11-18 16:04:13,214:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0179 (0.0190)
+2022-11-18 16:04:13,281:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0172 (0.0188)
+2022-11-18 16:04:13,347:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0196 (0.0189)
+2022-11-18 16:04:13,413:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0182 (0.0188)
+2022-11-18 16:04:13,482:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0193 (0.0188)
+2022-11-18 16:04:13,552:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0174 (0.0187)
+2022-11-18 16:04:13,621:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0181 (0.0187)
+2022-11-18 16:04:13,694:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0187 (0.0187)
+2022-11-18 16:04:13,787:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0192 (0.0187)
+2022-11-18 16:04:13,857:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0204 (0.0188)
+2022-11-18 16:04:13,925:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0217 (0.0190)
+2022-11-18 16:04:13,987:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0208 (0.0191)
+2022-11-18 16:04:14,033:INFO: - Computing ADE (validation)
+2022-11-18 16:04:14,319:INFO: 		 ADE on hotel                     dataset:	 1.0334292650222778
+2022-11-18 16:04:14,753:INFO: 		 ADE on univ                      dataset:	 1.0373529195785522
+2022-11-18 16:04:15,077:INFO: 		 ADE on zara1                     dataset:	 1.1782639026641846
+2022-11-18 16:04:15,558:INFO: 		 ADE on zara2                     dataset:	 0.8309194445610046
+2022-11-18 16:04:15,559:INFO: Average validation:	ADE  0.9696	FDE  1.6559
+2022-11-18 16:04:15,572:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_679.pth.tar
+2022-11-18 16:04:15,572:INFO: 
+===> EPOCH: 680 (P3)
+2022-11-18 16:04:15,573:INFO: - Computing loss (training)
+2022-11-18 16:04:15,838:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0501 (0.0501)
+2022-11-18 16:04:15,911:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0388 (0.0445)
+2022-11-18 16:04:15,981:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0456 (0.0448)
+2022-11-18 16:04:16,031:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0484 (0.0454)
+2022-11-18 16:04:16,390:INFO: Dataset: univ                Batch:  1/15	Loss 0.0206 (0.0206)
+2022-11-18 16:04:16,477:INFO: Dataset: univ                Batch:  2/15	Loss 0.0223 (0.0215)
+2022-11-18 16:04:16,561:INFO: Dataset: univ                Batch:  3/15	Loss 0.0221 (0.0217)
+2022-11-18 16:04:16,646:INFO: Dataset: univ                Batch:  4/15	Loss 0.0214 (0.0216)
+2022-11-18 16:04:16,727:INFO: Dataset: univ                Batch:  5/15	Loss 0.0218 (0.0217)
+2022-11-18 16:04:16,817:INFO: Dataset: univ                Batch:  6/15	Loss 0.0224 (0.0218)
+2022-11-18 16:04:16,911:INFO: Dataset: univ                Batch:  7/15	Loss 0.0222 (0.0218)
+2022-11-18 16:04:16,996:INFO: Dataset: univ                Batch:  8/15	Loss 0.0201 (0.0216)
+2022-11-18 16:04:17,081:INFO: Dataset: univ                Batch:  9/15	Loss 0.0201 (0.0215)
+2022-11-18 16:04:17,165:INFO: Dataset: univ                Batch: 10/15	Loss 0.0201 (0.0213)
+2022-11-18 16:04:17,250:INFO: Dataset: univ                Batch: 11/15	Loss 0.0208 (0.0213)
+2022-11-18 16:04:17,336:INFO: Dataset: univ                Batch: 12/15	Loss 0.0205 (0.0212)
+2022-11-18 16:04:17,420:INFO: Dataset: univ                Batch: 13/15	Loss 0.0213 (0.0212)
+2022-11-18 16:04:17,505:INFO: Dataset: univ                Batch: 14/15	Loss 0.0211 (0.0212)
+2022-11-18 16:04:17,541:INFO: Dataset: univ                Batch: 15/15	Loss 0.0206 (0.0212)
+2022-11-18 16:04:17,855:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0242 (0.0242)
+2022-11-18 16:04:17,922:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0268 (0.0254)
+2022-11-18 16:04:17,992:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0265 (0.0258)
+2022-11-18 16:04:18,058:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0252 (0.0256)
+2022-11-18 16:04:18,125:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0246 (0.0254)
+2022-11-18 16:04:18,193:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0247 (0.0253)
+2022-11-18 16:04:18,263:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0244 (0.0252)
+2022-11-18 16:04:18,330:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0251 (0.0252)
+2022-11-18 16:04:18,687:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0176 (0.0176)
+2022-11-18 16:04:18,760:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0181 (0.0179)
+2022-11-18 16:04:18,833:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0215 (0.0191)
+2022-11-18 16:04:18,903:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0206 (0.0194)
+2022-11-18 16:04:18,973:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0183 (0.0192)
+2022-11-18 16:04:19,046:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0177 (0.0189)
+2022-11-18 16:04:19,116:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0168 (0.0187)
+2022-11-18 16:04:19,199:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0190 (0.0187)
+2022-11-18 16:04:19,268:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0202 (0.0189)
+2022-11-18 16:04:19,337:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0200 (0.0190)
+2022-11-18 16:04:19,406:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0179 (0.0189)
+2022-11-18 16:04:19,476:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0191 (0.0189)
+2022-11-18 16:04:19,545:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0208 (0.0191)
+2022-11-18 16:04:19,616:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0189 (0.0190)
+2022-11-18 16:04:19,687:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0199 (0.0191)
+2022-11-18 16:04:19,756:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0185 (0.0191)
+2022-11-18 16:04:19,828:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0183 (0.0190)
+2022-11-18 16:04:19,894:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0158 (0.0189)
+2022-11-18 16:04:19,940:INFO: - Computing ADE (validation)
+2022-11-18 16:04:20,231:INFO: 		 ADE on hotel                     dataset:	 1.044757604598999
+2022-11-18 16:04:20,621:INFO: 		 ADE on univ                      dataset:	 1.0279706716537476
+2022-11-18 16:04:20,918:INFO: 		 ADE on zara1                     dataset:	 1.1534359455108643
+2022-11-18 16:04:21,410:INFO: 		 ADE on zara2                     dataset:	 0.8403728008270264
+2022-11-18 16:04:21,410:INFO: Average validation:	ADE  0.9674	FDE  1.6491
+2022-11-18 16:04:21,424:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_680.pth.tar
+2022-11-18 16:04:21,424:INFO: 
+===> EPOCH: 681 (P3)
+2022-11-18 16:04:21,424:INFO: - Computing loss (training)
+2022-11-18 16:04:21,676:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0449 (0.0449)
+2022-11-18 16:04:21,745:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0499 (0.0475)
+2022-11-18 16:04:21,814:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0440 (0.0463)
+2022-11-18 16:04:21,865:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0422 (0.0457)
+2022-11-18 16:04:22,193:INFO: Dataset: univ                Batch:  1/15	Loss 0.0208 (0.0208)
+2022-11-18 16:04:22,277:INFO: Dataset: univ                Batch:  2/15	Loss 0.0219 (0.0214)
+2022-11-18 16:04:22,353:INFO: Dataset: univ                Batch:  3/15	Loss 0.0217 (0.0215)
+2022-11-18 16:04:22,429:INFO: Dataset: univ                Batch:  4/15	Loss 0.0199 (0.0211)
+2022-11-18 16:04:22,506:INFO: Dataset: univ                Batch:  5/15	Loss 0.0205 (0.0210)
+2022-11-18 16:04:22,582:INFO: Dataset: univ                Batch:  6/15	Loss 0.0222 (0.0212)
+2022-11-18 16:04:22,656:INFO: Dataset: univ                Batch:  7/15	Loss 0.0226 (0.0214)
+2022-11-18 16:04:22,731:INFO: Dataset: univ                Batch:  8/15	Loss 0.0224 (0.0215)
+2022-11-18 16:04:22,805:INFO: Dataset: univ                Batch:  9/15	Loss 0.0203 (0.0213)
+2022-11-18 16:04:22,880:INFO: Dataset: univ                Batch: 10/15	Loss 0.0219 (0.0214)
+2022-11-18 16:04:22,957:INFO: Dataset: univ                Batch: 11/15	Loss 0.0200 (0.0213)
+2022-11-18 16:04:23,031:INFO: Dataset: univ                Batch: 12/15	Loss 0.0207 (0.0212)
+2022-11-18 16:04:23,106:INFO: Dataset: univ                Batch: 13/15	Loss 0.0206 (0.0212)
+2022-11-18 16:04:23,182:INFO: Dataset: univ                Batch: 14/15	Loss 0.0209 (0.0211)
+2022-11-18 16:04:23,216:INFO: Dataset: univ                Batch: 15/15	Loss 0.0223 (0.0212)
+2022-11-18 16:04:23,529:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0245 (0.0245)
+2022-11-18 16:04:23,598:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0244 (0.0244)
+2022-11-18 16:04:23,670:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0255 (0.0248)
+2022-11-18 16:04:23,740:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0243 (0.0247)
+2022-11-18 16:04:23,810:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0252 (0.0248)
+2022-11-18 16:04:23,880:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0242 (0.0247)
+2022-11-18 16:04:23,949:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0249 (0.0247)
+2022-11-18 16:04:24,012:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0267 (0.0249)
+2022-11-18 16:04:24,332:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0175 (0.0175)
+2022-11-18 16:04:24,402:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0189 (0.0182)
+2022-11-18 16:04:24,474:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0207 (0.0190)
+2022-11-18 16:04:24,543:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0176 (0.0186)
+2022-11-18 16:04:24,614:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0192 (0.0187)
+2022-11-18 16:04:24,685:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0202 (0.0190)
+2022-11-18 16:04:24,754:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0195 (0.0191)
+2022-11-18 16:04:24,823:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0191 (0.0191)
+2022-11-18 16:04:24,892:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0204 (0.0192)
+2022-11-18 16:04:24,960:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0183 (0.0191)
+2022-11-18 16:04:25,032:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0190 (0.0191)
+2022-11-18 16:04:25,100:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0182 (0.0190)
+2022-11-18 16:04:25,169:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0159 (0.0188)
+2022-11-18 16:04:25,237:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0181 (0.0187)
+2022-11-18 16:04:25,304:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0176 (0.0186)
+2022-11-18 16:04:25,372:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0202 (0.0187)
+2022-11-18 16:04:25,440:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0189 (0.0188)
+2022-11-18 16:04:25,502:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0168 (0.0187)
+2022-11-18 16:04:25,546:INFO: - Computing ADE (validation)
+2022-11-18 16:04:25,847:INFO: 		 ADE on hotel                     dataset:	 1.0405617952346802
+2022-11-18 16:04:26,228:INFO: 		 ADE on univ                      dataset:	 1.0310077667236328
+2022-11-18 16:04:26,530:INFO: 		 ADE on zara1                     dataset:	 1.1561897993087769
+2022-11-18 16:04:26,992:INFO: 		 ADE on zara2                     dataset:	 0.82211834192276
+2022-11-18 16:04:26,992:INFO: Average validation:	ADE  0.9622	FDE  1.6374
+2022-11-18 16:04:27,006:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_681.pth.tar
+2022-11-18 16:04:27,006:INFO: 
+===> EPOCH: 682 (P3)
+2022-11-18 16:04:27,007:INFO: - Computing loss (training)
+2022-11-18 16:04:27,260:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0473 (0.0473)
+2022-11-18 16:04:27,327:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0471 (0.0472)
+2022-11-18 16:04:27,394:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0440 (0.0461)
+2022-11-18 16:04:27,445:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0440 (0.0457)
+2022-11-18 16:04:27,764:INFO: Dataset: univ                Batch:  1/15	Loss 0.0204 (0.0204)
+2022-11-18 16:04:27,844:INFO: Dataset: univ                Batch:  2/15	Loss 0.0207 (0.0206)
+2022-11-18 16:04:27,917:INFO: Dataset: univ                Batch:  3/15	Loss 0.0213 (0.0208)
+2022-11-18 16:04:27,991:INFO: Dataset: univ                Batch:  4/15	Loss 0.0217 (0.0211)
+2022-11-18 16:04:28,063:INFO: Dataset: univ                Batch:  5/15	Loss 0.0215 (0.0211)
+2022-11-18 16:04:28,138:INFO: Dataset: univ                Batch:  6/15	Loss 0.0195 (0.0209)
+2022-11-18 16:04:28,210:INFO: Dataset: univ                Batch:  7/15	Loss 0.0219 (0.0210)
+2022-11-18 16:04:28,284:INFO: Dataset: univ                Batch:  8/15	Loss 0.0221 (0.0212)
+2022-11-18 16:04:28,356:INFO: Dataset: univ                Batch:  9/15	Loss 0.0204 (0.0211)
+2022-11-18 16:04:28,428:INFO: Dataset: univ                Batch: 10/15	Loss 0.0213 (0.0211)
+2022-11-18 16:04:28,499:INFO: Dataset: univ                Batch: 11/15	Loss 0.0217 (0.0212)
+2022-11-18 16:04:28,573:INFO: Dataset: univ                Batch: 12/15	Loss 0.0214 (0.0212)
+2022-11-18 16:04:28,647:INFO: Dataset: univ                Batch: 13/15	Loss 0.0202 (0.0211)
+2022-11-18 16:04:28,720:INFO: Dataset: univ                Batch: 14/15	Loss 0.0209 (0.0211)
+2022-11-18 16:04:28,753:INFO: Dataset: univ                Batch: 15/15	Loss 0.0205 (0.0211)
+2022-11-18 16:04:29,070:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0259 (0.0259)
+2022-11-18 16:04:29,136:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0237 (0.0248)
+2022-11-18 16:04:29,204:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0229 (0.0241)
+2022-11-18 16:04:29,274:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0237 (0.0240)
+2022-11-18 16:04:29,342:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0250 (0.0242)
+2022-11-18 16:04:29,410:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0261 (0.0246)
+2022-11-18 16:04:29,477:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0257 (0.0247)
+2022-11-18 16:04:29,538:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0239 (0.0246)
+2022-11-18 16:04:29,840:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0191 (0.0191)
+2022-11-18 16:04:29,908:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0195 (0.0193)
+2022-11-18 16:04:29,978:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0182 (0.0189)
+2022-11-18 16:04:30,047:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0202 (0.0192)
+2022-11-18 16:04:30,116:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0191 (0.0192)
+2022-11-18 16:04:30,184:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0172 (0.0189)
+2022-11-18 16:04:30,250:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0172 (0.0186)
+2022-11-18 16:04:30,317:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0181 (0.0186)
+2022-11-18 16:04:30,382:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0195 (0.0187)
+2022-11-18 16:04:30,448:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0191 (0.0187)
+2022-11-18 16:04:30,517:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0191 (0.0187)
+2022-11-18 16:04:30,583:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0183 (0.0187)
+2022-11-18 16:04:30,648:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0171 (0.0186)
+2022-11-18 16:04:30,715:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0165 (0.0184)
+2022-11-18 16:04:30,782:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0214 (0.0186)
+2022-11-18 16:04:30,849:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0188 (0.0186)
+2022-11-18 16:04:30,915:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0189 (0.0186)
+2022-11-18 16:04:30,976:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0156 (0.0185)
+2022-11-18 16:04:31,022:INFO: - Computing ADE (validation)
+2022-11-18 16:04:31,300:INFO: 		 ADE on hotel                     dataset:	 1.0477538108825684
+2022-11-18 16:04:31,695:INFO: 		 ADE on univ                      dataset:	 1.0275962352752686
+2022-11-18 16:04:32,008:INFO: 		 ADE on zara1                     dataset:	 1.1335134506225586
+2022-11-18 16:04:32,475:INFO: 		 ADE on zara2                     dataset:	 0.8232173919677734
+2022-11-18 16:04:32,475:INFO: Average validation:	ADE  0.9599	FDE  1.6357
+2022-11-18 16:04:32,487:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_682.pth.tar
+2022-11-18 16:04:32,487:INFO: 
+===> EPOCH: 683 (P3)
+2022-11-18 16:04:32,488:INFO: - Computing loss (training)
+2022-11-18 16:04:32,752:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0472 (0.0472)
+2022-11-18 16:04:32,823:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0409 (0.0441)
+2022-11-18 16:04:32,893:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0450 (0.0444)
+2022-11-18 16:04:32,943:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0531 (0.0458)
+2022-11-18 16:04:33,285:INFO: Dataset: univ                Batch:  1/15	Loss 0.0213 (0.0213)
+2022-11-18 16:04:33,363:INFO: Dataset: univ                Batch:  2/15	Loss 0.0210 (0.0211)
+2022-11-18 16:04:33,438:INFO: Dataset: univ                Batch:  3/15	Loss 0.0208 (0.0210)
+2022-11-18 16:04:33,511:INFO: Dataset: univ                Batch:  4/15	Loss 0.0208 (0.0210)
+2022-11-18 16:04:33,584:INFO: Dataset: univ                Batch:  5/15	Loss 0.0210 (0.0210)
+2022-11-18 16:04:33,660:INFO: Dataset: univ                Batch:  6/15	Loss 0.0202 (0.0209)
+2022-11-18 16:04:33,732:INFO: Dataset: univ                Batch:  7/15	Loss 0.0213 (0.0209)
+2022-11-18 16:04:33,804:INFO: Dataset: univ                Batch:  8/15	Loss 0.0213 (0.0210)
+2022-11-18 16:04:33,877:INFO: Dataset: univ                Batch:  9/15	Loss 0.0214 (0.0210)
+2022-11-18 16:04:33,949:INFO: Dataset: univ                Batch: 10/15	Loss 0.0214 (0.0211)
+2022-11-18 16:04:34,024:INFO: Dataset: univ                Batch: 11/15	Loss 0.0214 (0.0211)
+2022-11-18 16:04:34,098:INFO: Dataset: univ                Batch: 12/15	Loss 0.0197 (0.0210)
+2022-11-18 16:04:34,170:INFO: Dataset: univ                Batch: 13/15	Loss 0.0220 (0.0210)
+2022-11-18 16:04:34,244:INFO: Dataset: univ                Batch: 14/15	Loss 0.0197 (0.0210)
+2022-11-18 16:04:34,276:INFO: Dataset: univ                Batch: 15/15	Loss 0.0239 (0.0210)
+2022-11-18 16:04:34,575:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0248 (0.0248)
+2022-11-18 16:04:34,643:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0245 (0.0246)
+2022-11-18 16:04:34,709:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0231 (0.0241)
+2022-11-18 16:04:34,777:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0261 (0.0246)
+2022-11-18 16:04:34,844:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0260 (0.0249)
+2022-11-18 16:04:34,912:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0237 (0.0247)
+2022-11-18 16:04:34,977:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0238 (0.0245)
+2022-11-18 16:04:35,038:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0230 (0.0244)
+2022-11-18 16:04:35,343:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0190 (0.0190)
+2022-11-18 16:04:35,411:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0167 (0.0179)
+2022-11-18 16:04:35,478:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0176 (0.0178)
+2022-11-18 16:04:35,546:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0193 (0.0182)
+2022-11-18 16:04:35,613:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0147 (0.0175)
+2022-11-18 16:04:35,683:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0169 (0.0174)
+2022-11-18 16:04:35,749:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0206 (0.0179)
+2022-11-18 16:04:35,817:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0192 (0.0180)
+2022-11-18 16:04:35,883:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0179 (0.0180)
+2022-11-18 16:04:35,950:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0196 (0.0182)
+2022-11-18 16:04:36,018:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0195 (0.0183)
+2022-11-18 16:04:36,086:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0188 (0.0183)
+2022-11-18 16:04:36,153:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0173 (0.0183)
+2022-11-18 16:04:36,221:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0179 (0.0182)
+2022-11-18 16:04:36,289:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0178 (0.0182)
+2022-11-18 16:04:36,358:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0191 (0.0183)
+2022-11-18 16:04:36,426:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0175 (0.0182)
+2022-11-18 16:04:36,489:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0200 (0.0183)
+2022-11-18 16:04:36,532:INFO: - Computing ADE (validation)
+2022-11-18 16:04:36,818:INFO: 		 ADE on hotel                     dataset:	 1.025159478187561
+2022-11-18 16:04:37,209:INFO: 		 ADE on univ                      dataset:	 1.0295754671096802
+2022-11-18 16:04:37,503:INFO: 		 ADE on zara1                     dataset:	 1.1575363874435425
+2022-11-18 16:04:37,996:INFO: 		 ADE on zara2                     dataset:	 0.8179653882980347
+2022-11-18 16:04:37,996:INFO: Average validation:	ADE  0.9592	FDE  1.6312
+2022-11-18 16:04:38,009:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_683.pth.tar
+2022-11-18 16:04:38,009:INFO: 
+===> EPOCH: 684 (P3)
+2022-11-18 16:04:38,010:INFO: - Computing loss (training)
+2022-11-18 16:04:38,292:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0409 (0.0409)
+2022-11-18 16:04:38,365:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0521 (0.0465)
+2022-11-18 16:04:38,438:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0482 (0.0470)
+2022-11-18 16:04:38,491:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0399 (0.0458)
+2022-11-18 16:04:38,820:INFO: Dataset: univ                Batch:  1/15	Loss 0.0203 (0.0203)
+2022-11-18 16:04:38,900:INFO: Dataset: univ                Batch:  2/15	Loss 0.0211 (0.0207)
+2022-11-18 16:04:38,981:INFO: Dataset: univ                Batch:  3/15	Loss 0.0217 (0.0210)
+2022-11-18 16:04:39,062:INFO: Dataset: univ                Batch:  4/15	Loss 0.0204 (0.0208)
+2022-11-18 16:04:39,143:INFO: Dataset: univ                Batch:  5/15	Loss 0.0215 (0.0210)
+2022-11-18 16:04:39,223:INFO: Dataset: univ                Batch:  6/15	Loss 0.0208 (0.0210)
+2022-11-18 16:04:39,303:INFO: Dataset: univ                Batch:  7/15	Loss 0.0207 (0.0209)
+2022-11-18 16:04:39,380:INFO: Dataset: univ                Batch:  8/15	Loss 0.0215 (0.0210)
+2022-11-18 16:04:39,457:INFO: Dataset: univ                Batch:  9/15	Loss 0.0217 (0.0211)
+2022-11-18 16:04:39,536:INFO: Dataset: univ                Batch: 10/15	Loss 0.0209 (0.0211)
+2022-11-18 16:04:39,615:INFO: Dataset: univ                Batch: 11/15	Loss 0.0209 (0.0210)
+2022-11-18 16:04:39,694:INFO: Dataset: univ                Batch: 12/15	Loss 0.0201 (0.0209)
+2022-11-18 16:04:39,772:INFO: Dataset: univ                Batch: 13/15	Loss 0.0215 (0.0210)
+2022-11-18 16:04:39,852:INFO: Dataset: univ                Batch: 14/15	Loss 0.0199 (0.0209)
+2022-11-18 16:04:39,887:INFO: Dataset: univ                Batch: 15/15	Loss 0.0220 (0.0209)
+2022-11-18 16:04:40,208:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0229 (0.0229)
+2022-11-18 16:04:40,279:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0240 (0.0235)
+2022-11-18 16:04:40,351:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0235 (0.0235)
+2022-11-18 16:04:40,425:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0247 (0.0238)
+2022-11-18 16:04:40,497:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0209 (0.0232)
+2022-11-18 16:04:40,569:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0269 (0.0239)
+2022-11-18 16:04:40,640:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0257 (0.0242)
+2022-11-18 16:04:40,704:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0232 (0.0240)
+2022-11-18 16:04:41,041:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0193 (0.0193)
+2022-11-18 16:04:41,114:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0185 (0.0189)
+2022-11-18 16:04:41,186:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0189 (0.0189)
+2022-11-18 16:04:41,262:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0174 (0.0185)
+2022-11-18 16:04:41,335:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0179 (0.0184)
+2022-11-18 16:04:41,409:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0172 (0.0182)
+2022-11-18 16:04:41,481:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0201 (0.0185)
+2022-11-18 16:04:41,553:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0188 (0.0185)
+2022-11-18 16:04:41,623:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0179 (0.0184)
+2022-11-18 16:04:41,694:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0171 (0.0183)
+2022-11-18 16:04:41,767:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0171 (0.0182)
+2022-11-18 16:04:41,837:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0185 (0.0182)
+2022-11-18 16:04:41,909:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0173 (0.0181)
+2022-11-18 16:04:41,981:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0167 (0.0180)
+2022-11-18 16:04:42,052:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0179 (0.0180)
+2022-11-18 16:04:42,126:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0190 (0.0181)
+2022-11-18 16:04:42,198:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0206 (0.0182)
+2022-11-18 16:04:42,263:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0157 (0.0181)
+2022-11-18 16:04:42,305:INFO: - Computing ADE (validation)
+2022-11-18 16:04:42,599:INFO: 		 ADE on hotel                     dataset:	 1.0282933712005615
+2022-11-18 16:04:42,970:INFO: 		 ADE on univ                      dataset:	 1.0191913843154907
+2022-11-18 16:04:43,284:INFO: 		 ADE on zara1                     dataset:	 1.109818458557129
+2022-11-18 16:04:43,754:INFO: 		 ADE on zara2                     dataset:	 0.8105974793434143
+2022-11-18 16:04:43,754:INFO: Average validation:	ADE  0.9484	FDE  1.6098
+2022-11-18 16:04:43,766:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_684.pth.tar
+2022-11-18 16:04:43,767:INFO: 
+===> EPOCH: 685 (P3)
+2022-11-18 16:04:43,767:INFO: - Computing loss (training)
+2022-11-18 16:04:44,039:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0452 (0.0452)
+2022-11-18 16:04:44,113:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0500 (0.0476)
+2022-11-18 16:04:44,187:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0398 (0.0449)
+2022-11-18 16:04:44,240:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0516 (0.0460)
+2022-11-18 16:04:44,558:INFO: Dataset: univ                Batch:  1/15	Loss 0.0209 (0.0209)
+2022-11-18 16:04:44,637:INFO: Dataset: univ                Batch:  2/15	Loss 0.0215 (0.0212)
+2022-11-18 16:04:44,711:INFO: Dataset: univ                Batch:  3/15	Loss 0.0211 (0.0212)
+2022-11-18 16:04:44,783:INFO: Dataset: univ                Batch:  4/15	Loss 0.0193 (0.0207)
+2022-11-18 16:04:44,855:INFO: Dataset: univ                Batch:  5/15	Loss 0.0208 (0.0207)
+2022-11-18 16:04:44,930:INFO: Dataset: univ                Batch:  6/15	Loss 0.0196 (0.0205)
+2022-11-18 16:04:45,001:INFO: Dataset: univ                Batch:  7/15	Loss 0.0217 (0.0207)
+2022-11-18 16:04:45,073:INFO: Dataset: univ                Batch:  8/15	Loss 0.0202 (0.0206)
+2022-11-18 16:04:45,144:INFO: Dataset: univ                Batch:  9/15	Loss 0.0212 (0.0207)
+2022-11-18 16:04:45,215:INFO: Dataset: univ                Batch: 10/15	Loss 0.0213 (0.0208)
+2022-11-18 16:04:45,287:INFO: Dataset: univ                Batch: 11/15	Loss 0.0203 (0.0207)
+2022-11-18 16:04:45,359:INFO: Dataset: univ                Batch: 12/15	Loss 0.0225 (0.0209)
+2022-11-18 16:04:45,431:INFO: Dataset: univ                Batch: 13/15	Loss 0.0208 (0.0209)
+2022-11-18 16:04:45,505:INFO: Dataset: univ                Batch: 14/15	Loss 0.0211 (0.0209)
+2022-11-18 16:04:45,537:INFO: Dataset: univ                Batch: 15/15	Loss 0.0229 (0.0209)
+2022-11-18 16:04:45,843:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0240 (0.0240)
+2022-11-18 16:04:45,913:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0243 (0.0242)
+2022-11-18 16:04:45,985:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0220 (0.0235)
+2022-11-18 16:04:46,054:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0247 (0.0238)
+2022-11-18 16:04:46,124:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0241 (0.0238)
+2022-11-18 16:04:46,213:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0230 (0.0237)
+2022-11-18 16:04:46,280:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0235 (0.0237)
+2022-11-18 16:04:46,341:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0243 (0.0237)
+2022-11-18 16:04:46,653:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0178 (0.0178)
+2022-11-18 16:04:46,728:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0161 (0.0169)
+2022-11-18 16:04:46,801:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0192 (0.0176)
+2022-11-18 16:04:46,878:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0193 (0.0181)
+2022-11-18 16:04:46,953:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0196 (0.0184)
+2022-11-18 16:04:47,028:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0191 (0.0185)
+2022-11-18 16:04:47,101:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0185 (0.0185)
+2022-11-18 16:04:47,174:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0175 (0.0184)
+2022-11-18 16:04:47,245:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0184 (0.0184)
+2022-11-18 16:04:47,318:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0195 (0.0185)
+2022-11-18 16:04:47,393:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0177 (0.0184)
+2022-11-18 16:04:47,465:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0163 (0.0182)
+2022-11-18 16:04:47,537:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0192 (0.0183)
+2022-11-18 16:04:47,611:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0152 (0.0181)
+2022-11-18 16:04:47,684:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0200 (0.0182)
+2022-11-18 16:04:47,757:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0162 (0.0181)
+2022-11-18 16:04:47,831:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0171 (0.0180)
+2022-11-18 16:04:47,898:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0165 (0.0180)
+2022-11-18 16:04:47,940:INFO: - Computing ADE (validation)
+2022-11-18 16:04:48,224:INFO: 		 ADE on hotel                     dataset:	 1.0346797704696655
+2022-11-18 16:04:48,595:INFO: 		 ADE on univ                      dataset:	 1.0237644910812378
+2022-11-18 16:04:48,882:INFO: 		 ADE on zara1                     dataset:	 1.1425138711929321
+2022-11-18 16:04:49,368:INFO: 		 ADE on zara2                     dataset:	 0.8099245429039001
+2022-11-18 16:04:49,368:INFO: Average validation:	ADE  0.9528	FDE  1.6172
+2022-11-18 16:04:49,382:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_685.pth.tar
+2022-11-18 16:04:49,382:INFO: 
+===> EPOCH: 686 (P3)
+2022-11-18 16:04:49,383:INFO: - Computing loss (training)
+2022-11-18 16:04:49,663:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0508 (0.0508)
+2022-11-18 16:04:49,737:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0464 (0.0486)
+2022-11-18 16:04:49,809:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0476 (0.0483)
+2022-11-18 16:04:49,862:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0361 (0.0461)
+2022-11-18 16:04:50,241:INFO: Dataset: univ                Batch:  1/15	Loss 0.0206 (0.0206)
+2022-11-18 16:04:50,319:INFO: Dataset: univ                Batch:  2/15	Loss 0.0213 (0.0210)
+2022-11-18 16:04:50,396:INFO: Dataset: univ                Batch:  3/15	Loss 0.0225 (0.0215)
+2022-11-18 16:04:50,475:INFO: Dataset: univ                Batch:  4/15	Loss 0.0212 (0.0214)
+2022-11-18 16:04:50,551:INFO: Dataset: univ                Batch:  5/15	Loss 0.0216 (0.0215)
+2022-11-18 16:04:50,630:INFO: Dataset: univ                Batch:  6/15	Loss 0.0215 (0.0215)
+2022-11-18 16:04:50,707:INFO: Dataset: univ                Batch:  7/15	Loss 0.0202 (0.0213)
+2022-11-18 16:04:50,784:INFO: Dataset: univ                Batch:  8/15	Loss 0.0203 (0.0212)
+2022-11-18 16:04:50,861:INFO: Dataset: univ                Batch:  9/15	Loss 0.0220 (0.0212)
+2022-11-18 16:04:50,939:INFO: Dataset: univ                Batch: 10/15	Loss 0.0199 (0.0211)
+2022-11-18 16:04:51,017:INFO: Dataset: univ                Batch: 11/15	Loss 0.0206 (0.0211)
+2022-11-18 16:04:51,095:INFO: Dataset: univ                Batch: 12/15	Loss 0.0208 (0.0211)
+2022-11-18 16:04:51,174:INFO: Dataset: univ                Batch: 13/15	Loss 0.0196 (0.0209)
+2022-11-18 16:04:51,251:INFO: Dataset: univ                Batch: 14/15	Loss 0.0199 (0.0209)
+2022-11-18 16:04:51,284:INFO: Dataset: univ                Batch: 15/15	Loss 0.0187 (0.0208)
+2022-11-18 16:04:51,583:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0235 (0.0235)
+2022-11-18 16:04:51,651:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0222 (0.0229)
+2022-11-18 16:04:51,717:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0226 (0.0228)
+2022-11-18 16:04:51,783:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0231 (0.0229)
+2022-11-18 16:04:51,849:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0239 (0.0231)
+2022-11-18 16:04:51,917:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0238 (0.0232)
+2022-11-18 16:04:51,982:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0247 (0.0234)
+2022-11-18 16:04:52,042:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0257 (0.0237)
+2022-11-18 16:04:52,348:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0200 (0.0200)
+2022-11-18 16:04:52,415:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0170 (0.0184)
+2022-11-18 16:04:52,484:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0173 (0.0180)
+2022-11-18 16:04:52,552:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0177 (0.0180)
+2022-11-18 16:04:52,618:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0179 (0.0179)
+2022-11-18 16:04:52,687:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0176 (0.0179)
+2022-11-18 16:04:52,752:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0159 (0.0175)
+2022-11-18 16:04:52,819:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0185 (0.0177)
+2022-11-18 16:04:52,884:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0178 (0.0177)
+2022-11-18 16:04:52,949:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0190 (0.0178)
+2022-11-18 16:04:53,014:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0196 (0.0180)
+2022-11-18 16:04:53,082:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0178 (0.0180)
+2022-11-18 16:04:53,147:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0193 (0.0181)
+2022-11-18 16:04:53,213:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0182 (0.0181)
+2022-11-18 16:04:53,280:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0170 (0.0180)
+2022-11-18 16:04:53,348:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0181 (0.0180)
+2022-11-18 16:04:53,414:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0165 (0.0179)
+2022-11-18 16:04:53,475:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0167 (0.0179)
+2022-11-18 16:04:53,516:INFO: - Computing ADE (validation)
+2022-11-18 16:04:53,801:INFO: 		 ADE on hotel                     dataset:	 1.0370985269546509
+2022-11-18 16:04:54,171:INFO: 		 ADE on univ                      dataset:	 1.0195879936218262
+2022-11-18 16:04:54,456:INFO: 		 ADE on zara1                     dataset:	 1.0999155044555664
+2022-11-18 16:04:54,951:INFO: 		 ADE on zara2                     dataset:	 0.8101129531860352
+2022-11-18 16:04:54,951:INFO: Average validation:	ADE  0.9484	FDE  1.6070
+2022-11-18 16:04:54,963:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_686.pth.tar
+2022-11-18 16:04:54,964:INFO: 
+===> EPOCH: 687 (P3)
+2022-11-18 16:04:54,964:INFO: - Computing loss (training)
+2022-11-18 16:04:55,220:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0383 (0.0383)
+2022-11-18 16:04:55,286:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0443 (0.0413)
+2022-11-18 16:04:55,352:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0542 (0.0454)
+2022-11-18 16:04:55,400:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0496 (0.0462)
+2022-11-18 16:04:55,708:INFO: Dataset: univ                Batch:  1/15	Loss 0.0199 (0.0199)
+2022-11-18 16:04:55,788:INFO: Dataset: univ                Batch:  2/15	Loss 0.0191 (0.0195)
+2022-11-18 16:04:55,862:INFO: Dataset: univ                Batch:  3/15	Loss 0.0215 (0.0202)
+2022-11-18 16:04:55,936:INFO: Dataset: univ                Batch:  4/15	Loss 0.0208 (0.0203)
+2022-11-18 16:04:56,011:INFO: Dataset: univ                Batch:  5/15	Loss 0.0224 (0.0208)
+2022-11-18 16:04:56,085:INFO: Dataset: univ                Batch:  6/15	Loss 0.0212 (0.0208)
+2022-11-18 16:04:56,158:INFO: Dataset: univ                Batch:  7/15	Loss 0.0202 (0.0207)
+2022-11-18 16:04:56,232:INFO: Dataset: univ                Batch:  8/15	Loss 0.0205 (0.0207)
+2022-11-18 16:04:56,303:INFO: Dataset: univ                Batch:  9/15	Loss 0.0209 (0.0207)
+2022-11-18 16:04:56,375:INFO: Dataset: univ                Batch: 10/15	Loss 0.0206 (0.0207)
+2022-11-18 16:04:56,449:INFO: Dataset: univ                Batch: 11/15	Loss 0.0210 (0.0208)
+2022-11-18 16:04:56,521:INFO: Dataset: univ                Batch: 12/15	Loss 0.0197 (0.0207)
+2022-11-18 16:04:56,595:INFO: Dataset: univ                Batch: 13/15	Loss 0.0218 (0.0208)
+2022-11-18 16:04:56,669:INFO: Dataset: univ                Batch: 14/15	Loss 0.0206 (0.0207)
+2022-11-18 16:04:56,701:INFO: Dataset: univ                Batch: 15/15	Loss 0.0222 (0.0208)
+2022-11-18 16:04:57,015:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0220 (0.0220)
+2022-11-18 16:04:57,083:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0227 (0.0223)
+2022-11-18 16:04:57,149:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0262 (0.0237)
+2022-11-18 16:04:57,216:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0230 (0.0235)
+2022-11-18 16:04:57,285:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0247 (0.0238)
+2022-11-18 16:04:57,353:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0222 (0.0235)
+2022-11-18 16:04:57,418:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0248 (0.0237)
+2022-11-18 16:04:57,478:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0218 (0.0235)
+2022-11-18 16:04:57,835:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0156 (0.0156)
+2022-11-18 16:04:57,901:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0200 (0.0178)
+2022-11-18 16:04:57,968:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0189 (0.0182)
+2022-11-18 16:04:58,036:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0168 (0.0179)
+2022-11-18 16:04:58,103:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0193 (0.0181)
+2022-11-18 16:04:58,172:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0198 (0.0184)
+2022-11-18 16:04:58,237:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0158 (0.0180)
+2022-11-18 16:04:58,303:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0182 (0.0181)
+2022-11-18 16:04:58,369:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0171 (0.0180)
+2022-11-18 16:04:58,435:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0187 (0.0180)
+2022-11-18 16:04:58,501:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0156 (0.0178)
+2022-11-18 16:04:58,568:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0194 (0.0179)
+2022-11-18 16:04:58,635:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0194 (0.0180)
+2022-11-18 16:04:58,703:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0170 (0.0180)
+2022-11-18 16:04:58,769:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0180 (0.0180)
+2022-11-18 16:04:58,835:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0156 (0.0178)
+2022-11-18 16:04:58,903:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0163 (0.0177)
+2022-11-18 16:04:58,964:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0175 (0.0177)
+2022-11-18 16:04:59,009:INFO: - Computing ADE (validation)
+2022-11-18 16:04:59,296:INFO: 		 ADE on hotel                     dataset:	 1.0241584777832031
+2022-11-18 16:04:59,680:INFO: 		 ADE on univ                      dataset:	 1.0181478261947632
+2022-11-18 16:04:59,989:INFO: 		 ADE on zara1                     dataset:	 1.1375449895858765
+2022-11-18 16:05:00,461:INFO: 		 ADE on zara2                     dataset:	 0.8017534613609314
+2022-11-18 16:05:00,461:INFO: Average validation:	ADE  0.9460	FDE  1.6023
+2022-11-18 16:05:00,473:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_687.pth.tar
+2022-11-18 16:05:00,473:INFO: 
+===> EPOCH: 688 (P3)
+2022-11-18 16:05:00,474:INFO: - Computing loss (training)
+2022-11-18 16:05:00,735:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0461 (0.0461)
+2022-11-18 16:05:00,805:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0432 (0.0447)
+2022-11-18 16:05:00,876:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0470 (0.0455)
+2022-11-18 16:05:00,928:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0505 (0.0463)
+2022-11-18 16:05:01,236:INFO: Dataset: univ                Batch:  1/15	Loss 0.0220 (0.0220)
+2022-11-18 16:05:01,311:INFO: Dataset: univ                Batch:  2/15	Loss 0.0202 (0.0211)
+2022-11-18 16:05:01,387:INFO: Dataset: univ                Batch:  3/15	Loss 0.0215 (0.0212)
+2022-11-18 16:05:01,463:INFO: Dataset: univ                Batch:  4/15	Loss 0.0201 (0.0210)
+2022-11-18 16:05:01,537:INFO: Dataset: univ                Batch:  5/15	Loss 0.0217 (0.0211)
+2022-11-18 16:05:01,611:INFO: Dataset: univ                Batch:  6/15	Loss 0.0210 (0.0211)
+2022-11-18 16:05:01,682:INFO: Dataset: univ                Batch:  7/15	Loss 0.0216 (0.0212)
+2022-11-18 16:05:01,756:INFO: Dataset: univ                Batch:  8/15	Loss 0.0189 (0.0209)
+2022-11-18 16:05:01,827:INFO: Dataset: univ                Batch:  9/15	Loss 0.0205 (0.0208)
+2022-11-18 16:05:01,899:INFO: Dataset: univ                Batch: 10/15	Loss 0.0210 (0.0208)
+2022-11-18 16:05:01,977:INFO: Dataset: univ                Batch: 11/15	Loss 0.0189 (0.0207)
+2022-11-18 16:05:02,053:INFO: Dataset: univ                Batch: 12/15	Loss 0.0221 (0.0208)
+2022-11-18 16:05:02,130:INFO: Dataset: univ                Batch: 13/15	Loss 0.0200 (0.0207)
+2022-11-18 16:05:02,209:INFO: Dataset: univ                Batch: 14/15	Loss 0.0206 (0.0207)
+2022-11-18 16:05:02,242:INFO: Dataset: univ                Batch: 15/15	Loss 0.0215 (0.0207)
+2022-11-18 16:05:02,541:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0225 (0.0225)
+2022-11-18 16:05:02,610:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0226 (0.0226)
+2022-11-18 16:05:02,681:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0236 (0.0229)
+2022-11-18 16:05:02,749:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0230 (0.0229)
+2022-11-18 16:05:02,819:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0230 (0.0229)
+2022-11-18 16:05:02,887:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0234 (0.0230)
+2022-11-18 16:05:02,954:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0228 (0.0230)
+2022-11-18 16:05:03,016:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0253 (0.0232)
+2022-11-18 16:05:03,318:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0187 (0.0187)
+2022-11-18 16:05:03,384:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0169 (0.0178)
+2022-11-18 16:05:03,453:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0163 (0.0173)
+2022-11-18 16:05:03,519:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0183 (0.0176)
+2022-11-18 16:05:03,590:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0165 (0.0173)
+2022-11-18 16:05:03,658:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0177 (0.0174)
+2022-11-18 16:05:03,724:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0179 (0.0175)
+2022-11-18 16:05:03,790:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0185 (0.0176)
+2022-11-18 16:05:03,857:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0185 (0.0177)
+2022-11-18 16:05:03,923:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0174 (0.0177)
+2022-11-18 16:05:03,991:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0157 (0.0175)
+2022-11-18 16:05:04,058:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0189 (0.0176)
+2022-11-18 16:05:04,124:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0176 (0.0176)
+2022-11-18 16:05:04,191:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0195 (0.0178)
+2022-11-18 16:05:04,258:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0181 (0.0178)
+2022-11-18 16:05:04,325:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0172 (0.0177)
+2022-11-18 16:05:04,392:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0160 (0.0176)
+2022-11-18 16:05:04,453:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0164 (0.0176)
+2022-11-18 16:05:04,496:INFO: - Computing ADE (validation)
+2022-11-18 16:05:04,783:INFO: 		 ADE on hotel                     dataset:	 1.0252516269683838
+2022-11-18 16:05:05,190:INFO: 		 ADE on univ                      dataset:	 1.0177136659622192
+2022-11-18 16:05:05,500:INFO: 		 ADE on zara1                     dataset:	 1.109315037727356
+2022-11-18 16:05:05,969:INFO: 		 ADE on zara2                     dataset:	 0.788229763507843
+2022-11-18 16:05:05,969:INFO: Average validation:	ADE  0.9393	FDE  1.5903
+2022-11-18 16:05:05,982:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_688.pth.tar
+2022-11-18 16:05:05,982:INFO: 
+===> EPOCH: 689 (P3)
+2022-11-18 16:05:05,983:INFO: - Computing loss (training)
+2022-11-18 16:05:06,241:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0533 (0.0533)
+2022-11-18 16:05:06,310:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0460 (0.0498)
+2022-11-18 16:05:06,376:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0458 (0.0484)
+2022-11-18 16:05:06,425:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0358 (0.0463)
+2022-11-18 16:05:06,744:INFO: Dataset: univ                Batch:  1/15	Loss 0.0201 (0.0201)
+2022-11-18 16:05:06,826:INFO: Dataset: univ                Batch:  2/15	Loss 0.0199 (0.0200)
+2022-11-18 16:05:06,905:INFO: Dataset: univ                Batch:  3/15	Loss 0.0205 (0.0202)
+2022-11-18 16:05:06,985:INFO: Dataset: univ                Batch:  4/15	Loss 0.0204 (0.0202)
+2022-11-18 16:05:07,061:INFO: Dataset: univ                Batch:  5/15	Loss 0.0199 (0.0201)
+2022-11-18 16:05:07,140:INFO: Dataset: univ                Batch:  6/15	Loss 0.0208 (0.0202)
+2022-11-18 16:05:07,216:INFO: Dataset: univ                Batch:  7/15	Loss 0.0215 (0.0204)
+2022-11-18 16:05:07,293:INFO: Dataset: univ                Batch:  8/15	Loss 0.0217 (0.0206)
+2022-11-18 16:05:07,367:INFO: Dataset: univ                Batch:  9/15	Loss 0.0208 (0.0206)
+2022-11-18 16:05:07,443:INFO: Dataset: univ                Batch: 10/15	Loss 0.0213 (0.0207)
+2022-11-18 16:05:07,519:INFO: Dataset: univ                Batch: 11/15	Loss 0.0205 (0.0207)
+2022-11-18 16:05:07,597:INFO: Dataset: univ                Batch: 12/15	Loss 0.0188 (0.0205)
+2022-11-18 16:05:07,672:INFO: Dataset: univ                Batch: 13/15	Loss 0.0208 (0.0205)
+2022-11-18 16:05:07,749:INFO: Dataset: univ                Batch: 14/15	Loss 0.0228 (0.0207)
+2022-11-18 16:05:07,783:INFO: Dataset: univ                Batch: 15/15	Loss 0.0208 (0.0207)
+2022-11-18 16:05:08,084:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0219 (0.0219)
+2022-11-18 16:05:08,150:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0247 (0.0233)
+2022-11-18 16:05:08,216:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0234 (0.0234)
+2022-11-18 16:05:08,281:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0232 (0.0233)
+2022-11-18 16:05:08,350:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0237 (0.0234)
+2022-11-18 16:05:08,417:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0222 (0.0232)
+2022-11-18 16:05:08,482:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0221 (0.0230)
+2022-11-18 16:05:08,542:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0235 (0.0231)
+2022-11-18 16:05:08,855:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0178 (0.0178)
+2022-11-18 16:05:08,925:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0187 (0.0182)
+2022-11-18 16:05:08,996:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0176 (0.0180)
+2022-11-18 16:05:09,064:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0179 (0.0180)
+2022-11-18 16:05:09,132:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0184 (0.0181)
+2022-11-18 16:05:09,204:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0190 (0.0182)
+2022-11-18 16:05:09,271:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0161 (0.0179)
+2022-11-18 16:05:09,340:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0190 (0.0180)
+2022-11-18 16:05:09,407:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0178 (0.0180)
+2022-11-18 16:05:09,474:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0179 (0.0180)
+2022-11-18 16:05:09,542:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0162 (0.0178)
+2022-11-18 16:05:09,611:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0148 (0.0176)
+2022-11-18 16:05:09,678:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0175 (0.0176)
+2022-11-18 16:05:09,747:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0161 (0.0175)
+2022-11-18 16:05:09,815:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0170 (0.0174)
+2022-11-18 16:05:09,884:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0172 (0.0174)
+2022-11-18 16:05:09,953:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0154 (0.0173)
+2022-11-18 16:05:10,015:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0192 (0.0174)
+2022-11-18 16:05:10,073:INFO: - Computing ADE (validation)
+2022-11-18 16:05:10,368:INFO: 		 ADE on hotel                     dataset:	 1.0348378419876099
+2022-11-18 16:05:10,763:INFO: 		 ADE on univ                      dataset:	 1.0110952854156494
+2022-11-18 16:05:11,056:INFO: 		 ADE on zara1                     dataset:	 1.0968782901763916
+2022-11-18 16:05:11,516:INFO: 		 ADE on zara2                     dataset:	 0.7962217926979065
+2022-11-18 16:05:11,516:INFO: Average validation:	ADE  0.9386	FDE  1.5874
+2022-11-18 16:05:11,529:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_689.pth.tar
+2022-11-18 16:05:11,529:INFO: 
+===> EPOCH: 690 (P3)
+2022-11-18 16:05:11,530:INFO: - Computing loss (training)
+2022-11-18 16:05:11,784:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0414 (0.0414)
+2022-11-18 16:05:11,852:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0440 (0.0427)
+2022-11-18 16:05:11,918:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0505 (0.0454)
+2022-11-18 16:05:11,967:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0502 (0.0463)
+2022-11-18 16:05:12,300:INFO: Dataset: univ                Batch:  1/15	Loss 0.0201 (0.0201)
+2022-11-18 16:05:12,382:INFO: Dataset: univ                Batch:  2/15	Loss 0.0216 (0.0208)
+2022-11-18 16:05:12,461:INFO: Dataset: univ                Batch:  3/15	Loss 0.0205 (0.0207)
+2022-11-18 16:05:12,539:INFO: Dataset: univ                Batch:  4/15	Loss 0.0230 (0.0212)
+2022-11-18 16:05:12,618:INFO: Dataset: univ                Batch:  5/15	Loss 0.0219 (0.0214)
+2022-11-18 16:05:12,696:INFO: Dataset: univ                Batch:  6/15	Loss 0.0184 (0.0208)
+2022-11-18 16:05:12,773:INFO: Dataset: univ                Batch:  7/15	Loss 0.0205 (0.0207)
+2022-11-18 16:05:12,848:INFO: Dataset: univ                Batch:  8/15	Loss 0.0206 (0.0207)
+2022-11-18 16:05:12,923:INFO: Dataset: univ                Batch:  9/15	Loss 0.0215 (0.0208)
+2022-11-18 16:05:12,999:INFO: Dataset: univ                Batch: 10/15	Loss 0.0209 (0.0208)
+2022-11-18 16:05:13,078:INFO: Dataset: univ                Batch: 11/15	Loss 0.0196 (0.0207)
+2022-11-18 16:05:13,153:INFO: Dataset: univ                Batch: 12/15	Loss 0.0196 (0.0206)
+2022-11-18 16:05:13,230:INFO: Dataset: univ                Batch: 13/15	Loss 0.0222 (0.0207)
+2022-11-18 16:05:13,307:INFO: Dataset: univ                Batch: 14/15	Loss 0.0197 (0.0206)
+2022-11-18 16:05:13,340:INFO: Dataset: univ                Batch: 15/15	Loss 0.0196 (0.0206)
+2022-11-18 16:05:13,648:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0230 (0.0230)
+2022-11-18 16:05:13,716:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0226 (0.0228)
+2022-11-18 16:05:13,785:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0245 (0.0234)
+2022-11-18 16:05:13,858:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0201 (0.0225)
+2022-11-18 16:05:13,924:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0243 (0.0228)
+2022-11-18 16:05:13,991:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0224 (0.0227)
+2022-11-18 16:05:14,057:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0238 (0.0229)
+2022-11-18 16:05:14,117:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0225 (0.0229)
+2022-11-18 16:05:14,420:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0156 (0.0156)
+2022-11-18 16:05:14,492:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0209 (0.0183)
+2022-11-18 16:05:14,565:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0182 (0.0183)
+2022-11-18 16:05:14,635:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0164 (0.0178)
+2022-11-18 16:05:14,708:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0174 (0.0177)
+2022-11-18 16:05:14,780:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0182 (0.0178)
+2022-11-18 16:05:14,849:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0187 (0.0179)
+2022-11-18 16:05:14,919:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0173 (0.0178)
+2022-11-18 16:05:14,989:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0162 (0.0176)
+2022-11-18 16:05:15,058:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0179 (0.0177)
+2022-11-18 16:05:15,129:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0179 (0.0177)
+2022-11-18 16:05:15,200:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0189 (0.0178)
+2022-11-18 16:05:15,269:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0185 (0.0178)
+2022-11-18 16:05:15,340:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0161 (0.0177)
+2022-11-18 16:05:15,411:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0163 (0.0176)
+2022-11-18 16:05:15,481:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0163 (0.0175)
+2022-11-18 16:05:15,552:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0162 (0.0174)
+2022-11-18 16:05:15,617:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0153 (0.0173)
+2022-11-18 16:05:15,659:INFO: - Computing ADE (validation)
+2022-11-18 16:05:15,952:INFO: 		 ADE on hotel                     dataset:	 1.0356229543685913
+2022-11-18 16:05:16,335:INFO: 		 ADE on univ                      dataset:	 1.0170782804489136
+2022-11-18 16:05:16,646:INFO: 		 ADE on zara1                     dataset:	 1.091915249824524
+2022-11-18 16:05:17,113:INFO: 		 ADE on zara2                     dataset:	 0.7845521569252014
+2022-11-18 16:05:17,113:INFO: Average validation:	ADE  0.9372	FDE  1.5872
+2022-11-18 16:05:17,126:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_690.pth.tar
+2022-11-18 16:05:17,126:INFO: 
+===> EPOCH: 691 (P3)
+2022-11-18 16:05:17,127:INFO: - Computing loss (training)
+2022-11-18 16:05:17,376:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0503 (0.0503)
+2022-11-18 16:05:17,445:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0438 (0.0470)
+2022-11-18 16:05:17,511:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0486 (0.0475)
+2022-11-18 16:05:17,560:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0406 (0.0463)
+2022-11-18 16:05:17,874:INFO: Dataset: univ                Batch:  1/15	Loss 0.0212 (0.0212)
+2022-11-18 16:05:17,953:INFO: Dataset: univ                Batch:  2/15	Loss 0.0202 (0.0207)
+2022-11-18 16:05:18,033:INFO: Dataset: univ                Batch:  3/15	Loss 0.0212 (0.0209)
+2022-11-18 16:05:18,111:INFO: Dataset: univ                Batch:  4/15	Loss 0.0208 (0.0209)
+2022-11-18 16:05:18,190:INFO: Dataset: univ                Batch:  5/15	Loss 0.0203 (0.0208)
+2022-11-18 16:05:18,269:INFO: Dataset: univ                Batch:  6/15	Loss 0.0190 (0.0204)
+2022-11-18 16:05:18,345:INFO: Dataset: univ                Batch:  7/15	Loss 0.0212 (0.0205)
+2022-11-18 16:05:18,422:INFO: Dataset: univ                Batch:  8/15	Loss 0.0212 (0.0206)
+2022-11-18 16:05:18,497:INFO: Dataset: univ                Batch:  9/15	Loss 0.0208 (0.0206)
+2022-11-18 16:05:18,572:INFO: Dataset: univ                Batch: 10/15	Loss 0.0199 (0.0206)
+2022-11-18 16:05:18,647:INFO: Dataset: univ                Batch: 11/15	Loss 0.0200 (0.0205)
+2022-11-18 16:05:18,724:INFO: Dataset: univ                Batch: 12/15	Loss 0.0206 (0.0205)
+2022-11-18 16:05:18,801:INFO: Dataset: univ                Batch: 13/15	Loss 0.0214 (0.0206)
+2022-11-18 16:05:18,877:INFO: Dataset: univ                Batch: 14/15	Loss 0.0205 (0.0206)
+2022-11-18 16:05:18,911:INFO: Dataset: univ                Batch: 15/15	Loss 0.0176 (0.0206)
+2022-11-18 16:05:19,217:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0239 (0.0239)
+2022-11-18 16:05:19,288:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0234 (0.0237)
+2022-11-18 16:05:19,362:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0248 (0.0240)
+2022-11-18 16:05:19,431:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0225 (0.0237)
+2022-11-18 16:05:19,501:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0227 (0.0235)
+2022-11-18 16:05:19,572:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0217 (0.0231)
+2022-11-18 16:05:19,642:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0216 (0.0229)
+2022-11-18 16:05:19,705:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0209 (0.0227)
+2022-11-18 16:05:20,018:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0173 (0.0173)
+2022-11-18 16:05:20,086:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0163 (0.0168)
+2022-11-18 16:05:20,158:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0171 (0.0169)
+2022-11-18 16:05:20,224:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0164 (0.0168)
+2022-11-18 16:05:20,291:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0187 (0.0172)
+2022-11-18 16:05:20,359:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0155 (0.0169)
+2022-11-18 16:05:20,425:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0183 (0.0171)
+2022-11-18 16:05:20,491:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0164 (0.0170)
+2022-11-18 16:05:20,557:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0176 (0.0170)
+2022-11-18 16:05:20,622:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0183 (0.0172)
+2022-11-18 16:05:20,690:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0202 (0.0174)
+2022-11-18 16:05:20,757:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0170 (0.0174)
+2022-11-18 16:05:20,822:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0176 (0.0174)
+2022-11-18 16:05:20,889:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0168 (0.0174)
+2022-11-18 16:05:20,956:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0178 (0.0174)
+2022-11-18 16:05:21,023:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0182 (0.0174)
+2022-11-18 16:05:21,089:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0160 (0.0174)
+2022-11-18 16:05:21,150:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0149 (0.0172)
+2022-11-18 16:05:21,196:INFO: - Computing ADE (validation)
+2022-11-18 16:05:21,478:INFO: 		 ADE on hotel                     dataset:	 1.0178264379501343
+2022-11-18 16:05:21,850:INFO: 		 ADE on univ                      dataset:	 1.0129823684692383
+2022-11-18 16:05:22,143:INFO: 		 ADE on zara1                     dataset:	 1.0760778188705444
+2022-11-18 16:05:22,600:INFO: 		 ADE on zara2                     dataset:	 0.7853965163230896
+2022-11-18 16:05:22,600:INFO: Average validation:	ADE  0.9334	FDE  1.5776
+2022-11-18 16:05:22,613:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_691.pth.tar
+2022-11-18 16:05:22,613:INFO: 
+===> EPOCH: 692 (P3)
+2022-11-18 16:05:22,613:INFO: - Computing loss (training)
+2022-11-18 16:05:22,882:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0515 (0.0515)
+2022-11-18 16:05:22,950:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0460 (0.0486)
+2022-11-18 16:05:23,017:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0468 (0.0480)
+2022-11-18 16:05:23,066:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0384 (0.0462)
+2022-11-18 16:05:23,377:INFO: Dataset: univ                Batch:  1/15	Loss 0.0203 (0.0203)
+2022-11-18 16:05:23,454:INFO: Dataset: univ                Batch:  2/15	Loss 0.0207 (0.0205)
+2022-11-18 16:05:23,530:INFO: Dataset: univ                Batch:  3/15	Loss 0.0217 (0.0208)
+2022-11-18 16:05:23,607:INFO: Dataset: univ                Batch:  4/15	Loss 0.0211 (0.0209)
+2022-11-18 16:05:23,684:INFO: Dataset: univ                Batch:  5/15	Loss 0.0205 (0.0208)
+2022-11-18 16:05:23,758:INFO: Dataset: univ                Batch:  6/15	Loss 0.0200 (0.0207)
+2022-11-18 16:05:23,831:INFO: Dataset: univ                Batch:  7/15	Loss 0.0205 (0.0207)
+2022-11-18 16:05:23,905:INFO: Dataset: univ                Batch:  8/15	Loss 0.0220 (0.0208)
+2022-11-18 16:05:23,977:INFO: Dataset: univ                Batch:  9/15	Loss 0.0198 (0.0207)
+2022-11-18 16:05:24,050:INFO: Dataset: univ                Batch: 10/15	Loss 0.0191 (0.0206)
+2022-11-18 16:05:24,126:INFO: Dataset: univ                Batch: 11/15	Loss 0.0202 (0.0205)
+2022-11-18 16:05:24,198:INFO: Dataset: univ                Batch: 12/15	Loss 0.0206 (0.0205)
+2022-11-18 16:05:24,270:INFO: Dataset: univ                Batch: 13/15	Loss 0.0207 (0.0206)
+2022-11-18 16:05:24,346:INFO: Dataset: univ                Batch: 14/15	Loss 0.0201 (0.0205)
+2022-11-18 16:05:24,379:INFO: Dataset: univ                Batch: 15/15	Loss 0.0202 (0.0205)
+2022-11-18 16:05:24,691:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0212 (0.0212)
+2022-11-18 16:05:24,763:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0217 (0.0215)
+2022-11-18 16:05:24,833:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0218 (0.0216)
+2022-11-18 16:05:24,902:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0232 (0.0220)
+2022-11-18 16:05:24,974:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0234 (0.0223)
+2022-11-18 16:05:25,044:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0244 (0.0226)
+2022-11-18 16:05:25,113:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0230 (0.0227)
+2022-11-18 16:05:25,177:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0206 (0.0225)
+2022-11-18 16:05:25,481:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0171 (0.0171)
+2022-11-18 16:05:25,551:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0155 (0.0163)
+2022-11-18 16:05:25,625:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0188 (0.0172)
+2022-11-18 16:05:25,695:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0168 (0.0171)
+2022-11-18 16:05:25,769:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0179 (0.0173)
+2022-11-18 16:05:25,841:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0181 (0.0174)
+2022-11-18 16:05:25,910:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0171 (0.0174)
+2022-11-18 16:05:25,981:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0172 (0.0173)
+2022-11-18 16:05:26,049:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0185 (0.0175)
+2022-11-18 16:05:26,119:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0156 (0.0173)
+2022-11-18 16:05:26,192:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0151 (0.0171)
+2022-11-18 16:05:26,261:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0190 (0.0173)
+2022-11-18 16:05:26,330:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0165 (0.0172)
+2022-11-18 16:05:26,402:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0181 (0.0173)
+2022-11-18 16:05:26,472:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0162 (0.0172)
+2022-11-18 16:05:26,543:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0171 (0.0172)
+2022-11-18 16:05:26,615:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0171 (0.0172)
+2022-11-18 16:05:26,679:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0163 (0.0171)
+2022-11-18 16:05:26,721:INFO: - Computing ADE (validation)
+2022-11-18 16:05:27,001:INFO: 		 ADE on hotel                     dataset:	 1.0263780355453491
+2022-11-18 16:05:27,386:INFO: 		 ADE on univ                      dataset:	 1.0084757804870605
+2022-11-18 16:05:27,692:INFO: 		 ADE on zara1                     dataset:	 1.0918891429901123
+2022-11-18 16:05:28,174:INFO: 		 ADE on zara2                     dataset:	 0.7845749855041504
+2022-11-18 16:05:28,174:INFO: Average validation:	ADE  0.9322	FDE  1.5811
+2022-11-18 16:05:28,186:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_692.pth.tar
+2022-11-18 16:05:28,186:INFO: 
+===> EPOCH: 693 (P3)
+2022-11-18 16:05:28,187:INFO: - Computing loss (training)
+2022-11-18 16:05:28,464:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0439 (0.0439)
+2022-11-18 16:05:28,537:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0404 (0.0422)
+2022-11-18 16:05:28,606:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0551 (0.0462)
+2022-11-18 16:05:28,658:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0471 (0.0464)
+2022-11-18 16:05:28,964:INFO: Dataset: univ                Batch:  1/15	Loss 0.0209 (0.0209)
+2022-11-18 16:05:29,038:INFO: Dataset: univ                Batch:  2/15	Loss 0.0191 (0.0200)
+2022-11-18 16:05:29,114:INFO: Dataset: univ                Batch:  3/15	Loss 0.0202 (0.0201)
+2022-11-18 16:05:29,192:INFO: Dataset: univ                Batch:  4/15	Loss 0.0204 (0.0202)
+2022-11-18 16:05:29,265:INFO: Dataset: univ                Batch:  5/15	Loss 0.0201 (0.0202)
+2022-11-18 16:05:29,340:INFO: Dataset: univ                Batch:  6/15	Loss 0.0204 (0.0202)
+2022-11-18 16:05:29,412:INFO: Dataset: univ                Batch:  7/15	Loss 0.0232 (0.0206)
+2022-11-18 16:05:29,486:INFO: Dataset: univ                Batch:  8/15	Loss 0.0203 (0.0205)
+2022-11-18 16:05:29,557:INFO: Dataset: univ                Batch:  9/15	Loss 0.0203 (0.0205)
+2022-11-18 16:05:29,629:INFO: Dataset: univ                Batch: 10/15	Loss 0.0215 (0.0206)
+2022-11-18 16:05:29,703:INFO: Dataset: univ                Batch: 11/15	Loss 0.0202 (0.0206)
+2022-11-18 16:05:29,775:INFO: Dataset: univ                Batch: 12/15	Loss 0.0201 (0.0205)
+2022-11-18 16:05:29,846:INFO: Dataset: univ                Batch: 13/15	Loss 0.0200 (0.0205)
+2022-11-18 16:05:29,919:INFO: Dataset: univ                Batch: 14/15	Loss 0.0205 (0.0205)
+2022-11-18 16:05:29,952:INFO: Dataset: univ                Batch: 15/15	Loss 0.0192 (0.0205)
+2022-11-18 16:05:30,259:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0222 (0.0222)
+2022-11-18 16:05:30,328:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0215 (0.0218)
+2022-11-18 16:05:30,398:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0225 (0.0221)
+2022-11-18 16:05:30,467:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0210 (0.0218)
+2022-11-18 16:05:30,537:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0218 (0.0218)
+2022-11-18 16:05:30,609:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0252 (0.0224)
+2022-11-18 16:05:30,678:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0210 (0.0222)
+2022-11-18 16:05:30,742:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0236 (0.0223)
+2022-11-18 16:05:31,048:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0147 (0.0147)
+2022-11-18 16:05:31,121:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0177 (0.0160)
+2022-11-18 16:05:31,195:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0175 (0.0165)
+2022-11-18 16:05:31,265:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0182 (0.0169)
+2022-11-18 16:05:31,336:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0171 (0.0169)
+2022-11-18 16:05:31,408:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0155 (0.0167)
+2022-11-18 16:05:31,478:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0169 (0.0167)
+2022-11-18 16:05:31,548:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0197 (0.0170)
+2022-11-18 16:05:31,618:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0167 (0.0170)
+2022-11-18 16:05:31,688:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0150 (0.0168)
+2022-11-18 16:05:31,756:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0176 (0.0169)
+2022-11-18 16:05:31,822:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0197 (0.0171)
+2022-11-18 16:05:31,887:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0171 (0.0171)
+2022-11-18 16:05:31,957:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0162 (0.0170)
+2022-11-18 16:05:32,025:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0185 (0.0171)
+2022-11-18 16:05:32,092:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0154 (0.0170)
+2022-11-18 16:05:32,160:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0175 (0.0171)
+2022-11-18 16:05:32,222:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0166 (0.0170)
+2022-11-18 16:05:32,265:INFO: - Computing ADE (validation)
+2022-11-18 16:05:32,542:INFO: 		 ADE on hotel                     dataset:	 1.0132381916046143
+2022-11-18 16:05:32,927:INFO: 		 ADE on univ                      dataset:	 1.009209156036377
+2022-11-18 16:05:33,223:INFO: 		 ADE on zara1                     dataset:	 1.0856854915618896
+2022-11-18 16:05:33,724:INFO: 		 ADE on zara2                     dataset:	 0.7878804802894592
+2022-11-18 16:05:33,724:INFO: Average validation:	ADE  0.9327	FDE  1.5761
+2022-11-18 16:05:33,737:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_693.pth.tar
+2022-11-18 16:05:33,737:INFO: 
+===> EPOCH: 694 (P3)
+2022-11-18 16:05:33,737:INFO: - Computing loss (training)
+2022-11-18 16:05:33,997:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0435 (0.0435)
+2022-11-18 16:05:34,069:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0608 (0.0521)
+2022-11-18 16:05:34,140:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0358 (0.0464)
+2022-11-18 16:05:34,191:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0464 (0.0464)
+2022-11-18 16:05:34,519:INFO: Dataset: univ                Batch:  1/15	Loss 0.0209 (0.0209)
+2022-11-18 16:05:34,595:INFO: Dataset: univ                Batch:  2/15	Loss 0.0200 (0.0205)
+2022-11-18 16:05:34,670:INFO: Dataset: univ                Batch:  3/15	Loss 0.0198 (0.0203)
+2022-11-18 16:05:34,743:INFO: Dataset: univ                Batch:  4/15	Loss 0.0210 (0.0204)
+2022-11-18 16:05:34,820:INFO: Dataset: univ                Batch:  5/15	Loss 0.0203 (0.0204)
+2022-11-18 16:05:34,894:INFO: Dataset: univ                Batch:  6/15	Loss 0.0201 (0.0204)
+2022-11-18 16:05:34,966:INFO: Dataset: univ                Batch:  7/15	Loss 0.0196 (0.0203)
+2022-11-18 16:05:35,039:INFO: Dataset: univ                Batch:  8/15	Loss 0.0220 (0.0205)
+2022-11-18 16:05:35,111:INFO: Dataset: univ                Batch:  9/15	Loss 0.0208 (0.0205)
+2022-11-18 16:05:35,183:INFO: Dataset: univ                Batch: 10/15	Loss 0.0205 (0.0205)
+2022-11-18 16:05:35,255:INFO: Dataset: univ                Batch: 11/15	Loss 0.0202 (0.0205)
+2022-11-18 16:05:35,328:INFO: Dataset: univ                Batch: 12/15	Loss 0.0208 (0.0205)
+2022-11-18 16:05:35,399:INFO: Dataset: univ                Batch: 13/15	Loss 0.0197 (0.0204)
+2022-11-18 16:05:35,473:INFO: Dataset: univ                Batch: 14/15	Loss 0.0206 (0.0205)
+2022-11-18 16:05:35,505:INFO: Dataset: univ                Batch: 15/15	Loss 0.0191 (0.0204)
+2022-11-18 16:05:35,802:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0234 (0.0234)
+2022-11-18 16:05:35,870:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0226 (0.0230)
+2022-11-18 16:05:35,936:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0217 (0.0226)
+2022-11-18 16:05:36,001:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0200 (0.0219)
+2022-11-18 16:05:36,071:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0236 (0.0223)
+2022-11-18 16:05:36,138:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0224 (0.0223)
+2022-11-18 16:05:36,212:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0228 (0.0224)
+2022-11-18 16:05:36,271:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0205 (0.0222)
+2022-11-18 16:05:36,576:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0155 (0.0155)
+2022-11-18 16:05:36,643:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0193 (0.0173)
+2022-11-18 16:05:36,709:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0166 (0.0171)
+2022-11-18 16:05:36,775:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0154 (0.0167)
+2022-11-18 16:05:36,846:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0157 (0.0165)
+2022-11-18 16:05:36,915:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0172 (0.0166)
+2022-11-18 16:05:36,981:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0182 (0.0168)
+2022-11-18 16:05:37,047:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0169 (0.0168)
+2022-11-18 16:05:37,113:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0162 (0.0168)
+2022-11-18 16:05:37,179:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0174 (0.0168)
+2022-11-18 16:05:37,246:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0160 (0.0168)
+2022-11-18 16:05:37,313:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0197 (0.0170)
+2022-11-18 16:05:37,379:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0180 (0.0171)
+2022-11-18 16:05:37,446:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0176 (0.0171)
+2022-11-18 16:05:37,512:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0145 (0.0169)
+2022-11-18 16:05:37,579:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0161 (0.0169)
+2022-11-18 16:05:37,647:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0165 (0.0169)
+2022-11-18 16:05:37,708:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0177 (0.0169)
+2022-11-18 16:05:37,750:INFO: - Computing ADE (validation)
+2022-11-18 16:05:38,044:INFO: 		 ADE on hotel                     dataset:	 1.0177968740463257
+2022-11-18 16:05:38,431:INFO: 		 ADE on univ                      dataset:	 1.00664484500885
+2022-11-18 16:05:38,736:INFO: 		 ADE on zara1                     dataset:	 1.0565803050994873
+2022-11-18 16:05:39,236:INFO: 		 ADE on zara2                     dataset:	 0.7652449607849121
+2022-11-18 16:05:39,236:INFO: Average validation:	ADE  0.9216	FDE  1.5558
+2022-11-18 16:05:39,249:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_694.pth.tar
+2022-11-18 16:05:39,249:INFO: 
+===> EPOCH: 695 (P3)
+2022-11-18 16:05:39,250:INFO: - Computing loss (training)
+2022-11-18 16:05:39,525:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0471 (0.0471)
+2022-11-18 16:05:39,595:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0421 (0.0447)
+2022-11-18 16:05:39,665:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0460 (0.0451)
+2022-11-18 16:05:39,717:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0532 (0.0464)
+2022-11-18 16:05:40,038:INFO: Dataset: univ                Batch:  1/15	Loss 0.0209 (0.0209)
+2022-11-18 16:05:40,118:INFO: Dataset: univ                Batch:  2/15	Loss 0.0210 (0.0210)
+2022-11-18 16:05:40,197:INFO: Dataset: univ                Batch:  3/15	Loss 0.0191 (0.0203)
+2022-11-18 16:05:40,276:INFO: Dataset: univ                Batch:  4/15	Loss 0.0194 (0.0201)
+2022-11-18 16:05:40,356:INFO: Dataset: univ                Batch:  5/15	Loss 0.0215 (0.0204)
+2022-11-18 16:05:40,435:INFO: Dataset: univ                Batch:  6/15	Loss 0.0209 (0.0205)
+2022-11-18 16:05:40,511:INFO: Dataset: univ                Batch:  7/15	Loss 0.0204 (0.0205)
+2022-11-18 16:05:40,588:INFO: Dataset: univ                Batch:  8/15	Loss 0.0185 (0.0202)
+2022-11-18 16:05:40,662:INFO: Dataset: univ                Batch:  9/15	Loss 0.0196 (0.0202)
+2022-11-18 16:05:40,738:INFO: Dataset: univ                Batch: 10/15	Loss 0.0227 (0.0204)
+2022-11-18 16:05:40,814:INFO: Dataset: univ                Batch: 11/15	Loss 0.0206 (0.0204)
+2022-11-18 16:05:40,890:INFO: Dataset: univ                Batch: 12/15	Loss 0.0206 (0.0204)
+2022-11-18 16:05:40,966:INFO: Dataset: univ                Batch: 13/15	Loss 0.0198 (0.0204)
+2022-11-18 16:05:41,043:INFO: Dataset: univ                Batch: 14/15	Loss 0.0210 (0.0204)
+2022-11-18 16:05:41,076:INFO: Dataset: univ                Batch: 15/15	Loss 0.0193 (0.0204)
+2022-11-18 16:05:41,386:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0226 (0.0226)
+2022-11-18 16:05:41,452:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0221 (0.0224)
+2022-11-18 16:05:41,518:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0223 (0.0223)
+2022-11-18 16:05:41,584:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0217 (0.0222)
+2022-11-18 16:05:41,652:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0226 (0.0222)
+2022-11-18 16:05:41,720:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0204 (0.0219)
+2022-11-18 16:05:41,785:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0235 (0.0222)
+2022-11-18 16:05:41,845:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0216 (0.0221)
+2022-11-18 16:05:42,154:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0177 (0.0177)
+2022-11-18 16:05:42,220:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0177 (0.0177)
+2022-11-18 16:05:42,286:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0162 (0.0172)
+2022-11-18 16:05:42,352:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0165 (0.0170)
+2022-11-18 16:05:42,422:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0149 (0.0166)
+2022-11-18 16:05:42,490:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0160 (0.0165)
+2022-11-18 16:05:42,556:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0153 (0.0163)
+2022-11-18 16:05:42,621:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0165 (0.0163)
+2022-11-18 16:05:42,688:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0147 (0.0161)
+2022-11-18 16:05:42,753:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0185 (0.0164)
+2022-11-18 16:05:42,819:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0197 (0.0167)
+2022-11-18 16:05:42,886:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0176 (0.0168)
+2022-11-18 16:05:42,952:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0162 (0.0167)
+2022-11-18 16:05:43,019:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0157 (0.0166)
+2022-11-18 16:05:43,086:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0159 (0.0166)
+2022-11-18 16:05:43,153:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0175 (0.0166)
+2022-11-18 16:05:43,220:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0201 (0.0168)
+2022-11-18 16:05:43,281:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0166 (0.0168)
+2022-11-18 16:05:43,331:INFO: - Computing ADE (validation)
+2022-11-18 16:05:43,613:INFO: 		 ADE on hotel                     dataset:	 1.0184197425842285
+2022-11-18 16:05:43,989:INFO: 		 ADE on univ                      dataset:	 1.0061167478561401
+2022-11-18 16:05:44,294:INFO: 		 ADE on zara1                     dataset:	 1.0744478702545166
+2022-11-18 16:05:44,790:INFO: 		 ADE on zara2                     dataset:	 0.7724019289016724
+2022-11-18 16:05:44,790:INFO: Average validation:	ADE  0.9250	FDE  1.5622
+2022-11-18 16:05:44,803:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_695.pth.tar
+2022-11-18 16:05:44,803:INFO: 
+===> EPOCH: 696 (P3)
+2022-11-18 16:05:44,804:INFO: - Computing loss (training)
+2022-11-18 16:05:45,064:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0460 (0.0460)
+2022-11-18 16:05:45,132:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0508 (0.0484)
+2022-11-18 16:05:45,200:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0402 (0.0455)
+2022-11-18 16:05:45,249:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0513 (0.0464)
+2022-11-18 16:05:45,569:INFO: Dataset: univ                Batch:  1/15	Loss 0.0200 (0.0200)
+2022-11-18 16:05:45,648:INFO: Dataset: univ                Batch:  2/15	Loss 0.0208 (0.0204)
+2022-11-18 16:05:45,725:INFO: Dataset: univ                Batch:  3/15	Loss 0.0201 (0.0203)
+2022-11-18 16:05:45,798:INFO: Dataset: univ                Batch:  4/15	Loss 0.0203 (0.0203)
+2022-11-18 16:05:45,870:INFO: Dataset: univ                Batch:  5/15	Loss 0.0206 (0.0204)
+2022-11-18 16:05:45,951:INFO: Dataset: univ                Batch:  6/15	Loss 0.0200 (0.0203)
+2022-11-18 16:05:46,023:INFO: Dataset: univ                Batch:  7/15	Loss 0.0200 (0.0203)
+2022-11-18 16:05:46,094:INFO: Dataset: univ                Batch:  8/15	Loss 0.0211 (0.0204)
+2022-11-18 16:05:46,166:INFO: Dataset: univ                Batch:  9/15	Loss 0.0189 (0.0202)
+2022-11-18 16:05:46,239:INFO: Dataset: univ                Batch: 10/15	Loss 0.0193 (0.0201)
+2022-11-18 16:05:46,310:INFO: Dataset: univ                Batch: 11/15	Loss 0.0203 (0.0201)
+2022-11-18 16:05:46,383:INFO: Dataset: univ                Batch: 12/15	Loss 0.0209 (0.0202)
+2022-11-18 16:05:46,454:INFO: Dataset: univ                Batch: 13/15	Loss 0.0197 (0.0202)
+2022-11-18 16:05:46,526:INFO: Dataset: univ                Batch: 14/15	Loss 0.0225 (0.0203)
+2022-11-18 16:05:46,559:INFO: Dataset: univ                Batch: 15/15	Loss 0.0204 (0.0203)
+2022-11-18 16:05:46,862:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0217 (0.0217)
+2022-11-18 16:05:46,930:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0236 (0.0226)
+2022-11-18 16:05:46,999:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0220 (0.0224)
+2022-11-18 16:05:47,066:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0203 (0.0218)
+2022-11-18 16:05:47,133:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0239 (0.0222)
+2022-11-18 16:05:47,200:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0217 (0.0221)
+2022-11-18 16:05:47,266:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0210 (0.0219)
+2022-11-18 16:05:47,326:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0222 (0.0220)
+2022-11-18 16:05:47,612:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0160 (0.0160)
+2022-11-18 16:05:47,680:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0181 (0.0170)
+2022-11-18 16:05:47,747:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0154 (0.0165)
+2022-11-18 16:05:47,817:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0163 (0.0164)
+2022-11-18 16:05:47,887:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0173 (0.0166)
+2022-11-18 16:05:47,955:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0173 (0.0167)
+2022-11-18 16:05:48,020:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0177 (0.0169)
+2022-11-18 16:05:48,087:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0174 (0.0169)
+2022-11-18 16:05:48,152:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0148 (0.0167)
+2022-11-18 16:05:48,218:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0198 (0.0170)
+2022-11-18 16:05:48,285:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0157 (0.0169)
+2022-11-18 16:05:48,351:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0161 (0.0168)
+2022-11-18 16:05:48,417:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0156 (0.0167)
+2022-11-18 16:05:48,483:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0154 (0.0166)
+2022-11-18 16:05:48,552:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0183 (0.0167)
+2022-11-18 16:05:48,619:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0173 (0.0168)
+2022-11-18 16:05:48,685:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0148 (0.0167)
+2022-11-18 16:05:48,746:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0176 (0.0167)
+2022-11-18 16:05:48,792:INFO: - Computing ADE (validation)
+2022-11-18 16:05:49,068:INFO: 		 ADE on hotel                     dataset:	 1.0261776447296143
+2022-11-18 16:05:49,455:INFO: 		 ADE on univ                      dataset:	 1.001044750213623
+2022-11-18 16:05:49,753:INFO: 		 ADE on zara1                     dataset:	 1.0880491733551025
+2022-11-18 16:05:50,238:INFO: 		 ADE on zara2                     dataset:	 0.7709067463874817
+2022-11-18 16:05:50,238:INFO: Average validation:	ADE  0.9231	FDE  1.5584
+2022-11-18 16:05:50,250:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_696.pth.tar
+2022-11-18 16:05:50,250:INFO: 
+===> EPOCH: 697 (P3)
+2022-11-18 16:05:50,251:INFO: - Computing loss (training)
+2022-11-18 16:05:50,517:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0544 (0.0544)
+2022-11-18 16:05:50,585:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0461 (0.0502)
+2022-11-18 16:05:50,653:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0429 (0.0477)
+2022-11-18 16:05:50,703:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0404 (0.0464)
+2022-11-18 16:05:51,021:INFO: Dataset: univ                Batch:  1/15	Loss 0.0200 (0.0200)
+2022-11-18 16:05:51,102:INFO: Dataset: univ                Batch:  2/15	Loss 0.0200 (0.0200)
+2022-11-18 16:05:51,181:INFO: Dataset: univ                Batch:  3/15	Loss 0.0199 (0.0200)
+2022-11-18 16:05:51,258:INFO: Dataset: univ                Batch:  4/15	Loss 0.0194 (0.0198)
+2022-11-18 16:05:51,337:INFO: Dataset: univ                Batch:  5/15	Loss 0.0210 (0.0201)
+2022-11-18 16:05:51,414:INFO: Dataset: univ                Batch:  6/15	Loss 0.0194 (0.0199)
+2022-11-18 16:05:51,489:INFO: Dataset: univ                Batch:  7/15	Loss 0.0209 (0.0201)
+2022-11-18 16:05:51,565:INFO: Dataset: univ                Batch:  8/15	Loss 0.0201 (0.0201)
+2022-11-18 16:05:51,640:INFO: Dataset: univ                Batch:  9/15	Loss 0.0210 (0.0202)
+2022-11-18 16:05:51,716:INFO: Dataset: univ                Batch: 10/15	Loss 0.0229 (0.0204)
+2022-11-18 16:05:51,793:INFO: Dataset: univ                Batch: 11/15	Loss 0.0205 (0.0204)
+2022-11-18 16:05:51,868:INFO: Dataset: univ                Batch: 12/15	Loss 0.0196 (0.0204)
+2022-11-18 16:05:51,944:INFO: Dataset: univ                Batch: 13/15	Loss 0.0192 (0.0203)
+2022-11-18 16:05:52,020:INFO: Dataset: univ                Batch: 14/15	Loss 0.0207 (0.0203)
+2022-11-18 16:05:52,054:INFO: Dataset: univ                Batch: 15/15	Loss 0.0197 (0.0203)
+2022-11-18 16:05:52,363:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0212 (0.0212)
+2022-11-18 16:05:52,431:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0246 (0.0229)
+2022-11-18 16:05:52,496:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0210 (0.0222)
+2022-11-18 16:05:52,563:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0242 (0.0227)
+2022-11-18 16:05:52,632:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0212 (0.0224)
+2022-11-18 16:05:52,699:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0211 (0.0222)
+2022-11-18 16:05:52,765:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0206 (0.0220)
+2022-11-18 16:05:52,826:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0204 (0.0218)
+2022-11-18 16:05:53,243:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0176 (0.0176)
+2022-11-18 16:05:53,317:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0178 (0.0177)
+2022-11-18 16:05:53,391:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0181 (0.0178)
+2022-11-18 16:05:53,466:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0158 (0.0174)
+2022-11-18 16:05:53,536:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0176 (0.0174)
+2022-11-18 16:05:53,609:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0179 (0.0175)
+2022-11-18 16:05:53,679:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0164 (0.0173)
+2022-11-18 16:05:53,749:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0168 (0.0173)
+2022-11-18 16:05:53,818:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0183 (0.0174)
+2022-11-18 16:05:53,887:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0150 (0.0171)
+2022-11-18 16:05:53,957:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0164 (0.0170)
+2022-11-18 16:05:54,028:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0163 (0.0170)
+2022-11-18 16:05:54,098:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0159 (0.0169)
+2022-11-18 16:05:54,170:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0178 (0.0170)
+2022-11-18 16:05:54,241:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0155 (0.0169)
+2022-11-18 16:05:54,311:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0164 (0.0168)
+2022-11-18 16:05:54,382:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0160 (0.0168)
+2022-11-18 16:05:54,447:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0137 (0.0166)
+2022-11-18 16:05:54,494:INFO: - Computing ADE (validation)
+2022-11-18 16:05:54,776:INFO: 		 ADE on hotel                     dataset:	 1.0053706169128418
+2022-11-18 16:05:55,148:INFO: 		 ADE on univ                      dataset:	 0.9982448220252991
+2022-11-18 16:05:55,436:INFO: 		 ADE on zara1                     dataset:	 1.0599485635757446
+2022-11-18 16:05:55,949:INFO: 		 ADE on zara2                     dataset:	 0.7674843072891235
+2022-11-18 16:05:55,949:INFO: Average validation:	ADE  0.9176	FDE  1.5510
+2022-11-18 16:05:55,962:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_697.pth.tar
+2022-11-18 16:05:55,962:INFO: 
+===> EPOCH: 698 (P3)
+2022-11-18 16:05:55,963:INFO: - Computing loss (training)
+2022-11-18 16:05:56,223:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0438 (0.0438)
+2022-11-18 16:05:56,290:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0442 (0.0440)
+2022-11-18 16:05:56,357:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0443 (0.0441)
+2022-11-18 16:05:56,406:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0570 (0.0464)
+2022-11-18 16:05:56,741:INFO: Dataset: univ                Batch:  1/15	Loss 0.0215 (0.0215)
+2022-11-18 16:05:56,822:INFO: Dataset: univ                Batch:  2/15	Loss 0.0197 (0.0206)
+2022-11-18 16:05:56,899:INFO: Dataset: univ                Batch:  3/15	Loss 0.0203 (0.0205)
+2022-11-18 16:05:56,980:INFO: Dataset: univ                Batch:  4/15	Loss 0.0210 (0.0206)
+2022-11-18 16:05:57,055:INFO: Dataset: univ                Batch:  5/15	Loss 0.0201 (0.0205)
+2022-11-18 16:05:57,132:INFO: Dataset: univ                Batch:  6/15	Loss 0.0192 (0.0203)
+2022-11-18 16:05:57,207:INFO: Dataset: univ                Batch:  7/15	Loss 0.0195 (0.0202)
+2022-11-18 16:05:57,284:INFO: Dataset: univ                Batch:  8/15	Loss 0.0191 (0.0201)
+2022-11-18 16:05:57,358:INFO: Dataset: univ                Batch:  9/15	Loss 0.0214 (0.0202)
+2022-11-18 16:05:57,433:INFO: Dataset: univ                Batch: 10/15	Loss 0.0217 (0.0204)
+2022-11-18 16:05:57,511:INFO: Dataset: univ                Batch: 11/15	Loss 0.0204 (0.0204)
+2022-11-18 16:05:57,585:INFO: Dataset: univ                Batch: 12/15	Loss 0.0197 (0.0203)
+2022-11-18 16:05:57,661:INFO: Dataset: univ                Batch: 13/15	Loss 0.0204 (0.0203)
+2022-11-18 16:05:57,737:INFO: Dataset: univ                Batch: 14/15	Loss 0.0197 (0.0203)
+2022-11-18 16:05:57,771:INFO: Dataset: univ                Batch: 15/15	Loss 0.0228 (0.0203)
+2022-11-18 16:05:58,075:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0218 (0.0218)
+2022-11-18 16:05:58,141:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0229 (0.0224)
+2022-11-18 16:05:58,208:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0210 (0.0218)
+2022-11-18 16:05:58,278:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0217 (0.0218)
+2022-11-18 16:05:58,344:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0224 (0.0219)
+2022-11-18 16:05:58,411:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0204 (0.0217)
+2022-11-18 16:05:58,477:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0219 (0.0217)
+2022-11-18 16:05:58,537:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0220 (0.0217)
+2022-11-18 16:05:58,842:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0182 (0.0182)
+2022-11-18 16:05:58,908:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0169 (0.0176)
+2022-11-18 16:05:58,976:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0169 (0.0174)
+2022-11-18 16:05:59,046:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0181 (0.0176)
+2022-11-18 16:05:59,113:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0167 (0.0174)
+2022-11-18 16:05:59,182:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0166 (0.0173)
+2022-11-18 16:05:59,248:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0175 (0.0173)
+2022-11-18 16:05:59,314:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0173 (0.0173)
+2022-11-18 16:05:59,379:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0150 (0.0170)
+2022-11-18 16:05:59,445:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0145 (0.0168)
+2022-11-18 16:05:59,511:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0162 (0.0167)
+2022-11-18 16:05:59,577:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0154 (0.0166)
+2022-11-18 16:05:59,643:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0173 (0.0167)
+2022-11-18 16:05:59,709:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0160 (0.0166)
+2022-11-18 16:05:59,777:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0173 (0.0167)
+2022-11-18 16:05:59,843:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0153 (0.0166)
+2022-11-18 16:05:59,910:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0182 (0.0167)
+2022-11-18 16:05:59,970:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0144 (0.0166)
+2022-11-18 16:06:00,017:INFO: - Computing ADE (validation)
+2022-11-18 16:06:00,296:INFO: 		 ADE on hotel                     dataset:	 1.0115430355072021
+2022-11-18 16:06:00,680:INFO: 		 ADE on univ                      dataset:	 0.9986853003501892
+2022-11-18 16:06:00,978:INFO: 		 ADE on zara1                     dataset:	 1.0728861093521118
+2022-11-18 16:06:01,464:INFO: 		 ADE on zara2                     dataset:	 0.7631382942199707
+2022-11-18 16:06:01,464:INFO: Average validation:	ADE  0.9173	FDE  1.5484
+2022-11-18 16:06:01,476:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_698.pth.tar
+2022-11-18 16:06:01,477:INFO: 
+===> EPOCH: 699 (P3)
+2022-11-18 16:06:01,477:INFO: - Computing loss (training)
+2022-11-18 16:06:01,737:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0469 (0.0469)
+2022-11-18 16:06:01,808:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0516 (0.0494)
+2022-11-18 16:06:01,877:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0403 (0.0462)
+2022-11-18 16:06:01,933:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0474 (0.0464)
+2022-11-18 16:06:02,254:INFO: Dataset: univ                Batch:  1/15	Loss 0.0194 (0.0194)
+2022-11-18 16:06:02,329:INFO: Dataset: univ                Batch:  2/15	Loss 0.0208 (0.0201)
+2022-11-18 16:06:02,406:INFO: Dataset: univ                Batch:  3/15	Loss 0.0184 (0.0195)
+2022-11-18 16:06:02,482:INFO: Dataset: univ                Batch:  4/15	Loss 0.0199 (0.0196)
+2022-11-18 16:06:02,555:INFO: Dataset: univ                Batch:  5/15	Loss 0.0201 (0.0197)
+2022-11-18 16:06:02,630:INFO: Dataset: univ                Batch:  6/15	Loss 0.0196 (0.0197)
+2022-11-18 16:06:02,701:INFO: Dataset: univ                Batch:  7/15	Loss 0.0205 (0.0198)
+2022-11-18 16:06:02,773:INFO: Dataset: univ                Batch:  8/15	Loss 0.0216 (0.0200)
+2022-11-18 16:06:02,846:INFO: Dataset: univ                Batch:  9/15	Loss 0.0212 (0.0202)
+2022-11-18 16:06:02,918:INFO: Dataset: univ                Batch: 10/15	Loss 0.0198 (0.0201)
+2022-11-18 16:06:02,990:INFO: Dataset: univ                Batch: 11/15	Loss 0.0197 (0.0201)
+2022-11-18 16:06:03,064:INFO: Dataset: univ                Batch: 12/15	Loss 0.0216 (0.0202)
+2022-11-18 16:06:03,136:INFO: Dataset: univ                Batch: 13/15	Loss 0.0212 (0.0203)
+2022-11-18 16:06:03,210:INFO: Dataset: univ                Batch: 14/15	Loss 0.0189 (0.0202)
+2022-11-18 16:06:03,243:INFO: Dataset: univ                Batch: 15/15	Loss 0.0249 (0.0202)
+2022-11-18 16:06:03,547:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0236 (0.0236)
+2022-11-18 16:06:03,618:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0197 (0.0216)
+2022-11-18 16:06:03,692:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0209 (0.0213)
+2022-11-18 16:06:03,761:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0208 (0.0212)
+2022-11-18 16:06:03,833:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0232 (0.0215)
+2022-11-18 16:06:03,903:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0214 (0.0215)
+2022-11-18 16:06:03,972:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0219 (0.0216)
+2022-11-18 16:06:04,035:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0215 (0.0216)
+2022-11-18 16:06:04,361:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0192 (0.0192)
+2022-11-18 16:06:04,431:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0168 (0.0180)
+2022-11-18 16:06:04,496:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0142 (0.0166)
+2022-11-18 16:06:04,562:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0176 (0.0169)
+2022-11-18 16:06:04,628:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0178 (0.0171)
+2022-11-18 16:06:04,699:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0177 (0.0172)
+2022-11-18 16:06:04,764:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0172 (0.0172)
+2022-11-18 16:06:04,830:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0150 (0.0169)
+2022-11-18 16:06:04,896:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0200 (0.0172)
+2022-11-18 16:06:04,961:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0163 (0.0171)
+2022-11-18 16:06:05,028:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0187 (0.0173)
+2022-11-18 16:06:05,095:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0152 (0.0171)
+2022-11-18 16:06:05,160:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0139 (0.0169)
+2022-11-18 16:06:05,227:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0168 (0.0169)
+2022-11-18 16:06:05,294:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0146 (0.0167)
+2022-11-18 16:06:05,361:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0160 (0.0167)
+2022-11-18 16:06:05,427:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0148 (0.0166)
+2022-11-18 16:06:05,488:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0146 (0.0165)
+2022-11-18 16:06:05,532:INFO: - Computing ADE (validation)
+2022-11-18 16:06:05,824:INFO: 		 ADE on hotel                     dataset:	 1.0158061981201172
+2022-11-18 16:06:06,218:INFO: 		 ADE on univ                      dataset:	 0.999443769454956
+2022-11-18 16:06:06,509:INFO: 		 ADE on zara1                     dataset:	 1.0546066761016846
+2022-11-18 16:06:06,985:INFO: 		 ADE on zara2                     dataset:	 0.7619032263755798
+2022-11-18 16:06:06,985:INFO: Average validation:	ADE  0.9164	FDE  1.5452
+2022-11-18 16:06:06,999:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_699.pth.tar
+2022-11-18 16:06:06,999:INFO: 
+===> EPOCH: 700 (P3)
+2022-11-18 16:06:07,000:INFO: - Computing loss (training)
+2022-11-18 16:06:07,257:INFO: Dataset: hotel               Batch: 1/4	Loss 0.0480 (0.0480)
+2022-11-18 16:06:07,328:INFO: Dataset: hotel               Batch: 2/4	Loss 0.0442 (0.0461)
+2022-11-18 16:06:07,396:INFO: Dataset: hotel               Batch: 3/4	Loss 0.0529 (0.0482)
+2022-11-18 16:06:07,446:INFO: Dataset: hotel               Batch: 4/4	Loss 0.0381 (0.0465)
+2022-11-18 16:06:07,760:INFO: Dataset: univ                Batch:  1/15	Loss 0.0208 (0.0208)
+2022-11-18 16:06:07,848:INFO: Dataset: univ                Batch:  2/15	Loss 0.0185 (0.0198)
+2022-11-18 16:06:07,930:INFO: Dataset: univ                Batch:  3/15	Loss 0.0203 (0.0199)
+2022-11-18 16:06:08,012:INFO: Dataset: univ                Batch:  4/15	Loss 0.0201 (0.0200)
+2022-11-18 16:06:08,092:INFO: Dataset: univ                Batch:  5/15	Loss 0.0205 (0.0201)
+2022-11-18 16:06:08,175:INFO: Dataset: univ                Batch:  6/15	Loss 0.0207 (0.0202)
+2022-11-18 16:06:08,256:INFO: Dataset: univ                Batch:  7/15	Loss 0.0200 (0.0201)
+2022-11-18 16:06:08,337:INFO: Dataset: univ                Batch:  8/15	Loss 0.0202 (0.0202)
+2022-11-18 16:06:08,416:INFO: Dataset: univ                Batch:  9/15	Loss 0.0199 (0.0201)
+2022-11-18 16:06:08,497:INFO: Dataset: univ                Batch: 10/15	Loss 0.0199 (0.0201)
+2022-11-18 16:06:08,580:INFO: Dataset: univ                Batch: 11/15	Loss 0.0194 (0.0200)
+2022-11-18 16:06:08,659:INFO: Dataset: univ                Batch: 12/15	Loss 0.0204 (0.0201)
+2022-11-18 16:06:08,740:INFO: Dataset: univ                Batch: 13/15	Loss 0.0210 (0.0201)
+2022-11-18 16:06:08,821:INFO: Dataset: univ                Batch: 14/15	Loss 0.0209 (0.0202)
+2022-11-18 16:06:08,857:INFO: Dataset: univ                Batch: 15/15	Loss 0.0201 (0.0202)
+2022-11-18 16:06:09,163:INFO: Dataset: zara1               Batch: 1/8	Loss 0.0200 (0.0200)
+2022-11-18 16:06:09,234:INFO: Dataset: zara1               Batch: 2/8	Loss 0.0221 (0.0210)
+2022-11-18 16:06:09,304:INFO: Dataset: zara1               Batch: 3/8	Loss 0.0211 (0.0211)
+2022-11-18 16:06:09,376:INFO: Dataset: zara1               Batch: 4/8	Loss 0.0201 (0.0208)
+2022-11-18 16:06:09,446:INFO: Dataset: zara1               Batch: 5/8	Loss 0.0239 (0.0214)
+2022-11-18 16:06:09,516:INFO: Dataset: zara1               Batch: 6/8	Loss 0.0224 (0.0216)
+2022-11-18 16:06:09,584:INFO: Dataset: zara1               Batch: 7/8	Loss 0.0213 (0.0215)
+2022-11-18 16:06:09,647:INFO: Dataset: zara1               Batch: 8/8	Loss 0.0212 (0.0215)
+2022-11-18 16:06:09,963:INFO: Dataset: zara2               Batch:  1/18	Loss 0.0172 (0.0172)
+2022-11-18 16:06:10,036:INFO: Dataset: zara2               Batch:  2/18	Loss 0.0182 (0.0177)
+2022-11-18 16:06:10,105:INFO: Dataset: zara2               Batch:  3/18	Loss 0.0135 (0.0161)
+2022-11-18 16:06:10,177:INFO: Dataset: zara2               Batch:  4/18	Loss 0.0188 (0.0168)
+2022-11-18 16:06:10,248:INFO: Dataset: zara2               Batch:  5/18	Loss 0.0145 (0.0164)
+2022-11-18 16:06:10,321:INFO: Dataset: zara2               Batch:  6/18	Loss 0.0159 (0.0163)
+2022-11-18 16:06:10,390:INFO: Dataset: zara2               Batch:  7/18	Loss 0.0171 (0.0164)
+2022-11-18 16:06:10,460:INFO: Dataset: zara2               Batch:  8/18	Loss 0.0152 (0.0162)
+2022-11-18 16:06:10,529:INFO: Dataset: zara2               Batch:  9/18	Loss 0.0150 (0.0161)
+2022-11-18 16:06:10,598:INFO: Dataset: zara2               Batch: 10/18	Loss 0.0174 (0.0162)
+2022-11-18 16:06:10,668:INFO: Dataset: zara2               Batch: 11/18	Loss 0.0203 (0.0166)
+2022-11-18 16:06:10,738:INFO: Dataset: zara2               Batch: 12/18	Loss 0.0165 (0.0166)
+2022-11-18 16:06:10,807:INFO: Dataset: zara2               Batch: 13/18	Loss 0.0154 (0.0165)
+2022-11-18 16:06:10,878:INFO: Dataset: zara2               Batch: 14/18	Loss 0.0146 (0.0163)
+2022-11-18 16:06:10,948:INFO: Dataset: zara2               Batch: 15/18	Loss 0.0163 (0.0163)
+2022-11-18 16:06:11,017:INFO: Dataset: zara2               Batch: 16/18	Loss 0.0171 (0.0164)
+2022-11-18 16:06:11,088:INFO: Dataset: zara2               Batch: 17/18	Loss 0.0160 (0.0163)
+2022-11-18 16:06:11,152:INFO: Dataset: zara2               Batch: 18/18	Loss 0.0170 (0.0164)
+2022-11-18 16:06:11,193:INFO: - Computing ADE (validation)
+2022-11-18 16:06:11,475:INFO: 		 ADE on hotel                     dataset:	 1.019062876701355
+2022-11-18 16:06:11,859:INFO: 		 ADE on univ                      dataset:	 0.9968257546424866
+2022-11-18 16:06:12,148:INFO: 		 ADE on zara1                     dataset:	 1.0665812492370605
+2022-11-18 16:06:12,633:INFO: 		 ADE on zara2                     dataset:	 0.7595105171203613
+2022-11-18 16:06:12,633:INFO: Average validation:	ADE  0.9151	FDE  1.5441
+2022-11-18 16:06:12,646:INFO:  --> Model Saved in ./models/E1//P3/CRMF_epoch_700.pth.tar
+2022-11-18 16:06:12,646:INFO: 
+===> EPOCH: 701 (P4)
+2022-11-18 16:06:12,647:INFO: - Computing loss (training)
+2022-11-18 16:06:12,835:INFO: Dataset: hotel               Batch: 1/4	Loss 4.0698 (4.0698)
+2022-11-18 16:06:12,838:INFO: Dataset: hotel               Batch: 2/4	Loss 4.0656 (4.0677)
+2022-11-18 16:06:12,840:INFO: Dataset: hotel               Batch: 3/4	Loss 4.0666 (4.0674)
+2022-11-18 16:06:12,844:INFO: Dataset: hotel               Batch: 4/4	Loss 4.0633 (4.0666)
+2022-11-18 16:06:13,083:INFO: Dataset: univ                Batch:  1/15	Loss 4.4176 (4.4176)
+2022-11-18 16:06:13,089:INFO: Dataset: univ                Batch:  2/15	Loss 4.4192 (4.4184)
+2022-11-18 16:06:13,165:INFO: Dataset: univ                Batch:  3/15	Loss 4.4156 (4.4174)
+2022-11-18 16:06:13,167:INFO: Dataset: univ                Batch:  4/15	Loss 4.4116 (4.4159)
+2022-11-18 16:06:13,168:INFO: Dataset: univ                Batch:  5/15	Loss 4.4125 (4.4152)
+2022-11-18 16:06:13,171:INFO: Dataset: univ                Batch:  6/15	Loss 4.4160 (4.4154)
+2022-11-18 16:06:13,172:INFO: Dataset: univ                Batch:  7/15	Loss 4.4099 (4.4146)
+2022-11-18 16:06:13,173:INFO: Dataset: univ                Batch:  8/15	Loss 4.4053 (4.4135)
+2022-11-18 16:06:13,175:INFO: Dataset: univ                Batch:  9/15	Loss 4.4073 (4.4129)
+2022-11-18 16:06:13,176:INFO: Dataset: univ                Batch: 10/15	Loss 4.4026 (4.4118)
+2022-11-18 16:06:13,178:INFO: Dataset: univ                Batch: 11/15	Loss 4.4004 (4.4108)
+2022-11-18 16:06:13,180:INFO: Dataset: univ                Batch: 12/15	Loss 4.3975 (4.4097)
+2022-11-18 16:06:13,181:INFO: Dataset: univ                Batch: 13/15	Loss 4.3977 (4.4088)
+2022-11-18 16:06:13,182:INFO: Dataset: univ                Batch: 14/15	Loss 4.3944 (4.4077)
+2022-11-18 16:06:13,184:INFO: Dataset: univ                Batch: 15/15	Loss 4.3946 (4.4076)
+2022-11-18 16:06:13,423:INFO: Dataset: zara1               Batch: 1/8	Loss 4.8830 (4.8830)
+2022-11-18 16:06:13,426:INFO: Dataset: zara1               Batch: 2/8	Loss 4.8815 (4.8821)
+2022-11-18 16:06:13,427:INFO: Dataset: zara1               Batch: 3/8	Loss 4.8771 (4.8804)
+2022-11-18 16:06:13,428:INFO: Dataset: zara1               Batch: 4/8	Loss 4.8756 (4.8792)
+2022-11-18 16:06:13,432:INFO: Dataset: zara1               Batch: 5/8	Loss 4.8626 (4.8757)
+2022-11-18 16:06:13,451:INFO: Dataset: zara1               Batch: 6/8	Loss 4.8746 (4.8755)
+2022-11-18 16:06:13,452:INFO: Dataset: zara1               Batch: 7/8	Loss 4.8737 (4.8753)
+2022-11-18 16:06:13,453:INFO: Dataset: zara1               Batch: 8/8	Loss 4.8710 (4.8748)
+2022-11-18 16:06:13,738:INFO: Dataset: zara2               Batch:  1/18	Loss 4.9150 (4.9150)
+2022-11-18 16:06:13,739:INFO: Dataset: zara2               Batch:  2/18	Loss 4.9141 (4.9146)
+2022-11-18 16:06:13,742:INFO: Dataset: zara2               Batch:  3/18	Loss 4.9083 (4.9126)
+2022-11-18 16:06:13,743:INFO: Dataset: zara2               Batch:  4/18	Loss 4.9149 (4.9133)
+2022-11-18 16:06:13,745:INFO: Dataset: zara2               Batch:  5/18	Loss 4.9143 (4.9135)
+2022-11-18 16:06:13,749:INFO: Dataset: zara2               Batch:  6/18	Loss 4.9078 (4.9126)
+2022-11-18 16:06:13,750:INFO: Dataset: zara2               Batch:  7/18	Loss 4.9087 (4.9120)
+2022-11-18 16:06:13,752:INFO: Dataset: zara2               Batch:  8/18	Loss 4.9070 (4.9115)
+2022-11-18 16:06:13,753:INFO: Dataset: zara2               Batch:  9/18	Loss 4.9060 (4.9109)
+2022-11-18 16:06:13,754:INFO: Dataset: zara2               Batch: 10/18	Loss 4.8996 (4.9098)
+2022-11-18 16:06:13,755:INFO: Dataset: zara2               Batch: 11/18	Loss 4.9033 (4.9091)
+2022-11-18 16:06:13,757:INFO: Dataset: zara2               Batch: 12/18	Loss 4.9050 (4.9088)
+2022-11-18 16:06:13,758:INFO: Dataset: zara2               Batch: 13/18	Loss 4.8964 (4.9079)
+2022-11-18 16:06:13,760:INFO: Dataset: zara2               Batch: 14/18	Loss 4.8926 (4.9068)
+2022-11-18 16:06:13,762:INFO: Dataset: zara2               Batch: 15/18	Loss 4.9007 (4.9064)
+2022-11-18 16:06:13,764:INFO: Dataset: zara2               Batch: 16/18	Loss 4.8931 (4.9056)
+2022-11-18 16:06:13,765:INFO: Dataset: zara2               Batch: 17/18	Loss 4.8898 (4.9046)
+2022-11-18 16:06:13,767:INFO: Dataset: zara2               Batch: 18/18	Loss 4.8907 (4.9040)
+2022-11-18 16:06:13,809:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:14,088:INFO: 		 ADE on eth                       dataset:	 1.8247572183609009
+2022-11-18 16:06:14,088:INFO: Average validation o:	ADE  1.8248	FDE  2.7385
+2022-11-18 16:06:14,089:INFO: - Computing loss (validation)
+2022-11-18 16:06:14,276:INFO: Dataset: hotel               Batch: 1/2	Loss 4.0118 (4.0118)
+2022-11-18 16:06:14,277:INFO: Dataset: hotel               Batch: 2/2	Loss 4.0079 (4.0114)
+2022-11-18 16:06:14,503:INFO: Dataset: univ                Batch: 1/3	Loss 4.3476 (4.3476)
+2022-11-18 16:06:14,505:INFO: Dataset: univ                Batch: 2/3	Loss 4.3482 (4.3479)
+2022-11-18 16:06:14,507:INFO: Dataset: univ                Batch: 3/3	Loss 4.3478 (4.3479)
+2022-11-18 16:06:14,740:INFO: Dataset: zara1               Batch: 1/2	Loss 4.8669 (4.8669)
+2022-11-18 16:06:14,741:INFO: Dataset: zara1               Batch: 2/2	Loss 4.8774 (4.8695)
+2022-11-18 16:06:14,968:INFO: Dataset: zara2               Batch: 1/5	Loss 4.8907 (4.8907)
+2022-11-18 16:06:14,980:INFO: Dataset: zara2               Batch: 2/5	Loss 4.8947 (4.8927)
+2022-11-18 16:06:14,981:INFO: Dataset: zara2               Batch: 3/5	Loss 4.8966 (4.8940)
+2022-11-18 16:06:14,981:INFO: Dataset: zara2               Batch: 4/5	Loss 4.8892 (4.8927)
+2022-11-18 16:06:14,983:INFO: Dataset: zara2               Batch: 5/5	Loss 4.8915 (4.8925)
+2022-11-18 16:06:15,040:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_701.pth.tar
+2022-11-18 16:06:15,040:INFO: 
+===> EPOCH: 702 (P4)
+2022-11-18 16:06:15,041:INFO: - Computing loss (training)
+2022-11-18 16:06:15,228:INFO: Dataset: hotel               Batch: 1/4	Loss 3.9936 (3.9936)
+2022-11-18 16:06:15,232:INFO: Dataset: hotel               Batch: 2/4	Loss 4.0018 (3.9979)
+2022-11-18 16:06:15,239:INFO: Dataset: hotel               Batch: 3/4	Loss 3.9959 (3.9972)
+2022-11-18 16:06:15,240:INFO: Dataset: hotel               Batch: 4/4	Loss 3.9916 (3.9963)
+2022-11-18 16:06:15,484:INFO: Dataset: univ                Batch:  1/15	Loss 4.3410 (4.3410)
+2022-11-18 16:06:15,487:INFO: Dataset: univ                Batch:  2/15	Loss 4.3399 (4.3404)
+2022-11-18 16:06:15,491:INFO: Dataset: univ                Batch:  3/15	Loss 4.3400 (4.3403)
+2022-11-18 16:06:15,525:INFO: Dataset: univ                Batch:  4/15	Loss 4.3400 (4.3402)
+2022-11-18 16:06:15,526:INFO: Dataset: univ                Batch:  5/15	Loss 4.3336 (4.3389)
+2022-11-18 16:06:15,535:INFO: Dataset: univ                Batch:  6/15	Loss 4.3362 (4.3385)
+2022-11-18 16:06:15,536:INFO: Dataset: univ                Batch:  7/15	Loss 4.3333 (4.3377)
+2022-11-18 16:06:15,537:INFO: Dataset: univ                Batch:  8/15	Loss 4.3283 (4.3366)
+2022-11-18 16:06:15,539:INFO: Dataset: univ                Batch:  9/15	Loss 4.3296 (4.3358)
+2022-11-18 16:06:15,540:INFO: Dataset: univ                Batch: 10/15	Loss 4.3276 (4.3349)
+2022-11-18 16:06:15,541:INFO: Dataset: univ                Batch: 11/15	Loss 4.3264 (4.3342)
+2022-11-18 16:06:15,543:INFO: Dataset: univ                Batch: 12/15	Loss 4.3235 (4.3333)
+2022-11-18 16:06:15,544:INFO: Dataset: univ                Batch: 13/15	Loss 4.3224 (4.3324)
+2022-11-18 16:06:15,546:INFO: Dataset: univ                Batch: 14/15	Loss 4.3228 (4.3317)
+2022-11-18 16:06:15,547:INFO: Dataset: univ                Batch: 15/15	Loss 4.3171 (4.3315)
+2022-11-18 16:06:15,806:INFO: Dataset: zara1               Batch: 1/8	Loss 4.7918 (4.7918)
+2022-11-18 16:06:15,808:INFO: Dataset: zara1               Batch: 2/8	Loss 4.7877 (4.7898)
+2022-11-18 16:06:15,810:INFO: Dataset: zara1               Batch: 3/8	Loss 4.7764 (4.7852)
+2022-11-18 16:06:15,813:INFO: Dataset: zara1               Batch: 4/8	Loss 4.7705 (4.7814)
+2022-11-18 16:06:15,814:INFO: Dataset: zara1               Batch: 5/8	Loss 4.7811 (4.7814)
+2022-11-18 16:06:15,882:INFO: Dataset: zara1               Batch: 6/8	Loss 4.7700 (4.7795)
+2022-11-18 16:06:15,887:INFO: Dataset: zara1               Batch: 7/8	Loss 4.7726 (4.7785)
+2022-11-18 16:06:15,891:INFO: Dataset: zara1               Batch: 8/8	Loss 4.7634 (4.7770)
+2022-11-18 16:06:16,152:INFO: Dataset: zara2               Batch:  1/18	Loss 4.8340 (4.8340)
+2022-11-18 16:06:16,154:INFO: Dataset: zara2               Batch:  2/18	Loss 4.8431 (4.8384)
+2022-11-18 16:06:16,157:INFO: Dataset: zara2               Batch:  3/18	Loss 4.8337 (4.8367)
+2022-11-18 16:06:16,158:INFO: Dataset: zara2               Batch:  4/18	Loss 4.8353 (4.8363)
+2022-11-18 16:06:16,209:INFO: Dataset: zara2               Batch:  5/18	Loss 4.8277 (4.8346)
+2022-11-18 16:06:16,217:INFO: Dataset: zara2               Batch:  6/18	Loss 4.8297 (4.8338)
+2022-11-18 16:06:16,219:INFO: Dataset: zara2               Batch:  7/18	Loss 4.8324 (4.8336)
+2022-11-18 16:06:16,220:INFO: Dataset: zara2               Batch:  8/18	Loss 4.8239 (4.8322)
+2022-11-18 16:06:16,221:INFO: Dataset: zara2               Batch:  9/18	Loss 4.8198 (4.8309)
+2022-11-18 16:06:16,222:INFO: Dataset: zara2               Batch: 10/18	Loss 4.8185 (4.8296)
+2022-11-18 16:06:16,224:INFO: Dataset: zara2               Batch: 11/18	Loss 4.8154 (4.8283)
+2022-11-18 16:06:16,226:INFO: Dataset: zara2               Batch: 12/18	Loss 4.8172 (4.8274)
+2022-11-18 16:06:16,227:INFO: Dataset: zara2               Batch: 13/18	Loss 4.8225 (4.8270)
+2022-11-18 16:06:16,228:INFO: Dataset: zara2               Batch: 14/18	Loss 4.8099 (4.8259)
+2022-11-18 16:06:16,229:INFO: Dataset: zara2               Batch: 15/18	Loss 4.8146 (4.8251)
+2022-11-18 16:06:16,230:INFO: Dataset: zara2               Batch: 16/18	Loss 4.8106 (4.8243)
+2022-11-18 16:06:16,232:INFO: Dataset: zara2               Batch: 17/18	Loss 4.8114 (4.8235)
+2022-11-18 16:06:16,234:INFO: Dataset: zara2               Batch: 18/18	Loss 4.8060 (4.8227)
+2022-11-18 16:06:16,283:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:16,567:INFO: 		 ADE on eth                       dataset:	 1.7954068183898926
+2022-11-18 16:06:16,567:INFO: Average validation o:	ADE  1.7954	FDE  2.6575
+2022-11-18 16:06:16,568:INFO: - Computing loss (validation)
+2022-11-18 16:06:16,756:INFO: Dataset: hotel               Batch: 1/2	Loss 3.9440 (3.9440)
+2022-11-18 16:06:16,758:INFO: Dataset: hotel               Batch: 2/2	Loss 3.9555 (3.9449)
+2022-11-18 16:06:17,003:INFO: Dataset: univ                Batch: 1/3	Loss 4.2693 (4.2693)
+2022-11-18 16:06:17,004:INFO: Dataset: univ                Batch: 2/3	Loss 4.2626 (4.2664)
+2022-11-18 16:06:17,006:INFO: Dataset: univ                Batch: 3/3	Loss 4.2669 (4.2666)
+2022-11-18 16:06:17,241:INFO: Dataset: zara1               Batch: 1/2	Loss 4.7574 (4.7574)
+2022-11-18 16:06:17,243:INFO: Dataset: zara1               Batch: 2/2	Loss 4.7661 (4.7598)
+2022-11-18 16:06:17,480:INFO: Dataset: zara2               Batch: 1/5	Loss 4.8091 (4.8091)
+2022-11-18 16:06:17,481:INFO: Dataset: zara2               Batch: 2/5	Loss 4.8180 (4.8133)
+2022-11-18 16:06:17,484:INFO: Dataset: zara2               Batch: 3/5	Loss 4.8172 (4.8146)
+2022-11-18 16:06:17,485:INFO: Dataset: zara2               Batch: 4/5	Loss 4.8173 (4.8153)
+2022-11-18 16:06:17,486:INFO: Dataset: zara2               Batch: 5/5	Loss 4.8133 (4.8149)
+2022-11-18 16:06:17,545:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_702.pth.tar
+2022-11-18 16:06:17,545:INFO: 
+===> EPOCH: 703 (P4)
+2022-11-18 16:06:17,546:INFO: - Computing loss (training)
+2022-11-18 16:06:17,732:INFO: Dataset: hotel               Batch: 1/4	Loss 3.9242 (3.9242)
+2022-11-18 16:06:17,735:INFO: Dataset: hotel               Batch: 2/4	Loss 3.9279 (3.9260)
+2022-11-18 16:06:17,738:INFO: Dataset: hotel               Batch: 3/4	Loss 3.9144 (3.9223)
+2022-11-18 16:06:17,740:INFO: Dataset: hotel               Batch: 4/4	Loss 3.9303 (3.9237)
+2022-11-18 16:06:17,993:INFO: Dataset: univ                Batch:  1/15	Loss 4.2639 (4.2639)
+2022-11-18 16:06:18,040:INFO: Dataset: univ                Batch:  2/15	Loss 4.2624 (4.2632)
+2022-11-18 16:06:18,042:INFO: Dataset: univ                Batch:  3/15	Loss 4.2590 (4.2619)
+2022-11-18 16:06:18,043:INFO: Dataset: univ                Batch:  4/15	Loss 4.2575 (4.2609)
+2022-11-18 16:06:18,044:INFO: Dataset: univ                Batch:  5/15	Loss 4.2579 (4.2603)
+2022-11-18 16:06:18,051:INFO: Dataset: univ                Batch:  6/15	Loss 4.2577 (4.2599)
+2022-11-18 16:06:18,052:INFO: Dataset: univ                Batch:  7/15	Loss 4.2567 (4.2594)
+2022-11-18 16:06:18,053:INFO: Dataset: univ                Batch:  8/15	Loss 4.2518 (4.2583)
+2022-11-18 16:06:18,055:INFO: Dataset: univ                Batch:  9/15	Loss 4.2525 (4.2577)
+2022-11-18 16:06:18,056:INFO: Dataset: univ                Batch: 10/15	Loss 4.2476 (4.2567)
+2022-11-18 16:06:18,057:INFO: Dataset: univ                Batch: 11/15	Loss 4.2450 (4.2556)
+2022-11-18 16:06:18,059:INFO: Dataset: univ                Batch: 12/15	Loss 4.2451 (4.2547)
+2022-11-18 16:06:18,060:INFO: Dataset: univ                Batch: 13/15	Loss 4.2419 (4.2537)
+2022-11-18 16:06:18,062:INFO: Dataset: univ                Batch: 14/15	Loss 4.2431 (4.2529)
+2022-11-18 16:06:18,063:INFO: Dataset: univ                Batch: 15/15	Loss 4.2317 (4.2528)
+2022-11-18 16:06:18,306:INFO: Dataset: zara1               Batch: 1/8	Loss 4.6759 (4.6759)
+2022-11-18 16:06:18,309:INFO: Dataset: zara1               Batch: 2/8	Loss 4.6892 (4.6827)
+2022-11-18 16:06:18,310:INFO: Dataset: zara1               Batch: 3/8	Loss 4.6756 (4.6803)
+2022-11-18 16:06:18,312:INFO: Dataset: zara1               Batch: 4/8	Loss 4.6744 (4.6787)
+2022-11-18 16:06:18,316:INFO: Dataset: zara1               Batch: 5/8	Loss 4.6695 (4.6769)
+2022-11-18 16:06:18,358:INFO: Dataset: zara1               Batch: 6/8	Loss 4.6715 (4.6759)
+2022-11-18 16:06:18,362:INFO: Dataset: zara1               Batch: 7/8	Loss 4.6718 (4.6754)
+2022-11-18 16:06:18,367:INFO: Dataset: zara1               Batch: 8/8	Loss 4.6664 (4.6743)
+2022-11-18 16:06:18,621:INFO: Dataset: zara2               Batch:  1/18	Loss 4.7433 (4.7433)
+2022-11-18 16:06:18,714:INFO: Dataset: zara2               Batch:  2/18	Loss 4.7490 (4.7461)
+2022-11-18 16:06:18,718:INFO: Dataset: zara2               Batch:  3/18	Loss 4.7558 (4.7493)
+2022-11-18 16:06:18,719:INFO: Dataset: zara2               Batch:  4/18	Loss 4.7406 (4.7472)
+2022-11-18 16:06:18,721:INFO: Dataset: zara2               Batch:  5/18	Loss 4.7485 (4.7475)
+2022-11-18 16:06:18,725:INFO: Dataset: zara2               Batch:  6/18	Loss 4.7493 (4.7478)
+2022-11-18 16:06:18,726:INFO: Dataset: zara2               Batch:  7/18	Loss 4.7488 (4.7479)
+2022-11-18 16:06:18,728:INFO: Dataset: zara2               Batch:  8/18	Loss 4.7375 (4.7465)
+2022-11-18 16:06:18,729:INFO: Dataset: zara2               Batch:  9/18	Loss 4.7367 (4.7453)
+2022-11-18 16:06:18,730:INFO: Dataset: zara2               Batch: 10/18	Loss 4.7292 (4.7436)
+2022-11-18 16:06:18,732:INFO: Dataset: zara2               Batch: 11/18	Loss 4.7315 (4.7425)
+2022-11-18 16:06:18,734:INFO: Dataset: zara2               Batch: 12/18	Loss 4.7242 (4.7408)
+2022-11-18 16:06:18,735:INFO: Dataset: zara2               Batch: 13/18	Loss 4.7203 (4.7393)
+2022-11-18 16:06:18,737:INFO: Dataset: zara2               Batch: 14/18	Loss 4.7277 (4.7384)
+2022-11-18 16:06:18,739:INFO: Dataset: zara2               Batch: 15/18	Loss 4.7280 (4.7377)
+2022-11-18 16:06:18,741:INFO: Dataset: zara2               Batch: 16/18	Loss 4.7222 (4.7368)
+2022-11-18 16:06:18,742:INFO: Dataset: zara2               Batch: 17/18	Loss 4.7188 (4.7357)
+2022-11-18 16:06:18,744:INFO: Dataset: zara2               Batch: 18/18	Loss 4.7057 (4.7343)
+2022-11-18 16:06:18,786:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:19,062:INFO: 		 ADE on eth                       dataset:	 1.797957181930542
+2022-11-18 16:06:19,062:INFO: Average validation o:	ADE  1.7980	FDE  2.6739
+2022-11-18 16:06:19,063:INFO: - Computing loss (validation)
+2022-11-18 16:06:19,251:INFO: Dataset: hotel               Batch: 1/2	Loss 3.8719 (3.8719)
+2022-11-18 16:06:19,252:INFO: Dataset: hotel               Batch: 2/2	Loss 3.8473 (3.8706)
+2022-11-18 16:06:19,489:INFO: Dataset: univ                Batch: 1/3	Loss 4.1714 (4.1714)
+2022-11-18 16:06:19,498:INFO: Dataset: univ                Batch: 2/3	Loss 4.1785 (4.1750)
+2022-11-18 16:06:19,499:INFO: Dataset: univ                Batch: 3/3	Loss 4.1825 (4.1775)
+2022-11-18 16:06:19,737:INFO: Dataset: zara1               Batch: 1/2	Loss 4.6388 (4.6388)
+2022-11-18 16:06:19,738:INFO: Dataset: zara1               Batch: 2/2	Loss 4.6448 (4.6401)
+2022-11-18 16:06:19,991:INFO: Dataset: zara2               Batch: 1/5	Loss 4.7210 (4.7210)
+2022-11-18 16:06:19,994:INFO: Dataset: zara2               Batch: 2/5	Loss 4.7328 (4.7271)
+2022-11-18 16:06:19,995:INFO: Dataset: zara2               Batch: 3/5	Loss 4.7291 (4.7277)
+2022-11-18 16:06:19,997:INFO: Dataset: zara2               Batch: 4/5	Loss 4.7318 (4.7287)
+2022-11-18 16:06:19,999:INFO: Dataset: zara2               Batch: 5/5	Loss 4.7315 (4.7293)
+2022-11-18 16:06:20,057:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_703.pth.tar
+2022-11-18 16:06:20,057:INFO: 
+===> EPOCH: 704 (P4)
+2022-11-18 16:06:20,057:INFO: - Computing loss (training)
+2022-11-18 16:06:20,236:INFO: Dataset: hotel               Batch: 1/4	Loss 3.8421 (3.8421)
+2022-11-18 16:06:20,239:INFO: Dataset: hotel               Batch: 2/4	Loss 3.8439 (3.8430)
+2022-11-18 16:06:20,241:INFO: Dataset: hotel               Batch: 3/4	Loss 3.8389 (3.8416)
+2022-11-18 16:06:20,251:INFO: Dataset: hotel               Batch: 4/4	Loss 3.8495 (3.8431)
+2022-11-18 16:06:20,511:INFO: Dataset: univ                Batch:  1/15	Loss 4.1815 (4.1815)
+2022-11-18 16:06:20,515:INFO: Dataset: univ                Batch:  2/15	Loss 4.1783 (4.1800)
+2022-11-18 16:06:20,517:INFO: Dataset: univ                Batch:  3/15	Loss 4.1732 (4.1777)
+2022-11-18 16:06:20,518:INFO: Dataset: univ                Batch:  4/15	Loss 4.1669 (4.1751)
+2022-11-18 16:06:20,569:INFO: Dataset: univ                Batch:  5/15	Loss 4.1730 (4.1747)
+2022-11-18 16:06:20,577:INFO: Dataset: univ                Batch:  6/15	Loss 4.1649 (4.1731)
+2022-11-18 16:06:20,578:INFO: Dataset: univ                Batch:  7/15	Loss 4.1685 (4.1723)
+2022-11-18 16:06:20,580:INFO: Dataset: univ                Batch:  8/15	Loss 4.1649 (4.1714)
+2022-11-18 16:06:20,581:INFO: Dataset: univ                Batch:  9/15	Loss 4.1643 (4.1705)
+2022-11-18 16:06:20,582:INFO: Dataset: univ                Batch: 10/15	Loss 4.1538 (4.1690)
+2022-11-18 16:06:20,585:INFO: Dataset: univ                Batch: 11/15	Loss 4.1547 (4.1678)
+2022-11-18 16:06:20,586:INFO: Dataset: univ                Batch: 12/15	Loss 4.1626 (4.1673)
+2022-11-18 16:06:20,587:INFO: Dataset: univ                Batch: 13/15	Loss 4.1593 (4.1667)
+2022-11-18 16:06:20,589:INFO: Dataset: univ                Batch: 14/15	Loss 4.1525 (4.1657)
+2022-11-18 16:06:20,590:INFO: Dataset: univ                Batch: 15/15	Loss 4.1490 (4.1655)
+2022-11-18 16:06:20,835:INFO: Dataset: zara1               Batch: 1/8	Loss 4.5701 (4.5701)
+2022-11-18 16:06:20,837:INFO: Dataset: zara1               Batch: 2/8	Loss 4.5663 (4.5683)
+2022-11-18 16:06:20,839:INFO: Dataset: zara1               Batch: 3/8	Loss 4.5562 (4.5641)
+2022-11-18 16:06:20,842:INFO: Dataset: zara1               Batch: 4/8	Loss 4.5619 (4.5635)
+2022-11-18 16:06:20,843:INFO: Dataset: zara1               Batch: 5/8	Loss 4.5564 (4.5620)
+2022-11-18 16:06:20,858:INFO: Dataset: zara1               Batch: 6/8	Loss 4.5554 (4.5610)
+2022-11-18 16:06:20,860:INFO: Dataset: zara1               Batch: 7/8	Loss 4.5552 (4.5602)
+2022-11-18 16:06:20,861:INFO: Dataset: zara1               Batch: 8/8	Loss 4.5468 (4.5587)
+2022-11-18 16:06:21,096:INFO: Dataset: zara2               Batch:  1/18	Loss 4.6531 (4.6531)
+2022-11-18 16:06:21,117:INFO: Dataset: zara2               Batch:  2/18	Loss 4.6532 (4.6532)
+2022-11-18 16:06:21,120:INFO: Dataset: zara2               Batch:  3/18	Loss 4.6630 (4.6564)
+2022-11-18 16:06:21,123:INFO: Dataset: zara2               Batch:  4/18	Loss 4.6468 (4.6538)
+2022-11-18 16:06:21,169:INFO: Dataset: zara2               Batch:  5/18	Loss 4.6432 (4.6518)
+2022-11-18 16:06:21,181:INFO: Dataset: zara2               Batch:  6/18	Loss 4.6410 (4.6499)
+2022-11-18 16:06:21,182:INFO: Dataset: zara2               Batch:  7/18	Loss 4.6416 (4.6487)
+2022-11-18 16:06:21,183:INFO: Dataset: zara2               Batch:  8/18	Loss 4.6359 (4.6471)
+2022-11-18 16:06:21,184:INFO: Dataset: zara2               Batch:  9/18	Loss 4.6316 (4.6453)
+2022-11-18 16:06:21,186:INFO: Dataset: zara2               Batch: 10/18	Loss 4.6240 (4.6433)
+2022-11-18 16:06:21,188:INFO: Dataset: zara2               Batch: 11/18	Loss 4.6312 (4.6423)
+2022-11-18 16:06:21,189:INFO: Dataset: zara2               Batch: 12/18	Loss 4.6286 (4.6411)
+2022-11-18 16:06:21,190:INFO: Dataset: zara2               Batch: 13/18	Loss 4.6257 (4.6399)
+2022-11-18 16:06:21,192:INFO: Dataset: zara2               Batch: 14/18	Loss 4.6217 (4.6385)
+2022-11-18 16:06:21,193:INFO: Dataset: zara2               Batch: 15/18	Loss 4.6349 (4.6382)
+2022-11-18 16:06:21,194:INFO: Dataset: zara2               Batch: 16/18	Loss 4.6126 (4.6366)
+2022-11-18 16:06:21,196:INFO: Dataset: zara2               Batch: 17/18	Loss 4.6189 (4.6356)
+2022-11-18 16:06:21,198:INFO: Dataset: zara2               Batch: 18/18	Loss 4.6187 (4.6347)
+2022-11-18 16:06:21,241:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:21,517:INFO: 		 ADE on eth                       dataset:	 1.8121063709259033
+2022-11-18 16:06:21,517:INFO: Average validation o:	ADE  1.8121	FDE  2.7070
+2022-11-18 16:06:21,518:INFO: - Computing loss (validation)
+2022-11-18 16:06:21,715:INFO: Dataset: hotel               Batch: 1/2	Loss 3.7855 (3.7855)
+2022-11-18 16:06:21,716:INFO: Dataset: hotel               Batch: 2/2	Loss 3.7962 (3.7862)
+2022-11-18 16:06:21,953:INFO: Dataset: univ                Batch: 1/3	Loss 4.0831 (4.0831)
+2022-11-18 16:06:21,955:INFO: Dataset: univ                Batch: 2/3	Loss 4.0744 (4.0791)
+2022-11-18 16:06:21,957:INFO: Dataset: univ                Batch: 3/3	Loss 4.0735 (4.0773)
+2022-11-18 16:06:22,220:INFO: Dataset: zara1               Batch: 1/2	Loss 4.5069 (4.5069)
+2022-11-18 16:06:22,221:INFO: Dataset: zara1               Batch: 2/2	Loss 4.5087 (4.5073)
+2022-11-18 16:06:22,464:INFO: Dataset: zara2               Batch: 1/5	Loss 4.6309 (4.6309)
+2022-11-18 16:06:22,465:INFO: Dataset: zara2               Batch: 2/5	Loss 4.6270 (4.6290)
+2022-11-18 16:06:22,467:INFO: Dataset: zara2               Batch: 3/5	Loss 4.6334 (4.6305)
+2022-11-18 16:06:22,478:INFO: Dataset: zara2               Batch: 4/5	Loss 4.6279 (4.6298)
+2022-11-18 16:06:22,479:INFO: Dataset: zara2               Batch: 5/5	Loss 4.6421 (4.6325)
+2022-11-18 16:06:22,536:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_704.pth.tar
+2022-11-18 16:06:22,536:INFO: 
+===> EPOCH: 705 (P4)
+2022-11-18 16:06:22,537:INFO: - Computing loss (training)
+2022-11-18 16:06:22,731:INFO: Dataset: hotel               Batch: 1/4	Loss 3.7562 (3.7562)
+2022-11-18 16:06:22,734:INFO: Dataset: hotel               Batch: 2/4	Loss 3.7598 (3.7579)
+2022-11-18 16:06:22,737:INFO: Dataset: hotel               Batch: 3/4	Loss 3.7341 (3.7499)
+2022-11-18 16:06:22,738:INFO: Dataset: hotel               Batch: 4/4	Loss 3.7630 (3.7521)
+2022-11-18 16:06:22,984:INFO: Dataset: univ                Batch:  1/15	Loss 4.0827 (4.0827)
+2022-11-18 16:06:22,986:INFO: Dataset: univ                Batch:  2/15	Loss 4.0762 (4.0797)
+2022-11-18 16:06:22,988:INFO: Dataset: univ                Batch:  3/15	Loss 4.0783 (4.0792)
+2022-11-18 16:06:22,989:INFO: Dataset: univ                Batch:  4/15	Loss 4.0754 (4.0783)
+2022-11-18 16:06:23,070:INFO: Dataset: univ                Batch:  5/15	Loss 4.0726 (4.0772)
+2022-11-18 16:06:23,079:INFO: Dataset: univ                Batch:  6/15	Loss 4.0723 (4.0764)
+2022-11-18 16:06:23,080:INFO: Dataset: univ                Batch:  7/15	Loss 4.0681 (4.0752)
+2022-11-18 16:06:23,081:INFO: Dataset: univ                Batch:  8/15	Loss 4.0665 (4.0742)
+2022-11-18 16:06:23,082:INFO: Dataset: univ                Batch:  9/15	Loss 4.0633 (4.0730)
+2022-11-18 16:06:23,084:INFO: Dataset: univ                Batch: 10/15	Loss 4.0671 (4.0723)
+2022-11-18 16:06:23,086:INFO: Dataset: univ                Batch: 11/15	Loss 4.0570 (4.0709)
+2022-11-18 16:06:23,087:INFO: Dataset: univ                Batch: 12/15	Loss 4.0605 (4.0701)
+2022-11-18 16:06:23,089:INFO: Dataset: univ                Batch: 13/15	Loss 4.0563 (4.0689)
+2022-11-18 16:06:23,090:INFO: Dataset: univ                Batch: 14/15	Loss 4.0496 (4.0675)
+2022-11-18 16:06:23,091:INFO: Dataset: univ                Batch: 15/15	Loss 4.0325 (4.0672)
+2022-11-18 16:06:23,335:INFO: Dataset: zara1               Batch: 1/8	Loss 4.4369 (4.4369)
+2022-11-18 16:06:23,340:INFO: Dataset: zara1               Batch: 2/8	Loss 4.4287 (4.4331)
+2022-11-18 16:06:23,341:INFO: Dataset: zara1               Batch: 3/8	Loss 4.4317 (4.4327)
+2022-11-18 16:06:23,343:INFO: Dataset: zara1               Batch: 4/8	Loss 4.4461 (4.4361)
+2022-11-18 16:06:23,346:INFO: Dataset: zara1               Batch: 5/8	Loss 4.4216 (4.4333)
+2022-11-18 16:06:23,378:INFO: Dataset: zara1               Batch: 6/8	Loss 4.4338 (4.4334)
+2022-11-18 16:06:23,380:INFO: Dataset: zara1               Batch: 7/8	Loss 4.4136 (4.4304)
+2022-11-18 16:06:23,381:INFO: Dataset: zara1               Batch: 8/8	Loss 4.4198 (4.4293)
+2022-11-18 16:06:23,621:INFO: Dataset: zara2               Batch:  1/18	Loss 4.5413 (4.5413)
+2022-11-18 16:06:23,624:INFO: Dataset: zara2               Batch:  2/18	Loss 4.5477 (4.5444)
+2022-11-18 16:06:23,628:INFO: Dataset: zara2               Batch:  3/18	Loss 4.5311 (4.5399)
+2022-11-18 16:06:23,630:INFO: Dataset: zara2               Batch:  4/18	Loss 4.5357 (4.5388)
+2022-11-18 16:06:23,659:INFO: Dataset: zara2               Batch:  5/18	Loss 4.5493 (4.5407)
+2022-11-18 16:06:23,665:INFO: Dataset: zara2               Batch:  6/18	Loss 4.5260 (4.5383)
+2022-11-18 16:06:23,667:INFO: Dataset: zara2               Batch:  7/18	Loss 4.5269 (4.5367)
+2022-11-18 16:06:23,668:INFO: Dataset: zara2               Batch:  8/18	Loss 4.5390 (4.5369)
+2022-11-18 16:06:23,669:INFO: Dataset: zara2               Batch:  9/18	Loss 4.4967 (4.5329)
+2022-11-18 16:06:23,670:INFO: Dataset: zara2               Batch: 10/18	Loss 4.5366 (4.5333)
+2022-11-18 16:06:23,672:INFO: Dataset: zara2               Batch: 11/18	Loss 4.5025 (4.5306)
+2022-11-18 16:06:23,674:INFO: Dataset: zara2               Batch: 12/18	Loss 4.5280 (4.5303)
+2022-11-18 16:06:23,675:INFO: Dataset: zara2               Batch: 13/18	Loss 4.5020 (4.5281)
+2022-11-18 16:06:23,676:INFO: Dataset: zara2               Batch: 14/18	Loss 4.5097 (4.5269)
+2022-11-18 16:06:23,677:INFO: Dataset: zara2               Batch: 15/18	Loss 4.5105 (4.5257)
+2022-11-18 16:06:23,679:INFO: Dataset: zara2               Batch: 16/18	Loss 4.5024 (4.5243)
+2022-11-18 16:06:23,680:INFO: Dataset: zara2               Batch: 17/18	Loss 4.4851 (4.5221)
+2022-11-18 16:06:23,682:INFO: Dataset: zara2               Batch: 18/18	Loss 4.5209 (4.5220)
+2022-11-18 16:06:23,727:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:24,013:INFO: 		 ADE on eth                       dataset:	 1.786146640777588
+2022-11-18 16:06:24,013:INFO: Average validation o:	ADE  1.7861	FDE  2.6111
+2022-11-18 16:06:24,014:INFO: - Computing loss (validation)
+2022-11-18 16:06:24,197:INFO: Dataset: hotel               Batch: 1/2	Loss 3.6884 (3.6884)
+2022-11-18 16:06:24,198:INFO: Dataset: hotel               Batch: 2/2	Loss 3.7190 (3.6902)
+2022-11-18 16:06:24,438:INFO: Dataset: univ                Batch: 1/3	Loss 3.9657 (3.9657)
+2022-11-18 16:06:24,442:INFO: Dataset: univ                Batch: 2/3	Loss 3.9568 (3.9615)
+2022-11-18 16:06:24,444:INFO: Dataset: univ                Batch: 3/3	Loss 3.9697 (3.9643)
+2022-11-18 16:06:24,685:INFO: Dataset: zara1               Batch: 1/2	Loss 4.3622 (4.3622)
+2022-11-18 16:06:24,686:INFO: Dataset: zara1               Batch: 2/2	Loss 4.3509 (4.3598)
+2022-11-18 16:06:24,920:INFO: Dataset: zara2               Batch: 1/5	Loss 4.5161 (4.5161)
+2022-11-18 16:06:24,929:INFO: Dataset: zara2               Batch: 2/5	Loss 4.5203 (4.5182)
+2022-11-18 16:06:24,930:INFO: Dataset: zara2               Batch: 3/5	Loss 4.5216 (4.5193)
+2022-11-18 16:06:24,942:INFO: Dataset: zara2               Batch: 4/5	Loss 4.5280 (4.5214)
+2022-11-18 16:06:24,943:INFO: Dataset: zara2               Batch: 5/5	Loss 4.5266 (4.5224)
+2022-11-18 16:06:25,001:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_705.pth.tar
+2022-11-18 16:06:25,001:INFO: 
+===> EPOCH: 706 (P4)
+2022-11-18 16:06:25,002:INFO: - Computing loss (training)
+2022-11-18 16:06:25,202:INFO: Dataset: hotel               Batch: 1/4	Loss 3.6495 (3.6495)
+2022-11-18 16:06:25,205:INFO: Dataset: hotel               Batch: 2/4	Loss 3.6529 (3.6513)
+2022-11-18 16:06:25,210:INFO: Dataset: hotel               Batch: 3/4	Loss 3.6413 (3.6479)
+2022-11-18 16:06:25,211:INFO: Dataset: hotel               Batch: 4/4	Loss 3.6512 (3.6485)
+2022-11-18 16:06:25,471:INFO: Dataset: univ                Batch:  1/15	Loss 3.9718 (3.9718)
+2022-11-18 16:06:25,477:INFO: Dataset: univ                Batch:  2/15	Loss 3.9633 (3.9678)
+2022-11-18 16:06:25,479:INFO: Dataset: univ                Batch:  3/15	Loss 3.9659 (3.9671)
+2022-11-18 16:06:25,480:INFO: Dataset: univ                Batch:  4/15	Loss 3.9742 (3.9691)
+2022-11-18 16:06:25,500:INFO: Dataset: univ                Batch:  5/15	Loss 3.9608 (3.9674)
+2022-11-18 16:06:25,504:INFO: Dataset: univ                Batch:  6/15	Loss 3.9597 (3.9661)
+2022-11-18 16:06:25,505:INFO: Dataset: univ                Batch:  7/15	Loss 3.9568 (3.9647)
+2022-11-18 16:06:25,507:INFO: Dataset: univ                Batch:  8/15	Loss 3.9538 (3.9633)
+2022-11-18 16:06:25,508:INFO: Dataset: univ                Batch:  9/15	Loss 3.9568 (3.9625)
+2022-11-18 16:06:25,509:INFO: Dataset: univ                Batch: 10/15	Loss 3.9481 (3.9613)
+2022-11-18 16:06:25,511:INFO: Dataset: univ                Batch: 11/15	Loss 3.9388 (3.9594)
+2022-11-18 16:06:25,513:INFO: Dataset: univ                Batch: 12/15	Loss 3.9407 (3.9580)
+2022-11-18 16:06:25,514:INFO: Dataset: univ                Batch: 13/15	Loss 3.9430 (3.9569)
+2022-11-18 16:06:25,516:INFO: Dataset: univ                Batch: 14/15	Loss 3.9491 (3.9562)
+2022-11-18 16:06:25,517:INFO: Dataset: univ                Batch: 15/15	Loss 3.9229 (3.9557)
+2022-11-18 16:06:25,770:INFO: Dataset: zara1               Batch: 1/8	Loss 4.3173 (4.3173)
+2022-11-18 16:06:25,773:INFO: Dataset: zara1               Batch: 2/8	Loss 4.2937 (4.3056)
+2022-11-18 16:06:25,776:INFO: Dataset: zara1               Batch: 3/8	Loss 4.2916 (4.3007)
+2022-11-18 16:06:25,778:INFO: Dataset: zara1               Batch: 4/8	Loss 4.2666 (4.2927)
+2022-11-18 16:06:25,780:INFO: Dataset: zara1               Batch: 5/8	Loss 4.2918 (4.2925)
+2022-11-18 16:06:25,834:INFO: Dataset: zara1               Batch: 6/8	Loss 4.2612 (4.2873)
+2022-11-18 16:06:25,838:INFO: Dataset: zara1               Batch: 7/8	Loss 4.2711 (4.2852)
+2022-11-18 16:06:25,842:INFO: Dataset: zara1               Batch: 8/8	Loss 4.2484 (4.2812)
+2022-11-18 16:06:26,115:INFO: Dataset: zara2               Batch:  1/18	Loss 4.4172 (4.4172)
+2022-11-18 16:06:26,118:INFO: Dataset: zara2               Batch:  2/18	Loss 4.4103 (4.4139)
+2022-11-18 16:06:26,159:INFO: Dataset: zara2               Batch:  3/18	Loss 4.4279 (4.4184)
+2022-11-18 16:06:26,162:INFO: Dataset: zara2               Batch:  4/18	Loss 4.4062 (4.4154)
+2022-11-18 16:06:26,164:INFO: Dataset: zara2               Batch:  5/18	Loss 4.4168 (4.4157)
+2022-11-18 16:06:26,170:INFO: Dataset: zara2               Batch:  6/18	Loss 4.3854 (4.4109)
+2022-11-18 16:06:26,171:INFO: Dataset: zara2               Batch:  7/18	Loss 4.4088 (4.4106)
+2022-11-18 16:06:26,172:INFO: Dataset: zara2               Batch:  8/18	Loss 4.3796 (4.4068)
+2022-11-18 16:06:26,173:INFO: Dataset: zara2               Batch:  9/18	Loss 4.4090 (4.4071)
+2022-11-18 16:06:26,174:INFO: Dataset: zara2               Batch: 10/18	Loss 4.4022 (4.4066)
+2022-11-18 16:06:26,183:INFO: Dataset: zara2               Batch: 11/18	Loss 4.3845 (4.4046)
+2022-11-18 16:06:26,185:INFO: Dataset: zara2               Batch: 12/18	Loss 4.3803 (4.4024)
+2022-11-18 16:06:26,186:INFO: Dataset: zara2               Batch: 13/18	Loss 4.3734 (4.3998)
+2022-11-18 16:06:26,187:INFO: Dataset: zara2               Batch: 14/18	Loss 4.3619 (4.3972)
+2022-11-18 16:06:26,188:INFO: Dataset: zara2               Batch: 15/18	Loss 4.3803 (4.3960)
+2022-11-18 16:06:26,190:INFO: Dataset: zara2               Batch: 16/18	Loss 4.3614 (4.3940)
+2022-11-18 16:06:26,191:INFO: Dataset: zara2               Batch: 17/18	Loss 4.3680 (4.3925)
+2022-11-18 16:06:26,192:INFO: Dataset: zara2               Batch: 18/18	Loss 4.3592 (4.3909)
+2022-11-18 16:06:26,242:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:26,515:INFO: 		 ADE on eth                       dataset:	 1.8015387058258057
+2022-11-18 16:06:26,515:INFO: Average validation o:	ADE  1.8015	FDE  2.6722
+2022-11-18 16:06:26,516:INFO: - Computing loss (validation)
+2022-11-18 16:06:26,701:INFO: Dataset: hotel               Batch: 1/2	Loss 3.5799 (3.5799)
+2022-11-18 16:06:26,702:INFO: Dataset: hotel               Batch: 2/2	Loss 3.5901 (3.5806)
+2022-11-18 16:06:26,935:INFO: Dataset: univ                Batch: 1/3	Loss 3.8370 (3.8370)
+2022-11-18 16:06:26,936:INFO: Dataset: univ                Batch: 2/3	Loss 3.8326 (3.8350)
+2022-11-18 16:06:26,942:INFO: Dataset: univ                Batch: 3/3	Loss 3.8377 (3.8359)
+2022-11-18 16:06:27,187:INFO: Dataset: zara1               Batch: 1/2	Loss 4.1930 (4.1930)
+2022-11-18 16:06:27,191:INFO: Dataset: zara1               Batch: 2/2	Loss 4.1864 (4.1911)
+2022-11-18 16:06:27,423:INFO: Dataset: zara2               Batch: 1/5	Loss 4.4000 (4.4000)
+2022-11-18 16:06:27,426:INFO: Dataset: zara2               Batch: 2/5	Loss 4.3909 (4.3953)
+2022-11-18 16:06:27,447:INFO: Dataset: zara2               Batch: 3/5	Loss 4.3854 (4.3919)
+2022-11-18 16:06:27,448:INFO: Dataset: zara2               Batch: 4/5	Loss 4.3882 (4.3910)
+2022-11-18 16:06:27,448:INFO: Dataset: zara2               Batch: 5/5	Loss 4.4111 (4.3953)
+2022-11-18 16:06:27,510:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_706.pth.tar
+2022-11-18 16:06:27,511:INFO: 
+===> EPOCH: 707 (P4)
+2022-11-18 16:06:27,511:INFO: - Computing loss (training)
+2022-11-18 16:06:27,714:INFO: Dataset: hotel               Batch: 1/4	Loss 3.5302 (3.5302)
+2022-11-18 16:06:27,716:INFO: Dataset: hotel               Batch: 2/4	Loss 3.5406 (3.5353)
+2022-11-18 16:06:27,718:INFO: Dataset: hotel               Batch: 3/4	Loss 3.5107 (3.5273)
+2022-11-18 16:06:27,719:INFO: Dataset: hotel               Batch: 4/4	Loss 3.5478 (3.5308)
+2022-11-18 16:06:27,953:INFO: Dataset: univ                Batch:  1/15	Loss 3.8540 (3.8540)
+2022-11-18 16:06:27,960:INFO: Dataset: univ                Batch:  2/15	Loss 3.8453 (3.8495)
+2022-11-18 16:06:28,017:INFO: Dataset: univ                Batch:  3/15	Loss 3.8344 (3.8448)
+2022-11-18 16:06:28,020:INFO: Dataset: univ                Batch:  4/15	Loss 3.8317 (3.8415)
+2022-11-18 16:06:28,021:INFO: Dataset: univ                Batch:  5/15	Loss 3.8381 (3.8409)
+2022-11-18 16:06:28,024:INFO: Dataset: univ                Batch:  6/15	Loss 3.8414 (3.8409)
+2022-11-18 16:06:28,026:INFO: Dataset: univ                Batch:  7/15	Loss 3.8272 (3.8391)
+2022-11-18 16:06:28,027:INFO: Dataset: univ                Batch:  8/15	Loss 3.8266 (3.8375)
+2022-11-18 16:06:28,028:INFO: Dataset: univ                Batch:  9/15	Loss 3.8170 (3.8353)
+2022-11-18 16:06:28,029:INFO: Dataset: univ                Batch: 10/15	Loss 3.8238 (3.8342)
+2022-11-18 16:06:28,032:INFO: Dataset: univ                Batch: 11/15	Loss 3.8199 (3.8329)
+2022-11-18 16:06:28,033:INFO: Dataset: univ                Batch: 12/15	Loss 3.8214 (3.8319)
+2022-11-18 16:06:28,034:INFO: Dataset: univ                Batch: 13/15	Loss 3.8092 (3.8303)
+2022-11-18 16:06:28,036:INFO: Dataset: univ                Batch: 14/15	Loss 3.8159 (3.8293)
+2022-11-18 16:06:28,038:INFO: Dataset: univ                Batch: 15/15	Loss 3.8068 (3.8290)
+2022-11-18 16:06:28,284:INFO: Dataset: zara1               Batch: 1/8	Loss 4.1265 (4.1265)
+2022-11-18 16:06:28,286:INFO: Dataset: zara1               Batch: 2/8	Loss 4.1072 (4.1170)
+2022-11-18 16:06:28,288:INFO: Dataset: zara1               Batch: 3/8	Loss 4.1349 (4.1239)
+2022-11-18 16:06:28,293:INFO: Dataset: zara1               Batch: 4/8	Loss 4.1156 (4.1218)
+2022-11-18 16:06:28,295:INFO: Dataset: zara1               Batch: 5/8	Loss 4.0993 (4.1174)
+2022-11-18 16:06:28,350:INFO: Dataset: zara1               Batch: 6/8	Loss 4.1089 (4.1161)
+2022-11-18 16:06:28,355:INFO: Dataset: zara1               Batch: 7/8	Loss 4.0995 (4.1135)
+2022-11-18 16:06:28,359:INFO: Dataset: zara1               Batch: 8/8	Loss 4.0793 (4.1100)
+2022-11-18 16:06:28,649:INFO: Dataset: zara2               Batch:  1/18	Loss 4.2726 (4.2726)
+2022-11-18 16:06:28,653:INFO: Dataset: zara2               Batch:  2/18	Loss 4.2791 (4.2761)
+2022-11-18 16:06:28,700:INFO: Dataset: zara2               Batch:  3/18	Loss 4.2639 (4.2719)
+2022-11-18 16:06:28,702:INFO: Dataset: zara2               Batch:  4/18	Loss 4.2598 (4.2689)
+2022-11-18 16:06:28,703:INFO: Dataset: zara2               Batch:  5/18	Loss 4.2803 (4.2714)
+2022-11-18 16:06:28,715:INFO: Dataset: zara2               Batch:  6/18	Loss 4.2375 (4.2666)
+2022-11-18 16:06:28,716:INFO: Dataset: zara2               Batch:  7/18	Loss 4.2473 (4.2640)
+2022-11-18 16:06:28,717:INFO: Dataset: zara2               Batch:  8/18	Loss 4.2559 (4.2630)
+2022-11-18 16:06:28,718:INFO: Dataset: zara2               Batch:  9/18	Loss 4.2345 (4.2596)
+2022-11-18 16:06:28,720:INFO: Dataset: zara2               Batch: 10/18	Loss 4.2094 (4.2542)
+2022-11-18 16:06:28,721:INFO: Dataset: zara2               Batch: 11/18	Loss 4.2404 (4.2528)
+2022-11-18 16:06:28,722:INFO: Dataset: zara2               Batch: 12/18	Loss 4.2257 (4.2504)
+2022-11-18 16:06:28,724:INFO: Dataset: zara2               Batch: 13/18	Loss 4.2251 (4.2486)
+2022-11-18 16:06:28,725:INFO: Dataset: zara2               Batch: 14/18	Loss 4.2270 (4.2469)
+2022-11-18 16:06:28,726:INFO: Dataset: zara2               Batch: 15/18	Loss 4.2243 (4.2454)
+2022-11-18 16:06:28,727:INFO: Dataset: zara2               Batch: 16/18	Loss 4.2195 (4.2438)
+2022-11-18 16:06:28,728:INFO: Dataset: zara2               Batch: 17/18	Loss 4.2069 (4.2415)
+2022-11-18 16:06:28,730:INFO: Dataset: zara2               Batch: 18/18	Loss 4.2190 (4.2403)
+2022-11-18 16:06:28,778:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:29,059:INFO: 		 ADE on eth                       dataset:	 1.845679521560669
+2022-11-18 16:06:29,060:INFO: Average validation o:	ADE  1.8457	FDE  2.7959
+2022-11-18 16:06:29,061:INFO: - Computing loss (validation)
+2022-11-18 16:06:29,241:INFO: Dataset: hotel               Batch: 1/2	Loss 3.4563 (3.4563)
+2022-11-18 16:06:29,242:INFO: Dataset: hotel               Batch: 2/2	Loss 3.4469 (3.4555)
+2022-11-18 16:06:29,485:INFO: Dataset: univ                Batch: 1/3	Loss 3.6966 (3.6966)
+2022-11-18 16:06:29,487:INFO: Dataset: univ                Batch: 2/3	Loss 3.6910 (3.6937)
+2022-11-18 16:06:29,488:INFO: Dataset: univ                Batch: 3/3	Loss 3.6782 (3.6893)
+2022-11-18 16:06:29,738:INFO: Dataset: zara1               Batch: 1/2	Loss 3.9963 (3.9963)
+2022-11-18 16:06:29,739:INFO: Dataset: zara1               Batch: 2/2	Loss 4.0051 (3.9986)
+2022-11-18 16:06:29,971:INFO: Dataset: zara2               Batch: 1/5	Loss 4.2427 (4.2427)
+2022-11-18 16:06:29,972:INFO: Dataset: zara2               Batch: 2/5	Loss 4.2562 (4.2495)
+2022-11-18 16:06:29,974:INFO: Dataset: zara2               Batch: 3/5	Loss 4.2418 (4.2469)
+2022-11-18 16:06:29,975:INFO: Dataset: zara2               Batch: 4/5	Loss 4.2506 (4.2478)
+2022-11-18 16:06:29,977:INFO: Dataset: zara2               Batch: 5/5	Loss 4.2547 (4.2491)
+2022-11-18 16:06:30,034:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_707.pth.tar
+2022-11-18 16:06:30,034:INFO: 
+===> EPOCH: 708 (P4)
+2022-11-18 16:06:30,035:INFO: - Computing loss (training)
+2022-11-18 16:06:30,218:INFO: Dataset: hotel               Batch: 1/4	Loss 3.4263 (3.4263)
+2022-11-18 16:06:30,220:INFO: Dataset: hotel               Batch: 2/4	Loss 3.3916 (3.4096)
+2022-11-18 16:06:30,222:INFO: Dataset: hotel               Batch: 3/4	Loss 3.3894 (3.4030)
+2022-11-18 16:06:30,236:INFO: Dataset: hotel               Batch: 4/4	Loss 3.3683 (3.3969)
+2022-11-18 16:06:30,478:INFO: Dataset: univ                Batch:  1/15	Loss 3.7090 (3.7090)
+2022-11-18 16:06:30,558:INFO: Dataset: univ                Batch:  2/15	Loss 3.7019 (3.7054)
+2022-11-18 16:06:30,559:INFO: Dataset: univ                Batch:  3/15	Loss 3.7033 (3.7047)
+2022-11-18 16:06:30,560:INFO: Dataset: univ                Batch:  4/15	Loss 3.6851 (3.7000)
+2022-11-18 16:06:30,562:INFO: Dataset: univ                Batch:  5/15	Loss 3.6900 (3.6980)
+2022-11-18 16:06:30,567:INFO: Dataset: univ                Batch:  6/15	Loss 3.6997 (3.6982)
+2022-11-18 16:06:30,569:INFO: Dataset: univ                Batch:  7/15	Loss 3.6956 (3.6979)
+2022-11-18 16:06:30,570:INFO: Dataset: univ                Batch:  8/15	Loss 3.6832 (3.6960)
+2022-11-18 16:06:30,571:INFO: Dataset: univ                Batch:  9/15	Loss 3.6702 (3.6932)
+2022-11-18 16:06:30,573:INFO: Dataset: univ                Batch: 10/15	Loss 3.6702 (3.6909)
+2022-11-18 16:06:30,575:INFO: Dataset: univ                Batch: 11/15	Loss 3.6827 (3.6901)
+2022-11-18 16:06:30,577:INFO: Dataset: univ                Batch: 12/15	Loss 3.6626 (3.6879)
+2022-11-18 16:06:30,578:INFO: Dataset: univ                Batch: 13/15	Loss 3.6653 (3.6862)
+2022-11-18 16:06:30,580:INFO: Dataset: univ                Batch: 14/15	Loss 3.6612 (3.6844)
+2022-11-18 16:06:30,582:INFO: Dataset: univ                Batch: 15/15	Loss 3.6404 (3.6838)
+2022-11-18 16:06:30,816:INFO: Dataset: zara1               Batch: 1/8	Loss 3.9304 (3.9304)
+2022-11-18 16:06:30,820:INFO: Dataset: zara1               Batch: 2/8	Loss 3.9176 (3.9245)
+2022-11-18 16:06:30,827:INFO: Dataset: zara1               Batch: 3/8	Loss 3.9118 (3.9205)
+2022-11-18 16:06:30,829:INFO: Dataset: zara1               Batch: 4/8	Loss 3.9115 (3.9179)
+2022-11-18 16:06:30,832:INFO: Dataset: zara1               Batch: 5/8	Loss 3.9340 (3.9211)
+2022-11-18 16:06:30,918:INFO: Dataset: zara1               Batch: 6/8	Loss 3.9021 (3.9181)
+2022-11-18 16:06:30,922:INFO: Dataset: zara1               Batch: 7/8	Loss 3.9038 (3.9162)
+2022-11-18 16:06:30,927:INFO: Dataset: zara1               Batch: 8/8	Loss 3.8937 (3.9138)
+2022-11-18 16:06:31,213:INFO: Dataset: zara2               Batch:  1/18	Loss 4.1295 (4.1295)
+2022-11-18 16:06:31,215:INFO: Dataset: zara2               Batch:  2/18	Loss 4.0904 (4.1106)
+2022-11-18 16:06:31,254:INFO: Dataset: zara2               Batch:  3/18	Loss 4.0634 (4.0958)
+2022-11-18 16:06:31,255:INFO: Dataset: zara2               Batch:  4/18	Loss 4.0773 (4.0912)
+2022-11-18 16:06:31,257:INFO: Dataset: zara2               Batch:  5/18	Loss 4.0892 (4.0907)
+2022-11-18 16:06:31,261:INFO: Dataset: zara2               Batch:  6/18	Loss 4.0670 (4.0866)
+2022-11-18 16:06:31,262:INFO: Dataset: zara2               Batch:  7/18	Loss 4.0946 (4.0877)
+2022-11-18 16:06:31,263:INFO: Dataset: zara2               Batch:  8/18	Loss 4.0925 (4.0884)
+2022-11-18 16:06:31,265:INFO: Dataset: zara2               Batch:  9/18	Loss 4.0516 (4.0845)
+2022-11-18 16:06:31,266:INFO: Dataset: zara2               Batch: 10/18	Loss 4.0738 (4.0835)
+2022-11-18 16:06:31,268:INFO: Dataset: zara2               Batch: 11/18	Loss 4.0478 (4.0804)
+2022-11-18 16:06:31,270:INFO: Dataset: zara2               Batch: 12/18	Loss 4.0657 (4.0791)
+2022-11-18 16:06:31,271:INFO: Dataset: zara2               Batch: 13/18	Loss 4.0628 (4.0779)
+2022-11-18 16:06:31,272:INFO: Dataset: zara2               Batch: 14/18	Loss 4.0237 (4.0740)
+2022-11-18 16:06:31,274:INFO: Dataset: zara2               Batch: 15/18	Loss 4.0723 (4.0739)
+2022-11-18 16:06:31,276:INFO: Dataset: zara2               Batch: 16/18	Loss 4.0622 (4.0731)
+2022-11-18 16:06:31,278:INFO: Dataset: zara2               Batch: 17/18	Loss 4.0352 (4.0710)
+2022-11-18 16:06:31,280:INFO: Dataset: zara2               Batch: 18/18	Loss 4.0397 (4.0696)
+2022-11-18 16:06:31,326:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:31,601:INFO: 		 ADE on eth                       dataset:	 1.7928602695465088
+2022-11-18 16:06:31,601:INFO: Average validation o:	ADE  1.7929	FDE  2.6642
+2022-11-18 16:06:31,602:INFO: - Computing loss (validation)
+2022-11-18 16:06:31,787:INFO: Dataset: hotel               Batch: 1/2	Loss 3.3136 (3.3136)
+2022-11-18 16:06:31,788:INFO: Dataset: hotel               Batch: 2/2	Loss 3.3138 (3.3136)
+2022-11-18 16:06:32,053:INFO: Dataset: univ                Batch: 1/3	Loss 3.5216 (3.5216)
+2022-11-18 16:06:32,055:INFO: Dataset: univ                Batch: 2/3	Loss 3.5238 (3.5227)
+2022-11-18 16:06:32,057:INFO: Dataset: univ                Batch: 3/3	Loss 3.5216 (3.5223)
+2022-11-18 16:06:32,293:INFO: Dataset: zara1               Batch: 1/2	Loss 3.7803 (3.7803)
+2022-11-18 16:06:32,295:INFO: Dataset: zara1               Batch: 2/2	Loss 3.7877 (3.7821)
+2022-11-18 16:06:32,532:INFO: Dataset: zara2               Batch: 1/5	Loss 4.0831 (4.0831)
+2022-11-18 16:06:32,533:INFO: Dataset: zara2               Batch: 2/5	Loss 4.0699 (4.0767)
+2022-11-18 16:06:32,535:INFO: Dataset: zara2               Batch: 3/5	Loss 4.0690 (4.0741)
+2022-11-18 16:06:32,540:INFO: Dataset: zara2               Batch: 4/5	Loss 4.0963 (4.0798)
+2022-11-18 16:06:32,541:INFO: Dataset: zara2               Batch: 5/5	Loss 4.0945 (4.0828)
+2022-11-18 16:06:32,600:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_708.pth.tar
+2022-11-18 16:06:32,600:INFO: 
+===> EPOCH: 709 (P4)
+2022-11-18 16:06:32,601:INFO: - Computing loss (training)
+2022-11-18 16:06:32,796:INFO: Dataset: hotel               Batch: 1/4	Loss 3.2698 (3.2698)
+2022-11-18 16:06:32,799:INFO: Dataset: hotel               Batch: 2/4	Loss 3.2570 (3.2637)
+2022-11-18 16:06:32,802:INFO: Dataset: hotel               Batch: 3/4	Loss 3.2209 (3.2499)
+2022-11-18 16:06:32,803:INFO: Dataset: hotel               Batch: 4/4	Loss 3.2214 (3.2449)
+2022-11-18 16:06:33,056:INFO: Dataset: univ                Batch:  1/15	Loss 3.5431 (3.5431)
+2022-11-18 16:06:33,059:INFO: Dataset: univ                Batch:  2/15	Loss 3.5401 (3.5416)
+2022-11-18 16:06:33,061:INFO: Dataset: univ                Batch:  3/15	Loss 3.5415 (3.5416)
+2022-11-18 16:06:33,111:INFO: Dataset: univ                Batch:  4/15	Loss 3.5334 (3.5396)
+2022-11-18 16:06:33,116:INFO: Dataset: univ                Batch:  5/15	Loss 3.5285 (3.5375)
+2022-11-18 16:06:33,138:INFO: Dataset: univ                Batch:  6/15	Loss 3.5231 (3.5350)
+2022-11-18 16:06:33,139:INFO: Dataset: univ                Batch:  7/15	Loss 3.5251 (3.5335)
+2022-11-18 16:06:33,141:INFO: Dataset: univ                Batch:  8/15	Loss 3.5321 (3.5334)
+2022-11-18 16:06:33,142:INFO: Dataset: univ                Batch:  9/15	Loss 3.5129 (3.5310)
+2022-11-18 16:06:33,143:INFO: Dataset: univ                Batch: 10/15	Loss 3.5111 (3.5289)
+2022-11-18 16:06:33,146:INFO: Dataset: univ                Batch: 11/15	Loss 3.5038 (3.5266)
+2022-11-18 16:06:33,149:INFO: Dataset: univ                Batch: 12/15	Loss 3.4911 (3.5238)
+2022-11-18 16:06:33,151:INFO: Dataset: univ                Batch: 13/15	Loss 3.5062 (3.5225)
+2022-11-18 16:06:33,152:INFO: Dataset: univ                Batch: 14/15	Loss 3.4817 (3.5195)
+2022-11-18 16:06:33,153:INFO: Dataset: univ                Batch: 15/15	Loss 3.4906 (3.5192)
+2022-11-18 16:06:33,390:INFO: Dataset: zara1               Batch: 1/8	Loss 3.7215 (3.7215)
+2022-11-18 16:06:33,398:INFO: Dataset: zara1               Batch: 2/8	Loss 3.6939 (3.7081)
+2022-11-18 16:06:33,399:INFO: Dataset: zara1               Batch: 3/8	Loss 3.6963 (3.7043)
+2022-11-18 16:06:33,411:INFO: Dataset: zara1               Batch: 4/8	Loss 3.6771 (3.6969)
+2022-11-18 16:06:33,412:INFO: Dataset: zara1               Batch: 5/8	Loss 3.6827 (3.6942)
+2022-11-18 16:06:33,462:INFO: Dataset: zara1               Batch: 6/8	Loss 3.7193 (3.6984)
+2022-11-18 16:06:33,466:INFO: Dataset: zara1               Batch: 7/8	Loss 3.6873 (3.6970)
+2022-11-18 16:06:33,470:INFO: Dataset: zara1               Batch: 8/8	Loss 3.6653 (3.6933)
+2022-11-18 16:06:33,735:INFO: Dataset: zara2               Batch:  1/18	Loss 3.9065 (3.9065)
+2022-11-18 16:06:33,738:INFO: Dataset: zara2               Batch:  2/18	Loss 3.9193 (3.9129)
+2022-11-18 16:06:33,742:INFO: Dataset: zara2               Batch:  3/18	Loss 3.8998 (3.9085)
+2022-11-18 16:06:33,743:INFO: Dataset: zara2               Batch:  4/18	Loss 3.8952 (3.9052)
+2022-11-18 16:06:33,785:INFO: Dataset: zara2               Batch:  5/18	Loss 3.9311 (3.9108)
+2022-11-18 16:06:33,795:INFO: Dataset: zara2               Batch:  6/18	Loss 3.8945 (3.9083)
+2022-11-18 16:06:33,796:INFO: Dataset: zara2               Batch:  7/18	Loss 3.8927 (3.9064)
+2022-11-18 16:06:33,797:INFO: Dataset: zara2               Batch:  8/18	Loss 3.8532 (3.8997)
+2022-11-18 16:06:33,798:INFO: Dataset: zara2               Batch:  9/18	Loss 3.8524 (3.8949)
+2022-11-18 16:06:33,799:INFO: Dataset: zara2               Batch: 10/18	Loss 3.8673 (3.8920)
+2022-11-18 16:06:33,802:INFO: Dataset: zara2               Batch: 11/18	Loss 3.8755 (3.8905)
+2022-11-18 16:06:33,803:INFO: Dataset: zara2               Batch: 12/18	Loss 3.8484 (3.8872)
+2022-11-18 16:06:33,804:INFO: Dataset: zara2               Batch: 13/18	Loss 3.8326 (3.8830)
+2022-11-18 16:06:33,805:INFO: Dataset: zara2               Batch: 14/18	Loss 3.8685 (3.8820)
+2022-11-18 16:06:33,806:INFO: Dataset: zara2               Batch: 15/18	Loss 3.8666 (3.8810)
+2022-11-18 16:06:33,807:INFO: Dataset: zara2               Batch: 16/18	Loss 3.8557 (3.8796)
+2022-11-18 16:06:33,809:INFO: Dataset: zara2               Batch: 17/18	Loss 3.8310 (3.8764)
+2022-11-18 16:06:33,810:INFO: Dataset: zara2               Batch: 18/18	Loss 3.8772 (3.8764)
+2022-11-18 16:06:33,853:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:34,133:INFO: 		 ADE on eth                       dataset:	 1.8204882144927979
+2022-11-18 16:06:34,133:INFO: Average validation o:	ADE  1.8205	FDE  2.7476
+2022-11-18 16:06:34,134:INFO: - Computing loss (validation)
+2022-11-18 16:06:34,325:INFO: Dataset: hotel               Batch: 1/2	Loss 3.1548 (3.1548)
+2022-11-18 16:06:34,326:INFO: Dataset: hotel               Batch: 2/2	Loss 3.1404 (3.1539)
+2022-11-18 16:06:34,550:INFO: Dataset: univ                Batch: 1/3	Loss 3.3344 (3.3344)
+2022-11-18 16:06:34,551:INFO: Dataset: univ                Batch: 2/3	Loss 3.3381 (3.3362)
+2022-11-18 16:06:34,552:INFO: Dataset: univ                Batch: 3/3	Loss 3.3278 (3.3339)
+2022-11-18 16:06:34,772:INFO: Dataset: zara1               Batch: 1/2	Loss 3.5437 (3.5437)
+2022-11-18 16:06:34,775:INFO: Dataset: zara1               Batch: 2/2	Loss 3.5174 (3.5372)
+2022-11-18 16:06:35,020:INFO: Dataset: zara2               Batch: 1/5	Loss 3.8574 (3.8574)
+2022-11-18 16:06:35,021:INFO: Dataset: zara2               Batch: 2/5	Loss 3.9059 (3.8823)
+2022-11-18 16:06:35,023:INFO: Dataset: zara2               Batch: 3/5	Loss 3.8911 (3.8852)
+2022-11-18 16:06:35,024:INFO: Dataset: zara2               Batch: 4/5	Loss 3.8749 (3.8827)
+2022-11-18 16:06:35,025:INFO: Dataset: zara2               Batch: 5/5	Loss 3.9389 (3.8937)
+2022-11-18 16:06:35,085:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_709.pth.tar
+2022-11-18 16:06:35,086:INFO: 
+===> EPOCH: 710 (P4)
+2022-11-18 16:06:35,087:INFO: - Computing loss (training)
+2022-11-18 16:06:35,287:INFO: Dataset: hotel               Batch: 1/4	Loss 3.0888 (3.0888)
+2022-11-18 16:06:35,291:INFO: Dataset: hotel               Batch: 2/4	Loss 3.0408 (3.0641)
+2022-11-18 16:06:35,293:INFO: Dataset: hotel               Batch: 3/4	Loss 3.0945 (3.0745)
+2022-11-18 16:06:35,295:INFO: Dataset: hotel               Batch: 4/4	Loss 3.0683 (3.0734)
+2022-11-18 16:06:35,585:INFO: Dataset: univ                Batch:  1/15	Loss 3.3727 (3.3727)
+2022-11-18 16:06:35,587:INFO: Dataset: univ                Batch:  2/15	Loss 3.3577 (3.3652)
+2022-11-18 16:06:35,588:INFO: Dataset: univ                Batch:  3/15	Loss 3.3430 (3.3581)
+2022-11-18 16:06:35,591:INFO: Dataset: univ                Batch:  4/15	Loss 3.3408 (3.3538)
+2022-11-18 16:06:35,592:INFO: Dataset: univ                Batch:  5/15	Loss 3.3509 (3.3532)
+2022-11-18 16:06:35,598:INFO: Dataset: univ                Batch:  6/15	Loss 3.3254 (3.3486)
+2022-11-18 16:06:35,600:INFO: Dataset: univ                Batch:  7/15	Loss 3.3495 (3.3487)
+2022-11-18 16:06:35,601:INFO: Dataset: univ                Batch:  8/15	Loss 3.3195 (3.3452)
+2022-11-18 16:06:35,603:INFO: Dataset: univ                Batch:  9/15	Loss 3.3255 (3.3431)
+2022-11-18 16:06:35,604:INFO: Dataset: univ                Batch: 10/15	Loss 3.3279 (3.3416)
+2022-11-18 16:06:35,606:INFO: Dataset: univ                Batch: 11/15	Loss 3.3257 (3.3401)
+2022-11-18 16:06:35,608:INFO: Dataset: univ                Batch: 12/15	Loss 3.3175 (3.3384)
+2022-11-18 16:06:35,609:INFO: Dataset: univ                Batch: 13/15	Loss 3.3204 (3.3371)
+2022-11-18 16:06:35,611:INFO: Dataset: univ                Batch: 14/15	Loss 3.3045 (3.3349)
+2022-11-18 16:06:35,612:INFO: Dataset: univ                Batch: 15/15	Loss 3.2982 (3.3344)
+2022-11-18 16:06:35,846:INFO: Dataset: zara1               Batch: 1/8	Loss 3.4705 (3.4705)
+2022-11-18 16:06:35,848:INFO: Dataset: zara1               Batch: 2/8	Loss 3.4881 (3.4792)
+2022-11-18 16:06:35,854:INFO: Dataset: zara1               Batch: 3/8	Loss 3.4880 (3.4819)
+2022-11-18 16:06:35,856:INFO: Dataset: zara1               Batch: 4/8	Loss 3.4376 (3.4725)
+2022-11-18 16:06:35,857:INFO: Dataset: zara1               Batch: 5/8	Loss 3.4586 (3.4697)
+2022-11-18 16:06:35,934:INFO: Dataset: zara1               Batch: 6/8	Loss 3.4081 (3.4599)
+2022-11-18 16:06:35,939:INFO: Dataset: zara1               Batch: 7/8	Loss 3.4015 (3.4513)
+2022-11-18 16:06:35,943:INFO: Dataset: zara1               Batch: 8/8	Loss 3.4013 (3.4461)
+2022-11-18 16:06:36,196:INFO: Dataset: zara2               Batch:  1/18	Loss 3.7463 (3.7463)
+2022-11-18 16:06:36,207:INFO: Dataset: zara2               Batch:  2/18	Loss 3.7225 (3.7347)
+2022-11-18 16:06:36,211:INFO: Dataset: zara2               Batch:  3/18	Loss 3.6484 (3.7063)
+2022-11-18 16:06:36,213:INFO: Dataset: zara2               Batch:  4/18	Loss 3.7096 (3.7071)
+2022-11-18 16:06:36,231:INFO: Dataset: zara2               Batch:  5/18	Loss 3.6778 (3.7009)
+2022-11-18 16:06:36,237:INFO: Dataset: zara2               Batch:  6/18	Loss 3.6373 (3.6910)
+2022-11-18 16:06:36,238:INFO: Dataset: zara2               Batch:  7/18	Loss 3.7262 (3.6963)
+2022-11-18 16:06:36,239:INFO: Dataset: zara2               Batch:  8/18	Loss 3.6938 (3.6960)
+2022-11-18 16:06:36,240:INFO: Dataset: zara2               Batch:  9/18	Loss 3.6596 (3.6915)
+2022-11-18 16:06:36,241:INFO: Dataset: zara2               Batch: 10/18	Loss 3.6090 (3.6835)
+2022-11-18 16:06:36,242:INFO: Dataset: zara2               Batch: 11/18	Loss 3.6705 (3.6824)
+2022-11-18 16:06:36,244:INFO: Dataset: zara2               Batch: 12/18	Loss 3.6563 (3.6802)
+2022-11-18 16:06:36,245:INFO: Dataset: zara2               Batch: 13/18	Loss 3.6526 (3.6781)
+2022-11-18 16:06:36,247:INFO: Dataset: zara2               Batch: 14/18	Loss 3.6174 (3.6740)
+2022-11-18 16:06:36,248:INFO: Dataset: zara2               Batch: 15/18	Loss 3.5487 (3.6659)
+2022-11-18 16:06:36,249:INFO: Dataset: zara2               Batch: 16/18	Loss 3.6152 (3.6626)
+2022-11-18 16:06:36,250:INFO: Dataset: zara2               Batch: 17/18	Loss 3.6187 (3.6600)
+2022-11-18 16:06:36,252:INFO: Dataset: zara2               Batch: 18/18	Loss 3.5954 (3.6567)
+2022-11-18 16:06:36,305:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:36,589:INFO: 		 ADE on eth                       dataset:	 1.8172613382339478
+2022-11-18 16:06:36,589:INFO: Average validation o:	ADE  1.8173	FDE  2.7294
+2022-11-18 16:06:36,590:INFO: - Computing loss (validation)
+2022-11-18 16:06:36,770:INFO: Dataset: hotel               Batch: 1/2	Loss 2.9779 (2.9779)
+2022-11-18 16:06:36,771:INFO: Dataset: hotel               Batch: 2/2	Loss 2.9472 (2.9760)
+2022-11-18 16:06:37,019:INFO: Dataset: univ                Batch: 1/3	Loss 3.1446 (3.1446)
+2022-11-18 16:06:37,021:INFO: Dataset: univ                Batch: 2/3	Loss 3.1172 (3.1316)
+2022-11-18 16:06:37,022:INFO: Dataset: univ                Batch: 3/3	Loss 3.1091 (3.1247)
+2022-11-18 16:06:37,247:INFO: Dataset: zara1               Batch: 1/2	Loss 3.2624 (3.2624)
+2022-11-18 16:06:37,248:INFO: Dataset: zara1               Batch: 2/2	Loss 3.2672 (3.2633)
+2022-11-18 16:06:37,498:INFO: Dataset: zara2               Batch: 1/5	Loss 3.7288 (3.7288)
+2022-11-18 16:06:37,500:INFO: Dataset: zara2               Batch: 2/5	Loss 3.6710 (3.6990)
+2022-11-18 16:06:37,502:INFO: Dataset: zara2               Batch: 3/5	Loss 3.6784 (3.6922)
+2022-11-18 16:06:37,515:INFO: Dataset: zara2               Batch: 4/5	Loss 3.6949 (3.6929)
+2022-11-18 16:06:37,516:INFO: Dataset: zara2               Batch: 5/5	Loss 3.6364 (3.6814)
+2022-11-18 16:06:37,574:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_710.pth.tar
+2022-11-18 16:06:37,574:INFO: 
+===> EPOCH: 711 (P4)
+2022-11-18 16:06:37,574:INFO: - Computing loss (training)
+2022-11-18 16:06:37,762:INFO: Dataset: hotel               Batch: 1/4	Loss 2.9205 (2.9205)
+2022-11-18 16:06:37,764:INFO: Dataset: hotel               Batch: 2/4	Loss 2.8411 (2.8830)
+2022-11-18 16:06:37,765:INFO: Dataset: hotel               Batch: 3/4	Loss 2.9059 (2.8908)
+2022-11-18 16:06:37,768:INFO: Dataset: hotel               Batch: 4/4	Loss 2.8400 (2.8827)
+2022-11-18 16:06:37,993:INFO: Dataset: univ                Batch:  1/15	Loss 3.1569 (3.1569)
+2022-11-18 16:06:38,099:INFO: Dataset: univ                Batch:  2/15	Loss 3.1604 (3.1587)
+2022-11-18 16:06:38,100:INFO: Dataset: univ                Batch:  3/15	Loss 3.1250 (3.1478)
+2022-11-18 16:06:38,101:INFO: Dataset: univ                Batch:  4/15	Loss 3.1440 (3.1468)
+2022-11-18 16:06:38,104:INFO: Dataset: univ                Batch:  5/15	Loss 3.1524 (3.1480)
+2022-11-18 16:06:38,109:INFO: Dataset: univ                Batch:  6/15	Loss 3.1077 (3.1417)
+2022-11-18 16:06:38,111:INFO: Dataset: univ                Batch:  7/15	Loss 3.1435 (3.1420)
+2022-11-18 16:06:38,112:INFO: Dataset: univ                Batch:  8/15	Loss 3.1199 (3.1391)
+2022-11-18 16:06:38,113:INFO: Dataset: univ                Batch:  9/15	Loss 3.1363 (3.1388)
+2022-11-18 16:06:38,115:INFO: Dataset: univ                Batch: 10/15	Loss 3.1229 (3.1372)
+2022-11-18 16:06:38,117:INFO: Dataset: univ                Batch: 11/15	Loss 3.1197 (3.1356)
+2022-11-18 16:06:38,119:INFO: Dataset: univ                Batch: 12/15	Loss 3.1157 (3.1339)
+2022-11-18 16:06:38,120:INFO: Dataset: univ                Batch: 13/15	Loss 3.1002 (3.1315)
+2022-11-18 16:06:38,122:INFO: Dataset: univ                Batch: 14/15	Loss 3.0935 (3.1287)
+2022-11-18 16:06:38,124:INFO: Dataset: univ                Batch: 15/15	Loss 3.1452 (3.1290)
+2022-11-18 16:06:38,354:INFO: Dataset: zara1               Batch: 1/8	Loss 3.1878 (3.1878)
+2022-11-18 16:06:38,357:INFO: Dataset: zara1               Batch: 2/8	Loss 3.2077 (3.1973)
+2022-11-18 16:06:38,370:INFO: Dataset: zara1               Batch: 3/8	Loss 3.1774 (3.1901)
+2022-11-18 16:06:38,371:INFO: Dataset: zara1               Batch: 4/8	Loss 3.1976 (3.1918)
+2022-11-18 16:06:38,373:INFO: Dataset: zara1               Batch: 5/8	Loss 3.2008 (3.1936)
+2022-11-18 16:06:38,388:INFO: Dataset: zara1               Batch: 6/8	Loss 3.1935 (3.1936)
+2022-11-18 16:06:38,389:INFO: Dataset: zara1               Batch: 7/8	Loss 3.1044 (3.1822)
+2022-11-18 16:06:38,390:INFO: Dataset: zara1               Batch: 8/8	Loss 3.1049 (3.1730)
+2022-11-18 16:06:38,649:INFO: Dataset: zara2               Batch:  1/18	Loss 3.4187 (3.4187)
+2022-11-18 16:06:38,654:INFO: Dataset: zara2               Batch:  2/18	Loss 3.4390 (3.4291)
+2022-11-18 16:06:38,655:INFO: Dataset: zara2               Batch:  3/18	Loss 3.5197 (3.4591)
+2022-11-18 16:06:38,701:INFO: Dataset: zara2               Batch:  4/18	Loss 3.4030 (3.4461)
+2022-11-18 16:06:38,702:INFO: Dataset: zara2               Batch:  5/18	Loss 3.4524 (3.4473)
+2022-11-18 16:06:38,719:INFO: Dataset: zara2               Batch:  6/18	Loss 3.4305 (3.4446)
+2022-11-18 16:06:38,721:INFO: Dataset: zara2               Batch:  7/18	Loss 3.4287 (3.4421)
+2022-11-18 16:06:38,722:INFO: Dataset: zara2               Batch:  8/18	Loss 3.4145 (3.4383)
+2022-11-18 16:06:38,723:INFO: Dataset: zara2               Batch:  9/18	Loss 3.4004 (3.4343)
+2022-11-18 16:06:38,724:INFO: Dataset: zara2               Batch: 10/18	Loss 3.4476 (3.4357)
+2022-11-18 16:06:38,726:INFO: Dataset: zara2               Batch: 11/18	Loss 3.4058 (3.4332)
+2022-11-18 16:06:38,727:INFO: Dataset: zara2               Batch: 12/18	Loss 3.4166 (3.4319)
+2022-11-18 16:06:38,729:INFO: Dataset: zara2               Batch: 13/18	Loss 3.4134 (3.4306)
+2022-11-18 16:06:38,730:INFO: Dataset: zara2               Batch: 14/18	Loss 3.3916 (3.4277)
+2022-11-18 16:06:38,731:INFO: Dataset: zara2               Batch: 15/18	Loss 3.4282 (3.4277)
+2022-11-18 16:06:38,732:INFO: Dataset: zara2               Batch: 16/18	Loss 3.3997 (3.4258)
+2022-11-18 16:06:38,734:INFO: Dataset: zara2               Batch: 17/18	Loss 3.3522 (3.4218)
+2022-11-18 16:06:38,735:INFO: Dataset: zara2               Batch: 18/18	Loss 3.3486 (3.4185)
+2022-11-18 16:06:38,793:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:39,073:INFO: 		 ADE on eth                       dataset:	 1.8014339208602905
+2022-11-18 16:06:39,073:INFO: Average validation o:	ADE  1.8014	FDE  2.6743
+2022-11-18 16:06:39,074:INFO: - Computing loss (validation)
+2022-11-18 16:06:39,262:INFO: Dataset: hotel               Batch: 1/2	Loss 2.7790 (2.7790)
+2022-11-18 16:06:39,263:INFO: Dataset: hotel               Batch: 2/2	Loss 2.8346 (2.7830)
+2022-11-18 16:06:39,512:INFO: Dataset: univ                Batch: 1/3	Loss 2.8998 (2.8998)
+2022-11-18 16:06:39,523:INFO: Dataset: univ                Batch: 2/3	Loss 2.9028 (2.9014)
+2022-11-18 16:06:39,524:INFO: Dataset: univ                Batch: 3/3	Loss 2.8924 (2.8985)
+2022-11-18 16:06:39,764:INFO: Dataset: zara1               Batch: 1/2	Loss 2.9796 (2.9796)
+2022-11-18 16:06:39,766:INFO: Dataset: zara1               Batch: 2/2	Loss 2.9378 (2.9695)
+2022-11-18 16:06:40,001:INFO: Dataset: zara2               Batch: 1/5	Loss 3.4520 (3.4520)
+2022-11-18 16:06:40,003:INFO: Dataset: zara2               Batch: 2/5	Loss 3.4232 (3.4372)
+2022-11-18 16:06:40,011:INFO: Dataset: zara2               Batch: 3/5	Loss 3.4868 (3.4544)
+2022-11-18 16:06:40,012:INFO: Dataset: zara2               Batch: 4/5	Loss 3.4590 (3.4555)
+2022-11-18 16:06:40,014:INFO: Dataset: zara2               Batch: 5/5	Loss 3.4320 (3.4510)
+2022-11-18 16:06:40,070:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_711.pth.tar
+2022-11-18 16:06:40,070:INFO: 
+===> EPOCH: 712 (P4)
+2022-11-18 16:06:40,071:INFO: - Computing loss (training)
+2022-11-18 16:06:40,273:INFO: Dataset: hotel               Batch: 1/4	Loss 2.7201 (2.7201)
+2022-11-18 16:06:40,275:INFO: Dataset: hotel               Batch: 2/4	Loss 2.6372 (2.6783)
+2022-11-18 16:06:40,278:INFO: Dataset: hotel               Batch: 3/4	Loss 2.6699 (2.6755)
+2022-11-18 16:06:40,280:INFO: Dataset: hotel               Batch: 4/4	Loss 2.6791 (2.6761)
+2022-11-18 16:06:40,537:INFO: Dataset: univ                Batch:  1/15	Loss 2.9433 (2.9433)
+2022-11-18 16:06:40,540:INFO: Dataset: univ                Batch:  2/15	Loss 2.9268 (2.9349)
+2022-11-18 16:06:40,543:INFO: Dataset: univ                Batch:  3/15	Loss 2.9349 (2.9349)
+2022-11-18 16:06:40,584:INFO: Dataset: univ                Batch:  4/15	Loss 2.9169 (2.9302)
+2022-11-18 16:06:40,585:INFO: Dataset: univ                Batch:  5/15	Loss 2.9313 (2.9304)
+2022-11-18 16:06:40,589:INFO: Dataset: univ                Batch:  6/15	Loss 2.9245 (2.9295)
+2022-11-18 16:06:40,591:INFO: Dataset: univ                Batch:  7/15	Loss 2.9255 (2.9289)
+2022-11-18 16:06:40,592:INFO: Dataset: univ                Batch:  8/15	Loss 2.9102 (2.9263)
+2022-11-18 16:06:40,593:INFO: Dataset: univ                Batch:  9/15	Loss 2.8862 (2.9221)
+2022-11-18 16:06:40,594:INFO: Dataset: univ                Batch: 10/15	Loss 2.8876 (2.9187)
+2022-11-18 16:06:40,597:INFO: Dataset: univ                Batch: 11/15	Loss 2.8929 (2.9164)
+2022-11-18 16:06:40,598:INFO: Dataset: univ                Batch: 12/15	Loss 2.8898 (2.9141)
+2022-11-18 16:06:40,600:INFO: Dataset: univ                Batch: 13/15	Loss 2.8850 (2.9117)
+2022-11-18 16:06:40,601:INFO: Dataset: univ                Batch: 14/15	Loss 2.8744 (2.9093)
+2022-11-18 16:06:40,602:INFO: Dataset: univ                Batch: 15/15	Loss 2.8170 (2.9083)
+2022-11-18 16:06:40,857:INFO: Dataset: zara1               Batch: 1/8	Loss 2.8669 (2.8669)
+2022-11-18 16:06:40,859:INFO: Dataset: zara1               Batch: 2/8	Loss 2.9304 (2.8992)
+2022-11-18 16:06:40,861:INFO: Dataset: zara1               Batch: 3/8	Loss 2.8201 (2.8728)
+2022-11-18 16:06:40,864:INFO: Dataset: zara1               Batch: 4/8	Loss 2.9043 (2.8808)
+2022-11-18 16:06:40,866:INFO: Dataset: zara1               Batch: 5/8	Loss 2.8875 (2.8822)
+2022-11-18 16:06:40,905:INFO: Dataset: zara1               Batch: 6/8	Loss 2.9117 (2.8871)
+2022-11-18 16:06:40,909:INFO: Dataset: zara1               Batch: 7/8	Loss 2.8585 (2.8825)
+2022-11-18 16:06:40,913:INFO: Dataset: zara1               Batch: 8/8	Loss 2.8870 (2.8829)
+2022-11-18 16:06:41,180:INFO: Dataset: zara2               Batch:  1/18	Loss 3.2628 (3.2628)
+2022-11-18 16:06:41,236:INFO: Dataset: zara2               Batch:  2/18	Loss 3.2078 (3.2354)
+2022-11-18 16:06:41,238:INFO: Dataset: zara2               Batch:  3/18	Loss 3.1859 (3.2199)
+2022-11-18 16:06:41,241:INFO: Dataset: zara2               Batch:  4/18	Loss 3.1628 (3.2064)
+2022-11-18 16:06:41,243:INFO: Dataset: zara2               Batch:  5/18	Loss 3.2442 (3.2148)
+2022-11-18 16:06:41,245:INFO: Dataset: zara2               Batch:  6/18	Loss 3.1492 (3.2041)
+2022-11-18 16:06:41,247:INFO: Dataset: zara2               Batch:  7/18	Loss 3.1593 (3.1976)
+2022-11-18 16:06:41,248:INFO: Dataset: zara2               Batch:  8/18	Loss 3.2065 (3.1988)
+2022-11-18 16:06:41,249:INFO: Dataset: zara2               Batch:  9/18	Loss 3.1493 (3.1927)
+2022-11-18 16:06:41,250:INFO: Dataset: zara2               Batch: 10/18	Loss 3.1713 (3.1906)
+2022-11-18 16:06:41,252:INFO: Dataset: zara2               Batch: 11/18	Loss 3.1903 (3.1905)
+2022-11-18 16:06:41,254:INFO: Dataset: zara2               Batch: 12/18	Loss 3.1322 (3.1862)
+2022-11-18 16:06:41,255:INFO: Dataset: zara2               Batch: 13/18	Loss 3.1159 (3.1808)
+2022-11-18 16:06:41,256:INFO: Dataset: zara2               Batch: 14/18	Loss 3.1813 (3.1808)
+2022-11-18 16:06:41,258:INFO: Dataset: zara2               Batch: 15/18	Loss 3.1032 (3.1749)
+2022-11-18 16:06:41,260:INFO: Dataset: zara2               Batch: 16/18	Loss 3.1237 (3.1718)
+2022-11-18 16:06:41,262:INFO: Dataset: zara2               Batch: 17/18	Loss 3.0807 (3.1664)
+2022-11-18 16:06:41,264:INFO: Dataset: zara2               Batch: 18/18	Loss 3.1024 (3.1633)
+2022-11-18 16:06:41,308:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:41,574:INFO: 		 ADE on eth                       dataset:	 1.823938012123108
+2022-11-18 16:06:41,575:INFO: Average validation o:	ADE  1.8239	FDE  2.7188
+2022-11-18 16:06:41,576:INFO: - Computing loss (validation)
+2022-11-18 16:06:41,759:INFO: Dataset: hotel               Batch: 1/2	Loss 2.5829 (2.5829)
+2022-11-18 16:06:41,760:INFO: Dataset: hotel               Batch: 2/2	Loss 2.5169 (2.5784)
+2022-11-18 16:06:41,997:INFO: Dataset: univ                Batch: 1/3	Loss 2.6559 (2.6559)
+2022-11-18 16:06:41,999:INFO: Dataset: univ                Batch: 2/3	Loss 2.6381 (2.6473)
+2022-11-18 16:06:42,000:INFO: Dataset: univ                Batch: 3/3	Loss 2.6837 (2.6595)
+2022-11-18 16:06:42,233:INFO: Dataset: zara1               Batch: 1/2	Loss 2.6492 (2.6492)
+2022-11-18 16:06:42,234:INFO: Dataset: zara1               Batch: 2/2	Loss 2.7045 (2.6634)
+2022-11-18 16:06:42,476:INFO: Dataset: zara2               Batch: 1/5	Loss 3.2135 (3.2135)
+2022-11-18 16:06:42,477:INFO: Dataset: zara2               Batch: 2/5	Loss 3.1979 (3.2064)
+2022-11-18 16:06:42,480:INFO: Dataset: zara2               Batch: 3/5	Loss 3.2036 (3.2055)
+2022-11-18 16:06:42,481:INFO: Dataset: zara2               Batch: 4/5	Loss 3.2243 (3.2103)
+2022-11-18 16:06:42,482:INFO: Dataset: zara2               Batch: 5/5	Loss 3.1862 (3.2057)
+2022-11-18 16:06:42,546:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_712.pth.tar
+2022-11-18 16:06:42,546:INFO: 
+===> EPOCH: 713 (P4)
+2022-11-18 16:06:42,547:INFO: - Computing loss (training)
+2022-11-18 16:06:42,748:INFO: Dataset: hotel               Batch: 1/4	Loss 2.4370 (2.4370)
+2022-11-18 16:06:42,751:INFO: Dataset: hotel               Batch: 2/4	Loss 2.4590 (2.4480)
+2022-11-18 16:06:42,754:INFO: Dataset: hotel               Batch: 3/4	Loss 2.4671 (2.4542)
+2022-11-18 16:06:42,760:INFO: Dataset: hotel               Batch: 4/4	Loss 2.4857 (2.4594)
+2022-11-18 16:06:43,006:INFO: Dataset: univ                Batch:  1/15	Loss 2.7074 (2.7074)
+2022-11-18 16:06:43,009:INFO: Dataset: univ                Batch:  2/15	Loss 2.7042 (2.7056)
+2022-11-18 16:06:43,011:INFO: Dataset: univ                Batch:  3/15	Loss 2.7164 (2.7092)
+2022-11-18 16:06:43,016:INFO: Dataset: univ                Batch:  4/15	Loss 2.6823 (2.7024)
+2022-11-18 16:06:43,057:INFO: Dataset: univ                Batch:  5/15	Loss 2.6642 (2.6950)
+2022-11-18 16:06:43,064:INFO: Dataset: univ                Batch:  6/15	Loss 2.6991 (2.6958)
+2022-11-18 16:06:43,065:INFO: Dataset: univ                Batch:  7/15	Loss 2.6745 (2.6926)
+2022-11-18 16:06:43,066:INFO: Dataset: univ                Batch:  8/15	Loss 2.6620 (2.6889)
+2022-11-18 16:06:43,068:INFO: Dataset: univ                Batch:  9/15	Loss 2.6488 (2.6846)
+2022-11-18 16:06:43,069:INFO: Dataset: univ                Batch: 10/15	Loss 2.6433 (2.6807)
+2022-11-18 16:06:43,071:INFO: Dataset: univ                Batch: 11/15	Loss 2.6801 (2.6806)
+2022-11-18 16:06:43,073:INFO: Dataset: univ                Batch: 12/15	Loss 2.6687 (2.6795)
+2022-11-18 16:06:43,074:INFO: Dataset: univ                Batch: 13/15	Loss 2.6350 (2.6763)
+2022-11-18 16:06:43,076:INFO: Dataset: univ                Batch: 14/15	Loss 2.6584 (2.6750)
+2022-11-18 16:06:43,077:INFO: Dataset: univ                Batch: 15/15	Loss 2.6809 (2.6750)
+2022-11-18 16:06:43,326:INFO: Dataset: zara1               Batch: 1/8	Loss 2.6289 (2.6289)
+2022-11-18 16:06:43,329:INFO: Dataset: zara1               Batch: 2/8	Loss 2.6495 (2.6399)
+2022-11-18 16:06:43,331:INFO: Dataset: zara1               Batch: 3/8	Loss 2.5541 (2.6106)
+2022-11-18 16:06:43,332:INFO: Dataset: zara1               Batch: 4/8	Loss 2.5580 (2.5985)
+2022-11-18 16:06:43,335:INFO: Dataset: zara1               Batch: 5/8	Loss 2.5442 (2.5877)
+2022-11-18 16:06:43,402:INFO: Dataset: zara1               Batch: 6/8	Loss 2.5432 (2.5799)
+2022-11-18 16:06:43,406:INFO: Dataset: zara1               Batch: 7/8	Loss 2.5856 (2.5807)
+2022-11-18 16:06:43,411:INFO: Dataset: zara1               Batch: 8/8	Loss 2.5884 (2.5817)
+2022-11-18 16:06:43,679:INFO: Dataset: zara2               Batch:  1/18	Loss 2.9378 (2.9378)
+2022-11-18 16:06:43,683:INFO: Dataset: zara2               Batch:  2/18	Loss 2.9578 (2.9479)
+2022-11-18 16:06:43,686:INFO: Dataset: zara2               Batch:  3/18	Loss 2.9364 (2.9442)
+2022-11-18 16:06:43,688:INFO: Dataset: zara2               Batch:  4/18	Loss 2.9679 (2.9503)
+2022-11-18 16:06:43,722:INFO: Dataset: zara2               Batch:  5/18	Loss 2.9409 (2.9484)
+2022-11-18 16:06:43,726:INFO: Dataset: zara2               Batch:  6/18	Loss 2.9247 (2.9447)
+2022-11-18 16:06:43,727:INFO: Dataset: zara2               Batch:  7/18	Loss 2.9043 (2.9387)
+2022-11-18 16:06:43,729:INFO: Dataset: zara2               Batch:  8/18	Loss 2.8986 (2.9333)
+2022-11-18 16:06:43,730:INFO: Dataset: zara2               Batch:  9/18	Loss 2.8632 (2.9257)
+2022-11-18 16:06:43,731:INFO: Dataset: zara2               Batch: 10/18	Loss 2.8869 (2.9218)
+2022-11-18 16:06:43,733:INFO: Dataset: zara2               Batch: 11/18	Loss 2.9091 (2.9206)
+2022-11-18 16:06:43,734:INFO: Dataset: zara2               Batch: 12/18	Loss 2.8166 (2.9123)
+2022-11-18 16:06:43,735:INFO: Dataset: zara2               Batch: 13/18	Loss 2.8952 (2.9108)
+2022-11-18 16:06:43,737:INFO: Dataset: zara2               Batch: 14/18	Loss 2.8454 (2.9064)
+2022-11-18 16:06:43,738:INFO: Dataset: zara2               Batch: 15/18	Loss 2.8648 (2.9038)
+2022-11-18 16:06:43,739:INFO: Dataset: zara2               Batch: 16/18	Loss 2.8239 (2.8991)
+2022-11-18 16:06:43,741:INFO: Dataset: zara2               Batch: 17/18	Loss 2.9241 (2.9006)
+2022-11-18 16:06:43,743:INFO: Dataset: zara2               Batch: 18/18	Loss 2.8929 (2.9002)
+2022-11-18 16:06:43,784:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:44,055:INFO: 		 ADE on eth                       dataset:	 1.8343390226364136
+2022-11-18 16:06:44,056:INFO: Average validation o:	ADE  1.8343	FDE  2.7707
+2022-11-18 16:06:44,057:INFO: - Computing loss (validation)
+2022-11-18 16:06:44,248:INFO: Dataset: hotel               Batch: 1/2	Loss 2.3619 (2.3619)
+2022-11-18 16:06:44,249:INFO: Dataset: hotel               Batch: 2/2	Loss 2.4150 (2.3658)
+2022-11-18 16:06:44,485:INFO: Dataset: univ                Batch: 1/3	Loss 2.4056 (2.4056)
+2022-11-18 16:06:44,487:INFO: Dataset: univ                Batch: 2/3	Loss 2.4185 (2.4119)
+2022-11-18 16:06:44,489:INFO: Dataset: univ                Batch: 3/3	Loss 2.4108 (2.4116)
+2022-11-18 16:06:44,752:INFO: Dataset: zara1               Batch: 1/2	Loss 2.3595 (2.3595)
+2022-11-18 16:06:44,753:INFO: Dataset: zara1               Batch: 2/2	Loss 2.3295 (2.3522)
+2022-11-18 16:06:44,994:INFO: Dataset: zara2               Batch: 1/5	Loss 2.9548 (2.9548)
+2022-11-18 16:06:44,995:INFO: Dataset: zara2               Batch: 2/5	Loss 2.9644 (2.9595)
+2022-11-18 16:06:44,996:INFO: Dataset: zara2               Batch: 3/5	Loss 2.9297 (2.9491)
+2022-11-18 16:06:44,998:INFO: Dataset: zara2               Batch: 4/5	Loss 2.9372 (2.9461)
+2022-11-18 16:06:45,003:INFO: Dataset: zara2               Batch: 5/5	Loss 2.9685 (2.9504)
+2022-11-18 16:06:45,059:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_713.pth.tar
+2022-11-18 16:06:45,059:INFO: 
+===> EPOCH: 714 (P4)
+2022-11-18 16:06:45,060:INFO: - Computing loss (training)
+2022-11-18 16:06:45,281:INFO: Dataset: hotel               Batch: 1/4	Loss 2.2088 (2.2088)
+2022-11-18 16:06:45,283:INFO: Dataset: hotel               Batch: 2/4	Loss 2.2381 (2.2229)
+2022-11-18 16:06:45,285:INFO: Dataset: hotel               Batch: 3/4	Loss 2.2426 (2.2292)
+2022-11-18 16:06:45,287:INFO: Dataset: hotel               Batch: 4/4	Loss 2.2706 (2.2363)
+2022-11-18 16:06:45,543:INFO: Dataset: univ                Batch:  1/15	Loss 2.4846 (2.4846)
+2022-11-18 16:06:45,547:INFO: Dataset: univ                Batch:  2/15	Loss 2.4573 (2.4723)
+2022-11-18 16:06:45,578:INFO: Dataset: univ                Batch:  3/15	Loss 2.4789 (2.4745)
+2022-11-18 16:06:45,581:INFO: Dataset: univ                Batch:  4/15	Loss 2.4623 (2.4718)
+2022-11-18 16:06:45,582:INFO: Dataset: univ                Batch:  5/15	Loss 2.4477 (2.4668)
+2022-11-18 16:06:45,587:INFO: Dataset: univ                Batch:  6/15	Loss 2.4062 (2.4578)
+2022-11-18 16:06:45,589:INFO: Dataset: univ                Batch:  7/15	Loss 2.4514 (2.4568)
+2022-11-18 16:06:45,590:INFO: Dataset: univ                Batch:  8/15	Loss 2.4169 (2.4515)
+2022-11-18 16:06:45,591:INFO: Dataset: univ                Batch:  9/15	Loss 2.4270 (2.4487)
+2022-11-18 16:06:45,593:INFO: Dataset: univ                Batch: 10/15	Loss 2.3879 (2.4428)
+2022-11-18 16:06:45,594:INFO: Dataset: univ                Batch: 11/15	Loss 2.4189 (2.4405)
+2022-11-18 16:06:45,596:INFO: Dataset: univ                Batch: 12/15	Loss 2.4166 (2.4386)
+2022-11-18 16:06:45,598:INFO: Dataset: univ                Batch: 13/15	Loss 2.4190 (2.4369)
+2022-11-18 16:06:45,599:INFO: Dataset: univ                Batch: 14/15	Loss 2.3986 (2.4337)
+2022-11-18 16:06:45,600:INFO: Dataset: univ                Batch: 15/15	Loss 2.3840 (2.4333)
+2022-11-18 16:06:45,865:INFO: Dataset: zara1               Batch: 1/8	Loss 2.3286 (2.3286)
+2022-11-18 16:06:45,868:INFO: Dataset: zara1               Batch: 2/8	Loss 2.3184 (2.3237)
+2022-11-18 16:06:45,872:INFO: Dataset: zara1               Batch: 3/8	Loss 2.2906 (2.3126)
+2022-11-18 16:06:45,874:INFO: Dataset: zara1               Batch: 4/8	Loss 2.3347 (2.3186)
+2022-11-18 16:06:45,876:INFO: Dataset: zara1               Batch: 5/8	Loss 2.2414 (2.3042)
+2022-11-18 16:06:45,922:INFO: Dataset: zara1               Batch: 6/8	Loss 2.2306 (2.2926)
+2022-11-18 16:06:45,926:INFO: Dataset: zara1               Batch: 7/8	Loss 2.2103 (2.2806)
+2022-11-18 16:06:45,929:INFO: Dataset: zara1               Batch: 8/8	Loss 2.2555 (2.2781)
+2022-11-18 16:06:46,169:INFO: Dataset: zara2               Batch:  1/18	Loss 2.7146 (2.7146)
+2022-11-18 16:06:46,172:INFO: Dataset: zara2               Batch:  2/18	Loss 2.6743 (2.6948)
+2022-11-18 16:06:46,269:INFO: Dataset: zara2               Batch:  3/18	Loss 2.6254 (2.6711)
+2022-11-18 16:06:46,271:INFO: Dataset: zara2               Batch:  4/18	Loss 2.6735 (2.6717)
+2022-11-18 16:06:46,272:INFO: Dataset: zara2               Batch:  5/18	Loss 2.6798 (2.6732)
+2022-11-18 16:06:46,275:INFO: Dataset: zara2               Batch:  6/18	Loss 2.5322 (2.6495)
+2022-11-18 16:06:46,276:INFO: Dataset: zara2               Batch:  7/18	Loss 2.6919 (2.6554)
+2022-11-18 16:06:46,278:INFO: Dataset: zara2               Batch:  8/18	Loss 2.6623 (2.6563)
+2022-11-18 16:06:46,279:INFO: Dataset: zara2               Batch:  9/18	Loss 2.6071 (2.6508)
+2022-11-18 16:06:46,280:INFO: Dataset: zara2               Batch: 10/18	Loss 2.6716 (2.6530)
+2022-11-18 16:06:46,283:INFO: Dataset: zara2               Batch: 11/18	Loss 2.6175 (2.6494)
+2022-11-18 16:06:46,284:INFO: Dataset: zara2               Batch: 12/18	Loss 2.6297 (2.6476)
+2022-11-18 16:06:46,285:INFO: Dataset: zara2               Batch: 13/18	Loss 2.5905 (2.6434)
+2022-11-18 16:06:46,286:INFO: Dataset: zara2               Batch: 14/18	Loss 2.5672 (2.6380)
+2022-11-18 16:06:46,288:INFO: Dataset: zara2               Batch: 15/18	Loss 2.6001 (2.6356)
+2022-11-18 16:06:46,289:INFO: Dataset: zara2               Batch: 16/18	Loss 2.6705 (2.6379)
+2022-11-18 16:06:46,291:INFO: Dataset: zara2               Batch: 17/18	Loss 2.5799 (2.6343)
+2022-11-18 16:06:46,293:INFO: Dataset: zara2               Batch: 18/18	Loss 2.5521 (2.6303)
+2022-11-18 16:06:46,337:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:46,609:INFO: 		 ADE on eth                       dataset:	 1.8087679147720337
+2022-11-18 16:06:46,610:INFO: Average validation o:	ADE  1.8088	FDE  2.6965
+2022-11-18 16:06:46,611:INFO: - Computing loss (validation)
+2022-11-18 16:06:46,794:INFO: Dataset: hotel               Batch: 1/2	Loss 2.1542 (2.1542)
+2022-11-18 16:06:46,795:INFO: Dataset: hotel               Batch: 2/2	Loss 2.0918 (2.1491)
+2022-11-18 16:06:47,041:INFO: Dataset: univ                Batch: 1/3	Loss 2.1493 (2.1493)
+2022-11-18 16:06:47,043:INFO: Dataset: univ                Batch: 2/3	Loss 2.1628 (2.1560)
+2022-11-18 16:06:47,045:INFO: Dataset: univ                Batch: 3/3	Loss 2.1678 (2.1597)
+2022-11-18 16:06:47,278:INFO: Dataset: zara1               Batch: 1/2	Loss 2.0230 (2.0230)
+2022-11-18 16:06:47,279:INFO: Dataset: zara1               Batch: 2/2	Loss 2.0901 (2.0405)
+2022-11-18 16:06:47,515:INFO: Dataset: zara2               Batch: 1/5	Loss 2.6635 (2.6635)
+2022-11-18 16:06:47,517:INFO: Dataset: zara2               Batch: 2/5	Loss 2.6476 (2.6553)
+2022-11-18 16:06:47,519:INFO: Dataset: zara2               Batch: 3/5	Loss 2.7126 (2.6754)
+2022-11-18 16:06:47,520:INFO: Dataset: zara2               Batch: 4/5	Loss 2.6679 (2.6735)
+2022-11-18 16:06:47,522:INFO: Dataset: zara2               Batch: 5/5	Loss 2.7523 (2.6893)
+2022-11-18 16:06:47,577:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_714.pth.tar
+2022-11-18 16:06:47,577:INFO: 
+===> EPOCH: 715 (P4)
+2022-11-18 16:06:47,578:INFO: - Computing loss (training)
+2022-11-18 16:06:47,785:INFO: Dataset: hotel               Batch: 1/4	Loss 2.0756 (2.0756)
+2022-11-18 16:06:47,796:INFO: Dataset: hotel               Batch: 2/4	Loss 2.0532 (2.0649)
+2022-11-18 16:06:47,799:INFO: Dataset: hotel               Batch: 3/4	Loss 1.9953 (2.0428)
+2022-11-18 16:06:47,800:INFO: Dataset: hotel               Batch: 4/4	Loss 1.8579 (2.0118)
+2022-11-18 16:06:48,067:INFO: Dataset: univ                Batch:  1/15	Loss 2.2406 (2.2406)
+2022-11-18 16:06:48,070:INFO: Dataset: univ                Batch:  2/15	Loss 2.2052 (2.2238)
+2022-11-18 16:06:48,073:INFO: Dataset: univ                Batch:  3/15	Loss 2.2170 (2.2215)
+2022-11-18 16:06:48,076:INFO: Dataset: univ                Batch:  4/15	Loss 2.1884 (2.2141)
+2022-11-18 16:06:48,126:INFO: Dataset: univ                Batch:  5/15	Loss 2.2137 (2.2140)
+2022-11-18 16:06:48,134:INFO: Dataset: univ                Batch:  6/15	Loss 2.1918 (2.2100)
+2022-11-18 16:06:48,136:INFO: Dataset: univ                Batch:  7/15	Loss 2.2065 (2.2095)
+2022-11-18 16:06:48,137:INFO: Dataset: univ                Batch:  8/15	Loss 2.1750 (2.2056)
+2022-11-18 16:06:48,138:INFO: Dataset: univ                Batch:  9/15	Loss 2.1676 (2.2016)
+2022-11-18 16:06:48,140:INFO: Dataset: univ                Batch: 10/15	Loss 2.1396 (2.1959)
+2022-11-18 16:06:48,141:INFO: Dataset: univ                Batch: 11/15	Loss 2.1893 (2.1953)
+2022-11-18 16:06:48,143:INFO: Dataset: univ                Batch: 12/15	Loss 2.1757 (2.1937)
+2022-11-18 16:06:48,145:INFO: Dataset: univ                Batch: 13/15	Loss 2.1566 (2.1908)
+2022-11-18 16:06:48,146:INFO: Dataset: univ                Batch: 14/15	Loss 2.1760 (2.1897)
+2022-11-18 16:06:48,147:INFO: Dataset: univ                Batch: 15/15	Loss 2.1402 (2.1890)
+2022-11-18 16:06:48,425:INFO: Dataset: zara1               Batch: 1/8	Loss 1.9841 (1.9841)
+2022-11-18 16:06:48,427:INFO: Dataset: zara1               Batch: 2/8	Loss 2.0375 (2.0101)
+2022-11-18 16:06:48,428:INFO: Dataset: zara1               Batch: 3/8	Loss 1.9844 (2.0015)
+2022-11-18 16:06:48,429:INFO: Dataset: zara1               Batch: 4/8	Loss 1.9781 (1.9958)
+2022-11-18 16:06:48,430:INFO: Dataset: zara1               Batch: 5/8	Loss 1.9142 (1.9798)
+2022-11-18 16:06:48,468:INFO: Dataset: zara1               Batch: 6/8	Loss 1.9865 (1.9809)
+2022-11-18 16:06:48,469:INFO: Dataset: zara1               Batch: 7/8	Loss 1.9170 (1.9710)
+2022-11-18 16:06:48,470:INFO: Dataset: zara1               Batch: 8/8	Loss 2.0355 (1.9775)
+2022-11-18 16:06:48,736:INFO: Dataset: zara2               Batch:  1/18	Loss 2.4023 (2.4023)
+2022-11-18 16:06:48,737:INFO: Dataset: zara2               Batch:  2/18	Loss 2.4683 (2.4349)
+2022-11-18 16:06:48,740:INFO: Dataset: zara2               Batch:  3/18	Loss 2.4186 (2.4293)
+2022-11-18 16:06:48,760:INFO: Dataset: zara2               Batch:  4/18	Loss 2.3507 (2.4115)
+2022-11-18 16:06:48,761:INFO: Dataset: zara2               Batch:  5/18	Loss 2.4284 (2.4150)
+2022-11-18 16:06:48,775:INFO: Dataset: zara2               Batch:  6/18	Loss 2.3661 (2.4073)
+2022-11-18 16:06:48,776:INFO: Dataset: zara2               Batch:  7/18	Loss 2.4068 (2.4072)
+2022-11-18 16:06:48,777:INFO: Dataset: zara2               Batch:  8/18	Loss 2.4291 (2.4100)
+2022-11-18 16:06:48,778:INFO: Dataset: zara2               Batch:  9/18	Loss 2.4112 (2.4101)
+2022-11-18 16:06:48,780:INFO: Dataset: zara2               Batch: 10/18	Loss 2.2598 (2.3955)
+2022-11-18 16:06:48,781:INFO: Dataset: zara2               Batch: 11/18	Loss 2.3118 (2.3883)
+2022-11-18 16:06:48,782:INFO: Dataset: zara2               Batch: 12/18	Loss 2.4118 (2.3904)
+2022-11-18 16:06:48,784:INFO: Dataset: zara2               Batch: 13/18	Loss 2.3179 (2.3848)
+2022-11-18 16:06:48,785:INFO: Dataset: zara2               Batch: 14/18	Loss 2.3729 (2.3840)
+2022-11-18 16:06:48,786:INFO: Dataset: zara2               Batch: 15/18	Loss 2.2582 (2.3758)
+2022-11-18 16:06:48,787:INFO: Dataset: zara2               Batch: 16/18	Loss 2.3606 (2.3749)
+2022-11-18 16:06:48,788:INFO: Dataset: zara2               Batch: 17/18	Loss 2.2428 (2.3680)
+2022-11-18 16:06:48,790:INFO: Dataset: zara2               Batch: 18/18	Loss 2.3148 (2.3659)
+2022-11-18 16:06:48,833:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:49,103:INFO: 		 ADE on eth                       dataset:	 1.804555892944336
+2022-11-18 16:06:49,104:INFO: Average validation o:	ADE  1.8046	FDE  2.7408
+2022-11-18 16:06:49,105:INFO: - Computing loss (validation)
+2022-11-18 16:06:49,277:INFO: Dataset: hotel               Batch: 1/2	Loss 1.9324 (1.9324)
+2022-11-18 16:06:49,278:INFO: Dataset: hotel               Batch: 2/2	Loss 1.9616 (1.9344)
+2022-11-18 16:06:49,528:INFO: Dataset: univ                Batch: 1/3	Loss 1.9075 (1.9075)
+2022-11-18 16:06:49,529:INFO: Dataset: univ                Batch: 2/3	Loss 1.8909 (1.8993)
+2022-11-18 16:06:49,530:INFO: Dataset: univ                Batch: 3/3	Loss 1.9373 (1.9114)
+2022-11-18 16:06:49,808:INFO: Dataset: zara1               Batch: 1/2	Loss 1.7206 (1.7206)
+2022-11-18 16:06:49,810:INFO: Dataset: zara1               Batch: 2/2	Loss 1.7896 (1.7372)
+2022-11-18 16:06:50,109:INFO: Dataset: zara2               Batch: 1/5	Loss 2.4322 (2.4322)
+2022-11-18 16:06:50,111:INFO: Dataset: zara2               Batch: 2/5	Loss 2.4334 (2.4328)
+2022-11-18 16:06:50,113:INFO: Dataset: zara2               Batch: 3/5	Loss 2.4706 (2.4456)
+2022-11-18 16:06:50,115:INFO: Dataset: zara2               Batch: 4/5	Loss 2.4230 (2.4398)
+2022-11-18 16:06:50,116:INFO: Dataset: zara2               Batch: 5/5	Loss 2.3936 (2.4305)
+2022-11-18 16:06:50,184:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_715.pth.tar
+2022-11-18 16:06:50,184:INFO: 
+===> EPOCH: 716 (P4)
+2022-11-18 16:06:50,185:INFO: - Computing loss (training)
+2022-11-18 16:06:50,436:INFO: Dataset: hotel               Batch: 1/4	Loss 1.8237 (1.8237)
+2022-11-18 16:06:50,438:INFO: Dataset: hotel               Batch: 2/4	Loss 1.8130 (1.8182)
+2022-11-18 16:06:50,442:INFO: Dataset: hotel               Batch: 3/4	Loss 1.7662 (1.8007)
+2022-11-18 16:06:50,444:INFO: Dataset: hotel               Batch: 4/4	Loss 1.7528 (1.7923)
+2022-11-18 16:06:50,746:INFO: Dataset: univ                Batch:  1/15	Loss 1.9697 (1.9697)
+2022-11-18 16:06:50,752:INFO: Dataset: univ                Batch:  2/15	Loss 1.9764 (1.9729)
+2022-11-18 16:06:50,793:INFO: Dataset: univ                Batch:  3/15	Loss 1.9506 (1.9656)
+2022-11-18 16:06:50,796:INFO: Dataset: univ                Batch:  4/15	Loss 1.9486 (1.9613)
+2022-11-18 16:06:50,799:INFO: Dataset: univ                Batch:  5/15	Loss 1.9402 (1.9577)
+2022-11-18 16:06:50,805:INFO: Dataset: univ                Batch:  6/15	Loss 1.9519 (1.9567)
+2022-11-18 16:06:50,807:INFO: Dataset: univ                Batch:  7/15	Loss 1.9523 (1.9560)
+2022-11-18 16:06:50,809:INFO: Dataset: univ                Batch:  8/15	Loss 1.9432 (1.9542)
+2022-11-18 16:06:50,811:INFO: Dataset: univ                Batch:  9/15	Loss 1.9865 (1.9585)
+2022-11-18 16:06:50,813:INFO: Dataset: univ                Batch: 10/15	Loss 1.9484 (1.9574)
+2022-11-18 16:06:50,816:INFO: Dataset: univ                Batch: 11/15	Loss 1.9351 (1.9555)
+2022-11-18 16:06:50,817:INFO: Dataset: univ                Batch: 12/15	Loss 1.9328 (1.9536)
+2022-11-18 16:06:50,819:INFO: Dataset: univ                Batch: 13/15	Loss 1.9184 (1.9507)
+2022-11-18 16:06:50,820:INFO: Dataset: univ                Batch: 14/15	Loss 1.9125 (1.9479)
+2022-11-18 16:06:50,822:INFO: Dataset: univ                Batch: 15/15	Loss 1.8553 (1.9471)
+2022-11-18 16:06:51,070:INFO: Dataset: zara1               Batch: 1/8	Loss 1.6816 (1.6816)
+2022-11-18 16:06:51,072:INFO: Dataset: zara1               Batch: 2/8	Loss 1.7259 (1.7037)
+2022-11-18 16:06:51,078:INFO: Dataset: zara1               Batch: 3/8	Loss 1.7031 (1.7035)
+2022-11-18 16:06:51,081:INFO: Dataset: zara1               Batch: 4/8	Loss 1.7343 (1.7110)
+2022-11-18 16:06:51,082:INFO: Dataset: zara1               Batch: 5/8	Loss 1.6627 (1.7026)
+2022-11-18 16:06:51,131:INFO: Dataset: zara1               Batch: 6/8	Loss 1.7043 (1.7029)
+2022-11-18 16:06:51,132:INFO: Dataset: zara1               Batch: 7/8	Loss 1.6356 (1.6935)
+2022-11-18 16:06:51,133:INFO: Dataset: zara1               Batch: 8/8	Loss 1.6867 (1.6928)
+2022-11-18 16:06:51,398:INFO: Dataset: zara2               Batch:  1/18	Loss 2.1805 (2.1805)
+2022-11-18 16:06:51,401:INFO: Dataset: zara2               Batch:  2/18	Loss 2.2191 (2.2005)
+2022-11-18 16:06:51,403:INFO: Dataset: zara2               Batch:  3/18	Loss 2.1136 (2.1729)
+2022-11-18 16:06:51,433:INFO: Dataset: zara2               Batch:  4/18	Loss 2.1920 (2.1776)
+2022-11-18 16:06:51,434:INFO: Dataset: zara2               Batch:  5/18	Loss 2.0896 (2.1603)
+2022-11-18 16:06:51,439:INFO: Dataset: zara2               Batch:  6/18	Loss 2.1634 (2.1608)
+2022-11-18 16:06:51,440:INFO: Dataset: zara2               Batch:  7/18	Loss 2.1529 (2.1598)
+2022-11-18 16:06:51,441:INFO: Dataset: zara2               Batch:  8/18	Loss 2.1406 (2.1573)
+2022-11-18 16:06:51,443:INFO: Dataset: zara2               Batch:  9/18	Loss 2.1304 (2.1544)
+2022-11-18 16:06:51,444:INFO: Dataset: zara2               Batch: 10/18	Loss 2.1093 (2.1494)
+2022-11-18 16:06:51,447:INFO: Dataset: zara2               Batch: 11/18	Loss 2.1853 (2.1526)
+2022-11-18 16:06:51,449:INFO: Dataset: zara2               Batch: 12/18	Loss 2.0845 (2.1467)
+2022-11-18 16:06:51,450:INFO: Dataset: zara2               Batch: 13/18	Loss 2.0953 (2.1431)
+2022-11-18 16:06:51,451:INFO: Dataset: zara2               Batch: 14/18	Loss 2.0273 (2.1354)
+2022-11-18 16:06:51,453:INFO: Dataset: zara2               Batch: 15/18	Loss 2.0554 (2.1295)
+2022-11-18 16:06:51,455:INFO: Dataset: zara2               Batch: 16/18	Loss 2.0663 (2.1257)
+2022-11-18 16:06:51,457:INFO: Dataset: zara2               Batch: 17/18	Loss 1.9712 (2.1167)
+2022-11-18 16:06:51,459:INFO: Dataset: zara2               Batch: 18/18	Loss 1.9894 (2.1103)
+2022-11-18 16:06:51,508:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:51,799:INFO: 		 ADE on eth                       dataset:	 1.8000417947769165
+2022-11-18 16:06:51,800:INFO: Average validation o:	ADE  1.8000	FDE  2.6794
+2022-11-18 16:06:51,801:INFO: - Computing loss (validation)
+2022-11-18 16:06:51,981:INFO: Dataset: hotel               Batch: 1/2	Loss 1.7324 (1.7324)
+2022-11-18 16:06:51,983:INFO: Dataset: hotel               Batch: 2/2	Loss 1.6805 (1.7285)
+2022-11-18 16:06:52,221:INFO: Dataset: univ                Batch: 1/3	Loss 1.6813 (1.6813)
+2022-11-18 16:06:52,222:INFO: Dataset: univ                Batch: 2/3	Loss 1.7065 (1.6938)
+2022-11-18 16:06:52,228:INFO: Dataset: univ                Batch: 3/3	Loss 1.6266 (1.6741)
+2022-11-18 16:06:52,459:INFO: Dataset: zara1               Batch: 1/2	Loss 1.4548 (1.4548)
+2022-11-18 16:06:52,460:INFO: Dataset: zara1               Batch: 2/2	Loss 1.4537 (1.4545)
+2022-11-18 16:06:52,707:INFO: Dataset: zara2               Batch: 1/5	Loss 2.1974 (2.1974)
+2022-11-18 16:06:52,712:INFO: Dataset: zara2               Batch: 2/5	Loss 2.0798 (2.1399)
+2022-11-18 16:06:52,713:INFO: Dataset: zara2               Batch: 3/5	Loss 2.2609 (2.1820)
+2022-11-18 16:06:52,714:INFO: Dataset: zara2               Batch: 4/5	Loss 2.2018 (2.1867)
+2022-11-18 16:06:52,715:INFO: Dataset: zara2               Batch: 5/5	Loss 2.1568 (2.1810)
+2022-11-18 16:06:52,770:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_716.pth.tar
+2022-11-18 16:06:52,770:INFO: 
+===> EPOCH: 717 (P4)
+2022-11-18 16:06:52,771:INFO: - Computing loss (training)
+2022-11-18 16:06:52,972:INFO: Dataset: hotel               Batch: 1/4	Loss 1.6341 (1.6341)
+2022-11-18 16:06:52,974:INFO: Dataset: hotel               Batch: 2/4	Loss 1.5812 (1.6064)
+2022-11-18 16:06:52,975:INFO: Dataset: hotel               Batch: 3/4	Loss 1.5875 (1.5998)
+2022-11-18 16:06:52,977:INFO: Dataset: hotel               Batch: 4/4	Loss 1.5177 (1.5855)
+2022-11-18 16:06:53,221:INFO: Dataset: univ                Batch:  1/15	Loss 1.7363 (1.7363)
+2022-11-18 16:06:53,223:INFO: Dataset: univ                Batch:  2/15	Loss 1.7368 (1.7365)
+2022-11-18 16:06:53,225:INFO: Dataset: univ                Batch:  3/15	Loss 1.7746 (1.7497)
+2022-11-18 16:06:53,262:INFO: Dataset: univ                Batch:  4/15	Loss 1.7066 (1.7400)
+2022-11-18 16:06:53,264:INFO: Dataset: univ                Batch:  5/15	Loss 1.7101 (1.7343)
+2022-11-18 16:06:53,270:INFO: Dataset: univ                Batch:  6/15	Loss 1.7418 (1.7356)
+2022-11-18 16:06:53,271:INFO: Dataset: univ                Batch:  7/15	Loss 1.7370 (1.7358)
+2022-11-18 16:06:53,273:INFO: Dataset: univ                Batch:  8/15	Loss 1.7215 (1.7340)
+2022-11-18 16:06:53,274:INFO: Dataset: univ                Batch:  9/15	Loss 1.7084 (1.7312)
+2022-11-18 16:06:53,276:INFO: Dataset: univ                Batch: 10/15	Loss 1.6988 (1.7283)
+2022-11-18 16:06:53,278:INFO: Dataset: univ                Batch: 11/15	Loss 1.6674 (1.7227)
+2022-11-18 16:06:53,280:INFO: Dataset: univ                Batch: 12/15	Loss 1.7201 (1.7225)
+2022-11-18 16:06:53,281:INFO: Dataset: univ                Batch: 13/15	Loss 1.6836 (1.7195)
+2022-11-18 16:06:53,283:INFO: Dataset: univ                Batch: 14/15	Loss 1.6992 (1.7181)
+2022-11-18 16:06:53,284:INFO: Dataset: univ                Batch: 15/15	Loss 1.6564 (1.7175)
+2022-11-18 16:06:53,553:INFO: Dataset: zara1               Batch: 1/8	Loss 1.4256 (1.4256)
+2022-11-18 16:06:53,558:INFO: Dataset: zara1               Batch: 2/8	Loss 1.3853 (1.4060)
+2022-11-18 16:06:53,560:INFO: Dataset: zara1               Batch: 3/8	Loss 1.5040 (1.4356)
+2022-11-18 16:06:53,561:INFO: Dataset: zara1               Batch: 4/8	Loss 1.4028 (1.4278)
+2022-11-18 16:06:53,565:INFO: Dataset: zara1               Batch: 5/8	Loss 1.4648 (1.4353)
+2022-11-18 16:06:53,595:INFO: Dataset: zara1               Batch: 6/8	Loss 1.4197 (1.4327)
+2022-11-18 16:06:53,597:INFO: Dataset: zara1               Batch: 7/8	Loss 1.4026 (1.4283)
+2022-11-18 16:06:53,598:INFO: Dataset: zara1               Batch: 8/8	Loss 1.4595 (1.4318)
+2022-11-18 16:06:53,842:INFO: Dataset: zara2               Batch:  1/18	Loss 1.9316 (1.9316)
+2022-11-18 16:06:53,843:INFO: Dataset: zara2               Batch:  2/18	Loss 1.9784 (1.9565)
+2022-11-18 16:06:53,845:INFO: Dataset: zara2               Batch:  3/18	Loss 1.8961 (1.9385)
+2022-11-18 16:06:53,848:INFO: Dataset: zara2               Batch:  4/18	Loss 1.9579 (1.9435)
+2022-11-18 16:06:53,884:INFO: Dataset: zara2               Batch:  5/18	Loss 1.9206 (1.9392)
+2022-11-18 16:06:53,889:INFO: Dataset: zara2               Batch:  6/18	Loss 1.7836 (1.9149)
+2022-11-18 16:06:53,890:INFO: Dataset: zara2               Batch:  7/18	Loss 1.8561 (1.9063)
+2022-11-18 16:06:53,891:INFO: Dataset: zara2               Batch:  8/18	Loss 1.9356 (1.9095)
+2022-11-18 16:06:53,893:INFO: Dataset: zara2               Batch:  9/18	Loss 1.8634 (1.9039)
+2022-11-18 16:06:53,894:INFO: Dataset: zara2               Batch: 10/18	Loss 1.8883 (1.9022)
+2022-11-18 16:06:53,896:INFO: Dataset: zara2               Batch: 11/18	Loss 1.8625 (1.8987)
+2022-11-18 16:06:53,897:INFO: Dataset: zara2               Batch: 12/18	Loss 1.8611 (1.8959)
+2022-11-18 16:06:53,899:INFO: Dataset: zara2               Batch: 13/18	Loss 1.8025 (1.8878)
+2022-11-18 16:06:53,900:INFO: Dataset: zara2               Batch: 14/18	Loss 1.8291 (1.8835)
+2022-11-18 16:06:53,901:INFO: Dataset: zara2               Batch: 15/18	Loss 1.8596 (1.8818)
+2022-11-18 16:06:53,902:INFO: Dataset: zara2               Batch: 16/18	Loss 1.9648 (1.8868)
+2022-11-18 16:06:53,904:INFO: Dataset: zara2               Batch: 17/18	Loss 1.7600 (1.8797)
+2022-11-18 16:06:53,906:INFO: Dataset: zara2               Batch: 18/18	Loss 1.7538 (1.8732)
+2022-11-18 16:06:53,951:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:54,230:INFO: 		 ADE on eth                       dataset:	 1.7830946445465088
+2022-11-18 16:06:54,231:INFO: Average validation o:	ADE  1.7831	FDE  2.6521
+2022-11-18 16:06:54,231:INFO: - Computing loss (validation)
+2022-11-18 16:06:54,417:INFO: Dataset: hotel               Batch: 1/2	Loss 1.5362 (1.5362)
+2022-11-18 16:06:54,419:INFO: Dataset: hotel               Batch: 2/2	Loss 1.5431 (1.5368)
+2022-11-18 16:06:54,660:INFO: Dataset: univ                Batch: 1/3	Loss 1.4462 (1.4462)
+2022-11-18 16:06:54,667:INFO: Dataset: univ                Batch: 2/3	Loss 1.4791 (1.4621)
+2022-11-18 16:06:54,669:INFO: Dataset: univ                Batch: 3/3	Loss 1.4358 (1.4545)
+2022-11-18 16:06:54,925:INFO: Dataset: zara1               Batch: 1/2	Loss 1.1899 (1.1899)
+2022-11-18 16:06:54,929:INFO: Dataset: zara1               Batch: 2/2	Loss 1.2375 (1.2022)
+2022-11-18 16:06:55,189:INFO: Dataset: zara2               Batch: 1/5	Loss 1.9383 (1.9383)
+2022-11-18 16:06:55,190:INFO: Dataset: zara2               Batch: 2/5	Loss 1.9280 (1.9332)
+2022-11-18 16:06:55,196:INFO: Dataset: zara2               Batch: 3/5	Loss 1.9825 (1.9497)
+2022-11-18 16:06:55,197:INFO: Dataset: zara2               Batch: 4/5	Loss 1.9323 (1.9452)
+2022-11-18 16:06:55,198:INFO: Dataset: zara2               Batch: 5/5	Loss 1.9520 (1.9465)
+2022-11-18 16:06:55,258:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_717.pth.tar
+2022-11-18 16:06:55,258:INFO: 
+===> EPOCH: 718 (P4)
+2022-11-18 16:06:55,259:INFO: - Computing loss (training)
+2022-11-18 16:06:55,464:INFO: Dataset: hotel               Batch: 1/4	Loss 1.3756 (1.3756)
+2022-11-18 16:06:55,467:INFO: Dataset: hotel               Batch: 2/4	Loss 1.3929 (1.3843)
+2022-11-18 16:06:55,468:INFO: Dataset: hotel               Batch: 3/4	Loss 1.3985 (1.3886)
+2022-11-18 16:06:55,470:INFO: Dataset: hotel               Batch: 4/4	Loss 1.4445 (1.3973)
+2022-11-18 16:06:55,726:INFO: Dataset: univ                Batch:  1/15	Loss 1.5054 (1.5054)
+2022-11-18 16:06:55,730:INFO: Dataset: univ                Batch:  2/15	Loss 1.5230 (1.5145)
+2022-11-18 16:06:55,732:INFO: Dataset: univ                Batch:  3/15	Loss 1.5326 (1.5208)
+2022-11-18 16:06:55,735:INFO: Dataset: univ                Batch:  4/15	Loss 1.5439 (1.5264)
+2022-11-18 16:06:55,776:INFO: Dataset: univ                Batch:  5/15	Loss 1.5015 (1.5214)
+2022-11-18 16:06:55,780:INFO: Dataset: univ                Batch:  6/15	Loss 1.5211 (1.5214)
+2022-11-18 16:06:55,782:INFO: Dataset: univ                Batch:  7/15	Loss 1.5437 (1.5244)
+2022-11-18 16:06:55,783:INFO: Dataset: univ                Batch:  8/15	Loss 1.5137 (1.5230)
+2022-11-18 16:06:55,785:INFO: Dataset: univ                Batch:  9/15	Loss 1.4960 (1.5199)
+2022-11-18 16:06:55,787:INFO: Dataset: univ                Batch: 10/15	Loss 1.4880 (1.5166)
+2022-11-18 16:06:55,789:INFO: Dataset: univ                Batch: 11/15	Loss 1.4731 (1.5128)
+2022-11-18 16:06:55,792:INFO: Dataset: univ                Batch: 12/15	Loss 1.4772 (1.5099)
+2022-11-18 16:06:55,794:INFO: Dataset: univ                Batch: 13/15	Loss 1.4575 (1.5057)
+2022-11-18 16:06:55,796:INFO: Dataset: univ                Batch: 14/15	Loss 1.4605 (1.5027)
+2022-11-18 16:06:55,797:INFO: Dataset: univ                Batch: 15/15	Loss 1.4804 (1.5024)
+2022-11-18 16:06:56,053:INFO: Dataset: zara1               Batch: 1/8	Loss 1.2595 (1.2595)
+2022-11-18 16:06:56,055:INFO: Dataset: zara1               Batch: 2/8	Loss 1.1918 (1.2264)
+2022-11-18 16:06:56,060:INFO: Dataset: zara1               Batch: 3/8	Loss 1.1427 (1.1996)
+2022-11-18 16:06:56,067:INFO: Dataset: zara1               Batch: 4/8	Loss 1.1926 (1.1976)
+2022-11-18 16:06:56,070:INFO: Dataset: zara1               Batch: 5/8	Loss 1.1768 (1.1931)
+2022-11-18 16:06:56,119:INFO: Dataset: zara1               Batch: 6/8	Loss 1.2262 (1.1992)
+2022-11-18 16:06:56,120:INFO: Dataset: zara1               Batch: 7/8	Loss 1.1946 (1.1985)
+2022-11-18 16:06:56,121:INFO: Dataset: zara1               Batch: 8/8	Loss 1.2464 (1.2034)
+2022-11-18 16:06:56,382:INFO: Dataset: zara2               Batch:  1/18	Loss 1.6929 (1.6929)
+2022-11-18 16:06:56,384:INFO: Dataset: zara2               Batch:  2/18	Loss 1.6360 (1.6652)
+2022-11-18 16:06:56,390:INFO: Dataset: zara2               Batch:  3/18	Loss 1.7169 (1.6813)
+2022-11-18 16:06:56,391:INFO: Dataset: zara2               Batch:  4/18	Loss 1.7129 (1.6891)
+2022-11-18 16:06:56,406:INFO: Dataset: zara2               Batch:  5/18	Loss 1.6740 (1.6861)
+2022-11-18 16:06:56,411:INFO: Dataset: zara2               Batch:  6/18	Loss 1.6795 (1.6850)
+2022-11-18 16:06:56,413:INFO: Dataset: zara2               Batch:  7/18	Loss 1.6716 (1.6831)
+2022-11-18 16:06:56,414:INFO: Dataset: zara2               Batch:  8/18	Loss 1.6551 (1.6799)
+2022-11-18 16:06:56,415:INFO: Dataset: zara2               Batch:  9/18	Loss 1.6900 (1.6811)
+2022-11-18 16:06:56,416:INFO: Dataset: zara2               Batch: 10/18	Loss 1.6260 (1.6753)
+2022-11-18 16:06:56,418:INFO: Dataset: zara2               Batch: 11/18	Loss 1.7380 (1.6814)
+2022-11-18 16:06:56,420:INFO: Dataset: zara2               Batch: 12/18	Loss 1.6398 (1.6779)
+2022-11-18 16:06:56,422:INFO: Dataset: zara2               Batch: 13/18	Loss 1.5673 (1.6692)
+2022-11-18 16:06:56,423:INFO: Dataset: zara2               Batch: 14/18	Loss 1.5409 (1.6598)
+2022-11-18 16:06:56,424:INFO: Dataset: zara2               Batch: 15/18	Loss 1.6778 (1.6612)
+2022-11-18 16:06:56,425:INFO: Dataset: zara2               Batch: 16/18	Loss 1.7133 (1.6644)
+2022-11-18 16:06:56,427:INFO: Dataset: zara2               Batch: 17/18	Loss 1.5842 (1.6595)
+2022-11-18 16:06:56,429:INFO: Dataset: zara2               Batch: 18/18	Loss 1.6355 (1.6584)
+2022-11-18 16:06:56,473:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:56,756:INFO: 		 ADE on eth                       dataset:	 1.8100364208221436
+2022-11-18 16:06:56,756:INFO: Average validation o:	ADE  1.8100	FDE  2.7554
+2022-11-18 16:06:56,757:INFO: - Computing loss (validation)
+2022-11-18 16:06:56,975:INFO: Dataset: hotel               Batch: 1/2	Loss 1.3600 (1.3600)
+2022-11-18 16:06:56,977:INFO: Dataset: hotel               Batch: 2/2	Loss 1.4083 (1.3638)
+2022-11-18 16:06:57,235:INFO: Dataset: univ                Batch: 1/3	Loss 1.2829 (1.2829)
+2022-11-18 16:06:57,248:INFO: Dataset: univ                Batch: 2/3	Loss 1.2698 (1.2760)
+2022-11-18 16:06:57,253:INFO: Dataset: univ                Batch: 3/3	Loss 1.2143 (1.2569)
+2022-11-18 16:06:57,545:INFO: Dataset: zara1               Batch: 1/2	Loss 1.0136 (1.0136)
+2022-11-18 16:06:57,553:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8988 (0.9870)
+2022-11-18 16:06:57,807:INFO: Dataset: zara2               Batch: 1/5	Loss 1.6526 (1.6526)
+2022-11-18 16:06:57,808:INFO: Dataset: zara2               Batch: 2/5	Loss 1.7059 (1.6807)
+2022-11-18 16:06:57,809:INFO: Dataset: zara2               Batch: 3/5	Loss 1.7482 (1.7041)
+2022-11-18 16:06:57,810:INFO: Dataset: zara2               Batch: 4/5	Loss 1.7523 (1.7171)
+2022-11-18 16:06:57,811:INFO: Dataset: zara2               Batch: 5/5	Loss 1.7921 (1.7317)
+2022-11-18 16:06:57,894:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_718.pth.tar
+2022-11-18 16:06:57,895:INFO: 
+===> EPOCH: 719 (P4)
+2022-11-18 16:06:57,896:INFO: - Computing loss (training)
+2022-11-18 16:06:58,175:INFO: Dataset: hotel               Batch: 1/4	Loss 1.2381 (1.2381)
+2022-11-18 16:06:58,177:INFO: Dataset: hotel               Batch: 2/4	Loss 1.2540 (1.2461)
+2022-11-18 16:06:58,179:INFO: Dataset: hotel               Batch: 3/4	Loss 1.1921 (1.2281)
+2022-11-18 16:06:58,183:INFO: Dataset: hotel               Batch: 4/4	Loss 1.2446 (1.2308)
+2022-11-18 16:06:58,472:INFO: Dataset: univ                Batch:  1/15	Loss 1.3214 (1.3214)
+2022-11-18 16:06:58,477:INFO: Dataset: univ                Batch:  2/15	Loss 1.3283 (1.3251)
+2022-11-18 16:06:58,479:INFO: Dataset: univ                Batch:  3/15	Loss 1.3308 (1.3271)
+2022-11-18 16:06:58,485:INFO: Dataset: univ                Batch:  4/15	Loss 1.2970 (1.3196)
+2022-11-18 16:06:58,532:INFO: Dataset: univ                Batch:  5/15	Loss 1.3538 (1.3274)
+2022-11-18 16:06:58,535:INFO: Dataset: univ                Batch:  6/15	Loss 1.3166 (1.3257)
+2022-11-18 16:06:58,537:INFO: Dataset: univ                Batch:  7/15	Loss 1.3186 (1.3247)
+2022-11-18 16:06:58,538:INFO: Dataset: univ                Batch:  8/15	Loss 1.3012 (1.3215)
+2022-11-18 16:06:58,539:INFO: Dataset: univ                Batch:  9/15	Loss 1.3167 (1.3209)
+2022-11-18 16:06:58,541:INFO: Dataset: univ                Batch: 10/15	Loss 1.2823 (1.3171)
+2022-11-18 16:06:58,544:INFO: Dataset: univ                Batch: 11/15	Loss 1.2940 (1.3151)
+2022-11-18 16:06:58,545:INFO: Dataset: univ                Batch: 12/15	Loss 1.2756 (1.3123)
+2022-11-18 16:06:58,546:INFO: Dataset: univ                Batch: 13/15	Loss 1.2885 (1.3103)
+2022-11-18 16:06:58,548:INFO: Dataset: univ                Batch: 14/15	Loss 1.2906 (1.3090)
+2022-11-18 16:06:58,549:INFO: Dataset: univ                Batch: 15/15	Loss 1.2421 (1.3082)
+2022-11-18 16:06:58,796:INFO: Dataset: zara1               Batch: 1/8	Loss 1.0203 (1.0203)
+2022-11-18 16:06:58,803:INFO: Dataset: zara1               Batch: 2/8	Loss 1.0169 (1.0186)
+2022-11-18 16:06:58,805:INFO: Dataset: zara1               Batch: 3/8	Loss 1.0733 (1.0345)
+2022-11-18 16:06:58,806:INFO: Dataset: zara1               Batch: 4/8	Loss 1.0178 (1.0308)
+2022-11-18 16:06:58,807:INFO: Dataset: zara1               Batch: 5/8	Loss 0.9263 (1.0099)
+2022-11-18 16:06:58,850:INFO: Dataset: zara1               Batch: 6/8	Loss 1.0884 (1.0226)
+2022-11-18 16:06:58,854:INFO: Dataset: zara1               Batch: 7/8	Loss 0.9443 (1.0121)
+2022-11-18 16:06:58,858:INFO: Dataset: zara1               Batch: 8/8	Loss 1.0206 (1.0130)
+2022-11-18 16:06:59,165:INFO: Dataset: zara2               Batch:  1/18	Loss 1.5812 (1.5812)
+2022-11-18 16:06:59,167:INFO: Dataset: zara2               Batch:  2/18	Loss 1.4542 (1.5176)
+2022-11-18 16:06:59,168:INFO: Dataset: zara2               Batch:  3/18	Loss 1.4623 (1.4988)
+2022-11-18 16:06:59,169:INFO: Dataset: zara2               Batch:  4/18	Loss 1.5209 (1.5043)
+2022-11-18 16:06:59,171:INFO: Dataset: zara2               Batch:  5/18	Loss 1.4344 (1.4910)
+2022-11-18 16:06:59,192:INFO: Dataset: zara2               Batch:  6/18	Loss 1.4750 (1.4883)
+2022-11-18 16:06:59,194:INFO: Dataset: zara2               Batch:  7/18	Loss 1.4752 (1.4865)
+2022-11-18 16:06:59,195:INFO: Dataset: zara2               Batch:  8/18	Loss 1.3467 (1.4701)
+2022-11-18 16:06:59,196:INFO: Dataset: zara2               Batch:  9/18	Loss 1.4867 (1.4720)
+2022-11-18 16:06:59,198:INFO: Dataset: zara2               Batch: 10/18	Loss 1.4940 (1.4741)
+2022-11-18 16:06:59,199:INFO: Dataset: zara2               Batch: 11/18	Loss 1.4570 (1.4725)
+2022-11-18 16:06:59,201:INFO: Dataset: zara2               Batch: 12/18	Loss 1.4693 (1.4723)
+2022-11-18 16:06:59,202:INFO: Dataset: zara2               Batch: 13/18	Loss 1.4278 (1.4689)
+2022-11-18 16:06:59,203:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3927 (1.4636)
+2022-11-18 16:06:59,205:INFO: Dataset: zara2               Batch: 15/18	Loss 1.5049 (1.4665)
+2022-11-18 16:06:59,206:INFO: Dataset: zara2               Batch: 16/18	Loss 1.5539 (1.4720)
+2022-11-18 16:06:59,207:INFO: Dataset: zara2               Batch: 17/18	Loss 1.4674 (1.4717)
+2022-11-18 16:06:59,209:INFO: Dataset: zara2               Batch: 18/18	Loss 1.4359 (1.4701)
+2022-11-18 16:06:59,255:INFO: - Computing ADE (validation o)
+2022-11-18 16:06:59,540:INFO: 		 ADE on eth                       dataset:	 1.8235975503921509
+2022-11-18 16:06:59,541:INFO: Average validation o:	ADE  1.8236	FDE  2.7529
+2022-11-18 16:06:59,542:INFO: - Computing loss (validation)
+2022-11-18 16:06:59,731:INFO: Dataset: hotel               Batch: 1/2	Loss 1.2079 (1.2079)
+2022-11-18 16:06:59,732:INFO: Dataset: hotel               Batch: 2/2	Loss 1.2756 (1.2129)
+2022-11-18 16:06:59,956:INFO: Dataset: univ                Batch: 1/3	Loss 1.0549 (1.0549)
+2022-11-18 16:06:59,957:INFO: Dataset: univ                Batch: 2/3	Loss 1.1125 (1.0858)
+2022-11-18 16:06:59,961:INFO: Dataset: univ                Batch: 3/3	Loss 1.0832 (1.0849)
+2022-11-18 16:07:00,214:INFO: Dataset: zara1               Batch: 1/2	Loss 0.8034 (0.8034)
+2022-11-18 16:07:00,215:INFO: Dataset: zara1               Batch: 2/2	Loss 0.8340 (0.8118)
+2022-11-18 16:07:00,456:INFO: Dataset: zara2               Batch: 1/5	Loss 1.5221 (1.5221)
+2022-11-18 16:07:00,458:INFO: Dataset: zara2               Batch: 2/5	Loss 1.4826 (1.5024)
+2022-11-18 16:07:00,459:INFO: Dataset: zara2               Batch: 3/5	Loss 1.5871 (1.5316)
+2022-11-18 16:07:00,461:INFO: Dataset: zara2               Batch: 4/5	Loss 1.5538 (1.5373)
+2022-11-18 16:07:00,462:INFO: Dataset: zara2               Batch: 5/5	Loss 1.5550 (1.5404)
+2022-11-18 16:07:00,522:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_719.pth.tar
+2022-11-18 16:07:00,522:INFO: 
+===> EPOCH: 720 (P4)
+2022-11-18 16:07:00,523:INFO: - Computing loss (training)
+2022-11-18 16:07:00,719:INFO: Dataset: hotel               Batch: 1/4	Loss 1.1011 (1.1011)
+2022-11-18 16:07:00,720:INFO: Dataset: hotel               Batch: 2/4	Loss 1.1044 (1.1027)
+2022-11-18 16:07:00,722:INFO: Dataset: hotel               Batch: 3/4	Loss 1.0699 (1.0917)
+2022-11-18 16:07:00,724:INFO: Dataset: hotel               Batch: 4/4	Loss 1.0776 (1.0896)
+2022-11-18 16:07:00,959:INFO: Dataset: univ                Batch:  1/15	Loss 1.1738 (1.1738)
+2022-11-18 16:07:00,961:INFO: Dataset: univ                Batch:  2/15	Loss 1.1557 (1.1648)
+2022-11-18 16:07:00,963:INFO: Dataset: univ                Batch:  3/15	Loss 1.1624 (1.1641)
+2022-11-18 16:07:00,990:INFO: Dataset: univ                Batch:  4/15	Loss 1.1767 (1.1673)
+2022-11-18 16:07:00,992:INFO: Dataset: univ                Batch:  5/15	Loss 1.1393 (1.1617)
+2022-11-18 16:07:00,996:INFO: Dataset: univ                Batch:  6/15	Loss 1.1478 (1.1594)
+2022-11-18 16:07:00,998:INFO: Dataset: univ                Batch:  7/15	Loss 1.1247 (1.1547)
+2022-11-18 16:07:01,000:INFO: Dataset: univ                Batch:  8/15	Loss 1.1389 (1.1527)
+2022-11-18 16:07:01,001:INFO: Dataset: univ                Batch:  9/15	Loss 1.1343 (1.1507)
+2022-11-18 16:07:01,003:INFO: Dataset: univ                Batch: 10/15	Loss 1.1275 (1.1482)
+2022-11-18 16:07:01,005:INFO: Dataset: univ                Batch: 11/15	Loss 1.1265 (1.1464)
+2022-11-18 16:07:01,008:INFO: Dataset: univ                Batch: 12/15	Loss 1.1082 (1.1432)
+2022-11-18 16:07:01,009:INFO: Dataset: univ                Batch: 13/15	Loss 1.0888 (1.1389)
+2022-11-18 16:07:01,011:INFO: Dataset: univ                Batch: 14/15	Loss 1.1205 (1.1376)
+2022-11-18 16:07:01,012:INFO: Dataset: univ                Batch: 15/15	Loss 1.1390 (1.1376)
+2022-11-18 16:07:01,253:INFO: Dataset: zara1               Batch: 1/8	Loss 0.8697 (0.8697)
+2022-11-18 16:07:01,257:INFO: Dataset: zara1               Batch: 2/8	Loss 0.8728 (0.8711)
+2022-11-18 16:07:01,259:INFO: Dataset: zara1               Batch: 3/8	Loss 0.8993 (0.8804)
+2022-11-18 16:07:01,260:INFO: Dataset: zara1               Batch: 4/8	Loss 0.8092 (0.8631)
+2022-11-18 16:07:01,264:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8650 (0.8635)
+2022-11-18 16:07:01,289:INFO: Dataset: zara1               Batch: 6/8	Loss 0.9189 (0.8740)
+2022-11-18 16:07:01,290:INFO: Dataset: zara1               Batch: 7/8	Loss 0.8134 (0.8658)
+2022-11-18 16:07:01,291:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7914 (0.8586)
+2022-11-18 16:07:01,554:INFO: Dataset: zara2               Batch:  1/18	Loss 1.3536 (1.3536)
+2022-11-18 16:07:01,557:INFO: Dataset: zara2               Batch:  2/18	Loss 1.3615 (1.3572)
+2022-11-18 16:07:01,565:INFO: Dataset: zara2               Batch:  3/18	Loss 1.3858 (1.3658)
+2022-11-18 16:07:01,567:INFO: Dataset: zara2               Batch:  4/18	Loss 1.2760 (1.3463)
+2022-11-18 16:07:01,621:INFO: Dataset: zara2               Batch:  5/18	Loss 1.3082 (1.3385)
+2022-11-18 16:07:01,628:INFO: Dataset: zara2               Batch:  6/18	Loss 1.2602 (1.3251)
+2022-11-18 16:07:01,630:INFO: Dataset: zara2               Batch:  7/18	Loss 1.3394 (1.3273)
+2022-11-18 16:07:01,631:INFO: Dataset: zara2               Batch:  8/18	Loss 1.2003 (1.3123)
+2022-11-18 16:07:01,632:INFO: Dataset: zara2               Batch:  9/18	Loss 1.3305 (1.3143)
+2022-11-18 16:07:01,633:INFO: Dataset: zara2               Batch: 10/18	Loss 1.3557 (1.3184)
+2022-11-18 16:07:01,635:INFO: Dataset: zara2               Batch: 11/18	Loss 1.3169 (1.3183)
+2022-11-18 16:07:01,636:INFO: Dataset: zara2               Batch: 12/18	Loss 1.3328 (1.3195)
+2022-11-18 16:07:01,638:INFO: Dataset: zara2               Batch: 13/18	Loss 1.3038 (1.3183)
+2022-11-18 16:07:01,639:INFO: Dataset: zara2               Batch: 14/18	Loss 1.3066 (1.3175)
+2022-11-18 16:07:01,640:INFO: Dataset: zara2               Batch: 15/18	Loss 1.3083 (1.3169)
+2022-11-18 16:07:01,641:INFO: Dataset: zara2               Batch: 16/18	Loss 1.2134 (1.3107)
+2022-11-18 16:07:01,643:INFO: Dataset: zara2               Batch: 17/18	Loss 1.2351 (1.3063)
+2022-11-18 16:07:01,645:INFO: Dataset: zara2               Batch: 18/18	Loss 1.3267 (1.3074)
+2022-11-18 16:07:01,691:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:01,970:INFO: 		 ADE on eth                       dataset:	 1.8260639905929565
+2022-11-18 16:07:01,970:INFO: Average validation o:	ADE  1.8261	FDE  2.7436
+2022-11-18 16:07:01,971:INFO: - Computing loss (validation)
+2022-11-18 16:07:02,154:INFO: Dataset: hotel               Batch: 1/2	Loss 1.0863 (1.0863)
+2022-11-18 16:07:02,155:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0721 (1.0854)
+2022-11-18 16:07:02,394:INFO: Dataset: univ                Batch: 1/3	Loss 0.9700 (0.9700)
+2022-11-18 16:07:02,395:INFO: Dataset: univ                Batch: 2/3	Loss 0.9295 (0.9500)
+2022-11-18 16:07:02,396:INFO: Dataset: univ                Batch: 3/3	Loss 0.9108 (0.9386)
+2022-11-18 16:07:02,628:INFO: Dataset: zara1               Batch: 1/2	Loss 0.6794 (0.6794)
+2022-11-18 16:07:02,629:INFO: Dataset: zara1               Batch: 2/2	Loss 0.6623 (0.6751)
+2022-11-18 16:07:02,866:INFO: Dataset: zara2               Batch: 1/5	Loss 1.3551 (1.3551)
+2022-11-18 16:07:02,867:INFO: Dataset: zara2               Batch: 2/5	Loss 1.3378 (1.3468)
+2022-11-18 16:07:02,882:INFO: Dataset: zara2               Batch: 3/5	Loss 1.3893 (1.3607)
+2022-11-18 16:07:02,884:INFO: Dataset: zara2               Batch: 4/5	Loss 1.3846 (1.3667)
+2022-11-18 16:07:02,885:INFO: Dataset: zara2               Batch: 5/5	Loss 1.3974 (1.3729)
+2022-11-18 16:07:02,946:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_720.pth.tar
+2022-11-18 16:07:02,946:INFO: 
+===> EPOCH: 721 (P4)
+2022-11-18 16:07:02,947:INFO: - Computing loss (training)
+2022-11-18 16:07:03,134:INFO: Dataset: hotel               Batch: 1/4	Loss 1.0002 (1.0002)
+2022-11-18 16:07:03,138:INFO: Dataset: hotel               Batch: 2/4	Loss 0.9502 (0.9752)
+2022-11-18 16:07:03,141:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9882 (0.9796)
+2022-11-18 16:07:03,144:INFO: Dataset: hotel               Batch: 4/4	Loss 0.9405 (0.9731)
+2022-11-18 16:07:03,404:INFO: Dataset: univ                Batch:  1/15	Loss 1.0296 (1.0296)
+2022-11-18 16:07:03,406:INFO: Dataset: univ                Batch:  2/15	Loss 1.0129 (1.0218)
+2022-11-18 16:07:03,410:INFO: Dataset: univ                Batch:  3/15	Loss 1.0152 (1.0196)
+2022-11-18 16:07:03,461:INFO: Dataset: univ                Batch:  4/15	Loss 1.0105 (1.0172)
+2022-11-18 16:07:03,463:INFO: Dataset: univ                Batch:  5/15	Loss 0.9831 (1.0110)
+2022-11-18 16:07:03,466:INFO: Dataset: univ                Batch:  6/15	Loss 0.9380 (0.9997)
+2022-11-18 16:07:03,468:INFO: Dataset: univ                Batch:  7/15	Loss 1.0139 (1.0016)
+2022-11-18 16:07:03,469:INFO: Dataset: univ                Batch:  8/15	Loss 1.0205 (1.0038)
+2022-11-18 16:07:03,470:INFO: Dataset: univ                Batch:  9/15	Loss 0.9776 (1.0009)
+2022-11-18 16:07:03,472:INFO: Dataset: univ                Batch: 10/15	Loss 1.0048 (1.0012)
+2022-11-18 16:07:03,474:INFO: Dataset: univ                Batch: 11/15	Loss 0.9757 (0.9986)
+2022-11-18 16:07:03,475:INFO: Dataset: univ                Batch: 12/15	Loss 0.9618 (0.9955)
+2022-11-18 16:07:03,476:INFO: Dataset: univ                Batch: 13/15	Loss 0.9623 (0.9931)
+2022-11-18 16:07:03,478:INFO: Dataset: univ                Batch: 14/15	Loss 0.9674 (0.9915)
+2022-11-18 16:07:03,479:INFO: Dataset: univ                Batch: 15/15	Loss 0.9486 (0.9908)
+2022-11-18 16:07:03,715:INFO: Dataset: zara1               Batch: 1/8	Loss 0.7665 (0.7665)
+2022-11-18 16:07:03,719:INFO: Dataset: zara1               Batch: 2/8	Loss 0.7320 (0.7490)
+2022-11-18 16:07:03,722:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7615 (0.7532)
+2022-11-18 16:07:03,723:INFO: Dataset: zara1               Batch: 4/8	Loss 0.7067 (0.7424)
+2022-11-18 16:07:03,725:INFO: Dataset: zara1               Batch: 5/8	Loss 0.8095 (0.7560)
+2022-11-18 16:07:03,770:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6738 (0.7428)
+2022-11-18 16:07:03,775:INFO: Dataset: zara1               Batch: 7/8	Loss 0.7238 (0.7396)
+2022-11-18 16:07:03,779:INFO: Dataset: zara1               Batch: 8/8	Loss 0.7214 (0.7377)
+2022-11-18 16:07:04,067:INFO: Dataset: zara2               Batch:  1/18	Loss 1.2118 (1.2118)
+2022-11-18 16:07:04,071:INFO: Dataset: zara2               Batch:  2/18	Loss 1.1833 (1.1984)
+2022-11-18 16:07:04,073:INFO: Dataset: zara2               Batch:  3/18	Loss 1.2318 (1.2093)
+2022-11-18 16:07:04,077:INFO: Dataset: zara2               Batch:  4/18	Loss 1.1952 (1.2061)
+2022-11-18 16:07:04,099:INFO: Dataset: zara2               Batch:  5/18	Loss 1.2187 (1.2086)
+2022-11-18 16:07:04,105:INFO: Dataset: zara2               Batch:  6/18	Loss 1.1719 (1.2026)
+2022-11-18 16:07:04,106:INFO: Dataset: zara2               Batch:  7/18	Loss 1.1961 (1.2016)
+2022-11-18 16:07:04,107:INFO: Dataset: zara2               Batch:  8/18	Loss 1.1955 (1.2008)
+2022-11-18 16:07:04,108:INFO: Dataset: zara2               Batch:  9/18	Loss 1.1426 (1.1944)
+2022-11-18 16:07:04,109:INFO: Dataset: zara2               Batch: 10/18	Loss 1.1705 (1.1921)
+2022-11-18 16:07:04,110:INFO: Dataset: zara2               Batch: 11/18	Loss 1.1546 (1.1888)
+2022-11-18 16:07:04,112:INFO: Dataset: zara2               Batch: 12/18	Loss 1.1876 (1.1887)
+2022-11-18 16:07:04,113:INFO: Dataset: zara2               Batch: 13/18	Loss 1.1135 (1.1829)
+2022-11-18 16:07:04,114:INFO: Dataset: zara2               Batch: 14/18	Loss 1.1416 (1.1799)
+2022-11-18 16:07:04,116:INFO: Dataset: zara2               Batch: 15/18	Loss 1.1205 (1.1762)
+2022-11-18 16:07:04,117:INFO: Dataset: zara2               Batch: 16/18	Loss 1.1311 (1.1734)
+2022-11-18 16:07:04,118:INFO: Dataset: zara2               Batch: 17/18	Loss 1.1096 (1.1699)
+2022-11-18 16:07:04,120:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1453 (1.1687)
+2022-11-18 16:07:04,167:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:04,446:INFO: 		 ADE on eth                       dataset:	 1.8297231197357178
+2022-11-18 16:07:04,446:INFO: Average validation o:	ADE  1.8297	FDE  2.7500
+2022-11-18 16:07:04,447:INFO: - Computing loss (validation)
+2022-11-18 16:07:04,635:INFO: Dataset: hotel               Batch: 1/2	Loss 0.9777 (0.9777)
+2022-11-18 16:07:04,636:INFO: Dataset: hotel               Batch: 2/2	Loss 1.0086 (0.9800)
+2022-11-18 16:07:04,878:INFO: Dataset: univ                Batch: 1/3	Loss 0.8158 (0.8158)
+2022-11-18 16:07:04,879:INFO: Dataset: univ                Batch: 2/3	Loss 0.8374 (0.8263)
+2022-11-18 16:07:04,880:INFO: Dataset: univ                Batch: 3/3	Loss 0.7966 (0.8158)
+2022-11-18 16:07:05,122:INFO: Dataset: zara1               Batch: 1/2	Loss 0.5687 (0.5687)
+2022-11-18 16:07:05,123:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5801 (0.5715)
+2022-11-18 16:07:05,358:INFO: Dataset: zara2               Batch: 1/5	Loss 1.2207 (1.2207)
+2022-11-18 16:07:05,361:INFO: Dataset: zara2               Batch: 2/5	Loss 1.2149 (1.2178)
+2022-11-18 16:07:05,365:INFO: Dataset: zara2               Batch: 3/5	Loss 1.2647 (1.2337)
+2022-11-18 16:07:05,366:INFO: Dataset: zara2               Batch: 4/5	Loss 1.2439 (1.2362)
+2022-11-18 16:07:05,368:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1910 (1.2276)
+2022-11-18 16:07:05,435:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_721.pth.tar
+2022-11-18 16:07:05,435:INFO: 
+===> EPOCH: 722 (P4)
+2022-11-18 16:07:05,436:INFO: - Computing loss (training)
+2022-11-18 16:07:05,627:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8616 (0.8616)
+2022-11-18 16:07:05,630:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8897 (0.8754)
+2022-11-18 16:07:05,633:INFO: Dataset: hotel               Batch: 3/4	Loss 0.9135 (0.8882)
+2022-11-18 16:07:05,636:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8305 (0.8794)
+2022-11-18 16:07:05,879:INFO: Dataset: univ                Batch:  1/15	Loss 0.8674 (0.8674)
+2022-11-18 16:07:05,883:INFO: Dataset: univ                Batch:  2/15	Loss 0.8825 (0.8749)
+2022-11-18 16:07:05,885:INFO: Dataset: univ                Batch:  3/15	Loss 0.8489 (0.8667)
+2022-11-18 16:07:05,887:INFO: Dataset: univ                Batch:  4/15	Loss 0.9025 (0.8761)
+2022-11-18 16:07:05,927:INFO: Dataset: univ                Batch:  5/15	Loss 0.8641 (0.8734)
+2022-11-18 16:07:05,933:INFO: Dataset: univ                Batch:  6/15	Loss 0.8690 (0.8727)
+2022-11-18 16:07:05,934:INFO: Dataset: univ                Batch:  7/15	Loss 0.8653 (0.8717)
+2022-11-18 16:07:05,936:INFO: Dataset: univ                Batch:  8/15	Loss 0.8877 (0.8739)
+2022-11-18 16:07:05,937:INFO: Dataset: univ                Batch:  9/15	Loss 0.8635 (0.8727)
+2022-11-18 16:07:05,938:INFO: Dataset: univ                Batch: 10/15	Loss 0.8533 (0.8707)
+2022-11-18 16:07:05,941:INFO: Dataset: univ                Batch: 11/15	Loss 0.8446 (0.8684)
+2022-11-18 16:07:05,942:INFO: Dataset: univ                Batch: 12/15	Loss 0.8533 (0.8671)
+2022-11-18 16:07:05,943:INFO: Dataset: univ                Batch: 13/15	Loss 0.8538 (0.8661)
+2022-11-18 16:07:05,945:INFO: Dataset: univ                Batch: 14/15	Loss 0.8571 (0.8654)
+2022-11-18 16:07:05,946:INFO: Dataset: univ                Batch: 15/15	Loss 0.8415 (0.8651)
+2022-11-18 16:07:06,207:INFO: Dataset: zara1               Batch: 1/8	Loss 0.6509 (0.6509)
+2022-11-18 16:07:06,210:INFO: Dataset: zara1               Batch: 2/8	Loss 0.6553 (0.6530)
+2022-11-18 16:07:06,212:INFO: Dataset: zara1               Batch: 3/8	Loss 0.7199 (0.6742)
+2022-11-18 16:07:06,214:INFO: Dataset: zara1               Batch: 4/8	Loss 0.6637 (0.6719)
+2022-11-18 16:07:06,216:INFO: Dataset: zara1               Batch: 5/8	Loss 0.6013 (0.6574)
+2022-11-18 16:07:06,256:INFO: Dataset: zara1               Batch: 6/8	Loss 0.6133 (0.6493)
+2022-11-18 16:07:06,257:INFO: Dataset: zara1               Batch: 7/8	Loss 0.6403 (0.6481)
+2022-11-18 16:07:06,258:INFO: Dataset: zara1               Batch: 8/8	Loss 0.6232 (0.6456)
+2022-11-18 16:07:06,529:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9927 (0.9927)
+2022-11-18 16:07:06,533:INFO: Dataset: zara2               Batch:  2/18	Loss 1.0943 (1.0445)
+2022-11-18 16:07:06,536:INFO: Dataset: zara2               Batch:  3/18	Loss 1.0730 (1.0548)
+2022-11-18 16:07:06,543:INFO: Dataset: zara2               Batch:  4/18	Loss 1.0354 (1.0502)
+2022-11-18 16:07:06,605:INFO: Dataset: zara2               Batch:  5/18	Loss 1.0823 (1.0571)
+2022-11-18 16:07:06,612:INFO: Dataset: zara2               Batch:  6/18	Loss 1.0070 (1.0500)
+2022-11-18 16:07:06,614:INFO: Dataset: zara2               Batch:  7/18	Loss 1.0453 (1.0493)
+2022-11-18 16:07:06,615:INFO: Dataset: zara2               Batch:  8/18	Loss 1.0937 (1.0548)
+2022-11-18 16:07:06,616:INFO: Dataset: zara2               Batch:  9/18	Loss 1.0460 (1.0539)
+2022-11-18 16:07:06,617:INFO: Dataset: zara2               Batch: 10/18	Loss 1.0921 (1.0577)
+2022-11-18 16:07:06,619:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0060 (1.0531)
+2022-11-18 16:07:06,620:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9809 (1.0471)
+2022-11-18 16:07:06,622:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9931 (1.0429)
+2022-11-18 16:07:06,623:INFO: Dataset: zara2               Batch: 14/18	Loss 1.0888 (1.0463)
+2022-11-18 16:07:06,624:INFO: Dataset: zara2               Batch: 15/18	Loss 1.0459 (1.0463)
+2022-11-18 16:07:06,625:INFO: Dataset: zara2               Batch: 16/18	Loss 1.0549 (1.0468)
+2022-11-18 16:07:06,627:INFO: Dataset: zara2               Batch: 17/18	Loss 1.0320 (1.0460)
+2022-11-18 16:07:06,628:INFO: Dataset: zara2               Batch: 18/18	Loss 1.1097 (1.0493)
+2022-11-18 16:07:06,671:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:06,979:INFO: 		 ADE on eth                       dataset:	 1.8309091329574585
+2022-11-18 16:07:06,979:INFO: Average validation o:	ADE  1.8309	FDE  2.7147
+2022-11-18 16:07:06,980:INFO: - Computing loss (validation)
+2022-11-18 16:07:07,169:INFO: Dataset: hotel               Batch: 1/2	Loss 0.8969 (0.8969)
+2022-11-18 16:07:07,171:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8808 (0.8957)
+2022-11-18 16:07:07,430:INFO: Dataset: univ                Batch: 1/3	Loss 0.7323 (0.7323)
+2022-11-18 16:07:07,431:INFO: Dataset: univ                Batch: 2/3	Loss 0.6811 (0.7084)
+2022-11-18 16:07:07,431:INFO: Dataset: univ                Batch: 3/3	Loss 0.7306 (0.7142)
+2022-11-18 16:07:07,698:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4900 (0.4900)
+2022-11-18 16:07:07,699:INFO: Dataset: zara1               Batch: 2/2	Loss 0.5134 (0.4956)
+2022-11-18 16:07:07,981:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0537 (1.0537)
+2022-11-18 16:07:07,984:INFO: Dataset: zara2               Batch: 2/5	Loss 1.1310 (1.0923)
+2022-11-18 16:07:07,987:INFO: Dataset: zara2               Batch: 3/5	Loss 1.1138 (1.0994)
+2022-11-18 16:07:07,988:INFO: Dataset: zara2               Batch: 4/5	Loss 1.1012 (1.0999)
+2022-11-18 16:07:07,990:INFO: Dataset: zara2               Batch: 5/5	Loss 1.1100 (1.1019)
+2022-11-18 16:07:08,050:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_722.pth.tar
+2022-11-18 16:07:08,050:INFO: 
+===> EPOCH: 723 (P4)
+2022-11-18 16:07:08,051:INFO: - Computing loss (training)
+2022-11-18 16:07:08,262:INFO: Dataset: hotel               Batch: 1/4	Loss 0.8237 (0.8237)
+2022-11-18 16:07:08,269:INFO: Dataset: hotel               Batch: 2/4	Loss 0.8001 (0.8114)
+2022-11-18 16:07:08,271:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7893 (0.8044)
+2022-11-18 16:07:08,280:INFO: Dataset: hotel               Batch: 4/4	Loss 0.8170 (0.8065)
+2022-11-18 16:07:08,566:INFO: Dataset: univ                Batch:  1/15	Loss 0.7632 (0.7632)
+2022-11-18 16:07:08,567:INFO: Dataset: univ                Batch:  2/15	Loss 0.7726 (0.7679)
+2022-11-18 16:07:08,569:INFO: Dataset: univ                Batch:  3/15	Loss 0.7910 (0.7759)
+2022-11-18 16:07:08,570:INFO: Dataset: univ                Batch:  4/15	Loss 0.7676 (0.7739)
+2022-11-18 16:07:08,572:INFO: Dataset: univ                Batch:  5/15	Loss 0.7602 (0.7712)
+2022-11-18 16:07:08,615:INFO: Dataset: univ                Batch:  6/15	Loss 0.7613 (0.7696)
+2022-11-18 16:07:08,617:INFO: Dataset: univ                Batch:  7/15	Loss 0.7523 (0.7673)
+2022-11-18 16:07:08,618:INFO: Dataset: univ                Batch:  8/15	Loss 0.7513 (0.7653)
+2022-11-18 16:07:08,619:INFO: Dataset: univ                Batch:  9/15	Loss 0.7737 (0.7663)
+2022-11-18 16:07:08,621:INFO: Dataset: univ                Batch: 10/15	Loss 0.7604 (0.7658)
+2022-11-18 16:07:08,622:INFO: Dataset: univ                Batch: 11/15	Loss 0.7416 (0.7636)
+2022-11-18 16:07:08,626:INFO: Dataset: univ                Batch: 12/15	Loss 0.7546 (0.7629)
+2022-11-18 16:07:08,628:INFO: Dataset: univ                Batch: 13/15	Loss 0.7529 (0.7621)
+2022-11-18 16:07:08,629:INFO: Dataset: univ                Batch: 14/15	Loss 0.7281 (0.7597)
+2022-11-18 16:07:08,630:INFO: Dataset: univ                Batch: 15/15	Loss 0.7753 (0.7599)
+2022-11-18 16:07:08,893:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5669 (0.5669)
+2022-11-18 16:07:08,895:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5605 (0.5638)
+2022-11-18 16:07:08,898:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5748 (0.5677)
+2022-11-18 16:07:08,902:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5800 (0.5709)
+2022-11-18 16:07:08,905:INFO: Dataset: zara1               Batch: 5/8	Loss 0.5775 (0.5721)
+2022-11-18 16:07:08,946:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5962 (0.5764)
+2022-11-18 16:07:08,947:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5565 (0.5735)
+2022-11-18 16:07:08,949:INFO: Dataset: zara1               Batch: 8/8	Loss 0.5900 (0.5753)
+2022-11-18 16:07:09,203:INFO: Dataset: zara2               Batch:  1/18	Loss 0.9284 (0.9284)
+2022-11-18 16:07:09,207:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8995 (0.9126)
+2022-11-18 16:07:09,216:INFO: Dataset: zara2               Batch:  3/18	Loss 0.9932 (0.9377)
+2022-11-18 16:07:09,266:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9328 (0.9365)
+2022-11-18 16:07:09,268:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8654 (0.9223)
+2022-11-18 16:07:09,273:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9475 (0.9261)
+2022-11-18 16:07:09,275:INFO: Dataset: zara2               Batch:  7/18	Loss 0.8991 (0.9224)
+2022-11-18 16:07:09,277:INFO: Dataset: zara2               Batch:  8/18	Loss 0.9706 (0.9283)
+2022-11-18 16:07:09,279:INFO: Dataset: zara2               Batch:  9/18	Loss 0.9800 (0.9336)
+2022-11-18 16:07:09,281:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9809 (0.9384)
+2022-11-18 16:07:09,285:INFO: Dataset: zara2               Batch: 11/18	Loss 1.0014 (0.9445)
+2022-11-18 16:07:09,287:INFO: Dataset: zara2               Batch: 12/18	Loss 0.9419 (0.9443)
+2022-11-18 16:07:09,289:INFO: Dataset: zara2               Batch: 13/18	Loss 0.9745 (0.9467)
+2022-11-18 16:07:09,290:INFO: Dataset: zara2               Batch: 14/18	Loss 0.9823 (0.9494)
+2022-11-18 16:07:09,292:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8997 (0.9462)
+2022-11-18 16:07:09,296:INFO: Dataset: zara2               Batch: 16/18	Loss 0.9420 (0.9460)
+2022-11-18 16:07:09,300:INFO: Dataset: zara2               Batch: 17/18	Loss 0.9642 (0.9470)
+2022-11-18 16:07:09,304:INFO: Dataset: zara2               Batch: 18/18	Loss 0.9431 (0.9468)
+2022-11-18 16:07:09,357:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:09,726:INFO: 		 ADE on eth                       dataset:	 1.791434407234192
+2022-11-18 16:07:09,726:INFO: Average validation o:	ADE  1.7914	FDE  2.7408
+2022-11-18 16:07:09,728:INFO: - Computing loss (validation)
+2022-11-18 16:07:09,945:INFO: Dataset: hotel               Batch: 1/2	Loss 0.8287 (0.8287)
+2022-11-18 16:07:09,946:INFO: Dataset: hotel               Batch: 2/2	Loss 0.8171 (0.8279)
+2022-11-18 16:07:10,200:INFO: Dataset: univ                Batch: 1/3	Loss 0.6201 (0.6201)
+2022-11-18 16:07:10,201:INFO: Dataset: univ                Batch: 2/3	Loss 0.6241 (0.6222)
+2022-11-18 16:07:10,202:INFO: Dataset: univ                Batch: 3/3	Loss 0.6425 (0.6293)
+2022-11-18 16:07:10,446:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4454 (0.4454)
+2022-11-18 16:07:10,449:INFO: Dataset: zara1               Batch: 2/2	Loss 0.4268 (0.4408)
+2022-11-18 16:07:10,736:INFO: Dataset: zara2               Batch: 1/5	Loss 1.0129 (1.0129)
+2022-11-18 16:07:10,739:INFO: Dataset: zara2               Batch: 2/5	Loss 1.0094 (1.0111)
+2022-11-18 16:07:10,741:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9785 (1.0003)
+2022-11-18 16:07:10,744:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9863 (0.9967)
+2022-11-18 16:07:10,745:INFO: Dataset: zara2               Batch: 5/5	Loss 0.9750 (0.9925)
+2022-11-18 16:07:10,815:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_723.pth.tar
+2022-11-18 16:07:10,815:INFO: 
+===> EPOCH: 724 (P4)
+2022-11-18 16:07:10,817:INFO: - Computing loss (training)
+2022-11-18 16:07:11,021:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7549 (0.7549)
+2022-11-18 16:07:11,024:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7440 (0.7494)
+2022-11-18 16:07:11,026:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7328 (0.7444)
+2022-11-18 16:07:11,028:INFO: Dataset: hotel               Batch: 4/4	Loss 0.7722 (0.7493)
+2022-11-18 16:07:11,270:INFO: Dataset: univ                Batch:  1/15	Loss 0.6991 (0.6991)
+2022-11-18 16:07:11,273:INFO: Dataset: univ                Batch:  2/15	Loss 0.6800 (0.6898)
+2022-11-18 16:07:11,307:INFO: Dataset: univ                Batch:  3/15	Loss 0.6716 (0.6842)
+2022-11-18 16:07:11,309:INFO: Dataset: univ                Batch:  4/15	Loss 0.6695 (0.6805)
+2022-11-18 16:07:11,310:INFO: Dataset: univ                Batch:  5/15	Loss 0.6473 (0.6744)
+2022-11-18 16:07:11,315:INFO: Dataset: univ                Batch:  6/15	Loss 0.6678 (0.6732)
+2022-11-18 16:07:11,316:INFO: Dataset: univ                Batch:  7/15	Loss 0.6711 (0.6729)
+2022-11-18 16:07:11,317:INFO: Dataset: univ                Batch:  8/15	Loss 0.6871 (0.6748)
+2022-11-18 16:07:11,319:INFO: Dataset: univ                Batch:  9/15	Loss 0.6677 (0.6740)
+2022-11-18 16:07:11,320:INFO: Dataset: univ                Batch: 10/15	Loss 0.6698 (0.6735)
+2022-11-18 16:07:11,322:INFO: Dataset: univ                Batch: 11/15	Loss 0.6793 (0.6741)
+2022-11-18 16:07:11,324:INFO: Dataset: univ                Batch: 12/15	Loss 0.6536 (0.6725)
+2022-11-18 16:07:11,325:INFO: Dataset: univ                Batch: 13/15	Loss 0.6607 (0.6715)
+2022-11-18 16:07:11,326:INFO: Dataset: univ                Batch: 14/15	Loss 0.6550 (0.6703)
+2022-11-18 16:07:11,328:INFO: Dataset: univ                Batch: 15/15	Loss 0.6375 (0.6700)
+2022-11-18 16:07:11,564:INFO: Dataset: zara1               Batch: 1/8	Loss 0.5412 (0.5412)
+2022-11-18 16:07:11,568:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5435 (0.5423)
+2022-11-18 16:07:11,582:INFO: Dataset: zara1               Batch: 3/8	Loss 0.5435 (0.5427)
+2022-11-18 16:07:11,583:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5143 (0.5360)
+2022-11-18 16:07:11,585:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4951 (0.5274)
+2022-11-18 16:07:11,671:INFO: Dataset: zara1               Batch: 6/8	Loss 0.5157 (0.5253)
+2022-11-18 16:07:11,675:INFO: Dataset: zara1               Batch: 7/8	Loss 0.5305 (0.5261)
+2022-11-18 16:07:11,680:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4964 (0.5228)
+2022-11-18 16:07:11,954:INFO: Dataset: zara2               Batch:  1/18	Loss 0.8653 (0.8653)
+2022-11-18 16:07:11,957:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8358 (0.8516)
+2022-11-18 16:07:11,982:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8807 (0.8606)
+2022-11-18 16:07:11,983:INFO: Dataset: zara2               Batch:  4/18	Loss 0.9026 (0.8715)
+2022-11-18 16:07:11,985:INFO: Dataset: zara2               Batch:  5/18	Loss 0.8519 (0.8676)
+2022-11-18 16:07:11,999:INFO: Dataset: zara2               Batch:  6/18	Loss 0.9104 (0.8750)
+2022-11-18 16:07:12,000:INFO: Dataset: zara2               Batch:  7/18	Loss 0.9186 (0.8815)
+2022-11-18 16:07:12,002:INFO: Dataset: zara2               Batch:  8/18	Loss 0.8671 (0.8795)
+2022-11-18 16:07:12,003:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8342 (0.8743)
+2022-11-18 16:07:12,004:INFO: Dataset: zara2               Batch: 10/18	Loss 0.9123 (0.8781)
+2022-11-18 16:07:12,005:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8095 (0.8724)
+2022-11-18 16:07:12,007:INFO: Dataset: zara2               Batch: 12/18	Loss 0.8287 (0.8691)
+2022-11-18 16:07:12,008:INFO: Dataset: zara2               Batch: 13/18	Loss 0.8686 (0.8690)
+2022-11-18 16:07:12,009:INFO: Dataset: zara2               Batch: 14/18	Loss 0.8391 (0.8670)
+2022-11-18 16:07:12,010:INFO: Dataset: zara2               Batch: 15/18	Loss 0.8389 (0.8650)
+2022-11-18 16:07:12,012:INFO: Dataset: zara2               Batch: 16/18	Loss 0.8419 (0.8637)
+2022-11-18 16:07:12,013:INFO: Dataset: zara2               Batch: 17/18	Loss 0.8002 (0.8599)
+2022-11-18 16:07:12,015:INFO: Dataset: zara2               Batch: 18/18	Loss 0.8210 (0.8581)
+2022-11-18 16:07:12,058:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:12,319:INFO: 		 ADE on eth                       dataset:	 1.8097782135009766
+2022-11-18 16:07:12,319:INFO: Average validation o:	ADE  1.8098	FDE  2.7214
+2022-11-18 16:07:12,320:INFO: - Computing loss (validation)
+2022-11-18 16:07:12,498:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7733 (0.7733)
+2022-11-18 16:07:12,499:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7879 (0.7743)
+2022-11-18 16:07:12,759:INFO: Dataset: univ                Batch: 1/3	Loss 0.5791 (0.5791)
+2022-11-18 16:07:12,762:INFO: Dataset: univ                Batch: 2/3	Loss 0.5405 (0.5590)
+2022-11-18 16:07:12,764:INFO: Dataset: univ                Batch: 3/3	Loss 0.5562 (0.5583)
+2022-11-18 16:07:13,016:INFO: Dataset: zara1               Batch: 1/2	Loss 0.4079 (0.4079)
+2022-11-18 16:07:13,017:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3843 (0.4023)
+2022-11-18 16:07:13,273:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8638 (0.8638)
+2022-11-18 16:07:13,274:INFO: Dataset: zara2               Batch: 2/5	Loss 0.9109 (0.8880)
+2022-11-18 16:07:13,275:INFO: Dataset: zara2               Batch: 3/5	Loss 0.9156 (0.8967)
+2022-11-18 16:07:13,277:INFO: Dataset: zara2               Batch: 4/5	Loss 0.9085 (0.8998)
+2022-11-18 16:07:13,278:INFO: Dataset: zara2               Batch: 5/5	Loss 0.8846 (0.8968)
+2022-11-18 16:07:13,338:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_724.pth.tar
+2022-11-18 16:07:13,339:INFO: 
+===> EPOCH: 725 (P4)
+2022-11-18 16:07:13,339:INFO: - Computing loss (training)
+2022-11-18 16:07:13,534:INFO: Dataset: hotel               Batch: 1/4	Loss 0.7220 (0.7220)
+2022-11-18 16:07:13,536:INFO: Dataset: hotel               Batch: 2/4	Loss 0.7001 (0.7111)
+2022-11-18 16:07:13,541:INFO: Dataset: hotel               Batch: 3/4	Loss 0.7099 (0.7107)
+2022-11-18 16:07:13,544:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6808 (0.7056)
+2022-11-18 16:07:13,792:INFO: Dataset: univ                Batch:  1/15	Loss 0.6047 (0.6047)
+2022-11-18 16:07:13,801:INFO: Dataset: univ                Batch:  2/15	Loss 0.6035 (0.6041)
+2022-11-18 16:07:13,805:INFO: Dataset: univ                Batch:  3/15	Loss 0.6023 (0.6035)
+2022-11-18 16:07:13,870:INFO: Dataset: univ                Batch:  4/15	Loss 0.5863 (0.5997)
+2022-11-18 16:07:13,871:INFO: Dataset: univ                Batch:  5/15	Loss 0.5952 (0.5989)
+2022-11-18 16:07:13,875:INFO: Dataset: univ                Batch:  6/15	Loss 0.5802 (0.5957)
+2022-11-18 16:07:13,876:INFO: Dataset: univ                Batch:  7/15	Loss 0.6155 (0.5988)
+2022-11-18 16:07:13,878:INFO: Dataset: univ                Batch:  8/15	Loss 0.6015 (0.5991)
+2022-11-18 16:07:13,879:INFO: Dataset: univ                Batch:  9/15	Loss 0.5933 (0.5985)
+2022-11-18 16:07:13,881:INFO: Dataset: univ                Batch: 10/15	Loss 0.5892 (0.5975)
+2022-11-18 16:07:13,883:INFO: Dataset: univ                Batch: 11/15	Loss 0.5970 (0.5975)
+2022-11-18 16:07:13,885:INFO: Dataset: univ                Batch: 12/15	Loss 0.5857 (0.5964)
+2022-11-18 16:07:13,886:INFO: Dataset: univ                Batch: 13/15	Loss 0.5980 (0.5965)
+2022-11-18 16:07:13,887:INFO: Dataset: univ                Batch: 14/15	Loss 0.5691 (0.5946)
+2022-11-18 16:07:13,888:INFO: Dataset: univ                Batch: 15/15	Loss 0.5524 (0.5942)
+2022-11-18 16:07:14,133:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4503 (0.4503)
+2022-11-18 16:07:14,137:INFO: Dataset: zara1               Batch: 2/8	Loss 0.5006 (0.4778)
+2022-11-18 16:07:14,138:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4845 (0.4801)
+2022-11-18 16:07:14,141:INFO: Dataset: zara1               Batch: 4/8	Loss 0.5191 (0.4902)
+2022-11-18 16:07:14,144:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4766 (0.4873)
+2022-11-18 16:07:14,155:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4718 (0.4848)
+2022-11-18 16:07:14,156:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4910 (0.4856)
+2022-11-18 16:07:14,157:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4608 (0.4830)
+2022-11-18 16:07:14,409:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7807 (0.7807)
+2022-11-18 16:07:14,412:INFO: Dataset: zara2               Batch:  2/18	Loss 0.8003 (0.7908)
+2022-11-18 16:07:14,414:INFO: Dataset: zara2               Batch:  3/18	Loss 0.8193 (0.8008)
+2022-11-18 16:07:14,461:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7986 (0.8002)
+2022-11-18 16:07:14,463:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7971 (0.7997)
+2022-11-18 16:07:14,468:INFO: Dataset: zara2               Batch:  6/18	Loss 0.8211 (0.8032)
+2022-11-18 16:07:14,470:INFO: Dataset: zara2               Batch:  7/18	Loss 0.7909 (0.8015)
+2022-11-18 16:07:14,471:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7590 (0.7957)
+2022-11-18 16:07:14,472:INFO: Dataset: zara2               Batch:  9/18	Loss 0.8209 (0.7985)
+2022-11-18 16:07:14,473:INFO: Dataset: zara2               Batch: 10/18	Loss 0.7572 (0.7944)
+2022-11-18 16:07:14,476:INFO: Dataset: zara2               Batch: 11/18	Loss 0.8003 (0.7950)
+2022-11-18 16:07:14,477:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7593 (0.7919)
+2022-11-18 16:07:14,478:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7637 (0.7896)
+2022-11-18 16:07:14,479:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7586 (0.7874)
+2022-11-18 16:07:14,480:INFO: Dataset: zara2               Batch: 15/18	Loss 0.7517 (0.7850)
+2022-11-18 16:07:14,482:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7469 (0.7827)
+2022-11-18 16:07:14,484:INFO: Dataset: zara2               Batch: 17/18	Loss 0.7502 (0.7807)
+2022-11-18 16:07:14,486:INFO: Dataset: zara2               Batch: 18/18	Loss 0.7647 (0.7799)
+2022-11-18 16:07:14,528:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:14,805:INFO: 		 ADE on eth                       dataset:	 1.8211015462875366
+2022-11-18 16:07:14,806:INFO: Average validation o:	ADE  1.8211	FDE  2.7943
+2022-11-18 16:07:14,807:INFO: - Computing loss (validation)
+2022-11-18 16:07:14,989:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7322 (0.7322)
+2022-11-18 16:07:14,990:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7448 (0.7331)
+2022-11-18 16:07:15,227:INFO: Dataset: univ                Batch: 1/3	Loss 0.5019 (0.5019)
+2022-11-18 16:07:15,228:INFO: Dataset: univ                Batch: 2/3	Loss 0.4915 (0.4971)
+2022-11-18 16:07:15,229:INFO: Dataset: univ                Batch: 3/3	Loss 0.5049 (0.4993)
+2022-11-18 16:07:15,455:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3783 (0.3783)
+2022-11-18 16:07:15,456:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3671 (0.3756)
+2022-11-18 16:07:15,687:INFO: Dataset: zara2               Batch: 1/5	Loss 0.8472 (0.8472)
+2022-11-18 16:07:15,689:INFO: Dataset: zara2               Batch: 2/5	Loss 0.8070 (0.8284)
+2022-11-18 16:07:15,691:INFO: Dataset: zara2               Batch: 3/5	Loss 0.8041 (0.8204)
+2022-11-18 16:07:15,693:INFO: Dataset: zara2               Batch: 4/5	Loss 0.8136 (0.8187)
+2022-11-18 16:07:15,694:INFO: Dataset: zara2               Batch: 5/5	Loss 0.7904 (0.8133)
+2022-11-18 16:07:15,754:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_725.pth.tar
+2022-11-18 16:07:15,755:INFO: 
+===> EPOCH: 726 (P4)
+2022-11-18 16:07:15,755:INFO: - Computing loss (training)
+2022-11-18 16:07:15,942:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6778 (0.6778)
+2022-11-18 16:07:15,943:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6737 (0.6758)
+2022-11-18 16:07:15,946:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6519 (0.6685)
+2022-11-18 16:07:15,948:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6967 (0.6730)
+2022-11-18 16:07:16,190:INFO: Dataset: univ                Batch:  1/15	Loss 0.5353 (0.5353)
+2022-11-18 16:07:16,244:INFO: Dataset: univ                Batch:  2/15	Loss 0.5497 (0.5430)
+2022-11-18 16:07:16,246:INFO: Dataset: univ                Batch:  3/15	Loss 0.5288 (0.5385)
+2022-11-18 16:07:16,248:INFO: Dataset: univ                Batch:  4/15	Loss 0.5436 (0.5398)
+2022-11-18 16:07:16,249:INFO: Dataset: univ                Batch:  5/15	Loss 0.5141 (0.5346)
+2022-11-18 16:07:16,256:INFO: Dataset: univ                Batch:  6/15	Loss 0.5265 (0.5332)
+2022-11-18 16:07:16,257:INFO: Dataset: univ                Batch:  7/15	Loss 0.5315 (0.5329)
+2022-11-18 16:07:16,258:INFO: Dataset: univ                Batch:  8/15	Loss 0.5231 (0.5315)
+2022-11-18 16:07:16,260:INFO: Dataset: univ                Batch:  9/15	Loss 0.5202 (0.5303)
+2022-11-18 16:07:16,261:INFO: Dataset: univ                Batch: 10/15	Loss 0.5340 (0.5306)
+2022-11-18 16:07:16,262:INFO: Dataset: univ                Batch: 11/15	Loss 0.5384 (0.5314)
+2022-11-18 16:07:16,264:INFO: Dataset: univ                Batch: 12/15	Loss 0.5316 (0.5314)
+2022-11-18 16:07:16,265:INFO: Dataset: univ                Batch: 13/15	Loss 0.5324 (0.5315)
+2022-11-18 16:07:16,267:INFO: Dataset: univ                Batch: 14/15	Loss 0.5222 (0.5308)
+2022-11-18 16:07:16,269:INFO: Dataset: univ                Batch: 15/15	Loss 0.4606 (0.5299)
+2022-11-18 16:07:16,518:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4395 (0.4395)
+2022-11-18 16:07:16,519:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4393 (0.4394)
+2022-11-18 16:07:16,522:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4532 (0.4439)
+2022-11-18 16:07:16,524:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4613 (0.4483)
+2022-11-18 16:07:16,525:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4738 (0.4534)
+2022-11-18 16:07:16,594:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4781 (0.4574)
+2022-11-18 16:07:16,599:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4183 (0.4520)
+2022-11-18 16:07:16,603:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4580 (0.4526)
+2022-11-18 16:07:16,880:INFO: Dataset: zara2               Batch:  1/18	Loss 0.7714 (0.7714)
+2022-11-18 16:07:16,885:INFO: Dataset: zara2               Batch:  2/18	Loss 0.7372 (0.7540)
+2022-11-18 16:07:16,888:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6980 (0.7348)
+2022-11-18 16:07:16,917:INFO: Dataset: zara2               Batch:  4/18	Loss 0.7408 (0.7362)
+2022-11-18 16:07:16,919:INFO: Dataset: zara2               Batch:  5/18	Loss 0.7442 (0.7379)
+2022-11-18 16:07:16,935:INFO: Dataset: zara2               Batch:  6/18	Loss 0.7177 (0.7344)
+2022-11-18 16:07:16,936:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6762 (0.7250)
+2022-11-18 16:07:16,938:INFO: Dataset: zara2               Batch:  8/18	Loss 0.7013 (0.7223)
+2022-11-18 16:07:16,939:INFO: Dataset: zara2               Batch:  9/18	Loss 0.7502 (0.7250)
+2022-11-18 16:07:16,940:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6758 (0.7203)
+2022-11-18 16:07:16,941:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6872 (0.7172)
+2022-11-18 16:07:16,943:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7077 (0.7164)
+2022-11-18 16:07:16,944:INFO: Dataset: zara2               Batch: 13/18	Loss 0.7196 (0.7167)
+2022-11-18 16:07:16,946:INFO: Dataset: zara2               Batch: 14/18	Loss 0.7453 (0.7184)
+2022-11-18 16:07:16,947:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6816 (0.7162)
+2022-11-18 16:07:16,948:INFO: Dataset: zara2               Batch: 16/18	Loss 0.7008 (0.7153)
+2022-11-18 16:07:16,949:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6937 (0.7138)
+2022-11-18 16:07:16,951:INFO: Dataset: zara2               Batch: 18/18	Loss 0.6720 (0.7117)
+2022-11-18 16:07:17,010:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:17,288:INFO: 		 ADE on eth                       dataset:	 1.823158621788025
+2022-11-18 16:07:17,289:INFO: Average validation o:	ADE  1.8232	FDE  2.7103
+2022-11-18 16:07:17,290:INFO: - Computing loss (validation)
+2022-11-18 16:07:17,476:INFO: Dataset: hotel               Batch: 1/2	Loss 0.7002 (0.7002)
+2022-11-18 16:07:17,478:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7200 (0.7015)
+2022-11-18 16:07:17,750:INFO: Dataset: univ                Batch: 1/3	Loss 0.4455 (0.4455)
+2022-11-18 16:07:17,751:INFO: Dataset: univ                Batch: 2/3	Loss 0.4522 (0.4488)
+2022-11-18 16:07:17,752:INFO: Dataset: univ                Batch: 3/3	Loss 0.4516 (0.4497)
+2022-11-18 16:07:17,982:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3515 (0.3515)
+2022-11-18 16:07:17,985:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3738 (0.3573)
+2022-11-18 16:07:18,231:INFO: Dataset: zara2               Batch: 1/5	Loss 0.7661 (0.7661)
+2022-11-18 16:07:18,232:INFO: Dataset: zara2               Batch: 2/5	Loss 0.7309 (0.7487)
+2022-11-18 16:07:18,236:INFO: Dataset: zara2               Batch: 3/5	Loss 0.7296 (0.7426)
+2022-11-18 16:07:18,237:INFO: Dataset: zara2               Batch: 4/5	Loss 0.7377 (0.7414)
+2022-11-18 16:07:18,238:INFO: Dataset: zara2               Batch: 5/5	Loss 0.7338 (0.7399)
+2022-11-18 16:07:18,301:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_726.pth.tar
+2022-11-18 16:07:18,301:INFO: 
+===> EPOCH: 727 (P4)
+2022-11-18 16:07:18,301:INFO: - Computing loss (training)
+2022-11-18 16:07:18,487:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6403 (0.6403)
+2022-11-18 16:07:18,490:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6473 (0.6439)
+2022-11-18 16:07:18,495:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6428 (0.6436)
+2022-11-18 16:07:18,497:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6749 (0.6490)
+2022-11-18 16:07:18,749:INFO: Dataset: univ                Batch:  1/15	Loss 0.4830 (0.4830)
+2022-11-18 16:07:18,754:INFO: Dataset: univ                Batch:  2/15	Loss 0.4970 (0.4893)
+2022-11-18 16:07:18,757:INFO: Dataset: univ                Batch:  3/15	Loss 0.4784 (0.4859)
+2022-11-18 16:07:18,761:INFO: Dataset: univ                Batch:  4/15	Loss 0.4845 (0.4856)
+2022-11-18 16:07:18,788:INFO: Dataset: univ                Batch:  5/15	Loss 0.4742 (0.4833)
+2022-11-18 16:07:18,792:INFO: Dataset: univ                Batch:  6/15	Loss 0.4852 (0.4836)
+2022-11-18 16:07:18,794:INFO: Dataset: univ                Batch:  7/15	Loss 0.4869 (0.4841)
+2022-11-18 16:07:18,795:INFO: Dataset: univ                Batch:  8/15	Loss 0.4826 (0.4839)
+2022-11-18 16:07:18,796:INFO: Dataset: univ                Batch:  9/15	Loss 0.4621 (0.4816)
+2022-11-18 16:07:18,798:INFO: Dataset: univ                Batch: 10/15	Loss 0.4710 (0.4804)
+2022-11-18 16:07:18,800:INFO: Dataset: univ                Batch: 11/15	Loss 0.4668 (0.4791)
+2022-11-18 16:07:18,802:INFO: Dataset: univ                Batch: 12/15	Loss 0.4617 (0.4777)
+2022-11-18 16:07:18,803:INFO: Dataset: univ                Batch: 13/15	Loss 0.4570 (0.4759)
+2022-11-18 16:07:18,805:INFO: Dataset: univ                Batch: 14/15	Loss 0.4616 (0.4749)
+2022-11-18 16:07:18,806:INFO: Dataset: univ                Batch: 15/15	Loss 0.4939 (0.4752)
+2022-11-18 16:07:19,058:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4277 (0.4277)
+2022-11-18 16:07:19,064:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4432 (0.4350)
+2022-11-18 16:07:19,065:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4339 (0.4347)
+2022-11-18 16:07:19,069:INFO: Dataset: zara1               Batch: 4/8	Loss 0.4414 (0.4364)
+2022-11-18 16:07:19,071:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4164 (0.4327)
+2022-11-18 16:07:19,122:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4029 (0.4273)
+2022-11-18 16:07:19,126:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4457 (0.4296)
+2022-11-18 16:07:19,130:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4264 (0.4293)
+2022-11-18 16:07:19,408:INFO: Dataset: zara2               Batch:  1/18	Loss 0.6718 (0.6718)
+2022-11-18 16:07:19,410:INFO: Dataset: zara2               Batch:  2/18	Loss 0.6997 (0.6855)
+2022-11-18 16:07:19,441:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6701 (0.6805)
+2022-11-18 16:07:19,442:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6734 (0.6788)
+2022-11-18 16:07:19,444:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6928 (0.6814)
+2022-11-18 16:07:19,463:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6534 (0.6761)
+2022-11-18 16:07:19,464:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6560 (0.6733)
+2022-11-18 16:07:19,465:INFO: Dataset: zara2               Batch:  8/18	Loss 0.6801 (0.6741)
+2022-11-18 16:07:19,467:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5990 (0.6664)
+2022-11-18 16:07:19,468:INFO: Dataset: zara2               Batch: 10/18	Loss 0.6389 (0.6637)
+2022-11-18 16:07:19,469:INFO: Dataset: zara2               Batch: 11/18	Loss 0.6039 (0.6584)
+2022-11-18 16:07:19,471:INFO: Dataset: zara2               Batch: 12/18	Loss 0.7019 (0.6622)
+2022-11-18 16:07:19,472:INFO: Dataset: zara2               Batch: 13/18	Loss 0.6374 (0.6603)
+2022-11-18 16:07:19,473:INFO: Dataset: zara2               Batch: 14/18	Loss 0.6218 (0.6575)
+2022-11-18 16:07:19,474:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6184 (0.6549)
+2022-11-18 16:07:19,476:INFO: Dataset: zara2               Batch: 16/18	Loss 0.6348 (0.6535)
+2022-11-18 16:07:19,477:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6624 (0.6540)
+2022-11-18 16:07:19,479:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5991 (0.6515)
+2022-11-18 16:07:19,524:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:19,832:INFO: 		 ADE on eth                       dataset:	 1.8015162944793701
+2022-11-18 16:07:19,832:INFO: Average validation o:	ADE  1.8015	FDE  2.7242
+2022-11-18 16:07:19,833:INFO: - Computing loss (validation)
+2022-11-18 16:07:20,013:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6782 (0.6782)
+2022-11-18 16:07:20,014:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6593 (0.6769)
+2022-11-18 16:07:20,251:INFO: Dataset: univ                Batch: 1/3	Loss 0.4059 (0.4059)
+2022-11-18 16:07:20,253:INFO: Dataset: univ                Batch: 2/3	Loss 0.4138 (0.4098)
+2022-11-18 16:07:20,255:INFO: Dataset: univ                Batch: 3/3	Loss 0.4028 (0.4076)
+2022-11-18 16:07:20,487:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3461 (0.3461)
+2022-11-18 16:07:20,488:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3416 (0.3449)
+2022-11-18 16:07:20,814:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6748 (0.6748)
+2022-11-18 16:07:20,816:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6718 (0.6734)
+2022-11-18 16:07:20,818:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6701 (0.6723)
+2022-11-18 16:07:20,819:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6817 (0.6747)
+2022-11-18 16:07:20,821:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6771 (0.6752)
+2022-11-18 16:07:20,882:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_727.pth.tar
+2022-11-18 16:07:20,882:INFO: 
+===> EPOCH: 728 (P4)
+2022-11-18 16:07:20,883:INFO: - Computing loss (training)
+2022-11-18 16:07:21,125:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6301 (0.6301)
+2022-11-18 16:07:21,130:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6441 (0.6371)
+2022-11-18 16:07:21,135:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6220 (0.6320)
+2022-11-18 16:07:21,138:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6288 (0.6314)
+2022-11-18 16:07:21,459:INFO: Dataset: univ                Batch:  1/15	Loss 0.4419 (0.4419)
+2022-11-18 16:07:21,464:INFO: Dataset: univ                Batch:  2/15	Loss 0.4391 (0.4404)
+2022-11-18 16:07:21,466:INFO: Dataset: univ                Batch:  3/15	Loss 0.4247 (0.4351)
+2022-11-18 16:07:21,468:INFO: Dataset: univ                Batch:  4/15	Loss 0.4218 (0.4317)
+2022-11-18 16:07:21,492:INFO: Dataset: univ                Batch:  5/15	Loss 0.4365 (0.4327)
+2022-11-18 16:07:21,497:INFO: Dataset: univ                Batch:  6/15	Loss 0.4419 (0.4341)
+2022-11-18 16:07:21,499:INFO: Dataset: univ                Batch:  7/15	Loss 0.4250 (0.4328)
+2022-11-18 16:07:21,501:INFO: Dataset: univ                Batch:  8/15	Loss 0.4310 (0.4326)
+2022-11-18 16:07:21,503:INFO: Dataset: univ                Batch:  9/15	Loss 0.4260 (0.4318)
+2022-11-18 16:07:21,505:INFO: Dataset: univ                Batch: 10/15	Loss 0.4215 (0.4308)
+2022-11-18 16:07:21,508:INFO: Dataset: univ                Batch: 11/15	Loss 0.4073 (0.4289)
+2022-11-18 16:07:21,510:INFO: Dataset: univ                Batch: 12/15	Loss 0.4177 (0.4280)
+2022-11-18 16:07:21,512:INFO: Dataset: univ                Batch: 13/15	Loss 0.4372 (0.4287)
+2022-11-18 16:07:21,514:INFO: Dataset: univ                Batch: 14/15	Loss 0.4211 (0.4282)
+2022-11-18 16:07:21,517:INFO: Dataset: univ                Batch: 15/15	Loss 0.4484 (0.4284)
+2022-11-18 16:07:21,786:INFO: Dataset: zara1               Batch: 1/8	Loss 0.4280 (0.4280)
+2022-11-18 16:07:21,788:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3917 (0.4098)
+2022-11-18 16:07:21,790:INFO: Dataset: zara1               Batch: 3/8	Loss 0.4185 (0.4125)
+2022-11-18 16:07:21,792:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3946 (0.4077)
+2022-11-18 16:07:21,795:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4136 (0.4090)
+2022-11-18 16:07:21,815:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3839 (0.4046)
+2022-11-18 16:07:21,816:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4362 (0.4086)
+2022-11-18 16:07:21,817:INFO: Dataset: zara1               Batch: 8/8	Loss 0.4281 (0.4108)
+2022-11-18 16:07:22,066:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5838 (0.5838)
+2022-11-18 16:07:22,082:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5781 (0.5811)
+2022-11-18 16:07:22,084:INFO: Dataset: zara2               Batch:  3/18	Loss 0.6238 (0.5953)
+2022-11-18 16:07:22,085:INFO: Dataset: zara2               Batch:  4/18	Loss 0.6048 (0.5977)
+2022-11-18 16:07:22,123:INFO: Dataset: zara2               Batch:  5/18	Loss 0.6199 (0.6020)
+2022-11-18 16:07:22,136:INFO: Dataset: zara2               Batch:  6/18	Loss 0.6039 (0.6024)
+2022-11-18 16:07:22,137:INFO: Dataset: zara2               Batch:  7/18	Loss 0.6226 (0.6055)
+2022-11-18 16:07:22,139:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5786 (0.6017)
+2022-11-18 16:07:22,140:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6009 (0.6016)
+2022-11-18 16:07:22,141:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5999 (0.6014)
+2022-11-18 16:07:22,143:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5903 (0.6004)
+2022-11-18 16:07:22,145:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5989 (0.6003)
+2022-11-18 16:07:22,146:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5883 (0.5993)
+2022-11-18 16:07:22,147:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5966 (0.5991)
+2022-11-18 16:07:22,149:INFO: Dataset: zara2               Batch: 15/18	Loss 0.6234 (0.6008)
+2022-11-18 16:07:22,150:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5709 (0.5990)
+2022-11-18 16:07:22,152:INFO: Dataset: zara2               Batch: 17/18	Loss 0.6066 (0.5994)
+2022-11-18 16:07:22,154:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5734 (0.5981)
+2022-11-18 16:07:22,209:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:22,525:INFO: 		 ADE on eth                       dataset:	 1.8060005903244019
+2022-11-18 16:07:22,525:INFO: Average validation o:	ADE  1.8060	FDE  2.6943
+2022-11-18 16:07:22,526:INFO: - Computing loss (validation)
+2022-11-18 16:07:22,722:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6599 (0.6599)
+2022-11-18 16:07:22,725:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6435 (0.6584)
+2022-11-18 16:07:22,973:INFO: Dataset: univ                Batch: 1/3	Loss 0.3681 (0.3681)
+2022-11-18 16:07:22,974:INFO: Dataset: univ                Batch: 2/3	Loss 0.3631 (0.3657)
+2022-11-18 16:07:22,976:INFO: Dataset: univ                Batch: 3/3	Loss 0.3838 (0.3719)
+2022-11-18 16:07:23,204:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3376 (0.3376)
+2022-11-18 16:07:23,205:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3332 (0.3366)
+2022-11-18 16:07:23,449:INFO: Dataset: zara2               Batch: 1/5	Loss 0.6409 (0.6409)
+2022-11-18 16:07:23,450:INFO: Dataset: zara2               Batch: 2/5	Loss 0.6164 (0.6288)
+2022-11-18 16:07:23,451:INFO: Dataset: zara2               Batch: 3/5	Loss 0.6121 (0.6231)
+2022-11-18 16:07:23,453:INFO: Dataset: zara2               Batch: 4/5	Loss 0.6087 (0.6196)
+2022-11-18 16:07:23,455:INFO: Dataset: zara2               Batch: 5/5	Loss 0.6130 (0.6182)
+2022-11-18 16:07:23,528:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_728.pth.tar
+2022-11-18 16:07:23,529:INFO: 
+===> EPOCH: 729 (P4)
+2022-11-18 16:07:23,529:INFO: - Computing loss (training)
+2022-11-18 16:07:23,720:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6258 (0.6258)
+2022-11-18 16:07:23,724:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6106 (0.6183)
+2022-11-18 16:07:23,726:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6134 (0.6168)
+2022-11-18 16:07:23,728:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6315 (0.6192)
+2022-11-18 16:07:24,031:INFO: Dataset: univ                Batch:  1/15	Loss 0.3995 (0.3995)
+2022-11-18 16:07:24,032:INFO: Dataset: univ                Batch:  2/15	Loss 0.3985 (0.3990)
+2022-11-18 16:07:24,034:INFO: Dataset: univ                Batch:  3/15	Loss 0.3949 (0.3976)
+2022-11-18 16:07:24,052:INFO: Dataset: univ                Batch:  4/15	Loss 0.4068 (0.4000)
+2022-11-18 16:07:24,055:INFO: Dataset: univ                Batch:  5/15	Loss 0.3750 (0.3952)
+2022-11-18 16:07:24,060:INFO: Dataset: univ                Batch:  6/15	Loss 0.3854 (0.3936)
+2022-11-18 16:07:24,062:INFO: Dataset: univ                Batch:  7/15	Loss 0.3931 (0.3935)
+2022-11-18 16:07:24,063:INFO: Dataset: univ                Batch:  8/15	Loss 0.3819 (0.3920)
+2022-11-18 16:07:24,065:INFO: Dataset: univ                Batch:  9/15	Loss 0.3924 (0.3921)
+2022-11-18 16:07:24,066:INFO: Dataset: univ                Batch: 10/15	Loss 0.3850 (0.3913)
+2022-11-18 16:07:24,069:INFO: Dataset: univ                Batch: 11/15	Loss 0.3776 (0.3902)
+2022-11-18 16:07:24,071:INFO: Dataset: univ                Batch: 12/15	Loss 0.3788 (0.3892)
+2022-11-18 16:07:24,073:INFO: Dataset: univ                Batch: 13/15	Loss 0.3829 (0.3887)
+2022-11-18 16:07:24,075:INFO: Dataset: univ                Batch: 14/15	Loss 0.3819 (0.3882)
+2022-11-18 16:07:24,076:INFO: Dataset: univ                Batch: 15/15	Loss 0.3747 (0.3881)
+2022-11-18 16:07:24,316:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3956 (0.3956)
+2022-11-18 16:07:24,319:INFO: Dataset: zara1               Batch: 2/8	Loss 0.4042 (0.3999)
+2022-11-18 16:07:24,321:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3951 (0.3982)
+2022-11-18 16:07:24,325:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3889 (0.3960)
+2022-11-18 16:07:24,327:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3920 (0.3953)
+2022-11-18 16:07:24,360:INFO: Dataset: zara1               Batch: 6/8	Loss 0.4101 (0.3976)
+2022-11-18 16:07:24,361:INFO: Dataset: zara1               Batch: 7/8	Loss 0.4009 (0.3980)
+2022-11-18 16:07:24,362:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3807 (0.3961)
+2022-11-18 16:07:24,619:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5335 (0.5335)
+2022-11-18 16:07:24,623:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5494 (0.5414)
+2022-11-18 16:07:24,643:INFO: Dataset: zara2               Batch:  3/18	Loss 0.5664 (0.5503)
+2022-11-18 16:07:24,645:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5446 (0.5488)
+2022-11-18 16:07:24,710:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5314 (0.5453)
+2022-11-18 16:07:24,716:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5479 (0.5457)
+2022-11-18 16:07:24,718:INFO: Dataset: zara2               Batch:  7/18	Loss 0.5651 (0.5488)
+2022-11-18 16:07:24,719:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5289 (0.5461)
+2022-11-18 16:07:24,720:INFO: Dataset: zara2               Batch:  9/18	Loss 0.6002 (0.5521)
+2022-11-18 16:07:24,721:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5482 (0.5517)
+2022-11-18 16:07:24,724:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5383 (0.5503)
+2022-11-18 16:07:24,725:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5338 (0.5491)
+2022-11-18 16:07:24,726:INFO: Dataset: zara2               Batch: 13/18	Loss 0.5271 (0.5475)
+2022-11-18 16:07:24,727:INFO: Dataset: zara2               Batch: 14/18	Loss 0.5555 (0.5480)
+2022-11-18 16:07:24,729:INFO: Dataset: zara2               Batch: 15/18	Loss 0.5279 (0.5466)
+2022-11-18 16:07:24,730:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5802 (0.5489)
+2022-11-18 16:07:24,732:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5756 (0.5504)
+2022-11-18 16:07:24,734:INFO: Dataset: zara2               Batch: 18/18	Loss 0.5639 (0.5510)
+2022-11-18 16:07:24,783:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:25,101:INFO: 		 ADE on eth                       dataset:	 1.810541033744812
+2022-11-18 16:07:25,101:INFO: Average validation o:	ADE  1.8105	FDE  2.7519
+2022-11-18 16:07:25,103:INFO: - Computing loss (validation)
+2022-11-18 16:07:25,313:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6476 (0.6476)
+2022-11-18 16:07:25,317:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6176 (0.6453)
+2022-11-18 16:07:25,601:INFO: Dataset: univ                Batch: 1/3	Loss 0.3380 (0.3380)
+2022-11-18 16:07:25,602:INFO: Dataset: univ                Batch: 2/3	Loss 0.3436 (0.3409)
+2022-11-18 16:07:25,603:INFO: Dataset: univ                Batch: 3/3	Loss 0.3436 (0.3418)
+2022-11-18 16:07:25,863:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3345 (0.3345)
+2022-11-18 16:07:25,864:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3170 (0.3306)
+2022-11-18 16:07:26,112:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5550 (0.5550)
+2022-11-18 16:07:26,114:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5780 (0.5673)
+2022-11-18 16:07:26,117:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5866 (0.5737)
+2022-11-18 16:07:26,118:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5512 (0.5683)
+2022-11-18 16:07:26,119:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5663 (0.5679)
+2022-11-18 16:07:26,175:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_729.pth.tar
+2022-11-18 16:07:26,175:INFO: 
+===> EPOCH: 730 (P4)
+2022-11-18 16:07:26,176:INFO: - Computing loss (training)
+2022-11-18 16:07:26,359:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6062 (0.6062)
+2022-11-18 16:07:26,362:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6274 (0.6169)
+2022-11-18 16:07:26,364:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6059 (0.6135)
+2022-11-18 16:07:26,367:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6028 (0.6117)
+2022-11-18 16:07:26,643:INFO: Dataset: univ                Batch:  1/15	Loss 0.3622 (0.3622)
+2022-11-18 16:07:26,650:INFO: Dataset: univ                Batch:  2/15	Loss 0.3600 (0.3612)
+2022-11-18 16:07:26,653:INFO: Dataset: univ                Batch:  3/15	Loss 0.3709 (0.3644)
+2022-11-18 16:07:26,655:INFO: Dataset: univ                Batch:  4/15	Loss 0.3574 (0.3627)
+2022-11-18 16:07:26,688:INFO: Dataset: univ                Batch:  5/15	Loss 0.3632 (0.3628)
+2022-11-18 16:07:26,695:INFO: Dataset: univ                Batch:  6/15	Loss 0.3497 (0.3606)
+2022-11-18 16:07:26,697:INFO: Dataset: univ                Batch:  7/15	Loss 0.3478 (0.3587)
+2022-11-18 16:07:26,699:INFO: Dataset: univ                Batch:  8/15	Loss 0.3495 (0.3576)
+2022-11-18 16:07:26,701:INFO: Dataset: univ                Batch:  9/15	Loss 0.3533 (0.3571)
+2022-11-18 16:07:26,702:INFO: Dataset: univ                Batch: 10/15	Loss 0.3397 (0.3553)
+2022-11-18 16:07:26,705:INFO: Dataset: univ                Batch: 11/15	Loss 0.3520 (0.3550)
+2022-11-18 16:07:26,708:INFO: Dataset: univ                Batch: 12/15	Loss 0.3620 (0.3556)
+2022-11-18 16:07:26,710:INFO: Dataset: univ                Batch: 13/15	Loss 0.3522 (0.3554)
+2022-11-18 16:07:26,712:INFO: Dataset: univ                Batch: 14/15	Loss 0.3344 (0.3539)
+2022-11-18 16:07:26,715:INFO: Dataset: univ                Batch: 15/15	Loss 0.3336 (0.3536)
+2022-11-18 16:07:26,955:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3717 (0.3717)
+2022-11-18 16:07:26,959:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3815 (0.3764)
+2022-11-18 16:07:26,962:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3827 (0.3787)
+2022-11-18 16:07:26,969:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3812 (0.3793)
+2022-11-18 16:07:26,970:INFO: Dataset: zara1               Batch: 5/8	Loss 0.4068 (0.3845)
+2022-11-18 16:07:27,023:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3962 (0.3864)
+2022-11-18 16:07:27,025:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3643 (0.3829)
+2022-11-18 16:07:27,026:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3911 (0.3838)
+2022-11-18 16:07:27,331:INFO: Dataset: zara2               Batch:  1/18	Loss 0.5056 (0.5056)
+2022-11-18 16:07:27,332:INFO: Dataset: zara2               Batch:  2/18	Loss 0.5294 (0.5175)
+2022-11-18 16:07:27,336:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4921 (0.5095)
+2022-11-18 16:07:27,338:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5243 (0.5132)
+2022-11-18 16:07:27,339:INFO: Dataset: zara2               Batch:  5/18	Loss 0.5491 (0.5198)
+2022-11-18 16:07:27,342:INFO: Dataset: zara2               Batch:  6/18	Loss 0.5217 (0.5201)
+2022-11-18 16:07:27,343:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4933 (0.5164)
+2022-11-18 16:07:27,344:INFO: Dataset: zara2               Batch:  8/18	Loss 0.5008 (0.5144)
+2022-11-18 16:07:27,346:INFO: Dataset: zara2               Batch:  9/18	Loss 0.5238 (0.5154)
+2022-11-18 16:07:27,347:INFO: Dataset: zara2               Batch: 10/18	Loss 0.5330 (0.5171)
+2022-11-18 16:07:27,349:INFO: Dataset: zara2               Batch: 11/18	Loss 0.5227 (0.5176)
+2022-11-18 16:07:27,351:INFO: Dataset: zara2               Batch: 12/18	Loss 0.5032 (0.5165)
+2022-11-18 16:07:27,353:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4952 (0.5147)
+2022-11-18 16:07:27,355:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4893 (0.5128)
+2022-11-18 16:07:27,357:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4745 (0.5103)
+2022-11-18 16:07:27,359:INFO: Dataset: zara2               Batch: 16/18	Loss 0.5132 (0.5105)
+2022-11-18 16:07:27,361:INFO: Dataset: zara2               Batch: 17/18	Loss 0.5069 (0.5103)
+2022-11-18 16:07:27,363:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4878 (0.5092)
+2022-11-18 16:07:27,411:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:27,690:INFO: 		 ADE on eth                       dataset:	 1.8066600561141968
+2022-11-18 16:07:27,691:INFO: Average validation o:	ADE  1.8067	FDE  2.7551
+2022-11-18 16:07:27,692:INFO: - Computing loss (validation)
+2022-11-18 16:07:27,873:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6323 (0.6323)
+2022-11-18 16:07:27,874:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6867 (0.6354)
+2022-11-18 16:07:28,116:INFO: Dataset: univ                Batch: 1/3	Loss 0.3190 (0.3190)
+2022-11-18 16:07:28,118:INFO: Dataset: univ                Batch: 2/3	Loss 0.3188 (0.3189)
+2022-11-18 16:07:28,124:INFO: Dataset: univ                Batch: 3/3	Loss 0.3089 (0.3153)
+2022-11-18 16:07:28,361:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3310 (0.3310)
+2022-11-18 16:07:28,362:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3140 (0.3267)
+2022-11-18 16:07:28,645:INFO: Dataset: zara2               Batch: 1/5	Loss 0.5142 (0.5142)
+2022-11-18 16:07:28,646:INFO: Dataset: zara2               Batch: 2/5	Loss 0.5379 (0.5264)
+2022-11-18 16:07:28,647:INFO: Dataset: zara2               Batch: 3/5	Loss 0.5187 (0.5239)
+2022-11-18 16:07:28,649:INFO: Dataset: zara2               Batch: 4/5	Loss 0.5131 (0.5210)
+2022-11-18 16:07:28,651:INFO: Dataset: zara2               Batch: 5/5	Loss 0.5311 (0.5231)
+2022-11-18 16:07:28,715:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_730.pth.tar
+2022-11-18 16:07:28,716:INFO: 
+===> EPOCH: 731 (P4)
+2022-11-18 16:07:28,717:INFO: - Computing loss (training)
+2022-11-18 16:07:28,921:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6134 (0.6134)
+2022-11-18 16:07:28,925:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5998 (0.6064)
+2022-11-18 16:07:28,930:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5954 (0.6028)
+2022-11-18 16:07:28,934:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6274 (0.6069)
+2022-11-18 16:07:29,217:INFO: Dataset: univ                Batch:  1/15	Loss 0.3211 (0.3211)
+2022-11-18 16:07:29,220:INFO: Dataset: univ                Batch:  2/15	Loss 0.3199 (0.3205)
+2022-11-18 16:07:29,223:INFO: Dataset: univ                Batch:  3/15	Loss 0.3330 (0.3246)
+2022-11-18 16:07:29,244:INFO: Dataset: univ                Batch:  4/15	Loss 0.3244 (0.3245)
+2022-11-18 16:07:29,246:INFO: Dataset: univ                Batch:  5/15	Loss 0.3304 (0.3257)
+2022-11-18 16:07:29,252:INFO: Dataset: univ                Batch:  6/15	Loss 0.3293 (0.3263)
+2022-11-18 16:07:29,254:INFO: Dataset: univ                Batch:  7/15	Loss 0.3226 (0.3258)
+2022-11-18 16:07:29,256:INFO: Dataset: univ                Batch:  8/15	Loss 0.3385 (0.3273)
+2022-11-18 16:07:29,257:INFO: Dataset: univ                Batch:  9/15	Loss 0.3125 (0.3257)
+2022-11-18 16:07:29,259:INFO: Dataset: univ                Batch: 10/15	Loss 0.3117 (0.3243)
+2022-11-18 16:07:29,261:INFO: Dataset: univ                Batch: 11/15	Loss 0.3201 (0.3239)
+2022-11-18 16:07:29,263:INFO: Dataset: univ                Batch: 12/15	Loss 0.3269 (0.3242)
+2022-11-18 16:07:29,265:INFO: Dataset: univ                Batch: 13/15	Loss 0.3293 (0.3246)
+2022-11-18 16:07:29,267:INFO: Dataset: univ                Batch: 14/15	Loss 0.3133 (0.3238)
+2022-11-18 16:07:29,268:INFO: Dataset: univ                Batch: 15/15	Loss 0.2993 (0.3235)
+2022-11-18 16:07:29,555:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3579 (0.3579)
+2022-11-18 16:07:29,559:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3880 (0.3738)
+2022-11-18 16:07:29,570:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3775 (0.3752)
+2022-11-18 16:07:29,574:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3819 (0.3768)
+2022-11-18 16:07:29,575:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3701 (0.3754)
+2022-11-18 16:07:29,621:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3771 (0.3757)
+2022-11-18 16:07:29,623:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3675 (0.3746)
+2022-11-18 16:07:29,624:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3631 (0.3734)
+2022-11-18 16:07:29,871:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4616 (0.4616)
+2022-11-18 16:07:29,874:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4677 (0.4647)
+2022-11-18 16:07:29,876:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4642 (0.4645)
+2022-11-18 16:07:29,917:INFO: Dataset: zara2               Batch:  4/18	Loss 0.5022 (0.4744)
+2022-11-18 16:07:29,918:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4706 (0.4736)
+2022-11-18 16:07:29,921:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4980 (0.4776)
+2022-11-18 16:07:29,922:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4657 (0.4759)
+2022-11-18 16:07:29,923:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4683 (0.4749)
+2022-11-18 16:07:29,925:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4480 (0.4719)
+2022-11-18 16:07:29,926:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4761 (0.4723)
+2022-11-18 16:07:29,928:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4816 (0.4732)
+2022-11-18 16:07:29,930:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4765 (0.4735)
+2022-11-18 16:07:29,931:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4610 (0.4725)
+2022-11-18 16:07:29,932:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4889 (0.4738)
+2022-11-18 16:07:29,933:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4654 (0.4732)
+2022-11-18 16:07:29,935:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4637 (0.4726)
+2022-11-18 16:07:29,937:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4621 (0.4720)
+2022-11-18 16:07:29,939:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4676 (0.4717)
+2022-11-18 16:07:29,982:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:30,267:INFO: 		 ADE on eth                       dataset:	 1.791709542274475
+2022-11-18 16:07:30,267:INFO: Average validation o:	ADE  1.7917	FDE  2.7373
+2022-11-18 16:07:30,268:INFO: - Computing loss (validation)
+2022-11-18 16:07:30,454:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6298 (0.6298)
+2022-11-18 16:07:30,476:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6150 (0.6289)
+2022-11-18 16:07:30,734:INFO: Dataset: univ                Batch: 1/3	Loss 0.2945 (0.2945)
+2022-11-18 16:07:30,735:INFO: Dataset: univ                Batch: 2/3	Loss 0.2846 (0.2893)
+2022-11-18 16:07:30,737:INFO: Dataset: univ                Batch: 3/3	Loss 0.2991 (0.2926)
+2022-11-18 16:07:31,014:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3272 (0.3272)
+2022-11-18 16:07:31,015:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3139 (0.3237)
+2022-11-18 16:07:31,285:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4996 (0.4996)
+2022-11-18 16:07:31,295:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4618 (0.4814)
+2022-11-18 16:07:31,299:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4840 (0.4823)
+2022-11-18 16:07:31,300:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4900 (0.4843)
+2022-11-18 16:07:31,301:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4795 (0.4834)
+2022-11-18 16:07:31,363:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_731.pth.tar
+2022-11-18 16:07:31,363:INFO: 
+===> EPOCH: 732 (P4)
+2022-11-18 16:07:31,364:INFO: - Computing loss (training)
+2022-11-18 16:07:31,581:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6171 (0.6171)
+2022-11-18 16:07:31,585:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6080 (0.6126)
+2022-11-18 16:07:31,593:INFO: Dataset: hotel               Batch: 3/4	Loss 0.5954 (0.6068)
+2022-11-18 16:07:31,594:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5961 (0.6049)
+2022-11-18 16:07:31,885:INFO: Dataset: univ                Batch:  1/15	Loss 0.2960 (0.2960)
+2022-11-18 16:07:31,889:INFO: Dataset: univ                Batch:  2/15	Loss 0.2971 (0.2966)
+2022-11-18 16:07:31,891:INFO: Dataset: univ                Batch:  3/15	Loss 0.2975 (0.2969)
+2022-11-18 16:07:31,897:INFO: Dataset: univ                Batch:  4/15	Loss 0.3017 (0.2981)
+2022-11-18 16:07:31,924:INFO: Dataset: univ                Batch:  5/15	Loss 0.3017 (0.2988)
+2022-11-18 16:07:31,929:INFO: Dataset: univ                Batch:  6/15	Loss 0.3077 (0.3002)
+2022-11-18 16:07:31,931:INFO: Dataset: univ                Batch:  7/15	Loss 0.2959 (0.2996)
+2022-11-18 16:07:31,932:INFO: Dataset: univ                Batch:  8/15	Loss 0.2965 (0.2992)
+2022-11-18 16:07:31,934:INFO: Dataset: univ                Batch:  9/15	Loss 0.2977 (0.2990)
+2022-11-18 16:07:31,935:INFO: Dataset: univ                Batch: 10/15	Loss 0.2963 (0.2988)
+2022-11-18 16:07:31,936:INFO: Dataset: univ                Batch: 11/15	Loss 0.2992 (0.2988)
+2022-11-18 16:07:31,939:INFO: Dataset: univ                Batch: 12/15	Loss 0.2921 (0.2982)
+2022-11-18 16:07:31,940:INFO: Dataset: univ                Batch: 13/15	Loss 0.2965 (0.2981)
+2022-11-18 16:07:31,942:INFO: Dataset: univ                Batch: 14/15	Loss 0.2913 (0.2976)
+2022-11-18 16:07:31,943:INFO: Dataset: univ                Batch: 15/15	Loss 0.2879 (0.2974)
+2022-11-18 16:07:32,212:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3507 (0.3507)
+2022-11-18 16:07:32,214:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3670 (0.3586)
+2022-11-18 16:07:32,218:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3751 (0.3642)
+2022-11-18 16:07:32,220:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3594 (0.3630)
+2022-11-18 16:07:32,222:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3745 (0.3650)
+2022-11-18 16:07:32,251:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3389 (0.3605)
+2022-11-18 16:07:32,253:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3658 (0.3613)
+2022-11-18 16:07:32,254:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3914 (0.3645)
+2022-11-18 16:07:32,534:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4601 (0.4601)
+2022-11-18 16:07:32,537:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4358 (0.4491)
+2022-11-18 16:07:32,542:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4534 (0.4504)
+2022-11-18 16:07:32,544:INFO: Dataset: zara2               Batch:  4/18	Loss 0.4266 (0.4444)
+2022-11-18 16:07:32,587:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4242 (0.4404)
+2022-11-18 16:07:32,591:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4447 (0.4411)
+2022-11-18 16:07:32,592:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4272 (0.4393)
+2022-11-18 16:07:32,594:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4675 (0.4432)
+2022-11-18 16:07:32,595:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4375 (0.4426)
+2022-11-18 16:07:32,596:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4264 (0.4410)
+2022-11-18 16:07:32,598:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4676 (0.4436)
+2022-11-18 16:07:32,600:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4376 (0.4430)
+2022-11-18 16:07:32,602:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4501 (0.4435)
+2022-11-18 16:07:32,603:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4262 (0.4423)
+2022-11-18 16:07:32,605:INFO: Dataset: zara2               Batch: 15/18	Loss 0.4063 (0.4399)
+2022-11-18 16:07:32,606:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4374 (0.4398)
+2022-11-18 16:07:32,608:INFO: Dataset: zara2               Batch: 17/18	Loss 0.4264 (0.4390)
+2022-11-18 16:07:32,610:INFO: Dataset: zara2               Batch: 18/18	Loss 0.4254 (0.4384)
+2022-11-18 16:07:32,656:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:32,992:INFO: 		 ADE on eth                       dataset:	 1.8125613927841187
+2022-11-18 16:07:32,992:INFO: Average validation o:	ADE  1.8126	FDE  2.7306
+2022-11-18 16:07:32,993:INFO: - Computing loss (validation)
+2022-11-18 16:07:33,183:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6257 (0.6257)
+2022-11-18 16:07:33,184:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6122 (0.6248)
+2022-11-18 16:07:33,428:INFO: Dataset: univ                Batch: 1/3	Loss 0.2656 (0.2656)
+2022-11-18 16:07:33,433:INFO: Dataset: univ                Batch: 2/3	Loss 0.2823 (0.2748)
+2022-11-18 16:07:33,435:INFO: Dataset: univ                Batch: 3/3	Loss 0.2689 (0.2727)
+2022-11-18 16:07:33,660:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3199 (0.3199)
+2022-11-18 16:07:33,661:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3241 (0.3210)
+2022-11-18 16:07:33,967:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4470 (0.4470)
+2022-11-18 16:07:33,968:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4549 (0.4510)
+2022-11-18 16:07:33,984:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4572 (0.4529)
+2022-11-18 16:07:33,984:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4334 (0.4480)
+2022-11-18 16:07:33,985:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4474 (0.4479)
+2022-11-18 16:07:34,044:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_732.pth.tar
+2022-11-18 16:07:34,044:INFO: 
+===> EPOCH: 733 (P4)
+2022-11-18 16:07:34,045:INFO: - Computing loss (training)
+2022-11-18 16:07:34,244:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5955 (0.5955)
+2022-11-18 16:07:34,247:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6187 (0.6068)
+2022-11-18 16:07:34,250:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6130 (0.6089)
+2022-11-18 16:07:34,253:INFO: Dataset: hotel               Batch: 4/4	Loss 0.5857 (0.6048)
+2022-11-18 16:07:34,497:INFO: Dataset: univ                Batch:  1/15	Loss 0.2700 (0.2700)
+2022-11-18 16:07:34,500:INFO: Dataset: univ                Batch:  2/15	Loss 0.2835 (0.2759)
+2022-11-18 16:07:34,554:INFO: Dataset: univ                Batch:  3/15	Loss 0.2726 (0.2748)
+2022-11-18 16:07:34,556:INFO: Dataset: univ                Batch:  4/15	Loss 0.2814 (0.2763)
+2022-11-18 16:07:34,558:INFO: Dataset: univ                Batch:  5/15	Loss 0.2778 (0.2766)
+2022-11-18 16:07:34,561:INFO: Dataset: univ                Batch:  6/15	Loss 0.2749 (0.2763)
+2022-11-18 16:07:34,562:INFO: Dataset: univ                Batch:  7/15	Loss 0.2793 (0.2767)
+2022-11-18 16:07:34,564:INFO: Dataset: univ                Batch:  8/15	Loss 0.2746 (0.2764)
+2022-11-18 16:07:34,565:INFO: Dataset: univ                Batch:  9/15	Loss 0.2741 (0.2762)
+2022-11-18 16:07:34,567:INFO: Dataset: univ                Batch: 10/15	Loss 0.2753 (0.2761)
+2022-11-18 16:07:34,569:INFO: Dataset: univ                Batch: 11/15	Loss 0.2711 (0.2756)
+2022-11-18 16:07:34,571:INFO: Dataset: univ                Batch: 12/15	Loss 0.2746 (0.2755)
+2022-11-18 16:07:34,572:INFO: Dataset: univ                Batch: 13/15	Loss 0.2784 (0.2758)
+2022-11-18 16:07:34,574:INFO: Dataset: univ                Batch: 14/15	Loss 0.2655 (0.2750)
+2022-11-18 16:07:34,576:INFO: Dataset: univ                Batch: 15/15	Loss 0.2586 (0.2748)
+2022-11-18 16:07:34,811:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3753 (0.3753)
+2022-11-18 16:07:34,814:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3313 (0.3515)
+2022-11-18 16:07:34,824:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3533 (0.3521)
+2022-11-18 16:07:34,826:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3652 (0.3553)
+2022-11-18 16:07:34,827:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3643 (0.3571)
+2022-11-18 16:07:34,866:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3467 (0.3554)
+2022-11-18 16:07:34,867:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3712 (0.3577)
+2022-11-18 16:07:34,869:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3441 (0.3563)
+2022-11-18 16:07:35,197:INFO: Dataset: zara2               Batch:  1/18	Loss 0.4151 (0.4151)
+2022-11-18 16:07:35,199:INFO: Dataset: zara2               Batch:  2/18	Loss 0.4030 (0.4091)
+2022-11-18 16:07:35,201:INFO: Dataset: zara2               Batch:  3/18	Loss 0.4070 (0.4084)
+2022-11-18 16:07:35,203:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3975 (0.4058)
+2022-11-18 16:07:35,220:INFO: Dataset: zara2               Batch:  5/18	Loss 0.4214 (0.4088)
+2022-11-18 16:07:35,252:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4047 (0.4082)
+2022-11-18 16:07:35,254:INFO: Dataset: zara2               Batch:  7/18	Loss 0.4220 (0.4103)
+2022-11-18 16:07:35,255:INFO: Dataset: zara2               Batch:  8/18	Loss 0.4007 (0.4090)
+2022-11-18 16:07:35,257:INFO: Dataset: zara2               Batch:  9/18	Loss 0.4196 (0.4101)
+2022-11-18 16:07:35,258:INFO: Dataset: zara2               Batch: 10/18	Loss 0.4176 (0.4108)
+2022-11-18 16:07:35,260:INFO: Dataset: zara2               Batch: 11/18	Loss 0.4230 (0.4120)
+2022-11-18 16:07:35,262:INFO: Dataset: zara2               Batch: 12/18	Loss 0.4182 (0.4125)
+2022-11-18 16:07:35,263:INFO: Dataset: zara2               Batch: 13/18	Loss 0.4066 (0.4120)
+2022-11-18 16:07:35,265:INFO: Dataset: zara2               Batch: 14/18	Loss 0.4059 (0.4116)
+2022-11-18 16:07:35,266:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3954 (0.4106)
+2022-11-18 16:07:35,267:INFO: Dataset: zara2               Batch: 16/18	Loss 0.4112 (0.4106)
+2022-11-18 16:07:35,269:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3922 (0.4096)
+2022-11-18 16:07:35,271:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3834 (0.4084)
+2022-11-18 16:07:35,319:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:35,659:INFO: 		 ADE on eth                       dataset:	 1.7661278247833252
+2022-11-18 16:07:35,660:INFO: Average validation o:	ADE  1.7661	FDE  2.6630
+2022-11-18 16:07:35,661:INFO: - Computing loss (validation)
+2022-11-18 16:07:35,901:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6233 (0.6233)
+2022-11-18 16:07:35,903:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6103 (0.6225)
+2022-11-18 16:07:36,176:INFO: Dataset: univ                Batch: 1/3	Loss 0.2562 (0.2562)
+2022-11-18 16:07:36,177:INFO: Dataset: univ                Batch: 2/3	Loss 0.2533 (0.2548)
+2022-11-18 16:07:36,183:INFO: Dataset: univ                Batch: 3/3	Loss 0.2557 (0.2551)
+2022-11-18 16:07:36,410:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3223 (0.3223)
+2022-11-18 16:07:36,411:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3080 (0.3185)
+2022-11-18 16:07:36,678:INFO: Dataset: zara2               Batch: 1/5	Loss 0.4210 (0.4210)
+2022-11-18 16:07:36,691:INFO: Dataset: zara2               Batch: 2/5	Loss 0.4094 (0.4153)
+2022-11-18 16:07:36,692:INFO: Dataset: zara2               Batch: 3/5	Loss 0.4307 (0.4206)
+2022-11-18 16:07:36,693:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4142 (0.4190)
+2022-11-18 16:07:36,695:INFO: Dataset: zara2               Batch: 5/5	Loss 0.4045 (0.4163)
+2022-11-18 16:07:36,764:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_733.pth.tar
+2022-11-18 16:07:36,764:INFO: 
+===> EPOCH: 734 (P4)
+2022-11-18 16:07:36,765:INFO: - Computing loss (training)
+2022-11-18 16:07:36,993:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6102 (0.6102)
+2022-11-18 16:07:36,995:INFO: Dataset: hotel               Batch: 2/4	Loss 0.5990 (0.6046)
+2022-11-18 16:07:36,997:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6016 (0.6036)
+2022-11-18 16:07:37,000:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6198 (0.6063)
+2022-11-18 16:07:37,254:INFO: Dataset: univ                Batch:  1/15	Loss 0.2598 (0.2598)
+2022-11-18 16:07:37,257:INFO: Dataset: univ                Batch:  2/15	Loss 0.2575 (0.2586)
+2022-11-18 16:07:37,298:INFO: Dataset: univ                Batch:  3/15	Loss 0.2669 (0.2613)
+2022-11-18 16:07:37,300:INFO: Dataset: univ                Batch:  4/15	Loss 0.2646 (0.2622)
+2022-11-18 16:07:37,301:INFO: Dataset: univ                Batch:  5/15	Loss 0.2616 (0.2621)
+2022-11-18 16:07:37,306:INFO: Dataset: univ                Batch:  6/15	Loss 0.2462 (0.2592)
+2022-11-18 16:07:37,307:INFO: Dataset: univ                Batch:  7/15	Loss 0.2460 (0.2573)
+2022-11-18 16:07:37,309:INFO: Dataset: univ                Batch:  8/15	Loss 0.2506 (0.2565)
+2022-11-18 16:07:37,310:INFO: Dataset: univ                Batch:  9/15	Loss 0.2572 (0.2566)
+2022-11-18 16:07:37,312:INFO: Dataset: univ                Batch: 10/15	Loss 0.2555 (0.2565)
+2022-11-18 16:07:37,315:INFO: Dataset: univ                Batch: 11/15	Loss 0.2503 (0.2560)
+2022-11-18 16:07:37,316:INFO: Dataset: univ                Batch: 12/15	Loss 0.2564 (0.2560)
+2022-11-18 16:07:37,317:INFO: Dataset: univ                Batch: 13/15	Loss 0.2467 (0.2552)
+2022-11-18 16:07:37,319:INFO: Dataset: univ                Batch: 14/15	Loss 0.2515 (0.2550)
+2022-11-18 16:07:37,321:INFO: Dataset: univ                Batch: 15/15	Loss 0.2598 (0.2550)
+2022-11-18 16:07:37,562:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3474 (0.3474)
+2022-11-18 16:07:37,566:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3486 (0.3480)
+2022-11-18 16:07:37,568:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3437 (0.3465)
+2022-11-18 16:07:37,570:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3457 (0.3463)
+2022-11-18 16:07:37,572:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3419 (0.3453)
+2022-11-18 16:07:37,599:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3556 (0.3472)
+2022-11-18 16:07:37,600:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3565 (0.3486)
+2022-11-18 16:07:37,602:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3510 (0.3488)
+2022-11-18 16:07:37,862:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3808 (0.3808)
+2022-11-18 16:07:37,864:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3887 (0.3847)
+2022-11-18 16:07:37,911:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3870 (0.3854)
+2022-11-18 16:07:37,912:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3714 (0.3821)
+2022-11-18 16:07:37,914:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3658 (0.3788)
+2022-11-18 16:07:37,917:INFO: Dataset: zara2               Batch:  6/18	Loss 0.4005 (0.3823)
+2022-11-18 16:07:37,919:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3837 (0.3825)
+2022-11-18 16:07:37,920:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3860 (0.3829)
+2022-11-18 16:07:37,921:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3872 (0.3834)
+2022-11-18 16:07:37,922:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3975 (0.3848)
+2022-11-18 16:07:37,925:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3844 (0.3847)
+2022-11-18 16:07:37,926:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3948 (0.3856)
+2022-11-18 16:07:37,927:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3684 (0.3842)
+2022-11-18 16:07:37,928:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3751 (0.3836)
+2022-11-18 16:07:37,930:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3845 (0.3836)
+2022-11-18 16:07:37,932:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3580 (0.3820)
+2022-11-18 16:07:37,934:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3727 (0.3815)
+2022-11-18 16:07:37,936:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3785 (0.3813)
+2022-11-18 16:07:37,980:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:38,273:INFO: 		 ADE on eth                       dataset:	 1.8033815622329712
+2022-11-18 16:07:38,273:INFO: Average validation o:	ADE  1.8034	FDE  2.7706
+2022-11-18 16:07:38,274:INFO: - Computing loss (validation)
+2022-11-18 16:07:38,465:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6219 (0.6219)
+2022-11-18 16:07:38,466:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6163 (0.6215)
+2022-11-18 16:07:38,707:INFO: Dataset: univ                Batch: 1/3	Loss 0.2350 (0.2350)
+2022-11-18 16:07:38,711:INFO: Dataset: univ                Batch: 2/3	Loss 0.2357 (0.2354)
+2022-11-18 16:07:38,712:INFO: Dataset: univ                Batch: 3/3	Loss 0.2479 (0.2395)
+2022-11-18 16:07:38,948:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3156 (0.3156)
+2022-11-18 16:07:38,950:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3167 (0.3159)
+2022-11-18 16:07:39,187:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3778 (0.3778)
+2022-11-18 16:07:39,188:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3877 (0.3832)
+2022-11-18 16:07:39,190:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3890 (0.3851)
+2022-11-18 16:07:39,191:INFO: Dataset: zara2               Batch: 4/5	Loss 0.4055 (0.3899)
+2022-11-18 16:07:39,193:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3797 (0.3880)
+2022-11-18 16:07:39,247:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_734.pth.tar
+2022-11-18 16:07:39,247:INFO: 
+===> EPOCH: 735 (P4)
+2022-11-18 16:07:39,248:INFO: - Computing loss (training)
+2022-11-18 16:07:39,457:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5965 (0.5965)
+2022-11-18 16:07:39,460:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6174 (0.6066)
+2022-11-18 16:07:39,462:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6088 (0.6074)
+2022-11-18 16:07:39,465:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6162 (0.6087)
+2022-11-18 16:07:39,704:INFO: Dataset: univ                Batch:  1/15	Loss 0.2320 (0.2320)
+2022-11-18 16:07:39,708:INFO: Dataset: univ                Batch:  2/15	Loss 0.2376 (0.2347)
+2022-11-18 16:07:39,709:INFO: Dataset: univ                Batch:  3/15	Loss 0.2469 (0.2386)
+2022-11-18 16:07:39,712:INFO: Dataset: univ                Batch:  4/15	Loss 0.2317 (0.2367)
+2022-11-18 16:07:39,753:INFO: Dataset: univ                Batch:  5/15	Loss 0.2424 (0.2379)
+2022-11-18 16:07:39,759:INFO: Dataset: univ                Batch:  6/15	Loss 0.2334 (0.2371)
+2022-11-18 16:07:39,761:INFO: Dataset: univ                Batch:  7/15	Loss 0.2383 (0.2373)
+2022-11-18 16:07:39,762:INFO: Dataset: univ                Batch:  8/15	Loss 0.2500 (0.2387)
+2022-11-18 16:07:39,763:INFO: Dataset: univ                Batch:  9/15	Loss 0.2398 (0.2388)
+2022-11-18 16:07:39,765:INFO: Dataset: univ                Batch: 10/15	Loss 0.2421 (0.2391)
+2022-11-18 16:07:39,767:INFO: Dataset: univ                Batch: 11/15	Loss 0.2304 (0.2383)
+2022-11-18 16:07:39,768:INFO: Dataset: univ                Batch: 12/15	Loss 0.2319 (0.2378)
+2022-11-18 16:07:39,770:INFO: Dataset: univ                Batch: 13/15	Loss 0.2380 (0.2378)
+2022-11-18 16:07:39,771:INFO: Dataset: univ                Batch: 14/15	Loss 0.2362 (0.2377)
+2022-11-18 16:07:39,772:INFO: Dataset: univ                Batch: 15/15	Loss 0.2328 (0.2376)
+2022-11-18 16:07:40,011:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3425 (0.3425)
+2022-11-18 16:07:40,015:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3495 (0.3457)
+2022-11-18 16:07:40,020:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3391 (0.3434)
+2022-11-18 16:07:40,021:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3326 (0.3404)
+2022-11-18 16:07:40,023:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3568 (0.3435)
+2022-11-18 16:07:40,049:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3297 (0.3411)
+2022-11-18 16:07:40,051:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3403 (0.3410)
+2022-11-18 16:07:40,052:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3524 (0.3422)
+2022-11-18 16:07:40,297:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3466 (0.3466)
+2022-11-18 16:07:40,298:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3690 (0.3585)
+2022-11-18 16:07:40,302:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3569 (0.3579)
+2022-11-18 16:07:40,303:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3599 (0.3585)
+2022-11-18 16:07:40,353:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3706 (0.3610)
+2022-11-18 16:07:40,362:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3593 (0.3607)
+2022-11-18 16:07:40,363:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3644 (0.3612)
+2022-11-18 16:07:40,365:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3703 (0.3623)
+2022-11-18 16:07:40,366:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3489 (0.3609)
+2022-11-18 16:07:40,367:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3560 (0.3604)
+2022-11-18 16:07:40,368:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3522 (0.3597)
+2022-11-18 16:07:40,370:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3385 (0.3578)
+2022-11-18 16:07:40,371:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3431 (0.3567)
+2022-11-18 16:07:40,372:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3289 (0.3548)
+2022-11-18 16:07:40,374:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3692 (0.3559)
+2022-11-18 16:07:40,375:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3561 (0.3559)
+2022-11-18 16:07:40,376:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3692 (0.3567)
+2022-11-18 16:07:40,378:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3645 (0.3571)
+2022-11-18 16:07:40,424:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:40,707:INFO: 		 ADE on eth                       dataset:	 1.826210379600525
+2022-11-18 16:07:40,707:INFO: Average validation o:	ADE  1.8262	FDE  2.7755
+2022-11-18 16:07:40,708:INFO: - Computing loss (validation)
+2022-11-18 16:07:40,886:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6229 (0.6229)
+2022-11-18 16:07:40,889:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6030 (0.6216)
+2022-11-18 16:07:41,125:INFO: Dataset: univ                Batch: 1/3	Loss 0.2233 (0.2233)
+2022-11-18 16:07:41,126:INFO: Dataset: univ                Batch: 2/3	Loss 0.2243 (0.2238)
+2022-11-18 16:07:41,127:INFO: Dataset: univ                Batch: 3/3	Loss 0.2291 (0.2255)
+2022-11-18 16:07:41,359:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3107 (0.3107)
+2022-11-18 16:07:41,360:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3211 (0.3133)
+2022-11-18 16:07:41,612:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3696 (0.3696)
+2022-11-18 16:07:41,615:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3570 (0.3632)
+2022-11-18 16:07:41,618:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3623 (0.3629)
+2022-11-18 16:07:41,620:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3575 (0.3616)
+2022-11-18 16:07:41,622:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3670 (0.3627)
+2022-11-18 16:07:41,681:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_735.pth.tar
+2022-11-18 16:07:41,681:INFO: 
+===> EPOCH: 736 (P4)
+2022-11-18 16:07:41,682:INFO: - Computing loss (training)
+2022-11-18 16:07:41,870:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6292 (0.6292)
+2022-11-18 16:07:41,873:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6011 (0.6152)
+2022-11-18 16:07:41,874:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6010 (0.6105)
+2022-11-18 16:07:41,877:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6184 (0.6118)
+2022-11-18 16:07:42,112:INFO: Dataset: univ                Batch:  1/15	Loss 0.2298 (0.2298)
+2022-11-18 16:07:42,115:INFO: Dataset: univ                Batch:  2/15	Loss 0.2274 (0.2286)
+2022-11-18 16:07:42,161:INFO: Dataset: univ                Batch:  3/15	Loss 0.2190 (0.2253)
+2022-11-18 16:07:42,162:INFO: Dataset: univ                Batch:  4/15	Loss 0.2254 (0.2253)
+2022-11-18 16:07:42,163:INFO: Dataset: univ                Batch:  5/15	Loss 0.2257 (0.2254)
+2022-11-18 16:07:42,167:INFO: Dataset: univ                Batch:  6/15	Loss 0.2233 (0.2250)
+2022-11-18 16:07:42,169:INFO: Dataset: univ                Batch:  7/15	Loss 0.2170 (0.2239)
+2022-11-18 16:07:42,171:INFO: Dataset: univ                Batch:  8/15	Loss 0.2215 (0.2236)
+2022-11-18 16:07:42,172:INFO: Dataset: univ                Batch:  9/15	Loss 0.2257 (0.2239)
+2022-11-18 16:07:42,173:INFO: Dataset: univ                Batch: 10/15	Loss 0.2228 (0.2237)
+2022-11-18 16:07:42,176:INFO: Dataset: univ                Batch: 11/15	Loss 0.2179 (0.2232)
+2022-11-18 16:07:42,177:INFO: Dataset: univ                Batch: 12/15	Loss 0.2214 (0.2231)
+2022-11-18 16:07:42,179:INFO: Dataset: univ                Batch: 13/15	Loss 0.2159 (0.2225)
+2022-11-18 16:07:42,180:INFO: Dataset: univ                Batch: 14/15	Loss 0.2191 (0.2223)
+2022-11-18 16:07:42,182:INFO: Dataset: univ                Batch: 15/15	Loss 0.2148 (0.2222)
+2022-11-18 16:07:42,418:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3363 (0.3363)
+2022-11-18 16:07:42,420:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3461 (0.3410)
+2022-11-18 16:07:42,423:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3283 (0.3363)
+2022-11-18 16:07:42,426:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3395 (0.3372)
+2022-11-18 16:07:42,428:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3198 (0.3335)
+2022-11-18 16:07:42,447:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3410 (0.3347)
+2022-11-18 16:07:42,448:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3422 (0.3358)
+2022-11-18 16:07:42,450:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3356 (0.3358)
+2022-11-18 16:07:42,686:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3324 (0.3324)
+2022-11-18 16:07:42,688:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3368 (0.3345)
+2022-11-18 16:07:42,691:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3411 (0.3366)
+2022-11-18 16:07:42,708:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3470 (0.3392)
+2022-11-18 16:07:42,710:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3366 (0.3387)
+2022-11-18 16:07:42,714:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3411 (0.3391)
+2022-11-18 16:07:42,715:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3268 (0.3373)
+2022-11-18 16:07:42,717:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3507 (0.3390)
+2022-11-18 16:07:42,718:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3286 (0.3378)
+2022-11-18 16:07:42,719:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3212 (0.3362)
+2022-11-18 16:07:42,721:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3384 (0.3364)
+2022-11-18 16:07:42,723:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3431 (0.3369)
+2022-11-18 16:07:42,724:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3363 (0.3368)
+2022-11-18 16:07:42,725:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3391 (0.3370)
+2022-11-18 16:07:42,726:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3231 (0.3361)
+2022-11-18 16:07:42,728:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3173 (0.3351)
+2022-11-18 16:07:42,730:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3405 (0.3354)
+2022-11-18 16:07:42,732:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3327 (0.3352)
+2022-11-18 16:07:42,778:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:43,057:INFO: 		 ADE on eth                       dataset:	 1.7825515270233154
+2022-11-18 16:07:43,057:INFO: Average validation o:	ADE  1.7826	FDE  2.6324
+2022-11-18 16:07:43,058:INFO: - Computing loss (validation)
+2022-11-18 16:07:43,253:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6199 (0.6199)
+2022-11-18 16:07:43,254:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6524 (0.6223)
+2022-11-18 16:07:43,508:INFO: Dataset: univ                Batch: 1/3	Loss 0.2102 (0.2102)
+2022-11-18 16:07:43,509:INFO: Dataset: univ                Batch: 2/3	Loss 0.2151 (0.2126)
+2022-11-18 16:07:43,510:INFO: Dataset: univ                Batch: 3/3	Loss 0.2136 (0.2129)
+2022-11-18 16:07:43,743:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3100 (0.3100)
+2022-11-18 16:07:43,744:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3131 (0.3107)
+2022-11-18 16:07:43,986:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3354 (0.3354)
+2022-11-18 16:07:43,988:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3481 (0.3419)
+2022-11-18 16:07:43,990:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3464 (0.3434)
+2022-11-18 16:07:43,991:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3310 (0.3403)
+2022-11-18 16:07:43,993:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3383 (0.3399)
+2022-11-18 16:07:44,047:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_736.pth.tar
+2022-11-18 16:07:44,047:INFO: 
+===> EPOCH: 737 (P4)
+2022-11-18 16:07:44,048:INFO: - Computing loss (training)
+2022-11-18 16:07:44,237:INFO: Dataset: hotel               Batch: 1/4	Loss 0.5978 (0.5978)
+2022-11-18 16:07:44,241:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6159 (0.6070)
+2022-11-18 16:07:44,243:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6224 (0.6125)
+2022-11-18 16:07:44,254:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6301 (0.6153)
+2022-11-18 16:07:44,516:INFO: Dataset: univ                Batch:  1/15	Loss 0.2084 (0.2084)
+2022-11-18 16:07:44,518:INFO: Dataset: univ                Batch:  2/15	Loss 0.2097 (0.2090)
+2022-11-18 16:07:44,520:INFO: Dataset: univ                Batch:  3/15	Loss 0.2066 (0.2081)
+2022-11-18 16:07:44,522:INFO: Dataset: univ                Batch:  4/15	Loss 0.2134 (0.2094)
+2022-11-18 16:07:44,591:INFO: Dataset: univ                Batch:  5/15	Loss 0.2128 (0.2101)
+2022-11-18 16:07:44,600:INFO: Dataset: univ                Batch:  6/15	Loss 0.2063 (0.2095)
+2022-11-18 16:07:44,601:INFO: Dataset: univ                Batch:  7/15	Loss 0.2049 (0.2088)
+2022-11-18 16:07:44,602:INFO: Dataset: univ                Batch:  8/15	Loss 0.2110 (0.2090)
+2022-11-18 16:07:44,604:INFO: Dataset: univ                Batch:  9/15	Loss 0.2112 (0.2093)
+2022-11-18 16:07:44,605:INFO: Dataset: univ                Batch: 10/15	Loss 0.2095 (0.2093)
+2022-11-18 16:07:44,606:INFO: Dataset: univ                Batch: 11/15	Loss 0.2098 (0.2094)
+2022-11-18 16:07:44,608:INFO: Dataset: univ                Batch: 12/15	Loss 0.2042 (0.2089)
+2022-11-18 16:07:44,610:INFO: Dataset: univ                Batch: 13/15	Loss 0.2067 (0.2087)
+2022-11-18 16:07:44,611:INFO: Dataset: univ                Batch: 14/15	Loss 0.2102 (0.2089)
+2022-11-18 16:07:44,612:INFO: Dataset: univ                Batch: 15/15	Loss 0.1871 (0.2085)
+2022-11-18 16:07:44,856:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3281 (0.3281)
+2022-11-18 16:07:44,858:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3281 (0.3281)
+2022-11-18 16:07:44,861:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3306 (0.3289)
+2022-11-18 16:07:44,865:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3236 (0.3276)
+2022-11-18 16:07:44,868:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3294 (0.3280)
+2022-11-18 16:07:44,918:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3264 (0.3277)
+2022-11-18 16:07:44,923:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3380 (0.3290)
+2022-11-18 16:07:44,927:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3387 (0.3299)
+2022-11-18 16:07:45,196:INFO: Dataset: zara2               Batch:  1/18	Loss 0.3101 (0.3101)
+2022-11-18 16:07:45,199:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3115 (0.3108)
+2022-11-18 16:07:45,204:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3286 (0.3167)
+2022-11-18 16:07:45,252:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3072 (0.3144)
+2022-11-18 16:07:45,253:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3065 (0.3128)
+2022-11-18 16:07:45,256:INFO: Dataset: zara2               Batch:  6/18	Loss 0.3212 (0.3143)
+2022-11-18 16:07:45,258:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3171 (0.3147)
+2022-11-18 16:07:45,259:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3158 (0.3148)
+2022-11-18 16:07:45,260:INFO: Dataset: zara2               Batch:  9/18	Loss 0.3128 (0.3146)
+2022-11-18 16:07:45,261:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3026 (0.3134)
+2022-11-18 16:07:45,263:INFO: Dataset: zara2               Batch: 11/18	Loss 0.3294 (0.3148)
+2022-11-18 16:07:45,265:INFO: Dataset: zara2               Batch: 12/18	Loss 0.3152 (0.3148)
+2022-11-18 16:07:45,266:INFO: Dataset: zara2               Batch: 13/18	Loss 0.3199 (0.3152)
+2022-11-18 16:07:45,267:INFO: Dataset: zara2               Batch: 14/18	Loss 0.3234 (0.3158)
+2022-11-18 16:07:45,268:INFO: Dataset: zara2               Batch: 15/18	Loss 0.3084 (0.3153)
+2022-11-18 16:07:45,270:INFO: Dataset: zara2               Batch: 16/18	Loss 0.3070 (0.3148)
+2022-11-18 16:07:45,271:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3113 (0.3146)
+2022-11-18 16:07:45,273:INFO: Dataset: zara2               Batch: 18/18	Loss 0.3328 (0.3155)
+2022-11-18 16:07:45,323:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:45,616:INFO: 		 ADE on eth                       dataset:	 1.8299895524978638
+2022-11-18 16:07:45,616:INFO: Average validation o:	ADE  1.8300	FDE  2.7656
+2022-11-18 16:07:45,617:INFO: - Computing loss (validation)
+2022-11-18 16:07:45,805:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6230 (0.6230)
+2022-11-18 16:07:45,806:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6307 (0.6236)
+2022-11-18 16:07:46,044:INFO: Dataset: univ                Batch: 1/3	Loss 0.2045 (0.2045)
+2022-11-18 16:07:46,045:INFO: Dataset: univ                Batch: 2/3	Loss 0.2035 (0.2040)
+2022-11-18 16:07:46,046:INFO: Dataset: univ                Batch: 3/3	Loss 0.1977 (0.2018)
+2022-11-18 16:07:46,284:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3095 (0.3095)
+2022-11-18 16:07:46,285:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3036 (0.3079)
+2022-11-18 16:07:46,523:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3164 (0.3164)
+2022-11-18 16:07:46,524:INFO: Dataset: zara2               Batch: 2/5	Loss 0.3213 (0.3188)
+2022-11-18 16:07:46,525:INFO: Dataset: zara2               Batch: 3/5	Loss 0.3185 (0.3187)
+2022-11-18 16:07:46,527:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3231 (0.3198)
+2022-11-18 16:07:46,528:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3190 (0.3197)
+2022-11-18 16:07:46,597:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_737.pth.tar
+2022-11-18 16:07:46,597:INFO: 
+===> EPOCH: 738 (P4)
+2022-11-18 16:07:46,598:INFO: - Computing loss (training)
+2022-11-18 16:07:46,787:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6253 (0.6253)
+2022-11-18 16:07:46,789:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6127 (0.6190)
+2022-11-18 16:07:46,792:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6101 (0.6161)
+2022-11-18 16:07:46,794:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6337 (0.6188)
+2022-11-18 16:07:47,039:INFO: Dataset: univ                Batch:  1/15	Loss 0.1965 (0.1965)
+2022-11-18 16:07:47,042:INFO: Dataset: univ                Batch:  2/15	Loss 0.2027 (0.1995)
+2022-11-18 16:07:47,044:INFO: Dataset: univ                Batch:  3/15	Loss 0.1982 (0.1991)
+2022-11-18 16:07:47,046:INFO: Dataset: univ                Batch:  4/15	Loss 0.1968 (0.1985)
+2022-11-18 16:07:47,047:INFO: Dataset: univ                Batch:  5/15	Loss 0.2009 (0.1990)
+2022-11-18 16:07:47,104:INFO: Dataset: univ                Batch:  6/15	Loss 0.1965 (0.1985)
+2022-11-18 16:07:47,105:INFO: Dataset: univ                Batch:  7/15	Loss 0.1973 (0.1984)
+2022-11-18 16:07:47,107:INFO: Dataset: univ                Batch:  8/15	Loss 0.1962 (0.1981)
+2022-11-18 16:07:47,108:INFO: Dataset: univ                Batch:  9/15	Loss 0.1991 (0.1982)
+2022-11-18 16:07:47,109:INFO: Dataset: univ                Batch: 10/15	Loss 0.1936 (0.1978)
+2022-11-18 16:07:47,111:INFO: Dataset: univ                Batch: 11/15	Loss 0.1938 (0.1974)
+2022-11-18 16:07:47,113:INFO: Dataset: univ                Batch: 12/15	Loss 0.1920 (0.1970)
+2022-11-18 16:07:47,115:INFO: Dataset: univ                Batch: 13/15	Loss 0.1963 (0.1970)
+2022-11-18 16:07:47,116:INFO: Dataset: univ                Batch: 14/15	Loss 0.1938 (0.1967)
+2022-11-18 16:07:47,117:INFO: Dataset: univ                Batch: 15/15	Loss 0.1856 (0.1966)
+2022-11-18 16:07:47,392:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3247 (0.3247)
+2022-11-18 16:07:47,395:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3277 (0.3264)
+2022-11-18 16:07:47,400:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3350 (0.3292)
+2022-11-18 16:07:47,401:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3212 (0.3272)
+2022-11-18 16:07:47,403:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3214 (0.3260)
+2022-11-18 16:07:47,470:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3198 (0.3249)
+2022-11-18 16:07:47,475:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3221 (0.3245)
+2022-11-18 16:07:47,479:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3230 (0.3243)
+2022-11-18 16:07:47,762:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2940 (0.2940)
+2022-11-18 16:07:47,800:INFO: Dataset: zara2               Batch:  2/18	Loss 0.3040 (0.2992)
+2022-11-18 16:07:47,801:INFO: Dataset: zara2               Batch:  3/18	Loss 0.3030 (0.3004)
+2022-11-18 16:07:47,804:INFO: Dataset: zara2               Batch:  4/18	Loss 0.3143 (0.3036)
+2022-11-18 16:07:47,805:INFO: Dataset: zara2               Batch:  5/18	Loss 0.3018 (0.3033)
+2022-11-18 16:07:47,809:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2860 (0.3005)
+2022-11-18 16:07:47,810:INFO: Dataset: zara2               Batch:  7/18	Loss 0.3028 (0.3008)
+2022-11-18 16:07:47,811:INFO: Dataset: zara2               Batch:  8/18	Loss 0.3004 (0.3008)
+2022-11-18 16:07:47,812:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2864 (0.2991)
+2022-11-18 16:07:47,814:INFO: Dataset: zara2               Batch: 10/18	Loss 0.3108 (0.3002)
+2022-11-18 16:07:47,816:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2834 (0.2987)
+2022-11-18 16:07:47,817:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2908 (0.2981)
+2022-11-18 16:07:47,818:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2994 (0.2982)
+2022-11-18 16:07:47,820:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2982 (0.2982)
+2022-11-18 16:07:47,822:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2908 (0.2977)
+2022-11-18 16:07:47,824:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2895 (0.2972)
+2022-11-18 16:07:47,825:INFO: Dataset: zara2               Batch: 17/18	Loss 0.3082 (0.2978)
+2022-11-18 16:07:47,827:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2987 (0.2978)
+2022-11-18 16:07:47,872:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:48,145:INFO: 		 ADE on eth                       dataset:	 1.793107032775879
+2022-11-18 16:07:48,145:INFO: Average validation o:	ADE  1.7931	FDE  2.7134
+2022-11-18 16:07:48,146:INFO: - Computing loss (validation)
+2022-11-18 16:07:48,328:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6256 (0.6256)
+2022-11-18 16:07:48,329:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6178 (0.6250)
+2022-11-18 16:07:48,564:INFO: Dataset: univ                Batch: 1/3	Loss 0.1910 (0.1910)
+2022-11-18 16:07:48,567:INFO: Dataset: univ                Batch: 2/3	Loss 0.1857 (0.1884)
+2022-11-18 16:07:48,569:INFO: Dataset: univ                Batch: 3/3	Loss 0.1994 (0.1915)
+2022-11-18 16:07:48,802:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3105 (0.3105)
+2022-11-18 16:07:48,803:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2923 (0.3055)
+2022-11-18 16:07:49,040:INFO: Dataset: zara2               Batch: 1/5	Loss 0.3022 (0.3022)
+2022-11-18 16:07:49,042:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2951 (0.2985)
+2022-11-18 16:07:49,047:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2955 (0.2975)
+2022-11-18 16:07:49,048:INFO: Dataset: zara2               Batch: 4/5	Loss 0.3091 (0.3005)
+2022-11-18 16:07:49,054:INFO: Dataset: zara2               Batch: 5/5	Loss 0.3045 (0.3012)
+2022-11-18 16:07:49,111:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_738.pth.tar
+2022-11-18 16:07:49,112:INFO: 
+===> EPOCH: 739 (P4)
+2022-11-18 16:07:49,112:INFO: - Computing loss (training)
+2022-11-18 16:07:49,305:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6379 (0.6379)
+2022-11-18 16:07:49,308:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6144 (0.6264)
+2022-11-18 16:07:49,310:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6122 (0.6215)
+2022-11-18 16:07:49,312:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6269 (0.6224)
+2022-11-18 16:07:49,552:INFO: Dataset: univ                Batch:  1/15	Loss 0.1867 (0.1867)
+2022-11-18 16:07:49,556:INFO: Dataset: univ                Batch:  2/15	Loss 0.1890 (0.1878)
+2022-11-18 16:07:49,557:INFO: Dataset: univ                Batch:  3/15	Loss 0.1893 (0.1883)
+2022-11-18 16:07:49,562:INFO: Dataset: univ                Batch:  4/15	Loss 0.1877 (0.1882)
+2022-11-18 16:07:49,594:INFO: Dataset: univ                Batch:  5/15	Loss 0.1859 (0.1878)
+2022-11-18 16:07:49,615:INFO: Dataset: univ                Batch:  6/15	Loss 0.1888 (0.1880)
+2022-11-18 16:07:49,616:INFO: Dataset: univ                Batch:  7/15	Loss 0.1830 (0.1872)
+2022-11-18 16:07:49,618:INFO: Dataset: univ                Batch:  8/15	Loss 0.1887 (0.1874)
+2022-11-18 16:07:49,619:INFO: Dataset: univ                Batch:  9/15	Loss 0.1834 (0.1870)
+2022-11-18 16:07:49,620:INFO: Dataset: univ                Batch: 10/15	Loss 0.1826 (0.1865)
+2022-11-18 16:07:49,622:INFO: Dataset: univ                Batch: 11/15	Loss 0.1850 (0.1864)
+2022-11-18 16:07:49,624:INFO: Dataset: univ                Batch: 12/15	Loss 0.1858 (0.1863)
+2022-11-18 16:07:49,625:INFO: Dataset: univ                Batch: 13/15	Loss 0.1844 (0.1862)
+2022-11-18 16:07:49,626:INFO: Dataset: univ                Batch: 14/15	Loss 0.1830 (0.1859)
+2022-11-18 16:07:49,627:INFO: Dataset: univ                Batch: 15/15	Loss 0.1727 (0.1858)
+2022-11-18 16:07:49,889:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3165 (0.3165)
+2022-11-18 16:07:49,891:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3247 (0.3205)
+2022-11-18 16:07:49,894:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3161 (0.3189)
+2022-11-18 16:07:49,896:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3144 (0.3178)
+2022-11-18 16:07:49,899:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3253 (0.3193)
+2022-11-18 16:07:49,946:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3135 (0.3183)
+2022-11-18 16:07:49,950:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3155 (0.3179)
+2022-11-18 16:07:49,955:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3318 (0.3194)
+2022-11-18 16:07:50,219:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2943 (0.2943)
+2022-11-18 16:07:50,224:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2815 (0.2872)
+2022-11-18 16:07:50,232:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2860 (0.2868)
+2022-11-18 16:07:50,233:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2789 (0.2848)
+2022-11-18 16:07:50,303:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2772 (0.2832)
+2022-11-18 16:07:50,309:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2772 (0.2822)
+2022-11-18 16:07:50,311:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2940 (0.2839)
+2022-11-18 16:07:50,312:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2868 (0.2843)
+2022-11-18 16:07:50,313:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2700 (0.2829)
+2022-11-18 16:07:50,314:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2698 (0.2816)
+2022-11-18 16:07:50,324:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2690 (0.2805)
+2022-11-18 16:07:50,325:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2846 (0.2808)
+2022-11-18 16:07:50,327:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2867 (0.2812)
+2022-11-18 16:07:50,328:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2788 (0.2811)
+2022-11-18 16:07:50,329:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2958 (0.2819)
+2022-11-18 16:07:50,330:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2843 (0.2821)
+2022-11-18 16:07:50,332:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2828 (0.2821)
+2022-11-18 16:07:50,334:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2760 (0.2818)
+2022-11-18 16:07:50,389:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:50,673:INFO: 		 ADE on eth                       dataset:	 1.7984592914581299
+2022-11-18 16:07:50,673:INFO: Average validation o:	ADE  1.7985	FDE  2.6990
+2022-11-18 16:07:50,674:INFO: - Computing loss (validation)
+2022-11-18 16:07:50,864:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6267 (0.6267)
+2022-11-18 16:07:50,865:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6260 (0.6266)
+2022-11-18 16:07:51,103:INFO: Dataset: univ                Batch: 1/3	Loss 0.1893 (0.1893)
+2022-11-18 16:07:51,106:INFO: Dataset: univ                Batch: 2/3	Loss 0.1729 (0.1803)
+2022-11-18 16:07:51,109:INFO: Dataset: univ                Batch: 3/3	Loss 0.1878 (0.1824)
+2022-11-18 16:07:51,353:INFO: Dataset: zara1               Batch: 1/2	Loss 0.3069 (0.3069)
+2022-11-18 16:07:51,354:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2892 (0.3028)
+2022-11-18 16:07:51,593:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2871 (0.2871)
+2022-11-18 16:07:51,594:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2848 (0.2860)
+2022-11-18 16:07:51,595:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2890 (0.2870)
+2022-11-18 16:07:51,596:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2845 (0.2864)
+2022-11-18 16:07:51,597:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2787 (0.2848)
+2022-11-18 16:07:51,656:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_739.pth.tar
+2022-11-18 16:07:51,656:INFO: 
+===> EPOCH: 740 (P4)
+2022-11-18 16:07:51,657:INFO: - Computing loss (training)
+2022-11-18 16:07:51,851:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6286 (0.6286)
+2022-11-18 16:07:51,853:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6267 (0.6277)
+2022-11-18 16:07:51,863:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6273 (0.6275)
+2022-11-18 16:07:51,865:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6158 (0.6257)
+2022-11-18 16:07:52,183:INFO: Dataset: univ                Batch:  1/15	Loss 0.1774 (0.1774)
+2022-11-18 16:07:52,188:INFO: Dataset: univ                Batch:  2/15	Loss 0.1780 (0.1777)
+2022-11-18 16:07:52,194:INFO: Dataset: univ                Batch:  3/15	Loss 0.1770 (0.1775)
+2022-11-18 16:07:52,198:INFO: Dataset: univ                Batch:  4/15	Loss 0.1786 (0.1778)
+2022-11-18 16:07:52,203:INFO: Dataset: univ                Batch:  5/15	Loss 0.1786 (0.1780)
+2022-11-18 16:07:52,208:INFO: Dataset: univ                Batch:  6/15	Loss 0.1783 (0.1780)
+2022-11-18 16:07:52,210:INFO: Dataset: univ                Batch:  7/15	Loss 0.1774 (0.1779)
+2022-11-18 16:07:52,211:INFO: Dataset: univ                Batch:  8/15	Loss 0.1741 (0.1774)
+2022-11-18 16:07:52,212:INFO: Dataset: univ                Batch:  9/15	Loss 0.1762 (0.1773)
+2022-11-18 16:07:52,214:INFO: Dataset: univ                Batch: 10/15	Loss 0.1767 (0.1772)
+2022-11-18 16:07:52,216:INFO: Dataset: univ                Batch: 11/15	Loss 0.1737 (0.1769)
+2022-11-18 16:07:52,218:INFO: Dataset: univ                Batch: 12/15	Loss 0.1768 (0.1769)
+2022-11-18 16:07:52,220:INFO: Dataset: univ                Batch: 13/15	Loss 0.1729 (0.1766)
+2022-11-18 16:07:52,223:INFO: Dataset: univ                Batch: 14/15	Loss 0.1740 (0.1764)
+2022-11-18 16:07:52,224:INFO: Dataset: univ                Batch: 15/15	Loss 0.1712 (0.1763)
+2022-11-18 16:07:52,459:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3193 (0.3193)
+2022-11-18 16:07:52,462:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3219 (0.3207)
+2022-11-18 16:07:52,464:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3222 (0.3212)
+2022-11-18 16:07:52,465:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3024 (0.3159)
+2022-11-18 16:07:52,467:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3034 (0.3135)
+2022-11-18 16:07:52,499:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3145 (0.3137)
+2022-11-18 16:07:52,500:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3181 (0.3143)
+2022-11-18 16:07:52,501:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3193 (0.3148)
+2022-11-18 16:07:52,747:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2764 (0.2764)
+2022-11-18 16:07:52,795:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2722 (0.2743)
+2022-11-18 16:07:52,796:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2713 (0.2733)
+2022-11-18 16:07:52,797:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2574 (0.2691)
+2022-11-18 16:07:52,798:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2826 (0.2716)
+2022-11-18 16:07:52,801:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2720 (0.2717)
+2022-11-18 16:07:52,802:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2811 (0.2731)
+2022-11-18 16:07:52,803:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2699 (0.2727)
+2022-11-18 16:07:52,805:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2620 (0.2713)
+2022-11-18 16:07:52,806:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2709 (0.2713)
+2022-11-18 16:07:52,808:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2651 (0.2707)
+2022-11-18 16:07:52,810:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2698 (0.2706)
+2022-11-18 16:07:52,811:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2641 (0.2701)
+2022-11-18 16:07:52,813:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2647 (0.2698)
+2022-11-18 16:07:52,815:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2619 (0.2692)
+2022-11-18 16:07:52,817:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2645 (0.2689)
+2022-11-18 16:07:52,819:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2537 (0.2680)
+2022-11-18 16:07:52,821:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2549 (0.2673)
+2022-11-18 16:07:52,868:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:53,151:INFO: 		 ADE on eth                       dataset:	 1.805328369140625
+2022-11-18 16:07:53,151:INFO: Average validation o:	ADE  1.8053	FDE  2.7127
+2022-11-18 16:07:53,152:INFO: - Computing loss (validation)
+2022-11-18 16:07:53,340:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6319 (0.6319)
+2022-11-18 16:07:53,341:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5802 (0.6282)
+2022-11-18 16:07:53,600:INFO: Dataset: univ                Batch: 1/3	Loss 0.1780 (0.1780)
+2022-11-18 16:07:53,602:INFO: Dataset: univ                Batch: 2/3	Loss 0.1714 (0.1744)
+2022-11-18 16:07:53,604:INFO: Dataset: univ                Batch: 3/3	Loss 0.1733 (0.1741)
+2022-11-18 16:07:53,845:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2999 (0.2999)
+2022-11-18 16:07:53,846:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3015 (0.3003)
+2022-11-18 16:07:54,070:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2647 (0.2647)
+2022-11-18 16:07:54,079:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2674 (0.2660)
+2022-11-18 16:07:54,084:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2697 (0.2673)
+2022-11-18 16:07:54,084:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2792 (0.2706)
+2022-11-18 16:07:54,085:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2670 (0.2699)
+2022-11-18 16:07:54,149:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_740.pth.tar
+2022-11-18 16:07:54,149:INFO: 
+===> EPOCH: 741 (P4)
+2022-11-18 16:07:54,150:INFO: - Computing loss (training)
+2022-11-18 16:07:54,355:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6037 (0.6037)
+2022-11-18 16:07:54,360:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6251 (0.6145)
+2022-11-18 16:07:54,361:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6456 (0.6251)
+2022-11-18 16:07:54,363:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6473 (0.6290)
+2022-11-18 16:07:54,620:INFO: Dataset: univ                Batch:  1/15	Loss 0.1690 (0.1690)
+2022-11-18 16:07:54,626:INFO: Dataset: univ                Batch:  2/15	Loss 0.1674 (0.1682)
+2022-11-18 16:07:54,630:INFO: Dataset: univ                Batch:  3/15	Loss 0.1708 (0.1690)
+2022-11-18 16:07:54,632:INFO: Dataset: univ                Batch:  4/15	Loss 0.1665 (0.1684)
+2022-11-18 16:07:54,718:INFO: Dataset: univ                Batch:  5/15	Loss 0.1675 (0.1682)
+2022-11-18 16:07:54,723:INFO: Dataset: univ                Batch:  6/15	Loss 0.1738 (0.1691)
+2022-11-18 16:07:54,724:INFO: Dataset: univ                Batch:  7/15	Loss 0.1670 (0.1688)
+2022-11-18 16:07:54,725:INFO: Dataset: univ                Batch:  8/15	Loss 0.1687 (0.1688)
+2022-11-18 16:07:54,727:INFO: Dataset: univ                Batch:  9/15	Loss 0.1677 (0.1687)
+2022-11-18 16:07:54,728:INFO: Dataset: univ                Batch: 10/15	Loss 0.1682 (0.1687)
+2022-11-18 16:07:54,730:INFO: Dataset: univ                Batch: 11/15	Loss 0.1686 (0.1686)
+2022-11-18 16:07:54,732:INFO: Dataset: univ                Batch: 12/15	Loss 0.1635 (0.1682)
+2022-11-18 16:07:54,733:INFO: Dataset: univ                Batch: 13/15	Loss 0.1661 (0.1681)
+2022-11-18 16:07:54,734:INFO: Dataset: univ                Batch: 14/15	Loss 0.1666 (0.1680)
+2022-11-18 16:07:54,735:INFO: Dataset: univ                Batch: 15/15	Loss 0.1592 (0.1679)
+2022-11-18 16:07:55,003:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3027 (0.3027)
+2022-11-18 16:07:55,006:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3056 (0.3041)
+2022-11-18 16:07:55,009:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3088 (0.3057)
+2022-11-18 16:07:55,013:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3171 (0.3086)
+2022-11-18 16:07:55,015:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3088 (0.3087)
+2022-11-18 16:07:55,042:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3175 (0.3102)
+2022-11-18 16:07:55,043:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3118 (0.3104)
+2022-11-18 16:07:55,044:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3105 (0.3105)
+2022-11-18 16:07:55,304:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2609 (0.2609)
+2022-11-18 16:07:55,316:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2498 (0.2552)
+2022-11-18 16:07:55,324:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2607 (0.2572)
+2022-11-18 16:07:55,326:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2572 (0.2572)
+2022-11-18 16:07:55,357:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2531 (0.2564)
+2022-11-18 16:07:55,365:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2611 (0.2572)
+2022-11-18 16:07:55,367:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2591 (0.2575)
+2022-11-18 16:07:55,368:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2527 (0.2569)
+2022-11-18 16:07:55,369:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2488 (0.2560)
+2022-11-18 16:07:55,370:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2472 (0.2550)
+2022-11-18 16:07:55,372:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2462 (0.2542)
+2022-11-18 16:07:55,373:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2775 (0.2560)
+2022-11-18 16:07:55,375:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2502 (0.2555)
+2022-11-18 16:07:55,376:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2537 (0.2554)
+2022-11-18 16:07:55,377:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2488 (0.2549)
+2022-11-18 16:07:55,378:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2515 (0.2547)
+2022-11-18 16:07:55,380:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2463 (0.2542)
+2022-11-18 16:07:55,381:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2550 (0.2542)
+2022-11-18 16:07:55,439:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:55,730:INFO: 		 ADE on eth                       dataset:	 1.7403254508972168
+2022-11-18 16:07:55,730:INFO: Average validation o:	ADE  1.7403	FDE  2.5808
+2022-11-18 16:07:55,731:INFO: - Computing loss (validation)
+2022-11-18 16:07:55,906:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6303 (0.6303)
+2022-11-18 16:07:55,911:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6214 (0.6296)
+2022-11-18 16:07:56,152:INFO: Dataset: univ                Batch: 1/3	Loss 0.1745 (0.1745)
+2022-11-18 16:07:56,160:INFO: Dataset: univ                Batch: 2/3	Loss 0.1617 (0.1673)
+2022-11-18 16:07:56,161:INFO: Dataset: univ                Batch: 3/3	Loss 0.1654 (0.1667)
+2022-11-18 16:07:56,398:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2944 (0.2944)
+2022-11-18 16:07:56,399:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3069 (0.2979)
+2022-11-18 16:07:56,655:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2559 (0.2559)
+2022-11-18 16:07:56,656:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2550 (0.2555)
+2022-11-18 16:07:56,657:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2533 (0.2547)
+2022-11-18 16:07:56,659:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2598 (0.2559)
+2022-11-18 16:07:56,660:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2587 (0.2565)
+2022-11-18 16:07:56,721:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_741.pth.tar
+2022-11-18 16:07:56,721:INFO: 
+===> EPOCH: 742 (P4)
+2022-11-18 16:07:56,722:INFO: - Computing loss (training)
+2022-11-18 16:07:56,933:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6387 (0.6387)
+2022-11-18 16:07:56,935:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6406 (0.6397)
+2022-11-18 16:07:56,938:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6110 (0.6300)
+2022-11-18 16:07:56,940:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6407 (0.6317)
+2022-11-18 16:07:57,197:INFO: Dataset: univ                Batch:  1/15	Loss 0.1601 (0.1601)
+2022-11-18 16:07:57,199:INFO: Dataset: univ                Batch:  2/15	Loss 0.1608 (0.1604)
+2022-11-18 16:07:57,294:INFO: Dataset: univ                Batch:  3/15	Loss 0.1578 (0.1596)
+2022-11-18 16:07:57,295:INFO: Dataset: univ                Batch:  4/15	Loss 0.1635 (0.1606)
+2022-11-18 16:07:57,296:INFO: Dataset: univ                Batch:  5/15	Loss 0.1633 (0.1611)
+2022-11-18 16:07:57,299:INFO: Dataset: univ                Batch:  6/15	Loss 0.1599 (0.1609)
+2022-11-18 16:07:57,300:INFO: Dataset: univ                Batch:  7/15	Loss 0.1595 (0.1607)
+2022-11-18 16:07:57,302:INFO: Dataset: univ                Batch:  8/15	Loss 0.1612 (0.1607)
+2022-11-18 16:07:57,303:INFO: Dataset: univ                Batch:  9/15	Loss 0.1589 (0.1605)
+2022-11-18 16:07:57,304:INFO: Dataset: univ                Batch: 10/15	Loss 0.1617 (0.1606)
+2022-11-18 16:07:57,307:INFO: Dataset: univ                Batch: 11/15	Loss 0.1633 (0.1609)
+2022-11-18 16:07:57,308:INFO: Dataset: univ                Batch: 12/15	Loss 0.1608 (0.1608)
+2022-11-18 16:07:57,309:INFO: Dataset: univ                Batch: 13/15	Loss 0.1546 (0.1604)
+2022-11-18 16:07:57,310:INFO: Dataset: univ                Batch: 14/15	Loss 0.1608 (0.1604)
+2022-11-18 16:07:57,312:INFO: Dataset: univ                Batch: 15/15	Loss 0.1637 (0.1604)
+2022-11-18 16:07:57,544:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3039 (0.3039)
+2022-11-18 16:07:57,548:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2999 (0.3020)
+2022-11-18 16:07:57,552:INFO: Dataset: zara1               Batch: 3/8	Loss 0.3016 (0.3019)
+2022-11-18 16:07:57,554:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3088 (0.3035)
+2022-11-18 16:07:57,557:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3273 (0.3078)
+2022-11-18 16:07:57,580:INFO: Dataset: zara1               Batch: 6/8	Loss 0.3055 (0.3074)
+2022-11-18 16:07:57,582:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3066 (0.3073)
+2022-11-18 16:07:57,584:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2983 (0.3063)
+2022-11-18 16:07:57,828:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2518 (0.2518)
+2022-11-18 16:07:57,848:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2412 (0.2463)
+2022-11-18 16:07:57,850:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2489 (0.2472)
+2022-11-18 16:07:57,901:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2498 (0.2478)
+2022-11-18 16:07:57,903:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2508 (0.2484)
+2022-11-18 16:07:57,909:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2430 (0.2474)
+2022-11-18 16:07:57,910:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2408 (0.2465)
+2022-11-18 16:07:57,912:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2462 (0.2464)
+2022-11-18 16:07:57,914:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2440 (0.2462)
+2022-11-18 16:07:57,916:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2429 (0.2459)
+2022-11-18 16:07:57,919:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2471 (0.2460)
+2022-11-18 16:07:57,921:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2502 (0.2463)
+2022-11-18 16:07:57,922:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2335 (0.2454)
+2022-11-18 16:07:57,924:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2466 (0.2455)
+2022-11-18 16:07:57,926:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2337 (0.2446)
+2022-11-18 16:07:57,929:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2280 (0.2436)
+2022-11-18 16:07:57,931:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2395 (0.2433)
+2022-11-18 16:07:57,933:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2261 (0.2424)
+2022-11-18 16:07:57,983:INFO: - Computing ADE (validation o)
+2022-11-18 16:07:58,283:INFO: 		 ADE on eth                       dataset:	 1.7725597620010376
+2022-11-18 16:07:58,284:INFO: Average validation o:	ADE  1.7726	FDE  2.6362
+2022-11-18 16:07:58,285:INFO: - Computing loss (validation)
+2022-11-18 16:07:58,495:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6302 (0.6302)
+2022-11-18 16:07:58,496:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6386 (0.6309)
+2022-11-18 16:07:58,765:INFO: Dataset: univ                Batch: 1/3	Loss 0.1697 (0.1697)
+2022-11-18 16:07:58,768:INFO: Dataset: univ                Batch: 2/3	Loss 0.1534 (0.1608)
+2022-11-18 16:07:58,770:INFO: Dataset: univ                Batch: 3/3	Loss 0.1586 (0.1600)
+2022-11-18 16:07:59,024:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2938 (0.2938)
+2022-11-18 16:07:59,025:INFO: Dataset: zara1               Batch: 2/2	Loss 0.3003 (0.2954)
+2022-11-18 16:07:59,300:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2464 (0.2464)
+2022-11-18 16:07:59,301:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2419 (0.2441)
+2022-11-18 16:07:59,302:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2534 (0.2473)
+2022-11-18 16:07:59,302:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2379 (0.2449)
+2022-11-18 16:07:59,303:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2426 (0.2444)
+2022-11-18 16:07:59,359:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_742.pth.tar
+2022-11-18 16:07:59,359:INFO: 
+===> EPOCH: 743 (P4)
+2022-11-18 16:07:59,360:INFO: - Computing loss (training)
+2022-11-18 16:07:59,554:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6308 (0.6308)
+2022-11-18 16:07:59,556:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6423 (0.6363)
+2022-11-18 16:07:59,558:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6291 (0.6338)
+2022-11-18 16:07:59,559:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6356 (0.6341)
+2022-11-18 16:07:59,854:INFO: Dataset: univ                Batch:  1/15	Loss 0.1530 (0.1530)
+2022-11-18 16:07:59,856:INFO: Dataset: univ                Batch:  2/15	Loss 0.1531 (0.1531)
+2022-11-18 16:07:59,857:INFO: Dataset: univ                Batch:  3/15	Loss 0.1560 (0.1540)
+2022-11-18 16:07:59,859:INFO: Dataset: univ                Batch:  4/15	Loss 0.1544 (0.1541)
+2022-11-18 16:07:59,861:INFO: Dataset: univ                Batch:  5/15	Loss 0.1569 (0.1546)
+2022-11-18 16:07:59,864:INFO: Dataset: univ                Batch:  6/15	Loss 0.1536 (0.1545)
+2022-11-18 16:07:59,865:INFO: Dataset: univ                Batch:  7/15	Loss 0.1559 (0.1547)
+2022-11-18 16:07:59,866:INFO: Dataset: univ                Batch:  8/15	Loss 0.1534 (0.1545)
+2022-11-18 16:07:59,868:INFO: Dataset: univ                Batch:  9/15	Loss 0.1539 (0.1544)
+2022-11-18 16:07:59,869:INFO: Dataset: univ                Batch: 10/15	Loss 0.1504 (0.1541)
+2022-11-18 16:07:59,871:INFO: Dataset: univ                Batch: 11/15	Loss 0.1526 (0.1539)
+2022-11-18 16:07:59,873:INFO: Dataset: univ                Batch: 12/15	Loss 0.1517 (0.1537)
+2022-11-18 16:07:59,875:INFO: Dataset: univ                Batch: 13/15	Loss 0.1541 (0.1537)
+2022-11-18 16:07:59,877:INFO: Dataset: univ                Batch: 14/15	Loss 0.1549 (0.1538)
+2022-11-18 16:07:59,879:INFO: Dataset: univ                Batch: 15/15	Loss 0.1492 (0.1538)
+2022-11-18 16:08:00,126:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3014 (0.3014)
+2022-11-18 16:08:00,129:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2936 (0.2974)
+2022-11-18 16:08:00,132:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2964 (0.2971)
+2022-11-18 16:08:00,134:INFO: Dataset: zara1               Batch: 4/8	Loss 0.3032 (0.2985)
+2022-11-18 16:08:00,135:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3124 (0.3013)
+2022-11-18 16:08:00,177:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2935 (0.3001)
+2022-11-18 16:08:00,181:INFO: Dataset: zara1               Batch: 7/8	Loss 0.3106 (0.3015)
+2022-11-18 16:08:00,185:INFO: Dataset: zara1               Batch: 8/8	Loss 0.3097 (0.3023)
+2022-11-18 16:08:00,459:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2365 (0.2365)
+2022-11-18 16:08:00,515:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2305 (0.2335)
+2022-11-18 16:08:00,516:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2410 (0.2359)
+2022-11-18 16:08:00,518:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2282 (0.2341)
+2022-11-18 16:08:00,520:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2260 (0.2325)
+2022-11-18 16:08:00,525:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2248 (0.2312)
+2022-11-18 16:08:00,526:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2391 (0.2325)
+2022-11-18 16:08:00,527:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2317 (0.2324)
+2022-11-18 16:08:00,529:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2425 (0.2336)
+2022-11-18 16:08:00,530:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2402 (0.2342)
+2022-11-18 16:08:00,531:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2237 (0.2332)
+2022-11-18 16:08:00,533:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2178 (0.2322)
+2022-11-18 16:08:00,534:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2261 (0.2317)
+2022-11-18 16:08:00,536:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2361 (0.2320)
+2022-11-18 16:08:00,538:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2297 (0.2318)
+2022-11-18 16:08:00,539:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2355 (0.2321)
+2022-11-18 16:08:00,541:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2305 (0.2320)
+2022-11-18 16:08:00,543:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2257 (0.2317)
+2022-11-18 16:08:00,596:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:00,867:INFO: 		 ADE on eth                       dataset:	 1.827144742012024
+2022-11-18 16:08:00,868:INFO: Average validation o:	ADE  1.8271	FDE  2.7900
+2022-11-18 16:08:00,869:INFO: - Computing loss (validation)
+2022-11-18 16:08:01,081:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6257 (0.6257)
+2022-11-18 16:08:01,082:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7191 (0.6321)
+2022-11-18 16:08:01,308:INFO: Dataset: univ                Batch: 1/3	Loss 0.1538 (0.1538)
+2022-11-18 16:08:01,309:INFO: Dataset: univ                Batch: 2/3	Loss 0.1527 (0.1532)
+2022-11-18 16:08:01,310:INFO: Dataset: univ                Batch: 3/3	Loss 0.1556 (0.1540)
+2022-11-18 16:08:01,556:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2945 (0.2945)
+2022-11-18 16:08:01,560:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2880 (0.2929)
+2022-11-18 16:08:01,801:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2390 (0.2390)
+2022-11-18 16:08:01,803:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2284 (0.2337)
+2022-11-18 16:08:01,804:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2313 (0.2329)
+2022-11-18 16:08:01,812:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2363 (0.2338)
+2022-11-18 16:08:01,823:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2330 (0.2336)
+2022-11-18 16:08:01,883:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_743.pth.tar
+2022-11-18 16:08:01,883:INFO: 
+===> EPOCH: 744 (P4)
+2022-11-18 16:08:01,884:INFO: - Computing loss (training)
+2022-11-18 16:08:02,080:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6478 (0.6478)
+2022-11-18 16:08:02,082:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6314 (0.6398)
+2022-11-18 16:08:02,084:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6446 (0.6415)
+2022-11-18 16:08:02,086:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6120 (0.6362)
+2022-11-18 16:08:02,323:INFO: Dataset: univ                Batch:  1/15	Loss 0.1485 (0.1485)
+2022-11-18 16:08:02,325:INFO: Dataset: univ                Batch:  2/15	Loss 0.1480 (0.1483)
+2022-11-18 16:08:02,356:INFO: Dataset: univ                Batch:  3/15	Loss 0.1501 (0.1489)
+2022-11-18 16:08:02,358:INFO: Dataset: univ                Batch:  4/15	Loss 0.1491 (0.1490)
+2022-11-18 16:08:02,359:INFO: Dataset: univ                Batch:  5/15	Loss 0.1477 (0.1487)
+2022-11-18 16:08:02,364:INFO: Dataset: univ                Batch:  6/15	Loss 0.1457 (0.1482)
+2022-11-18 16:08:02,366:INFO: Dataset: univ                Batch:  7/15	Loss 0.1475 (0.1481)
+2022-11-18 16:08:02,367:INFO: Dataset: univ                Batch:  8/15	Loss 0.1488 (0.1482)
+2022-11-18 16:08:02,368:INFO: Dataset: univ                Batch:  9/15	Loss 0.1489 (0.1483)
+2022-11-18 16:08:02,370:INFO: Dataset: univ                Batch: 10/15	Loss 0.1472 (0.1482)
+2022-11-18 16:08:02,371:INFO: Dataset: univ                Batch: 11/15	Loss 0.1481 (0.1482)
+2022-11-18 16:08:02,373:INFO: Dataset: univ                Batch: 12/15	Loss 0.1455 (0.1480)
+2022-11-18 16:08:02,374:INFO: Dataset: univ                Batch: 13/15	Loss 0.1453 (0.1477)
+2022-11-18 16:08:02,376:INFO: Dataset: univ                Batch: 14/15	Loss 0.1493 (0.1478)
+2022-11-18 16:08:02,377:INFO: Dataset: univ                Batch: 15/15	Loss 0.1467 (0.1478)
+2022-11-18 16:08:02,617:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3019 (0.3019)
+2022-11-18 16:08:02,621:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3020 (0.3019)
+2022-11-18 16:08:02,623:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2933 (0.2991)
+2022-11-18 16:08:02,624:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2988 (0.2990)
+2022-11-18 16:08:02,626:INFO: Dataset: zara1               Batch: 5/8	Loss 0.3001 (0.2992)
+2022-11-18 16:08:02,703:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2980 (0.2990)
+2022-11-18 16:08:02,707:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2994 (0.2991)
+2022-11-18 16:08:02,712:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2956 (0.2987)
+2022-11-18 16:08:02,990:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2172 (0.2172)
+2022-11-18 16:08:03,033:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2216 (0.2194)
+2022-11-18 16:08:03,035:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2181 (0.2190)
+2022-11-18 16:08:03,036:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2217 (0.2196)
+2022-11-18 16:08:03,037:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2275 (0.2211)
+2022-11-18 16:08:03,041:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2220 (0.2212)
+2022-11-18 16:08:03,042:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2271 (0.2221)
+2022-11-18 16:08:03,043:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2228 (0.2222)
+2022-11-18 16:08:03,045:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2280 (0.2229)
+2022-11-18 16:08:03,046:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2250 (0.2231)
+2022-11-18 16:08:03,047:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2195 (0.2227)
+2022-11-18 16:08:03,049:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2204 (0.2225)
+2022-11-18 16:08:03,050:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2162 (0.2221)
+2022-11-18 16:08:03,052:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2239 (0.2222)
+2022-11-18 16:08:03,054:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2242 (0.2224)
+2022-11-18 16:08:03,056:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2249 (0.2225)
+2022-11-18 16:08:03,058:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2114 (0.2219)
+2022-11-18 16:08:03,060:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2231 (0.2219)
+2022-11-18 16:08:03,105:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:03,382:INFO: 		 ADE on eth                       dataset:	 1.7851073741912842
+2022-11-18 16:08:03,382:INFO: Average validation o:	ADE  1.7851	FDE  2.7196
+2022-11-18 16:08:03,383:INFO: - Computing loss (validation)
+2022-11-18 16:08:03,570:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6349 (0.6349)
+2022-11-18 16:08:03,571:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6104 (0.6334)
+2022-11-18 16:08:03,806:INFO: Dataset: univ                Batch: 1/3	Loss 0.1503 (0.1503)
+2022-11-18 16:08:03,808:INFO: Dataset: univ                Batch: 2/3	Loss 0.1473 (0.1487)
+2022-11-18 16:08:03,811:INFO: Dataset: univ                Batch: 3/3	Loss 0.1479 (0.1485)
+2022-11-18 16:08:04,037:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2903 (0.2903)
+2022-11-18 16:08:04,038:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2920 (0.2907)
+2022-11-18 16:08:04,280:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2172 (0.2172)
+2022-11-18 16:08:04,283:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2278 (0.2227)
+2022-11-18 16:08:04,296:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2238 (0.2231)
+2022-11-18 16:08:04,297:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2244 (0.2234)
+2022-11-18 16:08:04,297:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2250 (0.2237)
+2022-11-18 16:08:04,358:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_744.pth.tar
+2022-11-18 16:08:04,358:INFO: 
+===> EPOCH: 745 (P4)
+2022-11-18 16:08:04,359:INFO: - Computing loss (training)
+2022-11-18 16:08:04,560:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6244 (0.6244)
+2022-11-18 16:08:04,562:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6587 (0.6411)
+2022-11-18 16:08:04,563:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6206 (0.6339)
+2022-11-18 16:08:04,565:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6585 (0.6383)
+2022-11-18 16:08:04,810:INFO: Dataset: univ                Batch:  1/15	Loss 0.1415 (0.1415)
+2022-11-18 16:08:04,814:INFO: Dataset: univ                Batch:  2/15	Loss 0.1404 (0.1409)
+2022-11-18 16:08:04,818:INFO: Dataset: univ                Batch:  3/15	Loss 0.1414 (0.1411)
+2022-11-18 16:08:04,821:INFO: Dataset: univ                Batch:  4/15	Loss 0.1452 (0.1421)
+2022-11-18 16:08:04,894:INFO: Dataset: univ                Batch:  5/15	Loss 0.1434 (0.1424)
+2022-11-18 16:08:04,900:INFO: Dataset: univ                Batch:  6/15	Loss 0.1435 (0.1425)
+2022-11-18 16:08:04,902:INFO: Dataset: univ                Batch:  7/15	Loss 0.1402 (0.1422)
+2022-11-18 16:08:04,903:INFO: Dataset: univ                Batch:  8/15	Loss 0.1436 (0.1423)
+2022-11-18 16:08:04,904:INFO: Dataset: univ                Batch:  9/15	Loss 0.1415 (0.1422)
+2022-11-18 16:08:04,905:INFO: Dataset: univ                Batch: 10/15	Loss 0.1440 (0.1424)
+2022-11-18 16:08:04,908:INFO: Dataset: univ                Batch: 11/15	Loss 0.1406 (0.1423)
+2022-11-18 16:08:04,909:INFO: Dataset: univ                Batch: 12/15	Loss 0.1441 (0.1424)
+2022-11-18 16:08:04,910:INFO: Dataset: univ                Batch: 13/15	Loss 0.1440 (0.1425)
+2022-11-18 16:08:04,912:INFO: Dataset: univ                Batch: 14/15	Loss 0.1420 (0.1425)
+2022-11-18 16:08:04,913:INFO: Dataset: univ                Batch: 15/15	Loss 0.1435 (0.1425)
+2022-11-18 16:08:05,148:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2845 (0.2845)
+2022-11-18 16:08:05,149:INFO: Dataset: zara1               Batch: 2/8	Loss 0.3061 (0.2948)
+2022-11-18 16:08:05,151:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2961 (0.2952)
+2022-11-18 16:08:05,152:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2896 (0.2940)
+2022-11-18 16:08:05,153:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2935 (0.2939)
+2022-11-18 16:08:05,171:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2999 (0.2948)
+2022-11-18 16:08:05,172:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2962 (0.2950)
+2022-11-18 16:08:05,173:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2985 (0.2953)
+2022-11-18 16:08:05,439:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2150 (0.2150)
+2022-11-18 16:08:05,442:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2159 (0.2154)
+2022-11-18 16:08:05,495:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2069 (0.2127)
+2022-11-18 16:08:05,497:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2110 (0.2123)
+2022-11-18 16:08:05,498:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2110 (0.2120)
+2022-11-18 16:08:05,503:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2074 (0.2112)
+2022-11-18 16:08:05,504:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2142 (0.2116)
+2022-11-18 16:08:05,505:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2242 (0.2131)
+2022-11-18 16:08:05,506:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2225 (0.2141)
+2022-11-18 16:08:05,508:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2202 (0.2147)
+2022-11-18 16:08:05,509:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2191 (0.2151)
+2022-11-18 16:08:05,511:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2095 (0.2146)
+2022-11-18 16:08:05,512:INFO: Dataset: zara2               Batch: 13/18	Loss 0.2127 (0.2145)
+2022-11-18 16:08:05,513:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2140 (0.2145)
+2022-11-18 16:08:05,515:INFO: Dataset: zara2               Batch: 15/18	Loss 0.2116 (0.2143)
+2022-11-18 16:08:05,517:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2127 (0.2142)
+2022-11-18 16:08:05,519:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2019 (0.2134)
+2022-11-18 16:08:05,521:INFO: Dataset: zara2               Batch: 18/18	Loss 0.2079 (0.2131)
+2022-11-18 16:08:05,564:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:05,839:INFO: 		 ADE on eth                       dataset:	 1.7874915599822998
+2022-11-18 16:08:05,839:INFO: Average validation o:	ADE  1.7875	FDE  2.7088
+2022-11-18 16:08:05,840:INFO: - Computing loss (validation)
+2022-11-18 16:08:06,029:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6360 (0.6360)
+2022-11-18 16:08:06,030:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6019 (0.6341)
+2022-11-18 16:08:06,252:INFO: Dataset: univ                Batch: 1/3	Loss 0.1464 (0.1464)
+2022-11-18 16:08:06,254:INFO: Dataset: univ                Batch: 2/3	Loss 0.1409 (0.1435)
+2022-11-18 16:08:06,256:INFO: Dataset: univ                Batch: 3/3	Loss 0.1435 (0.1435)
+2022-11-18 16:08:06,476:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2877 (0.2877)
+2022-11-18 16:08:06,478:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2912 (0.2885)
+2022-11-18 16:08:06,717:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2165 (0.2165)
+2022-11-18 16:08:06,721:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2093 (0.2132)
+2022-11-18 16:08:06,722:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2115 (0.2126)
+2022-11-18 16:08:06,723:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2192 (0.2144)
+2022-11-18 16:08:06,724:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2173 (0.2150)
+2022-11-18 16:08:06,786:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_745.pth.tar
+2022-11-18 16:08:06,786:INFO: 
+===> EPOCH: 746 (P4)
+2022-11-18 16:08:06,787:INFO: - Computing loss (training)
+2022-11-18 16:08:06,986:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6481 (0.6481)
+2022-11-18 16:08:06,988:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6374 (0.6427)
+2022-11-18 16:08:06,990:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6259 (0.6371)
+2022-11-18 16:08:06,991:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6529 (0.6396)
+2022-11-18 16:08:07,311:INFO: Dataset: univ                Batch:  1/15	Loss 0.1391 (0.1391)
+2022-11-18 16:08:07,312:INFO: Dataset: univ                Batch:  2/15	Loss 0.1391 (0.1391)
+2022-11-18 16:08:07,316:INFO: Dataset: univ                Batch:  3/15	Loss 0.1389 (0.1391)
+2022-11-18 16:08:07,317:INFO: Dataset: univ                Batch:  4/15	Loss 0.1381 (0.1388)
+2022-11-18 16:08:07,319:INFO: Dataset: univ                Batch:  5/15	Loss 0.1369 (0.1384)
+2022-11-18 16:08:07,321:INFO: Dataset: univ                Batch:  6/15	Loss 0.1385 (0.1384)
+2022-11-18 16:08:07,323:INFO: Dataset: univ                Batch:  7/15	Loss 0.1395 (0.1386)
+2022-11-18 16:08:07,324:INFO: Dataset: univ                Batch:  8/15	Loss 0.1379 (0.1385)
+2022-11-18 16:08:07,326:INFO: Dataset: univ                Batch:  9/15	Loss 0.1365 (0.1383)
+2022-11-18 16:08:07,327:INFO: Dataset: univ                Batch: 10/15	Loss 0.1374 (0.1382)
+2022-11-18 16:08:07,330:INFO: Dataset: univ                Batch: 11/15	Loss 0.1415 (0.1385)
+2022-11-18 16:08:07,331:INFO: Dataset: univ                Batch: 12/15	Loss 0.1355 (0.1382)
+2022-11-18 16:08:07,333:INFO: Dataset: univ                Batch: 13/15	Loss 0.1360 (0.1380)
+2022-11-18 16:08:07,335:INFO: Dataset: univ                Batch: 14/15	Loss 0.1357 (0.1379)
+2022-11-18 16:08:07,337:INFO: Dataset: univ                Batch: 15/15	Loss 0.1327 (0.1378)
+2022-11-18 16:08:07,588:INFO: Dataset: zara1               Batch: 1/8	Loss 0.3054 (0.3054)
+2022-11-18 16:08:07,591:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2906 (0.2980)
+2022-11-18 16:08:07,592:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2863 (0.2941)
+2022-11-18 16:08:07,594:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2886 (0.2926)
+2022-11-18 16:08:07,596:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2888 (0.2919)
+2022-11-18 16:08:07,651:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2954 (0.2924)
+2022-11-18 16:08:07,652:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2891 (0.2919)
+2022-11-18 16:08:07,653:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2937 (0.2921)
+2022-11-18 16:08:07,921:INFO: Dataset: zara2               Batch:  1/18	Loss 0.2090 (0.2090)
+2022-11-18 16:08:07,924:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2046 (0.2067)
+2022-11-18 16:08:07,927:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2044 (0.2060)
+2022-11-18 16:08:07,960:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1999 (0.2044)
+2022-11-18 16:08:07,961:INFO: Dataset: zara2               Batch:  5/18	Loss 0.2041 (0.2044)
+2022-11-18 16:08:07,966:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2045 (0.2044)
+2022-11-18 16:08:07,967:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2020 (0.2041)
+2022-11-18 16:08:07,968:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2155 (0.2055)
+2022-11-18 16:08:07,969:INFO: Dataset: zara2               Batch:  9/18	Loss 0.2060 (0.2056)
+2022-11-18 16:08:07,970:INFO: Dataset: zara2               Batch: 10/18	Loss 0.2150 (0.2065)
+2022-11-18 16:08:07,973:INFO: Dataset: zara2               Batch: 11/18	Loss 0.2057 (0.2064)
+2022-11-18 16:08:07,974:INFO: Dataset: zara2               Batch: 12/18	Loss 0.2131 (0.2069)
+2022-11-18 16:08:07,975:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1973 (0.2062)
+2022-11-18 16:08:07,976:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2026 (0.2059)
+2022-11-18 16:08:07,977:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1954 (0.2052)
+2022-11-18 16:08:07,979:INFO: Dataset: zara2               Batch: 16/18	Loss 0.2077 (0.2054)
+2022-11-18 16:08:07,981:INFO: Dataset: zara2               Batch: 17/18	Loss 0.2099 (0.2056)
+2022-11-18 16:08:07,983:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1976 (0.2052)
+2022-11-18 16:08:08,028:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:08,318:INFO: 		 ADE on eth                       dataset:	 1.8523732423782349
+2022-11-18 16:08:08,318:INFO: Average validation o:	ADE  1.8524	FDE  2.8048
+2022-11-18 16:08:08,319:INFO: - Computing loss (validation)
+2022-11-18 16:08:08,535:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6356 (0.6356)
+2022-11-18 16:08:08,536:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6213 (0.6346)
+2022-11-18 16:08:08,833:INFO: Dataset: univ                Batch: 1/3	Loss 0.1418 (0.1418)
+2022-11-18 16:08:08,836:INFO: Dataset: univ                Batch: 2/3	Loss 0.1339 (0.1375)
+2022-11-18 16:08:08,839:INFO: Dataset: univ                Batch: 3/3	Loss 0.1431 (0.1391)
+2022-11-18 16:08:09,141:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2869 (0.2869)
+2022-11-18 16:08:09,143:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2858 (0.2866)
+2022-11-18 16:08:09,446:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2069 (0.2069)
+2022-11-18 16:08:09,449:INFO: Dataset: zara2               Batch: 2/5	Loss 0.2082 (0.2076)
+2022-11-18 16:08:09,451:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2039 (0.2063)
+2022-11-18 16:08:09,453:INFO: Dataset: zara2               Batch: 4/5	Loss 0.2087 (0.2069)
+2022-11-18 16:08:09,454:INFO: Dataset: zara2               Batch: 5/5	Loss 0.2074 (0.2070)
+2022-11-18 16:08:09,517:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_746.pth.tar
+2022-11-18 16:08:09,517:INFO: 
+===> EPOCH: 747 (P4)
+2022-11-18 16:08:09,518:INFO: - Computing loss (training)
+2022-11-18 16:08:09,704:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6467 (0.6467)
+2022-11-18 16:08:09,723:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6358 (0.6413)
+2022-11-18 16:08:09,725:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6412 (0.6412)
+2022-11-18 16:08:09,726:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6377 (0.6406)
+2022-11-18 16:08:09,972:INFO: Dataset: univ                Batch:  1/15	Loss 0.1320 (0.1320)
+2022-11-18 16:08:09,976:INFO: Dataset: univ                Batch:  2/15	Loss 0.1328 (0.1324)
+2022-11-18 16:08:10,016:INFO: Dataset: univ                Batch:  3/15	Loss 0.1298 (0.1315)
+2022-11-18 16:08:10,020:INFO: Dataset: univ                Batch:  4/15	Loss 0.1344 (0.1322)
+2022-11-18 16:08:10,022:INFO: Dataset: univ                Batch:  5/15	Loss 0.1353 (0.1328)
+2022-11-18 16:08:10,025:INFO: Dataset: univ                Batch:  6/15	Loss 0.1333 (0.1329)
+2022-11-18 16:08:10,027:INFO: Dataset: univ                Batch:  7/15	Loss 0.1356 (0.1333)
+2022-11-18 16:08:10,028:INFO: Dataset: univ                Batch:  8/15	Loss 0.1362 (0.1336)
+2022-11-18 16:08:10,030:INFO: Dataset: univ                Batch:  9/15	Loss 0.1339 (0.1336)
+2022-11-18 16:08:10,032:INFO: Dataset: univ                Batch: 10/15	Loss 0.1342 (0.1337)
+2022-11-18 16:08:10,034:INFO: Dataset: univ                Batch: 11/15	Loss 0.1354 (0.1338)
+2022-11-18 16:08:10,036:INFO: Dataset: univ                Batch: 12/15	Loss 0.1332 (0.1338)
+2022-11-18 16:08:10,038:INFO: Dataset: univ                Batch: 13/15	Loss 0.1353 (0.1339)
+2022-11-18 16:08:10,039:INFO: Dataset: univ                Batch: 14/15	Loss 0.1309 (0.1337)
+2022-11-18 16:08:10,041:INFO: Dataset: univ                Batch: 15/15	Loss 0.1284 (0.1336)
+2022-11-18 16:08:10,318:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2909 (0.2909)
+2022-11-18 16:08:10,322:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2933 (0.2921)
+2022-11-18 16:08:10,329:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2902 (0.2913)
+2022-11-18 16:08:10,333:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2882 (0.2904)
+2022-11-18 16:08:10,338:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2934 (0.2911)
+2022-11-18 16:08:10,382:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2921 (0.2913)
+2022-11-18 16:08:10,384:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2880 (0.2908)
+2022-11-18 16:08:10,385:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2779 (0.2892)
+2022-11-18 16:08:10,647:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1937 (0.1937)
+2022-11-18 16:08:10,650:INFO: Dataset: zara2               Batch:  2/18	Loss 0.2042 (0.1986)
+2022-11-18 16:08:10,696:INFO: Dataset: zara2               Batch:  3/18	Loss 0.2043 (0.2003)
+2022-11-18 16:08:10,697:INFO: Dataset: zara2               Batch:  4/18	Loss 0.2021 (0.2007)
+2022-11-18 16:08:10,699:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1917 (0.1990)
+2022-11-18 16:08:10,704:INFO: Dataset: zara2               Batch:  6/18	Loss 0.2028 (0.1996)
+2022-11-18 16:08:10,705:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2091 (0.2009)
+2022-11-18 16:08:10,706:INFO: Dataset: zara2               Batch:  8/18	Loss 0.2007 (0.2009)
+2022-11-18 16:08:10,707:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1999 (0.2007)
+2022-11-18 16:08:10,708:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1956 (0.2002)
+2022-11-18 16:08:10,709:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1943 (0.1996)
+2022-11-18 16:08:10,711:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1995 (0.1996)
+2022-11-18 16:08:10,712:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1951 (0.1993)
+2022-11-18 16:08:10,714:INFO: Dataset: zara2               Batch: 14/18	Loss 0.2013 (0.1994)
+2022-11-18 16:08:10,715:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1958 (0.1992)
+2022-11-18 16:08:10,717:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1918 (0.1987)
+2022-11-18 16:08:10,719:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1912 (0.1982)
+2022-11-18 16:08:10,721:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1950 (0.1981)
+2022-11-18 16:08:10,765:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:11,030:INFO: 		 ADE on eth                       dataset:	 1.8135446310043335
+2022-11-18 16:08:11,030:INFO: Average validation o:	ADE  1.8135	FDE  2.7128
+2022-11-18 16:08:11,031:INFO: - Computing loss (validation)
+2022-11-18 16:08:11,227:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6332 (0.6332)
+2022-11-18 16:08:11,228:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6597 (0.6353)
+2022-11-18 16:08:11,465:INFO: Dataset: univ                Batch: 1/3	Loss 0.1363 (0.1363)
+2022-11-18 16:08:11,469:INFO: Dataset: univ                Batch: 2/3	Loss 0.1334 (0.1350)
+2022-11-18 16:08:11,470:INFO: Dataset: univ                Batch: 3/3	Loss 0.1353 (0.1351)
+2022-11-18 16:08:11,696:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2864 (0.2864)
+2022-11-18 16:08:11,697:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2798 (0.2847)
+2022-11-18 16:08:11,945:INFO: Dataset: zara2               Batch: 1/5	Loss 0.2006 (0.2006)
+2022-11-18 16:08:11,951:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1976 (0.1991)
+2022-11-18 16:08:11,952:INFO: Dataset: zara2               Batch: 3/5	Loss 0.2035 (0.2006)
+2022-11-18 16:08:11,959:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1978 (0.1999)
+2022-11-18 16:08:11,959:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1993 (0.1998)
+2022-11-18 16:08:12,020:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_747.pth.tar
+2022-11-18 16:08:12,020:INFO: 
+===> EPOCH: 748 (P4)
+2022-11-18 16:08:12,021:INFO: - Computing loss (training)
+2022-11-18 16:08:12,222:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6394 (0.6394)
+2022-11-18 16:08:12,225:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6347 (0.6371)
+2022-11-18 16:08:12,228:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6543 (0.6427)
+2022-11-18 16:08:12,229:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6357 (0.6415)
+2022-11-18 16:08:12,494:INFO: Dataset: univ                Batch:  1/15	Loss 0.1308 (0.1308)
+2022-11-18 16:08:12,497:INFO: Dataset: univ                Batch:  2/15	Loss 0.1262 (0.1284)
+2022-11-18 16:08:12,500:INFO: Dataset: univ                Batch:  3/15	Loss 0.1293 (0.1287)
+2022-11-18 16:08:12,537:INFO: Dataset: univ                Batch:  4/15	Loss 0.1319 (0.1294)
+2022-11-18 16:08:12,538:INFO: Dataset: univ                Batch:  5/15	Loss 0.1322 (0.1300)
+2022-11-18 16:08:12,543:INFO: Dataset: univ                Batch:  6/15	Loss 0.1315 (0.1302)
+2022-11-18 16:08:12,545:INFO: Dataset: univ                Batch:  7/15	Loss 0.1305 (0.1303)
+2022-11-18 16:08:12,547:INFO: Dataset: univ                Batch:  8/15	Loss 0.1290 (0.1301)
+2022-11-18 16:08:12,548:INFO: Dataset: univ                Batch:  9/15	Loss 0.1287 (0.1300)
+2022-11-18 16:08:12,549:INFO: Dataset: univ                Batch: 10/15	Loss 0.1323 (0.1302)
+2022-11-18 16:08:12,552:INFO: Dataset: univ                Batch: 11/15	Loss 0.1306 (0.1302)
+2022-11-18 16:08:12,554:INFO: Dataset: univ                Batch: 12/15	Loss 0.1302 (0.1302)
+2022-11-18 16:08:12,556:INFO: Dataset: univ                Batch: 13/15	Loss 0.1296 (0.1302)
+2022-11-18 16:08:12,557:INFO: Dataset: univ                Batch: 14/15	Loss 0.1271 (0.1299)
+2022-11-18 16:08:12,559:INFO: Dataset: univ                Batch: 15/15	Loss 0.1233 (0.1298)
+2022-11-18 16:08:12,819:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2828 (0.2828)
+2022-11-18 16:08:12,820:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2871 (0.2848)
+2022-11-18 16:08:12,822:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2881 (0.2860)
+2022-11-18 16:08:12,828:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2822 (0.2852)
+2022-11-18 16:08:12,829:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2913 (0.2864)
+2022-11-18 16:08:12,847:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2874 (0.2865)
+2022-11-18 16:08:12,849:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2847 (0.2863)
+2022-11-18 16:08:12,850:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2903 (0.2867)
+2022-11-18 16:08:13,096:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1973 (0.1973)
+2022-11-18 16:08:13,099:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1883 (0.1926)
+2022-11-18 16:08:13,101:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1996 (0.1946)
+2022-11-18 16:08:13,106:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1929 (0.1941)
+2022-11-18 16:08:13,119:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1892 (0.1932)
+2022-11-18 16:08:13,123:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1949 (0.1935)
+2022-11-18 16:08:13,124:INFO: Dataset: zara2               Batch:  7/18	Loss 0.2015 (0.1946)
+2022-11-18 16:08:13,125:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1920 (0.1943)
+2022-11-18 16:08:13,127:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1944 (0.1943)
+2022-11-18 16:08:13,128:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1910 (0.1939)
+2022-11-18 16:08:13,130:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1857 (0.1932)
+2022-11-18 16:08:13,132:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1896 (0.1929)
+2022-11-18 16:08:13,133:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1917 (0.1928)
+2022-11-18 16:08:13,134:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1990 (0.1933)
+2022-11-18 16:08:13,135:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1895 (0.1930)
+2022-11-18 16:08:13,137:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1834 (0.1924)
+2022-11-18 16:08:13,138:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1848 (0.1919)
+2022-11-18 16:08:13,140:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1848 (0.1916)
+2022-11-18 16:08:13,184:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:13,460:INFO: 		 ADE on eth                       dataset:	 1.817244052886963
+2022-11-18 16:08:13,460:INFO: Average validation o:	ADE  1.8172	FDE  2.7715
+2022-11-18 16:08:13,461:INFO: - Computing loss (validation)
+2022-11-18 16:08:13,646:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6386 (0.6386)
+2022-11-18 16:08:13,647:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5892 (0.6357)
+2022-11-18 16:08:13,888:INFO: Dataset: univ                Batch: 1/3	Loss 0.1322 (0.1322)
+2022-11-18 16:08:13,889:INFO: Dataset: univ                Batch: 2/3	Loss 0.1302 (0.1311)
+2022-11-18 16:08:13,898:INFO: Dataset: univ                Batch: 3/3	Loss 0.1324 (0.1315)
+2022-11-18 16:08:14,130:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2852 (0.2852)
+2022-11-18 16:08:14,134:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2763 (0.2830)
+2022-11-18 16:08:14,357:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1944 (0.1944)
+2022-11-18 16:08:14,358:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1917 (0.1930)
+2022-11-18 16:08:14,358:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1952 (0.1938)
+2022-11-18 16:08:14,359:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1930 (0.1936)
+2022-11-18 16:08:14,361:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1924 (0.1933)
+2022-11-18 16:08:14,417:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_748.pth.tar
+2022-11-18 16:08:14,417:INFO: 
+===> EPOCH: 749 (P4)
+2022-11-18 16:08:14,418:INFO: - Computing loss (training)
+2022-11-18 16:08:14,603:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6411 (0.6411)
+2022-11-18 16:08:14,606:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6406 (0.6408)
+2022-11-18 16:08:14,608:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6602 (0.6470)
+2022-11-18 16:08:14,610:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6208 (0.6421)
+2022-11-18 16:08:14,861:INFO: Dataset: univ                Batch:  1/15	Loss 0.1253 (0.1253)
+2022-11-18 16:08:14,863:INFO: Dataset: univ                Batch:  2/15	Loss 0.1263 (0.1258)
+2022-11-18 16:08:14,867:INFO: Dataset: univ                Batch:  3/15	Loss 0.1258 (0.1258)
+2022-11-18 16:08:14,892:INFO: Dataset: univ                Batch:  4/15	Loss 0.1288 (0.1265)
+2022-11-18 16:08:14,894:INFO: Dataset: univ                Batch:  5/15	Loss 0.1256 (0.1263)
+2022-11-18 16:08:14,899:INFO: Dataset: univ                Batch:  6/15	Loss 0.1260 (0.1263)
+2022-11-18 16:08:14,900:INFO: Dataset: univ                Batch:  7/15	Loss 0.1291 (0.1266)
+2022-11-18 16:08:14,902:INFO: Dataset: univ                Batch:  8/15	Loss 0.1249 (0.1264)
+2022-11-18 16:08:14,903:INFO: Dataset: univ                Batch:  9/15	Loss 0.1273 (0.1265)
+2022-11-18 16:08:14,904:INFO: Dataset: univ                Batch: 10/15	Loss 0.1268 (0.1265)
+2022-11-18 16:08:14,907:INFO: Dataset: univ                Batch: 11/15	Loss 0.1261 (0.1265)
+2022-11-18 16:08:14,908:INFO: Dataset: univ                Batch: 12/15	Loss 0.1271 (0.1266)
+2022-11-18 16:08:14,909:INFO: Dataset: univ                Batch: 13/15	Loss 0.1269 (0.1266)
+2022-11-18 16:08:14,911:INFO: Dataset: univ                Batch: 14/15	Loss 0.1259 (0.1265)
+2022-11-18 16:08:14,912:INFO: Dataset: univ                Batch: 15/15	Loss 0.1238 (0.1265)
+2022-11-18 16:08:15,159:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2824 (0.2824)
+2022-11-18 16:08:15,164:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2891 (0.2857)
+2022-11-18 16:08:15,168:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2780 (0.2828)
+2022-11-18 16:08:15,170:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2862 (0.2836)
+2022-11-18 16:08:15,173:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2846 (0.2838)
+2022-11-18 16:08:15,217:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2816 (0.2835)
+2022-11-18 16:08:15,222:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2875 (0.2841)
+2022-11-18 16:08:15,226:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2860 (0.2843)
+2022-11-18 16:08:15,509:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1842 (0.1842)
+2022-11-18 16:08:15,512:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1925 (0.1885)
+2022-11-18 16:08:15,521:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1828 (0.1867)
+2022-11-18 16:08:15,523:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1909 (0.1877)
+2022-11-18 16:08:15,560:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1904 (0.1883)
+2022-11-18 16:08:15,569:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1835 (0.1875)
+2022-11-18 16:08:15,570:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1846 (0.1870)
+2022-11-18 16:08:15,571:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1870 (0.1870)
+2022-11-18 16:08:15,572:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1834 (0.1866)
+2022-11-18 16:08:15,573:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1882 (0.1868)
+2022-11-18 16:08:15,574:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1826 (0.1864)
+2022-11-18 16:08:15,576:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1813 (0.1859)
+2022-11-18 16:08:15,577:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1880 (0.1861)
+2022-11-18 16:08:15,578:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1799 (0.1857)
+2022-11-18 16:08:15,579:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1854 (0.1856)
+2022-11-18 16:08:15,581:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1877 (0.1858)
+2022-11-18 16:08:15,582:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1870 (0.1858)
+2022-11-18 16:08:15,584:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1833 (0.1857)
+2022-11-18 16:08:15,628:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:15,904:INFO: 		 ADE on eth                       dataset:	 1.7725257873535156
+2022-11-18 16:08:15,904:INFO: Average validation o:	ADE  1.7725	FDE  2.6566
+2022-11-18 16:08:15,905:INFO: - Computing loss (validation)
+2022-11-18 16:08:16,100:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6317 (0.6317)
+2022-11-18 16:08:16,101:INFO: Dataset: hotel               Batch: 2/2	Loss 0.7021 (0.6360)
+2022-11-18 16:08:16,342:INFO: Dataset: univ                Batch: 1/3	Loss 0.1244 (0.1244)
+2022-11-18 16:08:16,343:INFO: Dataset: univ                Batch: 2/3	Loss 0.1301 (0.1270)
+2022-11-18 16:08:16,345:INFO: Dataset: univ                Batch: 3/3	Loss 0.1312 (0.1282)
+2022-11-18 16:08:16,573:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2795 (0.2795)
+2022-11-18 16:08:16,575:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2875 (0.2813)
+2022-11-18 16:08:16,807:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1871 (0.1871)
+2022-11-18 16:08:16,809:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1858 (0.1865)
+2022-11-18 16:08:16,812:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1868 (0.1866)
+2022-11-18 16:08:16,812:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1934 (0.1882)
+2022-11-18 16:08:16,813:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1844 (0.1875)
+2022-11-18 16:08:16,872:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_749.pth.tar
+2022-11-18 16:08:16,872:INFO: 
+===> EPOCH: 750 (P4)
+2022-11-18 16:08:16,873:INFO: - Computing loss (training)
+2022-11-18 16:08:17,080:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6395 (0.6395)
+2022-11-18 16:08:17,084:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6358 (0.6376)
+2022-11-18 16:08:17,088:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6426 (0.6392)
+2022-11-18 16:08:17,089:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6591 (0.6426)
+2022-11-18 16:08:17,347:INFO: Dataset: univ                Batch:  1/15	Loss 0.1236 (0.1236)
+2022-11-18 16:08:17,350:INFO: Dataset: univ                Batch:  2/15	Loss 0.1240 (0.1238)
+2022-11-18 16:08:17,353:INFO: Dataset: univ                Batch:  3/15	Loss 0.1252 (0.1242)
+2022-11-18 16:08:17,357:INFO: Dataset: univ                Batch:  4/15	Loss 0.1241 (0.1242)
+2022-11-18 16:08:17,397:INFO: Dataset: univ                Batch:  5/15	Loss 0.1246 (0.1243)
+2022-11-18 16:08:17,403:INFO: Dataset: univ                Batch:  6/15	Loss 0.1279 (0.1248)
+2022-11-18 16:08:17,405:INFO: Dataset: univ                Batch:  7/15	Loss 0.1237 (0.1246)
+2022-11-18 16:08:17,406:INFO: Dataset: univ                Batch:  8/15	Loss 0.1246 (0.1246)
+2022-11-18 16:08:17,407:INFO: Dataset: univ                Batch:  9/15	Loss 0.1225 (0.1244)
+2022-11-18 16:08:17,408:INFO: Dataset: univ                Batch: 10/15	Loss 0.1225 (0.1242)
+2022-11-18 16:08:17,411:INFO: Dataset: univ                Batch: 11/15	Loss 0.1203 (0.1238)
+2022-11-18 16:08:17,412:INFO: Dataset: univ                Batch: 12/15	Loss 0.1210 (0.1235)
+2022-11-18 16:08:17,413:INFO: Dataset: univ                Batch: 13/15	Loss 0.1226 (0.1235)
+2022-11-18 16:08:17,415:INFO: Dataset: univ                Batch: 14/15	Loss 0.1242 (0.1235)
+2022-11-18 16:08:17,416:INFO: Dataset: univ                Batch: 15/15	Loss 0.1234 (0.1235)
+2022-11-18 16:08:17,664:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2785 (0.2785)
+2022-11-18 16:08:17,667:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2845 (0.2814)
+2022-11-18 16:08:17,668:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2852 (0.2826)
+2022-11-18 16:08:17,670:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2794 (0.2818)
+2022-11-18 16:08:17,671:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2901 (0.2833)
+2022-11-18 16:08:17,726:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2785 (0.2825)
+2022-11-18 16:08:17,731:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2828 (0.2825)
+2022-11-18 16:08:17,735:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2793 (0.2821)
+2022-11-18 16:08:18,011:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1785 (0.1785)
+2022-11-18 16:08:18,012:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1778 (0.1781)
+2022-11-18 16:08:18,046:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1866 (0.1809)
+2022-11-18 16:08:18,047:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1810 (0.1809)
+2022-11-18 16:08:18,049:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1783 (0.1804)
+2022-11-18 16:08:18,055:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1880 (0.1819)
+2022-11-18 16:08:18,056:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1744 (0.1808)
+2022-11-18 16:08:18,058:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1816 (0.1809)
+2022-11-18 16:08:18,059:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1837 (0.1812)
+2022-11-18 16:08:18,060:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1746 (0.1805)
+2022-11-18 16:08:18,061:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1830 (0.1807)
+2022-11-18 16:08:18,063:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1819 (0.1808)
+2022-11-18 16:08:18,065:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1809 (0.1808)
+2022-11-18 16:08:18,066:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1798 (0.1807)
+2022-11-18 16:08:18,069:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1800 (0.1807)
+2022-11-18 16:08:18,071:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1795 (0.1806)
+2022-11-18 16:08:18,073:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1732 (0.1802)
+2022-11-18 16:08:18,075:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1845 (0.1804)
+2022-11-18 16:08:18,124:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:18,415:INFO: 		 ADE on eth                       dataset:	 1.8124024868011475
+2022-11-18 16:08:18,416:INFO: Average validation o:	ADE  1.8124	FDE  2.7135
+2022-11-18 16:08:18,417:INFO: - Computing loss (validation)
+2022-11-18 16:08:18,623:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6380 (0.6380)
+2022-11-18 16:08:18,624:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6100 (0.6361)
+2022-11-18 16:08:18,867:INFO: Dataset: univ                Batch: 1/3	Loss 0.1251 (0.1251)
+2022-11-18 16:08:18,870:INFO: Dataset: univ                Batch: 2/3	Loss 0.1243 (0.1247)
+2022-11-18 16:08:18,872:INFO: Dataset: univ                Batch: 3/3	Loss 0.1270 (0.1253)
+2022-11-18 16:08:19,137:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2790 (0.2790)
+2022-11-18 16:08:19,138:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2817 (0.2798)
+2022-11-18 16:08:19,394:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1787 (0.1787)
+2022-11-18 16:08:19,396:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1833 (0.1811)
+2022-11-18 16:08:19,400:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1790 (0.1804)
+2022-11-18 16:08:19,402:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1858 (0.1818)
+2022-11-18 16:08:19,403:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1840 (0.1822)
+2022-11-18 16:08:19,469:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_750.pth.tar
+2022-11-18 16:08:19,469:INFO: 
+===> EPOCH: 751 (P4)
+2022-11-18 16:08:19,470:INFO: - Computing loss (training)
+2022-11-18 16:08:19,689:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6507 (0.6507)
+2022-11-18 16:08:19,692:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6496 (0.6501)
+2022-11-18 16:08:19,693:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6411 (0.6470)
+2022-11-18 16:08:19,695:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6210 (0.6425)
+2022-11-18 16:08:19,950:INFO: Dataset: univ                Batch:  1/15	Loss 0.1194 (0.1194)
+2022-11-18 16:08:19,954:INFO: Dataset: univ                Batch:  2/15	Loss 0.1209 (0.1201)
+2022-11-18 16:08:19,962:INFO: Dataset: univ                Batch:  3/15	Loss 0.1196 (0.1199)
+2022-11-18 16:08:19,964:INFO: Dataset: univ                Batch:  4/15	Loss 0.1202 (0.1200)
+2022-11-18 16:08:20,018:INFO: Dataset: univ                Batch:  5/15	Loss 0.1202 (0.1200)
+2022-11-18 16:08:20,023:INFO: Dataset: univ                Batch:  6/15	Loss 0.1218 (0.1203)
+2022-11-18 16:08:20,025:INFO: Dataset: univ                Batch:  7/15	Loss 0.1200 (0.1203)
+2022-11-18 16:08:20,026:INFO: Dataset: univ                Batch:  8/15	Loss 0.1222 (0.1205)
+2022-11-18 16:08:20,027:INFO: Dataset: univ                Batch:  9/15	Loss 0.1211 (0.1206)
+2022-11-18 16:08:20,029:INFO: Dataset: univ                Batch: 10/15	Loss 0.1205 (0.1205)
+2022-11-18 16:08:20,031:INFO: Dataset: univ                Batch: 11/15	Loss 0.1228 (0.1208)
+2022-11-18 16:08:20,033:INFO: Dataset: univ                Batch: 12/15	Loss 0.1223 (0.1209)
+2022-11-18 16:08:20,034:INFO: Dataset: univ                Batch: 13/15	Loss 0.1212 (0.1209)
+2022-11-18 16:08:20,035:INFO: Dataset: univ                Batch: 14/15	Loss 0.1215 (0.1209)
+2022-11-18 16:08:20,037:INFO: Dataset: univ                Batch: 15/15	Loss 0.1180 (0.1209)
+2022-11-18 16:08:20,273:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2836 (0.2836)
+2022-11-18 16:08:20,277:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2798 (0.2817)
+2022-11-18 16:08:20,284:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2858 (0.2831)
+2022-11-18 16:08:20,286:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2809 (0.2825)
+2022-11-18 16:08:20,287:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2810 (0.2822)
+2022-11-18 16:08:20,338:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2793 (0.2818)
+2022-11-18 16:08:20,343:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2731 (0.2806)
+2022-11-18 16:08:20,347:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2777 (0.2802)
+2022-11-18 16:08:20,621:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1739 (0.1739)
+2022-11-18 16:08:20,622:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1779 (0.1759)
+2022-11-18 16:08:20,659:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1775 (0.1765)
+2022-11-18 16:08:20,661:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1802 (0.1774)
+2022-11-18 16:08:20,662:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1725 (0.1764)
+2022-11-18 16:08:20,675:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1770 (0.1765)
+2022-11-18 16:08:20,676:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1821 (0.1774)
+2022-11-18 16:08:20,677:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1809 (0.1778)
+2022-11-18 16:08:20,679:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1728 (0.1772)
+2022-11-18 16:08:20,680:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1760 (0.1771)
+2022-11-18 16:08:20,681:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1747 (0.1769)
+2022-11-18 16:08:20,683:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1746 (0.1767)
+2022-11-18 16:08:20,684:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1811 (0.1771)
+2022-11-18 16:08:20,686:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1711 (0.1767)
+2022-11-18 16:08:20,687:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1679 (0.1760)
+2022-11-18 16:08:20,688:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1769 (0.1761)
+2022-11-18 16:08:20,689:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1724 (0.1759)
+2022-11-18 16:08:20,691:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1708 (0.1757)
+2022-11-18 16:08:20,747:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:21,026:INFO: 		 ADE on eth                       dataset:	 1.7713861465454102
+2022-11-18 16:08:21,026:INFO: Average validation o:	ADE  1.7714	FDE  2.6537
+2022-11-18 16:08:21,027:INFO: - Computing loss (validation)
+2022-11-18 16:08:21,211:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6338 (0.6338)
+2022-11-18 16:08:21,213:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6617 (0.6362)
+2022-11-18 16:08:21,452:INFO: Dataset: univ                Batch: 1/3	Loss 0.1237 (0.1237)
+2022-11-18 16:08:21,453:INFO: Dataset: univ                Batch: 2/3	Loss 0.1247 (0.1242)
+2022-11-18 16:08:21,454:INFO: Dataset: univ                Batch: 3/3	Loss 0.1201 (0.1227)
+2022-11-18 16:08:21,690:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2756 (0.2756)
+2022-11-18 16:08:21,691:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2866 (0.2783)
+2022-11-18 16:08:21,919:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1782 (0.1782)
+2022-11-18 16:08:21,922:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1766 (0.1774)
+2022-11-18 16:08:21,924:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1767 (0.1772)
+2022-11-18 16:08:21,928:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1776 (0.1773)
+2022-11-18 16:08:21,929:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1780 (0.1774)
+2022-11-18 16:08:21,999:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_751.pth.tar
+2022-11-18 16:08:21,999:INFO: 
+===> EPOCH: 752 (P4)
+2022-11-18 16:08:21,999:INFO: - Computing loss (training)
+2022-11-18 16:08:22,196:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6411 (0.6411)
+2022-11-18 16:08:22,200:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6267 (0.6339)
+2022-11-18 16:08:22,209:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6651 (0.6442)
+2022-11-18 16:08:22,210:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6356 (0.6427)
+2022-11-18 16:08:22,465:INFO: Dataset: univ                Batch:  1/15	Loss 0.1178 (0.1178)
+2022-11-18 16:08:22,467:INFO: Dataset: univ                Batch:  2/15	Loss 0.1183 (0.1180)
+2022-11-18 16:08:22,470:INFO: Dataset: univ                Batch:  3/15	Loss 0.1185 (0.1182)
+2022-11-18 16:08:22,472:INFO: Dataset: univ                Batch:  4/15	Loss 0.1190 (0.1184)
+2022-11-18 16:08:22,517:INFO: Dataset: univ                Batch:  5/15	Loss 0.1186 (0.1184)
+2022-11-18 16:08:22,524:INFO: Dataset: univ                Batch:  6/15	Loss 0.1193 (0.1186)
+2022-11-18 16:08:22,525:INFO: Dataset: univ                Batch:  7/15	Loss 0.1213 (0.1189)
+2022-11-18 16:08:22,526:INFO: Dataset: univ                Batch:  8/15	Loss 0.1211 (0.1192)
+2022-11-18 16:08:22,528:INFO: Dataset: univ                Batch:  9/15	Loss 0.1192 (0.1192)
+2022-11-18 16:08:22,529:INFO: Dataset: univ                Batch: 10/15	Loss 0.1183 (0.1191)
+2022-11-18 16:08:22,530:INFO: Dataset: univ                Batch: 11/15	Loss 0.1173 (0.1189)
+2022-11-18 16:08:22,532:INFO: Dataset: univ                Batch: 12/15	Loss 0.1182 (0.1188)
+2022-11-18 16:08:22,534:INFO: Dataset: univ                Batch: 13/15	Loss 0.1182 (0.1188)
+2022-11-18 16:08:22,535:INFO: Dataset: univ                Batch: 14/15	Loss 0.1154 (0.1186)
+2022-11-18 16:08:22,536:INFO: Dataset: univ                Batch: 15/15	Loss 0.1152 (0.1185)
+2022-11-18 16:08:22,782:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2815 (0.2815)
+2022-11-18 16:08:22,784:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2819 (0.2817)
+2022-11-18 16:08:22,786:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2765 (0.2797)
+2022-11-18 16:08:22,787:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2764 (0.2789)
+2022-11-18 16:08:22,790:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2780 (0.2787)
+2022-11-18 16:08:22,834:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2786 (0.2787)
+2022-11-18 16:08:22,838:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2785 (0.2787)
+2022-11-18 16:08:22,842:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2769 (0.2785)
+2022-11-18 16:08:23,119:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1734 (0.1734)
+2022-11-18 16:08:23,124:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1793 (0.1763)
+2022-11-18 16:08:23,125:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1721 (0.1750)
+2022-11-18 16:08:23,127:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1605 (0.1709)
+2022-11-18 16:08:23,181:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1755 (0.1718)
+2022-11-18 16:08:23,189:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1759 (0.1725)
+2022-11-18 16:08:23,190:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1812 (0.1737)
+2022-11-18 16:08:23,191:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1681 (0.1730)
+2022-11-18 16:08:23,193:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1690 (0.1726)
+2022-11-18 16:08:23,194:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1747 (0.1728)
+2022-11-18 16:08:23,196:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1654 (0.1721)
+2022-11-18 16:08:23,197:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1688 (0.1719)
+2022-11-18 16:08:23,199:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1737 (0.1720)
+2022-11-18 16:08:23,200:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1666 (0.1716)
+2022-11-18 16:08:23,201:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1694 (0.1715)
+2022-11-18 16:08:23,202:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1734 (0.1716)
+2022-11-18 16:08:23,204:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1700 (0.1715)
+2022-11-18 16:08:23,206:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1682 (0.1713)
+2022-11-18 16:08:23,261:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:23,563:INFO: 		 ADE on eth                       dataset:	 1.8136541843414307
+2022-11-18 16:08:23,563:INFO: Average validation o:	ADE  1.8137	FDE  2.7395
+2022-11-18 16:08:23,565:INFO: - Computing loss (validation)
+2022-11-18 16:08:23,742:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6343 (0.6343)
+2022-11-18 16:08:23,744:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6527 (0.6361)
+2022-11-18 16:08:23,991:INFO: Dataset: univ                Batch: 1/3	Loss 0.1233 (0.1233)
+2022-11-18 16:08:23,993:INFO: Dataset: univ                Batch: 2/3	Loss 0.1197 (0.1215)
+2022-11-18 16:08:23,995:INFO: Dataset: univ                Batch: 3/3	Loss 0.1179 (0.1203)
+2022-11-18 16:08:24,245:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2760 (0.2760)
+2022-11-18 16:08:24,246:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2796 (0.2769)
+2022-11-18 16:08:24,503:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1730 (0.1730)
+2022-11-18 16:08:24,506:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1761 (0.1746)
+2022-11-18 16:08:24,507:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1712 (0.1735)
+2022-11-18 16:08:24,508:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1741 (0.1737)
+2022-11-18 16:08:24,509:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1705 (0.1731)
+2022-11-18 16:08:24,580:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_752.pth.tar
+2022-11-18 16:08:24,581:INFO: 
+===> EPOCH: 753 (P4)
+2022-11-18 16:08:24,581:INFO: - Computing loss (training)
+2022-11-18 16:08:24,777:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6411 (0.6411)
+2022-11-18 16:08:24,781:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6401 (0.6406)
+2022-11-18 16:08:24,784:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6437 (0.6416)
+2022-11-18 16:08:24,785:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6470 (0.6425)
+2022-11-18 16:08:25,074:INFO: Dataset: univ                Batch:  1/15	Loss 0.1169 (0.1169)
+2022-11-18 16:08:25,076:INFO: Dataset: univ                Batch:  2/15	Loss 0.1156 (0.1162)
+2022-11-18 16:08:25,079:INFO: Dataset: univ                Batch:  3/15	Loss 0.1174 (0.1166)
+2022-11-18 16:08:25,080:INFO: Dataset: univ                Batch:  4/15	Loss 0.1156 (0.1163)
+2022-11-18 16:08:25,082:INFO: Dataset: univ                Batch:  5/15	Loss 0.1167 (0.1164)
+2022-11-18 16:08:25,086:INFO: Dataset: univ                Batch:  6/15	Loss 0.1179 (0.1166)
+2022-11-18 16:08:25,088:INFO: Dataset: univ                Batch:  7/15	Loss 0.1150 (0.1164)
+2022-11-18 16:08:25,089:INFO: Dataset: univ                Batch:  8/15	Loss 0.1162 (0.1164)
+2022-11-18 16:08:25,091:INFO: Dataset: univ                Batch:  9/15	Loss 0.1172 (0.1164)
+2022-11-18 16:08:25,092:INFO: Dataset: univ                Batch: 10/15	Loss 0.1161 (0.1164)
+2022-11-18 16:08:25,095:INFO: Dataset: univ                Batch: 11/15	Loss 0.1167 (0.1164)
+2022-11-18 16:08:25,097:INFO: Dataset: univ                Batch: 12/15	Loss 0.1157 (0.1164)
+2022-11-18 16:08:25,099:INFO: Dataset: univ                Batch: 13/15	Loss 0.1157 (0.1163)
+2022-11-18 16:08:25,101:INFO: Dataset: univ                Batch: 14/15	Loss 0.1178 (0.1164)
+2022-11-18 16:08:25,103:INFO: Dataset: univ                Batch: 15/15	Loss 0.1160 (0.1164)
+2022-11-18 16:08:25,364:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2757 (0.2757)
+2022-11-18 16:08:25,368:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2760 (0.2758)
+2022-11-18 16:08:25,370:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2809 (0.2775)
+2022-11-18 16:08:25,375:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2741 (0.2766)
+2022-11-18 16:08:25,377:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2811 (0.2774)
+2022-11-18 16:08:25,420:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2797 (0.2778)
+2022-11-18 16:08:25,423:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2736 (0.2772)
+2022-11-18 16:08:25,427:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2744 (0.2769)
+2022-11-18 16:08:25,702:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1726 (0.1726)
+2022-11-18 16:08:25,703:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1617 (0.1670)
+2022-11-18 16:08:25,708:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1699 (0.1680)
+2022-11-18 16:08:25,709:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1646 (0.1672)
+2022-11-18 16:08:25,731:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1753 (0.1688)
+2022-11-18 16:08:25,736:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1654 (0.1683)
+2022-11-18 16:08:25,738:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1739 (0.1690)
+2022-11-18 16:08:25,739:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1641 (0.1684)
+2022-11-18 16:08:25,740:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1696 (0.1686)
+2022-11-18 16:08:25,741:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1694 (0.1686)
+2022-11-18 16:08:25,742:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1698 (0.1687)
+2022-11-18 16:08:25,744:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1679 (0.1687)
+2022-11-18 16:08:25,745:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1638 (0.1683)
+2022-11-18 16:08:25,746:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1676 (0.1682)
+2022-11-18 16:08:25,747:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1651 (0.1680)
+2022-11-18 16:08:25,748:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1666 (0.1679)
+2022-11-18 16:08:25,750:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1637 (0.1677)
+2022-11-18 16:08:25,751:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1627 (0.1674)
+2022-11-18 16:08:25,795:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:26,063:INFO: 		 ADE on eth                       dataset:	 1.8118507862091064
+2022-11-18 16:08:26,063:INFO: Average validation o:	ADE  1.8119	FDE  2.7371
+2022-11-18 16:08:26,064:INFO: - Computing loss (validation)
+2022-11-18 16:08:26,251:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6379 (0.6379)
+2022-11-18 16:08:26,252:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6047 (0.6359)
+2022-11-18 16:08:26,500:INFO: Dataset: univ                Batch: 1/3	Loss 0.1195 (0.1195)
+2022-11-18 16:08:26,502:INFO: Dataset: univ                Batch: 2/3	Loss 0.1161 (0.1177)
+2022-11-18 16:08:26,503:INFO: Dataset: univ                Batch: 3/3	Loss 0.1195 (0.1182)
+2022-11-18 16:08:26,726:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2756 (0.2756)
+2022-11-18 16:08:26,727:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2757 (0.2756)
+2022-11-18 16:08:26,970:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1704 (0.1704)
+2022-11-18 16:08:26,973:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1691 (0.1697)
+2022-11-18 16:08:26,973:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1679 (0.1692)
+2022-11-18 16:08:26,975:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1693 (0.1692)
+2022-11-18 16:08:26,976:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1696 (0.1693)
+2022-11-18 16:08:27,032:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_753.pth.tar
+2022-11-18 16:08:27,033:INFO: 
+===> EPOCH: 754 (P4)
+2022-11-18 16:08:27,033:INFO: - Computing loss (training)
+2022-11-18 16:08:27,229:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6570 (0.6570)
+2022-11-18 16:08:27,232:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6277 (0.6427)
+2022-11-18 16:08:27,234:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6496 (0.6451)
+2022-11-18 16:08:27,237:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6272 (0.6421)
+2022-11-18 16:08:27,483:INFO: Dataset: univ                Batch:  1/15	Loss 0.1147 (0.1147)
+2022-11-18 16:08:27,488:INFO: Dataset: univ                Batch:  2/15	Loss 0.1147 (0.1147)
+2022-11-18 16:08:27,534:INFO: Dataset: univ                Batch:  3/15	Loss 0.1146 (0.1147)
+2022-11-18 16:08:27,535:INFO: Dataset: univ                Batch:  4/15	Loss 0.1160 (0.1150)
+2022-11-18 16:08:27,536:INFO: Dataset: univ                Batch:  5/15	Loss 0.1150 (0.1150)
+2022-11-18 16:08:27,543:INFO: Dataset: univ                Batch:  6/15	Loss 0.1150 (0.1150)
+2022-11-18 16:08:27,544:INFO: Dataset: univ                Batch:  7/15	Loss 0.1148 (0.1149)
+2022-11-18 16:08:27,545:INFO: Dataset: univ                Batch:  8/15	Loss 0.1152 (0.1150)
+2022-11-18 16:08:27,547:INFO: Dataset: univ                Batch:  9/15	Loss 0.1142 (0.1149)
+2022-11-18 16:08:27,548:INFO: Dataset: univ                Batch: 10/15	Loss 0.1138 (0.1148)
+2022-11-18 16:08:27,549:INFO: Dataset: univ                Batch: 11/15	Loss 0.1143 (0.1147)
+2022-11-18 16:08:27,551:INFO: Dataset: univ                Batch: 12/15	Loss 0.1139 (0.1147)
+2022-11-18 16:08:27,553:INFO: Dataset: univ                Batch: 13/15	Loss 0.1158 (0.1147)
+2022-11-18 16:08:27,554:INFO: Dataset: univ                Batch: 14/15	Loss 0.1125 (0.1146)
+2022-11-18 16:08:27,555:INFO: Dataset: univ                Batch: 15/15	Loss 0.1134 (0.1146)
+2022-11-18 16:08:27,800:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2765 (0.2765)
+2022-11-18 16:08:27,801:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2744 (0.2754)
+2022-11-18 16:08:27,803:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2759 (0.2756)
+2022-11-18 16:08:27,807:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2774 (0.2760)
+2022-11-18 16:08:27,809:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2705 (0.2748)
+2022-11-18 16:08:27,866:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2764 (0.2751)
+2022-11-18 16:08:27,870:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2747 (0.2750)
+2022-11-18 16:08:27,874:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2800 (0.2755)
+2022-11-18 16:08:28,176:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1662 (0.1662)
+2022-11-18 16:08:28,178:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1631 (0.1647)
+2022-11-18 16:08:28,180:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1652 (0.1649)
+2022-11-18 16:08:28,184:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1676 (0.1656)
+2022-11-18 16:08:28,231:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1650 (0.1654)
+2022-11-18 16:08:28,236:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1650 (0.1654)
+2022-11-18 16:08:28,237:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1654 (0.1654)
+2022-11-18 16:08:28,238:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1654 (0.1654)
+2022-11-18 16:08:28,239:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1648 (0.1653)
+2022-11-18 16:08:28,240:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1689 (0.1657)
+2022-11-18 16:08:28,243:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1642 (0.1655)
+2022-11-18 16:08:28,244:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1660 (0.1656)
+2022-11-18 16:08:28,245:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1584 (0.1650)
+2022-11-18 16:08:28,246:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1633 (0.1649)
+2022-11-18 16:08:28,247:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1631 (0.1647)
+2022-11-18 16:08:28,248:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1601 (0.1645)
+2022-11-18 16:08:28,250:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1557 (0.1640)
+2022-11-18 16:08:28,252:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1627 (0.1639)
+2022-11-18 16:08:28,298:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:28,573:INFO: 		 ADE on eth                       dataset:	 1.8107339143753052
+2022-11-18 16:08:28,573:INFO: Average validation o:	ADE  1.8107	FDE  2.7454
+2022-11-18 16:08:28,574:INFO: - Computing loss (validation)
+2022-11-18 16:08:28,765:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6370 (0.6370)
+2022-11-18 16:08:28,765:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6164 (0.6357)
+2022-11-18 16:08:28,998:INFO: Dataset: univ                Batch: 1/3	Loss 0.1160 (0.1160)
+2022-11-18 16:08:29,003:INFO: Dataset: univ                Batch: 2/3	Loss 0.1171 (0.1165)
+2022-11-18 16:08:29,003:INFO: Dataset: univ                Batch: 3/3	Loss 0.1158 (0.1163)
+2022-11-18 16:08:29,236:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2751 (0.2751)
+2022-11-18 16:08:29,237:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2723 (0.2744)
+2022-11-18 16:08:29,487:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1702 (0.1702)
+2022-11-18 16:08:29,488:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1643 (0.1673)
+2022-11-18 16:08:29,500:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1668 (0.1671)
+2022-11-18 16:08:29,501:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1623 (0.1660)
+2022-11-18 16:08:29,511:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1648 (0.1658)
+2022-11-18 16:08:29,569:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_754.pth.tar
+2022-11-18 16:08:29,569:INFO: 
+===> EPOCH: 755 (P4)
+2022-11-18 16:08:29,570:INFO: - Computing loss (training)
+2022-11-18 16:08:29,760:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6397 (0.6397)
+2022-11-18 16:08:29,773:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6407 (0.6402)
+2022-11-18 16:08:29,774:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6347 (0.6384)
+2022-11-18 16:08:29,776:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6567 (0.6416)
+2022-11-18 16:08:30,067:INFO: Dataset: univ                Batch:  1/15	Loss 0.1121 (0.1121)
+2022-11-18 16:08:30,069:INFO: Dataset: univ                Batch:  2/15	Loss 0.1128 (0.1124)
+2022-11-18 16:08:30,073:INFO: Dataset: univ                Batch:  3/15	Loss 0.1126 (0.1125)
+2022-11-18 16:08:30,075:INFO: Dataset: univ                Batch:  4/15	Loss 0.1137 (0.1128)
+2022-11-18 16:08:30,076:INFO: Dataset: univ                Batch:  5/15	Loss 0.1128 (0.1128)
+2022-11-18 16:08:30,081:INFO: Dataset: univ                Batch:  6/15	Loss 0.1131 (0.1129)
+2022-11-18 16:08:30,083:INFO: Dataset: univ                Batch:  7/15	Loss 0.1131 (0.1129)
+2022-11-18 16:08:30,084:INFO: Dataset: univ                Batch:  8/15	Loss 0.1132 (0.1129)
+2022-11-18 16:08:30,086:INFO: Dataset: univ                Batch:  9/15	Loss 0.1121 (0.1128)
+2022-11-18 16:08:30,087:INFO: Dataset: univ                Batch: 10/15	Loss 0.1132 (0.1129)
+2022-11-18 16:08:30,088:INFO: Dataset: univ                Batch: 11/15	Loss 0.1127 (0.1128)
+2022-11-18 16:08:30,090:INFO: Dataset: univ                Batch: 12/15	Loss 0.1159 (0.1130)
+2022-11-18 16:08:30,092:INFO: Dataset: univ                Batch: 13/15	Loss 0.1120 (0.1130)
+2022-11-18 16:08:30,093:INFO: Dataset: univ                Batch: 14/15	Loss 0.1121 (0.1129)
+2022-11-18 16:08:30,095:INFO: Dataset: univ                Batch: 15/15	Loss 0.1119 (0.1129)
+2022-11-18 16:08:30,330:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2721 (0.2721)
+2022-11-18 16:08:30,333:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2795 (0.2755)
+2022-11-18 16:08:30,336:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2738 (0.2750)
+2022-11-18 16:08:30,339:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2739 (0.2747)
+2022-11-18 16:08:30,342:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2759 (0.2749)
+2022-11-18 16:08:30,373:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2776 (0.2754)
+2022-11-18 16:08:30,375:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2707 (0.2748)
+2022-11-18 16:08:30,376:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2700 (0.2742)
+2022-11-18 16:08:30,639:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1583 (0.1583)
+2022-11-18 16:08:30,642:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1549 (0.1568)
+2022-11-18 16:08:30,704:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1613 (0.1583)
+2022-11-18 16:08:30,707:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1605 (0.1588)
+2022-11-18 16:08:30,708:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1618 (0.1595)
+2022-11-18 16:08:30,713:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1625 (0.1599)
+2022-11-18 16:08:30,714:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1579 (0.1597)
+2022-11-18 16:08:30,715:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1588 (0.1596)
+2022-11-18 16:08:30,716:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1618 (0.1599)
+2022-11-18 16:08:30,717:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1623 (0.1601)
+2022-11-18 16:08:30,718:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1641 (0.1605)
+2022-11-18 16:08:30,720:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1621 (0.1606)
+2022-11-18 16:08:30,721:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1629 (0.1608)
+2022-11-18 16:08:30,723:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1605 (0.1608)
+2022-11-18 16:08:30,724:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1602 (0.1607)
+2022-11-18 16:08:30,725:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1588 (0.1606)
+2022-11-18 16:08:30,726:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1629 (0.1608)
+2022-11-18 16:08:30,728:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1603 (0.1607)
+2022-11-18 16:08:30,769:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:31,054:INFO: 		 ADE on eth                       dataset:	 1.788018822669983
+2022-11-18 16:08:31,054:INFO: Average validation o:	ADE  1.7880	FDE  2.7006
+2022-11-18 16:08:31,055:INFO: - Computing loss (validation)
+2022-11-18 16:08:31,246:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6349 (0.6349)
+2022-11-18 16:08:31,247:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6411 (0.6353)
+2022-11-18 16:08:31,493:INFO: Dataset: univ                Batch: 1/3	Loss 0.1159 (0.1159)
+2022-11-18 16:08:31,494:INFO: Dataset: univ                Batch: 2/3	Loss 0.1150 (0.1155)
+2022-11-18 16:08:31,496:INFO: Dataset: univ                Batch: 3/3	Loss 0.1131 (0.1146)
+2022-11-18 16:08:31,734:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2724 (0.2724)
+2022-11-18 16:08:31,737:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2764 (0.2733)
+2022-11-18 16:08:31,977:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1630 (0.1630)
+2022-11-18 16:08:31,978:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1634 (0.1632)
+2022-11-18 16:08:31,983:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1654 (0.1639)
+2022-11-18 16:08:31,987:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1626 (0.1636)
+2022-11-18 16:08:31,988:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1582 (0.1626)
+2022-11-18 16:08:32,048:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_755.pth.tar
+2022-11-18 16:08:32,049:INFO: 
+===> EPOCH: 756 (P4)
+2022-11-18 16:08:32,049:INFO: - Computing loss (training)
+2022-11-18 16:08:32,250:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6314 (0.6314)
+2022-11-18 16:08:32,253:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6415 (0.6364)
+2022-11-18 16:08:32,255:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6561 (0.6428)
+2022-11-18 16:08:32,258:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6316 (0.6410)
+2022-11-18 16:08:32,495:INFO: Dataset: univ                Batch:  1/15	Loss 0.1107 (0.1107)
+2022-11-18 16:08:32,535:INFO: Dataset: univ                Batch:  2/15	Loss 0.1129 (0.1118)
+2022-11-18 16:08:32,537:INFO: Dataset: univ                Batch:  3/15	Loss 0.1112 (0.1116)
+2022-11-18 16:08:32,538:INFO: Dataset: univ                Batch:  4/15	Loss 0.1123 (0.1118)
+2022-11-18 16:08:32,539:INFO: Dataset: univ                Batch:  5/15	Loss 0.1108 (0.1116)
+2022-11-18 16:08:32,543:INFO: Dataset: univ                Batch:  6/15	Loss 0.1138 (0.1119)
+2022-11-18 16:08:32,545:INFO: Dataset: univ                Batch:  7/15	Loss 0.1103 (0.1117)
+2022-11-18 16:08:32,546:INFO: Dataset: univ                Batch:  8/15	Loss 0.1115 (0.1117)
+2022-11-18 16:08:32,548:INFO: Dataset: univ                Batch:  9/15	Loss 0.1105 (0.1115)
+2022-11-18 16:08:32,549:INFO: Dataset: univ                Batch: 10/15	Loss 0.1102 (0.1114)
+2022-11-18 16:08:32,550:INFO: Dataset: univ                Batch: 11/15	Loss 0.1149 (0.1117)
+2022-11-18 16:08:32,553:INFO: Dataset: univ                Batch: 12/15	Loss 0.1111 (0.1116)
+2022-11-18 16:08:32,554:INFO: Dataset: univ                Batch: 13/15	Loss 0.1106 (0.1115)
+2022-11-18 16:08:32,555:INFO: Dataset: univ                Batch: 14/15	Loss 0.1111 (0.1115)
+2022-11-18 16:08:32,557:INFO: Dataset: univ                Batch: 15/15	Loss 0.1052 (0.1114)
+2022-11-18 16:08:32,808:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2718 (0.2718)
+2022-11-18 16:08:32,812:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2743 (0.2730)
+2022-11-18 16:08:32,816:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2725 (0.2728)
+2022-11-18 16:08:32,818:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2744 (0.2732)
+2022-11-18 16:08:32,819:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2726 (0.2731)
+2022-11-18 16:08:32,853:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2773 (0.2737)
+2022-11-18 16:08:32,857:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2715 (0.2734)
+2022-11-18 16:08:32,861:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2702 (0.2730)
+2022-11-18 16:08:33,132:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1580 (0.1580)
+2022-11-18 16:08:33,153:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1600 (0.1591)
+2022-11-18 16:08:33,155:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1586 (0.1589)
+2022-11-18 16:08:33,159:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1569 (0.1584)
+2022-11-18 16:08:33,161:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1593 (0.1586)
+2022-11-18 16:08:33,183:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1583 (0.1585)
+2022-11-18 16:08:33,184:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1598 (0.1587)
+2022-11-18 16:08:33,186:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1559 (0.1584)
+2022-11-18 16:08:33,187:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1566 (0.1582)
+2022-11-18 16:08:33,188:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1620 (0.1586)
+2022-11-18 16:08:33,189:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1561 (0.1584)
+2022-11-18 16:08:33,191:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1554 (0.1581)
+2022-11-18 16:08:33,192:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1590 (0.1582)
+2022-11-18 16:08:33,194:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1565 (0.1581)
+2022-11-18 16:08:33,195:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1538 (0.1578)
+2022-11-18 16:08:33,196:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1603 (0.1580)
+2022-11-18 16:08:33,197:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1534 (0.1577)
+2022-11-18 16:08:33,199:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1595 (0.1578)
+2022-11-18 16:08:33,243:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:33,522:INFO: 		 ADE on eth                       dataset:	 1.8205702304840088
+2022-11-18 16:08:33,522:INFO: Average validation o:	ADE  1.8206	FDE  2.7045
+2022-11-18 16:08:33,523:INFO: - Computing loss (validation)
+2022-11-18 16:08:33,709:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6389 (0.6389)
+2022-11-18 16:08:33,710:INFO: Dataset: hotel               Batch: 2/2	Loss 0.5785 (0.6350)
+2022-11-18 16:08:33,980:INFO: Dataset: univ                Batch: 1/3	Loss 0.1144 (0.1144)
+2022-11-18 16:08:33,981:INFO: Dataset: univ                Batch: 2/3	Loss 0.1105 (0.1124)
+2022-11-18 16:08:33,982:INFO: Dataset: univ                Batch: 3/3	Loss 0.1149 (0.1130)
+2022-11-18 16:08:34,228:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2728 (0.2728)
+2022-11-18 16:08:34,231:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2711 (0.2723)
+2022-11-18 16:08:34,464:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1588 (0.1588)
+2022-11-18 16:08:34,465:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1613 (0.1600)
+2022-11-18 16:08:34,467:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1587 (0.1596)
+2022-11-18 16:08:34,476:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1589 (0.1594)
+2022-11-18 16:08:34,481:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1606 (0.1597)
+2022-11-18 16:08:34,539:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_756.pth.tar
+2022-11-18 16:08:34,539:INFO: 
+===> EPOCH: 757 (P4)
+2022-11-18 16:08:34,540:INFO: - Computing loss (training)
+2022-11-18 16:08:34,742:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6409 (0.6409)
+2022-11-18 16:08:34,744:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6264 (0.6336)
+2022-11-18 16:08:34,747:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6450 (0.6376)
+2022-11-18 16:08:34,749:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6546 (0.6404)
+2022-11-18 16:08:34,988:INFO: Dataset: univ                Batch:  1/15	Loss 0.1092 (0.1092)
+2022-11-18 16:08:34,991:INFO: Dataset: univ                Batch:  2/15	Loss 0.1101 (0.1096)
+2022-11-18 16:08:35,053:INFO: Dataset: univ                Batch:  3/15	Loss 0.1107 (0.1100)
+2022-11-18 16:08:35,054:INFO: Dataset: univ                Batch:  4/15	Loss 0.1110 (0.1103)
+2022-11-18 16:08:35,056:INFO: Dataset: univ                Batch:  5/15	Loss 0.1082 (0.1098)
+2022-11-18 16:08:35,063:INFO: Dataset: univ                Batch:  6/15	Loss 0.1128 (0.1103)
+2022-11-18 16:08:35,065:INFO: Dataset: univ                Batch:  7/15	Loss 0.1112 (0.1104)
+2022-11-18 16:08:35,067:INFO: Dataset: univ                Batch:  8/15	Loss 0.1097 (0.1103)
+2022-11-18 16:08:35,068:INFO: Dataset: univ                Batch:  9/15	Loss 0.1092 (0.1102)
+2022-11-18 16:08:35,069:INFO: Dataset: univ                Batch: 10/15	Loss 0.1110 (0.1103)
+2022-11-18 16:08:35,071:INFO: Dataset: univ                Batch: 11/15	Loss 0.1097 (0.1102)
+2022-11-18 16:08:35,073:INFO: Dataset: univ                Batch: 12/15	Loss 0.1094 (0.1101)
+2022-11-18 16:08:35,075:INFO: Dataset: univ                Batch: 13/15	Loss 0.1109 (0.1102)
+2022-11-18 16:08:35,077:INFO: Dataset: univ                Batch: 14/15	Loss 0.1092 (0.1101)
+2022-11-18 16:08:35,078:INFO: Dataset: univ                Batch: 15/15	Loss 0.1072 (0.1101)
+2022-11-18 16:08:35,341:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2704 (0.2704)
+2022-11-18 16:08:35,344:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2716 (0.2710)
+2022-11-18 16:08:35,345:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2718 (0.2713)
+2022-11-18 16:08:35,350:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2701 (0.2710)
+2022-11-18 16:08:35,351:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2727 (0.2713)
+2022-11-18 16:08:35,391:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2759 (0.2721)
+2022-11-18 16:08:35,392:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2732 (0.2722)
+2022-11-18 16:08:35,394:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2698 (0.2720)
+2022-11-18 16:08:35,646:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1619 (0.1619)
+2022-11-18 16:08:35,648:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1517 (0.1570)
+2022-11-18 16:08:35,651:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1535 (0.1558)
+2022-11-18 16:08:35,653:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1537 (0.1553)
+2022-11-18 16:08:35,675:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1594 (0.1561)
+2022-11-18 16:08:35,679:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1584 (0.1565)
+2022-11-18 16:08:35,680:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1562 (0.1564)
+2022-11-18 16:08:35,681:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1550 (0.1562)
+2022-11-18 16:08:35,683:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1590 (0.1565)
+2022-11-18 16:08:35,684:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1532 (0.1562)
+2022-11-18 16:08:35,686:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1559 (0.1562)
+2022-11-18 16:08:35,688:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1534 (0.1560)
+2022-11-18 16:08:35,689:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1546 (0.1559)
+2022-11-18 16:08:35,690:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1536 (0.1557)
+2022-11-18 16:08:35,691:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1516 (0.1554)
+2022-11-18 16:08:35,692:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1556 (0.1554)
+2022-11-18 16:08:35,694:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1550 (0.1554)
+2022-11-18 16:08:35,696:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1522 (0.1552)
+2022-11-18 16:08:35,739:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:36,048:INFO: 		 ADE on eth                       dataset:	 1.8049542903900146
+2022-11-18 16:08:36,049:INFO: Average validation o:	ADE  1.8050	FDE  2.6776
+2022-11-18 16:08:36,050:INFO: - Computing loss (validation)
+2022-11-18 16:08:36,263:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6342 (0.6342)
+2022-11-18 16:08:36,265:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6379 (0.6345)
+2022-11-18 16:08:36,567:INFO: Dataset: univ                Batch: 1/3	Loss 0.1121 (0.1121)
+2022-11-18 16:08:36,569:INFO: Dataset: univ                Batch: 2/3	Loss 0.1118 (0.1119)
+2022-11-18 16:08:36,570:INFO: Dataset: univ                Batch: 3/3	Loss 0.1111 (0.1117)
+2022-11-18 16:08:36,873:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2717 (0.2717)
+2022-11-18 16:08:36,875:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2707 (0.2714)
+2022-11-18 16:08:37,116:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1587 (0.1587)
+2022-11-18 16:08:37,117:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1582 (0.1585)
+2022-11-18 16:08:37,135:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1551 (0.1574)
+2022-11-18 16:08:37,135:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1564 (0.1572)
+2022-11-18 16:08:37,136:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1564 (0.1570)
+2022-11-18 16:08:37,195:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_757.pth.tar
+2022-11-18 16:08:37,195:INFO: 
+===> EPOCH: 758 (P4)
+2022-11-18 16:08:37,196:INFO: - Computing loss (training)
+2022-11-18 16:08:37,387:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6405 (0.6405)
+2022-11-18 16:08:37,391:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6394 (0.6400)
+2022-11-18 16:08:37,392:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6381 (0.6394)
+2022-11-18 16:08:37,394:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6403 (0.6395)
+2022-11-18 16:08:37,652:INFO: Dataset: univ                Batch:  1/15	Loss 0.1085 (0.1085)
+2022-11-18 16:08:37,656:INFO: Dataset: univ                Batch:  2/15	Loss 0.1084 (0.1085)
+2022-11-18 16:08:37,660:INFO: Dataset: univ                Batch:  3/15	Loss 0.1077 (0.1082)
+2022-11-18 16:08:37,667:INFO: Dataset: univ                Batch:  4/15	Loss 0.1102 (0.1087)
+2022-11-18 16:08:37,675:INFO: Dataset: univ                Batch:  5/15	Loss 0.1082 (0.1086)
+2022-11-18 16:08:37,704:INFO: Dataset: univ                Batch:  6/15	Loss 0.1077 (0.1084)
+2022-11-18 16:08:37,706:INFO: Dataset: univ                Batch:  7/15	Loss 0.1092 (0.1085)
+2022-11-18 16:08:37,707:INFO: Dataset: univ                Batch:  8/15	Loss 0.1096 (0.1087)
+2022-11-18 16:08:37,709:INFO: Dataset: univ                Batch:  9/15	Loss 0.1101 (0.1088)
+2022-11-18 16:08:37,710:INFO: Dataset: univ                Batch: 10/15	Loss 0.1112 (0.1090)
+2022-11-18 16:08:37,712:INFO: Dataset: univ                Batch: 11/15	Loss 0.1087 (0.1090)
+2022-11-18 16:08:37,715:INFO: Dataset: univ                Batch: 12/15	Loss 0.1101 (0.1091)
+2022-11-18 16:08:37,717:INFO: Dataset: univ                Batch: 13/15	Loss 0.1081 (0.1090)
+2022-11-18 16:08:37,718:INFO: Dataset: univ                Batch: 14/15	Loss 0.1077 (0.1089)
+2022-11-18 16:08:37,720:INFO: Dataset: univ                Batch: 15/15	Loss 0.1080 (0.1089)
+2022-11-18 16:08:37,975:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2709 (0.2709)
+2022-11-18 16:08:37,977:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2742 (0.2725)
+2022-11-18 16:08:37,983:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2724 (0.2725)
+2022-11-18 16:08:37,986:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2713 (0.2722)
+2022-11-18 16:08:37,988:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2707 (0.2719)
+2022-11-18 16:08:38,014:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2678 (0.2712)
+2022-11-18 16:08:38,017:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2709 (0.2712)
+2022-11-18 16:08:38,021:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2698 (0.2710)
+2022-11-18 16:08:38,283:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1537 (0.1537)
+2022-11-18 16:08:38,312:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1552 (0.1545)
+2022-11-18 16:08:38,316:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1542 (0.1544)
+2022-11-18 16:08:38,318:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1510 (0.1535)
+2022-11-18 16:08:38,319:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1570 (0.1542)
+2022-11-18 16:08:38,324:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1512 (0.1536)
+2022-11-18 16:08:38,326:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1555 (0.1539)
+2022-11-18 16:08:38,327:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1572 (0.1544)
+2022-11-18 16:08:38,328:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1544 (0.1544)
+2022-11-18 16:08:38,329:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1524 (0.1542)
+2022-11-18 16:08:38,330:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1536 (0.1541)
+2022-11-18 16:08:38,332:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1514 (0.1539)
+2022-11-18 16:08:38,333:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1510 (0.1537)
+2022-11-18 16:08:38,335:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1552 (0.1538)
+2022-11-18 16:08:38,336:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1470 (0.1534)
+2022-11-18 16:08:38,338:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1493 (0.1531)
+2022-11-18 16:08:38,340:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1512 (0.1530)
+2022-11-18 16:08:38,342:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1499 (0.1528)
+2022-11-18 16:08:38,385:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:38,673:INFO: 		 ADE on eth                       dataset:	 1.791316032409668
+2022-11-18 16:08:38,673:INFO: Average validation o:	ADE  1.7913	FDE  2.6770
+2022-11-18 16:08:38,674:INFO: - Computing loss (validation)
+2022-11-18 16:08:38,864:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6352 (0.6352)
+2022-11-18 16:08:38,865:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6130 (0.6342)
+2022-11-18 16:08:39,104:INFO: Dataset: univ                Batch: 1/3	Loss 0.1110 (0.1110)
+2022-11-18 16:08:39,105:INFO: Dataset: univ                Batch: 2/3	Loss 0.1114 (0.1112)
+2022-11-18 16:08:39,116:INFO: Dataset: univ                Batch: 3/3	Loss 0.1089 (0.1104)
+2022-11-18 16:08:39,406:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2685 (0.2685)
+2022-11-18 16:08:39,408:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2766 (0.2705)
+2022-11-18 16:08:39,694:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1564 (0.1564)
+2022-11-18 16:08:39,695:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1561 (0.1562)
+2022-11-18 16:08:39,696:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1534 (0.1553)
+2022-11-18 16:08:39,700:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1515 (0.1544)
+2022-11-18 16:08:39,712:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1555 (0.1546)
+2022-11-18 16:08:39,772:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_758.pth.tar
+2022-11-18 16:08:39,772:INFO: 
+===> EPOCH: 759 (P4)
+2022-11-18 16:08:39,773:INFO: - Computing loss (training)
+2022-11-18 16:08:40,019:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6396 (0.6396)
+2022-11-18 16:08:40,024:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6484 (0.6441)
+2022-11-18 16:08:40,026:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6309 (0.6396)
+2022-11-18 16:08:40,029:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6342 (0.6388)
+2022-11-18 16:08:40,288:INFO: Dataset: univ                Batch:  1/15	Loss 0.1083 (0.1083)
+2022-11-18 16:08:40,291:INFO: Dataset: univ                Batch:  2/15	Loss 0.1081 (0.1082)
+2022-11-18 16:08:40,295:INFO: Dataset: univ                Batch:  3/15	Loss 0.1092 (0.1085)
+2022-11-18 16:08:40,335:INFO: Dataset: univ                Batch:  4/15	Loss 0.1066 (0.1080)
+2022-11-18 16:08:40,337:INFO: Dataset: univ                Batch:  5/15	Loss 0.1091 (0.1082)
+2022-11-18 16:08:40,340:INFO: Dataset: univ                Batch:  6/15	Loss 0.1076 (0.1081)
+2022-11-18 16:08:40,342:INFO: Dataset: univ                Batch:  7/15	Loss 0.1076 (0.1080)
+2022-11-18 16:08:40,343:INFO: Dataset: univ                Batch:  8/15	Loss 0.1078 (0.1080)
+2022-11-18 16:08:40,345:INFO: Dataset: univ                Batch:  9/15	Loss 0.1087 (0.1081)
+2022-11-18 16:08:40,346:INFO: Dataset: univ                Batch: 10/15	Loss 0.1089 (0.1082)
+2022-11-18 16:08:40,347:INFO: Dataset: univ                Batch: 11/15	Loss 0.1066 (0.1080)
+2022-11-18 16:08:40,349:INFO: Dataset: univ                Batch: 12/15	Loss 0.1065 (0.1079)
+2022-11-18 16:08:40,351:INFO: Dataset: univ                Batch: 13/15	Loss 0.1069 (0.1078)
+2022-11-18 16:08:40,352:INFO: Dataset: univ                Batch: 14/15	Loss 0.1079 (0.1078)
+2022-11-18 16:08:40,354:INFO: Dataset: univ                Batch: 15/15	Loss 0.1073 (0.1078)
+2022-11-18 16:08:40,602:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2725 (0.2725)
+2022-11-18 16:08:40,606:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2700 (0.2713)
+2022-11-18 16:08:40,610:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2683 (0.2704)
+2022-11-18 16:08:40,612:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2722 (0.2708)
+2022-11-18 16:08:40,614:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2704 (0.2708)
+2022-11-18 16:08:40,675:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2732 (0.2712)
+2022-11-18 16:08:40,676:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2676 (0.2707)
+2022-11-18 16:08:40,677:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2660 (0.2702)
+2022-11-18 16:08:40,943:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1483 (0.1483)
+2022-11-18 16:08:40,951:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1530 (0.1506)
+2022-11-18 16:08:40,953:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1509 (0.1507)
+2022-11-18 16:08:40,993:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1528 (0.1512)
+2022-11-18 16:08:40,994:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1514 (0.1512)
+2022-11-18 16:08:41,020:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1478 (0.1507)
+2022-11-18 16:08:41,021:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1522 (0.1509)
+2022-11-18 16:08:41,023:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1531 (0.1512)
+2022-11-18 16:08:41,024:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1549 (0.1516)
+2022-11-18 16:08:41,025:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1517 (0.1516)
+2022-11-18 16:08:41,026:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1528 (0.1517)
+2022-11-18 16:08:41,028:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1464 (0.1513)
+2022-11-18 16:08:41,030:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1474 (0.1510)
+2022-11-18 16:08:41,031:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1466 (0.1507)
+2022-11-18 16:08:41,032:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1519 (0.1508)
+2022-11-18 16:08:41,033:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1516 (0.1508)
+2022-11-18 16:08:41,035:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1504 (0.1508)
+2022-11-18 16:08:41,037:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1472 (0.1506)
+2022-11-18 16:08:41,081:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:41,409:INFO: 		 ADE on eth                       dataset:	 1.7824664115905762
+2022-11-18 16:08:41,409:INFO: Average validation o:	ADE  1.7825	FDE  2.6573
+2022-11-18 16:08:41,410:INFO: - Computing loss (validation)
+2022-11-18 16:08:41,609:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6343 (0.6343)
+2022-11-18 16:08:41,610:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6254 (0.6338)
+2022-11-18 16:08:41,841:INFO: Dataset: univ                Batch: 1/3	Loss 0.1094 (0.1094)
+2022-11-18 16:08:41,843:INFO: Dataset: univ                Batch: 2/3	Loss 0.1081 (0.1087)
+2022-11-18 16:08:41,845:INFO: Dataset: univ                Batch: 3/3	Loss 0.1107 (0.1093)
+2022-11-18 16:08:42,089:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2708 (0.2708)
+2022-11-18 16:08:42,090:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2665 (0.2697)
+2022-11-18 16:08:42,329:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1543 (0.1543)
+2022-11-18 16:08:42,337:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1548 (0.1546)
+2022-11-18 16:08:42,337:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1522 (0.1537)
+2022-11-18 16:08:42,339:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1479 (0.1524)
+2022-11-18 16:08:42,340:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1530 (0.1525)
+2022-11-18 16:08:42,398:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_759.pth.tar
+2022-11-18 16:08:42,398:INFO: 
+===> EPOCH: 760 (P4)
+2022-11-18 16:08:42,399:INFO: - Computing loss (training)
+2022-11-18 16:08:42,585:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6338 (0.6338)
+2022-11-18 16:08:42,588:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6355 (0.6346)
+2022-11-18 16:08:42,592:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6411 (0.6368)
+2022-11-18 16:08:42,594:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6434 (0.6379)
+2022-11-18 16:08:42,828:INFO: Dataset: univ                Batch:  1/15	Loss 0.1072 (0.1072)
+2022-11-18 16:08:42,831:INFO: Dataset: univ                Batch:  2/15	Loss 0.1074 (0.1073)
+2022-11-18 16:08:42,835:INFO: Dataset: univ                Batch:  3/15	Loss 0.1067 (0.1071)
+2022-11-18 16:08:42,864:INFO: Dataset: univ                Batch:  4/15	Loss 0.1078 (0.1072)
+2022-11-18 16:08:42,866:INFO: Dataset: univ                Batch:  5/15	Loss 0.1073 (0.1073)
+2022-11-18 16:08:42,875:INFO: Dataset: univ                Batch:  6/15	Loss 0.1066 (0.1071)
+2022-11-18 16:08:42,876:INFO: Dataset: univ                Batch:  7/15	Loss 0.1060 (0.1070)
+2022-11-18 16:08:42,878:INFO: Dataset: univ                Batch:  8/15	Loss 0.1066 (0.1069)
+2022-11-18 16:08:42,879:INFO: Dataset: univ                Batch:  9/15	Loss 0.1073 (0.1070)
+2022-11-18 16:08:42,881:INFO: Dataset: univ                Batch: 10/15	Loss 0.1082 (0.1071)
+2022-11-18 16:08:42,882:INFO: Dataset: univ                Batch: 11/15	Loss 0.1068 (0.1070)
+2022-11-18 16:08:42,884:INFO: Dataset: univ                Batch: 12/15	Loss 0.1064 (0.1070)
+2022-11-18 16:08:42,886:INFO: Dataset: univ                Batch: 13/15	Loss 0.1063 (0.1069)
+2022-11-18 16:08:42,887:INFO: Dataset: univ                Batch: 14/15	Loss 0.1064 (0.1069)
+2022-11-18 16:08:42,888:INFO: Dataset: univ                Batch: 15/15	Loss 0.1045 (0.1069)
+2022-11-18 16:08:43,151:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2695 (0.2695)
+2022-11-18 16:08:43,153:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2698 (0.2696)
+2022-11-18 16:08:43,155:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2710 (0.2701)
+2022-11-18 16:08:43,157:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2691 (0.2698)
+2022-11-18 16:08:43,161:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2706 (0.2700)
+2022-11-18 16:08:43,192:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2699 (0.2700)
+2022-11-18 16:08:43,193:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2666 (0.2695)
+2022-11-18 16:08:43,194:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2691 (0.2694)
+2022-11-18 16:08:43,443:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1478 (0.1478)
+2022-11-18 16:08:43,446:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1482 (0.1480)
+2022-11-18 16:08:43,448:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1499 (0.1486)
+2022-11-18 16:08:43,453:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1549 (0.1503)
+2022-11-18 16:08:43,480:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1509 (0.1504)
+2022-11-18 16:08:43,485:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1510 (0.1505)
+2022-11-18 16:08:43,486:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1486 (0.1502)
+2022-11-18 16:08:43,488:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1497 (0.1502)
+2022-11-18 16:08:43,489:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1461 (0.1497)
+2022-11-18 16:08:43,490:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1481 (0.1495)
+2022-11-18 16:08:43,493:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1459 (0.1492)
+2022-11-18 16:08:43,494:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1461 (0.1490)
+2022-11-18 16:08:43,496:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1470 (0.1488)
+2022-11-18 16:08:43,497:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1475 (0.1487)
+2022-11-18 16:08:43,498:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1472 (0.1486)
+2022-11-18 16:08:43,500:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1463 (0.1485)
+2022-11-18 16:08:43,502:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1513 (0.1486)
+2022-11-18 16:08:43,504:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1490 (0.1487)
+2022-11-18 16:08:43,548:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:43,843:INFO: 		 ADE on eth                       dataset:	 1.7720390558242798
+2022-11-18 16:08:43,843:INFO: Average validation o:	ADE  1.7720	FDE  2.6737
+2022-11-18 16:08:43,844:INFO: - Computing loss (validation)
+2022-11-18 16:08:44,029:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6330 (0.6330)
+2022-11-18 16:08:44,030:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6363 (0.6332)
+2022-11-18 16:08:44,296:INFO: Dataset: univ                Batch: 1/3	Loss 0.1104 (0.1104)
+2022-11-18 16:08:44,300:INFO: Dataset: univ                Batch: 2/3	Loss 0.1082 (0.1093)
+2022-11-18 16:08:44,302:INFO: Dataset: univ                Batch: 3/3	Loss 0.1063 (0.1083)
+2022-11-18 16:08:44,571:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2681 (0.2681)
+2022-11-18 16:08:44,572:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2712 (0.2690)
+2022-11-18 16:08:44,900:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1497 (0.1497)
+2022-11-18 16:08:44,902:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1496 (0.1497)
+2022-11-18 16:08:44,902:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1512 (0.1502)
+2022-11-18 16:08:44,903:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1514 (0.1505)
+2022-11-18 16:08:44,904:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1507 (0.1505)
+2022-11-18 16:08:44,967:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_760.pth.tar
+2022-11-18 16:08:44,967:INFO: 
+===> EPOCH: 761 (P4)
+2022-11-18 16:08:44,967:INFO: - Computing loss (training)
+2022-11-18 16:08:45,173:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6325 (0.6325)
+2022-11-18 16:08:45,175:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6412 (0.6368)
+2022-11-18 16:08:45,177:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6316 (0.6351)
+2022-11-18 16:08:45,191:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6457 (0.6369)
+2022-11-18 16:08:45,433:INFO: Dataset: univ                Batch:  1/15	Loss 0.1050 (0.1050)
+2022-11-18 16:08:45,436:INFO: Dataset: univ                Batch:  2/15	Loss 0.1044 (0.1047)
+2022-11-18 16:08:45,479:INFO: Dataset: univ                Batch:  3/15	Loss 0.1060 (0.1051)
+2022-11-18 16:08:45,481:INFO: Dataset: univ                Batch:  4/15	Loss 0.1072 (0.1056)
+2022-11-18 16:08:45,482:INFO: Dataset: univ                Batch:  5/15	Loss 0.1059 (0.1057)
+2022-11-18 16:08:45,485:INFO: Dataset: univ                Batch:  6/15	Loss 0.1053 (0.1056)
+2022-11-18 16:08:45,487:INFO: Dataset: univ                Batch:  7/15	Loss 0.1085 (0.1059)
+2022-11-18 16:08:45,488:INFO: Dataset: univ                Batch:  8/15	Loss 0.1070 (0.1061)
+2022-11-18 16:08:45,489:INFO: Dataset: univ                Batch:  9/15	Loss 0.1055 (0.1060)
+2022-11-18 16:08:45,491:INFO: Dataset: univ                Batch: 10/15	Loss 0.1067 (0.1061)
+2022-11-18 16:08:45,493:INFO: Dataset: univ                Batch: 11/15	Loss 0.1050 (0.1060)
+2022-11-18 16:08:45,495:INFO: Dataset: univ                Batch: 12/15	Loss 0.1059 (0.1060)
+2022-11-18 16:08:45,496:INFO: Dataset: univ                Batch: 13/15	Loss 0.1068 (0.1060)
+2022-11-18 16:08:45,497:INFO: Dataset: univ                Batch: 14/15	Loss 0.1058 (0.1060)
+2022-11-18 16:08:45,499:INFO: Dataset: univ                Batch: 15/15	Loss 0.1060 (0.1060)
+2022-11-18 16:08:45,760:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2686 (0.2686)
+2022-11-18 16:08:45,762:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2674 (0.2680)
+2022-11-18 16:08:45,764:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2700 (0.2687)
+2022-11-18 16:08:45,766:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2685 (0.2686)
+2022-11-18 16:08:45,768:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2664 (0.2682)
+2022-11-18 16:08:45,822:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2702 (0.2685)
+2022-11-18 16:08:45,827:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2673 (0.2683)
+2022-11-18 16:08:45,831:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2716 (0.2687)
+2022-11-18 16:08:46,103:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1440 (0.1440)
+2022-11-18 16:08:46,108:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1484 (0.1461)
+2022-11-18 16:08:46,116:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1463 (0.1462)
+2022-11-18 16:08:46,118:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1455 (0.1460)
+2022-11-18 16:08:46,170:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1445 (0.1457)
+2022-11-18 16:08:46,176:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1444 (0.1455)
+2022-11-18 16:08:46,177:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1497 (0.1461)
+2022-11-18 16:08:46,178:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1498 (0.1465)
+2022-11-18 16:08:46,179:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1486 (0.1468)
+2022-11-18 16:08:46,180:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1475 (0.1468)
+2022-11-18 16:08:46,183:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1455 (0.1467)
+2022-11-18 16:08:46,184:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1457 (0.1466)
+2022-11-18 16:08:46,185:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1516 (0.1470)
+2022-11-18 16:08:46,186:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1490 (0.1471)
+2022-11-18 16:08:46,187:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1488 (0.1473)
+2022-11-18 16:08:46,188:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1443 (0.1471)
+2022-11-18 16:08:46,190:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1431 (0.1468)
+2022-11-18 16:08:46,192:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1476 (0.1469)
+2022-11-18 16:08:46,235:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:46,522:INFO: 		 ADE on eth                       dataset:	 1.8211814165115356
+2022-11-18 16:08:46,522:INFO: Average validation o:	ADE  1.8212	FDE  2.8013
+2022-11-18 16:08:46,524:INFO: - Computing loss (validation)
+2022-11-18 16:08:46,719:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6339 (0.6339)
+2022-11-18 16:08:46,721:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6183 (0.6327)
+2022-11-18 16:08:46,969:INFO: Dataset: univ                Batch: 1/3	Loss 0.1064 (0.1064)
+2022-11-18 16:08:46,970:INFO: Dataset: univ                Batch: 2/3	Loss 0.1059 (0.1061)
+2022-11-18 16:08:46,972:INFO: Dataset: univ                Batch: 3/3	Loss 0.1101 (0.1073)
+2022-11-18 16:08:47,215:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2681 (0.2681)
+2022-11-18 16:08:47,216:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2690 (0.2683)
+2022-11-18 16:08:47,456:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1477 (0.1477)
+2022-11-18 16:08:47,458:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1490 (0.1484)
+2022-11-18 16:08:47,460:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1460 (0.1476)
+2022-11-18 16:08:47,461:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1510 (0.1484)
+2022-11-18 16:08:47,463:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1500 (0.1487)
+2022-11-18 16:08:47,518:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_761.pth.tar
+2022-11-18 16:08:47,518:INFO: 
+===> EPOCH: 762 (P4)
+2022-11-18 16:08:47,519:INFO: - Computing loss (training)
+2022-11-18 16:08:47,703:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6360 (0.6360)
+2022-11-18 16:08:47,705:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6359 (0.6360)
+2022-11-18 16:08:47,707:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6393 (0.6371)
+2022-11-18 16:08:47,709:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6294 (0.6359)
+2022-11-18 16:08:47,953:INFO: Dataset: univ                Batch:  1/15	Loss 0.1052 (0.1052)
+2022-11-18 16:08:47,955:INFO: Dataset: univ                Batch:  2/15	Loss 0.1056 (0.1054)
+2022-11-18 16:08:47,957:INFO: Dataset: univ                Batch:  3/15	Loss 0.1071 (0.1060)
+2022-11-18 16:08:47,967:INFO: Dataset: univ                Batch:  4/15	Loss 0.1055 (0.1058)
+2022-11-18 16:08:47,969:INFO: Dataset: univ                Batch:  5/15	Loss 0.1041 (0.1055)
+2022-11-18 16:08:47,979:INFO: Dataset: univ                Batch:  6/15	Loss 0.1054 (0.1055)
+2022-11-18 16:08:47,981:INFO: Dataset: univ                Batch:  7/15	Loss 0.1050 (0.1054)
+2022-11-18 16:08:47,982:INFO: Dataset: univ                Batch:  8/15	Loss 0.1045 (0.1053)
+2022-11-18 16:08:47,983:INFO: Dataset: univ                Batch:  9/15	Loss 0.1042 (0.1052)
+2022-11-18 16:08:47,985:INFO: Dataset: univ                Batch: 10/15	Loss 0.1060 (0.1052)
+2022-11-18 16:08:47,986:INFO: Dataset: univ                Batch: 11/15	Loss 0.1072 (0.1054)
+2022-11-18 16:08:47,989:INFO: Dataset: univ                Batch: 12/15	Loss 0.1042 (0.1053)
+2022-11-18 16:08:47,990:INFO: Dataset: univ                Batch: 13/15	Loss 0.1054 (0.1053)
+2022-11-18 16:08:47,991:INFO: Dataset: univ                Batch: 14/15	Loss 0.1051 (0.1053)
+2022-11-18 16:08:47,993:INFO: Dataset: univ                Batch: 15/15	Loss 0.1025 (0.1052)
+2022-11-18 16:08:48,254:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2675 (0.2675)
+2022-11-18 16:08:48,259:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2683 (0.2679)
+2022-11-18 16:08:48,260:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2683 (0.2680)
+2022-11-18 16:08:48,265:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2700 (0.2685)
+2022-11-18 16:08:48,268:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2689 (0.2686)
+2022-11-18 16:08:48,292:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2685 (0.2685)
+2022-11-18 16:08:48,293:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2659 (0.2681)
+2022-11-18 16:08:48,294:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2672 (0.2680)
+2022-11-18 16:08:48,546:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1495 (0.1495)
+2022-11-18 16:08:48,548:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1435 (0.1463)
+2022-11-18 16:08:48,551:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1481 (0.1469)
+2022-11-18 16:08:48,601:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1468 (0.1469)
+2022-11-18 16:08:48,602:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1484 (0.1472)
+2022-11-18 16:08:48,607:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1451 (0.1468)
+2022-11-18 16:08:48,608:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1491 (0.1472)
+2022-11-18 16:08:48,609:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1442 (0.1469)
+2022-11-18 16:08:48,610:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1480 (0.1470)
+2022-11-18 16:08:48,611:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1469 (0.1470)
+2022-11-18 16:08:48,614:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1418 (0.1465)
+2022-11-18 16:08:48,615:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1430 (0.1461)
+2022-11-18 16:08:48,616:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1453 (0.1461)
+2022-11-18 16:08:48,617:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1426 (0.1458)
+2022-11-18 16:08:48,618:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1447 (0.1457)
+2022-11-18 16:08:48,620:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1446 (0.1456)
+2022-11-18 16:08:48,622:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1420 (0.1454)
+2022-11-18 16:08:48,624:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1413 (0.1452)
+2022-11-18 16:08:48,668:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:48,984:INFO: 		 ADE on eth                       dataset:	 1.8415236473083496
+2022-11-18 16:08:48,984:INFO: Average validation o:	ADE  1.8415	FDE  2.8129
+2022-11-18 16:08:48,985:INFO: - Computing loss (validation)
+2022-11-18 16:08:49,209:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6312 (0.6312)
+2022-11-18 16:08:49,210:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6453 (0.6323)
+2022-11-18 16:08:49,434:INFO: Dataset: univ                Batch: 1/3	Loss 0.1072 (0.1072)
+2022-11-18 16:08:49,435:INFO: Dataset: univ                Batch: 2/3	Loss 0.1075 (0.1074)
+2022-11-18 16:08:49,436:INFO: Dataset: univ                Batch: 3/3	Loss 0.1046 (0.1064)
+2022-11-18 16:08:49,671:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2662 (0.2662)
+2022-11-18 16:08:49,672:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2724 (0.2677)
+2022-11-18 16:08:49,924:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1436 (0.1436)
+2022-11-18 16:08:49,925:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1485 (0.1460)
+2022-11-18 16:08:49,927:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1487 (0.1470)
+2022-11-18 16:08:49,929:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1477 (0.1471)
+2022-11-18 16:08:49,930:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1466 (0.1470)
+2022-11-18 16:08:49,993:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_762.pth.tar
+2022-11-18 16:08:49,993:INFO: 
+===> EPOCH: 763 (P4)
+2022-11-18 16:08:49,994:INFO: - Computing loss (training)
+2022-11-18 16:08:50,190:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6294 (0.6294)
+2022-11-18 16:08:50,193:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6360 (0.6329)
+2022-11-18 16:08:50,197:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6404 (0.6353)
+2022-11-18 16:08:50,198:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6341 (0.6351)
+2022-11-18 16:08:50,436:INFO: Dataset: univ                Batch:  1/15	Loss 0.1035 (0.1035)
+2022-11-18 16:08:50,438:INFO: Dataset: univ                Batch:  2/15	Loss 0.1056 (0.1045)
+2022-11-18 16:08:50,442:INFO: Dataset: univ                Batch:  3/15	Loss 0.1043 (0.1044)
+2022-11-18 16:08:50,445:INFO: Dataset: univ                Batch:  4/15	Loss 0.1053 (0.1046)
+2022-11-18 16:08:50,454:INFO: Dataset: univ                Batch:  5/15	Loss 0.1040 (0.1045)
+2022-11-18 16:08:50,463:INFO: Dataset: univ                Batch:  6/15	Loss 0.1041 (0.1044)
+2022-11-18 16:08:50,465:INFO: Dataset: univ                Batch:  7/15	Loss 0.1042 (0.1044)
+2022-11-18 16:08:50,466:INFO: Dataset: univ                Batch:  8/15	Loss 0.1047 (0.1044)
+2022-11-18 16:08:50,467:INFO: Dataset: univ                Batch:  9/15	Loss 0.1044 (0.1044)
+2022-11-18 16:08:50,469:INFO: Dataset: univ                Batch: 10/15	Loss 0.1044 (0.1044)
+2022-11-18 16:08:50,470:INFO: Dataset: univ                Batch: 11/15	Loss 0.1043 (0.1044)
+2022-11-18 16:08:50,472:INFO: Dataset: univ                Batch: 12/15	Loss 0.1065 (0.1046)
+2022-11-18 16:08:50,473:INFO: Dataset: univ                Batch: 13/15	Loss 0.1042 (0.1045)
+2022-11-18 16:08:50,475:INFO: Dataset: univ                Batch: 14/15	Loss 0.1050 (0.1046)
+2022-11-18 16:08:50,476:INFO: Dataset: univ                Batch: 15/15	Loss 0.1017 (0.1045)
+2022-11-18 16:08:50,722:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2670 (0.2670)
+2022-11-18 16:08:50,725:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2682 (0.2676)
+2022-11-18 16:08:50,728:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2684 (0.2679)
+2022-11-18 16:08:50,729:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2675 (0.2678)
+2022-11-18 16:08:50,731:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2668 (0.2676)
+2022-11-18 16:08:50,776:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2715 (0.2682)
+2022-11-18 16:08:50,777:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2658 (0.2678)
+2022-11-18 16:08:50,778:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2644 (0.2674)
+2022-11-18 16:08:51,029:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1447 (0.1447)
+2022-11-18 16:08:51,034:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1421 (0.1433)
+2022-11-18 16:08:51,036:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1421 (0.1428)
+2022-11-18 16:08:51,043:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1403 (0.1422)
+2022-11-18 16:08:51,079:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1451 (0.1427)
+2022-11-18 16:08:51,087:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1451 (0.1431)
+2022-11-18 16:08:51,088:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1449 (0.1434)
+2022-11-18 16:08:51,089:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1435 (0.1434)
+2022-11-18 16:08:51,091:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1419 (0.1432)
+2022-11-18 16:08:51,092:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1447 (0.1434)
+2022-11-18 16:08:51,093:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1458 (0.1436)
+2022-11-18 16:08:51,095:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1481 (0.1440)
+2022-11-18 16:08:51,096:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1404 (0.1437)
+2022-11-18 16:08:51,097:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1452 (0.1438)
+2022-11-18 16:08:51,098:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1465 (0.1440)
+2022-11-18 16:08:51,100:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1422 (0.1439)
+2022-11-18 16:08:51,101:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1419 (0.1438)
+2022-11-18 16:08:51,103:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1424 (0.1437)
+2022-11-18 16:08:51,148:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:51,428:INFO: 		 ADE on eth                       dataset:	 1.8169150352478027
+2022-11-18 16:08:51,428:INFO: Average validation o:	ADE  1.8169	FDE  2.7851
+2022-11-18 16:08:51,429:INFO: - Computing loss (validation)
+2022-11-18 16:08:51,617:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6328 (0.6328)
+2022-11-18 16:08:51,618:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6192 (0.6317)
+2022-11-18 16:08:51,883:INFO: Dataset: univ                Batch: 1/3	Loss 0.1051 (0.1051)
+2022-11-18 16:08:51,885:INFO: Dataset: univ                Batch: 2/3	Loss 0.1054 (0.1053)
+2022-11-18 16:08:51,886:INFO: Dataset: univ                Batch: 3/3	Loss 0.1065 (0.1056)
+2022-11-18 16:08:52,116:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2675 (0.2675)
+2022-11-18 16:08:52,120:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2660 (0.2671)
+2022-11-18 16:08:52,364:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1434 (0.1434)
+2022-11-18 16:08:52,366:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1458 (0.1446)
+2022-11-18 16:08:52,369:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1461 (0.1451)
+2022-11-18 16:08:52,370:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1475 (0.1457)
+2022-11-18 16:08:52,378:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1447 (0.1455)
+2022-11-18 16:08:52,438:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_763.pth.tar
+2022-11-18 16:08:52,438:INFO: 
+===> EPOCH: 764 (P4)
+2022-11-18 16:08:52,439:INFO: - Computing loss (training)
+2022-11-18 16:08:52,673:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6261 (0.6261)
+2022-11-18 16:08:52,678:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6328 (0.6294)
+2022-11-18 16:08:52,681:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6393 (0.6328)
+2022-11-18 16:08:52,684:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6406 (0.6341)
+2022-11-18 16:08:52,938:INFO: Dataset: univ                Batch:  1/15	Loss 0.1026 (0.1026)
+2022-11-18 16:08:52,943:INFO: Dataset: univ                Batch:  2/15	Loss 0.1029 (0.1027)
+2022-11-18 16:08:52,992:INFO: Dataset: univ                Batch:  3/15	Loss 0.1039 (0.1031)
+2022-11-18 16:08:52,994:INFO: Dataset: univ                Batch:  4/15	Loss 0.1039 (0.1033)
+2022-11-18 16:08:52,995:INFO: Dataset: univ                Batch:  5/15	Loss 0.1042 (0.1035)
+2022-11-18 16:08:52,998:INFO: Dataset: univ                Batch:  6/15	Loss 0.1051 (0.1037)
+2022-11-18 16:08:52,999:INFO: Dataset: univ                Batch:  7/15	Loss 0.1039 (0.1038)
+2022-11-18 16:08:53,000:INFO: Dataset: univ                Batch:  8/15	Loss 0.1049 (0.1039)
+2022-11-18 16:08:53,001:INFO: Dataset: univ                Batch:  9/15	Loss 0.1030 (0.1038)
+2022-11-18 16:08:53,003:INFO: Dataset: univ                Batch: 10/15	Loss 0.1033 (0.1037)
+2022-11-18 16:08:53,005:INFO: Dataset: univ                Batch: 11/15	Loss 0.1048 (0.1038)
+2022-11-18 16:08:53,007:INFO: Dataset: univ                Batch: 12/15	Loss 0.1040 (0.1038)
+2022-11-18 16:08:53,008:INFO: Dataset: univ                Batch: 13/15	Loss 0.1049 (0.1039)
+2022-11-18 16:08:53,010:INFO: Dataset: univ                Batch: 14/15	Loss 0.1029 (0.1038)
+2022-11-18 16:08:53,011:INFO: Dataset: univ                Batch: 15/15	Loss 0.1093 (0.1039)
+2022-11-18 16:08:53,251:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2676 (0.2676)
+2022-11-18 16:08:53,255:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2678 (0.2677)
+2022-11-18 16:08:53,272:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2672 (0.2676)
+2022-11-18 16:08:53,273:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2654 (0.2670)
+2022-11-18 16:08:53,275:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2695 (0.2676)
+2022-11-18 16:08:53,314:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2663 (0.2674)
+2022-11-18 16:08:53,315:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2664 (0.2672)
+2022-11-18 16:08:53,316:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2642 (0.2669)
+2022-11-18 16:08:53,623:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1415 (0.1415)
+2022-11-18 16:08:53,625:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1455 (0.1435)
+2022-11-18 16:08:53,628:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1442 (0.1437)
+2022-11-18 16:08:53,630:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1427 (0.1434)
+2022-11-18 16:08:53,668:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1484 (0.1444)
+2022-11-18 16:08:53,673:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1432 (0.1442)
+2022-11-18 16:08:53,674:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1421 (0.1439)
+2022-11-18 16:08:53,675:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1402 (0.1435)
+2022-11-18 16:08:53,676:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1427 (0.1434)
+2022-11-18 16:08:53,677:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1459 (0.1437)
+2022-11-18 16:08:53,680:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1412 (0.1435)
+2022-11-18 16:08:53,681:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1413 (0.1433)
+2022-11-18 16:08:53,683:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1416 (0.1432)
+2022-11-18 16:08:53,684:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1400 (0.1429)
+2022-11-18 16:08:53,685:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1405 (0.1427)
+2022-11-18 16:08:53,686:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1401 (0.1426)
+2022-11-18 16:08:53,688:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1389 (0.1424)
+2022-11-18 16:08:53,690:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1422 (0.1424)
+2022-11-18 16:08:53,733:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:54,070:INFO: 		 ADE on eth                       dataset:	 1.810306191444397
+2022-11-18 16:08:54,071:INFO: Average validation o:	ADE  1.8103	FDE  2.7693
+2022-11-18 16:08:54,072:INFO: - Computing loss (validation)
+2022-11-18 16:08:54,321:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6308 (0.6308)
+2022-11-18 16:08:54,322:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6388 (0.6313)
+2022-11-18 16:08:54,560:INFO: Dataset: univ                Batch: 1/3	Loss 0.1066 (0.1066)
+2022-11-18 16:08:54,562:INFO: Dataset: univ                Batch: 2/3	Loss 0.1042 (0.1052)
+2022-11-18 16:08:54,564:INFO: Dataset: univ                Batch: 3/3	Loss 0.1041 (0.1049)
+2022-11-18 16:08:54,804:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2669 (0.2669)
+2022-11-18 16:08:54,806:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2655 (0.2665)
+2022-11-18 16:08:55,049:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1441 (0.1441)
+2022-11-18 16:08:55,050:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1436 (0.1439)
+2022-11-18 16:08:55,052:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1434 (0.1437)
+2022-11-18 16:08:55,054:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1445 (0.1439)
+2022-11-18 16:08:55,055:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1449 (0.1441)
+2022-11-18 16:08:55,111:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_764.pth.tar
+2022-11-18 16:08:55,111:INFO: 
+===> EPOCH: 765 (P4)
+2022-11-18 16:08:55,112:INFO: - Computing loss (training)
+2022-11-18 16:08:55,319:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6343 (0.6343)
+2022-11-18 16:08:55,322:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6272 (0.6309)
+2022-11-18 16:08:55,324:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6350 (0.6323)
+2022-11-18 16:08:55,326:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6373 (0.6331)
+2022-11-18 16:08:55,580:INFO: Dataset: univ                Batch:  1/15	Loss 0.1027 (0.1027)
+2022-11-18 16:08:55,583:INFO: Dataset: univ                Batch:  2/15	Loss 0.1034 (0.1030)
+2022-11-18 16:08:55,585:INFO: Dataset: univ                Batch:  3/15	Loss 0.1035 (0.1032)
+2022-11-18 16:08:55,589:INFO: Dataset: univ                Batch:  4/15	Loss 0.1039 (0.1033)
+2022-11-18 16:08:55,635:INFO: Dataset: univ                Batch:  5/15	Loss 0.1033 (0.1033)
+2022-11-18 16:08:55,638:INFO: Dataset: univ                Batch:  6/15	Loss 0.1029 (0.1033)
+2022-11-18 16:08:55,640:INFO: Dataset: univ                Batch:  7/15	Loss 0.1034 (0.1033)
+2022-11-18 16:08:55,641:INFO: Dataset: univ                Batch:  8/15	Loss 0.1043 (0.1034)
+2022-11-18 16:08:55,643:INFO: Dataset: univ                Batch:  9/15	Loss 0.1043 (0.1035)
+2022-11-18 16:08:55,644:INFO: Dataset: univ                Batch: 10/15	Loss 0.1043 (0.1036)
+2022-11-18 16:08:55,647:INFO: Dataset: univ                Batch: 11/15	Loss 0.1035 (0.1036)
+2022-11-18 16:08:55,648:INFO: Dataset: univ                Batch: 12/15	Loss 0.1031 (0.1035)
+2022-11-18 16:08:55,649:INFO: Dataset: univ                Batch: 13/15	Loss 0.1022 (0.1034)
+2022-11-18 16:08:55,650:INFO: Dataset: univ                Batch: 14/15	Loss 0.1018 (0.1033)
+2022-11-18 16:08:55,652:INFO: Dataset: univ                Batch: 15/15	Loss 0.1021 (0.1033)
+2022-11-18 16:08:55,893:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2668 (0.2668)
+2022-11-18 16:08:55,897:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2665 (0.2666)
+2022-11-18 16:08:55,900:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2677 (0.2670)
+2022-11-18 16:08:55,902:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2679 (0.2672)
+2022-11-18 16:08:55,906:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2650 (0.2668)
+2022-11-18 16:08:55,941:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2655 (0.2666)
+2022-11-18 16:08:55,945:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2662 (0.2666)
+2022-11-18 16:08:55,949:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2650 (0.2664)
+2022-11-18 16:08:56,208:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1399 (0.1399)
+2022-11-18 16:08:56,210:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1414 (0.1407)
+2022-11-18 16:08:56,215:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1418 (0.1411)
+2022-11-18 16:08:56,252:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1403 (0.1409)
+2022-11-18 16:08:56,253:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1425 (0.1412)
+2022-11-18 16:08:56,282:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1438 (0.1417)
+2022-11-18 16:08:56,284:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1422 (0.1418)
+2022-11-18 16:08:56,285:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1387 (0.1414)
+2022-11-18 16:08:56,286:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1421 (0.1415)
+2022-11-18 16:08:56,287:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1429 (0.1416)
+2022-11-18 16:08:56,288:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1432 (0.1418)
+2022-11-18 16:08:56,290:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1425 (0.1418)
+2022-11-18 16:08:56,291:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1419 (0.1418)
+2022-11-18 16:08:56,293:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1412 (0.1418)
+2022-11-18 16:08:56,294:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1375 (0.1415)
+2022-11-18 16:08:56,295:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1374 (0.1412)
+2022-11-18 16:08:56,296:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1411 (0.1412)
+2022-11-18 16:08:56,298:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1381 (0.1411)
+2022-11-18 16:08:56,343:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:56,630:INFO: 		 ADE on eth                       dataset:	 1.7996829748153687
+2022-11-18 16:08:56,630:INFO: Average validation o:	ADE  1.7997	FDE  2.7207
+2022-11-18 16:08:56,631:INFO: - Computing loss (validation)
+2022-11-18 16:08:56,811:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6302 (0.6302)
+2022-11-18 16:08:56,812:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6377 (0.6307)
+2022-11-18 16:08:57,089:INFO: Dataset: univ                Batch: 1/3	Loss 0.1030 (0.1030)
+2022-11-18 16:08:57,093:INFO: Dataset: univ                Batch: 2/3	Loss 0.1058 (0.1043)
+2022-11-18 16:08:57,094:INFO: Dataset: univ                Batch: 3/3	Loss 0.1038 (0.1042)
+2022-11-18 16:08:57,342:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2662 (0.2662)
+2022-11-18 16:08:57,343:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2656 (0.2660)
+2022-11-18 16:08:57,612:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1403 (0.1403)
+2022-11-18 16:08:57,613:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1431 (0.1416)
+2022-11-18 16:08:57,614:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1448 (0.1427)
+2022-11-18 16:08:57,615:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1418 (0.1424)
+2022-11-18 16:08:57,616:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1446 (0.1429)
+2022-11-18 16:08:57,683:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_765.pth.tar
+2022-11-18 16:08:57,683:INFO: 
+===> EPOCH: 766 (P4)
+2022-11-18 16:08:57,684:INFO: - Computing loss (training)
+2022-11-18 16:08:57,872:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6346 (0.6346)
+2022-11-18 16:08:57,887:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6319 (0.6331)
+2022-11-18 16:08:57,895:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6352 (0.6338)
+2022-11-18 16:08:57,897:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6246 (0.6322)
+2022-11-18 16:08:58,192:INFO: Dataset: univ                Batch:  1/15	Loss 0.1022 (0.1022)
+2022-11-18 16:08:58,200:INFO: Dataset: univ                Batch:  2/15	Loss 0.1032 (0.1027)
+2022-11-18 16:08:58,237:INFO: Dataset: univ                Batch:  3/15	Loss 0.1024 (0.1026)
+2022-11-18 16:08:58,239:INFO: Dataset: univ                Batch:  4/15	Loss 0.1028 (0.1026)
+2022-11-18 16:08:58,240:INFO: Dataset: univ                Batch:  5/15	Loss 0.1046 (0.1030)
+2022-11-18 16:08:58,246:INFO: Dataset: univ                Batch:  6/15	Loss 0.1016 (0.1028)
+2022-11-18 16:08:58,247:INFO: Dataset: univ                Batch:  7/15	Loss 0.1029 (0.1028)
+2022-11-18 16:08:58,249:INFO: Dataset: univ                Batch:  8/15	Loss 0.1036 (0.1029)
+2022-11-18 16:08:58,251:INFO: Dataset: univ                Batch:  9/15	Loss 0.1030 (0.1029)
+2022-11-18 16:08:58,252:INFO: Dataset: univ                Batch: 10/15	Loss 0.1034 (0.1030)
+2022-11-18 16:08:58,254:INFO: Dataset: univ                Batch: 11/15	Loss 0.1020 (0.1029)
+2022-11-18 16:08:58,256:INFO: Dataset: univ                Batch: 12/15	Loss 0.1028 (0.1028)
+2022-11-18 16:08:58,258:INFO: Dataset: univ                Batch: 13/15	Loss 0.1032 (0.1029)
+2022-11-18 16:08:58,259:INFO: Dataset: univ                Batch: 14/15	Loss 0.1017 (0.1028)
+2022-11-18 16:08:58,261:INFO: Dataset: univ                Batch: 15/15	Loss 0.1009 (0.1027)
+2022-11-18 16:08:58,507:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2650 (0.2650)
+2022-11-18 16:08:58,509:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2686 (0.2667)
+2022-11-18 16:08:58,514:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2682 (0.2672)
+2022-11-18 16:08:58,517:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2640 (0.2663)
+2022-11-18 16:08:58,518:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2648 (0.2660)
+2022-11-18 16:08:58,578:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2678 (0.2663)
+2022-11-18 16:08:58,583:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2658 (0.2662)
+2022-11-18 16:08:58,587:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2638 (0.2660)
+2022-11-18 16:08:58,872:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1390 (0.1390)
+2022-11-18 16:08:58,885:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1401 (0.1396)
+2022-11-18 16:08:58,888:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1398 (0.1396)
+2022-11-18 16:08:58,890:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1398 (0.1397)
+2022-11-18 16:08:58,913:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1421 (0.1401)
+2022-11-18 16:08:58,928:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1401 (0.1401)
+2022-11-18 16:08:58,930:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1386 (0.1399)
+2022-11-18 16:08:58,931:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1403 (0.1399)
+2022-11-18 16:08:58,933:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1386 (0.1398)
+2022-11-18 16:08:58,935:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1421 (0.1400)
+2022-11-18 16:08:58,936:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1434 (0.1403)
+2022-11-18 16:08:58,940:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1380 (0.1401)
+2022-11-18 16:08:58,942:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1414 (0.1402)
+2022-11-18 16:08:58,943:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1394 (0.1401)
+2022-11-18 16:08:58,945:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1393 (0.1401)
+2022-11-18 16:08:58,947:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1413 (0.1402)
+2022-11-18 16:08:58,948:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1362 (0.1399)
+2022-11-18 16:08:58,951:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1405 (0.1399)
+2022-11-18 16:08:59,008:INFO: - Computing ADE (validation o)
+2022-11-18 16:08:59,339:INFO: 		 ADE on eth                       dataset:	 1.8023391962051392
+2022-11-18 16:08:59,340:INFO: Average validation o:	ADE  1.8023	FDE  2.7437
+2022-11-18 16:08:59,341:INFO: - Computing loss (validation)
+2022-11-18 16:08:59,544:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6300 (0.6300)
+2022-11-18 16:08:59,545:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6356 (0.6304)
+2022-11-18 16:08:59,822:INFO: Dataset: univ                Batch: 1/3	Loss 0.1043 (0.1043)
+2022-11-18 16:08:59,825:INFO: Dataset: univ                Batch: 2/3	Loss 0.1041 (0.1042)
+2022-11-18 16:08:59,832:INFO: Dataset: univ                Batch: 3/3	Loss 0.1023 (0.1036)
+2022-11-18 16:09:00,086:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2664 (0.2664)
+2022-11-18 16:09:00,087:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2632 (0.2655)
+2022-11-18 16:09:00,356:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1404 (0.1404)
+2022-11-18 16:09:00,358:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1400 (0.1402)
+2022-11-18 16:09:00,360:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1417 (0.1407)
+2022-11-18 16:09:00,362:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1438 (0.1415)
+2022-11-18 16:09:00,367:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1424 (0.1417)
+2022-11-18 16:09:00,427:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_766.pth.tar
+2022-11-18 16:09:00,427:INFO: 
+===> EPOCH: 767 (P4)
+2022-11-18 16:09:00,428:INFO: - Computing loss (training)
+2022-11-18 16:09:00,623:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6315 (0.6315)
+2022-11-18 16:09:00,632:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6348 (0.6331)
+2022-11-18 16:09:00,633:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6265 (0.6309)
+2022-11-18 16:09:00,634:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6339 (0.6314)
+2022-11-18 16:09:00,863:INFO: Dataset: univ                Batch:  1/15	Loss 0.1021 (0.1021)
+2022-11-18 16:09:00,869:INFO: Dataset: univ                Batch:  2/15	Loss 0.1027 (0.1024)
+2022-11-18 16:09:00,872:INFO: Dataset: univ                Batch:  3/15	Loss 0.1011 (0.1019)
+2022-11-18 16:09:00,873:INFO: Dataset: univ                Batch:  4/15	Loss 0.1036 (0.1024)
+2022-11-18 16:09:00,913:INFO: Dataset: univ                Batch:  5/15	Loss 0.1033 (0.1025)
+2022-11-18 16:09:00,931:INFO: Dataset: univ                Batch:  6/15	Loss 0.1031 (0.1026)
+2022-11-18 16:09:00,932:INFO: Dataset: univ                Batch:  7/15	Loss 0.1030 (0.1027)
+2022-11-18 16:09:00,934:INFO: Dataset: univ                Batch:  8/15	Loss 0.1023 (0.1026)
+2022-11-18 16:09:00,935:INFO: Dataset: univ                Batch:  9/15	Loss 0.1027 (0.1026)
+2022-11-18 16:09:00,937:INFO: Dataset: univ                Batch: 10/15	Loss 0.1016 (0.1025)
+2022-11-18 16:09:00,938:INFO: Dataset: univ                Batch: 11/15	Loss 0.1033 (0.1026)
+2022-11-18 16:09:00,940:INFO: Dataset: univ                Batch: 12/15	Loss 0.1010 (0.1025)
+2022-11-18 16:09:00,941:INFO: Dataset: univ                Batch: 13/15	Loss 0.1013 (0.1024)
+2022-11-18 16:09:00,943:INFO: Dataset: univ                Batch: 14/15	Loss 0.1017 (0.1023)
+2022-11-18 16:09:00,944:INFO: Dataset: univ                Batch: 15/15	Loss 0.0984 (0.1023)
+2022-11-18 16:09:01,198:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2635 (0.2635)
+2022-11-18 16:09:01,201:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2664 (0.2649)
+2022-11-18 16:09:01,203:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2710 (0.2666)
+2022-11-18 16:09:01,207:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2669 (0.2667)
+2022-11-18 16:09:01,208:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2630 (0.2659)
+2022-11-18 16:09:01,239:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2656 (0.2659)
+2022-11-18 16:09:01,240:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2647 (0.2657)
+2022-11-18 16:09:01,241:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2639 (0.2655)
+2022-11-18 16:09:01,488:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1407 (0.1407)
+2022-11-18 16:09:01,493:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1371 (0.1388)
+2022-11-18 16:09:01,536:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1406 (0.1394)
+2022-11-18 16:09:01,539:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1395 (0.1394)
+2022-11-18 16:09:01,540:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1396 (0.1394)
+2022-11-18 16:09:01,559:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1379 (0.1392)
+2022-11-18 16:09:01,560:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1413 (0.1395)
+2022-11-18 16:09:01,561:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1392 (0.1394)
+2022-11-18 16:09:01,562:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1401 (0.1395)
+2022-11-18 16:09:01,563:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1435 (0.1399)
+2022-11-18 16:09:01,565:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1376 (0.1397)
+2022-11-18 16:09:01,566:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1362 (0.1394)
+2022-11-18 16:09:01,568:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1365 (0.1392)
+2022-11-18 16:09:01,569:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1380 (0.1391)
+2022-11-18 16:09:01,570:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1406 (0.1392)
+2022-11-18 16:09:01,571:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1383 (0.1392)
+2022-11-18 16:09:01,572:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1379 (0.1391)
+2022-11-18 16:09:01,574:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1343 (0.1388)
+2022-11-18 16:09:01,617:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:01,896:INFO: 		 ADE on eth                       dataset:	 1.817631483078003
+2022-11-18 16:09:01,896:INFO: Average validation o:	ADE  1.8176	FDE  2.7326
+2022-11-18 16:09:01,897:INFO: - Computing loss (validation)
+2022-11-18 16:09:02,095:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6308 (0.6308)
+2022-11-18 16:09:02,096:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6220 (0.6300)
+2022-11-18 16:09:02,377:INFO: Dataset: univ                Batch: 1/3	Loss 0.1030 (0.1030)
+2022-11-18 16:09:02,378:INFO: Dataset: univ                Batch: 2/3	Loss 0.1026 (0.1028)
+2022-11-18 16:09:02,379:INFO: Dataset: univ                Batch: 3/3	Loss 0.1033 (0.1030)
+2022-11-18 16:09:02,622:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2647 (0.2647)
+2022-11-18 16:09:02,625:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2665 (0.2651)
+2022-11-18 16:09:02,900:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1415 (0.1415)
+2022-11-18 16:09:02,902:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1393 (0.1404)
+2022-11-18 16:09:02,904:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1427 (0.1412)
+2022-11-18 16:09:02,905:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1388 (0.1406)
+2022-11-18 16:09:02,907:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1405 (0.1406)
+2022-11-18 16:09:02,969:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_767.pth.tar
+2022-11-18 16:09:02,970:INFO: 
+===> EPOCH: 768 (P4)
+2022-11-18 16:09:02,970:INFO: - Computing loss (training)
+2022-11-18 16:09:03,179:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6337 (0.6337)
+2022-11-18 16:09:03,185:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6296 (0.6317)
+2022-11-18 16:09:03,188:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6273 (0.6302)
+2022-11-18 16:09:03,190:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6329 (0.6306)
+2022-11-18 16:09:03,452:INFO: Dataset: univ                Batch:  1/15	Loss 0.1011 (0.1011)
+2022-11-18 16:09:03,459:INFO: Dataset: univ                Batch:  2/15	Loss 0.1023 (0.1017)
+2022-11-18 16:09:03,462:INFO: Dataset: univ                Batch:  3/15	Loss 0.1026 (0.1020)
+2022-11-18 16:09:03,467:INFO: Dataset: univ                Batch:  4/15	Loss 0.1015 (0.1019)
+2022-11-18 16:09:03,519:INFO: Dataset: univ                Batch:  5/15	Loss 0.1019 (0.1019)
+2022-11-18 16:09:03,550:INFO: Dataset: univ                Batch:  6/15	Loss 0.1027 (0.1020)
+2022-11-18 16:09:03,555:INFO: Dataset: univ                Batch:  7/15	Loss 0.1025 (0.1021)
+2022-11-18 16:09:03,559:INFO: Dataset: univ                Batch:  8/15	Loss 0.1027 (0.1022)
+2022-11-18 16:09:03,564:INFO: Dataset: univ                Batch:  9/15	Loss 0.1018 (0.1021)
+2022-11-18 16:09:03,568:INFO: Dataset: univ                Batch: 10/15	Loss 0.1012 (0.1020)
+2022-11-18 16:09:03,572:INFO: Dataset: univ                Batch: 11/15	Loss 0.1034 (0.1021)
+2022-11-18 16:09:03,577:INFO: Dataset: univ                Batch: 12/15	Loss 0.1010 (0.1020)
+2022-11-18 16:09:03,579:INFO: Dataset: univ                Batch: 13/15	Loss 0.1016 (0.1020)
+2022-11-18 16:09:03,580:INFO: Dataset: univ                Batch: 14/15	Loss 0.0997 (0.1018)
+2022-11-18 16:09:03,581:INFO: Dataset: univ                Batch: 15/15	Loss 0.0996 (0.1018)
+2022-11-18 16:09:03,819:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2652 (0.2652)
+2022-11-18 16:09:03,821:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2660 (0.2656)
+2022-11-18 16:09:03,823:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2660 (0.2658)
+2022-11-18 16:09:03,825:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2645 (0.2655)
+2022-11-18 16:09:03,827:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2644 (0.2653)
+2022-11-18 16:09:03,866:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2657 (0.2653)
+2022-11-18 16:09:03,870:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2649 (0.2653)
+2022-11-18 16:09:03,875:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2635 (0.2651)
+2022-11-18 16:09:04,123:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1408 (0.1408)
+2022-11-18 16:09:04,127:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1381 (0.1395)
+2022-11-18 16:09:04,131:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1387 (0.1392)
+2022-11-18 16:09:04,133:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1387 (0.1391)
+2022-11-18 16:09:04,166:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1398 (0.1392)
+2022-11-18 16:09:04,176:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1373 (0.1389)
+2022-11-18 16:09:04,177:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1407 (0.1392)
+2022-11-18 16:09:04,178:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1388 (0.1391)
+2022-11-18 16:09:04,179:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1371 (0.1389)
+2022-11-18 16:09:04,181:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1377 (0.1388)
+2022-11-18 16:09:04,182:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1418 (0.1391)
+2022-11-18 16:09:04,184:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1370 (0.1389)
+2022-11-18 16:09:04,185:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1372 (0.1387)
+2022-11-18 16:09:04,186:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1353 (0.1385)
+2022-11-18 16:09:04,187:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1377 (0.1384)
+2022-11-18 16:09:04,189:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1364 (0.1383)
+2022-11-18 16:09:04,190:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1322 (0.1379)
+2022-11-18 16:09:04,192:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1366 (0.1379)
+2022-11-18 16:09:04,239:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:04,523:INFO: 		 ADE on eth                       dataset:	 1.7812539339065552
+2022-11-18 16:09:04,524:INFO: Average validation o:	ADE  1.7813	FDE  2.6702
+2022-11-18 16:09:04,525:INFO: - Computing loss (validation)
+2022-11-18 16:09:04,709:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6268 (0.6268)
+2022-11-18 16:09:04,710:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6616 (0.6296)
+2022-11-18 16:09:04,952:INFO: Dataset: univ                Batch: 1/3	Loss 0.1014 (0.1014)
+2022-11-18 16:09:04,954:INFO: Dataset: univ                Batch: 2/3	Loss 0.1024 (0.1019)
+2022-11-18 16:09:04,959:INFO: Dataset: univ                Batch: 3/3	Loss 0.1041 (0.1024)
+2022-11-18 16:09:05,211:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2653 (0.2653)
+2022-11-18 16:09:05,211:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2629 (0.2646)
+2022-11-18 16:09:05,453:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1381 (0.1381)
+2022-11-18 16:09:05,454:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1410 (0.1395)
+2022-11-18 16:09:05,454:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1408 (0.1399)
+2022-11-18 16:09:05,457:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1383 (0.1395)
+2022-11-18 16:09:05,458:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1396 (0.1396)
+2022-11-18 16:09:05,513:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_768.pth.tar
+2022-11-18 16:09:05,513:INFO: 
+===> EPOCH: 769 (P4)
+2022-11-18 16:09:05,514:INFO: - Computing loss (training)
+2022-11-18 16:09:05,705:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6363 (0.6363)
+2022-11-18 16:09:05,726:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6220 (0.6289)
+2022-11-18 16:09:05,728:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6324 (0.6301)
+2022-11-18 16:09:05,729:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6286 (0.6298)
+2022-11-18 16:09:05,968:INFO: Dataset: univ                Batch:  1/15	Loss 0.1005 (0.1005)
+2022-11-18 16:09:05,972:INFO: Dataset: univ                Batch:  2/15	Loss 0.1029 (0.1016)
+2022-11-18 16:09:05,974:INFO: Dataset: univ                Batch:  3/15	Loss 0.1011 (0.1014)
+2022-11-18 16:09:05,979:INFO: Dataset: univ                Batch:  4/15	Loss 0.1028 (0.1018)
+2022-11-18 16:09:06,012:INFO: Dataset: univ                Batch:  5/15	Loss 0.1022 (0.1019)
+2022-11-18 16:09:06,018:INFO: Dataset: univ                Batch:  6/15	Loss 0.1013 (0.1018)
+2022-11-18 16:09:06,020:INFO: Dataset: univ                Batch:  7/15	Loss 0.1015 (0.1017)
+2022-11-18 16:09:06,021:INFO: Dataset: univ                Batch:  8/15	Loss 0.1014 (0.1017)
+2022-11-18 16:09:06,022:INFO: Dataset: univ                Batch:  9/15	Loss 0.1031 (0.1018)
+2022-11-18 16:09:06,024:INFO: Dataset: univ                Batch: 10/15	Loss 0.1004 (0.1017)
+2022-11-18 16:09:06,027:INFO: Dataset: univ                Batch: 11/15	Loss 0.1010 (0.1016)
+2022-11-18 16:09:06,028:INFO: Dataset: univ                Batch: 12/15	Loss 0.1015 (0.1016)
+2022-11-18 16:09:06,029:INFO: Dataset: univ                Batch: 13/15	Loss 0.1004 (0.1015)
+2022-11-18 16:09:06,031:INFO: Dataset: univ                Batch: 14/15	Loss 0.1000 (0.1014)
+2022-11-18 16:09:06,032:INFO: Dataset: univ                Batch: 15/15	Loss 0.1013 (0.1014)
+2022-11-18 16:09:06,293:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2677 (0.2677)
+2022-11-18 16:09:06,297:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2640 (0.2657)
+2022-11-18 16:09:06,303:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2681 (0.2665)
+2022-11-18 16:09:06,305:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2652 (0.2662)
+2022-11-18 16:09:06,306:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2612 (0.2652)
+2022-11-18 16:09:06,338:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2651 (0.2652)
+2022-11-18 16:09:06,340:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2627 (0.2649)
+2022-11-18 16:09:06,341:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2637 (0.2647)
+2022-11-18 16:09:06,637:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1380 (0.1380)
+2022-11-18 16:09:06,639:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1348 (0.1365)
+2022-11-18 16:09:06,640:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1354 (0.1362)
+2022-11-18 16:09:06,643:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1400 (0.1372)
+2022-11-18 16:09:06,644:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1396 (0.1376)
+2022-11-18 16:09:06,650:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1350 (0.1372)
+2022-11-18 16:09:06,651:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1367 (0.1372)
+2022-11-18 16:09:06,652:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1389 (0.1374)
+2022-11-18 16:09:06,653:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1403 (0.1377)
+2022-11-18 16:09:06,654:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1379 (0.1377)
+2022-11-18 16:09:06,655:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1349 (0.1375)
+2022-11-18 16:09:06,657:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1350 (0.1372)
+2022-11-18 16:09:06,659:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1364 (0.1372)
+2022-11-18 16:09:06,660:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1403 (0.1374)
+2022-11-18 16:09:06,661:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1353 (0.1373)
+2022-11-18 16:09:06,662:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1348 (0.1371)
+2022-11-18 16:09:06,664:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1357 (0.1370)
+2022-11-18 16:09:06,665:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1356 (0.1370)
+2022-11-18 16:09:06,712:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:06,983:INFO: 		 ADE on eth                       dataset:	 1.807481050491333
+2022-11-18 16:09:06,984:INFO: Average validation o:	ADE  1.8075	FDE  2.7458
+2022-11-18 16:09:06,985:INFO: - Computing loss (validation)
+2022-11-18 16:09:07,157:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6269 (0.6269)
+2022-11-18 16:09:07,159:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6515 (0.6292)
+2022-11-18 16:09:07,389:INFO: Dataset: univ                Batch: 1/3	Loss 0.1027 (0.1027)
+2022-11-18 16:09:07,390:INFO: Dataset: univ                Batch: 2/3	Loss 0.1026 (0.1027)
+2022-11-18 16:09:07,393:INFO: Dataset: univ                Batch: 3/3	Loss 0.1006 (0.1020)
+2022-11-18 16:09:07,623:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2645 (0.2645)
+2022-11-18 16:09:07,627:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2633 (0.2642)
+2022-11-18 16:09:07,866:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1383 (0.1383)
+2022-11-18 16:09:07,869:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1405 (0.1395)
+2022-11-18 16:09:07,870:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1365 (0.1384)
+2022-11-18 16:09:07,872:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1417 (0.1393)
+2022-11-18 16:09:07,873:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1358 (0.1387)
+2022-11-18 16:09:07,929:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_769.pth.tar
+2022-11-18 16:09:07,929:INFO: 
+===> EPOCH: 770 (P4)
+2022-11-18 16:09:07,930:INFO: - Computing loss (training)
+2022-11-18 16:09:08,114:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6300 (0.6300)
+2022-11-18 16:09:08,117:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6283 (0.6292)
+2022-11-18 16:09:08,119:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6237 (0.6273)
+2022-11-18 16:09:08,122:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6369 (0.6289)
+2022-11-18 16:09:08,365:INFO: Dataset: univ                Batch:  1/15	Loss 0.0995 (0.0995)
+2022-11-18 16:09:08,367:INFO: Dataset: univ                Batch:  2/15	Loss 0.1017 (0.1005)
+2022-11-18 16:09:08,369:INFO: Dataset: univ                Batch:  3/15	Loss 0.1001 (0.1004)
+2022-11-18 16:09:08,370:INFO: Dataset: univ                Batch:  4/15	Loss 0.1010 (0.1005)
+2022-11-18 16:09:08,395:INFO: Dataset: univ                Batch:  5/15	Loss 0.1030 (0.1010)
+2022-11-18 16:09:08,436:INFO: Dataset: univ                Batch:  6/15	Loss 0.1017 (0.1011)
+2022-11-18 16:09:08,438:INFO: Dataset: univ                Batch:  7/15	Loss 0.1015 (0.1011)
+2022-11-18 16:09:08,439:INFO: Dataset: univ                Batch:  8/15	Loss 0.1012 (0.1012)
+2022-11-18 16:09:08,440:INFO: Dataset: univ                Batch:  9/15	Loss 0.1006 (0.1011)
+2022-11-18 16:09:08,442:INFO: Dataset: univ                Batch: 10/15	Loss 0.1015 (0.1011)
+2022-11-18 16:09:08,443:INFO: Dataset: univ                Batch: 11/15	Loss 0.1009 (0.1011)
+2022-11-18 16:09:08,448:INFO: Dataset: univ                Batch: 12/15	Loss 0.1008 (0.1011)
+2022-11-18 16:09:08,449:INFO: Dataset: univ                Batch: 13/15	Loss 0.1009 (0.1011)
+2022-11-18 16:09:08,450:INFO: Dataset: univ                Batch: 14/15	Loss 0.1012 (0.1011)
+2022-11-18 16:09:08,452:INFO: Dataset: univ                Batch: 15/15	Loss 0.0993 (0.1011)
+2022-11-18 16:09:08,723:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2647 (0.2647)
+2022-11-18 16:09:08,725:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2634 (0.2641)
+2022-11-18 16:09:08,727:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2656 (0.2646)
+2022-11-18 16:09:08,731:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2655 (0.2648)
+2022-11-18 16:09:08,733:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2638 (0.2646)
+2022-11-18 16:09:08,778:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2653 (0.2647)
+2022-11-18 16:09:08,783:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2634 (0.2645)
+2022-11-18 16:09:08,787:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2630 (0.2643)
+2022-11-18 16:09:09,063:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1346 (0.1346)
+2022-11-18 16:09:09,066:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1337 (0.1341)
+2022-11-18 16:09:09,069:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1379 (0.1355)
+2022-11-18 16:09:09,071:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1381 (0.1361)
+2022-11-18 16:09:09,130:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1386 (0.1366)
+2022-11-18 16:09:09,135:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1372 (0.1367)
+2022-11-18 16:09:09,136:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1377 (0.1369)
+2022-11-18 16:09:09,137:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1347 (0.1366)
+2022-11-18 16:09:09,139:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1372 (0.1367)
+2022-11-18 16:09:09,140:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1357 (0.1366)
+2022-11-18 16:09:09,142:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1380 (0.1367)
+2022-11-18 16:09:09,143:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1393 (0.1369)
+2022-11-18 16:09:09,145:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1368 (0.1369)
+2022-11-18 16:09:09,146:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1372 (0.1370)
+2022-11-18 16:09:09,147:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1329 (0.1367)
+2022-11-18 16:09:09,148:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1322 (0.1364)
+2022-11-18 16:09:09,150:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1343 (0.1363)
+2022-11-18 16:09:09,152:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1333 (0.1361)
+2022-11-18 16:09:09,197:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:09,467:INFO: 		 ADE on eth                       dataset:	 1.8445992469787598
+2022-11-18 16:09:09,467:INFO: Average validation o:	ADE  1.8446	FDE  2.8302
+2022-11-18 16:09:09,468:INFO: - Computing loss (validation)
+2022-11-18 16:09:09,651:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6303 (0.6303)
+2022-11-18 16:09:09,652:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6105 (0.6288)
+2022-11-18 16:09:09,887:INFO: Dataset: univ                Batch: 1/3	Loss 0.1024 (0.1024)
+2022-11-18 16:09:09,889:INFO: Dataset: univ                Batch: 2/3	Loss 0.1007 (0.1015)
+2022-11-18 16:09:09,903:INFO: Dataset: univ                Batch: 3/3	Loss 0.1017 (0.1015)
+2022-11-18 16:09:10,159:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2627 (0.2627)
+2022-11-18 16:09:10,160:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2677 (0.2638)
+2022-11-18 16:09:10,397:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1380 (0.1380)
+2022-11-18 16:09:10,398:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1365 (0.1372)
+2022-11-18 16:09:10,401:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1392 (0.1379)
+2022-11-18 16:09:10,402:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1394 (0.1382)
+2022-11-18 16:09:10,403:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1362 (0.1378)
+2022-11-18 16:09:10,461:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_770.pth.tar
+2022-11-18 16:09:10,461:INFO: 
+===> EPOCH: 771 (P4)
+2022-11-18 16:09:10,462:INFO: - Computing loss (training)
+2022-11-18 16:09:10,654:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6315 (0.6315)
+2022-11-18 16:09:10,657:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6279 (0.6297)
+2022-11-18 16:09:10,658:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6283 (0.6292)
+2022-11-18 16:09:10,661:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6226 (0.6280)
+2022-11-18 16:09:10,918:INFO: Dataset: univ                Batch:  1/15	Loss 0.1006 (0.1006)
+2022-11-18 16:09:10,921:INFO: Dataset: univ                Batch:  2/15	Loss 0.1003 (0.1004)
+2022-11-18 16:09:10,945:INFO: Dataset: univ                Batch:  3/15	Loss 0.1001 (0.1003)
+2022-11-18 16:09:10,947:INFO: Dataset: univ                Batch:  4/15	Loss 0.1019 (0.1007)
+2022-11-18 16:09:10,949:INFO: Dataset: univ                Batch:  5/15	Loss 0.1017 (0.1009)
+2022-11-18 16:09:10,963:INFO: Dataset: univ                Batch:  6/15	Loss 0.1015 (0.1010)
+2022-11-18 16:09:10,965:INFO: Dataset: univ                Batch:  7/15	Loss 0.1009 (0.1010)
+2022-11-18 16:09:10,966:INFO: Dataset: univ                Batch:  8/15	Loss 0.1007 (0.1009)
+2022-11-18 16:09:10,967:INFO: Dataset: univ                Batch:  9/15	Loss 0.1003 (0.1009)
+2022-11-18 16:09:10,968:INFO: Dataset: univ                Batch: 10/15	Loss 0.1010 (0.1009)
+2022-11-18 16:09:10,970:INFO: Dataset: univ                Batch: 11/15	Loss 0.1012 (0.1009)
+2022-11-18 16:09:10,972:INFO: Dataset: univ                Batch: 12/15	Loss 0.1007 (0.1009)
+2022-11-18 16:09:10,973:INFO: Dataset: univ                Batch: 13/15	Loss 0.1001 (0.1008)
+2022-11-18 16:09:10,974:INFO: Dataset: univ                Batch: 14/15	Loss 0.0995 (0.1007)
+2022-11-18 16:09:10,975:INFO: Dataset: univ                Batch: 15/15	Loss 0.0996 (0.1007)
+2022-11-18 16:09:11,222:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2610 (0.2610)
+2022-11-18 16:09:11,225:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2661 (0.2636)
+2022-11-18 16:09:11,226:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2656 (0.2643)
+2022-11-18 16:09:11,229:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2660 (0.2648)
+2022-11-18 16:09:11,230:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2642 (0.2647)
+2022-11-18 16:09:11,298:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2624 (0.2643)
+2022-11-18 16:09:11,303:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2632 (0.2641)
+2022-11-18 16:09:11,307:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2630 (0.2640)
+2022-11-18 16:09:11,601:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1325 (0.1325)
+2022-11-18 16:09:11,604:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1375 (0.1351)
+2022-11-18 16:09:11,608:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1332 (0.1345)
+2022-11-18 16:09:11,637:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1339 (0.1343)
+2022-11-18 16:09:11,638:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1347 (0.1344)
+2022-11-18 16:09:11,645:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1360 (0.1347)
+2022-11-18 16:09:11,646:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1369 (0.1350)
+2022-11-18 16:09:11,648:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1356 (0.1351)
+2022-11-18 16:09:11,649:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1361 (0.1352)
+2022-11-18 16:09:11,650:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1357 (0.1352)
+2022-11-18 16:09:11,652:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1344 (0.1352)
+2022-11-18 16:09:11,654:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1382 (0.1354)
+2022-11-18 16:09:11,655:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1360 (0.1355)
+2022-11-18 16:09:11,656:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1360 (0.1355)
+2022-11-18 16:09:11,658:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1386 (0.1357)
+2022-11-18 16:09:11,659:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1343 (0.1356)
+2022-11-18 16:09:11,660:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1324 (0.1354)
+2022-11-18 16:09:11,663:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1343 (0.1354)
+2022-11-18 16:09:11,720:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:11,997:INFO: 		 ADE on eth                       dataset:	 1.7944555282592773
+2022-11-18 16:09:11,998:INFO: Average validation o:	ADE  1.7945	FDE  2.6729
+2022-11-18 16:09:11,999:INFO: - Computing loss (validation)
+2022-11-18 16:09:12,188:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6280 (0.6280)
+2022-11-18 16:09:12,189:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6338 (0.6285)
+2022-11-18 16:09:12,448:INFO: Dataset: univ                Batch: 1/3	Loss 0.1012 (0.1012)
+2022-11-18 16:09:12,450:INFO: Dataset: univ                Batch: 2/3	Loss 0.1016 (0.1014)
+2022-11-18 16:09:12,451:INFO: Dataset: univ                Batch: 3/3	Loss 0.1006 (0.1011)
+2022-11-18 16:09:12,688:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2637 (0.2637)
+2022-11-18 16:09:12,695:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2627 (0.2634)
+2022-11-18 16:09:12,938:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1363 (0.1363)
+2022-11-18 16:09:12,939:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1369 (0.1366)
+2022-11-18 16:09:12,939:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1370 (0.1367)
+2022-11-18 16:09:12,941:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1379 (0.1370)
+2022-11-18 16:09:12,942:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1370 (0.1370)
+2022-11-18 16:09:12,998:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_771.pth.tar
+2022-11-18 16:09:12,998:INFO: 
+===> EPOCH: 772 (P4)
+2022-11-18 16:09:12,999:INFO: - Computing loss (training)
+2022-11-18 16:09:13,199:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6246 (0.6246)
+2022-11-18 16:09:13,202:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6252 (0.6249)
+2022-11-18 16:09:13,205:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6340 (0.6279)
+2022-11-18 16:09:13,206:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6243 (0.6273)
+2022-11-18 16:09:13,461:INFO: Dataset: univ                Batch:  1/15	Loss 0.1000 (0.1000)
+2022-11-18 16:09:13,465:INFO: Dataset: univ                Batch:  2/15	Loss 0.0996 (0.0998)
+2022-11-18 16:09:13,468:INFO: Dataset: univ                Batch:  3/15	Loss 0.1010 (0.1001)
+2022-11-18 16:09:13,470:INFO: Dataset: univ                Batch:  4/15	Loss 0.1015 (0.1005)
+2022-11-18 16:09:13,491:INFO: Dataset: univ                Batch:  5/15	Loss 0.1002 (0.1004)
+2022-11-18 16:09:13,494:INFO: Dataset: univ                Batch:  6/15	Loss 0.1012 (0.1005)
+2022-11-18 16:09:13,495:INFO: Dataset: univ                Batch:  7/15	Loss 0.1012 (0.1006)
+2022-11-18 16:09:13,496:INFO: Dataset: univ                Batch:  8/15	Loss 0.1008 (0.1006)
+2022-11-18 16:09:13,498:INFO: Dataset: univ                Batch:  9/15	Loss 0.1000 (0.1006)
+2022-11-18 16:09:13,499:INFO: Dataset: univ                Batch: 10/15	Loss 0.1011 (0.1006)
+2022-11-18 16:09:13,501:INFO: Dataset: univ                Batch: 11/15	Loss 0.1000 (0.1005)
+2022-11-18 16:09:13,503:INFO: Dataset: univ                Batch: 12/15	Loss 0.1017 (0.1006)
+2022-11-18 16:09:13,504:INFO: Dataset: univ                Batch: 13/15	Loss 0.0999 (0.1006)
+2022-11-18 16:09:13,506:INFO: Dataset: univ                Batch: 14/15	Loss 0.0988 (0.1004)
+2022-11-18 16:09:13,507:INFO: Dataset: univ                Batch: 15/15	Loss 0.0970 (0.1004)
+2022-11-18 16:09:13,766:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2643 (0.2643)
+2022-11-18 16:09:13,767:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2636 (0.2640)
+2022-11-18 16:09:13,769:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2631 (0.2637)
+2022-11-18 16:09:13,770:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2638 (0.2637)
+2022-11-18 16:09:13,772:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2641 (0.2638)
+2022-11-18 16:09:13,826:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2627 (0.2636)
+2022-11-18 16:09:13,830:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2634 (0.2636)
+2022-11-18 16:09:13,835:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2645 (0.2637)
+2022-11-18 16:09:14,099:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1327 (0.1327)
+2022-11-18 16:09:14,106:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1322 (0.1325)
+2022-11-18 16:09:14,108:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1337 (0.1329)
+2022-11-18 16:09:14,111:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1356 (0.1335)
+2022-11-18 16:09:14,155:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1357 (0.1340)
+2022-11-18 16:09:14,163:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1335 (0.1339)
+2022-11-18 16:09:14,165:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1356 (0.1341)
+2022-11-18 16:09:14,166:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1377 (0.1346)
+2022-11-18 16:09:14,167:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1379 (0.1350)
+2022-11-18 16:09:14,168:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1345 (0.1349)
+2022-11-18 16:09:14,169:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1354 (0.1350)
+2022-11-18 16:09:14,171:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1348 (0.1350)
+2022-11-18 16:09:14,172:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1324 (0.1348)
+2022-11-18 16:09:14,173:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1346 (0.1348)
+2022-11-18 16:09:14,174:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1343 (0.1347)
+2022-11-18 16:09:14,175:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1343 (0.1347)
+2022-11-18 16:09:14,177:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1346 (0.1347)
+2022-11-18 16:09:14,179:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1335 (0.1347)
+2022-11-18 16:09:14,222:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:14,492:INFO: 		 ADE on eth                       dataset:	 1.8102517127990723
+2022-11-18 16:09:14,493:INFO: Average validation o:	ADE  1.8103	FDE  2.7391
+2022-11-18 16:09:14,494:INFO: - Computing loss (validation)
+2022-11-18 16:09:14,675:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6291 (0.6291)
+2022-11-18 16:09:14,676:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6178 (0.6282)
+2022-11-18 16:09:14,921:INFO: Dataset: univ                Batch: 1/3	Loss 0.1030 (0.1030)
+2022-11-18 16:09:14,924:INFO: Dataset: univ                Batch: 2/3	Loss 0.0998 (0.1012)
+2022-11-18 16:09:14,926:INFO: Dataset: univ                Batch: 3/3	Loss 0.0997 (0.1007)
+2022-11-18 16:09:15,169:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2632 (0.2632)
+2022-11-18 16:09:15,171:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2627 (0.2630)
+2022-11-18 16:09:15,440:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1369 (0.1369)
+2022-11-18 16:09:15,447:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1342 (0.1355)
+2022-11-18 16:09:15,448:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1369 (0.1360)
+2022-11-18 16:09:15,449:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1357 (0.1359)
+2022-11-18 16:09:15,451:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1377 (0.1363)
+2022-11-18 16:09:15,512:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_772.pth.tar
+2022-11-18 16:09:15,513:INFO: 
+===> EPOCH: 773 (P4)
+2022-11-18 16:09:15,513:INFO: - Computing loss (training)
+2022-11-18 16:09:15,696:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6277 (0.6277)
+2022-11-18 16:09:15,700:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6210 (0.6242)
+2022-11-18 16:09:15,702:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6296 (0.6260)
+2022-11-18 16:09:15,716:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6294 (0.6266)
+2022-11-18 16:09:15,965:INFO: Dataset: univ                Batch:  1/15	Loss 0.0995 (0.0995)
+2022-11-18 16:09:15,980:INFO: Dataset: univ                Batch:  2/15	Loss 0.1002 (0.0998)
+2022-11-18 16:09:15,982:INFO: Dataset: univ                Batch:  3/15	Loss 0.1002 (0.1000)
+2022-11-18 16:09:15,983:INFO: Dataset: univ                Batch:  4/15	Loss 0.0999 (0.1000)
+2022-11-18 16:09:16,021:INFO: Dataset: univ                Batch:  5/15	Loss 0.1008 (0.1001)
+2022-11-18 16:09:16,026:INFO: Dataset: univ                Batch:  6/15	Loss 0.0995 (0.1000)
+2022-11-18 16:09:16,027:INFO: Dataset: univ                Batch:  7/15	Loss 0.1010 (0.1002)
+2022-11-18 16:09:16,029:INFO: Dataset: univ                Batch:  8/15	Loss 0.1004 (0.1002)
+2022-11-18 16:09:16,030:INFO: Dataset: univ                Batch:  9/15	Loss 0.1012 (0.1003)
+2022-11-18 16:09:16,031:INFO: Dataset: univ                Batch: 10/15	Loss 0.1004 (0.1003)
+2022-11-18 16:09:16,033:INFO: Dataset: univ                Batch: 11/15	Loss 0.0994 (0.1002)
+2022-11-18 16:09:16,035:INFO: Dataset: univ                Batch: 12/15	Loss 0.1002 (0.1002)
+2022-11-18 16:09:16,037:INFO: Dataset: univ                Batch: 13/15	Loss 0.0985 (0.1000)
+2022-11-18 16:09:16,038:INFO: Dataset: univ                Batch: 14/15	Loss 0.1012 (0.1001)
+2022-11-18 16:09:16,039:INFO: Dataset: univ                Batch: 15/15	Loss 0.0983 (0.1001)
+2022-11-18 16:09:16,281:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2647 (0.2647)
+2022-11-18 16:09:16,283:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2647 (0.2647)
+2022-11-18 16:09:16,285:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2652 (0.2649)
+2022-11-18 16:09:16,286:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2642 (0.2647)
+2022-11-18 16:09:16,288:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2633 (0.2644)
+2022-11-18 16:09:16,329:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2621 (0.2640)
+2022-11-18 16:09:16,334:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2618 (0.2637)
+2022-11-18 16:09:16,338:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2603 (0.2634)
+2022-11-18 16:09:16,589:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1307 (0.1307)
+2022-11-18 16:09:16,608:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1337 (0.1323)
+2022-11-18 16:09:16,610:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1345 (0.1331)
+2022-11-18 16:09:16,611:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1344 (0.1334)
+2022-11-18 16:09:16,613:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1327 (0.1333)
+2022-11-18 16:09:16,693:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1343 (0.1334)
+2022-11-18 16:09:16,698:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1327 (0.1333)
+2022-11-18 16:09:16,702:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1362 (0.1337)
+2022-11-18 16:09:16,706:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1358 (0.1340)
+2022-11-18 16:09:16,710:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1368 (0.1342)
+2022-11-18 16:09:16,714:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1334 (0.1342)
+2022-11-18 16:09:16,720:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1327 (0.1340)
+2022-11-18 16:09:16,723:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1333 (0.1340)
+2022-11-18 16:09:16,725:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1337 (0.1340)
+2022-11-18 16:09:16,727:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1333 (0.1339)
+2022-11-18 16:09:16,728:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1362 (0.1341)
+2022-11-18 16:09:16,730:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1331 (0.1340)
+2022-11-18 16:09:16,732:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1333 (0.1340)
+2022-11-18 16:09:16,774:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:17,042:INFO: 		 ADE on eth                       dataset:	 1.8161107301712036
+2022-11-18 16:09:17,042:INFO: Average validation o:	ADE  1.8161	FDE  2.7284
+2022-11-18 16:09:17,043:INFO: - Computing loss (validation)
+2022-11-18 16:09:17,228:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6280 (0.6280)
+2022-11-18 16:09:17,231:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6248 (0.6278)
+2022-11-18 16:09:17,475:INFO: Dataset: univ                Batch: 1/3	Loss 0.1002 (0.1002)
+2022-11-18 16:09:17,495:INFO: Dataset: univ                Batch: 2/3	Loss 0.1004 (0.1003)
+2022-11-18 16:09:17,497:INFO: Dataset: univ                Batch: 3/3	Loss 0.1005 (0.1004)
+2022-11-18 16:09:17,724:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2622 (0.2622)
+2022-11-18 16:09:17,725:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2637 (0.2626)
+2022-11-18 16:09:17,968:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1359 (0.1359)
+2022-11-18 16:09:17,969:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1363 (0.1361)
+2022-11-18 16:09:17,983:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1378 (0.1367)
+2022-11-18 16:09:17,984:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1319 (0.1355)
+2022-11-18 16:09:17,985:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1362 (0.1356)
+2022-11-18 16:09:18,040:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_773.pth.tar
+2022-11-18 16:09:18,040:INFO: 
+===> EPOCH: 774 (P4)
+2022-11-18 16:09:18,041:INFO: - Computing loss (training)
+2022-11-18 16:09:18,243:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6278 (0.6278)
+2022-11-18 16:09:18,248:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6215 (0.6248)
+2022-11-18 16:09:18,249:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6329 (0.6274)
+2022-11-18 16:09:18,250:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6177 (0.6258)
+2022-11-18 16:09:18,484:INFO: Dataset: univ                Batch:  1/15	Loss 0.1004 (0.1004)
+2022-11-18 16:09:18,486:INFO: Dataset: univ                Batch:  2/15	Loss 0.0999 (0.1002)
+2022-11-18 16:09:18,491:INFO: Dataset: univ                Batch:  3/15	Loss 0.1006 (0.1003)
+2022-11-18 16:09:18,493:INFO: Dataset: univ                Batch:  4/15	Loss 0.0996 (0.1002)
+2022-11-18 16:09:18,522:INFO: Dataset: univ                Batch:  5/15	Loss 0.1002 (0.1002)
+2022-11-18 16:09:18,535:INFO: Dataset: univ                Batch:  6/15	Loss 0.1002 (0.1002)
+2022-11-18 16:09:18,536:INFO: Dataset: univ                Batch:  7/15	Loss 0.0996 (0.1001)
+2022-11-18 16:09:18,538:INFO: Dataset: univ                Batch:  8/15	Loss 0.0999 (0.1000)
+2022-11-18 16:09:18,539:INFO: Dataset: univ                Batch:  9/15	Loss 0.0998 (0.1000)
+2022-11-18 16:09:18,540:INFO: Dataset: univ                Batch: 10/15	Loss 0.1001 (0.1000)
+2022-11-18 16:09:18,542:INFO: Dataset: univ                Batch: 11/15	Loss 0.0994 (0.1000)
+2022-11-18 16:09:18,544:INFO: Dataset: univ                Batch: 12/15	Loss 0.0995 (0.0999)
+2022-11-18 16:09:18,545:INFO: Dataset: univ                Batch: 13/15	Loss 0.0999 (0.0999)
+2022-11-18 16:09:18,547:INFO: Dataset: univ                Batch: 14/15	Loss 0.0991 (0.0999)
+2022-11-18 16:09:18,548:INFO: Dataset: univ                Batch: 15/15	Loss 0.0982 (0.0998)
+2022-11-18 16:09:18,785:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2648 (0.2648)
+2022-11-18 16:09:18,788:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2639 (0.2644)
+2022-11-18 16:09:18,796:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2635 (0.2641)
+2022-11-18 16:09:18,803:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2624 (0.2636)
+2022-11-18 16:09:18,805:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2629 (0.2634)
+2022-11-18 16:09:18,898:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2636 (0.2635)
+2022-11-18 16:09:18,903:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2632 (0.2634)
+2022-11-18 16:09:18,912:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2597 (0.2630)
+2022-11-18 16:09:19,183:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1317 (0.1317)
+2022-11-18 16:09:19,186:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1337 (0.1327)
+2022-11-18 16:09:19,202:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1374 (0.1342)
+2022-11-18 16:09:19,204:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1352 (0.1344)
+2022-11-18 16:09:19,234:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1326 (0.1341)
+2022-11-18 16:09:19,241:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1339 (0.1341)
+2022-11-18 16:09:19,242:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1347 (0.1342)
+2022-11-18 16:09:19,243:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1331 (0.1340)
+2022-11-18 16:09:19,245:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1353 (0.1342)
+2022-11-18 16:09:19,246:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1348 (0.1342)
+2022-11-18 16:09:19,247:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1361 (0.1344)
+2022-11-18 16:09:19,249:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1345 (0.1344)
+2022-11-18 16:09:19,250:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1324 (0.1342)
+2022-11-18 16:09:19,251:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1282 (0.1338)
+2022-11-18 16:09:19,252:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1339 (0.1338)
+2022-11-18 16:09:19,253:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1332 (0.1338)
+2022-11-18 16:09:19,254:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1314 (0.1336)
+2022-11-18 16:09:19,256:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1281 (0.1334)
+2022-11-18 16:09:19,304:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:19,571:INFO: 		 ADE on eth                       dataset:	 1.809066891670227
+2022-11-18 16:09:19,571:INFO: Average validation o:	ADE  1.8091	FDE  2.6765
+2022-11-18 16:09:19,572:INFO: - Computing loss (validation)
+2022-11-18 16:09:19,753:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6274 (0.6274)
+2022-11-18 16:09:19,755:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6311 (0.6277)
+2022-11-18 16:09:20,012:INFO: Dataset: univ                Batch: 1/3	Loss 0.0994 (0.0994)
+2022-11-18 16:09:20,014:INFO: Dataset: univ                Batch: 2/3	Loss 0.1002 (0.0998)
+2022-11-18 16:09:20,015:INFO: Dataset: univ                Batch: 3/3	Loss 0.1007 (0.1000)
+2022-11-18 16:09:20,249:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2622 (0.2622)
+2022-11-18 16:09:20,249:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2619 (0.2621)
+2022-11-18 16:09:20,481:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1350 (0.1350)
+2022-11-18 16:09:20,484:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1367 (0.1359)
+2022-11-18 16:09:20,495:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1358 (0.1358)
+2022-11-18 16:09:20,495:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1344 (0.1355)
+2022-11-18 16:09:20,498:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1330 (0.1350)
+2022-11-18 16:09:20,575:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_774.pth.tar
+2022-11-18 16:09:20,575:INFO: 
+===> EPOCH: 775 (P4)
+2022-11-18 16:09:20,576:INFO: - Computing loss (training)
+2022-11-18 16:09:20,760:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6261 (0.6261)
+2022-11-18 16:09:20,762:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6204 (0.6233)
+2022-11-18 16:09:20,765:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6274 (0.6246)
+2022-11-18 16:09:20,766:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6277 (0.6252)
+2022-11-18 16:09:21,005:INFO: Dataset: univ                Batch:  1/15	Loss 0.0987 (0.0987)
+2022-11-18 16:09:21,007:INFO: Dataset: univ                Batch:  2/15	Loss 0.0998 (0.0992)
+2022-11-18 16:09:21,022:INFO: Dataset: univ                Batch:  3/15	Loss 0.0999 (0.0994)
+2022-11-18 16:09:21,029:INFO: Dataset: univ                Batch:  4/15	Loss 0.1001 (0.0996)
+2022-11-18 16:09:21,090:INFO: Dataset: univ                Batch:  5/15	Loss 0.1000 (0.0997)
+2022-11-18 16:09:21,096:INFO: Dataset: univ                Batch:  6/15	Loss 0.0998 (0.0997)
+2022-11-18 16:09:21,097:INFO: Dataset: univ                Batch:  7/15	Loss 0.0998 (0.0997)
+2022-11-18 16:09:21,099:INFO: Dataset: univ                Batch:  8/15	Loss 0.1004 (0.0998)
+2022-11-18 16:09:21,100:INFO: Dataset: univ                Batch:  9/15	Loss 0.1006 (0.0999)
+2022-11-18 16:09:21,101:INFO: Dataset: univ                Batch: 10/15	Loss 0.0998 (0.0999)
+2022-11-18 16:09:21,104:INFO: Dataset: univ                Batch: 11/15	Loss 0.0994 (0.0998)
+2022-11-18 16:09:21,105:INFO: Dataset: univ                Batch: 12/15	Loss 0.0993 (0.0998)
+2022-11-18 16:09:21,106:INFO: Dataset: univ                Batch: 13/15	Loss 0.0982 (0.0997)
+2022-11-18 16:09:21,108:INFO: Dataset: univ                Batch: 14/15	Loss 0.0994 (0.0996)
+2022-11-18 16:09:21,109:INFO: Dataset: univ                Batch: 15/15	Loss 0.0969 (0.0996)
+2022-11-18 16:09:21,344:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2643 (0.2643)
+2022-11-18 16:09:21,361:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2646 (0.2645)
+2022-11-18 16:09:21,362:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2628 (0.2640)
+2022-11-18 16:09:21,363:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2630 (0.2637)
+2022-11-18 16:09:21,365:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2623 (0.2634)
+2022-11-18 16:09:21,414:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2617 (0.2632)
+2022-11-18 16:09:21,419:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2621 (0.2630)
+2022-11-18 16:09:21,423:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2603 (0.2627)
+2022-11-18 16:09:21,674:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1333 (0.1333)
+2022-11-18 16:09:21,676:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1368 (0.1351)
+2022-11-18 16:09:21,683:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1334 (0.1345)
+2022-11-18 16:09:21,685:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1342 (0.1345)
+2022-11-18 16:09:21,725:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1315 (0.1339)
+2022-11-18 16:09:21,754:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1304 (0.1333)
+2022-11-18 16:09:21,756:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1342 (0.1334)
+2022-11-18 16:09:21,757:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1323 (0.1333)
+2022-11-18 16:09:21,758:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1339 (0.1334)
+2022-11-18 16:09:21,759:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1328 (0.1333)
+2022-11-18 16:09:21,760:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1317 (0.1332)
+2022-11-18 16:09:21,762:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1336 (0.1332)
+2022-11-18 16:09:21,764:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1325 (0.1331)
+2022-11-18 16:09:21,765:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1338 (0.1332)
+2022-11-18 16:09:21,766:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1330 (0.1332)
+2022-11-18 16:09:21,767:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1318 (0.1331)
+2022-11-18 16:09:21,769:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1283 (0.1328)
+2022-11-18 16:09:21,771:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1330 (0.1328)
+2022-11-18 16:09:21,816:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:22,091:INFO: 		 ADE on eth                       dataset:	 1.7823128700256348
+2022-11-18 16:09:22,091:INFO: Average validation o:	ADE  1.7823	FDE  2.6809
+2022-11-18 16:09:22,092:INFO: - Computing loss (validation)
+2022-11-18 16:09:22,277:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6282 (0.6282)
+2022-11-18 16:09:22,278:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6151 (0.6274)
+2022-11-18 16:09:22,533:INFO: Dataset: univ                Batch: 1/3	Loss 0.0985 (0.0985)
+2022-11-18 16:09:22,535:INFO: Dataset: univ                Batch: 2/3	Loss 0.1005 (0.0995)
+2022-11-18 16:09:22,559:INFO: Dataset: univ                Batch: 3/3	Loss 0.1003 (0.0997)
+2022-11-18 16:09:22,796:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2624 (0.2624)
+2022-11-18 16:09:22,797:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2596 (0.2617)
+2022-11-18 16:09:23,023:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1347 (0.1347)
+2022-11-18 16:09:23,024:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1343 (0.1345)
+2022-11-18 16:09:23,026:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1317 (0.1336)
+2022-11-18 16:09:23,027:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1357 (0.1342)
+2022-11-18 16:09:23,029:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1353 (0.1344)
+2022-11-18 16:09:23,087:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_775.pth.tar
+2022-11-18 16:09:23,087:INFO: 
+===> EPOCH: 776 (P4)
+2022-11-18 16:09:23,088:INFO: - Computing loss (training)
+2022-11-18 16:09:23,281:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6297 (0.6297)
+2022-11-18 16:09:23,285:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6257 (0.6276)
+2022-11-18 16:09:23,287:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6219 (0.6257)
+2022-11-18 16:09:23,289:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6182 (0.6244)
+2022-11-18 16:09:23,528:INFO: Dataset: univ                Batch:  1/15	Loss 0.0986 (0.0986)
+2022-11-18 16:09:23,532:INFO: Dataset: univ                Batch:  2/15	Loss 0.0992 (0.0989)
+2022-11-18 16:09:23,535:INFO: Dataset: univ                Batch:  3/15	Loss 0.0994 (0.0991)
+2022-11-18 16:09:23,538:INFO: Dataset: univ                Batch:  4/15	Loss 0.1000 (0.0993)
+2022-11-18 16:09:23,565:INFO: Dataset: univ                Batch:  5/15	Loss 0.0990 (0.0992)
+2022-11-18 16:09:23,569:INFO: Dataset: univ                Batch:  6/15	Loss 0.0999 (0.0993)
+2022-11-18 16:09:23,571:INFO: Dataset: univ                Batch:  7/15	Loss 0.0998 (0.0994)
+2022-11-18 16:09:23,572:INFO: Dataset: univ                Batch:  8/15	Loss 0.1004 (0.0995)
+2022-11-18 16:09:23,574:INFO: Dataset: univ                Batch:  9/15	Loss 0.0995 (0.0995)
+2022-11-18 16:09:23,575:INFO: Dataset: univ                Batch: 10/15	Loss 0.0995 (0.0995)
+2022-11-18 16:09:23,577:INFO: Dataset: univ                Batch: 11/15	Loss 0.0996 (0.0995)
+2022-11-18 16:09:23,579:INFO: Dataset: univ                Batch: 12/15	Loss 0.0992 (0.0995)
+2022-11-18 16:09:23,581:INFO: Dataset: univ                Batch: 13/15	Loss 0.0988 (0.0994)
+2022-11-18 16:09:23,582:INFO: Dataset: univ                Batch: 14/15	Loss 0.0988 (0.0994)
+2022-11-18 16:09:23,583:INFO: Dataset: univ                Batch: 15/15	Loss 0.0979 (0.0994)
+2022-11-18 16:09:23,813:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2636 (0.2636)
+2022-11-18 16:09:23,816:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2629 (0.2632)
+2022-11-18 16:09:23,818:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2629 (0.2632)
+2022-11-18 16:09:23,823:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2625 (0.2630)
+2022-11-18 16:09:23,824:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2621 (0.2628)
+2022-11-18 16:09:23,847:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2621 (0.2627)
+2022-11-18 16:09:23,848:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2622 (0.2626)
+2022-11-18 16:09:23,849:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2605 (0.2624)
+2022-11-18 16:09:24,114:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1341 (0.1341)
+2022-11-18 16:09:24,117:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1323 (0.1332)
+2022-11-18 16:09:24,119:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1324 (0.1329)
+2022-11-18 16:09:24,178:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1296 (0.1321)
+2022-11-18 16:09:24,180:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1314 (0.1319)
+2022-11-18 16:09:24,184:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1324 (0.1320)
+2022-11-18 16:09:24,185:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1328 (0.1321)
+2022-11-18 16:09:24,186:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1382 (0.1330)
+2022-11-18 16:09:24,187:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1314 (0.1328)
+2022-11-18 16:09:24,188:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1337 (0.1329)
+2022-11-18 16:09:24,191:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1292 (0.1326)
+2022-11-18 16:09:24,192:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1310 (0.1324)
+2022-11-18 16:09:24,193:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1331 (0.1325)
+2022-11-18 16:09:24,195:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1319 (0.1325)
+2022-11-18 16:09:24,196:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1325 (0.1325)
+2022-11-18 16:09:24,198:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1329 (0.1325)
+2022-11-18 16:09:24,200:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1289 (0.1323)
+2022-11-18 16:09:24,202:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1316 (0.1322)
+2022-11-18 16:09:24,250:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:24,528:INFO: 		 ADE on eth                       dataset:	 1.8279005289077759
+2022-11-18 16:09:24,528:INFO: Average validation o:	ADE  1.8279	FDE  2.8013
+2022-11-18 16:09:24,529:INFO: - Computing loss (validation)
+2022-11-18 16:09:24,728:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6275 (0.6275)
+2022-11-18 16:09:24,732:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6201 (0.6270)
+2022-11-18 16:09:24,985:INFO: Dataset: univ                Batch: 1/3	Loss 0.0992 (0.0992)
+2022-11-18 16:09:24,987:INFO: Dataset: univ                Batch: 2/3	Loss 0.0982 (0.0987)
+2022-11-18 16:09:24,988:INFO: Dataset: univ                Batch: 3/3	Loss 0.1011 (0.0995)
+2022-11-18 16:09:25,224:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2617 (0.2617)
+2022-11-18 16:09:25,226:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2603 (0.2613)
+2022-11-18 16:09:25,487:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1304 (0.1304)
+2022-11-18 16:09:25,489:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1340 (0.1322)
+2022-11-18 16:09:25,493:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1363 (0.1337)
+2022-11-18 16:09:25,502:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1321 (0.1333)
+2022-11-18 16:09:25,502:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1361 (0.1338)
+2022-11-18 16:09:25,561:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_776.pth.tar
+2022-11-18 16:09:25,562:INFO: 
+===> EPOCH: 777 (P4)
+2022-11-18 16:09:25,562:INFO: - Computing loss (training)
+2022-11-18 16:09:25,758:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6212 (0.6212)
+2022-11-18 16:09:25,761:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6233 (0.6223)
+2022-11-18 16:09:25,764:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6248 (0.6231)
+2022-11-18 16:09:25,773:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6259 (0.6236)
+2022-11-18 16:09:26,019:INFO: Dataset: univ                Batch:  1/15	Loss 0.0980 (0.0980)
+2022-11-18 16:09:26,024:INFO: Dataset: univ                Batch:  2/15	Loss 0.0996 (0.0987)
+2022-11-18 16:09:26,103:INFO: Dataset: univ                Batch:  3/15	Loss 0.0989 (0.0988)
+2022-11-18 16:09:26,105:INFO: Dataset: univ                Batch:  4/15	Loss 0.0998 (0.0991)
+2022-11-18 16:09:26,107:INFO: Dataset: univ                Batch:  5/15	Loss 0.0998 (0.0992)
+2022-11-18 16:09:26,110:INFO: Dataset: univ                Batch:  6/15	Loss 0.0991 (0.0992)
+2022-11-18 16:09:26,111:INFO: Dataset: univ                Batch:  7/15	Loss 0.0999 (0.0993)
+2022-11-18 16:09:26,112:INFO: Dataset: univ                Batch:  8/15	Loss 0.1001 (0.0994)
+2022-11-18 16:09:26,114:INFO: Dataset: univ                Batch:  9/15	Loss 0.0982 (0.0992)
+2022-11-18 16:09:26,115:INFO: Dataset: univ                Batch: 10/15	Loss 0.1001 (0.0993)
+2022-11-18 16:09:26,117:INFO: Dataset: univ                Batch: 11/15	Loss 0.0981 (0.0992)
+2022-11-18 16:09:26,118:INFO: Dataset: univ                Batch: 12/15	Loss 0.0991 (0.0992)
+2022-11-18 16:09:26,120:INFO: Dataset: univ                Batch: 13/15	Loss 0.0992 (0.0992)
+2022-11-18 16:09:26,121:INFO: Dataset: univ                Batch: 14/15	Loss 0.0993 (0.0992)
+2022-11-18 16:09:26,123:INFO: Dataset: univ                Batch: 15/15	Loss 0.0976 (0.0992)
+2022-11-18 16:09:26,351:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2628 (0.2628)
+2022-11-18 16:09:26,354:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2622 (0.2625)
+2022-11-18 16:09:26,356:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2638 (0.2629)
+2022-11-18 16:09:26,357:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2616 (0.2626)
+2022-11-18 16:09:26,360:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2614 (0.2624)
+2022-11-18 16:09:26,401:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2627 (0.2625)
+2022-11-18 16:09:26,406:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2611 (0.2623)
+2022-11-18 16:09:26,410:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2613 (0.2622)
+2022-11-18 16:09:26,669:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1310 (0.1310)
+2022-11-18 16:09:26,672:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1332 (0.1321)
+2022-11-18 16:09:26,674:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1333 (0.1324)
+2022-11-18 16:09:26,678:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1308 (0.1320)
+2022-11-18 16:09:26,699:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1329 (0.1322)
+2022-11-18 16:09:26,705:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1316 (0.1321)
+2022-11-18 16:09:26,706:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1336 (0.1323)
+2022-11-18 16:09:26,707:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1338 (0.1325)
+2022-11-18 16:09:26,708:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1324 (0.1325)
+2022-11-18 16:09:26,710:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1322 (0.1325)
+2022-11-18 16:09:26,711:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1317 (0.1324)
+2022-11-18 16:09:26,713:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1320 (0.1324)
+2022-11-18 16:09:26,714:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1319 (0.1323)
+2022-11-18 16:09:26,715:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1323 (0.1323)
+2022-11-18 16:09:26,716:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1287 (0.1321)
+2022-11-18 16:09:26,718:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1308 (0.1320)
+2022-11-18 16:09:26,719:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1321 (0.1320)
+2022-11-18 16:09:26,721:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1267 (0.1318)
+2022-11-18 16:09:26,764:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:27,040:INFO: 		 ADE on eth                       dataset:	 1.836582899093628
+2022-11-18 16:09:27,040:INFO: Average validation o:	ADE  1.8366	FDE  2.7427
+2022-11-18 16:09:27,041:INFO: - Computing loss (validation)
+2022-11-18 16:09:27,235:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6275 (0.6275)
+2022-11-18 16:09:27,236:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6182 (0.6267)
+2022-11-18 16:09:27,463:INFO: Dataset: univ                Batch: 1/3	Loss 0.1000 (0.1000)
+2022-11-18 16:09:27,466:INFO: Dataset: univ                Batch: 2/3	Loss 0.0990 (0.0994)
+2022-11-18 16:09:27,468:INFO: Dataset: univ                Batch: 3/3	Loss 0.0986 (0.0992)
+2022-11-18 16:09:27,724:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2609 (0.2609)
+2022-11-18 16:09:27,725:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2611 (0.2609)
+2022-11-18 16:09:27,962:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1315 (0.1315)
+2022-11-18 16:09:27,963:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1353 (0.1334)
+2022-11-18 16:09:27,964:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1319 (0.1329)
+2022-11-18 16:09:27,965:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1323 (0.1328)
+2022-11-18 16:09:27,967:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1356 (0.1333)
+2022-11-18 16:09:28,027:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_777.pth.tar
+2022-11-18 16:09:28,027:INFO: 
+===> EPOCH: 778 (P4)
+2022-11-18 16:09:28,028:INFO: - Computing loss (training)
+2022-11-18 16:09:28,203:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6210 (0.6210)
+2022-11-18 16:09:28,206:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6217 (0.6214)
+2022-11-18 16:09:28,208:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6252 (0.6227)
+2022-11-18 16:09:28,210:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6239 (0.6229)
+2022-11-18 16:09:28,456:INFO: Dataset: univ                Batch:  1/15	Loss 0.0990 (0.0990)
+2022-11-18 16:09:28,459:INFO: Dataset: univ                Batch:  2/15	Loss 0.0994 (0.0992)
+2022-11-18 16:09:28,516:INFO: Dataset: univ                Batch:  3/15	Loss 0.0998 (0.0994)
+2022-11-18 16:09:28,517:INFO: Dataset: univ                Batch:  4/15	Loss 0.0999 (0.0995)
+2022-11-18 16:09:28,518:INFO: Dataset: univ                Batch:  5/15	Loss 0.0997 (0.0995)
+2022-11-18 16:09:28,522:INFO: Dataset: univ                Batch:  6/15	Loss 0.0985 (0.0994)
+2022-11-18 16:09:28,523:INFO: Dataset: univ                Batch:  7/15	Loss 0.0995 (0.0994)
+2022-11-18 16:09:28,525:INFO: Dataset: univ                Batch:  8/15	Loss 0.0994 (0.0994)
+2022-11-18 16:09:28,526:INFO: Dataset: univ                Batch:  9/15	Loss 0.0990 (0.0993)
+2022-11-18 16:09:28,528:INFO: Dataset: univ                Batch: 10/15	Loss 0.0993 (0.0993)
+2022-11-18 16:09:28,530:INFO: Dataset: univ                Batch: 11/15	Loss 0.0991 (0.0993)
+2022-11-18 16:09:28,532:INFO: Dataset: univ                Batch: 12/15	Loss 0.0981 (0.0992)
+2022-11-18 16:09:28,533:INFO: Dataset: univ                Batch: 13/15	Loss 0.0981 (0.0991)
+2022-11-18 16:09:28,534:INFO: Dataset: univ                Batch: 14/15	Loss 0.0974 (0.0990)
+2022-11-18 16:09:28,536:INFO: Dataset: univ                Batch: 15/15	Loss 0.0980 (0.0990)
+2022-11-18 16:09:28,772:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2635 (0.2635)
+2022-11-18 16:09:28,774:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2632 (0.2634)
+2022-11-18 16:09:28,776:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2624 (0.2630)
+2022-11-18 16:09:28,780:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2615 (0.2627)
+2022-11-18 16:09:28,781:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2623 (0.2626)
+2022-11-18 16:09:28,811:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2608 (0.2623)
+2022-11-18 16:09:28,812:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2621 (0.2623)
+2022-11-18 16:09:28,813:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2589 (0.2619)
+2022-11-18 16:09:29,096:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1310 (0.1310)
+2022-11-18 16:09:29,099:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1302 (0.1306)
+2022-11-18 16:09:29,101:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1292 (0.1301)
+2022-11-18 16:09:29,125:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1325 (0.1308)
+2022-11-18 16:09:29,127:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1312 (0.1309)
+2022-11-18 16:09:29,139:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1318 (0.1310)
+2022-11-18 16:09:29,140:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1326 (0.1313)
+2022-11-18 16:09:29,142:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1320 (0.1314)
+2022-11-18 16:09:29,143:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1347 (0.1317)
+2022-11-18 16:09:29,144:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1339 (0.1319)
+2022-11-18 16:09:29,145:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1327 (0.1320)
+2022-11-18 16:09:29,147:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1303 (0.1318)
+2022-11-18 16:09:29,148:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1346 (0.1320)
+2022-11-18 16:09:29,149:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1303 (0.1319)
+2022-11-18 16:09:29,151:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1300 (0.1318)
+2022-11-18 16:09:29,152:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1305 (0.1317)
+2022-11-18 16:09:29,153:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1289 (0.1315)
+2022-11-18 16:09:29,155:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1280 (0.1314)
+2022-11-18 16:09:29,199:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:29,470:INFO: 		 ADE on eth                       dataset:	 1.7808754444122314
+2022-11-18 16:09:29,471:INFO: Average validation o:	ADE  1.7809	FDE  2.6729
+2022-11-18 16:09:29,472:INFO: - Computing loss (validation)
+2022-11-18 16:09:29,662:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6260 (0.6260)
+2022-11-18 16:09:29,663:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6309 (0.6263)
+2022-11-18 16:09:29,920:INFO: Dataset: univ                Batch: 1/3	Loss 0.0979 (0.0979)
+2022-11-18 16:09:29,927:INFO: Dataset: univ                Batch: 2/3	Loss 0.0988 (0.0984)
+2022-11-18 16:09:29,928:INFO: Dataset: univ                Batch: 3/3	Loss 0.1003 (0.0989)
+2022-11-18 16:09:30,171:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2613 (0.2613)
+2022-11-18 16:09:30,173:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2583 (0.2605)
+2022-11-18 16:09:30,421:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1323 (0.1323)
+2022-11-18 16:09:30,424:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1322 (0.1322)
+2022-11-18 16:09:30,443:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1325 (0.1323)
+2022-11-18 16:09:30,444:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1335 (0.1326)
+2022-11-18 16:09:30,446:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1342 (0.1329)
+2022-11-18 16:09:30,505:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_778.pth.tar
+2022-11-18 16:09:30,505:INFO: 
+===> EPOCH: 779 (P4)
+2022-11-18 16:09:30,506:INFO: - Computing loss (training)
+2022-11-18 16:09:30,702:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6251 (0.6251)
+2022-11-18 16:09:30,704:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6246 (0.6248)
+2022-11-18 16:09:30,706:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6194 (0.6230)
+2022-11-18 16:09:30,708:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6182 (0.6222)
+2022-11-18 16:09:30,946:INFO: Dataset: univ                Batch:  1/15	Loss 0.0992 (0.0992)
+2022-11-18 16:09:30,949:INFO: Dataset: univ                Batch:  2/15	Loss 0.0989 (0.0990)
+2022-11-18 16:09:30,954:INFO: Dataset: univ                Batch:  3/15	Loss 0.0986 (0.0989)
+2022-11-18 16:09:30,971:INFO: Dataset: univ                Batch:  4/15	Loss 0.0988 (0.0989)
+2022-11-18 16:09:30,973:INFO: Dataset: univ                Batch:  5/15	Loss 0.1002 (0.0991)
+2022-11-18 16:09:30,987:INFO: Dataset: univ                Batch:  6/15	Loss 0.0985 (0.0990)
+2022-11-18 16:09:30,988:INFO: Dataset: univ                Batch:  7/15	Loss 0.0984 (0.0989)
+2022-11-18 16:09:30,990:INFO: Dataset: univ                Batch:  8/15	Loss 0.0988 (0.0989)
+2022-11-18 16:09:30,991:INFO: Dataset: univ                Batch:  9/15	Loss 0.0992 (0.0989)
+2022-11-18 16:09:30,992:INFO: Dataset: univ                Batch: 10/15	Loss 0.0983 (0.0989)
+2022-11-18 16:09:30,994:INFO: Dataset: univ                Batch: 11/15	Loss 0.0993 (0.0989)
+2022-11-18 16:09:30,996:INFO: Dataset: univ                Batch: 12/15	Loss 0.0990 (0.0989)
+2022-11-18 16:09:30,997:INFO: Dataset: univ                Batch: 13/15	Loss 0.0983 (0.0989)
+2022-11-18 16:09:30,999:INFO: Dataset: univ                Batch: 14/15	Loss 0.0980 (0.0988)
+2022-11-18 16:09:31,000:INFO: Dataset: univ                Batch: 15/15	Loss 0.0974 (0.0988)
+2022-11-18 16:09:31,244:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2633 (0.2633)
+2022-11-18 16:09:31,246:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2628 (0.2630)
+2022-11-18 16:09:31,247:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2625 (0.2629)
+2022-11-18 16:09:31,251:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2609 (0.2623)
+2022-11-18 16:09:31,253:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2600 (0.2618)
+2022-11-18 16:09:31,297:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2611 (0.2617)
+2022-11-18 16:09:31,301:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2618 (0.2617)
+2022-11-18 16:09:31,305:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2612 (0.2617)
+2022-11-18 16:09:31,620:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1296 (0.1296)
+2022-11-18 16:09:31,621:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1302 (0.1299)
+2022-11-18 16:09:31,622:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1302 (0.1300)
+2022-11-18 16:09:31,625:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1309 (0.1302)
+2022-11-18 16:09:31,626:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1337 (0.1309)
+2022-11-18 16:09:31,629:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1337 (0.1314)
+2022-11-18 16:09:31,630:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1338 (0.1317)
+2022-11-18 16:09:31,631:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1312 (0.1316)
+2022-11-18 16:09:31,633:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1332 (0.1318)
+2022-11-18 16:09:31,634:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1307 (0.1317)
+2022-11-18 16:09:31,635:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1326 (0.1318)
+2022-11-18 16:09:31,637:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1312 (0.1317)
+2022-11-18 16:09:31,639:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1296 (0.1316)
+2022-11-18 16:09:31,641:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1330 (0.1317)
+2022-11-18 16:09:31,642:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1324 (0.1317)
+2022-11-18 16:09:31,644:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1278 (0.1315)
+2022-11-18 16:09:31,646:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1267 (0.1312)
+2022-11-18 16:09:31,648:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1251 (0.1310)
+2022-11-18 16:09:31,692:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:31,966:INFO: 		 ADE on eth                       dataset:	 1.8088759183883667
+2022-11-18 16:09:31,966:INFO: Average validation o:	ADE  1.8089	FDE  2.7591
+2022-11-18 16:09:31,967:INFO: - Computing loss (validation)
+2022-11-18 16:09:32,163:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6262 (0.6262)
+2022-11-18 16:09:32,164:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6232 (0.6260)
+2022-11-18 16:09:32,416:INFO: Dataset: univ                Batch: 1/3	Loss 0.0975 (0.0975)
+2022-11-18 16:09:32,419:INFO: Dataset: univ                Batch: 2/3	Loss 0.0997 (0.0985)
+2022-11-18 16:09:32,420:INFO: Dataset: univ                Batch: 3/3	Loss 0.0989 (0.0986)
+2022-11-18 16:09:32,650:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2596 (0.2596)
+2022-11-18 16:09:32,652:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2622 (0.2602)
+2022-11-18 16:09:32,908:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1319 (0.1319)
+2022-11-18 16:09:32,910:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1352 (0.1336)
+2022-11-18 16:09:32,911:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1316 (0.1329)
+2022-11-18 16:09:32,913:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1321 (0.1327)
+2022-11-18 16:09:32,914:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1314 (0.1325)
+2022-11-18 16:09:32,978:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_779.pth.tar
+2022-11-18 16:09:32,978:INFO: 
+===> EPOCH: 780 (P4)
+2022-11-18 16:09:32,978:INFO: - Computing loss (training)
+2022-11-18 16:09:33,166:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6215 (0.6215)
+2022-11-18 16:09:33,169:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6245 (0.6229)
+2022-11-18 16:09:33,172:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6222 (0.6227)
+2022-11-18 16:09:33,173:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6156 (0.6215)
+2022-11-18 16:09:33,424:INFO: Dataset: univ                Batch:  1/15	Loss 0.0978 (0.0978)
+2022-11-18 16:09:33,426:INFO: Dataset: univ                Batch:  2/15	Loss 0.0991 (0.0984)
+2022-11-18 16:09:33,432:INFO: Dataset: univ                Batch:  3/15	Loss 0.0988 (0.0986)
+2022-11-18 16:09:33,433:INFO: Dataset: univ                Batch:  4/15	Loss 0.0987 (0.0986)
+2022-11-18 16:09:33,494:INFO: Dataset: univ                Batch:  5/15	Loss 0.0998 (0.0988)
+2022-11-18 16:09:33,501:INFO: Dataset: univ                Batch:  6/15	Loss 0.0997 (0.0990)
+2022-11-18 16:09:33,503:INFO: Dataset: univ                Batch:  7/15	Loss 0.0986 (0.0989)
+2022-11-18 16:09:33,504:INFO: Dataset: univ                Batch:  8/15	Loss 0.0983 (0.0988)
+2022-11-18 16:09:33,505:INFO: Dataset: univ                Batch:  9/15	Loss 0.0989 (0.0988)
+2022-11-18 16:09:33,506:INFO: Dataset: univ                Batch: 10/15	Loss 0.0990 (0.0989)
+2022-11-18 16:09:33,509:INFO: Dataset: univ                Batch: 11/15	Loss 0.0977 (0.0987)
+2022-11-18 16:09:33,511:INFO: Dataset: univ                Batch: 12/15	Loss 0.0990 (0.0988)
+2022-11-18 16:09:33,512:INFO: Dataset: univ                Batch: 13/15	Loss 0.0972 (0.0986)
+2022-11-18 16:09:33,513:INFO: Dataset: univ                Batch: 14/15	Loss 0.0985 (0.0986)
+2022-11-18 16:09:33,514:INFO: Dataset: univ                Batch: 15/15	Loss 0.0970 (0.0986)
+2022-11-18 16:09:33,751:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2614 (0.2614)
+2022-11-18 16:09:33,755:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2606 (0.2610)
+2022-11-18 16:09:33,756:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2612 (0.2611)
+2022-11-18 16:09:33,758:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2632 (0.2616)
+2022-11-18 16:09:33,761:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2612 (0.2615)
+2022-11-18 16:09:33,814:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2620 (0.2616)
+2022-11-18 16:09:33,818:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2611 (0.2615)
+2022-11-18 16:09:33,822:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2606 (0.2614)
+2022-11-18 16:09:34,096:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1302 (0.1302)
+2022-11-18 16:09:34,099:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1335 (0.1318)
+2022-11-18 16:09:34,100:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1312 (0.1316)
+2022-11-18 16:09:34,102:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1278 (0.1306)
+2022-11-18 16:09:34,140:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1260 (0.1297)
+2022-11-18 16:09:34,148:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1325 (0.1301)
+2022-11-18 16:09:34,149:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1321 (0.1304)
+2022-11-18 16:09:34,150:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1315 (0.1305)
+2022-11-18 16:09:34,152:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1325 (0.1307)
+2022-11-18 16:09:34,153:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1303 (0.1307)
+2022-11-18 16:09:34,155:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1316 (0.1308)
+2022-11-18 16:09:34,157:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1305 (0.1308)
+2022-11-18 16:09:34,158:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1296 (0.1307)
+2022-11-18 16:09:34,159:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1300 (0.1306)
+2022-11-18 16:09:34,160:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1313 (0.1307)
+2022-11-18 16:09:34,161:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1308 (0.1307)
+2022-11-18 16:09:34,163:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1303 (0.1307)
+2022-11-18 16:09:34,165:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1284 (0.1305)
+2022-11-18 16:09:34,209:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:34,489:INFO: 		 ADE on eth                       dataset:	 1.7844951152801514
+2022-11-18 16:09:34,489:INFO: Average validation o:	ADE  1.7845	FDE  2.6538
+2022-11-18 16:09:34,490:INFO: - Computing loss (validation)
+2022-11-18 16:09:34,677:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6259 (0.6259)
+2022-11-18 16:09:34,678:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6239 (0.6257)
+2022-11-18 16:09:34,938:INFO: Dataset: univ                Batch: 1/3	Loss 0.1009 (0.1009)
+2022-11-18 16:09:34,941:INFO: Dataset: univ                Batch: 2/3	Loss 0.0965 (0.0983)
+2022-11-18 16:09:34,942:INFO: Dataset: univ                Batch: 3/3	Loss 0.0987 (0.0984)
+2022-11-18 16:09:35,184:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2602 (0.2602)
+2022-11-18 16:09:35,187:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2588 (0.2599)
+2022-11-18 16:09:35,445:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1330 (0.1330)
+2022-11-18 16:09:35,447:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1296 (0.1313)
+2022-11-18 16:09:35,448:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1336 (0.1320)
+2022-11-18 16:09:35,450:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1327 (0.1322)
+2022-11-18 16:09:35,452:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1315 (0.1321)
+2022-11-18 16:09:35,534:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_780.pth.tar
+2022-11-18 16:09:35,534:INFO: 
+===> EPOCH: 781 (P4)
+2022-11-18 16:09:35,535:INFO: - Computing loss (training)
+2022-11-18 16:09:35,721:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6214 (0.6214)
+2022-11-18 16:09:35,723:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6197 (0.6206)
+2022-11-18 16:09:35,726:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6209 (0.6207)
+2022-11-18 16:09:35,728:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6208 (0.6207)
+2022-11-18 16:09:35,963:INFO: Dataset: univ                Batch:  1/15	Loss 0.0977 (0.0977)
+2022-11-18 16:09:35,974:INFO: Dataset: univ                Batch:  2/15	Loss 0.0983 (0.0980)
+2022-11-18 16:09:35,975:INFO: Dataset: univ                Batch:  3/15	Loss 0.0996 (0.0985)
+2022-11-18 16:09:35,979:INFO: Dataset: univ                Batch:  4/15	Loss 0.0983 (0.0984)
+2022-11-18 16:09:36,042:INFO: Dataset: univ                Batch:  5/15	Loss 0.0990 (0.0986)
+2022-11-18 16:09:36,048:INFO: Dataset: univ                Batch:  6/15	Loss 0.0990 (0.0986)
+2022-11-18 16:09:36,049:INFO: Dataset: univ                Batch:  7/15	Loss 0.0982 (0.0986)
+2022-11-18 16:09:36,050:INFO: Dataset: univ                Batch:  8/15	Loss 0.0978 (0.0985)
+2022-11-18 16:09:36,052:INFO: Dataset: univ                Batch:  9/15	Loss 0.0985 (0.0985)
+2022-11-18 16:09:36,053:INFO: Dataset: univ                Batch: 10/15	Loss 0.0997 (0.0986)
+2022-11-18 16:09:36,058:INFO: Dataset: univ                Batch: 11/15	Loss 0.0991 (0.0986)
+2022-11-18 16:09:36,060:INFO: Dataset: univ                Batch: 12/15	Loss 0.0982 (0.0986)
+2022-11-18 16:09:36,061:INFO: Dataset: univ                Batch: 13/15	Loss 0.0977 (0.0985)
+2022-11-18 16:09:36,062:INFO: Dataset: univ                Batch: 14/15	Loss 0.0980 (0.0985)
+2022-11-18 16:09:36,064:INFO: Dataset: univ                Batch: 15/15	Loss 0.0968 (0.0985)
+2022-11-18 16:09:36,296:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2629 (0.2629)
+2022-11-18 16:09:36,301:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2627 (0.2628)
+2022-11-18 16:09:36,303:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2622 (0.2626)
+2022-11-18 16:09:36,304:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2616 (0.2623)
+2022-11-18 16:09:36,307:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2608 (0.2620)
+2022-11-18 16:09:36,350:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2618 (0.2620)
+2022-11-18 16:09:36,354:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2588 (0.2615)
+2022-11-18 16:09:36,358:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2591 (0.2612)
+2022-11-18 16:09:36,600:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1310 (0.1310)
+2022-11-18 16:09:36,605:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1317 (0.1313)
+2022-11-18 16:09:36,649:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1299 (0.1309)
+2022-11-18 16:09:36,653:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1309 (0.1309)
+2022-11-18 16:09:36,654:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1307 (0.1308)
+2022-11-18 16:09:36,663:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1307 (0.1308)
+2022-11-18 16:09:36,664:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1313 (0.1309)
+2022-11-18 16:09:36,665:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1305 (0.1308)
+2022-11-18 16:09:36,666:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1299 (0.1307)
+2022-11-18 16:09:36,668:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1291 (0.1306)
+2022-11-18 16:09:36,669:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1285 (0.1304)
+2022-11-18 16:09:36,671:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1317 (0.1305)
+2022-11-18 16:09:36,672:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1289 (0.1304)
+2022-11-18 16:09:36,673:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1296 (0.1303)
+2022-11-18 16:09:36,674:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1301 (0.1303)
+2022-11-18 16:09:36,675:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1309 (0.1304)
+2022-11-18 16:09:36,677:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1302 (0.1303)
+2022-11-18 16:09:36,679:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1280 (0.1302)
+2022-11-18 16:09:36,727:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:36,996:INFO: 		 ADE on eth                       dataset:	 1.7896976470947266
+2022-11-18 16:09:36,996:INFO: Average validation o:	ADE  1.7897	FDE  2.6975
+2022-11-18 16:09:36,997:INFO: - Computing loss (validation)
+2022-11-18 16:09:37,175:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6259 (0.6259)
+2022-11-18 16:09:37,176:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6203 (0.6254)
+2022-11-18 16:09:37,435:INFO: Dataset: univ                Batch: 1/3	Loss 0.0995 (0.0995)
+2022-11-18 16:09:37,437:INFO: Dataset: univ                Batch: 2/3	Loss 0.0971 (0.0982)
+2022-11-18 16:09:37,439:INFO: Dataset: univ                Batch: 3/3	Loss 0.0981 (0.0982)
+2022-11-18 16:09:37,682:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2599 (0.2599)
+2022-11-18 16:09:37,682:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2585 (0.2596)
+2022-11-18 16:09:37,915:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1309 (0.1309)
+2022-11-18 16:09:37,917:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1298 (0.1303)
+2022-11-18 16:09:37,923:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1332 (0.1313)
+2022-11-18 16:09:37,926:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1329 (0.1317)
+2022-11-18 16:09:37,927:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1316 (0.1317)
+2022-11-18 16:09:37,989:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_781.pth.tar
+2022-11-18 16:09:37,989:INFO: 
+===> EPOCH: 782 (P4)
+2022-11-18 16:09:37,990:INFO: - Computing loss (training)
+2022-11-18 16:09:38,177:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6208 (0.6208)
+2022-11-18 16:09:38,181:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6193 (0.6200)
+2022-11-18 16:09:38,183:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6178 (0.6193)
+2022-11-18 16:09:38,186:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6240 (0.6201)
+2022-11-18 16:09:38,434:INFO: Dataset: univ                Batch:  1/15	Loss 0.0977 (0.0977)
+2022-11-18 16:09:38,439:INFO: Dataset: univ                Batch:  2/15	Loss 0.0988 (0.0982)
+2022-11-18 16:09:38,445:INFO: Dataset: univ                Batch:  3/15	Loss 0.0988 (0.0984)
+2022-11-18 16:09:38,446:INFO: Dataset: univ                Batch:  4/15	Loss 0.0983 (0.0984)
+2022-11-18 16:09:38,473:INFO: Dataset: univ                Batch:  5/15	Loss 0.0981 (0.0983)
+2022-11-18 16:09:38,495:INFO: Dataset: univ                Batch:  6/15	Loss 0.0982 (0.0983)
+2022-11-18 16:09:38,496:INFO: Dataset: univ                Batch:  7/15	Loss 0.0979 (0.0982)
+2022-11-18 16:09:38,498:INFO: Dataset: univ                Batch:  8/15	Loss 0.0993 (0.0983)
+2022-11-18 16:09:38,499:INFO: Dataset: univ                Batch:  9/15	Loss 0.0982 (0.0983)
+2022-11-18 16:09:38,500:INFO: Dataset: univ                Batch: 10/15	Loss 0.0989 (0.0984)
+2022-11-18 16:09:38,502:INFO: Dataset: univ                Batch: 11/15	Loss 0.0987 (0.0984)
+2022-11-18 16:09:38,504:INFO: Dataset: univ                Batch: 12/15	Loss 0.0984 (0.0984)
+2022-11-18 16:09:38,506:INFO: Dataset: univ                Batch: 13/15	Loss 0.0971 (0.0983)
+2022-11-18 16:09:38,507:INFO: Dataset: univ                Batch: 14/15	Loss 0.0980 (0.0983)
+2022-11-18 16:09:38,509:INFO: Dataset: univ                Batch: 15/15	Loss 0.0988 (0.0983)
+2022-11-18 16:09:38,769:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2623 (0.2623)
+2022-11-18 16:09:38,772:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2613 (0.2618)
+2022-11-18 16:09:38,784:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2617 (0.2618)
+2022-11-18 16:09:38,786:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2612 (0.2616)
+2022-11-18 16:09:38,788:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2610 (0.2615)
+2022-11-18 16:09:38,819:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2601 (0.2613)
+2022-11-18 16:09:38,823:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2611 (0.2613)
+2022-11-18 16:09:38,827:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2597 (0.2611)
+2022-11-18 16:09:39,115:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1289 (0.1289)
+2022-11-18 16:09:39,119:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1277 (0.1284)
+2022-11-18 16:09:39,122:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1313 (0.1294)
+2022-11-18 16:09:39,123:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1298 (0.1295)
+2022-11-18 16:09:39,125:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1324 (0.1301)
+2022-11-18 16:09:39,139:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1283 (0.1298)
+2022-11-18 16:09:39,141:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1311 (0.1300)
+2022-11-18 16:09:39,142:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1318 (0.1302)
+2022-11-18 16:09:39,143:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1279 (0.1300)
+2022-11-18 16:09:39,144:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1260 (0.1296)
+2022-11-18 16:09:39,145:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1327 (0.1299)
+2022-11-18 16:09:39,147:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1294 (0.1299)
+2022-11-18 16:09:39,149:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1320 (0.1300)
+2022-11-18 16:09:39,150:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1310 (0.1301)
+2022-11-18 16:09:39,151:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1312 (0.1302)
+2022-11-18 16:09:39,153:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1291 (0.1301)
+2022-11-18 16:09:39,154:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1282 (0.1300)
+2022-11-18 16:09:39,156:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1279 (0.1299)
+2022-11-18 16:09:39,200:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:39,477:INFO: 		 ADE on eth                       dataset:	 1.8089293241500854
+2022-11-18 16:09:39,477:INFO: Average validation o:	ADE  1.8089	FDE  2.7274
+2022-11-18 16:09:39,478:INFO: - Computing loss (validation)
+2022-11-18 16:09:39,664:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6254 (0.6254)
+2022-11-18 16:09:39,665:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6201 (0.6250)
+2022-11-18 16:09:39,909:INFO: Dataset: univ                Batch: 1/3	Loss 0.0963 (0.0963)
+2022-11-18 16:09:39,910:INFO: Dataset: univ                Batch: 2/3	Loss 0.0994 (0.0976)
+2022-11-18 16:09:39,912:INFO: Dataset: univ                Batch: 3/3	Loss 0.0988 (0.0980)
+2022-11-18 16:09:40,149:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2580 (0.2580)
+2022-11-18 16:09:40,152:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2638 (0.2593)
+2022-11-18 16:09:40,382:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1298 (0.1298)
+2022-11-18 16:09:40,384:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1296 (0.1297)
+2022-11-18 16:09:40,384:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1327 (0.1307)
+2022-11-18 16:09:40,386:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1317 (0.1310)
+2022-11-18 16:09:40,391:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1329 (0.1314)
+2022-11-18 16:09:40,450:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_782.pth.tar
+2022-11-18 16:09:40,450:INFO: 
+===> EPOCH: 783 (P4)
+2022-11-18 16:09:40,451:INFO: - Computing loss (training)
+2022-11-18 16:09:40,652:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6223 (0.6223)
+2022-11-18 16:09:40,654:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6145 (0.6184)
+2022-11-18 16:09:40,658:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6189 (0.6185)
+2022-11-18 16:09:40,659:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6232 (0.6193)
+2022-11-18 16:09:40,896:INFO: Dataset: univ                Batch:  1/15	Loss 0.0976 (0.0976)
+2022-11-18 16:09:40,912:INFO: Dataset: univ                Batch:  2/15	Loss 0.0981 (0.0979)
+2022-11-18 16:09:40,914:INFO: Dataset: univ                Batch:  3/15	Loss 0.1001 (0.0985)
+2022-11-18 16:09:40,916:INFO: Dataset: univ                Batch:  4/15	Loss 0.0981 (0.0984)
+2022-11-18 16:09:40,917:INFO: Dataset: univ                Batch:  5/15	Loss 0.0982 (0.0984)
+2022-11-18 16:09:40,967:INFO: Dataset: univ                Batch:  6/15	Loss 0.0978 (0.0983)
+2022-11-18 16:09:40,971:INFO: Dataset: univ                Batch:  7/15	Loss 0.0983 (0.0983)
+2022-11-18 16:09:40,976:INFO: Dataset: univ                Batch:  8/15	Loss 0.0984 (0.0983)
+2022-11-18 16:09:40,980:INFO: Dataset: univ                Batch:  9/15	Loss 0.0974 (0.0982)
+2022-11-18 16:09:40,984:INFO: Dataset: univ                Batch: 10/15	Loss 0.0986 (0.0982)
+2022-11-18 16:09:40,989:INFO: Dataset: univ                Batch: 11/15	Loss 0.0988 (0.0983)
+2022-11-18 16:09:40,995:INFO: Dataset: univ                Batch: 12/15	Loss 0.0983 (0.0983)
+2022-11-18 16:09:40,999:INFO: Dataset: univ                Batch: 13/15	Loss 0.0978 (0.0982)
+2022-11-18 16:09:41,002:INFO: Dataset: univ                Batch: 14/15	Loss 0.0970 (0.0982)
+2022-11-18 16:09:41,004:INFO: Dataset: univ                Batch: 15/15	Loss 0.0968 (0.0981)
+2022-11-18 16:09:41,265:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2634 (0.2634)
+2022-11-18 16:09:41,267:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2620 (0.2627)
+2022-11-18 16:09:41,269:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2609 (0.2621)
+2022-11-18 16:09:41,270:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2603 (0.2616)
+2022-11-18 16:09:41,272:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2611 (0.2615)
+2022-11-18 16:09:41,308:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2611 (0.2615)
+2022-11-18 16:09:41,312:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2587 (0.2611)
+2022-11-18 16:09:41,316:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2599 (0.2609)
+2022-11-18 16:09:41,568:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1289 (0.1289)
+2022-11-18 16:09:41,573:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1321 (0.1305)
+2022-11-18 16:09:41,576:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1300 (0.1303)
+2022-11-18 16:09:41,578:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1320 (0.1307)
+2022-11-18 16:09:41,594:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1273 (0.1301)
+2022-11-18 16:09:41,611:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1265 (0.1295)
+2022-11-18 16:09:41,612:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1300 (0.1295)
+2022-11-18 16:09:41,613:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1315 (0.1298)
+2022-11-18 16:09:41,615:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1319 (0.1300)
+2022-11-18 16:09:41,616:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1317 (0.1302)
+2022-11-18 16:09:41,617:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1286 (0.1301)
+2022-11-18 16:09:41,619:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1311 (0.1301)
+2022-11-18 16:09:41,620:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1292 (0.1301)
+2022-11-18 16:09:41,622:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1290 (0.1300)
+2022-11-18 16:09:41,623:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1282 (0.1299)
+2022-11-18 16:09:41,624:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1267 (0.1297)
+2022-11-18 16:09:41,625:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1294 (0.1296)
+2022-11-18 16:09:41,627:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1283 (0.1296)
+2022-11-18 16:09:41,672:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:41,957:INFO: 		 ADE on eth                       dataset:	 1.7781543731689453
+2022-11-18 16:09:41,957:INFO: Average validation o:	ADE  1.7782	FDE  2.6768
+2022-11-18 16:09:41,958:INFO: - Computing loss (validation)
+2022-11-18 16:09:42,152:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6248 (0.6248)
+2022-11-18 16:09:42,153:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6221 (0.6246)
+2022-11-18 16:09:42,393:INFO: Dataset: univ                Batch: 1/3	Loss 0.0975 (0.0975)
+2022-11-18 16:09:42,394:INFO: Dataset: univ                Batch: 2/3	Loss 0.0976 (0.0976)
+2022-11-18 16:09:42,396:INFO: Dataset: univ                Batch: 3/3	Loss 0.0981 (0.0977)
+2022-11-18 16:09:42,640:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2585 (0.2585)
+2022-11-18 16:09:42,641:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2607 (0.2591)
+2022-11-18 16:09:42,893:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1311 (0.1311)
+2022-11-18 16:09:42,898:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1305 (0.1308)
+2022-11-18 16:09:42,899:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1317 (0.1311)
+2022-11-18 16:09:42,900:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1305 (0.1310)
+2022-11-18 16:09:42,901:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1315 (0.1311)
+2022-11-18 16:09:42,972:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_783.pth.tar
+2022-11-18 16:09:42,972:INFO: 
+===> EPOCH: 784 (P4)
+2022-11-18 16:09:42,973:INFO: - Computing loss (training)
+2022-11-18 16:09:43,165:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6189 (0.6189)
+2022-11-18 16:09:43,170:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6159 (0.6173)
+2022-11-18 16:09:43,172:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6193 (0.6180)
+2022-11-18 16:09:43,173:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6216 (0.6186)
+2022-11-18 16:09:43,417:INFO: Dataset: univ                Batch:  1/15	Loss 0.0979 (0.0979)
+2022-11-18 16:09:43,419:INFO: Dataset: univ                Batch:  2/15	Loss 0.0985 (0.0982)
+2022-11-18 16:09:43,494:INFO: Dataset: univ                Batch:  3/15	Loss 0.0973 (0.0979)
+2022-11-18 16:09:43,496:INFO: Dataset: univ                Batch:  4/15	Loss 0.0976 (0.0978)
+2022-11-18 16:09:43,498:INFO: Dataset: univ                Batch:  5/15	Loss 0.0977 (0.0978)
+2022-11-18 16:09:43,500:INFO: Dataset: univ                Batch:  6/15	Loss 0.0993 (0.0980)
+2022-11-18 16:09:43,502:INFO: Dataset: univ                Batch:  7/15	Loss 0.0988 (0.0981)
+2022-11-18 16:09:43,503:INFO: Dataset: univ                Batch:  8/15	Loss 0.0984 (0.0981)
+2022-11-18 16:09:43,505:INFO: Dataset: univ                Batch:  9/15	Loss 0.0999 (0.0983)
+2022-11-18 16:09:43,506:INFO: Dataset: univ                Batch: 10/15	Loss 0.0970 (0.0982)
+2022-11-18 16:09:43,509:INFO: Dataset: univ                Batch: 11/15	Loss 0.0987 (0.0982)
+2022-11-18 16:09:43,510:INFO: Dataset: univ                Batch: 12/15	Loss 0.0969 (0.0981)
+2022-11-18 16:09:43,511:INFO: Dataset: univ                Batch: 13/15	Loss 0.0974 (0.0980)
+2022-11-18 16:09:43,513:INFO: Dataset: univ                Batch: 14/15	Loss 0.0971 (0.0980)
+2022-11-18 16:09:43,515:INFO: Dataset: univ                Batch: 15/15	Loss 0.0996 (0.0980)
+2022-11-18 16:09:43,749:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2628 (0.2628)
+2022-11-18 16:09:43,750:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2609 (0.2618)
+2022-11-18 16:09:43,756:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2604 (0.2613)
+2022-11-18 16:09:43,760:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2608 (0.2612)
+2022-11-18 16:09:43,764:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2604 (0.2610)
+2022-11-18 16:09:43,818:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2604 (0.2609)
+2022-11-18 16:09:43,823:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2609 (0.2609)
+2022-11-18 16:09:43,827:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2596 (0.2608)
+2022-11-18 16:09:44,104:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1273 (0.1273)
+2022-11-18 16:09:44,108:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1298 (0.1286)
+2022-11-18 16:09:44,114:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1310 (0.1293)
+2022-11-18 16:09:44,156:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1293 (0.1293)
+2022-11-18 16:09:44,158:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1315 (0.1297)
+2022-11-18 16:09:44,161:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1321 (0.1301)
+2022-11-18 16:09:44,162:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1285 (0.1299)
+2022-11-18 16:09:44,163:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1330 (0.1303)
+2022-11-18 16:09:44,164:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1318 (0.1305)
+2022-11-18 16:09:44,166:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1284 (0.1303)
+2022-11-18 16:09:44,167:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1296 (0.1302)
+2022-11-18 16:09:44,169:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1314 (0.1303)
+2022-11-18 16:09:44,171:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1262 (0.1300)
+2022-11-18 16:09:44,172:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1266 (0.1298)
+2022-11-18 16:09:44,173:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1298 (0.1298)
+2022-11-18 16:09:44,175:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1252 (0.1295)
+2022-11-18 16:09:44,177:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1282 (0.1294)
+2022-11-18 16:09:44,179:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1270 (0.1293)
+2022-11-18 16:09:44,221:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:44,511:INFO: 		 ADE on eth                       dataset:	 1.793134093284607
+2022-11-18 16:09:44,511:INFO: Average validation o:	ADE  1.7931	FDE  2.6988
+2022-11-18 16:09:44,512:INFO: - Computing loss (validation)
+2022-11-18 16:09:44,697:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6247 (0.6247)
+2022-11-18 16:09:44,698:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6202 (0.6243)
+2022-11-18 16:09:44,964:INFO: Dataset: univ                Batch: 1/3	Loss 0.0987 (0.0987)
+2022-11-18 16:09:44,966:INFO: Dataset: univ                Batch: 2/3	Loss 0.0966 (0.0977)
+2022-11-18 16:09:44,968:INFO: Dataset: univ                Batch: 3/3	Loss 0.0973 (0.0976)
+2022-11-18 16:09:45,216:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2586 (0.2586)
+2022-11-18 16:09:45,217:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2597 (0.2588)
+2022-11-18 16:09:45,456:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1308 (0.1308)
+2022-11-18 16:09:45,460:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1319 (0.1314)
+2022-11-18 16:09:45,463:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1315 (0.1314)
+2022-11-18 16:09:45,464:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1301 (0.1311)
+2022-11-18 16:09:45,465:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1294 (0.1308)
+2022-11-18 16:09:45,523:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_784.pth.tar
+2022-11-18 16:09:45,523:INFO: 
+===> EPOCH: 785 (P4)
+2022-11-18 16:09:45,524:INFO: - Computing loss (training)
+2022-11-18 16:09:45,719:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6162 (0.6162)
+2022-11-18 16:09:45,728:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6206 (0.6185)
+2022-11-18 16:09:45,738:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6177 (0.6182)
+2022-11-18 16:09:45,739:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6160 (0.6179)
+2022-11-18 16:09:45,975:INFO: Dataset: univ                Batch:  1/15	Loss 0.0978 (0.0978)
+2022-11-18 16:09:45,979:INFO: Dataset: univ                Batch:  2/15	Loss 0.0982 (0.0980)
+2022-11-18 16:09:45,980:INFO: Dataset: univ                Batch:  3/15	Loss 0.0976 (0.0979)
+2022-11-18 16:09:45,985:INFO: Dataset: univ                Batch:  4/15	Loss 0.0984 (0.0980)
+2022-11-18 16:09:46,019:INFO: Dataset: univ                Batch:  5/15	Loss 0.0969 (0.0978)
+2022-11-18 16:09:46,025:INFO: Dataset: univ                Batch:  6/15	Loss 0.0984 (0.0979)
+2022-11-18 16:09:46,027:INFO: Dataset: univ                Batch:  7/15	Loss 0.0984 (0.0980)
+2022-11-18 16:09:46,028:INFO: Dataset: univ                Batch:  8/15	Loss 0.0974 (0.0979)
+2022-11-18 16:09:46,029:INFO: Dataset: univ                Batch:  9/15	Loss 0.0981 (0.0979)
+2022-11-18 16:09:46,031:INFO: Dataset: univ                Batch: 10/15	Loss 0.0978 (0.0979)
+2022-11-18 16:09:46,033:INFO: Dataset: univ                Batch: 11/15	Loss 0.0989 (0.0980)
+2022-11-18 16:09:46,035:INFO: Dataset: univ                Batch: 12/15	Loss 0.0979 (0.0980)
+2022-11-18 16:09:46,036:INFO: Dataset: univ                Batch: 13/15	Loss 0.0980 (0.0980)
+2022-11-18 16:09:46,038:INFO: Dataset: univ                Batch: 14/15	Loss 0.0968 (0.0979)
+2022-11-18 16:09:46,039:INFO: Dataset: univ                Batch: 15/15	Loss 0.0961 (0.0979)
+2022-11-18 16:09:46,279:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2608 (0.2608)
+2022-11-18 16:09:46,283:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2597 (0.2602)
+2022-11-18 16:09:46,286:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2603 (0.2603)
+2022-11-18 16:09:46,290:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2622 (0.2607)
+2022-11-18 16:09:46,291:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2611 (0.2608)
+2022-11-18 16:09:46,342:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2619 (0.2610)
+2022-11-18 16:09:46,347:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2606 (0.2609)
+2022-11-18 16:09:46,351:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2583 (0.2606)
+2022-11-18 16:09:46,604:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1305 (0.1305)
+2022-11-18 16:09:46,608:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1316 (0.1311)
+2022-11-18 16:09:46,609:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1311 (0.1311)
+2022-11-18 16:09:46,610:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1272 (0.1300)
+2022-11-18 16:09:46,657:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1270 (0.1294)
+2022-11-18 16:09:46,665:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1318 (0.1298)
+2022-11-18 16:09:46,666:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1309 (0.1299)
+2022-11-18 16:09:46,667:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1300 (0.1299)
+2022-11-18 16:09:46,668:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1301 (0.1300)
+2022-11-18 16:09:46,669:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1305 (0.1300)
+2022-11-18 16:09:46,672:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1286 (0.1299)
+2022-11-18 16:09:46,673:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1280 (0.1297)
+2022-11-18 16:09:46,674:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1285 (0.1296)
+2022-11-18 16:09:46,675:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1285 (0.1296)
+2022-11-18 16:09:46,676:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1296 (0.1296)
+2022-11-18 16:09:46,677:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1256 (0.1293)
+2022-11-18 16:09:46,679:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1266 (0.1291)
+2022-11-18 16:09:46,681:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1276 (0.1290)
+2022-11-18 16:09:46,724:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:46,994:INFO: 		 ADE on eth                       dataset:	 1.8343044519424438
+2022-11-18 16:09:46,994:INFO: Average validation o:	ADE  1.8343	FDE  2.7777
+2022-11-18 16:09:46,996:INFO: - Computing loss (validation)
+2022-11-18 16:09:47,175:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6243 (0.6243)
+2022-11-18 16:09:47,176:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6180 (0.6239)
+2022-11-18 16:09:47,404:INFO: Dataset: univ                Batch: 1/3	Loss 0.0966 (0.0966)
+2022-11-18 16:09:47,412:INFO: Dataset: univ                Batch: 2/3	Loss 0.0983 (0.0975)
+2022-11-18 16:09:47,413:INFO: Dataset: univ                Batch: 3/3	Loss 0.0972 (0.0974)
+2022-11-18 16:09:47,661:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2586 (0.2586)
+2022-11-18 16:09:47,663:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2586 (0.2586)
+2022-11-18 16:09:47,896:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1294 (0.1294)
+2022-11-18 16:09:47,897:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1311 (0.1303)
+2022-11-18 16:09:47,898:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1312 (0.1306)
+2022-11-18 16:09:47,903:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1304 (0.1305)
+2022-11-18 16:09:47,904:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1302 (0.1305)
+2022-11-18 16:09:47,970:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_785.pth.tar
+2022-11-18 16:09:47,970:INFO: 
+===> EPOCH: 786 (P4)
+2022-11-18 16:09:47,970:INFO: - Computing loss (training)
+2022-11-18 16:09:48,149:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6181 (0.6181)
+2022-11-18 16:09:48,152:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6157 (0.6169)
+2022-11-18 16:09:48,154:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6195 (0.6178)
+2022-11-18 16:09:48,156:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6146 (0.6173)
+2022-11-18 16:09:48,414:INFO: Dataset: univ                Batch:  1/15	Loss 0.0977 (0.0977)
+2022-11-18 16:09:48,418:INFO: Dataset: univ                Batch:  2/15	Loss 0.0969 (0.0973)
+2022-11-18 16:09:48,421:INFO: Dataset: univ                Batch:  3/15	Loss 0.0987 (0.0978)
+2022-11-18 16:09:48,423:INFO: Dataset: univ                Batch:  4/15	Loss 0.0978 (0.0978)
+2022-11-18 16:09:48,494:INFO: Dataset: univ                Batch:  5/15	Loss 0.0979 (0.0978)
+2022-11-18 16:09:48,500:INFO: Dataset: univ                Batch:  6/15	Loss 0.0972 (0.0977)
+2022-11-18 16:09:48,502:INFO: Dataset: univ                Batch:  7/15	Loss 0.0979 (0.0977)
+2022-11-18 16:09:48,503:INFO: Dataset: univ                Batch:  8/15	Loss 0.0982 (0.0978)
+2022-11-18 16:09:48,504:INFO: Dataset: univ                Batch:  9/15	Loss 0.0975 (0.0978)
+2022-11-18 16:09:48,506:INFO: Dataset: univ                Batch: 10/15	Loss 0.0983 (0.0978)
+2022-11-18 16:09:48,508:INFO: Dataset: univ                Batch: 11/15	Loss 0.0978 (0.0978)
+2022-11-18 16:09:48,509:INFO: Dataset: univ                Batch: 12/15	Loss 0.0979 (0.0978)
+2022-11-18 16:09:48,510:INFO: Dataset: univ                Batch: 13/15	Loss 0.0975 (0.0978)
+2022-11-18 16:09:48,512:INFO: Dataset: univ                Batch: 14/15	Loss 0.0971 (0.0977)
+2022-11-18 16:09:48,513:INFO: Dataset: univ                Batch: 15/15	Loss 0.0982 (0.0977)
+2022-11-18 16:09:48,762:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2615 (0.2615)
+2022-11-18 16:09:48,768:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2619 (0.2617)
+2022-11-18 16:09:48,771:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2610 (0.2615)
+2022-11-18 16:09:48,773:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2602 (0.2612)
+2022-11-18 16:09:48,775:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2597 (0.2609)
+2022-11-18 16:09:48,854:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2604 (0.2608)
+2022-11-18 16:09:48,858:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2608 (0.2608)
+2022-11-18 16:09:48,863:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2580 (0.2605)
+2022-11-18 16:09:49,132:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1289 (0.1289)
+2022-11-18 16:09:49,134:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1309 (0.1299)
+2022-11-18 16:09:49,138:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1297 (0.1298)
+2022-11-18 16:09:49,179:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1294 (0.1297)
+2022-11-18 16:09:49,181:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1308 (0.1299)
+2022-11-18 16:09:49,185:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1292 (0.1298)
+2022-11-18 16:09:49,186:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1272 (0.1295)
+2022-11-18 16:09:49,188:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1276 (0.1292)
+2022-11-18 16:09:49,189:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1321 (0.1295)
+2022-11-18 16:09:49,190:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1301 (0.1296)
+2022-11-18 16:09:49,191:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1276 (0.1294)
+2022-11-18 16:09:49,193:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1286 (0.1293)
+2022-11-18 16:09:49,194:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1297 (0.1294)
+2022-11-18 16:09:49,195:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1296 (0.1294)
+2022-11-18 16:09:49,196:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1266 (0.1292)
+2022-11-18 16:09:49,198:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1276 (0.1291)
+2022-11-18 16:09:49,199:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1260 (0.1289)
+2022-11-18 16:09:49,201:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1266 (0.1288)
+2022-11-18 16:09:49,245:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:49,528:INFO: 		 ADE on eth                       dataset:	 1.7938432693481445
+2022-11-18 16:09:49,528:INFO: Average validation o:	ADE  1.7938	FDE  2.7038
+2022-11-18 16:09:49,529:INFO: - Computing loss (validation)
+2022-11-18 16:09:49,721:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6240 (0.6240)
+2022-11-18 16:09:49,722:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6191 (0.6236)
+2022-11-18 16:09:49,968:INFO: Dataset: univ                Batch: 1/3	Loss 0.0972 (0.0972)
+2022-11-18 16:09:49,971:INFO: Dataset: univ                Batch: 2/3	Loss 0.0961 (0.0966)
+2022-11-18 16:09:49,973:INFO: Dataset: univ                Batch: 3/3	Loss 0.0986 (0.0972)
+2022-11-18 16:09:50,214:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2576 (0.2576)
+2022-11-18 16:09:50,219:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2608 (0.2584)
+2022-11-18 16:09:50,473:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1274 (0.1274)
+2022-11-18 16:09:50,474:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1313 (0.1295)
+2022-11-18 16:09:50,474:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1299 (0.1296)
+2022-11-18 16:09:50,475:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1322 (0.1303)
+2022-11-18 16:09:50,476:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1301 (0.1302)
+2022-11-18 16:09:50,533:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_786.pth.tar
+2022-11-18 16:09:50,533:INFO: 
+===> EPOCH: 787 (P4)
+2022-11-18 16:09:50,533:INFO: - Computing loss (training)
+2022-11-18 16:09:50,726:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6132 (0.6132)
+2022-11-18 16:09:50,729:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6171 (0.6151)
+2022-11-18 16:09:50,732:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6179 (0.6160)
+2022-11-18 16:09:50,736:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6192 (0.6166)
+2022-11-18 16:09:50,985:INFO: Dataset: univ                Batch:  1/15	Loss 0.0973 (0.0973)
+2022-11-18 16:09:50,993:INFO: Dataset: univ                Batch:  2/15	Loss 0.0972 (0.0973)
+2022-11-18 16:09:50,995:INFO: Dataset: univ                Batch:  3/15	Loss 0.0984 (0.0976)
+2022-11-18 16:09:50,997:INFO: Dataset: univ                Batch:  4/15	Loss 0.0984 (0.0978)
+2022-11-18 16:09:51,037:INFO: Dataset: univ                Batch:  5/15	Loss 0.0982 (0.0979)
+2022-11-18 16:09:51,044:INFO: Dataset: univ                Batch:  6/15	Loss 0.0980 (0.0979)
+2022-11-18 16:09:51,046:INFO: Dataset: univ                Batch:  7/15	Loss 0.0975 (0.0978)
+2022-11-18 16:09:51,047:INFO: Dataset: univ                Batch:  8/15	Loss 0.0982 (0.0979)
+2022-11-18 16:09:51,049:INFO: Dataset: univ                Batch:  9/15	Loss 0.0975 (0.0978)
+2022-11-18 16:09:51,050:INFO: Dataset: univ                Batch: 10/15	Loss 0.0975 (0.0978)
+2022-11-18 16:09:51,053:INFO: Dataset: univ                Batch: 11/15	Loss 0.0976 (0.0978)
+2022-11-18 16:09:51,054:INFO: Dataset: univ                Batch: 12/15	Loss 0.0971 (0.0977)
+2022-11-18 16:09:51,055:INFO: Dataset: univ                Batch: 13/15	Loss 0.0971 (0.0977)
+2022-11-18 16:09:51,057:INFO: Dataset: univ                Batch: 14/15	Loss 0.0971 (0.0976)
+2022-11-18 16:09:51,058:INFO: Dataset: univ                Batch: 15/15	Loss 0.0971 (0.0976)
+2022-11-18 16:09:51,310:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2621 (0.2621)
+2022-11-18 16:09:51,313:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2603 (0.2612)
+2022-11-18 16:09:51,315:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2618 (0.2614)
+2022-11-18 16:09:51,317:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2611 (0.2613)
+2022-11-18 16:09:51,319:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2619 (0.2614)
+2022-11-18 16:09:51,387:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2584 (0.2609)
+2022-11-18 16:09:51,391:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2595 (0.2607)
+2022-11-18 16:09:51,395:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2576 (0.2604)
+2022-11-18 16:09:51,656:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1287 (0.1287)
+2022-11-18 16:09:51,659:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1257 (0.1274)
+2022-11-18 16:09:51,663:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1277 (0.1275)
+2022-11-18 16:09:51,666:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1286 (0.1278)
+2022-11-18 16:09:51,691:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1311 (0.1284)
+2022-11-18 16:09:51,711:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1287 (0.1285)
+2022-11-18 16:09:51,712:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1305 (0.1288)
+2022-11-18 16:09:51,714:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1284 (0.1287)
+2022-11-18 16:09:51,715:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1285 (0.1287)
+2022-11-18 16:09:51,716:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1293 (0.1288)
+2022-11-18 16:09:51,717:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1300 (0.1289)
+2022-11-18 16:09:51,719:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1284 (0.1289)
+2022-11-18 16:09:51,720:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1276 (0.1288)
+2022-11-18 16:09:51,722:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1271 (0.1286)
+2022-11-18 16:09:51,723:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1294 (0.1287)
+2022-11-18 16:09:51,724:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1276 (0.1286)
+2022-11-18 16:09:51,725:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1286 (0.1286)
+2022-11-18 16:09:51,727:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1280 (0.1286)
+2022-11-18 16:09:51,773:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:52,059:INFO: 		 ADE on eth                       dataset:	 1.7726904153823853
+2022-11-18 16:09:52,059:INFO: Average validation o:	ADE  1.7727	FDE  2.6763
+2022-11-18 16:09:52,060:INFO: - Computing loss (validation)
+2022-11-18 16:09:52,243:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6230 (0.6230)
+2022-11-18 16:09:52,244:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6252 (0.6232)
+2022-11-18 16:09:52,477:INFO: Dataset: univ                Batch: 1/3	Loss 0.0964 (0.0964)
+2022-11-18 16:09:52,479:INFO: Dataset: univ                Batch: 2/3	Loss 0.0974 (0.0969)
+2022-11-18 16:09:52,481:INFO: Dataset: univ                Batch: 3/3	Loss 0.0975 (0.0970)
+2022-11-18 16:09:52,714:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2580 (0.2580)
+2022-11-18 16:09:52,715:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2587 (0.2582)
+2022-11-18 16:09:52,986:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1274 (0.1274)
+2022-11-18 16:09:52,987:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1296 (0.1285)
+2022-11-18 16:09:52,988:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1312 (0.1295)
+2022-11-18 16:09:52,989:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1305 (0.1297)
+2022-11-18 16:09:52,989:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1311 (0.1300)
+2022-11-18 16:09:53,054:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_787.pth.tar
+2022-11-18 16:09:53,054:INFO: 
+===> EPOCH: 788 (P4)
+2022-11-18 16:09:53,055:INFO: - Computing loss (training)
+2022-11-18 16:09:53,237:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6139 (0.6139)
+2022-11-18 16:09:53,251:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6147 (0.6143)
+2022-11-18 16:09:53,253:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6180 (0.6155)
+2022-11-18 16:09:53,254:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6165 (0.6157)
+2022-11-18 16:09:53,501:INFO: Dataset: univ                Batch:  1/15	Loss 0.0986 (0.0986)
+2022-11-18 16:09:53,505:INFO: Dataset: univ                Batch:  2/15	Loss 0.0973 (0.0979)
+2022-11-18 16:09:53,510:INFO: Dataset: univ                Batch:  3/15	Loss 0.0968 (0.0975)
+2022-11-18 16:09:53,512:INFO: Dataset: univ                Batch:  4/15	Loss 0.0981 (0.0977)
+2022-11-18 16:09:53,546:INFO: Dataset: univ                Batch:  5/15	Loss 0.0988 (0.0979)
+2022-11-18 16:09:53,551:INFO: Dataset: univ                Batch:  6/15	Loss 0.0979 (0.0979)
+2022-11-18 16:09:53,552:INFO: Dataset: univ                Batch:  7/15	Loss 0.0974 (0.0978)
+2022-11-18 16:09:53,554:INFO: Dataset: univ                Batch:  8/15	Loss 0.0968 (0.0977)
+2022-11-18 16:09:53,555:INFO: Dataset: univ                Batch:  9/15	Loss 0.0979 (0.0977)
+2022-11-18 16:09:53,556:INFO: Dataset: univ                Batch: 10/15	Loss 0.0983 (0.0978)
+2022-11-18 16:09:53,558:INFO: Dataset: univ                Batch: 11/15	Loss 0.0972 (0.0977)
+2022-11-18 16:09:53,559:INFO: Dataset: univ                Batch: 12/15	Loss 0.0978 (0.0977)
+2022-11-18 16:09:53,561:INFO: Dataset: univ                Batch: 13/15	Loss 0.0969 (0.0976)
+2022-11-18 16:09:53,562:INFO: Dataset: univ                Batch: 14/15	Loss 0.0962 (0.0975)
+2022-11-18 16:09:53,563:INFO: Dataset: univ                Batch: 15/15	Loss 0.0962 (0.0975)
+2022-11-18 16:09:53,821:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2613 (0.2613)
+2022-11-18 16:09:53,823:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2605 (0.2609)
+2022-11-18 16:09:53,825:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2605 (0.2608)
+2022-11-18 16:09:53,829:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2613 (0.2609)
+2022-11-18 16:09:53,830:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2594 (0.2606)
+2022-11-18 16:09:53,858:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2600 (0.2605)
+2022-11-18 16:09:53,859:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2596 (0.2604)
+2022-11-18 16:09:53,861:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2593 (0.2602)
+2022-11-18 16:09:54,130:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1315 (0.1315)
+2022-11-18 16:09:54,133:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1284 (0.1300)
+2022-11-18 16:09:54,134:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1280 (0.1293)
+2022-11-18 16:09:54,136:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1309 (0.1297)
+2022-11-18 16:09:54,147:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1297 (0.1297)
+2022-11-18 16:09:54,167:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1296 (0.1297)
+2022-11-18 16:09:54,168:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1258 (0.1292)
+2022-11-18 16:09:54,169:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1289 (0.1292)
+2022-11-18 16:09:54,170:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1309 (0.1294)
+2022-11-18 16:09:54,171:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1262 (0.1291)
+2022-11-18 16:09:54,173:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1282 (0.1290)
+2022-11-18 16:09:54,174:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1281 (0.1289)
+2022-11-18 16:09:54,175:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1267 (0.1287)
+2022-11-18 16:09:54,177:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1278 (0.1287)
+2022-11-18 16:09:54,178:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1270 (0.1286)
+2022-11-18 16:09:54,179:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1237 (0.1283)
+2022-11-18 16:09:54,180:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1277 (0.1282)
+2022-11-18 16:09:54,182:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1303 (0.1283)
+2022-11-18 16:09:54,229:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:54,502:INFO: 		 ADE on eth                       dataset:	 1.7993462085723877
+2022-11-18 16:09:54,502:INFO: Average validation o:	ADE  1.7993	FDE  2.7050
+2022-11-18 16:09:54,503:INFO: - Computing loss (validation)
+2022-11-18 16:09:54,692:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6229 (0.6229)
+2022-11-18 16:09:54,693:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6237 (0.6230)
+2022-11-18 16:09:54,930:INFO: Dataset: univ                Batch: 1/3	Loss 0.0964 (0.0964)
+2022-11-18 16:09:54,940:INFO: Dataset: univ                Batch: 2/3	Loss 0.0967 (0.0965)
+2022-11-18 16:09:54,941:INFO: Dataset: univ                Batch: 3/3	Loss 0.0979 (0.0969)
+2022-11-18 16:09:55,181:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2583 (0.2583)
+2022-11-18 16:09:55,181:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2573 (0.2580)
+2022-11-18 16:09:55,416:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1273 (0.1273)
+2022-11-18 16:09:55,418:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1306 (0.1291)
+2022-11-18 16:09:55,419:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1308 (0.1297)
+2022-11-18 16:09:55,421:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1292 (0.1295)
+2022-11-18 16:09:55,422:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1306 (0.1297)
+2022-11-18 16:09:55,480:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_788.pth.tar
+2022-11-18 16:09:55,480:INFO: 
+===> EPOCH: 789 (P4)
+2022-11-18 16:09:55,481:INFO: - Computing loss (training)
+2022-11-18 16:09:55,665:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6184 (0.6184)
+2022-11-18 16:09:55,667:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6166 (0.6175)
+2022-11-18 16:09:55,670:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6118 (0.6156)
+2022-11-18 16:09:55,676:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6127 (0.6151)
+2022-11-18 16:09:55,929:INFO: Dataset: univ                Batch:  1/15	Loss 0.0968 (0.0968)
+2022-11-18 16:09:55,933:INFO: Dataset: univ                Batch:  2/15	Loss 0.0970 (0.0969)
+2022-11-18 16:09:55,935:INFO: Dataset: univ                Batch:  3/15	Loss 0.0980 (0.0972)
+2022-11-18 16:09:56,003:INFO: Dataset: univ                Batch:  4/15	Loss 0.0988 (0.0976)
+2022-11-18 16:09:56,004:INFO: Dataset: univ                Batch:  5/15	Loss 0.0980 (0.0977)
+2022-11-18 16:09:56,008:INFO: Dataset: univ                Batch:  6/15	Loss 0.0976 (0.0977)
+2022-11-18 16:09:56,010:INFO: Dataset: univ                Batch:  7/15	Loss 0.0983 (0.0978)
+2022-11-18 16:09:56,011:INFO: Dataset: univ                Batch:  8/15	Loss 0.0969 (0.0976)
+2022-11-18 16:09:56,012:INFO: Dataset: univ                Batch:  9/15	Loss 0.0977 (0.0977)
+2022-11-18 16:09:56,014:INFO: Dataset: univ                Batch: 10/15	Loss 0.0965 (0.0975)
+2022-11-18 16:09:56,016:INFO: Dataset: univ                Batch: 11/15	Loss 0.0974 (0.0975)
+2022-11-18 16:09:56,018:INFO: Dataset: univ                Batch: 12/15	Loss 0.0978 (0.0975)
+2022-11-18 16:09:56,019:INFO: Dataset: univ                Batch: 13/15	Loss 0.0971 (0.0975)
+2022-11-18 16:09:56,020:INFO: Dataset: univ                Batch: 14/15	Loss 0.0968 (0.0974)
+2022-11-18 16:09:56,021:INFO: Dataset: univ                Batch: 15/15	Loss 0.0962 (0.0974)
+2022-11-18 16:09:56,265:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2611 (0.2611)
+2022-11-18 16:09:56,267:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2600 (0.2605)
+2022-11-18 16:09:56,272:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2612 (0.2607)
+2022-11-18 16:09:56,273:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2600 (0.2606)
+2022-11-18 16:09:56,275:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2605 (0.2606)
+2022-11-18 16:09:56,343:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2605 (0.2605)
+2022-11-18 16:09:56,347:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2586 (0.2603)
+2022-11-18 16:09:56,351:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2592 (0.2601)
+2022-11-18 16:09:56,637:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1291 (0.1291)
+2022-11-18 16:09:56,640:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1265 (0.1278)
+2022-11-18 16:09:56,713:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1287 (0.1281)
+2022-11-18 16:09:56,714:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1289 (0.1283)
+2022-11-18 16:09:56,715:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1315 (0.1289)
+2022-11-18 16:09:56,718:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1258 (0.1284)
+2022-11-18 16:09:56,719:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1300 (0.1286)
+2022-11-18 16:09:56,721:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1293 (0.1287)
+2022-11-18 16:09:56,722:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1274 (0.1286)
+2022-11-18 16:09:56,723:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1281 (0.1285)
+2022-11-18 16:09:56,725:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1283 (0.1285)
+2022-11-18 16:09:56,727:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1274 (0.1284)
+2022-11-18 16:09:56,728:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1305 (0.1286)
+2022-11-18 16:09:56,729:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1289 (0.1286)
+2022-11-18 16:09:56,731:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1270 (0.1285)
+2022-11-18 16:09:56,733:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1270 (0.1284)
+2022-11-18 16:09:56,735:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1252 (0.1282)
+2022-11-18 16:09:56,736:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1266 (0.1281)
+2022-11-18 16:09:56,784:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:57,052:INFO: 		 ADE on eth                       dataset:	 1.791145920753479
+2022-11-18 16:09:57,052:INFO: Average validation o:	ADE  1.7911	FDE  2.7263
+2022-11-18 16:09:57,053:INFO: - Computing loss (validation)
+2022-11-18 16:09:57,242:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6233 (0.6233)
+2022-11-18 16:09:57,243:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6148 (0.6226)
+2022-11-18 16:09:57,481:INFO: Dataset: univ                Batch: 1/3	Loss 0.0973 (0.0973)
+2022-11-18 16:09:57,484:INFO: Dataset: univ                Batch: 2/3	Loss 0.0974 (0.0973)
+2022-11-18 16:09:57,490:INFO: Dataset: univ                Batch: 3/3	Loss 0.0956 (0.0967)
+2022-11-18 16:09:57,733:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2577 (0.2577)
+2022-11-18 16:09:57,734:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2584 (0.2579)
+2022-11-18 16:09:57,980:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1281 (0.1281)
+2022-11-18 16:09:57,983:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1303 (0.1292)
+2022-11-18 16:09:57,985:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1299 (0.1295)
+2022-11-18 16:09:57,986:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1297 (0.1295)
+2022-11-18 16:09:57,991:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1295 (0.1295)
+2022-11-18 16:09:58,045:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_789.pth.tar
+2022-11-18 16:09:58,045:INFO: 
+===> EPOCH: 790 (P4)
+2022-11-18 16:09:58,046:INFO: - Computing loss (training)
+2022-11-18 16:09:58,244:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6159 (0.6159)
+2022-11-18 16:09:58,249:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6137 (0.6148)
+2022-11-18 16:09:58,251:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6136 (0.6144)
+2022-11-18 16:09:58,253:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6147 (0.6145)
+2022-11-18 16:09:58,574:INFO: Dataset: univ                Batch:  1/15	Loss 0.0972 (0.0972)
+2022-11-18 16:09:58,577:INFO: Dataset: univ                Batch:  2/15	Loss 0.0975 (0.0973)
+2022-11-18 16:09:58,580:INFO: Dataset: univ                Batch:  3/15	Loss 0.0982 (0.0976)
+2022-11-18 16:09:58,582:INFO: Dataset: univ                Batch:  4/15	Loss 0.0971 (0.0975)
+2022-11-18 16:09:58,583:INFO: Dataset: univ                Batch:  5/15	Loss 0.0968 (0.0973)
+2022-11-18 16:09:58,587:INFO: Dataset: univ                Batch:  6/15	Loss 0.0976 (0.0974)
+2022-11-18 16:09:58,588:INFO: Dataset: univ                Batch:  7/15	Loss 0.0975 (0.0974)
+2022-11-18 16:09:58,590:INFO: Dataset: univ                Batch:  8/15	Loss 0.0974 (0.0974)
+2022-11-18 16:09:58,592:INFO: Dataset: univ                Batch:  9/15	Loss 0.0988 (0.0975)
+2022-11-18 16:09:58,594:INFO: Dataset: univ                Batch: 10/15	Loss 0.0973 (0.0975)
+2022-11-18 16:09:58,595:INFO: Dataset: univ                Batch: 11/15	Loss 0.0978 (0.0975)
+2022-11-18 16:09:58,598:INFO: Dataset: univ                Batch: 12/15	Loss 0.0977 (0.0975)
+2022-11-18 16:09:58,600:INFO: Dataset: univ                Batch: 13/15	Loss 0.0961 (0.0974)
+2022-11-18 16:09:58,602:INFO: Dataset: univ                Batch: 14/15	Loss 0.0965 (0.0973)
+2022-11-18 16:09:58,604:INFO: Dataset: univ                Batch: 15/15	Loss 0.0948 (0.0973)
+2022-11-18 16:09:58,874:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2609 (0.2609)
+2022-11-18 16:09:58,876:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2589 (0.2599)
+2022-11-18 16:09:58,883:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2629 (0.2609)
+2022-11-18 16:09:58,885:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2593 (0.2605)
+2022-11-18 16:09:58,886:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2602 (0.2604)
+2022-11-18 16:09:58,918:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2594 (0.2603)
+2022-11-18 16:09:58,922:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2593 (0.2601)
+2022-11-18 16:09:58,925:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2596 (0.2601)
+2022-11-18 16:09:59,226:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1280 (0.1280)
+2022-11-18 16:09:59,230:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1264 (0.1272)
+2022-11-18 16:09:59,233:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1289 (0.1278)
+2022-11-18 16:09:59,235:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1283 (0.1279)
+2022-11-18 16:09:59,236:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1281 (0.1279)
+2022-11-18 16:09:59,239:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1312 (0.1285)
+2022-11-18 16:09:59,240:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1287 (0.1286)
+2022-11-18 16:09:59,242:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1256 (0.1282)
+2022-11-18 16:09:59,243:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1300 (0.1284)
+2022-11-18 16:09:59,244:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1284 (0.1284)
+2022-11-18 16:09:59,246:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1261 (0.1282)
+2022-11-18 16:09:59,248:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1286 (0.1282)
+2022-11-18 16:09:59,250:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1260 (0.1281)
+2022-11-18 16:09:59,252:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1283 (0.1281)
+2022-11-18 16:09:59,254:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1291 (0.1282)
+2022-11-18 16:09:59,256:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1274 (0.1281)
+2022-11-18 16:09:59,258:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1252 (0.1280)
+2022-11-18 16:09:59,260:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1273 (0.1279)
+2022-11-18 16:09:59,307:INFO: - Computing ADE (validation o)
+2022-11-18 16:09:59,580:INFO: 		 ADE on eth                       dataset:	 1.7864179611206055
+2022-11-18 16:09:59,581:INFO: Average validation o:	ADE  1.7864	FDE  2.6930
+2022-11-18 16:09:59,582:INFO: - Computing loss (validation)
+2022-11-18 16:09:59,765:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6224 (0.6224)
+2022-11-18 16:09:59,766:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6202 (0.6222)
+2022-11-18 16:10:00,019:INFO: Dataset: univ                Batch: 1/3	Loss 0.0958 (0.0958)
+2022-11-18 16:10:00,023:INFO: Dataset: univ                Batch: 2/3	Loss 0.0970 (0.0965)
+2022-11-18 16:10:00,024:INFO: Dataset: univ                Batch: 3/3	Loss 0.0969 (0.0966)
+2022-11-18 16:10:00,345:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2577 (0.2577)
+2022-11-18 16:10:00,346:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2578 (0.2577)
+2022-11-18 16:10:00,650:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1289 (0.1289)
+2022-11-18 16:10:00,651:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1295 (0.1292)
+2022-11-18 16:10:00,653:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1310 (0.1298)
+2022-11-18 16:10:00,654:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1296 (0.1298)
+2022-11-18 16:10:00,655:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1274 (0.1293)
+2022-11-18 16:10:00,723:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_790.pth.tar
+2022-11-18 16:10:00,723:INFO: 
+===> EPOCH: 791 (P4)
+2022-11-18 16:10:00,724:INFO: - Computing loss (training)
+2022-11-18 16:10:00,979:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6155 (0.6155)
+2022-11-18 16:10:00,982:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6122 (0.6138)
+2022-11-18 16:10:00,986:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6127 (0.6134)
+2022-11-18 16:10:00,990:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6156 (0.6138)
+2022-11-18 16:10:01,248:INFO: Dataset: univ                Batch:  1/15	Loss 0.0970 (0.0970)
+2022-11-18 16:10:01,252:INFO: Dataset: univ                Batch:  2/15	Loss 0.0971 (0.0970)
+2022-11-18 16:10:01,254:INFO: Dataset: univ                Batch:  3/15	Loss 0.0967 (0.0969)
+2022-11-18 16:10:01,256:INFO: Dataset: univ                Batch:  4/15	Loss 0.0974 (0.0970)
+2022-11-18 16:10:01,302:INFO: Dataset: univ                Batch:  5/15	Loss 0.0979 (0.0972)
+2022-11-18 16:10:01,311:INFO: Dataset: univ                Batch:  6/15	Loss 0.0979 (0.0973)
+2022-11-18 16:10:01,312:INFO: Dataset: univ                Batch:  7/15	Loss 0.0971 (0.0973)
+2022-11-18 16:10:01,313:INFO: Dataset: univ                Batch:  8/15	Loss 0.0970 (0.0973)
+2022-11-18 16:10:01,315:INFO: Dataset: univ                Batch:  9/15	Loss 0.0974 (0.0973)
+2022-11-18 16:10:01,316:INFO: Dataset: univ                Batch: 10/15	Loss 0.0966 (0.0972)
+2022-11-18 16:10:01,317:INFO: Dataset: univ                Batch: 11/15	Loss 0.0978 (0.0973)
+2022-11-18 16:10:01,319:INFO: Dataset: univ                Batch: 12/15	Loss 0.0969 (0.0972)
+2022-11-18 16:10:01,320:INFO: Dataset: univ                Batch: 13/15	Loss 0.0969 (0.0972)
+2022-11-18 16:10:01,322:INFO: Dataset: univ                Batch: 14/15	Loss 0.0982 (0.0973)
+2022-11-18 16:10:01,323:INFO: Dataset: univ                Batch: 15/15	Loss 0.0952 (0.0972)
+2022-11-18 16:10:01,575:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2612 (0.2612)
+2022-11-18 16:10:01,577:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2598 (0.2605)
+2022-11-18 16:10:01,578:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2614 (0.2608)
+2022-11-18 16:10:01,580:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2595 (0.2605)
+2022-11-18 16:10:01,581:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2596 (0.2603)
+2022-11-18 16:10:01,617:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2591 (0.2601)
+2022-11-18 16:10:01,621:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2602 (0.2601)
+2022-11-18 16:10:01,624:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2586 (0.2600)
+2022-11-18 16:10:01,997:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1287 (0.1287)
+2022-11-18 16:10:01,999:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1296 (0.1291)
+2022-11-18 16:10:02,001:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1281 (0.1288)
+2022-11-18 16:10:02,002:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1262 (0.1281)
+2022-11-18 16:10:02,003:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1300 (0.1285)
+2022-11-18 16:10:02,007:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1308 (0.1289)
+2022-11-18 16:10:02,009:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1288 (0.1289)
+2022-11-18 16:10:02,010:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1279 (0.1288)
+2022-11-18 16:10:02,011:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1273 (0.1286)
+2022-11-18 16:10:02,013:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1261 (0.1284)
+2022-11-18 16:10:02,015:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1280 (0.1283)
+2022-11-18 16:10:02,018:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1287 (0.1284)
+2022-11-18 16:10:02,020:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1263 (0.1282)
+2022-11-18 16:10:02,022:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1291 (0.1283)
+2022-11-18 16:10:02,024:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1262 (0.1281)
+2022-11-18 16:10:02,026:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1246 (0.1279)
+2022-11-18 16:10:02,028:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1271 (0.1279)
+2022-11-18 16:10:02,030:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1254 (0.1277)
+2022-11-18 16:10:02,079:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:02,362:INFO: 		 ADE on eth                       dataset:	 1.8233667612075806
+2022-11-18 16:10:02,363:INFO: Average validation o:	ADE  1.8234	FDE  2.7719
+2022-11-18 16:10:02,364:INFO: - Computing loss (validation)
+2022-11-18 16:10:02,544:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6220 (0.6220)
+2022-11-18 16:10:02,545:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6198 (0.6218)
+2022-11-18 16:10:02,792:INFO: Dataset: univ                Batch: 1/3	Loss 0.0960 (0.0960)
+2022-11-18 16:10:02,794:INFO: Dataset: univ                Batch: 2/3	Loss 0.0959 (0.0959)
+2022-11-18 16:10:02,795:INFO: Dataset: univ                Batch: 3/3	Loss 0.0978 (0.0965)
+2022-11-18 16:10:03,029:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2573 (0.2573)
+2022-11-18 16:10:03,033:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2584 (0.2576)
+2022-11-18 16:10:03,284:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1291 (0.1291)
+2022-11-18 16:10:03,287:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1283 (0.1287)
+2022-11-18 16:10:03,288:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1301 (0.1292)
+2022-11-18 16:10:03,290:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1281 (0.1289)
+2022-11-18 16:10:03,292:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1299 (0.1291)
+2022-11-18 16:10:03,351:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_791.pth.tar
+2022-11-18 16:10:03,351:INFO: 
+===> EPOCH: 792 (P4)
+2022-11-18 16:10:03,352:INFO: - Computing loss (training)
+2022-11-18 16:10:03,544:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6119 (0.6119)
+2022-11-18 16:10:03,548:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6138 (0.6128)
+2022-11-18 16:10:03,558:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6128 (0.6128)
+2022-11-18 16:10:03,560:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6151 (0.6132)
+2022-11-18 16:10:03,812:INFO: Dataset: univ                Batch:  1/15	Loss 0.0976 (0.0976)
+2022-11-18 16:10:03,815:INFO: Dataset: univ                Batch:  2/15	Loss 0.0969 (0.0972)
+2022-11-18 16:10:03,817:INFO: Dataset: univ                Batch:  3/15	Loss 0.0978 (0.0974)
+2022-11-18 16:10:03,818:INFO: Dataset: univ                Batch:  4/15	Loss 0.0974 (0.0974)
+2022-11-18 16:10:03,844:INFO: Dataset: univ                Batch:  5/15	Loss 0.0966 (0.0972)
+2022-11-18 16:10:03,848:INFO: Dataset: univ                Batch:  6/15	Loss 0.0976 (0.0973)
+2022-11-18 16:10:03,849:INFO: Dataset: univ                Batch:  7/15	Loss 0.0972 (0.0973)
+2022-11-18 16:10:03,851:INFO: Dataset: univ                Batch:  8/15	Loss 0.0980 (0.0974)
+2022-11-18 16:10:03,852:INFO: Dataset: univ                Batch:  9/15	Loss 0.0965 (0.0973)
+2022-11-18 16:10:03,853:INFO: Dataset: univ                Batch: 10/15	Loss 0.0975 (0.0973)
+2022-11-18 16:10:03,854:INFO: Dataset: univ                Batch: 11/15	Loss 0.0965 (0.0972)
+2022-11-18 16:10:03,856:INFO: Dataset: univ                Batch: 12/15	Loss 0.0977 (0.0972)
+2022-11-18 16:10:03,858:INFO: Dataset: univ                Batch: 13/15	Loss 0.0967 (0.0972)
+2022-11-18 16:10:03,859:INFO: Dataset: univ                Batch: 14/15	Loss 0.0970 (0.0972)
+2022-11-18 16:10:03,860:INFO: Dataset: univ                Batch: 15/15	Loss 0.0945 (0.0972)
+2022-11-18 16:10:04,117:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2605 (0.2605)
+2022-11-18 16:10:04,122:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2619 (0.2612)
+2022-11-18 16:10:04,123:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2616 (0.2613)
+2022-11-18 16:10:04,128:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2596 (0.2609)
+2022-11-18 16:10:04,129:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2599 (0.2607)
+2022-11-18 16:10:04,171:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2590 (0.2604)
+2022-11-18 16:10:04,172:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2584 (0.2601)
+2022-11-18 16:10:04,173:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2580 (0.2599)
+2022-11-18 16:10:04,439:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1286 (0.1286)
+2022-11-18 16:10:04,440:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1283 (0.1284)
+2022-11-18 16:10:04,493:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1249 (0.1273)
+2022-11-18 16:10:04,495:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1285 (0.1276)
+2022-11-18 16:10:04,496:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1299 (0.1280)
+2022-11-18 16:10:04,500:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1265 (0.1278)
+2022-11-18 16:10:04,501:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1297 (0.1280)
+2022-11-18 16:10:04,503:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1289 (0.1282)
+2022-11-18 16:10:04,504:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1309 (0.1285)
+2022-11-18 16:10:04,505:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1270 (0.1283)
+2022-11-18 16:10:04,508:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1277 (0.1283)
+2022-11-18 16:10:04,509:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1294 (0.1284)
+2022-11-18 16:10:04,510:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1285 (0.1284)
+2022-11-18 16:10:04,511:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1265 (0.1283)
+2022-11-18 16:10:04,513:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1255 (0.1281)
+2022-11-18 16:10:04,515:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1251 (0.1279)
+2022-11-18 16:10:04,517:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1244 (0.1277)
+2022-11-18 16:10:04,518:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1249 (0.1276)
+2022-11-18 16:10:04,565:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:04,839:INFO: 		 ADE on eth                       dataset:	 1.8147035837173462
+2022-11-18 16:10:04,839:INFO: Average validation o:	ADE  1.8147	FDE  2.7245
+2022-11-18 16:10:04,840:INFO: - Computing loss (validation)
+2022-11-18 16:10:05,030:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6216 (0.6216)
+2022-11-18 16:10:05,031:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6191 (0.6215)
+2022-11-18 16:10:05,293:INFO: Dataset: univ                Batch: 1/3	Loss 0.0961 (0.0961)
+2022-11-18 16:10:05,295:INFO: Dataset: univ                Batch: 2/3	Loss 0.0975 (0.0967)
+2022-11-18 16:10:05,296:INFO: Dataset: univ                Batch: 3/3	Loss 0.0955 (0.0963)
+2022-11-18 16:10:05,544:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2576 (0.2576)
+2022-11-18 16:10:05,548:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2571 (0.2575)
+2022-11-18 16:10:05,808:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1266 (0.1266)
+2022-11-18 16:10:05,813:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1279 (0.1273)
+2022-11-18 16:10:05,814:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1306 (0.1284)
+2022-11-18 16:10:05,815:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1297 (0.1287)
+2022-11-18 16:10:05,815:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1297 (0.1289)
+2022-11-18 16:10:05,880:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_792.pth.tar
+2022-11-18 16:10:05,880:INFO: 
+===> EPOCH: 793 (P4)
+2022-11-18 16:10:05,881:INFO: - Computing loss (training)
+2022-11-18 16:10:06,073:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6091 (0.6091)
+2022-11-18 16:10:06,077:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6170 (0.6132)
+2022-11-18 16:10:06,088:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6102 (0.6122)
+2022-11-18 16:10:06,090:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6139 (0.6125)
+2022-11-18 16:10:06,406:INFO: Dataset: univ                Batch:  1/15	Loss 0.0975 (0.0975)
+2022-11-18 16:10:06,408:INFO: Dataset: univ                Batch:  2/15	Loss 0.0974 (0.0975)
+2022-11-18 16:10:06,409:INFO: Dataset: univ                Batch:  3/15	Loss 0.0975 (0.0975)
+2022-11-18 16:10:06,412:INFO: Dataset: univ                Batch:  4/15	Loss 0.0969 (0.0973)
+2022-11-18 16:10:06,413:INFO: Dataset: univ                Batch:  5/15	Loss 0.0974 (0.0973)
+2022-11-18 16:10:06,417:INFO: Dataset: univ                Batch:  6/15	Loss 0.0982 (0.0975)
+2022-11-18 16:10:06,418:INFO: Dataset: univ                Batch:  7/15	Loss 0.0979 (0.0975)
+2022-11-18 16:10:06,420:INFO: Dataset: univ                Batch:  8/15	Loss 0.0976 (0.0975)
+2022-11-18 16:10:06,421:INFO: Dataset: univ                Batch:  9/15	Loss 0.0974 (0.0975)
+2022-11-18 16:10:06,422:INFO: Dataset: univ                Batch: 10/15	Loss 0.0973 (0.0975)
+2022-11-18 16:10:06,423:INFO: Dataset: univ                Batch: 11/15	Loss 0.0963 (0.0974)
+2022-11-18 16:10:06,425:INFO: Dataset: univ                Batch: 12/15	Loss 0.0965 (0.0973)
+2022-11-18 16:10:06,427:INFO: Dataset: univ                Batch: 13/15	Loss 0.0956 (0.0972)
+2022-11-18 16:10:06,429:INFO: Dataset: univ                Batch: 14/15	Loss 0.0958 (0.0971)
+2022-11-18 16:10:06,431:INFO: Dataset: univ                Batch: 15/15	Loss 0.0972 (0.0971)
+2022-11-18 16:10:06,698:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2589 (0.2589)
+2022-11-18 16:10:06,703:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2608 (0.2598)
+2022-11-18 16:10:06,705:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2615 (0.2604)
+2022-11-18 16:10:06,709:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2600 (0.2603)
+2022-11-18 16:10:06,713:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2601 (0.2603)
+2022-11-18 16:10:06,753:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2599 (0.2602)
+2022-11-18 16:10:06,755:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2586 (0.2600)
+2022-11-18 16:10:06,756:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2584 (0.2598)
+2022-11-18 16:10:07,037:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1286 (0.1286)
+2022-11-18 16:10:07,041:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1256 (0.1270)
+2022-11-18 16:10:07,043:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1279 (0.1273)
+2022-11-18 16:10:07,084:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1263 (0.1270)
+2022-11-18 16:10:07,087:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1279 (0.1272)
+2022-11-18 16:10:07,099:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1274 (0.1272)
+2022-11-18 16:10:07,101:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1304 (0.1276)
+2022-11-18 16:10:07,103:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1264 (0.1275)
+2022-11-18 16:10:07,106:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1277 (0.1275)
+2022-11-18 16:10:07,108:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1280 (0.1276)
+2022-11-18 16:10:07,110:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1299 (0.1278)
+2022-11-18 16:10:07,114:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1261 (0.1276)
+2022-11-18 16:10:07,116:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1276 (0.1276)
+2022-11-18 16:10:07,117:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1262 (0.1275)
+2022-11-18 16:10:07,119:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1266 (0.1275)
+2022-11-18 16:10:07,121:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1265 (0.1274)
+2022-11-18 16:10:07,123:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1262 (0.1273)
+2022-11-18 16:10:07,126:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1282 (0.1274)
+2022-11-18 16:10:07,179:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:07,694:INFO: 		 ADE on eth                       dataset:	 1.8242228031158447
+2022-11-18 16:10:07,694:INFO: Average validation o:	ADE  1.8242	FDE  2.7436
+2022-11-18 16:10:07,695:INFO: - Computing loss (validation)
+2022-11-18 16:10:07,895:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6210 (0.6210)
+2022-11-18 16:10:07,897:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6206 (0.6210)
+2022-11-18 16:10:08,173:INFO: Dataset: univ                Batch: 1/3	Loss 0.0968 (0.0968)
+2022-11-18 16:10:08,174:INFO: Dataset: univ                Batch: 2/3	Loss 0.0964 (0.0966)
+2022-11-18 16:10:08,175:INFO: Dataset: univ                Batch: 3/3	Loss 0.0954 (0.0962)
+2022-11-18 16:10:08,405:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2568 (0.2568)
+2022-11-18 16:10:08,406:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2588 (0.2574)
+2022-11-18 16:10:08,641:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1311 (0.1311)
+2022-11-18 16:10:08,645:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1255 (0.1283)
+2022-11-18 16:10:08,646:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1295 (0.1287)
+2022-11-18 16:10:08,647:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1276 (0.1284)
+2022-11-18 16:10:08,648:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1301 (0.1288)
+2022-11-18 16:10:08,708:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_793.pth.tar
+2022-11-18 16:10:08,708:INFO: 
+===> EPOCH: 794 (P4)
+2022-11-18 16:10:08,709:INFO: - Computing loss (training)
+2022-11-18 16:10:08,897:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6111 (0.6111)
+2022-11-18 16:10:08,910:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6119 (0.6115)
+2022-11-18 16:10:08,912:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6115 (0.6115)
+2022-11-18 16:10:08,914:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6129 (0.6117)
+2022-11-18 16:10:09,143:INFO: Dataset: univ                Batch:  1/15	Loss 0.0958 (0.0958)
+2022-11-18 16:10:09,204:INFO: Dataset: univ                Batch:  2/15	Loss 0.0965 (0.0962)
+2022-11-18 16:10:09,206:INFO: Dataset: univ                Batch:  3/15	Loss 0.0975 (0.0966)
+2022-11-18 16:10:09,207:INFO: Dataset: univ                Batch:  4/15	Loss 0.0969 (0.0967)
+2022-11-18 16:10:09,208:INFO: Dataset: univ                Batch:  5/15	Loss 0.0971 (0.0967)
+2022-11-18 16:10:09,212:INFO: Dataset: univ                Batch:  6/15	Loss 0.0968 (0.0968)
+2022-11-18 16:10:09,214:INFO: Dataset: univ                Batch:  7/15	Loss 0.0975 (0.0969)
+2022-11-18 16:10:09,215:INFO: Dataset: univ                Batch:  8/15	Loss 0.0975 (0.0969)
+2022-11-18 16:10:09,216:INFO: Dataset: univ                Batch:  9/15	Loss 0.0977 (0.0970)
+2022-11-18 16:10:09,218:INFO: Dataset: univ                Batch: 10/15	Loss 0.0965 (0.0970)
+2022-11-18 16:10:09,219:INFO: Dataset: univ                Batch: 11/15	Loss 0.0979 (0.0970)
+2022-11-18 16:10:09,221:INFO: Dataset: univ                Batch: 12/15	Loss 0.0974 (0.0971)
+2022-11-18 16:10:09,222:INFO: Dataset: univ                Batch: 13/15	Loss 0.0960 (0.0970)
+2022-11-18 16:10:09,224:INFO: Dataset: univ                Batch: 14/15	Loss 0.0975 (0.0970)
+2022-11-18 16:10:09,226:INFO: Dataset: univ                Batch: 15/15	Loss 0.0956 (0.0970)
+2022-11-18 16:10:09,470:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2593 (0.2593)
+2022-11-18 16:10:09,473:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2603 (0.2598)
+2022-11-18 16:10:09,476:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2614 (0.2604)
+2022-11-18 16:10:09,477:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2602 (0.2603)
+2022-11-18 16:10:09,480:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2607 (0.2604)
+2022-11-18 16:10:09,519:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2603 (0.2604)
+2022-11-18 16:10:09,523:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2576 (0.2599)
+2022-11-18 16:10:09,527:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2578 (0.2597)
+2022-11-18 16:10:09,797:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1269 (0.1269)
+2022-11-18 16:10:09,841:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1291 (0.1281)
+2022-11-18 16:10:09,844:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1276 (0.1279)
+2022-11-18 16:10:09,847:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1252 (0.1273)
+2022-11-18 16:10:09,848:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1289 (0.1276)
+2022-11-18 16:10:09,875:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1261 (0.1273)
+2022-11-18 16:10:09,877:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1259 (0.1272)
+2022-11-18 16:10:09,878:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1276 (0.1272)
+2022-11-18 16:10:09,879:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1274 (0.1272)
+2022-11-18 16:10:09,880:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1320 (0.1277)
+2022-11-18 16:10:09,882:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1278 (0.1277)
+2022-11-18 16:10:09,883:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1264 (0.1276)
+2022-11-18 16:10:09,885:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1267 (0.1276)
+2022-11-18 16:10:09,886:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1289 (0.1277)
+2022-11-18 16:10:09,887:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1249 (0.1275)
+2022-11-18 16:10:09,889:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1284 (0.1275)
+2022-11-18 16:10:09,890:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1241 (0.1273)
+2022-11-18 16:10:09,892:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1256 (0.1273)
+2022-11-18 16:10:09,934:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:10,206:INFO: 		 ADE on eth                       dataset:	 1.8132381439208984
+2022-11-18 16:10:10,206:INFO: Average validation o:	ADE  1.8132	FDE  2.6819
+2022-11-18 16:10:10,207:INFO: - Computing loss (validation)
+2022-11-18 16:10:10,388:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6207 (0.6207)
+2022-11-18 16:10:10,389:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6204 (0.6207)
+2022-11-18 16:10:10,629:INFO: Dataset: univ                Batch: 1/3	Loss 0.0952 (0.0952)
+2022-11-18 16:10:10,635:INFO: Dataset: univ                Batch: 2/3	Loss 0.0969 (0.0960)
+2022-11-18 16:10:10,636:INFO: Dataset: univ                Batch: 3/3	Loss 0.0964 (0.0961)
+2022-11-18 16:10:10,896:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2577 (0.2577)
+2022-11-18 16:10:10,897:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2558 (0.2572)
+2022-11-18 16:10:11,127:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1312 (0.1312)
+2022-11-18 16:10:11,128:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1272 (0.1293)
+2022-11-18 16:10:11,130:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1295 (0.1294)
+2022-11-18 16:10:11,131:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1268 (0.1287)
+2022-11-18 16:10:11,132:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1280 (0.1286)
+2022-11-18 16:10:11,201:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_794.pth.tar
+2022-11-18 16:10:11,201:INFO: 
+===> EPOCH: 795 (P4)
+2022-11-18 16:10:11,202:INFO: - Computing loss (training)
+2022-11-18 16:10:11,386:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6101 (0.6101)
+2022-11-18 16:10:11,388:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6125 (0.6113)
+2022-11-18 16:10:11,389:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6124 (0.6117)
+2022-11-18 16:10:11,391:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6079 (0.6111)
+2022-11-18 16:10:11,640:INFO: Dataset: univ                Batch:  1/15	Loss 0.0963 (0.0963)
+2022-11-18 16:10:11,643:INFO: Dataset: univ                Batch:  2/15	Loss 0.0975 (0.0968)
+2022-11-18 16:10:11,646:INFO: Dataset: univ                Batch:  3/15	Loss 0.0965 (0.0967)
+2022-11-18 16:10:11,667:INFO: Dataset: univ                Batch:  4/15	Loss 0.0975 (0.0969)
+2022-11-18 16:10:11,669:INFO: Dataset: univ                Batch:  5/15	Loss 0.0967 (0.0969)
+2022-11-18 16:10:11,679:INFO: Dataset: univ                Batch:  6/15	Loss 0.0972 (0.0969)
+2022-11-18 16:10:11,681:INFO: Dataset: univ                Batch:  7/15	Loss 0.0974 (0.0970)
+2022-11-18 16:10:11,682:INFO: Dataset: univ                Batch:  8/15	Loss 0.0965 (0.0969)
+2022-11-18 16:10:11,683:INFO: Dataset: univ                Batch:  9/15	Loss 0.0984 (0.0971)
+2022-11-18 16:10:11,685:INFO: Dataset: univ                Batch: 10/15	Loss 0.0980 (0.0971)
+2022-11-18 16:10:11,686:INFO: Dataset: univ                Batch: 11/15	Loss 0.0963 (0.0971)
+2022-11-18 16:10:11,688:INFO: Dataset: univ                Batch: 12/15	Loss 0.0979 (0.0971)
+2022-11-18 16:10:11,689:INFO: Dataset: univ                Batch: 13/15	Loss 0.0962 (0.0971)
+2022-11-18 16:10:11,691:INFO: Dataset: univ                Batch: 14/15	Loss 0.0952 (0.0969)
+2022-11-18 16:10:11,692:INFO: Dataset: univ                Batch: 15/15	Loss 0.0976 (0.0969)
+2022-11-18 16:10:11,933:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2603 (0.2603)
+2022-11-18 16:10:11,938:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2598 (0.2600)
+2022-11-18 16:10:11,940:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2600 (0.2600)
+2022-11-18 16:10:11,943:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2607 (0.2602)
+2022-11-18 16:10:11,947:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2594 (0.2600)
+2022-11-18 16:10:12,006:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2591 (0.2599)
+2022-11-18 16:10:12,010:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2589 (0.2597)
+2022-11-18 16:10:12,014:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2588 (0.2596)
+2022-11-18 16:10:12,290:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1275 (0.1275)
+2022-11-18 16:10:12,293:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1281 (0.1278)
+2022-11-18 16:10:12,300:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1250 (0.1269)
+2022-11-18 16:10:12,302:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1287 (0.1273)
+2022-11-18 16:10:12,374:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1285 (0.1275)
+2022-11-18 16:10:12,381:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1286 (0.1277)
+2022-11-18 16:10:12,382:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1262 (0.1275)
+2022-11-18 16:10:12,384:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1268 (0.1274)
+2022-11-18 16:10:12,385:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1284 (0.1275)
+2022-11-18 16:10:12,386:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1253 (0.1273)
+2022-11-18 16:10:12,388:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1279 (0.1274)
+2022-11-18 16:10:12,389:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1279 (0.1274)
+2022-11-18 16:10:12,390:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1250 (0.1272)
+2022-11-18 16:10:12,392:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1296 (0.1274)
+2022-11-18 16:10:12,393:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1279 (0.1274)
+2022-11-18 16:10:12,394:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1256 (0.1273)
+2022-11-18 16:10:12,396:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1250 (0.1272)
+2022-11-18 16:10:12,397:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1255 (0.1271)
+2022-11-18 16:10:12,458:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:12,743:INFO: 		 ADE on eth                       dataset:	 1.793859601020813
+2022-11-18 16:10:12,743:INFO: Average validation o:	ADE  1.7939	FDE  2.7011
+2022-11-18 16:10:12,744:INFO: - Computing loss (validation)
+2022-11-18 16:10:12,930:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6204 (0.6204)
+2022-11-18 16:10:12,933:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6187 (0.6202)
+2022-11-18 16:10:13,188:INFO: Dataset: univ                Batch: 1/3	Loss 0.0959 (0.0959)
+2022-11-18 16:10:13,189:INFO: Dataset: univ                Batch: 2/3	Loss 0.0963 (0.0961)
+2022-11-18 16:10:13,191:INFO: Dataset: univ                Batch: 3/3	Loss 0.0957 (0.0959)
+2022-11-18 16:10:13,427:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2568 (0.2568)
+2022-11-18 16:10:13,429:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2580 (0.2571)
+2022-11-18 16:10:13,692:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1302 (0.1302)
+2022-11-18 16:10:13,695:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1286 (0.1294)
+2022-11-18 16:10:13,696:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1269 (0.1286)
+2022-11-18 16:10:13,698:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1279 (0.1284)
+2022-11-18 16:10:13,698:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1286 (0.1285)
+2022-11-18 16:10:13,763:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_795.pth.tar
+2022-11-18 16:10:13,764:INFO: 
+===> EPOCH: 796 (P4)
+2022-11-18 16:10:13,765:INFO: - Computing loss (training)
+2022-11-18 16:10:13,950:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6127 (0.6127)
+2022-11-18 16:10:13,953:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6116 (0.6122)
+2022-11-18 16:10:13,956:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6078 (0.6108)
+2022-11-18 16:10:13,966:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6081 (0.6104)
+2022-11-18 16:10:14,215:INFO: Dataset: univ                Batch:  1/15	Loss 0.0965 (0.0965)
+2022-11-18 16:10:14,217:INFO: Dataset: univ                Batch:  2/15	Loss 0.0968 (0.0966)
+2022-11-18 16:10:14,281:INFO: Dataset: univ                Batch:  3/15	Loss 0.0971 (0.0968)
+2022-11-18 16:10:14,283:INFO: Dataset: univ                Batch:  4/15	Loss 0.0964 (0.0967)
+2022-11-18 16:10:14,285:INFO: Dataset: univ                Batch:  5/15	Loss 0.0978 (0.0969)
+2022-11-18 16:10:14,289:INFO: Dataset: univ                Batch:  6/15	Loss 0.0971 (0.0969)
+2022-11-18 16:10:14,291:INFO: Dataset: univ                Batch:  7/15	Loss 0.0973 (0.0970)
+2022-11-18 16:10:14,293:INFO: Dataset: univ                Batch:  8/15	Loss 0.0971 (0.0970)
+2022-11-18 16:10:14,294:INFO: Dataset: univ                Batch:  9/15	Loss 0.0970 (0.0970)
+2022-11-18 16:10:14,296:INFO: Dataset: univ                Batch: 10/15	Loss 0.0979 (0.0971)
+2022-11-18 16:10:14,298:INFO: Dataset: univ                Batch: 11/15	Loss 0.0964 (0.0970)
+2022-11-18 16:10:14,300:INFO: Dataset: univ                Batch: 12/15	Loss 0.0962 (0.0969)
+2022-11-18 16:10:14,302:INFO: Dataset: univ                Batch: 13/15	Loss 0.0964 (0.0969)
+2022-11-18 16:10:14,304:INFO: Dataset: univ                Batch: 14/15	Loss 0.0962 (0.0968)
+2022-11-18 16:10:14,305:INFO: Dataset: univ                Batch: 15/15	Loss 0.0954 (0.0968)
+2022-11-18 16:10:14,585:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2605 (0.2605)
+2022-11-18 16:10:14,607:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2608 (0.2607)
+2022-11-18 16:10:14,611:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2604 (0.2606)
+2022-11-18 16:10:14,613:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2603 (0.2605)
+2022-11-18 16:10:14,619:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2597 (0.2603)
+2022-11-18 16:10:14,652:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2582 (0.2599)
+2022-11-18 16:10:14,653:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2575 (0.2596)
+2022-11-18 16:10:14,655:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2591 (0.2595)
+2022-11-18 16:10:14,963:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1258 (0.1258)
+2022-11-18 16:10:14,966:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1265 (0.1261)
+2022-11-18 16:10:14,970:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1246 (0.1256)
+2022-11-18 16:10:14,974:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1282 (0.1263)
+2022-11-18 16:10:15,004:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1273 (0.1265)
+2022-11-18 16:10:15,013:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1284 (0.1269)
+2022-11-18 16:10:15,015:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1282 (0.1271)
+2022-11-18 16:10:15,016:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1263 (0.1270)
+2022-11-18 16:10:15,018:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1280 (0.1271)
+2022-11-18 16:10:15,019:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1288 (0.1273)
+2022-11-18 16:10:15,021:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1266 (0.1272)
+2022-11-18 16:10:15,024:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1272 (0.1272)
+2022-11-18 16:10:15,025:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1281 (0.1273)
+2022-11-18 16:10:15,027:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1283 (0.1274)
+2022-11-18 16:10:15,028:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1273 (0.1274)
+2022-11-18 16:10:15,030:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1259 (0.1273)
+2022-11-18 16:10:15,032:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1256 (0.1272)
+2022-11-18 16:10:15,034:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1229 (0.1270)
+2022-11-18 16:10:15,086:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:15,428:INFO: 		 ADE on eth                       dataset:	 1.7991842031478882
+2022-11-18 16:10:15,429:INFO: Average validation o:	ADE  1.7992	FDE  2.7059
+2022-11-18 16:10:15,430:INFO: - Computing loss (validation)
+2022-11-18 16:10:15,658:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6199 (0.6199)
+2022-11-18 16:10:15,660:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6206 (0.6199)
+2022-11-18 16:10:15,941:INFO: Dataset: univ                Batch: 1/3	Loss 0.0948 (0.0948)
+2022-11-18 16:10:15,943:INFO: Dataset: univ                Batch: 2/3	Loss 0.0965 (0.0957)
+2022-11-18 16:10:15,945:INFO: Dataset: univ                Batch: 3/3	Loss 0.0962 (0.0959)
+2022-11-18 16:10:16,182:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2566 (0.2566)
+2022-11-18 16:10:16,186:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2582 (0.2570)
+2022-11-18 16:10:16,429:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1275 (0.1275)
+2022-11-18 16:10:16,433:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1295 (0.1285)
+2022-11-18 16:10:16,434:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1277 (0.1282)
+2022-11-18 16:10:16,440:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1299 (0.1286)
+2022-11-18 16:10:16,444:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1271 (0.1283)
+2022-11-18 16:10:16,507:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_796.pth.tar
+2022-11-18 16:10:16,507:INFO: 
+===> EPOCH: 797 (P4)
+2022-11-18 16:10:16,508:INFO: - Computing loss (training)
+2022-11-18 16:10:16,736:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6070 (0.6070)
+2022-11-18 16:10:16,742:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6124 (0.6097)
+2022-11-18 16:10:16,743:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6088 (0.6094)
+2022-11-18 16:10:16,745:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6109 (0.6097)
+2022-11-18 16:10:17,047:INFO: Dataset: univ                Batch:  1/15	Loss 0.0969 (0.0969)
+2022-11-18 16:10:17,049:INFO: Dataset: univ                Batch:  2/15	Loss 0.0967 (0.0968)
+2022-11-18 16:10:17,051:INFO: Dataset: univ                Batch:  3/15	Loss 0.0968 (0.0968)
+2022-11-18 16:10:17,054:INFO: Dataset: univ                Batch:  4/15	Loss 0.0975 (0.0970)
+2022-11-18 16:10:17,071:INFO: Dataset: univ                Batch:  5/15	Loss 0.0966 (0.0969)
+2022-11-18 16:10:17,089:INFO: Dataset: univ                Batch:  6/15	Loss 0.0977 (0.0970)
+2022-11-18 16:10:17,091:INFO: Dataset: univ                Batch:  7/15	Loss 0.0966 (0.0970)
+2022-11-18 16:10:17,092:INFO: Dataset: univ                Batch:  8/15	Loss 0.0979 (0.0971)
+2022-11-18 16:10:17,094:INFO: Dataset: univ                Batch:  9/15	Loss 0.0973 (0.0971)
+2022-11-18 16:10:17,095:INFO: Dataset: univ                Batch: 10/15	Loss 0.0975 (0.0971)
+2022-11-18 16:10:17,097:INFO: Dataset: univ                Batch: 11/15	Loss 0.0963 (0.0971)
+2022-11-18 16:10:17,099:INFO: Dataset: univ                Batch: 12/15	Loss 0.0957 (0.0969)
+2022-11-18 16:10:17,101:INFO: Dataset: univ                Batch: 13/15	Loss 0.0965 (0.0969)
+2022-11-18 16:10:17,102:INFO: Dataset: univ                Batch: 14/15	Loss 0.0958 (0.0968)
+2022-11-18 16:10:17,104:INFO: Dataset: univ                Batch: 15/15	Loss 0.0937 (0.0968)
+2022-11-18 16:10:17,391:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2596 (0.2596)
+2022-11-18 16:10:17,394:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2602 (0.2599)
+2022-11-18 16:10:17,395:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2608 (0.2602)
+2022-11-18 16:10:17,397:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2595 (0.2600)
+2022-11-18 16:10:17,401:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2597 (0.2600)
+2022-11-18 16:10:17,433:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2589 (0.2598)
+2022-11-18 16:10:17,435:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2582 (0.2596)
+2022-11-18 16:10:17,437:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2585 (0.2595)
+2022-11-18 16:10:17,707:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1255 (0.1255)
+2022-11-18 16:10:17,713:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1272 (0.1264)
+2022-11-18 16:10:17,714:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1259 (0.1263)
+2022-11-18 16:10:17,716:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1296 (0.1271)
+2022-11-18 16:10:17,735:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1270 (0.1271)
+2022-11-18 16:10:17,781:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1285 (0.1273)
+2022-11-18 16:10:17,782:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1267 (0.1272)
+2022-11-18 16:10:17,784:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1291 (0.1274)
+2022-11-18 16:10:17,786:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1272 (0.1274)
+2022-11-18 16:10:17,787:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1266 (0.1273)
+2022-11-18 16:10:17,788:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1262 (0.1272)
+2022-11-18 16:10:17,791:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1271 (0.1272)
+2022-11-18 16:10:17,792:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1272 (0.1272)
+2022-11-18 16:10:17,793:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1276 (0.1272)
+2022-11-18 16:10:17,794:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1250 (0.1271)
+2022-11-18 16:10:17,796:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1260 (0.1270)
+2022-11-18 16:10:17,797:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1244 (0.1268)
+2022-11-18 16:10:17,799:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1256 (0.1268)
+2022-11-18 16:10:17,852:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:18,179:INFO: 		 ADE on eth                       dataset:	 1.8111757040023804
+2022-11-18 16:10:18,179:INFO: Average validation o:	ADE  1.8112	FDE  2.7537
+2022-11-18 16:10:18,180:INFO: - Computing loss (validation)
+2022-11-18 16:10:18,399:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6187 (0.6187)
+2022-11-18 16:10:18,400:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6324 (0.6196)
+2022-11-18 16:10:18,658:INFO: Dataset: univ                Batch: 1/3	Loss 0.0950 (0.0950)
+2022-11-18 16:10:18,659:INFO: Dataset: univ                Batch: 2/3	Loss 0.0971 (0.0959)
+2022-11-18 16:10:18,660:INFO: Dataset: univ                Batch: 3/3	Loss 0.0954 (0.0958)
+2022-11-18 16:10:18,906:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2572 (0.2572)
+2022-11-18 16:10:18,907:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2561 (0.2569)
+2022-11-18 16:10:19,168:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1283 (0.1283)
+2022-11-18 16:10:19,171:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1259 (0.1270)
+2022-11-18 16:10:19,173:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1297 (0.1280)
+2022-11-18 16:10:19,176:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1284 (0.1281)
+2022-11-18 16:10:19,177:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1284 (0.1281)
+2022-11-18 16:10:19,241:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_797.pth.tar
+2022-11-18 16:10:19,241:INFO: 
+===> EPOCH: 798 (P4)
+2022-11-18 16:10:19,242:INFO: - Computing loss (training)
+2022-11-18 16:10:19,442:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6043 (0.6043)
+2022-11-18 16:10:19,449:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6092 (0.6067)
+2022-11-18 16:10:19,450:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6113 (0.6082)
+2022-11-18 16:10:19,463:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6134 (0.6090)
+2022-11-18 16:10:19,738:INFO: Dataset: univ                Batch:  1/15	Loss 0.0965 (0.0965)
+2022-11-18 16:10:19,740:INFO: Dataset: univ                Batch:  2/15	Loss 0.0963 (0.0964)
+2022-11-18 16:10:19,744:INFO: Dataset: univ                Batch:  3/15	Loss 0.0962 (0.0963)
+2022-11-18 16:10:19,745:INFO: Dataset: univ                Batch:  4/15	Loss 0.0967 (0.0964)
+2022-11-18 16:10:19,747:INFO: Dataset: univ                Batch:  5/15	Loss 0.0970 (0.0965)
+2022-11-18 16:10:19,752:INFO: Dataset: univ                Batch:  6/15	Loss 0.0966 (0.0965)
+2022-11-18 16:10:19,754:INFO: Dataset: univ                Batch:  7/15	Loss 0.0975 (0.0967)
+2022-11-18 16:10:19,755:INFO: Dataset: univ                Batch:  8/15	Loss 0.0966 (0.0967)
+2022-11-18 16:10:19,757:INFO: Dataset: univ                Batch:  9/15	Loss 0.0982 (0.0968)
+2022-11-18 16:10:19,758:INFO: Dataset: univ                Batch: 10/15	Loss 0.0971 (0.0968)
+2022-11-18 16:10:19,760:INFO: Dataset: univ                Batch: 11/15	Loss 0.0971 (0.0969)
+2022-11-18 16:10:19,762:INFO: Dataset: univ                Batch: 12/15	Loss 0.0963 (0.0968)
+2022-11-18 16:10:19,763:INFO: Dataset: univ                Batch: 13/15	Loss 0.0964 (0.0968)
+2022-11-18 16:10:19,765:INFO: Dataset: univ                Batch: 14/15	Loss 0.0961 (0.0967)
+2022-11-18 16:10:19,766:INFO: Dataset: univ                Batch: 15/15	Loss 0.0964 (0.0967)
+2022-11-18 16:10:20,032:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2605 (0.2605)
+2022-11-18 16:10:20,034:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2602 (0.2603)
+2022-11-18 16:10:20,037:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2609 (0.2605)
+2022-11-18 16:10:20,039:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2590 (0.2601)
+2022-11-18 16:10:20,045:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2590 (0.2599)
+2022-11-18 16:10:20,092:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2583 (0.2597)
+2022-11-18 16:10:20,093:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2579 (0.2594)
+2022-11-18 16:10:20,095:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2596 (0.2594)
+2022-11-18 16:10:20,389:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1245 (0.1245)
+2022-11-18 16:10:20,392:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1287 (0.1266)
+2022-11-18 16:10:20,394:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1249 (0.1260)
+2022-11-18 16:10:20,404:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1260 (0.1260)
+2022-11-18 16:10:20,406:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1281 (0.1264)
+2022-11-18 16:10:20,428:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1274 (0.1266)
+2022-11-18 16:10:20,429:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1268 (0.1266)
+2022-11-18 16:10:20,431:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1293 (0.1269)
+2022-11-18 16:10:20,432:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1282 (0.1271)
+2022-11-18 16:10:20,433:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1276 (0.1271)
+2022-11-18 16:10:20,435:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1267 (0.1271)
+2022-11-18 16:10:20,437:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1250 (0.1269)
+2022-11-18 16:10:20,438:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1277 (0.1270)
+2022-11-18 16:10:20,439:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1261 (0.1269)
+2022-11-18 16:10:20,441:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1266 (0.1269)
+2022-11-18 16:10:20,442:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1242 (0.1268)
+2022-11-18 16:10:20,443:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1254 (0.1267)
+2022-11-18 16:10:20,446:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1268 (0.1267)
+2022-11-18 16:10:20,498:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:20,788:INFO: 		 ADE on eth                       dataset:	 1.8491328954696655
+2022-11-18 16:10:20,788:INFO: Average validation o:	ADE  1.8491	FDE  2.7992
+2022-11-18 16:10:20,790:INFO: - Computing loss (validation)
+2022-11-18 16:10:21,009:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6188 (0.6188)
+2022-11-18 16:10:21,010:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6223 (0.6191)
+2022-11-18 16:10:21,249:INFO: Dataset: univ                Batch: 1/3	Loss 0.0963 (0.0963)
+2022-11-18 16:10:21,252:INFO: Dataset: univ                Batch: 2/3	Loss 0.0955 (0.0959)
+2022-11-18 16:10:21,259:INFO: Dataset: univ                Batch: 3/3	Loss 0.0950 (0.0956)
+2022-11-18 16:10:21,492:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2570 (0.2570)
+2022-11-18 16:10:21,493:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2563 (0.2568)
+2022-11-18 16:10:21,820:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1272 (0.1272)
+2022-11-18 16:10:21,824:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1279 (0.1276)
+2022-11-18 16:10:21,825:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1297 (0.1283)
+2022-11-18 16:10:21,828:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1278 (0.1281)
+2022-11-18 16:10:21,830:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1276 (0.1280)
+2022-11-18 16:10:21,902:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_798.pth.tar
+2022-11-18 16:10:21,902:INFO: 
+===> EPOCH: 799 (P4)
+2022-11-18 16:10:21,903:INFO: - Computing loss (training)
+2022-11-18 16:10:22,100:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6057 (0.6057)
+2022-11-18 16:10:22,108:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6119 (0.6088)
+2022-11-18 16:10:22,109:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6083 (0.6086)
+2022-11-18 16:10:22,111:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6073 (0.6084)
+2022-11-18 16:10:22,383:INFO: Dataset: univ                Batch:  1/15	Loss 0.0966 (0.0966)
+2022-11-18 16:10:22,386:INFO: Dataset: univ                Batch:  2/15	Loss 0.0962 (0.0964)
+2022-11-18 16:10:22,389:INFO: Dataset: univ                Batch:  3/15	Loss 0.0971 (0.0967)
+2022-11-18 16:10:22,422:INFO: Dataset: univ                Batch:  4/15	Loss 0.0964 (0.0966)
+2022-11-18 16:10:22,424:INFO: Dataset: univ                Batch:  5/15	Loss 0.0983 (0.0969)
+2022-11-18 16:10:22,429:INFO: Dataset: univ                Batch:  6/15	Loss 0.0978 (0.0970)
+2022-11-18 16:10:22,431:INFO: Dataset: univ                Batch:  7/15	Loss 0.0974 (0.0971)
+2022-11-18 16:10:22,433:INFO: Dataset: univ                Batch:  8/15	Loss 0.0972 (0.0971)
+2022-11-18 16:10:22,435:INFO: Dataset: univ                Batch:  9/15	Loss 0.0962 (0.0970)
+2022-11-18 16:10:22,436:INFO: Dataset: univ                Batch: 10/15	Loss 0.0960 (0.0969)
+2022-11-18 16:10:22,439:INFO: Dataset: univ                Batch: 11/15	Loss 0.0968 (0.0969)
+2022-11-18 16:10:22,440:INFO: Dataset: univ                Batch: 12/15	Loss 0.0960 (0.0968)
+2022-11-18 16:10:22,442:INFO: Dataset: univ                Batch: 13/15	Loss 0.0963 (0.0968)
+2022-11-18 16:10:22,443:INFO: Dataset: univ                Batch: 14/15	Loss 0.0953 (0.0967)
+2022-11-18 16:10:22,445:INFO: Dataset: univ                Batch: 15/15	Loss 0.0948 (0.0966)
+2022-11-18 16:10:22,739:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2600 (0.2600)
+2022-11-18 16:10:22,742:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2589 (0.2595)
+2022-11-18 16:10:22,745:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2597 (0.2595)
+2022-11-18 16:10:22,749:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2612 (0.2599)
+2022-11-18 16:10:22,752:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2596 (0.2599)
+2022-11-18 16:10:22,781:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2584 (0.2596)
+2022-11-18 16:10:22,784:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2592 (0.2596)
+2022-11-18 16:10:22,786:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2573 (0.2593)
+2022-11-18 16:10:23,052:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1249 (0.1249)
+2022-11-18 16:10:23,055:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1273 (0.1261)
+2022-11-18 16:10:23,057:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1297 (0.1273)
+2022-11-18 16:10:23,059:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1264 (0.1271)
+2022-11-18 16:10:23,100:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1264 (0.1269)
+2022-11-18 16:10:23,120:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1285 (0.1272)
+2022-11-18 16:10:23,121:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1247 (0.1269)
+2022-11-18 16:10:23,123:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1254 (0.1267)
+2022-11-18 16:10:23,124:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1267 (0.1267)
+2022-11-18 16:10:23,126:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1252 (0.1265)
+2022-11-18 16:10:23,127:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1274 (0.1266)
+2022-11-18 16:10:23,129:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1295 (0.1269)
+2022-11-18 16:10:23,130:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1274 (0.1269)
+2022-11-18 16:10:23,132:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1253 (0.1268)
+2022-11-18 16:10:23,133:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1261 (0.1267)
+2022-11-18 16:10:23,135:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1262 (0.1267)
+2022-11-18 16:10:23,136:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1254 (0.1266)
+2022-11-18 16:10:23,138:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1252 (0.1266)
+2022-11-18 16:10:23,184:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:23,472:INFO: 		 ADE on eth                       dataset:	 1.814862608909607
+2022-11-18 16:10:23,473:INFO: Average validation o:	ADE  1.8149	FDE  2.7953
+2022-11-18 16:10:23,474:INFO: - Computing loss (validation)
+2022-11-18 16:10:23,652:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6190 (0.6190)
+2022-11-18 16:10:23,653:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6171 (0.6188)
+2022-11-18 16:10:23,948:INFO: Dataset: univ                Batch: 1/3	Loss 0.0968 (0.0968)
+2022-11-18 16:10:23,950:INFO: Dataset: univ                Batch: 2/3	Loss 0.0955 (0.0962)
+2022-11-18 16:10:23,951:INFO: Dataset: univ                Batch: 3/3	Loss 0.0945 (0.0956)
+2022-11-18 16:10:24,200:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2565 (0.2565)
+2022-11-18 16:10:24,203:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2572 (0.2567)
+2022-11-18 16:10:24,440:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1295 (0.1295)
+2022-11-18 16:10:24,445:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1290 (0.1293)
+2022-11-18 16:10:24,446:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1257 (0.1281)
+2022-11-18 16:10:24,447:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1286 (0.1282)
+2022-11-18 16:10:24,448:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1266 (0.1279)
+2022-11-18 16:10:24,506:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_799.pth.tar
+2022-11-18 16:10:24,506:INFO: 
+===> EPOCH: 800 (P4)
+2022-11-18 16:10:24,507:INFO: - Computing loss (training)
+2022-11-18 16:10:24,713:INFO: Dataset: hotel               Batch: 1/4	Loss 0.6043 (0.6043)
+2022-11-18 16:10:24,715:INFO: Dataset: hotel               Batch: 2/4	Loss 0.6094 (0.6068)
+2022-11-18 16:10:24,717:INFO: Dataset: hotel               Batch: 3/4	Loss 0.6080 (0.6072)
+2022-11-18 16:10:24,719:INFO: Dataset: hotel               Batch: 4/4	Loss 0.6102 (0.6077)
+2022-11-18 16:10:24,967:INFO: Dataset: univ                Batch:  1/15	Loss 0.0963 (0.0963)
+2022-11-18 16:10:25,015:INFO: Dataset: univ                Batch:  2/15	Loss 0.0972 (0.0967)
+2022-11-18 16:10:25,018:INFO: Dataset: univ                Batch:  3/15	Loss 0.0970 (0.0968)
+2022-11-18 16:10:25,020:INFO: Dataset: univ                Batch:  4/15	Loss 0.0966 (0.0968)
+2022-11-18 16:10:25,023:INFO: Dataset: univ                Batch:  5/15	Loss 0.0970 (0.0968)
+2022-11-18 16:10:25,031:INFO: Dataset: univ                Batch:  6/15	Loss 0.0963 (0.0967)
+2022-11-18 16:10:25,033:INFO: Dataset: univ                Batch:  7/15	Loss 0.0967 (0.0967)
+2022-11-18 16:10:25,035:INFO: Dataset: univ                Batch:  8/15	Loss 0.0965 (0.0967)
+2022-11-18 16:10:25,038:INFO: Dataset: univ                Batch:  9/15	Loss 0.0971 (0.0967)
+2022-11-18 16:10:25,041:INFO: Dataset: univ                Batch: 10/15	Loss 0.0970 (0.0968)
+2022-11-18 16:10:25,044:INFO: Dataset: univ                Batch: 11/15	Loss 0.0965 (0.0967)
+2022-11-18 16:10:25,048:INFO: Dataset: univ                Batch: 12/15	Loss 0.0967 (0.0967)
+2022-11-18 16:10:25,050:INFO: Dataset: univ                Batch: 13/15	Loss 0.0961 (0.0967)
+2022-11-18 16:10:25,052:INFO: Dataset: univ                Batch: 14/15	Loss 0.0959 (0.0966)
+2022-11-18 16:10:25,054:INFO: Dataset: univ                Batch: 15/15	Loss 0.0946 (0.0966)
+2022-11-18 16:10:25,315:INFO: Dataset: zara1               Batch: 1/8	Loss 0.2600 (0.2600)
+2022-11-18 16:10:25,321:INFO: Dataset: zara1               Batch: 2/8	Loss 0.2595 (0.2597)
+2022-11-18 16:10:25,325:INFO: Dataset: zara1               Batch: 3/8	Loss 0.2606 (0.2600)
+2022-11-18 16:10:25,327:INFO: Dataset: zara1               Batch: 4/8	Loss 0.2583 (0.2596)
+2022-11-18 16:10:25,329:INFO: Dataset: zara1               Batch: 5/8	Loss 0.2587 (0.2594)
+2022-11-18 16:10:25,361:INFO: Dataset: zara1               Batch: 6/8	Loss 0.2598 (0.2594)
+2022-11-18 16:10:25,362:INFO: Dataset: zara1               Batch: 7/8	Loss 0.2590 (0.2594)
+2022-11-18 16:10:25,363:INFO: Dataset: zara1               Batch: 8/8	Loss 0.2578 (0.2592)
+2022-11-18 16:10:25,629:INFO: Dataset: zara2               Batch:  1/18	Loss 0.1250 (0.1250)
+2022-11-18 16:10:25,632:INFO: Dataset: zara2               Batch:  2/18	Loss 0.1268 (0.1259)
+2022-11-18 16:10:25,635:INFO: Dataset: zara2               Batch:  3/18	Loss 0.1256 (0.1258)
+2022-11-18 16:10:25,674:INFO: Dataset: zara2               Batch:  4/18	Loss 0.1265 (0.1260)
+2022-11-18 16:10:25,676:INFO: Dataset: zara2               Batch:  5/18	Loss 0.1261 (0.1260)
+2022-11-18 16:10:25,680:INFO: Dataset: zara2               Batch:  6/18	Loss 0.1275 (0.1262)
+2022-11-18 16:10:25,682:INFO: Dataset: zara2               Batch:  7/18	Loss 0.1277 (0.1265)
+2022-11-18 16:10:25,683:INFO: Dataset: zara2               Batch:  8/18	Loss 0.1266 (0.1265)
+2022-11-18 16:10:25,685:INFO: Dataset: zara2               Batch:  9/18	Loss 0.1273 (0.1266)
+2022-11-18 16:10:25,686:INFO: Dataset: zara2               Batch: 10/18	Loss 0.1265 (0.1266)
+2022-11-18 16:10:25,690:INFO: Dataset: zara2               Batch: 11/18	Loss 0.1253 (0.1265)
+2022-11-18 16:10:25,691:INFO: Dataset: zara2               Batch: 12/18	Loss 0.1275 (0.1266)
+2022-11-18 16:10:25,693:INFO: Dataset: zara2               Batch: 13/18	Loss 0.1265 (0.1266)
+2022-11-18 16:10:25,694:INFO: Dataset: zara2               Batch: 14/18	Loss 0.1258 (0.1265)
+2022-11-18 16:10:25,695:INFO: Dataset: zara2               Batch: 15/18	Loss 0.1268 (0.1265)
+2022-11-18 16:10:25,697:INFO: Dataset: zara2               Batch: 16/18	Loss 0.1266 (0.1265)
+2022-11-18 16:10:25,699:INFO: Dataset: zara2               Batch: 17/18	Loss 0.1229 (0.1263)
+2022-11-18 16:10:25,701:INFO: Dataset: zara2               Batch: 18/18	Loss 0.1275 (0.1264)
+2022-11-18 16:10:25,763:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:26,063:INFO: 		 ADE on eth                       dataset:	 1.782148003578186
+2022-11-18 16:10:26,063:INFO: Average validation o:	ADE  1.7821	FDE  2.6824
+2022-11-18 16:10:26,064:INFO: - Computing loss (validation)
+2022-11-18 16:10:26,257:INFO: Dataset: hotel               Batch: 1/2	Loss 0.6182 (0.6182)
+2022-11-18 16:10:26,258:INFO: Dataset: hotel               Batch: 2/2	Loss 0.6215 (0.6184)
+2022-11-18 16:10:26,507:INFO: Dataset: univ                Batch: 1/3	Loss 0.0940 (0.0940)
+2022-11-18 16:10:26,509:INFO: Dataset: univ                Batch: 2/3	Loss 0.0954 (0.0946)
+2022-11-18 16:10:26,510:INFO: Dataset: univ                Batch: 3/3	Loss 0.0979 (0.0955)
+2022-11-18 16:10:26,735:INFO: Dataset: zara1               Batch: 1/2	Loss 0.2569 (0.2569)
+2022-11-18 16:10:26,735:INFO: Dataset: zara1               Batch: 2/2	Loss 0.2556 (0.2566)
+2022-11-18 16:10:26,966:INFO: Dataset: zara2               Batch: 1/5	Loss 0.1285 (0.1285)
+2022-11-18 16:10:26,967:INFO: Dataset: zara2               Batch: 2/5	Loss 0.1285 (0.1285)
+2022-11-18 16:10:26,968:INFO: Dataset: zara2               Batch: 3/5	Loss 0.1258 (0.1276)
+2022-11-18 16:10:26,970:INFO: Dataset: zara2               Batch: 4/5	Loss 0.1293 (0.1281)
+2022-11-18 16:10:26,971:INFO: Dataset: zara2               Batch: 5/5	Loss 0.1263 (0.1277)
+2022-11-18 16:10:27,034:INFO:  --> Model Saved in ./models/E1//P4/CRMF_epoch_800.pth.tar
+2022-11-18 16:10:27,034:INFO: 
+===> EPOCH: 801 (P5)
+2022-11-18 16:10:27,035:INFO: - Computing loss (training)
+2022-11-18 16:10:27,334:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8505 (1.8505)
+2022-11-18 16:10:27,385:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:27,661:INFO: 		 ADE on eth                       dataset:	 1.7993115186691284
+2022-11-18 16:10:27,661:INFO: Average validation o:	ADE  1.7993	FDE  2.7516
+2022-11-18 16:10:27,675:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_801.pth.tar
+2022-11-18 16:10:27,675:INFO: 
+===> EPOCH: 802 (P5)
+2022-11-18 16:10:27,676:INFO: - Computing loss (training)
+2022-11-18 16:10:27,909:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6009 (1.6009)
+2022-11-18 16:10:27,953:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:28,243:INFO: 		 ADE on eth                       dataset:	 1.7830536365509033
+2022-11-18 16:10:28,243:INFO: Average validation o:	ADE  1.7831	FDE  2.6727
+2022-11-18 16:10:28,259:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_802.pth.tar
+2022-11-18 16:10:28,259:INFO: 
+===> EPOCH: 803 (P5)
+2022-11-18 16:10:28,259:INFO: - Computing loss (training)
+2022-11-18 16:10:28,508:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8074 (1.8074)
+2022-11-18 16:10:28,558:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:28,881:INFO: 		 ADE on eth                       dataset:	 1.8071390390396118
+2022-11-18 16:10:28,881:INFO: Average validation o:	ADE  1.8071	FDE  2.6641
+2022-11-18 16:10:28,882:INFO: 
+===> EPOCH: 804 (P5)
+2022-11-18 16:10:28,883:INFO: - Computing loss (training)
+2022-11-18 16:10:29,164:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8501 (1.8501)
+2022-11-18 16:10:29,213:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:29,486:INFO: 		 ADE on eth                       dataset:	 1.804836630821228
+2022-11-18 16:10:29,487:INFO: Average validation o:	ADE  1.8048	FDE  2.7713
+2022-11-18 16:10:29,487:INFO: 
+===> EPOCH: 805 (P5)
+2022-11-18 16:10:29,488:INFO: - Computing loss (training)
+2022-11-18 16:10:29,764:INFO: Dataset: eth                 Batch: 1/1	Loss 1.9079 (1.9079)
+2022-11-18 16:10:29,807:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:30,088:INFO: 		 ADE on eth                       dataset:	 1.8291993141174316
+2022-11-18 16:10:30,088:INFO: Average validation o:	ADE  1.8292	FDE  2.7491
+2022-11-18 16:10:30,088:INFO: 
+===> EPOCH: 806 (P5)
+2022-11-18 16:10:30,089:INFO: - Computing loss (training)
+2022-11-18 16:10:30,341:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7724 (1.7724)
+2022-11-18 16:10:30,388:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:30,676:INFO: 		 ADE on eth                       dataset:	 1.8277307748794556
+2022-11-18 16:10:30,676:INFO: Average validation o:	ADE  1.8277	FDE  2.7222
+2022-11-18 16:10:30,677:INFO: 
+===> EPOCH: 807 (P5)
+2022-11-18 16:10:30,678:INFO: - Computing loss (training)
+2022-11-18 16:10:30,938:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8449 (1.8449)
+2022-11-18 16:10:30,983:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:31,261:INFO: 		 ADE on eth                       dataset:	 1.8197417259216309
+2022-11-18 16:10:31,261:INFO: Average validation o:	ADE  1.8197	FDE  2.7552
+2022-11-18 16:10:31,262:INFO: 
+===> EPOCH: 808 (P5)
+2022-11-18 16:10:31,263:INFO: - Computing loss (training)
+2022-11-18 16:10:31,532:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7234 (1.7234)
+2022-11-18 16:10:31,581:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:31,878:INFO: 		 ADE on eth                       dataset:	 1.7770476341247559
+2022-11-18 16:10:31,878:INFO: Average validation o:	ADE  1.7770	FDE  2.6155
+2022-11-18 16:10:31,893:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_808.pth.tar
+2022-11-18 16:10:31,893:INFO: 
+===> EPOCH: 809 (P5)
+2022-11-18 16:10:31,894:INFO: - Computing loss (training)
+2022-11-18 16:10:32,175:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7700 (1.7700)
+2022-11-18 16:10:32,223:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:32,515:INFO: 		 ADE on eth                       dataset:	 1.7933231592178345
+2022-11-18 16:10:32,515:INFO: Average validation o:	ADE  1.7933	FDE  2.6935
+2022-11-18 16:10:32,516:INFO: 
+===> EPOCH: 810 (P5)
+2022-11-18 16:10:32,517:INFO: - Computing loss (training)
+2022-11-18 16:10:32,751:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6264 (1.6264)
+2022-11-18 16:10:32,793:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:33,059:INFO: 		 ADE on eth                       dataset:	 1.8671255111694336
+2022-11-18 16:10:33,059:INFO: Average validation o:	ADE  1.8671	FDE  2.8128
+2022-11-18 16:10:33,060:INFO: 
+===> EPOCH: 811 (P5)
+2022-11-18 16:10:33,061:INFO: - Computing loss (training)
+2022-11-18 16:10:33,311:INFO: Dataset: eth                 Batch: 1/1	Loss 2.1207 (2.1207)
+2022-11-18 16:10:33,356:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:33,636:INFO: 		 ADE on eth                       dataset:	 1.8138046264648438
+2022-11-18 16:10:33,636:INFO: Average validation o:	ADE  1.8138	FDE  2.7117
+2022-11-18 16:10:33,637:INFO: 
+===> EPOCH: 812 (P5)
+2022-11-18 16:10:33,638:INFO: - Computing loss (training)
+2022-11-18 16:10:33,884:INFO: Dataset: eth                 Batch: 1/1	Loss 2.1458 (2.1458)
+2022-11-18 16:10:33,930:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:34,203:INFO: 		 ADE on eth                       dataset:	 1.7914248704910278
+2022-11-18 16:10:34,204:INFO: Average validation o:	ADE  1.7914	FDE  2.6400
+2022-11-18 16:10:34,204:INFO: 
+===> EPOCH: 813 (P5)
+2022-11-18 16:10:34,205:INFO: - Computing loss (training)
+2022-11-18 16:10:34,466:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8133 (1.8133)
+2022-11-18 16:10:34,511:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:34,782:INFO: 		 ADE on eth                       dataset:	 1.7989051342010498
+2022-11-18 16:10:34,782:INFO: Average validation o:	ADE  1.7989	FDE  2.7108
+2022-11-18 16:10:34,783:INFO: 
+===> EPOCH: 814 (P5)
+2022-11-18 16:10:34,784:INFO: - Computing loss (training)
+2022-11-18 16:10:35,043:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7679 (1.7679)
+2022-11-18 16:10:35,090:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:35,370:INFO: 		 ADE on eth                       dataset:	 1.8219612836837769
+2022-11-18 16:10:35,370:INFO: Average validation o:	ADE  1.8220	FDE  2.7366
+2022-11-18 16:10:35,371:INFO: 
+===> EPOCH: 815 (P5)
+2022-11-18 16:10:35,372:INFO: - Computing loss (training)
+2022-11-18 16:10:35,619:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6738 (1.6738)
+2022-11-18 16:10:35,662:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:35,937:INFO: 		 ADE on eth                       dataset:	 1.794632077217102
+2022-11-18 16:10:35,937:INFO: Average validation o:	ADE  1.7946	FDE  2.7024
+2022-11-18 16:10:35,937:INFO: 
+===> EPOCH: 816 (P5)
+2022-11-18 16:10:35,938:INFO: - Computing loss (training)
+2022-11-18 16:10:36,180:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6628 (1.6628)
+2022-11-18 16:10:36,223:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:36,491:INFO: 		 ADE on eth                       dataset:	 1.7788710594177246
+2022-11-18 16:10:36,491:INFO: Average validation o:	ADE  1.7789	FDE  2.6817
+2022-11-18 16:10:36,492:INFO: 
+===> EPOCH: 817 (P5)
+2022-11-18 16:10:36,493:INFO: - Computing loss (training)
+2022-11-18 16:10:36,743:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7330 (1.7330)
+2022-11-18 16:10:36,785:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:37,060:INFO: 		 ADE on eth                       dataset:	 1.8202016353607178
+2022-11-18 16:10:37,060:INFO: Average validation o:	ADE  1.8202	FDE  2.7226
+2022-11-18 16:10:37,061:INFO: 
+===> EPOCH: 818 (P5)
+2022-11-18 16:10:37,062:INFO: - Computing loss (training)
+2022-11-18 16:10:37,306:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8310 (1.8310)
+2022-11-18 16:10:37,349:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:37,625:INFO: 		 ADE on eth                       dataset:	 1.776137351989746
+2022-11-18 16:10:37,625:INFO: Average validation o:	ADE  1.7761	FDE  2.6665
+2022-11-18 16:10:37,641:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_818.pth.tar
+2022-11-18 16:10:37,642:INFO: 
+===> EPOCH: 819 (P5)
+2022-11-18 16:10:37,642:INFO: - Computing loss (training)
+2022-11-18 16:10:37,901:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7878 (1.7878)
+2022-11-18 16:10:37,949:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:38,212:INFO: 		 ADE on eth                       dataset:	 1.805434226989746
+2022-11-18 16:10:38,212:INFO: Average validation o:	ADE  1.8054	FDE  2.7136
+2022-11-18 16:10:38,212:INFO: 
+===> EPOCH: 820 (P5)
+2022-11-18 16:10:38,213:INFO: - Computing loss (training)
+2022-11-18 16:10:38,454:INFO: Dataset: eth                 Batch: 1/1	Loss 1.5605 (1.5605)
+2022-11-18 16:10:38,499:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:38,758:INFO: 		 ADE on eth                       dataset:	 1.7910528182983398
+2022-11-18 16:10:38,758:INFO: Average validation o:	ADE  1.7911	FDE  2.6919
+2022-11-18 16:10:38,758:INFO: 
+===> EPOCH: 821 (P5)
+2022-11-18 16:10:38,759:INFO: - Computing loss (training)
+2022-11-18 16:10:38,997:INFO: Dataset: eth                 Batch: 1/1	Loss 1.9251 (1.9251)
+2022-11-18 16:10:39,044:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:39,316:INFO: 		 ADE on eth                       dataset:	 1.8127660751342773
+2022-11-18 16:10:39,316:INFO: Average validation o:	ADE  1.8128	FDE  2.7526
+2022-11-18 16:10:39,317:INFO: 
+===> EPOCH: 822 (P5)
+2022-11-18 16:10:39,318:INFO: - Computing loss (training)
+2022-11-18 16:10:39,567:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6549 (1.6549)
+2022-11-18 16:10:39,616:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:39,888:INFO: 		 ADE on eth                       dataset:	 1.8021918535232544
+2022-11-18 16:10:39,889:INFO: Average validation o:	ADE  1.8022	FDE  2.7330
+2022-11-18 16:10:39,889:INFO: 
+===> EPOCH: 823 (P5)
+2022-11-18 16:10:39,890:INFO: - Computing loss (training)
+2022-11-18 16:10:40,137:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8554 (1.8554)
+2022-11-18 16:10:40,183:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:40,456:INFO: 		 ADE on eth                       dataset:	 1.8051663637161255
+2022-11-18 16:10:40,457:INFO: Average validation o:	ADE  1.8052	FDE  2.7043
+2022-11-18 16:10:40,457:INFO: 
+===> EPOCH: 824 (P5)
+2022-11-18 16:10:40,458:INFO: - Computing loss (training)
+2022-11-18 16:10:40,703:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7495 (1.7495)
+2022-11-18 16:10:40,747:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:41,020:INFO: 		 ADE on eth                       dataset:	 1.8031219244003296
+2022-11-18 16:10:41,021:INFO: Average validation o:	ADE  1.8031	FDE  2.7385
+2022-11-18 16:10:41,021:INFO: 
+===> EPOCH: 825 (P5)
+2022-11-18 16:10:41,022:INFO: - Computing loss (training)
+2022-11-18 16:10:41,270:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6016 (1.6016)
+2022-11-18 16:10:41,317:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:41,594:INFO: 		 ADE on eth                       dataset:	 1.8069297075271606
+2022-11-18 16:10:41,595:INFO: Average validation o:	ADE  1.8069	FDE  2.6948
+2022-11-18 16:10:41,595:INFO: 
+===> EPOCH: 826 (P5)
+2022-11-18 16:10:41,596:INFO: - Computing loss (training)
+2022-11-18 16:10:41,846:INFO: Dataset: eth                 Batch: 1/1	Loss 2.0507 (2.0507)
+2022-11-18 16:10:41,891:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:42,155:INFO: 		 ADE on eth                       dataset:	 1.807228922843933
+2022-11-18 16:10:42,155:INFO: Average validation o:	ADE  1.8072	FDE  2.7177
+2022-11-18 16:10:42,156:INFO: 
+===> EPOCH: 827 (P5)
+2022-11-18 16:10:42,157:INFO: - Computing loss (training)
+2022-11-18 16:10:42,403:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7862 (1.7862)
+2022-11-18 16:10:42,448:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:42,723:INFO: 		 ADE on eth                       dataset:	 1.797884225845337
+2022-11-18 16:10:42,724:INFO: Average validation o:	ADE  1.7979	FDE  2.7345
+2022-11-18 16:10:42,724:INFO: 
+===> EPOCH: 828 (P5)
+2022-11-18 16:10:42,725:INFO: - Computing loss (training)
+2022-11-18 16:10:42,979:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8909 (1.8909)
+2022-11-18 16:10:43,026:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:43,302:INFO: 		 ADE on eth                       dataset:	 1.8111093044281006
+2022-11-18 16:10:43,302:INFO: Average validation o:	ADE  1.8111	FDE  2.7051
+2022-11-18 16:10:43,303:INFO: 
+===> EPOCH: 829 (P5)
+2022-11-18 16:10:43,304:INFO: - Computing loss (training)
+2022-11-18 16:10:43,544:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7237 (1.7237)
+2022-11-18 16:10:43,589:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:43,855:INFO: 		 ADE on eth                       dataset:	 1.7958369255065918
+2022-11-18 16:10:43,855:INFO: Average validation o:	ADE  1.7958	FDE  2.7184
+2022-11-18 16:10:43,855:INFO: 
+===> EPOCH: 830 (P5)
+2022-11-18 16:10:43,856:INFO: - Computing loss (training)
+2022-11-18 16:10:44,109:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7036 (1.7036)
+2022-11-18 16:10:44,151:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:44,426:INFO: 		 ADE on eth                       dataset:	 1.8404717445373535
+2022-11-18 16:10:44,426:INFO: Average validation o:	ADE  1.8405	FDE  2.7786
+2022-11-18 16:10:44,427:INFO: 
+===> EPOCH: 831 (P5)
+2022-11-18 16:10:44,428:INFO: - Computing loss (training)
+2022-11-18 16:10:44,687:INFO: Dataset: eth                 Batch: 1/1	Loss 2.0394 (2.0394)
+2022-11-18 16:10:44,749:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:45,048:INFO: 		 ADE on eth                       dataset:	 1.7663770914077759
+2022-11-18 16:10:45,048:INFO: Average validation o:	ADE  1.7664	FDE  2.6094
+2022-11-18 16:10:45,062:INFO:  --> Model Saved in ./models/E1//P5/CRMF_epoch_831.pth.tar
+2022-11-18 16:10:45,062:INFO: 
+===> EPOCH: 832 (P5)
+2022-11-18 16:10:45,063:INFO: - Computing loss (training)
+2022-11-18 16:10:45,308:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8599 (1.8599)
+2022-11-18 16:10:45,354:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:45,643:INFO: 		 ADE on eth                       dataset:	 1.8177409172058105
+2022-11-18 16:10:45,643:INFO: Average validation o:	ADE  1.8177	FDE  2.7709
+2022-11-18 16:10:45,644:INFO: 
+===> EPOCH: 833 (P5)
+2022-11-18 16:10:45,645:INFO: - Computing loss (training)
+2022-11-18 16:10:45,908:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8930 (1.8930)
+2022-11-18 16:10:45,952:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:46,271:INFO: 		 ADE on eth                       dataset:	 1.79158353805542
+2022-11-18 16:10:46,271:INFO: Average validation o:	ADE  1.7916	FDE  2.6505
+2022-11-18 16:10:46,272:INFO: 
+===> EPOCH: 834 (P5)
+2022-11-18 16:10:46,273:INFO: - Computing loss (training)
+2022-11-18 16:10:46,558:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8805 (1.8805)
+2022-11-18 16:10:46,600:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:46,901:INFO: 		 ADE on eth                       dataset:	 1.8259752988815308
+2022-11-18 16:10:46,901:INFO: Average validation o:	ADE  1.8260	FDE  2.7402
+2022-11-18 16:10:46,902:INFO: 
+===> EPOCH: 835 (P5)
+2022-11-18 16:10:46,903:INFO: - Computing loss (training)
+2022-11-18 16:10:47,176:INFO: Dataset: eth                 Batch: 1/1	Loss 1.4667 (1.4667)
+2022-11-18 16:10:47,222:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:47,541:INFO: 		 ADE on eth                       dataset:	 1.8012206554412842
+2022-11-18 16:10:47,542:INFO: Average validation o:	ADE  1.8012	FDE  2.7286
+2022-11-18 16:10:47,542:INFO: 
+===> EPOCH: 836 (P5)
+2022-11-18 16:10:47,544:INFO: - Computing loss (training)
+2022-11-18 16:10:47,825:INFO: Dataset: eth                 Batch: 1/1	Loss 2.3237 (2.3237)
+2022-11-18 16:10:47,878:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:48,215:INFO: 		 ADE on eth                       dataset:	 1.8256603479385376
+2022-11-18 16:10:48,215:INFO: Average validation o:	ADE  1.8257	FDE  2.7612
+2022-11-18 16:10:48,216:INFO: 
+===> EPOCH: 837 (P5)
+2022-11-18 16:10:48,216:INFO: - Computing loss (training)
+2022-11-18 16:10:48,511:INFO: Dataset: eth                 Batch: 1/1	Loss 2.0645 (2.0645)
+2022-11-18 16:10:48,558:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:48,856:INFO: 		 ADE on eth                       dataset:	 1.7786040306091309
+2022-11-18 16:10:48,857:INFO: Average validation o:	ADE  1.7786	FDE  2.6485
+2022-11-18 16:10:48,857:INFO: 
+===> EPOCH: 838 (P5)
+2022-11-18 16:10:48,858:INFO: - Computing loss (training)
+2022-11-18 16:10:49,103:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7638 (1.7638)
+2022-11-18 16:10:49,150:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:49,447:INFO: 		 ADE on eth                       dataset:	 1.7741589546203613
+2022-11-18 16:10:49,448:INFO: Average validation o:	ADE  1.7742	FDE  2.6641
+2022-11-18 16:10:49,448:INFO: 
+===> EPOCH: 839 (P5)
+2022-11-18 16:10:49,449:INFO: - Computing loss (training)
+2022-11-18 16:10:49,723:INFO: Dataset: eth                 Batch: 1/1	Loss 1.8497 (1.8497)
+2022-11-18 16:10:49,770:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:50,046:INFO: 		 ADE on eth                       dataset:	 1.7867333889007568
+2022-11-18 16:10:50,046:INFO: Average validation o:	ADE  1.7867	FDE  2.6351
+2022-11-18 16:10:50,047:INFO: 
+===> EPOCH: 840 (P5)
+2022-11-18 16:10:50,048:INFO: - Computing loss (training)
+2022-11-18 16:10:50,307:INFO: Dataset: eth                 Batch: 1/1	Loss 2.0956 (2.0956)
+2022-11-18 16:10:50,352:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:50,647:INFO: 		 ADE on eth                       dataset:	 1.8165384531021118
+2022-11-18 16:10:50,647:INFO: Average validation o:	ADE  1.8165	FDE  2.7191
+2022-11-18 16:10:50,648:INFO: 
+===> EPOCH: 841 (P5)
+2022-11-18 16:10:50,649:INFO: - Computing loss (training)
+2022-11-18 16:10:50,918:INFO: Dataset: eth                 Batch: 1/1	Loss 1.5040 (1.5040)
+2022-11-18 16:10:50,969:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:51,330:INFO: 		 ADE on eth                       dataset:	 1.8123557567596436
+2022-11-18 16:10:51,330:INFO: Average validation o:	ADE  1.8124	FDE  2.6764
+2022-11-18 16:10:51,331:INFO: 
+===> EPOCH: 842 (P5)
+2022-11-18 16:10:51,332:INFO: - Computing loss (training)
+2022-11-18 16:10:51,640:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6995 (1.6995)
+2022-11-18 16:10:51,697:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:52,047:INFO: 		 ADE on eth                       dataset:	 1.78668212890625
+2022-11-18 16:10:52,047:INFO: Average validation o:	ADE  1.7867	FDE  2.6572
+2022-11-18 16:10:52,048:INFO: 
+===> EPOCH: 843 (P5)
+2022-11-18 16:10:52,049:INFO: - Computing loss (training)
+2022-11-18 16:10:52,306:INFO: Dataset: eth                 Batch: 1/1	Loss 2.0026 (2.0026)
+2022-11-18 16:10:52,350:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:52,645:INFO: 		 ADE on eth                       dataset:	 1.783448576927185
+2022-11-18 16:10:52,645:INFO: Average validation o:	ADE  1.7834	FDE  2.6891
+2022-11-18 16:10:52,646:INFO: 
+===> EPOCH: 844 (P5)
+2022-11-18 16:10:52,647:INFO: - Computing loss (training)
+2022-11-18 16:10:52,902:INFO: Dataset: eth                 Batch: 1/1	Loss 1.6580 (1.6580)
+2022-11-18 16:10:52,955:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:53,238:INFO: 		 ADE on eth                       dataset:	 1.7758855819702148
+2022-11-18 16:10:53,239:INFO: Average validation o:	ADE  1.7759	FDE  2.6598
+2022-11-18 16:10:53,239:INFO: 
+===> EPOCH: 845 (P5)
+2022-11-18 16:10:53,240:INFO: - Computing loss (training)
+2022-11-18 16:10:53,481:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7728 (1.7728)
+2022-11-18 16:10:53,528:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:53,813:INFO: 		 ADE on eth                       dataset:	 1.800603985786438
+2022-11-18 16:10:53,814:INFO: Average validation o:	ADE  1.8006	FDE  2.7200
+2022-11-18 16:10:53,814:INFO: 
+===> EPOCH: 846 (P5)
+2022-11-18 16:10:53,815:INFO: - Computing loss (training)
+2022-11-18 16:10:54,059:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7148 (1.7148)
+2022-11-18 16:10:54,102:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:54,388:INFO: 		 ADE on eth                       dataset:	 1.8174717426300049
+2022-11-18 16:10:54,388:INFO: Average validation o:	ADE  1.8175	FDE  2.7634
+2022-11-18 16:10:54,389:INFO: 
+===> EPOCH: 847 (P5)
+2022-11-18 16:10:54,390:INFO: - Computing loss (training)
+2022-11-18 16:10:54,671:INFO: Dataset: eth                 Batch: 1/1	Loss 1.4412 (1.4412)
+2022-11-18 16:10:54,722:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:54,999:INFO: 		 ADE on eth                       dataset:	 1.790416955947876
+2022-11-18 16:10:54,999:INFO: Average validation o:	ADE  1.7904	FDE  2.6455
+2022-11-18 16:10:54,999:INFO: 
+===> EPOCH: 848 (P5)
+2022-11-18 16:10:55,000:INFO: - Computing loss (training)
+2022-11-18 16:10:55,245:INFO: Dataset: eth                 Batch: 1/1	Loss 1.7354 (1.7354)
+2022-11-18 16:10:55,295:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:55,587:INFO: 		 ADE on eth                       dataset:	 1.7809916734695435
+2022-11-18 16:10:55,588:INFO: Average validation o:	ADE  1.7810	FDE  2.6843
+2022-11-18 16:10:55,588:INFO: 
+===> EPOCH: 849 (P5)
+2022-11-18 16:10:55,589:INFO: - Computing loss (training)
+2022-11-18 16:10:55,898:INFO: Dataset: eth                 Batch: 1/1	Loss 1.9589 (1.9589)
+2022-11-18 16:10:55,955:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:56,279:INFO: 		 ADE on eth                       dataset:	 1.79241943359375
+2022-11-18 16:10:56,279:INFO: Average validation o:	ADE  1.7924	FDE  2.7378
+2022-11-18 16:10:56,280:INFO: 
+===> EPOCH: 850 (P5)
+2022-11-18 16:10:56,281:INFO: - Computing loss (training)
+2022-11-18 16:10:56,565:INFO: Dataset: eth                 Batch: 1/1	Loss 1.9117 (1.9117)
+2022-11-18 16:10:56,613:INFO: - Computing ADE (validation o)
+2022-11-18 16:10:56,888:INFO: 		 ADE on eth                       dataset:	 1.8258264064788818
+2022-11-18 16:10:56,888:INFO: Average validation o:	ADE  1.8258	FDE  2.7432
Index: train.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nfrom torch.utils.tensorboard import SummaryWriter\nfrom tqdm import tqdm\n\nfrom loader import data_loader\nfrom parser_file import get_training_parser\nfrom utils import *\nfrom models import CRMF\nfrom losses import erm_loss, irm_loss\nfrom torch.nn.utils import clip_grad_norm_\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nfrom visualize import draw_image, draw_solo, draw_solo_all\n\n\ndef main(args):\n    # Set environment variables\n    set_seed_globally(args.seed)\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_num\n    model_name = set_name_experiment(args)\n    print('model name: ', model_name)\n    if not os.path.exists(args.tfdir + '/' + model_name):\n        os.makedirs(args.tfdir + '/' + model_name)\n\n    writer = SummaryWriter(log_dir=args.tfdir + '/' + model_name, flush_secs=10)\n\n    logging.info(\"Initializing Training Set\")\n    train_envs_path, train_envs_name = get_envs_path(args.dataset_name, \"train\", args.filter_envs)\n    train_loaders = [data_loader(args, train_env_path, train_env_name) for train_env_path, train_env_name in\n                     zip(train_envs_path, train_envs_name)]\n\n    logging.info(\"Initializing Validation Set\")\n    val_envs_path, val_envs_name = get_envs_path(args.dataset_name, \"val\",\n                                                 args.filter_envs)  # +'-'+args.filter_envs_pretrain)\n    val_loaders = [data_loader(args, val_env_path, val_env_name) for val_env_path, val_env_name in\n                   zip(val_envs_path, val_envs_name)]\n\n    logging.info(\"Initializing Validation O Set\")\n    valo_envs_path, valo_envs_name = get_envs_path(args.dataset_name, \"test\", args.filter_envs)\n\n    valo_loaders = [data_loader(args, valo_env_path, valo_env_name, test=True) for valo_env_path, valo_env_name in zip(valo_envs_path, valo_envs_name)]\n    finetune_loaders = [data_loader(args, valo_env_path, valo_env_name, finetune=True) for valo_env_path, valo_env_name in zip(valo_envs_path, valo_envs_name)]\n\n    # training routine length\n    num_batches_train = min([len(train_loader) for train_loader in train_loaders])\n    num_batches_val = min([len(val_loader) for val_loader in val_loaders])\n    num_batches_valo = min([len(valo_loader) for valo_loader in valo_loaders])\n    num_batches_finetune = min([len(finetune_loader) for finetune_loader in finetune_loaders])\n\n    # bring different dataset all together for simplicity of the next functions\n    train_dataset = {'loaders': train_loaders, 'names': train_envs_name, 'num_batches': num_batches_train}\n    valid_dataset = {'loaders': val_loaders, 'names': val_envs_name, 'num_batches': num_batches_val}\n    valido_dataset = {'loaders': valo_loaders, 'names': valo_envs_name, 'num_batches': num_batches_valo}\n    finetune_dataset = {'loaders': finetune_loaders, 'names': valo_envs_name, 'num_batches': num_batches_finetune}\n\n    for dataset, ds_name in zip((train_dataset, valid_dataset, valido_dataset, finetune_dataset),\n                                ('Train', 'Validation', 'Validation O', 'Finetune')):\n        print(ds_name + ' dataset: ', dataset)\n\n    args.n_units = (\n            [args.traj_lstm_hidden_size]\n            + [int(x) for x in args.hidden_units.strip().split(\",\")]\n            + [args.graph_lstm_hidden_size]\n    )\n    args.n_heads = [int(x) for x in args.heads.strip().split(\",\")]\n\n    # create the model\n    model = CRMF(args).cuda()\n\n    # style related optimizer\n    optimizers = {\n        'variational': torch.optim.Adam(\n            [\n                {\"params\": model.coupling_layers_s.parameters(), 'lr': args.lrvariation},\n                {\"params\": model.coupling_layers_theta.parameters(), 'lr': args.lrvariation},\n                {\"params\": model.x_to_s.parameters(), 'lr': args.lrvariation},\n            ]\n        ),\n        'par': torch.optim.Adam(\n                [\n                    {\"params\": model.theta, 'lr': args.lrpar},\n                ]\n            ),\n        'var': torch.optim.Adam(\n            model.variant_encoder.parameters(),\n            lr=args.lrvar,\n        ),\n        'map': torch.optim.Adam(\n            model.mapping.parameters(),\n            lr=args.lrmap,\n        ),\n        'inv': torch.optim.Adam(\n            [\n                {\"params\": model.invariant_encoder.parameters(), 'lr': args.lrinv},\n                {\"params\": model.coupling_layers_z.parameters(), 'lr': args.lrinv},\n            ]\n        ),\n        'future_decoder': torch.optim.Adam(\n            model.future_decoder.parameters(),\n            lr=args.lrfut,\n        ),\n        'past_decoder': torch.optim.Adam(\n            model.past_decoder.parameters(),\n            lr=args.lrpast,\n        )\n    }\n\n    if args.resume:\n        load_all_model(args, model, optimizers)\n        model.cuda()\n\n    # TRAINING HAPPENS IN 4 STEPS:\n    assert (len(args.num_epochs) == 6)\n    # 1. Train the invariant encoder along with the future decoder to learn z\n    # 2. Train everything except invariant encoder to learn the other variant latent variables\n    training_steps = {f'P{i}': [sum(args.num_epochs[:i - 1]), sum(args.num_epochs[:i])] for i in range(1, 8)}\n    print(training_steps)\n\n    def get_training_step(epoch):\n        for step, r in training_steps.items():\n            if r[0] < epoch <= r[1]:\n                return step\n\n    # SOME TEST\n    if args.testonly == 1:\n        print('SIMPLY VALIDATE MODEL:')\n        validate_ade(args, model, valid_dataset, 300, 'P3', writer, 'validation', write=False)\n    elif args.testonly == 2:\n        print('TEST TIME MODIF:')\n        validate_ade(args, model, valid_dataset, 500, 'P4', writer, 'training', write=False)\n\n    if args.testonly != 0:\n        writer.close()\n        return\n\n    min_metric = 1e10\n    metric = min_metric\n    for epoch in range(args.start_epoch, sum(args.num_epochs) + 1):\n\n        training_step = get_training_step(epoch)\n        if training_step in [\"P1\", \"P2\", \"P3\"]:\n            continue\n        logging.info(f\"\\n===> EPOCH: {epoch} ({training_step})\")\n\n        if training_step in [\"P1\", \"P2\"]:\n            freeze(True, (model.coupling_layers_s, model.coupling_layers_theta, model.x_to_s,\n                          model.past_decoder, model.future_decoder, model.coupling_layers_z, model.mapping))\n            freeze(False, (model.invariant_encoder, model.variant_encoder))\n\n        elif training_step == 'P3':\n            freeze(True, (model.variant_encoder, model.coupling_layers_s,\n                          model.coupling_layers_theta, model.x_to_s, model.mapping))\n            freeze(False, (model.invariant_encoder, model.future_decoder, model.coupling_layers_z, model.past_decoder))\n\n        elif training_step == 'P4':\n            freeze(True, (model.invariant_encoder, model.mapping, model.coupling_layers_z))\n            freeze(False, (model.variant_encoder, model.coupling_layers_s, model.coupling_layers_theta,\n                           model.x_to_s, model.past_decoder, model.future_decoder))\n\n        elif training_step == 'P5':\n            freeze(True, (model.invariant_encoder, model.variant_encoder, model.coupling_layers_s, model.coupling_layers_theta,\n                          model.x_to_s, model.past_decoder, model.future_decoder, model.coupling_layers_z))\n            freeze(False, (model.mapping,))\n\n        elif training_step == 'P6':\n            freeze(True, (model.invariant_encoder, model.variant_encoder, model.past_decoder, model.future_decoder,\n                          model.mapping, model.coupling_layers_z, model.coupling_layers_s, model.coupling_layers_theta))\n            freeze(False, (model.x_to_s,))\n\n        if training_step == \"P6\":\n            train_all(args, model, optimizers, finetune_dataset, epoch, training_step, valo_envs_name, writer, stage='training')\n        else:\n            train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer, stage='training')\n\n        if training_step not in [\"P1\", \"P2\"]:\n            with torch.no_grad():\n                if training_step == \"P3\":\n                    metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')\n                    validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage='validation')\n                    validate_ade(args, model, train_dataset, epoch, training_step, writer, stage='training')\n\n                elif training_step == \"P4\":\n                    validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage='validation')\n                    validate_ade(args, model, train_dataset, epoch, training_step, writer, stage='training')\n\n                elif training_step == \"P5\":\n                    metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')\n                    train_all(args, model, optimizers, valid_dataset, epoch, training_step, val_envs_name, writer, stage='validation')\n\n                else:\n                    metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')\n\n        if training_step == \"P6\":\n            if metric < min_metric:\n                min_metric = metric\n                save_all_model(args, model, model_name, optimizers, metric, epoch, training_step)\n                print(f'\\n{\"_\" * 150}\\n')\n        else:\n            save_all_model(args, model, model_name, optimizers, metric, epoch, training_step)\n\n    writer.close()\n\n\ndef train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer,\n              stage,\n              update=True):\n    \"\"\"\n    Train the entire model for an epoch\n\n    Args:\n        - model (CausalMotionModel): model to train\n        - optimizers: inv and style optimizers to use\n        - datasets: train dataset (and pretrain dataset if finetuning)\n        - stage (str): either 'validation' or 'training': says on which dataset we calculate the loss (and only backprop on 'training')\n    \"\"\"\n    model.train()\n    logging.info(f\"- Computing loss ({stage})\")\n\n    assert (stage in ['training', 'validation'])\n\n    # Homogenous batches\n    total_loss_meter = AverageMeter(\"Total Loss\", \":.4f\")\n    e_loss_meter = AverageMeter(\"ELBO Loss\", \":.4f\")\n    p_loss_meter = AverageMeter(\"Prediction Loss\", \":.4f\")\n    step = (epoch - 1) * args.batch_size\n    for train_idx, train_loader in enumerate(train_dataset['loaders']):\n        loss_meter = AverageMeter(\"Loss\", \":.4f\")\n        progress = ProgressMeter(len(train_loader), [loss_meter],\n                                 prefix=\"Dataset: {:<20}\".format(train_envs_name[train_idx]))\n        for batch_idx, batch in enumerate(train_loader):\n            batch = [tensor.cuda() for tensor in batch]\n            (\n                obs_traj,\n                fut_traj,\n                obs_traj_rel,\n                fut_traj_rel,\n                seq_start_end,\n            ) = batch\n\n            # reset gradients\n            for opt in optimizers.values():\n                opt.zero_grad()\n\n            if training_step in [\"P1\", \"P2\"]:\n                past_pred_rel_inv, past_pred_rel_var = model(batch, training_step)\n\n                # compute reconstruction loss between output and past\n                l2_loss_rel_inv = torch.stack([l2_loss(past_pred_rel_inv, obs_traj_rel, mode=\"raw\")],\n                                              dim=1)\n                l2_loss_rel_var = torch.stack([l2_loss(past_pred_rel_var, obs_traj_rel, mode=\"raw\")], dim=1)\n\n                # empirical risk (ERM classic loss)\n                loss_sum_even, loss_sum_odd = erm_loss(l2_loss_rel_inv, seq_start_end, obs_traj_rel.shape[0])\n                loss_inv = loss_sum_even + loss_sum_odd\n\n                loss_sum_even, loss_sum_odd = erm_loss(l2_loss_rel_var, seq_start_end, obs_traj_rel.shape[0])\n                loss_var = loss_sum_even + loss_sum_odd\n\n                loss = loss_var + loss_inv\n\n            elif training_step == \"P3\":\n                q_ygx, E = model(batch, training_step)\n\n                loss_sum_even_p, loss_sum_odd_p = erm_loss(torch.log(q_ygx), seq_start_end, fut_traj_rel.shape[0])\n                predict_loss = loss_sum_even_p + loss_sum_odd_p\n\n                loss_sum_even_e, loss_sum_odd_e = erm_loss(torch.divide(E, q_ygx), seq_start_end,\n                                                           fut_traj_rel.shape[0])\n                elbo_loss = loss_sum_even_e + loss_sum_odd_e\n\n                loss = (- predict_loss) + (- elbo_loss)\n\n                e_loss_meter.update(elbo_loss.item(), obs_traj.shape[1])\n                p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])\n\n            elif training_step == \"P4\":\n                q_ygthetax, E = model(batch, training_step, env_idx=train_idx)\n\n                loss_sum_even_p, loss_sum_odd_p = erm_loss(torch.log(q_ygthetax), seq_start_end, fut_traj_rel.shape[0])\n\n                predict_loss = loss_sum_even_p + loss_sum_odd_p\n\n                loss_sum_even_e, loss_sum_odd_e = erm_loss(torch.divide(E, q_ygthetax), seq_start_end, fut_traj_rel.shape[0])\n\n                elbo_loss = loss_sum_even_e + loss_sum_odd_e\n\n                stacked_loss = torch.cat((-predict_loss, -elbo_loss))\n\n                loss = torch.sum(stacked_loss)\n\n                e_loss_meter.update(elbo_loss.item(), obs_traj.shape[1])\n                p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])\n\n            elif training_step == \"P5\":\n                pred_theta = model(batch, training_step)\n\n                l2_loss_pred = l2_loss(pred_theta, model.theta[train_idx], mode=\"sum\") / pred_theta.shape[0]\n\n                loss = l2_loss_pred\n\n            else:\n                q_ygthetax, E = model(batch, training_step)\n\n                loss_sum_even_p, loss_sum_odd_p = erm_loss(torch.log(q_ygthetax), seq_start_end, fut_traj_rel.shape[0])\n                predict_loss = loss_sum_even_p + loss_sum_odd_p\n\n                loss_sum_even_e, loss_sum_odd_e = erm_loss(torch.divide(E, q_ygthetax), seq_start_end, fut_traj_rel.shape[0])\n                elbo_loss = loss_sum_even_e + loss_sum_odd_e\n\n                stacked_loss = torch.cat((-predict_loss, -elbo_loss))\n\n                loss = torch.sum(stacked_loss)\n\n                e_loss_meter.update(elbo_loss.item(), obs_traj.shape[1])\n                p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])\n\n            # backpropagate if needed\n            if stage == 'training' and update:\n                loss.backward(retain_graph=True)\n\n                # choose which optimizer to use depending on the training step\n                if training_step in ['P1', 'P2', 'P3']: optimizers['inv'].step()\n                if training_step in ['P3', 'P4']: optimizers['future_decoder'].step()\n                if training_step in ['P3', 'P4']: optimizers['past_decoder'].step()\n                if training_step in ['P1', 'P2', 'P4']: optimizers['var'].step()\n                if training_step in ['P4', 'P6']: optimizers['variational'].step()\n                if training_step in ['P5']: optimizers['map'].step()\n                if training_step in ['P4']: optimizers['par'].step()\n\n            total_loss_meter.update(loss.item(), obs_traj.shape[1])\n            loss_meter.update(loss.item(), obs_traj.shape[1])\n            progress.display(batch_idx + 1)\n\n    if training_step in \"P1\":\n        writer.add_scalar(f\"STGAT_loss_p1/{stage}\", total_loss_meter.avg, epoch)\n    elif training_step in \"P2\":\n        writer.add_scalar(f\"STGAT_loss_p2/{stage}\", total_loss_meter.avg, epoch)\n    elif training_step == \"P3\":\n        writer.add_scalar(f\"pred_loss/{stage}\", p_loss_meter.avg, epoch)\n        writer.add_scalar(f\"elbo_loss/{stage}\", e_loss_meter.avg, epoch)\n        writer.add_scalar(f\"variational_loss/{stage}\", total_loss_meter.avg, epoch)\n    elif training_step == \"P4\":\n        writer.add_scalar(f\"variational_loss/{stage}\", total_loss_meter.avg, epoch)\n        writer.add_scalar(f\"elbo_loss/{stage}\", e_loss_meter.avg, epoch)\n        writer.add_scalar(f\"pred_loss/{stage}\", p_loss_meter.avg, epoch)\n        writer.add_scalar(f\"theta_hotel/{stage}\", torch.norm(model.theta[0]), epoch)\n        writer.add_scalar(f\"theta_univ/{stage}\", torch.norm(model.theta[1]), epoch)\n        writer.add_scalar(f\"theta_zara1/{stage}\", torch.norm(model.theta[2]), epoch)\n        writer.add_scalar(f\"theta_zara2/{stage}\", torch.norm(model.theta[3]), epoch)\n    elif training_step == \"P5\":\n        writer.add_scalar(f\"theta_loss/{stage}\", total_loss_meter.avg, epoch)\n\n    else:\n        writer.add_scalar(f\"variational_loss/{stage}\", total_loss_meter.avg, epoch)\n        writer.add_scalar(f\"elbo_loss/{stage}\", e_loss_meter.avg, epoch)\n        writer.add_scalar(f\"pred_loss/{stage}\", p_loss_meter.avg, epoch)\n\n\ndef validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage, write=True):\n    \"\"\"\n    Evaluate the performances on the validation set\n\n    Args:\n        - stage (str): either 'validation' or 'training': says on which dataset the metrics are computed\n    \"\"\"\n    model.eval()\n\n    assert (stage in ['training', 'validation', 'validation o'])\n    ade_tot_meter, fde_tot_meter = AverageMeter(\"ADE\", \":.4f\"), AverageMeter(\"FDE\", \":.4f\")\n\n    logging.info(f\"- Computing ADE ({stage})\")\n    with torch.no_grad():\n        for val_idx, (loader, loader_name) in enumerate(zip(valid_dataset['loaders'], valid_dataset['names'])):\n            ade_meter, fde_meter = AverageMeter(\"ADE\", \":.4f\"), AverageMeter(\"FDE\", \":.4f\")\n\n            for batch_idx, batch in enumerate(loader):\n                batch = [tensor.cuda() for tensor in batch]\n                (obs_traj, fut_traj, _, _, _) = batch\n\n                if training_step == \"P3\":\n                    pred_fut_traj_rel = model(batch, training_step)\n                else:\n                    pred_fut_traj_rel = model(batch, training_step, env_idx=val_idx)\n\n                # from relative path to absolute path\n                pred_fut_traj = relative_to_abs(pred_fut_traj_rel, obs_traj[-1, :, :2])\n\n                # compute ADE and FDE metrics\n                ade_, fde_ = cal_ade_fde(fut_traj, pred_fut_traj)\n\n                ade_, fde_ = ade_ / (obs_traj.shape[1] * fut_traj.shape[0]), fde_ / (obs_traj.shape[1])\n                ade_meter.update(ade_, obs_traj.shape[1])\n                fde_meter.update(fde_, obs_traj.shape[1])\n                ade_tot_meter.update(ade_, obs_traj.shape[1])\n                fde_tot_meter.update(fde_, obs_traj.shape[1])\n\n            logging.info(f'\\t\\t ADE on {loader_name:<25} dataset:\\t {ade_meter.avg}')\n\n    logging.info(f\"Average {stage}:\\tADE  {ade_tot_meter.avg:.4f}\\tFDE  {fde_tot_meter.avg:.4f}\")\n    repoch = epoch\n    if write:\n        writer.add_scalar(f\"ade/{stage}\", ade_tot_meter.avg, repoch)\n        writer.add_scalar(f\"fde/{stage}\", fde_tot_meter.avg, repoch)\n\n    ## SAVE VISUALIZATIONS\n    # if epoch % 1 == 0 and stage == 'validation':\n    # if (stage == 'validation' and rp != None and epoch % 3 == 0) or force and write:\n\n    #     obs = [b[0] for b in rp]\n    #     fut = [b[1] for b in rp]\n    #     pred = [relative_to_abs(model(b, ts), b[0][-1, :, :2]) for b in rp]\n    #     res = [[obs[i], fut[i], pred[i]] for i in range(len(rp))]\n    #     fig, array = draw_image(res)\n    #     fig.savefig(f'images/visu/pred{epoch}.png')\n    #     writer.add_image(\"Some paths\", array, epoch)\n\n    return ade_tot_meter.avg\n\n\ndef compute_ade_(pred_fut_traj_rel, obs_traj, fut_traj):\n    pred_fut_traj = relative_to_abs(pred_fut_traj_rel, obs_traj[-1, :, :2])\n    ade_, fde_ = cal_ade_fde(fut_traj, pred_fut_traj)\n    ade_ = ade_ / (fut_traj.shape[0] * obs_traj.shape[1])\n    return ade_\n\n\ndef compute_ade_single(pred_fut_traj_rel, obs_traj, fut_traj, wto):\n    return compute_ade_(pred_fut_traj_rel[:, wto * NUMBER_PERSONS:NUMBER_PERSONS * (wto + 1)],\n                        obs_traj[:, wto * NUMBER_PERSONS:NUMBER_PERSONS * (wto + 1)],\n                        fut_traj[:, wto * NUMBER_PERSONS:NUMBER_PERSONS * (wto + 1)])\n\n\ndef plot_grad_flow(named_parameters):\n    \"\"\"\n    Plots the gradients flowing through different layers in the net during training.\n    Can be used for checking for possible gradient vanishing / exploding problems.\n\n    Usage: Plug this function in Trainer class after loss.backwards() as\n    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow\n    \"\"\"\n    ave_grads = []\n    max_grads = []\n    layers = []\n    for n, p in named_parameters:\n        if (p.requires_grad) and (\"bias\" not in n):\n            if p.grad is not None:\n                layers.append(n)\n                ave_grads.append(p.grad.abs().mean())\n                max_grads.append(p.grad.abs().max())\n\n    print(\"done\")\n    # plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n    # plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n    # plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"k\")\n    # plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n    # plt.xlim(left=0, right=len(ave_grads))\n    # plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n    # plt.xlabel(\"Layers\")\n    # plt.ylabel(\"average gradient\")\n    # plt.title(\"Gradient flow\")\n    # plt.grid(True)\n    # plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n    #             Line2D([0], [0], color=\"b\", lw=4),\n    #             Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n    # plt.show()\n\n\nif __name__ == \"__main__\":\n    print('Using GPU: ' + str(torch.cuda.is_available()))\n    input_args = get_training_parser().parse_args()\n    print('Arguments for training: ', input_args)\n    set_logger(os.path.join(input_args.log_dir, \"train.log\"))\n    main(input_args)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/train.py b/train.py
--- a/train.py	
+++ b/train.py	
@@ -7,6 +7,7 @@
 from utils import *
 from models import CRMF
 from losses import erm_loss, irm_loss
+from torch.optim.lr_scheduler import OneCycleLR
 from torch.nn.utils import clip_grad_norm_
 import matplotlib.pyplot as plt
 from matplotlib.lines import Line2D
@@ -38,8 +39,10 @@
     logging.info("Initializing Validation O Set")
     valo_envs_path, valo_envs_name = get_envs_path(args.dataset_name, "test", args.filter_envs)
 
-    valo_loaders = [data_loader(args, valo_env_path, valo_env_name, test=True) for valo_env_path, valo_env_name in zip(valo_envs_path, valo_envs_name)]
-    finetune_loaders = [data_loader(args, valo_env_path, valo_env_name, finetune=True) for valo_env_path, valo_env_name in zip(valo_envs_path, valo_envs_name)]
+    valo_loaders = [data_loader(args, valo_env_path, valo_env_name, test=True) for valo_env_path, valo_env_name in
+                    zip(valo_envs_path, valo_envs_name)]
+    finetune_loaders = [data_loader(args, valo_env_path, valo_env_name, finetune=True) for valo_env_path, valo_env_name
+                        in zip(valo_envs_path, valo_envs_name)]
 
     # training routine length
     num_batches_train = min([len(train_loader) for train_loader in train_loaders])
@@ -73,17 +76,20 @@
             [
                 {"params": model.coupling_layers_s.parameters(), 'lr': args.lrvariation},
                 {"params": model.coupling_layers_theta.parameters(), 'lr': args.lrvariation},
-                {"params": model.x_to_s.parameters(), 'lr': args.lrvariation},
+                {"params": model.coupling_layers_z.parameters(), 'lr': args.lrinv},
             ]
         ),
         'par': torch.optim.Adam(
-                [
-                    {"params": model.theta, 'lr': args.lrpar},
-                ]
-            ),
+            [
+                {"params": model.theta, 'lr': args.lrpar},
+            ]
+        ),
         'var': torch.optim.Adam(
-            model.variant_encoder.parameters(),
-            lr=args.lrvar,
+            [
+                {"params": model.x_to_s.parameters(), 'lr': args.lrvar},
+                {"params": model.variant_encoder.parameters(), 'lr': args.lrvar}
+            ]
+
         ),
         'map': torch.optim.Adam(
             model.mapping.parameters(),
@@ -92,7 +98,6 @@
         'inv': torch.optim.Adam(
             [
                 {"params": model.invariant_encoder.parameters(), 'lr': args.lrinv},
-                {"params": model.coupling_layers_z.parameters(), 'lr': args.lrinv},
             ]
         ),
         'future_decoder': torch.optim.Adam(
@@ -105,15 +110,50 @@
         )
     }
 
+    num_batches = 0
+    for train_loader in train_loaders:
+        num_batches += len(train_loader)
+
+    training_steps = np.array([sum(args.num_epochs[:i]) - sum(args.num_epochs[:i - 1]) for i in range(1, 8)])
+    total_steps = num_batches * training_steps
+    lr_schedulers = {
+        "P1": {
+            'past_decoder': OneCycleLR(optimizers['past_decoder'], max_lr=1e-3, total_steps=int(total_steps[0]),
+                                       pct_start=0.3),
+            'inv': OneCycleLR(optimizers['inv'], max_lr=1e-6, total_steps=int(total_steps[0]), pct_start=0.3),
+            'variational': OneCycleLR(optimizers['variational'], max_lr=1e-3, total_steps=int(total_steps[0]),
+                                      pct_start=0.3),
+        },
+        "P2": {
+            'variational': OneCycleLR(optimizers['variational'], max_lr=1e-3, total_steps=int(total_steps[1]),
+                                      pct_start=0.3),
+            'var': OneCycleLR(optimizers['var'], max_lr=1e-6, total_steps=int(total_steps[1]), pct_start=0.3),
+            'past_decoder': OneCycleLR(optimizers['past_decoder'], max_lr=1e-3, total_steps=int(total_steps[3]),
+                                       pct_start=0.3),
+            'par': OneCycleLR(optimizers['par'], max_lr=1e-3, total_steps=int(total_steps[1]), pct_start=0.3),
+        },
+        "P3": {
+            'future_decoder': OneCycleLR(optimizers['future_decoder'], max_lr=1e-3, total_steps=int(total_steps[2]), pct_start=0.3),
+        },
+        "P4": {
+            'map': OneCycleLR(optimizers['map'], max_lr=1e-3, total_steps=int(total_steps[3]), pct_start=0.3),
+        },
+
+        "P5": {
+            'variational': OneCycleLR(optimizers['variational'], max_lr=1e-3, total_steps=int(total_steps[4]),
+                                      pct_start=0.3),
+        }
+    }
+
     if args.resume:
-        load_all_model(args, model, optimizers)
+        load_all_model(args, model, optimizers, lr_schedulers, num_batches)
         model.cuda()
 
     # TRAINING HAPPENS IN 4 STEPS:
-    assert (len(args.num_epochs) == 6)
+    assert (len(args.num_epochs) == 5)
     # 1. Train the invariant encoder along with the future decoder to learn z
     # 2. Train everything except invariant encoder to learn the other variant latent variables
-    training_steps = {f'P{i}': [sum(args.num_epochs[:i - 1]), sum(args.num_epochs[:i])] for i in range(1, 8)}
+    training_steps = {f'P{i}': [sum(args.num_epochs[:i - 1]), sum(args.num_epochs[:i])] for i in range(1, 7)}
     print(training_steps)
 
     def get_training_step(epoch):
@@ -133,64 +173,66 @@
         writer.close()
         return
 
+    beta_scheduler2 = get_beta(training_steps['P2'][0], 100, 0.01)
+    beta_scheduler1 = get_beta(training_steps['P1'][0], 50, 0.01)
     min_metric = 1e10
     metric = min_metric
     for epoch in range(args.start_epoch, sum(args.num_epochs) + 1):
 
         training_step = get_training_step(epoch)
-        if training_step in ["P1", "P2", "P3"]:
+        if training_step in ["P1"]:
             continue
         logging.info(f"\n===> EPOCH: {epoch} ({training_step})")
 
-        if training_step in ["P1", "P2"]:
-            freeze(True, (model.coupling_layers_s, model.coupling_layers_theta, model.x_to_s,
-                          model.past_decoder, model.future_decoder, model.coupling_layers_z, model.mapping))
-            freeze(False, (model.invariant_encoder, model.variant_encoder))
+        if training_step == 'P1':
+            freeze(True, (model.variant_encoder, model.coupling_layers_s, model.future_decoder,
+                          model.coupling_layers_theta, model.x_to_s, model.mapping))
+            freeze(False, (model.invariant_encoder, model.coupling_layers_z, model.past_decoder))
+
+        elif training_step == 'P2':
+            freeze(True, (
+            model.invariant_encoder, model.coupling_layers_z, model.future_decoder, model.x_to_s, model.mapping))
+            freeze(False,
+                   (model.variant_encoder, model.coupling_layers_theta, model.coupling_layers_s, model.past_decoder))
 
         elif training_step == 'P3':
-            freeze(True, (model.variant_encoder, model.coupling_layers_s,
-                          model.coupling_layers_theta, model.x_to_s, model.mapping))
-            freeze(False, (model.invariant_encoder, model.future_decoder, model.coupling_layers_z, model.past_decoder))
+            freeze(True, (model.invariant_encoder, model.mapping, model.coupling_layers_z, model.variant_encoder, model.coupling_layers_s, model.coupling_layers_theta,
+                           model.x_to_s, model.past_decoder))
+            freeze(False, (model.future_decoder,))
 
         elif training_step == 'P4':
-            freeze(True, (model.invariant_encoder, model.mapping, model.coupling_layers_z))
-            freeze(False, (model.variant_encoder, model.coupling_layers_s, model.coupling_layers_theta,
-                           model.x_to_s, model.past_decoder, model.future_decoder))
-
-        elif training_step == 'P5':
             freeze(True, (model.invariant_encoder, model.variant_encoder, model.coupling_layers_s, model.coupling_layers_theta,
                           model.x_to_s, model.past_decoder, model.future_decoder, model.coupling_layers_z))
             freeze(False, (model.mapping,))
 
-        elif training_step == 'P6':
+        elif training_step == 'P5':
             freeze(True, (model.invariant_encoder, model.variant_encoder, model.past_decoder, model.future_decoder,
                           model.mapping, model.coupling_layers_z, model.coupling_layers_s, model.coupling_layers_theta))
             freeze(False, (model.x_to_s,))
 
-        if training_step == "P6":
-            train_all(args, model, optimizers, finetune_dataset, epoch, training_step, valo_envs_name, writer, stage='training')
+        if training_step == "P5":
+            train_all(args, model, optimizers, finetune_dataset, epoch, training_step, valo_envs_name, writer, beta_scheduler1, beta_scheduler2, stage='training')
         else:
-            train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer, stage='training')
+            train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer, beta_scheduler1, beta_scheduler2, stage='training')
 
-        if training_step not in ["P1", "P2"]:
-            with torch.no_grad():
-                if training_step == "P3":
-                    metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')
-                    validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage='validation')
-                    validate_ade(args, model, train_dataset, epoch, training_step, writer, stage='training')
+        with torch.no_grad():
+            if training_step == "P1":
+                train_all(args, model, optimizers, valid_dataset, epoch, training_step, val_envs_name, writer, beta_scheduler1, beta_scheduler2, stage='validation')
+
+            elif training_step == "P2":
+                train_all(args, model, optimizers, valid_dataset, epoch, training_step, val_envs_name, writer, beta_scheduler1, beta_scheduler2, stage='validation')
+
+            elif training_step == "P3":
+                validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage='validation')
 
-                elif training_step == "P4":
-                    validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage='validation')
-                    validate_ade(args, model, train_dataset, epoch, training_step, writer, stage='training')
-
-                elif training_step == "P5":
-                    metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')
-                    train_all(args, model, optimizers, valid_dataset, epoch, training_step, val_envs_name, writer, stage='validation')
+            elif training_step == "P4":
+                metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')
+                train_all(args, model, optimizers, valid_dataset, epoch, training_step, val_envs_name, writer, beta_scheduler1, beta_scheduler2,  stage='validation')
 
-                else:
-                    metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')
+            else:
+                metric = validate_ade(args, model, valido_dataset, epoch, training_step, writer, stage='validation o')
 
-        if training_step == "P6":
+        if training_step == "P5":
             if metric < min_metric:
                 min_metric = metric
                 save_all_model(args, model, model_name, optimizers, metric, epoch, training_step)
@@ -201,7 +243,7 @@
     writer.close()
 
 
-def train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer,
+def train_all(args, model, optimizers, train_dataset, epoch, training_step, train_envs_name, writer, beta_scheduler1, beta_scheduler2,
               stage,
               update=True):
     """
@@ -222,7 +264,6 @@
     total_loss_meter = AverageMeter("Total Loss", ":.4f")
     e_loss_meter = AverageMeter("ELBO Loss", ":.4f")
     p_loss_meter = AverageMeter("Prediction Loss", ":.4f")
-    step = (epoch - 1) * args.batch_size
     for train_idx, train_loader in enumerate(train_dataset['loaders']):
         loss_meter = AverageMeter("Loss", ":.4f")
         progress = ProgressMeter(len(train_loader), [loss_meter],
@@ -241,119 +282,104 @@
             for opt in optimizers.values():
                 opt.zero_grad()
 
-            if training_step in ["P1", "P2"]:
-                past_pred_rel_inv, past_pred_rel_var = model(batch, training_step)
-
-                # compute reconstruction loss between output and past
-                l2_loss_rel_inv = torch.stack([l2_loss(past_pred_rel_inv, obs_traj_rel, mode="raw")],
-                                              dim=1)
-                l2_loss_rel_var = torch.stack([l2_loss(past_pred_rel_var, obs_traj_rel, mode="raw")], dim=1)
-
-                # empirical risk (ERM classic loss)
-                loss_sum_even, loss_sum_odd = erm_loss(l2_loss_rel_inv, seq_start_end, obs_traj_rel.shape[0])
-                loss_inv = loss_sum_even + loss_sum_odd
-
-                loss_sum_even, loss_sum_odd = erm_loss(l2_loss_rel_var, seq_start_end, obs_traj_rel.shape[0])
-                loss_var = loss_sum_even + loss_sum_odd
+            if training_step == "P1":
+                beta = beta_scheduler1.beta_val(epoch)
+                recon_loss, elbo_loss = model(batch, training_step)
 
-                loss = loss_var + loss_inv
-
-            elif training_step == "P3":
-                q_ygx, E = model(batch, training_step)
-
-                loss_sum_even_p, loss_sum_odd_p = erm_loss(torch.log(q_ygx), seq_start_end, fut_traj_rel.shape[0])
-                predict_loss = loss_sum_even_p + loss_sum_odd_p
-
-                loss_sum_even_e, loss_sum_odd_e = erm_loss(torch.divide(E, q_ygx), seq_start_end,
-                                                           fut_traj_rel.shape[0])
-                elbo_loss = loss_sum_even_e + loss_sum_odd_e
-
-                loss = (- predict_loss) + (- elbo_loss)
+                loss = (- recon_loss) + (- beta * elbo_loss)
 
                 e_loss_meter.update(elbo_loss.item(), obs_traj.shape[1])
-                p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])
-
-            elif training_step == "P4":
-                q_ygthetax, E = model(batch, training_step, env_idx=train_idx)
-
-                loss_sum_even_p, loss_sum_odd_p = erm_loss(torch.log(q_ygthetax), seq_start_end, fut_traj_rel.shape[0])
-
-                predict_loss = loss_sum_even_p + loss_sum_odd_p
-
-                loss_sum_even_e, loss_sum_odd_e = erm_loss(torch.divide(E, q_ygthetax), seq_start_end, fut_traj_rel.shape[0])
+                p_loss_meter.update(recon_loss.item(), obs_traj.shape[1])
 
-                elbo_loss = loss_sum_even_e + loss_sum_odd_e
+            elif training_step == "P2":
+                beta = beta_scheduler2.beta_val(epoch)
+                recon_loss, elbo_loss = model(batch, training_step, env_idx=train_idx)
 
-                stacked_loss = torch.cat((-predict_loss, -elbo_loss))
-
-                loss = torch.sum(stacked_loss)
-
+                loss = (- recon_loss) + (- beta * elbo_loss)
                 e_loss_meter.update(elbo_loss.item(), obs_traj.shape[1])
-                p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])
+                p_loss_meter.update(recon_loss.item(), obs_traj.shape[1])
 
-            elif training_step == "P5":
-                pred_theta = model(batch, training_step)
+            elif training_step == "P3":
+                pred_loss = model(batch, training_step, env_idx=train_idx)
 
-                l2_loss_pred = l2_loss(pred_theta, model.theta[train_idx], mode="sum") / pred_theta.shape[0]
+                loss = - pred_loss
 
-                loss = l2_loss_pred
+            elif training_step == "P4":
+                loss = model(batch, training_step, env_idx=train_idx)
 
             else:
-                q_ygthetax, E = model(batch, training_step)
-
-                loss_sum_even_p, loss_sum_odd_p = erm_loss(torch.log(q_ygthetax), seq_start_end, fut_traj_rel.shape[0])
-                predict_loss = loss_sum_even_p + loss_sum_odd_p
+                recon_loss, elbo_loss = model(batch, training_step)
 
-                loss_sum_even_e, loss_sum_odd_e = erm_loss(torch.divide(E, q_ygthetax), seq_start_end, fut_traj_rel.shape[0])
-                elbo_loss = loss_sum_even_e + loss_sum_odd_e
-
-                stacked_loss = torch.cat((-predict_loss, -elbo_loss))
-
-                loss = torch.sum(stacked_loss)
+                loss = (- recon_loss) + (- elbo_loss)
 
                 e_loss_meter.update(elbo_loss.item(), obs_traj.shape[1])
-                p_loss_meter.update(predict_loss.item(), obs_traj.shape[1])
+                p_loss_meter.update(recon_loss.item(), obs_traj.shape[1])
 
             # backpropagate if needed
             if stage == 'training' and update:
-                loss.backward(retain_graph=True)
+                loss.backward()
 
+                # lr_scheduler_optims = lr_schedulers[training_step]
                 # choose which optimizer to use depending on the training step
-                if training_step in ['P1', 'P2', 'P3']: optimizers['inv'].step()
-                if training_step in ['P3', 'P4']: optimizers['future_decoder'].step()
-                if training_step in ['P3', 'P4']: optimizers['past_decoder'].step()
-                if training_step in ['P1', 'P2', 'P4']: optimizers['var'].step()
-                if training_step in ['P4', 'P6']: optimizers['variational'].step()
-                if training_step in ['P5']: optimizers['map'].step()
-                if training_step in ['P4']: optimizers['par'].step()
+                if training_step in ['P1']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['inv'].step()
+                    optimizers['inv'].step()
+
+                if training_step in ['P3']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['future_decoder'].step()
+                    optimizers['future_decoder'].step()
+
+                if training_step in ['P2', 'P2']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['past_decoder'].step()
+                    optimizers['past_decoder'].step()
+
+                if training_step in ['P2', 'P5']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['var'].step()
+                    optimizers['var'].step()
+
+                if training_step in ['P1', 'P2']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['variational'].step()
+                    optimizers['variational'].step()
+
+                if training_step in ['P4']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['map'].step()
+                    optimizers['map'].step()
+
+                if training_step in ['P2']:
+                    # if lr_scheduler_optims is not None:
+                    #     lr_scheduler_optims['par'].step()
+                    optimizers['par'].step()
 
             total_loss_meter.update(loss.item(), obs_traj.shape[1])
             loss_meter.update(loss.item(), obs_traj.shape[1])
             progress.display(batch_idx + 1)
 
-    if training_step in "P1":
-        writer.add_scalar(f"STGAT_loss_p1/{stage}", total_loss_meter.avg, epoch)
-    elif training_step in "P2":
-        writer.add_scalar(f"STGAT_loss_p2/{stage}", total_loss_meter.avg, epoch)
-    elif training_step == "P3":
-        writer.add_scalar(f"pred_loss/{stage}", p_loss_meter.avg, epoch)
+    if training_step == "P1":
+        writer.add_scalar(f"reconstruction_loss/{stage}", p_loss_meter.avg, epoch)
         writer.add_scalar(f"elbo_loss/{stage}", e_loss_meter.avg, epoch)
         writer.add_scalar(f"variational_loss/{stage}", total_loss_meter.avg, epoch)
-    elif training_step == "P4":
+    elif training_step == "P2":
         writer.add_scalar(f"variational_loss/{stage}", total_loss_meter.avg, epoch)
         writer.add_scalar(f"elbo_loss/{stage}", e_loss_meter.avg, epoch)
-        writer.add_scalar(f"pred_loss/{stage}", p_loss_meter.avg, epoch)
-        writer.add_scalar(f"theta_hotel/{stage}", torch.norm(model.theta[0]), epoch)
-        writer.add_scalar(f"theta_univ/{stage}", torch.norm(model.theta[1]), epoch)
-        writer.add_scalar(f"theta_zara1/{stage}", torch.norm(model.theta[2]), epoch)
-        writer.add_scalar(f"theta_zara2/{stage}", torch.norm(model.theta[3]), epoch)
-    elif training_step == "P5":
+        writer.add_scalar(f"reconstruction_loss/{stage}", p_loss_meter.avg, epoch)
+        writer.add_scalar(f"theta_hotel/{stage}", torch.mean(model.theta[0]), epoch)
+        writer.add_scalar(f"theta_univ/{stage}", torch.mean(model.theta[1]), epoch)
+        writer.add_scalar(f"theta_zara1/{stage}", torch.mean(model.theta[2]), epoch)
+        writer.add_scalar(f"theta_zara2/{stage}", torch.mean(model.theta[3]), epoch)
+    elif training_step == "P3":
+        writer.add_scalar(f"pred_loss/{stage}", total_loss_meter.avg, epoch)
+    elif training_step == "P4":
         writer.add_scalar(f"theta_loss/{stage}", total_loss_meter.avg, epoch)
-
     else:
         writer.add_scalar(f"variational_loss/{stage}", total_loss_meter.avg, epoch)
         writer.add_scalar(f"elbo_loss/{stage}", e_loss_meter.avg, epoch)
-        writer.add_scalar(f"pred_loss/{stage}", p_loss_meter.avg, epoch)
+        writer.add_scalar(f"recon_loss/{stage}", p_loss_meter.avg, epoch)
 
 
 def validate_ade(args, model, valid_dataset, epoch, training_step, writer, stage, write=True):
@@ -377,7 +403,7 @@
                 batch = [tensor.cuda() for tensor in batch]
                 (obs_traj, fut_traj, _, _, _) = batch
 
-                if training_step == "P3":
+                if stage == "validation o":
                     pred_fut_traj_rel = model(batch, training_step)
                 else:
                     pred_fut_traj_rel = model(batch, training_step, env_idx=val_idx)
Index: models.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport random\nfrom utils import *\nimport math\nfrom torch.distributions import MultivariateNormal, Gamma, Poisson\n\n\ndef get_noise(shape, noise_type):\n    if noise_type == \"gaussian\":\n        return torch.randn(*shape).cuda()\n    elif noise_type == \"uniform\":\n        return torch.rand(*shape).sub_(0.5).mul_(2.0).cuda()\n    raise ValueError('Unrecognized noise type \"%s\"' % noise_type)\n\n\nclass CouplingLayer(nn.Module):\n    \"\"\"Coupling layer in RealNVP.\n    Args:\n        in_channels (int): Number of channels in the input.\n        mid_channels (int): Number of channels in the `s` and `t` network.\n        num_blocks (int): Number of residual blocks in the `s` and `t` network.\n        mask_type (MaskType): One of `MaskType.CHECKERBOARD` or `MaskType.CHANNEL_WISE`.\n        reverse_mask (bool): Whether to reverse the mask. Useful for alternating masks.\n    \"\"\"\n    def __init__(self, latent_dim, reverse_mask, hidden_dims=None):\n        super(CouplingLayer, self).__init__()\n        # Save mask info\n        self.reverse_mask = reverse_mask\n\n        # Build scale and translate network\n        if hidden_dims is None:\n            hidden_dims = [16, 32]\n\n        modules = []\n        in_channels = latent_dim // 2\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Linear(in_channels, h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        modules.append(nn.Linear(hidden_dims[-1], latent_dim))\n        self.st_net = nn.Sequential(*modules)\n\n        # Learnable scale for s\n        self.rescale = nn.utils.weight_norm(Rescale(latent_dim // 2))\n\n    def forward(self, x, sldj=None, reverse=False):\n        # Channel-wise mask\n        if self.reverse_mask:\n            x_id, x_change = x.chunk(2, dim=1)\n        else:\n            x_change, x_id = x.chunk(2, dim=1)\n\n        st = self.st_net(x_id)\n        s, t = st.chunk(2, dim=1)\n        s = self.rescale(torch.tanh(s))\n\n        # Scale and translate\n        if reverse:\n            inv_exp_s = s.mul(-1).exp()\n            if torch.isnan(inv_exp_s).any():\n                raise RuntimeError('Scale factor has NaN entries')\n            x_change = x_change * inv_exp_s - t\n        else:\n            exp_s = s.exp()\n            if torch.isnan(exp_s).any():\n                raise RuntimeError('Scale factor has NaN entries')\n            x_change = (x_change + t) * exp_s\n\n            # Add log-determinant of the Jacobian\n            sldj += s.view(s.size(0), -1).sum(-1)\n\n        if self.reverse_mask:\n            x = torch.cat((x_id, x_change), dim=1)\n        else:\n            x = torch.cat((x_change, x_id), dim=1)\n\n        return x, sldj\n\nclass Rescale(nn.Module):\n    \"\"\"Per-channel rescaling. Need a proper `nn.Module` so we can wrap it\n    with `torch.nn.utils.weight_norm`.\n    Args:\n        num_channels (int): Number of channels in the input.\n    \"\"\"\n    def __init__(self, num_channels):\n        super(Rescale, self).__init__()\n        self.weight = nn.Parameter(torch.ones(num_channels))\n\n    def forward(self, x):\n        x = self.weight * x\n        return x\n\n\nclass BatchMultiHeadGraphAttention(nn.Module):\n    def __init__(self, n_head, f_in, f_out, attn_dropout, bias=True):\n        super(BatchMultiHeadGraphAttention, self).__init__()\n        self.n_head = n_head\n        self.f_in = f_in\n        self.f_out = f_out\n        self.w = nn.Parameter(torch.Tensor(n_head, f_in, f_out))\n        self.a_src = nn.Parameter(torch.Tensor(n_head, f_out, 1))\n        self.a_dst = nn.Parameter(torch.Tensor(n_head, f_out, 1))\n\n        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n        self.softmax = nn.Softmax(dim=-1)\n        self.dropout = nn.Dropout(attn_dropout)\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(f_out))\n            nn.init.constant_(self.bias, 0)\n        else:\n            self.register_parameter(\"bias\", None)\n\n        nn.init.xavier_uniform_(self.w, gain=1.414)\n        nn.init.xavier_uniform_(self.a_src, gain=1.414)\n        nn.init.xavier_uniform_(self.a_dst, gain=1.414)\n\n    def forward(self, h):\n        bs, n = h.size()[:2]\n        h_prime = torch.matmul(h.unsqueeze(1), self.w)\n        attn_src = torch.matmul(h_prime, self.a_src)\n        attn_dst = torch.matmul(h_prime, self.a_dst)\n        attn = attn_src.expand(-1, -1, -1, n) + attn_dst.expand(-1, -1, -1, n).permute(\n            0, 1, 3, 2\n        )\n        attn = self.leaky_relu(attn)\n        attn = self.softmax(attn)\n        attn = self.dropout(attn)\n        output = torch.matmul(attn, h_prime)\n        if self.bias is not None:\n            return output + self.bias, attn\n        else:\n            return output, attn\n\n    def __repr__(self):\n        return (\n                self.__class__.__name__\n                + \" (\"\n                + str(self.n_head)\n                + \" -> \"\n                + str(self.f_in)\n                + \" -> \"\n                + str(self.f_out)\n                + \")\"\n        )\n\n\nclass GAT(nn.Module):\n    def __init__(self, n_units, n_heads, dropout=0.2, alpha=0.2):\n        super(GAT, self).__init__()\n        self.n_layer = len(n_units) - 1\n        self.dropout = dropout\n        self.layer_stack = nn.ModuleList()\n\n        for i in range(self.n_layer):\n            f_in = n_units[i] * n_heads[i - 1] if i else n_units[i]\n            self.layer_stack.append(\n                BatchMultiHeadGraphAttention(\n                    n_heads[i], f_in=f_in, f_out=n_units[i + 1], attn_dropout=dropout\n                )\n            )\n\n        self.norm_list = [\n            torch.nn.InstanceNorm1d(32).cuda(),\n            torch.nn.InstanceNorm1d(64).cuda(),\n        ]\n\n    def forward(self, x):\n        bs, n = x.size()[:2]\n        for i, gat_layer in enumerate(self.layer_stack):\n            x = self.norm_list[i](x.permute(0, 2, 1)).permute(0, 2, 1)\n            x, attn = gat_layer(x)\n            if i + 1 == self.n_layer:\n                x = x.squeeze(dim=1)\n            else:\n                x = F.elu(x.transpose(1, 2).contiguous().view(bs, n, -1))\n                x = F.dropout(x, self.dropout, training=self.training)\n        else:\n            return x\n\n\nclass GATEncoder(nn.Module):\n    def __init__(self, n_units, n_heads, dropout, alpha):\n        super(GATEncoder, self).__init__()\n        self.gat_net = GAT(n_units, n_heads, dropout, alpha)\n\n    def forward(self, obs_traj_embedding, seq_start_end):\n        graph_embeded_data = []\n        for start, end in seq_start_end.data:\n            curr_seq_embedding_traj = obs_traj_embedding[:, start:end, :]\n            curr_seq_graph_embedding = self.gat_net(curr_seq_embedding_traj)\n            graph_embeded_data.append(curr_seq_graph_embedding)\n        graph_embeded_data = torch.cat(graph_embeded_data, dim=1)\n        return graph_embeded_data\n\n\nclass STGAT_encoder_inv(nn.Module):\n    def __init__(\n            self,\n            obs_len,\n            fut_len,\n            n_coordinates,\n            traj_lstm_hidden_size,\n            n_units,\n            n_heads,\n            graph_network_out_dims,\n            dropout,\n            alpha,\n            graph_lstm_hidden_size,\n            z_dim,\n            hidden_dims=None,\n            add_confidence=True,\n    ):\n        super(STGAT_encoder_inv, self).__init__()\n\n        self.obs_len = obs_len\n        self.fut_len = fut_len\n\n        self.gatencoder = GATEncoder(\n            n_units=n_units, n_heads=n_heads, dropout=dropout, alpha=alpha\n        )\n\n        if hidden_dims is None:\n            hidden_dims = [32, 64]\n\n        modules = []\n        in_channels = traj_lstm_hidden_size + graph_lstm_hidden_size\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Linear(in_channels, h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        self.z_dim = z_dim\n        self.mapping = nn.Sequential(*modules)\n        self.fc_mu = nn.Linear(hidden_dims[-1], z_dim)\n        self.fc_var = nn.Linear(hidden_dims[-1], z_dim)\n\n        self.graph_lstm_hidden_size = graph_lstm_hidden_size\n        self.traj_lstm_hidden_size = traj_lstm_hidden_size\n        self.n_coordinates = n_coordinates\n        self.add_confidence = add_confidence\n\n        self.traj_lstm_model = nn.LSTMCell(\n            n_coordinates + add_confidence,\n            traj_lstm_hidden_size\n        )\n        self.graph_lstm_model = nn.LSTMCell(\n            graph_network_out_dims,\n            graph_lstm_hidden_size\n        )\n        self.traj_hidden2pos = nn.Linear(\n            traj_lstm_hidden_size,\n            n_coordinates\n        )\n        self.traj_gat_hidden2pos = nn.Linear(\n            traj_lstm_hidden_size + graph_lstm_hidden_size,\n            n_coordinates\n        )\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.LSTMCell):\n                m.weight_hh.data.normal_(0, 0.1)\n                m.weight_ih.data.normal_(0, 0.1)\n                m.bias_hh.data.zero_()\n                m.bias_ih.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.1)\n                m.bias.data.zero_()\n\n    def init_hidden_traj_lstm(self, batch):\n        return (\n            torch.randn(batch, self.traj_lstm_hidden_size).cuda(),\n            torch.randn(batch, self.traj_lstm_hidden_size).cuda(),\n        )\n\n    def init_hidden_graph_lstm(self, batch):\n        return (\n            torch.randn(batch, self.graph_lstm_hidden_size).cuda(),\n            torch.randn(batch, self.graph_lstm_hidden_size).cuda(),\n        )\n\n    def forward(\n            self,\n            batch,\n            training_step=3,\n    ):\n        (_, _, obs_traj_rel, _, seq_start_end) = batch\n        num_peds = obs_traj_rel.shape[1]\n        traj_lstm_h_t, traj_lstm_c_t = self.init_hidden_traj_lstm(num_peds)\n        graph_lstm_h_t, graph_lstm_c_t = self.init_hidden_graph_lstm(num_peds)\n        pred_traj_rel = []\n        traj_lstm_hidden_states = []\n        graph_lstm_hidden_states = []\n\n        # traj_lstm (used in step 1,2,3)\n        for i in range(self.obs_len):\n            traj_lstm_h_t, traj_lstm_c_t = self.traj_lstm_model(\n                obs_traj_rel[i], (traj_lstm_h_t, traj_lstm_c_t)\n            )\n            if training_step == \"P1\":\n                output = self.traj_hidden2pos(traj_lstm_h_t)\n                pred_traj_rel += [output]\n            else:\n                traj_lstm_hidden_states += [traj_lstm_h_t]\n\n        # graph_lstm (used in step 2,3)\n        if training_step != \"P1\":\n            graph_lstm_input = self.gatencoder(\n                torch.stack(traj_lstm_hidden_states), seq_start_end\n            )\n            for i in range(self.obs_len):\n                graph_lstm_h_t, graph_lstm_c_t = self.graph_lstm_model(\n                    graph_lstm_input[i], (graph_lstm_h_t, graph_lstm_c_t)\n                )\n                if training_step == \"P2\":\n                    encoded_before_noise_hidden = torch.cat(\n                        (traj_lstm_hidden_states[i], graph_lstm_h_t), dim=1\n                    )\n                    output = self.traj_gat_hidden2pos(encoded_before_noise_hidden)\n                    pred_traj_rel += [output]\n                else:\n                    graph_lstm_hidden_states += [graph_lstm_h_t]\n\n        if training_step in [\"P1\", \"P2\"]:\n            return torch.stack(pred_traj_rel)\n\n        else:\n\n            encoded_before_noise_hidden = torch.cat((traj_lstm_hidden_states[-1], graph_lstm_hidden_states[-1]), dim=1)\n            mu = self.fc_mu(self.mapping(encoded_before_noise_hidden))\n            logvar = self.fc_var(self.mapping(encoded_before_noise_hidden))\n\n            z = MultivariateNormal(mu, torch.diag_embed(torch.exp(logvar)))\n\n            return z\n\n\nclass STGAT_encoder_var(nn.Module):\n    def __init__(\n            self,\n            obs_len,\n            fut_len,\n            n_coordinates,\n            traj_lstm_hidden_size,\n            n_units,\n            n_heads,\n            graph_network_out_dims,\n            dropout,\n            alpha,\n            graph_lstm_hidden_size,\n            add_confidence=True,\n    ):\n        super(STGAT_encoder_var, self).__init__()\n\n        self.obs_len = obs_len\n        self.fut_len = fut_len\n\n        self.gatencoder = GATEncoder(\n            n_units=n_units, n_heads=n_heads, dropout=dropout, alpha=alpha\n        )\n\n        self.graph_lstm_hidden_size = graph_lstm_hidden_size\n        self.traj_lstm_hidden_size = traj_lstm_hidden_size\n        self.n_coordinates = n_coordinates\n        self.add_confidence = add_confidence\n\n        self.traj_lstm_model = nn.LSTMCell(\n            n_coordinates + add_confidence,\n            traj_lstm_hidden_size\n        )\n        self.graph_lstm_model = nn.LSTMCell(\n            graph_network_out_dims,\n            graph_lstm_hidden_size\n        )\n        self.traj_hidden2pos = nn.Linear(\n            traj_lstm_hidden_size,\n            n_coordinates\n        )\n        self.traj_gat_hidden2pos = nn.Linear(\n            traj_lstm_hidden_size + graph_lstm_hidden_size,\n            n_coordinates\n        )\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.LSTMCell):\n                m.weight_hh.data.normal_(0, 0.1)\n                m.weight_ih.data.normal_(0, 0.1)\n                m.bias_hh.data.zero_()\n                m.bias_ih.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.1)\n                m.bias.data.zero_()\n\n    def init_hidden_traj_lstm(self, batch):\n        return (\n            torch.randn(batch, self.traj_lstm_hidden_size).cuda(),\n            torch.randn(batch, self.traj_lstm_hidden_size).cuda(),\n        )\n\n    def init_hidden_graph_lstm(self, batch):\n        return (\n            torch.randn(batch, self.graph_lstm_hidden_size).cuda(),\n            torch.randn(batch, self.graph_lstm_hidden_size).cuda(),\n        )\n\n    def forward(\n            self,\n            batch,\n            training_step=3,\n    ):\n        (_, _, obs_traj_rel, _, seq_start_end) = batch\n        num_peds = obs_traj_rel.shape[1]\n        traj_lstm_h_t, traj_lstm_c_t = self.init_hidden_traj_lstm(num_peds)\n        graph_lstm_h_t, graph_lstm_c_t = self.init_hidden_graph_lstm(num_peds)\n        pred_traj_rel = []\n        traj_lstm_hidden_states = []\n        graph_lstm_hidden_states = []\n\n        # traj_lstm (used in step 1,2,3)\n        for i in range(self.obs_len):\n            traj_lstm_h_t, traj_lstm_c_t = self.traj_lstm_model(\n                obs_traj_rel[i], (traj_lstm_h_t, traj_lstm_c_t)\n            )\n            if training_step == \"P1\":\n                output = self.traj_hidden2pos(traj_lstm_h_t)\n                pred_traj_rel += [output]\n            else:\n                traj_lstm_hidden_states += [traj_lstm_h_t]\n\n        # graph_lstm (used in step 2,3)\n        if training_step != \"P1\":\n            graph_lstm_input = self.gatencoder(\n                torch.stack(traj_lstm_hidden_states), seq_start_end\n            )\n            for i in range(self.obs_len):\n                graph_lstm_h_t, graph_lstm_c_t = self.graph_lstm_model(\n                    graph_lstm_input[i], (graph_lstm_h_t, graph_lstm_c_t)\n                )\n                if training_step == \"P2\":\n                    encoded_before_noise_hidden = torch.cat(\n                        (traj_lstm_hidden_states[i], graph_lstm_h_t), dim=1\n                    )\n                    output = self.traj_gat_hidden2pos(encoded_before_noise_hidden)\n                    pred_traj_rel += [output]\n                else:\n                    graph_lstm_hidden_states += [graph_lstm_h_t]\n\n        if training_step in [\"P1\", \"P2\"]:\n            return torch.stack(pred_traj_rel)\n\n        else:\n            return torch.cat((graph_lstm_hidden_states[-1], traj_lstm_hidden_states[-1]), dim=1)\n\n\nclass future_STGAT_decoder(nn.Module):\n    def __init__(\n            self,\n            obs_len,\n            fut_len,\n            n_coordinates,\n            s_dim,\n            z_dim,\n            teacher_forcing_ratio=0.5,\n            var_p=0.5,\n    ):\n        super(future_STGAT_decoder, self).__init__()\n\n        self.obs_len = obs_len\n        self.fut_len = fut_len\n        self.var_p = var_p\n        self.teacher_forcing_ratio = teacher_forcing_ratio\n\n        self.n_coordinates = n_coordinates\n        self.pred_lstm_hidden_size1 = z_dim + s_dim\n        self.pred_lstm_hidden_size2 = z_dim\n        self.pred_hidden2pos = nn.ModuleList([nn.Linear(self.pred_lstm_hidden_size1, n_coordinates),\n                                              nn.Linear(self.pred_lstm_hidden_size2, n_coordinates)])\n        self.pred_lstm_model = nn.ModuleList([nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size1),\n                                              nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size2)])\n\n        self.scale = torch.tensor(1.).cuda().requires_grad_()\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.LSTMCell):\n                m.weight_hh.data.normal_(0, 0.1)\n                m.weight_ih.data.normal_(0, 0.1)\n                m.bias_hh.data.zero_()\n                m.bias_ih.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.1)\n                m.bias.data.zero_()\n\n    def forward(\n            self,\n            batch,\n            pred_lstm_hidden,\n            training_step,\n            variant_feats,\n    ):\n\n        (_, _, obs_traj_rel, fut_traj_rel, seq_start_end) = batch\n        pred_traj_rel = []\n\n        input_t = obs_traj_rel[self.obs_len - 1, :, :self.n_coordinates]\n        output = input_t\n        pred_lstm_c_t = torch.zeros_like(pred_lstm_hidden).cuda()\n        p = []\n        # during training\n        if self.training:\n            for i in range(self.fut_len):\n                if i >= 1:\n                    teacher_force = random.random() < self.teacher_forcing_ratio\n                    if teacher_force:\n                        input_t = fut_traj_rel[i - 1, :, :self.n_coordinates]  # with teacher help\n                    else:\n                        input_t = output\n\n                if variant_feats:\n                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[0](input_t,\n                                                                              (pred_lstm_hidden, pred_lstm_c_t))\n                    output = self.pred_hidden2pos[0](pred_lstm_hidden)\n                else:\n                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[1](input_t,\n                                                                              (pred_lstm_hidden, pred_lstm_c_t))\n                    output = self.pred_hidden2pos[1](pred_lstm_hidden)\n\n                pred_traj_rel += [output]\n                p += [MultivariateNormal(output, torch.diag_embed(self.var_p * torch.ones(fut_traj_rel.size(1), 2).cuda()))]\n\n            return p\n\n        # during test\n        else:\n            for i in range(self.fut_len):\n                if training_step == \"P3\":\n                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[1](output, (pred_lstm_hidden, pred_lstm_c_t))\n                    output = self.pred_hidden2pos[1](pred_lstm_hidden)\n                else:\n                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[0](output, (pred_lstm_hidden, pred_lstm_c_t))\n                    output = self.pred_hidden2pos[0](pred_lstm_hidden)\n                pred_traj_rel += [output]\n\n            return torch.stack(pred_traj_rel)\n\n\nclass past_decoder(nn.Module):\n    def __init__(\n            self,\n            obs_len,\n            n_coordinates,\n            z_dim,\n            s_dim,\n    ):\n        super(past_decoder, self).__init__()\n\n        self.obs_len = obs_len\n\n        self.n_coordinates = n_coordinates\n        self.pred_lstm_hidden_size1 = z_dim + s_dim\n        self.pred_lstm_hidden_size2 = z_dim\n        self.pred_hidden2pos = nn.ModuleList([nn.Linear(self.pred_lstm_hidden_size1, n_coordinates),\n                                              nn.Linear(self.pred_lstm_hidden_size2, n_coordinates)])\n        self.pred_lstm_model = nn.ModuleList([nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size1),\n                                              nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size2)])\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.LSTMCell):\n                m.weight_hh.data.normal_(0, 0.1)\n                m.weight_ih.data.normal_(0, 0.1)\n                m.bias_hh.data.zero_()\n                m.bias_ih.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.1)\n                m.bias.data.zero_()\n\n    def forward(\n            self,\n            batch,\n            pred_lstm_hidden,\n            variant_feats\n    ):\n\n        (_, _, obs_traj_rel, _, seq_start_end) = batch\n        pred_traj_rel = []\n\n        pred_lstm_c_t = torch.zeros_like(pred_lstm_hidden).cuda()\n        for i in range(self.obs_len):\n            if i >= 1:\n                input_t = obs_traj_rel[i - 1, :, :self.n_coordinates]\n            else:\n                input_t = obs_traj_rel[0, :, :self.n_coordinates]\n\n            if variant_feats:\n                pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[0](input_t, (pred_lstm_hidden, pred_lstm_c_t))\n                output = self.pred_hidden2pos[0](pred_lstm_hidden)\n            else:\n                pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[1](input_t, (pred_lstm_hidden, pred_lstm_c_t))\n                output = self.pred_hidden2pos[1](pred_lstm_hidden)\n\n            pred_traj_rel += [output]\n\n        return torch.stack(pred_traj_rel)\n\n\nclass regressor(nn.Module):\n    def __init__(self,\n                 obs_len,\n                 n_coordinates,\n                 latent_dim: int,\n                 hidden_dims=None,\n                 **kwargs) -> None:\n        super(regressor, self).__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [32, 64]\n\n        modules = []\n        in_channels = n_coordinates * obs_len\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Linear(in_channels, h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        self.mapping = nn.Sequential(*modules)\n        self.final = nn.Linear(hidden_dims[-1], latent_dim)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.LSTMCell):\n                m.weight_hh.data.normal_(0, 0.1)\n                m.weight_ih.data.normal_(0, 0.1)\n                m.bias_hh.data.zero_()\n                m.bias_ih.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.1)\n                m.bias.data.zero_()\n\n    def forward(self, input):\n        result = torch.flatten(input.permute(1, 0, 2), start_dim=1)\n        pred_theta = self.final(self.mapping(result))\n\n        return pred_theta\n\n\nclass simple_mapping(nn.Module):\n    def __init__(self,\n                 traj_lstm_hidden_size: int,\n                 graph_lstm_hidden_size: int,\n                 latent_dim,\n                 s_dim: int,\n                 hidden_dims=None,\n                 **kwargs) -> None:\n        super(simple_mapping, self).__init__()\n\n        if hidden_dims is None:\n            hidden_dims = [32, 64]\n\n        modules = []\n        in_channels = latent_dim + traj_lstm_hidden_size + graph_lstm_hidden_size\n        for h_dim in hidden_dims:\n            modules.append(\n                nn.Sequential(\n                    nn.Linear(in_channels, h_dim),\n                    nn.LeakyReLU())\n            )\n            in_channels = h_dim\n\n        self.s_dim = s_dim\n        self.mapping = nn.Sequential(*modules)\n        self.fc_mu = nn.Linear(hidden_dims[-1], s_dim)\n        self.fc_logvar = nn.Linear(hidden_dims[-1], s_dim)\n\n        self._initialize_weights()\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.LSTMCell):\n                m.weight_hh.data.normal_(0, 0.1)\n                m.weight_ih.data.normal_(0, 0.1)\n                m.bias_hh.data.zero_()\n                m.bias_ih.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.1)\n                m.bias.data.zero_()\n\n    def forward(self, hidden_states, theta):\n        if len(theta.size()) == 1:\n            theta_rep = theta.repeat(hidden_states.shape[0], 1)\n        else:\n            theta_rep = theta\n        mu = self.fc_mu(self.mapping(torch.cat((hidden_states, theta_rep), dim=1)))\n        logvar = self.fc_logvar(self.mapping(torch.cat((hidden_states, theta_rep), dim=1)))\n\n        ps = MultivariateNormal(mu, torch.diag_embed(torch.exp(logvar)))\n\n        return ps\n\n\nclass CRMF(nn.Module):\n    def __init__(self, args):\n        super(CRMF, self).__init__()\n\n        self.obs_len = args.obs_len\n        self.z_dim = args.z_dim\n        self.latent_dim = args.latent_dim\n        self.fut_len = args.fut_len\n        self.num_samples = args.num_samples\n        self.n_coordinates = args.n_coordinates\n\n        self.theta = nn.Parameter(torch.rand(args.num_envs, args.latent_dim))\n        self.coupling_layers_z = nn.ModuleList([\n            CouplingLayer(args.z_dim, reverse_mask=False),\n            CouplingLayer(args.z_dim, reverse_mask=True),\n            CouplingLayer(args.z_dim, reverse_mask=False)\n        ])\n        self.coupling_layers_s = nn.ModuleList([\n            CouplingLayer(args.s_dim, reverse_mask=False),\n            CouplingLayer(args.s_dim, reverse_mask=True),\n            CouplingLayer(args.s_dim, reverse_mask=False)\n        ])\n        self.coupling_layers_theta = nn.ModuleList([\n            CouplingLayer(args.latent_dim, reverse_mask=False),\n            CouplingLayer(args.latent_dim, reverse_mask=True),\n            CouplingLayer(args.latent_dim, reverse_mask=False)\n        ])\n        self.pw = MultivariateNormal(torch.zeros(args.z_dim).cuda(), torch.diag(torch.ones(args.z_dim).cuda()))\n\n        self.covee = torch.diag(torch.ones(args.latent_dim).cuda())\n        self.covww = torch.diag(torch.ones(args.latent_dim).cuda())\n        self.covwe = 0.5 * torch.diag(torch.ones(args.latent_dim).cuda())\n        temp1 = torch.cat((self.covww, self.covwe), dim=1)\n        temp2 = torch.cat((self.covwe, self.covee), dim=1)\n        covmat = torch.cat((temp1, temp2), dim=0)\n\n        self.pwe = MultivariateNormal(torch.zeros(args.latent_dim + args.s_dim).cuda(), covmat)\n        self.pe = MultivariateNormal(torch.zeros(args.latent_dim).cuda(), torch.diag(torch.ones(args.latent_dim).cuda()))\n\n        self.invariant_encoder = STGAT_encoder_inv(args.obs_len, args.fut_len, args.n_coordinates,\n                                                   args.traj_lstm_hidden_size, args.n_units, args.n_heads,\n                                                   args.graph_network_out_dims, args.dropout, args.alpha,\n                                                   args.graph_lstm_hidden_size,\n                                                   args.z_dim, None, args.add_confidence)\n\n        self.variant_encoder = STGAT_encoder_var(args.obs_len, args.fut_len, args.n_coordinates,\n                                                 args.traj_lstm_hidden_size, args.n_units, args.n_heads,\n                                                 args.graph_network_out_dims, args.dropout, args.alpha,\n                                                 args.graph_lstm_hidden_size, args.add_confidence)\n\n        self.x_to_s = simple_mapping(args.traj_lstm_hidden_size, args.graph_lstm_hidden_size, args.latent_dim, args.s_dim)\n\n        self.mapping = regressor(args.obs_len, args.n_coordinates, args.latent_dim)\n\n        self.past_decoder = past_decoder(args.obs_len, args.n_coordinates, args.z_dim, args.s_dim)\n\n        self.future_decoder = future_STGAT_decoder(args.obs_len, args.fut_len, args.n_coordinates, args.s_dim,\n                                                   args.z_dim, args.teachingratio)\n\n    def forward(self, batch, training_step, **kwargs):\n\n        obs_traj, fut_traj, obs_traj_rel, fut_traj_rel, seq_start_end, = batch\n\n        if self.training:\n            if training_step in [\"P1\", \"P2\"]:\n                pred_traj_rel_inv = self.invariant_encoder(batch, training_step)\n                pred_traj_rel_var = self.variant_encoder(batch, training_step)\n\n                return pred_traj_rel_inv, pred_traj_rel_var\n\n            elif training_step == 'P3':\n                # calculate q(y|x)\n                first_E = []\n                q_zgx = self.invariant_encoder(batch, training_step)\n                for _ in range(self.num_samples):\n                    z_vec = q_zgx.rsample()\n                    p_ygz = self.future_decoder(batch, z_vec, training_step, False)\n                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])\n                    first_E.append(torch.prod(proby_mat, dim=0))\n\n                q_ygx = torch.mean(torch.stack(first_E), dim=0)\n                first_E = []\n                for _ in range(self.num_samples):\n                    z_vec = q_zgx.rsample()\n                    qprob_z = q_zgx.log_prob(z_vec)\n\n                    # calculate log(p(z))\n                    sldj = torch.zeros(z_vec.shape[0], device=z_vec.device)\n                    z_vec_c = z_vec\n                    for coupling in self.coupling_layers_z:\n                        z_vec_c, sldj = coupling(z_vec_c, sldj)\n\n                    prob_z = self.pw.log_prob(z_vec_c) + sldj\n\n                    # calculate log(p(x|z))\n                    pred_past_rel = self.past_decoder(batch, z_vec, False)\n                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode=\"raw\") - \\\n                                          0.5 * 1 / obs_traj_rel.shape[0] * torch.log(torch.tensor(2 * math.pi * 0.5))\n\n                    # calculate p(y|z,x)\n                    p_ygz = self.future_decoder(batch, z_vec, training_step, False)\n                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])\n                    p_ygz = torch.prod(proby_mat, dim=0)\n\n                    A1 = torch.multiply(p_ygz, reconstruction_loss)\n                    A2 = torch.multiply(p_ygz, prob_z - qprob_z)\n\n                    first_E.append(A1 + A2)\n\n                E = torch.mean(torch.stack(first_E), dim=0)\n\n                return q_ygx, E\n\n            elif training_step == \"P4\":\n                env_idx = kwargs.get(\"env_idx\")\n                concat_hidden_states = self.variant_encoder(batch, training_step)\n\n                first_E = []\n                q_zgx = self.invariant_encoder(batch, training_step)\n                q_sgthetax = self.x_to_s(concat_hidden_states, self.theta[env_idx])\n\n                # calculate q(y|theta, x)\n                for _ in range(self.num_samples):\n                    z_vec = q_zgx.rsample()\n                    s_vec = q_sgthetax.rsample()\n                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)\n                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])\n\n                    first_E.append(torch.prod(proby_mat, dim=0))\n\n                qygthetax = torch.mean(torch.stack(first_E), dim=0)\n\n                first_E = []\n                for _ in range(self.num_samples):\n                    z_vec = q_zgx.rsample()\n                    s_vec = q_sgthetax.rsample()\n\n                    # calculate log(q(z|x))\n                    log_qzgx = q_zgx.log_prob(z_vec)\n\n                    # calculate log(q(s|theta, x))\n                    log_qsgthetax = q_sgthetax.log_prob(s_vec)\n\n                    # calculate log(p(z))\n                    sldj = torch.zeros(z_vec.shape[0], device=z_vec.device)\n                    z_vec_c = z_vec\n                    for coupling in self.coupling_layers_z:\n                        z_vec_c, sldj = coupling(z_vec_c, sldj)\n\n                    log_pz = self.pw.log_prob(z_vec_c) + sldj\n\n                    # calculate log(p(s|theta))\n                    sldj_s = torch.zeros(s_vec.shape[0], device=z_vec.device)\n                    s_vec_c = s_vec\n                    for coupling in self.coupling_layers_s:\n                        s_vec_c, sldj_s = coupling(s_vec_c, sldj_s)\n\n                    sldj_t = torch.zeros(s_vec.shape[0], device=z_vec.device)\n                    t_vec_c = self.theta[env_idx].unsqueeze(0)\n                    for coupling in self.coupling_layers_theta:\n                        t_vec_c, sldj_t = coupling(t_vec_c, sldj_t)\n\n                    mean = torch.matmul(torch.matmul(self.covwe, torch.inverse(self.covee)), t_vec_c.squeeze())\n                    covmat = self.covww - torch.matmul(torch.matmul(self.covwe, torch.inverse(self.covee)), self.covwe)\n                    pwe = MultivariateNormal(mean, covmat)\n                    log_psgtheta = pwe.log_prob(s_vec_c) + sldj_s\n\n                    # calculate p(y|z,s,x)\n                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)\n                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])\n                    p_ygzs = torch.prod(proby_mat, dim=0)\n\n                    # calculate log(p(x|z,s))\n                    pred_past_rel = self.past_decoder(batch, torch.cat((z_vec, s_vec), dim=1), True)\n                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode=\"raw\") - \\\n                                          0.5 * 1 / obs_traj_rel.shape[0] * torch.log(torch.tensor(2 * math.pi * 0.5))\n\n                    A1 = torch.multiply(p_ygzs, reconstruction_loss)\n                    A2 = torch.multiply(p_ygzs, log_pz + log_psgtheta - log_qzgx - log_qsgthetax)\n\n                    first_E.append(A1 + A2)\n\n                E = torch.mean(torch.stack(first_E), dim=0)\n\n                return qygthetax, E\n\n            elif training_step == \"P5\":\n                pred_theta = self.mapping(obs_traj_rel)\n\n                return pred_theta\n\n            else:\n                pred_theta = self.mapping(obs_traj_rel)\n                concat_hidden_states = self.variant_encoder(batch, training_step)\n\n                first_E = []\n                q_zgx = self.invariant_encoder(batch, training_step)\n                q_sgthetax = self.x_to_s(concat_hidden_states, pred_theta)\n\n                # calculate q(y|theta, x)\n                for _ in range(self.num_samples):\n                    z_vec = q_zgx.rsample()\n                    s_vec = q_sgthetax.rsample()\n                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)\n                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])\n\n                    first_E.append(torch.prod(proby_mat, dim=0))\n\n                qygthetax = torch.mean(torch.stack(first_E), dim=0)\n\n                first_E = []\n                for _ in range(self.num_samples):\n                    z_vec = q_zgx.rsample()\n                    s_vec = q_sgthetax.rsample()\n\n                    # calculate log(q(z|x))\n                    log_qzgx = q_zgx.log_prob(z_vec)\n\n                    # calculate log(q(s|theta, x))\n                    log_qsgthetax = q_sgthetax.log_prob(s_vec)\n\n                    # calculate log(p(z))\n                    sldj = torch.zeros(z_vec.shape[0], device=z_vec.device)\n                    z_vec_c = z_vec\n                    for coupling in self.coupling_layers_z:\n                        z_vec_c, sldj = coupling(z_vec_c, sldj)\n\n                    log_pz = self.pw.log_prob(z_vec_c) + sldj\n\n                    # calculate log(p(s|theta))\n                    sldj_s = torch.zeros(s_vec.shape[0], device=z_vec.device)\n                    s_vec_c = s_vec\n                    for coupling in self.coupling_layers_s:\n                        s_vec_c, sldj_s = coupling(s_vec_c, sldj_s)\n\n                    sldj_t = torch.zeros(s_vec.shape[0], device=z_vec.device)\n                    t_vec_c = pred_theta\n                    for coupling in self.coupling_layers_theta:\n                        t_vec_c, sldj_t = coupling(t_vec_c, sldj_t)\n\n                    log_psgtheta = self.pwe.log_prob(torch.cat((s_vec_c, t_vec_c), dim=1)) + sldj_s - self.pe.log_prob(t_vec_c)\n\n                    # calculate p(y|x, s, z)\n                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)\n                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])\n                    p_ygzs = torch.prod(proby_mat, dim=0)\n\n                    # calculate log(p(x|s,z))\n                    pred_past_rel = self.past_decoder(batch, torch.cat((z_vec, s_vec), dim=1), True)\n                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode=\"raw\") - \\\n                                          0.5 * 1 / obs_traj_rel.shape[0] * torch.log(torch.tensor(2 * math.pi * 0.5))\n\n                    A1 = torch.multiply(p_ygzs, reconstruction_loss)\n                    A2 = torch.multiply(p_ygzs, log_pz + log_psgtheta - log_qzgx - log_qsgthetax)\n\n                    first_E.append(A1 + A2)\n\n                E = torch.mean(torch.stack(first_E), dim=0)\n\n                return qygthetax, E\n\n        else:\n            if training_step == \"P3\":\n                q_zgx = self.invariant_encoder(batch, training_step)\n\n                # P(z|x)\n                z_vec = q_zgx.sample()\n\n                # p(y|z,c)\n                pred_traj_rel = self.future_decoder(batch, z_vec, training_step, False)\n\n            elif training_step == \"P4\":\n                env_idx = kwargs.get(\"env_idx\")\n                concat_hidden_states = self.variant_encoder(batch, training_step)\n\n                ps = self.x_to_s(concat_hidden_states, self.theta[env_idx])\n                p_zgx = self.invariant_encoder(batch, training_step)\n\n                # p(s|theta,x)\n                s_vec = ps.sample()\n\n                # P(z|x)\n                z_vec = p_zgx.sample()\n\n                # p(y|z,c)\n                pred_traj_rel = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)\n\n            elif training_step == \"P7\":\n                env_idx = kwargs.get(\"env_idx\")\n                if env_idx is None:\n                    concat_hidden_states = self.variant_encoder(batch, training_step)\n                    pred_theta = self.mapping(concat_hidden_states)\n                    ps = self.x_to_s(concat_hidden_states, pred_theta)\n                    q_zgx = self.invariant_encoder(batch, training_step)\n\n                else:\n                    concat_hidden_states = self.variant_encoder(batch, training_step)\n                    ps = self.x_to_s(concat_hidden_states, self.theta[env_idx])\n                    q_zgx = self.invariant_encoder(batch, training_step)\n\n                return q_zgx, ps\n\n            else:\n                concat_hidden_states = self.variant_encoder(batch, training_step)\n                pred_theta = self.mapping(concat_hidden_states)\n                ps = self.x_to_s(concat_hidden_states, pred_theta)\n                p_zgx = self.invariant_encoder(batch, training_step)\n\n                # p(s|theta,x)\n                s_vec = ps.sample()\n\n                # P(z|x)\n                z_vec = p_zgx.sample()\n\n                # p(y|z,c)\n                pred_traj_rel = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)\n\n            return pred_traj_rel\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/models.py b/models.py
--- a/models.py	
+++ b/models.py	
@@ -15,6 +15,38 @@
     raise ValueError('Unrecognized noise type "%s"' % noise_type)
 
 
+class ActNorm(torch.jit.ScriptModule):
+    def __init__(self, num_features: int):
+        super().__init__()
+        self.scale = torch.nn.Parameter(torch.zeros(num_features))
+        self.bias = torch.nn.Parameter(torch.zeros(num_features))
+        self.register_buffer("_initialized", torch.tensor(False))
+
+    def reset_(self):
+        self._initialized = torch.tensor(False)
+        return self
+
+    def _check_input_dim(self, x: torch.Tensor) -> None:
+        raise NotImplementedError()  # pragma: no cover
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        self._check_input_dim(x)
+        if x.dim() > 2:
+            x = x.transpose(1, -1)
+        if not self._initialized:
+            self.scale.data = 1 / x.detach().reshape(-1, x.shape[-1]).std(
+                0, unbiased=False
+            )
+            self.bias.data = -self.scale * x.detach().reshape(
+                -1, x.shape[-1]
+            ).mean(0)
+            self._initialized = torch.tensor(True)
+        x = self.scale * x + self.bias
+        if x.dim() > 2:
+            x = x.transpose(1, -1)
+        return x
+
+
 class CouplingLayer(nn.Module):
     """Coupling layer in RealNVP.
     Args:
@@ -24,6 +56,7 @@
         mask_type (MaskType): One of `MaskType.CHECKERBOARD` or `MaskType.CHANNEL_WISE`.
         reverse_mask (bool): Whether to reverse the mask. Useful for alternating masks.
     """
+
     def __init__(self, latent_dim, reverse_mask, hidden_dims=None):
         super(CouplingLayer, self).__init__()
         # Save mask info
@@ -31,7 +64,7 @@
 
         # Build scale and translate network
         if hidden_dims is None:
-            hidden_dims = [16, 32]
+            hidden_dims = [8, 16]
 
         modules = []
         in_channels = latent_dim // 2
@@ -82,12 +115,14 @@
 
         return x, sldj
 
+
 class Rescale(nn.Module):
     """Per-channel rescaling. Need a proper `nn.Module` so we can wrap it
     with `torch.nn.utils.weight_norm`.
     Args:
         num_channels (int): Number of channels in the input.
     """
+
     def __init__(self, num_channels):
         super(Rescale, self).__init__()
         self.weight = nn.Parameter(torch.ones(num_channels))
@@ -308,42 +343,36 @@
             traj_lstm_h_t, traj_lstm_c_t = self.traj_lstm_model(
                 obs_traj_rel[i], (traj_lstm_h_t, traj_lstm_c_t)
             )
-            if training_step == "P1":
-                output = self.traj_hidden2pos(traj_lstm_h_t)
-                pred_traj_rel += [output]
-            else:
-                traj_lstm_hidden_states += [traj_lstm_h_t]
+
+            traj_lstm_hidden_states += [traj_lstm_h_t]
 
         # graph_lstm (used in step 2,3)
-        if training_step != "P1":
-            graph_lstm_input = self.gatencoder(
-                torch.stack(traj_lstm_hidden_states), seq_start_end
-            )
-            for i in range(self.obs_len):
-                graph_lstm_h_t, graph_lstm_c_t = self.graph_lstm_model(
-                    graph_lstm_input[i], (graph_lstm_h_t, graph_lstm_c_t)
-                )
-                if training_step == "P2":
-                    encoded_before_noise_hidden = torch.cat(
-                        (traj_lstm_hidden_states[i], graph_lstm_h_t), dim=1
-                    )
-                    output = self.traj_gat_hidden2pos(encoded_before_noise_hidden)
-                    pred_traj_rel += [output]
-                else:
-                    graph_lstm_hidden_states += [graph_lstm_h_t]
+        graph_lstm_input = self.gatencoder(
+            torch.stack(traj_lstm_hidden_states), seq_start_end
+        )
+        for i in range(self.obs_len):
+            graph_lstm_h_t, graph_lstm_c_t = self.graph_lstm_model(
+                graph_lstm_input[i], (graph_lstm_h_t, graph_lstm_c_t)
+            )
+            graph_lstm_hidden_states += [graph_lstm_h_t]
 
-        if training_step in ["P1", "P2"]:
-            return torch.stack(pred_traj_rel)
-
-        else:
-
-            encoded_before_noise_hidden = torch.cat((traj_lstm_hidden_states[-1], graph_lstm_hidden_states[-1]), dim=1)
-            mu = self.fc_mu(self.mapping(encoded_before_noise_hidden))
-            logvar = self.fc_var(self.mapping(encoded_before_noise_hidden))
+        encoded_before_noise_hidden = torch.cat((traj_lstm_hidden_states[-1], graph_lstm_hidden_states[-1]), dim=1)
+        mu = self.fc_mu(self.mapping(encoded_before_noise_hidden))
+        logvar = self.fc_var(self.mapping(encoded_before_noise_hidden))
 
-            z = MultivariateNormal(mu, torch.diag_embed(torch.exp(logvar)))
+        return mu, logvar
 
-            return z
+    def reparameterize(self, mu, logvar):
+        """
+        Will a single z be enough ti compute the expectation
+        for the loss??
+        :param mu: (Tensor) Mean of the latent Gaussian
+        :param logvar: (Tensor) Standard deviation of the latent Gaussian
+        :return:
+        """
+        std = torch.exp(0.5 * logvar)
+        eps = torch.randn_like(std)
+        return eps * std + mu
 
 
 class STGAT_encoder_var(nn.Module):
@@ -435,35 +464,21 @@
             traj_lstm_h_t, traj_lstm_c_t = self.traj_lstm_model(
                 obs_traj_rel[i], (traj_lstm_h_t, traj_lstm_c_t)
             )
-            if training_step == "P1":
-                output = self.traj_hidden2pos(traj_lstm_h_t)
-                pred_traj_rel += [output]
-            else:
-                traj_lstm_hidden_states += [traj_lstm_h_t]
+
+            traj_lstm_hidden_states += [traj_lstm_h_t]
 
         # graph_lstm (used in step 2,3)
-        if training_step != "P1":
-            graph_lstm_input = self.gatencoder(
-                torch.stack(traj_lstm_hidden_states), seq_start_end
-            )
-            for i in range(self.obs_len):
-                graph_lstm_h_t, graph_lstm_c_t = self.graph_lstm_model(
-                    graph_lstm_input[i], (graph_lstm_h_t, graph_lstm_c_t)
-                )
-                if training_step == "P2":
-                    encoded_before_noise_hidden = torch.cat(
-                        (traj_lstm_hidden_states[i], graph_lstm_h_t), dim=1
-                    )
-                    output = self.traj_gat_hidden2pos(encoded_before_noise_hidden)
-                    pred_traj_rel += [output]
-                else:
-                    graph_lstm_hidden_states += [graph_lstm_h_t]
+        graph_lstm_input = self.gatencoder(
+            torch.stack(traj_lstm_hidden_states), seq_start_end
+        )
+        for i in range(self.obs_len):
+            graph_lstm_h_t, graph_lstm_c_t = self.graph_lstm_model(
+                graph_lstm_input[i], (graph_lstm_h_t, graph_lstm_c_t)
+            )
+
+            graph_lstm_hidden_states += [graph_lstm_h_t]
 
-        if training_step in ["P1", "P2"]:
-            return torch.stack(pred_traj_rel)
-
-        else:
-            return torch.cat((graph_lstm_hidden_states[-1], traj_lstm_hidden_states[-1]), dim=1)
+        return torch.cat((graph_lstm_hidden_states[-1], traj_lstm_hidden_states[-1]), dim=1)
 
 
 class future_STGAT_decoder(nn.Module):
@@ -485,14 +500,9 @@
         self.teacher_forcing_ratio = teacher_forcing_ratio
 
         self.n_coordinates = n_coordinates
-        self.pred_lstm_hidden_size1 = z_dim + s_dim
-        self.pred_lstm_hidden_size2 = z_dim
-        self.pred_hidden2pos = nn.ModuleList([nn.Linear(self.pred_lstm_hidden_size1, n_coordinates),
-                                              nn.Linear(self.pred_lstm_hidden_size2, n_coordinates)])
-        self.pred_lstm_model = nn.ModuleList([nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size1),
-                                              nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size2)])
-
-        self.scale = torch.tensor(1.).cuda().requires_grad_()
+        self.pred_lstm_hidden_size = z_dim + s_dim
+        self.pred_hidden2pos = nn.Linear(self.pred_lstm_hidden_size, n_coordinates)
+        self.pred_lstm_model = nn.LSTMCell(n_coordinates, self.pred_lstm_hidden_size)
 
         self._initialize_weights()
 
@@ -511,8 +521,6 @@
             self,
             batch,
             pred_lstm_hidden,
-            training_step,
-            variant_feats,
     ):
 
         (_, _, obs_traj_rel, fut_traj_rel, seq_start_end) = batch
@@ -521,7 +529,6 @@
         input_t = obs_traj_rel[self.obs_len - 1, :, :self.n_coordinates]
         output = input_t
         pred_lstm_c_t = torch.zeros_like(pred_lstm_hidden).cuda()
-        p = []
         # during training
         if self.training:
             for i in range(self.fut_len):
@@ -532,29 +539,18 @@
                     else:
                         input_t = output
 
-                if variant_feats:
-                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[0](input_t,
-                                                                              (pred_lstm_hidden, pred_lstm_c_t))
-                    output = self.pred_hidden2pos[0](pred_lstm_hidden)
-                else:
-                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[1](input_t,
-                                                                              (pred_lstm_hidden, pred_lstm_c_t))
-                    output = self.pred_hidden2pos[1](pred_lstm_hidden)
+                pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model(input_t, (pred_lstm_hidden, pred_lstm_c_t))
+                output = self.pred_hidden2pos(pred_lstm_hidden)
 
                 pred_traj_rel += [output]
-                p += [MultivariateNormal(output, torch.diag_embed(self.var_p * torch.ones(fut_traj_rel.size(1), 2).cuda()))]
 
-            return p
+            return torch.stack(pred_traj_rel)
 
         # during test
         else:
             for i in range(self.fut_len):
-                if training_step == "P3":
-                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[1](output, (pred_lstm_hidden, pred_lstm_c_t))
-                    output = self.pred_hidden2pos[1](pred_lstm_hidden)
-                else:
-                    pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model[0](output, (pred_lstm_hidden, pred_lstm_c_t))
-                    output = self.pred_hidden2pos[0](pred_lstm_hidden)
+                pred_lstm_hidden, pred_lstm_c_t = self.pred_lstm_model(output, (pred_lstm_hidden, pred_lstm_c_t))
+                output = self.pred_hidden2pos(pred_lstm_hidden)
                 pred_traj_rel += [output]
 
             return torch.stack(pred_traj_rel)
@@ -712,12 +708,23 @@
             theta_rep = theta.repeat(hidden_states.shape[0], 1)
         else:
             theta_rep = theta
+
         mu = self.fc_mu(self.mapping(torch.cat((hidden_states, theta_rep), dim=1)))
         logvar = self.fc_logvar(self.mapping(torch.cat((hidden_states, theta_rep), dim=1)))
 
-        ps = MultivariateNormal(mu, torch.diag_embed(torch.exp(logvar)))
+        return mu, logvar
 
-        return ps
+    def reparameterize(self, mu, logvar):
+        """
+        Will a single z be enough ti compute the expectation
+        for the loss??
+        :param mu: (Tensor) Mean of the latent Gaussian
+        :param logvar: (Tensor) Standard deviation of the latent Gaussian
+        :return:
+        """
+        std = torch.exp(0.5 * logvar)
+        eps = torch.randn_like(std)
+        return eps * std + mu
 
 
 class CRMF(nn.Module):
@@ -752,12 +759,6 @@
         self.covee = torch.diag(torch.ones(args.latent_dim).cuda())
         self.covww = torch.diag(torch.ones(args.latent_dim).cuda())
         self.covwe = 0.5 * torch.diag(torch.ones(args.latent_dim).cuda())
-        temp1 = torch.cat((self.covww, self.covwe), dim=1)
-        temp2 = torch.cat((self.covwe, self.covee), dim=1)
-        covmat = torch.cat((temp1, temp2), dim=0)
-
-        self.pwe = MultivariateNormal(torch.zeros(args.latent_dim + args.s_dim).cuda(), covmat)
-        self.pe = MultivariateNormal(torch.zeros(args.latent_dim).cuda(), torch.diag(torch.ones(args.latent_dim).cuda()))
 
         self.invariant_encoder = STGAT_encoder_inv(args.obs_len, args.fut_len, args.n_coordinates,
                                                    args.traj_lstm_hidden_size, args.n_units, args.n_heads,
@@ -770,8 +771,8 @@
                                                  args.graph_network_out_dims, args.dropout, args.alpha,
                                                  args.graph_lstm_hidden_size, args.add_confidence)
 
-        self.x_to_s = simple_mapping(args.traj_lstm_hidden_size, args.graph_lstm_hidden_size, args.latent_dim, args.s_dim)
-
+        self.x_to_s = simple_mapping(args.traj_lstm_hidden_size, args.graph_lstm_hidden_size, args.latent_dim,
+                                     args.s_dim)
         self.mapping = regressor(args.obs_len, args.n_coordinates, args.latent_dim)
 
         self.past_decoder = past_decoder(args.obs_len, args.n_coordinates, args.z_dim, args.s_dim)
@@ -784,27 +785,19 @@
         obs_traj, fut_traj, obs_traj_rel, fut_traj_rel, seq_start_end, = batch
 
         if self.training:
-            if training_step in ["P1", "P2"]:
-                pred_traj_rel_inv = self.invariant_encoder(batch, training_step)
-                pred_traj_rel_var = self.variant_encoder(batch, training_step)
+            if training_step == 'P1':
+                mu, logvar = self.invariant_encoder(batch, training_step)
 
-                return pred_traj_rel_inv, pred_traj_rel_var
-
-            elif training_step == 'P3':
-                # calculate q(y|x)
                 first_E = []
-                q_zgx = self.invariant_encoder(batch, training_step)
+                recon_loss = []
                 for _ in range(self.num_samples):
-                    z_vec = q_zgx.rsample()
-                    p_ygz = self.future_decoder(batch, z_vec, training_step, False)
-                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])
-                    first_E.append(torch.prod(proby_mat, dim=0))
+                    # calculate log(p(x|z))
+                    z_vec = self.invariant_encoder.reparameterize(mu, logvar)
+                    pred_past_rel = self.past_decoder(batch, z_vec, False)
+                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode="average")
 
-                q_ygx = torch.mean(torch.stack(first_E), dim=0)
-                first_E = []
-                for _ in range(self.num_samples):
-                    z_vec = q_zgx.rsample()
-                    qprob_z = q_zgx.log_prob(z_vec)
+                    lqz = - 0.5 * torch.sum((z_vec - mu) ** 2 / torch.exp(logvar), dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) - torch.log(torch.prod(torch.exp(logvar), dim=1))
 
                     # calculate log(p(z))
                     sldj = torch.zeros(z_vec.shape[0], device=z_vec.device)
@@ -812,56 +805,31 @@
                     for coupling in self.coupling_layers_z:
                         z_vec_c, sldj = coupling(z_vec_c, sldj)
 
-                    prob_z = self.pw.log_prob(z_vec_c) + sldj
-
-                    # calculate log(p(x|z))
-                    pred_past_rel = self.past_decoder(batch, z_vec, False)
-                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode="raw") - \
-                                          0.5 * 1 / obs_traj_rel.shape[0] * torch.log(torch.tensor(2 * math.pi * 0.5))
+                    lpz = - 0.5 * torch.sum(z_vec_c ** 2, dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) + sldj
 
-                    # calculate p(y|z,x)
-                    p_ygz = self.future_decoder(batch, z_vec, training_step, False)
-                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])
-                    p_ygz = torch.prod(proby_mat, dim=0)
-
-                    A1 = torch.multiply(p_ygz, reconstruction_loss)
-                    A2 = torch.multiply(p_ygz, prob_z - qprob_z)
-
-                    first_E.append(A1 + A2)
+                    first_E.append(lpz - lqz)
+                    recon_loss.append(reconstruction_loss)
 
                 E = torch.mean(torch.stack(first_E), dim=0)
+                recon_loss = torch.mean(torch.stack(recon_loss), dim=0)
+                E = torch.mean(E)
 
-                return q_ygx, E
+                return recon_loss, E
 
-            elif training_step == "P4":
+            elif training_step == "P2":
                 env_idx = kwargs.get("env_idx")
                 concat_hidden_states = self.variant_encoder(batch, training_step)
+                mus, logvars = self.x_to_s(concat_hidden_states, self.theta[env_idx])
+                muz, logvarz = self.invariant_encoder(batch, training_step)
 
                 first_E = []
-                q_zgx = self.invariant_encoder(batch, training_step)
-                q_sgthetax = self.x_to_s(concat_hidden_states, self.theta[env_idx])
-
-                # calculate q(y|theta, x)
+                recon_loss = []
                 for _ in range(self.num_samples):
-                    z_vec = q_zgx.rsample()
-                    s_vec = q_sgthetax.rsample()
-                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)
-                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])
-
-                    first_E.append(torch.prod(proby_mat, dim=0))
-
-                qygthetax = torch.mean(torch.stack(first_E), dim=0)
-
-                first_E = []
-                for _ in range(self.num_samples):
-                    z_vec = q_zgx.rsample()
-                    s_vec = q_sgthetax.rsample()
+                    z_vec = self.invariant_encoder.reparameterize(muz, logvarz)
 
-                    # calculate log(q(z|x))
-                    log_qzgx = q_zgx.log_prob(z_vec)
-
-                    # calculate log(q(s|theta, x))
-                    log_qsgthetax = q_sgthetax.log_prob(s_vec)
+                    lqz = - 0.5 * torch.sum((z_vec - muz) ** 2 / torch.exp(logvarz), dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) - torch.log(torch.prod(torch.exp(logvarz), dim=1))
 
                     # calculate log(p(z))
                     sldj = torch.zeros(z_vec.shape[0], device=z_vec.device)
@@ -869,15 +837,22 @@
                     for coupling in self.coupling_layers_z:
                         z_vec_c, sldj = coupling(z_vec_c, sldj)
 
-                    log_pz = self.pw.log_prob(z_vec_c) + sldj
+                    lpz = - 0.5 * torch.sum(z_vec_c ** 2, dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) + sldj
+
+                    s_vec = self.x_to_s.reparameterize(mus, logvars)
+
+                    # calculate log(q(s|theta, x))
+                    lqs = - 0.5 * torch.sum((s_vec - mus) ** 2 / torch.exp(logvars), dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) - torch.log(torch.prod(torch.exp(logvars), dim=1))
 
                     # calculate log(p(s|theta))
-                    sldj_s = torch.zeros(s_vec.shape[0], device=z_vec.device)
+                    sldj_s = torch.zeros(s_vec.shape[0], device=s_vec.device)
                     s_vec_c = s_vec
                     for coupling in self.coupling_layers_s:
                         s_vec_c, sldj_s = coupling(s_vec_c, sldj_s)
 
-                    sldj_t = torch.zeros(s_vec.shape[0], device=z_vec.device)
+                    sldj_t = torch.zeros(s_vec.shape[0], device=s_vec.device)
                     t_vec_c = self.theta[env_idx].unsqueeze(0)
                     for coupling in self.coupling_layers_theta:
                         t_vec_c, sldj_t = coupling(t_vec_c, sldj_t)
@@ -885,61 +860,63 @@
                     mean = torch.matmul(torch.matmul(self.covwe, torch.inverse(self.covee)), t_vec_c.squeeze())
                     covmat = self.covww - torch.matmul(torch.matmul(self.covwe, torch.inverse(self.covee)), self.covwe)
                     pwe = MultivariateNormal(mean, covmat)
-                    log_psgtheta = pwe.log_prob(s_vec_c) + sldj_s
-
-                    # calculate p(y|z,s,x)
-                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)
-                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])
-                    p_ygzs = torch.prod(proby_mat, dim=0)
+                    lps = pwe.log_prob(s_vec_c) + sldj_s
 
                     # calculate log(p(x|z,s))
                     pred_past_rel = self.past_decoder(batch, torch.cat((z_vec, s_vec), dim=1), True)
-                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode="raw") - \
-                                          0.5 * 1 / obs_traj_rel.shape[0] * torch.log(torch.tensor(2 * math.pi * 0.5))
+                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode="average")
 
-                    A1 = torch.multiply(p_ygzs, reconstruction_loss)
-                    A2 = torch.multiply(p_ygzs, log_pz + log_psgtheta - log_qzgx - log_qsgthetax)
-
-                    first_E.append(A1 + A2)
+                    recon_loss.append(reconstruction_loss)
+                    first_E.append(lpz + lps - lqz - lqs)
 
                 E = torch.mean(torch.stack(first_E), dim=0)
+                recon_loss = torch.mean(torch.stack(recon_loss), dim=0)
+                E = torch.mean(E)
+
+                return recon_loss, E
+
+            elif training_step == "P3":
+                env_idx = kwargs.get("env_idx")
+                concat_hidden_states = self.variant_encoder(batch, training_step)
+                mus, logvars = self.x_to_s(concat_hidden_states, self.theta[env_idx])
+                muz, logvarz = self.invariant_encoder(batch, training_step)
+
+                pred_loss = []
+                for _ in range(self.num_samples):
+                    z_vec = self.invariant_encoder.reparameterize(muz, logvarz)
+                    s_vec = self.x_to_s.reparameterize(mus, logvars)
+
+                    # calculate log(p(x|z,s))
+                    pred_fut_rel = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1))
+                    prediction_loss = - l2_loss(pred_fut_rel, fut_traj_rel, mode="average")
 
-                return qygthetax, E
+                    pred_loss.append(prediction_loss)
 
-            elif training_step == "P5":
+                pred_loss = torch.mean(torch.stack(pred_loss), dim=0)
+
+                return pred_loss
+
+            elif training_step == "P4":
+                env_idx = kwargs.get("env_idx")
                 pred_theta = self.mapping(obs_traj_rel)
+                l2_loss_pred = l2_loss(pred_theta, self.theta[env_idx], mode="sum") / pred_theta.shape[0]
 
-                return pred_theta
+                return l2_loss_pred
 
             else:
                 pred_theta = self.mapping(obs_traj_rel)
                 concat_hidden_states = self.variant_encoder(batch, training_step)
+                mus, logvars = self.x_to_s(concat_hidden_states, pred_theta)
+                muz, logvarz = self.invariant_encoder(batch, training_step)
 
                 first_E = []
-                q_zgx = self.invariant_encoder(batch, training_step)
-                q_sgthetax = self.x_to_s(concat_hidden_states, pred_theta)
-
-                # calculate q(y|theta, x)
+                recon_loss = []
                 for _ in range(self.num_samples):
-                    z_vec = q_zgx.rsample()
-                    s_vec = q_sgthetax.rsample()
-                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)
-                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])
-
-                    first_E.append(torch.prod(proby_mat, dim=0))
-
-                qygthetax = torch.mean(torch.stack(first_E), dim=0)
-
-                first_E = []
-                for _ in range(self.num_samples):
-                    z_vec = q_zgx.rsample()
-                    s_vec = q_sgthetax.rsample()
+                    z_vec = self.invariant_encoder.reparameterize(muz, logvarz)
 
-                    # calculate log(q(z|x))
-                    log_qzgx = q_zgx.log_prob(z_vec)
-
-                    # calculate log(q(s|theta, x))
-                    log_qsgthetax = q_sgthetax.log_prob(s_vec)
+                    lqz = - 0.5 * torch.sum((z_vec - muz) ** 2 / torch.exp(logvarz),
+                                            dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) - torch.log(torch.prod(torch.exp(logvarz), dim=1))
 
                     # calculate log(p(z))
                     sldj = torch.zeros(z_vec.shape[0], device=z_vec.device)
@@ -947,71 +924,51 @@
                     for coupling in self.coupling_layers_z:
                         z_vec_c, sldj = coupling(z_vec_c, sldj)
 
-                    log_pz = self.pw.log_prob(z_vec_c) + sldj
+                    lpz = - 0.5 * torch.sum(z_vec_c ** 2, dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) + sldj
+
+                    s_vec = self.x_to_s.reparameterize(mus, logvars)
+
+                    # calculate log(q(s|theta, x))
+                    lqs = - 0.5 * torch.sum((s_vec - mus) ** 2 / torch.exp(logvars),
+                                            dim=1) - self.z_dim / 2 * torch.log(
+                        torch.tensor(2 * math.pi)) - torch.log(torch.prod(torch.exp(logvars), dim=1))
 
                     # calculate log(p(s|theta))
-                    sldj_s = torch.zeros(s_vec.shape[0], device=z_vec.device)
+                    sldj_s = torch.zeros(s_vec.shape[0], device=s_vec.device)
                     s_vec_c = s_vec
                     for coupling in self.coupling_layers_s:
                         s_vec_c, sldj_s = coupling(s_vec_c, sldj_s)
 
-                    sldj_t = torch.zeros(s_vec.shape[0], device=z_vec.device)
+                    sldj_t = torch.zeros(s_vec.shape[0], device=s_vec.device)
                     t_vec_c = pred_theta
                     for coupling in self.coupling_layers_theta:
                         t_vec_c, sldj_t = coupling(t_vec_c, sldj_t)
 
-                    log_psgtheta = self.pwe.log_prob(torch.cat((s_vec_c, t_vec_c), dim=1)) + sldj_s - self.pe.log_prob(t_vec_c)
+                    mean = torch.matmul(torch.matmul(self.covwe, torch.inverse(self.covee)), torch.mean(t_vec_c, dim=0)) # TODO replace mean of t_vec_c
+                    covmat = self.covww - torch.matmul(torch.matmul(self.covwe, torch.inverse(self.covee)), self.covwe)
+                    pwe = MultivariateNormal(mean, covmat)
+                    lps = pwe.log_prob(s_vec_c) + sldj_s
 
-                    # calculate p(y|x, s, z)
-                    p_ygz = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)
-                    proby_mat = torch.stack([torch.exp(p_ygz[i].log_prob(fut_traj_rel[i])) for i in range(fut_traj.shape[0])])
-                    p_ygzs = torch.prod(proby_mat, dim=0)
-
-                    # calculate log(p(x|s,z))
+                    # calculate log(p(x|z,s))
                     pred_past_rel = self.past_decoder(batch, torch.cat((z_vec, s_vec), dim=1), True)
-                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode="raw") - \
-                                          0.5 * 1 / obs_traj_rel.shape[0] * torch.log(torch.tensor(2 * math.pi * 0.5))
+                    reconstruction_loss = - l2_loss(pred_past_rel, obs_traj_rel, mode="average")
 
-                    A1 = torch.multiply(p_ygzs, reconstruction_loss)
-                    A2 = torch.multiply(p_ygzs, log_pz + log_psgtheta - log_qzgx - log_qsgthetax)
-
-                    first_E.append(A1 + A2)
+                    recon_loss.append(reconstruction_loss)
+                    first_E.append(lpz + lps - lqz - lqs)
 
                 E = torch.mean(torch.stack(first_E), dim=0)
+                recon_loss = torch.mean(torch.stack(recon_loss), dim=0)
+                E = torch.mean(E)
 
-                return qygthetax, E
+                return recon_loss, E
 
         else:
-            if training_step == "P3":
-                q_zgx = self.invariant_encoder(batch, training_step)
-
-                # P(z|x)
-                z_vec = q_zgx.sample()
-
-                # p(y|z,c)
-                pred_traj_rel = self.future_decoder(batch, z_vec, training_step, False)
-
-            elif training_step == "P4":
-                env_idx = kwargs.get("env_idx")
-                concat_hidden_states = self.variant_encoder(batch, training_step)
-
-                ps = self.x_to_s(concat_hidden_states, self.theta[env_idx])
-                p_zgx = self.invariant_encoder(batch, training_step)
-
-                # p(s|theta,x)
-                s_vec = ps.sample()
-
-                # P(z|x)
-                z_vec = p_zgx.sample()
-
-                # p(y|z,c)
-                pred_traj_rel = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)
-
-            elif training_step == "P7":
+            if training_step == "P8":
                 env_idx = kwargs.get("env_idx")
                 if env_idx is None:
                     concat_hidden_states = self.variant_encoder(batch, training_step)
-                    pred_theta = self.mapping(concat_hidden_states)
+                    pred_theta = self.mapping(obs_traj_rel)
                     ps = self.x_to_s(concat_hidden_states, pred_theta)
                     q_zgx = self.invariant_encoder(batch, training_step)
 
@@ -1023,18 +980,22 @@
                 return q_zgx, ps
 
             else:
+                env_idx = kwargs.get("env_idx")
                 concat_hidden_states = self.variant_encoder(batch, training_step)
-                pred_theta = self.mapping(concat_hidden_states)
-                ps = self.x_to_s(concat_hidden_states, pred_theta)
-                p_zgx = self.invariant_encoder(batch, training_step)
+                if env_idx is None:
+                    pred_theta = self.mapping(obs_traj_rel)
+                else:
+                    pred_theta = self.theta[env_idx]
+                mus, logvars = self.x_to_s(concat_hidden_states, pred_theta)
+                muz, logvarz = self.invariant_encoder(batch, training_step)
 
                 # p(s|theta,x)
-                s_vec = ps.sample()
+                s_vec = self.x_to_s.reparameterize(mus, logvars)
 
                 # P(z|x)
-                z_vec = p_zgx.sample()
+                z_vec = self.invariant_encoder.reparameterize(muz, logvarz)
 
                 # p(y|z,c)
-                pred_traj_rel = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1), training_step, True)
+                pred_traj_rel = self.future_decoder(batch, torch.cat((z_vec, s_vec), dim=1))
 
             return pred_traj_rel
Index: parser_file.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import argparse\nfrom utils import int_tuple\n\n\ndef get_evaluation_parser():\n    parser = get_training_parser()\n    parser.add_argument(\"--dset_type\", default=\"test\", type=str)\n    parser.add_argument(\"--noise_mix_type\", default=\"global\")\n    parser.add_argument('--metrics', type=str, default='accuracy', choices=['accuracy', 'collision', 'qualitative'],\n                        help='evaluate metrics')\n    return parser\n\n\ndef get_training_parser():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--log_dir\", default=\"./log/\", help=\"Directory containing logging file\")\n    parser.add_argument(\"--model_dir\", default=\"./models/E1/\", help=\"Directory containing logging file\")\n    parser.add_argument(\"--tfdir\", default='./runs/E1/', type=str)\n\n    # dataset\n    parser.add_argument(\"--dataset_name\", default=\"eth\", type=str)\n    parser.add_argument(\"--num_envs\", default=4, type=int, help=\"Number of environments in the dataset\")\n    parser.add_argument(\"--finetune_ratio\", default=0.2, type=float, help=\"Number of batches to be used in finetuning\")\n    parser.add_argument(\"--delim\", default=\"\\t\")\n    parser.add_argument(\"--obs_len\", default=8, type=int)\n    parser.add_argument(\"--fut_len\", default=12, type=int)\n    parser.add_argument(\"--skip\", default=1, type=int)\n    parser.add_argument(\"--n_coordinates\", type=int, default=2, help=\"Number of coordinates\")\n    parser.add_argument(\"--filter_envs\", type=str, default=\"\",\n                        help=\"Filter only certain environments (i.e 0.1-0.3-0.5)\")\n    parser.add_argument(\"--filter_envs_pretrain\", type=str, default=\"\",\n                        help=\"Say which env were used during pretraining (for contrastive loss) (i.e 0.1-0.3-0.5)\")\n    parser.add_argument('--reduce', default=0, type=int)\n    parser.add_argument('--reduceall', default=0, type=int)\n    parser.add_argument('--testonly', default=0, type=int,\n                        help='Only test model. 0 -> training, 1 -> testing, 3 -> testing with refinement')  # 0 is normal train, 1 is test, 2 is test with k, 3 is ttr\n    # randomness\n    parser.add_argument(\"--seed\", type=int, default=72, help=\"Random seed\")\n    parser.add_argument(\"--num_samples\", type=int, default=10, help=\"Number of samples to calculate MC expectations\")\n\n    # architecture (STGAT)\n    parser.add_argument(\"--traj_lstm_hidden_size\", default=32, type=int)\n    parser.add_argument(\"--heads\", type=str, default=\"4,1\", help=\"Heads in each layer, splitted with comma\")\n    parser.add_argument(\"--hidden-units\", type=str, default=\"16\",\n                        help=\"Hidden units in each hidden layer, splitted with comma\")\n    parser.add_argument(\"--graph_network_out_dims\", type=int, default=32,\n                        help=\"dims of every node after through GAT module\")\n    parser.add_argument(\"--graph_lstm_hidden_size\", default=32, type=int)\n    parser.add_argument(\"--dropout\", type=float, default=0, help=\"Dropout rate (1 - keep probability)\")\n    parser.add_argument(\"--alpha\", type=float, default=0.2, help=\"Alpha for the leaky_relu\")\n    parser.add_argument('--teachingratio', default=0.0, type=float,\n                        help=\"The probability of using ground truth future trajectories instead of model predictions during training\")\n    # architecture (VE)\n    parser.add_argument('--latent_dim', type=int, default=2, help=\"Dimension of latent selection variables\")\n    parser.add_argument(\"--z_dim\", type=int, default=64, help=\"Dimension of z latent variable\")\n    parser.add_argument(\"--s_dim\", type=int, default=2, help=\"Dimension of s latent variable\")\n\n    # computation\n    parser.add_argument(\"--loader_num_workers\", default=6, type=int)\n    parser.add_argument(\"--gpu_num\", default=\"1\", type=str)\n\n    # training\n    parser.add_argument(\"--best_k\", default=20, type=int)\n    parser.add_argument(\"--batch_size\", default='16', type=str)\n    parser.add_argument(\"--batch_method\", default='hom', type=str,\n                        help='Use Homogeneous (hom), Heterogeneous (het) or alternated homogeneous (alt) batches during training')\n    parser.add_argument(\"--shuffle\", default=True, type=bool)\n    # spurious feature\n    parser.add_argument(\"--add_confidence\", default=False, type=bool)\n    parser.add_argument(\"--domain_shifts\", default='0', type=str,\n                        help='domain_shifts per environment: hotel,univ,zara1,zara2,eth')\n\n    parser.add_argument(\"--start-epoch\", default=1, type=int, metavar=\"N\",\n                        help=\"manual epoch number (useful on restarts)\")\n    parser.add_argument(\"--use_gpu\", default=1, type=int)\n\n    # general training\n    parser.add_argument(\"--finetune\", default=\"\", type=str)\n    parser.add_argument(\"--num_epochs\", default='150-100-50-100-100-50', type=lambda x: int_tuple(x, '-'))  # '150-100-150',\n    parser.add_argument(\"--resume\", default=\"\",\n                        type=str, metavar=\"PATH\", help=\"path to latest checkpoint (default: none)\")\n\n    # learning rates\n    parser.add_argument(\"--lrvar\", default=1e-3, type=float,\n                        help=\"initial learning rate for variant encoder optimizer\")\n    parser.add_argument(\"--lrvariation\", default=1e-3, type=float,\n                        help=\"initial learning rate variational models optimizer\")\n    parser.add_argument('--lrinv', default=1e-3, type=float,\n                        help=\"initial learning rate for the invariant encoder optimizer\")\n    parser.add_argument('--lrfut', default=1e-3, type=float,\n                        help=\"initial learning rate for the future decoder optimizer\")\n    parser.add_argument('--lrpast', default=1e-3, type=float,\n                        help=\"initial learning rate for the past decoder optimizer\")\n    parser.add_argument('--lrmap', default=1e-3, type=float,\n                        help=\"initial learning rate for the regressor optimizer\")\n    parser.add_argument('--lrpar', default=1e-3, type=float,\n                        help=\"initial learning rate for the parameters optimizer\")\n\n    # other parameters to test after\n    parser.add_argument('--addloss', default=0, type=float)\n\n    parser.add_argument('--ttr', default=0, type=int, help=\"Number of steps of refinement during test time\")\n    parser.add_argument('--ttrlr', default=0, type=float, help=\"initial learning rate for the refinement optimizer\")\n\n    # method\n    parser.add_argument(\"--irm\", default=5.0, type=float, help='IRM parameter (lambda)')\n    parser.add_argument(\"--vrex\", default=0.0, type=float, help='v-REx parameter (beta)')\n\n    parser.add_argument(\"--unbiased\", default=False, type=bool, help='')\n\n    return parser\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/parser_file.py b/parser_file.py
--- a/parser_file.py	
+++ b/parser_file.py	
@@ -51,9 +51,9 @@
     parser.add_argument('--teachingratio', default=0.0, type=float,
                         help="The probability of using ground truth future trajectories instead of model predictions during training")
     # architecture (VE)
-    parser.add_argument('--latent_dim', type=int, default=2, help="Dimension of latent selection variables")
-    parser.add_argument("--z_dim", type=int, default=64, help="Dimension of z latent variable")
-    parser.add_argument("--s_dim", type=int, default=2, help="Dimension of s latent variable")
+    parser.add_argument('--latent_dim', type=int, default=16, help="Dimension of latent selection variables")
+    parser.add_argument("--z_dim", type=int, default=16, help="Dimension of z latent variable")
+    parser.add_argument("--s_dim", type=int, default=16, help="Dimension of s latent variable")
 
     # computation
     parser.add_argument("--loader_num_workers", default=6, type=int)
@@ -61,7 +61,7 @@
 
     # training
     parser.add_argument("--best_k", default=20, type=int)
-    parser.add_argument("--batch_size", default='16', type=str)
+    parser.add_argument("--batch_size", default='64', type=str)
     parser.add_argument("--batch_method", default='hom', type=str,
                         help='Use Homogeneous (hom), Heterogeneous (het) or alternated homogeneous (alt) batches during training')
     parser.add_argument("--shuffle", default=True, type=bool)
@@ -76,7 +76,7 @@
 
     # general training
     parser.add_argument("--finetune", default="", type=str)
-    parser.add_argument("--num_epochs", default='150-100-50-100-100-50', type=lambda x: int_tuple(x, '-'))  # '150-100-150',
+    parser.add_argument("--num_epochs", default='200-400-100-100-50', type=lambda x: int_tuple(x, '-'))  # '150-100-150',
     parser.add_argument("--resume", default="",
                         type=str, metavar="PATH", help="path to latest checkpoint (default: none)")
 
